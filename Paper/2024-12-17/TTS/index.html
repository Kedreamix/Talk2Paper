<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2024-12-17  Efficient Generative Modeling with Residual Vector Quantization-Based   Tokens">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-fb8685059f388de8b82e0bc6c456ffc6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    66 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-17-更新"><a href="#2024-12-17-更新" class="headerlink" title="2024-12-17 更新"></a>2024-12-17 更新</h1><h2 id="Efficient-Generative-Modeling-with-Residual-Vector-Quantization-Based-Tokens"><a href="#Efficient-Generative-Modeling-with-Residual-Vector-Quantization-Based-Tokens" class="headerlink" title="Efficient Generative Modeling with Residual Vector Quantization-Based   Tokens"></a>Efficient Generative Modeling with Residual Vector Quantization-Based   Tokens</h2><p><strong>Authors:Jaehyeon Kim, Taehong Moon, Keon Lee, Jaewoong Cho</strong></p>
<p>We explore the use of Residual Vector Quantization (RVQ) for high-fidelity generation in vector-quantized generative models. This quantization technique maintains higher data fidelity by employing more in-depth tokens. However, increasing the token number in generative models leads to slower inference speeds. To this end, we introduce ResGen, an efficient RVQ-based discrete diffusion model that generates high-fidelity samples without compromising sampling speed. Our key idea is a direct prediction of vector embedding of collective tokens rather than individual ones. Moreover, we demonstrate that our proposed token masking and multi-token prediction method can be formulated within a principled probabilistic framework using a discrete diffusion process and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation} on ImageNet 256x256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models. The project page can be found at <a target="_blank" rel="noopener" href="https://resgen-genai.github.io/">https://resgen-genai.github.io</a> </p>
<blockquote>
<p>我们探索了残差向量量化（RVQ）在高保真生成向量量化生成模型中的应用。这种量化技术通过采用更深入的令牌来保持更高的数据保真度。然而，在生成模型中增加令牌数量会导致推理速度变慢。为此，我们引入了ResGen，这是一种基于RVQ的高效离散扩散模型，能够在不牺牲采样速度的情况下生成高保真样本。我们的关键想法是预测集体令牌的向量嵌入，而不是单个令牌。此外，我们证明，通过离散扩散过程和变分推断，我们提出的令牌遮挡和多令牌预测方法可以在有原则的概率框架内制定。我们在两个不同的模态的挑战性任务上验证了所提出方法的有效性和通用性：在ImageNet 256x256上进行条件图像生成和零样本文本到语音合成。实验结果表明，ResGen在这两项任务中的表现均优于自回归模型，在不牺牲采样速度的情况下实现了卓越的性能。此外，随着我们扩大RVQ的深度，我们的生成模型与类似规模的基准模型相比，在生成保真度或采样速度方面表现出优势。项目页面可在<a target="_blank" rel="noopener" href="https://resgen-genai.github.io找到./">https://resgen-genai.github.io找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10208v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本摘要介绍了Residual Vector Quantization（RVQ）在高保真生成向量量化生成模型中的应用。文章指出，RVQ技术通过采用更深入的标记来维持更高的数据保真度，但增加生成模型中的标记数量会导致推理速度变慢。为此，文章提出了一种高效的基于RVQ的离散扩散模型ResGen，能够在不降低采样速度的情况下生成高保真样本。其主要思想是直接预测集体标记的向量嵌入，而不是单独预测。此外，文章还展示了通过离散扩散过程和变分推断来制定该方法的原理性概率框架。通过在不同模态的两个挑战性任务上验证方法的有效性：在ImageNet 256x256上进行条件图像生成和零样本文本到语音合成。实验结果表明，ResGen在两项任务中的表现均优于自回归模型，且在不降低采样速度的情况下实现了优越的性能。随着RVQ深度的增加，我们的生成模型的生成保真度会提高或采样速度会更快，相较于类似规模的基准模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究使用Residual Vector Quantization (RVQ)在高保真生成向量量化生成模型中的应用。</li>
<li>RVQ技术通过采用更深入的标记维持高数据保真度，但增加标记数量会降低推理速度。</li>
<li>提出一种高效的基于RVQ的离散扩散模型ResGen，能在不降低采样速度的情况下生成高保真样本。</li>
<li>ResGen通过直接预测集体标记的向量嵌入来提高效率。</li>
<li>该方法可通过离散扩散过程和变分推断的框架进行原理性概率表述。</li>
<li>在条件图像生成和零样本文本到语音合成任务上验证了方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10208">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文试图解决的问题是如何在向量量化生成模型中实现高保真度数据生成，同时保持较快的采样速度。具体来说，论文探讨了使用残差向量量化（Residual Vector Quantization, RVQ）技术来提高数据重建质量，同时提出了一个高效的RVQ基的离散扩散模型（ResGen），用于在不牺牲采样速度的情况下生成高保真度样本。论文的主要贡献包括：

<ol>
<li><p><strong>直接预测集体标记的向量嵌入</strong>：ResGen通过直接预测集体标记的向量嵌入而不是逐个预测每个标记，从而解耦了采样复杂度与序列长度和深度的关系。</p>
</li>
<li><p><strong>基于离散扩散过程和变分推断的概率框架</strong>：论文展示了所提出的标记掩蔽和多标记预测方法可以被纳入一个基于离散扩散过程和变分推断的原则化概率框架中。</p>
</li>
<li><p><strong>在不同模态上验证方法的有效性和泛化能力</strong>：通过在ImageNet上的有条件图像生成任务和零样本文本到语音合成任务上的实验，论文证明了ResGen在这些任务上相较于自回归模型具有优越的性能，且没有牺牲采样速度。</p>
</li>
<li><p><strong>分析模型特性和超参数的影响</strong>：通过消融研究，论文还分析了不同超参数（如采样步骤）对生成质量的影响，并考察了模型在不同RVQ深度下的特性。</p>
</li>
</ol>
<p>总的来说，这篇论文旨在通过引入新的RVQ基生成模型ResGen，解决在生成高分辨率、高保真度数据时面临的质量和计算效率之间的平衡问题。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，以下是与本研究相关的一些工作：

<ol>
<li><p><strong>向量量化（VQ）标记基础的生成模型</strong>：</p>
<ul>
<li>VQ-GAN (Esser et al., 2021) 和 DALLE (Ramesh et al., 2021) 利用离散表示进行图像合成，使用变换器（transformers）来实现高质量的数据生成，同时保持计算资源的可管理性。</li>
</ul>
</li>
<li><p><strong>离散扩散模型</strong>：</p>
<ul>
<li>MaskGIT (Chang et al., 2022) 和 VQ-Diffusion (Gu et al., 2022) 专注于对平面标记序列进行掩蔽标记预测，与自回归模型相比提高了采样效率。</li>
<li>GIVT (Tschannen et al., 2023) 引入了一种方法，用基于高斯混合的向量预测代替基于softmax的标记预测，逐步填充掩蔽位置。</li>
</ul>
</li>
<li><p><strong>基于RVQ的生成模型</strong>：</p>
<ul>
<li>RQ-Transformer (Lee et al., 2022) 是第一个展示在RVQ标记上使用自回归模型进行生成建模的研究，其计算复杂度随着序列长度和深度的乘积增加。</li>
<li>Vall-E (Wang et al., 2023) 预测第一深度的标记，然后顺序地预测每个深度的剩余标记。</li>
<li>SoundStorm (Borsos et al., 2023) 使用掩蔽标记预测给定语义标记生成标记，但其采样时间复杂度仍随残差量化深度线性增加。</li>
<li>NaturalSpeech 2 (Shen et al., 2024) 在RVQ嵌入空间中使用基于扩散的生成建模而不是标记生成。</li>
<li>CLaM-TTS (Kim et al., 2024) 使用向量预测进行多标记预测，但以自回归方式沿序列长度操作。</li>
</ul>
</li>
<li><p><strong>其他生成模型方法</strong>：</p>
<ul>
<li>VAR (Tian et al., 2024) 和 MAR (Li et al., 2024) 提出了替代基于标记的自回归建模的新范式。VAR引入了粗到细的下一尺度预测机制，有效地捕获了图像中的层次结构。MAR通过使用基于扩散的方法在连续值空间中对概率进行建模，消除了对离散标记的依赖，简化了流程同时保持了强大的性能。</li>
</ul>
</li>
</ol>
<p>这些相关工作提供了不同的视角和技术，用于处理基于标记的生成模型中的挑战，特别是在处理高分辨率、高保真度数据生成时的质量和计算效率之间的平衡问题。ResGen方法与这些工作相比，提供了一个更有效的解决方案，通过预测掩蔽标记的向量嵌入，从序列长度和标记深度中解耦采样时间复杂度。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过提出一个名为ResGen的高效RVQ（Residual Vector Quantization）基的离散扩散模型来解决高保真度数据生成与快速采样之间的平衡问题。以下是ResGen解决该问题的关键方法：

<h3 id="1-直接预测集体标记的向量嵌入"><a href="#1-直接预测集体标记的向量嵌入" class="headerlink" title="1. 直接预测集体标记的向量嵌入"></a>1. 直接预测集体标记的向量嵌入</h3><p>ResGen的核心思想是直接预测集体标记的向量嵌入，而不是逐个预测每个标记。这种方法通过对累积嵌入的预测，允许模型在不同深度上估计相关联的标记，与残差量化过程自然对齐。这样，ResGen能够从序列长度和深度中解耦采样复杂度，生成高保真度样本而不影响采样速度。</p>
<h3 id="2-掩蔽标记策略和多标记预测"><a href="#2-掩蔽标记策略和多标记预测" class="headerlink" title="2. 掩蔽标记策略和多标记预测"></a>2. 掩蔽标记策略和多标记预测</h3><ul>
<li><p><strong>掩蔽标记策略</strong>：ResGen采用一种特别适合RVQ标记的掩蔽策略，从最高量化层开始逐步掩蔽标记，利用RVQ的层次性质，其中更深层次的标记捕获更精细的细节。</p>
</li>
<li><p><strong>多标记预测</strong>：ResGen在训练和解码阶段通过聚焦于预测聚合向量嵌入z而不是直接预测目标标记，有效预测掩蔽标记。这种方法避免了沿深度的标记条件独立性，这对于模型性能可能是有害的。</p>
</li>
</ul>
<h3 id="3-基于离散扩散过程和变分推断的概率框架"><a href="#3-基于离散扩散过程和变分推断的概率框架" class="headerlink" title="3. 基于离散扩散过程和变分推断的概率框架"></a>3. 基于离散扩散过程和变分推断的概率框架</h3><p>ResGen将掩蔽标记预测过程纳入一个基于离散扩散模型和变分推断的原则化概率框架中。这种观点允许将ResGen视为一个基于似然的生成过程，并为其设计提供了理论基础。</p>
<ul>
<li><p><strong>前向离散扩散过程</strong>：将标记掩蔽过程解释为离散扩散模型在标记序列上的前向扩散步骤，逐渐将完全未掩蔽的标记序列转化为完全掩蔽的序列。</p>
</li>
<li><p><strong>反向离散扩散过程</strong>：旨在从部分掩蔽的序列中重建原始标记，定义了重建原始标记的概率，并计算数据对数似然的变分下界。</p>
</li>
</ul>
<h3 id="4-实验验证"><a href="#4-实验验证" class="headerlink" title="4. 实验验证"></a>4. 实验验证</h3><p>论文通过在ImageNet上的有条件图像生成任务和零样本文本到语音合成任务上的实验验证了ResGen的有效性和泛化能力。实验结果显示，ResGen在这些任务上的性能超过了自回归模型，并且在RVQ深度扩展时，与同样大小的基线生成模型相比，展现了更高的生成保真度或更快的采样速度。</p>
<h3 id="5-消融研究"><a href="#5-消融研究" class="headerlink" title="5. 消融研究"></a>5. 消融研究</h3><p>论文还通过消融研究分析了不同超参数（例如采样步骤）对生成质量的影响，并考察了模型在不同RVQ深度下的特性。</p>
<p>通过这些方法，ResGen有效地解决了在向量量化生成模型中生成高保真度数据与保持快速采样速度之间的平衡问题。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 论文中进行了以下实验来验证ResGen模型的有效性和泛化能力：

<h3 id="1-实验设置"><a href="#1-实验设置" class="headerlink" title="1. 实验设置"></a>1. 实验设置</h3><ul>
<li><p><strong>实验任务</strong>：</p>
<ul>
<li><strong>视觉领域</strong>：在ImageNet数据集上进行条件图像生成任务，分辨率为256×256。</li>
<li><strong>音频领域</strong>：使用Voicebox启发的两个任务进行评估：(1) 继续任务（continuation），给定文本和3秒的真实语音段落，目标是生成与提供段落相同风格的无缝语音；(2) 跨句子任务（cross-sentence），给定文本、3秒语音段落及其与文本不同的转录，目标是生成用提供段落风格朗读文本的语音。</li>
</ul>
</li>
<li><p><strong>评估指标</strong>：</p>
<ul>
<li><strong>视觉任务</strong>：使用Fréchet Inception Distance (FID) 来比较与其他最先进图像生成模型。</li>
<li><strong>音频任务</strong>：使用字符错误率（CER）、单词错误率（WER）和说话人相似度（SIM）等客观指标进行评估。</li>
</ul>
</li>
<li><p><strong>基线和训练配置</strong>：与包括自回归模型和非自回归模型在内的最新生成模型进行比较，并详细说明ResGen模型的训练配置。</p>
</li>
</ul>
<h3 id="2-实验结果"><a href="#2-实验结果" class="headerlink" title="2. 实验结果"></a>2. 实验结果</h3><ul>
<li><p><strong>视觉任务</strong>：</p>
<ul>
<li>展示了ResGen在保持参数效率的同时，与基线模型相比在准确性和推理时间上的优势。</li>
<li>使用FID指标评估生成质量，比较不同模型在生成质量、内存效率和生成速度方面的表现。</li>
</ul>
</li>
<li><p><strong>音频任务</strong>：</p>
<ul>
<li>与生成RVQ标记的自回归模型进行比较，展示了ResGen在WER、CER和SIM指标上的优势，以及需要更少的推理步骤。</li>
<li>与最新的TTS模型进行比较，展示了ResGen在计算效率和准确性方面的优势。</li>
</ul>
</li>
</ul>
<h3 id="3-消融研究"><a href="#3-消融研究" class="headerlink" title="3. 消融研究"></a>3. 消融研究</h3><ul>
<li>分析了不同采样超参数（如采样步骤数、top-p值和温度缩放）对模型行为和生成质量的影响。</li>
<li>展示了增加采样步骤数、调整top-p值和温度对生成质量的改善。</li>
</ul>
<p>这些实验验证了ResGen在不同模态上的强大性能，包括图像生成和文本到语音合成任务，并与现有的自回归和非自回归方法进行了比较。实验结果显示，ResGen在保持高保真度的同时，提供了更快的采样速度和更好的内存效率。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，以下是一些可以进一步探索的点：

<h3 id="1-利用键值（KV）缓存优化采样速度"><a href="#1-利用键值（KV）缓存优化采样速度" class="headerlink" title="1. 利用键值（KV）缓存优化采样速度"></a>1. 利用键值（KV）缓存优化采样速度</h3><p>论文提到，通过在变换器架构中使用键值（KV）缓存，可以重用已经完全填充的标记的预计算KV值，减少冗余计算。这种策略可以显著提高采样速度和降低整体计算开销，是一个值得未来研究的方向。</p>
<h3 id="2-探索新的量化方法"><a href="#2-探索新的量化方法" class="headerlink" title="2. 探索新的量化方法"></a>2. 探索新的量化方法</h3><p>论文提到了残差向量量化（RVQ）标记，但也提到了有限标量量化（FSQ）可能带来额外的好处。将ResGen方法扩展到支持FSQ可能需要不同的标记化和嵌入过程，探索这一方向可能带来新的量化策略和改进的生成性能。</p>
<h3 id="3-提供理论解释"><a href="#3-提供理论解释" class="headerlink" title="3. 提供理论解释"></a>3. 提供理论解释</h3><p>论文中提到，ResGen方法在相对较少的迭代次数下就实现了高质量的生成，相比传统的扩散模型更有效。论文假设这种效率来自于解掩过程而不是去噪过程，因为基于完全解掩的标记进行预测比基于噪声输入的预测更容易。尽管如此，论文缺乏对这一现象的理论证明，提供正式的理论分析解释是一个有前景的未来研究方向。</p>
<h3 id="4-扩展到其他模态和任务"><a href="#4-扩展到其他模态和任务" class="headerlink" title="4. 扩展到其他模态和任务"></a>4. 扩展到其他模态和任务</h3><p>论文中的实验主要集中在图像和语音合成任务上，但ResGen模型的框架是通用的。将其扩展到其他模态（如视频或更复杂的音频任务）和不同类型的生成任务（如无条件生成或交互式生成）是另一个值得探索的方向。</p>
<h3 id="5-提高模型的泛化能力"><a href="#5-提高模型的泛化能力" class="headerlink" title="5. 提高模型的泛化能力"></a>5. 提高模型的泛化能力</h3><p>虽然ResGen在特定任务上表现出色，但提高模型对不同领域和分布的泛化能力始终是一个重要的研究方向。这可能包括在更多样化的数据集上训练模型，或者开发更先进的技术来适应不同的生成场景。</p>
<h3 id="6-优化模型的计算效率"><a href="#6-优化模型的计算效率" class="headerlink" title="6. 优化模型的计算效率"></a>6. 优化模型的计算效率</h3><p>尽管ResGen在内存效率和采样速度方面已经表现出色，但进一步优化模型的计算效率，特别是在资源受限的环境中，仍然是一个重要的研究方向。</p>
<h3 id="7-探索不同的训练策略"><a href="#7-探索不同的训练策略" class="headerlink" title="7. 探索不同的训练策略"></a>7. 探索不同的训练策略</h3><p>论文中提到了一些训练策略，如使用混合高斯分布来表示潜在嵌入的分布。探索不同的训练策略和正则化技术，以进一步提高模型的性能和稳定性，也是一个有价值的研究方向。</p>
<p>这些方向不仅可以推动ResGen模型的发展，也有助于推动整个生成模型领域的进步。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文提出了一个名为ResGen的高效RVQ（Residual Vector Quantization）基的离散扩散模型，旨在解决在向量量化生成模型中生成高保真度数据与保持快速采样速度之间的平衡问题。以下是论文的主要内容总结：

<h3 id="1-研究动机"><a href="#1-研究动机" class="headerlink" title="1. 研究动机"></a>1. 研究动机</h3><ul>
<li>向量量化生成模型在高保真度数据生成方面面临挑战，尤其是在平衡生成质量和计算效率方面。</li>
</ul>
<h3 id="2-ResGen模型"><a href="#2-ResGen模型" class="headerlink" title="2. ResGen模型"></a>2. ResGen模型</h3><ul>
<li><strong>核心思想</strong>：直接预测集体标记的向量嵌入而不是逐个预测每个标记，以解耦采样复杂度与序列长度和深度的关系。</li>
<li><strong>掩蔽标记策略</strong>：采用特别适合RVQ标记的掩蔽策略，从最高量化层开始逐步掩蔽标记。</li>
<li><strong>多标记预测</strong>：通过聚焦于预测聚合向量嵌入而不是直接预测目标标记，有效预测掩蔽标记。</li>
</ul>
<h3 id="3-概率框架"><a href="#3-概率框架" class="headerlink" title="3. 概率框架"></a>3. 概率框架</h3><ul>
<li>将掩蔽标记预测过程纳入基于离散扩散模型和变分推断的原则化概率框架中，提供了理论基础。</li>
</ul>
<h3 id="4-实验验证-1"><a href="#4-实验验证-1" class="headerlink" title="4. 实验验证"></a>4. 实验验证</h3><ul>
<li>在ImageNet上的有条件图像生成任务和零样本文本到语音合成任务上的实验验证了ResGen的有效性和泛化能力。</li>
<li>实验结果显示，ResGen在这些任务上的性能超过了自回归模型，并且在RVQ深度扩展时，与同样大小的基线生成模型相比，展现了更高的生成保真度或更快的采样速度。</li>
</ul>
<h3 id="5-消融研究-1"><a href="#5-消融研究-1" class="headerlink" title="5. 消融研究"></a>5. 消融研究</h3><ul>
<li>分析了不同采样超参数对模型行为和生成质量的影响，包括采样步骤数、top-p值和温度缩放。</li>
</ul>
<h3 id="6-结论与未来方向"><a href="#6-结论与未来方向" class="headerlink" title="6. 结论与未来方向"></a>6. 结论与未来方向</h3><ul>
<li>ResGen通过直接预测集体标记的向量嵌入，有效地解决了生成高保真度数据与快速采样之间的平衡问题。</li>
<li>提出了进一步探索的方向，包括利用键值缓存优化采样速度、探索新的量化方法、提供理论解释、扩展到其他模态和任务等。</li>
</ul>
<p>总的来说，这篇论文提出了一个创新的解决方案来提高向量量化生成模型的效率和保真度，并在多个任务上验证了其有效性，为未来的研究提供了新的方向。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient Generative Modeling with Residual Vector Quantization-Based Tokens，链接是 https://arxiv.org/pdf/2412.10208 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2412.10208 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e2e7bb56de76aee2b4d2d345f6953248.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbbc45c9a53d87922d7eabae1e2a6beb.jpg" align="middle">
</details>



<h2 id="CosyVoice-2-Scalable-Streaming-Speech-Synthesis-with-Large-Language-Models"><a href="#CosyVoice-2-Scalable-Streaming-Speech-Synthesis-with-Large-Language-Models" class="headerlink" title="CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language   Models"></a>CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language   Models</h2><p><strong>Authors:Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, Jingren Zhou</strong></p>
<p>In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at <a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice2">https://funaudiollm.github.io/cosyvoice2</a>. </p>
<blockquote>
<p>在之前的工作中，我们介绍了CosyVoice，这是一个基于监督离散语音标记的多语种语音合成模型。通过采用两种流行的生成模型——语言模型和流匹配，进行渐进式语义解码，CosyVoice在语境中学习语音时表现出了高度的语调自然性、内容一致性和说话人相似性。最近，多模态大型语言模型（LLM）取得了显著进展，其中语音合成的响应延迟和实时因子在交互体验中发挥了关键作用。因此，在本报告中，我们提出了一种改进的流式语音合成模型CosyVoice 2，它包含了全面和系统的优化。具体来说，我们引入有限标量量化来提高语音标记的码本利用率。对于文本-语音LM，我们简化了模型架构，允许直接使用预训练的大型语言模型作为骨干。此外，我们开发了一种块感知因果流匹配模型，以支持各种合成场景，能够在单个模型内实现流式和非流式合成。通过在大规模多语种数据集上进行训练，CosyVoice 2达到了与人类相当的自然度、极短的响应延迟和几乎无损的合成质量。我们邀请读者在<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice">https://funaudiollm.github.io/cosyvoice</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10117v1">PDF</a> Tech report, work in progress</p>
<p><strong>摘要</strong></p>
<p>CosyVoice 2是一款优化的流式语音合成模型，它引入了有限标量量化改进语音令牌的代码本利用率，简化了文本语音LM模型架构以直接使用预训练的大型语言模型作为主干，并开发了分块感知因果流匹配模型以支持各种合成场景。在大型多语种数据集上训练后，CosyVoice 2实现了与人类自然度相当、响应延迟极小、流式传输模式下几乎无损的合成质量。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>CosyVoice 2是CosyVoice模型的改进版本，专注于流式语音合成。</li>
<li>该模型引入了有限标量量化以提高语音令牌的代码本利用率。</li>
<li>模型架构简化，允许直接使用预训练的大型语言模型（LLM）。</li>
<li>开发了分块感知因果流匹配模型，支持各种合成场景。</li>
<li>模型能够在单一模型中实现流式和非流式合成。</li>
<li>CosyVoice 2在大型多语种数据集上训练，达到人类自然度相当的水平。</li>
<li>模型具有极小的响应延迟，且流式传输模式下的合成质量几乎无损。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10117">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文介绍了一个名为CosyVoice 2的改进型流式语音合成模型，旨在解决以下问题：

<ol>
<li><p><strong>实时性和响应延迟</strong>：在交互式体验中，如语音聊天应用，响应延迟和实时性对于用户体验至关重要。传统的非流式（离线）语音合成模型在输入完整文本后需要合成整个话语才能返回波形，导致高延迟。</p>
</li>
<li><p><strong>多模态大型语言模型（LLMs）的应用</strong>：随着多模态大型语言模型（LLMs）的显著进展，如何将这些模型有效地应用于语音合成，尤其是在流式合成场景中，成为一个挑战。</p>
</li>
<li><p><strong>语音合成质量</strong>：尽管最近的零样本（zero-shot）TTS模型在特定说话者上实现了高保真度和自然度，但它们通常以非流式模式运行，影响了实时性和用户体验。</p>
</li>
<li><p><strong>多语言和跨语言能力</strong>：现有模型在处理多语言和跨语言数据时可能会遇到性能下降的问题，尤其是在具有字符集重叠的语言之间。</p>
</li>
<li><p><strong>控制性和定制化</strong>：现有模型难以通过文本指令控制声学特征（如音色）以适应角色扮演等应用场景，且在唱歌等任务上表现不佳。</p>
</li>
<li><p><strong>模型架构和优化</strong>：为了实现流式和非流式合成的统一，并提高语音合成的自然度、内容一致性和说话者相似性，需要对模型架构进行简化和优化。</p>
</li>
</ol>
<p>综上所述，CosyVoice 2通过引入有限标量量化（FSQ）以提高语音标记的码本利用率，简化文本到语音的语言模型架构以直接使用预训练的LLM作为骨干网络，并开发块感知因果流匹配模型以支持多样化的合成场景，旨在实现高质量、可扩展且互动性强的文本到语音合成。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 根据这篇论文的内容，相关研究可以分为以下几个领域：

<ol>
<li><p><strong>神经文本到语音（TTS）合成模型</strong>：</p>
<ul>
<li>近年来，神经TTS合成模型因其在特定说话者上实现高保真度和自然度而受到广泛关注，超越了传统的拼接和统计参数方法。</li>
</ul>
</li>
<li><p><strong>零样本TTS模型</strong>：</p>
<ul>
<li>零样本TTS模型能够通过模仿参考语音的音色、韵律和风格来合成任何说话者的语音，这些模型从大规模训练数据中受益，实现了与人类语音几乎无法区分的合成质量和自然度。</li>
</ul>
</li>
<li><p><strong>基于编码器的语言模型</strong>：</p>
<ul>
<li>这类模型使用语音编码器模型提取离散的语音表示，并采用自回归或掩蔽的语言模型来预测语音标记，然后将这些标记通过编码器声码器合成为波形。</li>
</ul>
</li>
<li><p><strong>非自回归（NAR）语音合成</strong>：</p>
<ul>
<li>受到图像生成技术启发，去噪扩散和流匹配模型被引入到NAR语音合成中，以提高语音质量和多样性。</li>
</ul>
</li>
<li><p><strong>混合系统</strong>：</p>
<ul>
<li>结合文本到编码器的语言模型和编码器到特征的扩散模型，利用两种生成模型的优势，实现高多样性、韵律一致性和语音质量。</li>
</ul>
</li>
<li><p><strong>流式合成</strong>：</p>
<ul>
<li>为了减少延迟并改善用户体验，研究者探索了基于语言模型的零样本TTS模型的流式合成。</li>
</ul>
</li>
<li><p><strong>多模态大型语言模型（LLMs）</strong>：</p>
<ul>
<li>随着LLMs的显著进展，如何将这些模型有效地应用于语音合成，尤其是在流式合成场景中，成为一个挑战。</li>
</ul>
</li>
</ol>
<p>具体到论文中提及的一些研究工作，包括但不限于：</p>
<ul>
<li><strong>Tacotron</strong> [1]：一个端到端的语音合成系统，将文本直接转换为波形。</li>
<li><strong>WaveNet</strong> [2]：基于深度学习的语音合成系统，能够生成高质量的语音。</li>
<li><strong>Deep Voice 3</strong> [3]：一个能够处理大量说话者的神经文本到语音模型。</li>
<li><strong>FastSpeech</strong> [5]：一个快速且可控的文本到语音模型。</li>
<li><strong>VALL-E</strong> [14]：一个基于神经编解码器的语言模型，实现了零样本文本到语音合成。</li>
<li><strong>DiffWave</strong> [22]：一个基于去噪扩散概率模型的语音合成系统。</li>
<li><strong>Flow Matching</strong> [24]：一种用于生成建模的流匹配方法。</li>
</ul>
<p>这些研究构成了CosyVoice 2的研究背景，并为其提供了理论和技术基础。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过提出CosyVoice 2模型解决了上述问题，具体解决方案如下：

<h3 id="1-统一流式和非流式合成框架"><a href="#1-统一流式和非流式合成框架" class="headerlink" title="1. 统一流式和非流式合成框架"></a>1. 统一流式和非流式合成框架</h3><p>CosyVoice 2通过提出统一的文本-语音语言模型和块感知因果流匹配模型，实现了流式和非流式合成的统一。这意味着模型可以灵活地在流式和非流式模式之间切换，从而减少部署复杂性，并适应不同的应用场景。</p>
<h3 id="2-有限标量量化（FSQ）"><a href="#2-有限标量量化（FSQ）" class="headerlink" title="2. 有限标量量化（FSQ）"></a>2. 有限标量量化（FSQ）</h3><p>为了提高码本利用率并捕获更多的语音信息，CosyVoice 2采用了有限标量量化（FSQ）来替代传统的向量量化（VQ）。FSQ通过将中间表示投影到低维空间并量化，从而更有效地利用码本并保持语音信息。</p>
<h3 id="3-简化的语言模型架构"><a href="#3-简化的语言模型架构" class="headerlink" title="3. 简化的语言模型架构"></a>3. 简化的语言模型架构</h3><p>CosyVoice 2简化了语言模型架构，通过移除文本编码器和说话者嵌入，允许直接使用预训练的大型文本语言模型（LLMs）作为骨干网络。这增强了模型对上下文的理解能力，并避免了信息泄露。</p>
<h3 id="4-块感知因果流匹配模型"><a href="#4-块感知因果流匹配模型" class="headerlink" title="4. 块感知因果流匹配模型"></a>4. 块感知因果流匹配模型</h3><p>为了支持流式合成，CosyVoice 2开发了块感知因果流匹配模型，该模型可以处理不同长度的语音标记和Mel特征之间的帧率不匹配问题。通过使用最优传输（OT）流来匹配概率密度路径，模型能够在流式模式下有效地合成Mel频谱图。</p>
<h3 id="5-增强的指令性TTS容量"><a href="#5-增强的指令性TTS容量" class="headerlink" title="5. 增强的指令性TTS容量"></a>5. 增强的指令性TTS容量</h3><p>CosyVoice 2通过整合指令性数据集到基础训练集中，增强了模型对情感、口音、角色风格和细粒度控制的指令性生成能力。这使得用户能够更灵活地生成多样化和生动的语音。</p>
<h3 id="6-多说话者微调（mSFT）"><a href="#6-多说话者微调（mSFT）" class="headerlink" title="6. 多说话者微调（mSFT）"></a>6. 多说话者微调（mSFT）</h3><p>为了改善特定说话者的生成质量和说话者相似性，CosyVoice 2引入了多说话者微调（mSFT），在多个说话者上同时微调预训练模型，以保持全面的韵律和发音覆盖，并减少对预训练模型的灾难性遗忘。</p>
<h3 id="7-强化学习用于SFT"><a href="#7-强化学习用于SFT" class="headerlink" title="7. 强化学习用于SFT"></a>7. 强化学习用于SFT</h3><p>CosyVoice 2在微调阶段采用强化学习，使用说话者相似性（SS）和识别词错误率（WER）作为奖励函数，以改善说话者相似性和发音准确性。</p>
<p>通过这些系统性的修改和优化，CosyVoice 2不仅实现了高质量的合成效果，还降低了部署要求，使其适用于流式和非流式应用，代表了可扩展、高质量和互动文本到语音合成的重要进步。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 论文中进行了一系列实验来评估CosyVoice 2的性能，以下是实验的详细情况：

<h3 id="1-训练数据"><a href="#1-训练数据" class="headerlink" title="1. 训练数据"></a>1. 训练数据</h3><ul>
<li><strong>Speech Tokenizer训练数据</strong>：使用了包含中文和英文数据的200,000小时数据集来训练Speech Tokenizer。</li>
<li><strong>CosyVoice 2训练数据</strong>：与前一个版本共享训练数据，使用内部语音处理工具生成伪文本标签，并使用内部对齐模型过滤低质量数据。</li>
</ul>
<h3 id="2-评估设置"><a href="#2-评估设置" class="headerlink" title="2. 评估设置"></a>2. 评估设置</h3><ul>
<li><strong>测试集</strong>：使用了两个测试集，一个是基于Librispeech的test-clean，另一个是在SEED设置下进行的更广泛的测试，包括中文和英文样本。</li>
<li><strong>基准测试</strong>：为日语和韩语准备了测试集，以评估模型在这些语言上的性能。</li>
</ul>
<h3 id="3-评估指标"><a href="#3-评估指标" class="headerlink" title="3. 评估指标"></a>3. 评估指标</h3><ul>
<li><strong>内容一致性</strong>：使用词错误率（WER）或字符错误率（CER）来评估。</li>
<li><strong>说话者相似性</strong>：通过计算提示和生成话语的说话者嵌入之间的余弦相似度来评估。</li>
<li><strong>语音质量</strong>：使用自然度MOS（NMOS）分数来评估客观质量。</li>
</ul>
<h3 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4. 实验结果"></a>4. 实验结果</h3><ul>
<li><strong>Speech Tokenizer评估</strong>：比较了向量量化（VQ）和有限标量量化（FSQ）在SenseVoice-Large编码器中的效果，发现FSQ能够完全利用码本并保持更多的有效信息。</li>
<li><strong>与基线比较</strong>：在Librispeech test-clean子集上，CosyVoice 2在所有评估指标上均优于多个基线模型，显示出与人类话语相当的合成质量。</li>
<li><strong>模块消融研究</strong>：通过逐步分析文本到语音语言模型的修改，包括LLM初始化、移除说话者嵌入和使用FSQ，发现这些修改显著提高了内容一致性，同时保持了说话者相似性。</li>
<li><strong>流式模块影响评估</strong>：评估了流式语言模型和流式流匹配模型对合成性能的影响，发现流式LM在典型案例中的影响较小，但在具有挑战性的案例中可能会失去一些上下文信息。</li>
<li><strong>日语和韩语基准测试</strong>：CosyVoice 2在韩语上的表现明显优于日语，这可能与日语和中文之间的字符集重叠有关。</li>
<li><strong>指令性生成评估</strong>：创建了一个包含290个样本的中文测试集，评估了CosyVoice 2在遵循指令方面的性能，包括情感、语速、方言和角色扮演等。</li>
<li><strong>说话者微调模型评估</strong>：展示了即使只有400个音频记录，目标说话者也能实现合理的语音合成性能。</li>
<li><strong>强化学习用于SFT</strong>：通过使用不同的奖励函数对特定说话者进行微调，评估了强化学习在改善说话者相似性和发音准确性方面的有效性。</li>
</ul>
<p>这些实验全面评估了CosyVoice 2在不同方面的表现，并与现有的基线模型进行了比较，证明了其在流式语音合成领域的优越性能。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，以下是一些可以进一步探索的点：

<ol>
<li><p><strong>支持更多语言</strong>：</p>
<ul>
<li>扩展CosyVoice 2以支持更多的语言，尤其是那些与中文、英文等没有字符集重叠的语言，以提高多语言合成的性能。</li>
</ul>
</li>
<li><p><strong>改善音色控制</strong>：</p>
<ul>
<li>研究如何通过文本指令控制声学特征（如音色）以适应角色扮演等应用场景。</li>
</ul>
</li>
<li><p><strong>增强唱歌性能</strong>：</p>
<ul>
<li>针对CosyVoice 2在唱歌任务上的表现不佳的问题，研究如何改进模型以更好地处理音乐和节奏。</li>
</ul>
</li>
<li><p><strong>优化流式合成性能</strong>：</p>
<ul>
<li>进一步优化流式合成中的语言模型和流匹配模型，以减少在具有挑战性的案例中丢失的上下文信息。</li>
</ul>
</li>
<li><p><strong>提高数据效率</strong>：</p>
<ul>
<li>探索数据增强、迁移学习等技术，以减少模型对大量训练数据的依赖，提高数据效率。</li>
</ul>
</li>
<li><p><strong>改进强化学习</strong>：</p>
<ul>
<li>研究如何更有效地结合偏好偏差和可微分的ASR奖励，以提高模型在特定说话者上的微调性能。</li>
</ul>
</li>
<li><p><strong>多模态融合</strong>：</p>
<ul>
<li>探索如何将CosyVoice 2与其他模态（如视觉、情感等）融合，以实现更丰富的交互体验。</li>
</ul>
</li>
<li><p><strong>模型泛化能力</strong>：</p>
<ul>
<li>评估和提高模型在面对新的、未见过的数据时的泛化能力，尤其是在跨语言和跨领域的场景中。</li>
</ul>
</li>
<li><p><strong>实时应用优化</strong>：</p>
<ul>
<li>针对实时应用（如语音助手、虚拟主播等），进一步优化模型的响应速度和资源消耗。</li>
</ul>
</li>
<li><p><strong>提高模型解释性</strong>：</p>
<ul>
<li>提高模型的可解释性，让研究人员和用户更好地理解模型的决策过程和潜在的偏差。</li>
</ul>
</li>
<li><p><strong>多任务学习</strong>：</p>
<ul>
<li>探索多任务学习框架，将语音合成与其他语音相关任务（如语音识别、语音增强等）结合起来，以提高模型的效能和鲁棒性。</li>
</ul>
</li>
<li><p><strong>模型压缩和加速</strong>：</p>
<ul>
<li>研究模型压缩和加速技术，使CosyVoice 2能够在资源受限的设备上运行，如移动设备和嵌入式系统。</li>
</ul>
</li>
</ol>
<p>这些探索点可以帮助推动语音合成技术的发展，提高模型的性能和适用性，以及拓展其在各种应用场景中的使用。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文介绍了CosyVoice 2，一个改进的流式语音合成模型，旨在实现高质量、可扩展且互动性强的文本到语音合成。以下是论文的主要内容总结：

<h3 id="1-动机与目标"><a href="#1-动机与目标" class="headerlink" title="1. 动机与目标"></a>1. 动机与目标</h3><ul>
<li>针对现有零样本TTS模型在实时性和流式合成方面的局限性，提出了CosyVoice 2模型。</li>
<li>目标是在保持高自然度和说话者相似性的同时，减少响应延迟并实现流式合成。</li>
</ul>
<h3 id="2-CosyVoice-2模型"><a href="#2-CosyVoice-2模型" class="headerlink" title="2. CosyVoice 2模型"></a>2. CosyVoice 2模型</h3><ul>
<li><strong>统一框架</strong>：提出了一个统一的框架，支持流式和非流式合成。</li>
<li><strong>有限标量量化（FSQ）</strong>：采用FSQ改善码本利用率并捕获更多语音信息。</li>
<li><strong>简化的语言模型架构</strong>：使用预训练的大型文本语言模型（LLMs）作为骨干网络，增强上下文理解能力。</li>
<li><strong>块感知因果流匹配模型</strong>：支持不同合成场景，包括流式和非流式合成。</li>
<li><strong>增强的指令性TTS容量</strong>：允许更细致的控制，如情感、口音、角色风格等。</li>
<li><strong>多说话者微调（mSFT）</strong>：在多个说话者上同时微调，提高特定说话者的合成质量。</li>
<li><strong>强化学习用于SFT</strong>：使用ASR和SS奖励优化说话者相似性和发音准确性。</li>
</ul>
<h3 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h3><ul>
<li>在多个测试集上评估CosyVoice 2，包括Librispeech、SEED设置和日语、韩语测试集。</li>
<li>与多个基线模型比较，证明了CosyVoice 2在内容一致性、说话者相似性和语音质量方面的优越性能。</li>
<li>进行了模块消融研究，验证了模型各部分的有效性。</li>
<li>评估了流式合成模块对合成性能的影响。</li>
<li>测试了模型对指令性生成的控制能力。</li>
<li>展示了多说话者微调和强化学习在改善特定说话者合成质量方面的有效性。</li>
</ul>
<h3 id="4-结论与局限性"><a href="#4-结论与局限性" class="headerlink" title="4. 结论与局限性"></a>4. 结论与局限性</h3><ul>
<li>CosyVoice 2通过一系列系统性修改和优化，实现了高质量的合成效果，并降低了部署要求。</li>
<li>论文也指出了模型的局限性，如支持语言数量有限、无法通过文本指令控制声学特征等，并提出了未来研究的方向。</li>
</ul>
<p>总的来说，CosyVoice 2代表了在流式、高质量文本到语音合成领域的一个重要进步，通过结合最新的语言模型技术和系统优化，提供了一个强大且灵活的合成解决方案。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models，链接是 https://arxiv.org/pdf/2412.10117 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2412.10117 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-157afc0141847d69c27ac24ce045c8fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-013cedb83ae1d2a08609293706433d33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb8685059f388de8b82e0bc6c456ffc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf6e7db4f0253c8902f2ba241b051a88.jpg" align="middle">
</details>



<h2 id="AMuSeD-An-Attentive-Deep-Neural-Network-for-Multimodal-Sarcasm-Detection-Incorporating-Bi-modal-Data-Augmentation"><a href="#AMuSeD-An-Attentive-Deep-Neural-Network-for-Multimodal-Sarcasm-Detection-Incorporating-Bi-modal-Data-Augmentation" class="headerlink" title="AMuSeD: An Attentive Deep Neural Network for Multimodal Sarcasm   Detection Incorporating Bi-modal Data Augmentation"></a>AMuSeD: An Attentive Deep Neural Network for Multimodal Sarcasm   Detection Incorporating Bi-modal Data Augmentation</h2><p><strong>Authors:Xiyuan Gao, Shubhi Bansal, Kushaan Gowda, Zhu Li, Shekhar Nayak, Nagendra Kumar, Matt Coler</strong></p>
<p>Detecting sarcasm effectively requires a nuanced understanding of context, including vocal tones and facial expressions. The progression towards multimodal computational methods in sarcasm detection, however, faces challenges due to the scarcity of data. To address this, we present AMuSeD (Attentive deep neural network for MUltimodal Sarcasm dEtection incorporating bi-modal Data augmentation). This approach utilizes the Multimodal Sarcasm Detection Dataset (MUStARD) and introduces a two-phase bimodal data augmentation strategy. The first phase involves generating varied text samples through Back Translation from several secondary languages. The second phase involves the refinement of a FastSpeech 2-based speech synthesis system, tailored specifically for sarcasm to retain sarcastic intonations. Alongside a cloud-based Text-to-Speech (TTS) service, this Fine-tuned FastSpeech 2 system produces corresponding audio for the text augmentations. We also investigate various attention mechanisms for effectively merging text and audio data, finding self-attention to be the most efficient for bimodal integration. Our experiments reveal that this combined augmentation and attention approach achieves a significant F1-score of 81.0% in text-audio modalities, surpassing even models that use three modalities from the MUStARD dataset. </p>
<blockquote>
<p>有效地检测讽刺需要深入理解语境，包括语音和面部表情。然而，由于数据稀缺，讽刺检测的多模态计算方法的发展面临挑战。为了解决这一问题，我们提出了AMuSeD（用于多模态讽刺检测的注意力深度神经网络，并引入双模态数据增强）。该方法利用多模态讽刺检测数据集（MUStARD），并采用两阶段双模态数据增强策略。第一阶段是通过反向翻译从几种次要语言生成不同的文本样本。第二阶段涉及针对讽刺进行定制的FastSpeech 2语音合成系统的改进，以保留讽刺语调。与基于云的文本到语音（TTS）服务相结合，经过精细调整的FastSpeech 2系统为文本增强生成相应的音频。我们还研究了各种注意力机制，以有效地合并文本和音频数据，发现自我注意力对于双模态集成最为有效。我们的实验表明，这种结合增强和注意力的方法实现了文本和音频模态的F1分数达到81.0%，甚至超过了使用MUStARD数据集三模态的模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10103v1">PDF</a> This is a preprint version of the paper, submitted and under review   at the IEEE Transactions on Affective Computing</p>
<p><strong>Summary</strong></p>
<p>基于上下文语境、语音及面部表情来识别讽刺需要敏锐地理解和深入剖析的技术支持。因缺乏讽刺检测的多模态计算方法的必要数据，带来了极大的挑战。为了克服这一问题，本文提出AMuSeD方法，借助多模态讽刺检测数据集MUStARD并采用两阶段双模态数据增强策略。第一阶段利用多种次级语言的翻译手段生成多种文本样本，第二阶段对专门针对讽刺进行定制的FastSpeech 2语音合成系统进行精细化改进并保留讽刺语调。结合云端的文本转语音（TTS）服务，该优化后的FastSpeech 2系统为文本扩充生成相应的音频数据。同时，本文探讨了多种有效的文本和音频数据整合机制，发现自我注意力机制对于双模态融合最为高效。实验证明，该组合策略和注意力机制在文本与音频模态的讽刺检测上取得了显著的F1分数，达到81%，甚至超越了使用MUStARD数据集的三模态模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>有效检测讽刺需要理解上下文语境、语音和面部表情。</li>
<li>数据缺乏是多模态计算方法在讽刺检测中的一大挑战。</li>
<li>提出AMuSeD方法，结合多模态数据集MUStARD进行双模态数据增强。</li>
<li>两阶段数据增强策略包括利用翻译生成文本样本和定制语音合成系统以保留讽刺语调。</li>
<li>利用云TTS服务为文本扩充生成音频数据。</li>
<li>自我注意力机制在整合文本和音频数据上最为高效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10103">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文试图解决的主要问题是有效检测讽刺（sarcasm）的挑战，尤其是在多模态（multimodal）情境中的检测。讽刺是一种复杂的语言现象，它通常传达与字面意思相反的情感或意图，需要对上下文有细致的理解，包括声音的语调和面部表情。论文中提到的关键挑战包括：

<ol>
<li><p><strong>数据稀缺性</strong>：多模态讽刺检测领域面临的一个主要问题是可用于训练和测试模型的数据量有限。</p>
</li>
<li><p><strong>模态整合</strong>：如何有效地整合不同的信息模态（如文本、音频和视觉数据）以提高讽刺检测的准确性是一个关键问题。</p>
</li>
<li><p><strong>上下文理解</strong>：讽刺的理解需要对语境有深入的理解，包括言语和非言语信号，这对于计算方法来说是一个重大挑战。</p>
</li>
</ol>
<p>为了解决这些问题，论文提出了一个名为AMuSeD（Attentive deep neural network for MUltimodal Sarcasm dEtection incorporating bi-modal Data augmentation）的方法，它利用多模态讽刺检测数据集（MUStARD）并引入了一个两阶段的双模态数据增强策略，包括通过回译（Back Translation）生成变化的文本样本和使用基于FastSpeech 2的语音合成系统来保留讽刺的语调。此外，论文还研究了各种注意力机制，以有效地融合文本和音频数据，并发现自注意力（self-attention）对于双模态集成最为高效。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 根据提供的论文内容，相关研究可以分为以下几个领域：

<h3 id="多模态讽刺检测（Multimodal-Sarcasm-Detection）"><a href="#多模态讽刺检测（Multimodal-Sarcasm-Detection）" class="headerlink" title="多模态讽刺检测（Multimodal Sarcasm Detection）"></a>多模态讽刺检测（Multimodal Sarcasm Detection）</h3><ol>
<li><p><strong>晚期融合方法（Late Fusion Approach）</strong>：</p>
<ul>
<li>Schifanella等人 [7] 研究了社交媒体中文本和图像之间的相互作用，并强调了这些模态在传达讽刺对比信息中的关键作用。</li>
<li>Ding等人 [20] 提出了一个融合框架，将文本、音频和视频模态的特征串联起来进行处理。</li>
</ul>
</li>
<li><p><strong>早期融合方法（Early Fusion Approach）</strong>：</p>
<ul>
<li>Castro等人 [8] 将文本、音频和视频特征结合起来，输入支持向量机（SVM）以评估多模态数据整合在讽刺检测中的有效性。</li>
</ul>
</li>
<li><p><strong>基于注意力的融合（Attention-based Fusion）</strong>：</p>
<ul>
<li>Chauhan等人 [22] 应用注意力机制来学习不同模态中话语片段之间的关系。</li>
<li>Zhang等人 [23] 使用对比注意力机制捕获不同模态之间的不相关性。</li>
</ul>
</li>
</ol>
<h3 id="数据增强在多模态讽刺检测中的应用（Data-Augmentation-in-Multimodal-Sarcasm-Detection）"><a href="#数据增强在多模态讽刺检测中的应用（Data-Augmentation-in-Multimodal-Sarcasm-Detection）" class="headerlink" title="数据增强在多模态讽刺检测中的应用（Data Augmentation in Multimodal Sarcasm Detection）"></a>数据增强在多模态讽刺检测中的应用（Data Augmentation in Multimodal Sarcasm Detection）</h3><ol>
<li><p><strong>文本数据增强（Textual Data Augmentation）</strong>：</p>
<ul>
<li>Rico等人 [12] 通过将文本翻译到不同语言再翻译回原文本来生成额外样本，这种方法被称为Back Translation。</li>
</ul>
</li>
<li><p><strong>音频数据增强（Audio Data Augmentation）</strong>：</p>
<ul>
<li>Gao等人 [27] 展示了速度和音量变化在讽刺检测中的有效性。</li>
</ul>
</li>
<li><p><strong>多模态数据增强（Multimodal Data Augmentation）</strong>：</p>
<ul>
<li>Huang等人 [30] 引入了一种通过分割原始序列来增强多模态情感分类数据的方法。</li>
</ul>
</li>
</ol>
<p>以上列出的研究工作为本论文提出的AMuSeD方法提供了理论基础和技术支持，特别是在多模态数据处理、特征提取和融合技术方面。这些相关研究展示了多模态数据在讽刺检测中的潜力，并强调了数据增强技术在提高模型性能和泛化能力中的重要性。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过以下步骤解决多模态讽刺检测中的问题：

<h3 id="1-数据增强策略（Data-Augmentation-Strategy）"><a href="#1-数据增强策略（Data-Augmentation-Strategy）" class="headerlink" title="1. 数据增强策略（Data Augmentation Strategy）"></a>1. 数据增强策略（Data Augmentation Strategy）</h3><ul>
<li><strong>文本增强</strong>：使用Back Translation技术，将原始讽刺文本翻译成不同的语言，然后再翻译回原始语言，以生成多样化的文本样本。</li>
<li><strong>音频增强</strong>：利用基于FastSpeech 2的语音合成系统，针对讽刺语调进行微调，以保留讽刺的语调特征，生成相应的音频数据。</li>
<li><strong>双模态数据增强</strong>：结合文本增强和音频增强，生成与增强文本对应的音频样本，以保持语义一致性。</li>
</ul>
<h3 id="2-特征提取（Feature-Extraction）"><a href="#2-特征提取（Feature-Extraction）" class="headerlink" title="2. 特征提取（Feature Extraction）"></a>2. 特征提取（Feature Extraction）</h3><ul>
<li><strong>文本特征提取</strong>：使用BERT模型来提取文本数据的高维嵌入，捕捉语言的细微差别。</li>
<li><strong>音频特征提取</strong>：利用VGGish模型来提取音频数据的特征，该模型在音频分类任务中表现良好。</li>
</ul>
<h3 id="3-特征融合（Feature-Fusion）"><a href="#3-特征融合（Feature-Fusion）" class="headerlink" title="3. 特征融合（Feature Fusion）"></a>3. 特征融合（Feature Fusion）</h3><ul>
<li><strong>自注意力机制</strong>：采用自注意力机制来合并文本和音频数据，使模型能够关注两种模态中与讽刺检测相关的特征。</li>
</ul>
<h3 id="4-预测（Prediction）"><a href="#4-预测（Prediction）" class="headerlink" title="4. 预测（Prediction）"></a>4. 预测（Prediction）</h3><ul>
<li><strong>全连接层</strong>：将融合后的特征通过全连接层进行处理，以输出最终的讽刺标签。</li>
</ul>
<h3 id="实验设置（Experimental-Setup）"><a href="#实验设置（Experimental-Setup）" class="headerlink" title="实验设置（Experimental Setup）"></a>实验设置（Experimental Setup）</h3><ul>
<li><strong>数据集</strong>：使用MUStARD数据集，包含来自美国电视情景喜剧的文本、音频和视频数据。</li>
<li><strong>比较基准</strong>：与现有的多模态讽刺检测方法进行比较。</li>
<li><strong>训练细节</strong>：使用二元交叉熵损失函数和Adam优化器进行模型训练。</li>
<li><strong>评估指标</strong>：采用精确度（P）、召回率（R）和F1分数（F1）作为评估指标。</li>
</ul>
<h3 id="实验结果（Experimental-Results）"><a href="#实验结果（Experimental-Results）" class="headerlink" title="实验结果（Experimental Results）"></a>实验结果（Experimental Results）</h3><ul>
<li><strong>整体模型性能</strong>：AMuSeD模型在多模态数据上达到了81.0%的F1分数，超过了使用MUStARD数据集三种模态的模型。</li>
<li><strong>数据增强分析</strong>：分析了增强数据的大小和不同语音合成器对模型性能的影响。</li>
<li><strong>注意力机制的重要性</strong>：评估了不同注意力机制对模型性能的影响，并探讨了跳跃连接（skip connections）的作用。</li>
</ul>
<p>通过这些步骤，论文提出了一个创新的双模态数据增强方法，并展示了自注意力机制在多模态讽刺检测中的有效性，从而显著提高了讽刺检测的准确性。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，作者进行了以下实验：

<h3 id="1-数据集描述和基准方法比较"><a href="#1-数据集描述和基准方法比较" class="headerlink" title="1. 数据集描述和基准方法比较"></a>1. 数据集描述和基准方法比较</h3><ul>
<li><strong>数据集</strong>：使用MUStARD数据集，包含690个音频视觉话语样本，均衡分为讽刺和非讽刺样本。</li>
<li><strong>基准方法</strong>：将提出的AMuSeD模型与多种现有方法进行比较，包括SVM、Ie Attention &amp; Ia Attention、ConAttSD、M2Seq2Seq和GEMA等。</li>
</ul>
<h3 id="2-整体模型性能评估"><a href="#2-整体模型性能评估" class="headerlink" title="2. 整体模型性能评估"></a>2. 整体模型性能评估</h3><ul>
<li>对AMuSeD模型进行评估，比较其与基准方法在精确度（P）、召回率（R）和F1分数（F1）上的性能差异。</li>
</ul>
<h3 id="3-数据增强分析"><a href="#3-数据增强分析" class="headerlink" title="3. 数据增强分析"></a>3. 数据增强分析</h3><ul>
<li><strong>数据量的影响</strong>：通过创建不同大小（4倍、16倍和20倍）的增强数据集，分析数据量对模型性能的影响。</li>
<li><strong>语音合成器的影响</strong>：在4倍增强数据集上，比较不同语音合成器（Amazon Polly、预训练的FastSpeech 2和微调的FastSpeech 2）对模型性能的影响。</li>
</ul>
<h3 id="4-注意力机制的效果分析"><a href="#4-注意力机制的效果分析" class="headerlink" title="4. 注意力机制的效果分析"></a>4. 注意力机制的效果分析</h3><ul>
<li><strong>有无注意力机制</strong>：比较有无注意力机制的模型在20倍增强数据集上的性能。</li>
<li><strong>不同注意力模型</strong>：评估并比较不同注意力机制（包括并行CoAttention、词级注意力、交叉注意力和自注意力）的效果。</li>
</ul>
<h3 id="5-BERT与GloVe作为文本特征提取器的比较"><a href="#5-BERT与GloVe作为文本特征提取器的比较" class="headerlink" title="5. BERT与GloVe作为文本特征提取器的比较"></a>5. BERT与GloVe作为文本特征提取器的比较</h3><ul>
<li>分析BERT和GloVe作为文本特征提取器在原始和增强数据集上的性能差异。</li>
</ul>
<h3 id="6-多模态的作用"><a href="#6-多模态的作用" class="headerlink" title="6. 多模态的作用"></a>6. 多模态的作用</h3><ul>
<li>评估仅使用音频、仅使用文本以及融合音频和文本模态对模型性能的影响。</li>
</ul>
<p>这些实验旨在全面评估AMuSeD模型的性能，并分析不同因素（如数据增强策略、语音合成器选择、注意力机制）对多模态讽刺检测的影响。通过这些实验，作者能够验证他们提出的方法在提高讽刺检测准确性方面的有效性，并与现有技术进行比较。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，以下是一些可以进一步探索的点：

<h3 id="1-提高数据增强技术的准确性和质量"><a href="#1-提高数据增强技术的准确性和质量" class="headerlink" title="1. 提高数据增强技术的准确性和质量"></a>1. 提高数据增强技术的准确性和质量</h3><ul>
<li><strong>改进Back Translation</strong>：探索更多样化的语言对以减少重复样本，并提高翻译的准确性和多样性。</li>
<li><strong>改进语音合成</strong>：研究更先进的文本到语音（TTS）技术，以更准确地反映讽刺的微妙之处。</li>
</ul>
<h3 id="2-视频模态的整合"><a href="#2-视频模态的整合" class="headerlink" title="2. 视频模态的整合"></a>2. 视频模态的整合</h3><ul>
<li><strong>视频数据增强</strong>：考虑将视频数据纳入当前的双模态增强框架中，以提供更全面的讽刺视角。</li>
</ul>
<h3 id="3-上下文信息的增强"><a href="#3-上下文信息的增强" class="headerlink" title="3. 上下文信息的增强"></a>3. 上下文信息的增强</h3><ul>
<li><strong>整合更多上下文信息</strong>：加入说话者信息和周围话语等上下文元素，可能会显著提高模型的预测准确性。</li>
</ul>
<h3 id="4-探索更先进的模型架构"><a href="#4-探索更先进的模型架构" class="headerlink" title="4. 探索更先进的模型架构"></a>4. 探索更先进的模型架构</h3><ul>
<li><strong>研究CoAttention等架构</strong>：这些高级架构可能在处理模态间交互方面提供更优越的性能和更深入的见解。</li>
</ul>
<h3 id="5-跨文化和语言的应用"><a href="#5-跨文化和语言的应用" class="headerlink" title="5. 跨文化和语言的应用"></a>5. 跨文化和语言的应用</h3><ul>
<li><strong>文化和语言多样性</strong>：研究讽刺在不同文化和语言中的表现形式，以提高模型在全球范围内的适用性。</li>
</ul>
<h3 id="6-改善源数据质量"><a href="#6-改善源数据质量" class="headerlink" title="6. 改善源数据质量"></a>6. 改善源数据质量</h3><ul>
<li><strong>减少偏见和提高代表性</strong>：改进原始数据集的质量和多样性，以减少数据增强过程中的偏差。</li>
</ul>
<h3 id="7-计算资源需求"><a href="#7-计算资源需求" class="headerlink" title="7. 计算资源需求"></a>7. 计算资源需求</h3><ul>
<li><strong>优化计算资源</strong>：考虑到处理和分析多模态数据所需的计算资源，需要优化模型以适应资源有限的环境。</li>
</ul>
<h3 id="8-同步文本和音频模态"><a href="#8-同步文本和音频模态" class="headerlink" title="8. 同步文本和音频模态"></a>8. 同步文本和音频模态</h3><ul>
<li><strong>提高同步精度</strong>：确保文本和音频模态有效地同步，以准确反映讽刺的上下文。</li>
</ul>
<p>这些探索点可以帮助未来的研究在多模态讽刺检测领域取得更深入的进展，并提高模型在实际应用中的有效性和适用性。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文的主要内容包括以下几个方面：

<h3 id="1-研究问题和动机"><a href="#1-研究问题和动机" class="headerlink" title="1. 研究问题和动机"></a>1. 研究问题和动机</h3><ul>
<li>论文旨在解决有效检测讽刺的问题，尤其是在多模态情境下，考虑到讽刺理解需要结合语境、语调、面部表情等多种信息。</li>
</ul>
<h3 id="2-AMuSeD方法"><a href="#2-AMuSeD方法" class="headerlink" title="2. AMuSeD方法"></a>2. AMuSeD方法</h3><ul>
<li>提出了AMuSeD（Attentive deep neural network for MUltimodal Sarcasm dEtection incorporating bi-modal Data augmentation），一个多模态讽刺检测方法，利用文本和音频数据增强，并采用自注意力机制进行特征融合。</li>
</ul>
<h3 id="3-数据增强策略"><a href="#3-数据增强策略" class="headerlink" title="3. 数据增强策略"></a>3. 数据增强策略</h3><ul>
<li>通过Back Translation生成多样化的文本样本，以及使用基于FastSpeech 2的语音合成系统生成音频样本，以解决数据稀缺问题。</li>
</ul>
<h3 id="4-特征提取和融合"><a href="#4-特征提取和融合" class="headerlink" title="4. 特征提取和融合"></a>4. 特征提取和融合</h3><ul>
<li>使用BERT提取文本特征，VGGish提取音频特征，并采用自注意力机制融合这些特征，以增强模型检测讽刺的能力。</li>
</ul>
<h3 id="5-实验设置和评估"><a href="#5-实验设置和评估" class="headerlink" title="5. 实验设置和评估"></a>5. 实验设置和评估</h3><ul>
<li>使用MUStARD数据集进行实验，与多个基准方法比较，评估模型的精确度、召回率和F1分数。</li>
</ul>
<h3 id="6-实验结果"><a href="#6-实验结果" class="headerlink" title="6. 实验结果"></a>6. 实验结果</h3><ul>
<li>实验结果显示，AMuSeD模型在多模态数据上达到了81.0%的F1分数，超过了一些使用三种模态数据的模型。</li>
</ul>
<h3 id="7-进一步探索的点"><a href="#7-进一步探索的点" class="headerlink" title="7. 进一步探索的点"></a>7. 进一步探索的点</h3><ul>
<li>论文提出了未来研究的方向，包括改进数据增强技术、整合视频模态、增强上下文信息、探索更先进的模型架构等。</li>
</ul>
<h3 id="8-结论"><a href="#8-结论" class="headerlink" title="8. 结论"></a>8. 结论</h3><ul>
<li>论文得出结论，AMuSeD方法通过双模态数据增强和自注意力机制显著提高了讽刺检测的能力，并为多模态情感分析领域提供了新的视角和方法。</li>
</ul>
<p>总的来说，这篇论文通过引入新的数据增强技术和自注意力机制，有效地提高了多模态讽刺检测的性能，并为未来的研究提供了有价值的见解和方向。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AMuSeD: An Attentive Deep Neural Network for Multimodal Sarcasm Detection Incorporating Bi-modal Data Augmentation，链接是 https://arxiv.org/pdf/2412.10103 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2412.10103 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5b5d5daa28c8135c54c643fc89c3f08f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f877427e40a4e774980cb291287f52b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ae68ead03480139f3bb9ac95eba46a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c955855d1c62d91ecf0d807cf5a1ffc3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee0681d95de8700c617d92c98f65b8ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cda9ae0a7a832ce13fe7da6abf9ae55a.jpg" align="middle">
</details>



<h2 id="CSSinger-End-to-End-Chunkwise-Streaming-Singing-Voice-Synthesis-System-Based-on-Conditional-Variational-Autoencoder"><a href="#CSSinger-End-to-End-Chunkwise-Streaming-Singing-Voice-Synthesis-System-Based-on-Conditional-Variational-Autoencoder" class="headerlink" title="CSSinger: End-to-End Chunkwise Streaming Singing Voice Synthesis System   Based on Conditional Variational Autoencoder"></a>CSSinger: End-to-End Chunkwise Streaming Singing Voice Synthesis System   Based on Conditional Variational Autoencoder</h2><p><strong>Authors:Jianwei Cui, Yu Gu, Shihao Chen, Jie Zhang, Liping Chen, Lirong Dai</strong></p>
<p>Singing Voice Synthesis (SVS) aims to generate singing voices of high fidelity and expressiveness. Conventional SVS systems usually utilize an acoustic model to transform a music score into acoustic features, followed by a vocoder to reconstruct the singing voice. It was recently shown that end-to-end modeling is effective in the fields of SVS and Text to Speech (TTS). In this work, we thus present a fully end-to-end SVS method together with a chunkwise streaming inference to address the latency issue for practical usages. Note that this is the first attempt to fully implement end-to-end streaming audio synthesis using latent representations in VAE. We have made specific improvements to enhance the performance of streaming SVS using latent representations. Experimental results demonstrate that the proposed method achieves synthesized audio with high expressiveness and pitch accuracy in both streaming SVS and TTS tasks. </p>
<blockquote>
<p>歌声合成（SVS）旨在生成高保真和富有表现力的歌声。传统的SVS系统通常使用声学模型将乐谱转换为声学特征，然后使用声码器重建歌声。最近的研究表明，端到端建模在SVS和文本到语音（TTS）领域是有效的。因此，在这项工作中，我们提出了一种完全端到端的SVS方法，并结合分块流式推理来解决实际使用中的延迟问题。需要注意的是，这是首次尝试在VAE中使用潜在表示来实现完全端到端的流式音频合成。为了提高使用潜在表示的流式SVS的性能，我们已经进行了特定的改进。实验结果表明，该方法在流式SVS和TTS任务中均实现了高表现力和音高准确度的合成音频。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08918v2">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种全新的端到端歌唱声音合成（SVS）方法，该方法采用分段流式推理以解决实际应用中的延迟问题。此方法首次尝试完全使用潜在表示进行端到流音频合成，并在SVS和文本到语音（TTS）任务中取得了良好的表现。实验结果表明，该方法合成的音频具有高表现力和音高准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种全新的端到端歌唱声音合成（SVS）方法，包括分段流式推理的使用来解决实际应用中的延迟问题。</li>
<li>采用端到端的音频合成方法，首次尝试完全使用潜在表示进行端到流音频合成。</li>
<li>在SVS和文本到语音（TTS）任务中实现了高度表现和音高准确性的合成音频。</li>
<li>引入改进的流式SVS性能优化措施，使用潜在表示来提高性能。</li>
<li>实验结果表明，该方法合成的音频质量较高，并具有广泛的应用前景。</li>
<li>该方法对于解决实时音频合成中的延迟问题具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08918">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文提出了一个基于条件变分自编码器（Conditional Variational Autoencoder, VAE）的端到端分块流式（chunkwise streaming）歌唱语音合成系统（ChunkStreamSinger, CSSinger），旨在解决以下问题：

<ol>
<li><p><strong>实时性和计算资源限制</strong>：传统的歌唱语音合成（Singing Voice Synthesis, SVS）系统和文本到语音（Text to Speech, TTS）系统通常包含一个声学模型和一个声码器，这些系统在处理长序列时计算量大，实时性能不佳，尤其是在边缘设备和在线网络服务中部署时。</p>
</li>
<li><p><strong>端到端流式音频合成</strong>：这是首次尝试完全实现使用VAE中的潜在表示进行端到端流式音频合成。流式合成对于减少延迟和提高实时性至关重要。</p>
</li>
<li><p><strong>潜在表示用于声码器</strong>：使用VAE的潜在表示作为声码器的输入并不直观，尤其是对于歌唱语音，直接使用潜在表示进行音频生成会导致合成音频质量下降。</p>
</li>
<li><p><strong>提高流式SVS性能</strong>：论文中提出了特定的改进措施，以增强使用潜在表示的流式SVS性能。</p>
</li>
</ol>
<p>综上所述，论文的主要贡献在于提出了一种新的端到端流式歌唱语音合成方法，该方法不仅能够生成高表达性和音高准确性的合成音频，而且相较于传统的平行计算模型，能够显著降低延迟，提高实时性，更好地适应边缘设备和在线服务等实际应用场景。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 相关研究主要涉及以下几个领域：

<ol>
<li><p><strong>Singing Voice Synthesis (SVS)</strong>: 论文提到了多个基于深度学习的SVS系统，例如XiaoiceSing、ByteSing、VISinger和VISinger2。这些系统利用深度神经网络生成基于乐谱的高质量歌唱语音。</p>
</li>
<li><p><strong>Text-to-Speech (TTS)</strong>: 论文中提到了TTS系统中使用的端到端建模方法，如VITS (Kim, Kong, and Son 2021) 和SiFiSinger (Cui et al. 2024)，它们基于条件VAE框架进行声学建模和音频合成。</p>
</li>
<li><p><strong>Conditional Variational Autoencoder (VAE)</strong>: VAE作为一种深度生成模型，在语音合成领域有广泛应用，如VITS、VISinger和SiFiSinger等系统，它们利用VAE学习潜在空间表示，以提高合成音频的自然度和真实感。</p>
</li>
<li><p><strong>Streaming and Autoregressive Modeling</strong>: 论文中提到了一些研究工作，它们关注于流式自回归建模，例如Tacotron2和WaveNet。这些模型分别用于声学建模和波形生成，但存在推理效率低下和生成速度慢的问题。</p>
</li>
<li><p><strong>Neural Audio Synthesis</strong>: 论文中提到了HiFi-GAN，这是一个用于高保真语音合成的生成对抗网络，它在CSSinger系统中被用于流式音频合成。</p>
</li>
<li><p><strong>Efficient Neural Audio Synthesis</strong>: 论文中提到了WaveRNN，这是一个用于实时应用的神经音频合成模型，它通过减小模型大小和权重稀疏化来加速生成过程。</p>
</li>
</ol>
<p>这些相关研究为CSSinger系统的提出提供了理论基础和技术背景，CSSinger系统在这些研究的基础上，通过引入流式处理和潜在表示的改进，旨在实现更高效、更自然的歌唱语音合成。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过提出一个名为ChunkStreamSinger（CSSinger）的系统来解决上述问题，具体解决方案包括以下几个关键点：

<ol>
<li><p><strong>端到端条件VAE框架</strong>：</p>
<ul>
<li>使用条件变分自编码器（VAE）来学习音乐评分（如歌词、音高、时长）的潜在表示，并直接生成合成音频。</li>
<li>该框架包括一个先验编码器（Prior Encoder）和一个后验编码器（Posterior Encoder），用于学习潜在空间的表示，并从这些表示中直接生成合成音频。</li>
</ul>
</li>
<li><p><strong>分块流式推理（Chunkwise Streaming Inference）</strong>：</p>
<ul>
<li>采用分块方式处理音频合成任务，降低实时处理的延迟，并减少计算资源消耗。</li>
<li>在声码器（vocoder）中使用因果转置卷积（causal transposed convolutions）来生成音频，避免了传统填充方法导致的音质下降问题。</li>
</ul>
</li>
<li><p><strong>自然填充（Natural Padding）策略</strong>：</p>
<ul>
<li>为了避免在因果卷积中使用传统常数填充（如零填充或复制填充）导致的音质问题，论文提出了自然填充策略。</li>
<li>该策略通过在训练过程中动态地使用实际的潜在表示作为填充，而不是固定的常数值，以提高音频合成质量。</li>
</ul>
</li>
<li><p><strong>ChunkStream Decoder</strong>：</p>
<ul>
<li>为了实现完全的流式推理，提出了ChunkStream Decoder，它将潜在表示的生成转换为基于块的流式处理。</li>
<li>该解码器借鉴了Emformer模型的机制，将完整的输入特征向量分解为多个固定长度的块，并在各个块之间提供上下文信息，确保注意力机制可以在每个块上并行计算。</li>
</ul>
</li>
<li><p><strong>因果平滑层（Causal Smooth Layer）</strong>：</p>
<ul>
<li>为了减少块之间的边界效应，引入了因果平滑层，该层由一维因果卷积和LayerNorm层组成。</li>
<li>这层保留了前一个块的相同层级的特征，并为下一个块提供特征，以增强合成音频的自然度和流畅性。</li>
</ul>
</li>
<li><p><strong>实验验证</strong>：</p>
<ul>
<li>在多个中文歌唱语音数据集和一个TTS数据集上进行了实验，验证了所提出方法在主观和客观评价指标上相较于现有系统的优越性。</li>
<li>展示了CSSinger-FS在MOS评分和客观评价指标上的优势，并在不同硬件配置下评估了模型的延迟和实时性。</li>
</ul>
</li>
</ol>
<p>通过这些技术方案，论文成功地实现了一个低延迟、高效率的端到端流式歌唱语音合成系统，该系统不仅提高了合成音频的质量，还降低了实时应用中的延迟，使其更适合于边缘设备和在线服务等资源受限环境。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 论文中进行了一系列实验来评估所提出的ChunkStreamSinger（CSSinger）系统的性能，包括主观和客观的评估。以下是实验的详细内容：

<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul>
<li><strong>Opencpop2</strong>：一个公开的高质量中文歌唱语音数据集，包含100首普通话歌曲，由专业女歌手演唱，包含3,756个话语，总时长5.2小时。</li>
<li><strong>PopCS</strong>：另一个中文普通话歌唱语音数据集，包含117首歌曲，总时长约5.89小时，由专业女歌手演唱。</li>
<li><strong>Baker3</strong>：一个中文女性语音数据集，包含约12小时的标准普通话女性语音，包含10,000个句子，平均每个句子16个字符。</li>
</ul>
<h3 id="比较方法"><a href="#比较方法" class="headerlink" title="比较方法"></a>比较方法</h3><ul>
<li><strong>Recording</strong>：使用真实的歌唱语音音频作为基准。</li>
<li><strong>SiFiSinger</strong>：一个完全并行推理的基线系统。</li>
<li><strong>CSSinger-SS</strong>：CSSinger的半流式方法，只有HiFiGAN声码器以分块流式方式运行。</li>
<li><strong>CSSinger-SS-NP</strong>：CSSinger-SS加上自然填充（Natural Padding）的半流式结构。</li>
<li><strong>CSSingerFS</strong>（提出的模型）：CSSinger的全流式框架，使用ChunkStream Decoder实现潜在表示z的流式推理。</li>
</ul>
<h3 id="实施细节"><a href="#实施细节" class="headerlink" title="实施细节"></a>实施细节</h3><ul>
<li>使用44.1KHz采样的音频处理Opencpop和PopCS数据集，而Baker数据集的音频采样率为48KHz，被重新采样到16KHz。</li>
<li>模型隐藏层大小设置为192，FFN隐藏层的通道数为768。</li>
<li>Acoustic Model Decoder和ChunkStream Decoder都有4个注意力层。</li>
<li>使用diffsptk4提取80维melcepstrum（mcep）作为声学特征。</li>
<li>所有实验中HiFi-GAN生成器的训练随机切片大小设置为20，CSSinger模型的块大小也为20。</li>
</ul>
<h3 id="主要结果与分析"><a href="#主要结果与分析" class="headerlink" title="主要结果与分析"></a>主要结果与分析</h3><ul>
<li><strong>主观评估（MOS测试）</strong>：在Opencpop和PopCS数据集上进行，随机选择20个样本，邀请20名母语人士进行主观评估。</li>
<li><strong>客观评估</strong>：计算合成音频与真实音频之间的基频均方根误差（F0 RMSE）、基频相关系数（F0 Corr）、无声&#x2F;有声错误率（U&#x2F;UV Err）、Mel-cepstrum均方误差（MSE）和Mel-cepstral失真（MCD）。</li>
</ul>
<h3 id="延迟评估"><a href="#延迟评估" class="headerlink" title="延迟评估"></a>延迟评估</h3><ul>
<li>测量从输入特征到合成音频生成的延迟（秒）、从输入特征到音频处理完成的总处理时间（秒）和实时因子（RTF）作为系统效率的指标。</li>
<li>在GPU、CPU和单核单线程CPU限制（CPU-Limited）三种情况下测试延迟指标。</li>
</ul>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul>
<li>CSSinger-FS在多个客观和主观指标上表现出色，优于现有的并行和半流式系统。</li>
<li>在GPU和CPU硬件配置下，CSSinger-FS显示出较低的延迟和优越的实时推理效率，这对于高实时需求的应用和计算资源受限的场景非常重要。</li>
</ul>
<p>这些实验全面评估了CSSinger系统的性能，证明了其在歌唱语音合成任务中的有效性和优越性。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 尽管论文提出的ChunkStreamSinger（CSSinger）系统在歌唱语音合成方面取得了显著的性能提升，但仍有一些领域可以进一步探索和改进：

<ol>
<li><p><strong>模型泛化能力</strong>：</p>
<ul>
<li>在更多种类和风格的歌唱数据集上测试模型的泛化能力，包括不同语言、不同性别和不同演唱技巧的数据集。</li>
</ul>
</li>
<li><p><strong>更深入的客观评估</strong>：</p>
<ul>
<li>开发更精细的客观评价指标，以更全面地评估合成音频的自然度、清晰度和音乐性等。</li>
</ul>
</li>
<li><p><strong>实时性能优化</strong>：</p>
<ul>
<li>进一步优化模型以降低延迟和计算资源消耗，特别是在资源受限的设备上，如移动设备或嵌入式系统。</li>
</ul>
</li>
<li><p><strong>多风格和多歌手建模</strong>：</p>
<ul>
<li>探索如何使模型能够处理多个歌手和不同演唱风格，提供更丰富的合成选项。</li>
</ul>
</li>
<li><p><strong>端到端训练的改进</strong>：</p>
<ul>
<li>研究如何改进端到端训练过程，可能包括更好的损失函数、优化算法或正则化技术，以进一步提高合成质量。</li>
</ul>
</li>
<li><p><strong>声码器的改进</strong>：</p>
<ul>
<li>研究新型声码器或改进现有声码器，以生成更高保真度和更自然的音频。</li>
</ul>
</li>
<li><p><strong>上下文感知建模</strong>：</p>
<ul>
<li>提高模型对歌曲上下文的感知能力，例如，理解和模拟歌曲的情绪和节奏变化。</li>
</ul>
</li>
<li><p><strong>交互式应用</strong>：</p>
<ul>
<li>探索CSSinger在交互式应用中的潜力，如卡拉OK系统或音乐教育软件，其中实时反馈和调整是必要的。</li>
</ul>
</li>
<li><p><strong>模型解释性</strong>：</p>
<ul>
<li>提高模型的可解释性，帮助研究人员和开发者更好地理解模型的决策过程和潜在空间的结构。</li>
</ul>
</li>
<li><p><strong>多任务学习</strong>：</p>
<ul>
<li>考虑将歌唱语音合成与其他任务（如音乐生成、节奏识别）结合起来，以创建更全面的音乐生成系统。</li>
</ul>
</li>
<li><p><strong>模型鲁棒性测试</strong>：</p>
<ul>
<li>对模型进行鲁棒性测试，尤其是在面对不完整或噪声数据时的性能。</li>
</ul>
</li>
<li><p><strong>跨领域应用</strong>：</p>
<ul>
<li>探索CSSinger在其他领域的应用，如语音编码、语音增强或语音转换。</li>
</ul>
</li>
</ol>
<p>这些探索方向不仅可以推动歌唱语音合成技术的发展，还可能为整个语音合成和音频处理领域带来新的突破。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文提出了一个基于条件变分自编码器（Conditional Variational Autoencoder, VAE）的端到端分块流式（chunkwise streaming）歌唱语音合成系统ChunkStreamSinger（CSSinger）。以下是论文的主要内容总结：

<ol>
<li><p><strong>问题陈述</strong>：</p>
<ul>
<li>论文旨在解决传统歌唱语音合成（SVS）系统中存在的实时性和计算资源限制问题，特别是在边缘设备和在线网络服务中部署时的高延迟问题。</li>
</ul>
</li>
<li><p><strong>系统架构</strong>：</p>
<ul>
<li>CSSinger基于SiFiSinger构建，采用条件VAE框架，包含先验编码器和后验编码器，直接从音乐评分生成合成音频。</li>
<li>系统支持分块流式推理，以减少延迟并提高实时性。</li>
</ul>
</li>
<li><p><strong>关键技术</strong>：</p>
<ul>
<li><strong>自然填充（Natural Padding）</strong>：解决VAE潜在表示用于声码器输入时的音质下降问题。</li>
<li><strong>ChunkStream Decoder</strong>：实现完全流式推理，将潜在表示的生成转换为基于块的流式处理。</li>
<li><strong>因果平滑层（Causal Smooth Layer）</strong>：减少块之间的边界效应，增强合成音频的自然度和流畅性。</li>
</ul>
</li>
<li><p><strong>实验验证</strong>：</p>
<ul>
<li>在两个中文歌唱语音数据集和一个文本到语音（TTS）数据集上进行了实验，验证了CSSinger系统在主观和客观评价指标上相较于现有系统的优越性。</li>
<li>展示了CSSinger在不同硬件配置下（GPU和CPU）的低延迟和优越的实时推理效率。</li>
</ul>
</li>
<li><p><strong>结论</strong>：</p>
<ul>
<li>CSSinger-FS（全流式框架）在MOS评分和客观评价指标上优于现有的并行和半流式系统。</li>
<li>该系统不仅提高了合成音频的质量，还降低了实时应用中的延迟，使其更适合于边缘设备和在线服务等资源受限环境。</li>
</ul>
</li>
<li><p><strong>未来工作</strong>：</p>
<ul>
<li>论文提出了一些可以进一步探索的方向，包括模型泛化能力的提升、实时性能的进一步优化、多风格和多歌手建模等。</li>
</ul>
</li>
</ol>
<p>总体而言，这篇论文提出了一个创新的端到端流式歌唱语音合成系统，通过引入先进的技术方案，有效地提高了合成音频的质量和实时性，为SVS领域的发展提供了新的方向。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CSSinger: End-to-End Chunkwise Streaming Singing Voice Synthesis System Based on Conditional Variational Autoencoder，链接是 https://arxiv.org/pdf/2412.08918 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2412.08918 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2ad9a7cbc916482dfe30fb14963938fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6ec655dffedb8217a0036c312b35fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd6c931b350f7c2a12489cb7dee75028.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b2ab67bc4a7feea7de1b1bcbefc1d25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-761e8346bb886976fd93d7c51cc904bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a7dbd5b98e050ba9eca4d27d4cbd65b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23a41de53e1b9ee13dddda8ce665c227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b7199fa13af85804dc4f522e51bad7a.jpg" align="middle">
</details>




                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-17/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-17/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-17/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2421a6952de4175341721627fed291bb.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-12-17  Still "Talking About Large Language Models" Some Clarifications
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-aedee62a5f4db8bbd3171ebe66a15ab9.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-17  UniMed-CLIP Towards a Unified Image-Text Pretraining Paradigm for   Diverse Medical Imaging Modalities
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17204.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
