<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="视频理解">
    <meta name="description" content="视频理解 方向最新论文已更新，请持续关注 Update in 2024-12-17  Apollo An Exploration of Video Understanding in Large Multimodal Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>视频理解 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-cd6c6174cef352cc65a92a90ee1bb736.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">视频理解</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">视频理解</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                视频理解
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    47 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-17-更新"><a href="#2024-12-17-更新" class="headerlink" title="2024-12-17 更新"></a>2024-12-17 更新</h1><h2 id="Apollo-An-Exploration-of-Video-Understanding-in-Large-Multimodal-Models"><a href="#Apollo-An-Exploration-of-Video-Understanding-in-Large-Multimodal-Models" class="headerlink" title="Apollo: An Exploration of Video Understanding in Large Multimodal Models"></a>Apollo: An Exploration of Video Understanding in Large Multimodal Models</h2><p><strong>Authors:Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, Xide Xia</strong></p>
<p>Despite the rapid integration of video perception capabilities into Large Multimodal Models (LMMs), the underlying mechanisms driving their video understanding remain poorly understood. Consequently, many design decisions in this domain are made without proper justification or analysis. The high computational cost of training and evaluating such models, coupled with limited open research, hinders the development of video-LMMs. To address this, we present a comprehensive study that helps uncover what effectively drives video understanding in LMMs.   We begin by critically examining the primary contributors to the high computational requirements associated with video-LMM research and discover Scaling Consistency, wherein design and training decisions made on smaller models and datasets (up to a critical size) effectively transfer to larger models. Leveraging these insights, we explored many video-specific aspects of video-LMMs, including video sampling, architectures, data composition, training schedules, and more. For example, we demonstrated that fps sampling during training is vastly preferable to uniform frame sampling and which vision encoders are the best for video representation.   Guided by these findings, we introduce Apollo, a state-of-the-art family of LMMs that achieve superior performance across different model sizes. Our models can perceive hour-long videos efficiently, with Apollo-3B outperforming most existing $7$B models with an impressive 55.1 on LongVideoBench. Apollo-7B is state-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on Video-MME. </p>
<blockquote>
<p>尽管视频感知能力已迅速融入大型多模态模型（LMMs）中，但驱动其视频理解的底层机制仍知之甚少。因此，该领域的许多设计决策都是在没有适当论证或分析的情况下做出的。此类模型训练和评估的计算成本高昂，再加上开放研究有限，阻碍了视频LMM的发展。为了解决这一问题，我们进行了一项全面的研究，旨在揭示在LMMs中有效驱动视频理解的因素。</p>
</blockquote>
<p>我们首先仔细研究了与视频LMM研究相关的高计算要求的主要贡献者，并发现了“规模一致性”，即在小模型和数据集上做出的设计和训练决策（直至达到临界规模）可以有效地转移到大型模型上。利用这些见解，我们探索了视频LMM的视频特定方面，包括视频采样、架构、数据组成、训练计划等。例如，我们证明了在训练过程中使用fps采样远胜于均匀帧采样，以及哪种视觉编码器最适合视频表示。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10360v1">PDF</a> <a target="_blank" rel="noopener" href="https://apollo-lmms.github.io/">https://apollo-lmms.github.io</a></p>
<p><strong>摘要</strong></p>
<p>尽管大型多模态模型（LMMs）已迅速融入视频感知能力，但驱动其视频理解的底层机制仍鲜为人知。因此，该领域的许多设计决策都是没有经过适当的验证或分析而做出的。训练和评价此类模型的高计算成本，以及开放研究的有限性，阻碍了视频LMM的发展。为了帮助揭示LMM中有效驱动视频理解的因素，我们进行了一项综合研究。我们首先对与视频LMM研究相关的高计算要求的主要贡献者进行了批判性审查，并发现了规模一致性，即在小模型和数据集上做出的设计和训练决策可以有效地转移到大型模型。利用这些见解，我们探索了视频LMM的许多视频特定方面，包括视频采样、架构、数据组成、训练计划等。例如，我们证明训练过程中的fps采样远优于均匀帧采样，并确定了哪种视觉编码器最适合视频表示。在这些发现的指导下，我们推出了Apollo，一个最先进的LMM家族，在不同规模的模型中表现卓越。我们的模型可以高效地感知长达一小时的视频，其中Apollo-3B在LongVideoBench上得分高达55.1，超越大多数现有的7B模型。Apollo-7B在MLVU上得分70.9，在Video-MME上得分63.3，成为7B LMMs中的最新前沿技术。</p>
<p><strong>要点</strong></p>
<ol>
<li>大型多模态模型（LMMs）的视频理解机制尚待深入理解。</li>
<li>当前设计决策往往缺乏适当的验证和分析。</li>
<li>视频LMM的研究受到高计算成本和有限开放研究的阻碍。</li>
<li>发现“规模一致性”，即小模型和数据集上的设计和训练决策可转移到大型模型。</li>
<li>深入研究了视频LMM的视频特定方面，包括视频采样、架构、数据组成等。</li>
<li>表明训练过程中的fps采样优于均匀帧采样。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10360">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文试图解决的主要问题是视频大型多模态模型（video-LMMs）在理解和处理视频内容方面的效率和性能问题。具体来说，论文关注以下几个核心问题：

<ol>
<li><p><strong>视频理解的机制</strong>：尽管大型多模态模型（LMMs）已经迅速融入视频感知能力，但其背后的视频理解机制尚不清楚。这导致许多设计决策缺乏适当的理由或分析。</p>
</li>
<li><p><strong>计算成本</strong>：训练和评估这类模型需要高昂的计算成本，同时，开放研究有限，这些都阻碍了视频-LMMs的发展。</p>
</li>
<li><p><strong>视频-LMMs的设计和训练</strong>：许多关于视频-LMMs设计的基本问题尚未得到解答，例如视频应该如何采样、哪种视觉编码器能产生最优表示、视频令牌的最佳重采样实践等。</p>
</li>
<li><p><strong>视频基准测试的效率和质量</strong>：现有的视频问题回答基准测试存在效率低下和冗余的问题，需要更有效的评估方法。</p>
</li>
<li><p><strong>模型规模与性能的关系</strong>：传统的扩展法则需要训练多个不同规模的模型来预测性能，这对于LMMs来说不切实际。论文探索了是否可以放宽这些扩展法则，以便在不训练多个模型的情况下预测性能。</p>
</li>
</ol>
<p>论文通过一系列实验和分析，提出了“Apollo”这一家族的LMMs，旨在通过系统地探索视频-LMMs的设计空间，解决上述问题，并推动视频理解领域的发展。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 根据提供的论文内容，以下是一些与本研究相关的工作：

<ol>
<li><p><strong>视频理解模型的发展</strong>：</p>
<ul>
<li>Hoffmann et al. (2022) 和 Yang et al. (2024) 讨论了语言和图像-语言模型的快速进步。</li>
<li>Li et al. (2023a, 2025), Liu et al. (2024d), 和 Li et al. (2024b) 讨论了视频-LMMs面临的计算需求和复杂设计空间。</li>
</ul>
</li>
<li><p><strong>视频采样和编码器选择</strong>：</p>
<ul>
<li>Xu et al. (2024b), Kim et al. (2024), 和 Wu (2024) 讨论了直接将图像-LMMs扩展到视频领域的方法。</li>
<li>Zhang et al. (2024e), Maaz et al. (2023) 提出了视频特定微调方法。</li>
<li>Zhang et al. (2024e) 提出了更长上下文窗口的设计选择。</li>
<li>Li et al. (2024a, c) 讨论了多模态混合方法。</li>
<li>Wang et al. (2024c) 提出了代理工作流。</li>
<li>Zohar et al. (2024) 讨论了自训练方法。</li>
</ul>
</li>
<li><p><strong>视频基准测试</strong>：</p>
<ul>
<li>Fu et al. (2024), Wu et al. (2024), Zhou et al. (2024), 和 Patraucean et al. (2023) 提出了多个视频问题回答基准。</li>
<li>Goyal et al. (2017) 和 Chen et al. (2024a) 分析了图像问题回答基准的性能驱动因素和数据泄露问题。</li>
</ul>
</li>
<li><p><strong>模型扩展性和设计决策</strong>：</p>
<ul>
<li>Aghajanyan et al. (2023) 和 Yu et al. (2023) 观察了LMM预训练中的扩展法则。</li>
<li>Hoffmann et al. (2022) 讨论了基于模型大小预测性能的传统扩展法则。</li>
</ul>
</li>
<li><p><strong>视频采样和令牌重采样</strong>：</p>
<ul>
<li>Jaegle et al. (2021) 提出了Perceiver Resampler，用于减少令牌数量并实现有效的长上下文建模。</li>
<li>Laurençon et al. (2024b) 讨论了感知器重采样器作为减少令牌数量的有效方法。</li>
</ul>
</li>
<li><p><strong>视频表示和编码器组合</strong>：</p>
<ul>
<li>Shi et al. (2024) 测试了多种编码器集成方法，并发现通道级连接在几乎所有配置中都是首选。</li>
</ul>
</li>
<li><p><strong>视频令牌集成和训练协议</strong>：</p>
<ul>
<li>Zhao et al. (2024a) 系统地消融了视频和文本令牌集成策略。</li>
</ul>
</li>
</ol>
<p>这些相关工作为本研究提供了理论基础和实验背景，帮助作者深入探讨视频-LMMs的设计空间，并提出了改进视频理解性能的方法。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过以下几个步骤解决视频大型多模态模型（video-LMMs）在理解和处理视频内容方面的效率和性能问题：

<h3 id="1-系统探索视频建模设计空间"><a href="#1-系统探索视频建模设计空间" class="headerlink" title="1. 系统探索视频建模设计空间"></a>1. 系统探索视频建模设计空间</h3><ul>
<li>论文首先对影响大型多模态模型（LMMs）在视频语言任务中性能的关键架构设计选择进行了分析，包括视频采样、视频表示、视频令牌重采样和视频令牌集成策略。</li>
<li>通过实验比较不同的设计选择，例如不同的采样策略、视觉编码器组合、令牌重采样方法和集成策略，以确定哪些因素对模型性能有显著影响。</li>
</ul>
<h3 id="2-引入“Scaling-Consistency”概念"><a href="#2-引入“Scaling-Consistency”概念" class="headerlink" title="2. 引入“Scaling Consistency”概念"></a>2. 引入“Scaling Consistency”概念</h3><ul>
<li>论文提出了“Scaling Consistency”现象，即在较小的模型和数据集上有效的设计决策可以有效地转移到更大的模型上。</li>
<li>利用这一发现，研究者可以在较小的模型上进行设计决策的实验，然后将这些决策应用到更大的模型上，从而减少计算成本并加速研究进程。</li>
</ul>
<h3 id="3-评估和优化现有视频基准测试"><a href="#3-评估和优化现有视频基准测试" class="headerlink" title="3. 评估和优化现有视频基准测试"></a>3. 评估和优化现有视频基准测试</h3><ul>
<li>论文分析了现有视频问题回答基准测试的质量、冗余性，并基于这些分析引入了一个新的基准测试套件ApolloBench。</li>
<li>ApolloBench旨在提供一个更有效和高效的评估方法，通过精选问题和分类，减少评估时间并提供详细的洞察。</li>
</ul>
<h3 id="4-训练策略和数据混合的探索"><a href="#4-训练策略和数据混合的探索" class="headerlink" title="4. 训练策略和数据混合的探索"></a>4. 训练策略和数据混合的探索</h3><ul>
<li>论文探讨了不同的训练计划和协议，包括单阶段、多阶段训练以及视频编码器的训练时机和数据。</li>
<li>通过实验确定最佳的训练策略和数据组成，以优化模型性能。</li>
</ul>
<h3 id="5-引入Apollo模型家族"><a href="#5-引入Apollo模型家族" class="headerlink" title="5. 引入Apollo模型家族"></a>5. 引入Apollo模型家族</h3><ul>
<li>基于上述发现，论文介绍了Apollo，这是一个在不同模型规模下都能实现优越性能的LMMs家族。</li>
<li>Apollo模型能够高效感知长达一小时的视频，并在多个基准测试中取得了优于现有模型的性能。</li>
</ul>
<h3 id="6-提供资源和指导"><a href="#6-提供资源和指导" class="headerlink" title="6. 提供资源和指导"></a>6. 提供资源和指导</h3><ul>
<li>论文不仅提出了解决方案，还提供了实现这些解决方案的详细资源和指导，包括模型架构、训练细节和数据集信息，以便其他研究者可以复现和扩展这些工作。</li>
</ul>
<p>通过这些步骤，论文旨在提供一个全面的解决方案，以提高视频-LMMs的效率和性能，并推动该领域的进一步研究。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 根据提供的论文内容，作者进行了一系列实验来探索和验证他们的发现和方法。以下是论文中提到的主要实验：

<h3 id="1-视频采样实验"><a href="#1-视频采样实验" class="headerlink" title="1. 视频采样实验"></a>1. 视频采样实验</h3><ul>
<li>比较了不同的视频采样策略，包括均匀采样（uniform sampling）和每秒帧数（fps）采样，以及它们对模型性能的影响。</li>
<li>分析了每秒帧数（fps）和每秒令牌数（tps）对模型性能的影响，并探讨了它们之间的权衡。</li>
</ul>
<h3 id="2-视觉编码器实验"><a href="#2-视觉编码器实验" class="headerlink" title="2. 视觉编码器实验"></a>2. 视觉编码器实验</h3><ul>
<li>测试了不同的视觉编码器，包括单一编码器和编码器组合，以确定哪些编码器对视频表示最有效。</li>
<li>评估了语言监督和自监督编码器的性能差异。</li>
</ul>
<h3 id="3-视频令牌重采样实验"><a href="#3-视频令牌重采样实验" class="headerlink" title="3. 视频令牌重采样实验"></a>3. 视频令牌重采样实验</h3><ul>
<li>测试了不同的视频令牌重采样方法，包括多层感知机（MLP）上投影+平均池化、2D卷积+平均池化和Perceiver重采样器。</li>
</ul>
<h3 id="4-视频令牌集成实验"><a href="#4-视频令牌集成实验" class="headerlink" title="4. 视频令牌集成实验"></a>4. 视频令牌集成实验</h3><ul>
<li>探索了不同的视频和文本令牌集成策略，包括直接插入、分隔符、文本时间戳和结合分隔符与时间戳的方法。</li>
</ul>
<h3 id="5-训练计划实验"><a href="#5-训练计划实验" class="headerlink" title="5. 训练计划实验"></a>5. 训练计划实验</h3><ul>
<li>系统评估了不同的训练计划对模型性能的影响，包括单阶段、双阶段和三阶段训练协议。</li>
</ul>
<h3 id="6-数据组成实验"><a href="#6-数据组成实验" class="headerlink" title="6. 数据组成实验"></a>6. 数据组成实验</h3><ul>
<li>研究了不同文本、图像和视频数据混合比例对模型性能的影响。</li>
</ul>
<h3 id="7-扩展一致性（Scaling-Consistency）实验"><a href="#7-扩展一致性（Scaling-Consistency）实验" class="headerlink" title="7. 扩展一致性（Scaling Consistency）实验"></a>7. 扩展一致性（Scaling Consistency）实验</h3><ul>
<li>通过在不同大小的模型上进行实验，探索了在较小模型上有效的设计决策是否可以可靠地转移到更大的模型上。</li>
</ul>
<h3 id="8-Apollo模型性能评估"><a href="#8-Apollo模型性能评估" class="headerlink" title="8. Apollo模型性能评估"></a>8. Apollo模型性能评估</h3><ul>
<li>在多个基准测试中评估了Apollo模型的性能，包括TempCompass、MLVU、Perception-Test、Video-MME、LongVideoBench和ApolloBench。</li>
</ul>
<p>这些实验旨在全面探索视频-LMMs的设计空间，并验证作者提出的方法和发现。通过这些实验，作者能够识别出影响视频-LMMs性能的关键因素，并提出了一套有效的设计和训练策略。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，以下是一些可以进一步探索的研究方向：

<h3 id="1-分离架构的探索"><a href="#1-分离架构的探索" class="headerlink" title="1. 分离架构的探索"></a>1. 分离架构的探索</h3><p>论文中提到了使用统一架构处理图像和视频输入的方法。可以进一步探索分离架构，其中图像由图像编码器处理，视频由图像和视频编码器共同处理，以揭示性能优势和更好的模态处理。</p>
<h3 id="2-视频和图像编码器的训练策略"><a href="#2-视频和图像编码器的训练策略" class="headerlink" title="2. 视频和图像编码器的训练策略"></a>2. 视频和图像编码器的训练策略</h3><p>在分离架构中，训练视频和图像编码器并在监督微调（SFT）中评估它们对性能的个体贡献，可能揭示最优训练策略。</p>
<h3 id="3-混合数据训练"><a href="#3-混合数据训练" class="headerlink" title="3. 混合数据训练"></a>3. 混合数据训练</h3><p>在统一架构中，同时训练两个编码器处理混合的图像和视频数据可能有助于确定哪个编码器影响观察到的性能下降，从而实现针对性改进。</p>
<h3 id="4-扩展一致性（Scaling-Consistency）的进一步研究"><a href="#4-扩展一致性（Scaling-Consistency）的进一步研究" class="headerlink" title="4. 扩展一致性（Scaling Consistency）的进一步研究"></a>4. 扩展一致性（Scaling Consistency）的进一步研究</h3><p>进一步确认扩展一致性是否适用于更广泛的模型大小范围，确保其对更大模型的可靠性。</p>
<h3 id="5-基于记忆的LMM方法"><a href="#5-基于记忆的LMM方法" class="headerlink" title="5. 基于记忆的LMM方法"></a>5. 基于记忆的LMM方法</h3><p>论文没有探索基于记忆的LMM方法，例如记忆银行或像Q-Former中文本条件池化这样的帧检索方法。评估这些技术可能会测试它们是否适合多轮对话的假设。</p>
<h3 id="6-对话评估基准"><a href="#6-对话评估基准" class="headerlink" title="6. 对话评估基准"></a>6. 对话评估基准</h3><p>当前基准主要使用学术多项选择格式，这不足以评估对话能力。开发专门的对话评估基准对于更准确地测量和增强模型在现实世界场景中的对话性能至关重要。</p>
<h3 id="7-优化视频采样策略"><a href="#7-优化视频采样策略" class="headerlink" title="7. 优化视频采样策略"></a>7. 优化视频采样策略</h3><p>根据视频感知任务的具体要求定制视频采样策略，以优化模型性能。</p>
<h3 id="8-探索不同的数据增强和合成数据技术"><a href="#8-探索不同的数据增强和合成数据技术" class="headerlink" title="8. 探索不同的数据增强和合成数据技术"></a>8. 探索不同的数据增强和合成数据技术</h3><p>使用不同的数据增强技术和合成数据方法来提高模型的泛化能力和鲁棒性。</p>
<h3 id="9-跨模态迁移学习"><a href="#9-跨模态迁移学习" class="headerlink" title="9. 跨模态迁移学习"></a>9. 跨模态迁移学习</h3><p>研究如何通过跨模态迁移学习提高视频-LMMs的性能，特别是在有限标注数据的情况下。</p>
<h3 id="10-实际应用中的模型部署和效率"><a href="#10-实际应用中的模型部署和效率" class="headerlink" title="10. 实际应用中的模型部署和效率"></a>10. 实际应用中的模型部署和效率</h3><p>研究如何将这些大型多模态模型有效地部署到实际应用中，并保持高效率和低延迟。</p>
<p>这些方向能够帮助研究社区更深入地理解视频-LMMs，并推动该领域在视频理解任务中取得更好的性能和应用效果。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文《Apollo: An Exploration of Video Understanding in Large Multimodal Models》主要探讨了在大型多模态模型（LMMs）中视频理解的效率和性能问题。以下是论文的主要内容总结：

<h3 id="1-研究背景与挑战"><a href="#1-研究背景与挑战" class="headerlink" title="1. 研究背景与挑战"></a>1. 研究背景与挑战</h3><ul>
<li>视频-LMMs虽然在视频感知能力上有所进展，但其背后的视频理解机制仍不明确。</li>
<li>视频-LMMs面临更高的计算需求和更复杂的设计空间。</li>
</ul>
<h3 id="2-视频-LMMs设计的关键问题"><a href="#2-视频-LMMs设计的关键问题" class="headerlink" title="2. 视频-LMMs设计的关键问题"></a>2. 视频-LMMs设计的关键问题</h3><ul>
<li>如何采样视频、选择视觉编码器、重采样视频令牌等基本问题尚未得到解答。</li>
</ul>
<h3 id="3-系统探索与发现"><a href="#3-系统探索与发现" class="headerlink" title="3. 系统探索与发现"></a>3. 系统探索与发现</h3><ul>
<li>通过实验分析了视频采样、视觉编码器、令牌重采样和集成策略等视频-LMMs设计的关键方面。</li>
<li>发现了“扩展一致性”现象，即在较小模型上有效的设计决策可以有效地转移到更大的模型上。</li>
</ul>
<h3 id="4-视频基准测试分析"><a href="#4-视频基准测试分析" class="headerlink" title="4. 视频基准测试分析"></a>4. 视频基准测试分析</h3><ul>
<li>分析了现有视频问题回答基准测试的质量、冗余性，并基于这些分析引入了新的基准测试套件ApolloBench。</li>
</ul>
<h3 id="5-Apollo模型家族"><a href="#5-Apollo模型家族" class="headerlink" title="5. Apollo模型家族"></a>5. Apollo模型家族</h3><ul>
<li>基于研究发现，提出了Apollo，一系列在不同模型规模下都能实现优越性能的LMMs。</li>
<li>Apollo模型能够高效感知长达一小时的视频，并在多个基准测试中取得了优于现有模型的性能。</li>
</ul>
<h3 id="6-主要贡献"><a href="#6-主要贡献" class="headerlink" title="6. 主要贡献"></a>6. 主要贡献</h3><ul>
<li>系统地探索了视频建模设计空间，发现关键因素，并提供未来研究的可行见解。</li>
<li>通过引入ApolloBench，解决了评估效率问题，并提供了详细的视频感知任务洞察。</li>
<li>提出了Apollo模型家族，实现了视频理解多个基准上的最先进结果。</li>
</ul>
<h3 id="7-未来工作"><a href="#7-未来工作" class="headerlink" title="7. 未来工作"></a>7. 未来工作</h3><ul>
<li>提出了进一步探索的方向，包括分离架构、训练策略、基于记忆的LMM方法和对话评估基准等。</li>
</ul>
<p>论文通过一系列实验和分析，提供了对视频-LMMs设计和训练的深入理解，并提出了改进视频理解性能的有效方法。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Apollo: An Exploration of Video Understanding in Large Multimodal Models，链接是 https://arxiv.org/pdf/2412.10360 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2412.10360 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8a70a50118f4499eba127f7ce2243549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83c0a28a7e05130948864adf0f2c3a81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eca9f23712daaf1871bc1eecdf535221.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf1023aba3659baaeb78867ab7262819.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2899615fd6b9f1a437fa2a14bd7c395.jpg" align="middle">
</details>



<h2 id="IQViC-In-context-Question-Adaptive-Vision-Compressor-for-Long-term-Video-Understanding-LMMs"><a href="#IQViC-In-context-Question-Adaptive-Vision-Compressor-for-Long-term-Video-Understanding-LMMs" class="headerlink" title="IQViC: In-context, Question Adaptive Vision Compressor for Long-term   Video Understanding LMMs"></a>IQViC: In-context, Question Adaptive Vision Compressor for Long-term   Video Understanding LMMs</h2><p><strong>Authors:Sosuke Yamao, Natsuki Miyahara, Yuki Harazono, Shun Takeuchi</strong></p>
<p>With the increasing complexity of video data and the need for more efficient long-term temporal understanding, existing long-term video understanding methods often fail to accurately capture and analyze extended video sequences. These methods typically struggle to maintain performance over longer durations and to handle the intricate dependencies within the video content. To address these limitations, we propose a simple yet effective large multi-modal model framework for long-term video understanding that incorporates a novel visual compressor, the In-context, Question Adaptive Visual Compressor (IQViC). The key idea, inspired by humans’ selective attention and in-context memory mechanisms, is to introduce a novel visual compressor and incorporate efficient memory management techniques to enhance long-term video question answering. Our framework utilizes IQViC, a transformer-based visual compressor, enabling question-conditioned in-context compression, unlike existing methods that rely on full video visual features. This selectively extracts relevant information, significantly reducing memory token requirements. Through extensive experiments on a new dataset based on InfiniBench for long-term video understanding, and standard benchmarks used for existing methods’ evaluation, we demonstrate the effectiveness of our proposed IQViC framework and its superiority over state-of-the-art methods in terms of video understanding accuracy and memory efficiency. </p>
<blockquote>
<p>随着视频数据的复杂性不断增加和对更高效长期时间理解的需求，现有的长期视频理解方法往往无法准确捕获和分析扩展的视频序列。这些方法通常很难在较长的时间内保持性能，并且难以处理视频内容中的复杂依赖关系。为了解决这些局限性，我们提出了一种简单有效的长期视频理解的大型多模态模型框架，该框架结合了新型视觉压缩机——上下文感知问题适应性视觉压缩机（IQViC）。我们的关键想法是借鉴人类的选择性注意力和上下文记忆机制，引入新型视觉压缩机并结合高效的内存管理技术，以提高长期视频问答的效果。我们的框架使用基于变压器的IQViC视觉压缩机，不同于现有方法依赖于全视频视觉特征，实现了上下文感知的问题条件压缩。这有选择地提取相关信息，显著减少内存令牌需求。我们在基于InfiniBench的长期视频理解的新数据集以及现有方法评估的标准基准测试上进行了广泛的实验，证明了IQViC框架的有效性，并在视频理解准确性和内存效率方面展现了其优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09907v1">PDF</a> </p>
<p><strong>Summary</strong><br>视频理解的复杂性不断上升，对长期时序理解的需求日益增强。现有的长期视频理解方法难以准确捕捉和分析延伸视频序列。为此，我们提出了一个简单有效的大型多模态模型框架，用于长期视频理解，并引入了新颖的视觉压缩机——上下文自适应问题感知视觉压缩机（IQViC）。该框架利用IQViC进行上下文压缩，不同于依赖全视频视觉特征的传统方法。通过在新建的基于InfiniBench的长期视频理解数据集和现有方法评估标准基准上的广泛实验，证明了IQViC框架的有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>长期视频理解面临挑战：难以准确捕捉和分析延伸视频序列。</li>
<li>现有方法在处理复杂依赖性和维持长期性能上表现不足。</li>
<li>引入IQViC框架，结合新颖的视觉压缩机和高效内存管理技术。</li>
<li>IQViC利用上下文信息实现问题感知的压缩方式。</li>
<li>IQViC框架通过选择性提取相关信息，显著减少内存令牌需求。</li>
<li>在基于InfiniBench的新数据集和现有基准上的实验证明了IQViC的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09907">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文试图解决的问题是如何有效地理解和分析长期视频数据。具体来说，论文指出现有的长期视频理解方法在处理和分析较长视频序列时常常无法准确捕捉和分析视频内容，这些方法在视频时长增加时难以保持性能，并且难以处理视频内容内部的复杂依赖关系。为了克服这些限制，论文提出了一个新颖的大型多模态模型框架（LMMs），用于长期视频理解，该框架包含了一个名为In-context, Question Adaptive Visual Compressor（IQViC）的新型视觉压缩器。这个框架的核心思想是借鉴人类的选择性注意力和上下文记忆机制，通过引入新型视觉压缩器和高效的内存管理技术来增强长期视频问答能力。</p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，相关研究主要包括以下几个方面：

<ol>
<li><p><strong>大型多模态模型（Large Multi-modal Models）</strong>：</p>
<ul>
<li>论文提到了BLIP系列模型，该模型通过Querying-transformer连接冻结的图像编码器和冻结的大型语言模型（LLMs），显著减少了可训练参数的数量。</li>
<li>LLaVA系列模型，通过整合CLIP预训练的视觉编码器和Vicuna语言解码器，针对指令遵循任务设计了新构建的数据集。</li>
<li>其他一些模型，如[33–37]中提到的，能够处理多个图像和视频。</li>
</ul>
</li>
<li><p><strong>长期视频理解（Long-term Video Understanding）</strong>：</p>
<ul>
<li>论文讨论了LongVA、MM-Vid和Artemis等框架，这些框架分别通过长上下文传递、视频理解的协同组合和利用文本提示及感兴趣区域注释来解决复杂视频数据的指代表达理解挑战。</li>
<li>论文还提到了一些记忆增强框架，如MALMM、MovieChat和Flash-VStream，这些框架通过存储视频帧的视觉特征到短期或长期记忆库中，并基于存储的特征和输入问题生成答案。</li>
</ul>
</li>
<li><p><strong>上下文压缩（In-context Compression）</strong>：</p>
<ul>
<li>论文提到了GIST、Autocompressor和In-context Autoencoder (ICAE)等方法，这些方法专注于使用预训练的LLMs将长上下文压缩成更短的表示。</li>
<li>这些方法在压缩效率和架构简单性方面具有优势，但主要限于文本模态，对于视觉数据的应用是一个挑战。</li>
</ul>
</li>
</ol>
<p>这些相关研究为论文提出的IQViC框架提供了理论基础和技术背景，特别是在处理长期视频数据和跨模态任务中的LLMs应用方面。论文通过这些相关工作，进一步提出了一个针对长期视频理解的新型视觉压缩器和LMM框架，旨在提高视频理解的准确性和内存效率。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过提出一个名为In-context, Question Adaptive Visual Compressor (IQViC)的新型视觉压缩器来解决长期视频理解的问题。以下是该方法的关键解决方案和步骤：

<h3 id="1-引入IQViC视觉压缩器"><a href="#1-引入IQViC视觉压缩器" class="headerlink" title="1. 引入IQViC视觉压缩器"></a>1. 引入IQViC视觉压缩器</h3><ul>
<li><strong>选择性信息提取</strong>：IQViC基于Transformer的视觉压缩器能够根据问题条件进行上下文压缩，选择性地提取与问题相关的重要信息，显著减少记忆标记的需求。</li>
</ul>
<h3 id="2-基于问题的上下文压缩"><a href="#2-基于问题的上下文压缩" class="headerlink" title="2. 基于问题的上下文压缩"></a>2. 基于问题的上下文压缩</h3><ul>
<li><strong>上下文记忆机制</strong>：IQViC通过上下文记忆机制，将基于问题提取的视觉信息压缩成上下文，并在时间上压缩这些上下文以保持内存效率。</li>
</ul>
<h3 id="3-简单而有效的LMM框架"><a href="#3-简单而有效的LMM框架" class="headerlink" title="3. 简单而有效的LMM框架"></a>3. 简单而有效的LMM框架</h3><ul>
<li><strong>长期视频理解框架</strong>：提出了一个新的大型多模态模型（LMM）框架，该框架结合了IQViC视觉压缩器和上下文记忆机制，以提高长期视频问答的能力。</li>
</ul>
<h3 id="4-训练和推理方法"><a href="#4-训练和推理方法" class="headerlink" title="4. 训练和推理方法"></a>4. 训练和推理方法</h3><ul>
<li><p><strong>两步训练方法</strong>：由于直接在视频帧上训练会导致内存消耗过大，论文采用了两步训练方法。首先，使用图像QA数据集训练视觉压缩器和嵌入模块；其次，使用视频QA数据集对LLM进行微调。</p>
</li>
<li><p><strong>推理方法</strong>：在推理过程中，视频帧首先被视觉压缩器顺序压缩以构建上下文记忆，然后这个上下文记忆与文本提示一起作为输入提供给解码器以生成文本响应。</p>
</li>
</ul>
<h3 id="5-实验验证"><a href="#5-实验验证" class="headerlink" title="5. 实验验证"></a>5. 实验验证</h3><ul>
<li><strong>定量评估</strong>：通过在新的长期视频理解数据集InﬁniBench-Vision和标准基准测试上进行广泛的实验，论文证明了IQViC框架在视频理解准确性和内存效率方面优于现有方法。</li>
</ul>
<p>总结来说，论文通过引入一个能够根据问题自适应压缩视觉信息的新型视觉压缩器，并结合有效的上下文记忆管理技术，提出了一个简单而有效的框架来提高长期视频理解的性能和效率。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 论文中进行了以下实验来验证IQViC框架的有效性：

<h3 id="1-长期视频问答（Long-term-Video-QA）"><a href="#1-长期视频问答（Long-term-Video-QA）" class="headerlink" title="1. 长期视频问答（Long-term Video QA）"></a>1. 长期视频问答（Long-term Video QA）</h3><ul>
<li><strong>数据集</strong>：使用InﬁniBench-Vision数据集，这是从InﬁniBench中筛选出来的子集，包含265个视频和599个开放性问答对，平均视频长度为49分钟。</li>
<li><strong>比较方法</strong>：与现有的最先进方法进行比较，包括MovieChat、MA-LMM和Flash-VStream。</li>
<li><strong>评估指标</strong>：使用GPT模型来计算准确性和得分，基于预测和真实答案来确定预测是否正确，并给出0到5之间的得分。</li>
<li><strong>结果</strong>：IQViC在长期视频理解中取得了最高的平均得分，超过了现有的最先进方法，并且在记忆架构和记忆标记使用上更为简单和高效。</li>
</ul>
<h3 id="2-短期视频问答（Short-term-Video-QA）"><a href="#2-短期视频问答（Short-term-Video-QA）" class="headerlink" title="2. 短期视频问答（Short-term Video QA）"></a>2. 短期视频问答（Short-term Video QA）</h3><ul>
<li><strong>数据集</strong>：在NExT-QA、MSVD-QA和MSRVTT-QA这三个短期视频问答基准上进行实验，这些基准包含时长从10秒到40秒不等的短期视频。</li>
<li><strong>比较方法</strong>：与现有的最先进方法进行比较。</li>
<li><strong>评估指标</strong>：同样使用GPT模型来计算准确性和得分。</li>
<li><strong>结果</strong>：IQViC在所有数据集上都取得了最好的得分，并且在MSRVTT-QA上比之前最好的方法Flash-VStream提高了4.3%的准确性。</li>
</ul>
<h3 id="3-消融研究（Ablation-Study）"><a href="#3-消融研究（Ablation-Study）" class="headerlink" title="3. 消融研究（Ablation Study）"></a>3. 消融研究（Ablation Study）</h3><ul>
<li><strong>压缩器比较</strong>：使用LLaVA-Bench（in-the-wild）图像理解基准数据集来评估视觉压缩器的压缩性能，比较了IQViC的视觉压缩器与平均池化等标准特征压缩技术的性能。</li>
<li><strong>上下文标记大小消融</strong>：研究了上下文标记数量对VQA性能的影响，发现即使只使用一个上下文标记，IQViC也保持了惊人的高准确率。</li>
</ul>
<h3 id="4-案例研究（Case-Study）"><a href="#4-案例研究（Case-Study）" class="headerlink" title="4. 案例研究（Case Study）"></a>4. 案例研究（Case Study）</h3><ul>
<li><strong>示例分析</strong>：通过InﬁniBench-Vision数据集中的一个问答对示例来展示IQViC框架的性能，与其他方法相比，IQViC能够成功回答需要深层上下文理解的问题，而其他方法则未能给出正确答案或产生幻觉答案。</li>
</ul>
<p>这些实验全面评估了IQViC框架在处理长期和短期视频问答任务时的性能，并与现有最先进方法进行了比较，证明了IQViC在视频理解准确性和内存效率方面的优势。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，以下是一些可以进一步探索的点：

<h3 id="1-动态记忆更新"><a href="#1-动态记忆更新" class="headerlink" title="1. 动态记忆更新"></a>1. 动态记忆更新</h3><ul>
<li>论文中提到，为了准确响应新视频问题，需要根据每个文本提示动态更新记忆。当前需要对每个查询重新处理整个视频，这在计算上是昂贵且不切实际的。未来的工作可以集中在开发新的记忆更新技术，以优化记忆状态，并根据问题的关联性动态调整更新幅度。</li>
</ul>
<h3 id="2-端到端训练"><a href="#2-端到端训练" class="headerlink" title="2. 端到端训练"></a>2. 端到端训练</h3><ul>
<li>由于在训练期间高VRAM消耗，论文中的IQViC框架未能实现端到端训练。未来的研究可以探索端到端训练策略，包括滑动窗口或采样技术，并集成长上下文视觉-语言模型，以改善视频理解。</li>
</ul>
<h3 id="3-多模态信息融合"><a href="#3-多模态信息融合" class="headerlink" title="3. 多模态信息融合"></a>3. 多模态信息融合</h3><ul>
<li>与现有方法一样，IQViC目前仅关注视觉信息，忽略了字幕、音频和时间信息等可能有价值的外部知识源。未来的工作可以探索将IQViC扩展为多模态信息压缩器，整合这些外部知识源以显著提高长期视频理解能力。</li>
</ul>
<h3 id="4-因果和时间推理能力"><a href="#4-因果和时间推理能力" class="headerlink" title="4. 因果和时间推理能力"></a>4. 因果和时间推理能力</h3><ul>
<li>论文提出未来的工作将集中于扩展IQViC框架以在压缩和记忆机制中纳入时间信息，以增强因果和时间推理能力。</li>
</ul>
<h3 id="5-扩展到其他模态"><a href="#5-扩展到其他模态" class="headerlink" title="5. 扩展到其他模态"></a>5. 扩展到其他模态</h3><ul>
<li>论文建议将IQViC应用于其他模态，如音频和3D数据，以探索其在不同数据类型上的适用性和有效性。</li>
</ul>
<h3 id="6-优化和加速推理过程"><a href="#6-优化和加速推理过程" class="headerlink" title="6. 优化和加速推理过程"></a>6. 优化和加速推理过程</h3><ul>
<li>研究如何优化IQViC框架以加速推理过程，使其适用于实时或近实时的应用场景。</li>
</ul>
<h3 id="7-鲁棒性和泛化能力"><a href="#7-鲁棒性和泛化能力" class="headerlink" title="7. 鲁棒性和泛化能力"></a>7. 鲁棒性和泛化能力</h3><ul>
<li>对IQViC框架在不同类型、风格和复杂度的视频内容上的鲁棒性和泛化能力进行进一步的研究和评估。</li>
</ul>
<p>这些探索点可以帮助提升IQViC框架的性能，并扩展其在视频理解和多模态信息处理领域的应用范围。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 论文的主要内容可以总结如下：

<ol>
<li><p><strong>问题陈述</strong>：</p>
<ul>
<li>论文指出，现有的长期视频理解方法在处理和分析较长视频序列时常常无法准确捕捉和分析视频内容，特别是在维持性能和处理视频内容内部复杂依赖关系方面存在挑战。</li>
</ul>
</li>
<li><p><strong>IQViC框架</strong>：</p>
<ul>
<li>提出了一个名为In-context, Question Adaptive Visual Compressor (IQViC)的新型视觉压缩器，用于长期视频理解的大型多模态模型（LMMs）框架。</li>
<li>IQViC框架的核心思想是模仿人类的选择性注意力和上下文记忆机制，通过引入新型视觉压缩器和高效的内存管理技术来增强长期视频问答能力。</li>
</ul>
</li>
<li><p><strong>方法论</strong>：</p>
<ul>
<li>IQViC利用基于Transformer的视觉压缩器进行上下文压缩，选择性地提取与问题相关的重要信息，显著减少记忆标记的需求。</li>
<li>该框架包括四个主要组件：视觉编码器、视觉压缩器、上下文记忆和解码器。</li>
<li>采用了两步训练方法，首先使用图像QA数据集训练视觉压缩器和嵌入模块，然后使用视频QA数据集对LLM进行微调。</li>
</ul>
</li>
<li><p><strong>实验验证</strong>：</p>
<ul>
<li>在长期视频理解数据集InﬁniBench-Vision和标准基准测试上进行了广泛的实验，证明了IQViC框架在视频理解准确性和内存效率方面优于现有方法。</li>
<li>IQViC在长期和短期视频问答任务中均展现出优越的性能，并且在记忆架构和记忆标记使用上更为简单和高效。</li>
</ul>
</li>
<li><p><strong>未来工作</strong>：</p>
<ul>
<li>论文提出了几个未来研究方向，包括动态记忆更新、端到端训练、多模态信息融合、因果和时间推理能力的增强，以及其他模态数据的应用。</li>
</ul>
</li>
</ol>
<p>综上所述，论文提出了一个创新的框架来解决长期视频理解中的挑战，并通过一系列实验验证了其有效性。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IQViC: In-context, Question Adaptive Vision Compressor for Long-term Video Understanding LMMs，链接是 https://arxiv.org/pdf/2412.09907 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2412.09907 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7e77acbbdf7bd678beab9808b42d341c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3e0283edf39dbb7828bfa8c4e17c4b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6a76b92fc68f3da184d3f740375bed3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b86085fe1ed0b78d57f2010b8694083.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cd5b11aadac42b7c5cdf1fbae433acc.jpg" align="middle">
</details>



<h2 id="ViCaS-A-Dataset-for-Combining-Holistic-and-Pixel-level-Video-Understanding-using-Captions-with-Grounded-Segmentation"><a href="#ViCaS-A-Dataset-for-Combining-Holistic-and-Pixel-level-Video-Understanding-using-Captions-with-Grounded-Segmentation" class="headerlink" title="ViCaS: A Dataset for Combining Holistic and Pixel-level Video   Understanding using Captions with Grounded Segmentation"></a>ViCaS: A Dataset for Combining Holistic and Pixel-level Video   Understanding using Captions with Grounded Segmentation</h2><p><strong>Authors:Ali Athar, Xueqing Deng, Liang-Chieh Chen</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have expanded research in video understanding, primarily focusing on high-level tasks such as video captioning and question-answering. Meanwhile, a smaller body of work addresses dense, pixel-precise segmentation tasks, which typically involve category-guided or referral-based object segmentation. Although both research directions are essential for developing models with human-level video comprehension, they have largely evolved separately, with distinct benchmarks and architectures. This paper aims to unify these efforts by introducing ViCaS, a new dataset containing thousands of challenging videos, each annotated with detailed, human-written captions and temporally consistent, pixel-accurate masks for multiple objects with phrase grounding. Our benchmark evaluates models on both holistic&#x2F;high-level understanding and language-guided, pixel-precise segmentation. We also present carefully validated evaluation measures and propose an effective model architecture that can tackle our benchmark. Project page: <a target="_blank" rel="noopener" href="https://ali2500.github.io/vicas-project/">https://ali2500.github.io/vicas-project/</a> </p>
<blockquote>
<p>近期多模态大型语言模型（MLLMs）的进步推动了视频理解研究的发展，主要研究重点是高级任务，如视频描述和问答。与此同时，一小部分工作关注密集的、像素精确的分割任务，这通常涉及基于类别指导或基于引用的对象分割。虽然这两个研究方向对于开发具有人类水平视频理解能力的模型都至关重要，但它们大多独立发展，具有不同的基准测试和架构。本文旨在通过引入ViCaS数据集来统一这些努力，该数据集包含数千个具有挑战性的视频，每个视频都经过详细的人类手写描述和与语言描述一致的像素精确的多对象掩膜标注。我们的基准测试对整体&#x2F;高级理解和语言指导的像素精确分割进行了评估。我们还提供了经过仔细验证的评价指标，并提出了能有效应对基准测试模型架构。项目页面：<a target="_blank" rel="noopener" href="https://ali2500.github.io/vicas-project/">https://ali2500.github.io/vicas-project/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09754v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期多模态大型语言模型（MLLMs）的进步推动了视频理解研究的扩展，主要关注高级任务如视频描述和问答。同时，一小部分工作致力于密集、像素精确的分割任务，通常涉及类别指导或引用基础的对象分割。虽然这两个研究方向对于开发具有人类水平视频理解能力的模型至关重要，但它们大多独立发展，具有不同的基准和架构。本文旨在通过引入ViCaS数据集统一这些工作，该数据集包含数千个具有挑战性的视频，每个视频都经过详细的人类书写描述和与语言一致的像素精确掩膜进行标注。该基准评估模型在整体高级理解和语言指导的像素精确分割方面的表现。同时提供了经过仔细验证的评价指标和有效的模型架构来应对这一基准测试。更多详情访问：<a target="_blank" rel="noopener" href="https://ali2500.github.io/vicas-project/">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型的进步促进了视频理解研究的发展。</li>
<li>视频理解研究包括高级任务如视频描述和问答，以及密集、像素精确的分割任务。</li>
<li>目前的研究大多集中在两个独立的研究方向上，分别关注高级理解和像素精确分割任务。</li>
<li>本文引入了ViCaS数据集，包含了多个挑战视频并进行了详细的标注。</li>
<li>该数据集注重整体高级理解和语言指导的像素精确分割评估。</li>
<li>项目提供了仔细验证的评价指标和有效的模型架构以应对这一基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09754">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文试图解决的问题是如何将视频理解中的两个主要研究方向——整体性（holistic）视频理解和像素级（pixel-level）视频理解——结合起来。具体来说，论文指出：

<ol>
<li><p><strong>整体性视频理解</strong>：这个方向主要关注于视频的高级任务，如视频描述（video captioning）和视频问答（question-answering），这些任务侧重于生成视频的高级、概述性文本输出。</p>
</li>
<li><p><strong>像素级视频理解</strong>：这个方向主要关注于预测空间密集的输出，例如基于文本提示的视频中关键对象和角色的定位，这在实际应用中非常重要，例如自动驾驶机器人和视频编辑。</p>
</li>
</ol>
<p>尽管这两个研究方向对于开发具有人类级别视频理解能力的模型都至关重要，但它们在很大程度上是独立发展的，有着不同的基准测试和架构。因此，论文提出了一个新的数据集ViCaS（Video Captioning with Grounded Segmentation），旨在统一这些努力，通过提供包含详细人类编写的字幕和多个对象的像素精确的掩码来评估模型的整体性&#x2F;高级理解和语言引导的像素级分割能力。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，相关研究可以分为以下几个领域：

<h3 id="1-整体性视频理解（Holistic-Video-Understanding）"><a href="#1-整体性视频理解（Holistic-Video-Understanding）" class="headerlink" title="1. 整体性视频理解（Holistic Video Understanding）"></a>1. 整体性视频理解（Holistic Video Understanding）</h3><ul>
<li><p><strong>视频分类（Video Classification）</strong>：</p>
<ul>
<li>早期的视频理解任务之一，通过活动识别数据集（如HMDB、UCF101）流行开来，并随着更大数据集（如Kinetics）的扩展而发展。</li>
<li>早期深度学习方法使用3D CNNs捕获时空信息，随后基于transformer的注意力架构出现，提供了更好的性能。</li>
</ul>
</li>
<li><p><strong>视频描述和问答（Video Captioning and Question-Answering）</strong>：</p>
<ul>
<li>早期数据集（如MSVD、MSR-VTT、TGIF-QA）为视频描述奠定了基础，并后来被改编为问答基准。</li>
<li>最初的方法结合了CNNs进行视觉推理和RNNs进行文本生成。随着transformers的流行，出现了如VideoBERT和UniVL等架构，它们通过学习视觉和语言的共享表示来统一这两种模态。</li>
</ul>
</li>
<li><p><strong>多模态大型语言模型（Multimodal Large Language Models, MLLMs）</strong>：</p>
<ul>
<li>随着LLMs的流行，研究扩展到处理视觉输入（如图像和视频）的多模态模型。</li>
<li>这项研究得到了大规模视频描述数据集和多任务视频理解基准的支持。</li>
</ul>
</li>
</ul>
<h3 id="2-像素级视频理解（Pixel-level-Video-Understanding）"><a href="#2-像素级视频理解（Pixel-level-Video-Understanding）" class="headerlink" title="2. 像素级视频理解（Pixel-level Video Understanding）"></a>2. 像素级视频理解（Pixel-level Video Understanding）</h3><ul>
<li><p><strong>对象跟踪和分割（Object Tracking and Segmentation）</strong>：</p>
<ul>
<li>对象定位和跟踪是一个深入研究的问题，从早期的边界框级对象跟踪发展到像素精确的视频分割。</li>
<li>随着模型架构的进步，出现了基于预定义类别的对象分割任务和给定第一帧真实掩码或点的特定对象分割任务。</li>
</ul>
</li>
<li><p><strong>语言引导分割（Language-Guided Segmentation）</strong>：</p>
<ul>
<li>近期在视觉-语言模型方面的进展推动了多个语言引导的视频分割数据集的发展，包括“开放词汇”数据集和基于引用的视频分割基准。</li>
<li>当前的方法使用文本和图像主干来编码文本提示和视频帧，然后通过transformer解码器和分割头部生成掩码。</li>
</ul>
</li>
</ul>
<p>这些相关研究为ViCaS数据集的提出提供了背景和动机，ViCaS旨在通过结合这两种类型的视频理解任务来填补现有研究之间的空白。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过以下几个关键步骤来解决将整体性视频理解和像素级视频理解结合起来的问题：

<h3 id="1-引入ViCaS数据集"><a href="#1-引入ViCaS数据集" class="headerlink" title="1. 引入ViCaS数据集"></a>1. 引入ViCaS数据集</h3><ul>
<li><strong>数据集构建</strong>：创建了一个包含7,331个视频的数据集，每个视频都有详细的人类编写的字幕和多个对象的像素精确的掩码，这些字幕和掩码在时间上是一致的，并且与短语进行了锚定（phrase grounding）。</li>
</ul>
<h3 id="2-设计基准测试"><a href="#2-设计基准测试" class="headerlink" title="2. 设计基准测试"></a>2. 设计基准测试</h3><ul>
<li><strong>视频字幕任务（Video Captioning）</strong>：要求模型生成一个开放式文本摘要，解释视频中的事件，包括描述显著的对象和背景元素。</li>
<li><strong>语言引导的视频实例分割（Language-Guided Video Instance Segmentation, LG-VIS）</strong>：要求模型基于文本提示预测多个对象的时间一致性分割掩码。</li>
</ul>
<h3 id="3-提出评估措施"><a href="#3-提出评估措施" class="headerlink" title="3. 提出评估措施"></a>3. 提出评估措施</h3><ul>
<li><strong>视频字幕评估</strong>：通过全面的用户研究验证开放式文本相似度的评估措施，选择了基于开源模型的评估方法，并在用户研究中得到验证。</li>
<li><strong>LG-VIS评估</strong>：选择了Track mean Average Precision (mAP)作为主要的评估指标，因为它被广泛用于视频分割基准测试，并且可以适应多对象预测。</li>
</ul>
<h3 id="4-提出Video-LLaVA-Seg模型"><a href="#4-提出Video-LLaVA-Seg模型" class="headerlink" title="4. 提出Video-LLaVA-Seg模型"></a>4. 提出Video-LLaVA-Seg模型</h3><ul>
<li><strong>模型架构</strong>：提出了一个有效的基线模型Video-LLaVA-Seg，该模型可以处理视频字幕和LG-VIS任务，使用单一的、端到端训练的模型。<ul>
<li><strong>视觉特征</strong>：输入视频帧到视觉主干网络，产生一组视频特征。</li>
<li><strong>多模态LLM</strong>：将视觉特征通过投影MLP处理后与文本提示嵌入连接，输入到多模态LLM，输出嵌入。</li>
<li><strong>分割网络</strong>：使用单独的分割主干网络在高分辨率下编码视频帧，产生多尺度特征，与<SEG>标记一起输入到掩码解码器以获得最终的分割掩码。</li>
</ul>
</li>
</ul>
<h3 id="5-实施和消融研究"><a href="#5-实施和消融研究" class="headerlink" title="5. 实施和消融研究"></a>5. 实施和消融研究</h3><ul>
<li><strong>实现细节</strong>：提供了模型的详细实现和训练超参数。</li>
<li><strong>消融研究</strong>：通过消融研究验证了慢速-快速池化和任务特定训练对模型性能的影响。</li>
</ul>
<p>通过这些步骤，论文不仅提供了一个新的数据集和基准测试来评估视频理解的两个方面，而且还提出了一个有效的模型架构来处理这些任务，并验证了所提出的评估措施的有效性。这些贡献为视频理解领域提供了新的研究方向和工具。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，作者进行了以下实验：

<h3 id="1-用户研究以验证评估措施"><a href="#1-用户研究以验证评估措施" class="headerlink" title="1. 用户研究以验证评估措施"></a>1. 用户研究以验证评估措施</h3><ul>
<li><strong>视频字幕评分用户研究</strong>：为了确定视频字幕任务的评估措施，作者进行了一个两阶段的用户研究，比较了多种评分方法，包括经典的文本相似度度量、基于嵌入的相似度度量和基于最近的大型语言模型（LLM）的评分方法。用户研究的目的是评估这些方法与人类评分之间的相关性以及它们对正面和负面样本的鲁棒性。</li>
</ul>
<h3 id="2-模型架构实验"><a href="#2-模型架构实验" class="headerlink" title="2. 模型架构实验"></a>2. 模型架构实验</h3><ul>
<li><strong>消融研究</strong>：作者进行了消融研究来评估慢速-快速池化（slow-fast pooling）和针对特定任务训练的影响。这些实验包括：<ul>
<li>移除慢速-快速池化，使用相同数量的帧来评估其对视频字幕准确性（Caption Accuracy, CA）和语言引导的视频实例分割（Language-Guided Video Instance Segmentation, LG-VIS）性能的影响。</li>
<li>仅对视频字幕任务训练模型，以及仅对LG-VIS任务训练模型，以观察多任务训练的效果。</li>
</ul>
</li>
</ul>
<h3 id="3-基准测试结果"><a href="#3-基准测试结果" class="headerlink" title="3. 基准测试结果"></a>3. 基准测试结果</h3><ul>
<li><strong>模型性能评估</strong>：作者提出了Video-LLaVA-Seg模型，并在ViCaS数据集的验证集和测试集上评估了其性能。性能指标包括视频字幕的Caption Accuracy（CA）和LG-VIS的mAP（mean Average Precision）。作者将Video-LLaVA-Seg与其他现有方法（如LLaVA-Next和一些特定的Referral-VOS方法）进行了比较。</li>
</ul>
<h3 id="4-实施细节"><a href="#4-实施细节" class="headerlink" title="4. 实施细节"></a>4. 实施细节</h3><ul>
<li><strong>模型实现和训练</strong>：作者提供了Video-LLaVA-Seg模型的详细实现细节和训练超参数，并在不同阶段对模型进行了训练和微调，包括仅用于视频字幕的训练和结合视频字幕及LG-VIS的联合训练。</li>
</ul>
<p>这些实验旨在验证所提出的评估措施的有效性，展示Video-LLaVA-Seg模型在处理视频字幕和LG-VIS任务时的性能，并与现有方法进行比较。通过这些实验，作者证明了他们的方法在结合整体性和像素级视频理解方面的有效性和优越性。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 尽管论文提出了ViCaS数据集和相应的评估方法以及模型架构，但仍有一些领域可以进一步探索：

<h3 id="1-模型架构的改进"><a href="#1-模型架构的改进" class="headerlink" title="1. 模型架构的改进"></a>1. 模型架构的改进</h3><ul>
<li><strong>多模态融合方法</strong>：探索不同的视觉和语言融合方法，以更有效地结合这两种模态的信息。</li>
<li><strong>注意力机制</strong>：研究更复杂的注意力机制，以更好地定位视频和文本之间的相关部分。</li>
<li><strong>跨模态注意力</strong>：进一步探索跨模态注意力，以增强模型对视频内容和语言描述之间对应关系的理解。</li>
</ul>
<h3 id="2-数据集扩展和多样化"><a href="#2-数据集扩展和多样化" class="headerlink" title="2. 数据集扩展和多样化"></a>2. 数据集扩展和多样化</h3><ul>
<li><strong>多语言字幕</strong>：扩展数据集以包含多语言字幕，以增强模型的泛化能力，并支持跨语言的视频理解任务。</li>
<li><strong>更复杂的场景</strong>：增加更多复杂背景和动态场景的视频，以测试和提高模型在更具挑战性条件下的性能。</li>
<li><strong>更广泛的对象和事件</strong>：扩大数据集中对象和事件的多样性，以覆盖更广泛的日常生活和专业领域。</li>
</ul>
<h3 id="3-评估指标的完善"><a href="#3-评估指标的完善" class="headerlink" title="3. 评估指标的完善"></a>3. 评估指标的完善</h3><ul>
<li><strong>细粒度评估</strong>：开发更细粒度的评估指标，以更精确地衡量模型在特定类型的对象或事件上的性能。</li>
<li><strong>用户研究</strong>：进行更广泛的用户研究，以验证评估指标的可靠性和模型输出的可用性。</li>
</ul>
<h3 id="4-应用研究"><a href="#4-应用研究" class="headerlink" title="4. 应用研究"></a>4. 应用研究</h3><ul>
<li><strong>视频编辑和摘要</strong>：研究如何利用模型进行视频编辑和自动生成视频摘要。</li>
<li><strong>视频问答系统</strong>：开发基于模型的视频问答系统，以提供更丰富的用户体验。</li>
<li><strong>多模态交互系统</strong>：探索如何将模型集成到多模态交互系统中，以实现更自然的人机交互。</li>
</ul>
<h3 id="5-计算效率和可扩展性"><a href="#5-计算效率和可扩展性" class="headerlink" title="5. 计算效率和可扩展性"></a>5. 计算效率和可扩展性</h3><ul>
<li><strong>模型压缩和加速</strong>：研究模型压缩和加速技术，以使模型能够在资源受限的设备上运行。</li>
<li><strong>大规模训练和推理</strong>：探索如何有效地在大规模数据集上进行训练和推理，以提高模型的可扩展性和实用性。</li>
</ul>
<h3 id="6-鲁棒性和偏差分析"><a href="#6-鲁棒性和偏差分析" class="headerlink" title="6. 鲁棒性和偏差分析"></a>6. 鲁棒性和偏差分析</h3><ul>
<li><strong>对抗攻击和防御</strong>：研究模型对对抗攻击的鲁棒性，并开发相应的防御机制。</li>
<li><strong>偏差检测和缓解</strong>：分析模型是否存在偏差，并探索方法来减少这些偏差，以提高模型的公平性和可解释性。</li>
</ul>
<p>这些探索方向不仅可以推动视频理解技术的发展，还可能带来新的应用场景和研究问题。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文的主要内容包括以下几个方面：

<ol>
<li><p><strong>问题陈述</strong>：论文指出了视频理解领域中两个主要研究方向——整体性视频理解和像素级视频理解——之间的鸿沟，并提出了一个新的数据集ViCaS来弥合这一差距。</p>
</li>
<li><p><strong>ViCaS数据集</strong>：介绍了一个新的视频数据集ViCaS，包含数千个挑战性视频，每个视频都有详细的人类编写的字幕和多个对象的像素精确的掩码，这些字幕和掩码在时间上是一致的，并且与短语进行了锚定。</p>
</li>
<li><p><strong>基准测试设计</strong>：提出了一个基准测试，包括两个任务：视频字幕（要求模型生成描述视频事件和对象的详细文本）和语言引导的视频实例分割（LG-VIS，要求模型基于文本提示预测多个对象的时间一致性分割掩码）。</p>
</li>
<li><p><strong>评估措施</strong>：通过全面的用户研究验证了开放式文本相似度的评估措施，并选择了基于开源模型的评估方法。</p>
</li>
<li><p><strong>Video-LLaVA-Seg模型</strong>：提出了一个有效的端到端训练架构Video-LLaVA-Seg，它可以处理视频字幕和LG-VIS任务。</p>
</li>
<li><p><strong>实验</strong>：进行了广泛的实验，包括用户研究以验证评估措施的有效性，消融研究来评估慢速-快速池化和任务特定训练的影响，以及基准测试结果来展示Video-LLaVA-Seg模型的性能。</p>
</li>
<li><p><strong>结论</strong>：论文希望ViCaS数据集和Video-LLaVA-Seg模型能够推动视频理解领域的研究，特别是在结合整体性和像素级视频理解方面。</p>
</li>
</ol>
<p>总体而言，这篇论文通过引入一个新的数据集、设计基准测试、提出评估措施和开发有效的模型架构，为视频理解领域提供了新的研究方向和工具。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation，链接是 https://arxiv.org/pdf/2412.09754 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2412.09754 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0ea843c9aeade495d57693b4f60a40e7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66f591ac1e0ebbac34b1c7597e57be68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42fdb9ac12ee801686ce3027a897e58a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd6c6174cef352cc65a92a90ee1bb736.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eb5024954f36ad393424d8099cfcf3f.jpg" align="middle">
</details>




                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-17/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-17/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">视频理解</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-17/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2f260aaa0dbc816d63c11e53641c516f.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2024-12-17  RemDet Rethinking Efficient Model Design for UAV Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-17/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-18541fcb8a81be6520ecca03c49eba2e.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2024-12-17  RAID-Database human Responses to Affine Image Distortions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11676k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
