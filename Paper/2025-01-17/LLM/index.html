<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  Multimodal LLMs Can Reason about Aesthetics in Zero-Shot">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-08a66ea4ab4c504da314507094064799.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-17-æ›´æ–°"><a href="#2025-01-17-æ›´æ–°" class="headerlink" title="2025-01-17 æ›´æ–°"></a>2025-01-17 æ›´æ–°</h1><h2 id="Multimodal-LLMs-Can-Reason-about-Aesthetics-in-Zero-Shot"><a href="#Multimodal-LLMs-Can-Reason-about-Aesthetics-in-Zero-Shot" class="headerlink" title="Multimodal LLMs Can Reason about Aesthetics in Zero-Shot"></a>Multimodal LLMs Can Reason about Aesthetics in Zero-Shot</h2><p><strong>Authors:Ruixiang Jiang, Changwen Chen</strong></p>
<p>We present the first study on how Multimodal LLMsâ€™ (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method for human preference modeling and perform a systematic correlation analysis between MLLMsâ€™ responses and human preference. Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity. ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMsâ€™ reasoning ability for aesthetics. Our findings offer valuable insights into MLLMs for art and can benefit a wide range of downstream applications, such as style transfer and artistic image generation. Code available at <a target="_blank" rel="noopener" href="https://github.com/songrise/MLLM4Art">https://github.com/songrise/MLLM4Art</a>. </p>
<blockquote>
<p>æˆ‘ä»¬é¦–æ¬¡ç ”ç©¶äº†å¦‚ä½•æ¿€å‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ¥è¯„ä¼°è‰ºæœ¯ä½œå“çš„å®¡ç¾ä»·å€¼ã€‚ä¸ºäº†æ¨åŠ¨è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬æ„å»ºäº†MM-StyleBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŸºå‡†æµ‹è¯•è‰ºæœ¯é£æ ¼åŒ–çš„æ–°å‹é«˜è´¨é‡æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§äººç±»åå¥½å»ºæ¨¡çš„åŸåˆ™æ–¹æ³•ï¼Œå¹¶å¯¹MLLMsçš„å“åº”ä¸äººç±»åå¥½ä¹‹é—´è¿›è¡Œäº†ç³»ç»Ÿçš„ç›¸å…³æ€§åˆ†æã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†MLLMsåœ¨è‰ºæœ¯è¯„ä¼°ä¸­å­˜åœ¨çš„å†…åœ¨å¹»è§‰é—®é¢˜ï¼Œè¿™ä¸å“åº”ä¸»è§‚æ€§æœ‰å…³ã€‚æˆ‘ä»¬æå‡ºäº†ArtCoTï¼Œè¯æ˜è‰ºæœ¯ç‰¹å®šä»»åŠ¡åˆ†è§£å’Œä½¿ç”¨å…·ä½“è¯­è¨€å¯ä»¥æé«˜MLLMsçš„å®¡ç¾æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºMLLMsåœ¨è‰ºæœ¯é¢†åŸŸçš„åº”ç”¨æä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶å¯ä»¥å¹¿æ³›åº”ç”¨äºä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚é£æ ¼è½¬æ¢å’Œè‰ºæœ¯å›¾åƒç”Ÿæˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/songrise/MLLM4Art%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/songrise/MLLM4Artæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09012v1">PDF</a> WIP, Homepage <a target="_blank" rel="noopener" href="https://github.com/songrise/MLLM4Art">https://github.com/songrise/MLLM4Art</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æ¿€å‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ä»¥è¯„ä¼°è‰ºæœ¯ä½œå“çš„å®¡ç¾ä»·å€¼ã€‚ç ”ç©¶æ„å»ºäº†MM-StyleBenchæ•°æ®é›†ï¼Œç”¨äºåŸºå‡†æµ‹è¯•è‰ºæœ¯é£æ ¼åŒ–ã€‚é€šè¿‡å¼€å‘äººç±»åå¥½å»ºæ¨¡çš„æ–¹æ³•ï¼Œå¹¶ç³»ç»Ÿåœ°åˆ†æäº†MLLMsçš„å›åº”ä¸äººç±»åå¥½çš„ç›¸å…³æ€§ã€‚å®éªŒæ­ç¤ºäº†MLLMsåœ¨è‰ºæœ¯è¯„ä»·ä¸­çš„å†…åœ¨å¹»è§‰é—®é¢˜ï¼Œä¸å›åº”çš„ä¸»è§‚æ€§ç›¸å…³ã€‚æå‡ºäº†ArtCoTï¼Œè¡¨æ˜è‰ºæœ¯ç‰¹å®šä»»åŠ¡åˆ†è§£å’Œä½¿ç”¨å…·ä½“è¯­è¨€å¯ä»¥æé«˜MLLMsçš„å®¡ç¾æ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶ä¸ºMLLMsåœ¨è‰ºæœ¯é¢†åŸŸçš„åº”ç”¨æä¾›äº†å®è´µè§è§£ï¼Œå¹¶æœ‰ç›Šäºé£æ ¼è½¬æ¢å’Œè‰ºæœ¯å›¾åƒç”Ÿæˆç­‰ä¸‹æ¸¸åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯„ä¼°è‰ºæœ¯ä½œå“å®¡ç¾ä»·å€¼ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ„å»ºäº†MM-StyleBenchæ•°æ®é›†ï¼Œä¸ºè‰ºæœ¯é£æ ¼åŒ–æä¾›äº†åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>é€šè¿‡äººç±»åå¥½å»ºæ¨¡çš„æ–¹æ³•ï¼Œåˆ†æäº†MLLMsçš„å›åº”ä¸äººç±»åå¥½çš„ç›¸å…³æ€§ã€‚</li>
<li>æ­ç¤ºäº†MLLMsåœ¨è‰ºæœ¯è¯„ä»·ä¸­å­˜åœ¨çš„å†…åœ¨å¹»è§‰é—®é¢˜ï¼Œä¸å›åº”çš„ä¸»è§‚æ€§ç›¸å…³ã€‚</li>
<li>æå‡ºäº†ArtCoTæ–¹æ³•ï¼Œé€šè¿‡è‰ºæœ¯ç‰¹å®šä»»åŠ¡åˆ†è§£å’Œä½¿ç”¨å…·ä½“è¯­è¨€æé«˜äº†MLLMsçš„å®¡ç¾æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æˆæœå¯¹MLLMsåœ¨è‰ºæœ¯é¢†åŸŸçš„åº”ç”¨å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
<li>ç ”ç©¶æœ‰ç›Šäºé£æ ¼è½¬æ¢å’Œè‰ºæœ¯å›¾åƒç”Ÿæˆç­‰ä¸‹æ¸¸åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-efe26e0f6037342dd0da7e42db87b35f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7479ee8ca56672de6fca439f00f17799.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c9c51b88a4e27f50a2eeea89283d20b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36466d6623908a54b07c52fef63db9e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0459e8a6b7257143c430b52d4a78f8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e93a31c13d60971a8578ec9d445a62ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-799288b3b5e7d9e856b02ccc1098c021.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c46bbc14136cf893fc68ddf07ddf97b2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ToMATO-Verbalizing-the-Mental-States-of-Role-Playing-LLMs-for-Benchmarking-Theory-of-Mind"><a href="#ToMATO-Verbalizing-the-Mental-States-of-Role-Playing-LLMs-for-Benchmarking-Theory-of-Mind" class="headerlink" title="ToMATO: Verbalizing the Mental States of Role-Playing LLMs for   Benchmarking Theory of Mind"></a>ToMATO: Verbalizing the Mental States of Role-Playing LLMs for   Benchmarking Theory of Mind</h2><p><strong>Authors:Kazutoshi Shinoda, Nobukatsu Hojo, Kyosuke Nishida, Saki Mizuno, Keita Suzuki, Ryo Masumura, Hiroaki Sugiyama, Kuniko Saito</strong></p>
<p>Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in three aspects: 1) they assess a limited range of mental states such as beliefs, 2) false beliefs are not comprehensively explored, and 3) the diverse personality traits of characters are overlooked. To address these challenges, we introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over conversations. ToMATO is generated via LLM-LLM conversations featuring information asymmetry. By employing a prompting method that requires role-playing LLMs to verbalize their thoughts before each utterance, we capture both first- and second-order mental states across five categories: belief, intention, desire, emotion, and knowledge. These verbalized thoughts serve as answers to questions designed to assess the mental states of characters within conversations. Furthermore, the information asymmetry introduced by hiding thoughts from others induces the generation of false beliefs about various mental states. Assigning distinct personality traits to LLMs further diversifies both utterances and thoughts. ToMATO consists of 5.4k questions, 753 conversations, and 15 personality trait patterns. Our analysis shows that this dataset construction approach frequently generates false beliefs due to the information asymmetry between role-playing LLMs, and effectively reflects diverse personalities. We evaluate nine LLMs on ToMATO and find that even GPT-4o mini lags behind human performance, especially in understanding false beliefs, and lacks robustness to various personality traits. </p>
<blockquote>
<p>ç°æœ‰çš„å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰åŸºå‡†ä¸çœŸå®ä¸–ç•Œåœºæ™¯åœ¨ä¸‰ä¸ªæ–¹é¢å­˜åœ¨åˆ†æ­§ï¼š1ï¼‰å®ƒä»¬è¯„ä¼°çš„å¿ƒæ€èŒƒå›´æœ‰é™ï¼Œå¦‚ä¿¡å¿µï¼›2ï¼‰å¯¹é”™è¯¯ä¿¡å¿µæ²¡æœ‰è¿›è¡Œå…¨é¢æ¢ç´¢ï¼›3ï¼‰å¿½ç•¥äº†è§’è‰²çš„å¤šæ ·æ€§æ ¼ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ToMATOï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¿ƒæ™ºç†è®ºåŸºå‡†ï¼Œä»¥å¯¹è¯å½¢å¼çš„å¤šé¡¹é€‰æ‹©é¢˜è¿›è¡Œè¯„ä¼°ã€‚ToMATOæ˜¯é€šè¿‡LLM-LLMå¯¹è¯ç”Ÿæˆçš„ï¼Œä»¥ä¿¡æ¯ä¸å¯¹ç§°ä¸ºç‰¹å¾ã€‚é€šè¿‡é‡‡ç”¨ä¸€ç§æç¤ºæ–¹æ³•ï¼Œè¦æ±‚æ‰®æ¼”è§’è‰²çš„LLMåœ¨æ¯æ¬¡å‘è¨€å‰å…ˆè¡¨è¾¾ä»–ä»¬çš„æƒ³æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ•æ‰åˆ°äº”ä¸ªç±»åˆ«ä¸­çš„ä¸€é˜¶å’ŒäºŒé˜¶å¿ƒæ€ï¼šä¿¡å¿µã€æ„å›¾ã€æ¬²æœ›ã€æƒ…æ„Ÿå’ŒçŸ¥è¯†ã€‚è¿™äº›è¡¨è¾¾å‡ºæ¥çš„æƒ³æ³•å¯ä»¥ä½œä¸ºè®¾è®¡é—®é¢˜çš„ç­”æ¡ˆï¼Œä»¥è¯„ä¼°å¯¹è¯ä¸­è§’è‰²çš„å¿ƒæ€ã€‚æ­¤å¤–ï¼Œé€šè¿‡éšè—ä»–äººçš„æƒ³æ³•æ¥å¼•å…¥ä¿¡æ¯ä¸å¯¹ç§°ä¼šå¯¼è‡´å¯¹å„ç§å¿ƒæ€çš„è™šå‡ä¿¡å¿µçš„äº§ç”Ÿã€‚ç»™LLMåˆ†é…ä¸åŒçš„æ€§æ ¼ç‰¹å¾è¿›ä¸€æ­¥ä½¿å‘è¨€å’Œæƒ³æ³•å¤šæ ·åŒ–ã€‚ToMATOåŒ…å«5400ä¸ªé—®é¢˜ã€753ä¸ªå¯¹è¯å’Œ15ç§æ€§æ ¼ç‰¹å¾æ¨¡å¼ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œç”±äºè§’è‰²æ‰®æ¼”LLMä¹‹é—´çš„ä¿¡æ¯ä¸å¯¹ç§°ï¼Œè¿™ç§æ•°æ®é›†æ„å»ºæ–¹æ³•ç»å¸¸ä¼šäº§ç”Ÿè™šå‡ä¿¡å¿µï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°åæ˜ ä¸åŒçš„ä¸ªæ€§ã€‚æˆ‘ä»¬åœ¨ToMATOä¸Šè¯„ä¼°äº†ä¹ä¸ªLLMï¼Œå‘ç°å³ä½¿æ˜¯GPT-4o miniä¹Ÿè½åäºäººç±»çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£è™šå‡ä¿¡å¿µæ–¹é¢ï¼Œå¹¶ä¸”å¯¹ä¸åŒçš„æ€§æ ¼ç‰¹å¾ç¼ºä¹ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08838v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°æœ‰çš„å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰åŸºå‡†æµ‹è¯•åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬è¯„ä¼°çš„å¿ƒç†çŠ¶æ€èŒƒå›´æœ‰é™ã€æœªèƒ½å…¨é¢æ¢ç´¢é”™è¯¯ä¿¡å¿µä»¥åŠå¿½ç•¥è§’è‰²çš„ä¸åŒä¸ªæ€§ç‰¹å¾ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ToMATOï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¿ƒæ™ºç†è®ºåŸºå‡†æµ‹è¯•ï¼Œä»¥å¯¹è¯å½¢å¼çš„å¤šé¡¹é€‰æ‹©é¢˜è¿›è¡Œè¯„ä¼°ã€‚ToMATOé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹é—´çš„å¯¹è¯ç”Ÿæˆï¼Œå¹¶åˆ©ç”¨ä¿¡æ¯ä¸å¯¹ç§°çš„ç‰¹ç‚¹ã€‚é€šè¿‡ä½¿ç”¨è¦æ±‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¯æ¬¡å‘è¨€å‰æ€è€ƒå¹¶è¡¨è¾¾å…¶æƒ³æ³•çš„æç¤ºæ–¹æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ•æ‰äº”ä¸ªç±»åˆ«ï¼ˆä¿¡å¿µã€æ„å›¾ã€æ¬²æœ›ã€æƒ…æ„Ÿå’ŒçŸ¥è¯†ï¼‰ä¸­çš„ç¬¬ä¸€å’Œç¬¬äºŒé˜¶å¿ƒç†çŠ¶æ€ã€‚è¿™äº›è¡¨è¾¾çš„æƒ³æ³•å¯ä»¥ä½œä¸ºè®¾è®¡é—®é¢˜ä»¥è¯„ä¼°å¯¹è¯ä¸­è§’è‰²çš„å¿ƒç†çŠ¶æ€æ—¶çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œé€šè¿‡éšè—ä»–äººçš„æƒ³æ³•æ¥å¼•å…¥ä¿¡æ¯ä¸å¯¹ç§°æ€§ä¼šå¯¼è‡´å„ç§å¿ƒç†çŠ¶æ€ä¸Šçš„é”™è¯¯ä¿¡å¿µçš„äº§ç”Ÿã€‚ç»™å¤§å‹è¯­è¨€æ¨¡å‹åˆ†é…ä¸åŒçš„ä¸ªæ€§ç‰¹å¾å¯ä»¥è¿›ä¸€æ­¥ä½¿å‘è¨€å’Œæƒ³æ³•å¤šæ ·åŒ–ã€‚ToMATOåŒ…å«5.4kä¸ªé—®é¢˜ã€753ä¸ªå¯¹è¯å’Œ15ç§ä¸ªæ€§ç‰¹å¾æ¨¡å¼ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼Œç”±äºè§’è‰²æ‰®æ¼”å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„ä¿¡æ¯ä¸å¯¹ç§°æ€§ï¼Œè¿™ç§æ•°æ®é›†æ„å»ºæ–¹æ³•ç»å¸¸äº§ç”Ÿé”™è¯¯ä¿¡å¿µï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°åæ˜ ä¸åŒçš„ä¸ªæ€§ç‰¹å¾ã€‚æˆ‘ä»¬å¯¹ä¹ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ToMATOä¸Šçš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯GPT-4o miniä¹Ÿè½åäºäººç±»çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£é”™è¯¯ä¿¡å¿µæ–¹é¢ç¼ºä¹å¯¹ä¸åŒä¸ªæ€§ç‰¹å¾çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰å¿ƒæ™ºç†è®ºåŸºå‡†æµ‹è¯•åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦ä½“ç°åœ¨è¯„ä¼°çš„å¿ƒç†çŠ¶æ€èŒƒå›´æœ‰é™ã€æœªèƒ½å…¨é¢æ¢ç´¢é”™è¯¯ä¿¡å¿µå’Œå¿½ç•¥è§’è‰²ä¸ªæ€§ç‰¹å¾æ–¹é¢ã€‚</li>
<li>ToMATOæ˜¯ä¸€ç§æ–°çš„å¿ƒæ™ºç†è®ºåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹é—´çš„å¯¹è¯ç”Ÿæˆï¼Œèƒ½å¤Ÿæ•æ‰ç¬¬ä¸€å’Œç¬¬äºŒé˜¶å¿ƒç†çŠ¶æ€ã€‚</li>
<li>ToMATOé€šè¿‡å¼•å…¥ä¿¡æ¯ä¸å¯¹ç§°æ€§å’Œè§’è‰²ä¸ªæ€§ç‰¹å¾æ¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œåœºæ™¯ï¼Œä»è€Œç”ŸæˆåŒ…å«é”™è¯¯ä¿¡å¿µçš„å¯¹è¯ã€‚</li>
<li>ToMATOæ•°æ®é›†åŒ…å«å¤§é‡é—®é¢˜å’Œå¯¹è¯ï¼Œæœ‰æ•ˆåæ˜ ä¸åŒä¸ªæ€§ç‰¹å¾çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„äº¤äº’ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿åœ¨ç†è§£é”™è¯¯ä¿¡å¿µæ–¹é¢ï¼Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ä»æœ‰äººç±»è¡¨ç°å·®è·ï¼Œä¸”ç¼ºä¹å¯¹ä¸åŒä¸ªæ€§ç‰¹å¾çš„ç¨³å¥æ€§ã€‚</li>
<li>ToMATOä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¿ƒæ™ºç†è®ºæ–¹é¢çš„èƒ½åŠ›æä¾›äº†æ›´å…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7bf65b642f65978bd8b1a3997579c69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b27cd6ff5d8d7ffa49efef364d808c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20384eef71dec7cd829662140f66d4e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15a389b553dd0c5d66bb795eeeb0571e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0127665a95b1454f4df14ca30b3253f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0cd6256eba54eb291caede63c2e366f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-ChatGPT-for-Face-Presentation-Attack-Detection-in-Zero-and-Few-Shot-in-Context-Learning"><a href="#Exploring-ChatGPT-for-Face-Presentation-Attack-Detection-in-Zero-and-Few-Shot-in-Context-Learning" class="headerlink" title="Exploring ChatGPT for Face Presentation Attack Detection in Zero and   Few-Shot in-Context Learning"></a>Exploring ChatGPT for Face Presentation Attack Detection in Zero and   Few-Shot in-Context Learning</h2><p><strong>Authors:Alain Komaty, Hatef Otroshi Shahreza, Anjith George, Sebastien Marcel</strong></p>
<p>This study highlights the potential of ChatGPT (specifically GPT-4o) as a competitive alternative for Face Presentation Attack Detection (PAD), outperforming several PAD models, including commercial solutions, in specific scenarios. Our results show that GPT-4o demonstrates high consistency, particularly in few-shot in-context learning, where its performance improves as more examples are provided (reference data). We also observe that detailed prompts enable the model to provide scores reliably, a behavior not observed with concise prompts. Additionally, explanation-seeking prompts slightly enhance the modelâ€™s performance by improving its interpretability. Remarkably, the model exhibits emergent reasoning capabilities, correctly predicting the attack type (print or replay) with high accuracy in few-shot scenarios, despite not being explicitly instructed to classify attack types. Despite these strengths, GPT-4o faces challenges in zero-shot tasks, where its performance is limited compared to specialized PAD systems. Experiments were conducted on a subset of the SOTERIA dataset, ensuring compliance with data privacy regulations by using only data from consenting individuals. These findings underscore GPT-4oâ€™s promise in PAD applications, laying the groundwork for future research to address broader data privacy concerns and improve cross-dataset generalization. Code available here: <a target="_blank" rel="noopener" href="https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad">https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad</a> </p>
<blockquote>
<p>æœ¬ç ”ç©¶å¼ºè°ƒäº†ChatGPTï¼ˆç‰¹åˆ«æ˜¯GPT-4oï¼‰åœ¨äººè„¸è¯†åˆ«æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰ä¸­çš„æ½œåœ¨ç«äº‰åŠ›ã€‚åœ¨ç‰¹å®šåœºæ™¯ä¸­ï¼ŒGPT-4oçš„æ€§èƒ½ä¼˜äºå¤šä¸ªPADæ¨¡å‹ï¼ŒåŒ…æ‹¬å•†ä¸šè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGPT-4oåœ¨é«˜ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ çš„åœºæ™¯ä¸­ï¼Œå…¶æ€§èƒ½ä¼šéšç€æä¾›æ›´å¤šç¤ºä¾‹ï¼ˆå‚è€ƒæ•°æ®ï¼‰è€Œæé«˜ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œè¯¦ç»†çš„æç¤ºä½¿æ¨¡å‹èƒ½å¤Ÿå¯é åœ°æä¾›åˆ†æ•°ï¼Œè¿™ä¸€è¡Œä¸ºåœ¨ç®€æ´çš„æç¤ºä¸‹å¹¶æœªè§‚å¯Ÿåˆ°ã€‚æ­¤å¤–ï¼Œå¯»æ±‚è§£é‡Šçš„æç¤ºç•¥å¾®æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æ”¹å–„äº†å…¶å¯è§£é‡Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æœªæ˜ç¡®æŒ‡ç¤ºå…¶åˆ†ç±»æ”»å‡»ç±»å‹ï¼Œè¯¥æ¨¡å‹ä»å±•ç°å‡ºæ–°å…´æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å°‘é‡åœºæ™¯ä¸­å‡†ç¡®é¢„æµ‹æ”»å‡»ç±»å‹ï¼ˆæ‰“å°æˆ–å›æ”¾ï¼‰ã€‚å°½ç®¡æœ‰è¿™äº›ä¼˜ç‚¹ï¼ŒGPT-4oåœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå…¶æ€§èƒ½ä¸ä¸“ç”¨PADç³»ç»Ÿç›¸æ¯”æœ‰é™ã€‚å®éªŒæ˜¯åœ¨SOTERIAæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ä¸Šè¿›è¡Œçš„ï¼Œä»…ä½¿ç”¨åŒæ„ä¸ªäººçš„æ•°æ®ï¼Œä»¥ç¡®ä¿ç¬¦åˆæ•°æ®éšç§æ³•è§„ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†GPT-4oåœ¨PADåº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œä»¥è§£å†³æ›´å¹¿æ³›çš„æ•°æ®éšç§æ‹…å¿§å¹¶æ”¹è¿›è·¨æ•°æ®é›†æ³›åŒ–ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad">https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08799v1">PDF</a> Accepted in WACV workshop 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ChatGPTï¼ˆç‰¹åˆ«æ˜¯GPT-4oï¼‰åœ¨äººè„¸è¯†åˆ«æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰ä¸­çš„æ½œåŠ›ï¼Œå¹¶åœ¨ç‰¹å®šåœºæ™¯ä¸­è¡¨ç°å‡ºè¶…è¶Šå¤šç§PADæ¨¡å‹ï¼ˆåŒ…æ‹¬å•†ä¸šè§£å†³æ–¹æ¡ˆï¼‰çš„æ€§èƒ½ã€‚GPT-4oåœ¨é«˜ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰ä¸Šä¸‹æ–‡å­¦ä¹ çš„å°‘æ•°åœºæ™¯ä¸­çš„è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œè¯¦ç»†çš„æç¤ºæœ‰åŠ©äºæ¨¡å‹æä¾›æ›´å¯é çš„è¯„åˆ†ï¼ŒåŒæ—¶è§£é‡Šæ€§æç¤ºç•¥å¾®æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚å°½ç®¡GPT-4oåœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä½†å…¶ä»ç„¶å…·å¤‡å‡ºè‰²çš„æ½œåŠ›ï¼Œä¸”å…¶åœ¨é¢„æµ‹æ”»å‡»ç±»å‹æ–¹é¢è¡¨ç°å‡ºæƒŠäººçš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ˜¯åœ¨éµå®ˆæ•°æ®éšç§æ³•è§„çš„å‰æä¸‹è¿›è¡Œçš„å®éªŒï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oåœ¨äººè„¸è¯†åˆ«æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰ä¸­å…·æœ‰æ½œåŠ›ï¼Œèƒ½ä½œä¸ºæœ‰ç«äº‰åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>GPT-4oåœ¨ç‰¹å®šåœºæ™¯ä¸­è¡¨ç°ä¼˜äºå¤šç§PADæ¨¡å‹ï¼ŒåŒ…æ‹¬å•†ä¸šè§£å†³æ–¹æ¡ˆã€‚</li>
<li>GPT-4oåœ¨é«˜ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰ä¸Šä¸‹æ–‡å­¦ä¹ çš„å°‘æ•°åœºæ™¯ä¸­ã€‚</li>
<li>è¯¦ç»†çš„æç¤ºèƒ½æé«˜GPT-4oæ¨¡å‹çš„å¯é æ€§ã€‚</li>
<li>è§£é‡Šæ€§æç¤ºå¯ç•¥å¾®æé«˜GPT-4oæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>GPT-4oå…·å¤‡é¢„æµ‹æ”»å‡»ç±»å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6e4eaf9ed9577a6401727cd591ee741d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78f21b44cc764b4d7829741379e2616d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08a66ea4ab4c504da314507094064799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2a389416c7f232aa922db22f4b699ae.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="How-Developers-Interact-with-AI-A-Taxonomy-of-Human-AI-Collaboration-in-Software-Engineering"><a href="#How-Developers-Interact-with-AI-A-Taxonomy-of-Human-AI-Collaboration-in-Software-Engineering" class="headerlink" title="How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in   Software Engineering"></a>How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in   Software Engineering</h2><p><strong>Authors:Christoph Treude, Marco A. Gerosa</strong></p>
<p>Artificial intelligence (AI), including large language models and generative AI, is emerging as a significant force in software development, offering developers powerful tools that span the entire development lifecycle. Although software engineering research has extensively studied AI tools in software development, the specific types of interactions between developers and these AI-powered tools have only recently begun to receive attention. Understanding and improving these interactions has the potential to improve productivity, trust, and efficiency in AI-driven workflows. In this paper, we propose a taxonomy of interaction types between developers and AI tools, identifying eleven distinct interaction types, such as auto-complete code suggestions, command-driven actions, and conversational assistance. Building on this taxonomy, we outline a research agenda focused on optimizing AI interactions, improving developer control, and addressing trust and usability challenges in AI-assisted development. By establishing a structured foundation for studying developer-AI interactions, this paper aims to stimulate research on creating more effective, adaptive AI tools for software development. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆå¼AIï¼Œæ­£åœ¨è½¯ä»¶å¼€å‘é¢†åŸŸå½¢æˆä¸€è‚¡å¼ºå¤§çš„åŠ›é‡ï¼Œä¸ºå¼€å‘è€…æä¾›äº†è´¯ç©¿æ•´ä¸ªå¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å¼ºå¤§å·¥å…·ã€‚å°½ç®¡è½¯ä»¶å·¥ç¨‹ç ”ç©¶å·²ç»å¯¹è½¯ä»¶å¼€å‘ä¸­çš„AIå·¥å…·è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å¼€å‘è€…ä¸è¿™äº›AIå·¥å…·ä¹‹é—´çš„å…·ä½“äº¤äº’ç±»å‹æœ€è¿‘æ‰å¼€å§‹å—åˆ°å…³æ³¨ã€‚ç†è§£å’Œæ”¹è¿›è¿™äº›äº¤äº’åœ¨AIé©±åŠ¨çš„å·¥ä½œæµç¨‹ä¸­å…·æœ‰æé«˜ç”Ÿäº§åŠ›ã€ä¿¡ä»»å’Œæ•ˆç‡çš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¼€å‘è€…ä¸AIå·¥å…·ä¹‹é—´äº¤äº’ç±»å‹çš„åˆ†ç±»ï¼Œç¡®å®šäº†åŒ…æ‹¬è‡ªåŠ¨å®Œæˆä»£ç å»ºè®®ã€å‘½ä»¤é©±åŠ¨æ“ä½œå’Œå¯¹è¯è¾…åŠ©ç­‰11ç§ä¸åŒçš„äº¤äº’ç±»å‹ã€‚åŸºäºè¿™ä¸€åˆ†ç±»ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†ä¸€é¡¹ä»¥ä¼˜åŒ–AIäº¤äº’ã€æé«˜å¼€å‘è€…æ§åˆ¶å’Œè§£å†³AIè¾…åŠ©å¼€å‘ä¸­çš„ä¿¡ä»»å’Œå¯ç”¨æ€§æŒ‘æˆ˜ä¸ºé‡ç‚¹çš„ç ”ç©¶è®®ç¨‹ã€‚é€šè¿‡å»ºç«‹ç ”ç©¶å¼€å‘è€…ä¸AIä¹‹é—´äº¤äº’çš„ç»“æ„åŒ–åŸºç¡€ï¼Œæœ¬æ–‡æ—¨åœ¨æ¿€å‘å…³äºåˆ›å»ºæ›´æœ‰æ•ˆã€è‡ªé€‚åº”çš„AIå·¥å…·ç”¨äºè½¯ä»¶å¼€å‘çš„æ›´å¤šç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08774v1">PDF</a> Accepted at 2nd ACM International Conference on AI Foundation Models   and Software Engineering (FORGE 2025)</p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆå¼AIï¼Œæ­£åœ¨æˆä¸ºè½¯ä»¶å¼€å‘é¢†åŸŸçš„é‡è¦åŠ›é‡ï¼Œä¸ºå¼€å‘è€…æä¾›è·¨è¶Šæ•´ä¸ªå¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å¼ºå¤§å·¥å…·ã€‚å°½ç®¡è½¯ä»¶å·¥ç¨‹ç ”ç©¶å·²ç»å¯¹AIå·¥å…·è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å¼€å‘äººå‘˜ä¸è¿™äº›AIå·¥å…·ä¹‹é—´çš„ç‰¹å®šäº¤äº’ç±»å‹æœ€è¿‘æ‰å¼€å§‹å—åˆ°å…³æ³¨ã€‚ç†è§£å’Œæ”¹è¿›è¿™äº›äº¤äº’æœ‰æœ›æé«˜AIé©±åŠ¨å·¥ä½œæµç¨‹ä¸­çš„ç”Ÿäº§åŠ›ã€ä¿¡ä»»å’Œæ•ˆç‡ã€‚æœ¬æ–‡æå‡ºäº†å¼€å‘äººå‘˜ä¸AIå·¥å…·ä¹‹é—´äº¤äº’ç±»å‹çš„åˆ†ç±»ï¼Œç¡®å®šäº†åŒ…æ‹¬è‡ªåŠ¨å®Œæˆä»£ç å»ºè®®ã€å‘½ä»¤é©±åŠ¨æ“ä½œå’Œå¯¹è¯è¾…åŠ©ç­‰11ç§ä¸åŒçš„äº¤äº’ç±»å‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥ä¼˜åŒ–AIäº¤äº’ã€æé«˜å¼€å‘è€…æ§åˆ¶å’Œè§£å†³AIè¾…åŠ©å¼€å‘ä¸­çš„ä¿¡ä»»å’Œå¯ç”¨æ€§æŒ‘æˆ˜ä¸ºé‡ç‚¹çš„ç ”ç©¶è®®ç¨‹ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ä¸ºå¼€å‘è€…ä¸AIä¹‹é—´çš„äº¤äº’å»ºç«‹ç»“æ„åŒ–åŸºç¡€ï¼Œåˆºæ¿€å…³äºåˆ›å»ºæ›´æœ‰æ•ˆã€é€‚åº”æ€§æ›´å¼ºçš„AIå·¥å…·çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨è½¯ä»¶å¼€å‘ä¸­æ‰®æ¼”ç€æ—¥ç›Šé‡è¦çš„è§’è‰²ï¼Œæ¶µç›–æ•´ä¸ªå¼€å‘ç”Ÿå‘½å‘¨æœŸã€‚</li>
<li>å¼€å‘è€…ä¸AIå·¥å…·çš„ç‰¹å®šäº¤äº’ç±»å‹å¼€å§‹å—åˆ°å…³æ³¨ã€‚</li>
<li>ç†è§£å’Œæ”¹è¿›è¿™äº›äº¤äº’å¯¹äºæé«˜AIå·¥ä½œæµç¨‹ä¸­çš„ç”Ÿäº§åŠ›ã€ä¿¡ä»»å’Œæ•ˆç‡è‡³å…³é‡è¦ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¼€å‘è€…ä¸AIå·¥å…·ä¹‹é—´äº¤äº’ç±»å‹çš„åˆ†ç±»æ¡†æ¶ï¼Œç¡®å®šäº†åŒ…æ‹¬è‡ªåŠ¨å®Œæˆä»£ç å»ºè®®ã€å‘½ä»¤é©±åŠ¨æ“ä½œå’Œå¯¹è¯è¾…åŠ©åœ¨å†…çš„11ç§äº¤äº’ç±»å‹ã€‚</li>
<li>ç ”ç©¶è®®ç¨‹çš„é‡ç‚¹æ˜¯ä¼˜åŒ–AIäº¤äº’ã€æé«˜å¼€å‘è€…å¯¹AIå·¥å…·çš„æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>ä¿¡ä»»åº¦å’Œå¯ç”¨æ€§æ˜¯AIè¾…åŠ©å¼€å‘ä¸­éœ€è¦è§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0166ba82d3d9b81112abfdd397dfae7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fe52c341fd69ab3d2f005924f36e0d3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhanced-Large-Language-Models-for-Effective-Screening-of-Depression-and-Anxiety"><a href="#Enhanced-Large-Language-Models-for-Effective-Screening-of-Depression-and-Anxiety" class="headerlink" title="Enhanced Large Language Models for Effective Screening of Depression and   Anxiety"></a>Enhanced Large Language Models for Effective Screening of Depression and   Anxiety</h2><p><strong>Authors:June M. Liu, Mengxia Gao, Sahand Sabour, Zhuang Chen, Minlie Huang, Tatia M. C. Lee</strong></p>
<p>Depressive and anxiety disorders are widespread, necessitating timely identification and management. Recent advances in Large Language Models (LLMs) offer potential solutions, yet high costs and ethical concerns about training data remain challenges. This paper introduces a pipeline for synthesizing clinical interviews, resulting in 1,157 interactive dialogues (PsyInterview), and presents EmoScan, an LLM-based emotional disorder screening system. EmoScan distinguishes between coarse (e.g., anxiety or depressive disorders) and fine disorders (e.g., major depressive disorders) and conducts high-quality interviews. Evaluations showed that EmoScan exceeded the performance of base models and other LLMs like GPT-4 in screening emotional disorders (F1-score&#x3D;0.7467). It also delivers superior explanations (BERTScore&#x3D;0.9408) and demonstrates robust generalizability (F1-score of 0.67 on an external dataset). Furthermore, EmoScan outperforms baselines in interviewing skills, as validated by automated ratings and human evaluations. This work highlights the importance of scalable data-generative pipelines for developing effective mental health LLM tools. </p>
<blockquote>
<p>æŠ‘éƒç—‡å’Œç„¦è™‘ç—‡å¹¿æ³›å­˜åœ¨ï¼Œéœ€è¦åŠæ—¶è¯†åˆ«å’Œç®¡ç†ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ä¸ºè¿™äº›é—®é¢˜æä¾›äº†æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†é«˜æ˜‚çš„æˆæœ¬å’Œå…³äºè®­ç»ƒæ•°æ®çš„ä¼¦ç†é—®é¢˜ä»ç„¶æ˜¯æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€æ¡åˆæˆä¸´åºŠè®¿è°ˆçš„æµç¨‹ï¼Œç”Ÿæˆäº†1157ä¸ªäº’åŠ¨å¼å¯¹è¯ï¼ˆâ€œPsyInterviewâ€ï¼‰ï¼Œå¹¶å±•ç¤ºäº†åŸºäºLLMçš„æƒ…æ„Ÿéšœç¢ç­›æŸ¥ç³»ç»ŸEmoScanã€‚EmoScanèƒ½å¤ŸåŒºåˆ†ç²—ç²’åº¦ï¼ˆä¾‹å¦‚ï¼Œç„¦è™‘æˆ–æŠ‘éƒéšœç¢ï¼‰å’Œç»†ç²’åº¦ï¼ˆä¾‹å¦‚ï¼Œé‡åº¦æŠ‘éƒç—‡ï¼‰çš„éšœç¢ï¼Œå¹¶è¿›è¡Œé«˜è´¨é‡çš„è®¿è°ˆã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåœ¨ç­›æŸ¥æƒ…æ„Ÿéšœç¢æ–¹é¢ï¼ŒEmoScanè¶…è¿‡äº†åŸºå‡†æ¨¡å‹å’Œå…¶ä»–LLMï¼ˆå¦‚GPT-4ï¼‰ï¼ˆF1åˆ†æ•°ä¸º0.7467ï¼‰ã€‚å®ƒè¿˜æä¾›å‡ºè‰²çš„è§£é‡Šèƒ½åŠ›ï¼ˆBERTåˆ†æ•°ä¸º0.9408ï¼‰ï¼Œå¹¶æ˜¾ç¤ºå‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼ˆå¤–éƒ¨æ•°æ®é›†ä¸Šçš„F1åˆ†æ•°ä¸º0.67ï¼‰ã€‚æ­¤å¤–ï¼Œç»è‡ªåŠ¨åŒ–è¯„ä¼°å’Œäººå·¥è¯„ä¼°éªŒè¯ï¼ŒEmoScanåœ¨è®¿è°ˆæŠ€èƒ½æ–¹é¢ä¼˜äºåŸºçº¿ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ä¸ºå‘å±•æœ‰æ•ˆçš„ç²¾ç¥å¥åº·LLMå·¥å…·æ„å»ºå¯æ‰©å±•çš„æ•°æ®ç”Ÿæˆæµç¨‹çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08769v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŠ‘éƒç—‡å’Œç„¦è™‘ç—‡ç­›æŸ¥ç³»ç»Ÿç ”ç©¶ã€‚è¯¥ç ”ç©¶é€šè¿‡åˆæˆä¸´åºŠè®¿è°ˆæ•°æ®ï¼Œæ„å»ºäº†æƒ…æ„Ÿéšœç¢ç­›æŸ¥ç³»ç»ŸEmoScanã€‚è¯¥ç³»ç»Ÿèƒ½åŒºåˆ†ç²—ç•¥ï¼ˆå¦‚æŠ‘éƒç—‡æˆ–ç„¦è™‘ç—‡ï¼‰å’Œç²¾ç»†éšœç¢ï¼ˆå¦‚é‡åº¦æŠ‘éƒç—‡ï¼‰ï¼Œå¹¶å±•ç°å‡ºä¼˜ç§€çš„è®¿è°ˆèƒ½åŠ›å’Œè§£é‡Šæ€§èƒ½ã€‚ç›¸è¾ƒäºå…¶ä»–LLMæ¨¡å‹ï¼ŒEmoScanåœ¨æƒ…æ„Ÿéšœç¢ç­›æŸ¥ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆèƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼ºè°ƒäº†åœ¨æ„å»ºæœ‰æ•ˆå¿ƒç†å¥åº·LLMå·¥å…·ä¸­ï¼Œéœ€è¦ä¾èµ–å¯æ‰©å±•çš„æ•°æ®ç”Ÿæˆç®¡é“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æŠ‘éƒç—‡å’Œç„¦è™‘ç—‡ç­›æŸ¥æ–¹é¢å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
<li>EmoScanæ˜¯ä¸€ä¸ªåŸºäºLLMçš„æƒ…æ„Ÿéšœç¢ç­›æŸ¥ç³»ç»Ÿï¼Œèƒ½å¤Ÿè¯†åˆ«ä¸åŒç±»å‹çš„æƒ…æ„Ÿéšœç¢ã€‚</li>
<li>EmoScanç›¸è¾ƒäºåŸºç¡€æ¨¡å‹å’Œå…¶ä»–LLMæ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆèƒ½ã€‚</li>
<li>EmoScanåœ¨è®¿è°ˆæŠ€èƒ½å’Œè§£é‡Šæ€§èƒ½ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>EmoScanå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šå–å¾—è¾ƒå¥½çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶é€šè¿‡åˆæˆä¸´åºŠè®¿è°ˆæ•°æ®æ„å»ºEmoScanï¼Œå¼ºè°ƒå¯æ‰©å±•æ•°æ®ç”Ÿæˆç®¡é“çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1decb5111b491c3c2f768f79c8a8d466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0eb8ab62e7544b84dd7616b7a44da3ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5c856e1f73d6ad38ebe710a9921b3dd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Leveraging-LLM-Agents-for-Translating-Network-Configurations"><a href="#Leveraging-LLM-Agents-for-Translating-Network-Configurations" class="headerlink" title="Leveraging LLM Agents for Translating Network Configurations"></a>Leveraging LLM Agents for Translating Network Configurations</h2><p><strong>Authors:Yunze Wei, Xiaohui Xie, Yiwei Zuo, Tianshuo Hu, Xinyi Chen, Kaiwen Chi, Yong Cui</strong></p>
<p>Configuration translation is a critical and frequent task in network operations. When a network device is damaged or outdated, administrators need to replace it to maintain service continuity. The replacement devices may originate from different vendors, necessitating configuration translation to ensure seamless network operation. However, translating configurations manually is a labor-intensive and error-prone process. In this paper, we propose an intent-based framework for translating network configuration with Large Language Model (LLM) Agents. The core of our approach is an Intent-based Retrieval Augmented Generation (IRAG) module that systematically splits a configuration file into fragments, extracts intents, and generates accurate translations. We also design a two-stage verification method to validate the syntax and semantics correctness of the translated configurations. We implement and evaluate the proposed method on real-world network configurations. Experimental results show that our method achieves 97.74% syntax correctness, outperforming state-of-the-art methods in translation accuracy. </p>
<blockquote>
<p>é…ç½®ç¿»è¯‘æ˜¯ç½‘ç»œæ“ä½œä¸­çš„ä¸€é¡¹é‡è¦ä¸”é¢‘ç¹çš„ä»»åŠ¡ã€‚å½“ç½‘ç»œè®¾å¤‡æŸåæˆ–è¿‡æ—¶æ—¶ï¼Œç®¡ç†å‘˜éœ€è¦æ›¿æ¢å®ƒä»¥ä¿æŒæœåŠ¡è¿ç»­æ€§ã€‚è¿™äº›æ›¿ä»£è®¾å¤‡å¯èƒ½æ¥è‡ªä¸åŒçš„ä¾›åº”å•†ï¼Œéœ€è¦è¿›è¡Œé…ç½®ç¿»è¯‘ä»¥ç¡®ä¿ç½‘ç»œæ— ç¼è¿è¡Œã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨è¿›è¡Œé…ç½®ç¿»è¯‘æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†å‹å’Œæ˜“å‡ºé”™çš„è¿‡ç¨‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ„å›¾çš„åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¿›è¡Œç½‘ç»œé…ç½®ç¿»è¯‘æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºæ„å›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆIRAGï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ç³»ç»Ÿåœ°åˆ†å‰²é…ç½®æ–‡ä»¶ä¸ºç‰‡æ®µï¼Œæå–æ„å›¾ï¼Œå¹¶ç”Ÿæˆå‡†ç¡®çš„ç¿»è¯‘ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§ä¸¤é˜¶æ®µéªŒè¯æ–¹æ³•æ¥éªŒè¯ç¿»è¯‘é…ç½®çš„è¯­æ³•å’Œè¯­ä¹‰æ­£ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œçš„ç½‘ç»œé…ç½®ä¸Šå®ç°å¹¶è¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†97.74%çš„è¯­æ³•æ­£ç¡®ç‡ï¼Œåœ¨ç¿»è¯‘å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08760v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç½‘ç»œé…ç½®ç¿»è¯‘æ˜¯ç½‘ç»œæ“ä½œä¸­çš„ä¸€é¡¹é‡è¦ä¸”é¢‘ç¹çš„ä»»åŠ¡ã€‚å½“ç½‘ç»œè®¾å¤‡æŸåæˆ–è¿‡æ—¶ï¼Œç®¡ç†å‘˜éœ€è¦æ›¿æ¢è®¾å¤‡ä»¥ç»´æŒæœåŠ¡è¿ç»­æ€§ã€‚ç”±äºæ›¿æ¢è®¾å¤‡å¯èƒ½æ¥è‡ªä¸åŒçš„ä¾›åº”å•†ï¼Œå› æ­¤éœ€è¦è¿›è¡Œé…ç½®ç¿»è¯‘ä»¥ç¡®ä¿ç½‘ç»œæ— ç¼è¿è¡Œã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨ç¿»è¯‘é…ç½®æ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†ä¸”å®¹æ˜“å‡ºé”™çš„è¿‡ç¨‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ„å›¾çš„åˆ©ç”¨ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¿›è¡Œç½‘ç»œé…ç½®ç¿»è¯‘çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯æ„å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆIRAGï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ç³»ç»Ÿåœ°åˆ†å‰²é…ç½®æ–‡ä»¶ç‰‡æ®µã€æå–æ„å›¾å¹¶ç”Ÿæˆå‡†ç¡®çš„ç¿»è¯‘ã€‚åŒæ—¶è®¾è®¡äº†ä¸¤é˜¶æ®µéªŒè¯æ–¹æ³•æ¥éªŒè¯ç¿»è¯‘é…ç½®çš„è¯­æ³•å’Œè¯­ä¹‰æ­£ç¡®æ€§ã€‚åœ¨çœŸå®ç½‘ç»œé…ç½®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†97.74%çš„è¯­æ³•æ­£ç¡®æ€§ï¼Œåœ¨ç¿»è¯‘å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç½‘ç»œé…ç½®ç¿»è¯‘æ˜¯ç½‘ç»œæ“ä½œä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå°¤å…¶åœ¨æ›¿æ¢è®¾å¤‡æ—¶ã€‚</li>
<li>æ›¿æ¢è®¾å¤‡å¯èƒ½æ¥è‡ªä¸åŒä¾›åº”å•†ï¼Œéœ€è¦è¿›è¡Œé…ç½®ç¿»è¯‘ä»¥ç¡®ä¿æ— ç¼ç½‘ç»œè¿æ¥ã€‚</li>
<li>æ‰‹åŠ¨é…ç½®ç¿»è¯‘æ—¢åŠ³åŠ¨å¯†é›†åˆå®¹æ˜“å‡ºé”™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ„å›¾çš„åˆ©ç”¨ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç½‘ç»œé…ç½®ç¿»è¯‘çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯IRAGæ¨¡å—ï¼Œèƒ½ç³»ç»Ÿåœ°åˆ†å‰²é…ç½®æ–‡ä»¶å¹¶æå–æ„å›¾ï¼Œç”Ÿæˆå‡†ç¡®ç¿»è¯‘ã€‚</li>
<li>è®¾è®¡äº†ä¸¤é˜¶æ®µéªŒè¯æ–¹æ³•æ¥ç¡®ä¿ç¿»è¯‘çš„è¯­æ³•å’Œè¯­ä¹‰æ­£ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f40fe064da63f3cde81a01dba5fc6910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d2e49164fe0cdadc3a777b577b9c4a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be23387b2f468587f0cbad9ad0aae393.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bbdaf5ac69041548f350df0a01b44ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67cc0df8c2887df68f64a4e3418f9c88.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-Inherent-Limits-of-Pretrained-LLMs-The-Unexpected-Convergence-of-Instruction-Tuning-and-In-Context-Learning-Capabilities"><a href="#The-Inherent-Limits-of-Pretrained-LLMs-The-Unexpected-Convergence-of-Instruction-Tuning-and-In-Context-Learning-Capabilities" class="headerlink" title="The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of   Instruction Tuning and In-Context Learning Capabilities"></a>The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of   Instruction Tuning and In-Context Learning Capabilities</h2><p><strong>Authors:Irina Bigoulaeva, Harish Tayyar Madabushi, Iryna Gurevych</strong></p>
<p>Large Language Models (LLMs), trained on extensive web-scale corpora, have demonstrated remarkable abilities across diverse tasks, especially as they are scaled up. Nevertheless, even state-of-the-art models struggle in certain cases, sometimes failing at problems solvable by young children, indicating that traditional notions of task complexity are insufficient for explaining LLM capabilities. However, exploring LLM capabilities is complicated by the fact that most widely-used models are also â€œinstruction-tunedâ€ to respond appropriately to prompts. With the goal of disentangling the factors influencing LLM performance, we investigate whether instruction-tuned models possess fundamentally different capabilities from base models that are prompted using in-context examples. Through extensive experiments across various model families, scales and task types, which included instruction tuning 90 different LLMs, we demonstrate that the performance of instruction-tuned models is significantly correlated with the in-context performance of their base counterparts. By clarifying what instruction-tuning contributes, we extend prior research into in-context learning, which suggests that base models use priors from pretraining data to solve tasks. Specifically, we extend this understanding to instruction-tuned models, suggesting that their pretraining data similarly sets a limiting boundary on the tasks they can solve, with the added influence of the instruction-tuning dataset. </p>
<blockquote>
<p>åŸºäºå¤§è§„æ¨¡ç½‘ç»œè¯­æ–™åº“è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯éšç€è§„æ¨¡çš„æ‰©å¤§ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹ä¹Ÿä¼šé‡åˆ°å›°éš¾ï¼Œæœ‰æ—¶ç”šè‡³æ— æ³•è§£å†³ä¸€äº›å„¿ç«¥å°±èƒ½è§£å†³çš„é—®é¢˜ï¼Œè¿™è¡¨æ˜ä¼ ç»Ÿçš„ä»»åŠ¡å¤æ‚æ€§è§‚å¿µä¸è¶³ä»¥è§£é‡ŠLLMçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ¢ç´¢LLMçš„èƒ½åŠ›ä¹‹æ‰€ä»¥å¤æ‚ï¼Œæ˜¯å› ä¸ºå¤§å¤šæ•°å¹¿æ³›ä½¿ç”¨çš„æ¨¡å‹ä¹Ÿæ˜¯â€œæŒ‡ä»¤è°ƒæ•´â€çš„ï¼Œä»¥é€‚å½“åœ°å¯¹æç¤ºåšå‡ºååº”ã€‚ä¸ºäº†å¼„æ¸…å½±å“LLMæ€§èƒ½çš„å› ç´ ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸ä»…ä½¿ç”¨ä¸Šä¸‹æ–‡ç¤ºä¾‹æç¤ºçš„åŸºç¡€æ¨¡å‹æ˜¯å¦å…·å¤‡æ ¹æœ¬ä¸åŒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å„ç§æ¨¡å‹å®¶æ—ã€è§„æ¨¡å’Œä»»åŠ¡ç±»å‹è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå…¶ä¸­åŒ…æ‹¬å¯¹90ç§ä¸åŒLLMè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„æ€§èƒ½ä¸å…¶åŸºç¡€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­çš„æ€§èƒ½å¯†åˆ‡ç›¸å…³ã€‚é€šè¿‡æ¾„æ¸…æŒ‡ä»¤è°ƒæ•´çš„è´¡çŒ®ï¼Œæˆ‘ä»¬å¯¹å…ˆå‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç ”ç©¶è¿›è¡Œäº†æ‰©å±•ï¼Œç ”ç©¶è¡¨æ˜åŸºç¡€æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒæ•°æ®çš„å…ˆéªŒçŸ¥è¯†æ¥è§£å†³é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è¿™ä¸€ç†è§£æ‰©å±•åˆ°æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œè¡¨æ˜å®ƒä»¬çš„é¢„è®­ç»ƒæ•°æ®åŒæ ·è®¾å®šäº†å®ƒä»¬èƒ½å¤Ÿè§£å†³çš„ä»»åŠ¡çš„è¾¹ç•Œï¼Œå¹¶å—åˆ°æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08716v1">PDF</a> The code for this paper is available at:   <a target="_blank" rel="noopener" href="https://github.com/UKPLab/arxiv2025-inherent-limits-plms">https://github.com/UKPLab/arxiv2025-inherent-limits-plms</a></p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ ·åŒ–çš„ä»»åŠ¡ä¸Šå±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹ä¹Ÿä¼šé‡åˆ°å›°éš¾ï¼Œæœ‰æ—¶ç”šè‡³æ— æ³•è§£å†³å„¿ç«¥èƒ½å¤Ÿè§£å†³çš„é—®é¢˜ï¼Œè¿™è¡¨æ˜ä¼ ç»Ÿå¯¹ä»»åŠ¡å¤æ‚æ€§çš„è®¤è¯†ä¸è¶³ä»¥è§£é‡ŠLLMçš„èƒ½åŠ›ã€‚ä¸ºäº†æ¢ç©¶LLMçš„èƒ½åŠ›ï¼Œç ”ç©¶äººå‘˜æ¢è®¨äº†æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸åŸºäºç¤ºä¾‹æç¤ºçš„åŸºç¡€æ¨¡å‹ä¹‹é—´æ˜¯å¦å­˜åœ¨æ ¹æœ¬æ€§çš„èƒ½åŠ›å·®å¼‚ã€‚é€šè¿‡å¯¹å„ç§æ¨¡å‹å®¶æ—ã€è§„æ¨¡å’Œä»»åŠ¡ç±»å‹è¿›è¡Œå¤§é‡å®éªŒï¼ŒåŒ…æ‹¬ä¸º90ç§ä¸åŒçš„LLMè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„æ€§èƒ½ä¸å…¶åŸºç¡€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ã€‚è¿™æ‰©å±•äº†å…ˆå‰å…³äºä¸Šä¸‹æ–‡å­¦ä¹ çš„ç ”ç©¶ï¼Œè¡¨æ˜åŸºç¡€æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒæ•°æ®ä¸­çš„å…ˆéªŒçŸ¥è¯†æ¥è§£å†³é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è¿™ç§ç†è§£æ‰©å±•åˆ°æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œå®ƒä»¬çš„é¢„è®­ç»ƒæ•°æ®åŒæ ·è®¾å®šäº†å®ƒä»¬èƒ½å¤Ÿè§£å†³é—®é¢˜çš„é™åˆ¶è¾¹ç•Œï¼Œè€ŒæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†åˆ™äº§ç”Ÿäº†é¢å¤–çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ã€‚</li>
<li>ä¼ ç»Ÿå¯¹ä»»åŠ¡å¤æ‚æ€§çš„è®¤è¯†ä¸è¶³ä»¥è§£é‡ŠLLMçš„æ‰€æœ‰èƒ½åŠ›ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„æ€§èƒ½ä¸åŸºç¡€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´æœ‰åŠ©äºæå‡LLMçš„æ€§èƒ½ï¼Œä½†å…¶èƒ½åŠ›ä»ç„¶å—åˆ°é¢„è®­ç»ƒæ•°æ®çš„é™åˆ¶ã€‚</li>
<li>åŸºç¡€æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒæ•°æ®ä¸­çš„å…ˆéªŒçŸ¥è¯†æ¥è§£å†³é—®é¢˜ã€‚</li>
<li>é¢„è®­ç»ƒæ•°æ®å¯¹LLMèƒ½è§£å†³çš„é—®é¢˜ç±»å‹è®¾ç½®æœ‰è¾¹ç•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02fa363ebf44f38c82502e765e9e82de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2bd31ab7c2bd622fea418ee4b05c3ba4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb40159fbe559efda302febceb354f44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09186c2077cec7f8d0002c0747441421.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Knowledge-Graph-based-Retrieval-Augmented-Generation-for-Schema-Matching"><a href="#Knowledge-Graph-based-Retrieval-Augmented-Generation-for-Schema-Matching" class="headerlink" title="Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching"></a>Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching</h2><p><strong>Authors:Chuangtao Ma, Sriom Chakrabarti, Arijit Khan, BÃ¡lint MolnÃ¡r</strong></p>
<p>Traditional similarity-based schema matching methods are incapable of resolving semantic ambiguities and conflicts in domain-specific complex mapping scenarios due to missing commonsense and domain-specific knowledge. The hallucination problem of large language models (LLMs) also makes it challenging for LLM-based schema matching to address the above issues. Therefore, we propose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema Matching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces novel vector-based, graph traversal-based, and query-based graph retrievals, as well as a hybrid approach and ranking schemes that identify the most relevant subgraphs from external large knowledge graphs (KGs). We showcase that KG-based retrieval-augmented LLMs are capable of generating more accurate results for complex matching cases without any re-training. Our experimental results show that KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g., Jellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the MIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the pre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and 21.97% in terms of precision and F1 score on the Synthea dataset, respectively. The results also demonstrate that our approach is more efficient in end-to-end schema matching, and scales to retrieve from large KGs. Our case studies on the dataset from the real-world schema matching scenario exhibit that the hallucination problem of LLMs for schema matching is well mitigated by our solution. </p>
<blockquote>
<p>ä¼ ç»ŸåŸºäºç›¸ä¼¼åº¦çš„æ¨¡å¼åŒ¹é…æ–¹æ³•æ— æ³•è§£å†³ç‰¹å®šé¢†åŸŸå¤æ‚æ˜ å°„åœºæ™¯ä¸­çš„è¯­ä¹‰æ¨¡ç³Šå’Œå†²çªé—®é¢˜ï¼Œå› ä¸ºç¼ºä¹å¸¸è¯†å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ä¹Ÿä½¿å¾—åŸºäºLLMçš„æ¨¡å¼åŒ¹é…éš¾ä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å‹ç”¨äºæ¨¡å¼åŒ¹é…ï¼Œç§°ä¸ºKG-RAG4SMã€‚ç‰¹åˆ«åœ°ï¼ŒKG-RAG4SMå¼•å…¥äº†åŸºäºæ–°å‹å‘é‡ã€åŸºäºå›¾éå†å’ŒåŸºäºæŸ¥è¯¢çš„å›¾æ£€ç´¢ï¼Œä»¥åŠæ··åˆæ–¹æ³•å’Œæ’åæ–¹æ¡ˆï¼Œè¿™äº›æ–¹æ¡ˆå¯ä»¥ä»å¤–éƒ¨å¤§å‹çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­è¯†åˆ«å‡ºæœ€ç›¸å…³çš„å­å›¾ã€‚æˆ‘ä»¬å±•ç¤ºäº†åŸºäºçŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºçš„LLMèƒ½å¤Ÿåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä¸ºå¤æ‚åŒ¹é…æ¡ˆä¾‹ç”Ÿæˆæ›´å‡†ç¡®çš„ç»“æœã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨MIMICæ•°æ®é›†ä¸Šï¼ŒKG-RAG4SMåœ¨ç²¾ç¡®åº¦å’ŒF1åˆ†æ•°æ–¹é¢åˆ†åˆ«æ¯”åŸºäºLLMçš„æœ€å…ˆè¿›æ–¹æ³•ï¼ˆå¦‚Jellyfish-8Bï¼‰é«˜å‡º35.89%å’Œ30.50%ï¼›åœ¨Syntheaæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨GPT-4o-miniçš„KG-RAG4SMåœ¨ç²¾ç¡®åº¦å’ŒF1åˆ†æ•°æ–¹é¢åˆ†åˆ«æ¯”åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ˆå¦‚SMATï¼‰é«˜å‡º69.20%å’Œ21.97%ã€‚ç»“æœè¿˜è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç«¯åˆ°ç«¯æ¨¡å¼åŒ¹é…æ–¹é¢æ›´åŠ é«˜æ•ˆï¼Œå¹¶èƒ½æ‰©å±•åˆ°ä»å¤§å‹KGä¸­è¿›è¡Œæ£€ç´¢ã€‚æˆ‘ä»¬åœ¨æ¥è‡ªç°å®ä¸–ç•Œæ¨¡å¼åŒ¹é…åœºæ™¯çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå¾ˆå¥½åœ°ç¼“è§£äº†LLMåœ¨æ¨¡å¼åŒ¹é…ä¸­çš„å¹»è§‰é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08686v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å‹åœ¨æ¨¡å¼åŒ¹é…ä¸­çš„åº”ç”¨ï¼Œç§°ä¸ºKG-RAG4SMã€‚è¯¥æ¨¡å‹è§£å†³äº†ä¼ ç»ŸåŸºäºç›¸ä¼¼åº¦çš„æ¨¡å¼åŒ¹é…æ–¹æ³•æ— æ³•è§£å†³çš„è¯­ä¹‰æ¨¡ç³Šå’Œå†²çªé—®é¢˜ï¼Œé€šè¿‡å¼•å…¥åŸºäºå‘é‡ã€å›¾éå†å’ŒæŸ¥è¯¢çš„æ£€ç´¢æ–¹æ³•ï¼Œä»¥åŠæ··åˆæ–¹æ³•å’Œæ’åæ–¹æ¡ˆï¼Œä»å¤–éƒ¨å¤§å‹çŸ¥è¯†å›¾è°±ä¸­è¯†åˆ«æœ€ç›¸å…³çš„å­å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKG-RAG4SMåœ¨å¤æ‚åŒ¹é…æ¡ˆä¾‹ä¸­è¡¨ç°æ›´ç²¾ç¡®ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚åœ¨MIMICå’ŒSyntheaæ•°æ®é›†ä¸Šçš„ç»“æœæ˜¾ç¤ºï¼ŒKG-RAG4SMç›¸è¾ƒäºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•æœ‰æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆæœ‰æ•ˆç¼“è§£äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨¡å¼åŒ¹é…ä¸­çš„è™šæ„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»ŸåŸºäºç›¸ä¼¼åº¦çš„æ¨¡å¼åŒ¹é…æ–¹æ³•å­˜åœ¨è¯­ä¹‰æ¨¡ç³Šå’Œå†²çªé—®é¢˜ï¼Œæ— æ³•åº”å¯¹ç‰¹å®šé¢†åŸŸçš„å¤æ‚æ˜ å°„åœºæ™¯ã€‚</li>
<li>KG-RAG4SMæ¨¡å‹é€šè¿‡å¼•å…¥æ–°å‹å‘é‡ã€å›¾éå†å’ŒæŸ¥è¯¢æ£€ç´¢æŠ€æœ¯ï¼Œç»“åˆæ··åˆæ–¹æ³•å’Œæ’åæ–¹æ¡ˆï¼Œä»å¤–éƒ¨å¤§å‹çŸ¥è¯†å›¾è°±ä¸­è¯†åˆ«æœ€ç›¸å…³çš„å­å›¾ï¼Œæé«˜äº†æ¨¡å¼åŒ¹é…çš„å‡†ç¡®æ€§ã€‚</li>
<li>KG-RAG4SMæ— éœ€é‡æ–°è®­ç»ƒå³å¯ç”Ÿæˆæ›´ç²¾ç¡®çš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚åŒ¹é…æ¡ˆä¾‹ä¸­ã€‚</li>
<li>åœ¨MIMICå’ŒSyntheaæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKG-RAG4SMç›¸è¾ƒäºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•æœ‰æ˜¾è‘—æé«˜ï¼Œåˆ†åˆ«æé«˜äº†35.89%å’Œ69.20%çš„ç²¾åº¦ã€‚</li>
<li>KG-RAG4SMæ–¹æ¡ˆä¸å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆï¼Œæœ‰æ•ˆç¼“è§£äº†è¯­è¨€æ¨¡å‹çš„è™šæ„é—®é¢˜ã€‚</li>
<li>KG-RAG4SMæ¨¡å‹æ›´é€‚ç”¨äºç«¯åˆ°ç«¯çš„æ¨¡å¼åŒ¹é…ï¼Œå¹¶èƒ½ä»å¤§å‹çŸ¥è¯†å›¾è°±ä¸­é«˜æ•ˆæ£€ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08c8858c92bb334dcd4758d3d660ae96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfdf7983c7dac854ecd1454c69f40c8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3212638b3a7f076088ac7816bb32ca4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94c91aff18b79f343d9f8d083a318d08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88fdfb068edf9b353b8eeab044889a29.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Augmenting-Smart-Contract-Decompiler-Output-through-Fine-grained-Dependency-Analysis-and-LLM-facilitated-Semantic-Recovery"><a href="#Augmenting-Smart-Contract-Decompiler-Output-through-Fine-grained-Dependency-Analysis-and-LLM-facilitated-Semantic-Recovery" class="headerlink" title="Augmenting Smart Contract Decompiler Output through Fine-grained   Dependency Analysis and LLM-facilitated Semantic Recovery"></a>Augmenting Smart Contract Decompiler Output through Fine-grained   Dependency Analysis and LLM-facilitated Semantic Recovery</h2><p><strong>Authors:Zeqin Liao, Yuhong Nan, Zixu Gao, Henglong Liang, Sicheng Hao, Peifan Reng, Zibin Zheng</strong></p>
<p>Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SAâ€™s accuracy in control and data flow analysis and LLMâ€™s capability in semantic prediction. More specifically, \system{} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39% for method boundaries, 90.39% for variable types, and 80.65% for contract attributes. </p>
<blockquote>
<p>åç¼–è¯‘å™¨æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºç¨‹åºåˆ†æä»»åŠ¡çš„åå‘å·¥ç¨‹å·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨‹åºç†è§£å’Œæ¼æ´æ£€æµ‹æ–¹é¢ã€‚ç„¶è€Œï¼Œå½“å‰ç”¨äºSolidityæ™ºèƒ½åˆçº¦çš„åç¼–è¯‘å™¨åœ¨é‡æ„åŸå§‹æºä»£ç æ—¶é¢ä¸´é‡å¤§å±€é™ã€‚å…·ä½“æ¥è¯´ï¼Œå½“å‰æœ€å…ˆè¿›åç¼–è¯‘å™¨çš„ç“¶é¢ˆåœ¨äºæ–¹æ³•è¯†åˆ«ä¸å‡†ç¡®ã€å˜é‡ç±»å‹æ¢å¤ä¸æ­£ç¡®ä»¥åŠç¼ºå¤±çš„åˆåŒå±æ€§ã€‚è¿™äº›ç¼ºé™·é˜»ç¢äº†ä¸‹æ¸¸ä»»åŠ¡å’Œç¨‹åºé€»è¾‘ç†è§£ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SmartHaloï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆé™æ€åˆ†æï¼ˆSAï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æé«˜åç¼–è¯‘å™¨çš„è¾“å‡ºè´¨é‡ã€‚SmartHaloåˆ©ç”¨SAåœ¨æ§åˆ¶å’Œæ•°æ®æµåˆ†ææ–¹é¢çš„å‡†ç¡®æ€§ï¼Œä»¥åŠLLMåœ¨è¯­ä¹‰é¢„æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒSmartHaloé€šè¿‡é™æ€åˆ†ææ„å»ºäº†æ–°çš„æ•°æ®ç»“æ„â€”â€”ä¾èµ–å›¾ï¼ˆDGï¼‰ï¼Œä»¥æå–è¯­ä¹‰ä¾èµ–å…³ç³»ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨DGæ¥åˆ›å»ºç”¨äºä¼˜åŒ–LLMçš„æç¤ºã€‚æœ€åï¼Œé€šè¿‡ç¬¦å·æ‰§è¡Œå’Œå½¢å¼åŒ–éªŒè¯éªŒè¯LLMè¾“å‡ºçš„æ­£ç¡®æ€§ã€‚åœ¨ç”±éšæœºé€‰æ‹©çš„465ä¸ªæ™ºèƒ½åˆçº¦æ–¹æ³•ç»„æˆçš„æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸å½“å‰æœ€å…ˆè¿›åç¼–è¯‘å™¨ç›¸æ¯”ï¼ˆå¦‚Gigahorseï¼‰ï¼ŒSmartHaloæ˜¾è‘—æé«˜äº†åç¼–è¯‘ä»£ç çš„è´¨é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°†GPT-4oä¸SmartHaloé›†æˆè¿›ä¸€æ­¥æé«˜äº†å…¶æ€§èƒ½ï¼Œæ–¹æ³•è¾¹ç•Œçš„ç²¾ç¡®ç‡ä¸º87.39%ï¼Œå˜é‡ç±»å‹çš„ç²¾ç¡®ç‡ä¸º90.39%ï¼ŒåˆåŒå±æ€§çš„ç²¾ç¡®ç‡ä¸º80.65%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08670v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ™ºèƒ½åˆçº¦åç¼–è¯‘æ˜¯ç¨‹åºåˆ†æä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œå°¤å…¶åœ¨ç¨‹åºç†è§£å’Œæ¼æ´æ£€æµ‹æ–¹é¢ã€‚ç„¶è€Œï¼Œå½“å‰Solidityæ™ºèƒ½åˆçº¦çš„åç¼–è¯‘å·¥å…·å­˜åœ¨é‡å¤§å±€é™ã€‚SmartHaloæ˜¯ä¸€ç§æ–°çš„åç¼–è¯‘ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆé™æ€åˆ†æå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æé«˜åç¼–è¯‘è¾“å‡ºè´¨é‡ã€‚SmartHaloåˆ©ç”¨é™æ€åˆ†æçš„å‡†ç¡®æ€§å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰é¢„æµ‹èƒ½åŠ›ï¼Œæ„å»ºä¾èµ–å›¾æ¥æå–è¯­ä¹‰ä¾èµ–å…³ç³»ï¼Œå¹¶é€šè¿‡ç¬¦å·æ‰§è¡Œå’Œå½¢å¼éªŒè¯éªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºæ­£ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSmartHaloåœ¨æ–¹æ³•è¾¹ç•Œã€å˜é‡ç±»å‹å’ŒåˆåŒå±æ€§æ–¹é¢æ˜¾è‘—æé«˜äº†åç¼–è¯‘ä»£ç çš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åç¼–è¯‘åœ¨æ™ºèƒ½åˆçº¦åˆ†æä¸­è‡³å…³é‡è¦ï¼Œä½†å½“å‰å·¥å…·å­˜åœ¨å±€é™ã€‚</li>
<li>SmartHaloç»“åˆäº†é™æ€åˆ†æå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æé«˜åç¼–è¯‘æ•ˆæœã€‚</li>
<li>ä¾èµ–å›¾çš„æ„å»ºæœ‰åŠ©äºæå–è¯­ä¹‰ä¾èµ–å…³ç³»ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæå‡è¯­ä¹‰é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>SmartHaloé€šè¿‡ç¬¦å·æ‰§è¡Œå’Œå½¢å¼éªŒè¯éªŒè¯è¾“å‡ºæ­£ç¡®æ€§ã€‚</li>
<li>åœ¨å®éªŒè¯„ä¼°ä¸­ï¼ŒSmartHaloæ˜¾è‘—æé«˜åç¼–è¯‘ä»£ç è´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4cabf25b9568d7ce8c82ac4d04cae682.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb3e0ffa895e311fdb83268f5475f684.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Compression-with-Global-Guidance-Towards-Training-free-High-Resolution-MLLMs-Acceleration"><a href="#Compression-with-Global-Guidance-Towards-Training-free-High-Resolution-MLLMs-Acceleration" class="headerlink" title="Compression with Global Guidance: Towards Training-free High-Resolution   MLLMs Acceleration"></a>Compression with Global Guidance: Towards Training-free High-Resolution   MLLMs Acceleration</h2><p><strong>Authors:Xuyang Liu, Ziming Wang, Yuhang Han, Yingyao Wang, Jiale Yuan, Jun Song, Bo Zheng, Linfeng Zhang, Siteng Huang, Honggang Chen</strong></p>
<p>Multimodal large language models (MLLMs) have attracted considerable attention due to their exceptional performance in visual content understanding and reasoning. However, their inference efficiency has been a notable concern, as the increasing length of multimodal contexts leads to quadratic complexity. Token compression techniques, which reduce the number of visual tokens, have demonstrated their effectiveness in reducing computational costs. Yet, these approaches have struggled to keep pace with the rapid advancements in MLLMs, especially the AnyRes strategy in the context of high-resolution image understanding. In this paper, we propose a novel token compression method, GlobalCom$^2$, tailored for high-resolution MLLMs that receive both the thumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the thumbnail as the â€œcommanderâ€ of the entire token compression process, directing the allocation of retention ratios and the specific compression for each crop. In this way, redundant tokens are eliminated while important local details are adaptively preserved to the highest extent feasible. Empirical results across 10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between performance and efficiency, and consistently outperforms state-of-the-art token compression methods with LLaVA-NeXT-7B&#x2F;13B models. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/xuyang-liu16/GlobalCom2">https://github.com/xuyang-liu16/GlobalCom2</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”±äºå¯¹è§†è§‰å†…å®¹ç†è§£å’Œæ¨ç†çš„å‡ºè‰²è¡¨ç°è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œéšç€å¤šæ¨¡æ€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œå®ƒä»¬çš„æ¨ç†æ•ˆç‡æˆä¸ºä¸€ä¸ªçªå‡ºé—®é¢˜ã€‚ä»¤ç‰Œå‹ç¼©æŠ€æœ¯é€šè¿‡å‡å°‘è§†è§‰ä»¤ç‰Œçš„æ•°é‡ï¼Œå·²ç»è¯æ˜å…¶åœ¨é™ä½è®¡ç®—æˆæœ¬æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾ˆéš¾è·Ÿä¸ŠMLLMsçš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£æ–¹é¢çš„AnyResç­–ç•¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä»¤ç‰Œå‹ç¼©æ–¹æ³•GlobalCom$^2$ï¼Œå®ƒæ˜¯ä¸ºé«˜åˆ†è¾¨ç‡çš„MLLMsè®¾è®¡çš„ï¼Œè¿™äº›æ¨¡å‹åŒæ—¶æ¥æ”¶ç¼©ç•¥å›¾å’Œå¤šä¸ªè£å‰ªå›¾åƒã€‚GlobalCom$^2$å°†æ¥è‡ªç¼©ç•¥å›¾çš„ä»¤ç‰Œä½œä¸ºæ•´ä¸ªä»¤ç‰Œå‹ç¼©è¿‡ç¨‹çš„â€œæŒ‡æŒ¥å®˜â€ï¼ŒæŒ‡å¯¼ä¿ç•™æ¯”ä¾‹çš„åˆ†é…å’Œæ¯ç§è£å‰ªå›¾åƒçš„å…·ä½“å‹ç¼©ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå†—ä½™ä»¤ç‰Œè¢«æ¶ˆé™¤ï¼ŒåŒæ—¶é‡è¦å±€éƒ¨ç»†èŠ‚å¾—åˆ°å°½å¯èƒ½å¤§çš„è‡ªé€‚åº”ä¿ç•™ã€‚åœ¨10ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç»éªŒç»“æœè¡¨æ˜ï¼ŒGlobalCom$^2$åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ€ä¼˜å¹³è¡¡ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºæœ€æ–°çš„ä»¤ç‰Œå‹ç¼©æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨LLaVA-NeXT-7B&#x2F;13Bæ¨¡å‹çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/xuyang-liu16/GlobalCom2%E3%80%82">https://github.com/xuyang-liu16/GlobalCom2ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05179v2">PDF</a> Our code is released at   \url{<a target="_blank" rel="noopener" href="https://github.com/xuyang-liu16/GlobalCom2%7D">https://github.com/xuyang-liu16/GlobalCom2}</a></p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å†…å®¹ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†æ¨ç†æ•ˆç‡ä¸€ç›´æ˜¯å…³æ³¨çš„é—®é¢˜ã€‚éšç€å¤šæ¨¡æ€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œè®¡ç®—å¤æ‚åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹é«˜åˆ†è¾¨ç‡MLLMsçš„æ–°å‹ä»¤ç‰Œå‹ç¼©æ–¹æ³•GlobalCom$^2$ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç¼©ç•¥å›¾å’Œå¤šä¸ªè£å‰ªå›¾åƒç”Ÿæˆçš„ä»¤ç‰Œä½œä¸ºâ€œæŒ‡æŒ¥å®˜â€ï¼Œç®¡ç†å‹ç¼©è¿‡ç¨‹ä¸­çš„ä¿ç•™æ¯”ç‡å¹¶ä¸ºæ¯ä¸ªè£å‰ªå›¾åƒæä¾›ç‰¹å®šå‹ç¼©ã€‚æ­¤æ–¹æ³•åœ¨æ¶ˆé™¤å†—ä½™ä»¤ç‰Œçš„åŒæ—¶è‡ªé€‚åº”åœ°ä¿ç•™é‡è¦å±€éƒ¨ç»†èŠ‚ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚ä¸æœ€æ–°ä»¤ç‰Œå‹ç¼©æ–¹æ³•ç›¸æ¯”ï¼ŒGlobalCom$^2$åœ¨LLaVA-NeXT-7B&#x2F;13Bæ¨¡å‹ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å†…å®¹ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æ¨ç†æ•ˆç‡æˆä¸ºå…³æ³¨ç„¦ç‚¹ã€‚</li>
<li>éšç€å¤šæ¨¡æ€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œè®¡ç®—å¤æ‚åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ï¼Œå¯¼è‡´æ•ˆç‡ä¸‹é™ã€‚</li>
<li>ä»¤ç‰Œå‹ç¼©æŠ€æœ¯å¯ä»¥æœ‰æ•ˆé™ä½è®¡ç®—æˆæœ¬ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´å¯¹é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ä¸Šçš„æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä»¤ç‰Œå‹ç¼©æ–¹æ³•â€”â€”GlobalCom$^2$ï¼Œä¸“é—¨é’ˆå¯¹é«˜åˆ†è¾¨ç‡MLLMsã€‚</li>
<li>GlobalCom$^2$åˆ©ç”¨ç¼©ç•¥å›¾å’Œå¤šä¸ªè£å‰ªå›¾åƒç”Ÿæˆçš„ä»¤ç‰Œä½œä¸ºæ•´ä¸ªå‹ç¼©è¿‡ç¨‹çš„â€œæŒ‡æŒ¥å®˜â€ï¼Œä¼˜åŒ–äº†ä»¤ç‰Œåˆ†é…å’Œå‹ç¼©ã€‚</li>
<li>GlobalCom$^2$åœ¨æ¶ˆé™¤å†—ä½™ä»¤ç‰Œçš„åŒæ—¶è‡ªé€‚åº”åœ°ä¿ç•™é‡è¦å±€éƒ¨ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-56b238adf49d9a460f1d5b6d3d79185f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4c8ce6585465ee54d3f2b442dcd7906.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="2-OLMo-2-Furious"><a href="#2-OLMo-2-Furious" class="headerlink" title="2 OLMo 2 Furious"></a>2 OLMo 2 Furious</h2><p><strong>Authors:Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi</strong></p>
<p>We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T&quot;ulu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly â€“ models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†OLMo 2ï¼Œè¿™æ˜¯æˆ‘ä»¬å…¨æ–°å¼€å‘çš„å®Œå…¨å¼€æ”¾è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€ä»£äº§å“ã€‚OLMo 2åŒ…æ‹¬æ”¹è¿›æ¶æ„å’Œè®­ç»ƒæ–¹æ¡ˆçš„å¯†é›†è‡ªå›å½’æ¨¡å‹ã€é¢„è®­ç»ƒæ•°æ®æ··åˆä»¥åŠæŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ”¹è¿›æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ¡ˆå®ç°äº†æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§å’Œæ¯ä»¤ç‰Œæ•ˆç‡ã€‚æˆ‘ä»¬æ›´æ–°çš„é¢„è®­ç»ƒæ•°æ®æ··åˆå¼•å…¥äº†ä¸€ç§æ–°çš„ä¸“é—¨æ•°æ®æ··åˆDolmino Mix 1124ï¼Œå½“é€šè¿‡åæœŸè¯¾ç¨‹è®­ç»ƒï¼ˆå³åœ¨é¢„è®­ç»ƒçš„é€€ç«é˜¶æ®µå¼•å…¥ä¸“é—¨æ•°æ®ï¼‰æ—¶ï¼Œå®ƒèƒ½åœ¨è®¸å¤šä¸‹æ¸¸ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜æ¨¡å‹çš„èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬ç»“åˆäº†Tâ€œulu 3çš„æœ€ä½³å®è·µæ¥å¼€å‘OLMo 2-Instructï¼Œä¾§é‡äºè®¸å¯æ•°æ®ï¼Œå¹¶æ‰©å±•äº†æˆ‘ä»¬æœ€ç»ˆé˜¶æ®µçš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚æˆ‘ä»¬çš„OLMo 2åŸºç¡€æ¨¡å‹ä½äºæ€§èƒ½ä¸è®¡ç®—ä¹‹é—´çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œé€šå¸¸ä¸è¯¸å¦‚Llama 3.1å’ŒQwen 2.5ç­‰ä»…å…¬å¼€æƒé‡æ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´ä½³ï¼ŒåŒæ—¶ä½¿ç”¨çš„æµ®ç‚¹è¿ç®—è¾ƒå°‘ï¼Œå¹¶ä¸”å…·æœ‰å®Œå…¨é€æ˜çš„è®­ç»ƒæ•°æ®ã€ä»£ç å’Œæ–¹æ¡ˆã€‚æˆ‘ä»¬å®Œå…¨å¼€æ”¾çš„OLMo 2-Instructæ¨¡å‹ä¸ç›¸åŒè§„æ¨¡çš„ä»…å…¬å¼€æƒé‡æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬Qwen 2.5ã€Llama 3.1å’ŒGemma 2ã€‚æˆ‘ä»¬å…¬å¼€æ‰€æœ‰OLMo 2åˆ¶å“-è§„æ¨¡ä¸º7Bå’Œ13Bçš„æ¨¡å‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’ŒåæœŸè®­ç»ƒçš„æ¨¡å‹ã€å®Œæ•´çš„è®­ç»ƒæ•°æ®ã€è®­ç»ƒä»£ç å’Œæ–¹æ¡ˆã€è®­ç»ƒæ—¥å¿—ä»¥åŠæˆåƒä¸Šä¸‡çš„ä¸­é—´æ£€æŸ¥ç‚¹ã€‚æœ€ç»ˆæŒ‡ä»¤æ¨¡å‹å¯åœ¨Ai2 Playgroundä¸Šä½œä¸ºå…è´¹ç ”ç©¶æ¼”ç¤ºä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00656v2">PDF</a> Model demo available at playground.allenai.org</p>
<p><strong>æ‘˜è¦</strong><br>    OLMo 2ä½œä¸ºå…¨æ–°ä¸€ä»£å®Œå…¨å¼€æ”¾çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬å¯†é›†çš„è‡ªå›å½’æ¨¡å‹ï¼Œæ”¹è¿›äº†æ¶æ„å’Œè®­ç»ƒé…æ–¹ã€é¢„è®­ç»ƒæ•°æ®æ··åˆç‰©å’ŒæŒ‡ä»¤è°ƒæ•´é…æ–¹ã€‚å…¶æ”¹è¿›åçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒé…æ–¹æé«˜äº†è®­ç»ƒç¨³å®šæ€§å’Œæ¯æ ‡è®°çš„æ•ˆç‡ã€‚æ–°çš„é¢„è®­ç»ƒæ•°æ®æ··åˆç‰©Dolmino Mix 1124é€šè¿‡åæœŸè¯¾ç¨‹è®­ç»ƒæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­çš„èƒ½åŠ›ã€‚OLMo 2-Instructç»“åˆäº†æœ€ä½³å®è·µï¼Œä¾§é‡äºè®¸å¯æ•°æ®ï¼Œå¹¶åˆ©ç”¨å¯éªŒè¯å¥–åŠ±æ‰©å±•äº†æœ€ç»ˆé˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ ã€‚OLMo 2åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—æ€§èƒ½æ–¹é¢å¤„äºå‰æ²¿åœ°ä½ï¼Œä¸Llama 3.1å’ŒQwen 2.5ç­‰ä»…å¼€æ”¾æƒé‡æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨æ›´å°‘çš„FLOPså’Œå®Œå…¨é€æ˜çš„è®­ç»ƒæ•°æ®ã€ä»£ç å’Œé…æ–¹ã€‚OLMo 2çš„æ‰€æœ‰æ¨¡å‹å’Œç›¸å…³æ•°æ®å‡å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>OLMo 2æ˜¯æ–°ä¸€ä»£å®Œå…¨å¼€æ”¾çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å«å¯†é›†çš„è‡ªå›å½’æ¨¡å‹ã€‚</li>
<li>æ”¹è¿›äº†æ¨¡å‹æ¶æ„å’Œè®­ç»ƒé…æ–¹ï¼Œæé«˜äº†è®­ç»ƒç¨³å®šæ€§å’Œæ¯æ ‡è®°çš„æ•ˆç‡ã€‚</li>
<li>å¼•å…¥æ–°çš„é¢„è®­ç»ƒæ•°æ®æ··åˆç‰©Dolmino Mix 1124ï¼Œé€šè¿‡åæœŸè¯¾ç¨‹è®­ç»ƒæé«˜äº†æ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ç»“åˆæœ€ä½³å®è·µï¼Œå¼€å‘OLMo 2-Instructï¼Œä¾§é‡äºè®¸å¯æ•°æ®ï¼Œå¹¶æ‰©å±•äº†æœ€ç»ˆé˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>OLMo 2åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—æ€§èƒ½æ–¹é¢å¤„äºå‰æ²¿åœ°ä½ï¼Œä¸ç«äº‰å¯¹æ‰‹ç›¸æ¯”ä½¿ç”¨æ›´å°‘çš„è®¡ç®—èµ„æºã€‚</li>
<li>OLMo 2çš„æ‰€æœ‰æ¨¡å‹å’Œç›¸å…³æ•°æ®å‡å·²å…¬å¼€å‘å¸ƒï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’ŒåæœŸè®­ç»ƒçš„æ¨¡å‹ã€å®Œæ•´çš„è®­ç»ƒæ•°æ®ã€è®­ç»ƒä»£ç å’Œé…æ–¹ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-243f27207f9f5152bdc4bfcd51f62502.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7173f4505ff3b8c57862e12f3c278b4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfa0d0b8d24471c154832ccf65a3059d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3596cc659d937eb50356bad4a4a3e28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a503b421a7aa8a4a6b8072378a47c4ca.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MADiff-Text-Guided-Fashion-Image-Editing-with-Mask-Prediction-and-Attention-Enhanced-Diffusion"><a href="#MADiff-Text-Guided-Fashion-Image-Editing-with-Mask-Prediction-and-Attention-Enhanced-Diffusion" class="headerlink" title="MADiff: Text-Guided Fashion Image Editing with Mask Prediction and   Attention-Enhanced Diffusion"></a>MADiff: Text-Guided Fashion Image Editing with Mask Prediction and   Attention-Enhanced Diffusion</h2><p><strong>Authors:Zechao Zhan, Dehong Gao, Jinxia Zhang, Jiale Huang, Yang Hu, Xin Wang</strong></p>
<p>Text-guided image editing model has achieved great success in general domain. However, directly applying these models to the fashion domain may encounter two issues: (1) Inaccurate localization of editing region; (2) Weak editing magnitude. To address these issues, the MADiff model is proposed. Specifically, to more accurately identify editing region, the MaskNet is proposed, in which the foreground region, densepose and mask prompts from large language model are fed into a lightweight UNet to predict the mask for editing region. To strengthen the editing magnitude, the Attention-Enhanced Diffusion Model is proposed, where the noise map, attention map, and the mask from MaskNet are fed into the proposed Attention Processor to produce a refined noise map. By integrating the refined noise map into the diffusion model, the edited image can better align with the target prompt. Given the absence of benchmarks in fashion image editing, we constructed a dataset named Fashion-E, comprising 28390 image-text pairs in the training set, and 2639 image-text pairs for four types of fashion tasks in the evaluation set. Extensive experiments on Fashion-E demonstrate that our proposed method can accurately predict the mask of editing region and significantly enhance editing magnitude in fashion image editing compared to the state-of-the-art methods. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨é€šç”¨é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œç›´æ¥å°†è¿™äº›æ¨¡å‹åº”ç”¨äºæ—¶å°šé¢†åŸŸå¯èƒ½ä¼šé‡åˆ°ä¸¤ä¸ªé—®é¢˜ï¼šï¼ˆ1ï¼‰ç¼–è¾‘åŒºåŸŸå®šä½ä¸å‡†ç¡®ï¼›ï¼ˆ2ï¼‰ç¼–è¾‘å¹…åº¦è¾ƒå¼±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†MADiffæ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œä¸ºäº†æ›´å‡†ç¡®åœ°è¯†åˆ«ç¼–è¾‘åŒºåŸŸï¼Œæå‡ºäº†MaskNetï¼Œå…¶ä¸­å°†å‰æ™¯åŒºåŸŸã€denseposeå’Œå¤§è¯­è¨€æ¨¡å‹çš„é®ç½©æç¤ºè¾“å…¥åˆ°è½»é‡çº§çš„UNetä¸­ï¼Œä»¥é¢„æµ‹ç¼–è¾‘åŒºåŸŸçš„é®ç½©ã€‚ä¸ºäº†å¢å¼ºç¼–è¾‘å¹…åº¦ï¼Œæå‡ºäº†æ³¨æ„åŠ›å¢å¼ºæ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­å°†å™ªå£°å›¾ã€æ³¨æ„åŠ›å›¾å’ŒMaskNetçš„é®ç½©è¾“å…¥åˆ°æå‡ºçš„æ³¨æ„åŠ›å¤„ç†å™¨ä¸­ï¼Œä»¥äº§ç”Ÿç²¾ç»†çš„å™ªå£°å›¾ã€‚é€šè¿‡å°†ç²¾ç»†çš„å™ªå£°å›¾é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç¼–è¾‘åçš„å›¾åƒå¯ä»¥æ›´å¥½åœ°ä¸ç›®æ ‡æç¤ºå¯¹é½ã€‚é‰´äºæ—¶å°šå›¾åƒç¼–è¾‘ç¼ºä¹åŸºå‡†æµ‹è¯•é›†ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºFashion-Eçš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«è®­ç»ƒé›†ä¸­çš„28390ä¸ªå›¾åƒæ–‡æœ¬å¯¹å’Œè¯„ä¼°é›†ä¸­çš„2639ä¸ªå›¾åƒæ–‡æœ¬å¯¹ç”¨äºå››ç§æ—¶å°šä»»åŠ¡ã€‚åœ¨Fashion-Eä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¯ä»¥å‡†ç¡®é¢„æµ‹ç¼–è¾‘åŒºåŸŸçš„é®ç½©ï¼Œå¹¶åœ¨æ—¶å°šå›¾åƒç¼–è¾‘ä¸­æ˜¾è‘—å¢å¼ºç¼–è¾‘å¹…åº¦ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20062v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨æ—¶å°šé¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³å®šä½ç¼–è¾‘åŒºåŸŸä¸å‡†ç¡®å’Œç¼–è¾‘å¼ºåº¦å¼±çš„é—®é¢˜ï¼Œæå‡ºäº†MADiffæ¨¡å‹ã€‚åˆ©ç”¨MaskNetæ›´å‡†ç¡®è¯†åˆ«ç¼–è¾‘åŒºåŸŸï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å‰æ™¯åŒºåŸŸã€å¯†é›†å§¿æ€å’Œæ©è†œæç¤ºè¾“å…¥è½»é‡çº§UNetæ¥é¢„æµ‹ç¼–è¾‘åŒºåŸŸçš„æ©è†œã€‚åŒæ—¶ï¼Œæå‡ºäº†å¢å¼ºå‹æ³¨æ„åŠ›æ‰©æ•£æ¨¡å‹æ¥å¢å¼ºç¼–è¾‘å¼ºåº¦ï¼Œå°†å™ªå£°å›¾ã€æ³¨æ„åŠ›å›¾å’ŒMaskNetçš„æ©è†œè¾“å…¥åˆ°æå‡ºçš„æ³¨æ„åŠ›å¤„ç†å™¨ä¸­ï¼Œç”Ÿæˆä¼˜åŒ–åçš„å™ªå£°å›¾ã€‚æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­åï¼Œç¼–è¾‘åçš„å›¾åƒèƒ½æ›´å¥½åœ°ç¬¦åˆç›®æ ‡æç¤ºã€‚ç”±äºç¼ºä¹æ—¶å°šå›¾åƒç¼–è¾‘çš„åŸºå‡†æµ‹è¯•é›†ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºFashion-Eçš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†çš„28390ä¸ªå›¾åƒæ–‡æœ¬å¯¹å’Œè¯„ä¼°é›†çš„2639ä¸ªå›¾åƒæ–‡æœ¬å¯¹ï¼Œåˆ†ä¸ºå››ç§æ—¶å°šä»»åŠ¡ã€‚åœ¨Fashion-Eä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å‡†ç¡®é¢„æµ‹ç¼–è¾‘åŒºåŸŸçš„æ©è†œï¼Œå¹¶åœ¨æ—¶å°šå›¾åƒç¼–è¾‘ä¸­æ˜¾è‘—æé«˜ç¼–è¾‘å¼ºåº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨æ—¶å°šé¢†åŸŸé¢ä¸´å®šä½ç¼–è¾‘åŒºåŸŸä¸å‡†ç¡®å’Œç¼–è¾‘å¼ºåº¦å¼±çš„é—®é¢˜ã€‚</li>
<li>MADiffæ¨¡å‹è¢«æå‡ºä»¥è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œå…¶ä¸­åŒ…æ‹¬MaskNetå’Œå¢å¼ºå‹æ³¨æ„åŠ›æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>MaskNetåˆ©ç”¨å‰æ™¯åŒºåŸŸã€å¯†é›†å§¿æ€å’Œæ©è†œæç¤ºæ¥æ›´å‡†ç¡®åœ°è¯†åˆ«ç¼–è¾‘åŒºåŸŸã€‚</li>
<li>å¢å¼ºå‹æ³¨æ„åŠ›æ‰©æ•£æ¨¡å‹é€šè¿‡ç”Ÿæˆä¼˜åŒ–åçš„å™ªå£°å›¾æ¥å¢å¼ºç¼–è¾‘å¼ºåº¦ã€‚</li>
<li>æ•´åˆä¼˜åŒ–åçš„å™ªå£°å›¾åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½¿ç¼–è¾‘åçš„å›¾åƒæ›´ç¬¦åˆç›®æ ‡æç¤ºã€‚</li>
<li>ç¼ºä¹æ—¶å°šå›¾åƒç¼–è¾‘çš„åŸºå‡†æµ‹è¯•é›†ï¼Œå› æ­¤æ„å»ºäº†Fashion-Eæ•°æ®é›†è¿›è¡Œå®éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1bb79bc75437ab9e89b0e1b335a0d238.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c907c746d73aad0896920241206e03ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66f0ebf2a421aee0d399e09ec5808b32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ec4956246822859578b32b0dfa3353b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c012f3b3dbd75c337c0ea00a5b6eaf9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6d3cef7914b88c975cfe2400103130a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Swin-fMRI-Transformer-Predicts-Early-Neurodevelopmental-Outcomes-from-Neonatal-fMRI"><a href="#Swin-fMRI-Transformer-Predicts-Early-Neurodevelopmental-Outcomes-from-Neonatal-fMRI" class="headerlink" title="Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from   Neonatal fMRI"></a>Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from   Neonatal fMRI</h2><p><strong>Authors:Patrick Styll, Dowon Kim, Jiook Cha</strong></p>
<p>Brain development in the first few months of human life is a critical phase characterized by rapid structural growth and functional organization. Accurately predicting developmental outcomes during this time is crucial for identifying delays and enabling timely interventions. This study introduces the SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III composite scores using neonatal fMRI data from the Developing Human Connectome Project (dHCP). To enhance predictive accuracy, we apply dimensionality reduction via group independent component analysis (ICA) and pretrain SwiFT on large adult fMRI datasets to address the challenges of limited neonatal data. Our analysis shows that SwiFT significantly outperforms baseline models in predicting cognitive, motor, and language outcomes, leveraging both single-label and multi-label prediction strategies. The modelâ€™s attention-based architecture processes spatiotemporal data end-to-end, delivering superior predictive performance. Additionally, we use Integrated Gradients with Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial representations linked to early cognitive and behavioral development. These findings underscore the potential of Transformer models to advance neurodevelopmental research and clinical practice. </p>
<blockquote>
<p>äººç±»ç”Ÿå‘½å¤´å‡ ä¸ªæœˆçš„å¤§è„‘å‘è‚²æ˜¯ä¸€ä¸ªå…³é”®é˜¶æ®µï¼Œç‰¹ç‚¹æ˜¯ç»“æ„è¿…é€Ÿå¢é•¿å’ŒåŠŸèƒ½ç»„ç»‡åŒ–ã€‚å‡†ç¡®é¢„æµ‹è¿™ä¸€æ—¶æœŸçš„å‘è‚²ç»“æœæ˜¯è‡³å…³é‡è¦çš„ï¼Œæœ‰åŠ©äºå‘ç°å‘è‚²å»¶è¿Ÿï¼Œä»¥ä¾¿åŠæ—¶é‡‡å–å¹²é¢„æªæ–½ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†SwiFTï¼ˆSwin 4D fMRI Transformerï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨åˆ©ç”¨å‘å±•äººç±»è¿æ¥ç»„é¡¹ç›®ï¼ˆdHCï¼‰çš„æ–°ç”Ÿå„¿fMRIæ•°æ®é¢„æµ‹Bayley-IIIç»¼åˆå¾—åˆ†ã€‚ä¸ºäº†æé«˜é¢„æµ‹ç²¾åº¦ï¼Œæˆ‘ä»¬é€šè¿‡ç¾¤ä½“ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰è¿›è¡Œé™ç»´ï¼Œå¹¶åœ¨å¤§å‹æˆäººfMRIæ•°æ®é›†ä¸Šé¢„å…ˆè®­ç»ƒSwiFTï¼Œä»¥åº”å¯¹æ–°ç”Ÿå„¿æ•°æ®é‡æœ‰é™æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œåœ¨é‡‡ç”¨å•æ ‡ç­¾å’Œå¤šæ ‡ç­¾é¢„æµ‹ç­–ç•¥çš„æƒ…å†µä¸‹ï¼ŒSwiFTåœ¨é¢„æµ‹è®¤çŸ¥ã€è¿åŠ¨å’Œè¯­è¨€ç»“æœæ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„æ³¨æ„åŠ›æ¶æ„èƒ½å¤Ÿç«¯åˆ°ç«¯åœ°å¤„ç†æ—¶ç©ºæ•°æ®ï¼Œæä¾›å‡ºè‰²çš„é¢„æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨é›†æˆæ¢¯åº¦ä¸å¹³æ»‘æ¢¯åº¦å¹³æ–¹ï¼ˆIG-SQï¼‰æ¥è§£é‡Šé¢„æµ‹ç»“æœï¼Œè¯†åˆ«ä¸æ—©æœŸè®¤çŸ¥å’Œå‘è‚²è¡Œä¸ºç›¸å…³çš„ç¥ç»ç©ºé—´è¡¨å¾ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†Transformeræ¨¡å‹åœ¨ç¥ç»å‘è‚²ç ”ç©¶å’Œä¸´åºŠå®è·µä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07783v2">PDF</a> fMRI Transformer, Developing Human Connectome Project, Bayley Scales   of Infant Development, Personalized Therapy, XAI</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶åˆ©ç”¨SwiFTæ¨¡å‹é¢„æµ‹æ–°ç”Ÿå„¿æ—©æœŸçš„è®¤çŸ¥ã€è¿åŠ¨å’Œè¯­è¨€å‘å±•ç»“æœã€‚ç ”ç©¶ä½¿ç”¨æ–°ç”Ÿå„¿çš„å››ç»´åŠŸèƒ½ç£å…±æŒ¯æˆåƒæ•°æ®ï¼ˆfMRIï¼‰ï¼Œé€šè¿‡ç¾¤ä½“ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰é™ä½æ•°æ®ç»´åº¦ï¼Œå¹¶åœ¨å¤§å‹æˆäººfMRIæ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚æ¨¡å‹é‡‡ç”¨åŸºäºæ³¨æ„åŠ›çš„æ¶æ„å¤„ç†æ—¶ç©ºæ•°æ®ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶ä½¿ç”¨Integrated Gradients with Smoothgrad sQuareï¼ˆIG-SQï¼‰è§£é‡Šé¢„æµ‹ç»“æœï¼Œå‘ç°ä¸æ—©æœŸè®¤çŸ¥å’Œè¡Œä¸ºçš„ç¥ç»ç©ºé—´è¡¨å¾æœ‰å…³ã€‚è¿™äº›å‘ç°å±•ç¤ºäº†Transformeræ¨¡å‹åœ¨ç¥ç»å‘è‚²ç ”ç©¶å’Œä¸´åºŠå®è·µä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼ºè°ƒæ–°ç”Ÿå„¿æ—©æœŸå¤§è„‘å‘å±•çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå‡†ç¡®é¢„æµ‹å…¶å‘å±•ç»“æœæ˜¯é‰´åˆ«å‘å±•å»¶è¿Ÿå’Œå®æ–½åŠæ—¶å¹²é¢„çš„å…³é”®ã€‚</li>
<li>ä»‹ç»äº†SwiFTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¢«è®¾è®¡ç”¨äºåŸºäºæ–°ç”Ÿå„¿fMRIæ•°æ®é¢„æµ‹Bayley-IIIç»¼åˆåˆ†æ•°ã€‚</li>
<li>é€šè¿‡ç¾¤ä½“ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰è¿›è¡Œæ•°æ®é™ç»´ï¼Œä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>SwiFTæ¨¡å‹åœ¨é¢„æµ‹è®¤çŸ¥ã€è¿åŠ¨å’Œè¯­è¨€å‘å±•ç»“æœæ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„æ¶æ„å¤„ç†æ—¶ç©ºæ•°æ®ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨Integrated Gradients with Smoothgrad sQuareï¼ˆIG-SQï¼‰æ¥è§£é‡Šé¢„æµ‹ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa0df2eeb4b15eff532cb0d08193a6f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d328781d323c7117c9705a22f9718df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4204a77d4e87a2bdc3ecb1d034d7df86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11d8174efd58a28824c1344952418ba9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Understanding-Multimodal-LLMs-the-Mechanistic-Interpretability-of-Llava-in-Visual-Question-Answering"><a href="#Understanding-Multimodal-LLMs-the-Mechanistic-Interpretability-of-Llava-in-Visual-Question-Answering" class="headerlink" title="Understanding Multimodal LLMs: the Mechanistic Interpretability of Llava   in Visual Question Answering"></a>Understanding Multimodal LLMs: the Mechanistic Interpretability of Llava   in Visual Question Answering</h2><p><strong>Authors:Zeping Yu, Sophia Ananiadou</strong></p>
<p>Understanding the mechanisms behind Large Language Models (LLMs) is crucial for designing improved models and strategies. While recent studies have yielded valuable insights into the mechanisms of textual LLMs, the mechanisms of Multi-modal Large Language Models (MLLMs) remain underexplored. In this paper, we apply mechanistic interpretability methods to analyze the visual question answering (VQA) mechanisms in the first MLLM, Llava. We compare the mechanisms between VQA and textual QA (TQA) in color answering tasks and find that: a) VQA exhibits a mechanism similar to the in-context learning mechanism observed in TQA; b) the visual features exhibit significant interpretability when projecting the visual embeddings into the embedding space; and c) Llava enhances the existing capabilities of the corresponding textual LLM Vicuna during visual instruction tuning. Based on these findings, we develop an interpretability tool to help users and researchers identify important visual locations for final predictions, aiding in the understanding of visual hallucination. Our method demonstrates faster and more effective results compared to existing interpretability approaches. Code: \url{<a target="_blank" rel="noopener" href="https://github.com/zepingyu0512/llava-mechanism%7D">https://github.com/zepingyu0512/llava-mechanism}</a> </p>
<blockquote>
<p>äº†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœºåˆ¶å¯¹äºè®¾è®¡æ”¹è¿›æ¨¡å‹å’Œç­–ç•¥è‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶å·²ç»å¯¹æ–‡æœ¬LLMçš„æœºåˆ¶äº§ç”Ÿäº†æœ‰ä»·å€¼çš„è§è§£ï¼Œä½†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æœºåˆ¶ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨æœºæ¢°è§£é‡Šæ€§æ–¹æ³•æ¥åˆ†æç¬¬ä¸€ä¸ªMLLMâ€”â€”Llavaçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æœºåˆ¶ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†é¢œè‰²å›ç­”ä»»åŠ¡ä¸­VQAå’Œæ–‡æœ¬QAï¼ˆTQAï¼‰çš„æœºåˆ¶ï¼Œå¹¶å‘ç°ï¼šaï¼‰VQAè¡¨ç°å‡ºä¸TQAä¸­è§‚å¯Ÿåˆ°çš„ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶ç›¸ä¼¼çš„æœºåˆ¶ï¼›bï¼‰å°†è§†è§‰åµŒå…¥æŠ•å½±åˆ°åµŒå…¥ç©ºé—´æ—¶ï¼Œè§†è§‰ç‰¹å¾è¡¨ç°å‡ºé‡è¦çš„è§£é‡Šæ€§ï¼›c åœ¨è§†è§‰æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­ï¼ŒLlavaå¢å¼ºäº†ç›¸åº”æ–‡æœ¬LLMVicunaçš„ç°æœ‰åŠŸèƒ½ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè§£é‡Šæ€§å·¥å…·ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å’Œç ”ç©¶è€…ç¡®å®šæœ€ç»ˆé¢„æµ‹ä¸­çš„é‡è¦è§†è§‰ä½ç½®ï¼Œæœ‰åŠ©äºç†è§£è§†è§‰å¹»è§‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰è§£é‡Šæ–¹æ³•ç›¸æ¯”ï¼Œå±•ç°å‡ºæ›´å¿«å’Œæ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚ä»£ç ï¼š\url{<a target="_blank" rel="noopener" href="https://github.com/zepingyu0512/llava-mechanism%7D%E3%80%82">https://github.com/zepingyu0512/llava-mechanism}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10950v2">PDF</a> preprint</p>
<p><strong>Summary</strong><br>è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„æœºåˆ¶åœ¨å­¦æœ¯ç•Œä¾ç„¶å¾…æ¢ç´¢ã€‚æœ¬ç ”ç©¶è¿ç”¨æœºåˆ¶è§£é‡Šæ–¹æ³•å¯¹é¦–ä¸ªMLLMæ¨¡å‹Llavaçš„VQAæœºåˆ¶è¿›è¡Œåˆ†æï¼Œå‘ç°å…¶ä¸æ–‡æœ¬é—®ç­”ï¼ˆTQAï¼‰æœºåˆ¶å­˜åœ¨ç›¸ä¼¼æ€§ï¼Œè§†è§‰ç‰¹å¾åœ¨åµŒå…¥ç©ºé—´å…·æœ‰æ˜¾è‘—çš„å¯è§£é‡Šæ€§ï¼Œä¸”Llavaåœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´æ—¶å¢å¼ºäº†å¯¹åº”æ–‡æœ¬LLMçš„èƒ½åŠ›ã€‚æ®æ­¤å¼€å‘äº†ä¸€ç§å¯è§£é‡Šæ€§å·¥å…·ï¼Œæœ‰åŠ©äºç”¨æˆ·å’Œç ”ç©¶è€…è¯†åˆ«æœ€ç»ˆé¢„æµ‹çš„é‡è¦è§†è§‰ä½ç½®ï¼Œæ›´å¥½åœ°ç†è§£è§†è§‰å¹»è§‰ç°è±¡ã€‚æ­¤å·¥å…·ç›¸æ¯”ç°æœ‰æ–¹æ³•æ›´é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„VQAæœºåˆ¶å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>Llavaæ¨¡å‹åœ¨VQAä¸­å±•ç°å‡ºä¸æ–‡æœ¬é—®ç­”ï¼ˆTQAï¼‰ç±»ä¼¼çš„æœºåˆ¶ã€‚</li>
<li>è§†è§‰ç‰¹å¾åœ¨åµŒå…¥ç©ºé—´ä¸­å…·æœ‰æ˜¾è‘—çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>Llavaåœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´æ—¶å¢å¼ºäº†å¯¹åº”çš„æ–‡æœ¬LLMèƒ½åŠ›ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°çš„å¯è§£é‡Šæ€§å·¥å…·ï¼Œæœ‰åŠ©äºè¯†åˆ«æœ€ç»ˆé¢„æµ‹çš„é‡è¦è§†è§‰ä½ç½®ã€‚</li>
<li>è¯¥å·¥å…·å¯¹ç†è§£è§†è§‰å¹»è§‰ç°è±¡æœ‰é‡è¦ä½œç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33d08cc15f737bf0a05d8c9000d42828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0761ee7807a145ef8651562dd653f55f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21863f3ba894eb56b8dc15af92714044.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75bc5c76a4258184aea87ad0cf3c5f1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5548623650eb19a86c9a6af56f52eb91.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ASTER-Natural-and-Multi-language-Unit-Test-Generation-with-LLMs"><a href="#ASTER-Natural-and-Multi-language-Unit-Test-Generation-with-LLMs" class="headerlink" title="ASTER: Natural and Multi-language Unit Test Generation with LLMs"></a>ASTER: Natural and Multi-language Unit Test Generation with LLMs</h2><p><strong>Authors:Rangeet Pan, Myeongsoo Kim, Rahul Krishna, Raju Pavuluri, Saurabh Sinha</strong></p>
<p>Implementing automated unit tests is an important but time-consuming activity in software development. To assist developers in this task, many techniques for automating unit test generation have been developed. However, despite this effort, usable tools exist for very few programming languages. Moreover, studies have found that automatically generated tests suffer poor readability and do not resemble developer-written tests. In this work, we present a rigorous investigation of how large language models (LLMs) can help bridge the gap. We describe a generic pipeline that incorporates static analysis to guide LLMs in generating compilable and high-coverage test cases. We illustrate how the pipeline can be applied to different programming languages, specifically Java and Python, and to complex software requiring environment mocking. We conducted an empirical study to assess the quality of the generated tests in terms of code coverage and test naturalness â€“ evaluating them on standard as well as enterprise Java applications and a large Python benchmark. Our results demonstrate that LLM-based test generation, when guided by static analysis, can be competitive with, and even outperform, state-of-the-art test-generation techniques in coverage achieved while also producing considerably more natural test cases that developers find easy to understand. We also present the results of a user study, conducted with 161 professional developers, that highlights the naturalness characteristics of the tests generated by our approach. </p>
<blockquote>
<p>åœ¨è½¯ä»¶å¼€å‘ä¸­ï¼Œå®æ–½è‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•æ˜¯ä¸€é¡¹é‡è¦ä¸”è€—æ—¶çš„æ´»åŠ¨ã€‚ä¸ºäº†ååŠ©å¼€å‘äººå‘˜å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼Œå·²ç»å¼€å‘äº†è®¸å¤šè‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•ç”ŸæˆæŠ€æœ¯ã€‚ç„¶è€Œï¼Œå°½ç®¡ä»˜å‡ºäº†è¿™äº›åŠªåŠ›ï¼Œåªæœ‰å¾ˆå°‘å‡ ç§ç¼–ç¨‹è¯­è¨€æœ‰å¯ç”¨çš„å·¥å…·ã€‚è€Œä¸”ï¼Œç ”ç©¶è¡¨æ˜ï¼Œè‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•å¯è¯»æ€§è¾ƒå·®ï¼Œå¹¶ä¸ç±»ä¼¼äºå¼€å‘äººå‘˜ç¼–å†™çš„æµ‹è¯•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•å¸®åŠ©ç¼©å°å·®è·è¿›è¡Œäº†ä¸¥æ ¼çš„ç ”ç©¶ã€‚æˆ‘ä»¬æè¿°äº†ä¸€ä¸ªé€šç”¨ç®¡é“ï¼Œå®ƒç»“åˆäº†é™æ€åˆ†ææ¥æŒ‡å¯¼LLMç”Ÿæˆå¯ç¼–è¯‘å’Œé«˜è¦†ç›–ç‡çš„æµ‹è¯•ç”¨ä¾‹ã€‚æˆ‘ä»¬è¯´æ˜äº†è¯¥ç®¡é“å¯ä»¥åº”ç”¨äºä¸åŒçš„ç¼–ç¨‹è¯­è¨€ï¼Œç‰¹åˆ«æ˜¯Javaå’ŒPythonï¼Œä»¥åŠéœ€è¦ç¯å¢ƒæ¨¡æ‹Ÿçš„å¤æ‚è½¯ä»¶ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å®è¯ç ”ç©¶ï¼Œä»¥è¯„ä¼°ç”Ÿæˆçš„æµ‹è¯•åœ¨ä»£ç è¦†ç›–ç‡å’Œæµ‹è¯•è‡ªç„¶æ€§æ–¹é¢çš„è´¨é‡â€”â€”åœ¨æ ‡å‡†å’Œä¼ä¸šJavaåº”ç”¨ç¨‹åºä»¥åŠå¤§å‹PythonåŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“åŸºäºé™æ€åˆ†ææŒ‡å¯¼æ—¶ï¼ŒLLMçš„æµ‹è¯•ç”Ÿæˆå¯ä»¥ä¸æœ€æ–°çš„æµ‹è¯•ç”ŸæˆæŠ€æœ¯åœ¨è¦†ç›–ç‡æ–¹é¢ç«äº‰ï¼Œç”šè‡³è¡¨ç°æ›´å¥½ï¼ŒåŒæ—¶ç”Ÿæˆçš„å¼€å‘äººå‘˜æ›´å®¹æ˜“ç†è§£çš„æµ‹è¯•æ¡ˆä¾‹æ›´ä¸ºè‡ªç„¶ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸161åä¸“ä¸šå¼€å‘äººå‘˜è¿›è¡Œçš„ç”¨æˆ·ç ”ç©¶ç»“æœï¼Œçªå‡ºäº†æˆ‘ä»¬æ–¹æ³•ç”Ÿæˆçš„æµ‹è¯•çš„ç›´è§‚æ€§ç‰¹ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03093v3">PDF</a> Accepted at ICSE-SEIP, 2025</p>
<p><strong>æ‘˜è¦</strong><br>è‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„é‡è¦ç¯èŠ‚ï¼Œä½†è€—è´¹å¤§é‡æ—¶é—´ã€‚è®¸å¤šè‡ªåŠ¨ç”Ÿæˆå•å…ƒæµ‹è¯•çš„æŠ€æœ¯å·²å¼€å‘å‡ºæ¥ï¼Œä»¥å¸®åŠ©å¼€å‘äººå‘˜å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œé’ˆå¯¹å¤šç§ç¼–ç¨‹è¯­è¨€çš„å¯ç”¨å·¥å…·ä»ç„¶å¾ˆå°‘ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•å¯è¯»æ€§è¾ƒå·®ï¼Œä¸å¼€å‘äººå‘˜ç¼–å†™çš„æµ‹è¯•æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•å¼¥åˆè¿™ä¸€é¸¿æ²Ÿã€‚æè¿°äº†ä¸€ä¸ªç»“åˆé™æ€åˆ†ææ¥æŒ‡å¯¼LLMç”Ÿæˆå¯ç¼–è¯‘å’Œé«˜è¦†ç›–ç‡çš„æµ‹è¯•ç”¨ä¾‹çš„é€šç”¨ç®¡é“ã€‚è¯¥ç®¡é“å¯åº”ç”¨äºä¸åŒçš„ç¼–ç¨‹è¯­è¨€ï¼ˆç‰¹åˆ«æ˜¯Javaå’ŒPythonï¼‰ä»¥åŠéœ€è¦ç¯å¢ƒæ¨¡æ‹Ÿçš„å¤æ‚è½¯ä»¶ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å®è¯ç ”ç©¶ï¼Œä»¥è¯„ä¼°ç”Ÿæˆçš„æµ‹è¯•åœ¨ä»£ç è¦†ç›–ç‡å’Œæµ‹è¯•è‡ªç„¶æ€§æ–¹é¢çš„è´¨é‡ï¼Œå¹¶åœ¨æ ‡å‡†å’Œä¼ä¸šJavaåº”ç”¨ç¨‹åºä»¥åŠå¤§å‹PythonåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨é™æ€åˆ†ææŒ‡å¯¼ä¸‹ï¼ŒåŸºäºLLMçš„æµ‹è¯•ç”Ÿæˆå¯ä¸æœ€å…ˆè¿›çš„æµ‹è¯•ç”ŸæˆæŠ€æœ¯ç«äº‰ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢è¡¨ç°æ›´å¥½ï¼ŒåŒæ—¶ç”Ÿæˆæ›´è‡ªç„¶çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¼€å‘äººå‘˜æ›´å®¹æ˜“ç†è§£ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸161åä¸“ä¸šå¼€å‘äººå‘˜è¿›è¡Œçš„ç”¨æˆ·ç ”ç©¶ç»“æœï¼Œçªå‡ºäº†æˆ‘ä»¬æ–¹æ³•ç”Ÿæˆçš„æµ‹è¯•çš„ç›´è§‚ç‰¹æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ç”Ÿæˆå•å…ƒæµ‹è¯•æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå¯å¸®åŠ©è§£å†³è½¯ä»¶å¼€å‘ä¸­çš„æ—¶é—´ç“¶é¢ˆã€‚</li>
<li>ç›®å‰è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•å­˜åœ¨å¯è¯»æ€§å·®çš„é—®é¢˜ï¼Œä¸äººå·¥ç¼–å†™çš„æµ‹è¯•æœ‰å¾ˆå¤§å·®è·ã€‚</li>
<li>é€šè¿‡ç»“åˆé™æ€åˆ†æï¼ŒLLMèƒ½å¤Ÿç”Ÿæˆå¯ç¼–è¯‘å’Œé«˜è¦†ç›–ç‡çš„æµ‹è¯•ç”¨ä¾‹ã€‚</li>
<li>æ‰€æè¿°çš„æ–¹æ³•å¯åº”ç”¨äºå¤šç§ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬Javaå’ŒPythonï¼Œä»¥åŠå¤æ‚çš„è½¯ä»¶ç¯å¢ƒã€‚</li>
<li>å®è¯ç ”ç©¶è¯æ˜ï¼Œåœ¨ä»£ç è¦†ç›–ç‡å’Œæµ‹è¯•è‡ªç„¶æ€§æ–¹é¢ï¼ŒLLMç”Ÿæˆçš„æµ‹è¯•è´¨é‡è¾ƒé«˜ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒåŸºäºLLMçš„æµ‹è¯•ç”Ÿæˆæ–¹æ³•åœ¨æŸäº›æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa3b2b22dc15129d3e87ff26c8ebdbc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29bef733beb345c3dcaba081cdebe652.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a271f6448c8517b67b4d10b7e61d67e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e509661f37647f4450d007be05aae37a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66dc74b7858318a14420ee9e4589566b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a723ce8ece09609cb25466fbcc2b37b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e13de2395bc7dd669184111f0e48dbbc.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Trustworthy-Responsible-and-Safe-AI-A-Comprehensive-Architectural-Framework-for-AI-Safety-with-Challenges-and-Mitigations"><a href="#Trustworthy-Responsible-and-Safe-AI-A-Comprehensive-Architectural-Framework-for-AI-Safety-with-Challenges-and-Mitigations" class="headerlink" title="Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural   Framework for AI Safety with Challenges and Mitigations"></a>Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural   Framework for AI Safety with Challenges and Mitigations</h2><p><strong>Authors:Chen Chen, Xueluan Gong, Ziyao Liu, Weifeng Jiang, Si Qi Goh, Kwok-Yan Lam</strong></p>
<p>AI Safety is an emerging area of critical importance to the safe adoption and deployment of AI systems. With the rapid proliferation of AI and especially with the recent advancement of Generative AI (or GAI), the technology ecosystem behind the design, development, adoption, and deployment of AI systems has drastically changed, broadening the scope of AI Safety to address impacts on public safety and national security. In this paper, we propose a novel architectural framework for understanding and analyzing AI Safety; defining its characteristics from three perspectives: Trustworthy AI, Responsible AI, and Safe AI. We provide an extensive review of current research and advancements in AI safety from these perspectives, highlighting their key challenges and mitigation approaches. Through examples from state-of-the-art technologies, particularly Large Language Models (LLMs), we present innovative mechanism, methodologies, and techniques for designing and testing AI safety. Our goal is to promote advancement in AI safety research, and ultimately enhance peopleâ€™s trust in digital transformation. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½å®‰å…¨æ˜¯å®‰å…¨é‡‡ç”¨å’Œéƒ¨ç½²äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ä¸€ä¸ªè‡³å…³é‡è¦çš„æ–°å…´é¢†åŸŸã€‚éšç€äººå·¥æ™ºèƒ½çš„è¿…é€Ÿæ™®åŠï¼Œå°¤å…¶æ˜¯ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGAIï¼‰çš„æœ€æ–°å‘å±•ï¼Œäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è®¾è®¡ã€å¼€å‘ã€é‡‡ç”¨å’Œéƒ¨ç½²èƒŒåçš„æŠ€æœ¯ç”Ÿæ€ç³»ç»Ÿå‘ç”Ÿäº†å·¨å¤§å˜åŒ–ï¼Œæ‰©å¤§äº†äººå·¥æ™ºèƒ½å®‰å…¨çš„èŒƒå›´ï¼Œä»¥è§£å†³å¯¹å…¬å…±å®‰å…¨å’Œå›½å®¶å®‰å…¨çš„å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç†è§£å’Œåˆ†æäººå·¥æ™ºèƒ½å®‰å…¨çš„æ–°å‹æ¶æ„æ¡†æ¶ï¼Œä»å¯ä¿¡äººå·¥æ™ºèƒ½ã€è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½å’Œå®‰å…¨äººå·¥æ™ºèƒ½ä¸‰ä¸ªè§’åº¦å®šä¹‰å…¶ç‰¹ç‚¹ã€‚æˆ‘ä»¬ä»è¿™äº›è§’åº¦å¯¹äººå·¥æ™ºèƒ½å®‰å…¨æ–¹é¢çš„å½“å‰ç ”ç©¶å’Œè¿›å±•è¿›è¡Œäº†å…¨é¢çš„å›é¡¾ï¼Œçªå‡ºäº†å…¶ä¸»è¦æŒ‘æˆ˜å’Œç¼“è§£æ–¹æ³•ã€‚é€šè¿‡å°–ç«¯æŠ€æœ¯çš„ä¾‹å­ï¼Œå°¤å…¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è®¾è®¡æµ‹è¯•äººå·¥æ™ºèƒ½å®‰å…¨çš„åˆ›æ–°æœºåˆ¶ã€æ–¹æ³•å’ŒæŠ€å·§ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨åŠ¨äººå·¥æ™ºèƒ½å®‰å…¨ç ”ç©¶çš„è¿›æ­¥ï¼Œå¹¶æœ€ç»ˆå¢å¼ºäººä»¬å¯¹æ•°å­—è½¬å‹çš„ä¿¡ä»»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12935v3">PDF</a> </p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½å®‰å…¨å¯¹äºäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨é‡‡çº³å’Œéƒ¨ç½²è‡³å…³é‡è¦ã€‚éšç€äººå·¥æ™ºèƒ½å°¤å…¶æ˜¯ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼Œäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è®¾è®¡ã€å¼€å‘ã€é‡‡çº³å’Œéƒ¨ç½²çš„æŠ€æœ¯ç”Ÿæ€ç³»ç»Ÿå‘ç”Ÿäº†å·¨å¤§å˜åŒ–ï¼Œæ‰©å¤§äº†äººå·¥æ™ºèƒ½å®‰å…¨çš„èŒƒå›´ä»¥è§£å†³å¯¹å…¬å…±å®‰å…¨å’Œå›½å®¶å®‰å…¨çš„å½±å“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„æ¶æ„æ¡†æ¶ï¼Œä»å¯ä¿¡äººå·¥æ™ºèƒ½ã€è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½å’Œå®‰å…¨äººå·¥æ™ºèƒ½ä¸‰ä¸ªè§’åº¦ç†è§£å’Œåˆ†æäººå·¥æ™ºèƒ½å®‰å…¨çš„ç‰¹ç‚¹ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†å½“å‰çš„äººå·¥æ™ºèƒ½å®‰å…¨ç ”ç©¶åŠè¿›å±•ï¼Œé‡ç‚¹ä»‹ç»äº†å…¶å…³é”®æŒ‘æˆ˜å’Œç¼“è§£æ–¹æ³•ã€‚é€šè¿‡æœ€å…ˆè¿›æŠ€æœ¯çš„ä¾‹å­ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è®¾è®¡æµ‹è¯•äººå·¥æ™ºèƒ½å®‰å…¨æ€§çš„åˆ›æ–°æœºåˆ¶ã€æ–¹æ³•å’ŒæŠ€å·§ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨åŠ¨äººå·¥æ™ºèƒ½å®‰å…¨ç ”ç©¶çš„è¿›æ­¥ï¼Œæœ€ç»ˆå¢å¼ºäººä»¬å¯¹æ•°å­—åŒ–è½¬å‹çš„ä¿¡ä»»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIå®‰å…¨å¯¹äºAIç³»ç»Ÿçš„é‡‡çº³å’Œéƒ¨ç½²è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆå¼AIå¿«é€Ÿå‘å±•çš„èƒŒæ™¯ä¸‹ã€‚</li>
<li>AIå®‰å…¨èŒƒå›´æ‰©å¤§ï¼Œæ¶‰åŠå…¬å…±å®‰å…¨å’Œå›½å®¶å®‰å…¨ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„æ¶æ„æ¡†æ¶ï¼Œä»å¯ä¿¡AIã€è´Ÿè´£ä»»çš„AIå’Œå®‰å…¨AIä¸‰ä¸ªè§’åº¦ç†è§£å’Œåˆ†æAIå®‰å…¨ç‰¹ç‚¹ã€‚</li>
<li>å…¨é¢å›é¡¾äº†å½“å‰AIå®‰å…¨ç ”ç©¶çš„è¿›å±•ï¼ŒåŒ…æ‹¬å…³é”®æŒ‘æˆ˜å’Œç¼“è§£æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¾‹å­ï¼Œå±•ç¤ºäº†è®¾è®¡æµ‹è¯•AIå®‰å…¨æ€§çš„åˆ›æ–°æœºåˆ¶ã€æ–¹æ³•å’ŒæŠ€å·§ã€‚</li>
<li>ç›®æ ‡æ˜¯æ¨åŠ¨AIå®‰å…¨ç ”ç©¶çš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-999f4dc962f736dc4a4743cc26b6b761.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68015b457ba876db4e6f44147dd59170.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Beyond-Boundaries-Learning-a-Universal-Entity-Taxonomy-across-Datasets-and-Languages-for-Open-Named-Entity-Recognition"><a href="#Beyond-Boundaries-Learning-a-Universal-Entity-Taxonomy-across-Datasets-and-Languages-for-Open-Named-Entity-Recognition" class="headerlink" title="Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets   and Languages for Open Named Entity Recognition"></a>Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets   and Languages for Open Named Entity Recognition</h2><p><strong>Authors:Yuming Yang, Wantong Zhao, Caishuang Huang, Junjie Ye, Xiao Wang, Huiyuan Zheng, Yang Nan, Yuran Wang, Xueying Xu, Kaixin Huang, Yunke Zhang, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Open Named Entity Recognition (NER), which involves identifying arbitrary types of entities from arbitrary domains, remains challenging for Large Language Models (LLMs). Recent studies suggest that fine-tuning LLMs on extensive NER data can boost their performance. However, training directly on existing datasets neglects their inconsistent entity definitions and redundant data, limiting LLMs to dataset-specific learning and hindering out-of-domain adaptation. To address this, we present B2NERD, a compact dataset designed to guide LLMsâ€™ generalization in Open NER under a universal entity taxonomy. B2NERD is refined from 54 existing English and Chinese datasets using a two-step process. First, we detect inconsistent entity definitions across datasets and clarify them by distinguishable label names to construct a universal taxonomy of 400+ entity types. Second, we address redundancy using a data pruning strategy that selects fewer samples with greater category and semantic diversity. Comprehensive evaluation shows that B2NERD significantly enhances LLMsâ€™ Open NER capabilities. Our B2NER models, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3 out-of-domain benchmarks across 15 datasets and 6 languages. The data, models, and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/B2NER">https://github.com/UmeanNever/B2NER</a>. </p>
<blockquote>
<p>å¼€æ”¾å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»ç„¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„æˆæŒ‘æˆ˜ï¼Œè¿™æ¶‰åŠåˆ°ä»ä»»æ„é¢†åŸŸè¯†åˆ«ä»»æ„ç±»å‹çš„å®ä½“ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯¹LLMè¿›è¡Œå¤§è§„æ¨¡çš„NERæ•°æ®å¾®è°ƒå¯ä»¥æé«˜å…¶æ€§èƒ½ã€‚ç„¶è€Œï¼Œç›´æ¥åœ¨ç°æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒä¼šå¿½ç•¥å…¶ä¸ä¸€è‡´çš„å®ä½“å®šä¹‰å’Œå†—ä½™æ•°æ®ï¼Œé™åˆ¶LLMåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„å­¦ä¹ ï¼Œå¹¶é˜»ç¢å…¶åœ¨åŸŸå¤–çš„é€‚åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†B2NERDï¼Œè¿™æ˜¯ä¸€ä¸ªç´§å‡‘çš„æ•°æ®é›†ï¼Œæ—¨åœ¨åœ¨é€šç”¨å®ä½“åˆ†ç±»æ³•ä¸‹æŒ‡å¯¼LLMåœ¨å¼€æ”¾NERä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚B2NERDæ˜¯é€šè¿‡ä¸¤æ­¥è¿‡ç¨‹ä»ç°æœ‰çš„54ä¸ªè‹±æ–‡å’Œä¸­æ–‡æ•°æ®é›†ä¸­æç‚¼å‡ºæ¥çš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ£€æµ‹æ•°æ®é›†ä¹‹é—´ä¸ä¸€è‡´çš„å®ä½“å®šä¹‰ï¼Œå¹¶é€šè¿‡å¯åŒºåˆ†çš„æ ‡ç­¾åç§°æ¥æ¾„æ¸…å®ƒä»¬ï¼Œä»¥æ„å»ºåŒ…å«400å¤šä¸ªå®ä½“ç±»å‹çš„é€šç”¨åˆ†ç±»æ³•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æ•°æ®ä¿®å‰ªç­–ç•¥æ¥è§£å†³å†—ä½™é—®é¢˜ï¼Œé€‰æ‹©å…·æœ‰æ›´å¤§ç±»åˆ«å’Œè¯­ä¹‰å¤šæ ·æ€§çš„æ›´å°‘æ ·æœ¬ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒB2NERDæ˜¾è‘—æé«˜äº†LLMçš„å¼€æ”¾NERèƒ½åŠ›ã€‚æˆ‘ä»¬çš„B2NERæ¨¡å‹åœ¨B2NERDä¸Šçš„è¡¨ç°ä¼˜äºGPT-4 6.8è‡³12.0 F1ç‚¹ï¼Œå¹¶åœ¨è·¨è¶Š15ä¸ªæ•°æ®é›†å’Œ6ç§è¯­è¨€çš„ä¸‰ä¸ªåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ä»¥å‰çš„æ–¹æ³•ã€‚æ•°æ®ã€æ¨¡å‹å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/UmeanNever/B2NER%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/UmeanNever/B2NERä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11192v2">PDF</a> Accepted at COLING 2025. Camera-ready version updated. Project page:   <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/B2NER">https://github.com/UmeanNever/B2NER</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¼€æ”¾å‘½åå®ä½“è¯†åˆ«ï¼ˆOpen NERï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰æ•°æ®é›†çš„ä¸ä¸€è‡´å®ä½“å®šä¹‰å’Œå†—ä½™æ•°æ®é—®é¢˜ï¼Œæå‡ºB2NERDæ•°æ®é›†ã€‚é€šè¿‡æ„å»ºé€šç”¨å®ä½“åˆ†ç±»æ³•å¯¹LLMè¿›è¡Œå¼•å¯¼ï¼Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚B2NERDæ•°æ®é›†ç»è¿‡ä¸¤æ­¥ç²¾ç‚¼è€Œæˆï¼Œèƒ½æ˜¾è‘—æé«˜LLMçš„å¼€æ”¾NERæ€§èƒ½ã€‚ä½¿ç”¨B2NERDè®­ç»ƒçš„æ¨¡å‹è¡¨ç°ä¼˜äºGPT-4å’Œå…¶ä»–æ–¹æ³•ï¼Œåœ¨æŸäº›å‡ºåŸŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾å‘½åå®ä½“è¯†åˆ«ï¼ˆOpen NERï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å­˜åœ¨ä¸ä¸€è‡´çš„å®ä½“å®šä¹‰å’Œå†—ä½™æ•°æ®é—®é¢˜ã€‚</li>
<li>B2NERDæ•°æ®é›†æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡æ„å»ºé€šç”¨å®ä½“åˆ†ç±»æ³•æé«˜LLMçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>B2NERDæ•°æ®é›†é€šè¿‡ä¸¤æ­¥ç²¾ç‚¼è€Œæˆï¼Œèƒ½æœ‰æ•ˆæé«˜LLMçš„å¼€æ”¾NERæ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨B2NERDè®­ç»ƒçš„æ¨¡å‹è¡¨ç°ä¼˜äºGPT-4å’Œå…¶ä»–æ–¹æ³•ã€‚</li>
<li>B2NERDæ•°æ®é›†åœ¨å¤šä¸ªå‡ºåŸŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºã€‚</li>
<li>B2NERDæ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-17\./crop_LLM/2406.11192v2/page_0_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e2bf00d3d1009c43dea1a0ee5306396.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8693989a513dc6559821426bf40fb6a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e822db8b677017ba0851bd696d91f816.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cb337eae999584af0e6df949878c2c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f27a62e4372c5fe21988552df3c0a7f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f66dd62c57e3102eff191cba47d094e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Parallelizing-Linear-Transformers-with-the-Delta-Rule-over-Sequence-Length"><a href="#Parallelizing-Linear-Transformers-with-the-Delta-Rule-over-Sequence-Length" class="headerlink" title="Parallelizing Linear Transformers with the Delta Rule over Sequence   Length"></a>Parallelizing Linear Transformers with the Delta Rule over Sequence   Length</h2><p><strong>Authors:Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, Yoon Kim</strong></p>
<p>Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention. However, these models still underperform transformers especially on tasks that require in-context retrieval. While more expressive variants of linear transformers which replace the additive update in linear transformers with the delta rule (DeltaNet) have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware. This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices. This algorithm allows us to scale up DeltaNet to standard language modeling settings. We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks. We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrids outperform strong transformer baselines. </p>
<blockquote>
<p>çº¿æ€§æ³¨æ„åŠ›Transformerï¼ˆå³çº¿æ€§Transformerï¼‰å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹æœ€è¿‘è¢«æè®®ä½œä¸ºsoftmaxæ³¨æ„åŠ›Transformerçš„çº¿æ€§æ—¶é—´æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨éœ€è¦ä¸Šä¸‹æ–‡æ£€ç´¢çš„ä»»åŠ¡ä¸Šä»ç„¶è¡¨ç°ä¸ä½³ã€‚è™½ç„¶å‘ç°ç”¨å¢é‡è§„åˆ™ï¼ˆDeltaNetï¼‰æ›¿ä»£çº¿æ€§Transformerä¸­çš„åŠ æ³•æ›´æ–°å¯ä»¥æ›´æœ‰æ•ˆåœ°è¿›è¡Œè”æƒ³å›å¿†ï¼Œä½†ç°æœ‰è®­ç»ƒæ­¤ç±»æ¨¡å‹çš„ç®—æ³•å¹¶ä¸æ”¯æŒåºåˆ—é•¿åº¦å¹¶è¡ŒåŒ–ï¼Œå› æ­¤åœ¨ç°ä»£ç¡¬ä»¶ä¸Šè¿›è¡Œè®­ç»ƒçš„æ•ˆç‡ä¸é«˜ã€‚æœ¬ç ”ç©¶æè¿°äº†ä¸€ç§ç”¨äºè®­ç»ƒå…·æœ‰å¢é‡è§„åˆ™çš„çº¿æ€§Transformerçš„é«˜æ•ˆç¡¬ä»¶ç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨èŠ‚çœå†…å­˜çš„è¡¨ç¤ºå½¢å¼æ¥è®¡ç®—HouseholderçŸ©é˜µçš„ä¹˜ç§¯ã€‚è¯¥ç®—æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†DeltaNetæ‰©å±•åˆ°æ ‡å‡†è¯­è¨€å»ºæ¨¡è®¾ç½®ã€‚æˆ‘ä»¬å¯¹ä¸€ä¸ªè§„æ¨¡ä¸º1.3Bçš„æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒï¼Œé’ˆå¯¹æ€»è®¡çº¦æ•°åäº¿çš„ç¬¦å·è¿›è¡Œäº†ä¸ºæœŸçº¦æ•°ç™¾äº¿çš„è¿­ä»£æ›´æ–°å¤„ç†æ¬¡æ•°è¿›è¡Œæ›´æ–°ï¼Œå‘ç°å®ƒåœ¨å›°æƒ‘åº¦å’Œä¸‹æ¸¸ä»»åŠ¡çš„é›¶æ ·æœ¬æ€§èƒ½æ–¹é¢éƒ½è¶…è¿‡äº†æœ€è¿‘çš„çº¿æ€§æ—¶é—´åŸºçº¿æ¨¡å‹ï¼Œå¦‚Mambaå’ŒGLAã€‚æˆ‘ä»¬è¿˜å°è¯•äº†ä¸¤ä¸ªæ··åˆæ¨¡å‹ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹ç»“åˆäº†DeltaNetå±‚å’Œï¼ˆ1ï¼‰æ¯éš”ä¸€å±‚é‡‡ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›å±‚æˆ–ï¼ˆ2ï¼‰ä¸¤ä¸ªå…¨å±€æ³¨æ„åŠ›å±‚ï¼Œå‘ç°è¿™äº›æ··åˆæ¨¡å‹çš„è¡¨ç°ä¼˜äºå¼ºå¤§çš„TransformeråŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06484v6">PDF</a> Final camera ready</p>
<p><strong>Summary</strong></p>
<p>çº¿æ€§å˜å‹å™¨ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹è¢«è§†ä¸ºsoftmaxæ³¨æ„åŠ›å˜å‹å™¨çš„çº¿æ€§æ—¶é—´æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨éœ€è¦ä¸Šä¸‹æ–‡æ£€ç´¢çš„ä»»åŠ¡ä¸­ï¼Œè¿™äº›æ¨¡å‹çš„è¡¨ç°ä»ä¸åŠä¼ ç»Ÿå˜å‹å™¨ã€‚ç ”ç©¶å‘ç°ï¼Œç”¨DeltaNetæ›¿æ¢çº¿æ€§å˜å‹å™¨ä¸­çš„åŠ æ³•æ›´æ–°èƒ½æé«˜å…¶åœ¨è”æƒ³å›å¿†æ–¹é¢çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç°æœ‰è®­ç»ƒæ­¤ç±»æ¨¡å‹çš„ç®—æ³•æ— æ³•å¹¶è¡Œå¤„ç†åºåˆ—é•¿åº¦ï¼Œå› æ­¤åœ¨ç°ä»£ç¡¬ä»¶ä¸Šè®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è®­ç»ƒDeltaNetçš„é«˜æ•ˆç®—æ³•ï¼Œåˆ©ç”¨è®¡ç®—HouseholderçŸ©é˜µä¹˜ç§¯çš„å­˜å‚¨é«˜æ•ˆè¡¨ç¤ºæ–¹æ³•ã€‚é€šè¿‡è¯¥ç®—æ³•ï¼Œæˆ‘ä»¬èƒ½åœ¨æ ‡å‡†è¯­è¨€å»ºæ¨¡ç¯å¢ƒä¸­æ‰©å±•DeltaNetçš„åº”ç”¨èŒƒå›´ã€‚ç»è¿‡åœ¨æ¨¡å‹è®­ç»ƒä¸­è¿ç”¨å¤§è§„æ¨¡å‚æ•°å’Œæ•°æ®æ ‡è®°çš„åŠªåŠ›ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†è®¸å¤šä¸»æµçš„çº¿æ€§æ¨¡å‹å¹¶å±•ç¤ºäº†å…¶å¯¹ä¸€ç³»åˆ—ä¸‹æ¸¸ä»»åŠ¡çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç»“åˆDeltaNetå±‚ä¸æ»‘åŠ¨çª—å£æ³¨æ„åŠ›å±‚æˆ–å…¨å±€æ³¨æ„åŠ›å±‚çš„æ··åˆæ¨¡å‹è¡¨ç°æ›´ä¸ºå‡ºè‰²ï¼Œç”šè‡³è¶…è¿‡äº†å¼ºå¤§çš„ä¼ ç»Ÿå˜å‹å™¨åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº¿æ€§å˜å‹å™¨å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ä½œä¸ºçº¿æ€§æ—¶é—´æ›¿ä»£æ–¹æ¡ˆè¢«æå‡ºã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨éœ€è¦ä¸Šä¸‹æ–‡æ£€ç´¢çš„ä»»åŠ¡ä¸­è¡¨ç°è¾ƒå¼±ã€‚</li>
<li>DeltaNeté€šè¿‡æ›¿æ¢çº¿æ€§å˜å‹å™¨ä¸­çš„åŠ æ³•æ›´æ–°æ¥æé«˜è¡¨ç°ã€‚</li>
<li>ç°æœ‰è®­ç»ƒç®—æ³•åœ¨ç°ä»£ç¡¬ä»¶ä¸Šæ•ˆç‡è¾ƒä½ï¼Œæ— æ³•å¹¶è¡Œå¤„ç†åºåˆ—é•¿åº¦ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§ç¡¬ä»¶é«˜æ•ˆçš„ç®—æ³•è®­ç»ƒDeltaNetï¼Œåˆ©ç”¨äº†è®¡ç®—HouseholderçŸ©é˜µä¹˜ç§¯çš„é«˜æ•ˆè¡¨ç¤ºæ–¹æ³•ã€‚</li>
<li>DeltaNetçš„æ€§èƒ½è¶…è¿‡äº†ä¸»æµçš„çº¿æ€§æ¨¡å‹å¹¶å±•ç¤ºäº†å…¶å¯¹ä¸€ç³»åˆ—ä¸‹æ¸¸ä»»åŠ¡çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06484">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-318491da6c37394ac4fd2e590484d391.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b20aa5d576dd0683147202321cac60d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Scaling-White-Box-Transformers-for-Vision"><a href="#Scaling-White-Box-Transformers-for-Vision" class="headerlink" title="Scaling White-Box Transformers for Vision"></a>Scaling White-Box Transformers for Vision</h2><p><strong>Authors:Jinrui Yang, Xianhang Li, Druv Pai, Yuyin Zhou, Yi Ma, Yaodong Yu, Cihang Xie</strong></p>
<p>CRATE, a white-box transformer architecture designed to learn compressed and sparse representations, offers an intriguing alternative to standard vision transformers (ViTs) due to its inherent mathematical interpretability. Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of CRATE remains an open question which this paper aims to address. Specifically, we propose CRATE-$\alpha$, featuring strategic yet minimal modifications to the sparse coding block in the CRATE architecture design, and a light training recipe designed to improve the scalability of CRATE. Through extensive experiments, we demonstrate that CRATE-$\alpha$ can effectively scale with larger model sizes and datasets. For example, our CRATE-$\alpha$-B substantially outperforms the prior best CRATE-B model accuracy on ImageNet classification by 3.7%, achieving an accuracy of 83.2%. Meanwhile, when scaling further, our CRATE-$\alpha$-L obtains an ImageNet classification accuracy of 85.1%. More notably, these model performance improvements are achieved while preserving, and potentially even enhancing the interpretability of learned CRATE models, as we demonstrate through showing that the learned token representations of increasingly larger trained CRATE-$\alpha$ models yield increasingly higher-quality unsupervised object segmentation of images. The project page is <a target="_blank" rel="noopener" href="https://rayjryang.github.io/CRATE-alpha/">https://rayjryang.github.io/CRATE-alpha/</a>. </p>
<blockquote>
<p>CRATEæ˜¯ä¸€ç§ç™½ç›’å˜å‹å™¨æ¶æ„ï¼Œæ—¨åœ¨å­¦ä¹ å‹ç¼©å’Œç¨€ç–è¡¨ç¤ºï¼Œç”±äºå…¶å›ºæœ‰çš„æ•°å­¦å¯è§£é‡Šæ€§ï¼Œå®ƒä¸ºæ ‡å‡†è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰æä¾›äº†å¼•äººå…¥èƒœçš„æ›¿ä»£æ–¹æ¡ˆã€‚å°½ç®¡å·²ç»å¯¹è¯­è¨€å’Œè§†è§‰å˜å‹å™¨çš„è§„æ¨¡è¡Œä¸ºè¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†CRATEçš„å¯æ‰©å±•æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†CRATE-$\alpha$ï¼Œå®ƒé’ˆå¯¹CRATEæ¶æ„è®¾è®¡ä¸­çš„ç¨€ç–ç¼–ç å—è¿›è¡Œäº†æˆ˜ç•¥æ€§çš„æœ€å°ä¿®æ”¹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è½»é‡çº§çš„è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥æé«˜CRATEçš„å¯æ‰©å±•æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†CRATE-$\alpha$å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”æ›´å¤§çš„æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„CRATE-$\alpha$-Båœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€ä½³CRATE-Bæ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†3.7%ï¼Œè¾¾åˆ°äº†83.2%çš„å‡†ç¡®ç‡ã€‚åŒæ—¶ï¼Œå½“æˆ‘ä»¬è¿›ä¸€æ­¥æ‰©å¤§è§„æ¨¡æ—¶ï¼Œæˆ‘ä»¬çš„CRATE-$\alpha$-Låœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†85.1%çš„å‡†ç¡®ç‡ã€‚æ›´å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹æ€§èƒ½çš„æå‡æ˜¯åœ¨ä¿æŒç”šè‡³æé«˜CRATEæ¨¡å‹çš„è§£é‡Šæ€§çš„å‰æä¸‹å®ç°çš„ï¼Œæˆ‘ä»¬é€šè¿‡å±•ç¤ºè¶Šæ¥è¶Šå¤§ã€ç»è¿‡è®­ç»ƒçš„CRATE-$\alpha$æ¨¡å‹çš„å­¦ä¹ ä»¤ç‰Œè¡¨ç¤ºèƒ½å¸¦æ¥å›¾åƒçš„æ— ç›‘ç£å¯¹è±¡åˆ†å‰²çš„è´¨é‡è¶Šæ¥è¶Šé«˜æ¥è¯æ˜è¿™ä¸€ç‚¹ã€‚é¡¹ç›®é¡µé¢ä¸ºï¼š[<a target="_blank" rel="noopener" href="https://rayjryang.github.io/CRATE-alpha/]">https://rayjryang.github.io/CRATE-alpha/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20299v4">PDF</a> project page: <a target="_blank" rel="noopener" href="https://rayjryang.github.io/CRATE-alpha/">https://rayjryang.github.io/CRATE-alpha/</a></p>
<p><strong>æ‘˜è¦</strong><br>    CRATEä½œä¸ºä¸€ç§ç™½ç›’å˜å‹å™¨æ¶æ„ï¼Œæ—¨åœ¨å­¦ä¹ å‹ç¼©å’Œç¨€ç–è¡¨ç¤ºï¼Œæä¾›äº†ä¸€ç§å¼•äººæ³¨ç›®çš„æ›¿ä»£ä¼ ç»Ÿè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„é€‰æ‹©ï¼Œå› å…¶å›ºæœ‰çš„æ•°å­¦å¯è§£é‡Šæ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³CRATEå¯æ‰©å±•æ€§çš„é—®é¢˜ï¼Œé€šè¿‡æå‡ºCRATE-$\alpha$ï¼Œå¯¹CRATEæ¶æ„è®¾è®¡ä¸­çš„ç¨€ç–ç¼–ç å—è¿›è¡Œæˆ˜ç•¥ä¸”è½»å¾®çš„ä¿®æ”¹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è½»é‡çº§çš„è®­ç»ƒç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒCRATE-$\alpha$å¯ä»¥æœ‰æ•ˆåœ°éšç€æ¨¡å‹å°ºå¯¸å’Œæ•°æ®é›†çš„å¢å¤§è€Œæ‰©å±•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„CRATE-$\alpha$-Båœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æœ€ä½³CRATE-Bæ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜3.7%ï¼Œè¾¾åˆ°83.2%ã€‚æ›´è¿›ä¸€æ­¥æ‰©å±•æ—¶ï¼Œæˆ‘ä»¬çš„CRATE-$\alpha$-Låœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°85.1%çš„å‡†ç¡®ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹æ€§èƒ½çš„æå‡æ˜¯åœ¨ä¿æŒç”šè‡³æé«˜CRATEæ¨¡å‹çš„è§£é‡Šæ€§ä¸‹å®ç°çš„ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹è¶Šæ¥è¶Šå¤§è®­ç»ƒçš„CRATE-$\alpha$æ¨¡å‹çš„æ ‡è®°è¡¨ç¤ºè¿›è¡Œå±•ç¤ºï¼Œè¿™äº›è¡¨ç¤ºè¶Šæ¥è¶Šèƒ½æ— ç›‘ç£åœ°ç²¾å‡†åˆ†å‰²å›¾åƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CRATEä½œä¸ºä¸€ç§ç™½ç›’å˜å‹å™¨æ¶æ„ï¼Œå…·æœ‰å­¦ä¹ å‹ç¼©å’Œç¨€ç–è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œæä¾›äº†ä¸€ç§æ›¿ä»£ä¼ ç»Ÿè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„é€‰æ‹©ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡æå‡ºCRATE-$\alpha$ï¼Œæ—¨åœ¨è§£å†³CRATEçš„å¯æ‰©å±•æ€§é—®é¢˜ï¼ŒåŒ…æ‹¬å¯¹ç¨€ç–ç¼–ç å—çš„æˆ˜ç•¥ä¿®æ”¹å’Œè½»é‡çº§è®­ç»ƒç­–ç•¥ã€‚</li>
<li>CRATE-$\alpha$èƒ½å¤Ÿåœ¨æ¨¡å‹å°ºå¯¸å’Œæ•°æ®é›†å¢å¤§çš„æƒ…å†µä¸‹æœ‰æ•ˆæ‰©å±•ã€‚</li>
<li>CRATE-$\alpha$åœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šè¾ƒä¹‹å‰æ¨¡å‹æœ‰æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>è®­ç»ƒçš„CRATE-$\alpha$æ¨¡å‹çš„æ ‡è®°è¡¨ç¤ºèƒ½å¤Ÿæ— ç›‘ç£åœ°ç²¾å‡†åˆ†å‰²å›¾åƒï¼Œè¿™æ˜¾ç¤ºäº†å…¶æ€§èƒ½å’ŒåŠŸèƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚</li>
<li>CRATE-$\alpha$çš„æ”¹è¿›ä¸ä»…ä»…æ˜¯æ€§èƒ½ä¸Šçš„æå‡ï¼Œä¹Ÿæ¶‰åŠæ¨¡å‹çš„å¯è§£é‡Šæ€§å’ŒåŠŸèƒ½çš„å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.20299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a24ba35722ac2eee920763a420af1edd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21125489eedd37271b85d39e3352cf47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4091fdfceb0a4347ae574f228e226555.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0325f158223eb224c38cf96d4a215cb6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e2a8cc888dfff4e16af91c98a51243aa.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  Leveraging LLM Agents for Translating Network Configurations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-90bc905918b957744c42ba51ddd347f0.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  Reliable Vertical Ground Reaction Force Estimation with Smart Insole   During Walking
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17259.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
