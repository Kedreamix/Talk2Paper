<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-01-17  IDEA Image Description Enhanced CLIP-Adapter">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3637d6df7ff87ad6cd8b7767011881b8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    46 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-17-更新"><a href="#2025-01-17-更新" class="headerlink" title="2025-01-17 更新"></a>2025-01-17 更新</h1><h2 id="IDEA-Image-Description-Enhanced-CLIP-Adapter"><a href="#IDEA-Image-Description-Enhanced-CLIP-Adapter" class="headerlink" title="IDEA: Image Description Enhanced CLIP-Adapter"></a>IDEA: Image Description Enhanced CLIP-Adapter</h2><p><strong>Authors:Zhipeng Ye, Feng Jiang, Qiufeng Wang, Kaizhu Huang, Jiaqi Huang</strong></p>
<p>CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the model’s performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named “IMD-11”. Our code and data are released at <a target="_blank" rel="noopener" href="https://github.com/FourierAI/IDEA">https://github.com/FourierAI/IDEA</a>. </p>
<blockquote>
<p>CLIP（对比语言-图像预训练）在模式识别和计算机视觉领域取得了巨大的成功。将CLIP应用于下游任务（例如零样本或小样本分类）是多模态学习的热门话题。然而，当前的研究主要集中于文本的提示学习或视觉的适配器调整，而没有充分利用图像-文本对之间的互补信息和关联。在本文中，我们提出了一种图像描述增强CLIP适配器（IDEA）方法，将CLIP适应于小样本图像分类任务。该方法通过利用图像的视觉特征和文本描述来捕获细微特征。IDEA是一种无训练的CLIP方法，它在多个任务上的表现可与或超过最新模型。此外，我们引入了可训练IDEA（T-IDEA），它通过添加两个轻量级可学习组件（即投影器和可学习潜在空间）来扩展IDEA，进一步提高了模型的性能，并在11个数据集上实现了最新结果。作为一个重要贡献，我们采用了Llama模型，并设计了一个全面的流程来生成这11个数据集的图像文本描述，共产生1,637,795个图像-文本对，名为“IMD-11”。我们的代码和数据已发布在<a target="_blank" rel="noopener" href="https://github.com/FourierAI/IDEA%E4%B8%8A%E3%80%82">https://github.com/FourierAI/IDEA上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08816v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CLIP在模式识别和计算机视觉领域取得了巨大成功，其在下游任务（如零样本或小样本分类）的迁移学习已成为多模态学习的热门话题。然而，当前研究主要关注文本提示学习或视觉适配器调整，未充分利用图像文本对之间的互补信息和相关性。本文提出了一种基于图像描述增强的CLIP适配器（IDEA）方法，用于适应小样本图像分类任务。该方法通过利用视觉特征和图像文本描述来捕捉精细特征。IDEA是一种针对CLIP的无训练方法，在多个任务上的性能可与或超越现有最先进的模型。此外，本文还介绍了通过添加两个轻量级学习组件（即投影器和可学习潜在空间）对IDEA进行扩展的可训练IDEA（T-IDEA），进一步提高了模型性能，并在11个数据集上实现了最新结果。同时，本文利用Llama模型设计了一个全面的管道来生成11个数据集的图像文本描述，产生了总计163万7千7百9十五个图像文本对，名为“IMD-11”。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP在模式识别和计算机视觉领域表现出卓越性能。</li>
<li>在下游任务（如小样本分类）的迁移学习是CLIP的热门应用方向。</li>
<li>当前研究未充分利用图像和文本之间的互补信息和相关性。</li>
<li>IDEA方法通过结合视觉特征和图像文本描述，适应小样本图像分类任务。</li>
<li>IDEA是一种针对CLIP的无训练方法，性能优越。</li>
<li>T-IDEA通过添加学习组件进一步提高模型性能，并在多个数据集上实现最新结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08816">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5b95da945eac159dfa1ea766fd44cf7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7819e00679e650231320734168624399.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Exploring-ChatGPT-for-Face-Presentation-Attack-Detection-in-Zero-and-Few-Shot-in-Context-Learning"><a href="#Exploring-ChatGPT-for-Face-Presentation-Attack-Detection-in-Zero-and-Few-Shot-in-Context-Learning" class="headerlink" title="Exploring ChatGPT for Face Presentation Attack Detection in Zero and   Few-Shot in-Context Learning"></a>Exploring ChatGPT for Face Presentation Attack Detection in Zero and   Few-Shot in-Context Learning</h2><p><strong>Authors:Alain Komaty, Hatef Otroshi Shahreza, Anjith George, Sebastien Marcel</strong></p>
<p>This study highlights the potential of ChatGPT (specifically GPT-4o) as a competitive alternative for Face Presentation Attack Detection (PAD), outperforming several PAD models, including commercial solutions, in specific scenarios. Our results show that GPT-4o demonstrates high consistency, particularly in few-shot in-context learning, where its performance improves as more examples are provided (reference data). We also observe that detailed prompts enable the model to provide scores reliably, a behavior not observed with concise prompts. Additionally, explanation-seeking prompts slightly enhance the model’s performance by improving its interpretability. Remarkably, the model exhibits emergent reasoning capabilities, correctly predicting the attack type (print or replay) with high accuracy in few-shot scenarios, despite not being explicitly instructed to classify attack types. Despite these strengths, GPT-4o faces challenges in zero-shot tasks, where its performance is limited compared to specialized PAD systems. Experiments were conducted on a subset of the SOTERIA dataset, ensuring compliance with data privacy regulations by using only data from consenting individuals. These findings underscore GPT-4o’s promise in PAD applications, laying the groundwork for future research to address broader data privacy concerns and improve cross-dataset generalization. Code available here: <a target="_blank" rel="noopener" href="https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad">https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad</a> </p>
<blockquote>
<p>本研究重点探讨了ChatGPT（特别是GPT-4o）在人脸识别攻击检测（PAD）中的潜力，其在特定场景下对包括商业解决方案在内的多个PAD模型具有竞争优势。我们的结果表明，GPT-4o表现出高度的一致性，特别是在小样本上下文学习环境中，其性能会随着提供的示例数量增多而提高（参考数据）。我们还观察到，详细的提示使模型能够可靠地提供分数，这一行为在简洁提示下并未观察到。此外，寻求解释的提示略微提高了模型的性能，改善了其可解释性。值得注意的是，尽管没有明确要求分类攻击类型，该模型仍展现出涌现的推理能力，能够在小样本场景中准确预测攻击类型（打印或回放），且准确率极高。尽管有这些优点，GPT-4o在零样本任务中仍面临挑战，其性能与专业的PAD系统相比有限。实验是在SOTERIA数据集的一个子集上进行的，仅使用同意个人的数据，以确保符合数据隐私法规。这些发现强调了GPT-4o在PAD应用中的潜力，为未来研究解决更广泛的数据隐私问题和改进跨数据集泛化能力奠定了基础。相关代码可在此处找到：<a target="_blank" rel="noopener" href="https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad">https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08799v1">PDF</a> Accepted in WACV workshop 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了ChatGPT（特别是GPT-4o）在面部呈现攻击检测（PAD）中的潜力，并在特定场景下表现出超越多种PAD模型（包括商业解决方案）的性能。GPT-4o在少量样本的情况下展现出卓越的一致性，随着提供更多样本，其性能有所提升。详细的提示能使模型得分更可靠，而简洁的提示则未观察到这种行为。此外，寻求解释的提示略微提高了模型的解释性和性能。尽管GPT-4o未在零样本任务中表现出色，但它能正确预测攻击类型（打印或回放），展现出推理能力。实验在SOTERIA数据集的一个子集上进行，并严格遵守数据隐私法规，仅使用同意参与者的数据。这些发现突显了GPT-4o在PAD应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4o在面部呈现攻击检测（PAD）中表现出强大的竞争力，超越了多种PAD模型。</li>
<li>GPT-4o在少量样本的情况下具有卓越的一致性和性能。</li>
<li>详细的提示能使GPT-4o的得分更可靠。</li>
<li>寻求解释的提示可以提高GPT-4o的解释性和性能。</li>
<li>GPT-4o展现出正确的攻击类型预测能力，具有推理能力。</li>
<li>实验在符合数据隐私法规的SOTERIA数据集子集上进行。</li>
<li>研究为GPT-4o在PAD应用中的潜力奠定了基础，未来研究可关注数据隐私和跨数据集泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08799">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6e4eaf9ed9577a6401727cd591ee741d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78f21b44cc764b4d7829741379e2616d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08a66ea4ab4c504da314507094064799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2a389416c7f232aa922db22f4b699ae.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learner-Generalizes-Across-AI-Generated-Image-Detection"><a href="#Few-Shot-Learner-Generalizes-Across-AI-Generated-Image-Detection" class="headerlink" title="Few-Shot Learner Generalizes Across AI-Generated Image Detection"></a>Few-Shot Learner Generalizes Across AI-Generated Image Detection</h2><p><strong>Authors:Shiyu Wu, Jing Liu, Jing Li, Yequan Wang</strong></p>
<p>Current fake image detectors trained on large synthetic image datasets perform satisfactorily on limited studied generative models. However, they suffer a notable performance decline over unseen models. Besides, collecting adequate training data from online generative models is often expensive or infeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a novel AI-generated image detector which learns a specialized metric space to effectively distinguish unseen fake images by utilizing very few samples. Experiments show FSD achieves state-of-the-art performance by $+7.4%$ average ACC on GenImage dataset. More importantly, our method is better capable of capturing the intra-category common features in unseen images without further training. </p>
<blockquote>
<p>当前在大型合成图像数据集上训练的虚假图像检测器，在有限的生成模型上的表现尚可。然而，它们在未见模型上的性能会出现显著下降。此外，从在线生成模型中收集足够的训练数据通常成本高昂或不可行。为了克服这些问题，我们提出了小样检测器（FSD），这是一种新型的AI生成图像检测器。通过利用极少的样本，它学习专门的度量空间，以有效区分未见过的虚假图像。实验表明，FSD在GenImage数据集上的平均准确率提高了+7.4%，达到最新技术水平。更重要的是，我们的方法无需进一步训练，便能更好地捕捉未见图像中的类别内通用特征。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08763v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的基于AI技术的图像检测器——Few-Shot Detector（FSD）。该检测器通过利用极少量样本学习专门的度量空间，有效区分未见过的虚假图像。实验表明，FSD在GenImage数据集上的平均准确率达到了领先水平，提高了7.4%。更重要的是，该方法能够更好地捕捉未见图像中的类别内通用特征，无需进一步训练。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Detector（FSD）是一种新型的AI生成的图像检测器，旨在解决现有虚假图像检测器在面对未见模型时性能下降的问题。</li>
<li>FSD通过学习专门的度量空间来区分虚假图像，仅利用少量样本即可实现有效检测。</li>
<li>实验表明，FSD在GenImage数据集上的平均准确率较之前的方法有所提高，达到了领先水平。</li>
<li>FSD能够捕捉未见图像中的类别内通用特征，这是其与其他检测器的重要区别。</li>
<li>FSD的优势在于其对于训练数据的获取需求较小，即使面临有限或昂贵的训练数据情况，也能表现出良好的性能。</li>
<li>FSD的提出为虚假图像检测领域提供了新的思路和方法，有望在未来得到广泛应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a7be3274d6331414dbdabcfff67852e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbaf68487cb3b56696c9623cd3894b7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e60e3a8e7ae890dfd5b2f0f4633a2b0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b238014150011da2dbeafcb55dac0a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95e630b63a431837b369f821daa7733d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Normalize-Then-Propagate-Efficient-Homophilous-Regularization-for-Few-shot-Semi-Supervised-Node-Classification"><a href="#Normalize-Then-Propagate-Efficient-Homophilous-Regularization-for-Few-shot-Semi-Supervised-Node-Classification" class="headerlink" title="Normalize Then Propagate: Efficient Homophilous Regularization for   Few-shot Semi-Supervised Node Classification"></a>Normalize Then Propagate: Efficient Homophilous Regularization for   Few-shot Semi-Supervised Node Classification</h2><p><strong>Authors:Baoming Zhang, MingCai Chen, Jianqing Song, Shuangjie Li, Jie Zhang, Chongjun Wang</strong></p>
<p>Graph Neural Networks (GNNs) have demonstrated remarkable ability in semi-supervised node classification. However, most existing GNNs rely heavily on a large amount of labeled data for training, which is labor-intensive and requires extensive domain knowledge. In this paper, we first analyze the restrictions of GNNs generalization from the perspective of supervision signals in the context of few-shot semi-supervised node classification. To address these challenges, we propose a novel algorithm named NormProp, which utilizes the homophily assumption of unlabeled nodes to generate additional supervision signals, thereby enhancing the generalization against label scarcity. The key idea is to efficiently capture both the class information and the consistency of aggregation during message passing, via decoupling the direction and Euclidean norm of node representations. Moreover, we conduct a theoretical analysis to determine the upper bound of Euclidean norm, and then propose homophilous regularization to constraint the consistency of unlabeled nodes. Extensive experiments demonstrate that NormProp achieve state-of-the-art performance under low-label rate scenarios with low computational complexity. </p>
<blockquote>
<p>图神经网络（GNNs）在半监督节点分类中表现出了显著的能力。然而，大多数现有的GNNs严重依赖于大量的标注数据进行训练，这既耗费人力又需要广泛的专业知识。本文首先分析了少样本半监督节点分类背景下，监督信号对GNNs泛化的限制。为了解决这些挑战，我们提出了一种名为NormProp的新型算法，它利用未标记节点的同亲假设来生成额外的监督信号，从而提高对标签稀缺的泛化能力。关键思想是通过解耦节点表示的方向和欧几里得范数，有效地捕获类信息和聚合过程中的一致性。此外，我们还进行了理论分析，以确定欧几里得范数的上限，然后提出同亲正则化来约束未标记节点的一致性。大量实验表明，NormProp在低标签率场景下实现了最先进的性能，且计算复杂度低。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08581v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文分析了图神经网络（GNNs）在少样本半监督节点分类中的泛化限制，并提出了一种名为NormProp的新型算法。该算法利用未标记节点的同质性假设生成额外的监督信号，以提高对标签稀缺的泛化能力。其核心思想是通过解耦节点表示的方向和欧几里得范数，有效地捕获类信息和聚合过程中的一致性。同时，进行了理论分析和实验验证，表明NormProp在低标签率场景下实现了最佳性能，且计算复杂度低。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GNNs在少样本半监督节点分类中面临挑战，需要大量标注数据训练，但标注数据获取困难且需要大量领域知识。</li>
<li>NormProp算法利用未标记节点的同质性假设生成额外的监督信号，以提高模型泛化能力。</li>
<li>NormProp通过解耦节点表示的方向和欧几里得范数，捕获类信息和聚合过程中的一致性。</li>
<li>进行了理论分析和实验验证，确定了欧几里得范数的上限。</li>
<li>NormProp提出了同质性正则化来约束未标记节点的一致性。</li>
<li>NormProp在低标签率场景下实现了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08581">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3cc2f22ae09bc589b75d29389cc2d301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9032b5182979a998a1440a30e6e40720.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53b8cd0b446aa9f2cadb4ee5b720b025.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3637d6df7ff87ad6cd8b7767011881b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2fce375eac960685d33bd596952b66b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Get-Rid-of-Isolation-A-Continuous-Multi-task-Spatio-Temporal-Learning-Framework"><a href="#Get-Rid-of-Isolation-A-Continuous-Multi-task-Spatio-Temporal-Learning-Framework" class="headerlink" title="Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning   Framework"></a>Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning   Framework</h2><p><strong>Authors:Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Yanjiang Chen, Liheng Yu, Xu Wang, Yang Wang</strong></p>
<p>Spatiotemporal learning has become a pivotal technique to enable urban intelligence. Traditional spatiotemporal models mostly focus on a specific task by assuming a same distribution between training and testing sets. However, given that urban systems are usually dynamic, multi-sourced with imbalanced data distributions, current specific task-specific models fail to generalize to new urban conditions and adapt to new domains without explicitly modeling interdependencies across various dimensions and types of urban data. To this end, we argue that there is an essential to propose a Continuous Multi-task Spatio-Temporal learning framework (CMuST) to empower collective urban intelligence, which reforms the urban spatiotemporal learning from single-domain to cooperatively multi-dimensional and multi-task learning. Specifically, CMuST proposes a new multi-dimensional spatiotemporal interaction network (MSTI) to allow cross-interactions between context and main observations as well as self-interactions within spatial and temporal aspects to be exposed, which is also the core for capturing task-level commonality and personalization. To ensure continuous task learning, a novel Rolling Adaptation training scheme (RoAda) is devised, which not only preserves task uniqueness by constructing data summarization-driven task prompts, but also harnesses correlated patterns among tasks by iterative model behavior modeling. We further establish a benchmark of three cities for multi-task spatiotemporal learning, and empirically demonstrate the superiority of CMuST via extensive evaluations on these datasets. The impressive improvements on both few-shot streaming data and new domain tasks against existing SOAT methods are achieved. Code is available at <a target="_blank" rel="noopener" href="https://github.com/DILab-USTCSZ/CMuST">https://github.com/DILab-USTCSZ/CMuST</a>. </p>
<blockquote>
<p>时空学习已成为实现城市智能化的关键技术。传统的时空模型大多是通过假设训练集和测试集之间的相同分布来专注于特定任务。然而，鉴于城市系统通常是动态的、多源头的且数据分布不均衡，当前针对特定任务的模型无法推广到新的城市条件，并且无法适应新的领域，而没有明确地对城市数据的各种维度和类型之间的依赖关系进行建模。为此，我们提出需要提出一种连续多任务时空学习框架（CMuST），以实现集体城市智能化，该框架将城市时空学习从单域改革为合作的多维度多任务学习。具体来说，CMuST提出了一种新的多维时空交互网络（MSTI），允许上下文和主要观察结果之间的交叉交互以及空间和时间内部的自我交互，这也是捕捉任务级共性和个性化的核心。为了确保连续任务学习，设计了一种新型的滚动适应训练方案（RoAda），它不仅通过构建数据摘要驱动的任务提示来保持任务的独特性，而且还通过迭代模型行为建模来利用任务之间的相关模式。我们进一步建立了三个城市的多任务时空学习基准，并通过在这些数据集上进行广泛评估，实证证明了CMuST的优越性。与现有的SOAT方法相比，在少量流式数据和新任务域上的表现令人印象深刻。代码可在<a target="_blank" rel="noopener" href="https://github.com/DILab-USTCSZ/CMuST%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DILab-USTCSZ/CMuST找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10524v2">PDF</a> Accepted by NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>时空学习已成为实现城市智能化的关键技术。传统时空模型大多专注于特定任务，假设训练集和测试集之间具有相同的分布。然而，由于城市系统的动态性和多源性以及数据分布的不平衡性，当前针对特定任务的模型无法泛化到新的城市条件并适应新领域，没有显式地对城市数据的各种维度和类型之间的依赖关系进行建模。因此，提出一种连续多任务时空学习框架（CMuST）来推动集体城市智能化是必要的，它将城市时空学习从单域改革为合作的多维度多任务学习。特别是，CMuST提出一种新的多维度时空交互网络（MSTI），以允许上下文和主观测之间的交叉交互以及空间和时间的自我交互，这也是捕捉任务级别共性及个性化的核心所在。为了确保连续任务学习，设计了一种新型的滚动适应训练方案（RoAda），这不仅通过构建数据摘要驱动的任务提示来保持任务的独特性，而且还通过迭代模型行为建模来利用任务之间的相关模式。通过在城市多任务时空学习的基准测试中进行了实证演示，CMuST的优越性得到了广泛评估。对于少量流式数据和新的领域任务，相较于现有的顶尖方法，CMuST取得了令人印象深刻的改进。代码可通过<a target="_blank" rel="noopener" href="https://github.com/DILab-USTCSZ/CMuST%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/DILab-USTCSZ/CMuST访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>城市时空学习对于实现城市智能化至关重要。</li>
<li>传统时空模型难以适应城市系统的动态性和数据分布的不平衡性。</li>
<li>提出了一种新的连续多任务时空学习框架（CMuST），从单域改革为多维度多任务学习。</li>
<li>CMuST通过多维度时空交互网络（MSTI）捕捉任务级别共性及个性化。</li>
<li>滚动适应训练方案（RoAda）能确保连续任务学习并利用任务间的相关模式。</li>
<li>CMuST在基准测试中表现优越，特别是在处理少量流式数据和新的领域任务时。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10524">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-915f6ddb7f7341ffa8c684a5a8ad6076.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5814b1cd87ab246943140964dab05d6b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RobustEMD-Domain-Robust-Matching-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#RobustEMD-Domain-Robust-Matching-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical   Image Segmentation"></a>RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical   Image Segmentation</h2><p><strong>Authors:Yazhou Zhu, Minxian Li, Qiaolin Ye, Shidong Wang, Tong Xin, Haofeng Zhang</strong></p>
<p>Few-shot medical image segmentation (FSMIS) aims to perform the limited annotated data learning in the medical image analysis scope. Despite the progress has been achieved, current FSMIS models are all trained and deployed on the same data domain, as is not consistent with the clinical reality that medical imaging data is always across different data domains (e.g. imaging modalities, institutions and equipment sequences). How to enhance the FSMIS models to generalize well across the different specific medical imaging domains? In this paper, we focus on the matching mechanism of the few-shot semantic segmentation models and introduce an Earth Mover’s Distance (EMD) calculation based domain robust matching mechanism for the cross-domain scenario. Specifically, we formulate the EMD transportation process between the foreground support-query features, the texture structure aware weights generation method, which proposes to perform the sobel based image gradient calculation over the nodes, is introduced in the EMD matching flow to restrain the domain relevant nodes. Besides, the point set level distance measurement metric is introduced to calculated the cost for the transportation from support set nodes to query set nodes. To evaluate the performance of our model, we conduct experiments on three scenarios (i.e., cross-modal, cross-sequence and cross-institution), which includes eight medical datasets and involves three body regions, and the results demonstrate that our model achieves the SoTA performance against the compared models. </p>
<blockquote>
<p>少量标注数据的医疗图像分割（FSMIS）旨在实现医疗图像分析范围内的有限标注数据学习。尽管已经取得了一些进展，但目前的FSMIS模型都是在同一数据域中进行训练和部署的，这与医学成像数据总是跨不同数据域（例如成像模式、机构和设备序列）的临床现实不一致。如何增强FSMIS模型，使其在不同的特定医学成像域之间具有良好的通用性？在本文中，我们关注少量语义分割模型的匹配机制，并引入一种基于地球移动距离（EMD）计算的域稳健匹配机制，用于跨域场景。具体来说，我们制定了前景支持查询特征之间的EMD传输过程，并介绍了基于纹理结构感知的权重生成方法，该方法建议在节点上执行基于Sobel的图像梯度计算。在EMD匹配流中引入这种方法来约束与域相关的节点。此外，还引入了点集级距离测量指标，用于计算从支持集节点到查询集节点的运输成本。为了评估我们模型的性能，我们在三种场景（即跨模态、跨序列和跨机构）下进行了实验，包括八个医学数据集和三个身体区域，结果表明我们的模型与对比模型相比达到了最新性能水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01110v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于地球移动距离（EMD）的跨域鲁棒匹配机制被引入到了少样本医学图像分割模型中，解决了模型在不同特定医学成像域中的泛化问题。该机制通过制定前景支持查询特征之间的EMD运输过程，并引入基于Sobel的图像梯度计算来约束域相关节点，同时在点集级别引入距离测量指标来计算运输成本。实验表明，该模型在跨模态、跨序列和跨机构等三种场景下均取得了最先进的性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSMIS旨在解决医学图像分析领域有限标注数据的学习问题。</li>
<li>当前FSMIS模型局限于同一数据域，但医学成像数据通常涉及不同的数据域。</li>
<li>引入基于地球移动距离（EMD）的跨域鲁棒匹配机制，以解决模型在不同医学成像域中的泛化问题。</li>
<li>EMD匹配机制包括前景支持查询特征之间的EMD运输过程，以及基于Sobel的图像梯度计算来约束域相关节点。</li>
<li>点集级别的距离测量指标被用于计算运输成本。</li>
<li>模型在跨模态、跨序列和跨机构的三种场景下进行了实验验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01110">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ead464cd01cc60a5a1bc36b7226bc26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9694b36b6f2c838aa4e62edcc696614d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ca9d0e32e4b4033b35a3a671a4b8c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d79e93e1b52bcd63795303434c3a06e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PACE-Marrying-generalization-in-PArameter-efficient-fine-tuning-with-Consistency-rEgularization"><a href="#PACE-Marrying-generalization-in-PArameter-efficient-fine-tuning-with-Consistency-rEgularization" class="headerlink" title="PACE: Marrying generalization in PArameter-efficient fine-tuning with   Consistency rEgularization"></a>PACE: Marrying generalization in PArameter-efficient fine-tuning with   Consistency rEgularization</h2><p><strong>Authors:Yao Ni, Shan Zhang, Piotr Koniusz</strong></p>
<p>Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained transformers to downstream tasks. However, the optimization of tasks performance often comes at the cost of generalizability in fine-tuned models. To address this issue, we theoretically connect smaller weight gradient norms during training and larger datasets to the improvements in model generalization. Motivated by this connection, we propose reducing gradient norms for enhanced generalization and aligning fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Yet, naive alignment does not guarantee gradient reduction and can potentially cause gradient explosion, complicating efforts to manage gradients. To address such an issue, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization. We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations. Theoretical analysis shows that PACE not only implicitly regularizes gradients for enhanced generalization, but also implicitly aligns the fine-tuned and pre-trained models to retain knowledge. Experimental evidence supports our theories. PACE surpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC, few-shot learning, domain adaptation) showcasing its potential for resource-efficient fine-tuning. It also improves LoRA in text classification (GLUE) and mathematical reasoning (GSM-8K). The code is available at <a target="_blank" rel="noopener" href="https://github.com/MaxwellYaoNi/PACE">https://github.com/MaxwellYaoNi/PACE</a> </p>
<blockquote>
<p>参数高效微调（PEFT）能够有效地使预训练转换器适应下游任务。然而，任务性能的优化往往以微调模型的泛化能力为代价。为了解决这一问题，我们从理论上将训练过程中较小的权重梯度范数和较大的数据集与模型泛化改进联系起来。受此联系的启发，我们提出通过减小梯度范数来提高泛化能力，并通过使微调模型与预训练模型对齐来保留大规模预训练数据中的知识。然而，简单的对齐并不能保证梯度的减小，并且可能导致梯度爆炸，使梯度的管理变得复杂。为了解决这一问题，我们提出了PACE，将参数高效的精细调整与一致性正则化相结合。我们通过乘性噪声扰动从适配器学习到的特征，并确保微调后的模型在不同扰动下对同一样本保持一致。理论分析表明，PACE不仅隐式地正则化梯度以提高泛化能力，而且隐式地对微调模型和预训练模型进行对齐以保留知识。实验证据支持我们的理论。PACE在视觉适应任务（VTAB-1k、FGVC、小样本学习和域适应）上超越了现有的PEFT方法，展示了其资源高效微调的优势。它在文本分类（GLUE）和数学推理（GSM-8K）方面的LoRA也有所改进。代码可在<a target="_blank" rel="noopener" href="https://github.com/MaxwellYaoNi/PACE">https://github.com/MaxwellYaoNi/PACE</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17137v4">PDF</a> Accepted by NeurIPS 2024 as a spotlight</p>
<p><strong>Summary</strong></p>
<p>该文介绍了如何通过参数优化和一致性正则化技术改进预训练模型的微调过程。研究提出了一种新的方法PACE，该方法结合了参数高效微调（PEFT）和一致性正则化，以提高模型的泛化能力。通过理论分析和实验验证，PACE在视觉适应任务（VTAB-1k、FGVC、少样本学习和域适应）以及文本分类和数学推理任务中均表现出优越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PEFT有助于适应预训练模型到下游任务，但可能牺牲模型的泛化能力。</li>
<li>较小权重梯度范数和大数据集与模型泛化改进之间存在理论联系。</li>
<li>为了提高泛化能力，提出了PACE方法，结合了参数高效微调与一致性正则化。</li>
<li>PACE通过引入特征扰动来确保模型对同一样本在不同扰动下的一致性。</li>
<li>PACE不仅隐式地正则化梯度以提高泛化能力，而且隐式地对微调模型和预训练模型进行对齐以保留知识。</li>
<li>实验证据表明，PACE在多种任务上超越了现有的PEFT方法，包括视觉适应、文本分类和数学推理。</li>
<li>PACE方法的代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17137">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7dee999cb3f746532cf08769018775eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d483b4955bc826cf15c7ef9982c7a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1252ba28b19890b47a99bcf91ef8bb2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CrossFi-A-Cross-Domain-Wi-Fi-Sensing-Framework-Based-on-Siamese-Network"><a href="#CrossFi-A-Cross-Domain-Wi-Fi-Sensing-Framework-Based-on-Siamese-Network" class="headerlink" title="CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network"></a>CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network</h2><p><strong>Authors:Zijian Zhao, Tingwei Chen, Zhijie Cai, Xiaoyang Li, Hang Li, Qimei Chen, Guangxu Zhu</strong></p>
<p>In recent years, Wi-Fi sensing has garnered significant attention due to its numerous benefits, such as privacy protection, low cost, and penetration ability. Extensive research has been conducted in this field, focusing on areas such as gesture recognition, people identification, and fall detection. However, many data-driven methods encounter challenges related to domain shift, where the model fails to perform well in environments different from the training data. One major factor contributing to this issue is the limited availability of Wi-Fi sensing datasets, which makes models learn excessive irrelevant information and over-fit to the training set. Unfortunately, collecting large-scale Wi-Fi sensing datasets across diverse scenarios is a challenging task. To address this problem, we propose CrossFi, a siamese network-based approach that excels in both in-domain scenario and cross-domain scenario, including few-shot, zero-shot scenarios, and even works in few-shot new-class scenario where testing set contains new categories. The core component of CrossFi is a sample-similarity calculation network called CSi-Net, which improves the structure of the siamese network by using an attention mechanism to capture similarity information, instead of simply calculating the distance or cosine similarity. Based on it, we develop an extra Weight-Net that can generate a template for each class, so that our CrossFi can work in different scenarios. Experimental results demonstrate that our CrossFi achieves state-of-the-art performance across various scenarios. In gesture recognition task, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72% in one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario, and 84.75% in one-shot new-class scenario. The code for our model is publicly available at <a target="_blank" rel="noopener" href="https://github.com/RS2002/CrossFi">https://github.com/RS2002/CrossFi</a>. </p>
<blockquote>
<p>近年来，Wi-Fi感知因其隐私保护、低成本和穿透能力等诸多优势而备受关注。在此领域，已经进行了大量研究，主要集中在手势识别、人员识别和跌倒检测等方面。然而，许多数据驱动的方法在面临领域偏移挑战时效果不佳，即在不同于训练数据的环境中模型表现不佳。导致此问题的一个主要因素是Wi-Fi感知数据集的可获取性有限，这使得模型学习了过多的无关信息并对训练集过度拟合。不幸的是，在多种场景下收集大规模的Wi-Fi感知数据集是一项具有挑战性的任务。为了解决这个问题，我们提出了CrossFi，这是一种基于孪生网络的方法，在域内场景和跨域场景中都表现出色，包括小样本次、零样本场景，甚至在新类别小样本次场景中也能工作，其中测试集包含新类别。CrossFi的核心组件是一个名为CSi-Net的样本相似性计算网络，它通过注意力机制改进了孪生网络的架构，以捕获相似性信息，而不是简单地计算距离或余弦相似性。基于此，我们开发了一个额外的Weight-Net，可以为每个类别生成一个模板，使我们的CrossFi能够在不同的场景中工作。实验结果表明，CrossFi在各种场景中实现了最先进的性能。在手势识别任务中，CrossFi在域内场景中的准确率为98.17%，在单样本跨域场景中的准确率为91.72%，在零样本跨域场景中的准确率为64.81%，在新类别单样本场景中的准确率为84.75%。我们的模型代码公开在<a target="_blank" rel="noopener" href="https://github.com/RS2002/CrossFi">https://github.com/RS2002/CrossFi</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10919v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Wi-Fi感知技术近年来备受关注，但数据驱动的方法面临域偏移问题。本文提出CrossFi方法，基于孪生网络和样本相似性计算网络CSi-Net，能在不同场景下实现优秀性能，包括少样本、零样本场景和新类别场景。实验结果表明，CrossFi在多种场景下达到最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Wi-Fi感知技术受到广泛关注，具有隐私保护、低成本和穿透能力等优点。</li>
<li>数据驱动的方法在Wi-Fi感知领域面临域偏移问题，模型在不同于训练数据的环境中表现不佳。</li>
<li>CrossFi方法基于孪生网络和样本相似性计算网络CSi-Net，能有效解决域偏移问题。</li>
<li>CrossFi在多种场景下表现优秀，包括少样本、零样本场景和新类别场景。</li>
<li>实验结果表明，CrossFi在姿态识别任务中达到98.17%的准确率。</li>
<li>CrossFi模型的代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10919">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b1741e4f78ba5b3a19386f3a48135c28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dee5bee53a257511993acf520173529.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00f07cb65b2b8e09ef0aaac908b0fcbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77ef47c0f958d1e58a3dffdc81b59387.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HeadGAP-Few-Shot-3D-Head-Avatar-via-Generalizable-Gaussian-Priors"><a href="#HeadGAP-Few-Shot-3D-Head-Avatar-via-Generalizable-Gaussian-Priors" class="headerlink" title="HeadGAP: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors"></a>HeadGAP: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors</h2><p><strong>Authors:Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu</strong></p>
<p>In this paper, we present a novel 3D head avatar creation approach capable of generalizing from few-shot in-the-wild data with high-fidelity and animatable robustness. Given the underconstrained nature of this problem, incorporating prior knowledge is essential. Therefore, we propose a framework comprising prior learning and avatar creation phases. The prior learning phase leverages 3D head priors derived from a large-scale multi-view dynamic dataset, and the avatar creation phase applies these priors for few-shot personalization. Our approach effectively captures these priors by utilizing a Gaussian Splatting-based auto-decoder network with part-based dynamic modeling. Our method employs identity-shared encoding with personalized latent codes for individual identities to learn the attributes of Gaussian primitives. During the avatar creation phase, we achieve fast head avatar personalization by leveraging inversion and fine-tuning strategies. Extensive experiments demonstrate that our model effectively exploits head priors and successfully generalizes them to few-shot personalization, achieving photo-realistic rendering quality, multi-view consistency, and stable animation. </p>
<blockquote>
<p>本文介绍了一种新型的三维头部化身创建方法，能够从少量野外数据中进行高度保真和可动画的稳健性概括。鉴于这个问题的约束性较小，融入先验知识至关重要。因此，我们提出了一个包含先验学习阶段和化身创建阶段的框架。先验学习阶段利用大规模多视角动态数据集衍生的三维头部先验知识，而化身创建阶段则应用这些先验知识来进行少量个性化设置。我们的方法有效地捕捉这些先验知识，采用基于高斯拼贴技术的自动解码器网络进行部分动态建模。我们的方法采用具有个性化潜在代码的身份共享编码来学习高斯基本元素的属性。在化身创建阶段，我们通过利用反转和微调策略实现了快速头部化身的个性化设置。大量实验表明，我们的模型有效地利用了头部先验知识，并成功将其推广到少量个性化设置上，实现了逼真的渲染质量、多视角一致性和稳定的动画效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.06019v2">PDF</a> Accepted to 3DV 2025. Project page: <a target="_blank" rel="noopener" href="https://headgap.github.io/">https://headgap.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的三维头部虚拟角色创建方法，该方法能够从小规模数据中进行泛化，并展现出高度保真和动画鲁棒性。方法包括两个阶段：先验学习阶段和角色创建阶段。先验学习阶段利用大规模多视角动态数据集提取的头部三维先验知识，角色创建阶段则应用这些先验进行个性化创建。该方法通过基于高斯拼贴技术的自动解码器网络和部分动态建模技术有效捕捉这些先验知识。方法采用共享身份编码和个性化潜在代码学习高斯基本属性的身份特征。在角色创建阶段，通过反转和微调策略实现快速头部角色个性化。实验证明，该模型有效地利用头部先验知识，成功泛化到小规模个性化数据，达到逼真的渲染质量、多视角一致性和稳定的动画效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新型三维头部虚拟角色创建方法，能够从少量数据中泛化并表现出高度保真和动画鲁棒性。</li>
<li>方法包括先验学习阶段和角色创建阶段，前者利用大规模多视角动态数据集提取头部三维先验知识。</li>
<li>采用基于高斯拼贴技术的自动解码器网络和部分动态建模技术捕捉先验知识。</li>
<li>方法采用共享身份编码和个性化潜在代码来学习高斯基本属性的身份特征。</li>
<li>通过反转和微调策略实现快速头部角色个性化。</li>
<li>模型有效泛化到小规模个性化数据，达到逼真的渲染质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.06019">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d9fb2f2c50947c8a3957043eb8f75c59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4768d6c4ded301cca943516e0c82a477.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d311be7ef2485c6d182f6edcab5978b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1e0cdef0dcc76901e207d436b1ec963.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ba7795064ff602ff61c8717c10338cc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Foundation-Language-Image-Model-of-the-Retina-FLAIR-Encoding-Expert-Knowledge-in-Text-Supervision"><a href="#A-Foundation-Language-Image-Model-of-the-Retina-FLAIR-Encoding-Expert-Knowledge-in-Text-Supervision" class="headerlink" title="A Foundation Language-Image Model of the Retina (FLAIR): Encoding Expert   Knowledge in Text Supervision"></a>A Foundation Language-Image Model of the Retina (FLAIR): Encoding Expert   Knowledge in Text Supervision</h2><p><strong>Authors:Julio Silva-Rodríguez, Hadi Chakor, Riadh Kobbi, Jose Dolz, Ismail Ben Ayed</strong></p>
<p>Foundation vision-language models are currently transforming computer vision, and are on the rise in medical imaging fueled by their very promising generalization capabilities. However, the initial attempts to transfer this new paradigm to medical imaging have shown less impressive performances than those observed in other domains, due to the significant domain shift and the complex, expert domain knowledge inherent to medical-imaging tasks. Motivated by the need for domain-expert foundation models, we present FLAIR, a pre-trained vision-language model for universal retinal fundus image understanding. To this end, we compiled 38 open-access, mostly categorical fundus imaging datasets from various sources, with up to 101 different target conditions and 288,307 images. We integrate the expert’s domain knowledge in the form of descriptive textual prompts, during both pre-training and zero-shot inference, enhancing the less-informative categorical supervision of the data. Such a textual expert’s knowledge, which we compiled from the relevant clinical literature and community standards, describes the fine-grained features of the pathologies as well as the hierarchies and dependencies between them. We report comprehensive evaluations, which illustrate the benefit of integrating expert knowledge and the strong generalization capabilities of FLAIR under difficult scenarios with domain shifts or unseen categories. When adapted with a lightweight linear probe, FLAIR outperforms fully-trained, dataset-focused models, more so in the few-shot regimes. Interestingly, FLAIR outperforms by a wide margin larger-scale generalist image-language models and retina domain-specific self-supervised networks, which emphasizes the potential of embedding experts’ domain knowledge and the limitations of generalist models in medical imaging. </p>
<blockquote>
<p>目前，基础视觉语言模型正在改变计算机视觉领域，并且在医疗成像领域呈现出上升趋势，这得益于其非常有前景的泛化能力。然而，将这一新范式应用于医疗成像的初步尝试，由于领域转移和医疗成像任务固有的复杂专家知识，其表现不如其他领域那么令人印象深刻。为了满足对领域专家基础模型的需求，我们提出了FLAIR，这是一个用于通用视网膜眼底图像理解的基础视觉语言模型。为此，我们从各种来源整合了38个开放访问的眼底成像数据集，涵盖多达101种不同目标疾病和288,307张图像。我们以描述性文本提示的形式整合专家的领域知识，这种知识是我们在相关临床文献和社区标准中编译的，描述了病理的细微特征以及它们之间的层次结构和依赖关系。我们在预训练和零样本推理中都融入了这种文本专家知识。我们进行了全面的评估，展示了融入专家知识的益处以及FLAIR在领域转移或未见类别等困难场景下的强大泛化能力。当使用轻量级线性探针进行适应时，FLAIR在少数类别中的表现优于专注于数据集的模型，特别是在视网膜疾病的识别方面。有趣的是，FLAIR大幅超越了大规模通用图像语言模型和视网膜领域的特定自监督网络，这强调了嵌入专家领域知识的潜力以及通用模型在医疗成像中的局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07898v2">PDF</a> Accepted in Medical Image Analysis. The pre-trained model is   available at: <a target="_blank" rel="noopener" href="https://github.com/jusiro/FLAIR">https://github.com/jusiro/FLAIR</a></p>
<p><strong>摘要</strong></p>
<p>基于跨领域迁移学习的视角，文章介绍了面向视网膜基金图像理解的预训练视觉语言模型FLAIR。为提高模型在医学成像领域的表现，研究团队整合了专业领域的知识，并以描述性的文本提示形式融入模型，强化了数据的类别监督信息。通过全面的评估，文章展示了整合专家知识的优势以及FLAIR在困难场景下的强大泛化能力。当使用轻量级线性探针进行适配时，FLAIR在少样本情况下表现优异，并大幅超越大规模的综合图像语言模型和视网膜领域的特定自监督网络，突显了嵌入专家领域知识的潜力以及通用模型在医学成像中的局限性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>跨领域迁移学习对于将视觉语言模型应用于医学成像具有重要意义。</li>
<li>在开发面向视网膜基金图像理解的模型FLAIR时，结合了专家领域知识，并以文本提示的形式融入模型训练与零样本推理过程。</li>
<li>文本提示包含精细的病理特征描述以及病理层次和依赖关系的描述。</li>
<li>通过全面的评估证明了整合专家知识的优势以及FLAIR的泛化能力。</li>
<li>在少样本情况下，使用轻量级线性探针适配的FLAIR表现突出。</li>
<li>与大规模的综合图像语言模型和视网膜领域的特定自监督网络相比，FLAIR大幅领先，突显了嵌入专家领域知识的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.07898">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f946287d4ae8e9c1080599b4b1fa7890.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11108b356cf04b56dac1a242c6cdb461.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83ddd50e8a99d0ed044f6290934095be.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Controlling-Equational-Reasoning-in-Large-Language-Models-with-Prompt-Interventions"><a href="#Controlling-Equational-Reasoning-in-Large-Language-Models-with-Prompt-Interventions" class="headerlink" title="Controlling Equational Reasoning in Large Language Models with Prompt   Interventions"></a>Controlling Equational Reasoning in Large Language Models with Prompt   Interventions</h2><p><strong>Authors:Jordan Meadows, Marco Valentino, Andre Freitas</strong></p>
<p>This paper investigates how hallucination rates in Large Language Models (LLMs) may be controlled via a symbolic data generation framework, exploring a fundamental relationship between the rate of certain mathematical errors and types of input intervention. Specifically, we systematically generate data for a derivation generation task using a symbolic engine, applying targeted interventions to prompts to perturb features of mathematical derivations such as the surface forms of symbols, equational tree structures, and mathematical context. We then evaluate the effect of prompt interventions across a range of LLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Our experiments suggest that T5-Large can outperform the few-shot performance of GPT-4 on various evaluation sets generated via the framework. However, an extensive evaluation based on human analysis, template-based error detection, and text generation metrics reveals model weaknesses beyond what the reference-based metrics singularly describe. We use these results to tie characteristic distributional footprints of interventions to the human evaluation of LLM derivation quality, potentially leading to significant control over fine-grained mathematical capabilities of language models with respect to specific types of errors. </p>
<blockquote>
<p>本文探讨如何通过符号数据生成框架控制大型语言模型（LLM）中的幻觉率，探索特定数学错误率与输入干预类型之间的基本关系。具体来说，我们使用符号引擎为推导生成任务系统地生成数据，对提示进行有针对性的干预，以扰动数学推导的特征，如符号的表面形式、方程式树结构和数学上下文。然后，我们在一系列LLM（包括微调T5模型、GPT和LLama基于的模型）中评估提示干预的效果。我们的实验表明，T5-Large可以在各种通过该框架生成的评估集上实现优于GPT-4的少样本表现。然而，基于人类分析、模板错误检测和文本生成指标的广泛评估揭示了模型在单一基准测试指标之外存在的弱点。我们使用这些结果将干预的分布特征与LLM推导质量的人类评估联系起来，这可能会为针对特定类型错误的控制语言模型的精细数学能力提供显著的控制力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09998v5">PDF</a> AAAI 2025 (7 pages)</p>
<p><strong>Summary</strong></p>
<p>这篇论文探讨如何通过符号数据生成框架控制大型语言模型（LLM）中的幻觉率，研究数学错误率与输入干预类型之间的基本关系。论文通过符号引擎系统地生成数据，针对推导生成任务应用有针对性的干预措施，扰动数学推导的特征，如符号的表面形式、方程式树结构和数学上下文。评估了不同LLMs在提示干预下的表现，包括微调T5模型、GPT和LLaMa模型。实验表明，T5-Large在某些评估集上的表现优于GPT-4。然而，基于人类分析、模板化错误检测和文本生成指标的全面评估揭示了模型除了参考基准指标所描述的弱点之外的其他弱点。研究结果将干预的分布特征与LLM推导质量的人类评估联系起来，有望实现对语言模型精细数学能力的显著控制，并减少特定类型的错误。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文探索了如何通过符号数据生成框架控制大型语言模型中的幻觉率。</li>
<li>研究了数学错误率与输入干预类型之间的基本关系。</li>
<li>通过符号引擎系统地生成数据，并应用有针对性的干预措施来扰动数学推导的特征。</li>
<li>评估了不同LLMs在提示干预下的表现。</li>
<li>T5-Large在某些评估集上的表现优于GPT-4。</li>
<li>全面评估揭示了模型除了参考基准指标所描述的弱点之外的其他弱点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.09998">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2359b75b2ee5e8f4ad4f33fcba9b16d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a30502526111a64dceb10edcc4688a15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42b73a19d8b3872ddbe3b8f95f5190e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e91f4cf9b699506343573d63f98eb7f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acd109d87dcdb77eb14ba15acaf7f7a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3c1ac4a53eefa6664e21702dae43ab4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dae1fb00d4461afb66decad012e72917.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-01-17  IDEA Image Description Enhanced CLIP-Adapter
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e2a8cc888dfff4e16af91c98a51243aa.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-01-17  Leveraging LLM Agents for Translating Network Configurations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11370.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
