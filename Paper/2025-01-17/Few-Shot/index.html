<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  IDEA Image Description Enhanced CLIP-Adapter">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3637d6df7ff87ad6cd8b7767011881b8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    46 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-17-æ›´æ–°"><a href="#2025-01-17-æ›´æ–°" class="headerlink" title="2025-01-17 æ›´æ–°"></a>2025-01-17 æ›´æ–°</h1><h2 id="IDEA-Image-Description-Enhanced-CLIP-Adapter"><a href="#IDEA-Image-Description-Enhanced-CLIP-Adapter" class="headerlink" title="IDEA: Image Description Enhanced CLIP-Adapter"></a>IDEA: Image Description Enhanced CLIP-Adapter</h2><p><strong>Authors:Zhipeng Ye, Feng Jiang, Qiufeng Wang, Kaizhu Huang, Jiaqi Huang</strong></p>
<p>CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the modelâ€™s performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named â€œIMD-11â€. Our code and data are released at <a target="_blank" rel="noopener" href="https://github.com/FourierAI/IDEA">https://github.com/FourierAI/IDEA</a>. </p>
<blockquote>
<p>CLIPï¼ˆå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼‰åœ¨æ¨¡å¼è¯†åˆ«å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚å°†CLIPåº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆä¾‹å¦‚é›¶æ ·æœ¬æˆ–å°æ ·æœ¬åˆ†ç±»ï¼‰æ˜¯å¤šæ¨¡æ€å­¦ä¹ çš„çƒ­é—¨è¯é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºæ–‡æœ¬çš„æç¤ºå­¦ä¹ æˆ–è§†è§‰çš„é€‚é…å™¨è°ƒæ•´ï¼Œè€Œæ²¡æœ‰å……åˆ†åˆ©ç”¨å›¾åƒ-æ–‡æœ¬å¯¹ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯å’Œå…³è”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å›¾åƒæè¿°å¢å¼ºCLIPé€‚é…å™¨ï¼ˆIDEAï¼‰æ–¹æ³•ï¼Œå°†CLIPé€‚åº”äºå°æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å›¾åƒçš„è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬æè¿°æ¥æ•è·ç»†å¾®ç‰¹å¾ã€‚IDEAæ˜¯ä¸€ç§æ— è®­ç»ƒçš„CLIPæ–¹æ³•ï¼Œå®ƒåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°å¯ä¸æˆ–è¶…è¿‡æœ€æ–°æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯è®­ç»ƒIDEAï¼ˆT-IDEAï¼‰ï¼Œå®ƒé€šè¿‡æ·»åŠ ä¸¤ä¸ªè½»é‡çº§å¯å­¦ä¹ ç»„ä»¶ï¼ˆå³æŠ•å½±å™¨å’Œå¯å­¦ä¹ æ½œåœ¨ç©ºé—´ï¼‰æ¥æ‰©å±•IDEAï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨11ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœã€‚ä½œä¸ºä¸€ä¸ªé‡è¦è´¡çŒ®ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†Llamaæ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„æµç¨‹æ¥ç”Ÿæˆè¿™11ä¸ªæ•°æ®é›†çš„å›¾åƒæ–‡æœ¬æè¿°ï¼Œå…±äº§ç”Ÿ1,637,795ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ï¼Œåä¸ºâ€œIMD-11â€ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/FourierAI/IDEA%E4%B8%8A%E3%80%82">https://github.com/FourierAI/IDEAä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08816v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CLIPåœ¨æ¨¡å¼è¯†åˆ«å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œå…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚é›¶æ ·æœ¬æˆ–å°æ ·æœ¬åˆ†ç±»ï¼‰çš„è¿ç§»å­¦ä¹ å·²æˆä¸ºå¤šæ¨¡æ€å­¦ä¹ çš„çƒ­é—¨è¯é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨æ–‡æœ¬æç¤ºå­¦ä¹ æˆ–è§†è§‰é€‚é…å™¨è°ƒæ•´ï¼Œæœªå……åˆ†åˆ©ç”¨å›¾åƒæ–‡æœ¬å¯¹ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯å’Œç›¸å…³æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒæè¿°å¢å¼ºçš„CLIPé€‚é…å™¨ï¼ˆIDEAï¼‰æ–¹æ³•ï¼Œç”¨äºé€‚åº”å°æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨è§†è§‰ç‰¹å¾å’Œå›¾åƒæ–‡æœ¬æè¿°æ¥æ•æ‰ç²¾ç»†ç‰¹å¾ã€‚IDEAæ˜¯ä¸€ç§é’ˆå¯¹CLIPçš„æ— è®­ç»ƒæ–¹æ³•ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯ä¸æˆ–è¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†é€šè¿‡æ·»åŠ ä¸¤ä¸ªè½»é‡çº§å­¦ä¹ ç»„ä»¶ï¼ˆå³æŠ•å½±å™¨å’Œå¯å­¦ä¹ æ½œåœ¨ç©ºé—´ï¼‰å¯¹IDEAè¿›è¡Œæ‰©å±•çš„å¯è®­ç»ƒIDEAï¼ˆT-IDEAï¼‰ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨11ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœã€‚åŒæ—¶ï¼Œæœ¬æ–‡åˆ©ç”¨Llamaæ¨¡å‹è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„ç®¡é“æ¥ç”Ÿæˆ11ä¸ªæ•°æ®é›†çš„å›¾åƒæ–‡æœ¬æè¿°ï¼Œäº§ç”Ÿäº†æ€»è®¡163ä¸‡7åƒ7ç™¾9åäº”ä¸ªå›¾åƒæ–‡æœ¬å¯¹ï¼Œåä¸ºâ€œIMD-11â€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPåœ¨æ¨¡å¼è¯†åˆ«å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚å°æ ·æœ¬åˆ†ç±»ï¼‰çš„è¿ç§»å­¦ä¹ æ˜¯CLIPçš„çƒ­é—¨åº”ç”¨æ–¹å‘ã€‚</li>
<li>å½“å‰ç ”ç©¶æœªå……åˆ†åˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯å’Œç›¸å…³æ€§ã€‚</li>
<li>IDEAæ–¹æ³•é€šè¿‡ç»“åˆè§†è§‰ç‰¹å¾å’Œå›¾åƒæ–‡æœ¬æè¿°ï¼Œé€‚åº”å°æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>IDEAæ˜¯ä¸€ç§é’ˆå¯¹CLIPçš„æ— è®­ç»ƒæ–¹æ³•ï¼Œæ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>T-IDEAé€šè¿‡æ·»åŠ å­¦ä¹ ç»„ä»¶è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€æ–°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b95da945eac159dfa1ea766fd44cf7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7819e00679e650231320734168624399.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Exploring-ChatGPT-for-Face-Presentation-Attack-Detection-in-Zero-and-Few-Shot-in-Context-Learning"><a href="#Exploring-ChatGPT-for-Face-Presentation-Attack-Detection-in-Zero-and-Few-Shot-in-Context-Learning" class="headerlink" title="Exploring ChatGPT for Face Presentation Attack Detection in Zero and   Few-Shot in-Context Learning"></a>Exploring ChatGPT for Face Presentation Attack Detection in Zero and   Few-Shot in-Context Learning</h2><p><strong>Authors:Alain Komaty, Hatef Otroshi Shahreza, Anjith George, Sebastien Marcel</strong></p>
<p>This study highlights the potential of ChatGPT (specifically GPT-4o) as a competitive alternative for Face Presentation Attack Detection (PAD), outperforming several PAD models, including commercial solutions, in specific scenarios. Our results show that GPT-4o demonstrates high consistency, particularly in few-shot in-context learning, where its performance improves as more examples are provided (reference data). We also observe that detailed prompts enable the model to provide scores reliably, a behavior not observed with concise prompts. Additionally, explanation-seeking prompts slightly enhance the modelâ€™s performance by improving its interpretability. Remarkably, the model exhibits emergent reasoning capabilities, correctly predicting the attack type (print or replay) with high accuracy in few-shot scenarios, despite not being explicitly instructed to classify attack types. Despite these strengths, GPT-4o faces challenges in zero-shot tasks, where its performance is limited compared to specialized PAD systems. Experiments were conducted on a subset of the SOTERIA dataset, ensuring compliance with data privacy regulations by using only data from consenting individuals. These findings underscore GPT-4oâ€™s promise in PAD applications, laying the groundwork for future research to address broader data privacy concerns and improve cross-dataset generalization. Code available here: <a target="_blank" rel="noopener" href="https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad">https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad</a> </p>
<blockquote>
<p>æœ¬ç ”ç©¶é‡ç‚¹æ¢è®¨äº†ChatGPTï¼ˆç‰¹åˆ«æ˜¯GPT-4oï¼‰åœ¨äººè„¸è¯†åˆ«æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰ä¸­çš„æ½œåŠ›ï¼Œå…¶åœ¨ç‰¹å®šåœºæ™¯ä¸‹å¯¹åŒ…æ‹¬å•†ä¸šè§£å†³æ–¹æ¡ˆåœ¨å†…çš„å¤šä¸ªPADæ¨¡å‹å…·æœ‰ç«äº‰ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGPT-4oè¡¨ç°å‡ºé«˜åº¦çš„ä¸€è‡´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ç¯å¢ƒä¸­ï¼Œå…¶æ€§èƒ½ä¼šéšç€æä¾›çš„ç¤ºä¾‹æ•°é‡å¢å¤šè€Œæé«˜ï¼ˆå‚è€ƒæ•°æ®ï¼‰ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œè¯¦ç»†çš„æç¤ºä½¿æ¨¡å‹èƒ½å¤Ÿå¯é åœ°æä¾›åˆ†æ•°ï¼Œè¿™ä¸€è¡Œä¸ºåœ¨ç®€æ´æç¤ºä¸‹å¹¶æœªè§‚å¯Ÿåˆ°ã€‚æ­¤å¤–ï¼Œå¯»æ±‚è§£é‡Šçš„æç¤ºç•¥å¾®æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œæ”¹å–„äº†å…¶å¯è§£é‡Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æ²¡æœ‰æ˜ç¡®è¦æ±‚åˆ†ç±»æ”»å‡»ç±»å‹ï¼Œè¯¥æ¨¡å‹ä»å±•ç°å‡ºæ¶Œç°çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å°æ ·æœ¬åœºæ™¯ä¸­å‡†ç¡®é¢„æµ‹æ”»å‡»ç±»å‹ï¼ˆæ‰“å°æˆ–å›æ”¾ï¼‰ï¼Œä¸”å‡†ç¡®ç‡æé«˜ã€‚å°½ç®¡æœ‰è¿™äº›ä¼˜ç‚¹ï¼ŒGPT-4oåœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå…¶æ€§èƒ½ä¸ä¸“ä¸šçš„PADç³»ç»Ÿç›¸æ¯”æœ‰é™ã€‚å®éªŒæ˜¯åœ¨SOTERIAæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ä¸Šè¿›è¡Œçš„ï¼Œä»…ä½¿ç”¨åŒæ„ä¸ªäººçš„æ•°æ®ï¼Œä»¥ç¡®ä¿ç¬¦åˆæ•°æ®éšç§æ³•è§„ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†GPT-4oåœ¨PADåº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥ç ”ç©¶è§£å†³æ›´å¹¿æ³›çš„æ•°æ®éšç§é—®é¢˜å’Œæ”¹è¿›è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›å¥ å®šäº†åŸºç¡€ã€‚ç›¸å…³ä»£ç å¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad">https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08799v1">PDF</a> Accepted in WACV workshop 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ChatGPTï¼ˆç‰¹åˆ«æ˜¯GPT-4oï¼‰åœ¨é¢éƒ¨å‘ˆç°æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰ä¸­çš„æ½œåŠ›ï¼Œå¹¶åœ¨ç‰¹å®šåœºæ™¯ä¸‹è¡¨ç°å‡ºè¶…è¶Šå¤šç§PADæ¨¡å‹ï¼ˆåŒ…æ‹¬å•†ä¸šè§£å†³æ–¹æ¡ˆï¼‰çš„æ€§èƒ½ã€‚GPT-4oåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹å±•ç°å‡ºå“è¶Šçš„ä¸€è‡´æ€§ï¼Œéšç€æä¾›æ›´å¤šæ ·æœ¬ï¼Œå…¶æ€§èƒ½æœ‰æ‰€æå‡ã€‚è¯¦ç»†çš„æç¤ºèƒ½ä½¿æ¨¡å‹å¾—åˆ†æ›´å¯é ï¼Œè€Œç®€æ´çš„æç¤ºåˆ™æœªè§‚å¯Ÿåˆ°è¿™ç§è¡Œä¸ºã€‚æ­¤å¤–ï¼Œå¯»æ±‚è§£é‡Šçš„æç¤ºç•¥å¾®æé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚å°½ç®¡GPT-4oæœªåœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒèƒ½æ­£ç¡®é¢„æµ‹æ”»å‡»ç±»å‹ï¼ˆæ‰“å°æˆ–å›æ”¾ï¼‰ï¼Œå±•ç°å‡ºæ¨ç†èƒ½åŠ›ã€‚å®éªŒåœ¨SOTERIAæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ä¸Šè¿›è¡Œï¼Œå¹¶ä¸¥æ ¼éµå®ˆæ•°æ®éšç§æ³•è§„ï¼Œä»…ä½¿ç”¨åŒæ„å‚ä¸è€…çš„æ•°æ®ã€‚è¿™äº›å‘ç°çªæ˜¾äº†GPT-4oåœ¨PADåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oåœ¨é¢éƒ¨å‘ˆç°æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ï¼Œè¶…è¶Šäº†å¤šç§PADæ¨¡å‹ã€‚</li>
<li>GPT-4oåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹å…·æœ‰å“è¶Šçš„ä¸€è‡´æ€§å’Œæ€§èƒ½ã€‚</li>
<li>è¯¦ç»†çš„æç¤ºèƒ½ä½¿GPT-4oçš„å¾—åˆ†æ›´å¯é ã€‚</li>
<li>å¯»æ±‚è§£é‡Šçš„æç¤ºå¯ä»¥æé«˜GPT-4oçš„è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>GPT-4oå±•ç°å‡ºæ­£ç¡®çš„æ”»å‡»ç±»å‹é¢„æµ‹èƒ½åŠ›ï¼Œå…·æœ‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒåœ¨ç¬¦åˆæ•°æ®éšç§æ³•è§„çš„SOTERIAæ•°æ®é›†å­é›†ä¸Šè¿›è¡Œã€‚</li>
<li>ç ”ç©¶ä¸ºGPT-4oåœ¨PADåº”ç”¨ä¸­çš„æ½œåŠ›å¥ å®šäº†åŸºç¡€ï¼Œæœªæ¥ç ”ç©¶å¯å…³æ³¨æ•°æ®éšç§å’Œè·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6e4eaf9ed9577a6401727cd591ee741d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78f21b44cc764b4d7829741379e2616d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08a66ea4ab4c504da314507094064799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2a389416c7f232aa922db22f4b699ae.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learner-Generalizes-Across-AI-Generated-Image-Detection"><a href="#Few-Shot-Learner-Generalizes-Across-AI-Generated-Image-Detection" class="headerlink" title="Few-Shot Learner Generalizes Across AI-Generated Image Detection"></a>Few-Shot Learner Generalizes Across AI-Generated Image Detection</h2><p><strong>Authors:Shiyu Wu, Jing Liu, Jing Li, Yequan Wang</strong></p>
<p>Current fake image detectors trained on large synthetic image datasets perform satisfactorily on limited studied generative models. However, they suffer a notable performance decline over unseen models. Besides, collecting adequate training data from online generative models is often expensive or infeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a novel AI-generated image detector which learns a specialized metric space to effectively distinguish unseen fake images by utilizing very few samples. Experiments show FSD achieves state-of-the-art performance by $+7.4%$ average ACC on GenImage dataset. More importantly, our method is better capable of capturing the intra-category common features in unseen images without further training. </p>
<blockquote>
<p>å½“å‰åœ¨å¤§å‹åˆæˆå›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒçš„è™šå‡å›¾åƒæ£€æµ‹å™¨ï¼Œåœ¨æœ‰é™çš„ç”Ÿæˆæ¨¡å‹ä¸Šçš„è¡¨ç°å°šå¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æœªè§æ¨¡å‹ä¸Šçš„æ€§èƒ½ä¼šå‡ºç°æ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œä»åœ¨çº¿ç”Ÿæˆæ¨¡å‹ä¸­æ”¶é›†è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®é€šå¸¸æˆæœ¬é«˜æ˜‚æˆ–ä¸å¯è¡Œã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å°æ ·æ£€æµ‹å™¨ï¼ˆFSDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹å™¨ã€‚é€šè¿‡åˆ©ç”¨æå°‘çš„æ ·æœ¬ï¼Œå®ƒå­¦ä¹ ä¸“é—¨çš„åº¦é‡ç©ºé—´ï¼Œä»¥æœ‰æ•ˆåŒºåˆ†æœªè§è¿‡çš„è™šå‡å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼ŒFSDåœ¨GenImageæ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†+7.4%ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— éœ€è¿›ä¸€æ­¥è®­ç»ƒï¼Œä¾¿èƒ½æ›´å¥½åœ°æ•æ‰æœªè§å›¾åƒä¸­çš„ç±»åˆ«å†…é€šç”¨ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08763v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºAIæŠ€æœ¯çš„å›¾åƒæ£€æµ‹å™¨â€”â€”Few-Shot Detectorï¼ˆFSDï¼‰ã€‚è¯¥æ£€æµ‹å™¨é€šè¿‡åˆ©ç”¨æå°‘é‡æ ·æœ¬å­¦ä¹ ä¸“é—¨çš„åº¦é‡ç©ºé—´ï¼Œæœ‰æ•ˆåŒºåˆ†æœªè§è¿‡çš„è™šå‡å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼ŒFSDåœ¨GenImageæ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®ç‡è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œæé«˜äº†7.4%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æœªè§å›¾åƒä¸­çš„ç±»åˆ«å†…é€šç”¨ç‰¹å¾ï¼Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Detectorï¼ˆFSDï¼‰æ˜¯ä¸€ç§æ–°å‹çš„AIç”Ÿæˆçš„å›¾åƒæ£€æµ‹å™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è™šå‡å›¾åƒæ£€æµ‹å™¨åœ¨é¢å¯¹æœªè§æ¨¡å‹æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>FSDé€šè¿‡å­¦ä¹ ä¸“é—¨çš„åº¦é‡ç©ºé—´æ¥åŒºåˆ†è™šå‡å›¾åƒï¼Œä»…åˆ©ç”¨å°‘é‡æ ·æœ¬å³å¯å®ç°æœ‰æ•ˆæ£€æµ‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFSDåœ¨GenImageæ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®ç‡è¾ƒä¹‹å‰çš„æ–¹æ³•æœ‰æ‰€æé«˜ï¼Œè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</li>
<li>FSDèƒ½å¤Ÿæ•æ‰æœªè§å›¾åƒä¸­çš„ç±»åˆ«å†…é€šç”¨ç‰¹å¾ï¼Œè¿™æ˜¯å…¶ä¸å…¶ä»–æ£€æµ‹å™¨çš„é‡è¦åŒºåˆ«ã€‚</li>
<li>FSDçš„ä¼˜åŠ¿åœ¨äºå…¶å¯¹äºè®­ç»ƒæ•°æ®çš„è·å–éœ€æ±‚è¾ƒå°ï¼Œå³ä½¿é¢ä¸´æœ‰é™æˆ–æ˜‚è´µçš„è®­ç»ƒæ•°æ®æƒ…å†µï¼Œä¹Ÿèƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>FSDçš„æå‡ºä¸ºè™šå‡å›¾åƒæ£€æµ‹é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰æœ›åœ¨æœªæ¥å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a7be3274d6331414dbdabcfff67852e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbaf68487cb3b56696c9623cd3894b7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e60e3a8e7ae890dfd5b2f0f4633a2b0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b238014150011da2dbeafcb55dac0a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95e630b63a431837b369f821daa7733d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Normalize-Then-Propagate-Efficient-Homophilous-Regularization-for-Few-shot-Semi-Supervised-Node-Classification"><a href="#Normalize-Then-Propagate-Efficient-Homophilous-Regularization-for-Few-shot-Semi-Supervised-Node-Classification" class="headerlink" title="Normalize Then Propagate: Efficient Homophilous Regularization for   Few-shot Semi-Supervised Node Classification"></a>Normalize Then Propagate: Efficient Homophilous Regularization for   Few-shot Semi-Supervised Node Classification</h2><p><strong>Authors:Baoming Zhang, MingCai Chen, Jianqing Song, Shuangjie Li, Jie Zhang, Chongjun Wang</strong></p>
<p>Graph Neural Networks (GNNs) have demonstrated remarkable ability in semi-supervised node classification. However, most existing GNNs rely heavily on a large amount of labeled data for training, which is labor-intensive and requires extensive domain knowledge. In this paper, we first analyze the restrictions of GNNs generalization from the perspective of supervision signals in the context of few-shot semi-supervised node classification. To address these challenges, we propose a novel algorithm named NormProp, which utilizes the homophily assumption of unlabeled nodes to generate additional supervision signals, thereby enhancing the generalization against label scarcity. The key idea is to efficiently capture both the class information and the consistency of aggregation during message passing, via decoupling the direction and Euclidean norm of node representations. Moreover, we conduct a theoretical analysis to determine the upper bound of Euclidean norm, and then propose homophilous regularization to constraint the consistency of unlabeled nodes. Extensive experiments demonstrate that NormProp achieve state-of-the-art performance under low-label rate scenarios with low computational complexity. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨åŠç›‘ç£èŠ‚ç‚¹åˆ†ç±»ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„GNNsä¸¥é‡ä¾èµ–äºå¤§é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™æ—¢è€—è´¹äººåŠ›åˆéœ€è¦å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†ã€‚æœ¬æ–‡é¦–å…ˆåˆ†æäº†å°‘æ ·æœ¬åŠç›‘ç£èŠ‚ç‚¹åˆ†ç±»èƒŒæ™¯ä¸‹ï¼Œç›‘ç£ä¿¡å·å¯¹GNNsæ³›åŒ–çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºNormPropçš„æ–°å‹ç®—æ³•ï¼Œå®ƒåˆ©ç”¨æœªæ ‡è®°èŠ‚ç‚¹çš„åŒäº²å‡è®¾æ¥ç”Ÿæˆé¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œæé«˜å¯¹æ ‡ç­¾ç¨€ç¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚å…³é”®æ€æƒ³æ˜¯é€šè¿‡è§£è€¦èŠ‚ç‚¹è¡¨ç¤ºçš„æ–¹å‘å’Œæ¬§å‡ é‡Œå¾—èŒƒæ•°ï¼Œæœ‰æ•ˆåœ°æ•è·ç±»ä¿¡æ¯å’Œèšåˆè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œä»¥ç¡®å®šæ¬§å‡ é‡Œå¾—èŒƒæ•°çš„ä¸Šé™ï¼Œç„¶åæå‡ºåŒäº²æ­£åˆ™åŒ–æ¥çº¦æŸæœªæ ‡è®°èŠ‚ç‚¹çš„ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNormPropåœ¨ä½æ ‡ç­¾ç‡åœºæ™¯ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”è®¡ç®—å¤æ‚åº¦ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08581v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ†æäº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨å°‘æ ·æœ¬åŠç›‘ç£èŠ‚ç‚¹åˆ†ç±»ä¸­çš„æ³›åŒ–é™åˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºNormPropçš„æ–°å‹ç®—æ³•ã€‚è¯¥ç®—æ³•åˆ©ç”¨æœªæ ‡è®°èŠ‚ç‚¹çš„åŒè´¨æ€§å‡è®¾ç”Ÿæˆé¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œä»¥æé«˜å¯¹æ ‡ç­¾ç¨€ç¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è§£è€¦èŠ‚ç‚¹è¡¨ç¤ºçš„æ–¹å‘å’Œæ¬§å‡ é‡Œå¾—èŒƒæ•°ï¼Œæœ‰æ•ˆåœ°æ•è·ç±»ä¿¡æ¯å’Œèšåˆè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œè¿›è¡Œäº†ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œè¡¨æ˜NormPropåœ¨ä½æ ‡ç­¾ç‡åœºæ™¯ä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œä¸”è®¡ç®—å¤æ‚åº¦ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GNNsåœ¨å°‘æ ·æœ¬åŠç›‘ç£èŠ‚ç‚¹åˆ†ç±»ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®è®­ç»ƒï¼Œä½†æ ‡æ³¨æ•°æ®è·å–å›°éš¾ä¸”éœ€è¦å¤§é‡é¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>NormPropç®—æ³•åˆ©ç”¨æœªæ ‡è®°èŠ‚ç‚¹çš„åŒè´¨æ€§å‡è®¾ç”Ÿæˆé¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œä»¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>NormPropé€šè¿‡è§£è€¦èŠ‚ç‚¹è¡¨ç¤ºçš„æ–¹å‘å’Œæ¬§å‡ é‡Œå¾—èŒƒæ•°ï¼Œæ•è·ç±»ä¿¡æ¯å’Œèšåˆè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚</li>
<li>è¿›è¡Œäº†ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œç¡®å®šäº†æ¬§å‡ é‡Œå¾—èŒƒæ•°çš„ä¸Šé™ã€‚</li>
<li>NormPropæå‡ºäº†åŒè´¨æ€§æ­£åˆ™åŒ–æ¥çº¦æŸæœªæ ‡è®°èŠ‚ç‚¹çš„ä¸€è‡´æ€§ã€‚</li>
<li>NormPropåœ¨ä½æ ‡ç­¾ç‡åœºæ™¯ä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3cc2f22ae09bc589b75d29389cc2d301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9032b5182979a998a1440a30e6e40720.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53b8cd0b446aa9f2cadb4ee5b720b025.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3637d6df7ff87ad6cd8b7767011881b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2fce375eac960685d33bd596952b66b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Get-Rid-of-Isolation-A-Continuous-Multi-task-Spatio-Temporal-Learning-Framework"><a href="#Get-Rid-of-Isolation-A-Continuous-Multi-task-Spatio-Temporal-Learning-Framework" class="headerlink" title="Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning   Framework"></a>Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning   Framework</h2><p><strong>Authors:Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Yanjiang Chen, Liheng Yu, Xu Wang, Yang Wang</strong></p>
<p>Spatiotemporal learning has become a pivotal technique to enable urban intelligence. Traditional spatiotemporal models mostly focus on a specific task by assuming a same distribution between training and testing sets. However, given that urban systems are usually dynamic, multi-sourced with imbalanced data distributions, current specific task-specific models fail to generalize to new urban conditions and adapt to new domains without explicitly modeling interdependencies across various dimensions and types of urban data. To this end, we argue that there is an essential to propose a Continuous Multi-task Spatio-Temporal learning framework (CMuST) to empower collective urban intelligence, which reforms the urban spatiotemporal learning from single-domain to cooperatively multi-dimensional and multi-task learning. Specifically, CMuST proposes a new multi-dimensional spatiotemporal interaction network (MSTI) to allow cross-interactions between context and main observations as well as self-interactions within spatial and temporal aspects to be exposed, which is also the core for capturing task-level commonality and personalization. To ensure continuous task learning, a novel Rolling Adaptation training scheme (RoAda) is devised, which not only preserves task uniqueness by constructing data summarization-driven task prompts, but also harnesses correlated patterns among tasks by iterative model behavior modeling. We further establish a benchmark of three cities for multi-task spatiotemporal learning, and empirically demonstrate the superiority of CMuST via extensive evaluations on these datasets. The impressive improvements on both few-shot streaming data and new domain tasks against existing SOAT methods are achieved. Code is available at <a target="_blank" rel="noopener" href="https://github.com/DILab-USTCSZ/CMuST">https://github.com/DILab-USTCSZ/CMuST</a>. </p>
<blockquote>
<p>æ—¶ç©ºå­¦ä¹ å·²æˆä¸ºå®ç°åŸå¸‚æ™ºèƒ½åŒ–çš„å…³é”®æŠ€æœ¯ã€‚ä¼ ç»Ÿçš„æ—¶ç©ºæ¨¡å‹å¤§å¤šæ˜¯é€šè¿‡å‡è®¾è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„ç›¸åŒåˆ†å¸ƒæ¥ä¸“æ³¨äºç‰¹å®šä»»åŠ¡ã€‚ç„¶è€Œï¼Œé‰´äºåŸå¸‚ç³»ç»Ÿé€šå¸¸æ˜¯åŠ¨æ€çš„ã€å¤šæºå¤´çš„ä¸”æ•°æ®åˆ†å¸ƒä¸å‡è¡¡ï¼Œå½“å‰é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹æ— æ³•æ¨å¹¿åˆ°æ–°çš„åŸå¸‚æ¡ä»¶ï¼Œå¹¶ä¸”æ— æ³•é€‚åº”æ–°çš„é¢†åŸŸï¼Œè€Œæ²¡æœ‰æ˜ç¡®åœ°å¯¹åŸå¸‚æ•°æ®çš„å„ç§ç»´åº¦å’Œç±»å‹ä¹‹é—´çš„ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºéœ€è¦æå‡ºä¸€ç§è¿ç»­å¤šä»»åŠ¡æ—¶ç©ºå­¦ä¹ æ¡†æ¶ï¼ˆCMuSTï¼‰ï¼Œä»¥å®ç°é›†ä½“åŸå¸‚æ™ºèƒ½åŒ–ï¼Œè¯¥æ¡†æ¶å°†åŸå¸‚æ—¶ç©ºå­¦ä¹ ä»å•åŸŸæ”¹é©ä¸ºåˆä½œçš„å¤šç»´åº¦å¤šä»»åŠ¡å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼ŒCMuSTæå‡ºäº†ä¸€ç§æ–°çš„å¤šç»´æ—¶ç©ºäº¤äº’ç½‘ç»œï¼ˆMSTIï¼‰ï¼Œå…è®¸ä¸Šä¸‹æ–‡å’Œä¸»è¦è§‚å¯Ÿç»“æœä¹‹é—´çš„äº¤å‰äº¤äº’ä»¥åŠç©ºé—´å’Œæ—¶é—´å†…éƒ¨çš„è‡ªæˆ‘äº¤äº’ï¼Œè¿™ä¹Ÿæ˜¯æ•æ‰ä»»åŠ¡çº§å…±æ€§å’Œä¸ªæ€§åŒ–çš„æ ¸å¿ƒã€‚ä¸ºäº†ç¡®ä¿è¿ç»­ä»»åŠ¡å­¦ä¹ ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„æ»šåŠ¨é€‚åº”è®­ç»ƒæ–¹æ¡ˆï¼ˆRoAdaï¼‰ï¼Œå®ƒä¸ä»…é€šè¿‡æ„å»ºæ•°æ®æ‘˜è¦é©±åŠ¨çš„ä»»åŠ¡æç¤ºæ¥ä¿æŒä»»åŠ¡çš„ç‹¬ç‰¹æ€§ï¼Œè€Œä¸”è¿˜é€šè¿‡è¿­ä»£æ¨¡å‹è¡Œä¸ºå»ºæ¨¡æ¥åˆ©ç”¨ä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ¨¡å¼ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å»ºç«‹äº†ä¸‰ä¸ªåŸå¸‚çš„å¤šä»»åŠ¡æ—¶ç©ºå­¦ä¹ åŸºå‡†ï¼Œå¹¶é€šè¿‡åœ¨è¿™äº›æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œå®è¯è¯æ˜äº†CMuSTçš„ä¼˜è¶Šæ€§ã€‚ä¸ç°æœ‰çš„SOATæ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å°‘é‡æµå¼æ•°æ®å’Œæ–°ä»»åŠ¡åŸŸä¸Šçš„è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DILab-USTCSZ/CMuST%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DILab-USTCSZ/CMuSTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10524v2">PDF</a> Accepted by NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æ—¶ç©ºå­¦ä¹ å·²æˆä¸ºå®ç°åŸå¸‚æ™ºèƒ½åŒ–çš„å…³é”®æŠ€æœ¯ã€‚ä¼ ç»Ÿæ—¶ç©ºæ¨¡å‹å¤§å¤šä¸“æ³¨äºç‰¹å®šä»»åŠ¡ï¼Œå‡è®¾è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´å…·æœ‰ç›¸åŒçš„åˆ†å¸ƒã€‚ç„¶è€Œï¼Œç”±äºåŸå¸‚ç³»ç»Ÿçš„åŠ¨æ€æ€§å’Œå¤šæºæ€§ä»¥åŠæ•°æ®åˆ†å¸ƒçš„ä¸å¹³è¡¡æ€§ï¼Œå½“å‰é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹æ— æ³•æ³›åŒ–åˆ°æ–°çš„åŸå¸‚æ¡ä»¶å¹¶é€‚åº”æ–°é¢†åŸŸï¼Œæ²¡æœ‰æ˜¾å¼åœ°å¯¹åŸå¸‚æ•°æ®çš„å„ç§ç»´åº¦å’Œç±»å‹ä¹‹é—´çš„ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚å› æ­¤ï¼Œæå‡ºä¸€ç§è¿ç»­å¤šä»»åŠ¡æ—¶ç©ºå­¦ä¹ æ¡†æ¶ï¼ˆCMuSTï¼‰æ¥æ¨åŠ¨é›†ä½“åŸå¸‚æ™ºèƒ½åŒ–æ˜¯å¿…è¦çš„ï¼Œå®ƒå°†åŸå¸‚æ—¶ç©ºå­¦ä¹ ä»å•åŸŸæ”¹é©ä¸ºåˆä½œçš„å¤šç»´åº¦å¤šä»»åŠ¡å­¦ä¹ ã€‚ç‰¹åˆ«æ˜¯ï¼ŒCMuSTæå‡ºä¸€ç§æ–°çš„å¤šç»´åº¦æ—¶ç©ºäº¤äº’ç½‘ç»œï¼ˆMSTIï¼‰ï¼Œä»¥å…è®¸ä¸Šä¸‹æ–‡å’Œä¸»è§‚æµ‹ä¹‹é—´çš„äº¤å‰äº¤äº’ä»¥åŠç©ºé—´å’Œæ—¶é—´çš„è‡ªæˆ‘äº¤äº’ï¼Œè¿™ä¹Ÿæ˜¯æ•æ‰ä»»åŠ¡çº§åˆ«å…±æ€§åŠä¸ªæ€§åŒ–çš„æ ¸å¿ƒæ‰€åœ¨ã€‚ä¸ºäº†ç¡®ä¿è¿ç»­ä»»åŠ¡å­¦ä¹ ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„æ»šåŠ¨é€‚åº”è®­ç»ƒæ–¹æ¡ˆï¼ˆRoAdaï¼‰ï¼Œè¿™ä¸ä»…é€šè¿‡æ„å»ºæ•°æ®æ‘˜è¦é©±åŠ¨çš„ä»»åŠ¡æç¤ºæ¥ä¿æŒä»»åŠ¡çš„ç‹¬ç‰¹æ€§ï¼Œè€Œä¸”è¿˜é€šè¿‡è¿­ä»£æ¨¡å‹è¡Œä¸ºå»ºæ¨¡æ¥åˆ©ç”¨ä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ¨¡å¼ã€‚é€šè¿‡åœ¨åŸå¸‚å¤šä»»åŠ¡æ—¶ç©ºå­¦ä¹ çš„åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å®è¯æ¼”ç¤ºï¼ŒCMuSTçš„ä¼˜è¶Šæ€§å¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ã€‚å¯¹äºå°‘é‡æµå¼æ•°æ®å’Œæ–°çš„é¢†åŸŸä»»åŠ¡ï¼Œç›¸è¾ƒäºç°æœ‰çš„é¡¶å°–æ–¹æ³•ï¼ŒCMuSTå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ”¹è¿›ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/DILab-USTCSZ/CMuST%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/DILab-USTCSZ/CMuSTè®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚æ—¶ç©ºå­¦ä¹ å¯¹äºå®ç°åŸå¸‚æ™ºèƒ½åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ—¶ç©ºæ¨¡å‹éš¾ä»¥é€‚åº”åŸå¸‚ç³»ç»Ÿçš„åŠ¨æ€æ€§å’Œæ•°æ®åˆ†å¸ƒçš„ä¸å¹³è¡¡æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¿ç»­å¤šä»»åŠ¡æ—¶ç©ºå­¦ä¹ æ¡†æ¶ï¼ˆCMuSTï¼‰ï¼Œä»å•åŸŸæ”¹é©ä¸ºå¤šç»´åº¦å¤šä»»åŠ¡å­¦ä¹ ã€‚</li>
<li>CMuSTé€šè¿‡å¤šç»´åº¦æ—¶ç©ºäº¤äº’ç½‘ç»œï¼ˆMSTIï¼‰æ•æ‰ä»»åŠ¡çº§åˆ«å…±æ€§åŠä¸ªæ€§åŒ–ã€‚</li>
<li>æ»šåŠ¨é€‚åº”è®­ç»ƒæ–¹æ¡ˆï¼ˆRoAdaï¼‰èƒ½ç¡®ä¿è¿ç»­ä»»åŠ¡å­¦ä¹ å¹¶åˆ©ç”¨ä»»åŠ¡é—´çš„ç›¸å…³æ¨¡å¼ã€‚</li>
<li>CMuSTåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å°‘é‡æµå¼æ•°æ®å’Œæ–°çš„é¢†åŸŸä»»åŠ¡æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-915f6ddb7f7341ffa8c684a5a8ad6076.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5814b1cd87ab246943140964dab05d6b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RobustEMD-Domain-Robust-Matching-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#RobustEMD-Domain-Robust-Matching-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical   Image Segmentation"></a>RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical   Image Segmentation</h2><p><strong>Authors:Yazhou Zhu, Minxian Li, Qiaolin Ye, Shidong Wang, Tong Xin, Haofeng Zhang</strong></p>
<p>Few-shot medical image segmentation (FSMIS) aims to perform the limited annotated data learning in the medical image analysis scope. Despite the progress has been achieved, current FSMIS models are all trained and deployed on the same data domain, as is not consistent with the clinical reality that medical imaging data is always across different data domains (e.g. imaging modalities, institutions and equipment sequences). How to enhance the FSMIS models to generalize well across the different specific medical imaging domains? In this paper, we focus on the matching mechanism of the few-shot semantic segmentation models and introduce an Earth Moverâ€™s Distance (EMD) calculation based domain robust matching mechanism for the cross-domain scenario. Specifically, we formulate the EMD transportation process between the foreground support-query features, the texture structure aware weights generation method, which proposes to perform the sobel based image gradient calculation over the nodes, is introduced in the EMD matching flow to restrain the domain relevant nodes. Besides, the point set level distance measurement metric is introduced to calculated the cost for the transportation from support set nodes to query set nodes. To evaluate the performance of our model, we conduct experiments on three scenarios (i.e., cross-modal, cross-sequence and cross-institution), which includes eight medical datasets and involves three body regions, and the results demonstrate that our model achieves the SoTA performance against the compared models. </p>
<blockquote>
<p>å°‘é‡æ ‡æ³¨æ•°æ®çš„åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ—¨åœ¨å®ç°åŒ»ç–—å›¾åƒåˆ†æèŒƒå›´å†…çš„æœ‰é™æ ‡æ³¨æ•°æ®å­¦ä¹ ã€‚å°½ç®¡å·²ç»å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ç›®å‰çš„FSMISæ¨¡å‹éƒ½æ˜¯åœ¨åŒä¸€æ•°æ®åŸŸä¸­è¿›è¡Œè®­ç»ƒå’Œéƒ¨ç½²çš„ï¼Œè¿™ä¸åŒ»å­¦æˆåƒæ•°æ®æ€»æ˜¯è·¨ä¸åŒæ•°æ®åŸŸï¼ˆä¾‹å¦‚æˆåƒæ¨¡å¼ã€æœºæ„å’Œè®¾å¤‡åºåˆ—ï¼‰çš„ä¸´åºŠç°å®ä¸ä¸€è‡´ã€‚å¦‚ä½•å¢å¼ºFSMISæ¨¡å‹ï¼Œä½¿å…¶åœ¨ä¸åŒçš„ç‰¹å®šåŒ»å­¦æˆåƒåŸŸä¹‹é—´å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨å°‘é‡è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„åŒ¹é…æœºåˆ¶ï¼Œå¹¶å¼•å…¥ä¸€ç§åŸºäºåœ°çƒç§»åŠ¨è·ç¦»ï¼ˆEMDï¼‰è®¡ç®—çš„åŸŸç¨³å¥åŒ¹é…æœºåˆ¶ï¼Œç”¨äºè·¨åŸŸåœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ¶å®šäº†å‰æ™¯æ”¯æŒæŸ¥è¯¢ç‰¹å¾ä¹‹é—´çš„EMDä¼ è¾“è¿‡ç¨‹ï¼Œå¹¶ä»‹ç»äº†åŸºäºçº¹ç†ç»“æ„æ„ŸçŸ¥çš„æƒé‡ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å»ºè®®åœ¨èŠ‚ç‚¹ä¸Šæ‰§è¡ŒåŸºäºSobelçš„å›¾åƒæ¢¯åº¦è®¡ç®—ã€‚åœ¨EMDåŒ¹é…æµä¸­å¼•å…¥è¿™ç§æ–¹æ³•æ¥çº¦æŸä¸åŸŸç›¸å…³çš„èŠ‚ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ç‚¹é›†çº§è·ç¦»æµ‹é‡æŒ‡æ ‡ï¼Œç”¨äºè®¡ç®—ä»æ”¯æŒé›†èŠ‚ç‚¹åˆ°æŸ¥è¯¢é›†èŠ‚ç‚¹çš„è¿è¾“æˆæœ¬ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç§åœºæ™¯ï¼ˆå³è·¨æ¨¡æ€ã€è·¨åºåˆ—å’Œè·¨æœºæ„ï¼‰ä¸‹è¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬å…«ä¸ªåŒ»å­¦æ•°æ®é›†å’Œä¸‰ä¸ªèº«ä½“åŒºåŸŸï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹ä¸å¯¹æ¯”æ¨¡å‹ç›¸æ¯”è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01110v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºåœ°çƒç§»åŠ¨è·ç¦»ï¼ˆEMDï¼‰çš„è·¨åŸŸé²æ£’åŒ¹é…æœºåˆ¶è¢«å¼•å…¥åˆ°äº†å°‘æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ä¸­ï¼Œè§£å†³äº†æ¨¡å‹åœ¨ä¸åŒç‰¹å®šåŒ»å­¦æˆåƒåŸŸä¸­çš„æ³›åŒ–é—®é¢˜ã€‚è¯¥æœºåˆ¶é€šè¿‡åˆ¶å®šå‰æ™¯æ”¯æŒæŸ¥è¯¢ç‰¹å¾ä¹‹é—´çš„EMDè¿è¾“è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥åŸºäºSobelçš„å›¾åƒæ¢¯åº¦è®¡ç®—æ¥çº¦æŸåŸŸç›¸å…³èŠ‚ç‚¹ï¼ŒåŒæ—¶åœ¨ç‚¹é›†çº§åˆ«å¼•å…¥è·ç¦»æµ‹é‡æŒ‡æ ‡æ¥è®¡ç®—è¿è¾“æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è·¨æ¨¡æ€ã€è·¨åºåˆ—å’Œè·¨æœºæ„ç­‰ä¸‰ç§åœºæ™¯ä¸‹å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSMISæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸæœ‰é™æ ‡æ³¨æ•°æ®çš„å­¦ä¹ é—®é¢˜ã€‚</li>
<li>å½“å‰FSMISæ¨¡å‹å±€é™äºåŒä¸€æ•°æ®åŸŸï¼Œä½†åŒ»å­¦æˆåƒæ•°æ®é€šå¸¸æ¶‰åŠä¸åŒçš„æ•°æ®åŸŸã€‚</li>
<li>å¼•å…¥åŸºäºåœ°çƒç§»åŠ¨è·ç¦»ï¼ˆEMDï¼‰çš„è·¨åŸŸé²æ£’åŒ¹é…æœºåˆ¶ï¼Œä»¥è§£å†³æ¨¡å‹åœ¨ä¸åŒåŒ»å­¦æˆåƒåŸŸä¸­çš„æ³›åŒ–é—®é¢˜ã€‚</li>
<li>EMDåŒ¹é…æœºåˆ¶åŒ…æ‹¬å‰æ™¯æ”¯æŒæŸ¥è¯¢ç‰¹å¾ä¹‹é—´çš„EMDè¿è¾“è¿‡ç¨‹ï¼Œä»¥åŠåŸºäºSobelçš„å›¾åƒæ¢¯åº¦è®¡ç®—æ¥çº¦æŸåŸŸç›¸å…³èŠ‚ç‚¹ã€‚</li>
<li>ç‚¹é›†çº§åˆ«çš„è·ç¦»æµ‹é‡æŒ‡æ ‡è¢«ç”¨äºè®¡ç®—è¿è¾“æˆæœ¬ã€‚</li>
<li>æ¨¡å‹åœ¨è·¨æ¨¡æ€ã€è·¨åºåˆ—å’Œè·¨æœºæ„çš„ä¸‰ç§åœºæ™¯ä¸‹è¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ead464cd01cc60a5a1bc36b7226bc26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9694b36b6f2c838aa4e62edcc696614d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ca9d0e32e4b4033b35a3a671a4b8c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d79e93e1b52bcd63795303434c3a06e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PACE-Marrying-generalization-in-PArameter-efficient-fine-tuning-with-Consistency-rEgularization"><a href="#PACE-Marrying-generalization-in-PArameter-efficient-fine-tuning-with-Consistency-rEgularization" class="headerlink" title="PACE: Marrying generalization in PArameter-efficient fine-tuning with   Consistency rEgularization"></a>PACE: Marrying generalization in PArameter-efficient fine-tuning with   Consistency rEgularization</h2><p><strong>Authors:Yao Ni, Shan Zhang, Piotr Koniusz</strong></p>
<p>Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained transformers to downstream tasks. However, the optimization of tasks performance often comes at the cost of generalizability in fine-tuned models. To address this issue, we theoretically connect smaller weight gradient norms during training and larger datasets to the improvements in model generalization. Motivated by this connection, we propose reducing gradient norms for enhanced generalization and aligning fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Yet, naive alignment does not guarantee gradient reduction and can potentially cause gradient explosion, complicating efforts to manage gradients. To address such an issue, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization. We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations. Theoretical analysis shows that PACE not only implicitly regularizes gradients for enhanced generalization, but also implicitly aligns the fine-tuned and pre-trained models to retain knowledge. Experimental evidence supports our theories. PACE surpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC, few-shot learning, domain adaptation) showcasing its potential for resource-efficient fine-tuning. It also improves LoRA in text classification (GLUE) and mathematical reasoning (GSM-8K). The code is available at <a target="_blank" rel="noopener" href="https://github.com/MaxwellYaoNi/PACE">https://github.com/MaxwellYaoNi/PACE</a> </p>
<blockquote>
<p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰èƒ½å¤Ÿæœ‰æ•ˆåœ°ä½¿é¢„è®­ç»ƒè½¬æ¢å™¨é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä»»åŠ¡æ€§èƒ½çš„ä¼˜åŒ–å¾€å¾€ä»¥å¾®è°ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸ºä»£ä»·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šå°†è®­ç»ƒè¿‡ç¨‹ä¸­è¾ƒå°çš„æƒé‡æ¢¯åº¦èŒƒæ•°å’Œè¾ƒå¤§çš„æ•°æ®é›†ä¸æ¨¡å‹æ³›åŒ–æ”¹è¿›è”ç³»èµ·æ¥ã€‚å—æ­¤è”ç³»çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å‡å°æ¢¯åº¦èŒƒæ•°æ¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ä½¿å¾®è°ƒæ¨¡å‹ä¸é¢„è®­ç»ƒæ¨¡å‹å¯¹é½æ¥ä¿ç•™å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®ä¸­çš„çŸ¥è¯†ã€‚ç„¶è€Œï¼Œç®€å•çš„å¯¹é½å¹¶ä¸èƒ½ä¿è¯æ¢¯åº¦çš„å‡å°ï¼Œå¹¶ä¸”å¯èƒ½å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼Œä½¿æ¢¯åº¦çš„ç®¡ç†å˜å¾—å¤æ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PACEï¼Œå°†å‚æ•°é«˜æ•ˆçš„ç²¾ç»†è°ƒæ•´ä¸ä¸€è‡´æ€§æ­£åˆ™åŒ–ç›¸ç»“åˆã€‚æˆ‘ä»¬é€šè¿‡ä¹˜æ€§å™ªå£°æ‰°åŠ¨ä»é€‚é…å™¨å­¦ä¹ åˆ°çš„ç‰¹å¾ï¼Œå¹¶ç¡®ä¿å¾®è°ƒåçš„æ¨¡å‹åœ¨ä¸åŒæ‰°åŠ¨ä¸‹å¯¹åŒä¸€æ ·æœ¬ä¿æŒä¸€è‡´ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒPACEä¸ä»…éšå¼åœ°æ­£åˆ™åŒ–æ¢¯åº¦ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸”éšå¼åœ°å¯¹å¾®è°ƒæ¨¡å‹å’Œé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¯¹é½ä»¥ä¿ç•™çŸ¥è¯†ã€‚å®éªŒè¯æ®æ”¯æŒæˆ‘ä»¬çš„ç†è®ºã€‚PACEåœ¨è§†è§‰é€‚åº”ä»»åŠ¡ï¼ˆVTAB-1kã€FGVCã€å°æ ·æœ¬å­¦ä¹ å’ŒåŸŸé€‚åº”ï¼‰ä¸Šè¶…è¶Šäº†ç°æœ‰çš„PEFTæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶èµ„æºé«˜æ•ˆå¾®è°ƒçš„ä¼˜åŠ¿ã€‚å®ƒåœ¨æ–‡æœ¬åˆ†ç±»ï¼ˆGLUEï¼‰å’Œæ•°å­¦æ¨ç†ï¼ˆGSM-8Kï¼‰æ–¹é¢çš„LoRAä¹Ÿæœ‰æ‰€æ”¹è¿›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MaxwellYaoNi/PACE">https://github.com/MaxwellYaoNi/PACE</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17137v4">PDF</a> Accepted by NeurIPS 2024 as a spotlight</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†å¦‚ä½•é€šè¿‡å‚æ•°ä¼˜åŒ–å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–æŠ€æœ¯æ”¹è¿›é¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•PACEï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼ŒPACEåœ¨è§†è§‰é€‚åº”ä»»åŠ¡ï¼ˆVTAB-1kã€FGVCã€å°‘æ ·æœ¬å­¦ä¹ å’ŒåŸŸé€‚åº”ï¼‰ä»¥åŠæ–‡æœ¬åˆ†ç±»å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PEFTæœ‰åŠ©äºé€‚åº”é¢„è®­ç»ƒæ¨¡å‹åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œä½†å¯èƒ½ç‰ºç‰²æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¾ƒå°æƒé‡æ¢¯åº¦èŒƒæ•°å’Œå¤§æ•°æ®é›†ä¸æ¨¡å‹æ³›åŒ–æ”¹è¿›ä¹‹é—´å­˜åœ¨ç†è®ºè”ç³»ã€‚</li>
<li>ä¸ºäº†æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œæå‡ºäº†PACEæ–¹æ³•ï¼Œç»“åˆäº†å‚æ•°é«˜æ•ˆå¾®è°ƒä¸ä¸€è‡´æ€§æ­£åˆ™åŒ–ã€‚</li>
<li>PACEé€šè¿‡å¼•å…¥ç‰¹å¾æ‰°åŠ¨æ¥ç¡®ä¿æ¨¡å‹å¯¹åŒä¸€æ ·æœ¬åœ¨ä¸åŒæ‰°åŠ¨ä¸‹çš„ä¸€è‡´æ€§ã€‚</li>
<li>PACEä¸ä»…éšå¼åœ°æ­£åˆ™åŒ–æ¢¯åº¦ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸”éšå¼åœ°å¯¹å¾®è°ƒæ¨¡å‹å’Œé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¯¹é½ä»¥ä¿ç•™çŸ¥è¯†ã€‚</li>
<li>å®éªŒè¯æ®è¡¨æ˜ï¼ŒPACEåœ¨å¤šç§ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„PEFTæ–¹æ³•ï¼ŒåŒ…æ‹¬è§†è§‰é€‚åº”ã€æ–‡æœ¬åˆ†ç±»å’Œæ•°å­¦æ¨ç†ã€‚</li>
<li>PACEæ–¹æ³•çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7dee999cb3f746532cf08769018775eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d483b4955bc826cf15c7ef9982c7a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1252ba28b19890b47a99bcf91ef8bb2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CrossFi-A-Cross-Domain-Wi-Fi-Sensing-Framework-Based-on-Siamese-Network"><a href="#CrossFi-A-Cross-Domain-Wi-Fi-Sensing-Framework-Based-on-Siamese-Network" class="headerlink" title="CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network"></a>CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network</h2><p><strong>Authors:Zijian Zhao, Tingwei Chen, Zhijie Cai, Xiaoyang Li, Hang Li, Qimei Chen, Guangxu Zhu</strong></p>
<p>In recent years, Wi-Fi sensing has garnered significant attention due to its numerous benefits, such as privacy protection, low cost, and penetration ability. Extensive research has been conducted in this field, focusing on areas such as gesture recognition, people identification, and fall detection. However, many data-driven methods encounter challenges related to domain shift, where the model fails to perform well in environments different from the training data. One major factor contributing to this issue is the limited availability of Wi-Fi sensing datasets, which makes models learn excessive irrelevant information and over-fit to the training set. Unfortunately, collecting large-scale Wi-Fi sensing datasets across diverse scenarios is a challenging task. To address this problem, we propose CrossFi, a siamese network-based approach that excels in both in-domain scenario and cross-domain scenario, including few-shot, zero-shot scenarios, and even works in few-shot new-class scenario where testing set contains new categories. The core component of CrossFi is a sample-similarity calculation network called CSi-Net, which improves the structure of the siamese network by using an attention mechanism to capture similarity information, instead of simply calculating the distance or cosine similarity. Based on it, we develop an extra Weight-Net that can generate a template for each class, so that our CrossFi can work in different scenarios. Experimental results demonstrate that our CrossFi achieves state-of-the-art performance across various scenarios. In gesture recognition task, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72% in one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario, and 84.75% in one-shot new-class scenario. The code for our model is publicly available at <a target="_blank" rel="noopener" href="https://github.com/RS2002/CrossFi">https://github.com/RS2002/CrossFi</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒWi-Fiæ„ŸçŸ¥å› å…¶éšç§ä¿æŠ¤ã€ä½æˆæœ¬å’Œç©¿é€èƒ½åŠ›ç­‰è¯¸å¤šä¼˜åŠ¿è€Œå¤‡å—å…³æ³¨ã€‚åœ¨æ­¤é¢†åŸŸï¼Œå·²ç»è¿›è¡Œäº†å¤§é‡ç ”ç©¶ï¼Œä¸»è¦é›†ä¸­åœ¨æ‰‹åŠ¿è¯†åˆ«ã€äººå‘˜è¯†åˆ«å’Œè·Œå€’æ£€æµ‹ç­‰æ–¹é¢ã€‚ç„¶è€Œï¼Œè®¸å¤šæ•°æ®é©±åŠ¨çš„æ–¹æ³•åœ¨é¢ä¸´é¢†åŸŸåç§»æŒ‘æˆ˜æ—¶æ•ˆæœä¸ä½³ï¼Œå³åœ¨ä¸åŒäºè®­ç»ƒæ•°æ®çš„ç¯å¢ƒä¸­æ¨¡å‹è¡¨ç°ä¸ä½³ã€‚å¯¼è‡´æ­¤é—®é¢˜çš„ä¸€ä¸ªä¸»è¦å› ç´ æ˜¯Wi-Fiæ„ŸçŸ¥æ•°æ®é›†çš„å¯è·å–æ€§æœ‰é™ï¼Œè¿™ä½¿å¾—æ¨¡å‹å­¦ä¹ äº†è¿‡å¤šçš„æ— å…³ä¿¡æ¯å¹¶å¯¹è®­ç»ƒé›†è¿‡åº¦æ‹Ÿåˆã€‚ä¸å¹¸çš„æ˜¯ï¼Œåœ¨å¤šç§åœºæ™¯ä¸‹æ”¶é›†å¤§è§„æ¨¡çš„Wi-Fiæ„ŸçŸ¥æ•°æ®é›†æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CrossFiï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå­ªç”Ÿç½‘ç»œçš„æ–¹æ³•ï¼Œåœ¨åŸŸå†…åœºæ™¯å’Œè·¨åŸŸåœºæ™¯ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å°æ ·æœ¬æ¬¡ã€é›¶æ ·æœ¬åœºæ™¯ï¼Œç”šè‡³åœ¨æ–°ç±»åˆ«å°æ ·æœ¬æ¬¡åœºæ™¯ä¸­ä¹Ÿèƒ½å·¥ä½œï¼Œå…¶ä¸­æµ‹è¯•é›†åŒ…å«æ–°ç±»åˆ«ã€‚CrossFiçš„æ ¸å¿ƒç»„ä»¶æ˜¯ä¸€ä¸ªåä¸ºCSi-Netçš„æ ·æœ¬ç›¸ä¼¼æ€§è®¡ç®—ç½‘ç»œï¼Œå®ƒé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æ”¹è¿›äº†å­ªç”Ÿç½‘ç»œçš„æ¶æ„ï¼Œä»¥æ•è·ç›¸ä¼¼æ€§ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ç®€å•åœ°è®¡ç®—è·ç¦»æˆ–ä½™å¼¦ç›¸ä¼¼æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé¢å¤–çš„Weight-Netï¼Œå¯ä»¥ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆä¸€ä¸ªæ¨¡æ¿ï¼Œä½¿æˆ‘ä»¬çš„CrossFièƒ½å¤Ÿåœ¨ä¸åŒçš„åœºæ™¯ä¸­å·¥ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrossFiåœ¨å„ç§åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­ï¼ŒCrossFiåœ¨åŸŸå†…åœºæ™¯ä¸­çš„å‡†ç¡®ç‡ä¸º98.17%ï¼Œåœ¨å•æ ·æœ¬è·¨åŸŸåœºæ™¯ä¸­çš„å‡†ç¡®ç‡ä¸º91.72%ï¼Œåœ¨é›¶æ ·æœ¬è·¨åŸŸåœºæ™¯ä¸­çš„å‡†ç¡®ç‡ä¸º64.81%ï¼Œåœ¨æ–°ç±»åˆ«å•æ ·æœ¬åœºæ™¯ä¸­çš„å‡†ç¡®ç‡ä¸º84.75%ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/RS2002/CrossFi">https://github.com/RS2002/CrossFi</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10919v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Wi-Fiæ„ŸçŸ¥æŠ€æœ¯è¿‘å¹´æ¥å¤‡å—å…³æ³¨ï¼Œä½†æ•°æ®é©±åŠ¨çš„æ–¹æ³•é¢ä¸´åŸŸåç§»é—®é¢˜ã€‚æœ¬æ–‡æå‡ºCrossFiæ–¹æ³•ï¼ŒåŸºäºå­ªç”Ÿç½‘ç»œå’Œæ ·æœ¬ç›¸ä¼¼æ€§è®¡ç®—ç½‘ç»œCSi-Netï¼Œèƒ½åœ¨ä¸åŒåœºæ™¯ä¸‹å®ç°ä¼˜ç§€æ€§èƒ½ï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬ã€é›¶æ ·æœ¬åœºæ™¯å’Œæ–°ç±»åˆ«åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrossFiåœ¨å¤šç§åœºæ™¯ä¸‹è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Wi-Fiæ„ŸçŸ¥æŠ€æœ¯å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå…·æœ‰éšç§ä¿æŠ¤ã€ä½æˆæœ¬å’Œç©¿é€èƒ½åŠ›ç­‰ä¼˜ç‚¹ã€‚</li>
<li>æ•°æ®é©±åŠ¨çš„æ–¹æ³•åœ¨Wi-Fiæ„ŸçŸ¥é¢†åŸŸé¢ä¸´åŸŸåç§»é—®é¢˜ï¼Œæ¨¡å‹åœ¨ä¸åŒäºè®­ç»ƒæ•°æ®çš„ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>CrossFiæ–¹æ³•åŸºäºå­ªç”Ÿç½‘ç»œå’Œæ ·æœ¬ç›¸ä¼¼æ€§è®¡ç®—ç½‘ç»œCSi-Netï¼Œèƒ½æœ‰æ•ˆè§£å†³åŸŸåç§»é—®é¢˜ã€‚</li>
<li>CrossFiåœ¨å¤šç§åœºæ™¯ä¸‹è¡¨ç°ä¼˜ç§€ï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬ã€é›¶æ ·æœ¬åœºæ™¯å’Œæ–°ç±»åˆ«åœºæ™¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCrossFiåœ¨å§¿æ€è¯†åˆ«ä»»åŠ¡ä¸­è¾¾åˆ°98.17%çš„å‡†ç¡®ç‡ã€‚</li>
<li>CrossFiæ¨¡å‹çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1741e4f78ba5b3a19386f3a48135c28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dee5bee53a257511993acf520173529.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00f07cb65b2b8e09ef0aaac908b0fcbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77ef47c0f958d1e58a3dffdc81b59387.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HeadGAP-Few-Shot-3D-Head-Avatar-via-Generalizable-Gaussian-Priors"><a href="#HeadGAP-Few-Shot-3D-Head-Avatar-via-Generalizable-Gaussian-Priors" class="headerlink" title="HeadGAP: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors"></a>HeadGAP: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors</h2><p><strong>Authors:Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu</strong></p>
<p>In this paper, we present a novel 3D head avatar creation approach capable of generalizing from few-shot in-the-wild data with high-fidelity and animatable robustness. Given the underconstrained nature of this problem, incorporating prior knowledge is essential. Therefore, we propose a framework comprising prior learning and avatar creation phases. The prior learning phase leverages 3D head priors derived from a large-scale multi-view dynamic dataset, and the avatar creation phase applies these priors for few-shot personalization. Our approach effectively captures these priors by utilizing a Gaussian Splatting-based auto-decoder network with part-based dynamic modeling. Our method employs identity-shared encoding with personalized latent codes for individual identities to learn the attributes of Gaussian primitives. During the avatar creation phase, we achieve fast head avatar personalization by leveraging inversion and fine-tuning strategies. Extensive experiments demonstrate that our model effectively exploits head priors and successfully generalizes them to few-shot personalization, achieving photo-realistic rendering quality, multi-view consistency, and stable animation. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ä¸‰ç»´å¤´éƒ¨åŒ–èº«åˆ›å»ºæ–¹æ³•ï¼Œèƒ½å¤Ÿä»å°‘é‡é‡å¤–æ•°æ®ä¸­è¿›è¡Œé«˜åº¦ä¿çœŸå’Œå¯åŠ¨ç”»çš„ç¨³å¥æ€§æ¦‚æ‹¬ã€‚é‰´äºè¿™ä¸ªé—®é¢˜çš„çº¦æŸæ€§è¾ƒå°ï¼Œèå…¥å…ˆéªŒçŸ¥è¯†è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«å…ˆéªŒå­¦ä¹ é˜¶æ®µå’ŒåŒ–èº«åˆ›å»ºé˜¶æ®µçš„æ¡†æ¶ã€‚å…ˆéªŒå­¦ä¹ é˜¶æ®µåˆ©ç”¨å¤§è§„æ¨¡å¤šè§†è§’åŠ¨æ€æ•°æ®é›†è¡ç”Ÿçš„ä¸‰ç»´å¤´éƒ¨å…ˆéªŒçŸ¥è¯†ï¼Œè€ŒåŒ–èº«åˆ›å»ºé˜¶æ®µåˆ™åº”ç”¨è¿™äº›å…ˆéªŒçŸ¥è¯†æ¥è¿›è¡Œå°‘é‡ä¸ªæ€§åŒ–è®¾ç½®ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æ•æ‰è¿™äº›å…ˆéªŒçŸ¥è¯†ï¼Œé‡‡ç”¨åŸºäºé«˜æ–¯æ‹¼è´´æŠ€æœ¯çš„è‡ªåŠ¨è§£ç å™¨ç½‘ç»œè¿›è¡Œéƒ¨åˆ†åŠ¨æ€å»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å…·æœ‰ä¸ªæ€§åŒ–æ½œåœ¨ä»£ç çš„èº«ä»½å…±äº«ç¼–ç æ¥å­¦ä¹ é«˜æ–¯åŸºæœ¬å…ƒç´ çš„å±æ€§ã€‚åœ¨åŒ–èº«åˆ›å»ºé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨åè½¬å’Œå¾®è°ƒç­–ç•¥å®ç°äº†å¿«é€Ÿå¤´éƒ¨åŒ–èº«çš„ä¸ªæ€§åŒ–è®¾ç½®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æœ‰æ•ˆåœ°åˆ©ç”¨äº†å¤´éƒ¨å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶æˆåŠŸå°†å…¶æ¨å¹¿åˆ°å°‘é‡ä¸ªæ€§åŒ–è®¾ç½®ä¸Šï¼Œå®ç°äº†é€¼çœŸçš„æ¸²æŸ“è´¨é‡ã€å¤šè§†è§’ä¸€è‡´æ€§å’Œç¨³å®šçš„åŠ¨ç”»æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.06019v2">PDF</a> Accepted to 3DV 2025. Project page: <a target="_blank" rel="noopener" href="https://headgap.github.io/">https://headgap.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰ç»´å¤´éƒ¨è™šæ‹Ÿè§’è‰²åˆ›å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»å°è§„æ¨¡æ•°æ®ä¸­è¿›è¡Œæ³›åŒ–ï¼Œå¹¶å±•ç°å‡ºé«˜åº¦ä¿çœŸå’ŒåŠ¨ç”»é²æ£’æ€§ã€‚æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šå…ˆéªŒå­¦ä¹ é˜¶æ®µå’Œè§’è‰²åˆ›å»ºé˜¶æ®µã€‚å…ˆéªŒå­¦ä¹ é˜¶æ®µåˆ©ç”¨å¤§è§„æ¨¡å¤šè§†è§’åŠ¨æ€æ•°æ®é›†æå–çš„å¤´éƒ¨ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ï¼Œè§’è‰²åˆ›å»ºé˜¶æ®µåˆ™åº”ç”¨è¿™äº›å…ˆéªŒè¿›è¡Œä¸ªæ€§åŒ–åˆ›å»ºã€‚è¯¥æ–¹æ³•é€šè¿‡åŸºäºé«˜æ–¯æ‹¼è´´æŠ€æœ¯çš„è‡ªåŠ¨è§£ç å™¨ç½‘ç»œå’Œéƒ¨åˆ†åŠ¨æ€å»ºæ¨¡æŠ€æœ¯æœ‰æ•ˆæ•æ‰è¿™äº›å…ˆéªŒçŸ¥è¯†ã€‚æ–¹æ³•é‡‡ç”¨å…±äº«èº«ä»½ç¼–ç å’Œä¸ªæ€§åŒ–æ½œåœ¨ä»£ç å­¦ä¹ é«˜æ–¯åŸºæœ¬å±æ€§çš„èº«ä»½ç‰¹å¾ã€‚åœ¨è§’è‰²åˆ›å»ºé˜¶æ®µï¼Œé€šè¿‡åè½¬å’Œå¾®è°ƒç­–ç•¥å®ç°å¿«é€Ÿå¤´éƒ¨è§’è‰²ä¸ªæ€§åŒ–ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹æœ‰æ•ˆåœ°åˆ©ç”¨å¤´éƒ¨å…ˆéªŒçŸ¥è¯†ï¼ŒæˆåŠŸæ³›åŒ–åˆ°å°è§„æ¨¡ä¸ªæ€§åŒ–æ•°æ®ï¼Œè¾¾åˆ°é€¼çœŸçš„æ¸²æŸ“è´¨é‡ã€å¤šè§†è§’ä¸€è‡´æ€§å’Œç¨³å®šçš„åŠ¨ç”»æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ä¸‰ç»´å¤´éƒ¨è™šæ‹Ÿè§’è‰²åˆ›å»ºæ–¹æ³•ï¼Œèƒ½å¤Ÿä»å°‘é‡æ•°æ®ä¸­æ³›åŒ–å¹¶è¡¨ç°å‡ºé«˜åº¦ä¿çœŸå’ŒåŠ¨ç”»é²æ£’æ€§ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬å…ˆéªŒå­¦ä¹ é˜¶æ®µå’Œè§’è‰²åˆ›å»ºé˜¶æ®µï¼Œå‰è€…åˆ©ç”¨å¤§è§„æ¨¡å¤šè§†è§’åŠ¨æ€æ•°æ®é›†æå–å¤´éƒ¨ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>é‡‡ç”¨åŸºäºé«˜æ–¯æ‹¼è´´æŠ€æœ¯çš„è‡ªåŠ¨è§£ç å™¨ç½‘ç»œå’Œéƒ¨åˆ†åŠ¨æ€å»ºæ¨¡æŠ€æœ¯æ•æ‰å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>æ–¹æ³•é‡‡ç”¨å…±äº«èº«ä»½ç¼–ç å’Œä¸ªæ€§åŒ–æ½œåœ¨ä»£ç æ¥å­¦ä¹ é«˜æ–¯åŸºæœ¬å±æ€§çš„èº«ä»½ç‰¹å¾ã€‚</li>
<li>é€šè¿‡åè½¬å’Œå¾®è°ƒç­–ç•¥å®ç°å¿«é€Ÿå¤´éƒ¨è§’è‰²ä¸ªæ€§åŒ–ã€‚</li>
<li>æ¨¡å‹æœ‰æ•ˆæ³›åŒ–åˆ°å°è§„æ¨¡ä¸ªæ€§åŒ–æ•°æ®ï¼Œè¾¾åˆ°é€¼çœŸçš„æ¸²æŸ“è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.06019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d9fb2f2c50947c8a3957043eb8f75c59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4768d6c4ded301cca943516e0c82a477.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d311be7ef2485c6d182f6edcab5978b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1e0cdef0dcc76901e207d436b1ec963.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ba7795064ff602ff61c8717c10338cc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Foundation-Language-Image-Model-of-the-Retina-FLAIR-Encoding-Expert-Knowledge-in-Text-Supervision"><a href="#A-Foundation-Language-Image-Model-of-the-Retina-FLAIR-Encoding-Expert-Knowledge-in-Text-Supervision" class="headerlink" title="A Foundation Language-Image Model of the Retina (FLAIR): Encoding Expert   Knowledge in Text Supervision"></a>A Foundation Language-Image Model of the Retina (FLAIR): Encoding Expert   Knowledge in Text Supervision</h2><p><strong>Authors:Julio Silva-RodrÃ­guez, Hadi Chakor, Riadh Kobbi, Jose Dolz, Ismail Ben Ayed</strong></p>
<p>Foundation vision-language models are currently transforming computer vision, and are on the rise in medical imaging fueled by their very promising generalization capabilities. However, the initial attempts to transfer this new paradigm to medical imaging have shown less impressive performances than those observed in other domains, due to the significant domain shift and the complex, expert domain knowledge inherent to medical-imaging tasks. Motivated by the need for domain-expert foundation models, we present FLAIR, a pre-trained vision-language model for universal retinal fundus image understanding. To this end, we compiled 38 open-access, mostly categorical fundus imaging datasets from various sources, with up to 101 different target conditions and 288,307 images. We integrate the expertâ€™s domain knowledge in the form of descriptive textual prompts, during both pre-training and zero-shot inference, enhancing the less-informative categorical supervision of the data. Such a textual expertâ€™s knowledge, which we compiled from the relevant clinical literature and community standards, describes the fine-grained features of the pathologies as well as the hierarchies and dependencies between them. We report comprehensive evaluations, which illustrate the benefit of integrating expert knowledge and the strong generalization capabilities of FLAIR under difficult scenarios with domain shifts or unseen categories. When adapted with a lightweight linear probe, FLAIR outperforms fully-trained, dataset-focused models, more so in the few-shot regimes. Interestingly, FLAIR outperforms by a wide margin larger-scale generalist image-language models and retina domain-specific self-supervised networks, which emphasizes the potential of embedding expertsâ€™ domain knowledge and the limitations of generalist models in medical imaging. </p>
<blockquote>
<p>ç›®å‰ï¼ŒåŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹æ­£åœ¨æ”¹å˜è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œå¹¶ä¸”åœ¨åŒ»ç–—æˆåƒé¢†åŸŸå‘ˆç°å‡ºä¸Šå‡è¶‹åŠ¿ï¼Œè¿™å¾—ç›Šäºå…¶éå¸¸æœ‰å‰æ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™ä¸€æ–°èŒƒå¼åº”ç”¨äºåŒ»ç–—æˆåƒçš„åˆæ­¥å°è¯•ï¼Œç”±äºé¢†åŸŸè½¬ç§»å’ŒåŒ»ç–—æˆåƒä»»åŠ¡å›ºæœ‰çš„å¤æ‚ä¸“å®¶çŸ¥è¯†ï¼Œå…¶è¡¨ç°ä¸å¦‚å…¶ä»–é¢†åŸŸé‚£ä¹ˆä»¤äººå°è±¡æ·±åˆ»ã€‚ä¸ºäº†æ»¡è¶³å¯¹é¢†åŸŸä¸“å®¶åŸºç¡€æ¨¡å‹çš„éœ€æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†FLAIRï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé€šç”¨è§†ç½‘è†œçœ¼åº•å›¾åƒç†è§£çš„åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»å„ç§æ¥æºæ•´åˆäº†38ä¸ªå¼€æ”¾è®¿é—®çš„çœ¼åº•æˆåƒæ•°æ®é›†ï¼Œæ¶µç›–å¤šè¾¾101ç§ä¸åŒç›®æ ‡ç–¾ç—…å’Œ288,307å¼ å›¾åƒã€‚æˆ‘ä»¬ä»¥æè¿°æ€§æ–‡æœ¬æç¤ºçš„å½¢å¼æ•´åˆä¸“å®¶çš„é¢†åŸŸçŸ¥è¯†ï¼Œè¿™ç§çŸ¥è¯†æ˜¯æˆ‘ä»¬åœ¨ç›¸å…³ä¸´åºŠæ–‡çŒ®å’Œç¤¾åŒºæ ‡å‡†ä¸­ç¼–è¯‘çš„ï¼Œæè¿°äº†ç—…ç†çš„ç»†å¾®ç‰¹å¾ä»¥åŠå®ƒä»¬ä¹‹é—´çš„å±‚æ¬¡ç»“æ„å’Œä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬åœ¨é¢„è®­ç»ƒå’Œé›¶æ ·æœ¬æ¨ç†ä¸­éƒ½èå…¥äº†è¿™ç§æ–‡æœ¬ä¸“å®¶çŸ¥è¯†ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œå±•ç¤ºäº†èå…¥ä¸“å®¶çŸ¥è¯†çš„ç›Šå¤„ä»¥åŠFLAIRåœ¨é¢†åŸŸè½¬ç§»æˆ–æœªè§ç±»åˆ«ç­‰å›°éš¾åœºæ™¯ä¸‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚å½“ä½¿ç”¨è½»é‡çº§çº¿æ€§æ¢é’ˆè¿›è¡Œé€‚åº”æ—¶ï¼ŒFLAIRåœ¨å°‘æ•°ç±»åˆ«ä¸­çš„è¡¨ç°ä¼˜äºä¸“æ³¨äºæ•°æ®é›†çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†ç½‘è†œç–¾ç—…çš„è¯†åˆ«æ–¹é¢ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒFLAIRå¤§å¹…è¶…è¶Šäº†å¤§è§„æ¨¡é€šç”¨å›¾åƒè¯­è¨€æ¨¡å‹å’Œè§†ç½‘è†œé¢†åŸŸçš„ç‰¹å®šè‡ªç›‘ç£ç½‘ç»œï¼Œè¿™å¼ºè°ƒäº†åµŒå…¥ä¸“å®¶é¢†åŸŸçŸ¥è¯†çš„æ½œåŠ›ä»¥åŠé€šç”¨æ¨¡å‹åœ¨åŒ»ç–—æˆåƒä¸­çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07898v2">PDF</a> Accepted in Medical Image Analysis. The pre-trained model is   available at: <a target="_blank" rel="noopener" href="https://github.com/jusiro/FLAIR">https://github.com/jusiro/FLAIR</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºè·¨é¢†åŸŸè¿ç§»å­¦ä¹ çš„è§†è§’ï¼Œæ–‡ç« ä»‹ç»äº†é¢å‘è§†ç½‘è†œåŸºé‡‘å›¾åƒç†è§£çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹FLAIRã€‚ä¸ºæé«˜æ¨¡å‹åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„è¡¨ç°ï¼Œç ”ç©¶å›¢é˜Ÿæ•´åˆäº†ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ï¼Œå¹¶ä»¥æè¿°æ€§çš„æ–‡æœ¬æç¤ºå½¢å¼èå…¥æ¨¡å‹ï¼Œå¼ºåŒ–äº†æ•°æ®çš„ç±»åˆ«ç›‘ç£ä¿¡æ¯ã€‚é€šè¿‡å…¨é¢çš„è¯„ä¼°ï¼Œæ–‡ç« å±•ç¤ºäº†æ•´åˆä¸“å®¶çŸ¥è¯†çš„ä¼˜åŠ¿ä»¥åŠFLAIRåœ¨å›°éš¾åœºæ™¯ä¸‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚å½“ä½¿ç”¨è½»é‡çº§çº¿æ€§æ¢é’ˆè¿›è¡Œé€‚é…æ—¶ï¼ŒFLAIRåœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å¤§å¹…è¶…è¶Šå¤§è§„æ¨¡çš„ç»¼åˆå›¾åƒè¯­è¨€æ¨¡å‹å’Œè§†ç½‘è†œé¢†åŸŸçš„ç‰¹å®šè‡ªç›‘ç£ç½‘ç»œï¼Œçªæ˜¾äº†åµŒå…¥ä¸“å®¶é¢†åŸŸçŸ¥è¯†çš„æ½œåŠ›ä»¥åŠé€šç”¨æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­çš„å±€é™æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è·¨é¢†åŸŸè¿ç§»å­¦ä¹ å¯¹äºå°†è§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨äºåŒ»å­¦æˆåƒå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>åœ¨å¼€å‘é¢å‘è§†ç½‘è†œåŸºé‡‘å›¾åƒç†è§£çš„æ¨¡å‹FLAIRæ—¶ï¼Œç»“åˆäº†ä¸“å®¶é¢†åŸŸçŸ¥è¯†ï¼Œå¹¶ä»¥æ–‡æœ¬æç¤ºçš„å½¢å¼èå…¥æ¨¡å‹è®­ç»ƒä¸é›¶æ ·æœ¬æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>æ–‡æœ¬æç¤ºåŒ…å«ç²¾ç»†çš„ç—…ç†ç‰¹å¾æè¿°ä»¥åŠç—…ç†å±‚æ¬¡å’Œä¾èµ–å…³ç³»çš„æè¿°ã€‚</li>
<li>é€šè¿‡å…¨é¢çš„è¯„ä¼°è¯æ˜äº†æ•´åˆä¸“å®¶çŸ¥è¯†çš„ä¼˜åŠ¿ä»¥åŠFLAIRçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ï¼Œä½¿ç”¨è½»é‡çº§çº¿æ€§æ¢é’ˆé€‚é…çš„FLAIRè¡¨ç°çªå‡ºã€‚</li>
<li>ä¸å¤§è§„æ¨¡çš„ç»¼åˆå›¾åƒè¯­è¨€æ¨¡å‹å’Œè§†ç½‘è†œé¢†åŸŸçš„ç‰¹å®šè‡ªç›‘ç£ç½‘ç»œç›¸æ¯”ï¼ŒFLAIRå¤§å¹…é¢†å…ˆï¼Œçªæ˜¾äº†åµŒå…¥ä¸“å®¶é¢†åŸŸçŸ¥è¯†çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.07898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f946287d4ae8e9c1080599b4b1fa7890.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11108b356cf04b56dac1a242c6cdb461.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83ddd50e8a99d0ed044f6290934095be.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Controlling-Equational-Reasoning-in-Large-Language-Models-with-Prompt-Interventions"><a href="#Controlling-Equational-Reasoning-in-Large-Language-Models-with-Prompt-Interventions" class="headerlink" title="Controlling Equational Reasoning in Large Language Models with Prompt   Interventions"></a>Controlling Equational Reasoning in Large Language Models with Prompt   Interventions</h2><p><strong>Authors:Jordan Meadows, Marco Valentino, Andre Freitas</strong></p>
<p>This paper investigates how hallucination rates in Large Language Models (LLMs) may be controlled via a symbolic data generation framework, exploring a fundamental relationship between the rate of certain mathematical errors and types of input intervention. Specifically, we systematically generate data for a derivation generation task using a symbolic engine, applying targeted interventions to prompts to perturb features of mathematical derivations such as the surface forms of symbols, equational tree structures, and mathematical context. We then evaluate the effect of prompt interventions across a range of LLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Our experiments suggest that T5-Large can outperform the few-shot performance of GPT-4 on various evaluation sets generated via the framework. However, an extensive evaluation based on human analysis, template-based error detection, and text generation metrics reveals model weaknesses beyond what the reference-based metrics singularly describe. We use these results to tie characteristic distributional footprints of interventions to the human evaluation of LLM derivation quality, potentially leading to significant control over fine-grained mathematical capabilities of language models with respect to specific types of errors. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨å¦‚ä½•é€šè¿‡ç¬¦å·æ•°æ®ç”Ÿæˆæ¡†æ¶æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰ç‡ï¼Œæ¢ç´¢ç‰¹å®šæ•°å­¦é”™è¯¯ç‡ä¸è¾“å…¥å¹²é¢„ç±»å‹ä¹‹é—´çš„åŸºæœ¬å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¦å·å¼•æ“ä¸ºæ¨å¯¼ç”Ÿæˆä»»åŠ¡ç³»ç»Ÿåœ°ç”Ÿæˆæ•°æ®ï¼Œå¯¹æç¤ºè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¹²é¢„ï¼Œä»¥æ‰°åŠ¨æ•°å­¦æ¨å¯¼çš„ç‰¹å¾ï¼Œå¦‚ç¬¦å·çš„è¡¨é¢å½¢å¼ã€æ–¹ç¨‹å¼æ ‘ç»“æ„å’Œæ•°å­¦ä¸Šä¸‹æ–‡ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨ä¸€ç³»åˆ—LLMï¼ˆåŒ…æ‹¬å¾®è°ƒT5æ¨¡å‹ã€GPTå’ŒLLamaåŸºäºçš„æ¨¡å‹ï¼‰ä¸­è¯„ä¼°æç¤ºå¹²é¢„çš„æ•ˆæœã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒT5-Largeå¯ä»¥åœ¨å„ç§é€šè¿‡è¯¥æ¡†æ¶ç”Ÿæˆçš„è¯„ä¼°é›†ä¸Šå®ç°ä¼˜äºGPT-4çš„å°‘æ ·æœ¬è¡¨ç°ã€‚ç„¶è€Œï¼ŒåŸºäºäººç±»åˆ†æã€æ¨¡æ¿é”™è¯¯æ£€æµ‹å’Œæ–‡æœ¬ç”ŸæˆæŒ‡æ ‡çš„å¹¿æ³›è¯„ä¼°æ­ç¤ºäº†æ¨¡å‹åœ¨å•ä¸€åŸºå‡†æµ‹è¯•æŒ‡æ ‡ä¹‹å¤–å­˜åœ¨çš„å¼±ç‚¹ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›ç»“æœå°†å¹²é¢„çš„åˆ†å¸ƒç‰¹å¾ä¸LLMæ¨å¯¼è´¨é‡çš„äººç±»è¯„ä¼°è”ç³»èµ·æ¥ï¼Œè¿™å¯èƒ½ä¼šä¸ºé’ˆå¯¹ç‰¹å®šç±»å‹é”™è¯¯çš„æ§åˆ¶è¯­è¨€æ¨¡å‹çš„ç²¾ç»†æ•°å­¦èƒ½åŠ›æä¾›æ˜¾è‘—çš„æ§åˆ¶åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09998v5">PDF</a> AAAI 2025 (7 pages)</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨å¦‚ä½•é€šè¿‡ç¬¦å·æ•°æ®ç”Ÿæˆæ¡†æ¶æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰ç‡ï¼Œç ”ç©¶æ•°å­¦é”™è¯¯ç‡ä¸è¾“å…¥å¹²é¢„ç±»å‹ä¹‹é—´çš„åŸºæœ¬å…³ç³»ã€‚è®ºæ–‡é€šè¿‡ç¬¦å·å¼•æ“ç³»ç»Ÿåœ°ç”Ÿæˆæ•°æ®ï¼Œé’ˆå¯¹æ¨å¯¼ç”Ÿæˆä»»åŠ¡åº”ç”¨æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„æªæ–½ï¼Œæ‰°åŠ¨æ•°å­¦æ¨å¯¼çš„ç‰¹å¾ï¼Œå¦‚ç¬¦å·çš„è¡¨é¢å½¢å¼ã€æ–¹ç¨‹å¼æ ‘ç»“æ„å’Œæ•°å­¦ä¸Šä¸‹æ–‡ã€‚è¯„ä¼°äº†ä¸åŒLLMsåœ¨æç¤ºå¹²é¢„ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å¾®è°ƒT5æ¨¡å‹ã€GPTå’ŒLLaMaæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒT5-Largeåœ¨æŸäº›è¯„ä¼°é›†ä¸Šçš„è¡¨ç°ä¼˜äºGPT-4ã€‚ç„¶è€Œï¼ŒåŸºäºäººç±»åˆ†æã€æ¨¡æ¿åŒ–é”™è¯¯æ£€æµ‹å’Œæ–‡æœ¬ç”ŸæˆæŒ‡æ ‡çš„å…¨é¢è¯„ä¼°æ­ç¤ºäº†æ¨¡å‹é™¤äº†å‚è€ƒåŸºå‡†æŒ‡æ ‡æ‰€æè¿°çš„å¼±ç‚¹ä¹‹å¤–çš„å…¶ä»–å¼±ç‚¹ã€‚ç ”ç©¶ç»“æœå°†å¹²é¢„çš„åˆ†å¸ƒç‰¹å¾ä¸LLMæ¨å¯¼è´¨é‡çš„äººç±»è¯„ä¼°è”ç³»èµ·æ¥ï¼Œæœ‰æœ›å®ç°å¯¹è¯­è¨€æ¨¡å‹ç²¾ç»†æ•°å­¦èƒ½åŠ›çš„æ˜¾è‘—æ§åˆ¶ï¼Œå¹¶å‡å°‘ç‰¹å®šç±»å‹çš„é”™è¯¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æ¢ç´¢äº†å¦‚ä½•é€šè¿‡ç¬¦å·æ•°æ®ç”Ÿæˆæ¡†æ¶æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰ç‡ã€‚</li>
<li>ç ”ç©¶äº†æ•°å­¦é”™è¯¯ç‡ä¸è¾“å…¥å¹²é¢„ç±»å‹ä¹‹é—´çš„åŸºæœ¬å…³ç³»ã€‚</li>
<li>é€šè¿‡ç¬¦å·å¼•æ“ç³»ç»Ÿåœ°ç”Ÿæˆæ•°æ®ï¼Œå¹¶åº”ç”¨æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„æªæ–½æ¥æ‰°åŠ¨æ•°å­¦æ¨å¯¼çš„ç‰¹å¾ã€‚</li>
<li>è¯„ä¼°äº†ä¸åŒLLMsåœ¨æç¤ºå¹²é¢„ä¸‹çš„è¡¨ç°ã€‚</li>
<li>T5-Largeåœ¨æŸäº›è¯„ä¼°é›†ä¸Šçš„è¡¨ç°ä¼˜äºGPT-4ã€‚</li>
<li>å…¨é¢è¯„ä¼°æ­ç¤ºäº†æ¨¡å‹é™¤äº†å‚è€ƒåŸºå‡†æŒ‡æ ‡æ‰€æè¿°çš„å¼±ç‚¹ä¹‹å¤–çš„å…¶ä»–å¼±ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.09998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2359b75b2ee5e8f4ad4f33fcba9b16d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a30502526111a64dceb10edcc4688a15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42b73a19d8b3872ddbe3b8f95f5190e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e91f4cf9b699506343573d63f98eb7f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acd109d87dcdb77eb14ba15acaf7f7a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3c1ac4a53eefa6664e21702dae43ab4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dae1fb00d4461afb66decad012e72917.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  IDEA Image Description Enhanced CLIP-Adapter
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e2a8cc888dfff4e16af91c98a51243aa.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  Leveraging LLM Agents for Translating Network Configurations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">11370.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
