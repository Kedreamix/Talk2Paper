<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-01-17  Subject Disentanglement Neural Network for Speech Envelope   Reconstruction from EEG">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9dc0da43cbf30e7013e1c4f64e6acd90.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    44 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-17-更新"><a href="#2025-01-17-更新" class="headerlink" title="2025-01-17 更新"></a>2025-01-17 更新</h1><h2 id="Subject-Disentanglement-Neural-Network-for-Speech-Envelope-Reconstruction-from-EEG"><a href="#Subject-Disentanglement-Neural-Network-for-Speech-Envelope-Reconstruction-from-EEG" class="headerlink" title="Subject Disentanglement Neural Network for Speech Envelope   Reconstruction from EEG"></a>Subject Disentanglement Neural Network for Speech Envelope   Reconstruction from EEG</h2><p><strong>Authors:Li Zhang, Jiyao Liu</strong></p>
<p>Reconstructing speech envelopes from EEG signals is essential for exploring neural mechanisms underlying speech perception. Yet, EEG variability across subjects and physiological artifacts complicate accurate reconstruction. To address this problem, we introduce Subject Disentangling Neural Network (SDN-Net), which disentangles subject identity information from reconstructed speech envelopes to enhance cross-subject reconstruction accuracy. SDN-Net integrates three key components: MLA-Codec, MPN-MI, and CTA-MTDNN. The MLA-Codec, a fully convolutional neural network, decodes EEG signals into speech envelopes. The CTA-MTDNN module, a multi-scale time-delay neural network with channel and temporal attention, extracts subject identity features from EEG signals. Lastly, the MPN-MI module, a mutual information estimator with a multi-layer perceptron, supervises the removal of subject identity information from the reconstructed speech envelope. Experiments on the Auditory EEG Decoding Dataset demonstrate that SDN-Net achieves superior performance in inner- and cross-subject speech envelope reconstruction compared to recent state-of-the-art methods. </p>
<blockquote>
<p>从脑电图信号重建语音包络对于探索语音感知的神经网络机制至关重要。然而，受试者之间的脑电图变异和生理伪迹使准确重建变得复杂。为了解决这个问题，我们引入了主体分离神经网络（SDN-Net），该网络可以从重建的语音包络中分离出主体身份信息，以提高跨主体重建的准确性。SDN-Net集成了三个关键组件：MLA-Codec、MPN-MI和CTA-MTDNN。MLA-Codec是一个全卷积神经网络，可将脑电图信号解码为语音包络。CTA-MTDNN模块是一个具有通道和时间注意力的多尺度时延神经网络，从脑电图信号中提取主体身份特征。最后，MPN-MI模块是一个多层感知机的互信息估计器，负责监督从重建的语音包络中去除主体身份信息。在听觉脑电图解码数据集上的实验表明，与最新的先进方法相比，SDN-Net在内部和跨主体语音包络重建方面实现了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08693v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于脑电图信号重构语音包络对于探索语音感知的神经网络机制至关重要。为解决受试者间脑电图信号的差异和生理噪声对准确重构的影响，我们引入了主体分离神经网络（SDN-Net）。它通过从重构的语音包络中分离出主体身份信息，提高了跨主体重构的准确性。SDN-Net集成了MLA-Codec、MPN-MI和CTA-MTDNN三个关键组件。实验证明，在听觉脑电图解码数据集上，SDN-Net相较于最新的先进方法，在个体内和跨个体的语音包络重构性能上实现了优越的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音包络重构对探索语音感知的神经网络机制至关重要。</li>
<li>EEG信号的个体差异和生理噪声会影响语音包络重构的准确性。</li>
<li>引入主体分离神经网络（SDN-Net）以提高跨主体重构的准确性。</li>
<li>SDN-Net集成了MLA-Codec、MPN-MI和CTA-MTDNN三个关键组件。</li>
<li>MLA-Codec是一个全卷积神经网络，能将EEG信号解码为语音包络。</li>
<li>CTA-MTDNN模块从EEG信号中提取主体特征。</li>
<li>MPN-MI模块通过多层感知器估计互信息，监督从重构的语音包络中移除主体身份信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08693">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eea2cd4f05af2be1372f98d2772d6f5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43bf91e5285eafa6a6529468717e81ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0efc1ea1a2b380aae6a9a3602a06bfe5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22821fa86bc4a6208cc6695f3671ed94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-069bdfdcd45c11dde7f40bc894f52eb9.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-17\./crop_Speech/2501.08693v1/page_3_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adapting-Whisper-for-Regional-Dialects-Enhancing-Public-Services-for-Vulnerable-Populations-in-the-United-Kingdom"><a href="#Adapting-Whisper-for-Regional-Dialects-Enhancing-Public-Services-for-Vulnerable-Populations-in-the-United-Kingdom" class="headerlink" title="Adapting Whisper for Regional Dialects: Enhancing Public Services for   Vulnerable Populations in the United Kingdom"></a>Adapting Whisper for Regional Dialects: Enhancing Public Services for   Vulnerable Populations in the United Kingdom</h2><p><strong>Authors:Melissa Torgbi, Andrew Clayman, Jordan J. Speight, Harish Tayyar Madabushi</strong></p>
<p>We collect novel data in the public service domain to evaluate the capability of the state-of-the-art automatic speech recognition (ASR) models in capturing regional differences in accents in the United Kingdom (UK), specifically focusing on two accents from Scotland with distinct dialects. This study addresses real-world problems where biased ASR models can lead to miscommunication in public services, disadvantaging individuals with regional accents particularly those in vulnerable populations. We first examine the out-of-the-box performance of the Whisper large-v3 model on a baseline dataset and our data. We then explore the impact of fine-tuning Whisper on the performance in the two UK regions and investigate the effectiveness of existing model evaluation techniques for our real-world application through manual inspection of model errors. We observe that the Whisper model has a higher word error rate (WER) on our test datasets compared to the baseline data and fine-tuning on a given data improves performance on the test dataset with the same domain and accent. The fine-tuned models also appear to show improved performance when applied to the test data outside of the region it was trained on suggesting that fine-tuned models may be transferable within parts of the UK. Our manual analysis of model outputs reveals the benefits and drawbacks of using WER as an evaluation metric and fine-tuning to adapt to regional dialects. </p>
<blockquote>
<p>我们收集公共服务领域的新数据，以评估最新自动语音识别（ASR）模型捕捉英国区域口音差异的能力，特别是重点关注苏格兰两种带有明显方言的口音。本研究解决现实世界中的问题，即存在偏见的ASR模型可能导致公共服务中的沟通障碍，尤其对具有区域口音的个人以及弱势群体不利。我们首先检查Whisper large-v3模型在基准数据集和我们数据上的开箱性能。然后，我们探索对Whisper进行微调对这两个英国地区性能的影响，并通过手动检查模型错误，调查现有模型评估技术在我们现实应用中的有效性。我们发现，与基准数据相比，Whisper模型在我们的测试数据集上的单词错误率（WER）更高，针对给定数据的微调可以改善具有相同领域和口音的测试数据集的性能。经过微调的模型在应用于训练区域外的测试数据时似乎也表现出更好的性能，这表明经过微调的模型可能在英国的某些地区具有可转移性。我们对模型输出的手动分析揭示了使用WER作为评估指标的利弊，以及微调以适应区域方言的适应性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08502v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本研究收集公共服务领域的新数据，评估当前自动语音识别（ASR）模型捕捉英国区域口音差异的能力，重点关注苏格兰两种具有明显方言特点的口音。研究解决现实问题，即带有偏见的ASR模型可能导致公共服务中的沟通障碍，对具有区域口音的个人尤其是弱势群体造成不利影响。研究发现，在未进行微调的情况下，Whisper大型V3模型在基准数据集和本研究数据集上的表现欠佳，对特定口音的识别存在较高词错误率（WER）。通过对模型进行微调并手动检查模型错误，发现微调后的模型在同一领域和口音的测试数据集上的表现有所提升，并且在非训练区域的测试数据上也有较好的表现，这表明微调后的模型在英国内部具有一定的可迁移性。此外，本研究还对使用WER作为评估指标和通过微调适应方言的利弊进行了分析。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>本研究关注ASR模型在捕捉英国区域口音差异方面的能力，特别是苏格兰的两种方言。</li>
<li>发现未调校的Whisper大型V3模型在特定口音识别上存在较高词错误率（WER）。</li>
<li>通过对模型进行微调，可提高模型在特定领域和口音测试数据集上的表现。</li>
<li>微调后的模型在非训练区域的测试数据上也有较好的表现，具有一定的可迁移性。</li>
<li>手动分析揭示了使用WER作为评估指标的利弊。</li>
<li>研究强调了解决ASR模型偏见问题的必要性，以避免公共服务中的沟通障碍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08502">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9eb44968e98effb2a2f00353ff1dbeda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a19a114d1ba2cf7b9c5240512e60c13c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae6951d83f02a98a5143a7a7af419673.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-126e5c097e1aaaea5eb37cc264eac69a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3dbb4c7d466eed67f73a6570966c88b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb7a0b32c5aa007bf586eb937af4772f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c3a35e33fba4b0ebe6fdbd1bab82d9f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Selective-Attention-Merging-for-low-resource-tasks-A-case-study-of-Child-ASR"><a href="#Selective-Attention-Merging-for-low-resource-tasks-A-case-study-of-Child-ASR" class="headerlink" title="Selective Attention Merging for low resource tasks: A case study of   Child ASR"></a>Selective Attention Merging for low resource tasks: A case study of   Child ASR</h2><p><strong>Authors:Natarajan Balaji Shankar, Zilai Wang, Eray Eren, Abeer Alwan</strong></p>
<p>While Speech Foundation Models (SFMs) excel in various speech tasks, their performance for low-resource tasks such as child Automatic Speech Recognition (ASR) is hampered by limited pretraining data. To address this, we explore different model merging techniques to leverage knowledge from models trained on larger, more diverse speech corpora. This paper also introduces Selective Attention (SA) Merge, a novel method that selectively merges task vectors from attention matrices to enhance SFM performance on low-resource tasks. Experiments on the MyST database show significant reductions in relative word error rate of up to 14%, outperforming existing model merging and data augmentation techniques. By combining data augmentation techniques with SA Merge, we achieve a new state-of-the-art WER of 8.69 on the MyST database for the Whisper-small model, highlighting the potential of SA Merge for improving low-resource ASR. </p>
<blockquote>
<p>语音基础模型（SFMs）在各种语音任务中表现出色，但在儿童自动语音识别（ASR）等低资源任务的性能受到有限预训练数据的限制。为了解决这一问题，我们探索了不同的模型合并技术，以利用在更大、更多样化的语音语料库上训练的模型的知识。本文还介绍了选择性注意（SA）合并，这是一种新的方法，它选择性地合并来自注意力矩阵的任务向量，以提高SFM在低资源任务上的性能。在MyST数据库上的实验表明，相对词错误率最高降低了14%，超过了现有的模型合并和数据增强技术。通过将数据增强技术与SA Merge相结合，我们在MyST数据库上为Whisper-small模型达到了新的最先进的词错误率（WER）8.69%，这突显了SA Merge在提高低资源ASR方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08468v1">PDF</a> To appear in ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了如何利用模型合并技术来提升低资源任务（如儿童自动语音识别ASR）的语音基础模型性能问题。研究介绍了选择性注意力合并技术（Selective Attention Merge），它通过选择性合并注意力矩阵中的任务向量，提高在低资源任务上的表现。实验显示，该方法在MyST数据库上的相对词错误率降低幅度达14%，与其他模型合并和数据增强技术相比表现更优越。通过与数据增强技术结合，利用选择性注意力合并技术实现的whisper小型模型在MyST数据库上的词错误率降至8.69%，展示了其提升低资源ASR性能的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究针对低资源任务的语音基础模型性能问题，尤其是儿童自动语音识别ASR领域，性能受到限制，需要进行模型合并来优化利用大型和多样的预训练数据知识。</li>
<li>研究提出了一种新的模型合并方法——选择性注意力合并技术（Selective Attention Merge），通过选择性合并注意力矩阵中的任务向量，提高低资源任务的性能。</li>
<li>实验表明选择性注意力合并技术显著降低了相对词错误率（WER），在某些场景下提升效果显著，相对于现有模型合并和数据增强技术有更好的表现。</li>
<li>在MyST数据库上进行的实验表明，当选择性注意力合并技术与数据增强技术结合使用时，可以进一步降低词错误率至8.69%，展现出该技术在改善低资源ASR任务中的巨大潜力。</li>
<li>该研究为低资源环境下的语音任务提供了一个有效的解决方案，特别是针对缺乏大规模预训练数据的场景。</li>
<li>研究结果具有广泛的应用前景，特别是在儿童语音识别、辅助通讯技术以及多语言语音识别等领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-abefc04e38b247dd5594b5f2ee5e2059.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b86cd52a1feb0b91e34affd21fc6b622.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f9a7aae936b6d6d34cb328eff3f6b09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-437eb00167dda0fa809f56754783cf8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29938d18a004d4b52fd2c21f28a60366.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SemTalk-Holistic-Co-speech-Motion-Generation-with-Frame-level-Semantic-Emphasis"><a href="#SemTalk-Holistic-Co-speech-Motion-Generation-with-Frame-level-Semantic-Emphasis" class="headerlink" title="SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic   Emphasis"></a>SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic   Emphasis</h2><p><strong>Authors:Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Ziqiang Dang, Jianqiang Ren, Liefeng Bo, Zhigang Tu</strong></p>
<p>A good co-speech motion generation cannot be achieved without a careful integration of common rhythmic motion and rare yet essential semantic motion. In this work, we propose SemTalk for holistic co-speech motion generation with frame-level semantic emphasis. Our key insight is to separately learn general motions and sparse motions, and then adaptively fuse them. In particular, rhythmic consistency learning is explored to establish rhythm-related base motion, ensuring a coherent foundation that synchronizes gestures with the speech rhythm. Subsequently, textit{semantic emphasis learning is designed to generate semantic-aware sparse motion, focusing on frame-level semantic cues. Finally, to integrate sparse motion into the base motion and generate semantic-emphasized co-speech gestures, we further leverage a learned semantic score for adaptive synthesis. Qualitative and quantitative comparisons on two public datasets demonstrate that our method outperforms the state-of-the-art, delivering high-quality co-speech motion with enhanced semantic richness over a stable base motion. </p>
<blockquote>
<p>良好的协同语音动作生成离不开常见的节奏动作和罕见但必要的语义动作的仔细融合。在这项工作中，我们提出了用于整体协同语音动作生成的SemTalk方法，具有帧级语义强调。我们的关键见解是分别学习一般运动和稀疏运动，然后自适应地融合它们。特别是，探索了节奏一致性学习来建立与节奏相关的基本运动，确保与语音节奏同步的手势连贯基础。随后，设计语义重点学习来生成具有语义感知的稀疏运动，侧重于帧级语义线索。最后，为了将稀疏动作融入基本动作并生成具有语义强调的协同语音手势，我们进一步利用学习到的语义分数进行自适应合成。在两项公共数据集上的定性和定量比较表明，我们的方法优于最新技术，在稳定的基本动作上提供高质量的协同语音动作，增强了语义丰富性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16563v2">PDF</a> 11 pages, 8 figures</p>
<p><strong>摘要</strong></p>
<p>共同语音运动生成需要综合考虑常见的节奏运动和罕见但重要的语义运动。本研究提出SemTalk方法，用于整体共同语音运动生成，具有帧级语义重点。我们的关键见解是分别学习一般运动和稀疏运动，然后自适应融合它们。特别探索了节奏一致性学习，以建立与节奏相关的基本运动，确保姿势与语音节奏的同步性。此外，设计语义重点学习以生成具有语义意识的稀疏运动，侧重于帧级语义线索。最后，为了将稀疏运动融入基本运动并生成具有语义强调的共同语音姿势，我们进一步利用学习到的语义分数进行自适应合成。在公共数据集上的定性和定量比较表明，我们的方法优于现有技术，生成高质量的共同语音运动，具有稳定的基本运动和增强的语义丰富性。</p>
<p><strong>要点</strong></p>
<ol>
<li>共同语音运动生成需要整合节奏运动和语义运动。</li>
<li>提出SemTalk方法，整体进行共同语音运动生成，引入帧级语义重点。</li>
<li>分别学习一般运动和稀疏运动，然后自适应融合。</li>
<li>通过节奏一致性学习建立与语音节奏同步的基本运动。</li>
<li>语义重点学习用于生成具有帧级语义线索的语义感知稀疏运动。</li>
<li>利用学习到的语义分数将稀疏运动融入基本运动。</li>
<li>在公共数据集上的表现优于现有技术，生成高质量共同语音运动，增强语义丰富性并保持稳定的基本运动。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16563">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5fbda91557b447d0d843dcafa617bc8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ea2f7d8193e1f32cf6e1cb15de3fa79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dd2a2dfae00d7c17206de4de1469f07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dc0da43cbf30e7013e1c4f64e6acd90.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CSL-L2M-Controllable-Song-Level-Lyric-to-Melody-Generation-Based-on-Conditional-Transformer-with-Fine-Grained-Lyric-and-Musical-Controls"><a href="#CSL-L2M-Controllable-Song-Level-Lyric-to-Melody-Generation-Based-on-Conditional-Transformer-with-Fine-Grained-Lyric-and-Musical-Controls" class="headerlink" title="CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on   Conditional Transformer with Fine-Grained Lyric and Musical Controls"></a>CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on   Conditional Transformer with Fine-Grained Lyric and Musical Controls</h2><p><strong>Authors:Li Chai, Donglin Wang</strong></p>
<p>Lyric-to-melody generation is a highly challenging task in the field of AI music generation. Due to the difficulty of learning strict yet weak correlations between lyrics and melodies, previous methods have suffered from weak controllability, low-quality and poorly structured generation. To address these challenges, we propose CSL-L2M, a controllable song-level lyric-to-melody generation method based on an in-attention Transformer decoder with fine-grained lyric and musical controls, which is able to generate full-song melodies matched with the given lyrics and user-specified musical attributes. Specifically, we first introduce REMI-Aligned, a novel music representation that incorporates strict syllable- and sentence-level alignments between lyrics and melodies, facilitating precise alignment modeling. Subsequently, sentence-level semantic lyric embeddings independently extracted from a sentence-wise Transformer encoder are combined with word-level part-of-speech embeddings and syllable-level tone embeddings as fine-grained controls to enhance the controllability of lyrics over melody generation. Then we introduce human-labeled musical tags, sentence-level statistical musical attributes, and learned musical features extracted from a pre-trained VQ-VAE as coarse-grained, fine-grained and high-fidelity controls, respectively, to the generation process, thereby enabling user control over melody generation. Finally, an in-attention Transformer decoder technique is leveraged to exert fine-grained control over the full-song melody generation with the aforementioned lyric and musical conditions. Experimental results demonstrate that our proposed CSL-L2M outperforms the state-of-the-art models, generating melodies with higher quality, better controllability and enhanced structure. Demos and source code are available at <a target="_blank" rel="noopener" href="https://lichaiustc.github.io/CSL-L2M/">https://lichaiustc.github.io/CSL-L2M/</a>. </p>
<blockquote>
<p>歌词到旋律的生成是人工智能音乐生成领域中的一个极具挑战性的任务。由于学习歌词和旋律之间严格但微弱的关联存在困难，以前的方法在可控性、质量和结构方面存在生成质量不高的问题。为了解决这些挑战，我们提出了基于注意力缺失的Transformer解码器的可控歌曲级别的歌词到旋律生成方法CSL-L2M。它能够根据给定的歌词和用户指定的音乐属性生成全曲的旋律。具体来说，我们首先引入了REMI-Aligned这一新型音乐表示方法，它结合了歌词和旋律之间的音节和句子级别的严格对齐，便于精确对齐建模。接着，从句子级的Transformer编码器中独立提取的语义歌词嵌入与词级的词性嵌入和音节级的音调嵌入相结合，作为精细控制来增强歌词对旋律生成的操控性。然后，我们将人类标注的音乐标签、句子级的统计音乐属性以及从预训练的VQ-VAE中提取的学习到的音乐特征分别引入为粗粒度、细粒度和高保真控制，以用于生成过程，从而使用户能够控制旋律的生成。最后，利用注意力缺失的Transformer解码器技术，根据前述的歌词和音乐条件对全曲的旋律生成进行精细控制。实验结果表明，我们提出的CSL-L2M优于现有模型，生成的旋律质量更高、可控性更好、结构更加增强。Demo和源代码可在[<a target="_blank" rel="noopener" href="https://lichaiustc.github.io/CSL-L2M/]%E8%AE%BF%E9%97%AE%E3%80%82">https://lichaiustc.github.io/CSL-L2M/]访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09887v2">PDF</a> Accepted at AAAI-25</p>
<p><strong>Summary</strong></p>
<p>基于AI音乐生成领域中的歌词到旋律生成任务的高度挑战性，前人方法存在可控性弱、生成质量低和结构不佳等问题。为此，我们提出了基于注意力缺失Transformer解码器的可控歌词级别歌词到旋律生成方法CSL-L2M。它通过精细的歌词和音乐控制，能够生成与给定歌词和用户指定的音乐属性相匹配的完整歌曲旋律。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI音乐生成中的歌词到旋律生成具有挑战性，因歌词与旋律间的严格但弱关联难以学习，前人方法存在可控性、质量及结构问题。</li>
<li>CSL-L2M方法基于注意力缺失的Transformer解码器，实现了歌曲级别的歌词到旋律生成。</li>
<li>REMI-Aligned音乐表示方法融入了歌词和旋律之间的音节和句子级别严格对齐，便于精准对齐建模。</li>
<li>CSL-L2M使用了细粒度的控制，包括句子级别的语义歌词嵌入、词级别的词性嵌入和音节级别的音调嵌入，增强了旋律生成的可控性。</li>
<li>引入了人类标注的音乐标签、句子级别的统计音乐属性和从预训练VQ-VAE中提取的学习音乐特征，分别为粗粒度、细粒度和高保真控制，使用户能够控制旋律生成。</li>
<li>实验结果证明，CSL-L2M相较于现有最先进的模型，生成的旋律质量更高、可控性更强、结构更佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09887">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4a18afdad81aac00bedc97bdb3f8a1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a18f931806d805e42b7cdfee2e5114dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30ee0e17620e8745e0f687266cefc87d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7407e1d2e2194b685e782c1105429f00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94af1045a82426ef42984c272194167a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-547b5783b3778f87a353e56b12ebd8d0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MSA-ASR-Efficient-Multilingual-Speaker-Attribution-with-frozen-ASR-Models"><a href="#MSA-ASR-Efficient-Multilingual-Speaker-Attribution-with-frozen-ASR-Models" class="headerlink" title="MSA-ASR: Efficient Multilingual Speaker Attribution with frozen ASR   Models"></a>MSA-ASR: Efficient Multilingual Speaker Attribution with frozen ASR   Models</h2><p><strong>Authors:Thai-Binh Nguyen, Alexander Waibel</strong></p>
<p>Speaker-attributed automatic speech recognition (SA-ASR) aims to transcribe speech while assigning transcripts to the corresponding speakers accurately. Existing methods often rely on complex modular systems or require extensive fine-tuning of joint modules, limiting their adaptability and general efficiency. This paper introduces a novel approach, leveraging a frozen multilingual ASR model to incorporate speaker attribution into the transcriptions, using only standard monolingual ASR datasets. Our method involves training a speaker module to predict speaker embeddings based on weak labels without requiring additional ASR model modifications. Despite being trained exclusively with non-overlapping monolingual data, our approach effectively extracts speaker attributes across diverse multilingual datasets, including those with overlapping speech. Experimental results demonstrate competitive performance compared to strong baselines, highlighting the model’s robustness and potential for practical applications. </p>
<blockquote>
<p>说话人属性自动语音识别（SA-ASR）旨在准确地将语音转录并分配给相应的说话人。现有方法通常依赖于复杂的模块化系统，或者需要对联合模块进行大量的微调，这限制了它们的适应性和总体效率。本文介绍了一种新方法，利用冻结的多语言ASR模型来结合说话人属性进行转录，仅使用标准单语言ASR数据集。我们的方法是通过训练说话人模块来预测基于弱标签的说话人嵌入，而无需额外的ASR模型修改。尽管只使用非重叠的单语言数据进行训练，但我们的方法可以有效地提取跨多种多语言数据集的说话人属性，包括具有重叠语音的情况。实验结果与强基线相比具有竞争力，突显了模型的稳健性和实际应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18152v2">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong>：<br>本文介绍了自动语音识别（ASR）中的说话人属性识别技术。传统方法依赖于复杂的模块化系统，需要大量精细调整联合模块，限制了其适应性和效率。本文提出了一种新方法，利用冻结的多语言ASR模型将说话人属性纳入转录中，仅使用标准单语言ASR数据集。该方法通过训练说话人模块预测基于弱标签的说话人嵌入，无需修改额外的ASR模型。尽管仅在非重叠单语言数据上进行训练，但该方法在多种多语言数据集上有效地提取了说话人属性，包括重叠语音数据集。实验结果表明，与强基线相比具有竞争力，突显了模型的稳健性和实际应用潜力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>SA-ASR技术旨在准确地将语音转录并分配给相应的说话人。</li>
<li>传统方法依赖于复杂的模块化系统，需要精细调整，限制了适应性和效率。</li>
<li>本文提出一种利用冻结的多语言ASR模型的方法，通过结合说话人模块和预测说话人嵌入技术实现说话人属性识别。</li>
<li>方法仅使用标准单语言ASR数据集进行训练。</li>
<li>通过弱标签预测说话人嵌入，无需额外修改ASR模型。</li>
<li>即使在非重叠单语言数据上训练，该方法也能在多种多语言数据集上有效提取说话人属性，包括处理重叠语音的情况。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18152">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-011b9dd39a34c1e00367cda42039cafe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f80a986b5a1719cbedab7bef8bae134d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb49c7905b649d0b831b4fb27d071f29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-174e8b63f9b9b1ba30537a8bd4aa765c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Diffusion-based-Unsupervised-Audio-visual-Speech-Enhancement"><a href="#Diffusion-based-Unsupervised-Audio-visual-Speech-Enhancement" class="headerlink" title="Diffusion-based Unsupervised Audio-visual Speech Enhancement"></a>Diffusion-based Unsupervised Audio-visual Speech Enhancement</h2><p><strong>Authors:Jean-Eudes Ayilo, Mostafa Sadeghi, Romain Serizel, Xavier Alameda-Pineda</strong></p>
<p>This paper proposes a new unsupervised audio-visual speech enhancement (AVSE) approach that combines a diffusion-based audio-visual speech generative model with a non-negative matrix factorization (NMF) noise model. First, the diffusion model is pre-trained on clean speech conditioned on corresponding video data to simulate the speech generative distribution. This pre-trained model is then paired with the NMF-based noise model to estimate clean speech iteratively. Specifically, a diffusion-based posterior sampling approach is implemented within the reverse diffusion process, where after each iteration, a speech estimate is obtained and used to update the noise parameters. Experimental results confirm that the proposed AVSE approach not only outperforms its audio-only counterpart but also generalizes better than a recent supervised-generative AVSE method. Additionally, the new inference algorithm offers a better balance between inference speed and performance compared to the previous diffusion-based method. Code and demo available at: <a target="_blank" rel="noopener" href="https://jeaneudesayilo.github.io/fast_UdiffSE">https://jeaneudesayilo.github.io/fast_UdiffSE</a> </p>
<blockquote>
<p>本文提出了一种新的无监督音视频语音增强（AVSE）方法，该方法结合了基于扩散的音视频语音生成模型和非负矩阵分解（NMF）噪声模型。首先，扩散模型在对应的视频数据条件下对干净语音进行预训练，以模拟语音生成分布。然后，将此预训练模型与基于NMF的噪声模型配对，以迭代方式估计干净语音。具体来说，在反向扩散过程中实现了基于扩散的后验采样方法，每次迭代后，都会获得一个语音估计值，并用于更新噪声参数。实验结果表明，所提出的AVSE方法不仅优于仅使用音频的对应方法，而且相较于最新的有监督生成AVSE方法具有更好的泛化能力。此外，与之前的基于扩散的方法相比，新的推理算法在推理速度和性能之间提供了更好的平衡。相关代码和演示可在：[<a target="_blank" rel="noopener" href="https://jeaneudesayilo.github.io/fast_UdiffSE]%EF%BC%88https://jeaneudesayilo.github.io/fast_UdiffSE%EF%BC%89%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://jeaneudesayilo.github.io/fast_UdiffSE]（https://jeaneudesayilo.github.io/fast_UdiffSE）上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05301v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出了一种新的无监督音频视觉语音增强（AVSE）方法，该方法结合了基于扩散的音频视觉语音生成模型与非负矩阵分解（NMF）噪声模型。首先，扩散模型在对应的视频数据上进行预训练清洁语音，以模拟语音生成分布。然后，将此预训练模型与基于NMF的噪声模型结合，以迭代方式估计清洁语音。具体实现了一种基于扩散的后验采样方法，在反向扩散过程中，每次迭代后都会获得语音估计并用于更新噪声参数。实验结果表明，所提出的AVSE方法不仅优于仅使用音频的对应方法，而且比一般监督生成AVSE方法的泛化能力更强。此外，新的推理算法在推理速度与性能之间达到了更好的平衡，相较于之前的扩散方法有所改进。相关代码与演示见：<a target="_blank" rel="noopener" href="https://jeaneudesayilo.github.io/fast_UdiffSE">链接</a>。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>本文提出了一种新的无监督音频视觉语音增强（AVSE）方法，融合了扩散模型和NMF噪声模型。</li>
<li>扩散模型通过模拟语音生成分布进行预训练，以提高性能。</li>
<li>结合预训练扩散模型和NMF噪声模型，通过迭代方式估计清洁语音。</li>
<li>采用基于扩散的后验采样方法，在反向扩散过程中更新噪声参数。</li>
<li>实验结果显示，该方法优于仅使用音频的对应方法，且泛化能力更强。</li>
<li>新推理算法在推理速度与性能之间达到了平衡，改进了之前的扩散方法。</li>
<li>相关代码与演示可在指定链接中找到。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-00722825b119fb723bbc76f4fc542364.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a89db5e782b93e39a01af218099eabbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fa2b0b95d7e602376a308ab8b238d5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ce0f004af01f0f28a70a9051a6e598e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Joint-Beam-Search-Integrating-CTC-Attention-and-Transducer-Decoders"><a href="#Joint-Beam-Search-Integrating-CTC-Attention-and-Transducer-Decoders" class="headerlink" title="Joint Beam Search Integrating CTC, Attention, and Transducer Decoders"></a>Joint Beam Search Integrating CTC, Attention, and Transducer Decoders</h2><p><strong>Authors:Yui Sudo, Muhammad Shakeel, Yosuke Fukumoto, Brian Yan, Jiatong Shi, Yifan Peng, Shinji Watanabe</strong></p>
<p>End-to-end automatic speech recognition (E2E-ASR) can be classified by its decoder architectures, such as connectionist temporal classification (CTC), recurrent neural network transducer (RNN-T), attention-based encoder-decoder, and Mask-CTC models. Each decoder architecture has advantages and disadvantages, leading practitioners to switch between these different models depending on application requirements. Instead of building separate models, we propose a joint modeling scheme where four decoders (CTC, RNN-T, attention, and Mask-CTC) share the same encoder – we refer to this as 4D modeling. The 4D model is trained jointly, which will bring model regularization and maximize the model robustness thanks to their complementary properties. To efficiently train the 4D model, we introduce a two-stage training strategy that stabilizes the joint training. In addition, we propose three novel joint beam search algorithms by combining three decoders (CTC, RNN-T, and attention) to further improve performance. These three beam search algorithms differ in which decoder is used as the primary decoder. We carefully evaluate the performance and computational tradeoffs associated with each algorithm. Experimental results demonstrate that the jointly trained 4D model outperforms the E2E-ASR models trained with only one individual decoder. Furthermore, we demonstrate that the proposed joint beam search algorithm outperforms the previously proposed CTC&#x2F;attention decoding. </p>
<blockquote>
<p>端到端自动语音识别（E2E-ASR）可按照其解码器架构进行分类，如连接时序分类（CTC）、循环神经网络转换器（RNN-T）、基于注意力的编码器解码器以及Mask-CTC模型等。每种解码器架构都有其优缺点，实际应用中需要根据应用需求在这些不同模型之间进行切换。我们提出了一种联合建模方案，四种解码器（CTC、RNN-T、注意力以及Mask-CTC）共享同一个编码器，我们称之为4D建模。4D模型进行联合训练，这带来了模型正则化，并因它们的互补性质而最大化模型的稳健性。为了有效训练4D模型，我们引入了一种两阶段训练策略，使联合训练更加稳定。此外，我们通过结合三种解码器（CTC、RNN-T和注意力）提出了三种新型联合束搜索算法，以进一步提高性能。这三种束搜索算法的区别在于所使用的主要解码器不同。我们仔细评估了每种算法的性能和计算折衷。实验结果表明，联合训练的4D模型优于仅使用单个解码器训练的E2E-ASR模型。此外，我们证明所提出的联合束搜索算法优于先前提出的CTC&#x2F;注意力解码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02950v2">PDF</a> accepted to IEEE&#x2F;ACM Transactions on Audio Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>本文介绍了端对端语音识别（E2E-ASR）的四种解码器架构（CTC、RNN-T、注意力编码器解码器和Mask-CTC）的联合建模方案。该4D模型通过共享同一编码器实现四种解码器的联合训练，从而提高模型的正则化和鲁棒性。文章还提出了两阶段训练策略和三种联合束搜索算法以优化模型性能和计算效率。实验结果表明，联合训练的4D模型优于仅使用单个解码器的E2E-ASR模型，而提出的联合束搜索算法也优于先前的CTC&#x2F;注意力解码。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>端对端语音识别（E2E-ASR）的解码器架构包括CTC、RNN-T、注意力编码器解码器和Mask-CTC。</li>
<li>4D模型实现了四种解码器的联合建模，通过共享同一编码器提高模型的正则化和鲁棒性。</li>
<li>提出了两阶段训练策略以稳定联合训练。</li>
<li>提出了三种联合束搜索算法，通过结合不同解码器进一步优化性能。</li>
<li>实验结果表明，联合训练的4D模型在性能上优于单一解码器的E2E-ASR模型。</li>
<li>联合束搜索算法在性能上优于传统的CTC&#x2F;注意力解码。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02950">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5ede79bc4617b103570356b29b008889.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-399f3139a5f462eb0efebf67f14c4a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bafebe490620995d18b56424502c1ac.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Artificial-Intelligence-for-Cochlear-Implants-Review-of-Strategies-Challenges-and-Perspectives"><a href="#Artificial-Intelligence-for-Cochlear-Implants-Review-of-Strategies-Challenges-and-Perspectives" class="headerlink" title="Artificial Intelligence for Cochlear Implants: Review of Strategies,   Challenges, and Perspectives"></a>Artificial Intelligence for Cochlear Implants: Review of Strategies,   Challenges, and Perspectives</h2><p><strong>Authors:Billel Essaid, Hamza Kheddar, Noureddine Batel, Muhammad E. H. Chowdhury, Abderrahmane Lakas</strong></p>
<p>Automatic speech recognition (ASR) plays a pivotal role in our daily lives, offering utility not only for interacting with machines but also for facilitating communication for individuals with partial or profound hearing impairments. The process involves receiving the speech signal in analog form, followed by various signal processing algorithms to make it compatible with devices of limited capacities, such as cochlear implants (CIs). Unfortunately, these implants, equipped with a finite number of electrodes, often result in speech distortion during synthesis. Despite efforts by researchers to enhance received speech quality using various state-of-the-art (SOTA) signal processing techniques, challenges persist, especially in scenarios involving multiple sources of speech, environmental noise, and other adverse conditions. The advent of new artificial intelligence (AI) methods has ushered in cutting-edge strategies to address the limitations and difficulties associated with traditional signal processing techniques dedicated to CIs. This review aims to comprehensively cover advancements in CI-based ASR and speech enhancement, among other related aspects. The primary objective is to provide a thorough overview of metrics and datasets, exploring the capabilities of AI algorithms in this biomedical field, and summarizing and commenting on the best results obtained. Additionally, the review will delve into potential applications and suggest future directions to bridge existing research gaps in this domain. </p>
<blockquote>
<p>自动语音识别（ASR）在我们的日常生活中扮演着至关重要的角色，它不仅用于与机器交互，还帮助部分或重度听力受损人士进行日常沟通。这一过程包括接收模拟形式的语音信号，然后通过各种信号处理算法使其与有限容量的设备（如耳蜗植入装置）兼容。然而，这些植入物电极数量有限，经常在合成过程中导致语音失真。尽管研究人员尝试使用各种最先进的信号处理技术和人工智能（AI）方法提升接收到的语音质量，但仍存在挑战，特别是在涉及多语音源、环境噪声和其他不利条件的情况下。基于人工智能的新方法的出现为传统信号处理技术带来的局限性带来了前沿解决方案，特别是在耳蜗植入物方面。本综述旨在全面涵盖耳蜗植入物相关的ASR和语音增强的进展以及其他相关方面。主要目的是提供关于该生物医学领域中AI算法能力的指标和数据集的全面概述，并总结评论最佳结果。此外，本综述还将深入探讨潜在应用，并针对当前领域内的研究空白提出未来研究方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.15442v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着语音识别技术在日常生活中的普及，助听器植入物（CI）在改善听障人士的听力方面发挥着重要作用。然而，由于电极数量的限制，CI在合成过程中常常导致语音失真。近年来，人工智能方法的发展为解决这个问题提供了新的策略。本文旨在全面回顾CI相关的语音识别和语音增强技术的进展，探讨AI算法在这一生物医学领域的能力，并对最佳结果进行总结和评论，同时探讨潜在的应用和未来研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音识别（ASR）在日常生活中扮演着重要角色，不仅用于与机器交互，还为听障人士提供沟通便利。</li>
<li>语音信号接收后需经过一系列算法处理以适配有限容量的设备，如助听器植入物（CI）。</li>
<li>CI面临的主要挑战之一是电极数量有限导致的语音失真问题。</li>
<li>人工智能方法为解决传统信号处理技术面临的挑战提供了新的策略。</li>
<li>本文全面回顾了CI在ASR和语音增强方面的进展，包括评估指标和数据集。</li>
<li>AI算法在CI领域的应用能力得到了深入探讨。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.15442">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8a85166c755b334111d591119665fde1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae10fa70ebfcce03263ff2b8a0751d51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-968f255f8810d9bbaa5dc11f673abe7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a75111a4cc89f4e38b6d146408b26b64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56ae89393672b4328b90aab511a4c4d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce57bebef63269f6706480ac583bb949.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="On-the-Effectiveness-of-ASR-Representations-in-Real-world-Noisy-Speech-Emotion-Recognition"><a href="#On-the-Effectiveness-of-ASR-Representations-in-Real-world-Noisy-Speech-Emotion-Recognition" class="headerlink" title="On the Effectiveness of ASR Representations in Real-world Noisy Speech   Emotion Recognition"></a>On the Effectiveness of ASR Representations in Real-world Noisy Speech   Emotion Recognition</h2><p><strong>Authors:Xiaohan Shi, Jiajun He, Xingfeng Li, Tomoki Toda</strong></p>
<p>This paper proposes an efficient attempt to noisy speech emotion recognition (NSER). Conventional NSER approaches have proven effective in mitigating the impact of artificial noise sources, such as white Gaussian noise, but are limited to non-stationary noises in real-world environments due to their complexity and uncertainty. To overcome this limitation, we introduce a new method for NSER by adopting the automatic speech recognition (ASR) model as a noise-robust feature extractor to eliminate non-vocal information in noisy speech. We first obtain intermediate layer information from the ASR model as a feature representation for emotional speech and then apply this representation for the downstream NSER task. Our experimental results show that 1) the proposed method achieves better NSER performance compared with the conventional noise reduction method, 2) outperforms self-supervised learning approaches, and 3) even outperforms text-based approaches using ASR transcription or the ground truth transcription of noisy speech. </p>
<blockquote>
<p>本文提出了一种有效的针对带噪语音情感识别（NSER）的尝试。传统的NSER方法已经证明在减少人工噪声源（如白高斯噪声）的影响方面是有效的，但由于其复杂性和不确定性，它们在现实环境中的非平稳噪声方面存在局限性。为了克服这一局限性，我们采用了一种新的NSER方法，通过采用自动语音识别（ASR）模型作为噪声鲁棒性特征提取器，消除带噪语音中的非语音信息。我们首先获取ASR模型的中间层信息作为情感语音的特征表示，然后将其应用于下游NSER任务。实验结果表明，1）所提出的方法在NSER性能上优于传统的降噪方法；2）表现优于自监督学习方法；3）甚至优于基于文本的ASR转录或有噪语音的地面真实转录方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07093v3">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing</p>
<p><strong>摘要</strong><br>基于自动语音识别（ASR）模型的噪声鲁棒特征提取器方法，提出一种针对含噪语音情感识别的有效尝试。传统的含噪语音情感识别方法虽然能有效减轻人工噪声源（如白高斯噪声）的影响，但在现实环境中的非稳态噪声下表现受限。新方法采用ASR模型作为噪声鲁棒特征提取器，消除含噪语音中的非语音信息。首先，从ASR模型中获取中间层信息作为情感语音的特征表示，然后将其应用于下游含噪语音情感识别任务。实验结果表明，该方法相较于传统降噪方法和自监督学习方法有更好的性能表现，甚至超越了基于ASR转录或含噪语音的地面真实转录的文本方法。</p>
<p><strong>要点</strong></p>
<ul>
<li>该方法引入ASR模型作为噪声鲁棒特征提取器，用于消除含噪语音中的非语音信息。</li>
<li>通过获取ASR模型的中间层信息作为情感语音的特征表示，进而应用于下游含噪语音情感识别任务。</li>
<li>实验结果显示，该方法在含噪语音情感识别方面表现优异，相较于传统降噪方法和自监督学习方法有更好的性能。</li>
<li>该方法超越了基于文本的方法，如使用ASR转录或含噪语音的地面真实转录。</li>
<li>采用ASR模型可有效处理非稳态噪声下的含噪语音情感识别问题。</li>
<li>该方法提供了一种新的视角来解决现实环境中非稳态噪声对语音情感识别的影响。</li>
<li>采用自动语音识别技术提升了含噪语音情感识别的准确度和鲁棒性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.07093">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f4fd9385e001a224cc6eca73d18ac9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a07f193b773cc50ce5f17a4d50537a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b014953829ab8a8822f69b27a5e250c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-108776a24294e4e6b23e12a2b11bfee2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Quilt-1M-One-Million-Image-Text-Pairs-for-Histopathology"><a href="#Quilt-1M-One-Million-Image-Text-Pairs-for-Histopathology" class="headerlink" title="Quilt-1M: One Million Image-Text Pairs for Histopathology"></a>Quilt-1M: One Million Image-Text Pairs for Histopathology</h2><p><strong>Authors:Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, Linda Shapiro</strong></p>
<p>Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has slowed comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate QUILT: a large-scale vision-language dataset consisting of $802, 144$ image and text pairs. QUILT was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine QUILT with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it as the largest vision-language histopathology dataset to date. We demonstrate the value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model outperforms state-of-the-art models on both zero-shot and linear probing tasks for classifying new histopathology images across $13$ diverse patch-level datasets of $8$ different sub-pathologies and cross-modal retrieval tasks. </p>
<blockquote>
<p>近期多模态应用的加速发展得益于网络上大量图像和文本数据的可用性。然而，医学领域，特别是在组织病理学方面，类似数据的稀缺性减缓了相应的进展。为了在组织病理学中实现类似的表示学习，我们转向YouTube这一未被开发的视频资源宝库，从中获取了来自专家医生的1087小时宝贵的教育组织病理学视频。从YouTube上，我们精心策划了QUILT：一个大规模视觉语言数据集，包含802,144个图像和文本对。QUILT是使用多种模型自动策划的，包括大型语言模型、手工算法、人类知识数据库和自动语音识别。相比之下，为组织病理学策划的最全面的数据集仅积累了大约20万样本。我们将QUILT与其他来源的数据集相结合，包括Twitter、研究论文和互联网上的一般来源，创建了一个更大的数据集：QUILT-1M，包含100万个配对图像文本样本，成为迄今为止最大的视觉语言组织病理学数据集。我们通过微调预训练的CLIP模型展示了QUILT-1M的价值。我们的模型在零样本和线性探测任务上超越了最新模型，在跨越13个不同补丁级别的数据集中对新的组织病理学图像进行分类任务跨越八种不同的子病理，并且在实际操作中的跨模态检索任务中也表现优异。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.11207v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了由于在线图像和文本数据的丰富性，多模态应用领域的最新加速进展。然而，医疗领域尤其是组织病理学中类似数据的稀缺性限制了相应的进展。为实现在组织病理学中的类似表示学习，该文本利用YouTube这一未开发资源，从中获取了1,087小时专家医生的珍贵教育组织病理学视频。基于此，构建了QUILT大规模视觉语言数据集，包含802,144个图像和文本对。该数据集是通过大型语言模型、手工算法、人类知识数据库和自动语音识别等多种模型自动构建的。与迄今为止为组织病理学整理的最全面的数据集相比，QUILT的样本量高出数倍。通过将QUILT与其他来源的数据集（包括Twitter、研究论文和互联网上的数据集）相结合，创建了更大的QUILT-1M数据集，包含1百万配对图像文本样本，成为迄今为止最大的视觉语言组织病理学数据集。通过对预训练的CLIP模型进行微调，证明了QUILT-1M的价值，在零样本和线性探测任务中对新组织病理学图像进行分类时表现出卓越性能，跨越了13个不同的补丁级别数据集，涉及8种不同的子病理学，并在跨模态检索任务中也表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态应用领域因在线图像和文本数据的丰富性而加速发展，但医疗领域尤其是组织病理学中类似数据的稀缺性限制了进展。</li>
<li>利用YouTube等未开发资源，获取教育组织病理学视频，构建QUILT视觉语言数据集。</li>
<li>QUILT数据集包含大量的图像和文本对，是通过多种模型自动构建的。</li>
<li>QUILT数据集与其他来源的数据集相结合，创建了更大的QUILT-1M数据集，成为最大的视觉语言组织病理学数据集。</li>
<li>通过微调预训练的CLIP模型，QUILT-1M在分类新组织病理学图像和跨模态检索任务中表现出卓越性能。</li>
<li>QUILT-1M的价值在于其能够跨越多个不同的病理学领域，包括对各种子病理学的分类。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.11207">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2b25af500058f5848b8f399b19cf6696.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3779518524003dbca38776ef7ecdec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13b2f7ae8c8e1a651031c3e91fe72747.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dd482029fd251b6727c43cf452682ae.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-32e3d5d9fbf7bd93624b69b8595da5bf.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-01-17  DynamicFace High-Quality and Consistent Video Face Swapping using   Composable 3D Facial Priors
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d81aa40b9446febcf258da28f6d4530e.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-01-17  SCOT Self-Supervised Contrastive Pretraining For Zero-Shot   Compositional Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
