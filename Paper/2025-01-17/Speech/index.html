<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  Subject Disentanglement Neural Network for Speech Envelope   Reconstruction from EEG">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9dc0da43cbf30e7013e1c4f64e6acd90.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    44 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-17-æ›´æ–°"><a href="#2025-01-17-æ›´æ–°" class="headerlink" title="2025-01-17 æ›´æ–°"></a>2025-01-17 æ›´æ–°</h1><h2 id="Subject-Disentanglement-Neural-Network-for-Speech-Envelope-Reconstruction-from-EEG"><a href="#Subject-Disentanglement-Neural-Network-for-Speech-Envelope-Reconstruction-from-EEG" class="headerlink" title="Subject Disentanglement Neural Network for Speech Envelope   Reconstruction from EEG"></a>Subject Disentanglement Neural Network for Speech Envelope   Reconstruction from EEG</h2><p><strong>Authors:Li Zhang, Jiyao Liu</strong></p>
<p>Reconstructing speech envelopes from EEG signals is essential for exploring neural mechanisms underlying speech perception. Yet, EEG variability across subjects and physiological artifacts complicate accurate reconstruction. To address this problem, we introduce Subject Disentangling Neural Network (SDN-Net), which disentangles subject identity information from reconstructed speech envelopes to enhance cross-subject reconstruction accuracy. SDN-Net integrates three key components: MLA-Codec, MPN-MI, and CTA-MTDNN. The MLA-Codec, a fully convolutional neural network, decodes EEG signals into speech envelopes. The CTA-MTDNN module, a multi-scale time-delay neural network with channel and temporal attention, extracts subject identity features from EEG signals. Lastly, the MPN-MI module, a mutual information estimator with a multi-layer perceptron, supervises the removal of subject identity information from the reconstructed speech envelope. Experiments on the Auditory EEG Decoding Dataset demonstrate that SDN-Net achieves superior performance in inner- and cross-subject speech envelope reconstruction compared to recent state-of-the-art methods. </p>
<blockquote>
<p>ä»è„‘ç”µå›¾ä¿¡å·é‡å»ºè¯­éŸ³åŒ…ç»œå¯¹äºæ¢ç´¢è¯­éŸ³æ„ŸçŸ¥çš„ç¥ç»ç½‘ç»œæœºåˆ¶è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå—è¯•è€…ä¹‹é—´çš„è„‘ç”µå›¾å˜å¼‚å’Œç”Ÿç†ä¼ªè¿¹ä½¿å‡†ç¡®é‡å»ºå˜å¾—å¤æ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸»ä½“åˆ†ç¦»ç¥ç»ç½‘ç»œï¼ˆSDN-Netï¼‰ï¼Œè¯¥ç½‘ç»œå¯ä»¥ä»é‡å»ºçš„è¯­éŸ³åŒ…ç»œä¸­åˆ†ç¦»å‡ºä¸»ä½“èº«ä»½ä¿¡æ¯ï¼Œä»¥æé«˜è·¨ä¸»ä½“é‡å»ºçš„å‡†ç¡®æ€§ã€‚SDN-Neté›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šMLA-Codecã€MPN-MIå’ŒCTA-MTDNNã€‚MLA-Codecæ˜¯ä¸€ä¸ªå…¨å·ç§¯ç¥ç»ç½‘ç»œï¼Œå¯å°†è„‘ç”µå›¾ä¿¡å·è§£ç ä¸ºè¯­éŸ³åŒ…ç»œã€‚CTA-MTDNNæ¨¡å—æ˜¯ä¸€ä¸ªå…·æœ‰é€šé“å’Œæ—¶é—´æ³¨æ„åŠ›çš„å¤šå°ºåº¦æ—¶å»¶ç¥ç»ç½‘ç»œï¼Œä»è„‘ç”µå›¾ä¿¡å·ä¸­æå–ä¸»ä½“èº«ä»½ç‰¹å¾ã€‚æœ€åï¼ŒMPN-MIæ¨¡å—æ˜¯ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºçš„äº’ä¿¡æ¯ä¼°è®¡å™¨ï¼Œè´Ÿè´£ç›‘ç£ä»é‡å»ºçš„è¯­éŸ³åŒ…ç»œä¸­å»é™¤ä¸»ä½“èº«ä»½ä¿¡æ¯ã€‚åœ¨å¬è§‰è„‘ç”µå›¾è§£ç æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°çš„å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒSDN-Netåœ¨å†…éƒ¨å’Œè·¨ä¸»ä½“è¯­éŸ³åŒ…ç»œé‡å»ºæ–¹é¢å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08693v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè„‘ç”µå›¾ä¿¡å·é‡æ„è¯­éŸ³åŒ…ç»œå¯¹äºæ¢ç´¢è¯­éŸ³æ„ŸçŸ¥çš„ç¥ç»ç½‘ç»œæœºåˆ¶è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³å—è¯•è€…é—´è„‘ç”µå›¾ä¿¡å·çš„å·®å¼‚å’Œç”Ÿç†å™ªå£°å¯¹å‡†ç¡®é‡æ„çš„å½±å“ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸»ä½“åˆ†ç¦»ç¥ç»ç½‘ç»œï¼ˆSDN-Netï¼‰ã€‚å®ƒé€šè¿‡ä»é‡æ„çš„è¯­éŸ³åŒ…ç»œä¸­åˆ†ç¦»å‡ºä¸»ä½“èº«ä»½ä¿¡æ¯ï¼Œæé«˜äº†è·¨ä¸»ä½“é‡æ„çš„å‡†ç¡®æ€§ã€‚SDN-Neté›†æˆäº†MLA-Codecã€MPN-MIå’ŒCTA-MTDNNä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å¬è§‰è„‘ç”µå›¾è§£ç æ•°æ®é›†ä¸Šï¼ŒSDN-Netç›¸è¾ƒäºæœ€æ–°çš„å…ˆè¿›æ–¹æ³•ï¼Œåœ¨ä¸ªä½“å†…å’Œè·¨ä¸ªä½“çš„è¯­éŸ³åŒ…ç»œé‡æ„æ€§èƒ½ä¸Šå®ç°äº†ä¼˜è¶Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åŒ…ç»œé‡æ„å¯¹æ¢ç´¢è¯­éŸ³æ„ŸçŸ¥çš„ç¥ç»ç½‘ç»œæœºåˆ¶è‡³å…³é‡è¦ã€‚</li>
<li>EEGä¿¡å·çš„ä¸ªä½“å·®å¼‚å’Œç”Ÿç†å™ªå£°ä¼šå½±å“è¯­éŸ³åŒ…ç»œé‡æ„çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥ä¸»ä½“åˆ†ç¦»ç¥ç»ç½‘ç»œï¼ˆSDN-Netï¼‰ä»¥æé«˜è·¨ä¸»ä½“é‡æ„çš„å‡†ç¡®æ€§ã€‚</li>
<li>SDN-Neté›†æˆäº†MLA-Codecã€MPN-MIå’ŒCTA-MTDNNä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>MLA-Codecæ˜¯ä¸€ä¸ªå…¨å·ç§¯ç¥ç»ç½‘ç»œï¼Œèƒ½å°†EEGä¿¡å·è§£ç ä¸ºè¯­éŸ³åŒ…ç»œã€‚</li>
<li>CTA-MTDNNæ¨¡å—ä»EEGä¿¡å·ä¸­æå–ä¸»ä½“ç‰¹å¾ã€‚</li>
<li>MPN-MIæ¨¡å—é€šè¿‡å¤šå±‚æ„ŸçŸ¥å™¨ä¼°è®¡äº’ä¿¡æ¯ï¼Œç›‘ç£ä»é‡æ„çš„è¯­éŸ³åŒ…ç»œä¸­ç§»é™¤ä¸»ä½“èº«ä»½ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eea2cd4f05af2be1372f98d2772d6f5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43bf91e5285eafa6a6529468717e81ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0efc1ea1a2b380aae6a9a3602a06bfe5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22821fa86bc4a6208cc6695f3671ed94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-069bdfdcd45c11dde7f40bc894f52eb9.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-17\./crop_Speech/2501.08693v1/page_3_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adapting-Whisper-for-Regional-Dialects-Enhancing-Public-Services-for-Vulnerable-Populations-in-the-United-Kingdom"><a href="#Adapting-Whisper-for-Regional-Dialects-Enhancing-Public-Services-for-Vulnerable-Populations-in-the-United-Kingdom" class="headerlink" title="Adapting Whisper for Regional Dialects: Enhancing Public Services for   Vulnerable Populations in the United Kingdom"></a>Adapting Whisper for Regional Dialects: Enhancing Public Services for   Vulnerable Populations in the United Kingdom</h2><p><strong>Authors:Melissa Torgbi, Andrew Clayman, Jordan J. Speight, Harish Tayyar Madabushi</strong></p>
<p>We collect novel data in the public service domain to evaluate the capability of the state-of-the-art automatic speech recognition (ASR) models in capturing regional differences in accents in the United Kingdom (UK), specifically focusing on two accents from Scotland with distinct dialects. This study addresses real-world problems where biased ASR models can lead to miscommunication in public services, disadvantaging individuals with regional accents particularly those in vulnerable populations. We first examine the out-of-the-box performance of the Whisper large-v3 model on a baseline dataset and our data. We then explore the impact of fine-tuning Whisper on the performance in the two UK regions and investigate the effectiveness of existing model evaluation techniques for our real-world application through manual inspection of model errors. We observe that the Whisper model has a higher word error rate (WER) on our test datasets compared to the baseline data and fine-tuning on a given data improves performance on the test dataset with the same domain and accent. The fine-tuned models also appear to show improved performance when applied to the test data outside of the region it was trained on suggesting that fine-tuned models may be transferable within parts of the UK. Our manual analysis of model outputs reveals the benefits and drawbacks of using WER as an evaluation metric and fine-tuning to adapt to regional dialects. </p>
<blockquote>
<p>æˆ‘ä»¬æ”¶é›†å…¬å…±æœåŠ¡é¢†åŸŸçš„æ–°æ•°æ®ï¼Œä»¥è¯„ä¼°æœ€æ–°è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æ•æ‰è‹±å›½åŒºåŸŸå£éŸ³å·®å¼‚çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é‡ç‚¹å…³æ³¨è‹æ ¼å…°ä¸¤ç§å¸¦æœ‰æ˜æ˜¾æ–¹è¨€çš„å£éŸ³ã€‚æœ¬ç ”ç©¶è§£å†³ç°å®ä¸–ç•Œä¸­çš„é—®é¢˜ï¼Œå³å­˜åœ¨åè§çš„ASRæ¨¡å‹å¯èƒ½å¯¼è‡´å…¬å…±æœåŠ¡ä¸­çš„æ²Ÿé€šéšœç¢ï¼Œå°¤å…¶å¯¹å…·æœ‰åŒºåŸŸå£éŸ³çš„ä¸ªäººä»¥åŠå¼±åŠ¿ç¾¤ä½“ä¸åˆ©ã€‚æˆ‘ä»¬é¦–å…ˆæ£€æŸ¥Whisper large-v3æ¨¡å‹åœ¨åŸºå‡†æ•°æ®é›†å’Œæˆ‘ä»¬æ•°æ®ä¸Šçš„å¼€ç®±æ€§èƒ½ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¢ç´¢å¯¹Whisperè¿›è¡Œå¾®è°ƒå¯¹è¿™ä¸¤ä¸ªè‹±å›½åœ°åŒºæ€§èƒ½çš„å½±å“ï¼Œå¹¶é€šè¿‡æ‰‹åŠ¨æ£€æŸ¥æ¨¡å‹é”™è¯¯ï¼Œè°ƒæŸ¥ç°æœ‰æ¨¡å‹è¯„ä¼°æŠ€æœ¯åœ¨æˆ‘ä»¬ç°å®åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸åŸºå‡†æ•°æ®ç›¸æ¯”ï¼ŒWhisperæ¨¡å‹åœ¨æˆ‘ä»¬çš„æµ‹è¯•æ•°æ®é›†ä¸Šçš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ›´é«˜ï¼Œé’ˆå¯¹ç»™å®šæ•°æ®çš„å¾®è°ƒå¯ä»¥æ”¹å–„å…·æœ‰ç›¸åŒé¢†åŸŸå’Œå£éŸ³çš„æµ‹è¯•æ•°æ®é›†çš„æ€§èƒ½ã€‚ç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨åº”ç”¨äºè®­ç»ƒåŒºåŸŸå¤–çš„æµ‹è¯•æ•°æ®æ—¶ä¼¼ä¹ä¹Ÿè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜ç»è¿‡å¾®è°ƒçš„æ¨¡å‹å¯èƒ½åœ¨è‹±å›½çš„æŸäº›åœ°åŒºå…·æœ‰å¯è½¬ç§»æ€§ã€‚æˆ‘ä»¬å¯¹æ¨¡å‹è¾“å‡ºçš„æ‰‹åŠ¨åˆ†ææ­ç¤ºäº†ä½¿ç”¨WERä½œä¸ºè¯„ä¼°æŒ‡æ ‡çš„åˆ©å¼Šï¼Œä»¥åŠå¾®è°ƒä»¥é€‚åº”åŒºåŸŸæ–¹è¨€çš„é€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08502v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶æ”¶é›†å…¬å…±æœåŠ¡é¢†åŸŸçš„æ–°æ•°æ®ï¼Œè¯„ä¼°å½“å‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æ•æ‰è‹±å›½åŒºåŸŸå£éŸ³å·®å¼‚çš„èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨è‹æ ¼å…°ä¸¤ç§å…·æœ‰æ˜æ˜¾æ–¹è¨€ç‰¹ç‚¹çš„å£éŸ³ã€‚ç ”ç©¶è§£å†³ç°å®é—®é¢˜ï¼Œå³å¸¦æœ‰åè§çš„ASRæ¨¡å‹å¯èƒ½å¯¼è‡´å…¬å…±æœåŠ¡ä¸­çš„æ²Ÿé€šéšœç¢ï¼Œå¯¹å…·æœ‰åŒºåŸŸå£éŸ³çš„ä¸ªäººå°¤å…¶æ˜¯å¼±åŠ¿ç¾¤ä½“é€ æˆä¸åˆ©å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æœªè¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒWhisperå¤§å‹V3æ¨¡å‹åœ¨åŸºå‡†æ•°æ®é›†å’Œæœ¬ç ”ç©¶æ•°æ®é›†ä¸Šçš„è¡¨ç°æ¬ ä½³ï¼Œå¯¹ç‰¹å®šå£éŸ³çš„è¯†åˆ«å­˜åœ¨è¾ƒé«˜è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚é€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶æ‰‹åŠ¨æ£€æŸ¥æ¨¡å‹é”™è¯¯ï¼Œå‘ç°å¾®è°ƒåçš„æ¨¡å‹åœ¨åŒä¸€é¢†åŸŸå’Œå£éŸ³çš„æµ‹è¯•æ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰æ‰€æå‡ï¼Œå¹¶ä¸”åœ¨éè®­ç»ƒåŒºåŸŸçš„æµ‹è¯•æ•°æ®ä¸Šä¹Ÿæœ‰è¾ƒå¥½çš„è¡¨ç°ï¼Œè¿™è¡¨æ˜å¾®è°ƒåçš„æ¨¡å‹åœ¨è‹±å›½å†…éƒ¨å…·æœ‰ä¸€å®šçš„å¯è¿ç§»æ€§ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å¯¹ä½¿ç”¨WERä½œä¸ºè¯„ä¼°æŒ‡æ ‡å’Œé€šè¿‡å¾®è°ƒé€‚åº”æ–¹è¨€çš„åˆ©å¼Šè¿›è¡Œäº†åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬ç ”ç©¶å…³æ³¨ASRæ¨¡å‹åœ¨æ•æ‰è‹±å›½åŒºåŸŸå£éŸ³å·®å¼‚æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯è‹æ ¼å…°çš„ä¸¤ç§æ–¹è¨€ã€‚</li>
<li>å‘ç°æœªè°ƒæ ¡çš„Whisperå¤§å‹V3æ¨¡å‹åœ¨ç‰¹å®šå£éŸ³è¯†åˆ«ä¸Šå­˜åœ¨è¾ƒé«˜è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>é€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯æé«˜æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸå’Œå£éŸ³æµ‹è¯•æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
<li>å¾®è°ƒåçš„æ¨¡å‹åœ¨éè®­ç»ƒåŒºåŸŸçš„æµ‹è¯•æ•°æ®ä¸Šä¹Ÿæœ‰è¾ƒå¥½çš„è¡¨ç°ï¼Œå…·æœ‰ä¸€å®šçš„å¯è¿ç§»æ€§ã€‚</li>
<li>æ‰‹åŠ¨åˆ†ææ­ç¤ºäº†ä½¿ç”¨WERä½œä¸ºè¯„ä¼°æŒ‡æ ‡çš„åˆ©å¼Šã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†è§£å†³ASRæ¨¡å‹åè§é—®é¢˜çš„å¿…è¦æ€§ï¼Œä»¥é¿å…å…¬å…±æœåŠ¡ä¸­çš„æ²Ÿé€šéšœç¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9eb44968e98effb2a2f00353ff1dbeda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a19a114d1ba2cf7b9c5240512e60c13c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae6951d83f02a98a5143a7a7af419673.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-126e5c097e1aaaea5eb37cc264eac69a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3dbb4c7d466eed67f73a6570966c88b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb7a0b32c5aa007bf586eb937af4772f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c3a35e33fba4b0ebe6fdbd1bab82d9f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Selective-Attention-Merging-for-low-resource-tasks-A-case-study-of-Child-ASR"><a href="#Selective-Attention-Merging-for-low-resource-tasks-A-case-study-of-Child-ASR" class="headerlink" title="Selective Attention Merging for low resource tasks: A case study of   Child ASR"></a>Selective Attention Merging for low resource tasks: A case study of   Child ASR</h2><p><strong>Authors:Natarajan Balaji Shankar, Zilai Wang, Eray Eren, Abeer Alwan</strong></p>
<p>While Speech Foundation Models (SFMs) excel in various speech tasks, their performance for low-resource tasks such as child Automatic Speech Recognition (ASR) is hampered by limited pretraining data. To address this, we explore different model merging techniques to leverage knowledge from models trained on larger, more diverse speech corpora. This paper also introduces Selective Attention (SA) Merge, a novel method that selectively merges task vectors from attention matrices to enhance SFM performance on low-resource tasks. Experiments on the MyST database show significant reductions in relative word error rate of up to 14%, outperforming existing model merging and data augmentation techniques. By combining data augmentation techniques with SA Merge, we achieve a new state-of-the-art WER of 8.69 on the MyST database for the Whisper-small model, highlighting the potential of SA Merge for improving low-resource ASR. </p>
<blockquote>
<p>è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰åœ¨å„ç§è¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å„¿ç«¥è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç­‰ä½èµ„æºä»»åŠ¡çš„æ€§èƒ½å—åˆ°æœ‰é™é¢„è®­ç»ƒæ•°æ®çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸åŒçš„æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œä»¥åˆ©ç”¨åœ¨æ›´å¤§ã€æ›´å¤šæ ·åŒ–çš„è¯­éŸ³è¯­æ–™åº“ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„çŸ¥è¯†ã€‚æœ¬æ–‡è¿˜ä»‹ç»äº†é€‰æ‹©æ€§æ³¨æ„ï¼ˆSAï¼‰åˆå¹¶ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå®ƒé€‰æ‹©æ€§åœ°åˆå¹¶æ¥è‡ªæ³¨æ„åŠ›çŸ©é˜µçš„ä»»åŠ¡å‘é‡ï¼Œä»¥æé«˜SFMåœ¨ä½èµ„æºä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åœ¨MySTæ•°æ®åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸å¯¹è¯é”™è¯¯ç‡æœ€é«˜é™ä½äº†14%ï¼Œè¶…è¿‡äº†ç°æœ‰çš„æ¨¡å‹åˆå¹¶å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ã€‚é€šè¿‡å°†æ•°æ®å¢å¼ºæŠ€æœ¯ä¸SA Mergeç›¸ç»“åˆï¼Œæˆ‘ä»¬åœ¨MySTæ•°æ®åº“ä¸Šä¸ºWhisper-smallæ¨¡å‹è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰8.69%ï¼Œè¿™çªæ˜¾äº†SA Mergeåœ¨æé«˜ä½èµ„æºASRæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08468v1">PDF</a> To appear in ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æ¨¡å‹åˆå¹¶æŠ€æœ¯æ¥æå‡ä½èµ„æºä»»åŠ¡ï¼ˆå¦‚å„¿ç«¥è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ASRï¼‰çš„è¯­éŸ³åŸºç¡€æ¨¡å‹æ€§èƒ½é—®é¢˜ã€‚ç ”ç©¶ä»‹ç»äº†é€‰æ‹©æ€§æ³¨æ„åŠ›åˆå¹¶æŠ€æœ¯ï¼ˆSelective Attention Mergeï¼‰ï¼Œå®ƒé€šè¿‡é€‰æ‹©æ€§åˆå¹¶æ³¨æ„åŠ›çŸ©é˜µä¸­çš„ä»»åŠ¡å‘é‡ï¼Œæé«˜åœ¨ä½èµ„æºä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨MySTæ•°æ®åº“ä¸Šçš„ç›¸å¯¹è¯é”™è¯¯ç‡é™ä½å¹…åº¦è¾¾14%ï¼Œä¸å…¶ä»–æ¨¡å‹åˆå¹¶å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ç›¸æ¯”è¡¨ç°æ›´ä¼˜è¶Šã€‚é€šè¿‡ä¸æ•°æ®å¢å¼ºæŠ€æœ¯ç»“åˆï¼Œåˆ©ç”¨é€‰æ‹©æ€§æ³¨æ„åŠ›åˆå¹¶æŠ€æœ¯å®ç°çš„whisperå°å‹æ¨¡å‹åœ¨MySTæ•°æ®åº“ä¸Šçš„è¯é”™è¯¯ç‡é™è‡³8.69%ï¼Œå±•ç¤ºäº†å…¶æå‡ä½èµ„æºASRæ€§èƒ½çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹ä½èµ„æºä»»åŠ¡çš„è¯­éŸ³åŸºç¡€æ¨¡å‹æ€§èƒ½é—®é¢˜ï¼Œå°¤å…¶æ˜¯å„¿ç«¥è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ASRé¢†åŸŸï¼Œæ€§èƒ½å—åˆ°é™åˆ¶ï¼Œéœ€è¦è¿›è¡Œæ¨¡å‹åˆå¹¶æ¥ä¼˜åŒ–åˆ©ç”¨å¤§å‹å’Œå¤šæ ·çš„é¢„è®­ç»ƒæ•°æ®çŸ¥è¯†ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹åˆå¹¶æ–¹æ³•â€”â€”é€‰æ‹©æ€§æ³¨æ„åŠ›åˆå¹¶æŠ€æœ¯ï¼ˆSelective Attention Mergeï¼‰ï¼Œé€šè¿‡é€‰æ‹©æ€§åˆå¹¶æ³¨æ„åŠ›çŸ©é˜µä¸­çš„ä»»åŠ¡å‘é‡ï¼Œæé«˜ä½èµ„æºä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜é€‰æ‹©æ€§æ³¨æ„åŠ›åˆå¹¶æŠ€æœ¯æ˜¾è‘—é™ä½äº†ç›¸å¯¹è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œåœ¨æŸäº›åœºæ™¯ä¸‹æå‡æ•ˆæœæ˜¾è‘—ï¼Œç›¸å¯¹äºç°æœ‰æ¨¡å‹åˆå¹¶å’Œæ•°æ®å¢å¼ºæŠ€æœ¯æœ‰æ›´å¥½çš„è¡¨ç°ã€‚</li>
<li>åœ¨MySTæ•°æ®åº“ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œå½“é€‰æ‹©æ€§æ³¨æ„åŠ›åˆå¹¶æŠ€æœ¯ä¸æ•°æ®å¢å¼ºæŠ€æœ¯ç»“åˆä½¿ç”¨æ—¶ï¼Œå¯ä»¥è¿›ä¸€æ­¥é™ä½è¯é”™è¯¯ç‡è‡³8.69%ï¼Œå±•ç°å‡ºè¯¥æŠ€æœ¯åœ¨æ”¹å–„ä½èµ„æºASRä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºä½èµ„æºç¯å¢ƒä¸‹çš„è¯­éŸ³ä»»åŠ¡æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç¼ºä¹å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®çš„åœºæ™¯ã€‚</li>
<li>ç ”ç©¶ç»“æœå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å„¿ç«¥è¯­éŸ³è¯†åˆ«ã€è¾…åŠ©é€šè®¯æŠ€æœ¯ä»¥åŠå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abefc04e38b247dd5594b5f2ee5e2059.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b86cd52a1feb0b91e34affd21fc6b622.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f9a7aae936b6d6d34cb328eff3f6b09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-437eb00167dda0fa809f56754783cf8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29938d18a004d4b52fd2c21f28a60366.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SemTalk-Holistic-Co-speech-Motion-Generation-with-Frame-level-Semantic-Emphasis"><a href="#SemTalk-Holistic-Co-speech-Motion-Generation-with-Frame-level-Semantic-Emphasis" class="headerlink" title="SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic   Emphasis"></a>SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic   Emphasis</h2><p><strong>Authors:Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Ziqiang Dang, Jianqiang Ren, Liefeng Bo, Zhigang Tu</strong></p>
<p>A good co-speech motion generation cannot be achieved without a careful integration of common rhythmic motion and rare yet essential semantic motion. In this work, we propose SemTalk for holistic co-speech motion generation with frame-level semantic emphasis. Our key insight is to separately learn general motions and sparse motions, and then adaptively fuse them. In particular, rhythmic consistency learning is explored to establish rhythm-related base motion, ensuring a coherent foundation that synchronizes gestures with the speech rhythm. Subsequently, textit{semantic emphasis learning is designed to generate semantic-aware sparse motion, focusing on frame-level semantic cues. Finally, to integrate sparse motion into the base motion and generate semantic-emphasized co-speech gestures, we further leverage a learned semantic score for adaptive synthesis. Qualitative and quantitative comparisons on two public datasets demonstrate that our method outperforms the state-of-the-art, delivering high-quality co-speech motion with enhanced semantic richness over a stable base motion. </p>
<blockquote>
<p>è‰¯å¥½çš„ååŒè¯­éŸ³åŠ¨ä½œç”Ÿæˆç¦»ä¸å¼€å¸¸è§çš„èŠ‚å¥åŠ¨ä½œå’Œç½•è§ä½†å¿…è¦çš„è¯­ä¹‰åŠ¨ä½œçš„ä»”ç»†èåˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºæ•´ä½“ååŒè¯­éŸ³åŠ¨ä½œç”Ÿæˆçš„SemTalkæ–¹æ³•ï¼Œå…·æœ‰å¸§çº§è¯­ä¹‰å¼ºè°ƒã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯åˆ†åˆ«å­¦ä¹ ä¸€èˆ¬è¿åŠ¨å’Œç¨€ç–è¿åŠ¨ï¼Œç„¶åè‡ªé€‚åº”åœ°èåˆå®ƒä»¬ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ¢ç´¢äº†èŠ‚å¥ä¸€è‡´æ€§å­¦ä¹ æ¥å»ºç«‹ä¸èŠ‚å¥ç›¸å…³çš„åŸºæœ¬è¿åŠ¨ï¼Œç¡®ä¿ä¸è¯­éŸ³èŠ‚å¥åŒæ­¥çš„æ‰‹åŠ¿è¿è´¯åŸºç¡€ã€‚éšåï¼Œè®¾è®¡è¯­ä¹‰é‡ç‚¹å­¦ä¹ æ¥ç”Ÿæˆå…·æœ‰è¯­ä¹‰æ„ŸçŸ¥çš„ç¨€ç–è¿åŠ¨ï¼Œä¾§é‡äºå¸§çº§è¯­ä¹‰çº¿ç´¢ã€‚æœ€åï¼Œä¸ºäº†å°†ç¨€ç–åŠ¨ä½œèå…¥åŸºæœ¬åŠ¨ä½œå¹¶ç”Ÿæˆå…·æœ‰è¯­ä¹‰å¼ºè°ƒçš„ååŒè¯­éŸ³æ‰‹åŠ¿ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åˆ©ç”¨å­¦ä¹ åˆ°çš„è¯­ä¹‰åˆ†æ•°è¿›è¡Œè‡ªé€‚åº”åˆæˆã€‚åœ¨ä¸¤é¡¹å…¬å…±æ•°æ®é›†ä¸Šçš„å®šæ€§å’Œå®šé‡æ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œåœ¨ç¨³å®šçš„åŸºæœ¬åŠ¨ä½œä¸Šæä¾›é«˜è´¨é‡çš„ååŒè¯­éŸ³åŠ¨ä½œï¼Œå¢å¼ºäº†è¯­ä¹‰ä¸°å¯Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16563v2">PDF</a> 11 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…±åŒè¯­éŸ³è¿åŠ¨ç”Ÿæˆéœ€è¦ç»¼åˆè€ƒè™‘å¸¸è§çš„èŠ‚å¥è¿åŠ¨å’Œç½•è§ä½†é‡è¦çš„è¯­ä¹‰è¿åŠ¨ã€‚æœ¬ç ”ç©¶æå‡ºSemTalkæ–¹æ³•ï¼Œç”¨äºæ•´ä½“å…±åŒè¯­éŸ³è¿åŠ¨ç”Ÿæˆï¼Œå…·æœ‰å¸§çº§è¯­ä¹‰é‡ç‚¹ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯åˆ†åˆ«å­¦ä¹ ä¸€èˆ¬è¿åŠ¨å’Œç¨€ç–è¿åŠ¨ï¼Œç„¶åè‡ªé€‚åº”èåˆå®ƒä»¬ã€‚ç‰¹åˆ«æ¢ç´¢äº†èŠ‚å¥ä¸€è‡´æ€§å­¦ä¹ ï¼Œä»¥å»ºç«‹ä¸èŠ‚å¥ç›¸å…³çš„åŸºæœ¬è¿åŠ¨ï¼Œç¡®ä¿å§¿åŠ¿ä¸è¯­éŸ³èŠ‚å¥çš„åŒæ­¥æ€§ã€‚æ­¤å¤–ï¼Œè®¾è®¡è¯­ä¹‰é‡ç‚¹å­¦ä¹ ä»¥ç”Ÿæˆå…·æœ‰è¯­ä¹‰æ„è¯†çš„ç¨€ç–è¿åŠ¨ï¼Œä¾§é‡äºå¸§çº§è¯­ä¹‰çº¿ç´¢ã€‚æœ€åï¼Œä¸ºäº†å°†ç¨€ç–è¿åŠ¨èå…¥åŸºæœ¬è¿åŠ¨å¹¶ç”Ÿæˆå…·æœ‰è¯­ä¹‰å¼ºè°ƒçš„å…±åŒè¯­éŸ³å§¿åŠ¿ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åˆ©ç”¨å­¦ä¹ åˆ°çš„è¯­ä¹‰åˆ†æ•°è¿›è¡Œè‡ªé€‚åº”åˆæˆã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®šæ€§å’Œå®šé‡æ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å…±åŒè¯­éŸ³è¿åŠ¨ï¼Œå…·æœ‰ç¨³å®šçš„åŸºæœ¬è¿åŠ¨å’Œå¢å¼ºçš„è¯­ä¹‰ä¸°å¯Œæ€§ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å…±åŒè¯­éŸ³è¿åŠ¨ç”Ÿæˆéœ€è¦æ•´åˆèŠ‚å¥è¿åŠ¨å’Œè¯­ä¹‰è¿åŠ¨ã€‚</li>
<li>æå‡ºSemTalkæ–¹æ³•ï¼Œæ•´ä½“è¿›è¡Œå…±åŒè¯­éŸ³è¿åŠ¨ç”Ÿæˆï¼Œå¼•å…¥å¸§çº§è¯­ä¹‰é‡ç‚¹ã€‚</li>
<li>åˆ†åˆ«å­¦ä¹ ä¸€èˆ¬è¿åŠ¨å’Œç¨€ç–è¿åŠ¨ï¼Œç„¶åè‡ªé€‚åº”èåˆã€‚</li>
<li>é€šè¿‡èŠ‚å¥ä¸€è‡´æ€§å­¦ä¹ å»ºç«‹ä¸è¯­éŸ³èŠ‚å¥åŒæ­¥çš„åŸºæœ¬è¿åŠ¨ã€‚</li>
<li>è¯­ä¹‰é‡ç‚¹å­¦ä¹ ç”¨äºç”Ÿæˆå…·æœ‰å¸§çº§è¯­ä¹‰çº¿ç´¢çš„è¯­ä¹‰æ„ŸçŸ¥ç¨€ç–è¿åŠ¨ã€‚</li>
<li>åˆ©ç”¨å­¦ä¹ åˆ°çš„è¯­ä¹‰åˆ†æ•°å°†ç¨€ç–è¿åŠ¨èå…¥åŸºæœ¬è¿åŠ¨ã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç”Ÿæˆé«˜è´¨é‡å…±åŒè¯­éŸ³è¿åŠ¨ï¼Œå¢å¼ºè¯­ä¹‰ä¸°å¯Œæ€§å¹¶ä¿æŒç¨³å®šçš„åŸºæœ¬è¿åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5fbda91557b447d0d843dcafa617bc8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ea2f7d8193e1f32cf6e1cb15de3fa79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dd2a2dfae00d7c17206de4de1469f07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dc0da43cbf30e7013e1c4f64e6acd90.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CSL-L2M-Controllable-Song-Level-Lyric-to-Melody-Generation-Based-on-Conditional-Transformer-with-Fine-Grained-Lyric-and-Musical-Controls"><a href="#CSL-L2M-Controllable-Song-Level-Lyric-to-Melody-Generation-Based-on-Conditional-Transformer-with-Fine-Grained-Lyric-and-Musical-Controls" class="headerlink" title="CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on   Conditional Transformer with Fine-Grained Lyric and Musical Controls"></a>CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on   Conditional Transformer with Fine-Grained Lyric and Musical Controls</h2><p><strong>Authors:Li Chai, Donglin Wang</strong></p>
<p>Lyric-to-melody generation is a highly challenging task in the field of AI music generation. Due to the difficulty of learning strict yet weak correlations between lyrics and melodies, previous methods have suffered from weak controllability, low-quality and poorly structured generation. To address these challenges, we propose CSL-L2M, a controllable song-level lyric-to-melody generation method based on an in-attention Transformer decoder with fine-grained lyric and musical controls, which is able to generate full-song melodies matched with the given lyrics and user-specified musical attributes. Specifically, we first introduce REMI-Aligned, a novel music representation that incorporates strict syllable- and sentence-level alignments between lyrics and melodies, facilitating precise alignment modeling. Subsequently, sentence-level semantic lyric embeddings independently extracted from a sentence-wise Transformer encoder are combined with word-level part-of-speech embeddings and syllable-level tone embeddings as fine-grained controls to enhance the controllability of lyrics over melody generation. Then we introduce human-labeled musical tags, sentence-level statistical musical attributes, and learned musical features extracted from a pre-trained VQ-VAE as coarse-grained, fine-grained and high-fidelity controls, respectively, to the generation process, thereby enabling user control over melody generation. Finally, an in-attention Transformer decoder technique is leveraged to exert fine-grained control over the full-song melody generation with the aforementioned lyric and musical conditions. Experimental results demonstrate that our proposed CSL-L2M outperforms the state-of-the-art models, generating melodies with higher quality, better controllability and enhanced structure. Demos and source code are available at <a target="_blank" rel="noopener" href="https://lichaiustc.github.io/CSL-L2M/">https://lichaiustc.github.io/CSL-L2M/</a>. </p>
<blockquote>
<p>æ­Œè¯åˆ°æ—‹å¾‹çš„ç”Ÿæˆæ˜¯äººå·¥æ™ºèƒ½éŸ³ä¹ç”Ÿæˆé¢†åŸŸä¸­çš„ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç”±äºå­¦ä¹ æ­Œè¯å’Œæ—‹å¾‹ä¹‹é—´ä¸¥æ ¼ä½†å¾®å¼±çš„å…³è”å­˜åœ¨å›°éš¾ï¼Œä»¥å‰çš„æ–¹æ³•åœ¨å¯æ§æ€§ã€è´¨é‡å’Œç»“æ„æ–¹é¢å­˜åœ¨ç”Ÿæˆè´¨é‡ä¸é«˜çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ³¨æ„åŠ›ç¼ºå¤±çš„Transformerè§£ç å™¨çš„å¯æ§æ­Œæ›²çº§åˆ«çš„æ­Œè¯åˆ°æ—‹å¾‹ç”Ÿæˆæ–¹æ³•CSL-L2Mã€‚å®ƒèƒ½å¤Ÿæ ¹æ®ç»™å®šçš„æ­Œè¯å’Œç”¨æˆ·æŒ‡å®šçš„éŸ³ä¹å±æ€§ç”Ÿæˆå…¨æ›²çš„æ—‹å¾‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†REMI-Alignedè¿™ä¸€æ–°å‹éŸ³ä¹è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒç»“åˆäº†æ­Œè¯å’Œæ—‹å¾‹ä¹‹é—´çš„éŸ³èŠ‚å’Œå¥å­çº§åˆ«çš„ä¸¥æ ¼å¯¹é½ï¼Œä¾¿äºç²¾ç¡®å¯¹é½å»ºæ¨¡ã€‚æ¥ç€ï¼Œä»å¥å­çº§çš„Transformerç¼–ç å™¨ä¸­ç‹¬ç«‹æå–çš„è¯­ä¹‰æ­Œè¯åµŒå…¥ä¸è¯çº§çš„è¯æ€§åµŒå…¥å’ŒéŸ³èŠ‚çº§çš„éŸ³è°ƒåµŒå…¥ç›¸ç»“åˆï¼Œä½œä¸ºç²¾ç»†æ§åˆ¶æ¥å¢å¼ºæ­Œè¯å¯¹æ—‹å¾‹ç”Ÿæˆçš„æ“æ§æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†äººç±»æ ‡æ³¨çš„éŸ³ä¹æ ‡ç­¾ã€å¥å­çº§çš„ç»Ÿè®¡éŸ³ä¹å±æ€§ä»¥åŠä»é¢„è®­ç»ƒçš„VQ-VAEä¸­æå–çš„å­¦ä¹ åˆ°çš„éŸ³ä¹ç‰¹å¾åˆ†åˆ«å¼•å…¥ä¸ºç²—ç²’åº¦ã€ç»†ç²’åº¦å’Œé«˜ä¿çœŸæ§åˆ¶ï¼Œä»¥ç”¨äºç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ§åˆ¶æ—‹å¾‹çš„ç”Ÿæˆã€‚æœ€åï¼Œåˆ©ç”¨æ³¨æ„åŠ›ç¼ºå¤±çš„Transformerè§£ç å™¨æŠ€æœ¯ï¼Œæ ¹æ®å‰è¿°çš„æ­Œè¯å’ŒéŸ³ä¹æ¡ä»¶å¯¹å…¨æ›²çš„æ—‹å¾‹ç”Ÿæˆè¿›è¡Œç²¾ç»†æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„CSL-L2Mä¼˜äºç°æœ‰æ¨¡å‹ï¼Œç”Ÿæˆçš„æ—‹å¾‹è´¨é‡æ›´é«˜ã€å¯æ§æ€§æ›´å¥½ã€ç»“æ„æ›´åŠ å¢å¼ºã€‚Demoå’Œæºä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://lichaiustc.github.io/CSL-L2M/]%E8%AE%BF%E9%97%AE%E3%80%82">https://lichaiustc.github.io/CSL-L2M/]è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09887v2">PDF</a> Accepted at AAAI-25</p>
<p><strong>Summary</strong></p>
<p>åŸºäºAIéŸ³ä¹ç”Ÿæˆé¢†åŸŸä¸­çš„æ­Œè¯åˆ°æ—‹å¾‹ç”Ÿæˆä»»åŠ¡çš„é«˜åº¦æŒ‘æˆ˜æ€§ï¼Œå‰äººæ–¹æ³•å­˜åœ¨å¯æ§æ€§å¼±ã€ç”Ÿæˆè´¨é‡ä½å’Œç»“æ„ä¸ä½³ç­‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ³¨æ„åŠ›ç¼ºå¤±Transformerè§£ç å™¨çš„å¯æ§æ­Œè¯çº§åˆ«æ­Œè¯åˆ°æ—‹å¾‹ç”Ÿæˆæ–¹æ³•CSL-L2Mã€‚å®ƒé€šè¿‡ç²¾ç»†çš„æ­Œè¯å’ŒéŸ³ä¹æ§åˆ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸ç»™å®šæ­Œè¯å’Œç”¨æˆ·æŒ‡å®šçš„éŸ³ä¹å±æ€§ç›¸åŒ¹é…çš„å®Œæ•´æ­Œæ›²æ—‹å¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIéŸ³ä¹ç”Ÿæˆä¸­çš„æ­Œè¯åˆ°æ—‹å¾‹ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› æ­Œè¯ä¸æ—‹å¾‹é—´çš„ä¸¥æ ¼ä½†å¼±å…³è”éš¾ä»¥å­¦ä¹ ï¼Œå‰äººæ–¹æ³•å­˜åœ¨å¯æ§æ€§ã€è´¨é‡åŠç»“æ„é—®é¢˜ã€‚</li>
<li>CSL-L2Mæ–¹æ³•åŸºäºæ³¨æ„åŠ›ç¼ºå¤±çš„Transformerè§£ç å™¨ï¼Œå®ç°äº†æ­Œæ›²çº§åˆ«çš„æ­Œè¯åˆ°æ—‹å¾‹ç”Ÿæˆã€‚</li>
<li>REMI-AlignedéŸ³ä¹è¡¨ç¤ºæ–¹æ³•èå…¥äº†æ­Œè¯å’Œæ—‹å¾‹ä¹‹é—´çš„éŸ³èŠ‚å’Œå¥å­çº§åˆ«ä¸¥æ ¼å¯¹é½ï¼Œä¾¿äºç²¾å‡†å¯¹é½å»ºæ¨¡ã€‚</li>
<li>CSL-L2Mä½¿ç”¨äº†ç»†ç²’åº¦çš„æ§åˆ¶ï¼ŒåŒ…æ‹¬å¥å­çº§åˆ«çš„è¯­ä¹‰æ­Œè¯åµŒå…¥ã€è¯çº§åˆ«çš„è¯æ€§åµŒå…¥å’ŒéŸ³èŠ‚çº§åˆ«çš„éŸ³è°ƒåµŒå…¥ï¼Œå¢å¼ºäº†æ—‹å¾‹ç”Ÿæˆçš„å¯æ§æ€§ã€‚</li>
<li>å¼•å…¥äº†äººç±»æ ‡æ³¨çš„éŸ³ä¹æ ‡ç­¾ã€å¥å­çº§åˆ«çš„ç»Ÿè®¡éŸ³ä¹å±æ€§å’Œä»é¢„è®­ç»ƒVQ-VAEä¸­æå–çš„å­¦ä¹ éŸ³ä¹ç‰¹å¾ï¼Œåˆ†åˆ«ä¸ºç²—ç²’åº¦ã€ç»†ç²’åº¦å’Œé«˜ä¿çœŸæ§åˆ¶ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ§åˆ¶æ—‹å¾‹ç”Ÿæˆã€‚</li>
<li>å®éªŒç»“æœè¯æ˜ï¼ŒCSL-L2Mç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œç”Ÿæˆçš„æ—‹å¾‹è´¨é‡æ›´é«˜ã€å¯æ§æ€§æ›´å¼ºã€ç»“æ„æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a18afdad81aac00bedc97bdb3f8a1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a18f931806d805e42b7cdfee2e5114dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30ee0e17620e8745e0f687266cefc87d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7407e1d2e2194b685e782c1105429f00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94af1045a82426ef42984c272194167a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-547b5783b3778f87a353e56b12ebd8d0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MSA-ASR-Efficient-Multilingual-Speaker-Attribution-with-frozen-ASR-Models"><a href="#MSA-ASR-Efficient-Multilingual-Speaker-Attribution-with-frozen-ASR-Models" class="headerlink" title="MSA-ASR: Efficient Multilingual Speaker Attribution with frozen ASR   Models"></a>MSA-ASR: Efficient Multilingual Speaker Attribution with frozen ASR   Models</h2><p><strong>Authors:Thai-Binh Nguyen, Alexander Waibel</strong></p>
<p>Speaker-attributed automatic speech recognition (SA-ASR) aims to transcribe speech while assigning transcripts to the corresponding speakers accurately. Existing methods often rely on complex modular systems or require extensive fine-tuning of joint modules, limiting their adaptability and general efficiency. This paper introduces a novel approach, leveraging a frozen multilingual ASR model to incorporate speaker attribution into the transcriptions, using only standard monolingual ASR datasets. Our method involves training a speaker module to predict speaker embeddings based on weak labels without requiring additional ASR model modifications. Despite being trained exclusively with non-overlapping monolingual data, our approach effectively extracts speaker attributes across diverse multilingual datasets, including those with overlapping speech. Experimental results demonstrate competitive performance compared to strong baselines, highlighting the modelâ€™s robustness and potential for practical applications. </p>
<blockquote>
<p>è¯´è¯äººå±æ€§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆSA-ASRï¼‰æ—¨åœ¨å‡†ç¡®åœ°å°†è¯­éŸ³è½¬å½•å¹¶åˆ†é…ç»™ç›¸åº”çš„è¯´è¯äººã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„æ¨¡å—åŒ–ç³»ç»Ÿï¼Œæˆ–è€…éœ€è¦å¯¹è”åˆæ¨¡å—è¿›è¡Œå¤§é‡çš„å¾®è°ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„é€‚åº”æ€§å’Œæ€»ä½“æ•ˆç‡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å†»ç»“çš„å¤šè¯­è¨€ASRæ¨¡å‹æ¥ç»“åˆè¯´è¯äººå±æ€§è¿›è¡Œè½¬å½•ï¼Œä»…ä½¿ç”¨æ ‡å‡†å•è¯­è¨€ASRæ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šè¿‡è®­ç»ƒè¯´è¯äººæ¨¡å—æ¥é¢„æµ‹åŸºäºå¼±æ ‡ç­¾çš„è¯´è¯äººåµŒå…¥ï¼Œè€Œæ— éœ€é¢å¤–çš„ASRæ¨¡å‹ä¿®æ”¹ã€‚å°½ç®¡åªä½¿ç”¨éé‡å çš„å•è¯­è¨€æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æå–è·¨å¤šç§å¤šè¯­è¨€æ•°æ®é›†çš„è¯´è¯äººå±æ€§ï¼ŒåŒ…æ‹¬å…·æœ‰é‡å è¯­éŸ³çš„æƒ…å†µã€‚å®éªŒç»“æœä¸å¼ºåŸºçº¿ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œçªæ˜¾äº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œå®é™…åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18152v2">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„è¯´è¯äººå±æ€§è¯†åˆ«æŠ€æœ¯ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤æ‚çš„æ¨¡å—åŒ–ç³»ç»Ÿï¼Œéœ€è¦å¤§é‡ç²¾ç»†è°ƒæ•´è”åˆæ¨¡å—ï¼Œé™åˆ¶äº†å…¶é€‚åº”æ€§å’Œæ•ˆç‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å†»ç»“çš„å¤šè¯­è¨€ASRæ¨¡å‹å°†è¯´è¯äººå±æ€§çº³å…¥è½¬å½•ä¸­ï¼Œä»…ä½¿ç”¨æ ‡å‡†å•è¯­è¨€ASRæ•°æ®é›†ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒè¯´è¯äººæ¨¡å—é¢„æµ‹åŸºäºå¼±æ ‡ç­¾çš„è¯´è¯äººåµŒå…¥ï¼Œæ— éœ€ä¿®æ”¹é¢å¤–çš„ASRæ¨¡å‹ã€‚å°½ç®¡ä»…åœ¨éé‡å å•è¯­è¨€æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†è¯¥æ–¹æ³•åœ¨å¤šç§å¤šè¯­è¨€æ•°æ®é›†ä¸Šæœ‰æ•ˆåœ°æå–äº†è¯´è¯äººå±æ€§ï¼ŒåŒ…æ‹¬é‡å è¯­éŸ³æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å¼ºåŸºçº¿ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œçªæ˜¾äº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œå®é™…åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>SA-ASRæŠ€æœ¯æ—¨åœ¨å‡†ç¡®åœ°å°†è¯­éŸ³è½¬å½•å¹¶åˆ†é…ç»™ç›¸åº”çš„è¯´è¯äººã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤æ‚çš„æ¨¡å—åŒ–ç³»ç»Ÿï¼Œéœ€è¦ç²¾ç»†è°ƒæ•´ï¼Œé™åˆ¶äº†é€‚åº”æ€§å’Œæ•ˆç‡ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å†»ç»“çš„å¤šè¯­è¨€ASRæ¨¡å‹çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè¯´è¯äººæ¨¡å—å’Œé¢„æµ‹è¯´è¯äººåµŒå…¥æŠ€æœ¯å®ç°è¯´è¯äººå±æ€§è¯†åˆ«ã€‚</li>
<li>æ–¹æ³•ä»…ä½¿ç”¨æ ‡å‡†å•è¯­è¨€ASRæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é€šè¿‡å¼±æ ‡ç­¾é¢„æµ‹è¯´è¯äººåµŒå…¥ï¼Œæ— éœ€é¢å¤–ä¿®æ”¹ASRæ¨¡å‹ã€‚</li>
<li>å³ä½¿åœ¨éé‡å å•è¯­è¨€æ•°æ®ä¸Šè®­ç»ƒï¼Œè¯¥æ–¹æ³•ä¹Ÿèƒ½åœ¨å¤šç§å¤šè¯­è¨€æ•°æ®é›†ä¸Šæœ‰æ•ˆæå–è¯´è¯äººå±æ€§ï¼ŒåŒ…æ‹¬å¤„ç†é‡å è¯­éŸ³çš„æƒ…å†µã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-011b9dd39a34c1e00367cda42039cafe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f80a986b5a1719cbedab7bef8bae134d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb49c7905b649d0b831b4fb27d071f29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-174e8b63f9b9b1ba30537a8bd4aa765c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Diffusion-based-Unsupervised-Audio-visual-Speech-Enhancement"><a href="#Diffusion-based-Unsupervised-Audio-visual-Speech-Enhancement" class="headerlink" title="Diffusion-based Unsupervised Audio-visual Speech Enhancement"></a>Diffusion-based Unsupervised Audio-visual Speech Enhancement</h2><p><strong>Authors:Jean-Eudes Ayilo, Mostafa Sadeghi, Romain Serizel, Xavier Alameda-Pineda</strong></p>
<p>This paper proposes a new unsupervised audio-visual speech enhancement (AVSE) approach that combines a diffusion-based audio-visual speech generative model with a non-negative matrix factorization (NMF) noise model. First, the diffusion model is pre-trained on clean speech conditioned on corresponding video data to simulate the speech generative distribution. This pre-trained model is then paired with the NMF-based noise model to estimate clean speech iteratively. Specifically, a diffusion-based posterior sampling approach is implemented within the reverse diffusion process, where after each iteration, a speech estimate is obtained and used to update the noise parameters. Experimental results confirm that the proposed AVSE approach not only outperforms its audio-only counterpart but also generalizes better than a recent supervised-generative AVSE method. Additionally, the new inference algorithm offers a better balance between inference speed and performance compared to the previous diffusion-based method. Code and demo available at: <a target="_blank" rel="noopener" href="https://jeaneudesayilo.github.io/fast_UdiffSE">https://jeaneudesayilo.github.io/fast_UdiffSE</a> </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— ç›‘ç£éŸ³è§†é¢‘è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºæ‰©æ•£çš„éŸ³è§†é¢‘è¯­éŸ³ç”Ÿæˆæ¨¡å‹å’Œéè´ŸçŸ©é˜µåˆ†è§£ï¼ˆNMFï¼‰å™ªå£°æ¨¡å‹ã€‚é¦–å…ˆï¼Œæ‰©æ•£æ¨¡å‹åœ¨å¯¹åº”çš„è§†é¢‘æ•°æ®æ¡ä»¶ä¸‹å¯¹å¹²å‡€è¯­éŸ³è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥æ¨¡æ‹Ÿè¯­éŸ³ç”Ÿæˆåˆ†å¸ƒã€‚ç„¶åï¼Œå°†æ­¤é¢„è®­ç»ƒæ¨¡å‹ä¸åŸºäºNMFçš„å™ªå£°æ¨¡å‹é…å¯¹ï¼Œä»¥è¿­ä»£æ–¹å¼ä¼°è®¡å¹²å‡€è¯­éŸ³ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­å®ç°äº†åŸºäºæ‰©æ•£çš„åéªŒé‡‡æ ·æ–¹æ³•ï¼Œæ¯æ¬¡è¿­ä»£åï¼Œéƒ½ä¼šè·å¾—ä¸€ä¸ªè¯­éŸ³ä¼°è®¡å€¼ï¼Œå¹¶ç”¨äºæ›´æ–°å™ªå£°å‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„AVSEæ–¹æ³•ä¸ä»…ä¼˜äºä»…ä½¿ç”¨éŸ³é¢‘çš„å¯¹åº”æ–¹æ³•ï¼Œè€Œä¸”ç›¸è¾ƒäºæœ€æ–°çš„æœ‰ç›‘ç£ç”ŸæˆAVSEæ–¹æ³•å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ä¹‹å‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ–°çš„æ¨ç†ç®—æ³•åœ¨æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½ä¹‹é—´æä¾›äº†æ›´å¥½çš„å¹³è¡¡ã€‚ç›¸å…³ä»£ç å’Œæ¼”ç¤ºå¯åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://jeaneudesayilo.github.io/fast_UdiffSE]%EF%BC%88https://jeaneudesayilo.github.io/fast_UdiffSE%EF%BC%89%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://jeaneudesayilo.github.io/fast_UdiffSE]ï¼ˆhttps://jeaneudesayilo.github.io/fast_UdiffSEï¼‰ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05301v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— ç›‘ç£éŸ³é¢‘è§†è§‰è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºæ‰©æ•£çš„éŸ³é¢‘è§†è§‰è¯­éŸ³ç”Ÿæˆæ¨¡å‹ä¸éè´ŸçŸ©é˜µåˆ†è§£ï¼ˆNMFï¼‰å™ªå£°æ¨¡å‹ã€‚é¦–å…ˆï¼Œæ‰©æ•£æ¨¡å‹åœ¨å¯¹åº”çš„è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ¸…æ´è¯­éŸ³ï¼Œä»¥æ¨¡æ‹Ÿè¯­éŸ³ç”Ÿæˆåˆ†å¸ƒã€‚ç„¶åï¼Œå°†æ­¤é¢„è®­ç»ƒæ¨¡å‹ä¸åŸºäºNMFçš„å™ªå£°æ¨¡å‹ç»“åˆï¼Œä»¥è¿­ä»£æ–¹å¼ä¼°è®¡æ¸…æ´è¯­éŸ³ã€‚å…·ä½“å®ç°äº†ä¸€ç§åŸºäºæ‰©æ•£çš„åéªŒé‡‡æ ·æ–¹æ³•ï¼Œåœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡è¿­ä»£åéƒ½ä¼šè·å¾—è¯­éŸ³ä¼°è®¡å¹¶ç”¨äºæ›´æ–°å™ªå£°å‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„AVSEæ–¹æ³•ä¸ä»…ä¼˜äºä»…ä½¿ç”¨éŸ³é¢‘çš„å¯¹åº”æ–¹æ³•ï¼Œè€Œä¸”æ¯”ä¸€èˆ¬ç›‘ç£ç”ŸæˆAVSEæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚æ­¤å¤–ï¼Œæ–°çš„æ¨ç†ç®—æ³•åœ¨æ¨ç†é€Ÿåº¦ä¸æ€§èƒ½ä¹‹é—´è¾¾åˆ°äº†æ›´å¥½çš„å¹³è¡¡ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ‰©æ•£æ–¹æ³•æœ‰æ‰€æ”¹è¿›ã€‚ç›¸å…³ä»£ç ä¸æ¼”ç¤ºè§ï¼š<a target="_blank" rel="noopener" href="https://jeaneudesayilo.github.io/fast_UdiffSE">é“¾æ¥</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— ç›‘ç£éŸ³é¢‘è§†è§‰è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰æ–¹æ³•ï¼Œèåˆäº†æ‰©æ•£æ¨¡å‹å’ŒNMFå™ªå£°æ¨¡å‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡æ¨¡æ‹Ÿè¯­éŸ³ç”Ÿæˆåˆ†å¸ƒè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>ç»“åˆé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’ŒNMFå™ªå£°æ¨¡å‹ï¼Œé€šè¿‡è¿­ä»£æ–¹å¼ä¼°è®¡æ¸…æ´è¯­éŸ³ã€‚</li>
<li>é‡‡ç”¨åŸºäºæ‰©æ•£çš„åéªŒé‡‡æ ·æ–¹æ³•ï¼Œåœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­æ›´æ–°å™ªå£°å‚æ•°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºä»…ä½¿ç”¨éŸ³é¢‘çš„å¯¹åº”æ–¹æ³•ï¼Œä¸”æ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚</li>
<li>æ–°æ¨ç†ç®—æ³•åœ¨æ¨ç†é€Ÿåº¦ä¸æ€§èƒ½ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ï¼Œæ”¹è¿›äº†ä¹‹å‰çš„æ‰©æ•£æ–¹æ³•ã€‚</li>
<li>ç›¸å…³ä»£ç ä¸æ¼”ç¤ºå¯åœ¨æŒ‡å®šé“¾æ¥ä¸­æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-00722825b119fb723bbc76f4fc542364.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a89db5e782b93e39a01af218099eabbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fa2b0b95d7e602376a308ab8b238d5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ce0f004af01f0f28a70a9051a6e598e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Joint-Beam-Search-Integrating-CTC-Attention-and-Transducer-Decoders"><a href="#Joint-Beam-Search-Integrating-CTC-Attention-and-Transducer-Decoders" class="headerlink" title="Joint Beam Search Integrating CTC, Attention, and Transducer Decoders"></a>Joint Beam Search Integrating CTC, Attention, and Transducer Decoders</h2><p><strong>Authors:Yui Sudo, Muhammad Shakeel, Yosuke Fukumoto, Brian Yan, Jiatong Shi, Yifan Peng, Shinji Watanabe</strong></p>
<p>End-to-end automatic speech recognition (E2E-ASR) can be classified by its decoder architectures, such as connectionist temporal classification (CTC), recurrent neural network transducer (RNN-T), attention-based encoder-decoder, and Mask-CTC models. Each decoder architecture has advantages and disadvantages, leading practitioners to switch between these different models depending on application requirements. Instead of building separate models, we propose a joint modeling scheme where four decoders (CTC, RNN-T, attention, and Mask-CTC) share the same encoder â€“ we refer to this as 4D modeling. The 4D model is trained jointly, which will bring model regularization and maximize the model robustness thanks to their complementary properties. To efficiently train the 4D model, we introduce a two-stage training strategy that stabilizes the joint training. In addition, we propose three novel joint beam search algorithms by combining three decoders (CTC, RNN-T, and attention) to further improve performance. These three beam search algorithms differ in which decoder is used as the primary decoder. We carefully evaluate the performance and computational tradeoffs associated with each algorithm. Experimental results demonstrate that the jointly trained 4D model outperforms the E2E-ASR models trained with only one individual decoder. Furthermore, we demonstrate that the proposed joint beam search algorithm outperforms the previously proposed CTC&#x2F;attention decoding. </p>
<blockquote>
<p>ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰å¯æŒ‰ç…§å…¶è§£ç å™¨æ¶æ„è¿›è¡Œåˆ†ç±»ï¼Œå¦‚è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰ã€å¾ªç¯ç¥ç»ç½‘ç»œè½¬æ¢å™¨ï¼ˆRNN-Tï¼‰ã€åŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è§£ç å™¨ä»¥åŠMask-CTCæ¨¡å‹ç­‰ã€‚æ¯ç§è§£ç å™¨æ¶æ„éƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®åº”ç”¨éœ€æ±‚åœ¨è¿™äº›ä¸åŒæ¨¡å‹ä¹‹é—´è¿›è¡Œåˆ‡æ¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆå»ºæ¨¡æ–¹æ¡ˆï¼Œå››ç§è§£ç å™¨ï¼ˆCTCã€RNN-Tã€æ³¨æ„åŠ›ä»¥åŠMask-CTCï¼‰å…±äº«åŒä¸€ä¸ªç¼–ç å™¨ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º4Då»ºæ¨¡ã€‚4Dæ¨¡å‹è¿›è¡Œè”åˆè®­ç»ƒï¼Œè¿™å¸¦æ¥äº†æ¨¡å‹æ­£åˆ™åŒ–ï¼Œå¹¶å› å®ƒä»¬çš„äº’è¡¥æ€§è´¨è€Œæœ€å¤§åŒ–æ¨¡å‹çš„ç¨³å¥æ€§ã€‚ä¸ºäº†æœ‰æ•ˆè®­ç»ƒ4Dæ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä½¿è”åˆè®­ç»ƒæ›´åŠ ç¨³å®šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆä¸‰ç§è§£ç å™¨ï¼ˆCTCã€RNN-Tå’Œæ³¨æ„åŠ›ï¼‰æå‡ºäº†ä¸‰ç§æ–°å‹è”åˆæŸæœç´¢ç®—æ³•ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚è¿™ä¸‰ç§æŸæœç´¢ç®—æ³•çš„åŒºåˆ«åœ¨äºæ‰€ä½¿ç”¨çš„ä¸»è¦è§£ç å™¨ä¸åŒã€‚æˆ‘ä»¬ä»”ç»†è¯„ä¼°äº†æ¯ç§ç®—æ³•çš„æ€§èƒ½å’Œè®¡ç®—æŠ˜è¡·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè”åˆè®­ç»ƒçš„4Dæ¨¡å‹ä¼˜äºä»…ä½¿ç”¨å•ä¸ªè§£ç å™¨è®­ç»ƒçš„E2E-ASRæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜æ‰€æå‡ºçš„è”åˆæŸæœç´¢ç®—æ³•ä¼˜äºå…ˆå‰æå‡ºçš„CTC&#x2F;æ³¨æ„åŠ›è§£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02950v2">PDF</a> accepted to IEEE&#x2F;ACM Transactions on Audio Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç«¯å¯¹ç«¯è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰çš„å››ç§è§£ç å™¨æ¶æ„ï¼ˆCTCã€RNN-Tã€æ³¨æ„åŠ›ç¼–ç å™¨è§£ç å™¨å’ŒMask-CTCï¼‰çš„è”åˆå»ºæ¨¡æ–¹æ¡ˆã€‚è¯¥4Dæ¨¡å‹é€šè¿‡å…±äº«åŒä¸€ç¼–ç å™¨å®ç°å››ç§è§£ç å™¨çš„è”åˆè®­ç»ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ­£åˆ™åŒ–å’Œé²æ£’æ€§ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œä¸‰ç§è”åˆæŸæœç´¢ç®—æ³•ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè”åˆè®­ç»ƒçš„4Dæ¨¡å‹ä¼˜äºä»…ä½¿ç”¨å•ä¸ªè§£ç å™¨çš„E2E-ASRæ¨¡å‹ï¼Œè€Œæå‡ºçš„è”åˆæŸæœç´¢ç®—æ³•ä¹Ÿä¼˜äºå…ˆå‰çš„CTC&#x2F;æ³¨æ„åŠ›è§£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«¯å¯¹ç«¯è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰çš„è§£ç å™¨æ¶æ„åŒ…æ‹¬CTCã€RNN-Tã€æ³¨æ„åŠ›ç¼–ç å™¨è§£ç å™¨å’ŒMask-CTCã€‚</li>
<li>4Dæ¨¡å‹å®ç°äº†å››ç§è§£ç å™¨çš„è”åˆå»ºæ¨¡ï¼Œé€šè¿‡å…±äº«åŒä¸€ç¼–ç å™¨æé«˜æ¨¡å‹çš„æ­£åˆ™åŒ–å’Œé²æ£’æ€§ã€‚</li>
<li>æå‡ºäº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥ç¨³å®šè”åˆè®­ç»ƒã€‚</li>
<li>æå‡ºäº†ä¸‰ç§è”åˆæŸæœç´¢ç®—æ³•ï¼Œé€šè¿‡ç»“åˆä¸åŒè§£ç å™¨è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè”åˆè®­ç»ƒçš„4Dæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºå•ä¸€è§£ç å™¨çš„E2E-ASRæ¨¡å‹ã€‚</li>
<li>è”åˆæŸæœç´¢ç®—æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„CTC&#x2F;æ³¨æ„åŠ›è§£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5ede79bc4617b103570356b29b008889.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-399f3139a5f462eb0efebf67f14c4a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bafebe490620995d18b56424502c1ac.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Artificial-Intelligence-for-Cochlear-Implants-Review-of-Strategies-Challenges-and-Perspectives"><a href="#Artificial-Intelligence-for-Cochlear-Implants-Review-of-Strategies-Challenges-and-Perspectives" class="headerlink" title="Artificial Intelligence for Cochlear Implants: Review of Strategies,   Challenges, and Perspectives"></a>Artificial Intelligence for Cochlear Implants: Review of Strategies,   Challenges, and Perspectives</h2><p><strong>Authors:Billel Essaid, Hamza Kheddar, Noureddine Batel, Muhammad E. H. Chowdhury, Abderrahmane Lakas</strong></p>
<p>Automatic speech recognition (ASR) plays a pivotal role in our daily lives, offering utility not only for interacting with machines but also for facilitating communication for individuals with partial or profound hearing impairments. The process involves receiving the speech signal in analog form, followed by various signal processing algorithms to make it compatible with devices of limited capacities, such as cochlear implants (CIs). Unfortunately, these implants, equipped with a finite number of electrodes, often result in speech distortion during synthesis. Despite efforts by researchers to enhance received speech quality using various state-of-the-art (SOTA) signal processing techniques, challenges persist, especially in scenarios involving multiple sources of speech, environmental noise, and other adverse conditions. The advent of new artificial intelligence (AI) methods has ushered in cutting-edge strategies to address the limitations and difficulties associated with traditional signal processing techniques dedicated to CIs. This review aims to comprehensively cover advancements in CI-based ASR and speech enhancement, among other related aspects. The primary objective is to provide a thorough overview of metrics and datasets, exploring the capabilities of AI algorithms in this biomedical field, and summarizing and commenting on the best results obtained. Additionally, the review will delve into potential applications and suggest future directions to bridge existing research gaps in this domain. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå®ƒä¸ä»…ç”¨äºä¸æœºå™¨äº¤äº’ï¼Œè¿˜å¸®åŠ©éƒ¨åˆ†æˆ–é‡åº¦å¬åŠ›å—æŸäººå£«è¿›è¡Œæ—¥å¸¸æ²Ÿé€šã€‚è¿™ä¸€è¿‡ç¨‹åŒ…æ‹¬æ¥æ”¶æ¨¡æ‹Ÿå½¢å¼çš„è¯­éŸ³ä¿¡å·ï¼Œç„¶åé€šè¿‡å„ç§ä¿¡å·å¤„ç†ç®—æ³•ä½¿å…¶ä¸æœ‰é™å®¹é‡çš„è®¾å¤‡ï¼ˆå¦‚è€³èœ—æ¤å…¥è£…ç½®ï¼‰å…¼å®¹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¤å…¥ç‰©ç”µææ•°é‡æœ‰é™ï¼Œç»å¸¸åœ¨åˆæˆè¿‡ç¨‹ä¸­å¯¼è‡´è¯­éŸ³å¤±çœŸã€‚å°½ç®¡ç ”ç©¶äººå‘˜å°è¯•ä½¿ç”¨å„ç§æœ€å…ˆè¿›çš„ä¿¡å·å¤„ç†æŠ€æœ¯å’Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ–¹æ³•æå‡æ¥æ”¶åˆ°çš„è¯­éŸ³è´¨é‡ï¼Œä½†ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤šè¯­éŸ³æºã€ç¯å¢ƒå™ªå£°å’Œå…¶ä»–ä¸åˆ©æ¡ä»¶çš„æƒ…å†µä¸‹ã€‚åŸºäºäººå·¥æ™ºèƒ½çš„æ–°æ–¹æ³•çš„å‡ºç°ä¸ºä¼ ç»Ÿä¿¡å·å¤„ç†æŠ€æœ¯å¸¦æ¥çš„å±€é™æ€§å¸¦æ¥äº†å‰æ²¿è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨è€³èœ—æ¤å…¥ç‰©æ–¹é¢ã€‚æœ¬ç»¼è¿°æ—¨åœ¨å…¨é¢æ¶µç›–è€³èœ—æ¤å…¥ç‰©ç›¸å…³çš„ASRå’Œè¯­éŸ³å¢å¼ºçš„è¿›å±•ä»¥åŠå…¶ä»–ç›¸å…³æ–¹é¢ã€‚ä¸»è¦ç›®çš„æ˜¯æä¾›å…³äºè¯¥ç”Ÿç‰©åŒ»å­¦é¢†åŸŸä¸­AIç®—æ³•èƒ½åŠ›çš„æŒ‡æ ‡å’Œæ•°æ®é›†çš„å…¨é¢æ¦‚è¿°ï¼Œå¹¶æ€»ç»“è¯„è®ºæœ€ä½³ç»“æœã€‚æ­¤å¤–ï¼Œæœ¬ç»¼è¿°è¿˜å°†æ·±å…¥æ¢è®¨æ½œåœ¨åº”ç”¨ï¼Œå¹¶é’ˆå¯¹å½“å‰é¢†åŸŸå†…çš„ç ”ç©¶ç©ºç™½æå‡ºæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.15442v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€è¯­éŸ³è¯†åˆ«æŠ€æœ¯åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æ™®åŠï¼ŒåŠ©å¬å™¨æ¤å…¥ç‰©ï¼ˆCIï¼‰åœ¨æ”¹å–„å¬éšœäººå£«çš„å¬åŠ›æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºç”µææ•°é‡çš„é™åˆ¶ï¼ŒCIåœ¨åˆæˆè¿‡ç¨‹ä¸­å¸¸å¸¸å¯¼è‡´è¯­éŸ³å¤±çœŸã€‚è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½æ–¹æ³•çš„å‘å±•ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜æä¾›äº†æ–°çš„ç­–ç•¥ã€‚æœ¬æ–‡æ—¨åœ¨å…¨é¢å›é¡¾CIç›¸å…³çš„è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³å¢å¼ºæŠ€æœ¯çš„è¿›å±•ï¼Œæ¢è®¨AIç®—æ³•åœ¨è¿™ä¸€ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„èƒ½åŠ›ï¼Œå¹¶å¯¹æœ€ä½³ç»“æœè¿›è¡Œæ€»ç»“å’Œè¯„è®ºï¼ŒåŒæ—¶æ¢è®¨æ½œåœ¨çš„åº”ç”¨å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œä¸ä»…ç”¨äºä¸æœºå™¨äº¤äº’ï¼Œè¿˜ä¸ºå¬éšœäººå£«æä¾›æ²Ÿé€šä¾¿åˆ©ã€‚</li>
<li>è¯­éŸ³ä¿¡å·æ¥æ”¶åéœ€ç»è¿‡ä¸€ç³»åˆ—ç®—æ³•å¤„ç†ä»¥é€‚é…æœ‰é™å®¹é‡çš„è®¾å¤‡ï¼Œå¦‚åŠ©å¬å™¨æ¤å…¥ç‰©ï¼ˆCIï¼‰ã€‚</li>
<li>CIé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç”µææ•°é‡æœ‰é™å¯¼è‡´çš„è¯­éŸ³å¤±çœŸé—®é¢˜ã€‚</li>
<li>äººå·¥æ™ºèƒ½æ–¹æ³•ä¸ºè§£å†³ä¼ ç»Ÿä¿¡å·å¤„ç†æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜æä¾›äº†æ–°çš„ç­–ç•¥ã€‚</li>
<li>æœ¬æ–‡å…¨é¢å›é¡¾äº†CIåœ¨ASRå’Œè¯­éŸ³å¢å¼ºæ–¹é¢çš„è¿›å±•ï¼ŒåŒ…æ‹¬è¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®é›†ã€‚</li>
<li>AIç®—æ³•åœ¨CIé¢†åŸŸçš„åº”ç”¨èƒ½åŠ›å¾—åˆ°äº†æ·±å…¥æ¢è®¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.15442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a85166c755b334111d591119665fde1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae10fa70ebfcce03263ff2b8a0751d51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-968f255f8810d9bbaa5dc11f673abe7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a75111a4cc89f4e38b6d146408b26b64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56ae89393672b4328b90aab511a4c4d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce57bebef63269f6706480ac583bb949.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="On-the-Effectiveness-of-ASR-Representations-in-Real-world-Noisy-Speech-Emotion-Recognition"><a href="#On-the-Effectiveness-of-ASR-Representations-in-Real-world-Noisy-Speech-Emotion-Recognition" class="headerlink" title="On the Effectiveness of ASR Representations in Real-world Noisy Speech   Emotion Recognition"></a>On the Effectiveness of ASR Representations in Real-world Noisy Speech   Emotion Recognition</h2><p><strong>Authors:Xiaohan Shi, Jiajun He, Xingfeng Li, Tomoki Toda</strong></p>
<p>This paper proposes an efficient attempt to noisy speech emotion recognition (NSER). Conventional NSER approaches have proven effective in mitigating the impact of artificial noise sources, such as white Gaussian noise, but are limited to non-stationary noises in real-world environments due to their complexity and uncertainty. To overcome this limitation, we introduce a new method for NSER by adopting the automatic speech recognition (ASR) model as a noise-robust feature extractor to eliminate non-vocal information in noisy speech. We first obtain intermediate layer information from the ASR model as a feature representation for emotional speech and then apply this representation for the downstream NSER task. Our experimental results show that 1) the proposed method achieves better NSER performance compared with the conventional noise reduction method, 2) outperforms self-supervised learning approaches, and 3) even outperforms text-based approaches using ASR transcription or the ground truth transcription of noisy speech. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„é’ˆå¯¹å¸¦å™ªè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆNSERï¼‰çš„å°è¯•ã€‚ä¼ ç»Ÿçš„NSERæ–¹æ³•å·²ç»è¯æ˜åœ¨å‡å°‘äººå·¥å™ªå£°æºï¼ˆå¦‚ç™½é«˜æ–¯å™ªå£°ï¼‰çš„å½±å“æ–¹é¢æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ç”±äºå…¶å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ï¼Œå®ƒä»¬åœ¨ç°å®ç¯å¢ƒä¸­çš„éå¹³ç¨³å™ªå£°æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ–°çš„NSERæ–¹æ³•ï¼Œé€šè¿‡é‡‡ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ä½œä¸ºå™ªå£°é²æ£’æ€§ç‰¹å¾æå–å™¨ï¼Œæ¶ˆé™¤å¸¦å™ªè¯­éŸ³ä¸­çš„éè¯­éŸ³ä¿¡æ¯ã€‚æˆ‘ä»¬é¦–å…ˆè·å–ASRæ¨¡å‹çš„ä¸­é—´å±‚ä¿¡æ¯ä½œä¸ºæƒ…æ„Ÿè¯­éŸ³çš„ç‰¹å¾è¡¨ç¤ºï¼Œç„¶åå°†å…¶åº”ç”¨äºä¸‹æ¸¸NSERä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ1ï¼‰æ‰€æå‡ºçš„æ–¹æ³•åœ¨NSERæ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„é™å™ªæ–¹æ³•ï¼›2ï¼‰è¡¨ç°ä¼˜äºè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼›3ï¼‰ç”šè‡³ä¼˜äºåŸºäºæ–‡æœ¬çš„ASRè½¬å½•æˆ–æœ‰å™ªè¯­éŸ³çš„åœ°é¢çœŸå®è½¬å½•æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07093v3">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„å™ªå£°é²æ£’ç‰¹å¾æå–å™¨æ–¹æ³•ï¼Œæå‡ºä¸€ç§é’ˆå¯¹å«å™ªè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„æœ‰æ•ˆå°è¯•ã€‚ä¼ ç»Ÿçš„å«å™ªè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•è™½ç„¶èƒ½æœ‰æ•ˆå‡è½»äººå·¥å™ªå£°æºï¼ˆå¦‚ç™½é«˜æ–¯å™ªå£°ï¼‰çš„å½±å“ï¼Œä½†åœ¨ç°å®ç¯å¢ƒä¸­çš„éç¨³æ€å™ªå£°ä¸‹è¡¨ç°å—é™ã€‚æ–°æ–¹æ³•é‡‡ç”¨ASRæ¨¡å‹ä½œä¸ºå™ªå£°é²æ£’ç‰¹å¾æå–å™¨ï¼Œæ¶ˆé™¤å«å™ªè¯­éŸ³ä¸­çš„éè¯­éŸ³ä¿¡æ¯ã€‚é¦–å…ˆï¼Œä»ASRæ¨¡å‹ä¸­è·å–ä¸­é—´å±‚ä¿¡æ¯ä½œä¸ºæƒ…æ„Ÿè¯­éŸ³çš„ç‰¹å¾è¡¨ç¤ºï¼Œç„¶åå°†å…¶åº”ç”¨äºä¸‹æ¸¸å«å™ªè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿé™å™ªæ–¹æ³•å’Œè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æœ‰æ›´å¥½çš„æ€§èƒ½è¡¨ç°ï¼Œç”šè‡³è¶…è¶Šäº†åŸºäºASRè½¬å½•æˆ–å«å™ªè¯­éŸ³çš„åœ°é¢çœŸå®è½¬å½•çš„æ–‡æœ¬æ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ul>
<li>è¯¥æ–¹æ³•å¼•å…¥ASRæ¨¡å‹ä½œä¸ºå™ªå£°é²æ£’ç‰¹å¾æå–å™¨ï¼Œç”¨äºæ¶ˆé™¤å«å™ªè¯­éŸ³ä¸­çš„éè¯­éŸ³ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡è·å–ASRæ¨¡å‹çš„ä¸­é—´å±‚ä¿¡æ¯ä½œä¸ºæƒ…æ„Ÿè¯­éŸ³çš„ç‰¹å¾è¡¨ç¤ºï¼Œè¿›è€Œåº”ç”¨äºä¸‹æ¸¸å«å™ªè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å«å™ªè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿé™å™ªæ–¹æ³•å’Œè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†åŸºäºæ–‡æœ¬çš„æ–¹æ³•ï¼Œå¦‚ä½¿ç”¨ASRè½¬å½•æˆ–å«å™ªè¯­éŸ³çš„åœ°é¢çœŸå®è½¬å½•ã€‚</li>
<li>é‡‡ç”¨ASRæ¨¡å‹å¯æœ‰æ•ˆå¤„ç†éç¨³æ€å™ªå£°ä¸‹çš„å«å™ªè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§æ–°çš„è§†è§’æ¥è§£å†³ç°å®ç¯å¢ƒä¸­éç¨³æ€å™ªå£°å¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„å½±å“ã€‚</li>
<li>é‡‡ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯æå‡äº†å«å™ªè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®åº¦å’Œé²æ£’æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.07093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f4fd9385e001a224cc6eca73d18ac9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a07f193b773cc50ce5f17a4d50537a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b014953829ab8a8822f69b27a5e250c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-108776a24294e4e6b23e12a2b11bfee2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Quilt-1M-One-Million-Image-Text-Pairs-for-Histopathology"><a href="#Quilt-1M-One-Million-Image-Text-Pairs-for-Histopathology" class="headerlink" title="Quilt-1M: One Million Image-Text Pairs for Histopathology"></a>Quilt-1M: One Million Image-Text Pairs for Histopathology</h2><p><strong>Authors:Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, Linda Shapiro</strong></p>
<p>Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has slowed comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate QUILT: a large-scale vision-language dataset consisting of $802, 144$ image and text pairs. QUILT was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine QUILT with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it as the largest vision-language histopathology dataset to date. We demonstrate the value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model outperforms state-of-the-art models on both zero-shot and linear probing tasks for classifying new histopathology images across $13$ diverse patch-level datasets of $8$ different sub-pathologies and cross-modal retrieval tasks. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€åº”ç”¨çš„åŠ é€Ÿå‘å±•å¾—ç›Šäºç½‘ç»œä¸Šå¤§é‡å›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„å¯ç”¨æ€§ã€‚ç„¶è€Œï¼ŒåŒ»å­¦é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨ç»„ç»‡ç—…ç†å­¦æ–¹é¢ï¼Œç±»ä¼¼æ•°æ®çš„ç¨€ç¼ºæ€§å‡ç¼“äº†ç›¸åº”çš„è¿›å±•ã€‚ä¸ºäº†åœ¨ç»„ç»‡ç—…ç†å­¦ä¸­å®ç°ç±»ä¼¼çš„è¡¨ç¤ºå­¦ä¹ ï¼Œæˆ‘ä»¬è½¬å‘YouTubeè¿™ä¸€æœªè¢«å¼€å‘çš„è§†é¢‘èµ„æºå®åº“ï¼Œä»ä¸­è·å–äº†æ¥è‡ªä¸“å®¶åŒ»ç”Ÿçš„1087å°æ—¶å®è´µçš„æ•™è‚²ç»„ç»‡ç—…ç†å­¦è§†é¢‘ã€‚ä»YouTubeä¸Šï¼Œæˆ‘ä»¬ç²¾å¿ƒç­–åˆ’äº†QUILTï¼šä¸€ä¸ªå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«802,144ä¸ªå›¾åƒå’Œæ–‡æœ¬å¯¹ã€‚QUILTæ˜¯ä½¿ç”¨å¤šç§æ¨¡å‹è‡ªåŠ¨ç­–åˆ’çš„ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ã€æ‰‹å·¥ç®—æ³•ã€äººç±»çŸ¥è¯†æ•°æ®åº“å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸ºç»„ç»‡ç—…ç†å­¦ç­–åˆ’çš„æœ€å…¨é¢çš„æ•°æ®é›†ä»…ç§¯ç´¯äº†å¤§çº¦20ä¸‡æ ·æœ¬ã€‚æˆ‘ä»¬å°†QUILTä¸å…¶ä»–æ¥æºçš„æ•°æ®é›†ç›¸ç»“åˆï¼ŒåŒ…æ‹¬Twitterã€ç ”ç©¶è®ºæ–‡å’Œäº’è”ç½‘ä¸Šçš„ä¸€èˆ¬æ¥æºï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ›´å¤§çš„æ•°æ®é›†ï¼šQUILT-1Mï¼ŒåŒ…å«100ä¸‡ä¸ªé…å¯¹å›¾åƒæ–‡æœ¬æ ·æœ¬ï¼Œæˆä¸ºè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„è§†è§‰è¯­è¨€ç»„ç»‡ç—…ç†å­¦æ•°æ®é›†ã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„CLIPæ¨¡å‹å±•ç¤ºäº†QUILT-1Mçš„ä»·å€¼ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œçº¿æ€§æ¢æµ‹ä»»åŠ¡ä¸Šè¶…è¶Šäº†æœ€æ–°æ¨¡å‹ï¼Œåœ¨è·¨è¶Š13ä¸ªä¸åŒè¡¥ä¸çº§åˆ«çš„æ•°æ®é›†ä¸­å¯¹æ–°çš„ç»„ç»‡ç—…ç†å­¦å›¾åƒè¿›è¡Œåˆ†ç±»ä»»åŠ¡è·¨è¶Šå…«ç§ä¸åŒçš„å­ç—…ç†ï¼Œå¹¶ä¸”åœ¨å®é™…æ“ä½œä¸­çš„è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°ä¼˜å¼‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.11207v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ç”±äºåœ¨çº¿å›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„ä¸°å¯Œæ€§ï¼Œå¤šæ¨¡æ€åº”ç”¨é¢†åŸŸçš„æœ€æ–°åŠ é€Ÿè¿›å±•ã€‚ç„¶è€Œï¼ŒåŒ»ç–—é¢†åŸŸå°¤å…¶æ˜¯ç»„ç»‡ç—…ç†å­¦ä¸­ç±»ä¼¼æ•°æ®çš„ç¨€ç¼ºæ€§é™åˆ¶äº†ç›¸åº”çš„è¿›å±•ã€‚ä¸ºå®ç°åœ¨ç»„ç»‡ç—…ç†å­¦ä¸­çš„ç±»ä¼¼è¡¨ç¤ºå­¦ä¹ ï¼Œè¯¥æ–‡æœ¬åˆ©ç”¨YouTubeè¿™ä¸€æœªå¼€å‘èµ„æºï¼Œä»ä¸­è·å–äº†1,087å°æ—¶ä¸“å®¶åŒ»ç”Ÿçš„çè´µæ•™è‚²ç»„ç»‡ç—…ç†å­¦è§†é¢‘ã€‚åŸºäºæ­¤ï¼Œæ„å»ºäº†QUILTå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«802,144ä¸ªå›¾åƒå’Œæ–‡æœ¬å¯¹ã€‚è¯¥æ•°æ®é›†æ˜¯é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ã€æ‰‹å·¥ç®—æ³•ã€äººç±»çŸ¥è¯†æ•°æ®åº“å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç­‰å¤šç§æ¨¡å‹è‡ªåŠ¨æ„å»ºçš„ã€‚ä¸è¿„ä»Šä¸ºæ­¢ä¸ºç»„ç»‡ç—…ç†å­¦æ•´ç†çš„æœ€å…¨é¢çš„æ•°æ®é›†ç›¸æ¯”ï¼ŒQUILTçš„æ ·æœ¬é‡é«˜å‡ºæ•°å€ã€‚é€šè¿‡å°†QUILTä¸å…¶ä»–æ¥æºçš„æ•°æ®é›†ï¼ˆåŒ…æ‹¬Twitterã€ç ”ç©¶è®ºæ–‡å’Œäº’è”ç½‘ä¸Šçš„æ•°æ®é›†ï¼‰ç›¸ç»“åˆï¼Œåˆ›å»ºäº†æ›´å¤§çš„QUILT-1Mæ•°æ®é›†ï¼ŒåŒ…å«1ç™¾ä¸‡é…å¯¹å›¾åƒæ–‡æœ¬æ ·æœ¬ï¼Œæˆä¸ºè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„è§†è§‰è¯­è¨€ç»„ç»‡ç—…ç†å­¦æ•°æ®é›†ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒçš„CLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯æ˜äº†QUILT-1Mçš„ä»·å€¼ï¼Œåœ¨é›¶æ ·æœ¬å’Œçº¿æ€§æ¢æµ‹ä»»åŠ¡ä¸­å¯¹æ–°ç»„ç»‡ç—…ç†å­¦å›¾åƒè¿›è¡Œåˆ†ç±»æ—¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè·¨è¶Šäº†13ä¸ªä¸åŒçš„è¡¥ä¸çº§åˆ«æ•°æ®é›†ï¼Œæ¶‰åŠ8ç§ä¸åŒçš„å­ç—…ç†å­¦ï¼Œå¹¶åœ¨è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åº”ç”¨é¢†åŸŸå› åœ¨çº¿å›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„ä¸°å¯Œæ€§è€ŒåŠ é€Ÿå‘å±•ï¼Œä½†åŒ»ç–—é¢†åŸŸå°¤å…¶æ˜¯ç»„ç»‡ç—…ç†å­¦ä¸­ç±»ä¼¼æ•°æ®çš„ç¨€ç¼ºæ€§é™åˆ¶äº†è¿›å±•ã€‚</li>
<li>åˆ©ç”¨YouTubeç­‰æœªå¼€å‘èµ„æºï¼Œè·å–æ•™è‚²ç»„ç»‡ç—…ç†å­¦è§†é¢‘ï¼Œæ„å»ºQUILTè§†è§‰è¯­è¨€æ•°æ®é›†ã€‚</li>
<li>QUILTæ•°æ®é›†åŒ…å«å¤§é‡çš„å›¾åƒå’Œæ–‡æœ¬å¯¹ï¼Œæ˜¯é€šè¿‡å¤šç§æ¨¡å‹è‡ªåŠ¨æ„å»ºçš„ã€‚</li>
<li>QUILTæ•°æ®é›†ä¸å…¶ä»–æ¥æºçš„æ•°æ®é›†ç›¸ç»“åˆï¼Œåˆ›å»ºäº†æ›´å¤§çš„QUILT-1Mæ•°æ®é›†ï¼Œæˆä¸ºæœ€å¤§çš„è§†è§‰è¯­è¨€ç»„ç»‡ç—…ç†å­¦æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼ŒQUILT-1Måœ¨åˆ†ç±»æ–°ç»„ç»‡ç—…ç†å­¦å›¾åƒå’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>QUILT-1Mçš„ä»·å€¼åœ¨äºå…¶èƒ½å¤Ÿè·¨è¶Šå¤šä¸ªä¸åŒçš„ç—…ç†å­¦é¢†åŸŸï¼ŒåŒ…æ‹¬å¯¹å„ç§å­ç—…ç†å­¦çš„åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.11207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b25af500058f5848b8f399b19cf6696.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3779518524003dbca38776ef7ecdec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13b2f7ae8c8e1a651031c3e91fe72747.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dd482029fd251b6727c43cf452682ae.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-32e3d5d9fbf7bd93624b69b8595da5bf.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  DynamicFace High-Quality and Consistent Video Face Swapping using   Composable 3D Facial Priors
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d81aa40b9446febcf258da28f6d4530e.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  SCOT Self-Supervised Contrastive Pretraining For Zero-Shot   Compositional Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
