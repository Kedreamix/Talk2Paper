<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  Vision Foundation Models for Computed Tomography">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-633ed0700f61ea6af0528ef501f0365a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-17-æ›´æ–°"><a href="#2025-01-17-æ›´æ–°" class="headerlink" title="2025-01-17 æ›´æ–°"></a>2025-01-17 æ›´æ–°</h1><h2 id="Vision-Foundation-Models-for-Computed-Tomography"><a href="#Vision-Foundation-Models-for-Computed-Tomography" class="headerlink" title="Vision Foundation Models for Computed Tomography"></a>Vision Foundation Models for Computed Tomography</h2><p><strong>Authors:Suraj Pai, Ibrahim Hadzic, Dennis Bontempi, Keno Bressem, Benjamin H. Kann, Andriy Fedorov, Raymond H. Mak, Hugo J. W. L. Aerts</strong></p>
<p>Foundation models (FMs) have shown transformative potential in radiology by performing diverse, complex tasks across imaging modalities. Here, we developed CT-FM, a large-scale 3D image-based pre-trained model designed explicitly for various radiological tasks. CT-FM was pre-trained using 148,000 computed tomography (CT) scans from the Imaging Data Commons through label-agnostic contrastive learning. We evaluated CT-FM across four categories of tasks, namely, whole-body and tumor segmentation, head CT triage, medical image retrieval, and semantic understanding, showing superior performance against state-of-the-art models. Beyond quantitative success, CT-FM demonstrated the ability to cluster regions anatomically and identify similar anatomical and structural concepts across scans. Furthermore, it remained robust across test-retest settings and indicated reasonable salient regions attached to its embeddings. This study demonstrates the value of large-scale medical imaging foundation models and by open-sourcing the model weights, code, and data, aims to support more adaptable, reliable, and interpretable AI solutions in radiology. </p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨æ”¾å°„å­¦ä¸­è¡¨ç°å‡ºäº†å˜é©æ€§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„æˆåƒæ¨¡å¼ä¸‹æ‰§è¡Œå¤šæ ·ä¸”å¤æ‚çš„ä»»åŠ¡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼€å‘äº†CT-FMï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºäº3Då›¾åƒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸“ä¸ºå„ç§æ”¾å°„å­¦ä»»åŠ¡è€Œè®¾è®¡ã€‚CT-FMä½¿ç”¨æ¥è‡ªå½±åƒæ•°æ®å…±äº«å¹³å°çš„14.8ä¸‡ä»½è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œé‡‡ç”¨æ ‡ç­¾æ— å…³çš„å¯¹æ¯”å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨å››ç±»ä»»åŠ¡ä¸­è¯„ä¼°äº†CT-FMï¼Œå³å…¨èº«å’Œè‚¿ç˜¤åˆ†å‰²ã€å¤´éƒ¨CTç­›æŸ¥ã€åŒ»å­¦å›¾åƒæ£€ç´¢å’Œè¯­ä¹‰ç†è§£ï¼Œå…¶æ€§èƒ½ä¼˜äºæœ€æ–°æ¨¡å‹ã€‚é™¤äº†å®šé‡æˆåŠŸä¹‹å¤–ï¼ŒCT-FMè¿˜å±•ç¤ºäº†åœ¨è§£å‰–åŒºåŸŸè¿›è¡Œèšç±»çš„èƒ½åŠ›ï¼Œå¹¶èƒ½åœ¨æ‰«æä¸­è¯†åˆ«ç›¸ä¼¼çš„è§£å‰–å’Œç»“æ„æ¦‚å¿µã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æµ‹è¯•é‡æµ‹ç¯å¢ƒä¸­è¡¨ç°ç¨³å¥ï¼Œå…¶åµŒå…¥çš„æ˜¾è‘—åŒºåŸŸä¹Ÿåˆç†ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†å¤§è§„æ¨¡åŒ»å­¦æˆåƒåŸºç¡€æ¨¡å‹çš„ä»·å€¼ï¼Œé€šè¿‡å¼€æºæ¨¡å‹æƒé‡ã€ä»£ç å’Œæ•°æ®ï¼Œæ—¨åœ¨æ”¯æŒæ›´å…·é€‚åº”æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§çš„æ”¾å°„å­¦äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09001v1">PDF</a> 6 figures, followed by 9 Extended Data Figures and a Supplementary   Information document</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡åŒ»å­¦æˆåƒåŸºç¡€æ¨¡å‹CT-FMåœ¨æ”¾å°„å­¦ä¸­çš„æ½œåŠ›è¯„ä¼°ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨æ¥è‡ªæˆåƒæ•°æ®å…±äº«ä¸­å¿ƒçš„14.8ä¸‡ä»½è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ‰«ææ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå±•ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡æ€è¡¨ç°èƒ½åŠ›ã€‚åœ¨å…¨èº«å’Œè‚¿ç˜¤åˆ†å‰²ã€å¤´éƒ¨CTè¯„ä¼°ã€åŒ»å­¦å›¾åƒæ£€ç´¢å’Œè¯­ä¹‰ç†è§£ç­‰ä»»åŠ¡ä¸­ï¼ŒCT-FMè¡¨ç°è¶…è¶Šç°æœ‰é¡¶å°–æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½è¿›è¡Œè§£å‰–å­¦åŒºåŸŸèšç±»ï¼Œè¯†åˆ«ä¸åŒæ‰«æé—´çš„ç›¸ä¼¼è§£å‰–ç»“æ„å’Œæ¦‚å¿µã€‚æ¨¡å‹çš„å¼€æºå°†ä¿ƒè¿›æ”¾å°„å­¦ä¸­é€‚åº”æ€§æ›´å¼ºã€æ›´å¯é å’Œå¯è§£é‡Šçš„AIè§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CT-FMæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºäº3Då›¾åƒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸“ä¸ºå„ç§æ”¾å°„å­¦ä»»åŠ¡è®¾è®¡ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨æ¥è‡ªæˆåƒæ•°æ®å…±äº«ä¸­å¿ƒçš„14.8ä¸‡ä»½CTæ‰«ææ•°æ®è¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>åœ¨å…¨èº«å’Œè‚¿ç˜¤åˆ†å‰²ã€å¤´éƒ¨CTè¯„ä¼°ã€åŒ»å­¦å›¾åƒæ£€ç´¢å’Œè¯­ä¹‰ç†è§£ç­‰ä»»åŠ¡ä¸­ï¼ŒCT-FMè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>CT-FMå…·å¤‡è§£å‰–å­¦åŒºåŸŸèšç±»èƒ½åŠ›ï¼Œå¹¶èƒ½è¯†åˆ«ä¸åŒæ‰«æé—´çš„ç›¸ä¼¼è§£å‰–ç»“æ„å’Œæ¦‚å¿µã€‚</li>
<li>æ¨¡å‹åœ¨æµ‹è¯•é‡æµ‹ç¯å¢ƒä¸­è¡¨ç°ç¨³å¥ï¼Œå¹¶èƒ½åˆç†è¯†åˆ«å…³é”®åŒºåŸŸã€‚</li>
<li>é€šè¿‡å¼€æºæ¨¡å‹æƒé‡ã€ä»£ç å’Œæ•°æ®ï¼Œæ”¯æŒæ›´é€‚åº”ã€å¯é å’Œå¯è§£é‡Šçš„æ”¾å°„å­¦AIè§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-64015eb4ec4af1dd0711991adaacb548.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8b640709322557dcfdc1051fdb227ae.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Revealing-Local-Structures-through-Machine-Learning-Fused-Multimodal-Spectroscopy"><a href="#Revealing-Local-Structures-through-Machine-Learning-Fused-Multimodal-Spectroscopy" class="headerlink" title="Revealing Local Structures through Machine-Learning- Fused Multimodal   Spectroscopy"></a>Revealing Local Structures through Machine-Learning- Fused Multimodal   Spectroscopy</h2><p><strong>Authors:Haili Jia, Yiming Chen, Gi-Hyeok Lee, Jacob Smith, Miaofang Chi, Wanli Yang, Maria K. Y. Chan</strong></p>
<p>Atomistic structures of materials offer valuable insights into their functionality. Determining these structures remains a fundamental challenge in materials science, especially for systems with defects. While both experimental and computational methods exist, each has limitations in resolving nanoscale structures. Core-level spectroscopies, such as x-ray absorption (XAS) or electron energy-loss spectroscopies (EELS), have been used to determine the local bonding environment and structure of materials. Recently, machine learning (ML) methods have been applied to extract structural and bonding information from XAS&#x2F;EELS, but most of these frameworks rely on a single data stream, which is often insufficient. In this work, we address this challenge by integrating multimodal ab initio simulations, experimental data acquisition, and ML techniques for structure characterization. Our goal is to determine local structures and properties using EELS and XAS data from multiple elements and edges. To showcase our approach, we use various lithium nickel manganese cobalt (NMC) oxide compounds which are used for lithium ion batteries, including those with oxygen vacancies and antisite defects, as the sample material system. We successfully inferred local element content, ranging from lithium to transition metals, with quantitative agreement with experimental data. Beyond improving prediction accuracy, we find that ML model based on multimodal spectroscopic data is able to determine whether local defects such as oxygen vacancy and antisites are present, a task which is impossible for single mode spectra or other experimental techniques. Furthermore, our framework is able to provide physical interpretability, bridging spectroscopy with the local atomic and electronic structures. </p>
<blockquote>
<p>ææ–™åŸå­ç»“æ„ä¸ºæˆ‘ä»¬æ·±å…¥äº†è§£å…¶åŠŸèƒ½æä¾›äº†å®è´µçš„è§è§£ã€‚ç¡®å®šè¿™äº›ç»“æ„ä»ç„¶æ˜¯ææ–™ç§‘å­¦é¢†åŸŸçš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå­˜åœ¨ç¼ºé™·çš„ç³»ç»Ÿã€‚è™½ç„¶å­˜åœ¨å®éªŒå’Œè®¡ç®—æ–¹æ³•ï¼Œä½†æ¯ç§æ–¹æ³•åœ¨è§£å†³çº³ç±³å°ºåº¦ç»“æ„æ–¹é¢éƒ½æœ‰å…¶å±€é™æ€§ã€‚èŠ¯å±‚å…‰è°±å­¦ï¼Œå¦‚Xå°„çº¿å¸æ”¶ï¼ˆXASï¼‰æˆ–ç”µå­èƒ½é‡æŸå¤±å…‰è°±å­¦ï¼ˆEELSï¼‰ï¼Œå·²è¢«ç”¨äºç¡®å®šææ–™çš„å±€éƒ¨é”®åˆç¯å¢ƒå’Œç»“æ„ã€‚æœ€è¿‘ï¼Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•å·²è¢«åº”ç”¨äºä»XAS&#x2F;EELSä¸­æå–ç»“æ„å’Œé”®åˆä¿¡æ¯ï¼Œä½†å¤§å¤šæ•°æ¡†æ¶éƒ½ä¾èµ–äºå•ä¸€æ•°æ®æµï¼Œè¿™é€šå¸¸æ˜¯ä¸å¤Ÿçš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é›†æˆå¤šæ¨¡å¼ä»å¤´æ¨¡æ‹Ÿã€å®éªŒæ•°æ®è·å–å’ŒMLæŠ€æœ¯æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä»¥è¿›è¡Œç»“æ„è¡¨å¾ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ©ç”¨æ¥è‡ªå¤šç§å…ƒç´ å’Œè¾¹ç¼˜çš„EELSå’ŒXASæ•°æ®æ¥ç¡®å®šå±€éƒ¨ç»“æ„å’Œå±æ€§ã€‚ä¸ºäº†å±•ç¤ºæˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å„ç§ç”¨äºé”‚ç¦»å­ç”µæ± çš„é”‚é•é”°é’´ï¼ˆNMCï¼‰æ°§åŒ–ç‰©åŒ–åˆç‰©ä½œä¸ºæ ·æœ¬ææ–™ç³»ç»Ÿï¼ŒåŒ…æ‹¬å…·æœ‰æ°§ç©ºä½å’Œåä½ç¼ºé™·çš„åŒ–åˆç‰©ã€‚æˆ‘ä»¬æˆåŠŸåœ°æ¨æ–­å‡ºäº†ä»é”‚åˆ°è¿‡æ¸¡é‡‘å±ç­‰å±€éƒ¨å…ƒç´ å«é‡ï¼Œå¹¶ä¸å®éªŒæ•°æ®å®šé‡å»åˆã€‚é™¤äº†æé«˜é¢„æµ‹ç²¾åº¦å¤–ï¼Œæˆ‘ä»¬å‘ç°åŸºäºå¤šæ¨¡å¼å…‰è°±æ•°æ®çš„MLæ¨¡å‹èƒ½å¤Ÿç¡®å®šæ˜¯å¦å­˜åœ¨å±€éƒ¨ç¼ºé™·ï¼Œå¦‚æ°§ç©ºä½å’Œåä½ï¼Œè¿™æ˜¯å•ä¸€æ¨¡å¼å…‰è°±æˆ–å…¶ä»–å®éªŒæŠ€æœ¯æ— æ³•å®Œæˆçš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæä¾›ç‰©ç†å¯è§£é‡Šæ€§ï¼Œå°†å…‰è°±ä¸å±€éƒ¨åŸå­å’Œç”µå­ç»“æ„ç›¸è”ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08919v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ææ–™åŸå­ç»“æ„çš„é‡è¦æ€§åŠå…¶åœ¨ç ”ç©¶ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨ç¼ºé™·çš„ç³»ç»Ÿä¸­ã€‚æ–‡ç« ç»“åˆäº†å¤šæ¨¡æ€ä»å¤´æ¨¡æ‹Ÿã€å®éªŒæ•°æ®é‡‡é›†å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œæ—¨åœ¨åˆ©ç”¨ç”µå­èƒ½é‡æŸå¤±è°±ï¼ˆEELSï¼‰å’ŒXå°„çº¿å¸æ”¶è°±ï¼ˆXASï¼‰æ•°æ®æ¥ç¡®å®šå±€éƒ¨ç»“æ„å’Œæ€§è´¨ã€‚æ–‡ç« ä»¥å«æœ‰æ°§ç©ºä½å’Œåä½ç¼ºé™·çš„é”‚é•é”°é’´æ°§åŒ–ç‰©ä¸ºä¾‹ï¼Œå±•ç¤ºäº†å…¶æˆåŠŸæ¨æ–­å‡ºå±€éƒ¨å…ƒç´ å«é‡çš„èƒ½åŠ›ï¼Œå¹¶ä¸å®éªŒç»“æœå®šé‡ä¸€è‡´ã€‚æ­¤å¤–ï¼ŒåŸºäºå¤šæ¨¡æ€å…‰è°±æ•°æ®çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿˜èƒ½å¤Ÿç¡®å®šå±€éƒ¨ç¼ºé™·çš„å­˜åœ¨ï¼Œè¿™æ˜¯ä¸€é¡¹å•ä¸€æ¨¡å¼å…‰è°±æˆ–å…¶ä»–å®éªŒæŠ€æœ¯æ— æ³•å®Œæˆçš„ä»»åŠ¡ã€‚è¯¥æ¡†æ¶è¿˜èƒ½å¤Ÿæä¾›ç‰©ç†å¯è§£é‡Šæ€§ï¼Œå°†å…‰è°±ä¸å±€éƒ¨åŸå­å’Œç”µå­ç»“æ„ç›¸è”ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ææ–™åŸå­ç»“æ„çš„ç ”ç©¶å¯¹äºç†è§£å…¶åŠŸèƒ½æ€§è‡³å…³é‡è¦ï¼Œä½†åœ¨å­˜åœ¨ç¼ºé™·çš„ç³»ç»Ÿä¸­ï¼Œç¡®å®šè¿™äº›ç»“æ„ä»æ˜¯ææ–™ç§‘å­¦ä¸­çš„åŸºæœ¬æŒ‘æˆ˜ã€‚</li>
<li>æ ¸å¿ƒå±‚æ¬¡å…‰è°±å­¦ï¼Œå¦‚XASå’ŒEELSï¼Œå·²è¢«ç”¨äºç¡®å®šææ–™çš„å±€éƒ¨é”®åˆç¯å¢ƒå’Œç»“æ„ã€‚</li>
<li>æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•å·²è¢«åº”ç”¨äºä»XAS&#x2F;EELSä¸­æå–ç»“æ„å’Œé”®åˆä¿¡æ¯ï¼Œä½†å¤§å¤šæ•°æ¡†æ¶ä¾èµ–äºå•ä¸€æ•°æ®æµï¼Œè¿™é€šå¸¸æ˜¯ä¸å¤Ÿçš„ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡ç»“åˆå¤šæ¨¡æ€ä»å¤´æ¨¡æ‹Ÿã€å®éªŒæ•°æ®é‡‡é›†å’ŒMLæŠ€æœ¯æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæ—¨åœ¨åˆ©ç”¨EELSå’ŒXASæ•°æ®ç¡®å®šå±€éƒ¨ç»“æ„å’Œæ€§è´¨ã€‚</li>
<li>ä»¥å«æœ‰æ°§ç©ºä½å’Œåä½ç¼ºé™·çš„é”‚é•é”°é’´æ°§åŒ–ç‰©ä¸ºä¾‹å­ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ¨æ–­å±€éƒ¨å…ƒç´ å«é‡æ–¹é¢çš„æˆåŠŸï¼Œå¹¶ä¸å®éªŒç»“æœä¸€è‡´ã€‚</li>
<li>åŸºäºå¤šæ¨¡æ€å…‰è°±æ•°æ®çš„MLæ¨¡å‹èƒ½å¤Ÿç¡®å®šå±€éƒ¨ç¼ºé™·çš„å­˜åœ¨ï¼Œè¿™æ˜¯å•ä¸€æ¨¡å¼å…‰è°±æˆ–å…¶ä»–å®éªŒæŠ€æœ¯æ— æ³•åšåˆ°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c4331c865c642fdb76cc82b90ea482f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b89b0fce566656d896b97d48a2f9618b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ee80cf7de858752ad2579f7aef2449a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55530ff58a8ace863b7eb1e996d6fe04.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-View-Transformers-for-Airway-To-Lung-Ratio-Inference-on-Cardiac-CT-Scans-The-C4R-Study"><a href="#Multi-View-Transformers-for-Airway-To-Lung-Ratio-Inference-on-Cardiac-CT-Scans-The-C4R-Study" class="headerlink" title="Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT   Scans: The C4R Study"></a>Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT   Scans: The C4R Study</h2><p><strong>Authors:Sneha N. Naik, Elsa D. Angelini, Eric A. Hoffman, Elizabeth C. Oelsner, R. Graham Barr, Benjamin M. Smith, Andrew F. Laine</strong></p>
<p>The ratio of airway tree lumen to lung size (ALR), assessed at full inspiration on high resolution full-lung computed tomography (CT), is a major risk factor for chronic obstructive pulmonary disease (COPD). There is growing interest to infer ALR from cardiac CT images, which are widely available in epidemiological cohorts, to investigate the relationship of ALR to severe COVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously, cardiac scans included approximately 2&#x2F;3 of the total lung volume with 5-6x greater slice thickness than high-resolution (HR) full-lung (FL) CT. In this study, we present a novel attention-based Multi-view Swin Transformer to infer FL ALR values from segmented cardiac CT scans. For the supervised training we exploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of Atherosclerosis (MESA). Our network significantly outperforms a proxy direct ALR inference on segmented cardiac CT scans and achieves accuracy and reproducibility comparable with a scan-rescan reproducibility of the FL ALR ground-truth. </p>
<blockquote>
<p>åœ¨é«˜åˆ†è¾¨ç‡å…¨è‚ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å…¨è‚ºå¸æ°”çŠ¶æ€ä¸‹ï¼Œæ°”é“æ ‘è…”ä¸è‚ºå¤§å°ä¹‹æ¯”ï¼ˆALRï¼‰æ˜¯æ…¢æ€§é˜»å¡æ€§è‚ºç–¾ç—…ï¼ˆCOPDï¼‰çš„ä¸»è¦é£é™©å› ç´ ã€‚äººä»¬è¶Šæ¥è¶Šæœ‰å…´è¶£ä»å¿ƒè„CTå›¾åƒä¸­æ¨æ–­ALRï¼Œè¿™äº›å›¾åƒåœ¨æµè¡Œç—…å­¦é˜Ÿåˆ—ä¸­å¹¿æ³›å­˜åœ¨ï¼Œä»¥ç ”ç©¶ALRä¸ä¸¥é‡COVID-19å’ŒSARS-CoV-2æ„ŸæŸ“åçš„æ€¥æ€§åé—ç—‡ï¼ˆPASCï¼‰ä¹‹é—´çš„å…³ç³»ã€‚ä»¥å‰çš„å¿ƒè„æ‰«æåŒ…æ‹¬äº†å¤§çº¦ä¸‰åˆ†ä¹‹äºŒçš„è‚ºä½“ç§¯ï¼Œåˆ‡ç‰‡åšåº¦æ˜¯é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å…¨è‚ºï¼ˆFLï¼‰CTçš„5-6å€ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹åŸºäºæ³¨æ„åŠ›çš„å¤šè§†è§’Swin Transformerï¼Œç”¨äºä»åˆ†å‰²çš„å¿ƒè„CTæ‰«æä¸­æ¨æ–­å…¨è‚ºALRå€¼ã€‚æˆ‘ä»¬åˆ©ç”¨åœ¨å¤šå…ƒç§æ—åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–ç ”ç©¶ï¼ˆMESAï¼‰ä¸­è·å¾—çš„å…¨è‚ºå’Œå¿ƒè„CTé…å¯¹å›¾åƒè¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒã€‚æˆ‘ä»¬çš„ç½‘ç»œåœ¨åˆ†å‰²çš„å¿ƒè„CTæ‰«æä¸Šçš„ç›´æ¥ALRæ¨æ–­è¡¨ç°æ˜¾è‘—ä¼˜è¶Šï¼Œå¹¶ä¸”å…¶å‡†ç¡®æ€§å’Œå¯é‡å¤æ€§å¯ä¸å…¨è‚ºALRçœŸå®å€¼çš„æ‰«æ-é‡æ–°æ‰«æå¯é‡å¤æ€§ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08902v1">PDF</a> Accepted to appear in Proceedings of International Symposium on   Biomedical Imaging (ISBI), 2025</p>
<p><strong>æ‘˜è¦</strong><br>     åˆ©ç”¨é«˜åˆ†è¾¨ç‡å…¨è‚ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨å®Œå…¨å¸æ°”çŠ¶æ€ä¸‹è¯„ä¼°çš„æ°”é“æ ‘è…”ä¸è‚ºå¤§å°ä¹‹æ¯”ï¼ˆALRï¼‰æ˜¯æ…¢æ€§é˜»å¡æ€§è‚ºç–¾ç—…ï¼ˆCOPDï¼‰çš„ä¸»è¦é£é™©å› ç´ ã€‚è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…´è¶£åœ¨äºï¼Œä»åœ¨æµè¡Œç—…å­¦é˜Ÿåˆ—ä¸­å¹¿æ³›å¯ç”¨çš„å¿ƒè„CTå›¾åƒæ¨æ–­ALRï¼Œä»¥ç ”ç©¶ALRä¸ä¸¥é‡COVID-19å’ŒSARS-CoV-2æ„ŸæŸ“åçš„æ€¥æ€§åé—ç—‡ï¼ˆPASCï¼‰ä¹‹é—´çš„å…³ç³»ã€‚å…ˆå‰çš„å¿ƒè„æ‰«æåŒ…æ‹¬å¤§çº¦ä¸‰åˆ†ä¹‹äºŒçš„è‚ºæ€»ä½“ç§¯ï¼Œåˆ‡ç‰‡åšåº¦æ˜¯é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å…¨è‚ºï¼ˆFLï¼‰CTçš„5-6å€ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„å¤šè§†å›¾Swin Transformerï¼Œå¯ä»¥ä»åˆ†æ®µçš„å¿ƒè„CTæ‰«ææ¨æ–­FL ALRå€¼ã€‚æˆ‘ä»¬åˆ©ç”¨åœ¨å¤šå…ƒç§æ—åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–ç ”ç©¶ï¼ˆMESAï¼‰ä¸­é‡‡é›†çš„é…å¯¹å…¨è‚ºå’Œå¿ƒè„CTè¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒã€‚æˆ‘ä»¬çš„ç½‘ç»œåœ¨åˆ†æ®µå¿ƒè„CTæ‰«æä¸Šçš„é—´æ¥ALRæ¨æ–­è¡¨ç°æ˜¾è‘—ä¼˜è¶Šï¼Œå¹¶ä¸”å…¶å‡†ç¡®æ€§å’Œå†ç°æ€§ä¸FL ALRçœŸå®å€¼çš„æ‰«æ-å†æ‰«æå†ç°æ€§ç›¸å½“ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ°”é“æ ‘è…”ä¸è‚ºå¤§å°ä¹‹æ¯”ï¼ˆALRï¼‰æ˜¯æ…¢æ€§é˜»å¡æ€§è‚ºç–¾ç—…ï¼ˆCOPDï¼‰çš„é‡è¦é£é™©å› ç´ ã€‚</li>
<li>å¿ƒè„CTå›¾åƒå¯ç”¨äºæ¨æ–­ALRï¼Œè¿™åœ¨æµè¡Œç—…å­¦ç ”ç©¶ä¸­å…·æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>å…ˆå‰çš„å¿ƒè„CTæ‰«æåªåŒ…æ‹¬å¤§çº¦ä¸‰åˆ†ä¹‹äºŒçš„è‚ºä½“ç§¯ï¼Œä¸”åˆ‡ç‰‡åšåº¦è¾ƒå¤§ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºæ³¨æ„åŠ›çš„å¤šè§†å›¾Swin Transformerï¼Œå¯ä»åˆ†æ®µçš„å¿ƒè„CTæ‰«ææ¨æ–­å…¨è‚ºALRå€¼ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨é…å¯¹å…¨è‚ºå’Œå¿ƒè„CTè¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒã€‚</li>
<li>è¯¥ç½‘ç»œåœ¨é—´æ¥ALRæ¨æ–­ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…¶æ€§èƒ½å’Œå‡†ç¡®æ€§å¯ä¸å…¨è‚ºCTæ‰«æçš„æ‰«æ-å†æ‰«æç»“æœç›¸æ¯”ã€‚</li>
<li>è¿™ç§æ–¹æ³•ä¸ºé€šè¿‡å¿ƒè„CTè¯„ä¼°ALRæä¾›äº†ä¸€ç§æ–°çš„å¯èƒ½é€”å¾„ï¼Œæœ‰åŠ©äºç ”ç©¶ALRä¸ä¸¥é‡COVID-19å’ŒPASCä¹‹é—´çš„å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13bf05f4e4b467af7e63fb57fcf9a047.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93c0b238c3e80bae67e8abb7e9f2d89e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cddbc4b7bc2822f8b8445a52ebec5608.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cyclical-accretion-regime-change-in-the-slow-X-ray-pulsar-4U-0114-65-observed-with-Chandra"><a href="#Cyclical-accretion-regime-change-in-the-slow-X-ray-pulsar-4U-0114-65-observed-with-Chandra" class="headerlink" title="Cyclical accretion regime change in the slow X-ray pulsar 4U 0114+65   observed with Chandra"></a>Cyclical accretion regime change in the slow X-ray pulsar 4U 0114+65   observed with Chandra</h2><p><strong>Authors:Graciela Sanjurjo-FerrÃ­n, Jose Miguel TorrejÃ³n, Konstantin Postnov, Michael Nowak, Jose JoaquÃ­n Rodes-Roca, Lida Oskinova, Jessica Planelles-Villalva, Norber Schulz</strong></p>
<p>4U 0114+65 is a high-mass X-ray binary system formed by the luminous supergiant B1Ia, known as V{*} V662 Cas, and one of the slowest rotating neutron stars (NS) with a spin period of about 2.6 hours. This fact provides a rare opportunity to study interesting details of the accretion within each individual pulse of the compact object. In this paper, we analyze 200 ks of Chandra grating data, divided into 9 uninterrupted observations around the orbit. The changes in the circumstellar absorption column through the orbit suggest an orbital inclination of $\sim$ $40^{\circ}$ with respect to the observer and a companion mass-loss rate of $\sim$ 8.6 10$^{-7}$ solar masses yr$^{-1}$. The peaks of the NS pulse show a large pulse-to-pulse variability. Three of them show an evolution from a brighter regime to a weaker one. We propose that the efficiency of Compton cooling in this source fluctuates throughout an accumulation cycle. After significant depletion of matter within the magnetosphere, since the settling velocity is $\sim \times$ 2 times lower than the free-fall velocity, the source gradually accumulates matter until the density exceeds a critical threshold. This increase in density triggers a transition to a more efficient Compton cooling regime, leading to a higher mass accretion rate and consequently to an increased brightness. </p>
<blockquote>
<p>4U 0114+65æ˜¯ä¸€ä¸ªé«˜è´¨é‡Xå°„çº¿åŒæ˜Ÿç³»ç»Ÿï¼Œç”±æ˜äº®çš„è¶…å·¨æ˜ŸB1Iaï¼ˆä¹Ÿç§°ä¸ºV{*} V662 Casï¼‰å’Œè‡ªè½¬å‘¨æœŸçº¦ä¸º2.6å°æ—¶çš„ä¸­å­æ˜Ÿï¼ˆNSï¼‰ç»„æˆã€‚è¿™ä¸€äº‹å®æä¾›äº†ä¸€ä¸ªéš¾å¾—çš„æœºä¼šï¼Œå¯ä»¥ç ”ç©¶ç´§å‡‘ç‰©ä½“æ¯ä¸ªè„‰å†²å†…çš„å¸ç§¯è¿‡ç¨‹çš„æœ‰è¶£ç»†èŠ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†é•¿è¾¾20ä¸‡ç§’çš„Chandraå…‰è°±æ•°æ®ï¼Œè¿™äº›æ•°æ®è¢«åˆ†ä¸ºå›´ç»•è½¨é“çš„9æ¬¡ä¸é—´æ–­è§‚æµ‹ã€‚æ’æ˜Ÿå‘¨å›´å¸æ”¶æŸ±åœ¨è½¨é“ä¸Šçš„å˜åŒ–è¡¨æ˜è½¨é“å€¾è§’çº¦ä¸º$ 40^{\circ}$ç›¸å¯¹äºè§‚å¯Ÿè€…ï¼Œå¹¶ä¸”ä¼´éšå¤©ä½“è´¨é‡æŸå¤±ç‡çº¦ä¸ºæ¯ç§’$ 8.6 \times 10^{-7}$ä¸ªå¤ªé˜³è´¨é‡ã€‚ä¸­å­æ˜Ÿè„‰å†²å³°å€¼æ˜¾ç¤ºå‡ºå·¨å¤§çš„è„‰å†²é—´å˜å¼‚æ€§ã€‚å…¶ä¸­ä¸‰ä¸ªè„‰å†²å³°å€¼è¡¨ç°å‡ºä»è¾ƒäº®çš„çŠ¶æ€å‘è¾ƒå¼±çš„çŠ¶æ€çš„è½¬å˜ã€‚æˆ‘ä»¬æå‡ºï¼Œæ­¤æºçš„åº·æ™®é¡¿å†·å´æ•ˆç‡åœ¨ä¸€ä¸ªç§¯ç´¯å‘¨æœŸå†…å­˜åœ¨æ³¢åŠ¨ã€‚ç”±äºç£å±‚å†…ç‰©è´¨çš„å¤§é‡æ¶ˆè€—ï¼Œç”±äºæ²‰é™é€Ÿåº¦çº¦ä¸ºè‡ªç”±è½ä½“é€Ÿåº¦çš„$\frac{1}{2}$å€ï¼Œç‰©è´¨é€æ¸åœ¨æºå¤„ç§¯ç´¯ï¼Œç›´åˆ°å¯†åº¦è¶…è¿‡ä¸´ç•Œé˜ˆå€¼ã€‚å¯†åº¦çš„å¢åŠ ä¼šå¼•å‘å‘æ›´æœ‰æ•ˆçš„åº·æ™®é¡¿å†·å´çŠ¶æ€çš„è½¬å˜ï¼Œä»è€Œå¯¼è‡´æ›´é«˜çš„ç‰©è´¨å¸ç§¯ç‡ï¼Œè¿›è€Œå¯¼è‡´äº®åº¦å¢åŠ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08702v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æè¿°äº†é«˜è´¨é‡Xå°„çº¿åŒæ˜Ÿç³»ç»Ÿ4U 0114+65çš„ç ”ç©¶ç»“æœï¼Œè¯¥ç³»ç»Ÿç”±è¶…å·¨æ˜ŸB1Iaï¼ˆç§°ä¸ºV{*} V662 Casï¼‰å’Œæœ€æ…¢çš„ä¸­å­æ˜Ÿä¹‹ä¸€ç»„æˆã€‚é€šè¿‡å¯¹è¯¥ç³»ç»Ÿçš„è§‚æµ‹æ•°æ®è¿›è¡Œåˆ†æï¼Œå‘ç°å…¶è½¨é“å€¾è§’çº¦ä¸º40Â°ï¼Œå¹¶æ¨æµ‹å…¶ä¼´æ˜Ÿè´¨é‡æŸå¤±ç‡çº¦ä¸ºæ¯å¹´æŸå¤±å¤ªé˜³è´¨é‡çš„8.6 x 10^-7å€ã€‚ä¸­å­æ˜Ÿè„‰å†²å³°å€¼è¡¨ç°å‡ºæ˜¾è‘—çš„è„‰å†²é—´å˜åŒ–ï¼Œè¿™äº›å˜åŒ–ä¸æºçš„åº·æ™®é¡¿å†·å´æ•ˆç‡æ³¢åŠ¨æœ‰å…³ã€‚åœ¨ç£å±‚ç‰©è´¨å¤§é‡è€—å°½åï¼Œç”±äºæ²‰é™é€Ÿåº¦çº¦ä¸ºè‡ªç”±è½ä½“é€Ÿåº¦çš„ä¸¤å€ï¼Œç‰©è´¨é€æ¸ç§¯ç´¯ç›´è‡³å¯†åº¦è¶…è¿‡ä¸´ç•Œé˜ˆå€¼ï¼Œè§¦å‘å‘æ›´æœ‰æ•ˆçš„åº·æ™®é¡¿å†·å´æœºåˆ¶çš„è½¬å˜ï¼Œå¯¼è‡´æ›´é«˜çš„ç‰©è´¨ç§¯ç´¯ç‡å’Œç›¸åº”çš„äº®åº¦å¢åŠ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4U 0114+65æ˜¯ä¸€ä¸ªç”±è¶…å·¨æ˜ŸB1Iaï¼ˆV{*} V662 Casï¼‰å’Œæœ€æ…¢æ—‹è½¬çš„ä¸­å­æ˜Ÿä¹‹ä¸€ç»„æˆçš„åŒæ˜Ÿç³»ç»Ÿï¼Œä¸ºç ”ç©¶è„‰å†²æ˜Ÿä½“çš„ç»†èŠ‚æä¾›äº†ç½•è§çš„æœºä¼šã€‚</li>
<li>é€šè¿‡åˆ†æè¯¥ç³»ç»Ÿé•¿è¾¾è¿‘ä¸¤å¹´çš„è¿ç»­è§‚æµ‹æ•°æ®ï¼Œå‘ç°è½¨é“å€¾è§’çº¦ä¸º40Â°ï¼Œæ¨æµ‹ä¼´æ˜Ÿçš„è´¨é‡æŸå¤±ç‡ä¸ºæ¯å¹´æŸå¤±å¤ªé˜³è´¨é‡çš„ç‰¹å®šå€¼ã€‚è¿™ä¸€å‘ç°å¯¹äºç†è§£åŒæ˜Ÿç³»ç»Ÿçš„æ¼”åŒ–å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ä¸­å­æ˜Ÿè„‰å†²å³°å€¼æ˜¾ç¤ºæ˜¾è‘—å˜åŒ–ã€‚è¿™ç§å˜åŒ–è¢«è®¤ä¸ºæ˜¯ç”±äºè¯¥ç³»ç»Ÿä¸­åº·æ™®é¡¿å†·å´æ•ˆç‡æ³¢åŠ¨å¯¼è‡´çš„ï¼Œè¿™å¯èƒ½è¿›ä¸€æ­¥æ­ç¤ºäº†ç‰©è´¨ç´¯ç§¯è¿‡ç¨‹çš„å¤æ‚æ€§ã€‚</li>
<li>åœ¨ç£å±‚ç‰©è´¨å¤§é‡è€—å°½åï¼Œç³»ç»Ÿçš„è¡Œä¸ºå‘ç”Ÿå˜åŒ–ï¼Œè¡¨æ˜ç‰©è´¨çš„ç§¯ç´¯ä¸å¯†åº¦é˜ˆå€¼ä¹‹é—´å­˜åœ¨å…³è”ã€‚å½“å¯†åº¦è¶…è¿‡ä¸´ç•Œé˜ˆå€¼æ—¶ï¼Œç³»ç»Ÿä¼šç»å†ä¸€ç§å‘æ›´æœ‰æ•ˆçš„å†·å´æœºåˆ¶è½¬å˜çš„è¿‡ç¨‹ã€‚è¿™ä¸€å‘ç°å¯¹äºç†è§£è„‰å†²æ˜Ÿçš„ç‰©ç†ç‰¹æ€§è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfffb4ed18ca26b1f50659b22e854327.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-316e4b960d69f998fbdb66e1d096c02d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cba12fc7b70e6d251d246223175185f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f5b407a2baba0444297398b6c7b5cb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db2dee9c8ff4bb8ad06d3ef9f56278a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1036a5b045ba0901cb346534dc88ecff.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TimeFlow-Longitudinal-Brain-Image-Registration-and-Aging-Progression-Analysis"><a href="#TimeFlow-Longitudinal-Brain-Image-Registration-and-Aging-Progression-Analysis" class="headerlink" title="TimeFlow: Longitudinal Brain Image Registration and Aging Progression   Analysis"></a>TimeFlow: Longitudinal Brain Image Registration and Aging Progression   Analysis</h2><p><strong>Authors:Bailiang Jian, Jiazhen Pan, Yitong Li, Fabian Bongratz, Ruochen Li, Daniel Rueckert, Benedikt Wiestler, Christian Wachinger</strong></p>
<p>Predicting future brain states is crucial for understanding healthy aging and neurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone for such analyses, has long been limited by its inability to forecast future developments, reliance on extensive, dense longitudinal data, and the need to balance registration accuracy with temporal smoothness. In this work, we present \emph{TimeFlow}, a novel framework for longitudinal brain MRI registration that overcomes all these challenges. Leveraging a U-Net architecture with temporal conditioning inspired by diffusion models, TimeFlow enables accurate longitudinal registration and facilitates prospective analyses through future image prediction. Unlike traditional methods that depend on explicit smoothness regularizers and dense sequential data, TimeFlow achieves temporal consistency and continuity without these constraints. Experimental results highlight its superior performance in both future timepoint prediction and registration accuracy compared to state-of-the-art methods. Additionally, TimeFlow supports novel biological brain aging analyses, effectively differentiating neurodegenerative conditions from healthy aging. It eliminates the need for segmentation, thereby avoiding the challenges of non-trivial annotation and inconsistent segmentation errors. TimeFlow paves the way for accurate, data-efficient, and annotation-free prospective analyses of brain aging and chronic diseases. </p>
<blockquote>
<p>é¢„æµ‹æœªæ¥çš„å¤§è„‘çŠ¶æ€å¯¹äºç†è§£å¥åº·è€åŒ–å’Œç¥ç»é€€è¡Œæ€§ç–¾ç—…è‡³å…³é‡è¦ã€‚çºµå‘è„‘MRIæ³¨å†Œæ˜¯æ­¤ç±»åˆ†æçš„æ ¸å¿ƒï¼Œé•¿æœŸä»¥æ¥ä¸€ç›´å—é™äºå…¶æ— æ³•é¢„æµ‹æœªæ¥å‘å±•ã€ä¾èµ–å¤§é‡å¯†é›†çš„çºµå‘æ•°æ®ä»¥åŠéœ€è¦åœ¨æ³¨å†Œç²¾åº¦ä¸æ—¶é—´å¹³æ»‘ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†\emph{TimeFlow}è¿™ä¸€å…¨æ–°çš„çºµå‘è„‘MRIæ³¨å†Œæ¡†æ¶ï¼Œå…‹æœäº†ä¸Šè¿°æ‰€æœ‰æŒ‘æˆ˜ã€‚TimeFlowåˆ©ç”¨å—æ‰©æ•£æ¨¡å‹å¯å‘çš„U-Netæ¶æ„è¿›è¡Œæ—¶é—´æ¡ä»¶å¤„ç†ï¼Œèƒ½å¤Ÿå®ç°å‡†ç¡®çš„çºµå‘æ³¨å†Œï¼Œå¹¶é€šè¿‡æœªæ¥å›¾åƒé¢„æµ‹è¿›è¡Œå‰ç»æ€§åˆ†æã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒTimeFlowæ— éœ€ä¾èµ–æ˜ç¡®çš„å¹³æ»‘æ­£åˆ™å™¨å’Œå¯†é›†çš„åºåˆ—æ•°æ®å³å¯å®ç°æ—¶é—´çš„ä¸€è‡´æ€§å’Œè¿ç»­æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒTimeFlowåœ¨æœªæ¥æ—¶é—´ç‚¹é¢„æµ‹å’Œæ³¨å†Œç²¾åº¦æ–¹é¢éƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒTimeFlowæ”¯æŒæ–°å‹è„‘è¡°è€ç”Ÿç‰©å­¦åˆ†æï¼Œå¯æœ‰æ•ˆåŒºåˆ†ç¥ç»é€€è¡Œæ€§ç–¾ç—…ä¸å¥åº·è¡°è€ã€‚å®ƒæ— éœ€åˆ†å‰²ï¼Œä»è€Œé¿å…äº†éå¹³å‡¡æ ‡æ³¨å’Œä¸ä¸€è‡´çš„åˆ†å‰²é”™è¯¯æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚TimeFlowä¸ºå‡†ç¡®ã€é«˜æ•ˆä¸”æ— æ ‡æ³¨çš„å‰ç»æ€§åˆ†æè„‘è¡°è€å’Œæ…¢æ€§ç—…é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTimeFlowçš„æ–°å‹çºµå‘è„‘MRIæ³¨å†Œæ¡†æ¶ï¼Œå®ƒå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå®ç°äº†å‡†ç¡®çš„çºµå‘æ³¨å†Œå’Œæœªæ¥å›¾åƒé¢„æµ‹ã€‚TimeFlowé‡‡ç”¨U-Netæ¶æ„ï¼Œå—æ‰©æ•£æ¨¡å‹å¯å‘è¿›è¡Œæ—¶é—´æ¡ä»¶å¤„ç†ï¼Œå¯åœ¨æ— éœ€æ˜ç¡®å¹³æ»‘æ­£åˆ™å™¨å’Œå¯†é›†åºåˆ—æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ—¶é—´è¿è´¯æ€§å’Œè¿ç»­æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeFlowåœ¨æœªæ¥æ—¶é—´ç‚¹é¢„æµ‹å’Œæ³¨å†Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶æ”¯æŒæ–°å‹ç”Ÿç‰©è„‘è¡°è€åˆ†æï¼Œå¯åŒºåˆ†ç¥ç»é€€è¡Œæ€§ç–¾ç—…å’Œå¥åº·è¡°è€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TimeFlowæ˜¯ä¸€ä¸ªæ–°å‹çš„çºµå‘è„‘MRIæ³¨å†Œæ¡†æ¶ï¼Œèƒ½å¤Ÿé¢„æµ‹æœªæ¥çš„å¤§è„‘çŠ¶æ€ã€‚</li>
<li>å®ƒå…‹æœäº†ç°æœ‰æ–¹æ³•çš„æŒ‘æˆ˜ï¼Œå¦‚æ— æ³•é¢„æµ‹æœªæ¥å‘å±•ã€å¯¹å¯†é›†çºµå‘æ•°æ®ä¾èµ–ä»¥åŠéœ€è¦åœ¨æ³¨å†Œç²¾åº¦å’Œæ—¶é—´å¹³æ»‘ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>TimeFlowé‡‡ç”¨U-Netæ¶æ„ï¼Œç»“åˆæ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ¡ä»¶ï¼Œå®ç°äº†å‡†ç¡®ä¸”è¿è´¯çš„çºµå‘æ³¨å†Œã€‚</li>
<li>è¯¥æ–¹æ³•å¯åœ¨æ— éœ€æ˜ç¡®å¹³æ»‘æ­£åˆ™å™¨å’Œå¯†é›†åºåˆ—æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ—¶é—´è¿è´¯æ€§å’Œè¿ç»­æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeFlowåœ¨æœªæ¥æ—¶é—´ç‚¹é¢„æµ‹å’Œæ³¨å†Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>TimeFlowæ”¯æŒæ–°å‹ç”Ÿç‰©è„‘è¡°è€åˆ†æï¼Œå¹¶èƒ½åŒºåˆ†ç¥ç»é€€è¡Œæ€§ç–¾ç—…å’Œå¥åº·è¡°è€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-490c338a9ca4eec8f1a45b15fc21f1df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40e1282e06ed9a6f74997f1a4161a68c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8796fa25580be129414d5561f9fef265.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Pressure-induced-topological-changes-in-Fermi-surface-of-two-dimensional-molecular-conductor"><a href="#Pressure-induced-topological-changes-in-Fermi-surface-of-two-dimensional-molecular-conductor" class="headerlink" title="Pressure-induced topological changes in Fermi surface of two-dimensional   molecular conductor"></a>Pressure-induced topological changes in Fermi surface of two-dimensional   molecular conductor</h2><p><strong>Authors:T. Kobayashi, K. Yoshimi, H. Ma, S. Sekine, H. Taniguchi, N. Matsunaga, A. Kawamoto, Y. Uwatoko</strong></p>
<p>We demonstrated X-ray structural analysis of the pressure-induced superconductor, $\betaâ€™$-ET$_2$ICl$<em>2$ under extremely high-pressure conditions, where ET denotes bis(ethylenedithio)tetrathiafulvalene. This material has been known as the highest transition temperature ($T_c$) superconductor among organic superconductors ($T_c&#x3D;14.2$ K at $8.2$ GPa). On the basis of the experimental results, ab-initio models were derived using the constrained random phase approximation. We revealed that the Lifshitz transition exists behind the Mott insulator-metal transition and found that the value of the on-site Coulomb interaction was halved to around $10$ GPa compared to that at ambient pressure. This study clarifies the enigmatic origins of high $T</em>{\rm c}$, and concurrently, provides a new understanding of the impacts of structural alterations in organic materials under high pressure on their electronic properties and the superconductivity process. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹å‹åŠ›è¯±å¯¼çš„è¶…å¯¼ä½“Î²â€™-ETâ‚‚IClâ‚‚è¿›è¡Œäº†Xå°„çº¿ç»“æ„åˆ†æï¼Œå…¶ä¸­ETä»£è¡¨åŒï¼ˆä¹™çƒ¯äºŒç¡«ä»£ï¼‰å››ç¡«å¯Œç“¦çƒ¯ã€‚è¿™ç§ææ–™åœ¨æœ‰æœºè¶…å¯¼ä½“ä¸­å…·æœ‰æœ€é«˜çš„è½¬å˜æ¸©åº¦ï¼ˆTcï¼‰ï¼Œåœ¨8.2 GPaæ—¶çš„Tcä¸º14.2 Kã€‚æ ¹æ®å®éªŒç»“æœï¼Œæˆ‘ä»¬ä½¿ç”¨çº¦æŸéšæœºç›¸ä½é€¼è¿‘æ¨å¯¼å‡ºäº†ä»å¤´æ¨¡å‹ã€‚æˆ‘ä»¬æ­ç¤ºäº†Lifshitzè½¬å˜å­˜åœ¨äºMottç»ç¼˜ä½“é‡‘å±è½¬å˜ä¹‹åï¼Œå¹¶å‘ç°ç°åœºåº“ä»‘ç›¸äº’ä½œç”¨å€¼åœ¨çº¦10 GPaæ—¶æ¯”ç¯å¢ƒå‹åŠ›ä¸‹çš„å€¼å‡å°‘äº†ä¸€åŠã€‚è¿™é¡¹ç ”ç©¶é˜æ˜äº†é«˜Tcçš„ç¥ç§˜èµ·æºï¼ŒåŒæ—¶æä¾›äº†æ–°çš„ç†è§£ï¼šåœ¨é«˜å‹åŠ›ä¸‹æœ‰æœºææ–™çš„ç»“æ„å˜åŒ–å¯¹å…¶ç”µå­ç‰¹æ€§å’Œè¶…å¯¼è¿‡ç¨‹çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08635v1">PDF</a> 8 pages, 4 figures; Supplemental Material: 9 pages, 6 figures,   accepted for publication in Phys. Rev. Materials (Letter)</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶å¯¹é«˜å‹ä¸‹å‹åŠ›è¯±å¯¼çš„è¶…å¯¼ä½“Î²â€™-ETâ‚‚IClâ‚‚ï¼ˆå…¶ä¸­ETä»£è¡¨åŒï¼ˆä¹™äºŒç¡«ä»£ï¼‰å››ç¡«å¯Œç“¦çƒ¯ï¼‰è¿›è¡Œäº†Xå°„çº¿ç»“æ„åˆ†æã€‚è¯¥ææ–™åœ¨æœ‰æœºè¶…å¯¼ä½“ä¸­å…·æœ‰æœ€é«˜çš„è½¬å˜æ¸©åº¦ï¼ˆTcï¼‰ã€‚åŸºäºå®éªŒç»“æœï¼Œé‡‡ç”¨çº¦æŸéšæœºç›¸ä½è¿‘ä¼¼æ³•æ¨å¯¼äº†ä»å¤´ç®—æ¨¡å‹ã€‚ç ”ç©¶æ­ç¤ºäº†Lifshitzè½¬å˜å­˜åœ¨äºMottç»ç¼˜ä½“-é‡‘å±è½¬å˜ä¹‹åï¼Œå¹¶å‘ç°ä¸å¸¸å‹ç›¸æ¯”ï¼Œåœ¨å¤§çº¦10 GPaæ—¶ï¼Œåœ¨ä½åº“ä»‘ç›¸äº’ä½œç”¨å€¼å‡åŠã€‚è¿™é¡¹ç ”ç©¶é˜æ˜äº†é«˜Tcçš„å¥¥ç§˜ï¼ŒåŒæ—¶æä¾›äº†æ–°çš„ç†è§£ï¼šåœ¨é«˜å‹ä¸‹æœ‰æœºææ–™çš„ç»“æ„å˜åŒ–å¯¹å…¶ç”µå­å±æ€§å’Œè¶…å¯¼è¿‡ç¨‹çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è¿›è¡Œäº†é«˜å‹ä¸‹è¶…å¯¼ä½“Î²â€™-ETâ‚‚IClâ‚‚çš„Xå°„çº¿ç»“æ„åˆ†æã€‚</li>
<li>è¯¥ææ–™åœ¨æœ‰æœºè¶…å¯¼ä½“ä¸­å±•ç°å‡ºæœ€é«˜çš„è½¬å˜æ¸©åº¦ï¼ˆTcï¼‰ã€‚</li>
<li>é€šè¿‡çº¦æŸéšæœºç›¸ä½è¿‘ä¼¼æ³•æ¨å¯¼äº†å®éªŒç»“æœçš„ä»å¤´ç®—æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†Lifshitzè½¬å˜ä¸Mottç»ç¼˜ä½“-é‡‘å±è½¬å˜ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>åœ¨çº¦10 GPaçš„å‹åŠ›ä¸‹ï¼Œåœ¨ä½åº“ä»‘ç›¸äº’ä½œç”¨å€¼ç›¸æ¯”å¸¸å‹æ¡ä»¶å‡åŠã€‚</li>
<li>ç ”ç©¶é˜æ˜äº†é«˜å‹ä¸‹æœ‰æœºææ–™ç»“æ„å˜åŒ–å¯¹ç”µå­å±æ€§å’Œè¶…å¯¼è¿‡ç¨‹çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-89d0d0a5676a4ffe4494f4ee07b567e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6e85d5a3ff499b8a1e39d173b69dce6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95649a5e0f7dde7e0fe3847bea457f13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddea445648fbebaeeb46213d3b229118.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Densely-Connected-Parameter-Efficient-Tuning-for-Referring-Image-Segmentation"><a href="#Densely-Connected-Parameter-Efficient-Tuning-for-Referring-Image-Segmentation" class="headerlink" title="Densely Connected Parameter-Efficient Tuning for Referring Image   Segmentation"></a>Densely Connected Parameter-Efficient Tuning for Referring Image   Segmentation</h2><p><strong>Authors:Jiaqi Huang, Zunnan Xu, Ting Liu, Yong Liu, Haonan Han, Kehong Yuan, Xiu Li</strong></p>
<p>In the domain of computer vision, Parameter-Efficient Tuning (PET) is increasingly replacing the traditional paradigm of pre-training followed by full fine-tuning. PET is particularly favored for its effectiveness in large foundation models, as it streamlines transfer learning costs and optimizes hardware utilization. However, the current PET methods are mainly designed for single-modal optimization. While some pioneering studies have undertaken preliminary explorations, they still remain at the level of aligned encoders (e.g., CLIP) and lack exploration of misaligned encoders. These methods show sub-optimal performance with misaligned encoders, as they fail to effectively align the multimodal features during fine-tuning. In this paper, we introduce DETRIS, a parameter-efficient tuning framework designed to enhance low-rank visual feature propagation by establishing dense interconnections between each layer and all preceding layers, which enables effective cross-modal feature interaction and adaptation to misaligned encoders. We also suggest using text adapters to improve textual features. Our simple yet efficient approach greatly surpasses state-of-the-art methods with 0.9% to 1.8% backbone parameter updates, evaluated on challenging benchmarks. Our project is available at \url{<a target="_blank" rel="noopener" href="https://github.com/jiaqihuang01/DETRIS%7D">https://github.com/jiaqihuang01/DETRIS}</a>. </p>
<blockquote>
<p>åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œå‚æ•°é«˜æ•ˆè°ƒæ•´ï¼ˆPETï¼‰æ­£è¶Šæ¥è¶Šå¤šåœ°å–ä»£ä¼ ç»Ÿçš„é¢„è®­ç»ƒåè¿›è¡Œå…¨é¢å¾®è°ƒçš„æ¨¡å¼ã€‚PETå› å…¶åœ¨å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§è€Œå—åˆ°é’çï¼Œå®ƒèƒ½ç®€åŒ–è¿ç§»å­¦ä¹ æˆæœ¬å¹¶ä¼˜åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚ç„¶è€Œï¼Œå½“å‰çš„PETæ–¹æ³•ä¸»è¦æ˜¯ä¸ºå•ä¸€æ¨¡æ€ä¼˜åŒ–è®¾è®¡çš„ã€‚è™½ç„¶ä¸€äº›å¼€åˆ›æ€§çš„ç ”ç©¶å·²ç»è¿›è¡Œäº†åˆæ­¥çš„æ¢ç´¢ï¼Œä½†å®ƒä»¬ä»ç„¶åœç•™åœ¨å¯¹é½ç¼–ç å™¨çš„å±‚é¢ï¼ˆä¾‹å¦‚CLIPï¼‰ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹æœªå¯¹é½ç¼–ç å™¨çš„æ¢ç´¢ã€‚è¿™äº›æ–¹æ³•åœ¨é¢ä¸´æœªå¯¹é½ç¼–ç å™¨æ—¶è¡¨ç°å‡ºæ€§èƒ½ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°å¯¹é½å¤šæ¨¡æ€ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DETRISï¼Œè¿™æ˜¯ä¸€ä¸ªå‚æ•°é«˜æ•ˆè°ƒæ•´æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨å„å±‚å’Œæ‰€æœ‰å‰å±‚ä¹‹é—´å»ºç«‹å¯†é›†è¿æ¥æ¥å¢å¼ºä½ç§©è§†è§‰ç‰¹å¾ä¼ æ’­ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„è·¨æ¨¡æ€ç‰¹å¾äº¤äº’å’Œé€‚åº”æœªå¯¹é½çš„ç¼–ç å™¨ã€‚æˆ‘ä»¬è¿˜å»ºè®®ä½¿ç”¨æ–‡æœ¬é€‚é…å™¨æ¥æ”¹å–„æ–‡æœ¬ç‰¹å¾ã€‚æˆ‘ä»¬ç®€å•è€Œé«˜æ•ˆçš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»¥æ›´æ–°0.9%è‡³1.8%çš„ä¸»å¹²å‚æ•°å¤§å¤§è¶…è¶Šäº†æœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯é€šè¿‡ç½‘å€<a target="_blank" rel="noopener" href="https://github.com/jiaqihuang01/DETRIS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jiaqihuang01/DETRISè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08580v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>å‚æ•°é«˜æ•ˆè°ƒæ•´ï¼ˆPETï¼‰æ–¹æ³•æ­£åœ¨æ”¹å˜è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„é¢„è®­ç»ƒåŠ å…¨å¾®è°ƒçš„ä¼ ç»Ÿæ¨¡å¼ã€‚PETç‰¹åˆ«é€‚ç”¨äºå¤§å‹åŸºç¡€æ¨¡å‹ï¼Œå› ä¸ºå®ƒèƒ½ä¼˜åŒ–ç¡¬ä»¶ä½¿ç”¨å¹¶é™ä½è¿ç§»å­¦ä¹ æˆæœ¬ã€‚ç„¶è€Œï¼Œå½“å‰PETæ–¹æ³•ä¸»è¦é’ˆå¯¹å•æ¨¡æ€ä¼˜åŒ–è¿›è¡Œè®¾è®¡ã€‚å°½ç®¡å·²æœ‰åˆæ­¥æ¢ç´¢ï¼Œä½†å®ƒä»¬ä¸»è¦åœç•™åœ¨å¯¹é½ç¼–ç å™¨ï¼ˆå¦‚CLIPï¼‰çš„å±‚é¢ï¼Œå¹¶æœªæ¢ç´¢é”™é…ç¼–ç å™¨ã€‚å½“ä½¿ç”¨é”™é…ç¼–ç å™¨æ—¶ï¼Œè¿™äº›æ–¹æ³•è¡¨ç°ä¸ä½³ï¼Œæ— æ³•æœ‰æ•ˆå¯¹é½å¤šæ¨¡æ€ç‰¹å¾è¿›è¡Œå¾®è°ƒã€‚æœ¬æ–‡ä»‹ç»DETRISï¼Œä¸€ç§å‚æ•°é«˜æ•ˆè°ƒæ•´æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨å„å±‚ä¸æ‰€æœ‰å‰ç½®å±‚ä¹‹é—´å»ºç«‹å¯†é›†äº’è”ï¼Œå¢å¼ºä½ç§©è§†è§‰ç‰¹å¾ä¼ æ’­ï¼Œå®ç°è·¨æ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆäº¤äº’å¹¶é€‚åº”é”™é…ç¼–ç å™¨ã€‚æœ¬æ–‡è¿˜å»ºè®®ä½¿ç”¨æ–‡æœ¬é€‚é…å™¨æ¥æ”¹å–„æ–‡æœ¬ç‰¹å¾ã€‚é€šè¿‡ç®€å•çš„ç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å¤§å¹…è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œä»…éœ€æ›´æ–°0.9%è‡³1.8%çš„ä¸»å¹²å‚æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PETæ–¹æ³•æ­£åœ¨æ”¹å˜è®¡ç®—æœºè§†è§‰çš„ä¼ ç»Ÿè®­ç»ƒæ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹åŸºç¡€æ¨¡å‹ä¸­ã€‚</li>
<li>å½“å‰PETæ–¹æ³•ä¸»è¦é¢å‘å•æ¨¡æ€ä¼˜åŒ–ï¼Œå¯¹äºé”™é…ç¼–ç å™¨çš„æ€§èƒ½è¡¨ç°ä¸ä½³ã€‚</li>
<li>DETRISæ¡†æ¶é€šè¿‡å¯†é›†äº’è”å¢å¼ºä½ç§©è§†è§‰ç‰¹å¾ä¼ æ’­ï¼Œæœ‰æ•ˆå®ç°è·¨æ¨¡æ€ç‰¹å¾äº¤äº’ã€‚</li>
<li>DETRISèƒ½é€‚åº”é”™é…ç¼–ç å™¨ï¼Œæé«˜äº†æ–¹æ³•çš„é²æ£’æ€§ã€‚</li>
<li>ä½¿ç”¨æ–‡æœ¬é€‚é…å™¨æ¥æ”¹å–„æ–‡æœ¬ç‰¹å¾çš„æ–¹æ³•è¢«æå‡ºã€‚</li>
<li>DETRISæ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5e0c12f1e35dc7a0e0249a9d0e12ada5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5debf1e09a4bc5f66cfad44dc7f3d71f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcbd58fdaedddbb6c612156dc14f2783.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e13981fd3ba89bb6b5c6935b2c0fb2fb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MIAFEx-An-Attention-based-Feature-Extraction-Method-for-Medical-Image-Classification"><a href="#MIAFEx-An-Attention-based-Feature-Extraction-Method-for-Medical-Image-Classification" class="headerlink" title="MIAFEx: An Attention-based Feature Extraction Method for Medical Image   Classification"></a>MIAFEx: An Attention-based Feature Extraction Method for Medical Image   Classification</h2><p><strong>Authors:Oscar Ramos-Soto, Jorge Ramos-Frutos, Ezequiel Perez-Zarate, Diego Oliva, Sandra E. Balderas-Mata</strong></p>
<p>Feature extraction techniques are crucial in medical image classification; however, classical feature extractors in addition to traditional machine learning classifiers often exhibit significant limitations in providing sufficient discriminative information for complex image sets. While Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown promise in feature extraction, they are prone to overfitting due to the inherent characteristics of medical imaging data, including small sample sizes or high intra-class variance. In this work, the Medical Image Attention-based Feature Extractor (MIAFEx) is proposed, a novel method that employs a learnable refinement mechanism to enhance the classification token within the Transformer encoder architecture. This mechanism adjusts the token based on learned weights, improving the extraction of salient features and enhancing the modelâ€™s adaptability to the challenges presented by medical imaging data. The MIAFEx output features quality is compared against classical feature extractors using traditional and hybrid classifiers. Also, the performance of these features is compared against modern CNN and ViT models in classification tasks, demonstrating its superiority in accuracy and robustness across multiple complex classification medical imaging datasets. This advantage is particularly pronounced in scenarios with limited training data, where traditional and modern models often struggle to generalize effectively. The source code of this proposal can be found at <a target="_blank" rel="noopener" href="https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx">https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx</a> </p>
<blockquote>
<p>ç‰¹å¾æå–æŠ€æœ¯åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé™¤äº†ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨å¤–ï¼Œç»å…¸çš„ç‰¹å¾æå–å™¨åœ¨ä¸ºå¤æ‚çš„å›¾åƒé›†æä¾›è¶³å¤Ÿçš„é‰´åˆ«ä¿¡æ¯æ—¶å¾€å¾€å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰åœ¨ç‰¹å¾æå–æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç”±äºåŒ»å­¦æˆåƒæ•°æ®å›ºæœ‰çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬æ ·æœ¬é‡å°æˆ–ç±»å†…æ–¹å·®é«˜ï¼Œå®ƒä»¬å®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæå‡ºäº†ä¸€ç§åŒ»å­¦å›¾åƒæ³¨æ„åŠ›ç‰¹å¾æå–å™¨ï¼ˆMIAFExï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨å¯å­¦ä¹ ç»†åŒ–æœºåˆ¶çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºTransformerç¼–ç å™¨æ¶æ„ä¸­çš„åˆ†ç±»ä»¤ç‰Œã€‚è¿™ç§æœºåˆ¶æ ¹æ®å­¦ä¹ åˆ°çš„æƒé‡è°ƒæ•´ä»¤ç‰Œï¼Œæé«˜äº†æ˜¾è‘—ç‰¹å¾çš„æå–èƒ½åŠ›ï¼Œå¹¶å¢å¼ºäº†æ¨¡å‹å¯¹åŒ»å­¦æˆåƒæ•°æ®æŒ‘æˆ˜çš„é€‚åº”æ€§ã€‚MIAFExè¾“å‡ºç‰¹å¾è´¨é‡ä¸ä½¿ç”¨ä¼ ç»Ÿå’Œæ··åˆåˆ†ç±»å™¨çš„ç»å…¸ç‰¹å¾æå–å™¨è¿›è¡Œäº†æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œè¿™äº›ç‰¹å¾åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸ç°ä»£çš„CNNå’ŒViTæ¨¡å‹ç›¸æ¯”ä¹Ÿè¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯æ˜å…¶åœ¨å¤šä¸ªå¤æ‚çš„åŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚ç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿå’Œç°ä»£æ¨¡å‹å¾€å¾€éš¾ä»¥æœ‰æ•ˆæ¨å¹¿ï¼ŒMIAFExçš„ä¼˜åŠ¿å°¤ä¸ºçªå‡ºã€‚è¯¥ææ¡ˆçš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFExæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08562v1">PDF</a> In preparation for Journal Submission</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ç‰¹å¾æå–æŠ€æœ¯çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿç‰¹å¾æå–å™¨ç»“åˆä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ†ç±»å™¨åœ¨å¤„ç†å¤æ‚å›¾åƒé›†æ—¶æä¾›è¶³å¤Ÿçš„åˆ¤åˆ«ä¿¡æ¯å­˜åœ¨å±€é™æ€§ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è™½å…·æœ‰æ½œåŠ›ï¼Œä½†ç”±äºåŒ»å­¦æˆåƒæ•°æ®çš„å°æ ·æœ¬é‡æˆ–é«˜ç±»å†…å·®å¼‚ç­‰å›ºæœ‰ç‰¹æ€§ï¼Œå®¹æ˜“å‘ç”Ÿè¿‡æ‹Ÿåˆã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åŒ»å­¦å›¾åƒæ³¨æ„åŠ›ç‰¹å¾æå–å™¨ï¼ˆMIAFExï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨å¯å­¦ä¹ ç»†åŒ–æœºåˆ¶çš„æ–°æ–¹æ³•ï¼Œç”¨äºå¢å¼ºTransformerç¼–ç å™¨æ¶æ„ä¸­çš„åˆ†ç±»ä»¤ç‰Œã€‚è¯¥æœºåˆ¶åŸºäºå­¦ä¹ åˆ°çš„æƒé‡è°ƒæ•´ä»¤ç‰Œï¼Œæé«˜äº†æ˜¾è‘—ç‰¹å¾çš„æå–ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹åŒ»å­¦æˆåƒæ•°æ®æŒ‘æˆ˜çš„é€‚åº”æ€§ã€‚MIAFExçš„ç‰¹å¾è´¨é‡ä¸ä¼ ç»Ÿå’Œæ··åˆåˆ†ç±»å™¨çš„ç»å…¸ç‰¹å¾æå–å™¨è¿›è¡Œäº†æ¯”è¾ƒï¼Œä¹Ÿä¸ç°ä»£CNNå’ŒViTæ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯æ˜å…¶åœ¨å¤šä¸ªå¤æ‚åŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¿™ç§ä¼˜åŠ¿å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ï¼Œç‰¹å¾æå–æŠ€æœ¯éå¸¸é‡è¦ï¼Œä½†ä¼ ç»Ÿç‰¹å¾æå–å™¨å’Œæœºå™¨å­¦ä¹ åˆ†ç±»å™¨åœ¨å¤„ç†å¤æ‚å›¾åƒé›†æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>CNNå’ŒViTåœ¨åŒ»å­¦å›¾åƒç‰¹å¾æå–ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å®¹æ˜“å› æ•°æ®ç‰¹æ€§ï¼ˆå¦‚å°æ ·æœ¬ã€é«˜ç±»å†…å·®å¼‚ï¼‰è€Œè¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>æ–°æå‡ºçš„MIAFExé‡‡ç”¨å¯å­¦ä¹ ç»†åŒ–æœºåˆ¶ï¼Œå¢å¼ºTransformerç¼–ç å™¨ä¸­çš„åˆ†ç±»ä»¤ç‰Œï¼Œæé«˜æ˜¾è‘—ç‰¹å¾æå–ã€‚</li>
<li>MIAFExçš„ç‰¹å¾è´¨é‡åœ¨ä¸å…¶ä»–ä¼ ç»Ÿå’Œç°ä»£æ¨¡å‹æ¯”è¾ƒä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™è®­ç»ƒæ•°æ®çš„åœºæ™¯ä¸‹ã€‚</li>
<li>MIAFExæ¨¡å‹åœ¨å¤šä¸ªå¤æ‚çš„åŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šå±•ç¤ºäº†é«˜å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶çš„æºä»£ç å¯åœ¨æŒ‡å®šé“¾æ¥æ‰¾åˆ°ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶åœ¨åŒ»å­¦å›¾åƒç‰¹å¾æå–ä¸­çš„åº”ç”¨ä¸ºæé«˜æ¨¡å‹æ€§èƒ½å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fa14d15147b642b3ed914029265be1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dae1fb00d4461afb66decad012e72917.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RWKV-UNet-Improving-UNet-with-Long-Range-Cooperation-for-Effective-Medical-Image-Segmentation"><a href="#RWKV-UNet-Improving-UNet-with-Long-Range-Cooperation-for-Effective-Medical-Image-Segmentation" class="headerlink" title="RWKV-UNet: Improving UNet with Long-Range Cooperation for Effective   Medical Image Segmentation"></a>RWKV-UNet: Improving UNet with Long-Range Cooperation for Effective   Medical Image Segmentation</h2><p><strong>Authors:Juntao Jiang, Jiangning Zhang, Weixuan Liu, Muxuan Gao, Xiaobin Hu, Xiaoxiao Yan, Feiyue Huang, Yong Liu</strong></p>
<p>In recent years, there have been significant advancements in deep learning for medical image analysis, especially with convolutional neural networks (CNNs) and transformer models. However, CNNs face limitations in capturing long-range dependencies while transformers suffer high computational complexities. To address this, we propose RWKV-UNet, a novel model that integrates the RWKV (Receptance Weighted Key Value) structure into the U-Net architecture. This integration enhances the modelâ€™s ability to capture long-range dependencies and improve contextual understanding, which is crucial for accurate medical image segmentation. We build a strong encoder with developed inverted residual RWKV (IR-RWKV) blocks combining CNNs and RWKVs. We also propose a Cross-Channel Mix (CCM) module to improve skip connections with multi-scale feature fusion, achieving global channel information integration. Experiments on benchmark datasets, including Synapse, ACDC, BUSI, CVC-ClinicDB, CVC-ColonDB, Kvasir-SEG, ISIC 2017 and GLAS show that RWKV-UNet achieves state-of-the-art performance on various types of medical image segmentation. Additionally, smaller variants, RWKV-UNet-S and RWKV-UNet-T, balance accuracy and computational efficiency, making them suitable for broader clinical applications. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå˜å‹å™¨æ¨¡å‹çš„åº”ç”¨ã€‚ç„¶è€Œï¼ŒCNNåœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè€Œå˜å‹å™¨åˆ™é¢ä¸´è®¡ç®—å¤æ‚æ€§é«˜çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RWKV-UNetæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§å°†RWKVï¼ˆReceptance Weighted Key Valueï¼‰ç»“æ„èå…¥U-Netæ¶æ„çš„æ–°å‹æ¨¡å‹ã€‚è¿™ç§èåˆå¢å¼ºäº†æ¨¡å‹æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å¹¶æé«˜ä¸Šä¸‹æ–‡ç†è§£çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºå‡†ç¡®çš„åŒ»å­¦å›¾åƒåˆ†å‰²è‡³å…³é‡è¦ã€‚æˆ‘ä»¬åˆ©ç”¨å…ˆè¿›çš„å€’æ®‹å·®RWKVï¼ˆIR-RWKVï¼‰å—æ„å»ºäº†ä¸€ä¸ªå¼ºå¤§çš„ç¼–ç å™¨ï¼Œç»“åˆäº†CNNå’ŒRWKVã€‚æˆ‘ä»¬è¿˜æå‡ºäº†è·¨é€šé“æ··åˆï¼ˆCCMï¼‰æ¨¡å—ï¼Œé€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆæ”¹è¿›è·³è¿‡è¿æ¥ï¼Œå®ç°å…¨å±€é€šé“ä¿¡æ¯é›†æˆã€‚åœ¨Synapseã€ACDCã€BUSIã€CVC-ClinicDBã€CVC-ColonDBã€Kvasir-SEGã€ISIC 2017å’ŒGLASç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRWKV-UNetåœ¨å„ç§ç±»å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¾ƒå°çš„å˜ä½“RWKV-UNet-Så’ŒRWKV-UNet-Tå¹³è¡¡äº†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œä½¿å®ƒä»¬æ›´é€‚åˆæ›´å¹¿æ³›çš„ä¸´åºŠåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08458v1">PDF</a> </p>
<p><strong>Summary</strong><br>     èåˆRWKVç»“æ„äºU-Netæ¶æ„ä¸­æå‡ºçš„RWKV-UNetæ¨¡å‹ï¼Œæœ‰æ•ˆæå‡äº†æ•æ‰é•¿æœŸä¾èµ–å…³ç³»åŠç†è§£ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œå¯¹äºç²¾å‡†åŒ»å­¦å›¾åƒåˆ†å‰²è‡³å…³é‡è¦ã€‚é€šè¿‡å¼•å…¥IR-RWKVå—å’ŒCross-Channel Mixæ¨¡å—ç­‰æŠ€æœ¯æ‰‹æ®µï¼Œæ¨¡å‹å±•ç°å‡ºå“è¶Šçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ€§èƒ½ï¼Œå®ç°äº†åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RWKV-UNetæ¨¡å‹ç»“åˆäº†RWKVç»“æ„å’ŒU-Netæ¶æ„ï¼Œå¢å¼ºäº†æ•æ‰é•¿æœŸä¾èµ–å…³ç³»å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å…¥IR-RWKVå—ï¼Œæ„å»ºäº†å¼ºå¤§çš„ç¼–ç å™¨ã€‚</li>
<li>Cross-Channel Mixæ¨¡å—æ”¹å–„äº†è·¨å°ºåº¦ç‰¹å¾çš„èåˆï¼Œå®ç°äº†å…¨å±€é€šé“ä¿¡æ¯é›†æˆã€‚</li>
<li>RWKV-UNetåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹æä¾›äº†å¹³è¡¡å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡çš„è¾ƒå°å˜ä½“ï¼Œå³RWKV-UNet-Så’ŒRWKV-UNet-Tã€‚</li>
<li>RWKV-UNetæ¨¡å‹å¯¹äºå„ç§ç±»å‹åŒ»å­¦å›¾åƒåˆ†å‰²å…·æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4421e2dbcd55b65958dffa70b582b80f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5048719cb31a4b20b7a8d832b7a03fa2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee089821375aa1fd5f7d2131b1c34b90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07dabcea76597467dd2478a6549c89b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bd079f7e1cb2ae183b9ca8cade7a181.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SEAL-Speaker-Error-Correction-using-Acoustic-conditioned-Large-Language-Models"><a href="#SEAL-Speaker-Error-Correction-using-Acoustic-conditioned-Large-Language-Models" class="headerlink" title="SEAL: Speaker Error Correction using Acoustic-conditioned Large Language   Models"></a>SEAL: Speaker Error Correction using Acoustic-conditioned Large Language   Models</h2><p><strong>Authors:Anurag Kumar, Rohit Paturi, Amber Afshan, Sundararajan Srinivasan</strong></p>
<p>Speaker Diarization (SD) is a crucial component of modern end-to-end ASR pipelines. Traditional SD systems, which are typically audio-based and operate independently of ASR, often introduce speaker errors, particularly during speaker transitions and overlapping speech. Recently, language models including fine-tuned large language models (LLMs) have shown to be effective as a second-pass speaker error corrector by leveraging lexical context in the transcribed output. In this work, we introduce a novel acoustic conditioning approach to provide more fine-grained information from the acoustic diarizer to the LLM. We also show that a simpler constrained decoding strategy reduces LLM hallucinations, while avoiding complicated post-processing. Our approach significantly reduces the speaker error rates by 24-43% across Fisher, Callhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD. </p>
<blockquote>
<p>è¯´è¯äººèšç±»ï¼ˆSpeaker Diarizationï¼ŒSDï¼‰æ˜¯ç°ä»£ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ä¼ ç»Ÿçš„SDç³»ç»Ÿé€šå¸¸æ˜¯åŸºäºéŸ³é¢‘çš„ï¼Œç‹¬ç«‹äºASRè¿è¡Œï¼Œå¾€å¾€ä¼šåœ¨è¯´è¯äººè½¬æ¢å’Œé‡å è¯­éŸ³çš„æƒ…å†µä¸‹å¼•å…¥è¯´è¯äººé”™è¯¯ã€‚æœ€è¿‘ï¼Œè¯­è¨€æ¨¡å‹åŒ…æ‹¬å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡åˆ©ç”¨è½¬å½•è¾“å‡ºä¸­çš„è¯æ±‡ä¸Šä¸‹æ–‡ï¼Œè¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ç¬¬ä¸€æ¬¡è¯´è¯äººé”™è¯¯æ ¡æ­£å™¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å£°å­¦æ¡ä»¶æ–¹æ³•ï¼Œä¸ºLLMæä¾›æ›´å¤šç²¾ç»†çš„æ¥è‡ªå£°å­¦èšç±»å™¨çš„ä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ›´ç®€å•çš„çº¦æŸè§£ç ç­–ç•¥å‡å°‘äº†LLMçš„å¹»è§‰ç°è±¡ï¼ŒåŒæ—¶é¿å…äº†å¤æ‚çš„åå¤„ç†è¿‡ç¨‹ã€‚ä¸ç¬¬ä¸€æ¬¡é€šè¿‡çš„å£°å­¦SDç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Fisherã€Callhomeå’ŒRT0subTitle&gt; æ•°æ®é›†ä¸Šå°†è¯´è¯äººé”™è¯¯ç‡é™ä½äº†é«˜è¾¾ç™¾åˆ†ä¹‹äºŒåå››è‡³å››åä¸‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08421v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†æ–°å‹å£°å­¦æ¡ä»¶åŒ–æ–¹æ³•åº”ç”¨äºè¯­éŸ³è¯†åˆ«é¢†åŸŸä¸­çš„è¯´è¯äººè¯†åˆ«ï¼ˆSpeaker Diarization, SDï¼‰ã€‚æ–‡ç« æå‡ºä¸€ç§æ–°çš„å£°å­¦æ¡ä»¶åŒ–ç­–ç•¥æ¥æ”¹è¿›ä¼ ç»Ÿç‹¬ç«‹æ“ä½œçš„ä¼ ç»ŸéŸ³é¢‘ä¸ºåŸºç¡€çš„ä¼ ç»Ÿè¯´è¯äººè¯†åˆ«æŠ€æœ¯çš„é—®é¢˜ï¼Œå¦‚è¯´è¯äººè½¬æ¢å’Œé‡å è¯­éŸ³æ—¶çš„è¯´è¯äººé”™è¯¯ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¬¬äºŒéçº é”™ï¼Œå°†å£°å­¦æ—¥è®°ï¼ˆacoustdic diarizerï¼‰æä¾›çš„ç²¾ç»†ç²’åº¦ä¿¡æ¯æ³¨å…¥å…¶ä¸­ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜æ›´ç®€å•çš„çº¦æŸè§£ç ç­–ç•¥å¯é™ä½LLMå¹»æƒ³ã€‚å®éªŒç»“æœåœ¨æ‰€æœ‰æµ‹è¯•çš„æ•°æ®é›†ä¸Šå‡å°‘äº†å¤šè¾¾é«˜è¾¾43%çš„è¯´è¯äººé”™è¯¯ç‡ã€‚æ€»ä½“æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å£°å­¦æŠ€æœ¯å’Œè¯­è¨€æ¨¡å‹ç´§å¯†ç»“åˆçš„è§£å†³æ–¹æ¡ˆï¼Œå®ç°äº†é«˜æ•ˆå‡†ç¡®çš„è¯´è¯äººè¯†åˆ«ã€‚ </p>
<p><strong>Key Takeaways</strong> </p>
<ol>
<li>ä¼ ç»Ÿè¯´è¯äººè¯†åˆ«æŠ€æœ¯åœ¨å¤„ç†éŸ³é¢‘æ—¶å¯èƒ½å­˜åœ¨è¯´è¯äººè½¬æ¢å’Œé‡å è¯­éŸ³æ—¶çš„é”™è¯¯é—®é¢˜ã€‚ </li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«è¯æ˜å¯ä»¥æœ‰æ•ˆåˆ©ç”¨è¯­å¢ƒè¿›è¡Œè¯´è¯äººé”™è¯¯çº æ­£ã€‚ </li>
<li>æ–°æå‡ºçš„å£°å­¦æ¡ä»¶åŒ–ç­–ç•¥æ—¨åœ¨å°†å£°å­¦æ—¥è®°ä¸­æä¾›çš„ç²¾ç»†ç²’åº¦ä¿¡æ¯æ³¨å…¥è¯­è¨€æ¨¡å‹ä»¥æå‡è¯´è¯äººè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚ </li>
<li>ç®€å•çº¦æŸè§£ç ç­–ç•¥èƒ½å‡å°‘è¯­è¨€æ¨¡å‹äº§ç”Ÿçš„å¹»è§‰ç°è±¡ï¼ŒåŒæ—¶é¿å…å¤æ‚çš„åæœŸå¤„ç†æ­¥éª¤ã€‚ </li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†åœ¨Fisherã€Callhomeå’ŒRT03-CTSæ•°æ®é›†ä¸Šçš„è¯´è¯äººé”™è¯¯ç‡ï¼Œæœ€é«˜å¯è¾¾43%ã€‚ </li>
<li>è¿™æ˜¯ç»“åˆäº†å£°å­¦æŠ€æœ¯å’Œè¯­è¨€æ¨¡å‹çš„ä¼˜ç§€ç¤ºä¾‹ï¼Œå……åˆ†å±•ç°äº†æŠ€æœ¯åœ¨æé«˜è¯­éŸ³è¯†åˆ«æ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d11091bc2a31c17002e1002d4775bb47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35e0300001739c316a30a51cb86cd5db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1e60de61f651c0bf974562561ef5adf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-641bc304f07669e08f36c47325fa26ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eec18a4ca80c4ddfd282c086d4aec71.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adaptive-Noise-Tolerant-Network-for-Image-Segmentation"><a href="#Adaptive-Noise-Tolerant-Network-for-Image-Segmentation" class="headerlink" title="Adaptive Noise-Tolerant Network for Image Segmentation"></a>Adaptive Noise-Tolerant Network for Image Segmentation</h2><p><strong>Authors:Weizhi Li</strong></p>
<p>Unlike image classification and annotation, for which deep network models have achieved dominating superior performances compared to traditional computer vision algorithms, deep learning for automatic image segmentation still faces critical challenges. One of such hurdles is to obtain ground-truth segmentations as the training labels for deep network training. Especially when we study biomedical images, such as histopathological images (histo-images), it is unrealistic to ask for manual segmentation labels as the ground truth for training due to the fine image resolution as well as the large image size and complexity. In this paper, instead of relying on clean segmentation labels, we study whether and how integrating imperfect or noisy segmentation results from off-the-shelf segmentation algorithms may help achieve better segmentation results through a new Adaptive Noise-Tolerant Network (ANTN) model. We extend the noisy label deep learning to image segmentation with two novel aspects: (1) multiple noisy labels can be integrated into one deep learning model; (2) noisy segmentation modeling, including probabilistic parameters, is adaptive, depending on the given testing image appearance. Implementation of the new ANTN model on both the synthetic data and real-world histo-images demonstrates its effectiveness and superiority over off-the-shelf and other existing deep-learning-based image segmentation algorithms. </p>
<blockquote>
<p>ä¸å›¾åƒåˆ†ç±»å’Œæ ‡æ³¨ä¸åŒï¼Œæ·±åº¦ç½‘ç»œæ¨¡å‹åœ¨ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰ç®—æ³•ä¸­å–å¾—äº†å“è¶Šçš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä½†æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨å›¾åƒåˆ†å‰²æ–¹é¢ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚å…¶ä¸­ä¸€ä¸ªéšœç¢æ˜¯è·å–çœŸå®åˆ†å‰²ä½œä¸ºæ·±åº¦ç½‘ç»œè®­ç»ƒçš„è®­ç»ƒæ ‡ç­¾ã€‚å°¤å…¶æ˜¯å½“æˆ‘ä»¬ç ”ç©¶ç”Ÿç‰©åŒ»å­¦å›¾åƒï¼Œå¦‚ç—…ç†å›¾åƒæ—¶ï¼Œç”±äºå›¾åƒåˆ†è¾¨ç‡é«˜ã€å›¾åƒå°ºå¯¸å¤§ä¸”å¤æ‚ï¼Œè¦æ±‚æ‰‹åŠ¨åˆ†å‰²æ ‡ç­¾ä½œä¸ºè®­ç»ƒçš„çœŸå®æ ‡å‡†æ˜¯ä¸ç°å®çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸ä¾èµ–å¹²å‡€çš„åˆ†å‰²æ ‡ç­¾ï¼Œè€Œæ˜¯ç ”ç©¶å°†ç°æˆçš„åˆ†å‰²ç®—æ³•äº§ç”Ÿçš„åˆ†å‰²ç»“æœä¸­å¯èƒ½å­˜åœ¨çš„é”™è¯¯æˆ–å™ªå£°æ•´åˆèµ·æ¥ï¼Œé€šè¿‡ä¸€ç§æ–°çš„è‡ªé€‚åº”å™ªå£°å®¹å¿ç½‘ç»œï¼ˆANTNï¼‰æ¨¡å‹ï¼Œæ¢è®¨å…¶æ˜¯å¦èƒ½ä»¥åŠå¦‚ä½•å¸®åŠ©æˆ‘ä»¬è·å¾—æ›´å¥½çš„åˆ†å‰²ç»“æœã€‚æˆ‘ä»¬å°†å™ªå£°æ ‡ç­¾æ·±åº¦å­¦ä¹ æ‰©å±•åˆ°å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªæ–°çš„è§‚ç‚¹ï¼šï¼ˆ1ï¼‰å¤šä¸ªå™ªå£°æ ‡ç­¾å¯ä»¥é›†æˆåˆ°ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼›ï¼ˆ2ï¼‰å™ªå£°åˆ†å‰²å»ºæ¨¡åŒ…æ‹¬æ¦‚ç‡å‚æ•°æ˜¯è‡ªé€‚åº”çš„ï¼Œè¿™å–å†³äºç»™å®šçš„æµ‹è¯•å›¾åƒå¤–è§‚ã€‚åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œç—…ç†å›¾åƒä¸Šå®ç°æ–°çš„ANTNæ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨ç°æˆå’Œå…¶ä»–ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒåˆ†å‰²ç®—æ³•ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07163v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯è·å–é«˜åˆ†è¾¨ç‡ã€å¤§å°ºå¯¸å’Œå¤æ‚ç”Ÿç‰©åŒ»å­¦å›¾åƒçš„æ‰‹åŠ¨åˆ†å‰²æ ‡ç­¾ä½œä¸ºè®­ç»ƒä¾æ®ä¸ç°å®ã€‚æœ¬æ–‡ç ”ç©¶å¦‚ä½•é€šè¿‡è‡ªé€‚åº”å™ªå£°å®¹å¿ç½‘ç»œï¼ˆANTNï¼‰æ¨¡å‹åˆ©ç”¨å¸‚åœºç°æˆçš„åˆ†å‰²ç®—æ³•äº§ç”Ÿçš„ä¸å®Œç¾æˆ–å¸¦å™ªå£°çš„åˆ†å‰²ç»“æœæ¥æå‡åˆ†å‰²æ•ˆæœã€‚è¯¥æ–¹æ³•å¯å°†å¤šä¸ªå™ªå£°æ ‡ç­¾é›†æˆåˆ°ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œå¹¶ä¸”å™ªå£°åˆ†å‰²å»ºæ¨¡å…·æœ‰é€‚åº”æ€§ï¼Œå–å†³äºç»™å®šçš„æµ‹è¯•å›¾åƒå¤–è§‚ã€‚åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œç—…ç†å›¾åƒä¸Šçš„å®éªŒè¯æ˜äº†ANTNæ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºè·å–æ‰‹åŠ¨åˆ†å‰²æ ‡ç­¾ä½œä¸ºè®­ç»ƒä¾æ®ä¸ç°å®ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»å’Œæ ‡æ³¨æ–¹é¢è¡¨ç°å“è¶Šï¼Œä½†åœ¨è‡ªåŠ¨å›¾åƒåˆ†å‰²æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥è‡ªé€‚åº”å™ªå£°å®¹å¿ç½‘ç»œï¼ˆANTNï¼‰æ¨¡å‹ï¼Œèƒ½åˆ©ç”¨ä¸å®Œç¾æˆ–å¸¦å™ªå£°çš„åˆ†å‰²ç»“æœã€‚</li>
<li>ANTNæ¨¡å‹å¯å°†å¤šä¸ªå™ªå£°æ ‡ç­¾é›†æˆåˆ°ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ã€‚</li>
<li>å™ªå£°åˆ†å‰²å»ºæ¨¡å…·æœ‰é€‚åº”æ€§ï¼Œèƒ½æ ¹æ®æµ‹è¯•å›¾åƒå¤–è§‚è°ƒæ•´ã€‚</li>
<li>åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œç—…ç†å›¾åƒä¸Šçš„å®éªŒè¯æ˜äº†ANTNæ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab62b745db79cf2ed793c48ced7bcf9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c2af8947e628bfe475b0b2486ef7864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c8f37798c2f489b11dffaf4aca3b111.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Pan-cancer-Classification-Model-using-Multi-view-Feature-Selection-Method-and-Ensemble-Classifier"><a href="#A-Pan-cancer-Classification-Model-using-Multi-view-Feature-Selection-Method-and-Ensemble-Classifier" class="headerlink" title="A Pan-cancer Classification Model using Multi-view Feature Selection   Method and Ensemble Classifier"></a>A Pan-cancer Classification Model using Multi-view Feature Selection   Method and Ensemble Classifier</h2><p><strong>Authors:Tareque Mohmud Chowdhury, Farzana Tabassum, Sabrina Islam, Abu Raihan Mostofa Kamal</strong></p>
<p>Accurately identifying cancer samples is crucial for precise diagnosis and effective patient treatment. Traditional methods falter with high-dimensional and high feature-to-sample count ratios, which are critical for classifying cancer samples. This study aims to develop a novel feature selection framework specifically for transcriptome data and propose two ensemble classifiers. For feature selection, we partition the transcriptome dataset vertically based on feature types. Then apply the Boruta feature selection process on each of the partitions, combine the results, and apply Boruta again on the combined result. We repeat the process with different parameters of Boruta and prepare the final feature set. Finally, we constructed two ensemble ML models based on LR, SVM and XGBoost classifiers with max voting and averaging probability approach. We used 10-fold cross-validation to ensure robust and reliable classification performance. With 97.11% accuracy and 0.9996 AUC value, our approach performs better compared to existing state-of-the-art methods to classify 33 types of cancers. A set of 12 types of cancer is traditionally challenging to differentiate between each other due to their similarity in tissue of origin. Our method accurately identifies over 90% of samples from these 12 types of cancers, which outperforms all known methods presented in existing literature. The gene set enrichment analysis reveals that our frameworkâ€™s selected features have enriched the pathways highly related to cancers. This study develops a feature selection framework to select features highly related to cancer development and leads to identifying different types of cancer samples with higher accuracy. </p>
<blockquote>
<p>å‡†ç¡®åœ°è¯†åˆ«ç™Œç—‡æ ·æœ¬å¯¹äºç²¾ç¡®è¯Šæ–­å’Œæ²»ç–—æ‚£è€…è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†é«˜ç»´åº¦å’Œé«˜ç‰¹å¾æ ·æœ¬è®¡æ•°æ¯”æ—¶ä¼šå‡ºç°é—®é¢˜ï¼Œè¿™å¯¹äºåˆ†ç±»ç™Œç—‡æ ·æœ¬è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§ä¸“é—¨ç”¨äºè½¬å½•ç»„æ•°æ®çš„æ–°å‹ç‰¹å¾é€‰æ‹©æ¡†æ¶ï¼Œå¹¶æå‡ºä¸¤ç§é›†æˆåˆ†ç±»å™¨ã€‚åœ¨ç‰¹å¾é€‰æ‹©æ–¹é¢ï¼Œæˆ‘ä»¬æ ¹æ®ç‰¹å¾ç±»å‹å‚ç›´åˆ’åˆ†è½¬å½•ç»„æ•°æ®é›†ã€‚ç„¶ååœ¨æ¯ä¸ªåˆ†åŒºä¸Šåº”ç”¨Borutaç‰¹å¾é€‰æ‹©è¿‡ç¨‹ï¼Œç»„åˆç»“æœï¼Œå¹¶åœ¨ç»„åˆç»“æœä¸Šå†æ¬¡åº”ç”¨Borutaã€‚æˆ‘ä»¬é‡å¤è¯¥è¿‡ç¨‹å¹¶ä½¿ç”¨Borutaçš„ä¸åŒå‚æ•°æ¥å‡†å¤‡æœ€ç»ˆç‰¹å¾é›†ã€‚æœ€åï¼Œæˆ‘ä»¬æ„å»ºäº†åŸºäºLRã€SVMå’ŒXGBooståˆ†ç±»å™¨çš„ä¸¤ä¸ªé›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé‡‡ç”¨æœ€å¤§æŠ•ç¥¨å’Œå¹³å‡æ¦‚ç‡æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨10å€äº¤å‰éªŒè¯æ¥ç¡®ä¿ç¨³å¥å¯é çš„åˆ†ç±»æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥97.11%çš„å‡†ç¡®ç‡å’Œ0.9996çš„AUCå€¼ï¼Œç›¸è¾ƒäºç›®å‰æœ€å…ˆè¿›çš„åˆ†ç±»æ–¹æ³•ï¼Œå¯¹33ç§ç±»å‹çš„ç™Œç—‡è¿›è¡Œåˆ†ç±»è¡¨ç°æ›´ä½³ã€‚ç”±äºç»„ç»‡èµ·æºçš„ç›¸ä¼¼æ€§ï¼Œä¼ ç»Ÿä¸ŠåŒºåˆ†è¿™å…¶ä¸­çš„åäºŒç§ç™Œç—‡ç±»å‹æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•å‡†ç¡®åœ°è¯†åˆ«äº†è¶…è¿‡90%çš„è¿™äº›åäºŒç§ç™Œç—‡ç±»å‹çš„æ ·æœ¬ï¼Œè¿™ä¼˜äºç°æœ‰æ–‡çŒ®ä¸­æåˆ°çš„æ‰€æœ‰å·²çŸ¥æ–¹æ³•ã€‚åŸºå› é›†å¯Œé›†åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶é€‰æ‹©çš„ç‰¹å¾ä¸é«˜åº¦ç›¸å…³çš„ç™Œç—‡é€”å¾„é«˜åº¦å¯Œé›†ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªç‰¹å¾é€‰æ‹©æ¡†æ¶æ¥é€‰æ‹©ä¸ç™Œç—‡å‘å±•é«˜åº¦ç›¸å…³çš„ç‰¹å¾ï¼Œå¯¼è‡´æ›´å‡†ç¡®åœ°åŒºåˆ†ä¸åŒç±»å‹çš„ç™Œç—‡æ ·æœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06805v1">PDF</a> 20 pages, 5 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§é’ˆå¯¹è½¬å½•ç»„æ•°æ®çš„æ–°å‹ç‰¹å¾é€‰æ‹©æ¡†æ¶ï¼Œå¹¶æå‡ºä¸¤ç§é›†æˆåˆ†ç±»å™¨ï¼Œä»¥æé«˜ç™Œç—‡æ ·æœ¬åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶é€šè¿‡Borutaç‰¹å¾é€‰æ‹©è¿‡ç¨‹å¯¹è½¬å½•ç»„æ•°æ®é›†è¿›è¡Œåˆ†åŒºå¹¶ç»„åˆç»“æœï¼Œæ„å»ºäº†ä¸¤ä¸ªåŸºäºLRã€SVMå’ŒXGBooståˆ†ç±»å™¨çš„é›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚åœ¨33ç§ç™Œç—‡åˆ†ç±»ä¸­ï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®ç‡ä¸º97.11%ï¼ŒAUCå€¼ä¸º0.9996ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨12ç§ä¼ ç»Ÿä¸Šéš¾ä»¥åŒºåˆ†çš„ç™Œç—‡ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å‡†ç¡®è¯†åˆ«è¶…è¿‡90%çš„æ ·æœ¬ï¼Œè¶…è¶Šäº†ç°æœ‰æ–‡çŒ®ä¸­çš„æ‰€æœ‰å·²çŸ¥æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§é’ˆå¯¹è½¬å½•ç»„æ•°æ®çš„ç‰¹å¾é€‰æ‹©æ¡†æ¶ï¼Œä»¥æé«˜ç™Œç—‡æ ·æœ¬åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡åˆ†åŒºå’Œç»„åˆç‰¹å¾é€‰æ‹©ç»“æœï¼Œä½¿ç”¨Borutaç‰¹å¾é€‰æ‹©è¿‡ç¨‹ã€‚</li>
<li>æ„å»ºäº†ä¸¤ä¸ªåŸºäºLRã€SVMå’ŒXGBooståˆ†ç±»å™¨çš„é›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>åœ¨33ç§ç™Œç—‡åˆ†ç±»ä¸­ï¼Œæ–¹æ³•çš„å‡†ç¡®ç‡å’ŒAUCå€¼å‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç‰¹åˆ«æ˜¯åœ¨åŒºåˆ†12ç§ä¼ ç»Ÿä¸Šéš¾ä»¥åŒºåˆ†çš„ç™Œç—‡æ—¶ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ã€‚</li>
<li>åŸºå› é›†å¯Œé›†åˆ†ææ˜¾ç¤ºï¼Œæ‰€é€‰ç‰¹å¾ä¸ç™Œç—‡ç›¸å…³é€”å¾„é«˜åº¦ç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06805">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c8e0ecde3067b063e4647614c5e16d8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6db956a6ccfa5d81dbcd7934360c9d83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31941f16a0bf80fd71789dea7835e0ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ad96c60166101cf8745500f3444bbb8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Rice-Leaf-Disease-Detection-A-Comparative-Study-Between-CNN-Transformer-and-Non-neural-Network-Architectures"><a href="#Rice-Leaf-Disease-Detection-A-Comparative-Study-Between-CNN-Transformer-and-Non-neural-Network-Architectures" class="headerlink" title="Rice Leaf Disease Detection: A Comparative Study Between CNN,   Transformer and Non-neural Network Architectures"></a>Rice Leaf Disease Detection: A Comparative Study Between CNN,   Transformer and Non-neural Network Architectures</h2><p><strong>Authors:Samia Mehnaz, Md. Touhidul Islam</strong></p>
<p>In nations such as Bangladesh, agriculture plays a vital role in providing livelihoods for a significant portion of the population. Identifying and classifying plant diseases early is critical to prevent their spread and minimize their impact on crop yield and quality. Various computer vision techniques can be used for such detection and classification. While CNNs have been dominant on such image classification tasks, vision transformers has become equally good in recent time also. In this paper we study the various computer vision techniques for Bangladeshi rice leaf disease detection. We use the Dhan-Shomadhan â€“ a Bangladeshi rice leaf disease dataset, to experiment with various CNN and ViT models. We also compared the performance of such deep neural network architecture with traditional machine learning architecture like Support Vector Machine(SVM). We leveraged transfer learning for better generalization with lower amount of training data. Among the models tested, ResNet50 exhibited the best performance over other CNN and transformer-based models making it the optimal choice for this task. </p>
<blockquote>
<p>åœ¨å­ŸåŠ æ‹‰å›½ç­‰å›½å®¶ï¼Œå†œä¸šåœ¨ä¸ºå¾ˆå¤§ä¸€éƒ¨åˆ†äººå£æä¾›ç”Ÿè®¡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ—©æœŸè¯†åˆ«å’Œåˆ†ç±»æ¤ç‰©ç–¾ç—…å¯¹é˜²æ­¢å…¶ä¼ æ’­ä»¥åŠå°½é‡å‡å°‘å…¶å¯¹ä½œç‰©äº§é‡å’Œè´¨é‡çš„å½±å“è‡³å…³é‡è¦ã€‚å¯ä»¥ä½¿ç”¨å„ç§è®¡ç®—æœºè§†è§‰æŠ€æœ¯è¿›è¡Œæ­¤ç±»æ£€æµ‹å’Œåˆ†ç±»ã€‚è™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨æ­¤ç±»å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰åœ¨æœ€è¿‘çš„æ—¶é—´ä¸­ä¹ŸåŒæ ·è¡¨ç°å‡ºè‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å­ŸåŠ æ‹‰å›½æ°´ç¨»å¶ç—…æ£€æµ‹çš„å„ç§è®¡ç®—æœºè§†è§‰æŠ€æœ¯ã€‚æˆ‘ä»¬ä½¿ç”¨å­ŸåŠ æ‹‰å›½æ°´ç¨»å¶ç—…æ•°æ®é›†Dhan-Shomadhanï¼Œå¯¹å„ç§CNNå’ŒViTæ¨¡å‹è¿›è¡Œå®éªŒã€‚æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†è¿™ç§æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ä¸è¯¸å¦‚æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ç­‰ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¶æ„çš„æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨è¿ç§»å­¦ä¹ ï¼Œä»¥åœ¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å®ç°æ›´å¥½çš„æ³›åŒ–ã€‚åœ¨æµ‹è¯•çš„æ¨¡å‹ä¸­ï¼ŒResNet50åœ¨å…¶ä»–CNNå’ŒåŸºäºTransformerçš„æ¨¡å‹ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œæˆä¸ºæ­¤ä»»åŠ¡çš„æœ€ä½³é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06740v1">PDF</a> 6 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨è®¡ç®—æœºè§†è§‰æŠ€æœ¯è¿›è¡Œå­ŸåŠ æ‹‰å›½æ°´ç¨»å¶ç‰‡ç—…å®³æ£€æµ‹çš„æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨å½“åœ°çš„Dhan-Shomadhanæ•°æ®é›†è¿›è¡Œå®éªŒï¼Œå¯¹æ¯”äº†CNNå’ŒViTæ¨¡å‹ä»¥åŠä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¶æ„SVMçš„æ€§èƒ½ã€‚å€ŸåŠ©è¿ç§»å­¦ä¹ ï¼Œæ¨¡å‹åœ¨å°‘é‡è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒResNet50ç›¸è¾ƒäºå…¶ä»–CNNå’ŒåŸºäºTransformerçš„æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œæˆä¸ºæ­¤ä»»åŠ¡çš„æœ€ä¼˜é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­ŸåŠ æ‹‰å›½å†œä¸šä¸­æ°´ç¨»å¶ç‰‡ç—…å®³æ£€æµ‹å¯¹é˜²æ­¢ç—…å®³æ‰©æ•£å’Œä¿éšœä½œç‰©äº§é‡ä¸è´¨é‡è‡³å…³é‡è¦ã€‚</li>
<li>ä½¿ç”¨äº†å¤šç§è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬CNNå’ŒViTæ¨¡å‹è¿›è¡Œç—…å®³æ£€æµ‹ä¸åˆ†ç±»ã€‚</li>
<li>åˆ©ç”¨Dhan-Shomadhanæ•°æ®é›†è¿›è¡Œå®éªŒï¼Œæ¶µç›–å¤šç§æ°´ç¨»å¶ç‰‡ç—…å®³ã€‚</li>
<li>è¿ç§»å­¦ä¹ å¢å¼ºäº†æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ResNet50æ¨¡å‹åœ¨å®éªŒä¸­è¡¨ç°æœ€ä½³ï¼Œç›¸è¾ƒäºå…¶ä»–CNNå’ŒTransformeræ¨¡å‹æ›´å…·ä¼˜åŠ¿ã€‚</li>
<li>CNNæ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¶æ„å¦‚SVMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b9a2875192e25383aa012b855034c25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0a763f1f45fa37c62ce320a2551c3cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a58bae6f03d7f23d21aa0a74dace69e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-633ed0700f61ea6af0528ef501f0365a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f99c56014ac4acb844d90b9d953c32c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da49fd9ee7daffe3ba0901aa559789b9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Integrating-Pause-Information-with-Word-Embeddings-in-Language-Models-for-Alzheimerâ€™s-Disease-Detection-from-Spontaneous-Speech"><a href="#Integrating-Pause-Information-with-Word-Embeddings-in-Language-Models-for-Alzheimerâ€™s-Disease-Detection-from-Spontaneous-Speech" class="headerlink" title="Integrating Pause Information with Word Embeddings in Language Models   for Alzheimerâ€™s Disease Detection from Spontaneous Speech"></a>Integrating Pause Information with Word Embeddings in Language Models   for Alzheimerâ€™s Disease Detection from Spontaneous Speech</h2><p><strong>Authors:Yu Pu, Wei-Qiang Zhang</strong></p>
<p>Alzheimerâ€™s disease (AD) is a progressive neurodegenerative disorder characterized by cognitive decline and memory loss. Early detection of AD is crucial for effective intervention and treatment. In this paper, we propose a novel approach to AD detection from spontaneous speech, which incorporates pause information into language models. Our method involves encoding pause information into embeddings and integrating them into the typical transformer-based language model, enabling it to capture both semantic and temporal features of speech data. We conduct experiments on the Alzheimerâ€™s Dementia Recognition through Spontaneous Speech (ADReSS) dataset and its extension, the ADReSSo dataset, comparing our method with existing approaches. Our method achieves an accuracy of 83.1% in the ADReSSo test set. The results demonstrate the effectiveness of our approach in discriminating between AD patients and healthy individuals, highlighting the potential of pauses as a valuable indicator for AD detection. By leveraging speech analysis as a non-invasive and cost-effective tool for AD detection, our research contributes to early diagnosis and improved management of this debilitating disease. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯ä¸€ç§è¿›è¡Œæ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œä»¥è®¤çŸ¥è¡°é€€å’Œè®°å¿†ä¸§å¤±ä¸ºç‰¹å¾ã€‚é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ—©æœŸå‘ç°å¯¹äºæœ‰æ•ˆçš„å¹²é¢„å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆåœé¡¿ä¿¡æ¯ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬ä»è‡ªå‘è¯­è¨€ä¸­æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œå¹¶å°†åœé¡¿ä¿¡æ¯ç¼–ç æˆåµŒå…¥å½¢å¼ï¼Œå¹¶å°†å…¶é›†æˆåˆ°å…¸å‹çš„åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿæ•æ‰è¯­éŸ³æ•°æ®çš„è¯­ä¹‰å’Œæ—¶é—´ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨é˜¿å°”èŒ¨æµ·é»˜æ°ç—‡é€šè¿‡è‡ªå‘è¯­è¨€è¿›è¡Œç—´å‘†è¯†åˆ«ï¼ˆADReSSï¼‰æ•°æ®é›†åŠå…¶æ‰©å±•æ•°æ®é›†ADReSSoä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ADReSSoæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†83.1%çš„å‡†ç¡®ç‡ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒºåˆ†é˜¿å°”èŒ¨æµ·é»˜ç—…æ‚£è€…å’Œå¥åº·ä¸ªä½“æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œçªæ˜¾äº†åœé¡¿ä½œä¸ºé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹çš„é‡è¦æŒ‡æ ‡çš„æ½œåŠ›ã€‚é€šè¿‡åˆ©ç”¨è¯­éŸ³åˆ†æä½œä¸ºéä¾µå…¥æ€§å’Œæˆæœ¬æ•ˆç›Šé«˜çš„å·¥å…·è¿›è¡Œé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸ºè¿™ç§ç–¾ç—…çš„æ—©æœŸè¯Šæ–­å’Œæ²»ç–—ç®¡ç†åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06727v1">PDF</a> accepted by ICASSP2025. Copyright 2025 IEEE. Personal use of this   material is permitted. Permission from IEEE must be obtained for all other   uses, in any current or future media, including reprinting&#x2F;republishing this   material for advertising or promotional purposes, creating new collective   works, for resale or redistribution to servers or lists, or reuse of any   copyrighted component</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆåœé¡¿ä¿¡æ¯æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†åœé¡¿ä¿¡æ¯ç¼–ç ä¸ºåµŒå…¥å½¢å¼ï¼Œå¹¶æ•´åˆåˆ°åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»è€Œæ•æ‰è¯­éŸ³æ•°æ®çš„è¯­ä¹‰å’Œæ—¶é—´ç‰¹å¾ã€‚åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—‡ç—´å‘†ç—‡é€šè¿‡è‡ªå‘è¯­è¨€è¯†åˆ«ï¼ˆADReSSï¼‰æ•°æ®é›†åŠå…¶æ‰©å±•æ•°æ®é›†ADReSSoä¸Šè¿›è¡Œå®éªŒï¼Œè¯¥æ–¹æ³•å®ç°äº†83.1%çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜åœé¡¿å¯èƒ½æ˜¯æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—‡çš„é‡è¦æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†ä¸€ç§åˆ©ç”¨åœé¡¿ä¿¡æ¯æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•æ•´åˆäº†åœé¡¿ä¿¡æ¯åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿæ•æ‰è¯­éŸ³æ•°æ®çš„è¯­ä¹‰å’Œæ—¶é—´ç‰¹å¾ã€‚</li>
<li>æ–¹æ³•åœ¨ADReSSå’ŒADReSSoæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•å‡†ç¡®ç‡ä¸º83.1%ï¼Œæ˜¾ç¤ºå‡ºåœé¡¿ä¿¡æ¯åœ¨ADæ£€æµ‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æ­¤æ–¹æ³•ä¸ºéä¾µå…¥æ€§å’Œæˆæœ¬æ•ˆç›Šé«˜çš„é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹å·¥å…·æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœæœ‰åŠ©äºé˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ—©æœŸè¯Šæ–­å’Œæ²»ç–—ç®¡ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-308725e1b9b06ad1106e766c290cc24f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb2c2249c93cb1b3fd3f2d5c839691b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3098c0571ec870e406842c8411e60103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-347961b619d3ce89ce0346b8f5bf1a8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-689538c34924c6431e8c3547658282c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bae574aeae43fac13816f0d2df39cc53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-717d8e5ae5a65d10cab61c16e1fed327.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Multi-task-Visual-Grounding-with-Coarse-to-Fine-Consistency-Constraints"><a href="#Multi-task-Visual-Grounding-with-Coarse-to-Fine-Consistency-Constraints" class="headerlink" title="Multi-task Visual Grounding with Coarse-to-Fine Consistency Constraints"></a>Multi-task Visual Grounding with Coarse-to-Fine Consistency Constraints</h2><p><strong>Authors:Ming Dai, Jian Li, Jiedong Zhuang, Xian Zhang, Wankou Yang</strong></p>
<p>Multi-task visual grounding involves the simultaneous execution of localization and segmentation in images based on textual expressions. The majority of advanced methods predominantly focus on transformer-based multimodal fusion, aiming to extract robust multimodal representations. However, ambiguity between referring expression comprehension (REC) and referring image segmentation (RIS) is error-prone, leading to inconsistencies between multi-task predictions. Besides, insufficient multimodal understanding directly contributes to biased target perception. To overcome these challenges, we propose a Coarse-to-fine Consistency Constraints Visual Grounding architecture ($\text{C}^3\text{VG}$), which integrates implicit and explicit modeling approaches within a two-stage framework. Initially, query and pixel decoders are employed to generate preliminary detection and segmentation outputs, a process referred to as the Rough Semantic Perception (RSP) stage. These coarse predictions are subsequently refined through the proposed Mask-guided Interaction Module (MIM) and a novel explicit bidirectional consistency constraint loss to ensure consistent representations across tasks, which we term the Refined Consistency Interaction (RCI) stage. Furthermore, to address the challenge of insufficient multimodal understanding, we leverage pre-trained models based on visual-linguistic fusion representations. Empirical evaluations on the RefCOCO, RefCOCO+, and RefCOCOg datasets demonstrate the efficacy and soundness of $\text{C}^3\text{VG}$, which significantly outperforms state-of-the-art REC and RIS methods by a substantial margin. Code and model will be available at \url{<a target="_blank" rel="noopener" href="https://github.com/Dmmm1997/C3VG%7D">https://github.com/Dmmm1997/C3VG}</a>. </p>
<blockquote>
<p>å¤šä»»åŠ¡è§†è§‰å®šä½æ¶‰åŠåŸºäºæ–‡æœ¬è¡¨è¾¾çš„åŒæ—¶å›¾åƒå®šä½å’Œåˆ†å‰²ã€‚å¤§å¤šæ•°å…ˆè¿›çš„æ–¹æ³•ä¸»è¦å…³æ³¨åŸºäºå˜å‹å™¨çš„å¤šæ¨¡å¼èåˆï¼Œæ—¨åœ¨æå–é²æ£’çš„å¤šæ¨¡å¼è¡¨ç¤ºã€‚ç„¶è€Œï¼ŒæŒ‡ä»£è¡¨è¾¾å¼ç†è§£ï¼ˆRECï¼‰å’ŒæŒ‡ä»£å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰ä¹‹é—´çš„æ­§ä¹‰å®¹æ˜“å‡ºé”™ï¼Œå¯¼è‡´å¤šä»»åŠ¡é¢„æµ‹ä¹‹é—´çš„ä¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œå¯¹å¤šæ¨¡å¼çš„ç†è§£ä¸è¶³ç›´æ¥å¯¼è‡´ç›®æ ‡æ„ŸçŸ¥çš„åè§ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»ç²—åˆ°ç»†çš„çš„ä¸€è‡´æ€§çº¦æŸè§†è§‰å®šä½æ¶æ„ï¼ˆC^3VGï¼‰ï¼Œè¯¥æ¶æ„åœ¨ä¸¤é˜¶æ®µæ¡†æ¶å†…é›†æˆäº†éšå¼å’Œæ˜¾å¼å»ºæ¨¡æ–¹æ³•ã€‚é¦–å…ˆï¼Œä½¿ç”¨æŸ¥è¯¢å’Œåƒç´ è§£ç å™¨ç”Ÿæˆåˆæ­¥çš„æ£€æµ‹å’Œåˆ†å‰²è¾“å‡ºï¼Œè¿™ä¸€è¿‡ç¨‹è¢«ç§°ä¸ºç²—ç³™è¯­ä¹‰æ„ŸçŸ¥ï¼ˆRSPï¼‰é˜¶æ®µã€‚è¿™äº›ç²—ç•¥çš„é¢„æµ‹éšåé€šè¿‡æå‡ºçš„Maskå¼•å¯¼äº¤äº’æ¨¡å—ï¼ˆMIMï¼‰å’Œä¸€ç§æ–°é¢–çš„æ˜¾å¼åŒå‘ä¸€è‡´æ€§çº¦æŸæŸå¤±è¿›è¡Œç»†åŒ–ï¼Œä»¥ç¡®ä¿ä»»åŠ¡é—´çš„è¡¨ç¤ºä¸€è‡´æ€§ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºç²¾ç»†åŒ–ä¸€è‡´æ€§äº¤äº’ï¼ˆRCIï¼‰é˜¶æ®µã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³å¯¹å¤šæ¨¡å¼ç†è§£ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºäºè§†è§‰è¯­è¨€èåˆè¡¨ç¤ºçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgæ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒC^3VGçš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œå®ƒæ˜¾è‘—ä¼˜äºæœ€æ–°çš„RECå’ŒRISæ–¹æ³•ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨\url{<a target="_blank" rel="noopener" href="https://github.com/Dmmm1997/C3VG%7D%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Dmmm1997/C3VG}ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06710v1">PDF</a> AAAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸º$\text{C}^3\text{VG}$çš„è§†è§‰å®šä½æ¶æ„ï¼Œè¯¥æ¶æ„é€šè¿‡æ•´åˆéšå¼å’Œæ˜¾å¼å»ºæ¨¡æ–¹æ³•è§£å†³å¤šä»»åŠ¡è§†è§‰å®šä½ä¸­çš„æŒ‡ä»£è¡¨è¾¾ç†è§£ä¸æŒ‡ä»£å›¾åƒåˆ†å‰²ä¹‹é—´çš„æ­§ä¹‰é—®é¢˜ã€‚æ¶æ„åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåˆæ­¥è¯­ä¹‰æ„ŸçŸ¥é˜¶æ®µå’Œç²¾ç»†ä¸€è‡´æ€§äº¤äº’é˜¶æ®µã€‚åˆæ­¥é˜¶æ®µåˆ©ç”¨æŸ¥è¯¢å’Œåƒç´ è§£ç å™¨ç”Ÿæˆåˆæ­¥æ£€æµ‹å’Œåˆ†å‰²è¾“å‡ºï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œé€šè¿‡æ©è†œå¼•å¯¼äº¤äº’æ¨¡å—å’Œæ–°å‹åŒå‘ä¸€è‡´æ€§çº¦æŸæŸå¤±å‡½æ•°å¯¹åˆæ­¥é¢„æµ‹è¿›è¡Œç²¾ç»†åŒ–ï¼Œç¡®ä¿è·¨ä»»åŠ¡çš„è¡¨ç¤ºä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œå€ŸåŠ©è§†è§‰-è¯­è¨€èåˆè¡¨ç¤ºçš„é¢„è®­ç»ƒæ¨¡å‹è§£å†³å¤šåª’ä½“æ¨¡æ€ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚åœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¯æ˜äº†$\text{C}^3\text{VG}$çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä»»åŠ¡è§†è§‰å®šä½æ¶‰åŠåŒæ—¶æ‰§è¡Œå›¾åƒçš„å®šä½å’Œåˆ†å‰²ä»»åŠ¡ï¼ŒåŸºäºæ–‡æœ¬è¡¨è¾¾è¿›è¡Œã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨äºåŸºäºè½¬æ¢å™¨çš„å¤šåª’ä½“èåˆï¼Œæ—¨åœ¨æå–ç¨³å¥çš„å¤šåª’ä½“è¡¨ç¤ºã€‚</li>
<li>æŒ‡ä»£è¡¨è¾¾ç†è§£ä¸æŒ‡ä»£å›¾åƒåˆ†å‰²ä¹‹é—´çš„æ­§ä¹‰å¯¼è‡´é¢„æµ‹ä¸ä¸€è‡´ã€‚</li>
<li>$\text{C}^3\text{VG}$æ¶æ„åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè§£å†³è¿™äº›é—®é¢˜ï¼šåˆæ­¥è¯­ä¹‰æ„ŸçŸ¥å’Œç²¾ç»†ä¸€è‡´æ€§äº¤äº’ã€‚</li>
<li>æ¶æ„åˆ©ç”¨æ©è†œå¼•å¯¼äº¤äº’æ¨¡å—å’Œä¸€è‡´æ€§çº¦æŸæŸå¤±å‡½æ•°ç¡®ä¿è·¨ä»»åŠ¡çš„ä¸€è‡´æ€§è¡¨ç¤ºã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€èåˆæ¨¡å‹è§£å†³å¤šåª’ä½“æ¨¡æ€ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¯æ˜äº†$\text{C}^3\text{VG}$çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7150f71a52b9b07d72269bb21cdd4a27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30b117a4245e895316bd8a8f0cf8b6b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5be611f5aa9b0ee0c8ea88fa0a01a6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-056b61c3e28eb1ae4eae7b9ca398a1be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df32453a4aa3b19bda906207d8701c8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1cb8a02182212851c256da2e4b3c42a4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RMTransformer-Accurate-Radio-Map-Construction-and-Coverage-Prediction"><a href="#RMTransformer-Accurate-Radio-Map-Construction-and-Coverage-Prediction" class="headerlink" title="RMTransformer: Accurate Radio Map Construction and Coverage Prediction"></a>RMTransformer: Accurate Radio Map Construction and Coverage Prediction</h2><p><strong>Authors:Yuxuan Li, Cheng Zhang, Wen Wang, Yongming Huang</strong></p>
<p>Radio map, or pathloss map prediction, is a crucial method for wireless network modeling and management. By leveraging deep learning to construct pathloss patterns from geographical maps, an accurate digital replica of the transmission environment could be established with less computational overhead and lower prediction error compared to traditional model-driven techniques. While existing state-of-the-art (SOTA) methods predominantly rely on convolutional architectures, this paper introduces a hybrid transformer-convolution model, termed RMTransformer, to enhance the accuracy of radio map prediction. The proposed model features a multi-scale transformer-based encoder for efficient feature extraction and a convolution-based decoder for precise pixel-level image reconstruction. Simulation results demonstrate that the proposed scheme significantly improves prediction accuracy, and over a 30% reduction in root mean square error (RMSE) is achieved compared to typical SOTA approaches. </p>
<blockquote>
<p>æ— çº¿ç”µåœ°å›¾æˆ–è·¯å¾„æŸè€—åœ°å›¾é¢„æµ‹æ˜¯æ— çº¿ç½‘ç»œå»ºæ¨¡å’Œç®¡ç†çš„é‡è¦æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨æ·±åº¦å­¦ä¹ ä»åœ°ç†åœ°å›¾æ„å»ºè·¯å¾„æŸè€—æ¨¡å¼ï¼Œå¯ä»¥å»ºç«‹ä¼ è¾“ç¯å¢ƒçš„ç²¾ç¡®æ•°å­—å‰¯æœ¬ï¼Œä¸ä¼ ç»Ÿçš„æ¨¡å‹é©±åŠ¨æŠ€æœ¯ç›¸æ¯”ï¼Œè®¡ç®—å¼€é”€æ›´å°ï¼Œé¢„æµ‹è¯¯å·®æ›´ä½ã€‚è™½ç„¶ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºå·ç§¯æ¶æ„ï¼Œä½†æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ··åˆçš„transformer-å·ç§¯æ¨¡å‹ï¼Œç§°ä¸ºRMTransformerï¼Œä»¥æé«˜æ— çº¿ç”µåœ°å›¾é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æ‰€æå‡ºçš„æ¨¡å‹é‡‡ç”¨åŸºäºå¤šå°ºåº¦transformerçš„ç¼–ç å™¨è¿›è¡Œé«˜æ•ˆç‰¹å¾æå–å’ŒåŸºäºå·ç§¯çš„è§£ç å™¨è¿›è¡Œç²¾ç¡®çš„åƒç´ çº§å›¾åƒé‡å»ºã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æé«˜é¢„æµ‹ç²¾åº¦ï¼Œä¸å…¸å‹çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰é™ä½äº†30%ä»¥ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05190v2">PDF</a> Submitted to IEEE VTC 2025 Spring</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ æ„å»ºåœ°ç†åœ°å›¾çš„è·¯å¾„æŸè€—æ¨¡å¼ä»¥è¿›è¡Œæ— çº¿ç”µåœ°å›¾é¢„æµ‹çš„æ–¹æ³•ã€‚æ–‡ç« æå‡ºä¸€ç§æ··åˆtransformer-å·ç§¯æ¨¡å‹RMTransformerï¼Œé‡‡ç”¨åŸºäºå¤šå°ºåº¦transformerçš„ç¼–ç å™¨è¿›è¡Œé«˜æ•ˆç‰¹å¾æå–å’ŒåŸºäºå·ç§¯çš„è§£ç å™¨è¿›è¡Œç²¾ç¡®åƒç´ çº§å›¾åƒé‡å»ºï¼Œæé«˜äº†æ— çº¿ç”µåœ°å›¾é¢„æµ‹çš„ç²¾åº¦ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆå®ç°äº†è¾ƒé«˜çš„é¢„æµ‹ç²¾åº¦ï¼Œä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œå‡æ–¹æ ¹è¯¯å·®é™ä½äº†è¶…è¿‡30%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— çº¿ç”µåœ°å›¾é¢„æµ‹åœ¨æ— çº¿ç½‘ç»œå»ºæ¨¡å’Œç®¡ç†ä¸­æ˜¯å…³é”®æ–¹æ³•ã€‚</li>
<li>æ·±åº¦å­¦ä¹ è¢«ç”¨äºä»åœ°ç†åœ°å›¾æ„å»ºè·¯å¾„æŸè€—æ¨¡å¼ã€‚</li>
<li>ç°æœ‰å…ˆè¿›æŠ€æœ¯ä¸»è¦ä¾èµ–å·ç§¯æ¶æ„ã€‚</li>
<li>è®ºæ–‡æå‡ºä¸€ç§æ··åˆçš„transformer-å·ç§¯æ¨¡å‹RMTransformerã€‚</li>
<li>RMTransformeråŒ…æ‹¬ä¸€ä¸ªå¤šå°ºåº¦åŸºäºtransformerçš„ç¼–ç å™¨å’ŒåŸºäºå·ç§¯çš„è§£ç å™¨ã€‚</li>
<li>ä»¿çœŸç»“æœæ˜¾ç¤ºRMTransformeræ˜¾è‘—æé«˜é¢„æµ‹ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-186c11b8b6c4f52544f7ff748ed6181d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27dd3eb6a1333f8a76329458f1e39aca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9ac3a916a8fce9602563053ed68b05a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94560f4f33436bf5dc340fb44c79cb07.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="D3RM-A-Discrete-Denoising-Diffusion-Refinement-Model-for-Piano-Transcription"><a href="#D3RM-A-Discrete-Denoising-Diffusion-Refinement-Model-for-Piano-Transcription" class="headerlink" title="D3RM: A Discrete Denoising Diffusion Refinement Model for Piano   Transcription"></a>D3RM: A Discrete Denoising Diffusion Refinement Model for Piano   Transcription</h2><p><strong>Authors:Hounsu Kim, Taegyun Kwon, Juhan Nam</strong></p>
<p>Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion modelâ€™s refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available in <a target="_blank" rel="noopener" href="https://github.com/hanshounsu/d3rm">https://github.com/hanshounsu/d3rm</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ç”±äºå…¶åœ¨å¯¹å¤æ‚æ•°æ®åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡æ—¶çš„å‡ºè‰²è¡¨ç°ï¼Œåœ¨ç”Ÿæˆé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚æ­¤å¤–ï¼Œåœ¨åˆ¤åˆ«ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†å‰²ï¼‰ä¸­ï¼Œå®ƒä»¬ä¹Ÿå±•ç°å‡ºäº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨éŸ³ä¹è‡ªåŠ¨è½¬å½•æ–¹é¢ä¹Ÿæœ‰æ‰€æ¢ç´¢ï¼Œä½†å…¶æ€§èƒ½å°šæœªè¾¾åˆ°ç«äº‰æ°´å¹³ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„ä¼˜åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ºé’¢ç´è½¬å½•æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„ã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨é‚»åŸŸæ³¨æ„åŠ›å±‚ä½œä¸ºå»å™ªæ¨¡å—ï¼Œæ ¹æ®é¢„è®­ç»ƒå£°å­¦æ¨¡å‹çš„å¾®è°ƒç‰¹å¾ï¼Œé€æ­¥é¢„æµ‹ç›®æ ‡é«˜åˆ†è¾¨ç‡çš„é’¢ç´ä¹è°±ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ä¼˜åŒ–æ•ˆæœï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹ç­–ç•¥ï¼Œåœ¨ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†é˜¶æ®µåº”ç”¨ä¸åŒçš„è¿‡æ¸¡çŠ¶æ€ã€‚åœ¨MAESTROæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨F1åˆ†æ•°æ–¹é¢ä¼˜äºä¹‹å‰çš„åŸºäºæ‰©æ•£çš„é’¢ç´è½¬å½•æ¨¡å‹å’ŒåŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hanshounsu/d3rm%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hanshounsu/d3rmä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05068v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨é’¢ç´éŸ³ä¹è½¬å½•æ–¹é¢çš„åº”ç”¨ã€‚è¯¥æ¨¡å‹åˆ©ç”¨é‚»åŸŸæ³¨æ„åŠ›å±‚ä½œä¸ºå»å™ªæ¨¡å—ï¼Œé€æ¸é¢„æµ‹ç›®æ ‡é«˜åˆ†è¾¨ç‡é’¢ç´å·ï¼ŒåŒæ—¶åŸºäºé¢„è®­ç»ƒçš„å£°å­¦æ¨¡å‹çš„å¾®è°ƒç‰¹å¾è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚ä¸ºæé«˜é¢„æµ‹ç²¾åº¦ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åœ¨ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†é˜¶æ®µåº”ç”¨ä¸åŒè¿‡æ¸¡çŠ¶æ€çš„æ–°ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MAESTROæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„æ‰©æ•£æ¨¡å‹é’¢ç´è½¬å½•æ–¹æ³•å’ŒåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¦»æ•£æ‰©æ•£æ¨¡å‹è¢«ç”¨äºé’¢ç´éŸ³ä¹è½¬å½•ï¼Œå±•ç¤ºå…¶åœ¨éŸ³ä¹ç”Ÿæˆé¢†åŸŸçš„æ½œåŠ›ã€‚</li>
<li>é‚»åŸŸæ³¨æ„åŠ›å±‚ä½œä¸ºå»å™ªæ¨¡å—ï¼Œæé«˜äº†æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçš„å£°å­¦æ¨¡å‹çš„å¾®è°ƒç‰¹å¾è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„é’¢ç´å·ã€‚</li>
<li>åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µé‡‡ç”¨ä¸åŒçš„è¿‡æ¸¡çŠ¶æ€ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨MAESTROæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ‰©æ•£æ¨¡å‹é’¢ç´è½¬å½•æ–¹æ³•å’ŒåŸºçº¿æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šä¾›ç ”ç©¶äººå‘˜å‚è€ƒå’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d1f8ac938795271e0f866fbccf3f8ed7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44a494d972de7b54258131ca8a6c6ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92d3dd22fcd9d11ebc0f706a7dfe86a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2016f224be32580eb20bd95e37c89a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db6c4ab16f403b2afdbca1b14f60b2ec.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis"><a href="#SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis" class="headerlink" title="SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis"></a>SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis</h2><p><strong>Authors:Runci Bai</strong></p>
<p>Brain tumors can result in neurological dysfunction, alterations in cognitive and psychological states, increased intracranial pressure, and the occurrence of seizures, thereby presenting a substantial risk to human life and health. The You Only Look Once(YOLO) series models have demonstrated superior accuracy in object detection for medical imaging. In this paper, we develop a novel SCC-YOLO architecture by integrating the SCConv attention mechanism into YOLOv9. The SCConv module reconstructs an efficient convolutional module by reducing spatial and channel redundancy among features, thereby enhancing the learning of image features. We investigate the impact of intergrating different attention mechanisms with the YOLOv9 model on brain tumor image detection using both the Br35H dataset and our self-made dataset(Brain_Tumor_Dataset). Experimental results show that on the Br35H dataset, SCC-YOLO achieved a 0.3% improvement in mAp50 compared to YOLOv9, while on our self-made dataset, SCC-YOLO exhibited a 0.5% improvement over YOLOv9. SCC-YOLO has reached state-of-the-art performance in brain tumor detection. Source code is available at : <a target="_blank" rel="noopener" href="https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master">https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master</a> </p>
<blockquote>
<p>è„‘è‚¿ç˜¤å¯èƒ½å¯¼è‡´ç¥ç»åŠŸèƒ½éšœç¢ã€è®¤çŸ¥å’Œå¿ƒç†çŠ¶æ€æ”¹å˜ã€é¢…å†…å‹å‡é«˜ä»¥åŠç™«ç—«å‘ä½œï¼Œä»è€Œå¯¹äººç±»ç”Ÿå‘½å’Œå¥åº·æ„æˆé‡å¤§é£é™©ã€‚You Only Look Onceï¼ˆYOLOï¼‰ç³»åˆ—æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç›®æ ‡æ£€æµ‹ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„å‡†ç¡®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†SCConvæ³¨æ„åŠ›æœºåˆ¶èå…¥YOLOv9ï¼Œå¼€å‘äº†ä¸€ç§æ–°å‹çš„SCC-YOLOæ¶æ„ã€‚SCConvæ¨¡å—é€šè¿‡å‡å°‘ç‰¹å¾ä¹‹é—´çš„ç©ºé—´å’Œé€šé“å†—ä½™æ€§ï¼Œé‡å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„å·ç§¯æ¨¡å—ï¼Œä»è€Œå¢å¼ºäº†å›¾åƒç‰¹å¾çš„å­¦ä¹ ã€‚æˆ‘ä»¬ä½¿ç”¨Br35Hæ•°æ®é›†å’Œæˆ‘ä»¬è‡ªåˆ¶çš„Brain_Tumor_Datasetæ•°æ®é›†ï¼Œç ”ç©¶äº†å°†ä¸åŒæ³¨æ„åŠ›æœºåˆ¶ä¸YOLOv9æ¨¡å‹é›†æˆå¯¹è„‘è‚¿ç˜¤å›¾åƒæ£€æµ‹çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Br35Hæ•°æ®é›†ä¸Šï¼ŒSCC-YOLOç›¸è¾ƒäºYOLOv9åœ¨mAp50ä¸Šæé«˜äº†0.3%ï¼›è€Œåœ¨æˆ‘ä»¬è‡ªåˆ¶çš„æ•°æ®åº“ä¸Šï¼ŒSCC-YOLOç›¸è¾ƒäºYOLOv9æé«˜äº†0.5%ã€‚SCC-YOLOå·²ç»è¾¾åˆ°äº†è„‘è‚¿ç˜¤æ£€æµ‹çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚æºä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master%E3%80%82">https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/masterã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03836v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡å¼€å‘äº†ä¸€ç§æ–°å‹çš„SCC-YOLOæ¶æ„ï¼Œé€šè¿‡æ•´åˆSCConvæ³¨æ„åŠ›æœºåˆ¶åˆ°YOLOv9ä¸­ï¼Œä»¥æé«˜å¯¹åŒ»å­¦å›¾åƒä¸­çš„è„‘è‚¿ç˜¤æ£€æµ‹ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Br35Hæ•°æ®é›†ä¸Šï¼ŒSCC-YOLOç›¸è¾ƒäºYOLOv9æé«˜äº†0.3%çš„mAp50ï¼›åœ¨è‡ªå®šä¹‰æ•°æ®é›†Brain_Tumor_Datasetä¸Šï¼Œå…¶æ£€æµ‹æ•ˆæœæå‡è¾¾åˆ°0.5%ã€‚å½“å‰æ¨¡å‹å·²æˆä¸ºè„‘è‚¿ç˜¤æ£€æµ‹é¢†åŸŸçš„å…ˆè¿›æˆæœä¹‹ä¸€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è„‘è‚¿ç˜¤å¯¹äººç±»ç”Ÿå‘½å’Œå¥åº·å­˜åœ¨é‡å¤§é£é™©ï¼Œå¯èƒ½å¯¼è‡´ç¥ç»åŠŸèƒ½éšœç¢ã€è®¤çŸ¥å’Œå¿ƒç†çŠ¶æ€æ”¹å˜ã€é¢…å†…å‹å‡é«˜å’Œç™«ç—«å‘ä½œã€‚</li>
<li>YOLOç³»åˆ—æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç›®æ ‡æ£€æµ‹ä¸­å…·æœ‰ä¼˜è¶Šçš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶å¼€å‘äº†SCC-YOLOæ¶æ„ï¼Œé€šè¿‡æ•´åˆSCConvæ³¨æ„åŠ›æœºåˆ¶åˆ°YOLOv9ä¸­ï¼Œæ—¨åœ¨æé«˜è„‘è‚¿ç˜¤æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>SCConvæ¨¡å—é€šè¿‡å‡å°‘ç‰¹å¾é—´çš„ç©ºé—´é€šé“å†—ä½™æ¥é‡æ„æœ‰æ•ˆçš„å·ç§¯æ¨¡å—ï¼Œä»è€Œå¢å¼ºå›¾åƒç‰¹å¾çš„å­¦ä¹ ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Br35Hæ•°æ®é›†å’Œè‡ªå®šä¹‰Brain_Tumor_Datasetæ•°æ®é›†ä¸Šï¼ŒSCC-YOLOç›¸è¾ƒäºYOLOv9å‡æœ‰æ‰€æå‡ã€‚</li>
<li>SCC-YOLOå·²è¾¾åˆ°è„‘è‚¿ç˜¤æ£€æµ‹çš„é¢†å…ˆæ°´å¹³ï¼Œæºä»£ç å·²å…¬å¼€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16881751de744c4feb2a10914442530f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0200fff1472ec9f1188675aa523f4815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3113e51cc7f179389457eeb3dd6aba4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fb20d29c3ccd29bb579a7100430c262.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7ebcc1540b1cb9d88926ff4ca93e495.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ae871e237ac575b1ca1551f2c82448f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-820225500646ae0b15fbed7ad322161f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d7f704436e94c77a517078cc0fb32f0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FashionFAE-Fine-grained-Attributes-Enhanced-Fashion-Vision-Language-Pre-training"><a href="#FashionFAE-Fine-grained-Attributes-Enhanced-Fashion-Vision-Language-Pre-training" class="headerlink" title="FashionFAE: Fine-grained Attributes Enhanced Fashion Vision-Language   Pre-training"></a>FashionFAE: Fine-grained Attributes Enhanced Fashion Vision-Language   Pre-training</h2><p><strong>Authors:Jiale Huang, Dehong Gao, Jinxia Zhang, Zechao Zhan, Yang Hu, Xin Wang</strong></p>
<p>Large-scale Vision-Language Pre-training (VLP) has demonstrated remarkable success in the general domain. However, in the fashion domain, items are distinguished by fine-grained attributes like texture and material, which are crucial for tasks such as retrieval. Existing models often fail to leverage these fine-grained attributes from both text and image modalities. To address the above issues, we propose a novel approach for the fashion domain, Fine-grained Attributes Enhanced VLP (FashionFAE), which focuses on the detailed characteristics of fashion data. An attribute-emphasized text prediction task is proposed to predict fine-grained attributes of the items. This forces the model to focus on the salient attributes from the text modality. Additionally, a novel attribute-promoted image reconstruction task is proposed, which further enhances the fine-grained ability of the model by leveraging the representative attributes from the image modality. Extensive experiments show that FashionFAE significantly outperforms State-Of-The-Art (SOTA) methods, achieving 2.9% and 5.2% improvements in retrieval on sub-test and full test sets, respectively, and a 1.6% average improvement in recognition tasks. </p>
<blockquote>
<p>å¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åœ¨é€šç”¨é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œåœ¨æ—¶å°šé¢†åŸŸï¼Œç‰©å“çš„åŒºåˆ«åœ¨äºç»†ç²’åº¦å±æ€§ï¼Œå¦‚è´¨åœ°å’Œæè´¨ï¼Œè¿™å¯¹äºæ£€ç´¢ç­‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç°æœ‰æ¨¡å‹å¾€å¾€æ— æ³•ä»æ–‡æœ¬å’Œå›¾åƒä¸¤ç§æ¨¡æ€ä¸­åˆ©ç”¨è¿™äº›ç»†ç²’åº¦å±æ€§ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬é’ˆå¯¹æ—¶å°šé¢†åŸŸæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³ç»†ç²’åº¦å±æ€§å¢å¼ºVLPï¼ˆFashionFAEï¼‰ï¼Œå®ƒä¸“æ³¨äºæ—¶å°šæ•°æ®çš„è¯¦ç»†ç‰¹å¾ã€‚æå‡ºäº†ä¸€ç§å±æ€§å¼ºè°ƒæ–‡æœ¬é¢„æµ‹ä»»åŠ¡ï¼Œç”¨äºé¢„æµ‹ç‰©å“çš„ç»†ç²’åº¦å±æ€§ã€‚è¿™è¿«ä½¿æ¨¡å‹å…³æ³¨æ–‡æœ¬æ¨¡æ€çš„æ˜¾è‘—å±æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„å±æ€§ä¿ƒè¿›å›¾åƒé‡å»ºä»»åŠ¡ï¼Œé€šè¿‡åˆ©ç”¨å›¾åƒæ¨¡æ€çš„ä»£è¡¨å±æ€§ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„ç»†ç²’åº¦èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFashionFAEæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨å­æµ‹è¯•é›†å’Œå…¨é›†æµ‹è¯•é›†çš„æ£€ç´¢ä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†2.9%å’Œ5.2%çš„å‡†ç¡®ç‡ï¼Œåœ¨è¯†åˆ«ä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†1.6%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19997v2">PDF</a> 5 pages, Accepted by ICASSP2025, full paper</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åœ¨é€šç”¨é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨æ—¶å°šé¢†åŸŸï¼Œç”±äºæ—¶å°šäº§å“ä¹‹é—´ä¸»è¦é€šè¿‡ç²¾ç»†å±æ€§ï¼ˆå¦‚çº¹ç†å’Œææ–™ï¼‰è¿›è¡ŒåŒºåˆ†ï¼Œç°æœ‰æ¨¡å‹å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨è¿™äº›ç²¾ç»†å±æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ—¶å°šé¢†åŸŸçš„æ–°å‹æ–¹æ³•â€”â€”ç²¾ç»†å±æ€§å¢å¼ºè§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆFashionFAEï¼‰ï¼Œå®ƒä¾§é‡äºæ•æ‰æ—¶å°šæ•°æ®çš„è¯¦ç»†ç‰¹å¾ã€‚é€šè¿‡æ„å»ºæ³¨é‡å±æ€§çš„æ–‡æœ¬é¢„æµ‹ä»»åŠ¡å’Œä¿ƒè¿›å±æ€§çš„å›¾åƒé‡å»ºä»»åŠ¡ï¼ŒFashionFAEèƒ½å¤Ÿä»æ–‡æœ¬å’Œå›¾åƒä¸­æå–å’Œå¼ºåŒ–è¿™äº›ç²¾ç»†å±æ€§ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒFashionFAEåœ¨æ£€ç´¢ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å­æµ‹è¯•é›†å’Œå…¨æµ‹è¯•é›†ä¸Šçš„æ£€ç´¢æ€§èƒ½åˆ†åˆ«æé«˜äº†2.9%å’Œ5.2%ï¼Œå¹¶åœ¨è¯†åˆ«ä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†1.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ¨¡å‹åœ¨æ—¶å°šé¢†åŸŸæ— æ³•å……åˆ†åˆ©ç”¨ç²¾ç»†å±æ€§ï¼ˆå¦‚çº¹ç†å’Œææ–™ï¼‰ã€‚</li>
<li>FashionFAEæ˜¯ä¸€ç§é’ˆå¯¹æ—¶å°šé¢†åŸŸçš„æ–°å‹é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æ•æ‰æ—¶å°šæ•°æ®çš„è¯¦ç»†ç‰¹å¾ã€‚</li>
<li>FashionFAEé€šè¿‡æ„å»ºæ³¨é‡å±æ€§çš„æ–‡æœ¬é¢„æµ‹ä»»åŠ¡æ¥æé«˜æ¨¡å‹å¯¹æ–‡æœ¬æ¨¡æ€ä¸­æ˜¾è‘—å±æ€§çš„å…³æ³¨ã€‚</li>
<li>FashionFAEå¼•å…¥äº†ä¸€ç§æ–°çš„å±æ€§ä¿ƒè¿›å›¾åƒé‡å»ºä»»åŠ¡ï¼Œé€šè¿‡åˆ©ç”¨å›¾åƒæ¨¡æ€ä¸­çš„ä»£è¡¨æ€§å±æ€§æ¥å¢å¼ºæ¨¡å‹çš„ç²¾ç»†å±æ€§èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒFashionFAEåœ¨æ—¶å°šæ£€ç´¢ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨å­æµ‹è¯•é›†å’Œå…¨æµ‹è¯•é›†ä¸Šï¼ŒFashionFAEçš„æ£€ç´¢æ€§èƒ½åˆ†åˆ«æé«˜äº†2.9%å’Œ5.2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b000e8dad3cacdb54143b84341eb286.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9603e429e48a5d2959e4b318a81750c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73d41d1e76fa53a3b4fe2a7ee0e85eb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5593878e966e9c41f0b6d9828ff416f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b86867b42e265ea1ebe265defe3442.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6aeefbcb52e0e978fe859442b06afddd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56ac66586e68001e088f6a65f4ceeeca.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Evaluation-of-radiomic-feature-harmonization-techniques-for-benign-and-malignant-pulmonary-nodules"><a href="#Evaluation-of-radiomic-feature-harmonization-techniques-for-benign-and-malignant-pulmonary-nodules" class="headerlink" title="Evaluation of radiomic feature harmonization techniques for benign and   malignant pulmonary nodules"></a>Evaluation of radiomic feature harmonization techniques for benign and   malignant pulmonary nodules</h2><p><strong>Authors:Claire Huchthausen, Menglin Shi, Gabriel L. A. de Sousa, Jonathan Colen, Emery Shelley, James Larner, Einsley Janowski, Krishni Wijesooriya</strong></p>
<p>BACKGROUND: Radiomics provides quantitative features of pulmonary nodules (PNs) which could aid lung cancer diagnosis, but medical image acquisition variability is an obstacle to clinical application. Acquisition effects may differ between radiomic features from benign vs. malignant PNs. PURPOSE: We evaluated how to account for differences between benign and malignant PNs when correcting radiomic featuresâ€™ acquisition dependency. METHODS: We used 567 chest CT scans grouped as benign, malignant, or lung cancer screening (mixed benign, malignant). ComBat harmonization was applied to extracted features for variation in 4 acquisition parameters. We compared: harmonizing without distinction, harmonizing with a covariate to preserve distinctions between subgroups, and harmonizing subgroups separately. Significant ($p\le0.05$) Kruskal-Wallis tests showed whether harmonization removed acquisition dependency. A LASSO-SVM pipeline was trained on successfully harmonized features to predict malignancy. To evaluate predictive information in these features, the trained harmonization estimators and predictive model were applied to unseen test sets. Harmonization and predictive performance were assessed for 10 trials of 5-fold cross-validation. RESULTS: An average 2.1% of features (95% CI:1.9-2.4%) were acquisition-independent when harmonized without distinction, 27.3% (95% CI:25.7-28.9%) when harmonized with a covariate, and 90.9% (95% CI:90.4-91.5%) when harmonized separately. Data harmonized separately or with a covariate trained models with higher ROC-AUC for screening scans than data harmonized without distinction between benign and malignant PNs (Delong test, adjusted $p\le0.05$). CONCLUSIONS: Radiomic features of benign and malignant PNs need different corrective transformations to recover acquisition-independent distributions. This can be done by harmonizing separately or with a covariate. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šæ”¾å°„å­¦æä¾›æœ‰å…³è‚ºç»“èŠ‚çš„å®šé‡ç‰¹å¾ï¼Œæœ‰åŠ©äºè‚ºç™Œçš„è¯Šæ–­ï¼Œä½†åŒ»å­¦å›¾åƒé‡‡é›†çš„å·®å¼‚æ€§æ˜¯ä¸´åºŠåº”ç”¨ä¸­çš„éšœç¢ã€‚è‰¯æ€§è‚ºç»“èŠ‚ä¸æ¶æ€§è‚ºç»“èŠ‚ä¹‹é—´çš„æ”¾å°„å­¦ç‰¹å¾å¯èƒ½å› é‡‡é›†æ•ˆæœè€Œæœ‰æ‰€ä¸åŒã€‚ç›®çš„ï¼šæˆ‘ä»¬è¯„ä¼°äº†åœ¨çº æ­£æ”¾å°„å­¦ç‰¹å¾çš„é‡‡é›†ä¾èµ–æ€§æ—¶ï¼Œå¦‚ä½•åŒºåˆ†è‰¯æ€§å’Œæ¶æ€§è‚ºç»“èŠ‚ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬ä½¿ç”¨567å¼ èƒ¸éƒ¨CTæ‰«æå›¾åƒï¼ŒæŒ‰è‰¯æ€§ã€æ¶æ€§æˆ–è‚ºç™Œç­›æŸ¥ï¼ˆæ··åˆè‰¯æ€§ã€æ¶æ€§ï¼‰è¿›è¡Œåˆ†ç»„ã€‚åº”ç”¨ComBatæ–¹æ³•å¯¹å››ä¸ªé‡‡é›†å‚æ•°çš„å·®å¼‚è¿›è¡Œåè°ƒå¤„ç†ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§åè°ƒæ–¹æ³•ï¼šæ— åŒºåˆ«çš„åè°ƒå¤„ç†ã€ç”¨åå˜é‡åè°ƒå¤„ç†ä»¥ä¿ç•™å„äºšç»„ä¹‹é—´çš„å·®å¼‚ä»¥åŠåˆ†åˆ«åè°ƒå¤„ç†äºšç»„ã€‚é€šè¿‡æ˜¾è‘—æ€§ï¼ˆpâ‰¤0.05ï¼‰çš„Kruskal-Wallisæ£€éªŒæ¥ç¡®å®šåè°ƒå¤„ç†æ˜¯å¦æ¶ˆé™¤äº†é‡‡é›†ä¾èµ–æ€§ã€‚åœ¨æˆåŠŸåè°ƒçš„ç‰¹å¾ä¸Šåº”ç”¨äº†LASSO-SVMç®¡é“æ¥é¢„æµ‹æ¶æ€§ç¨‹åº¦ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›ç‰¹å¾ä¸­çš„é¢„æµ‹ä¿¡æ¯ï¼Œå°†è®­ç»ƒå¥½çš„åè°ƒä¼°è®¡å™¨å’Œé¢„æµ‹æ¨¡å‹åº”ç”¨äºæœªè§è¿‡çš„æµ‹è¯•é›†ä¸Šã€‚å¯¹ç»è¿‡åæ¬¡5å€äº¤å‰éªŒè¯çš„åè°ƒå¤„ç†å’Œé¢„æµ‹æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœï¼šå½“æ— åŒºåˆ«åœ°è¿›è¡Œåè°ƒå¤„ç†æ—¶ï¼Œå¹³å‡æœ‰2.1%ï¼ˆ95%ç½®ä¿¡åŒºé—´ï¼š1.9%-2.4%ï¼‰çš„ç‰¹å¾æ˜¯ç‹¬ç«‹äºé‡‡é›†çš„ï¼›å½“ç”¨åå˜é‡è¿›è¡Œåè°ƒå¤„ç†æ—¶ï¼Œæœ‰27.3%ï¼ˆ95%ç½®ä¿¡åŒºé—´ï¼š25.7%-28.9%ï¼‰çš„ç‰¹å¾ç‹¬ç«‹äºé‡‡é›†ï¼›å½“åˆ†åˆ«è¿›è¡Œåè°ƒå¤„ç†æ—¶ï¼Œæœ‰90.9%ï¼ˆ95%ç½®ä¿¡åŒºé—´ï¼š90.4%-91.5%ï¼‰çš„ç‰¹å¾ç‹¬ç«‹äºé‡‡é›†ã€‚ä¸æ— åŒºåˆ«åœ°åè°ƒå¤„ç†è‰¯æ€§å’Œæ¶æ€§è‚ºç»“èŠ‚çš„æ•°æ®ç›¸æ¯”ï¼Œåˆ†åˆ«æˆ–å¸¦æœ‰åå˜é‡åè°ƒå¤„ç†çš„æ•°æ®ä¸ºç­›æŸ¥æ‰«ææä¾›äº†æ›´é«˜çš„ROC-AUCæ¨¡å‹ï¼ˆç»è¿‡è°ƒæ•´çš„Delongæ£€éªŒï¼Œpâ‰¤0.05ï¼‰ã€‚ç»“è®ºï¼šè‰¯æ€§å’Œæ¶æ€§è‚ºç»“èŠ‚çš„æ”¾å°„å­¦ç‰¹å¾éœ€è¦ä¸åŒçš„æ ¡æ­£è½¬æ¢æ¥æ¢å¤ç‹¬ç«‹äºé‡‡é›†çš„åˆ†å¸ƒã€‚è¿™å¯ä»¥é€šè¿‡åˆ†åˆ«åè°ƒå¤„ç†æˆ–ä½¿ç”¨åå˜é‡æ¥å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16758v2">PDF</a> 15 pages, 3 figures, plus supplemental material; updated author list,   corrected result in paragraph 3 of Discussion, updated Figure S1</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ”¾å°„ç»„å­¦ç‰¹å¾åœ¨è‚ºç»“èŠ‚è¯Šæ–­ä¸­çš„åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•æ¶ˆé™¤ä¸åŒå›¾åƒé‡‡é›†å¯¹æ”¾å°„ç»„å­¦ç‰¹å¾çš„å½±å“ã€‚é€šè¿‡å¯¹ä¸åŒé‡‡é›†å‚æ•°ä¸‹çš„æ”¾å°„ç»„å­¦ç‰¹å¾è¿›è¡Œæ ¡æ­£ï¼Œç ”ç©¶å‘ç°åœ¨åŒºåˆ†è‰¯æ¶æ€§è‚ºç»“èŠ‚æ—¶ï¼Œéœ€è¦åˆ†åˆ«è¿›è¡Œæ ¡æ­£ä»¥è·å–ç‹¬ç«‹çš„åˆ†å¸ƒç‰¹å¾ã€‚é€šè¿‡åº”ç”¨ç‰¹å®šçš„æ ¡æ­£æ–¹æ³•ï¼Œå¯ä»¥æé«˜å¯¹è‚ºç™Œç­›æŸ¥æ‰«æçš„é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„ç»„å­¦ç‰¹å¾å¯¹äºè‚ºç»“èŠ‚çš„è‰¯æ¶æ€§è¯Šæ–­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å›¾åƒé‡‡é›†å‚æ•°çš„å·®å¼‚ä¼šå¯¹æ”¾å°„ç»„å­¦ç‰¹å¾äº§ç”Ÿå½±å“ï¼Œä»è€Œå½±å“è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å¯¹æ”¾å°„ç»„å­¦ç‰¹å¾è¿›è¡Œæ ¡æ­£ï¼Œå¯ä»¥æ¶ˆé™¤å›¾åƒé‡‡é›†å‚æ•°çš„å½±å“ï¼Œæé«˜è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨åŒºåˆ†è‰¯æ¶æ€§è‚ºç»“èŠ‚æ—¶ï¼Œéœ€è¦åˆ†åˆ«è¿›è¡Œæ ¡æ­£ä»¥è·å¾—ç‹¬ç«‹çš„åˆ†å¸ƒç‰¹å¾ã€‚</li>
<li>åº”ç”¨ç‰¹å®šçš„æ ¡æ­£æ–¹æ³•å¯ä»¥æé«˜é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ ¡æ­£æ–¹æ³•åŒ…æ‹¬æ— åŒºåˆ«çš„æ ¡æ­£ã€å¸¦æœ‰åå˜é‡çš„æ ¡æ­£å’Œåˆ†åˆ«æ ¡æ­£ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbd335f95c59728fc1d639f373346b4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a290ffb49d344fd032b674b03f4fda.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-43adc801f1d3aa8a7f76510ab47e37b1.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  Speech Synthesis along Perceptual Voice Quality Dimensions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fd00bff6af50ecf21e69d3f599c69f96.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  Boosting Diffusion Guidance via Learning Degradation-Aware Models for   Blind Super Resolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19710k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
