<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-17  Vision Foundation Models for Computed Tomography">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-633ed0700f61ea6af0528ef501f0365a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-17-更新"><a href="#2025-01-17-更新" class="headerlink" title="2025-01-17 更新"></a>2025-01-17 更新</h1><h2 id="Vision-Foundation-Models-for-Computed-Tomography"><a href="#Vision-Foundation-Models-for-Computed-Tomography" class="headerlink" title="Vision Foundation Models for Computed Tomography"></a>Vision Foundation Models for Computed Tomography</h2><p><strong>Authors:Suraj Pai, Ibrahim Hadzic, Dennis Bontempi, Keno Bressem, Benjamin H. Kann, Andriy Fedorov, Raymond H. Mak, Hugo J. W. L. Aerts</strong></p>
<p>Foundation models (FMs) have shown transformative potential in radiology by performing diverse, complex tasks across imaging modalities. Here, we developed CT-FM, a large-scale 3D image-based pre-trained model designed explicitly for various radiological tasks. CT-FM was pre-trained using 148,000 computed tomography (CT) scans from the Imaging Data Commons through label-agnostic contrastive learning. We evaluated CT-FM across four categories of tasks, namely, whole-body and tumor segmentation, head CT triage, medical image retrieval, and semantic understanding, showing superior performance against state-of-the-art models. Beyond quantitative success, CT-FM demonstrated the ability to cluster regions anatomically and identify similar anatomical and structural concepts across scans. Furthermore, it remained robust across test-retest settings and indicated reasonable salient regions attached to its embeddings. This study demonstrates the value of large-scale medical imaging foundation models and by open-sourcing the model weights, code, and data, aims to support more adaptable, reliable, and interpretable AI solutions in radiology. </p>
<blockquote>
<p>基础模型（FMs）在放射学中表现出了变革性的潜力，能够在不同的成像模式下执行多样且复杂的任务。在这里，我们开发了CT-FM，这是一个大规模基于3D图像的预训练模型，专为各种放射学任务而设计。CT-FM使用来自影像数据共享平台的14.8万份计算机断层扫描（CT）数据进行预训练，采用标签无关的对比学习。我们在四类任务中评估了CT-FM，即全身和肿瘤分割、头部CT筛查、医学图像检索和语义理解，其性能优于最新模型。除了定量成功之外，CT-FM还展示了在解剖区域进行聚类的能力，并能在扫描中识别相似的解剖和结构概念。此外，它在测试重测环境中表现稳健，其嵌入的显著区域也合理。本研究展示了大规模医学成像基础模型的价值，通过开源模型权重、代码和数据，旨在支持更具适应性、可靠性和可解释性的放射学人工智能解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09001v1">PDF</a> 6 figures, followed by 9 Extended Data Figures and a Supplementary   Information document</p>
<p><strong>Summary</strong></p>
<p>大规模医学成像基础模型CT-FM在放射学中的潜力评估。该模型通过对比学习在来自成像数据共享中心的14.8万份计算机断层扫描（CT）扫描数据上进行预训练，展现出强大的跨模态表现能力。在全身和肿瘤分割、头部CT评估、医学图像检索和语义理解等任务中，CT-FM表现超越现有顶尖模型。此外，它还能进行解剖学区域聚类，识别不同扫描间的相似解剖结构和概念。模型的开源将促进放射学中适应性更强、更可靠和可解释的AI解决方案的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CT-FM是一个大规模基于3D图像的预训练模型，专为各种放射学任务设计。</li>
<li>该模型使用来自成像数据共享中心的14.8万份CT扫描数据进行预训练。</li>
<li>在全身和肿瘤分割、头部CT评估、医学图像检索和语义理解等任务中，CT-FM表现优越。</li>
<li>CT-FM具备解剖学区域聚类能力，并能识别不同扫描间的相似解剖结构和概念。</li>
<li>模型在测试重测环境中表现稳健，并能合理识别关键区域。</li>
<li>通过开源模型权重、代码和数据，支持更适应、可靠和可解释的放射学AI解决方案的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09001">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-64015eb4ec4af1dd0711991adaacb548.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8b640709322557dcfdc1051fdb227ae.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Revealing-Local-Structures-through-Machine-Learning-Fused-Multimodal-Spectroscopy"><a href="#Revealing-Local-Structures-through-Machine-Learning-Fused-Multimodal-Spectroscopy" class="headerlink" title="Revealing Local Structures through Machine-Learning- Fused Multimodal   Spectroscopy"></a>Revealing Local Structures through Machine-Learning- Fused Multimodal   Spectroscopy</h2><p><strong>Authors:Haili Jia, Yiming Chen, Gi-Hyeok Lee, Jacob Smith, Miaofang Chi, Wanli Yang, Maria K. Y. Chan</strong></p>
<p>Atomistic structures of materials offer valuable insights into their functionality. Determining these structures remains a fundamental challenge in materials science, especially for systems with defects. While both experimental and computational methods exist, each has limitations in resolving nanoscale structures. Core-level spectroscopies, such as x-ray absorption (XAS) or electron energy-loss spectroscopies (EELS), have been used to determine the local bonding environment and structure of materials. Recently, machine learning (ML) methods have been applied to extract structural and bonding information from XAS&#x2F;EELS, but most of these frameworks rely on a single data stream, which is often insufficient. In this work, we address this challenge by integrating multimodal ab initio simulations, experimental data acquisition, and ML techniques for structure characterization. Our goal is to determine local structures and properties using EELS and XAS data from multiple elements and edges. To showcase our approach, we use various lithium nickel manganese cobalt (NMC) oxide compounds which are used for lithium ion batteries, including those with oxygen vacancies and antisite defects, as the sample material system. We successfully inferred local element content, ranging from lithium to transition metals, with quantitative agreement with experimental data. Beyond improving prediction accuracy, we find that ML model based on multimodal spectroscopic data is able to determine whether local defects such as oxygen vacancy and antisites are present, a task which is impossible for single mode spectra or other experimental techniques. Furthermore, our framework is able to provide physical interpretability, bridging spectroscopy with the local atomic and electronic structures. </p>
<blockquote>
<p>材料原子结构为我们深入了解其功能提供了宝贵的见解。确定这些结构仍然是材料科学领域的一个基本挑战，特别是对于存在缺陷的系统。虽然存在实验和计算方法，但每种方法在解决纳米尺度结构方面都有其局限性。芯层光谱学，如X射线吸收（XAS）或电子能量损失光谱学（EELS），已被用于确定材料的局部键合环境和结构。最近，机器学习（ML）方法已被应用于从XAS&#x2F;EELS中提取结构和键合信息，但大多数框架都依赖于单一数据流，这通常是不够的。在这项工作中，我们通过集成多模式从头模拟、实验数据获取和ML技术来解决这一挑战，以进行结构表征。我们的目标是利用来自多种元素和边缘的EELS和XAS数据来确定局部结构和属性。为了展示我们的方法，我们使用了各种用于锂离子电池的锂镍锰钴（NMC）氧化物化合物作为样本材料系统，包括具有氧空位和反位缺陷的化合物。我们成功地推断出了从锂到过渡金属等局部元素含量，并与实验数据定量吻合。除了提高预测精度外，我们发现基于多模式光谱数据的ML模型能够确定是否存在局部缺陷，如氧空位和反位，这是单一模式光谱或其他实验技术无法完成的任务。此外，我们的框架能够提供物理可解释性，将光谱与局部原子和电子结构相联系。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08919v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了材料原子结构的重要性及其在研究中的挑战，特别是在存在缺陷的系统中。文章结合了多模态从头模拟、实验数据采集和机器学习技术，旨在利用电子能量损失谱（EELS）和X射线吸收谱（XAS）数据来确定局部结构和性质。文章以含有氧空位和反位缺陷的锂镍锰钴氧化物为例，展示了其成功推断出局部元素含量的能力，并与实验结果定量一致。此外，基于多模态光谱数据的机器学习模型还能够确定局部缺陷的存在，这是一项单一模式光谱或其他实验技术无法完成的任务。该框架还能够提供物理可解释性，将光谱与局部原子和电子结构相联系。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>材料原子结构的研究对于理解其功能性至关重要，但在存在缺陷的系统中，确定这些结构仍是材料科学中的基本挑战。</li>
<li>核心层次光谱学，如XAS和EELS，已被用于确定材料的局部键合环境和结构。</li>
<li>机器学习（ML）方法已被应用于从XAS&#x2F;EELS中提取结构和键合信息，但大多数框架依赖于单一数据流，这通常是不够的。</li>
<li>本文通过结合多模态从头模拟、实验数据采集和ML技术来解决这一挑战，旨在利用EELS和XAS数据确定局部结构和性质。</li>
<li>以含有氧空位和反位缺陷的锂镍锰钴氧化物为例子，展示了该方法在推断局部元素含量方面的成功，并与实验结果一致。</li>
<li>基于多模态光谱数据的ML模型能够确定局部缺陷的存在，这是单一模式光谱或其他实验技术无法做到的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08919">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c4331c865c642fdb76cc82b90ea482f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b89b0fce566656d896b97d48a2f9618b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ee80cf7de858752ad2579f7aef2449a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55530ff58a8ace863b7eb1e996d6fe04.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-View-Transformers-for-Airway-To-Lung-Ratio-Inference-on-Cardiac-CT-Scans-The-C4R-Study"><a href="#Multi-View-Transformers-for-Airway-To-Lung-Ratio-Inference-on-Cardiac-CT-Scans-The-C4R-Study" class="headerlink" title="Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT   Scans: The C4R Study"></a>Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT   Scans: The C4R Study</h2><p><strong>Authors:Sneha N. Naik, Elsa D. Angelini, Eric A. Hoffman, Elizabeth C. Oelsner, R. Graham Barr, Benjamin M. Smith, Andrew F. Laine</strong></p>
<p>The ratio of airway tree lumen to lung size (ALR), assessed at full inspiration on high resolution full-lung computed tomography (CT), is a major risk factor for chronic obstructive pulmonary disease (COPD). There is growing interest to infer ALR from cardiac CT images, which are widely available in epidemiological cohorts, to investigate the relationship of ALR to severe COVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously, cardiac scans included approximately 2&#x2F;3 of the total lung volume with 5-6x greater slice thickness than high-resolution (HR) full-lung (FL) CT. In this study, we present a novel attention-based Multi-view Swin Transformer to infer FL ALR values from segmented cardiac CT scans. For the supervised training we exploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of Atherosclerosis (MESA). Our network significantly outperforms a proxy direct ALR inference on segmented cardiac CT scans and achieves accuracy and reproducibility comparable with a scan-rescan reproducibility of the FL ALR ground-truth. </p>
<blockquote>
<p>在高分辨率全肺计算机断层扫描（CT）全肺吸气状态下，气道树腔与肺大小之比（ALR）是慢性阻塞性肺疾病（COPD）的主要风险因素。人们越来越有兴趣从心脏CT图像中推断ALR，这些图像在流行病学队列中广泛存在，以研究ALR与严重COVID-19和SARS-CoV-2感染后的急性后遗症（PASC）之间的关系。以前的心脏扫描包括了大约三分之二的肺体积，切片厚度是高分辨率（HR）全肺（FL）CT的5-6倍。在这项研究中，我们提出了一种新型基于注意力的多视角Swin Transformer，用于从分割的心脏CT扫描中推断全肺ALR值。我们利用在多元种族动脉粥样硬化研究（MESA）中获得的全肺和心脏CT配对图像进行有监督训练。我们的网络在分割的心脏CT扫描上的直接ALR推断表现显著优越，并且其准确性和可重复性可与全肺ALR真实值的扫描-重新扫描可重复性相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08902v1">PDF</a> Accepted to appear in Proceedings of International Symposium on   Biomedical Imaging (ISBI), 2025</p>
<p><strong>摘要</strong><br>     利用高分辨率全肺计算机断层扫描（CT）在完全吸气状态下评估的气道树腔与肺大小之比（ALR）是慢性阻塞性肺疾病（COPD）的主要风险因素。越来越多的研究兴趣在于，从在流行病学队列中广泛可用的心脏CT图像推断ALR，以研究ALR与严重COVID-19和SARS-CoV-2感染后的急性后遗症（PASC）之间的关系。先前的心脏扫描包括大约三分之二的肺总体积，切片厚度是高分辨率（HR）全肺（FL）CT的5-6倍。在这项研究中，我们提出了一种基于注意力的多视图Swin Transformer，可以从分段的心脏CT扫描推断FL ALR值。我们利用在多元种族动脉粥样硬化研究（MESA）中采集的配对全肺和心脏CT进行有监督训练。我们的网络在分段心脏CT扫描上的间接ALR推断表现显著优越，并且其准确性和再现性与FL ALR真实值的扫描-再扫描再现性相当。</p>
<p><strong>要点</strong></p>
<ol>
<li>气道树腔与肺大小之比（ALR）是慢性阻塞性肺疾病（COPD）的重要风险因素。</li>
<li>心脏CT图像可用于推断ALR，这在流行病学研究中具有广泛应用。</li>
<li>先前的心脏CT扫描只包括大约三分之二的肺体积，且切片厚度较大。</li>
<li>提出一种基于注意力的多视图Swin Transformer，可从分段的心脏CT扫描推断全肺ALR值。</li>
<li>该方法利用配对全肺和心脏CT进行有监督训练。</li>
<li>该网络在间接ALR推断上表现优异，其性能和准确性可与全肺CT扫描的扫描-再扫描结果相比。</li>
<li>这种方法为通过心脏CT评估ALR提供了一种新的可能途径，有助于研究ALR与严重COVID-19和PASC之间的关系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08902">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-13bf05f4e4b467af7e63fb57fcf9a047.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93c0b238c3e80bae67e8abb7e9f2d89e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cddbc4b7bc2822f8b8445a52ebec5608.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cyclical-accretion-regime-change-in-the-slow-X-ray-pulsar-4U-0114-65-observed-with-Chandra"><a href="#Cyclical-accretion-regime-change-in-the-slow-X-ray-pulsar-4U-0114-65-observed-with-Chandra" class="headerlink" title="Cyclical accretion regime change in the slow X-ray pulsar 4U 0114+65   observed with Chandra"></a>Cyclical accretion regime change in the slow X-ray pulsar 4U 0114+65   observed with Chandra</h2><p><strong>Authors:Graciela Sanjurjo-Ferrín, Jose Miguel Torrejón, Konstantin Postnov, Michael Nowak, Jose Joaquín Rodes-Roca, Lida Oskinova, Jessica Planelles-Villalva, Norber Schulz</strong></p>
<p>4U 0114+65 is a high-mass X-ray binary system formed by the luminous supergiant B1Ia, known as V{*} V662 Cas, and one of the slowest rotating neutron stars (NS) with a spin period of about 2.6 hours. This fact provides a rare opportunity to study interesting details of the accretion within each individual pulse of the compact object. In this paper, we analyze 200 ks of Chandra grating data, divided into 9 uninterrupted observations around the orbit. The changes in the circumstellar absorption column through the orbit suggest an orbital inclination of $\sim$ $40^{\circ}$ with respect to the observer and a companion mass-loss rate of $\sim$ 8.6 10$^{-7}$ solar masses yr$^{-1}$. The peaks of the NS pulse show a large pulse-to-pulse variability. Three of them show an evolution from a brighter regime to a weaker one. We propose that the efficiency of Compton cooling in this source fluctuates throughout an accumulation cycle. After significant depletion of matter within the magnetosphere, since the settling velocity is $\sim \times$ 2 times lower than the free-fall velocity, the source gradually accumulates matter until the density exceeds a critical threshold. This increase in density triggers a transition to a more efficient Compton cooling regime, leading to a higher mass accretion rate and consequently to an increased brightness. </p>
<blockquote>
<p>4U 0114+65是一个高质量X射线双星系统，由明亮的超巨星B1Ia（也称为V{*} V662 Cas）和自转周期约为2.6小时的中子星（NS）组成。这一事实提供了一个难得的机会，可以研究紧凑物体每个脉冲内的吸积过程的有趣细节。在本文中，我们分析了长达20万秒的Chandra光谱数据，这些数据被分为围绕轨道的9次不间断观测。恒星周围吸收柱在轨道上的变化表明轨道倾角约为$ 40^{\circ}$相对于观察者，并且伴随天体质量损失率约为每秒$ 8.6 \times 10^{-7}$个太阳质量。中子星脉冲峰值显示出巨大的脉冲间变异性。其中三个脉冲峰值表现出从较亮的状态向较弱的状态的转变。我们提出，此源的康普顿冷却效率在一个积累周期内存在波动。由于磁层内物质的大量消耗，由于沉降速度约为自由落体速度的$\frac{1}{2}$倍，物质逐渐在源处积累，直到密度超过临界阈值。密度的增加会引发向更有效的康普顿冷却状态的转变，从而导致更高的物质吸积率，进而导致亮度增加。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08702v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本描述了高质量X射线双星系统4U 0114+65的研究结果，该系统由超巨星B1Ia（称为V{*} V662 Cas）和最慢的中子星之一组成。通过对该系统的观测数据进行分析，发现其轨道倾角约为40°，并推测其伴星质量损失率约为每年损失太阳质量的8.6 x 10^-7倍。中子星脉冲峰值表现出显著的脉冲间变化，这些变化与源的康普顿冷却效率波动有关。在磁层物质大量耗尽后，由于沉降速度约为自由落体速度的两倍，物质逐渐积累直至密度超过临界阈值，触发向更有效的康普顿冷却机制的转变，导致更高的物质积累率和相应的亮度增加。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4U 0114+65是一个由超巨星B1Ia（V{*} V662 Cas）和最慢旋转的中子星之一组成的双星系统，为研究脉冲星体的细节提供了罕见的机会。</li>
<li>通过分析该系统长达近两年的连续观测数据，发现轨道倾角约为40°，推测伴星的质量损失率为每年损失太阳质量的特定值。这一发现对于理解双星系统的演化具有重要意义。</li>
<li>中子星脉冲峰值显示显著变化。这种变化被认为是由于该系统中康普顿冷却效率波动导致的，这可能进一步揭示了物质累积过程的复杂性。</li>
<li>在磁层物质大量耗尽后，系统的行为发生变化，表明物质的积累与密度阈值之间存在关联。当密度超过临界阈值时，系统会经历一种向更有效的冷却机制转变的过程。这一发现对于理解脉冲星的物理特性至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08702">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cfffb4ed18ca26b1f50659b22e854327.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-316e4b960d69f998fbdb66e1d096c02d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cba12fc7b70e6d251d246223175185f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f5b407a2baba0444297398b6c7b5cb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db2dee9c8ff4bb8ad06d3ef9f56278a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1036a5b045ba0901cb346534dc88ecff.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TimeFlow-Longitudinal-Brain-Image-Registration-and-Aging-Progression-Analysis"><a href="#TimeFlow-Longitudinal-Brain-Image-Registration-and-Aging-Progression-Analysis" class="headerlink" title="TimeFlow: Longitudinal Brain Image Registration and Aging Progression   Analysis"></a>TimeFlow: Longitudinal Brain Image Registration and Aging Progression   Analysis</h2><p><strong>Authors:Bailiang Jian, Jiazhen Pan, Yitong Li, Fabian Bongratz, Ruochen Li, Daniel Rueckert, Benedikt Wiestler, Christian Wachinger</strong></p>
<p>Predicting future brain states is crucial for understanding healthy aging and neurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone for such analyses, has long been limited by its inability to forecast future developments, reliance on extensive, dense longitudinal data, and the need to balance registration accuracy with temporal smoothness. In this work, we present \emph{TimeFlow}, a novel framework for longitudinal brain MRI registration that overcomes all these challenges. Leveraging a U-Net architecture with temporal conditioning inspired by diffusion models, TimeFlow enables accurate longitudinal registration and facilitates prospective analyses through future image prediction. Unlike traditional methods that depend on explicit smoothness regularizers and dense sequential data, TimeFlow achieves temporal consistency and continuity without these constraints. Experimental results highlight its superior performance in both future timepoint prediction and registration accuracy compared to state-of-the-art methods. Additionally, TimeFlow supports novel biological brain aging analyses, effectively differentiating neurodegenerative conditions from healthy aging. It eliminates the need for segmentation, thereby avoiding the challenges of non-trivial annotation and inconsistent segmentation errors. TimeFlow paves the way for accurate, data-efficient, and annotation-free prospective analyses of brain aging and chronic diseases. </p>
<blockquote>
<p>预测未来的大脑状态对于理解健康老化和神经退行性疾病至关重要。纵向脑MRI注册是此类分析的核心，长期以来一直受限于其无法预测未来发展、依赖大量密集的纵向数据以及需要在注册精度与时间平滑之间取得平衡。在这项工作中，我们提出了\emph{TimeFlow}这一全新的纵向脑MRI注册框架，克服了上述所有挑战。TimeFlow利用受扩散模型启发的U-Net架构进行时间条件处理，能够实现准确的纵向注册，并通过未来图像预测进行前瞻性分析。与传统方法不同，TimeFlow无需依赖明确的平滑正则器和密集的序列数据即可实现时间的一致性和连续性。实验结果表明，与最先进的方法相比，TimeFlow在未来时间点预测和注册精度方面都表现出卓越的性能。此外，TimeFlow支持新型脑衰老生物学分析，可有效区分神经退行性疾病与健康衰老。它无需分割，从而避免了非平凡标注和不一致的分割错误所带来的挑战。TimeFlow为准确、高效且无标注的前瞻性分析脑衰老和慢性病铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为TimeFlow的新型纵向脑MRI注册框架，它克服了现有方法的局限性，实现了准确的纵向注册和未来图像预测。TimeFlow采用U-Net架构，受扩散模型启发进行时间条件处理，可在无需明确平滑正则器和密集序列数据的情况下实现时间连贯性和连续性。实验结果表明，TimeFlow在未来时间点预测和注册准确性方面表现出卓越性能，并支持新型生物脑衰老分析，可区分神经退行性疾病和健康衰老。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TimeFlow是一个新型的纵向脑MRI注册框架，能够预测未来的大脑状态。</li>
<li>它克服了现有方法的挑战，如无法预测未来发展、对密集纵向数据依赖以及需要在注册精度和时间平滑之间取得平衡。</li>
<li>TimeFlow采用U-Net架构，结合扩散模型的时间条件，实现了准确且连贯的纵向注册。</li>
<li>该方法可在无需明确平滑正则器和密集序列数据的情况下实现时间连贯性和连续性。</li>
<li>实验结果表明，TimeFlow在未来时间点预测和注册准确性方面表现优异。</li>
<li>TimeFlow支持新型生物脑衰老分析，并能区分神经退行性疾病和健康衰老。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-490c338a9ca4eec8f1a45b15fc21f1df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40e1282e06ed9a6f74997f1a4161a68c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8796fa25580be129414d5561f9fef265.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Pressure-induced-topological-changes-in-Fermi-surface-of-two-dimensional-molecular-conductor"><a href="#Pressure-induced-topological-changes-in-Fermi-surface-of-two-dimensional-molecular-conductor" class="headerlink" title="Pressure-induced topological changes in Fermi surface of two-dimensional   molecular conductor"></a>Pressure-induced topological changes in Fermi surface of two-dimensional   molecular conductor</h2><p><strong>Authors:T. Kobayashi, K. Yoshimi, H. Ma, S. Sekine, H. Taniguchi, N. Matsunaga, A. Kawamoto, Y. Uwatoko</strong></p>
<p>We demonstrated X-ray structural analysis of the pressure-induced superconductor, $\beta’$-ET$_2$ICl$<em>2$ under extremely high-pressure conditions, where ET denotes bis(ethylenedithio)tetrathiafulvalene. This material has been known as the highest transition temperature ($T_c$) superconductor among organic superconductors ($T_c&#x3D;14.2$ K at $8.2$ GPa). On the basis of the experimental results, ab-initio models were derived using the constrained random phase approximation. We revealed that the Lifshitz transition exists behind the Mott insulator-metal transition and found that the value of the on-site Coulomb interaction was halved to around $10$ GPa compared to that at ambient pressure. This study clarifies the enigmatic origins of high $T</em>{\rm c}$, and concurrently, provides a new understanding of the impacts of structural alterations in organic materials under high pressure on their electronic properties and the superconductivity process. </p>
<blockquote>
<p>我们对压力诱导的超导体β’-ET₂ICl₂进行了X射线结构分析，其中ET代表双（乙烯二硫代）四硫富瓦烯。这种材料在有机超导体中具有最高的转变温度（Tc），在8.2 GPa时的Tc为14.2 K。根据实验结果，我们使用约束随机相位逼近推导出了从头模型。我们揭示了Lifshitz转变存在于Mott绝缘体金属转变之后，并发现现场库仑相互作用值在约10 GPa时比环境压力下的值减少了一半。这项研究阐明了高Tc的神秘起源，同时提供了新的理解：在高压力下有机材料的结构变化对其电子特性和超导过程的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08635v1">PDF</a> 8 pages, 4 figures; Supplemental Material: 9 pages, 6 figures,   accepted for publication in Phys. Rev. Materials (Letter)</p>
<p><strong>Summary</strong><br>     本研究对高压下压力诱导的超导体β’-ET₂ICl₂（其中ET代表双（乙二硫代）四硫富瓦烯）进行了X射线结构分析。该材料在有机超导体中具有最高的转变温度（Tc）。基于实验结果，采用约束随机相位近似法推导了从头算模型。研究揭示了Lifshitz转变存在于Mott绝缘体-金属转变之后，并发现与常压相比，在大约10 GPa时，在位库仑相互作用值减半。这项研究阐明了高Tc的奥秘，同时提供了新的理解：在高压下有机材料的结构变化对其电子属性和超导过程的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究进行了高压下超导体β’-ET₂ICl₂的X射线结构分析。</li>
<li>该材料在有机超导体中展现出最高的转变温度（Tc）。</li>
<li>通过约束随机相位近似法推导了实验结果的从头算模型。</li>
<li>研究揭示了Lifshitz转变与Mott绝缘体-金属转变之间的关系。</li>
<li>在约10 GPa的压力下，在位库仑相互作用值相比常压条件减半。</li>
<li>研究阐明了高压下有机材料结构变化对电子属性和超导过程的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-89d0d0a5676a4ffe4494f4ee07b567e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6e85d5a3ff499b8a1e39d173b69dce6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95649a5e0f7dde7e0fe3847bea457f13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddea445648fbebaeeb46213d3b229118.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Densely-Connected-Parameter-Efficient-Tuning-for-Referring-Image-Segmentation"><a href="#Densely-Connected-Parameter-Efficient-Tuning-for-Referring-Image-Segmentation" class="headerlink" title="Densely Connected Parameter-Efficient Tuning for Referring Image   Segmentation"></a>Densely Connected Parameter-Efficient Tuning for Referring Image   Segmentation</h2><p><strong>Authors:Jiaqi Huang, Zunnan Xu, Ting Liu, Yong Liu, Haonan Han, Kehong Yuan, Xiu Li</strong></p>
<p>In the domain of computer vision, Parameter-Efficient Tuning (PET) is increasingly replacing the traditional paradigm of pre-training followed by full fine-tuning. PET is particularly favored for its effectiveness in large foundation models, as it streamlines transfer learning costs and optimizes hardware utilization. However, the current PET methods are mainly designed for single-modal optimization. While some pioneering studies have undertaken preliminary explorations, they still remain at the level of aligned encoders (e.g., CLIP) and lack exploration of misaligned encoders. These methods show sub-optimal performance with misaligned encoders, as they fail to effectively align the multimodal features during fine-tuning. In this paper, we introduce DETRIS, a parameter-efficient tuning framework designed to enhance low-rank visual feature propagation by establishing dense interconnections between each layer and all preceding layers, which enables effective cross-modal feature interaction and adaptation to misaligned encoders. We also suggest using text adapters to improve textual features. Our simple yet efficient approach greatly surpasses state-of-the-art methods with 0.9% to 1.8% backbone parameter updates, evaluated on challenging benchmarks. Our project is available at \url{<a target="_blank" rel="noopener" href="https://github.com/jiaqihuang01/DETRIS%7D">https://github.com/jiaqihuang01/DETRIS}</a>. </p>
<blockquote>
<p>在计算机视觉领域，参数高效调整（PET）正越来越多地取代传统的预训练后进行全面微调的模式。PET因其在大规模基础模型中的有效性而受到青睐，它能简化迁移学习成本并优化硬件利用率。然而，当前的PET方法主要是为单一模态优化设计的。虽然一些开创性的研究已经进行了初步的探索，但它们仍然停留在对齐编码器的层面（例如CLIP），并且缺乏对未对齐编码器的探索。这些方法在面临未对齐编码器时表现出性能不佳，因为它们无法在微调过程中有效地对齐多模态特征。在本文中，我们介绍了DETRIS，这是一个参数高效调整框架，旨在通过在各层和所有前层之间建立密集连接来增强低秩视觉特征传播，从而实现有效的跨模态特征交互和适应未对齐的编码器。我们还建议使用文本适配器来改善文本特征。我们简单而高效的方法在具有挑战性的基准测试中，以更新0.9%至1.8%的主干参数大大超越了最新方法。我们的项目可通过网址<a target="_blank" rel="noopener" href="https://github.com/jiaqihuang01/DETRIS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jiaqihuang01/DETRIS获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08580v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>参数高效调整（PET）方法正在改变计算机视觉领域中的预训练加全微调的传统模式。PET特别适用于大型基础模型，因为它能优化硬件使用并降低迁移学习成本。然而，当前PET方法主要针对单模态优化进行设计。尽管已有初步探索，但它们主要停留在对齐编码器（如CLIP）的层面，并未探索错配编码器。当使用错配编码器时，这些方法表现不佳，无法有效对齐多模态特征进行微调。本文介绍DETRIS，一种参数高效调整框架，旨在通过在各层与所有前置层之间建立密集互联，增强低秩视觉特征传播，实现跨模态特征的有效交互并适应错配编码器。本文还建议使用文本适配器来改善文本特征。通过简单的策略，我们的方法在具有挑战性的基准测试中大幅超越了现有技术，仅需更新0.9%至1.8%的主干参数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PET方法正在改变计算机视觉的传统训练模式，特别是在大型基础模型中。</li>
<li>当前PET方法主要面向单模态优化，对于错配编码器的性能表现不佳。</li>
<li>DETRIS框架通过密集互联增强低秩视觉特征传播，有效实现跨模态特征交互。</li>
<li>DETRIS能适应错配编码器，提高了方法的鲁棒性。</li>
<li>使用文本适配器来改善文本特征的方法被提出。</li>
<li>DETRIS方法在具有挑战性的基准测试中显著超越了现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08580">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5e0c12f1e35dc7a0e0249a9d0e12ada5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5debf1e09a4bc5f66cfad44dc7f3d71f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcbd58fdaedddbb6c612156dc14f2783.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e13981fd3ba89bb6b5c6935b2c0fb2fb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MIAFEx-An-Attention-based-Feature-Extraction-Method-for-Medical-Image-Classification"><a href="#MIAFEx-An-Attention-based-Feature-Extraction-Method-for-Medical-Image-Classification" class="headerlink" title="MIAFEx: An Attention-based Feature Extraction Method for Medical Image   Classification"></a>MIAFEx: An Attention-based Feature Extraction Method for Medical Image   Classification</h2><p><strong>Authors:Oscar Ramos-Soto, Jorge Ramos-Frutos, Ezequiel Perez-Zarate, Diego Oliva, Sandra E. Balderas-Mata</strong></p>
<p>Feature extraction techniques are crucial in medical image classification; however, classical feature extractors in addition to traditional machine learning classifiers often exhibit significant limitations in providing sufficient discriminative information for complex image sets. While Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown promise in feature extraction, they are prone to overfitting due to the inherent characteristics of medical imaging data, including small sample sizes or high intra-class variance. In this work, the Medical Image Attention-based Feature Extractor (MIAFEx) is proposed, a novel method that employs a learnable refinement mechanism to enhance the classification token within the Transformer encoder architecture. This mechanism adjusts the token based on learned weights, improving the extraction of salient features and enhancing the model’s adaptability to the challenges presented by medical imaging data. The MIAFEx output features quality is compared against classical feature extractors using traditional and hybrid classifiers. Also, the performance of these features is compared against modern CNN and ViT models in classification tasks, demonstrating its superiority in accuracy and robustness across multiple complex classification medical imaging datasets. This advantage is particularly pronounced in scenarios with limited training data, where traditional and modern models often struggle to generalize effectively. The source code of this proposal can be found at <a target="_blank" rel="noopener" href="https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx">https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx</a> </p>
<blockquote>
<p>特征提取技术在医学图像分类中至关重要。然而，除了传统的机器学习分类器外，经典的特征提取器在为复杂的图像集提供足够的鉴别信息时往往存在显著局限性。卷积神经网络（CNNs）和视觉转换器（ViT）在特征提取方面显示出潜力，但由于医学成像数据固有的特性，包括样本量小或类内方差高，它们容易过度拟合。在本文中，提出了一种医学图像注意力特征提取器（MIAFEx），这是一种采用可学习细化机制的新方法，旨在增强Transformer编码器架构中的分类令牌。这种机制根据学习到的权重调整令牌，提高了显著特征的提取能力，并增强了模型对医学成像数据挑战的适应性。MIAFEx输出特征质量与使用传统和混合分类器的经典特征提取器进行了比较。此外，这些特征在分类任务中的性能与现代的CNN和ViT模型相比也进行了比较，证明其在多个复杂的医学图像分类数据集上的准确性和稳健性方面具有优势。特别是在训练数据有限的情况下，传统和现代模型往往难以有效推广，MIAFEx的优势尤为突出。该提案的源代码可在<a target="_blank" rel="noopener" href="https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08562v1">PDF</a> In preparation for Journal Submission</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在医学图像分类中特征提取技术的重要性。传统特征提取器结合传统机器学习分类器在处理复杂图像集时提供足够的判别信息存在局限性。卷积神经网络（CNN）和视觉转换器（ViT）虽具有潜力，但由于医学成像数据的小样本量或高类内差异等固有特性，容易发生过拟合。为此，本文提出了医学图像注意力特征提取器（MIAFEx），这是一种采用可学习细化机制的新方法，用于增强Transformer编码器架构中的分类令牌。该机制基于学习到的权重调整令牌，提高了显著特征的提取，增强了模型对医学成像数据挑战的适应性。MIAFEx的特征质量与传统和混合分类器的经典特征提取器进行了比较，也与现代CNN和ViT模型在分类任务中的性能进行了比较，证明其在多个复杂医学图像分类数据集上的准确性和稳健性方面的优越性。在训练数据有限的情况下，这种优势尤为突出。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分类中，特征提取技术非常重要，但传统特征提取器和机器学习分类器在处理复杂图像集时存在局限性。</li>
<li>CNN和ViT在医学图像特征提取中展现出潜力，但容易因数据特性（如小样本、高类内差异）而过度拟合。</li>
<li>新提出的MIAFEx采用可学习细化机制，增强Transformer编码器中的分类令牌，提高显著特征提取。</li>
<li>MIAFEx的特征质量在与其他传统和现代模型比较中表现出优越性能，特别是在有限训练数据的场景下。</li>
<li>MIAFEx模型在多个复杂的医学图像分类数据集上展示了高准确性和稳健性。</li>
<li>该研究的源代码可在指定链接找到。</li>
<li>注意力机制在医学图像特征提取中的应用为提高模型性能开辟了新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08562">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0fa14d15147b642b3ed914029265be1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dae1fb00d4461afb66decad012e72917.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RWKV-UNet-Improving-UNet-with-Long-Range-Cooperation-for-Effective-Medical-Image-Segmentation"><a href="#RWKV-UNet-Improving-UNet-with-Long-Range-Cooperation-for-Effective-Medical-Image-Segmentation" class="headerlink" title="RWKV-UNet: Improving UNet with Long-Range Cooperation for Effective   Medical Image Segmentation"></a>RWKV-UNet: Improving UNet with Long-Range Cooperation for Effective   Medical Image Segmentation</h2><p><strong>Authors:Juntao Jiang, Jiangning Zhang, Weixuan Liu, Muxuan Gao, Xiaobin Hu, Xiaoxiao Yan, Feiyue Huang, Yong Liu</strong></p>
<p>In recent years, there have been significant advancements in deep learning for medical image analysis, especially with convolutional neural networks (CNNs) and transformer models. However, CNNs face limitations in capturing long-range dependencies while transformers suffer high computational complexities. To address this, we propose RWKV-UNet, a novel model that integrates the RWKV (Receptance Weighted Key Value) structure into the U-Net architecture. This integration enhances the model’s ability to capture long-range dependencies and improve contextual understanding, which is crucial for accurate medical image segmentation. We build a strong encoder with developed inverted residual RWKV (IR-RWKV) blocks combining CNNs and RWKVs. We also propose a Cross-Channel Mix (CCM) module to improve skip connections with multi-scale feature fusion, achieving global channel information integration. Experiments on benchmark datasets, including Synapse, ACDC, BUSI, CVC-ClinicDB, CVC-ColonDB, Kvasir-SEG, ISIC 2017 and GLAS show that RWKV-UNet achieves state-of-the-art performance on various types of medical image segmentation. Additionally, smaller variants, RWKV-UNet-S and RWKV-UNet-T, balance accuracy and computational efficiency, making them suitable for broader clinical applications. </p>
<blockquote>
<p>近年来，深度学习在医学图像分析方面取得了显著进展，尤其是卷积神经网络（CNN）和变压器模型的应用。然而，CNN在捕捉长距离依赖关系方面存在局限性，而变压器则面临计算复杂性高的问题。为了解决这一问题，我们提出了RWKV-UNet模型，这是一种将RWKV（Receptance Weighted Key Value）结构融入U-Net架构的新型模型。这种融合增强了模型捕捉长距离依赖关系并提高上下文理解的能力，这对于准确的医学图像分割至关重要。我们利用先进的倒残差RWKV（IR-RWKV）块构建了一个强大的编码器，结合了CNN和RWKV。我们还提出了跨通道混合（CCM）模块，通过多尺度特征融合改进跳过连接，实现全局通道信息集成。在Synapse、ACDC、BUSI、CVC-ClinicDB、CVC-ColonDB、Kvasir-SEG、ISIC 2017和GLAS等基准数据集上的实验表明，RWKV-UNet在各种类型的医学图像分割上达到了最先进的性能。此外，较小的变体RWKV-UNet-S和RWKV-UNet-T平衡了准确性和计算效率，使它们更适合更广泛的临床应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08458v1">PDF</a> </p>
<p><strong>Summary</strong><br>     融合RWKV结构于U-Net架构中提出的RWKV-UNet模型，有效提升了捕捉长期依赖关系及理解上下文的能力，对于精准医学图像分割至关重要。通过引入IR-RWKV块和Cross-Channel Mix模块等技术手段，模型展现出卓越的医学图像分割性能，实现了在多个基准数据集上的最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RWKV-UNet模型结合了RWKV结构和U-Net架构，增强了捕捉长期依赖关系和上下文理解能力。</li>
<li>通过引入IR-RWKV块，构建了强大的编码器。</li>
<li>Cross-Channel Mix模块改善了跨尺度特征的融合，实现了全局通道信息集成。</li>
<li>RWKV-UNet在多个基准数据集上实现了最先进的医学图像分割性能。</li>
<li>该模型提供了平衡准确性与计算效率的较小变体，即RWKV-UNet-S和RWKV-UNet-T。</li>
<li>RWKV-UNet模型对于各种类型医学图像分割具有广泛应用潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08458">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4421e2dbcd55b65958dffa70b582b80f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5048719cb31a4b20b7a8d832b7a03fa2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee089821375aa1fd5f7d2131b1c34b90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07dabcea76597467dd2478a6549c89b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bd079f7e1cb2ae183b9ca8cade7a181.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SEAL-Speaker-Error-Correction-using-Acoustic-conditioned-Large-Language-Models"><a href="#SEAL-Speaker-Error-Correction-using-Acoustic-conditioned-Large-Language-Models" class="headerlink" title="SEAL: Speaker Error Correction using Acoustic-conditioned Large Language   Models"></a>SEAL: Speaker Error Correction using Acoustic-conditioned Large Language   Models</h2><p><strong>Authors:Anurag Kumar, Rohit Paturi, Amber Afshan, Sundararajan Srinivasan</strong></p>
<p>Speaker Diarization (SD) is a crucial component of modern end-to-end ASR pipelines. Traditional SD systems, which are typically audio-based and operate independently of ASR, often introduce speaker errors, particularly during speaker transitions and overlapping speech. Recently, language models including fine-tuned large language models (LLMs) have shown to be effective as a second-pass speaker error corrector by leveraging lexical context in the transcribed output. In this work, we introduce a novel acoustic conditioning approach to provide more fine-grained information from the acoustic diarizer to the LLM. We also show that a simpler constrained decoding strategy reduces LLM hallucinations, while avoiding complicated post-processing. Our approach significantly reduces the speaker error rates by 24-43% across Fisher, Callhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD. </p>
<blockquote>
<p>说话人聚类（Speaker Diarization，SD）是现代端到端自动语音识别（ASR）系统的重要组成部分。传统的SD系统通常是基于音频的，独立于ASR运行，往往会在说话人转换和重叠语音的情况下引入说话人错误。最近，语言模型包括微调的大型语言模型（LLMs）通过利用转录输出中的词汇上下文，被证明是有效的第一次说话人错误校正器。在这项工作中，我们引入了一种新颖的声学条件方法，为LLM提供更多精细的来自声学聚类器的信息。我们还表明，更简单的约束解码策略减少了LLM的幻觉现象，同时避免了复杂的后处理过程。与第一次通过的声学SD相比，我们的方法在Fisher、Callhome和RT0subTitle&gt; 数据集上将说话人错误率降低了高达百分之二十四至四十三。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08421v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了将新型声学条件化方法应用于语音识别领域中的说话人识别（Speaker Diarization, SD）。文章提出一种新的声学条件化策略来改进传统独立操作的传统音频为基础的传统说话人识别技术的问题，如说话人转换和重叠语音时的说话人错误。该研究通过引入大型语言模型（LLM）的第二遍纠错，将声学日记（acoustdic diarizer）提供的精细粒度信息注入其中。此外，研究表明更简单的约束解码策略可降低LLM幻想。实验结果在所有测试的数据集上减少了多达高达43%的说话人错误率。总体来说，这是一个将声学技术和语言模型紧密结合的解决方案，实现了高效准确的说话人识别。 </p>
<p><strong>Key Takeaways</strong> </p>
<ol>
<li>传统说话人识别技术在处理音频时可能存在说话人转换和重叠语音时的错误问题。 </li>
<li>大型语言模型（LLM）已被证明可以有效利用语境进行说话人错误纠正。 </li>
<li>新提出的声学条件化策略旨在将声学日记中提供的精细粒度信息注入语言模型以提升说话人识别的准确性。 </li>
<li>简单约束解码策略能减少语言模型产生的幻觉现象，同时避免复杂的后期处理步骤。 </li>
<li>该方法显著降低了在Fisher、Callhome和RT03-CTS数据集上的说话人错误率，最高可达43%。 </li>
<li>这是结合了声学技术和语言模型的优秀示例，充分展现了技术在提高语音识别效率方面的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08421">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d11091bc2a31c17002e1002d4775bb47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35e0300001739c316a30a51cb86cd5db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1e60de61f651c0bf974562561ef5adf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-641bc304f07669e08f36c47325fa26ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eec18a4ca80c4ddfd282c086d4aec71.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adaptive-Noise-Tolerant-Network-for-Image-Segmentation"><a href="#Adaptive-Noise-Tolerant-Network-for-Image-Segmentation" class="headerlink" title="Adaptive Noise-Tolerant Network for Image Segmentation"></a>Adaptive Noise-Tolerant Network for Image Segmentation</h2><p><strong>Authors:Weizhi Li</strong></p>
<p>Unlike image classification and annotation, for which deep network models have achieved dominating superior performances compared to traditional computer vision algorithms, deep learning for automatic image segmentation still faces critical challenges. One of such hurdles is to obtain ground-truth segmentations as the training labels for deep network training. Especially when we study biomedical images, such as histopathological images (histo-images), it is unrealistic to ask for manual segmentation labels as the ground truth for training due to the fine image resolution as well as the large image size and complexity. In this paper, instead of relying on clean segmentation labels, we study whether and how integrating imperfect or noisy segmentation results from off-the-shelf segmentation algorithms may help achieve better segmentation results through a new Adaptive Noise-Tolerant Network (ANTN) model. We extend the noisy label deep learning to image segmentation with two novel aspects: (1) multiple noisy labels can be integrated into one deep learning model; (2) noisy segmentation modeling, including probabilistic parameters, is adaptive, depending on the given testing image appearance. Implementation of the new ANTN model on both the synthetic data and real-world histo-images demonstrates its effectiveness and superiority over off-the-shelf and other existing deep-learning-based image segmentation algorithms. </p>
<blockquote>
<p>与图像分类和标注不同，深度网络模型在传统计算机视觉算法中取得了卓越的性能优势，但深度学习在自动图像分割方面仍然面临重大挑战。其中一个障碍是获取真实分割作为深度网络训练的训练标签。尤其是当我们研究生物医学图像，如病理图像时，由于图像分辨率高、图像尺寸大且复杂，要求手动分割标签作为训练的真实标准是不现实的。在本文中，我们并不依赖干净的分割标签，而是研究将现成的分割算法产生的分割结果中可能存在的错误或噪声整合起来，通过一种新的自适应噪声容忍网络（ANTN）模型，探讨其是否能以及如何帮助我们获得更好的分割结果。我们将噪声标签深度学习扩展到图像分割领域，并引入两个新的观点：（1）多个噪声标签可以集成到一个深度学习模型中；（2）噪声分割建模包括概率参数是自适应的，这取决于给定的测试图像外观。在合成数据和真实世界病理图像上实现新的ANTN模型，证明了其在现成和其他现有基于深度学习的图像分割算法中的有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07163v2">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像分割领域仍面临挑战，尤其是获取高分辨率、大尺寸和复杂生物医学图像的手动分割标签作为训练依据不现实。本文研究如何通过自适应噪声容忍网络（ANTN）模型利用市场现成的分割算法产生的不完美或带噪声的分割结果来提升分割效果。该方法可将多个噪声标签集成到一个深度学习模型中，并且噪声分割建模具有适应性，取决于给定的测试图像外观。在合成数据和真实世界病理图像上的实验证明了ANTN模型的有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割面临挑战，主要由于获取手动分割标签作为训练依据不现实。</li>
<li>现有深度学习模型在图像分类和标注方面表现卓越，但在自动图像分割方面仍面临挑战。</li>
<li>本文引入自适应噪声容忍网络（ANTN）模型，能利用不完美或带噪声的分割结果。</li>
<li>ANTN模型可将多个噪声标签集成到一个深度学习模型中。</li>
<li>噪声分割建模具有适应性，能根据测试图像外观调整。</li>
<li>在合成数据和真实世界病理图像上的实验证明了ANTN模型的有效性和优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07163">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab62b745db79cf2ed793c48ced7bcf9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c2af8947e628bfe475b0b2486ef7864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c8f37798c2f489b11dffaf4aca3b111.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Pan-cancer-Classification-Model-using-Multi-view-Feature-Selection-Method-and-Ensemble-Classifier"><a href="#A-Pan-cancer-Classification-Model-using-Multi-view-Feature-Selection-Method-and-Ensemble-Classifier" class="headerlink" title="A Pan-cancer Classification Model using Multi-view Feature Selection   Method and Ensemble Classifier"></a>A Pan-cancer Classification Model using Multi-view Feature Selection   Method and Ensemble Classifier</h2><p><strong>Authors:Tareque Mohmud Chowdhury, Farzana Tabassum, Sabrina Islam, Abu Raihan Mostofa Kamal</strong></p>
<p>Accurately identifying cancer samples is crucial for precise diagnosis and effective patient treatment. Traditional methods falter with high-dimensional and high feature-to-sample count ratios, which are critical for classifying cancer samples. This study aims to develop a novel feature selection framework specifically for transcriptome data and propose two ensemble classifiers. For feature selection, we partition the transcriptome dataset vertically based on feature types. Then apply the Boruta feature selection process on each of the partitions, combine the results, and apply Boruta again on the combined result. We repeat the process with different parameters of Boruta and prepare the final feature set. Finally, we constructed two ensemble ML models based on LR, SVM and XGBoost classifiers with max voting and averaging probability approach. We used 10-fold cross-validation to ensure robust and reliable classification performance. With 97.11% accuracy and 0.9996 AUC value, our approach performs better compared to existing state-of-the-art methods to classify 33 types of cancers. A set of 12 types of cancer is traditionally challenging to differentiate between each other due to their similarity in tissue of origin. Our method accurately identifies over 90% of samples from these 12 types of cancers, which outperforms all known methods presented in existing literature. The gene set enrichment analysis reveals that our framework’s selected features have enriched the pathways highly related to cancers. This study develops a feature selection framework to select features highly related to cancer development and leads to identifying different types of cancer samples with higher accuracy. </p>
<blockquote>
<p>准确地识别癌症样本对于精确诊断和治疗患者至关重要。传统方法在处理高维度和高特征样本计数比时会出现问题，这对于分类癌症样本至关重要。本研究旨在开发一种专门用于转录组数据的新型特征选择框架，并提出两种集成分类器。在特征选择方面，我们根据特征类型垂直划分转录组数据集。然后在每个分区上应用Boruta特征选择过程，组合结果，并在组合结果上再次应用Boruta。我们重复该过程并使用Boruta的不同参数来准备最终特征集。最后，我们构建了基于LR、SVM和XGBoost分类器的两个集成机器学习模型，采用最大投票和平均概率方法。我们使用10倍交叉验证来确保稳健可靠的分类性能。我们的方法以97.11%的准确率和0.9996的AUC值，相较于目前最先进的分类方法，对33种类型的癌症进行分类表现更佳。由于组织起源的相似性，传统上区分这其中的十二种癌症类型是一大挑战。我们的方法准确地识别了超过90%的这些十二种癌症类型的样本，这优于现有文献中提到的所有已知方法。基因集富集分析表明，我们的框架选择的特征与高度相关的癌症途径高度富集。本研究开发了一个特征选择框架来选择与癌症发展高度相关的特征，导致更准确地区分不同类型的癌症样本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06805v1">PDF</a> 20 pages, 5 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>本文研究旨在开发一种针对转录组数据的新型特征选择框架，并提出两种集成分类器，以提高癌症样本分类的准确性。该研究通过Boruta特征选择过程对转录组数据集进行分区并组合结果，构建了两个基于LR、SVM和XGBoost分类器的集成机器学习模型。在33种癌症分类中，该方法的准确率为97.11%，AUC值为0.9996，优于现有最先进的方法。特别是在12种传统上难以区分的癌症中，该方法能准确识别超过90%的样本，超越了现有文献中的所有已知方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究旨在开发一种针对转录组数据的特征选择框架，以提高癌症样本分类的准确性。</li>
<li>通过分区和组合特征选择结果，使用Boruta特征选择过程。</li>
<li>构建了两个基于LR、SVM和XGBoost分类器的集成机器学习模型。</li>
<li>在33种癌症分类中，方法的准确率和AUC值均表现优异。</li>
<li>特别是在区分12种传统上难以区分的癌症时，该方法表现出高准确性。</li>
<li>基因集富集分析显示，所选特征与癌症相关途径高度相关。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06805">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c8e0ecde3067b063e4647614c5e16d8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6db956a6ccfa5d81dbcd7934360c9d83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31941f16a0bf80fd71789dea7835e0ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ad96c60166101cf8745500f3444bbb8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Rice-Leaf-Disease-Detection-A-Comparative-Study-Between-CNN-Transformer-and-Non-neural-Network-Architectures"><a href="#Rice-Leaf-Disease-Detection-A-Comparative-Study-Between-CNN-Transformer-and-Non-neural-Network-Architectures" class="headerlink" title="Rice Leaf Disease Detection: A Comparative Study Between CNN,   Transformer and Non-neural Network Architectures"></a>Rice Leaf Disease Detection: A Comparative Study Between CNN,   Transformer and Non-neural Network Architectures</h2><p><strong>Authors:Samia Mehnaz, Md. Touhidul Islam</strong></p>
<p>In nations such as Bangladesh, agriculture plays a vital role in providing livelihoods for a significant portion of the population. Identifying and classifying plant diseases early is critical to prevent their spread and minimize their impact on crop yield and quality. Various computer vision techniques can be used for such detection and classification. While CNNs have been dominant on such image classification tasks, vision transformers has become equally good in recent time also. In this paper we study the various computer vision techniques for Bangladeshi rice leaf disease detection. We use the Dhan-Shomadhan – a Bangladeshi rice leaf disease dataset, to experiment with various CNN and ViT models. We also compared the performance of such deep neural network architecture with traditional machine learning architecture like Support Vector Machine(SVM). We leveraged transfer learning for better generalization with lower amount of training data. Among the models tested, ResNet50 exhibited the best performance over other CNN and transformer-based models making it the optimal choice for this task. </p>
<blockquote>
<p>在孟加拉国等国家，农业在为很大一部分人口提供生计方面发挥着至关重要的作用。早期识别和分类植物疾病对防止其传播以及尽量减少其对作物产量和质量的影响至关重要。可以使用各种计算机视觉技术进行此类检测和分类。虽然卷积神经网络（CNN）在此类图像分类任务中占据主导地位，但视觉变压器（ViT）在最近的时间中也同样表现出色。在本文中，我们研究了孟加拉国水稻叶病检测的各种计算机视觉技术。我们使用孟加拉国水稻叶病数据集Dhan-Shomadhan，对各种CNN和ViT模型进行实验。我们还比较了这种深度神经网络架构与诸如支持向量机（SVM）等传统机器学习架构的性能。我们利用迁移学习，以在少量训练数据的情况下实现更好的泛化。在测试的模型中，ResNet50在其他CNN和基于Transformer的模型中表现出最佳性能，成为此任务的最佳选择。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06740v1">PDF</a> 6 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文研究了使用计算机视觉技术进行孟加拉国水稻叶片病害检测的方法。通过利用当地的Dhan-Shomadhan数据集进行实验，对比了CNN和ViT模型以及传统机器学习架构SVM的性能。借助迁移学习，模型在少量训练数据上表现出更好的泛化能力。结果表明，ResNet50相较于其他CNN和基于Transformer的模型表现最佳，成为此任务的最优选择。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>孟加拉国农业中水稻叶片病害检测对防止病害扩散和保障作物产量与质量至关重要。</li>
<li>使用了多种计算机视觉技术，包括CNN和ViT模型进行病害检测与分类。</li>
<li>利用Dhan-Shomadhan数据集进行实验，涵盖多种水稻叶片病害。</li>
<li>迁移学习增强了模型在有限数据上的泛化能力。</li>
<li>ResNet50模型在实验中表现最佳，相较于其他CNN和Transformer模型更具优势。</li>
<li>CNN模型在此任务中的表现优于传统的机器学习架构如SVM。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06740">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4b9a2875192e25383aa012b855034c25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0a763f1f45fa37c62ce320a2551c3cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a58bae6f03d7f23d21aa0a74dace69e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-633ed0700f61ea6af0528ef501f0365a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f99c56014ac4acb844d90b9d953c32c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da49fd9ee7daffe3ba0901aa559789b9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Integrating-Pause-Information-with-Word-Embeddings-in-Language-Models-for-Alzheimer’s-Disease-Detection-from-Spontaneous-Speech"><a href="#Integrating-Pause-Information-with-Word-Embeddings-in-Language-Models-for-Alzheimer’s-Disease-Detection-from-Spontaneous-Speech" class="headerlink" title="Integrating Pause Information with Word Embeddings in Language Models   for Alzheimer’s Disease Detection from Spontaneous Speech"></a>Integrating Pause Information with Word Embeddings in Language Models   for Alzheimer’s Disease Detection from Spontaneous Speech</h2><p><strong>Authors:Yu Pu, Wei-Qiang Zhang</strong></p>
<p>Alzheimer’s disease (AD) is a progressive neurodegenerative disorder characterized by cognitive decline and memory loss. Early detection of AD is crucial for effective intervention and treatment. In this paper, we propose a novel approach to AD detection from spontaneous speech, which incorporates pause information into language models. Our method involves encoding pause information into embeddings and integrating them into the typical transformer-based language model, enabling it to capture both semantic and temporal features of speech data. We conduct experiments on the Alzheimer’s Dementia Recognition through Spontaneous Speech (ADReSS) dataset and its extension, the ADReSSo dataset, comparing our method with existing approaches. Our method achieves an accuracy of 83.1% in the ADReSSo test set. The results demonstrate the effectiveness of our approach in discriminating between AD patients and healthy individuals, highlighting the potential of pauses as a valuable indicator for AD detection. By leveraging speech analysis as a non-invasive and cost-effective tool for AD detection, our research contributes to early diagnosis and improved management of this debilitating disease. </p>
<blockquote>
<p>阿尔茨海默病（AD）是一种进行性神经退行性疾病，以认知衰退和记忆丧失为特征。阿尔茨海默病的早期发现对于有效的干预和治疗至关重要。在本文中，我们提出了一种结合停顿信息用于阿尔茨海默病检测的新方法。我们从自发语言中检测阿尔茨海默病，并将停顿信息编码成嵌入形式，并将其集成到典型的基于转换器的语言模型中，使其能够捕捉语音数据的语义和时间特征。我们在阿尔茨海默氏症通过自发语言进行痴呆识别（ADReSS）数据集及其扩展数据集ADReSSo上进行了实验，并将我们的方法与现有方法进行了比较。我们的方法在ADReSSo测试集上达到了83.1%的准确率。结果表明，我们的方法在区分阿尔茨海默病患者和健康个体方面非常有效，突显了停顿作为阿尔茨海默病检测的重要指标的潜力。通过利用语音分析作为非侵入性和成本效益高的工具进行阿尔茨海默病检测，我们的研究为这种疾病的早期诊断和治疗管理做出了贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06727v1">PDF</a> accepted by ICASSP2025. Copyright 2025 IEEE. Personal use of this   material is permitted. Permission from IEEE must be obtained for all other   uses, in any current or future media, including reprinting&#x2F;republishing this   material for advertising or promotional purposes, creating new collective   works, for resale or redistribution to servers or lists, or reuse of any   copyrighted component</p>
<p><strong>Summary</strong><br>     本文提出了一种结合停顿信息检测阿尔茨海默病的新方法。该方法将停顿信息编码为嵌入形式，并整合到基于转换器的语言模型中，从而捕捉语音数据的语义和时间特征。在阿尔茨海默症痴呆症通过自发语言识别（ADReSS）数据集及其扩展数据集ADReSSo上进行实验，该方法实现了83.1%的准确率，表明停顿可能是检测阿尔茨海默症的重要指标。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章介绍了一种利用停顿信息检测阿尔茨海默病的新方法。</li>
<li>该方法整合了停顿信息到语言模型中，使其能够捕捉语音数据的语义和时间特征。</li>
<li>方法在ADReSS和ADReSSo数据集上进行了实验验证。</li>
<li>实验结果显示该方法准确率为83.1%，显示出停顿信息在AD检测中的重要性。</li>
<li>此方法为非侵入性和成本效益高的阿尔茨海默病检测工具提供了新的可能性。</li>
<li>研究结果有助于阿尔茨海默病的早期诊断和治疗管理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06727">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-308725e1b9b06ad1106e766c290cc24f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb2c2249c93cb1b3fd3f2d5c839691b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3098c0571ec870e406842c8411e60103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-347961b619d3ce89ce0346b8f5bf1a8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-689538c34924c6431e8c3547658282c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bae574aeae43fac13816f0d2df39cc53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-717d8e5ae5a65d10cab61c16e1fed327.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Multi-task-Visual-Grounding-with-Coarse-to-Fine-Consistency-Constraints"><a href="#Multi-task-Visual-Grounding-with-Coarse-to-Fine-Consistency-Constraints" class="headerlink" title="Multi-task Visual Grounding with Coarse-to-Fine Consistency Constraints"></a>Multi-task Visual Grounding with Coarse-to-Fine Consistency Constraints</h2><p><strong>Authors:Ming Dai, Jian Li, Jiedong Zhuang, Xian Zhang, Wankou Yang</strong></p>
<p>Multi-task visual grounding involves the simultaneous execution of localization and segmentation in images based on textual expressions. The majority of advanced methods predominantly focus on transformer-based multimodal fusion, aiming to extract robust multimodal representations. However, ambiguity between referring expression comprehension (REC) and referring image segmentation (RIS) is error-prone, leading to inconsistencies between multi-task predictions. Besides, insufficient multimodal understanding directly contributes to biased target perception. To overcome these challenges, we propose a Coarse-to-fine Consistency Constraints Visual Grounding architecture ($\text{C}^3\text{VG}$), which integrates implicit and explicit modeling approaches within a two-stage framework. Initially, query and pixel decoders are employed to generate preliminary detection and segmentation outputs, a process referred to as the Rough Semantic Perception (RSP) stage. These coarse predictions are subsequently refined through the proposed Mask-guided Interaction Module (MIM) and a novel explicit bidirectional consistency constraint loss to ensure consistent representations across tasks, which we term the Refined Consistency Interaction (RCI) stage. Furthermore, to address the challenge of insufficient multimodal understanding, we leverage pre-trained models based on visual-linguistic fusion representations. Empirical evaluations on the RefCOCO, RefCOCO+, and RefCOCOg datasets demonstrate the efficacy and soundness of $\text{C}^3\text{VG}$, which significantly outperforms state-of-the-art REC and RIS methods by a substantial margin. Code and model will be available at \url{<a target="_blank" rel="noopener" href="https://github.com/Dmmm1997/C3VG%7D">https://github.com/Dmmm1997/C3VG}</a>. </p>
<blockquote>
<p>多任务视觉定位涉及基于文本表达的同时图像定位和分割。大多数先进的方法主要关注基于变压器的多模式融合，旨在提取鲁棒的多模式表示。然而，指代表达式理解（REC）和指代图像分割（RIS）之间的歧义容易出错，导致多任务预测之间的不一致。此外，对多模式的理解不足直接导致目标感知的偏见。为了克服这些挑战，我们提出了一种从粗到细的的一致性约束视觉定位架构（C^3VG），该架构在两阶段框架内集成了隐式和显式建模方法。首先，使用查询和像素解码器生成初步的检测和分割输出，这一过程被称为粗糙语义感知（RSP）阶段。这些粗略的预测随后通过提出的Mask引导交互模块（MIM）和一种新颖的显式双向一致性约束损失进行细化，以确保任务间的表示一致性，我们称之为精细化一致性交互（RCI）阶段。此外，为了解决对多模式理解不足的挑战，我们利用基于视觉语言融合表示的预训练模型。在RefCOCO、RefCOCO+和RefCOCOg数据集上的经验评估表明，C^3VG的有效性和稳健性，它显著优于最新的REC和RIS方法。代码和模型将在\url{<a target="_blank" rel="noopener" href="https://github.com/Dmmm1997/C3VG%7D%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Dmmm1997/C3VG}上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06710v1">PDF</a> AAAI2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为$\text{C}^3\text{VG}$的视觉定位架构，该架构通过整合隐式和显式建模方法解决多任务视觉定位中的指代表达理解与指代图像分割之间的歧义问题。架构分为两个阶段：初步语义感知阶段和精细一致性交互阶段。初步阶段利用查询和像素解码器生成初步检测和分割输出；在第二阶段，通过掩膜引导交互模块和新型双向一致性约束损失函数对初步预测进行精细化，确保跨任务的表示一致性。同时，借助视觉-语言融合表示的预训练模型解决多媒体模态理解不足的问题。在RefCOCO、RefCOCO+和RefCOCOg数据集上的实证评估证明了$\text{C}^3\text{VG}$的有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多任务视觉定位涉及同时执行图像的定位和分割任务，基于文本表达进行。</li>
<li>当前方法主要关注于基于转换器的多媒体融合，旨在提取稳健的多媒体表示。</li>
<li>指代表达理解与指代图像分割之间的歧义导致预测不一致。</li>
<li>$\text{C}^3\text{VG}$架构分为两个阶段解决这些问题：初步语义感知和精细一致性交互。</li>
<li>架构利用掩膜引导交互模块和一致性约束损失函数确保跨任务的一致性表示。</li>
<li>利用预训练的视觉-语言融合模型解决多媒体模态理解不足的问题。</li>
<li>在多个数据集上的实证评估证明了$\text{C}^3\text{VG}$的有效性和优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06710">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7150f71a52b9b07d72269bb21cdd4a27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30b117a4245e895316bd8a8f0cf8b6b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5be611f5aa9b0ee0c8ea88fa0a01a6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-056b61c3e28eb1ae4eae7b9ca398a1be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df32453a4aa3b19bda906207d8701c8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1cb8a02182212851c256da2e4b3c42a4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RMTransformer-Accurate-Radio-Map-Construction-and-Coverage-Prediction"><a href="#RMTransformer-Accurate-Radio-Map-Construction-and-Coverage-Prediction" class="headerlink" title="RMTransformer: Accurate Radio Map Construction and Coverage Prediction"></a>RMTransformer: Accurate Radio Map Construction and Coverage Prediction</h2><p><strong>Authors:Yuxuan Li, Cheng Zhang, Wen Wang, Yongming Huang</strong></p>
<p>Radio map, or pathloss map prediction, is a crucial method for wireless network modeling and management. By leveraging deep learning to construct pathloss patterns from geographical maps, an accurate digital replica of the transmission environment could be established with less computational overhead and lower prediction error compared to traditional model-driven techniques. While existing state-of-the-art (SOTA) methods predominantly rely on convolutional architectures, this paper introduces a hybrid transformer-convolution model, termed RMTransformer, to enhance the accuracy of radio map prediction. The proposed model features a multi-scale transformer-based encoder for efficient feature extraction and a convolution-based decoder for precise pixel-level image reconstruction. Simulation results demonstrate that the proposed scheme significantly improves prediction accuracy, and over a 30% reduction in root mean square error (RMSE) is achieved compared to typical SOTA approaches. </p>
<blockquote>
<p>无线电地图或路径损耗地图预测是无线网络建模和管理的重要方法。通过利用深度学习从地理地图构建路径损耗模式，可以建立传输环境的精确数字副本，与传统的模型驱动技术相比，计算开销更小，预测误差更低。虽然现有最先进的方法主要依赖于卷积架构，但本文引入了一种混合的transformer-卷积模型，称为RMTransformer，以提高无线电地图预测的准确性。所提出的模型采用基于多尺度transformer的编码器进行高效特征提取和基于卷积的解码器进行精确的像素级图像重建。仿真结果表明，该方案显著提高预测精度，与典型的最新方法相比，均方根误差（RMSE）降低了30%以上。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05190v2">PDF</a> Submitted to IEEE VTC 2025 Spring</p>
<p><strong>Summary</strong><br>     本文介绍了利用深度学习构建地理地图的路径损耗模式以进行无线电地图预测的方法。文章提出一种混合transformer-卷积模型RMTransformer，采用基于多尺度transformer的编码器进行高效特征提取和基于卷积的解码器进行精确像素级图像重建，提高了无线电地图预测的精度。仿真结果表明，该方案实现了较高的预测精度，与现有先进技术相比，均方根误差降低了超过30%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>无线电地图预测在无线网络建模和管理中是关键方法。</li>
<li>深度学习被用于从地理地图构建路径损耗模式。</li>
<li>现有先进技术主要依赖卷积架构。</li>
<li>论文提出一种混合的transformer-卷积模型RMTransformer。</li>
<li>RMTransformer包括一个多尺度基于transformer的编码器和基于卷积的解码器。</li>
<li>仿真结果显示RMTransformer显著提高预测精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-186c11b8b6c4f52544f7ff748ed6181d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27dd3eb6a1333f8a76329458f1e39aca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9ac3a916a8fce9602563053ed68b05a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94560f4f33436bf5dc340fb44c79cb07.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="D3RM-A-Discrete-Denoising-Diffusion-Refinement-Model-for-Piano-Transcription"><a href="#D3RM-A-Discrete-Denoising-Diffusion-Refinement-Model-for-Piano-Transcription" class="headerlink" title="D3RM: A Discrete Denoising Diffusion Refinement Model for Piano   Transcription"></a>D3RM: A Discrete Denoising Diffusion Refinement Model for Piano   Transcription</h2><p><strong>Authors:Hounsu Kim, Taegyun Kwon, Juhan Nam</strong></p>
<p>Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion model’s refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available in <a target="_blank" rel="noopener" href="https://github.com/hanshounsu/d3rm">https://github.com/hanshounsu/d3rm</a>. </p>
<blockquote>
<p>扩散模型由于其在对复杂数据分布进行建模时的出色表现，在生成领域得到了广泛应用。此外，在判别任务（如图像分割）中，它们也展现出了具有竞争力的结果。尽管扩散模型在音乐自动转录方面也有所探索，但其性能尚未达到竞争水平。本文中，我们重点关注离散扩散模型的优化能力，并为钢琴转录提出了一种新型架构。我们的模型利用邻域注意力层作为去噪模块，根据预训练声学模型的微调特征，逐步预测目标高分辨率的钢琴乐谱。为了进一步提高优化效果，我们设计了一种新型策略，在离散扩散模型的训练和推理阶段应用不同的过渡状态。在MAESTRO数据集上的实验表明，我们的方法在F1分数方面优于之前的基于扩散的钢琴转录模型和基线模型。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/hanshounsu/d3rm%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hanshounsu/d3rm中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05068v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了离散扩散模型在钢琴音乐转录方面的应用。该模型利用邻域注意力层作为去噪模块，逐渐预测目标高分辨率钢琴卷，同时基于预训练的声学模型的微调特征进行条件处理。为提高预测精度，本文还提出了一种在离散扩散模型的训练和推理阶段应用不同过渡状态的新策略。实验表明，该方法在MAESTRO数据集上的表现优于先前的扩散模型钢琴转录方法和基线模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>离散扩散模型被用于钢琴音乐转录，展示其在音乐生成领域的潜力。</li>
<li>邻域注意力层作为去噪模块，提高了模型的预测能力。</li>
<li>模型利用预训练的声学模型的微调特征进行条件处理，以生成高分辨率的钢琴卷。</li>
<li>在训练和推理阶段采用不同的过渡状态，增强了模型的性能。</li>
<li>模型在MAESTRO数据集上的表现优于其他扩散模型钢琴转录方法和基线模型。</li>
<li>模型代码已公开发布在GitHub上供研究人员参考和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05068">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d1f8ac938795271e0f866fbccf3f8ed7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44a494d972de7b54258131ca8a6c6ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92d3dd22fcd9d11ebc0f706a7dfe86a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2016f224be32580eb20bd95e37c89a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db6c4ab16f403b2afdbca1b14f60b2ec.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis"><a href="#SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis" class="headerlink" title="SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis"></a>SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis</h2><p><strong>Authors:Runci Bai</strong></p>
<p>Brain tumors can result in neurological dysfunction, alterations in cognitive and psychological states, increased intracranial pressure, and the occurrence of seizures, thereby presenting a substantial risk to human life and health. The You Only Look Once(YOLO) series models have demonstrated superior accuracy in object detection for medical imaging. In this paper, we develop a novel SCC-YOLO architecture by integrating the SCConv attention mechanism into YOLOv9. The SCConv module reconstructs an efficient convolutional module by reducing spatial and channel redundancy among features, thereby enhancing the learning of image features. We investigate the impact of intergrating different attention mechanisms with the YOLOv9 model on brain tumor image detection using both the Br35H dataset and our self-made dataset(Brain_Tumor_Dataset). Experimental results show that on the Br35H dataset, SCC-YOLO achieved a 0.3% improvement in mAp50 compared to YOLOv9, while on our self-made dataset, SCC-YOLO exhibited a 0.5% improvement over YOLOv9. SCC-YOLO has reached state-of-the-art performance in brain tumor detection. Source code is available at : <a target="_blank" rel="noopener" href="https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master">https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master</a> </p>
<blockquote>
<p>脑肿瘤可能导致神经功能障碍、认知和心理状态改变、颅内压升高以及癫痫发作，从而对人类生命和健康构成重大风险。You Only Look Once（YOLO）系列模型在医学图像目标检测中表现出了卓越的准确性。在本文中，我们通过将SCConv注意力机制融入YOLOv9，开发了一种新型的SCC-YOLO架构。SCConv模块通过减少特征之间的空间和通道冗余性，重建了一个高效的卷积模块，从而增强了图像特征的学习。我们使用Br35H数据集和我们自制的Brain_Tumor_Dataset数据集，研究了将不同注意力机制与YOLOv9模型集成对脑肿瘤图像检测的影响。实验结果表明，在Br35H数据集上，SCC-YOLO相较于YOLOv9在mAp50上提高了0.3%；而在我们自制的数据库上，SCC-YOLO相较于YOLOv9提高了0.5%。SCC-YOLO已经达到了脑肿瘤检测的最先进性能。源代码可访问：<a target="_blank" rel="noopener" href="https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master%E3%80%82">https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03836v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本论文开发了一种新型的SCC-YOLO架构，通过整合SCConv注意力机制到YOLOv9中，以提高对医学图像中的脑肿瘤检测精度。实验结果显示，在Br35H数据集上，SCC-YOLO相较于YOLOv9提高了0.3%的mAp50；在自定义数据集Brain_Tumor_Dataset上，其检测效果提升达到0.5%。当前模型已成为脑肿瘤检测领域的先进成果之一。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>脑肿瘤对人类生命和健康存在重大风险，可能导致神经功能障碍、认知和心理状态改变、颅内压升高和癫痫发作。</li>
<li>YOLO系列模型在医学图像目标检测中具有优越的准确性。</li>
<li>本研究开发了SCC-YOLO架构，通过整合SCConv注意力机制到YOLOv9中，旨在提高脑肿瘤检测的准确性。</li>
<li>SCConv模块通过减少特征间的空间通道冗余来重构有效的卷积模块，从而增强图像特征的学习。</li>
<li>实验结果显示，在Br35H数据集和自定义Brain_Tumor_Dataset数据集上，SCC-YOLO相较于YOLOv9均有所提升。</li>
<li>SCC-YOLO已达到脑肿瘤检测的领先水平，源代码已公开。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03836">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-16881751de744c4feb2a10914442530f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0200fff1472ec9f1188675aa523f4815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3113e51cc7f179389457eeb3dd6aba4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fb20d29c3ccd29bb579a7100430c262.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7ebcc1540b1cb9d88926ff4ca93e495.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ae871e237ac575b1ca1551f2c82448f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-820225500646ae0b15fbed7ad322161f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d7f704436e94c77a517078cc0fb32f0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FashionFAE-Fine-grained-Attributes-Enhanced-Fashion-Vision-Language-Pre-training"><a href="#FashionFAE-Fine-grained-Attributes-Enhanced-Fashion-Vision-Language-Pre-training" class="headerlink" title="FashionFAE: Fine-grained Attributes Enhanced Fashion Vision-Language   Pre-training"></a>FashionFAE: Fine-grained Attributes Enhanced Fashion Vision-Language   Pre-training</h2><p><strong>Authors:Jiale Huang, Dehong Gao, Jinxia Zhang, Zechao Zhan, Yang Hu, Xin Wang</strong></p>
<p>Large-scale Vision-Language Pre-training (VLP) has demonstrated remarkable success in the general domain. However, in the fashion domain, items are distinguished by fine-grained attributes like texture and material, which are crucial for tasks such as retrieval. Existing models often fail to leverage these fine-grained attributes from both text and image modalities. To address the above issues, we propose a novel approach for the fashion domain, Fine-grained Attributes Enhanced VLP (FashionFAE), which focuses on the detailed characteristics of fashion data. An attribute-emphasized text prediction task is proposed to predict fine-grained attributes of the items. This forces the model to focus on the salient attributes from the text modality. Additionally, a novel attribute-promoted image reconstruction task is proposed, which further enhances the fine-grained ability of the model by leveraging the representative attributes from the image modality. Extensive experiments show that FashionFAE significantly outperforms State-Of-The-Art (SOTA) methods, achieving 2.9% and 5.2% improvements in retrieval on sub-test and full test sets, respectively, and a 1.6% average improvement in recognition tasks. </p>
<blockquote>
<p>大规模视觉语言预训练（VLP）在通用领域已经取得了显著的成功。然而，在时尚领域，物品的区别在于细粒度属性，如质地和材质，这对于检索等任务至关重要。现有模型往往无法从文本和图像两种模态中利用这些细粒度属性。为了解决上述问题，我们针对时尚领域提出了一种新的方法，即细粒度属性增强VLP（FashionFAE），它专注于时尚数据的详细特征。提出了一种属性强调文本预测任务，用于预测物品的细粒度属性。这迫使模型关注文本模态的显著属性。此外，还提出了一种新的属性促进图像重建任务，通过利用图像模态的代表属性，进一步提高了模型的细粒度能力。大量实验表明，FashionFAE显著优于最新方法，在子测试集和全集测试集的检索任务上分别提高了2.9%和5.2%的准确率，在识别任务上平均提高了1.6%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19997v2">PDF</a> 5 pages, Accepted by ICASSP2025, full paper</p>
<p><strong>Summary</strong></p>
<p>大规模视觉语言预训练（VLP）在通用领域取得了显著的成功，但在时尚领域，由于时尚产品之间主要通过精细属性（如纹理和材料）进行区分，现有模型往往无法充分利用这些精细属性。为解决这一问题，本文提出了一种针对时尚领域的新型方法——精细属性增强视觉语言预训练（FashionFAE），它侧重于捕捉时尚数据的详细特征。通过构建注重属性的文本预测任务和促进属性的图像重建任务，FashionFAE能够从文本和图像中提取和强化这些精细属性信息。实验表明，FashionFAE在检索任务上显著优于现有方法，在子测试集和全测试集上的检索性能分别提高了2.9%和5.2%，并在识别任务上平均提高了1.6%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有模型在时尚领域无法充分利用精细属性（如纹理和材料）。</li>
<li>FashionFAE是一种针对时尚领域的新型预训练方法，旨在捕捉时尚数据的详细特征。</li>
<li>FashionFAE通过构建注重属性的文本预测任务来提高模型对文本模态中显著属性的关注。</li>
<li>FashionFAE引入了一种新的属性促进图像重建任务，通过利用图像模态中的代表性属性来增强模型的精细属性能力。</li>
<li>实验结果表明，FashionFAE在时尚检索任务上显著优于现有方法。</li>
<li>在子测试集和全测试集上，FashionFAE的检索性能分别提高了2.9%和5.2%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19997">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b000e8dad3cacdb54143b84341eb286.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9603e429e48a5d2959e4b318a81750c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73d41d1e76fa53a3b4fe2a7ee0e85eb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5593878e966e9c41f0b6d9828ff416f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b86867b42e265ea1ebe265defe3442.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6aeefbcb52e0e978fe859442b06afddd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56ac66586e68001e088f6a65f4ceeeca.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Evaluation-of-radiomic-feature-harmonization-techniques-for-benign-and-malignant-pulmonary-nodules"><a href="#Evaluation-of-radiomic-feature-harmonization-techniques-for-benign-and-malignant-pulmonary-nodules" class="headerlink" title="Evaluation of radiomic feature harmonization techniques for benign and   malignant pulmonary nodules"></a>Evaluation of radiomic feature harmonization techniques for benign and   malignant pulmonary nodules</h2><p><strong>Authors:Claire Huchthausen, Menglin Shi, Gabriel L. A. de Sousa, Jonathan Colen, Emery Shelley, James Larner, Einsley Janowski, Krishni Wijesooriya</strong></p>
<p>BACKGROUND: Radiomics provides quantitative features of pulmonary nodules (PNs) which could aid lung cancer diagnosis, but medical image acquisition variability is an obstacle to clinical application. Acquisition effects may differ between radiomic features from benign vs. malignant PNs. PURPOSE: We evaluated how to account for differences between benign and malignant PNs when correcting radiomic features’ acquisition dependency. METHODS: We used 567 chest CT scans grouped as benign, malignant, or lung cancer screening (mixed benign, malignant). ComBat harmonization was applied to extracted features for variation in 4 acquisition parameters. We compared: harmonizing without distinction, harmonizing with a covariate to preserve distinctions between subgroups, and harmonizing subgroups separately. Significant ($p\le0.05$) Kruskal-Wallis tests showed whether harmonization removed acquisition dependency. A LASSO-SVM pipeline was trained on successfully harmonized features to predict malignancy. To evaluate predictive information in these features, the trained harmonization estimators and predictive model were applied to unseen test sets. Harmonization and predictive performance were assessed for 10 trials of 5-fold cross-validation. RESULTS: An average 2.1% of features (95% CI:1.9-2.4%) were acquisition-independent when harmonized without distinction, 27.3% (95% CI:25.7-28.9%) when harmonized with a covariate, and 90.9% (95% CI:90.4-91.5%) when harmonized separately. Data harmonized separately or with a covariate trained models with higher ROC-AUC for screening scans than data harmonized without distinction between benign and malignant PNs (Delong test, adjusted $p\le0.05$). CONCLUSIONS: Radiomic features of benign and malignant PNs need different corrective transformations to recover acquisition-independent distributions. This can be done by harmonizing separately or with a covariate. </p>
<blockquote>
<p>背景：放射学提供有关肺结节的定量特征，有助于肺癌的诊断，但医学图像采集的差异性是临床应用中的障碍。良性肺结节与恶性肺结节之间的放射学特征可能因采集效果而有所不同。目的：我们评估了在纠正放射学特征的采集依赖性时，如何区分良性和恶性肺结节。方法：我们使用567张胸部CT扫描图像，按良性、恶性或肺癌筛查（混合良性、恶性）进行分组。应用ComBat方法对四个采集参数的差异进行协调处理。我们比较了三种协调方法：无区别的协调处理、用协变量协调处理以保留各亚组之间的差异以及分别协调处理亚组。通过显著性（p≤0.05）的Kruskal-Wallis检验来确定协调处理是否消除了采集依赖性。在成功协调的特征上应用了LASSO-SVM管道来预测恶性程度。为了评估这些特征中的预测信息，将训练好的协调估计器和预测模型应用于未见过的测试集上。对经过十次5倍交叉验证的协调处理和预测性能进行了评估。结果：当无区别地进行协调处理时，平均有2.1%（95%置信区间：1.9%-2.4%）的特征是独立于采集的；当用协变量进行协调处理时，有27.3%（95%置信区间：25.7%-28.9%）的特征独立于采集；当分别进行协调处理时，有90.9%（95%置信区间：90.4%-91.5%）的特征独立于采集。与无区别地协调处理良性和恶性肺结节的数据相比，分别或带有协变量协调处理的数据为筛查扫描提供了更高的ROC-AUC模型（经过调整的Delong检验，p≤0.05）。结论：良性和恶性肺结节的放射学特征需要不同的校正转换来恢复独立于采集的分布。这可以通过分别协调处理或使用协变量来实现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16758v2">PDF</a> 15 pages, 3 figures, plus supplemental material; updated author list,   corrected result in paragraph 3 of Discussion, updated Figure S1</p>
<p><strong>Summary</strong></p>
<p>本文研究了放射组学特征在肺结节诊断中的应用，并探讨了如何消除不同图像采集对放射组学特征的影响。通过对不同采集参数下的放射组学特征进行校正，研究发现在区分良恶性肺结节时，需要分别进行校正以获取独立的分布特征。通过应用特定的校正方法，可以提高对肺癌筛查扫描的预测模型的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>放射组学特征对于肺结节的良恶性诊断具有重要意义。</li>
<li>图像采集参数的差异会对放射组学特征产生影响，从而影响诊断的准确性。</li>
<li>通过对放射组学特征进行校正，可以消除图像采集参数的影响，提高诊断的准确性。</li>
<li>在区分良恶性肺结节时，需要分别进行校正以获得独立的分布特征。</li>
<li>应用特定的校正方法可以提高预测模型的性能。</li>
<li>校正方法包括无区别的校正、带有协变量的校正和分别校正等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16758">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dbd335f95c59728fc1d639f373346b4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a290ffb49d344fd032b674b03f4fda.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-43adc801f1d3aa8a7f76510ab47e37b1.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-01-17  Speech Synthesis along Perceptual Voice Quality Dimensions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fd00bff6af50ecf21e69d3f599c69f96.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-17  Boosting Diffusion Guidance via Learning Degradation-Aware Models for   Blind Super Resolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19710k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
