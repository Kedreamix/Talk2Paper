<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-09  Stratified GRPO Handling Structural Heterogeneity in Reinforcement   Learning of LLM Search Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-02305d22bafeeeab946b271855424e24~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945745&auth_key=1759945745-0-0-dcdd100f5695792c8d161590b5f6ed9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-09-æ›´æ–°"><a href="#2025-10-09-æ›´æ–°" class="headerlink" title="2025-10-09 æ›´æ–°"></a>2025-10-09 æ›´æ–°</h1><h2 id="Stratified-GRPO-Handling-Structural-Heterogeneity-in-Reinforcement-Learning-of-LLM-Search-Agents"><a href="#Stratified-GRPO-Handling-Structural-Heterogeneity-in-Reinforcement-Learning-of-LLM-Search-Agents" class="headerlink" title="Stratified GRPO: Handling Structural Heterogeneity in Reinforcement   Learning of LLM Search Agents"></a>Stratified GRPO: Handling Structural Heterogeneity in Reinforcement   Learning of LLM Search Agents</h2><p><strong>Authors:Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia</strong></p>
<p>Large language model (LLM) agents increasingly rely on external tools such as search engines to solve complex, multi-step problems, and reinforcement learning (RL) has become a key paradigm for training them. However, the trajectories of search agents are structurally heterogeneous, where variations in the number, placement, and outcomes of search calls lead to fundamentally different answer directions and reward distributions. Standard policy gradient methods, which use a single global baseline, suffer from what we identify and formalize as cross-stratum bias-an â€œapples-to-orangesâ€ comparison of heterogeneous trajectories. This cross-stratum bias distorts credit assignment and hinders exploration of complex, multi-step search strategies. To address this, we propose Stratified GRPO, whose central component, Stratified Advantage Normalization (SAN), partitions trajectories into homogeneous strata based on their structural properties and computes advantages locally within each stratum. This ensures that trajectories are evaluated only against their true peers. Our analysis proves that SAN eliminates cross-stratum bias, yields conditionally unbiased unit-variance estimates inside each stratum, and retains the global unbiasedness and unit-variance properties enjoyed by standard normalization, resulting in a more pure and scale-stable learning signal. To improve practical stability under finite-sample regimes, we further linearly blend SAN with the global estimator. Extensive experiments on diverse single-hop and multi-hop question-answering benchmarks demonstrate that Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3 points, achieving higher training rewards, greater training stability, and more effective search policies. These results establish stratification as a principled remedy for structural heterogeneity in RL for LLM search agents. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¶Šæ¥è¶Šä¾èµ–æœç´¢å¼•æ“ç­‰å¤–éƒ¨å·¥å…·æ¥è§£å†³å¤æ‚çš„å¤šæ­¥éª¤é—®é¢˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºè®­ç»ƒå®ƒä»¬çš„å…³é”®èŒƒå¼ã€‚ç„¶è€Œï¼Œæœç´¢ä»£ç†çš„è½¨è¿¹åœ¨ç»“æ„ä¸Šå…·æœ‰å¼‚è´¨æ€§ï¼Œæœç´¢å‘¼å«çš„æ•°é‡ã€ä½ç½®å’Œç»“æœçš„å˜åŒ–å¯¼è‡´ç­”æ¡ˆæ–¹å‘å’Œå¥–åŠ±åˆ†å¸ƒçš„æ ¹æœ¬ä¸åŒã€‚æ ‡å‡†ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä½¿ç”¨å•ä¸€çš„å…¨çƒåŸºçº¿ï¼Œè¿™ä¼šå¯¼è‡´æˆ‘ä»¬ç¡®å®šå’Œå½¢å¼åŒ–çš„è·¨é˜¶å±‚åè§â€”â€”å¯¹å¼‚è´¨è½¨è¿¹è¿›è¡Œâ€œè‹¹æœå’Œæ¡”å­â€çš„æ¯”è¾ƒã€‚è¿™ç§è·¨é˜¶å±‚åè§ä¼šæ‰­æ›²ä¿¡ç”¨åˆ†é…å¹¶é˜»ç¢å¤æ‚çš„å¤šæ­¥éª¤æœç´¢ç­–ç•¥çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚GRPOï¼Œå…¶æ ¸å¿ƒç»„ä»¶åˆ†å±‚ä¼˜åŠ¿å½’ä¸€åŒ–ï¼ˆSANï¼‰æ ¹æ®ç»“æ„ç‰¹æ€§å°†è½¨è¿¹åˆ’åˆ†ä¸ºå‡åŒ€çš„é˜¶å±‚ï¼Œå¹¶åœ¨æ¯ä¸ªé˜¶å±‚å†…éƒ¨å±€éƒ¨è®¡ç®—ä¼˜åŠ¿ã€‚è¿™ç¡®ä¿äº†è½¨è¿¹åªä¸å…¶çœŸæ­£çš„åŒè¡Œè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„åˆ†æè¯æ˜ï¼ŒSANæ¶ˆé™¤äº†è·¨é˜¶å±‚åè§ï¼Œåœ¨æ¯ä¸ªé˜¶å±‚å†…éƒ¨äº§ç”Ÿæœ‰æ¡ä»¶çš„æ— åå•ä½æ–¹å·®ä¼°è®¡ï¼Œå¹¶ä¿ç•™äº†æ ‡å‡†å½’ä¸€åŒ–æ‰€äº«æœ‰çš„å…¨å±€æ— åæ€§å’Œå•ä½æ–¹å·®å±æ€§ï¼Œä»è€Œäº§ç”Ÿæ›´çº¯å‡€å’Œæ›´ç¨³å®šçš„å­¦ä¹ ä¿¡å·ã€‚ä¸ºäº†åœ¨å®é™…æœ‰é™æ ·æœ¬æ¡ä»¶ä¸‹æé«˜ç¨³å®šæ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å°†SANä¸å…¨å±€ä¼°è®¡å™¨çº¿æ€§æ··åˆã€‚åœ¨å¤šç§å•è·³å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåˆ†å±‚GRPOåœ¨å„ä¸ªæ–¹é¢å§‹ç»ˆä¸”å¤§å¹…åº¦åœ°ä¼˜äºGRPOï¼Œæœ€é«˜å¯è¾¾11.3åˆ†ï¼Œå®ç°æ›´é«˜çš„è®­ç»ƒå¥–åŠ±ã€æ›´å¤§çš„è®­ç»ƒç¨³å®šæ€§å’Œæ›´æœ‰æ•ˆçš„æœç´¢ç­–ç•¥ã€‚è¿™äº›ç»“æœè¯æ˜äº†åˆ†å±‚æ˜¯è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æœç´¢ä»£ç†å¼ºåŒ–å­¦ä¹ ä¸­ç»“æ„æ€§å¼‚è´¨é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06214v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¶Šæ¥è¶Šå¤šåœ°ä¾èµ–æœç´¢å¼•æ“ç­‰å¤–éƒ¨å·¥å…·æ¥è§£å†³å¤æ‚çš„å¤šæ­¥éª¤é—®é¢˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºè®­ç»ƒå®ƒä»¬çš„å…³é”®èŒƒå¼ã€‚ç„¶è€Œï¼Œæœç´¢ä»£ç†çš„è½¨è¿¹åœ¨ç»“æ„ä¸Šå­˜åœ¨å¼‚è´¨æ€§ï¼Œæœç´¢è°ƒç”¨çš„æ•°é‡ã€ä½ç½®å’Œç»“æœçš„ä¸åŒä¼šå¯¼è‡´ç­”æ¡ˆæ–¹å‘å’Œå¥–åŠ±åˆ†å¸ƒçš„æ ¹æœ¬æ€§å·®å¼‚ã€‚æ ‡å‡†ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä½¿ç”¨å•ä¸€å…¨å±€åŸºå‡†çº¿ï¼Œä¼šé­å—æˆ‘ä»¬ç¡®å®šä¸ºè·¨é˜¶å±‚åå·®çš„å½±å“ï¼Œè¿™æ˜¯å¯¹ç»“æ„å¼‚è´¨æ€§è½¨è¿¹çš„â€œè‹¹æœä¸æ©™å­â€çš„æ¯”è¾ƒã€‚è¿™ç§è·¨é˜¶å±‚åå·®ä¼šæ‰­æ›²ä¿¡ç”¨åˆ†é…å¹¶é˜»ç¢å¤æ‚å¤šæ­¥éª¤æœç´¢ç­–ç•¥çš„æ¢ç´¢ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåˆ†å±‚GRPOï¼Œå…¶æ ¸å¿ƒç»„ä»¶åˆ†å±‚ä¼˜åŠ¿å½’ä¸€åŒ–ï¼ˆSANï¼‰æ ¹æ®ç»“æ„ç‰¹æ€§å°†è½¨è¿¹åˆ’åˆ†ä¸ºåŒè´¨é˜¶å±‚ï¼Œå¹¶åœ¨æ¯ä¸ªé˜¶å±‚å†…éƒ¨è¿›è¡Œå±€éƒ¨ä¼˜åŠ¿è®¡ç®—ã€‚è¿™ç¡®ä¿äº†å¯¹ç­‰è½¨è¿¹ä¹‹é—´çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„åˆ†æè¯æ˜SANæ¶ˆé™¤äº†è·¨é˜¶å±‚åå·®ï¼Œåœ¨æ¯ä¸ªé˜¶å±‚å†…éƒ¨äº§ç”Ÿæ¡ä»¶æ— åçš„å•ä½æ–¹å·®ä¼°è®¡ï¼Œå¹¶ä¿ç•™äº†æ ‡å‡†å½’ä¸€åŒ–æ‰€äº«æœ‰çš„å…¨å±€æ— åæ€§å’Œå•ä½æ–¹å·®å±æ€§ï¼Œä»è€Œäº§ç”Ÿæ›´çº¯å‡€ã€æ›´ç¨³å®šçš„å­¦ä¹ ä¿¡å·ã€‚ä¸ºäº†æé«˜æœ‰é™æ ·æœ¬ä¸‹çš„å®é™…ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å°†SANä¸å…¨å±€ä¼°è®¡å™¨è¿›è¡Œçº¿æ€§æ··åˆã€‚åœ¨å¤šæ ·çš„å•è·³å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåˆ†å±‚GRPOåœ¨è®­ç»ƒå¥–åŠ±ã€è®­ç»ƒç¨³å®šæ€§å’Œæœç´¢ç­–ç•¥æ–¹é¢å§‹ç»ˆä¸”å¤§å¹…åº¦åœ°ä¼˜äºGRPOï¼Œè¾¾åˆ°é«˜è¾¾11.3ç‚¹ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†åˆ†å±‚ä½œä¸ºè§£å†³LLMæœç´¢ä»£ç†ä¸­ç»“æ„å¼‚è´¨æ€§çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¶Šæ¥è¶Šå¤šä¾èµ–å¤–éƒ¨å·¥å…·å¦‚æœç´¢å¼•æ“è§£å†³å¤æ‚å¤šæ­¥éª¤é—®é¢˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆä¸ºå…³é”®è®­ç»ƒèŒƒå¼ã€‚</li>
<li>æœç´¢ä»£ç†è½¨è¿¹å­˜åœ¨ç»“æ„æ€§å¼‚è´¨æ€§ï¼Œå¯¼è‡´ç­”æ¡ˆæ–¹å‘å’Œå¥–åŠ±åˆ†å¸ƒå·®å¼‚ã€‚</li>
<li>æ ‡å‡†ç­–ç•¥æ¢¯åº¦æ–¹æ³•é­å—è·¨é˜¶å±‚åå·®å½±å“ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°å¼‚è´¨è½¨è¿¹ã€‚</li>
<li>æå‡ºåˆ†å±‚GRPOæ–¹æ³•ï¼Œæ ¸å¿ƒä¸ºåˆ†å±‚ä¼˜åŠ¿å½’ä¸€åŒ–ï¼ˆSANï¼‰ï¼Œå°†è½¨è¿¹æŒ‰ç»“æ„ç‰¹æ€§åˆ†å±‚ï¼Œå¹¶åœ¨å„å±‚å†…è¯„ä¼°ä¼˜åŠ¿ã€‚</li>
<li>SANæ¶ˆé™¤è·¨é˜¶å±‚åå·®ï¼Œæä¾›æ›´å‡†ç¡®çš„ä¿¡ç”¨åˆ†é…ï¼Œå¹¶ä¿ƒè¿›å¤æ‚å¤šæ­¥éª¤æœç´¢ç­–ç•¥çš„æ¢ç´¢ã€‚</li>
<li>SANä¸å…¨å±€ä¼°è®¡å™¨ç»“åˆï¼Œæé«˜æœ‰é™æ ·æœ¬ä¸‹çš„ç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-994d2e85b586aaf3a1e6afaaf7fd091a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945436&auth_key=1759945436-0-0-2d99cc46be9ae0fd3c1e2f3f47cf409e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Peeking-inside-the-Black-Box-Reinforcement-Learning-for-Explainable-and-Accurate-Relation-Extraction"><a href="#Peeking-inside-the-Black-Box-Reinforcement-Learning-for-Explainable-and-Accurate-Relation-Extraction" class="headerlink" title="Peeking inside the Black-Box: Reinforcement Learning for Explainable and   Accurate Relation Extraction"></a>Peeking inside the Black-Box: Reinforcement Learning for Explainable and   Accurate Relation Extraction</h2><p><strong>Authors:Xinyu Guo, Zhengliang Shi, Minglai Yang, Mahdi Rahimi, Mihai Surdeanu</strong></p>
<p>This paper introduces a framework for relation extraction (RE) that enhances both accuracy and explainability. The framework has two key components: (i) a reasoning mechanism that formulates relation extraction as a series of text-processing steps inspired by cognitive science, and (ii) an optimization process driven by reinforcement learning (RL) with a novel reward function designed to improve both task accuracy and explanation quality. We call our approach CogRE. Our framework addresses the lack of supervision for language-based explanations in traditional RE by promoting outputs that include important relation keywords. These keywords are drawn from a high-quality dictionary that is automatically constructed using an LLM. We evaluate our approach for the task of one-shot RE using two LLMs and two RE datasets. Our experiments show that CogRE improves explanation quality by addressing two common failure patterns in one-shot RE: poor attention focus and limited one-shot learning capability. For example, our cognitive-structured reasoning with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing prior reasoning-based designs. Optimizing this approach with RL using our reward further improves performance by +23.46% (absolute). Finally, human evaluation shows that our best model generates relational keywords closely aligned with gold labels, increasing human explanation quality ratings by 54% (relative). </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…³ç³»æŠ½å–ï¼ˆREï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æé«˜äº†å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶æœ‰ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆiï¼‰ä¸€ä¸ªå°†å…³ç³»æŠ½å–åˆ¶å®šä¸ºä¸€ç³»åˆ—å—è®¤çŸ¥ç§‘å­¦å¯å‘çš„æ–‡æœ¬å¤„ç†æ­¥éª¤çš„æ¨ç†æœºåˆ¶ï¼›ï¼ˆiiï¼‰ä¸€ä¸ªç”±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é©±åŠ¨çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»¥åŠä¸€ä¸ªæ–°å‹å¥–åŠ±å‡½æ•°ï¼Œæ—¨åœ¨æé«˜ä»»åŠ¡å‡†ç¡®æ€§å’Œè§£é‡Šè´¨é‡ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºCogREã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡é¼“åŠ±è¾“å‡ºåŒ…å«é‡è¦å…³ç³»å…³é”®è¯æ¥è§£å†³ä¼ ç»ŸREä¸­åŸºäºè¯­è¨€çš„è§£é‡Šç¼ºä¹ç›‘ç£çš„é—®é¢˜ã€‚è¿™äº›å…³é”®è¯æ¥è‡ªä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨æ„å»ºçš„é«˜è´¨é‡è¯å…¸ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªLLMå’Œä¸¤ä¸ªREæ•°æ®é›†ï¼Œé’ˆå¯¹ä¸€æ¬¡æ€§REä»»åŠ¡æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒCogREé€šè¿‡è§£å†³ä¸€æ¬¡æ€§REä¸­çš„ä¸¤ä¸ªå¸¸è§å¤±è´¥æ¨¡å¼â€”â€”æ³¨æ„åŠ›é›†ä¸­ä¸è¶³å’Œä¸€æ¬¡æ€§å­¦ä¹ èƒ½åŠ›æœ‰é™â€”â€”æ¥æé«˜è§£é‡Šè´¨é‡ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨One-shot NYT29ä¸Šçš„Qwen2.5-15B-Instructè¿›è¡Œè®¤çŸ¥ç»“æ„åŒ–æ¨ç†ï¼Œå®ç°24.65%çš„F1åˆ†æ•°ï¼Œè¶…è¶Šäº†å…ˆå‰çš„åŸºäºæ¨ç†çš„è®¾è®¡ã€‚ä½¿ç”¨æˆ‘ä»¬çš„å¥–åŠ±å‡½æ•°é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–æ­¤æ–¹æ³•ï¼Œæ€§èƒ½æé«˜äº†+23.46%ï¼ˆç»å¯¹å€¼ï¼‰ã€‚æœ€åï¼Œäººç±»è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹ç”Ÿæˆçš„å…³é”®è¯ä¸é»„é‡‘æ ‡å‡†ç´§å¯†å¯¹é½ï¼Œäººç±»è§£é‡Šè´¨é‡è¯„åˆ†æé«˜äº†54%ï¼ˆç›¸å¯¹ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06198v1">PDF</a> Working in process</p>
<p><strong>Summary</strong><br>å…³ç³»æŠ½å–æ¡†æ¶å¢å¼ºå‡†ç¡®æ€§ä¸è§£é‡Šæ€§ä»‹ç»ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼šï¼ˆä¸€ï¼‰å°†å…³ç³»æŠ½å–å…¬å¼åŒ–ä¸ºä¸€ç³»åˆ—å—è®¤çŸ¥ç§‘å­¦å¯å‘çš„æ–‡æœ¬å¤„ç†æ­¥éª¤çš„æ¨ç†æœºåˆ¶ï¼›ï¼ˆäºŒï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œè®¾è®¡æ–°å‹å¥–åŠ±å‡½æ•°ä»¥æé«˜ä»»åŠ¡å‡†ç¡®æ€§å’Œè§£é‡Šè´¨é‡ã€‚æˆ‘ä»¬ç§°è¿™ç§æ–¹æ³•ä¸ºCogREã€‚å®ƒé€šè¿‡ä¿ƒè¿›åŒ…å«é‡è¦å…³ç³»å…³é”®è¯çš„è¾“å‡ºï¼Œè§£å†³äº†ä¼ ç»Ÿå…³ç³»æŠ½å–ä¸­ç¼ºä¹è¯­è¨€è§£é‡Šçš„ç›‘ç£é—®é¢˜ã€‚è¿™äº›å…³é”®è¯æ¥è‡ªä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨æ„å»ºçš„é«˜è´¨é‡è¯å…¸ã€‚å®éªŒè¡¨æ˜ï¼ŒCogREé€šè¿‡è§£å†³ä¸€æ¬¡æ€§å…³ç³»æŠ½å–ä¸­çš„ä¸¤ä¸ªå¸¸è§å¤±è´¥æ¨¡å¼â€”â€”æ³¨æ„åŠ›ä¸è¶³å’Œæœ‰é™çš„ä¸€æ¬¡æ€§å­¦ä¹ èƒ½åŠ›ï¼Œæé«˜äº†è§£é‡Šè´¨é‡ã€‚ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå…³ç³»æŠ½å–æ•°æ®é›†è¿›è¡Œçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å…³ç³»æŠ½å–æ¡†æ¶CogREï¼Œæ—¨åœ¨æé«˜å‡†ç¡®æ€§å’Œè§£é‡Šæ€§ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼šå—è®¤çŸ¥ç§‘å­¦å¯å‘çš„æ¨ç†æœºåˆ¶ä»¥åŠå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>CogREè§£å†³äº†ä¼ ç»Ÿå…³ç³»æŠ½å–ä¸­ç¼ºä¹è¯­è¨€è§£é‡Šç›‘ç£çš„é—®é¢˜ï¼Œé€šè¿‡ä¿ƒè¿›åŒ…å«é‡è¦å…³ç³»å…³é”®è¯çš„è¾“å‡ºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCogREèƒ½æ”¹å–„ä¸€æ¬¡æ€§å…³ç³»æŠ½å–ä¸­çš„ä¸¤ä¸ªå¸¸è§é—®é¢˜ï¼šæ³¨æ„åŠ›ä¸è¶³å’Œæœ‰é™çš„ä¸€æ¬¡æ€§å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå…³ç³»æŠ½å–æ•°æ®é›†è¿›è¡Œçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒCogREæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
<li>äººç±»è¯„ä¼°æ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹ç”Ÿæˆçš„å…³é”®è¯ä¸é‡‘æ ‡å‡†æ ‡ç­¾é«˜åº¦ä¸€è‡´ï¼Œäººç±»è§£é‡Šè´¨é‡è¯„åˆ†æé«˜ç›¸å¯¹54%ã€‚</li>
<li>CogREæ¡†æ¶åœ¨å…³ç³»æŠ½å–ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜å‡†ç¡®æ€§å’Œè§£é‡Šæ€§çš„åœºæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06198">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-67d4d2ac7b9ff3ab93ff0b9407892816~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945444&auth_key=1759945444-0-0-ab8bfdf3f0a5fd7b6e583dc599e01cf4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1137a42b151a40635da9601035a5b286~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945452&auth_key=1759945452-0-0-d2659f65b6df6eaba1acc1f0be677290&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21649ad1f6da971e8e7a85765083f68a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945459&auth_key=1759945459-0-0-d84fb68eacda1b5af02e9f7345b597ff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PolyGraph-Discrepancy-a-classifier-based-metric-for-graph-generation"><a href="#PolyGraph-Discrepancy-a-classifier-based-metric-for-graph-generation" class="headerlink" title="PolyGraph Discrepancy: a classifier-based metric for graph generation"></a>PolyGraph Discrepancy: a classifier-based metric for graph generation</h2><p><strong>Authors:Markus Krimmel, Philip Hartout, Karsten Borgwardt, Dexiong Chen</strong></p>
<p>Existing methods for evaluating graph generative models primarily rely on Maximum Mean Discrepancy (MMD) metrics based on graph descriptors. While these metrics can rank generative models, they do not provide an absolute measure of performance. Their values are also highly sensitive to extrinsic parameters, namely kernel and descriptor parametrization, making them incomparable across different graph descriptors. We introduce PolyGraph Discrepancy (PGD), a new evaluation framework that addresses these limitations. It approximates the Jensen-Shannon distance of graph distributions by fitting binary classifiers to distinguish between real and generated graphs, featurized by these descriptors. The data log-likelihood of these classifiers approximates a variational lower bound on the JS distance between the two distributions. Resulting metrics are constrained to the unit interval [0,1] and are comparable across different graph descriptors. We further derive a theoretically grounded summary metric that combines these individual metrics to provide a maximally tight lower bound on the distance for the given descriptors. Thorough experiments demonstrate that PGD provides a more robust and insightful evaluation compared to MMD metrics. The PolyGraph framework for benchmarking graph generative models is made publicly available at <a target="_blank" rel="noopener" href="https://github.com/BorgwardtLab/polygraph-benchmark">https://github.com/BorgwardtLab/polygraph-benchmark</a>. </p>
<blockquote>
<p>ç°æœ‰è¯„ä¼°å›¾ç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºåŸºäºå›¾æè¿°ç¬¦çš„æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆMMDï¼‰æŒ‡æ ‡ã€‚è™½ç„¶è¿™äº›æŒ‡æ ‡å¯ä»¥å¯¹ç”Ÿæˆæ¨¡å‹è¿›è¡Œæ’åï¼Œä½†å®ƒä»¬å¹¶ä¸èƒ½æä¾›ç»å¯¹çš„æ€§èƒ½åº¦é‡ã€‚å®ƒä»¬çš„å€¼å¯¹å¤–éƒ¨å‚æ•°ï¼ˆå³å†…æ ¸å’Œæè¿°ç¬¦å‚æ•°åŒ–ï¼‰é«˜åº¦æ•æ„Ÿï¼Œå¯¼è‡´åœ¨ä¸åŒçš„å›¾æè¿°ç¬¦ä¹‹é—´æ— æ³•è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å¼•å…¥äº†PolyGraphå·®å¼‚ï¼ˆPGDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œè§£å†³äº†è¿™äº›é™åˆ¶ã€‚å®ƒé€šè¿‡æ‹ŸåˆäºŒè¿›åˆ¶åˆ†ç±»å™¨æ¥åŒºåˆ†çœŸå®å’Œç”Ÿæˆçš„å›¾ï¼Œç‰¹å¾ç”±è¿™äº›æè¿°ç¬¦æ„æˆï¼Œä»è€Œè¿‘ä¼¼å›¾åˆ†å¸ƒçš„Jensen-Shannonè·ç¦»ã€‚è¿™äº›åˆ†ç±»å™¨çš„æ•°æ®å¯¹æ•°ä¼¼ç„¶è¿‘ä¼¼ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„JSè·ç¦»çš„å¯å˜ä¸‹ç•Œã€‚æ‰€å¾—æŒ‡æ ‡é™åˆ¶åœ¨å•ä½åŒºé—´[0,1]å†…ï¼Œå¯åœ¨ä¸åŒçš„å›¾å½¢æè¿°ç¬¦ä¸­è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¨å¯¼å‡ºäº†ä¸€ä¸ªæœ‰ç†è®ºåŸºç¡€çš„æ±‡æ€»æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡ç»“åˆäº†è¿™äº›å•ä¸ªæŒ‡æ ‡ï¼Œä¸ºç»™å®šæè¿°ç¬¦çš„è·ç¦»æä¾›äº†æœ€ç´§å¯†çš„ä¸‹ç•Œã€‚é€šè¿‡å½»åº•å®éªŒè¯æ˜ï¼Œä¸MMDæŒ‡æ ‡ç›¸æ¯”ï¼ŒPGDæä¾›äº†æ›´ç¨³å¥å’Œæ·±å…¥çš„è¯„ä¼°ã€‚PolyGraphå›¾ç”Ÿæˆæ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/BorgwardtLab/polygraph-benchmark%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/BorgwardtLab/polygraph-benchmarkä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06122v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Graphç”Ÿæˆæ¨¡å‹è¯„ä¼°æ¡†æ¶PolyGraph Discrepancyï¼ˆPGDï¼‰ä»‹ç»ã€‚PGDé€šè¿‡æ‹ŸåˆäºŒå…ƒåˆ†ç±»å™¨æ¥åŒºåˆ†çœŸå®å’Œç”Ÿæˆçš„å›¾ï¼Œä»¥è¿‘ä¼¼è®¡ç®—å›¾åˆ†å¸ƒçš„Jensen-Shannonè·ç¦»ã€‚æ–°æ¡†æ¶è§£å†³äº†ç°æœ‰åŸºäºMMDåº¦é‡æ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚å‚æ•°æ•æ„Ÿæ€§å’Œè·¨ä¸åŒå›¾æè¿°ç¬¦çš„ä¸å¯æ¯”æ€§ã€‚PGDæä¾›å•ä½é—´éš”å†…çš„åº¦é‡å€¼ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å…¶è¾ƒMMDåº¦é‡æ›´ä¸ºç¨³å¥å’Œæ·±åˆ»ã€‚PolyGraphæ¡†æ¶å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PolyGraph Discrepancy (PGD)æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè§£å†³ç°æœ‰å›¾å½¢ç”Ÿæˆæ¨¡å‹è¯„ä¼°ä¸­çš„é™åˆ¶é—®é¢˜ã€‚</li>
<li>PGDé€šè¿‡æ‹ŸåˆäºŒå…ƒåˆ†ç±»å™¨æ¥åŒºåˆ†çœŸå®å’Œç”Ÿæˆçš„å›¾å½¢ï¼Œä»è€Œè¿‘ä¼¼è®¡ç®—å›¾åˆ†å¸ƒçš„Jensen-Shannonè·ç¦»ã€‚</li>
<li>ä¸åŸºäºMMDçš„åº¦é‡ç›¸æ¯”ï¼ŒPGDåº¦é‡å€¼è¢«çº¦æŸåœ¨å•ä½é—´éš”å†…ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„å›¾å½¢æè¿°ç¬¦ä¹‹é—´è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>PGDæä¾›äº†ä¸€ä¸ªç†è®ºä¸Šçš„åŸºç¡€æ‘˜è¦åº¦é‡ï¼Œç»“åˆäº†å„ä¸ªæŒ‡æ ‡ï¼Œä¸ºç»™å®šæè¿°ç¬¦æä¾›äº†æœ€ç´§å¯†çš„ä¸‹ç•Œè·ç¦»ä¼°è®¡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œä¸MMDåº¦é‡ç›¸æ¯”ï¼ŒPGDæä¾›äº†æ›´ä¸ºç¨³å¥å’Œæ·±åˆ»çš„è¯„ä¼°ç»“æœã€‚</li>
<li>PolyGraphæ¡†æ¶å¯ç”¨äºåŸºå‡†æµ‹è¯•å›¾å½¢ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å·²ç»å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/BorgwardtLab/polygraph-benchmark%E3%80%82">https://github.com/BorgwardtLab/polygraph-benchmarkã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4cb8981a72d16f457e08de622a4a766c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945466&auth_key=1759945466-0-0-9fd57b49e070dfc99f3f4f80232d4d05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-538f67e4f53b40d46c4d716aa1e44f94~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945473&auth_key=1759945473-0-0-ac9ccf51fdd008abad1018594926a6b2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-cd44077ec004bd7f5e06d17a61cd03a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4dba1c9a6a26874618223eed0a23382a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Influence-Functions-for-Efficient-Data-Selection-in-Reasoning"><a href="#Influence-Functions-for-Efficient-Data-Selection-in-Reasoning" class="headerlink" title="Influence Functions for Efficient Data Selection in Reasoning"></a>Influence Functions for Efficient Data Selection in Reasoning</h2><p><strong>Authors:Prateek Humane, Paolo Cudrano, Daniel Z. Kaplan, Matteo Matteucci, Supriyo Chakraborty, Irina Rish</strong></p>
<p>Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows that a small amount of high-quality data can outperform massive datasets. Yet, what constitutes â€œqualityâ€ remains ill-defined. Existing reasoning methods rely on indirect heuristics such as problem difficulty or trace length, while instruction-tuning has explored a broader range of automated selection strategies, but rarely in the context of reasoning. We propose to define reasoning data quality using influence functions, which measure the causal effect of individual CoT examples on downstream accuracy, and introduce influence-based pruning, which consistently outperforms perplexity and embedding-based baselines on math reasoning within a model family. </p>
<blockquote>
<p>å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®çš„å¾®è°ƒè¡¨æ˜ï¼Œå°‘é‡é«˜è´¨é‡æ•°æ®å¯ä»¥è¶…è¶Šå¤§è§„æ¨¡æ•°æ®é›†ã€‚ç„¶è€Œï¼Œâ€œè´¨é‡â€çš„å®šä¹‰ä»ç„¶ä¸æ˜ç¡®ã€‚ç°æœ‰çš„æ¨ç†æ–¹æ³•ä¾èµ–äºé—´æ¥çš„å¯å‘å¼æ–¹æ³•ï¼Œå¦‚é—®é¢˜éš¾åº¦æˆ–è·Ÿè¸ªé•¿åº¦ï¼Œè€ŒæŒ‡ä»¤è°ƒæ•´åˆ™æ¢ç´¢äº†æ›´å¹¿æ³›çš„è‡ªåŠ¨åŒ–é€‰æ‹©ç­–ç•¥ï¼Œä½†å¾ˆå°‘åœ¨æ¨ç†çš„æƒ…å¢ƒä¸­æ¢ç´¢ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨å½±å“å‡½æ•°æ¥å®šä¹‰æ¨ç†æ•°æ®è´¨é‡ï¼Œè¿™äº›å‡½æ•°è¡¡é‡ä¸ªåˆ«CoTç¤ºä¾‹å¯¹ä¸‹æ¸¸å‡†ç¡®æ€§çš„å› æœæ•ˆåº”ï¼Œå¹¶å¼•å…¥åŸºäºå½±å“çš„ä¿®å‰ªï¼Œå®ƒåœ¨æ¨¡å‹å®¶æ—çš„æ•°å­¦æ¨ç†ä¸Šå§‹ç»ˆä¼˜äºå›°æƒ‘åº¦å’ŒåŸºäºåµŒå…¥çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06108v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šé€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®è¿›è¡Œå¾®è°ƒæ˜¾ç¤ºï¼Œå°‘é‡é«˜è´¨é‡æ•°æ®å¯ä»¥è¶…è¶Šå¤§è§„æ¨¡æ•°æ®é›†çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œâ€œè´¨é‡â€çš„å®šä¹‰ä»ç„¶ä¸æ˜ç¡®ã€‚ç°æœ‰æ¨ç†æ–¹æ³•ä¾èµ–äºå¦‚é—®é¢˜éš¾åº¦æˆ–è½¨è¿¹é•¿åº¦çš„é—´æ¥å¯å‘å¼ä¿¡æ¯ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒå·²ç»æ¢ç´¢äº†æ›´å¹¿æ³›çš„è‡ªåŠ¨é€‰æ‹©ç­–ç•¥ï¼Œä½†å¾ˆå°‘ç”¨äºæ¨ç†èƒŒæ™¯ã€‚æœ¬æ–‡å»ºè®®ä½¿ç”¨å½±å“å‡½æ•°æ¥å®šä¹‰æ¨ç†æ•°æ®è´¨é‡ï¼Œå®ƒè¡¡é‡ä¸ªåˆ«CoTç¤ºä¾‹å¯¹ä¸‹æ¸¸å‡†ç¡®åº¦çš„å› æœæ•ˆåº”ï¼Œå¹¶å¼•å…¥åŸºäºå½±å“åŠ›çš„ä¿®å‰ªï¼Œå®ƒåœ¨æ¨¡å‹å®¶æ—çš„æ•°å­¦æ¨ç†ä¸ŠæŒç»­ä¼˜äºå›°æƒ‘åº¦å’ŒåŸºäºåµŒå…¥çš„åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å°‘é‡é«˜è´¨é‡æ•°æ®åœ¨é“¾å¼æ€ç»´å¾®è°ƒä¸­å¯è¶…è¶Šå¤§è§„æ¨¡æ•°æ®é›†è¡¨ç°ã€‚</li>
<li>ç›®å‰å¯¹â€œé«˜è´¨é‡â€æ•°æ®çš„å®šä¹‰å°šä¸æ¸…æ¥šã€‚</li>
<li>ç°æœ‰æ¨ç†æ–¹æ³•ä¸»è¦ä¾èµ–é—´æ¥å¯å‘å¼ä¿¡æ¯ï¼Œå¦‚é—®é¢˜éš¾åº¦å’Œè½¨è¿¹é•¿åº¦ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒåœ¨æ›´å¹¿æ³›çš„è‡ªåŠ¨é€‰æ‹©ç­–ç•¥æ–¹é¢æœ‰æ¢ç´¢ï¼Œä½†å¾ˆå°‘åº”ç”¨äºæ¨ç†åœºæ™¯ã€‚</li>
<li>å¼•å…¥å½±å“å‡½æ•°æ¥å®šä¹‰æ¨ç†æ•°æ®è´¨é‡ï¼Œè¡¡é‡å•ä¸ªCoTç¤ºä¾‹å¯¹ä¸‹æ¸¸å‡†ç¡®åº¦çš„ç›´æ¥å½±å“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2d6f0849da3a5124ffc267efb99c7524.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-61523c9da54f31dd9649892535ddbaf6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945501&auth_key=1759945501-0-0-ad5f64e00b889dc3ab53e6f9c753a9e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-60c397e43ef47adf17af649dc38c6d0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-303e6109dc43918b5665f65ab713d07e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Valley-of-Code-Reasoning-Scaling-Knowledge-Distillation-of-Large-Language-Models"><a href="#The-Valley-of-Code-Reasoning-Scaling-Knowledge-Distillation-of-Large-Language-Models" class="headerlink" title="The Valley of Code Reasoning: Scaling Knowledge Distillation of Large   Language Models"></a>The Valley of Code Reasoning: Scaling Knowledge Distillation of Large   Language Models</h2><p><strong>Authors:Muyu He, Muhammad Ali Shafique, Anand Kumar, Tsach Mackey, Nazneen Rajani</strong></p>
<p>Distilling the thinking traces of a Large Language Model (LLM) with reasoning capabilities into a smaller model has been proven effective. Yet, there is a scarcity of work done on how model performances scale with the quantity of distillation data. In this work, we study the scaling trend of distilling competitive coding skills on two small non-reasoning LLMs. We validate the hypothesis that there is a $\textit{valley of code reasoning}$: downstream performance on competitive coding first drops as data quantity increases, then it steadily increases in a sharper-than-log-linear fashion. Having identified the trend, we further fine-tune the models at two different distillation stages on the same data to ground conclusions on their respective learning phases. We learn that across stages in the low and medium-low data regimes, small models benefit significantly from easier coding questions than from harder ones. We also find that, surprisingly, the correctness of outputs in training data makes no difference to distillation outcomes. Our work represents a step forward in understanding the training dynamics of code reasoning distillation outside intuition </p>
<blockquote>
<p>å°†å…·æœ‰æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€è€ƒè½¨è¿¹è’¸é¦ä¸ºè¾ƒå°çš„æ¨¡å‹å·²ç»è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œå…³äºè’¸é¦æ•°æ®é‡ä¸æ¨¡å‹æ€§èƒ½å¦‚ä½•æ‰©å±•çš„ç ”ç©¶å·¥ä½œå¾ˆå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨ä¸¤ä¸ªå°å‹çš„éæ¨ç†LLMä¸Šè’¸é¦ç«äº‰ç¼–ç¨‹æŠ€èƒ½çš„æ‰©å±•è¶‹åŠ¿ã€‚æˆ‘ä»¬éªŒè¯äº†å­˜åœ¨ä¸€ä¸ªâ€œä»£ç æ¨ç†çš„è°·åŒºâ€çš„å‡è®¾ï¼šéšç€æ•°æ®é‡çš„å¢åŠ ï¼Œç«äº‰ç¼–ç¨‹çš„ä¸‹æ¸¸æ€§èƒ½é¦–å…ˆä¸‹é™ï¼Œç„¶åä»¥å¯¹æ•°çº¿æ€§æ›´é™¡å³­çš„æ–¹å¼ç¨³å®šå¢é•¿ã€‚åœ¨ç¡®å®šäº†è¿™ä¸€è¶‹åŠ¿åï¼Œæˆ‘ä»¬åœ¨ç›¸åŒçš„è’¸é¦æ•°æ®ä¸Šå¯¹ä¸¤ä¸ªä¸åŒè’¸é¦é˜¶æ®µçš„æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥æ ¹æ®å„è‡ªçš„å­¦ä¹ é˜¶æ®µå¾—å‡ºç»“è®ºã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ä½æ•°æ®å’Œä¸­ç­‰ä½æ•°æ®é˜¶æ®µï¼Œå°æ¨¡å‹ä»ç®€å•çš„ç¼–ç¨‹é—®é¢˜ä¸­è·å¾—çš„æ”¶ç›Šè¿œå¤§äºä»å¤æ‚é—®é¢˜ä¸­è·å¾—çš„æ”¶ç›Šã€‚æˆ‘ä»¬è¿˜æƒŠè®¶åœ°å‘ç°ï¼Œè®­ç»ƒæ•°æ®ä¸­è¾“å‡ºçš„æ­£ç¡®æ€§å¯¹è’¸é¦ç»“æœæ²¡æœ‰å½±å“ã€‚æˆ‘ä»¬çš„å·¥ä½œä»£è¡¨äº†ç†è§£ä»£ç æ¨ç†è’¸é¦è®­ç»ƒåŠ¨æ€æ–¹é¢çš„ä¸€å¤§è¿›æ­¥ï¼Œè¶…è¶Šäº†ç›´è§‰è®¤çŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06101v1">PDF</a> NeurIPS 2025 Workshop on Deep Learning for Code (DL4C), Project page:   <a target="_blank" rel="noopener" href="https://collinear.ai/valley-of-reasoning">https://collinear.ai/valley-of-reasoning</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›é€šè¿‡è’¸é¦æŠ€æœ¯è½¬åŒ–ä¸ºå°å‹æ¨¡å‹å·²è¢«è¯å®æœ‰æ•ˆã€‚ç„¶è€Œï¼Œå…³äºè’¸é¦æ•°æ®é‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“çš„ç ”ç©¶ä»æ˜¾ä¸è¶³ã€‚æœ¬ç ”ç©¶é’ˆå¯¹ç«äº‰ç¼–ç æŠ€èƒ½çš„è’¸é¦ï¼Œå‘ç°å­˜åœ¨â€œç¼–ç æ¨ç†è°·å€¼â€ï¼šéšç€æ•°æ®é‡çš„å¢åŠ ï¼Œä¸‹æ¸¸ç«äº‰ç¼–ç æ€§èƒ½å…ˆä¸‹é™åæ€¥å‰§ä¸Šå‡ï¼Œä¸”ä¸Šå‡é€Ÿåº¦è¶…è¿‡å¯¹æ•°çº¿æ€§ã€‚é€šè¿‡å¯¹æ¨¡å‹çš„éªŒè¯å’Œè°ƒä¼˜ï¼Œæˆ‘ä»¬å‘ç°æ—©æœŸè’¸é¦é˜¶æ®µçš„å°æ¨¡å‹æ›´å®¹æ˜“ä»ç®€å•çš„ç¼–ç é—®é¢˜ä¸­å—ç›Šè€Œéå¤æ‚é—®é¢˜ã€‚æ­¤å¤–ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè®­ç»ƒæ•°æ®çš„æ­£ç¡®æ€§å¯¹è’¸é¦ç»“æœå¹¶æ— å½±å“ã€‚æœ¬ç ”ç©¶ä¸ºç†è§£ä»£ç æ¨ç†è’¸é¦çš„è®­ç»ƒåŠ¨æ€æä¾›äº†æ–°è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¯ä»¥é€šè¿‡è’¸é¦æŠ€æœ¯è½¬åŒ–ä¸ºå°å‹æ¨¡å‹çš„æœ‰æ•ˆæ€§èƒ½ã€‚</li>
<li>å­˜åœ¨ä¸€ä¸ªè¢«ç§°ä¸ºâ€œç¼–ç æ¨ç†è°·å€¼â€çš„ç°è±¡ï¼Œå³åœ¨ç‰¹å®šæ•°æ®é‡èŒƒå›´å†…æ¨¡å‹æ€§èƒ½å…ˆä¸‹é™åä¸Šå‡ã€‚</li>
<li>åœ¨ä½æ•°æ®å’Œä¸­ç­‰ä½æ•°æ®é˜¶æ®µï¼Œå°å‹æ¨¡å‹ä»ç®€å•çš„ç¼–ç é—®é¢˜ä¸­å—ç›Šæ›´å¤§ã€‚</li>
<li>è®­ç»ƒæ•°æ®çš„æ­£ç¡®æ€§å¯¹è’¸é¦ç»“æœæ²¡æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>è’¸é¦æŠ€æœ¯åœ¨ç«äº‰ç¼–ç æŠ€èƒ½ä¸Šçš„åº”ç”¨æ˜¯å½“å‰ç ”ç©¶çš„é‡ç‚¹ã€‚</li>
<li>é€šè¿‡ç ”ç©¶æ¨¡å‹çš„è’¸é¦é˜¶æ®µå’Œä¸åŒçš„å­¦ä¹ é˜¶æ®µï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹æ€§èƒ½çš„å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fb2250f968741cb505b80f1f00d80134~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945522&auth_key=1759945522-0-0-1b34a04e653331fce5ada6ba7c528c98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-586d4651e53e92a6c0134b2f19505edc~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945530&auth_key=1759945530-0-0-57bdcf1a0e6f55a4c3fcfc7ef7b0fc8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-76bcdadcd6b0a7fdbb08951b6c63ceb0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945542&auth_key=1759945542-0-0-cfca588882efb07079f47d8f2dd5f5ff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-692cdaa79b3c12339d2ecb76bab9d948.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Learning-from-Failures-Understanding-LLM-Alignment-through-Failure-Aware-Inverse-RL"><a href="#Learning-from-Failures-Understanding-LLM-Alignment-through-Failure-Aware-Inverse-RL" class="headerlink" title="Learning from Failures: Understanding LLM Alignment through   Failure-Aware Inverse RL"></a>Learning from Failures: Understanding LLM Alignment through   Failure-Aware Inverse RL</h2><p><strong>Authors:Nyal Patel, Matthieu Bou, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with human preferences, yet the underlying reward signals they internalize remain hidden, posing a critical challenge for interpretability and safety. Existing approaches attempt to extract these latent incentives using Inverse Reinforcement Learning (IRL), but treat all preference pairs equally, often overlooking the most informative signals: those examples the extracted reward model misclassifies or assigns nearly equal scores, which we term \emph{failures}. We introduce a novel \emph{failure-aware} IRL algorithm that focuses on misclassified or difficult examples to recover the latent rewards defining model behaviors. By learning from these failures, our failure-aware IRL extracts reward functions that better reflect the true objectives behind RLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines across multiple metrics when applied to LLM detoxification, without requiring external classifiers or supervision. Crucially, failure-aware IRL yields rewards that better capture the true incentives learned during RLHF, enabling more effective re-RLHF training than standard IRL. This establishes failure-aware IRL as a robust, scalable method for auditing model alignment and reducing ambiguity in the IRL process. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ï¼Œä½†å®ƒä»¬å†…éƒ¨åŒ–çš„æ½œåœ¨å¥–åŠ±ä¿¡å·ä»ç„¶éšè”½ï¼Œä¸ºå¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§å¸¦æ¥äº†å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾ä½¿ç”¨é€†å‘å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰æ¥æå–è¿™äº›æ½œåœ¨æ¿€åŠ±å› ç´ ï¼Œä½†å¹³ç­‰å¯¹å¾…æ‰€æœ‰åå¥½å¯¹ï¼Œå¾€å¾€å¿½ç•¥äº†æœ€æœ‰ä¿¡æ¯é‡çš„ä¿¡å·ï¼šé‚£äº›æå–çš„å¥–åŠ±æ¨¡å‹é”™è¯¯åˆ†ç±»æˆ–åˆ†é…å‡ ä¹ç›¸ç­‰åˆ†æ•°çš„ä¾‹å­ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå¤±è´¥â€ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„â€œå¤±è´¥æ„ŸçŸ¥â€IRLç®—æ³•ï¼Œè¯¥ç®—æ³•ä¸“æ³¨äºé”™è¯¯åˆ†ç±»æˆ–éš¾ä»¥åˆ†ç±»çš„ä¾‹å­ï¼Œä»¥æ¢å¤å®šä¹‰æ¨¡å‹è¡Œä¸ºçš„æ½œåœ¨å¥–åŠ±ã€‚é€šè¿‡ä»è¿™äº›å¤±è´¥ä¸­å­¦ä¹ ï¼Œæˆ‘ä»¬çš„å¤±è´¥æ„ŸçŸ¥IRLèƒ½å¤Ÿä»RLHFä¸­æå–æ›´å¥½çš„åæ˜ çœŸå®ç›®æ ‡çš„å¥–åŠ±å‡½æ•°ã€‚æˆ‘ä»¬åœ¨LLMå‡€åŒ–ä»»åŠ¡ä¸Šåº”ç”¨å¤±è´¥æ„ŸçŸ¥IRLï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¿‡äº†ç°æœ‰IRLåŸºçº¿ï¼Œæ— éœ€å¤–éƒ¨åˆ†ç±»å™¨æˆ–ç›‘ç£ã€‚å…³é”®çš„æ˜¯ï¼Œå¤±è´¥æ„ŸçŸ¥IRLäº§ç”Ÿçš„å¥–åŠ±èƒ½æ›´å¥½åœ°æ•æ‰RLHFæœŸé—´å­¦ä¹ çš„çœŸå®æ¿€åŠ±ï¼Œä½¿é‡æ–°è¿›è¡ŒRLHFè®­ç»ƒæ¯”æ ‡å‡†IRLæ›´æœ‰æ•ˆã€‚è¿™è¯æ˜äº†å¤±è´¥æ„ŸçŸ¥IRLæ˜¯ä¸€ç§ç¨³å¥ã€å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯ç”¨äºå®¡è®¡æ¨¡å‹å¯¹é½åº¦å¹¶å‡å°‘IRLè¿‡ç¨‹ä¸­çš„æ¨¡ç³Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06092v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½ï¼Œä½†å†…éƒ¨å¥–åŠ±ä¿¡å·ä¿æŒéšè—ï¼Œç»™è§£é‡Šæ€§å’Œå®‰å…¨æ€§å¸¦æ¥æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å°è¯•ä½¿ç”¨é€†å‘å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰æå–æ½œåœ¨æ¿€åŠ±ï¼Œä½†å¹³ç­‰å¯¹å¾…æ‰€æœ‰åå¥½å¯¹ï¼Œå¾€å¾€å¿½è§†æœ€æœ‰ä¿¡æ¯çš„ä¿¡å·â€”â€”é‚£äº›æå–çš„å¥–åŠ±æ¨¡å‹è¯¯åˆ†ç±»æˆ–åˆ†é…è¿‘ä¼¼åˆ†æ•°çš„ä¾‹å­ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå¤±è´¥â€ï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¤±è´¥æ„ŸçŸ¥IRLç®—æ³•ï¼Œå®ƒä¸“æ³¨äºè¯¯åˆ†ç±»æˆ–å›°éš¾çš„ä¾‹å­æ¥æ¢å¤å®šä¹‰æ¨¡å‹è¡Œä¸ºçš„æ½œåœ¨å¥–åŠ±ã€‚é€šè¿‡ä»å¤±è´¥ä¸­å­¦ä¹ ï¼Œæˆ‘ä»¬çš„å¤±è´¥æ„ŸçŸ¥IRLæ›´å¥½åœ°åæ˜ äº†RLHFèƒŒåçš„çœŸæ­£ç›®æ ‡ã€‚åœ¨LLMå‡€åŒ–æ–¹é¢çš„åº”ç”¨æ˜¾ç¤ºï¼Œå®ƒåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„IRLåŸºçº¿ï¼Œä¸”æ— éœ€å¤–éƒ¨åˆ†ç±»å™¨æˆ–ç›‘ç£ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå¤±è´¥æ„ŸçŸ¥IRLäº§ç”Ÿçš„å¥–åŠ±èƒ½æ›´å¥½åœ°æ•æ‰RLHFæœŸé—´å­¦ä¹ çš„çœŸæ­£æ¿€åŠ±ï¼Œä½¿å†è®­ç»ƒRLHFæ¯”æ ‡å‡†IRLæ›´æœ‰æ•ˆã€‚è¿™ç¡®ç«‹äº†å¤±è´¥æ„ŸçŸ¥IRLä½œä¸ºå®¡æ ¸æ¨¡å‹å¯¹é½å’Œå‡å°‘IRLè¿‡ç¨‹ä¸­æ­§ä¹‰çš„ä¸€ç§ç¨³å¥ã€å¯æ‰©å±•çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½ï¼Œä½†å­˜åœ¨è§£é‡Šæ€§å’Œå®‰å…¨æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¹³ç­‰å¯¹å¾…æ‰€æœ‰åå¥½å¯¹ï¼Œå¿½è§†æœ€æœ‰ä¿¡æ¯çš„ä¿¡å·â€”â€”æ¨¡å‹è¯¯åˆ†ç±»æˆ–è¿‘ä¼¼åˆ†æ•°çš„ä¾‹å­ï¼ˆå¤±è´¥ï¼‰ã€‚</li>
<li>å¤±è´¥æ„ŸçŸ¥IRLç®—æ³•ä¸“æ³¨äºè¯¯åˆ†ç±»æˆ–å›°éš¾çš„ä¾‹å­æ¥æ¢å¤æ½œåœ¨å¥–åŠ±ã€‚</li>
<li>å¤±è´¥æ„ŸçŸ¥IRLèƒ½ä»å¤±è´¥ä¸­å­¦ä¹ ï¼Œæ›´å¥½åœ°åæ˜ RLHFèƒŒåçš„çœŸæ­£ç›®æ ‡ã€‚</li>
<li>åœ¨LLMå‡€åŒ–æ–¹é¢çš„åº”ç”¨æ˜¾ç¤ºï¼Œå¤±è´¥æ„ŸçŸ¥IRLåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ— éœ€å¤–éƒ¨ç›‘ç£ã€‚</li>
<li>å¤±è´¥æ„ŸçŸ¥IRLäº§ç”Ÿçš„å¥–åŠ±èƒ½æ›´å¥½åœ°æ•æ‰RLHFæœŸé—´çš„çœŸæ­£æ¿€åŠ±ï¼Œä½¿å†è®­ç»ƒæ›´æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cc05a99efa3a2c9dc3b7e437b78a1e50~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945556&auth_key=1759945556-0-0-80a8a82b5d5bb5e90dce38643bc735bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8cfdb2c0eaa2bd02210bf919e5897ed9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945565&auth_key=1759945565-0-0-16c20a6e81d65a97d2d06e1b8cb4bf7b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="When-Thinking-Drifts-Evidential-Grounding-for-Robust-Video-Reasoning"><a href="#When-Thinking-Drifts-Evidential-Grounding-for-Robust-Video-Reasoning" class="headerlink" title="When Thinking Drifts: Evidential Grounding for Robust Video Reasoning"></a>When Thinking Drifts: Evidential Grounding for Robust Video Reasoning</h2><p><strong>Authors:Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman</strong></p>
<p>Video reasoning, the task of enabling machines to infer from dynamic visual content through multi-step logic, is crucial for advanced AI. While the Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks, its application to video understanding remains underexplored. This paper presents a systematic analysis revealing that CoT often degrades performance in video reasoning, generating verbose but misleading internal monologues, and leading to hallucinated visual details and overridden correct intuitions - a phenomenon we term â€œvisual thinking driftâ€. We explain this drift through a Bayesian lens, positing that CoT traces often diverge from actual visual evidence, instead amplifying internal biases or language priors, causing models to storytell rather than engage in grounded reasoning. To counteract this, we introduce Visual Evidence Reward (VER), a novel reinforcement learning framework that explicitly rewards the generation of reasoning traces that are verifiably grounded in visual evidence. Comprehensive evaluation across 10 diverse video understanding benchmarks demonstrates that our Video-VER consistently achieves top performance. Our work sheds light on the distinct challenges of video-centric reasoning and encourages the development of AI that robustly grounds its inferences in visual evidence - for large multimodal models that not only â€œthink before answeringâ€, but also â€œsee while thinkingâ€. </p>
<blockquote>
<p>è§†é¢‘æ¨ç†ä»»åŠ¡æ˜¯é€šè¿‡å¤šæ­¥éª¤é€»è¾‘ä»åŠ¨æ€è§†è§‰å†…å®¹ä¸­è¿›è¡Œæ¨æ–­ï¼Œå¯¹äºé«˜çº§äººå·¥æ™ºèƒ½æ¥è¯´è‡³å…³é‡è¦ã€‚è™½ç„¶é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æœºåˆ¶åœ¨åŸºäºæ–‡æœ¬çš„ä»»åŠ¡ä¸­å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶åœ¨è§†é¢‘ç†è§£ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚æœ¬æ–‡å¯¹CoTåœ¨è§†é¢‘æ¨ç†ä¸­çš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œå‘ç°å®ƒç»å¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œäº§ç”Ÿå†—é•¿ä¸”å…·è¯¯å¯¼æ€§çš„å†…éƒ¨ç‹¬ç™½ï¼Œå¹¶å¯¼è‡´è™šæ„çš„è§†è§‰ç»†èŠ‚å’Œè¦†ç›–æ­£ç¡®çš„ç›´è§‰â€”â€”æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºâ€œè§†è§‰æ€ç»´æ¼‚ç§»â€ã€‚æˆ‘ä»¬é€šè¿‡è´å¶æ–¯è§†è§’è§£é‡Šäº†è¿™ç§æ¼‚ç§»ï¼Œè®¤ä¸ºCoTè½¨è¿¹å¾€å¾€ä¸å®é™…çš„è§†è§‰è¯æ®ç›¸æ‚–ï¼Œè€Œæ˜¯æ”¾å¤§äº†å†…éƒ¨åè§æˆ–è¯­è¨€å…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´æ¨¡å‹æ›´å€¾å‘äºè®²æ•…äº‹ï¼Œè€Œä¸æ˜¯è¿›è¡ŒåŸºäºäº‹å®çš„æ¨ç†ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§‰è¯æ®å¥–åŠ±ï¼ˆVERï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æ˜ç¡®å¥–åŠ±é‚£äº›æœ‰ç¡®å‡¿è§†è§‰è¯æ®æ”¯æŒçš„æ¨ç†è½¨è¿¹çš„ç”Ÿæˆã€‚åœ¨10ä¸ªå¤šæ ·åŒ–çš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§†é¢‘-VERæŒç»­å®ç°æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œæ­ç¤ºäº†ä»¥è§†é¢‘ä¸ºä¸­å¿ƒçš„æ¨ç†çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¹¶é¼“åŠ±å¼€å‘ä¸€ç§äººå·¥æ™ºèƒ½ï¼Œå…¶æ¨ç†èƒ½ç¨³å¥åœ°åŸºäºè§†è§‰è¯æ®â€”â€”å¯¹äºä¸ä»…â€œåœ¨å›ç­”ä¹‹å‰æ€è€ƒâ€ï¼Œè€Œä¸”â€œåœ¨æ€è€ƒæ—¶è§‚å¯Ÿâ€çš„å¤§å‹å¤šæ¨¡å¼æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06077v1">PDF</a> Accepted by NeurIPS 2025, Project page:   <a target="_blank" rel="noopener" href="https://vision.cs.utexas.edu/projects/video-ver/">https://vision.cs.utexas.edu/projects/video-ver/</a></p>
<p><strong>Summary</strong><br>åœ¨é«˜çº§äººå·¥æ™ºèƒ½ä¸­ï¼Œè§†é¢‘æ¨ç†ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå®ƒèƒ½å¤Ÿè®©æœºå™¨é€šè¿‡å¤šæ­¥éª¤é€»è¾‘ä»åŠ¨æ€è§†è§‰å†…å®¹ä¸­è¿›è¡Œæ¨æ–­ã€‚å°½ç®¡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æœºåˆ¶åœ¨æ–‡æœ¬ä»»åŠ¡ä¸­å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶åœ¨è§†é¢‘ç†è§£ä¸­çš„åº”ç”¨ä»ç„¶è¢«å¿½è§†ã€‚è¿™ç¯‡è®ºæ–‡æ­ç¤ºäº†CoTåœ¨è§†é¢‘æ¨ç†ä¸­æ€§èƒ½ä¸‹é™çš„è¡¨è±¡å¹¶é˜è¿°äº†è¿™ç§ç°è±¡çš„åŸå› â€”â€”â€œè§†è§‰æ€è€ƒæ¼‚ç§»â€ã€‚ä¸ºæ­¤è®ºæ–‡ä»è´å¶æ–¯è§†è§’è¿›è¡Œåˆ†æè§£é‡Šå¹¶æå‡ºè§†è§‰è¯æ®å¥–åŠ±ï¼ˆVERï¼‰æ¡†æ¶ä»¥å…‹æœè¿™ä¸€é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¥–åŠ±ä¸è§†è§‰è¯æ®ç›¸ç¬¦çš„æ¨ç†è½¨è¿¹æ¥å¯¹æŠ—è§†è§‰æ€è€ƒæ¼‚ç§»ã€‚å…¨é¢è¯„ä¼°åå¤§è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒVideo-VERå§‹ç»ˆå–å¾—é¡¶å°–è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†è§†é¢‘ä¸­å¿ƒæ¨ç†çš„ç‹¬ç‰¹æŒ‘æˆ˜å¹¶é¼“åŠ±å¼€å‘ä¸€ç§æ—¢è¦åœ¨å›ç­”å‰æ€è€ƒï¼Œåˆè¦åœ¨æ€è€ƒæ—¶è§‚å¯Ÿçš„äººå·¥æ™ºèƒ½æ¨¡å‹ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæœ¬è®ºæ–‡èšç„¦äºæå‡æœºå™¨çš„è§†é¢‘ç†è§£èƒ½åŠ›ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†é¢‘æ¨ç†æ˜¯é«˜çº§äººå·¥æ™ºèƒ½çš„é‡è¦ä»»åŠ¡ï¼Œæ¶‰åŠæœºå™¨ä»åŠ¨æ€è§†è§‰å†…å®¹ä¸­è¿›è¡Œæ¨æ–­çš„èƒ½åŠ›ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æœºåˆ¶åœ¨æ–‡æœ¬ä»»åŠ¡ä¸­å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨è§†é¢‘ç†è§£ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚</li>
<li>CoTåœ¨è§†é¢‘æ¨ç†ä¸­å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå‡ºç°â€œè§†è§‰æ€è€ƒæ¼‚ç§»â€ç°è±¡ã€‚è¿™ç§ç°è±¡è¡¨ç°ä¸ºæ¨¡å‹ç”Ÿæˆå†—é•¿ä¸”è¯¯å¯¼çš„å†…éƒ¨ç‹¬ç™½ï¼Œå¯¼è‡´è™šæ„çš„è§†è§‰ç»†èŠ‚å’Œè¦†ç›–æ­£ç¡®çš„ç›´è§‰ã€‚</li>
<li>æœ¬è®ºæ–‡ä»è´å¶æ–¯è§†è§’è§£é‡Šäº†è§†è§‰æ€è€ƒæ¼‚ç§»ç°è±¡ï¼Œå¹¶æå‡ºè§†è§‰è¯æ®å¥–åŠ±ï¼ˆVERï¼‰æ¡†æ¶ä»¥å…‹æœè¿™ä¸€é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d67d96f1c94fbe9aaaca732f58d71fc6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945572&auth_key=1759945572-0-0-fcc1c70461fab6edbfd7d9d6decfc46f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-31d37d3e23dc8e8628b6ee739d3382bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96390a83f9fe3c5094033200aa4980e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a60ceec6909734fca762b046025110d6.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-41e790c66f79b0a8c25d9ab60933d81d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945599&auth_key=1759945599-0-0-82c2b9c8ad89d742e159aa1c75aba156&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reasoning-under-Vision-Understanding-Visual-Spatial-Cognition-in-Vision-Language-Models-for-CAPTCHA"><a href="#Reasoning-under-Vision-Understanding-Visual-Spatial-Cognition-in-Vision-Language-Models-for-CAPTCHA" class="headerlink" title="Reasoning under Vision: Understanding Visual-Spatial Cognition in   Vision-Language Models for CAPTCHA"></a>Reasoning under Vision: Understanding Visual-Spatial Cognition in   Vision-Language Models for CAPTCHA</h2><p><strong>Authors:Python Song, Luke Tenyi Chang, Yun-Yun Tsai, Penghui Li, Junfeng Yang</strong></p>
<p>CAPTCHA, originally designed to distinguish humans from robots, has evolved into a real-world benchmark for assessing the spatial reasoning capabilities of vision-language models. In this work, we first show that step-by-step reasoning is crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent high-difficulty spatial reasoning tasks, and that current commercial vision-language models still struggle with such reasoning. In particular, we observe that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to effectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent). However, our findings indicate that requiring the model to perform step-by-step reasoning before generating the final coordinates can significantly enhance its solving accuracy, underscoring the severity of the gap. To systematically study this issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with reasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha, etc.) with step-by-step action solutions and grounding annotations. We further define five reasoning-oriented metrics that enable a comprehensive evaluation of models reasoning capabilities. To validate the effectiveness of reasoning, we also propose a general agentic VLM-based framework that incorporates the models inherent reasoning abilities. Our method achieves state-of-the-art performance across five high-difficulty CAPTCHA types, with an average solving accuracy of 83.9 percent, substantially surpassing existing baselines. These results reveal the limitations of current models and highlight the importance of reasoning in advancing visual-spatial challenges in the future. </p>
<blockquote>
<p>éªŒè¯ç ï¼ˆCAPTCHAï¼‰æœ€åˆæ˜¯ä¸ºäº†åŒºåˆ†äººç±»å’Œæœºå™¨äººè€Œè®¾è®¡çš„ï¼Œç°å·²æ¼”å˜ä¸ºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†èƒ½åŠ›çš„ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†é€æ­¥æ¨ç†å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§£å†³éªŒè¯ç ï¼ˆä»£è¡¨é«˜éš¾åº¦ç©ºé—´æ¨ç†ä»»åŠ¡ï¼‰è‡³å…³é‡è¦ï¼Œå¹¶ä¸”å½“å‰çš„å•†ä¸šè§†è§‰è¯­è¨€æ¨¡å‹ä»ç„¶éš¾ä»¥åº”å¯¹æ­¤ç±»æ¨ç†ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¤§å¤šæ•°å•†ä¸šVLMï¼ˆä¾‹å¦‚åŒå­åº§ã€å…‹åŠ³å¾·ã€GPTç­‰ï¼‰æ— æ³•è§£å†³éªŒè¯ç é—®é¢˜ï¼Œå› æ­¤å‡†ç¡®ç‡å¾ˆä½ï¼ˆçº¦ä¸º21.9%ï¼‰ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¦æ±‚æ¨¡å‹åœ¨ç”Ÿæˆæœ€ç»ˆåæ ‡ä¹‹å‰è¿›è¡Œé€æ­¥æ¨ç†å¯ä»¥æ˜¾è‘—æé«˜è§£å†³éªŒè¯ç çš„å‡†ç¡®æ€§ï¼Œè¿™å‡¸æ˜¾äº†å·®è·çš„ä¸¥é‡æ€§ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CAPTCHA-Xï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…·æœ‰æ¨ç†åŠŸèƒ½çš„ç°å®ä¸–ç•Œçš„CAPTCHAåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†ä¸ƒç§éªŒè¯ç ç±»åˆ«ï¼ˆå¦‚Gobangã€hCaptchaç­‰ï¼‰ï¼Œå…·æœ‰é€æ­¥è¡ŒåŠ¨è§£å†³æ–¹æ¡ˆå’Œæ¥åœ°æ³¨é‡Šã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å®šä¹‰äº†äº”ä¸ªé¢å‘æ¨ç†çš„æŒ‡æ ‡ï¼Œèƒ½å¤Ÿå¯¹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚ä¸ºäº†éªŒè¯æ¨ç†çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªåŸºäºé€šç”¨ä»£ç†çš„VLMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ¨¡å‹å›ºæœ‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ç§é«˜éš¾åº¦CAPTCHAç±»å‹ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡è§£å†³å‡†ç¡®ç‡ä¸º83.9%ï¼Œå¤§å¤§è¶…è¿‡äº†ç°æœ‰åŸºçº¿ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†å½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥åœ¨è§†è§‰ç©ºé—´æŒ‘æˆ˜ä¸­æ¨ç†çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06067v1">PDF</a> 14pages, 11figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>CAPTCHAæœ€åˆæ˜¯ä¸ºåŒºåˆ†äººç±»å’Œæœºå™¨äººè€Œè®¾è®¡çš„ï¼Œç°å·²æˆä¸ºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†èƒ½åŠ›çš„ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ã€‚æœ¬ç ”ç©¶é¦–å…ˆè¡¨æ˜ï¼Œé€æ­¥æ¨ç†å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§£å†³é«˜éš¾åº¦ç©ºé—´æ¨ç†ä»»åŠ¡è‡³å…³é‡è¦ï¼Œè€Œå½“å‰å•†ä¸šè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ­¤ç±»æ¨ç†ä»»åŠ¡æ—¶ä»é¢ä¸´å›°éš¾ã€‚ç‰¹åˆ«æ˜¯å¤§å¤šæ•°å•†ä¸šVLMï¼ˆå¦‚Geminiã€Claudeã€GPTç­‰ï¼‰åœ¨è§£å†³CAPTCHAæ—¶æ— æ³•å®ç°æœ‰æ•ˆæ¨ç†ï¼Œå‡†ç¡®ç‡ä»…ä¸ºçº¦21.9%ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè¦æ±‚æ¨¡å‹åœ¨ç”Ÿæˆæœ€ç»ˆåæ ‡ä¹‹å‰è¿›è¡Œé€æ­¥æ¨ç†å¯ä»¥æ˜¾è‘—æé«˜è§£å†³å‡†ç¡®æ€§ï¼Œè¿™çªæ˜¾äº†å½“å‰å·®è·çš„ä¸¥é‡æ€§ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¬¬ä¸€ä¸ªå¸¦æœ‰æ¨ç†åŠŸèƒ½çš„ç°å®ä¸–ç•ŒCAPTCHAåŸºå‡†æµ‹è¯•â€”â€”CAPTCHA-Xï¼Œæ¶µç›–ä¸ƒç§ç±»å‹çš„CAPTCHAï¼ˆå¦‚Gobangã€hCaptchaç­‰ï¼‰ï¼Œå¹¶æä¾›é€æ­¥è¡ŒåŠ¨è§£å†³æ–¹æ¡ˆå’Œæ³¨é‡Šã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†äº”ä¸ªé¢å‘æ¨ç†çš„æŒ‡æ ‡ï¼Œèƒ½å¤Ÿå¯¹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚ä¸ºäº†éªŒè¯æ¨ç†çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªåŸºäºVLMçš„é€šç”¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ¨¡å‹çš„å†…åœ¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ç§é«˜éš¾åº¦CAPTCHAç±»å‹ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡è§£å†³å‡†ç¡®ç‡ä¸º83.9%ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†å½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†æ¨ç†åœ¨æœªæ¥åº”å¯¹è§†è§‰ç©ºé—´æŒ‘æˆ˜ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CAPTCHAå·²æ¼”å˜ä¸ºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†èƒ½åŠ›çš„ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>é€æ­¥æ¨ç†å¯¹è§†è§‰è¯­è¨€æ¨¡å‹è§£å†³é«˜éš¾åº¦ç©ºé—´æ¨ç†ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å•†ä¸šè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§£å†³CAPTCHAæ—¶çš„å‡†ç¡®ç‡è¾ƒä½ï¼ˆçº¦21.9%ï¼‰ã€‚</li>
<li>è¦æ±‚æ¨¡å‹è¿›è¡Œé€æ­¥æ¨ç†å¯ä»¥æ˜¾è‘—æé«˜è§£å†³CAPTCHAçš„å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç°å®ä¸–ç•Œçš„CAPTCHAåŸºå‡†æµ‹è¯•â€”â€”CAPTCHA-Xï¼ŒåŒ…å«å¤šç§ç±»å‹çš„CAPTCHAå¹¶æä¾›é€æ­¥è¡ŒåŠ¨è§£å†³æ–¹æ¡ˆå’Œæ³¨é‡Šã€‚</li>
<li>å®šä¹‰äº†äº”ä¸ªé¢å‘æ¨ç†çš„æŒ‡æ ‡ï¼Œç”¨äºå…¨é¢è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºVLMçš„é€šç”¨æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ¨¡å‹çš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œå®ç°äº†åœ¨äº”ç§é«˜éš¾åº¦CAPTCHAç±»å‹ä¸Šçš„å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-703ae906ef6f4b4635b30c8e88e8588f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-409b9abc0368bea0ce792e314d4db33f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-2db54d583e00583da08d1f80c86fa495~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945683&auth_key=1759945683-0-0-38037938aae0cef1b685e2f3a7e46b44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bbe74eb9a3920cb2e5c895a6e8562219~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945690&auth_key=1759945690-0-0-592ae5c929c15557c9eb5835f578b0c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TelecomTS-A-Multi-Modal-Observability-Dataset-for-Time-Series-and-Language-Analysis"><a href="#TelecomTS-A-Multi-Modal-Observability-Dataset-for-Time-Series-and-Language-Analysis" class="headerlink" title="TelecomTS: A Multi-Modal Observability Dataset for Time Series and   Language Analysis"></a>TelecomTS: A Multi-Modal Observability Dataset for Time Series and   Language Analysis</h2><p><strong>Authors:Austin Feng, Andreas Varvarigos, Ioannis Panitsas, Daniela Fernandez, Jinbiao Wei, Yuwei Guo, Jialin Chen, Ali Maatouk, Leandros Tassiulas, Rex Ying</strong></p>
<p>Modern enterprises generate vast streams of time series metrics when monitoring complex systems, known as observability data. Unlike conventional time series from domains such as weather, observability data are zero-inflated, highly stochastic, and exhibit minimal temporal structure. Despite their importance, observability datasets are underrepresented in public benchmarks due to proprietary restrictions. Existing datasets are often anonymized and normalized, removing scale information and limiting their use for tasks beyond forecasting, such as anomaly detection, root-cause analysis, and multi-modal reasoning. To address this gap, we introduce TelecomTS, a large-scale observability dataset derived from a 5G telecommunications network. TelecomTS features heterogeneous, de-anonymized covariates with explicit scale information and supports a suite of downstream tasks, including anomaly detection, root-cause analysis, and a question-answering benchmark requiring multi-modal reasoning. Benchmarking state-of-the-art time series, language, and reasoning models reveals that existing approaches struggle with the abrupt, noisy, and high-variance dynamics of observability data. Our experiments also underscore the importance of preserving covariatesâ€™ absolute scale, emphasizing the need for foundation time series models that natively leverage scale information for practical observability applications. </p>
<blockquote>
<p>ç°ä»£ä¼ä¸šåœ¨ç›‘æ§å¤æ‚ç³»ç»Ÿæ—¶ï¼Œä¼šäº§ç”Ÿå¤§é‡çš„æ—¶é—´åºåˆ—æŒ‡æ ‡ï¼Œè¿™è¢«ç§°ä¸ºå¯è§‚å¯Ÿæ€§æ•°æ®ã€‚ä¸å¤©æ°”ç­‰é¢†åŸŸä¸­çš„ä¼ ç»Ÿæ—¶é—´åºåˆ—ä¸åŒï¼Œå¯è§‚å¯Ÿæ€§æ•°æ®å…·æœ‰é›¶è†¨èƒ€ã€é«˜åº¦éšæœºæ€§å’Œæœ€å°çš„æ—¶åºç»“æ„ã€‚å°½ç®¡å®ƒä»¬å¾ˆé‡è¦ï¼Œä½†ç”±äºä¸“æœ‰æƒé™çš„é™åˆ¶ï¼Œå¯è§‚å¯Ÿæ€§æ•°æ®é›†åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­çš„ä»£è¡¨æ€§ä¸è¶³ã€‚ç°æœ‰çš„æ•°æ®é›†é€šå¸¸è¢«åŒ¿ååŒ–å’Œæ ‡å‡†åŒ–ï¼Œä»è€Œæ¶ˆé™¤äº†è§„æ¨¡ä¿¡æ¯ï¼Œå¹¶é™åˆ¶äº†å®ƒä»¬åœ¨é¢„æµ‹ä¹‹å¤–çš„ä»»åŠ¡ï¼ˆå¦‚å¼‚å¸¸æ£€æµ‹ã€æ ¹æœ¬åŸå› åˆ†æå’Œå¤šæ¨¡å¼æ¨ç†ï¼‰çš„ä½¿ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†TelecomTSï¼Œè¿™æ˜¯ä¸€ä¸ªæ¥è‡ª5Gç”µä¿¡ç½‘ç»œçš„å¤§å‹å¯è§‚å¯Ÿæ€§æ•°æ®é›†ã€‚TelecomTSå…·æœ‰å¼‚è´¨çš„ã€éåŒ¿åçš„åå˜é‡å’Œæ˜ç¡®çš„è§„æ¨¡ä¿¡æ¯ï¼Œå¹¶æ”¯æŒä¸€ç³»åˆ—ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€æ ¹æœ¬åŸå› åˆ†æå’Œä¸€ä¸ªéœ€è¦å¤šæ¨¡å¼æ¨ç†çš„é—®ç­”åŸºå‡†æµ‹è¯•ã€‚å¯¹æœ€å…ˆè¿›çš„æ—¶åºã€è¯­è¨€å’Œæ¨ç†æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åº”å¯¹å¯è§‚å¯Ÿæ€§æ•°æ®çš„çªå‘ã€å™ªå£°å¤§å’Œé«˜æ–¹å·®åŠ¨æ€å˜åŒ–ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜å¼ºè°ƒäº†ä¿ç•™åå˜é‡ç»å¯¹è§„æ¨¡çš„é‡è¦æ€§ï¼Œå¼ºè°ƒäº†éœ€è¦åŸºç¡€æ—¶åºæ¨¡å‹æ¥åˆ©ç”¨è§„æ¨¡ä¿¡æ¯è¿›è¡Œå®é™…çš„è§‚å¯Ÿåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06063v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç°ä»£ä¼ä¸šç›‘æµ‹å¤æ‚ç³»ç»Ÿæ—¶äº§ç”Ÿå¤§é‡æ—¶é—´åºåˆ—æŒ‡æ ‡ï¼Œå½¢æˆå¯è§‚æ€§æ•°æ®ã€‚æ­¤ç±»æ•°æ®å…·æœ‰é›¶è†¨èƒ€ã€é«˜åº¦éšæœºæ€§ï¼Œä¸”å‡ ä¹ä¸å«æ—¶é—´ç»“æ„ä¿¡æ¯ã€‚å°½ç®¡å®ƒä»¬éå¸¸é‡è¦ï¼Œä½†ç”±äºäº§æƒé™åˆ¶ï¼Œå¯è§‚æ€§æ•°æ®é›†åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­ä»£è¡¨æ€§ä¸è¶³ã€‚ç°æœ‰æ•°æ®é›†ç»å¸¸è¢«åŒ¿ååŒ–å¤„ç†ä¸”è§„èŒƒåŒ–ï¼Œä¸§å¤±äº†è§„æ¨¡ä¿¡æ¯ï¼Œä½¿å…¶ä¸èƒ½ç”¨äºé™¤é¢„æµ‹ä»¥å¤–çš„ä»»åŠ¡ï¼Œå¦‚å¼‚å¸¸æ£€æµ‹ã€æ ¹æœ¬åŸå› åˆ†æåŠå¤šæ¨¡æ€æ¨ç†ç­‰ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TelecomTSâ€”â€”æºè‡ª5Gç”µä¿¡ç½‘ç»œçš„å¤§è§„æ¨¡å¯è§‚æ€§æ•°æ®é›†ã€‚TelecomTSå…·å¤‡å¼‚è´¨ã€éåŒ¿ååŒ–çš„åå˜é‡å’Œæ˜ç¡®çš„è§„æ¨¡ä¿¡æ¯ï¼Œå¹¶æ”¯æŒä¸€ç³»åˆ—ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€æ ¹æœ¬åŸå› åˆ†æåŠéœ€è¦å¤šæ¨¡æ€æ¨ç†çš„é—®ç­”åŸºå‡†æµ‹è¯•ç­‰ã€‚å¯¹å…ˆè¿›çš„æ—¶é—´åºåˆ—ã€è¯­è¨€å’Œæ¨ç†æ¨¡å‹çš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åº”å¯¹å¯è§‚æ€§æ•°æ®çš„çªå‘ã€å™ªå£°å¤§å’Œé«˜æ–¹å·®ç‰¹æ€§ã€‚æˆ‘ä»¬çš„å®éªŒä¹Ÿå¼ºè°ƒäº†ä¿ç•™åå˜é‡ç»å¯¹è§„æ¨¡çš„é‡è¦æ€§ï¼Œå¼ºè°ƒäº†éœ€è¦å¼€å‘åŸç”Ÿåˆ©ç”¨è§„æ¨¡ä¿¡æ¯çš„æ—¶åºæ¨¡å‹ä»¥åº”ç”¨äºå®é™…çš„å¯è§‚æ€§åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£ä¼ä¸šç”Ÿæˆå¤§é‡åŸºäºå¤æ‚ç³»ç»Ÿç›‘æµ‹çš„æ—¶é—´åºåˆ—æŒ‡æ ‡ï¼Œç§°ä¸ºå¯è§‚æ€§æ•°æ®ã€‚</li>
<li>å¯è§‚æ€§æ•°æ®å…·æœ‰é›¶è†¨èƒ€ã€é«˜åº¦éšæœºæ€§å’Œç¼ºä¹æ—¶é—´ç»“æ„çš„ç‰¹ç‚¹ã€‚</li>
<li>ç”±äºäº§æƒé™åˆ¶ï¼Œå¯è§‚æ€§æ•°æ®é›†åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­çš„ä»£è¡¨æ€§ä¸è¶³ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ç»å¸¸ç»è¿‡åŒ¿ååŒ–å’Œå½’ä¸€åŒ–å¤„ç†ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨é™¤é¢„æµ‹ä»¥å¤–çš„ä»»åŠ¡ï¼ˆå¦‚å¼‚å¸¸æ£€æµ‹ã€æ ¹æœ¬åŸå› åˆ†æå’Œå¤šæ¨¡æ€æ¨ç†ï¼‰ä¸­çš„åº”ç”¨ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†TelecomTSï¼Œå®ƒæ˜¯ä»5Gç”µä¿¡ç½‘ç»œä¸­è¡ç”Ÿå‡ºæ¥çš„ï¼Œå…·æœ‰å¼‚è´¨çš„ã€éåŒ¿ååŒ–çš„åå˜é‡å’Œæ˜ç¡®çš„è§„æ¨¡ä¿¡æ¯ã€‚</li>
<li>TelecomTSæ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€æ ¹æœ¬åŸå› åˆ†æå’Œéœ€è¦å¤šæ¨¡æ€æ¨ç†çš„é—®ç­”åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-537d719204c373e8068e8738a996d380~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945697&auth_key=1759945697-0-0-7550db5d5df0e864f416cc4d4f6de356&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-e3a2c08280b5f3114ff637329be08aac.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-48a47c3700dcca1fc3fc1501d8e25515~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945710&auth_key=1759945710-0-0-df154f63e068bc5463ca170fd1706f75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89b1265604d8d926e02903fc6bc1ad15~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945717&auth_key=1759945717-0-0-c6c816c7ff8a109ca9467edd6999d933&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-8c7f3dc35fbf6d7182da653b5a33858d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65b533aa2712d0d09b4447cd91293172.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd46c081369b4204cdd4dcd8ed5c58da.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ASPO-Asymmetric-Importance-Sampling-Policy-Optimization"><a href="#ASPO-Asymmetric-Importance-Sampling-Policy-Optimization" class="headerlink" title="ASPO: Asymmetric Importance Sampling Policy Optimization"></a>ASPO: Asymmetric Importance Sampling Policy Optimization</h2><p><strong>Authors:Jiakang Wang, Runze Liu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai</strong></p>
<p>Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at <a target="_blank" rel="noopener" href="https://github.com/wizard-III/Archer2.0">https://github.com/wizard-III/Archer2.0</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åè®­ç»ƒæ–¹æ³•ä¾èµ–äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„tokençº§åˆ«è£å‰ªæœºåˆ¶ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ç»“æœç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆOSRLï¼‰èŒƒå¼ä¸­çš„ä¸€ä¸ªåŸºæœ¬ç¼ºé™·ï¼šæ­£ä¼˜åŠ¿tokençš„é‡è¦æ€§é‡‡æ ·ï¼ˆISï¼‰æ¯”ç‡ä¸åŒ¹é…ï¼Œå¯¼è‡´æ­£è´Ÿé¢tokençš„tokenæƒé‡ä¸å¹³è¡¡ã€‚è¿™ç§ä¸åŒ¹é…ä¼šæŠ‘åˆ¶ä½æ¦‚ç‡tokençš„æ›´æ–°ï¼ŒåŒæ—¶è¿‡åº¦æ”¾å¤§å·²ç»é«˜æ¦‚ç‡çš„tokenã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸å¯¹ç§°é‡è¦æ€§é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆASPOï¼‰ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå³ç¿»è½¬æ­£ä¼˜åŠ¿tokençš„ISæ¯”ç‡ï¼Œä½¿å…¶æ›´æ–°æ–¹å‘ä¸è´Ÿé¢tokençš„å­¦ä¹ åŠ¨æ€ç›¸ä¸€è‡´ã€‚ASPOè¿˜è¿›ä¸€æ­¥èå…¥äº†ä¸€ç§æŸ”å’Œçš„åŒé‡è£å‰ªæœºåˆ¶ï¼Œä»¥ç¨³å®šæç«¯æ›´æ–°å¹¶ä¿æŒæ¢¯åº¦æµã€‚åœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å…¨é¢å®éªŒè¡¨æ˜ï¼ŒASPOæ˜¾è‘—å‡è½»äº†è¿‡æ—©æ”¶æ•›é—®é¢˜ï¼Œæé«˜äº†è®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶åœ¨åŸºäºGRPOçš„å¼ºåŸºçº¿ä¹‹ä¸Šå¢å¼ºäº†æœ€ç»ˆæ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æä¸ºOSRLä¸­tokençº§åˆ«æƒé‡çš„ä½œç”¨æä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†çº æ­£LLM RLä¸­ISçš„å…³é”®é‡è¦æ€§ã€‚ASPOçš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wizard-III/Archer2.0">https://github.com/wizard-III/Archer2.0</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06062v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åè®­ç»ƒæ–¹æ³•ä¾èµ–äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„tokençº§åˆ«å‰ªè¾‘æœºåˆ¶ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§æˆæœç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆOSRLï¼‰èŒƒå¼ä¸­çš„åŸºæœ¬ç¼ºé™·ï¼šæ­£å‘ä¼˜åŠ¿tokençš„é‡è¦æ€§é‡‡æ ·ï¼ˆISï¼‰æ¯”ç‡ä¸åŒ¹é…ï¼Œå¯¼è‡´æ­£è´Ÿtokençš„æƒé‡ä¸å¹³è¡¡ã€‚è¿™ç§ä¸åŒ¹é…ä¼šæŠ‘åˆ¶ä½æ¦‚ç‡tokençš„æ›´æ–°ï¼ŒåŒæ—¶è¿‡åº¦æ”¾å¤§å·²ç»é«˜æ¦‚ç‡çš„tokenã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸å¯¹ç§°é‡è¦æ€§é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆASPOï¼‰ï¼Œé‡‡ç”¨ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ç¿»è½¬æ­£å‘ä¼˜åŠ¿tokençš„ISæ¯”ç‡ï¼Œä½¿å…¶æ›´æ–°æ–¹å‘ä¸è´Ÿå‘tokençš„å­¦ä¹ åŠ¨æ€ä¸€è‡´ã€‚ASPOè¿˜ç»“åˆäº†è½¯åŒå‰ªè¾‘æœºåˆ¶ï¼Œä»¥ç¨³å®šæç«¯æ›´æ–°å¹¶ä¿æŒæ¢¯åº¦æµã€‚åœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒASPOæ˜¾è‘—ç¼“è§£äº†è¿‡æ—©æ”¶æ•›é—®é¢˜ï¼Œæé«˜äº†è®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶åœ¨åŸºäºGRPOçš„å¼ºåŸºçº¿ä¹‹ä¸Šå¢å¼ºäº†æœ€ç»ˆæ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†ææä¾›äº†å…³äºOSRLä¸­tokençº§åˆ«æƒé‡çš„æ–°è§è§£ï¼Œå¹¶å¼ºè°ƒäº†çº æ­£ISåœ¨LLM RLä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹åè®­ç»ƒæ–¹æ³•ä¾èµ–å¼ºåŒ–å­¦ä¹ ä¸­çš„tokençº§åˆ«å‰ªè¾‘æœºåˆ¶ã€‚</li>
<li>æˆæœç›‘ç£å¼ºåŒ–å­¦ä¹ èŒƒå¼ä¸­å­˜åœ¨åŸºæœ¬ç¼ºé™·ï¼šæ­£å‘ä¼˜åŠ¿tokençš„é‡è¦æ€§é‡‡æ ·æ¯”ç‡ä¸åŒ¹é…ï¼Œå¯¼è‡´æƒé‡ä¸å¹³è¡¡ã€‚</li>
<li>è¿™ç§ä¸åŒ¹é…ä¼šæŠ‘åˆ¶ä½æ¦‚ç‡tokençš„æ›´æ–°ï¼Œè¿‡åº¦æ”¾å¤§å·²é«˜æ¦‚ç‡çš„tokenã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ³•ï¼šä¸å¯¹ç§°é‡è¦æ€§é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆASPOï¼‰ã€‚</li>
<li>ASPOé€šè¿‡ç¿»è½¬æ­£å‘ä¼˜åŠ¿tokençš„ISæ¯”ç‡ï¼Œä½¿æ›´æ–°æ–¹å‘ä¸è´Ÿå‘tokençš„å­¦ä¹ åŠ¨æ€ä¸€è‡´ã€‚</li>
<li>ASPOç»“åˆäº†è½¯åŒå‰ªè¾‘æœºåˆ¶ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-02305d22bafeeeab946b271855424e24~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945745&auth_key=1759945745-0-0-dcdd100f5695792c8d161590b5f6ed9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-2f0d81c2a0639205bbbb549121089c3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b4f3e4260bf4cf38ea6ba0ac5a90f00.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="VideoMiner-Iteratively-Grounding-Key-Frames-of-Hour-Long-Videos-via-Tree-based-Group-Relative-Policy-Optimization"><a href="#VideoMiner-Iteratively-Grounding-Key-Frames-of-Hour-Long-Videos-via-Tree-based-Group-Relative-Policy-Optimization" class="headerlink" title="VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via   Tree-based Group Relative Policy Optimization"></a>VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via   Tree-based Group Relative Policy Optimization</h2><p><strong>Authors:Xinye Cao, Hongcan Guo, Jiawen Qian, Guoshun Nan, Chao Wang, Yuqi Pan, Tianhao Hou, Xiaojuan Wang, Yutong Gao</strong></p>
<p>Understanding hour-long videos with multi-modal large language models (MM-LLMs) enriches the landscape of human-centered AI applications. However, for end-to-end video understanding with LLMs, uniformly sampling video frames results in LLMs being overwhelmed by a vast amount of irrelevant information as video length increases. Existing hierarchical key frame extraction methods improve the accuracy of video understanding but still face two critical challenges. 1) How can the interference of extensive redundant information in long videos be mitigated? 2) How can a model dynamically adapt to complex hierarchical structures while accurately identifying key frames? To address these issues, we propose VideoMiner, which iteratively segments, captions, and clusters long videos, forming a hierarchical tree structure. The proposed VideoMiner progresses from long videos to events to frames while preserving temporal coherence, effectively addressing the first challenge. To precisely locate key frames, we introduce T-GRPO, a tree-based group relative policy optimization in reinforcement learning method that guides the exploration of the VideoMiner. The proposed T-GRPO is specifically designed for tree structures, integrating spatiotemporal information at the event level while being guided by the question, thus solving the second challenge. We achieve superior performance in all long-video understanding tasks and uncover several interesting insights. Our proposed T-GRPO surprisingly incentivizes the model to spontaneously generate a reasoning chain. Additionally, the designed tree growth auxin dynamically adjusts the expansion depth, obtaining accuracy and efficiency gains. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/caoxinye/VideoMiner">https://github.com/caoxinye/VideoMiner</a>. </p>
<blockquote>
<p>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMM-LLMsï¼‰ç†è§£é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ï¼Œä¸°å¯Œäº†ä»¥äººç±»ä¸ºä¸­å¿ƒçš„AIåº”ç”¨é¢†åŸŸã€‚ç„¶è€Œï¼Œåœ¨åˆ©ç”¨LLMsè¿›è¡Œç«¯åˆ°ç«¯è§†é¢‘ç†è§£æ—¶ï¼Œå¯¹è§†é¢‘å¸§è¿›è¡Œç»Ÿä¸€é‡‡æ ·ä¼šå¯¼è‡´LLMsé¢ä¸´å¤§é‡ä¸ç›¸å…³ä¿¡æ¯çš„å¹²æ‰°ï¼Œéšç€è§†é¢‘é•¿åº¦çš„å¢åŠ ï¼Œè¿™ç§å¹²æ‰°ä¼šæ„ˆå‘ä¸¥é‡ã€‚ç°æœ‰çš„å±‚æ¬¡åŒ–å…³é”®å¸§æå–æ–¹æ³•æé«˜äº†è§†é¢‘ç†è§£çš„å‡†ç¡®æ€§ï¼Œä½†ä»é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚1ï¼‰å¦‚ä½•å‡è½»é•¿è§†é¢‘ä¸­å¤§é‡å†—ä½™ä¿¡æ¯çš„å¹²æ‰°ï¼Ÿ2ï¼‰å¦‚ä½•åœ¨å‡†ç¡®è¯†åˆ«å…³é”®å¸§çš„åŒæ—¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€é€‚åº”å¤æ‚çš„å±‚æ¬¡ç»“æ„ï¼Ÿä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VideoMinerï¼Œå®ƒè¿­ä»£åœ°å¯¹é•¿è§†é¢‘è¿›è¡Œåˆ†æ®µã€æ ‡æ³¨å’Œèšç±»ï¼Œå½¢æˆå±‚æ¬¡æ ‘ç»“æ„ã€‚VideoMinerä»é•¿è§†é¢‘é€æ­¥è¿‡æ¸¡åˆ°äº‹ä»¶å†åˆ°å¸§ï¼ŒåŒæ—¶ä¿æŒæ—¶é—´è¿è´¯æ€§ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†ç²¾ç¡®åœ°å®šä½å…³é”®å¸§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ ‘çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•T-GRPOï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæŒ‡å¯¼VideoMinerçš„æ¢ç´¢ã€‚æˆ‘ä»¬ä¸“é—¨è®¾è®¡çš„T-GRPOæ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºæ ‘çŠ¶ç»“æ„ï¼Œåœ¨äº‹ä»¶å±‚é¢æ•´åˆæ—¶ç©ºä¿¡æ¯ï¼ŒåŒæ—¶å—é—®é¢˜çš„å¼•å¯¼ï¼Œä»è€Œè§£å†³äº†ç¬¬äºŒä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬åœ¨æ‰€æœ‰é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­éƒ½å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶å‘ç°äº†å‡ ä¸ªæœ‰è¶£çš„è§è§£ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºçš„T-GRPOæ¿€åŠ±æ¨¡å‹è‡ªå‘åœ°ç”Ÿæˆæ¨ç†é“¾ã€‚æ­¤å¤–ï¼Œè®¾è®¡çš„æ ‘ç”Ÿé•¿è¾…åŠ©ç´ èƒ½åŠ¨æ€è°ƒæ•´æ‰©å±•æ·±åº¦ï¼Œä»è€Œæé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/caoxinye/VideoMiner%E3%80%82">https://github.com/caoxinye/VideoMinerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06040v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>è§†é¢‘ç†è§£æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦åº”ç”¨ä¹‹ä¸€ã€‚åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ï¼Œä¼ ç»Ÿçš„é‡‡æ ·æ–¹æ³•ä¼šå¯¼è‡´æ¨¡å‹å—åˆ°å¤§é‡å†—ä½™ä¿¡æ¯çš„å¹²æ‰°ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†VideoMineræ¨¡å‹ï¼Œé€šè¿‡è¿­ä»£åˆ†å‰²ã€æ ‡æ³¨å’Œèšç±»é•¿è§†é¢‘ï¼Œå½¢æˆå±‚æ¬¡æ ‘ç»“æ„ï¼Œæœ‰æ•ˆåœ°ä»é•¿è§†é¢‘ä¸­æå–å…³é”®ä¿¡æ¯ã€‚ä¸ºç²¾ç¡®å®šä½å…³é”®å¸§ï¼Œå¼•å…¥åŸºäºæ ‘çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•T-GRPOï¼Œè§£å†³äº†åœ¨å¤æ‚å±‚æ¬¡ç»“æ„ä¸­å‡†ç¡®è¯†åˆ«å…³é”®å¸§çš„é—®é¢˜ã€‚VideoMineræ¨¡å‹åœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½è‡ªå‘äº§ç”Ÿæ¨ç†é“¾ï¼Œå…·æœ‰åŠ¨æ€è°ƒæ•´æ‰©å±•æ·±åº¦çš„èƒ½åŠ›ï¼Œå®ç°äº†å‡†ç¡®æ€§å’Œæ•ˆç‡çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤„ç†é•¿è§†é¢‘æ—¶ï¼Œä¼ ç»Ÿçš„å‡åŒ€é‡‡æ ·è§†é¢‘å¸§ä¼šå¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹å—åˆ°å¤§é‡å†—ä½™ä¿¡æ¯çš„å¹²æ‰°ã€‚</li>
<li>VideoMineræ¨¡å‹é€šè¿‡è¿­ä»£åˆ†å‰²ã€æ ‡æ³¨å’Œèšç±»é•¿è§†é¢‘ï¼Œå½¢æˆå±‚æ¬¡æ ‘ç»“æ„ï¼Œæé«˜è§†é¢‘ç†è§£çš„å‡†ç¡®æ€§ã€‚</li>
<li>T-GRPOæ–¹æ³•ç”¨äºç²¾ç¡®å®šä½å…³é”®å¸§ï¼Œè§£å†³äº†åœ¨å¤æ‚å±‚æ¬¡ç»“æ„ä¸­å‡†ç¡®è¯†åˆ«å…³é”®å¸§çš„æŒ‘æˆ˜ã€‚</li>
<li>VideoMineræ¨¡å‹åœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°å‡†ç¡®æ€§å’Œæ•ˆç‡çš„æå‡ã€‚</li>
<li>VideoMineræ¨¡å‹èƒ½è‡ªå‘äº§ç”Ÿæ¨ç†é“¾ï¼Œè¿™æœ‰åŠ©äºè¿›ä¸€æ­¥ç†è§£è§†é¢‘å†…å®¹çš„å†…åœ¨é€»è¾‘ã€‚</li>
<li>VideoMineré€šè¿‡è®¾è®¡çš„æ ‘å¢é•¿è¾…åŠ©å› å­åŠ¨æ€è°ƒæ•´æ‰©å±•æ·±åº¦ï¼Œä»¥é€‚åº”ä¸åŒçš„è§†é¢‘å†…å®¹å’Œä»»åŠ¡éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9a60850147ca4dd37b3ae7d3b4a7dede~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945766&auth_key=1759945766-0-0-dc72e6ff4a96155c01beb2d83f9ad0a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0568b92e06a0839297555fe43674e11f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945773&auth_key=1759945773-0-0-f580fa5c0f62f376ed54d3019157f0b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-351b47d90e63c3c080b5556737bfaf15.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ARISE-An-Adaptive-Resolution-Aware-Metric-for-Test-Time-Scaling-Evaluation-in-Large-Reasoning-Models"><a href="#ARISE-An-Adaptive-Resolution-Aware-Metric-for-Test-Time-Scaling-Evaluation-in-Large-Reasoning-Models" class="headerlink" title="ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling   Evaluation in Large Reasoning Models"></a>ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling   Evaluation in Large Reasoning Models</h2><p><strong>Authors:Zhangyue Yin, Qiushi Sun, Zhiyuan Zeng, Zhiyuan Yu, Qipeng Guo, Xuanjing Huang, Xipeng Qiu</strong></p>
<p>Test-time scaling has emerged as a transformative paradigm for enhancing the performance of large reasoning models, enabling dynamic allocation of computational resources during inference. However, as the landscape of reasoning models rapidly expands, a critical question remains: how can we systematically compare and evaluate the test-time scaling capabilities across different models? In this paper, we introduce ARISE (Adaptive Resolution-aware Scaling Evaluation), a novel metric specifically designed to assess the test-time scaling effectiveness of large reasoning models. Unlike existing evaluation approaches, ARISE incorporates two key innovations: (1) sample-level awareness that effectively penalizes negative scaling behaviors where increased computation leads to performance degradation, and (2) a dynamic sampling mechanism that mitigates the impact of accuracy fluctuations and token count instability on the final assessment. We conduct comprehensive experiments evaluating state-of-the-art reasoning models across diverse domains including mathematical reasoning, code generation, and agentic tasks. Our results demonstrate that ARISE provides a reliable and fine-grained measurement of test-time scaling capabilities, revealing significant variations in scaling efficiency across models. Notably, our evaluation identifies Claude Opus as exhibiting superior scaling characteristics compared to other contemporary reasoning models. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾å·²æˆä¸ºæé«˜å¤§å‹æ¨ç†æ¨¡å‹æ€§èƒ½çš„ä¸€ç§å˜é©æ€§èŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°è®¡ç®—èµ„æºçš„åŠ¨æ€åˆ†é…ã€‚ç„¶è€Œï¼Œéšç€æ¨ç†æ¨¡å‹æ™¯è§‚çš„å¿«é€Ÿå‘å±•ï¼Œä¸€ä¸ªå…³é”®é—®é¢˜ä»ç„¶å­˜åœ¨ï¼šæˆ‘ä»¬å¦‚ä½•ç³»ç»Ÿåœ°æ¯”è¾ƒå’Œè¯„ä¼°ä¸åŒæ¨¡å‹çš„æµ‹è¯•æ—¶ç¼©æ”¾èƒ½åŠ›ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ARISEï¼ˆè‡ªé€‚åº”åˆ†è¾¨ç‡æ„ŸçŸ¥ç¼©æ”¾è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹çš„æµ‹è¯•æ—¶ç¼©æ”¾æ•ˆæœçš„æ–°æŒ‡æ ‡ã€‚ä¸åŒäºç°æœ‰çš„è¯„ä¼°æ–¹æ³•ï¼ŒARISEç»“åˆäº†ä¸¤ç§å…³é”®åˆ›æ–°ï¼š1ï¼‰æ ·æœ¬çº§æ„ŸçŸ¥ï¼Œæœ‰æ•ˆåœ°æƒ©ç½šè´Ÿé¢ç¼©æ”¾è¡Œä¸ºï¼Œå³å¢åŠ è®¡ç®—å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼›2)åŠ¨æ€é‡‡æ ·æœºåˆ¶ï¼Œå‡è½»å‡†ç¡®æ€§æ³¢åŠ¨å’Œä»¤ç‰Œè®¡æ•°ä¸ç¨³å®šå¯¹æœ€ç»ˆè¯„ä¼°çš„å½±å“ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé¢†åŸŸè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œè¯„ä¼°äº†æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ï¼ŒåŒ…æ‹¬æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œä»£ç†ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒARISEæä¾›äº†å¯é çš„æµ‹è¯•æ—¶ç¼©æ”¾èƒ½åŠ›ç²¾ç»†æµ‹é‡ï¼Œæ­ç¤ºäº†æ¨¡å‹ä¹‹é—´åœ¨ç¼©æ”¾æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—å·®å¼‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è¯„ä¼°è®¤ä¸ºClaude Opusåœ¨ç¼©æ”¾ç‰¹æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜äºå…¶ä»–å½“ä»£æ¨ç†æ¨¡å‹çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06014v1">PDF</a> 19 pages, 7 figures</p>
<p><strong>Summary</strong><br>å¤§æ¨¡å‹æ¨ç†ä»»åŠ¡çš„æµ‹è¯•æ—¶ç¼©æ”¾è¯„ä¼°å¯¹äºæé«˜æ¨¡å‹æ€§èƒ½å¹¶åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºè‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è¯„ä¼°æŒ‡æ ‡ARISEï¼ˆè‡ªé€‚åº”åˆ†è¾¨ç‡æ„ŸçŸ¥ç¼©æ”¾è¯„ä¼°ï¼‰ï¼Œç”¨ä»¥è¯„ä¼°å¤§æ¨¡å‹çš„æµ‹è¯•æ—¶ç¼©æ”¾èƒ½åŠ›ã€‚ç›¸è¾ƒäºç°æœ‰çš„è¯„ä»·æ–¹å¼ï¼ŒARISEå¼•å…¥äº†ä¸¤ä¸ªåˆ›æ–°ç‚¹ï¼šä¸€æ˜¯å¯¹æ ·æœ¬çº§åˆ«çš„è®¡ç®—æ•ˆç›Šæ„ŸçŸ¥æƒ©ç½šæœºåˆ¶ï¼Œèƒ½æœ‰æ•ˆè§„é¿è´Ÿå‘è®¡ç®—å¯¼è‡´æ€§èƒ½é™ä½çš„æƒ…å†µï¼›äºŒæ˜¯é‡‡ç”¨åŠ¨æ€é‡‡æ ·æœºåˆ¶å‡å°‘å‡†ç¡®æ€§æ³¢åŠ¨å’Œæ ‡è®°ç¬¦è®¡æ•°ä¸ç¨³å®šå¯¹è¯„ä¼°çš„å½±å“ã€‚å®éªŒè¯æ˜ï¼ŒARISEèƒ½å¤Ÿå¯é ä¸”ç²¾ç»†åœ°è¡¡é‡æµ‹è¯•æ—¶ç¼©æ”¾èƒ½åŠ›ï¼Œæ­ç¤ºä¸åŒæ¨¡å‹åœ¨æ•ˆç‡ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œå¹¶ä¸”æŒ‡å‡ºClaude Opusåœ¨ç‰¹å®šåœºæ™¯ä¸­å…·æœ‰ä¼˜å¼‚çš„ç¼©æ”¾ç‰¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾å·²æˆä¸ºå¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹æ€§èƒ½çš„å…³é”®èŒƒå¼ï¼Œå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºã€‚</li>
<li>ARISEæ˜¯ä¸€ç§æ–°å‹çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹çš„æµ‹è¯•æ—¶ç¼©æ”¾æ•ˆæœã€‚</li>
<li>ARISEå¼•å…¥æ ·æœ¬çº§åˆ«æ„ŸçŸ¥æœºåˆ¶ï¼Œæœ‰æ•ˆæƒ©ç½šè´Ÿå‘è®¡ç®—å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>ARISEé‡‡ç”¨åŠ¨æ€é‡‡æ ·æœºåˆ¶æ¥å‡å°‘å‡†ç¡®æ€§æ³¢åŠ¨å’Œæ ‡è®°ç¬¦è®¡æ•°ä¸ç¨³å®šå¯¹è¯„ä¼°ç»“æœçš„æ½œåœ¨å½±å“ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºARISEèƒ½å¯é ã€ç²¾ç»†åœ°è¡¡é‡æ¨¡å‹çš„æµ‹è¯•æ—¶ç¼©æ”¾èƒ½åŠ›ã€‚</li>
<li>åœ¨å„ç§é¢†åŸŸä¸­ï¼Œå¦‚æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œæ™ºèƒ½ä»»åŠ¡ç­‰ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾èƒ½åŠ›åœ¨ä¸åŒæ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4fa0db6938d5f3f30238ea9f33d55cfb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945788&auth_key=1759945788-0-0-777c12c69780cee4a2705f9fe7693fd3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f766000e21c4019cd3bba58942706ed0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945795&auth_key=1759945795-0-0-a732d788b151efb0a5b0d13dd0f812a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f31afac3bfc769fc2d8c95645a249f2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945802&auth_key=1759945802-0-0-d839fe629d5641cdaaba9a160894b622&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Training-Free-Time-Series-Classification-via-In-Context-Reasoning-with-LLM-Agents"><a href="#Training-Free-Time-Series-Classification-via-In-Context-Reasoning-with-LLM-Agents" class="headerlink" title="Training-Free Time Series Classification via In-Context Reasoning with   LLM Agents"></a>Training-Free Time Series Classification via In-Context Reasoning with   LLM Agents</h2><p><strong>Authors:Songyuan Sui, Zihang Xu, Yu-Neng Chuang, Kwei-Herng Lai, Xia Hu</strong></p>
<p>Time series classification (TSC) spans diverse application scenarios, yet labeled data are often scarce, making task-specific training costly and inflexible. Recent reasoning-oriented large language models (LLMs) show promise in understanding temporal patterns, but purely zero-shot usage remains suboptimal. We propose FETA, a multi-agent framework for training-free TSC via exemplar-based in-context reasoning. FETA decomposes a multivariate series into channel-wise subproblems, retrieves a few structurally similar labeled examples for each channel, and leverages a reasoning LLM to compare the query against these exemplars, producing channel-level labels with self-assessed confidences; a confidence-weighted aggregator then fuses all channel decisions. This design eliminates the need for pretraining or fine-tuning, improves efficiency by pruning irrelevant channels and controlling input length, and enhances interpretability through exemplar grounding and confidence estimation. On nine challenging UEA datasets, FETA achieves strong accuracy under a fully training-free setting, surpassing multiple trained baselines. These results demonstrate that a multi-agent in-context reasoning framework can transform LLMs into competitive, plug-and-play TSC solvers without any parameter training. The code is available at <a target="_blank" rel="noopener" href="https://github.com/SongyuanSui/FETATSC">https://github.com/SongyuanSui/FETATSC</a>. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åˆ†ç±»ï¼ˆTSCï¼‰æ¶µç›–äº†å¤šç§åº”ç”¨åœºæ™¯ï¼Œä½†æ ‡æ³¨æ•°æ®é€šå¸¸ç¨€ç¼ºï¼Œä½¿å¾—é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæˆæœ¬é«˜æ˜‚ä¸”ä¸å¤Ÿçµæ´»ã€‚æœ€è¿‘ä»¥æ¨ç†ä¸ºå¯¼å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£æ—¶é—´æ¨¡å¼æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†çº¯é›¶æ ·æœ¬ä½¿ç”¨ä»ç„¶ä¸å¤Ÿç†æƒ³ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºèŒƒä¾‹çš„ä¸Šä¸‹æ–‡æ¨ç†æ— è®­ç»ƒTSCå¤šæ™ºèƒ½ä½“æ¡†æ¶FETAã€‚FETAå°†å¤šå…ƒæ—¶é—´åºåˆ—åˆ†è§£ä¸ºé€šé“çº§çš„å­é—®é¢˜ï¼Œä¸ºæ¯ä¸ªé€šé“æ£€ç´¢å‡ ä¸ªç»“æ„ç›¸ä¼¼çš„æ ‡æ³¨ç¤ºä¾‹ï¼Œå¹¶åˆ©ç”¨æ¨ç†LLMå°†æŸ¥è¯¢ä¸è¿™äº›èŒƒä¾‹è¿›è¡Œæ¯”è¾ƒï¼Œç”Ÿæˆå…·æœ‰è‡ªæˆ‘è¯„ä¼°ç½®ä¿¡åº¦çš„é€šé“çº§æ ‡ç­¾ï¼›ç„¶åï¼Œç½®ä¿¡åº¦åŠ æƒèšåˆå™¨èåˆæ‰€æœ‰é€šé“å†³ç­–ã€‚è¿™ç§è®¾è®¡æ— éœ€è¿›è¡Œé¢„è®­ç»ƒæˆ–å¾®è°ƒï¼Œé€šè¿‡åˆ é™¤æ— å…³é€šé“å’Œæ§åˆ¶è¾“å…¥é•¿åº¦æé«˜äº†æ•ˆç‡ï¼Œå¹¶é€šè¿‡èŒƒä¾‹å®šä½å’Œç½®ä¿¡åº¦ä¼°è®¡æé«˜äº†å¯è§£é‡Šæ€§ã€‚åœ¨ä¹ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„UEAæ•°æ®é›†ä¸Šï¼ŒFETAåœ¨æ— è®­ç»ƒè®¾ç½®ä¸­å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†å¤šä¸ªè®­ç»ƒåŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“ä¸Šä¸‹æ–‡æ¨ç†æ¡†æ¶å¯ä»¥å°†LLMè½¬åŒ–ä¸ºæ— éœ€ä»»ä½•å‚æ•°è®­ç»ƒçš„ç«äº‰æ€§ã€å³æ’å³ç”¨TSCæ±‚è§£å™¨ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SongyuanSui/FETATSC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SongyuanSui/FETATSCæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05950v1">PDF</a> 8 pages main content, 12 pages total including appendix, 1 figure</p>
<p><strong>Summary</strong><br>æ—¶é—´åºåˆ—åˆ†ç±»ï¼ˆTSCï¼‰åº”ç”¨åœºæ™¯å¹¿æ³›ï¼Œä½†ç¼ºä¹æ ‡æ³¨æ•°æ®å¯¼è‡´ä»»åŠ¡ç‰¹å®šè®­ç»ƒæˆæœ¬é«˜æ˜‚ä¸”ä¸çµæ´»ã€‚æå‡ºä¸€ç§åŸºäºèŒƒä¾‹çš„é›¶è®­ç»ƒTSCå¤šæ™ºèƒ½ä½“æ¡†æ¶FETAï¼Œé€šè¿‡èŒƒä¾‹ä¸Šä¸‹æ–‡æ¨ç†å®ç°æ— éœ€è®­ç»ƒã€‚FETAå°†å¤šå…ƒåºåˆ—åˆ†è§£ä¸ºé€šé“çº§å­é—®é¢˜ï¼Œæ£€ç´¢æ¯ä¸ªé€šé“çš„ç»“æ„ç›¸ä¼¼æ ‡æ³¨èŒƒä¾‹ï¼Œåˆ©ç”¨æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¯”è¾ƒæŸ¥è¯¢ä¸èŒƒä¾‹ï¼Œç”Ÿæˆå…·æœ‰è‡ªæˆ‘è¯„ä¼°ç½®ä¿¡åº¦çš„é€šé“çº§æ ‡ç­¾ï¼›ç½®ä¿¡åº¦åŠ æƒèšåˆå™¨èåˆæ‰€æœ‰é€šé“å†³ç­–ã€‚è¯¥è®¾è®¡æ— éœ€é¢„è®­ç»ƒæˆ–å¾®è°ƒï¼Œæé«˜äº†æ•ˆç‡ï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚åœ¨UEAæ•°æ®é›†ä¸Šï¼ŒFETAåœ¨é›¶è®­ç»ƒè®¾ç½®ä¸‹å®ç°é«˜å‡†ç¡®ç‡ï¼Œè¶…è¶Šå¤šä¸ªè®­ç»ƒåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—åˆ†ç±»ï¼ˆTSCï¼‰é¢ä¸´æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒæˆæœ¬é«˜ä¸”ç¼ºä¹çµæ´»æ€§ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºFETAçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºåŸºäºèŒƒä¾‹çš„é›¶è®­ç»ƒTSCã€‚</li>
<li>FETAå°†å¤šå…ƒåºåˆ—åˆ†è§£ä¸ºé€šé“çº§å­é—®é¢˜ï¼Œæ£€ç´¢ç»“æ„ç›¸ä¼¼çš„æ ‡æ³¨èŒƒä¾‹ã€‚</li>
<li>åˆ©ç”¨æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¯”è¾ƒæŸ¥è¯¢ä¸èŒƒä¾‹ï¼Œç”Ÿæˆé€šé“çº§æ ‡ç­¾ï¼Œå…·æœ‰è‡ªæˆ‘è¯„ä¼°ç½®ä¿¡åº¦ã€‚</li>
<li>ç½®ä¿¡åº¦åŠ æƒèšåˆå™¨èåˆæ‰€æœ‰é€šé“å†³ç­–ï¼Œæ— éœ€é¢„è®­ç»ƒæˆ–å¾®è°ƒã€‚</li>
<li>FETAæé«˜äº†æ•ˆç‡ï¼Œé€šè¿‡å‰”é™¤æ— å…³é€šé“å’Œæ§åˆ¶è¾“å…¥é•¿åº¦æ¥å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38b2e1614f21fcf8c4926fe7c4dabe0f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-79b2accfe7dcb236d9d5bf274ef05e99~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945817&auth_key=1759945817-0-0-33695d7c5783360e6f78e806b38ac722&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Label-Free-Biological-Reasoning-Synthetic-Dataset-Creation-via-Uncertainty-Filtering"><a href="#Towards-Label-Free-Biological-Reasoning-Synthetic-Dataset-Creation-via-Uncertainty-Filtering" class="headerlink" title="Towards Label-Free Biological Reasoning Synthetic Dataset Creation via   Uncertainty Filtering"></a>Towards Label-Free Biological Reasoning Synthetic Dataset Creation via   Uncertainty Filtering</h2><p><strong>Authors:Josefa Lia Stoisser, Lawrence Phillips, Aditya Misra, Tom A. Lamb, Philip Torr, Marc Boubnovski Martell, Julien Fauqueur, Kaspar MÃ¤rtens</strong></p>
<p>Synthetic chain-of-thought (CoT) traces are widely used to train large reasoning models (LRMs), improving generalization by providing step-level supervision. Yet most approaches require ground-truth labels to seed or filter these traces - an expensive bottleneck in domains like biology where wet-lab data are scarce. We propose a label-free alternative: uncertainty-based filtering, which uses a modelâ€™s own confidence - quantified through established uncertainty metrics like self-consistency and predictive perplexity - as a substitute for external labels. We sample multiple reasoning traces and retain only low-uncertainty subsets. Applied to biological perturbation prediction, a domain where wet-lab labels are especially costly, we show that the filtered subset has higher accuracy, and that supervised fine-tuning (SFT) on uncertainty-filtered data outperforms unfiltered synthetic data, narrows the gap to ground-truth training, and surpasses strong LRM baselines. Ablations show that per-class filtering corrects for class-specific uncertainty scales and that hybrid uncertainty metrics yield higher-quality datasets. Our results suggest that model-internal confidence is a powerful signal for efficient reasoning dataset creation, enabling LRMs in domains where supervision is expensive. </p>
<blockquote>
<p>åˆæˆæ€ç»´é“¾ï¼ˆCoTï¼‰è½¨è¿¹å¹¿æ³›åº”ç”¨äºè®­ç»ƒå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰ï¼Œé€šè¿‡æä¾›æ­¥éª¤çº§ç›‘ç£æ¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•éƒ½éœ€è¦çœŸå®æ ‡ç­¾æ¥æ’­ç§æˆ–è¿‡æ»¤è¿™äº›è½¨è¿¹ï¼Œè¿™åœ¨ç”Ÿç‰©å­¦ç­‰é¢†åŸŸæ˜¯ä¸€ä¸ªæ˜‚è´µçš„ç“¶é¢ˆï¼Œå› ä¸ºæ¹¿å®éªŒå®¤æ•°æ®éå¸¸ç¨€ç¼ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— æ ‡ç­¾çš„æ›¿ä»£æ–¹æ¡ˆï¼šåŸºäºä¸ç¡®å®šæ€§çš„è¿‡æ»¤ï¼Œå®ƒä½¿ç”¨æ¨¡å‹æœ¬èº«çš„ä¿¡å¿ƒâ€”â€”é€šè¿‡å·²å»ºç«‹çš„ä¸ç¡®å®šæ€§åº¦é‡æŒ‡æ ‡ï¼ˆå¦‚è‡ªæˆ‘ä¸€è‡´æ€§å’Œé¢„æµ‹å›°æƒ‘åº¦ï¼‰è¿›è¡Œé‡åŒ–ï¼Œä½œä¸ºå¤–éƒ¨æ ‡ç­¾çš„æ›¿ä»£å“ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªæ¨ç†è½¨è¿¹è¿›è¡Œé‡‡æ ·ï¼Œä»…ä¿ç•™ä½ä¸ç¡®å®šæ€§çš„å­é›†ã€‚åœ¨ç”Ÿç‰©æ‰°åŠ¨é¢„æµ‹é¢†åŸŸåº”ç”¨ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¹¿å®éªŒå®¤æ ‡ç­¾ç‰¹åˆ«æ˜‚è´µçš„åº”ç”¨é¢†åŸŸï¼Œæˆ‘ä»¬è¯æ˜äº†è¿‡æ»¤åçš„å­é›†å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”åœ¨ä¸ç¡®å®šæ€§è¿‡æ»¤æ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¼˜äºæœªè¿‡æ»¤çš„åˆæˆæ•°æ®ï¼Œç¼©å°äº†ä¸çœŸå®è®­ç»ƒæ•°æ®çš„å·®è·ï¼Œå¹¶è¶…è¶Šäº†å¼ºå¤§çš„LRMåŸºçº¿ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒæŒ‰ç±»åˆ«è¿‡æ»¤å¯ä»¥çº æ­£ç‰¹å®šç±»åˆ«çš„ä¸ç¡®å®šæ€§è§„æ¨¡ï¼Œæ··åˆä¸ç¡®å®šæ€§åº¦é‡æŒ‡æ ‡å¯ä»¥ç”Ÿæˆæ›´é«˜è´¨é‡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å†…éƒ¨ä¿¡å¿ƒæ˜¯ä¸€ä¸ªå¼ºå¤§çš„ä¿¡å·ï¼Œå¯ç”¨äºæœ‰æ•ˆåœ°åˆ›å»ºæ¨ç†æ•°æ®é›†ï¼Œä½¿ç›‘ç£æˆæœ¬é«˜æ˜‚çš„é¢†åŸŸä¹Ÿèƒ½ä½¿ç”¨å¤§å‹æ¨ç†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05871v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆæˆæ€ç»´é“¾ï¼ˆCoTï¼‰ç—•è¿¹è®­ç»ƒå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ–¹æ³•ã€‚å°½ç®¡å¤§å¤šæ•°æ–¹æ³•éœ€è¦çœŸå®æ ‡ç­¾æ¥ç­›é€‰æˆ–ç”Ÿæˆè¿™äº›ç—•è¿¹ï¼Œä½†åœ¨ç”Ÿç‰©é¢†åŸŸç­‰ç¼ºä¹å®éªŒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¿™ç§åšæ³•æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— æ ‡ç­¾æ›¿ä»£æ–¹æ³•â€”â€”åŸºäºä¸ç¡®å®šæ€§çš„ç­›é€‰ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ¨¡å‹è‡ªèº«çš„ç½®ä¿¡åº¦ä½œä¸ºå¤–éƒ¨æ ‡ç­¾çš„æ›¿ä»£å“ã€‚æ¨¡å‹é€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§ç­‰ä¸ç¡®å®šæ€§æŒ‡æ ‡è¿›è¡Œé‡åŒ–è¯„ä¼°ï¼Œå¹¶é€šè¿‡ç­›é€‰ä½ä¸ç¡®å®šæ€§çš„æ¨ç†ç—•è¿¹æ¥æé«˜å‡†ç¡®æ€§ã€‚åœ¨ç”Ÿç‰©æ‰°åŠ¨é¢„æµ‹é¢†åŸŸçš„åº”ç”¨è¡¨æ˜ï¼Œä½¿ç”¨ä¸ç¡®å®šæ€§ç­›é€‰çš„æ•°æ®è¿›è¡Œå¾®è°ƒä¼˜äºæœªç­›é€‰çš„åˆæˆæ•°æ®ï¼Œç¼©å°äº†ä¸çœŸå®è®­ç»ƒæ•°æ®çš„å·®è·ï¼Œå¹¶è¶…è¿‡äº†å¼ºå¤§çš„LRMåŸºçº¿ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å†…éƒ¨ä¿¡å¿ƒæ˜¯åˆ›å»ºé«˜æ•ˆæ¨ç†æ•°æ®é›†çš„æœ‰åŠ›ä¿¡å·ï¼Œå°¤å…¶é€‚ç”¨äºç›‘ç£æˆæœ¬é«˜æ˜‚çš„é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆæ€ç»´é“¾ï¼ˆCoTï¼‰ç—•è¿¹ç”¨äºè®­ç»ƒå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–çœŸå®æ ‡ç­¾æ¥ç”Ÿæˆæˆ–ç­›é€‰CoTç—•è¿¹ï¼Œè¿™åœ¨æŸäº›é¢†åŸŸå¦‚ç”Ÿç‰©å­¦ä¸­æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— æ ‡ç­¾æ–¹æ³•â€”â€”åŸºäºä¸ç¡®å®šæ€§çš„ç­›é€‰ï¼Œä½¿ç”¨æ¨¡å‹è‡ªèº«çš„ç½®ä¿¡åº¦æ›¿ä»£å¤–éƒ¨æ ‡ç­¾ã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§ç­‰ä¸ç¡®å®šæ€§æŒ‡æ ‡é‡åŒ–è¯„ä¼°æ¨¡å‹ã€‚</li>
<li>åœ¨ç”Ÿç‰©æ‰°åŠ¨é¢„æµ‹é¢†åŸŸåº”ç”¨æ­¤æ–¹æ³•ï¼Œå‘ç°ç­›é€‰åçš„æ•°æ®å­é›†å‡†ç¡®æ€§æ›´é«˜ã€‚</li>
<li>ä½¿ç”¨ä¸ç¡®å®šæ€§ç­›é€‰çš„æ•°æ®è¿›è¡Œå¾®è°ƒä¼˜äºæœªç­›é€‰çš„åˆæˆæ•°æ®ï¼Œå¹¶æ¥è¿‘çœŸå®è®­ç»ƒæ•°æ®çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8091884d9cb6a2188986a1566566b961~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945824&auth_key=1759945824-0-0-bc1c78bd28e10b3ef9ba9532d8046b51&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7939ce36e8a534c3e035afe4ecaca14e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945831&auth_key=1759945831-0-0-52b436624ba1ee9dc529537e53a180f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-d088385257ecdd4da4ae64ba4cdc1322.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbfa0a004eee0f205adb0a918199c008.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EEPO-Exploration-Enhanced-Policy-Optimization-via-Sample-Then-Forget"><a href="#EEPO-Exploration-Enhanced-Policy-Optimization-via-Sample-Then-Forget" class="headerlink" title="EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget"></a>EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget</h2><p><strong>Authors:Liang Chen, Xueting Han, Qizhou Wang, Bo Han, Jing Bai, Hinrich Schutze, Kam-Fai Wong</strong></p>
<p>Balancing exploration and exploitation remains a central challenge in reinforcement learning with verifiable rewards (RLVR) for large language models (LLMs). Current RLVR methods often overemphasize exploitation, leading to entropy collapse, diminished exploratory capacity, and ultimately limited performance gains. Although techniques that increase policy stochasticity can promote exploration, they frequently fail to escape dominant behavioral modes. This creates a self-reinforcing loop-repeatedly sampling and rewarding dominant modes-that further erodes exploration. We introduce Exploration-Enhanced Policy Optimization (EEPO), a framework that promotes exploration via two-stage rollouts with adaptive unlearning. In the first stage, the model generates half of the trajectories; it then undergoes a lightweight unlearning step to temporarily suppress these sampled responses, forcing the second stage to explore different regions of the output space. This sample-then-forget mechanism disrupts the self-reinforcing loop and promotes wider exploration during rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO, achieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on Llama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base. </p>
<blockquote>
<p>åœ¨å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¸­ï¼Œå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚å½“å‰çš„RLVRæ–¹æ³•é€šå¸¸è¿‡äºå¼ºè°ƒåˆ©ç”¨ï¼Œå¯¼è‡´ç†µå´©æºƒã€æ¢ç´¢èƒ½åŠ›å‡å¼±ï¼Œæœ€ç»ˆæ€§èƒ½æå‡æœ‰é™ã€‚è™½ç„¶å¢åŠ ç­–ç•¥éšæœºæ€§çš„æŠ€æœ¯å¯ä»¥ä¿ƒè¿›æ¢ç´¢ï¼Œä½†å®ƒä»¬ç»å¸¸æ— æ³•æ‘†è„±ä¸»å¯¼çš„è¡Œä¸ºæ¨¡å¼ã€‚è¿™åˆ›å»ºäº†ä¸€ä¸ªè‡ªæˆ‘å¼ºåŒ–çš„å¾ªç¯â€”â€”åå¤é‡‡æ ·å’Œå¥–åŠ±ä¸»å¯¼æ¨¡å¼â€”â€”è¿›ä¸€æ­¥ä¾µèš€äº†æ¢ç´¢ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¢ç´¢å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼ˆEEPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ä¸¤é˜¶æ®µæ¨å‡ºå’Œè‡ªé€‚åº”é—å¿˜æ¥ä¿ƒè¿›æ¢ç´¢çš„æ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹ç”Ÿæˆä¸€åŠçš„è½¨è¿¹ï¼›ç„¶åå®ƒç»å†ä¸€ä¸ªè½»é‡çº§çš„é—å¿˜æ­¥éª¤ï¼Œæš‚æ—¶æŠ‘åˆ¶è¿™äº›é‡‡æ ·åˆ°çš„å“åº”ï¼Œè¿«ä½¿ç¬¬äºŒé˜¶æ®µæ¢ç´¢è¾“å‡ºç©ºé—´çš„ä¸åŒåŒºåŸŸã€‚è¿™ç§é‡‡æ ·ç„¶åé—å¿˜çš„æœºåˆ¶ç ´åäº†è‡ªæˆ‘å¼ºåŒ–çš„å¾ªç¯ï¼Œå¹¶åœ¨æ¨å‡ºè¿‡ç¨‹ä¸­ä¿ƒè¿›äº†æ›´å¹¿æ³›çš„æ¢ç´¢ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEEPOçš„è¡¨ç°ä¼˜äºGRPOï¼Œåœ¨Qwen2.5-3Bä¸Šå¹³å‡ç›¸å¯¹å¢ç›Šä¸º24.3%ï¼Œåœ¨Llama3.2-3B-Instructä¸Šä¸º33.0%ï¼Œåœ¨Qwen3-8B-Baseä¸Šä¸º10.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05837v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¦æ¥æŒ‘æˆ˜æ—¶ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨æˆä¸ºæ ¸å¿ƒé—®é¢˜ã€‚å½“å‰RLVRæ–¹æ³•å¸¸å¸¸è¿‡åº¦é‡è§†åˆ©ç”¨ï¼Œå¯¼è‡´ç†µå´©æºƒã€æ¢ç´¢èƒ½åŠ›å‡å¼±å’Œæ€§èƒ½æå‡å—é™ã€‚å°½ç®¡å¢åŠ ç­–ç•¥éšæœºæ€§çš„æŠ€æœ¯å¯ä»¥ä¿ƒè¿›æ¢ç´¢ï¼Œä½†å®ƒä»¬å¸¸æ— æ³•æ‘†è„±ä¸»å¯¼çš„è¡Œä¸ºæ¨¡å¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥æ¢ç´¢å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼ˆEEPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ»šåŠ¨å’Œè‡ªé€‚åº”é—å¿˜æ¥ä¿ƒè¿›æ¢ç´¢ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹ç”Ÿæˆä¸€åŠè½¨è¿¹ï¼›ç„¶åç»å†è½»é‡çº§é—å¿˜æ­¥éª¤ï¼Œæš‚æ—¶æŠ‘åˆ¶è¿™äº›é‡‡æ ·å“åº”ï¼Œè¿«ä½¿ç¬¬äºŒé˜¶æ®µæ¢ç´¢è¾“å‡ºç©ºé—´çš„ä¸åŒåŒºåŸŸã€‚è¿™ç§é‡‡æ ·åé—å¿˜æœºåˆ¶æ‰“ç ´äº†è‡ªæˆ‘å¼ºåŒ–å¾ªç¯ï¼Œä¿ƒè¿›äº†æ»šåŠ¨è¿‡ç¨‹ä¸­çš„æ›´å¹¿æ³›æ¢ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸­ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨æ˜¯å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•å¸¸è¿‡åº¦é‡è§†åˆ©ç”¨ï¼Œå¯¼è‡´æ¢ç´¢èƒ½åŠ›å‡å¼±å’Œæ€§èƒ½å—é™ã€‚</li>
<li>å¢åŠ ç­–ç•¥éšæœºæ€§çš„æŠ€æœ¯è™½èƒ½ä¿ƒè¿›æ¢ç´¢ï¼Œä½†ä»å—ä¸»å¯¼è¡Œä¸ºæ¨¡å¼çš„é™åˆ¶ã€‚</li>
<li>EEPOæ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µæ»šåŠ¨å’Œè‡ªé€‚åº”é—å¿˜ä¿ƒè¿›æ¢ç´¢ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µç”Ÿæˆè½¨è¿¹ï¼Œç¬¬äºŒé˜¶æ®µå¼ºåˆ¶æ¢ç´¢ä¸åŒåŒºåŸŸï¼Œæ‰“ç ´è‡ªæˆ‘å¼ºåŒ–å¾ªç¯ã€‚</li>
<li>EEPOåœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜äºGRPOçš„æ€§èƒ½ï¼Œå–å¾—äº†æ˜¾è‘—çš„å¹³å‡ç›¸å¯¹å¢ç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d58be1f29616c5f9c5660a52fe5aa521~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945851&auth_key=1759945851-0-0-6b783d07e97c385541eeba1c31f891fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7a1ddb25bfeb2a48c8c2e3439593ad07~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945859&auth_key=1759945859-0-0-89f9f872d15c3d0f5db2a0f3e9e88094&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f0277ebb3c7e3e5b6a16ea84abf364b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945865&auth_key=1759945865-0-0-dbea93e9d4130b6190f9bc1e8adf6ee7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6882ff90d3baded7e0ba70f443cc46e1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945872&auth_key=1759945872-0-0-addad7285512f4e5618d7689c3fc27d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-399ce4539247a9958dc84aa20f23b402.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="VCoT-Grasp-Grasp-Foundation-Models-with-Visual-Chain-of-Thought-Reasoning-for-Language-driven-Grasp-Generation"><a href="#VCoT-Grasp-Grasp-Foundation-Models-with-Visual-Chain-of-Thought-Reasoning-for-Language-driven-Grasp-Generation" class="headerlink" title="VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought   Reasoning for Language-driven Grasp Generation"></a>VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought   Reasoning for Language-driven Grasp Generation</h2><p><strong>Authors:Haoran Zhang, Shuanghao Bai, Wanqi Zhou, Yuedi Zhang, Qi Zhang, Pengxiang Ding, Cheng Chi, Donglin Wang, Badong Chen</strong></p>
<p>Robotic grasping is one of the most fundamental tasks in robotic manipulation, and grasp detection&#x2F;generation has long been the subject of extensive research. Recently, language-driven grasp generation has emerged as a promising direction due to its practical interaction capabilities. However, most existing approaches either lack sufficient reasoning and generalization capabilities or depend on complex modular pipelines. Moreover, current grasp foundation models tend to overemphasize dialog and object semantics, resulting in inferior performance and restriction to single-object grasping. To maintain strong reasoning ability and generalization in cluttered environments, we propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates visual chain-of-thought reasoning to enhance visual understanding for grasp generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically focuses on visual inputs while providing interpretable reasoning traces. For training, we refine and introduce a large-scale dataset, VCoT-GraspSet, comprising 167K synthetic images with over 1.36M grasps, as well as 400+ real-world images with more than 1.2K grasps, annotated with intermediate bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot demonstrate that our method significantly improves grasp success rates and generalizes effectively to unseen objects, backgrounds, and distractors. More details can be found at <a target="_blank" rel="noopener" href="https://zhanghr2001.github.io/VCoT-Grasp.github.io">https://zhanghr2001.github.io/VCoT-Grasp.github.io</a>. </p>
<blockquote>
<p>æœºå™¨äººæŠ“å–æ˜¯æœºå™¨äººæ“ä½œä¸­æœ€åŸºæœ¬çš„ä»»åŠ¡ä¹‹ä¸€ï¼Œè€ŒæŠ“å–æ£€æµ‹&#x2F;ç”Ÿæˆä¸€ç›´æ˜¯å¹¿æ³›ç ”ç©¶çš„ä¸»é¢˜ã€‚æœ€è¿‘ï¼Œè¯­è¨€é©±åŠ¨æŠ“å–ç”Ÿæˆå› å…¶å®é™…äº¤äº’èƒ½åŠ›è€Œæˆä¸ºä¸€ä¸ªæœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•è¦ä¹ˆç¼ºä¹æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¦ä¹ˆä¾èµ–äºå¤æ‚çš„æ¨¡å—åŒ–æµç¨‹ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æŠ“å–åŸºç¡€æ¨¡å‹å¾€å¾€è¿‡äºå¼ºè°ƒå¯¹è¯å’Œå¯¹è±¡è¯­ä¹‰ï¼Œå¯¼è‡´æ€§èƒ½è¾ƒå·®ä¸”ä»…é™äºå•ä¸ªå¯¹è±¡çš„æŠ“å–ã€‚ä¸ºäº†åœ¨å¤„ç†æ‚ä¹±çš„ç¯å¢ƒä¸­ä¿æŒå¼ºå¤§çš„æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†VCoT-Graspï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„æŠ“å–åŸºç¡€æ¨¡å‹ï¼Œå®ƒç»“åˆäº†è§†è§‰æ€ç»´é“¾æ¨ç†ï¼Œä»¥å¢å¼ºå¯¹æŠ“å–ç”Ÿæˆçš„è§†è§‰ç†è§£ã€‚VCoT-Graspé‡‡ç”¨å¤šè½®å¤„ç†èŒƒå¼ï¼ŒåŠ¨æ€å…³æ³¨è§†è§‰è¾“å…¥ï¼ŒåŒæ—¶æä¾›å¯è§£é‡Šçš„æ¨ç†è½¨è¿¹ã€‚åœ¨è®­ç»ƒæ–¹é¢ï¼Œæˆ‘ä»¬å®Œå–„å¹¶ä»‹ç»äº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†VCoT-GraspSetï¼Œå…¶ä¸­åŒ…å«16.7ä¸‡å¼ åˆæˆå›¾åƒå’Œè¶…è¿‡136ä¸‡æ¬¡çš„æŠ“å–åŠ¨ä½œï¼Œä»¥åŠ400å¤šå¼ çœŸå®ä¸–ç•Œå›¾åƒå’Œè¶…è¿‡1200æ¬¡çš„æŠ“å–åŠ¨ä½œï¼Œè¿™äº›éƒ½é€šè¿‡ä¸­é—´è¾¹ç•Œæ¡†è¿›è¡Œäº†æ³¨é‡Šã€‚åœ¨VCoT-GraspSetå’ŒçœŸå®æœºå™¨äººä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æŠ“å–çš„æˆåŠŸç‡ï¼Œå¹¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„å¯¹è±¡ã€èƒŒæ™¯å’Œå¹²æ‰°ç‰©ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯å¯åœ¨<a target="_blank" rel="noopener" href="https://zhanghr2001.github.io/VCoT-Grasp.github.io">https://zhanghr2001.github.io/VCoT-Grasp.github.io</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05827v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†æœºå™¨äººæŠ“å–æ“ä½œä¸­çš„è§†è§‰é“¾å¼æ€ç»´æ¨ç†å¢å¼ºå‹æŠ“å–ç”Ÿæˆæ¨¡å‹VCoT-Graspã€‚è¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰é“¾å¼æ€ç»´æ¨ç†æŠ€æœ¯ï¼Œä»¥æé«˜æŠ“å–ç”Ÿæˆæ—¶çš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨å¤šè½®å¤„ç†æ¨¡å¼ï¼Œèƒ½åŠ¨æ€èšç„¦äºè§†è§‰è¾“å…¥å¹¶æä¾›å¯è§£é‡Šçš„æ¨ç†è½¨è¿¹ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ç”¨äºè®­ç»ƒVCoT-Graspçš„å¤§å‹æ•°æ®é›†VCoT-GraspSetã€‚è¯¥æ¨¡å‹åœ¨è™šæ‹Ÿå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­éƒ½è¡¨ç°å‡ºæ›´é«˜çš„æŠ“å–æˆåŠŸç‡å’Œå¯¹æœªè§ç‰©ä½“ã€èƒŒæ™¯å’Œå¹²æ‰°é¡¹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯å¯é€šè¿‡é“¾æ¥æŸ¥çœ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VCoT-Graspæ˜¯ä¸€ç§åŸºäºè§†è§‰é“¾å¼æ€ç»´æ¨ç†æŠ€æœ¯çš„æŠ“å–ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººæŠ“å–æ“ä½œçš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨å¤šè½®å¤„ç†æ¨¡å¼ï¼Œèƒ½å¤ŸåŠ¨æ€èšç„¦äºè§†è§‰è¾“å…¥ï¼ŒåŒæ—¶æä¾›å¯è§£é‡Šçš„æ¨ç†è½¨è¿¹ã€‚</li>
<li>VCoT-Graspé€šè¿‡ç»“åˆè§†è§‰é“¾å¼æ€ç»´æ¨ç†æŠ€æœ¯ï¼Œå¼ºåŒ–äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤æ‚çš„æ··ä¹±ç¯å¢ƒä¸­è¿›è¡Œå¼ºåŠ›çš„æŠ“å–æ“ä½œã€‚</li>
<li>VCoT-Graspçš„è®­ç»ƒæ•°æ®é›†VCoT-GraspSetåŒ…å«å¤§é‡åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒæ•°æ®ï¼Œå¹¶æ ‡æ³¨äº†ä¸­é—´è¾¹ç•Œæ¡†ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒVCoT-Graspåœ¨è™šæ‹Ÿå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­çš„æŠ“å–æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œä¸”å¯¹æœªè§ç‰©ä½“ã€èƒŒæ™¯å’Œå¹²æ‰°é¡¹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVCoT-Graspæ—¢å…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œåˆå…·å¤‡è¾ƒå¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”åœ¨å¯¹è¯å’Œå¯¹è±¡è¯­ä¹‰æ–¹é¢å–å¾—äº†å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-877a5b4de52fdd0654bb74aec8211ad2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945886&auth_key=1759945886-0-0-61d8c2214e4cf0101859f8e1ac9b5c84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2c35de8b635eb6d9feca04e2e3bf226~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945914&auth_key=1759945914-0-0-729c823dc8c0229d474081713c8e8a1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-d3d58868abf2cf474a7ae585ac4a57fb.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b175d8762741baf583d9dba8ca8c805~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973482&auth_key=1759973482-0-0-10ecec77da87ea42ef102815a90ea739&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-72e21f365f4e81f312f058f395b11447~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973488&auth_key=1759973488-0-0-48dddc682fa563c9e4dbe2bdf9c1edb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cc2048d57ce1dfcd0ea9ca9bdc51cd99~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973495&auth_key=1759973495-0-0-96fba6c2815991f0222004fa9ecd693b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03fe39005acfb7f73c54d363c65b0e8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973502&auth_key=1759973502-0-0-8cf0d988ded6e7ff87c66975f177829c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EMORL-TTS-Reinforcement-Learning-for-Fine-Grained-Emotion-Control-in-LLM-based-TTS"><a href="#EMORL-TTS-Reinforcement-Learning-for-Fine-Grained-Emotion-Control-in-LLM-based-TTS" class="headerlink" title="EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in   LLM-based TTS"></a>EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in   LLM-based TTS</h2><p><strong>Authors:Haoxun Li, Yu Liu, Yuqing Sun, Hanlei Shi, Leyuan Qu, Taihao Li</strong></p>
<p>Recent LLM-based TTS systems achieve strong quality and zero-shot ability, but lack fine-grained emotional control due to their reliance on discrete speech tokens. Existing approaches either limit emotions to categorical labels or cannot generalize to LLM-based architectures. We propose EMORL-TTS (Fine-grained Emotion-controllable TTS with Reinforcement Learning), a framework that unifies global intensity control in the VAD space with local emphasis regulation. Our method combines supervised fine-tuning with reinforcement learning guided by task-specific rewards for emotion category, intensity, and emphasis. Moreover, we further investigate how emphasis placement modulates fine-grained emotion intensity. Experiments show that EMORL-TTS improves emotion accuracy, intensity differentiation, and emphasis clarity, while preserving synthesis quality comparable to strong LLM-based baselines. </p>
<blockquote>
<p>æœ€è¿‘çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå…·æœ‰å¼ºå¤§çš„è´¨é‡å’Œé›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†ç”±äºå®ƒä»¬ä¾èµ–äºç¦»æ•£çš„è¯­éŸ³æ ‡è®°ï¼Œç¼ºä¹ç²¾ç»†çš„æƒ…ç»ªæ§åˆ¶ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå°†æƒ…ç»ªé™åˆ¶ä¸ºç±»åˆ«æ ‡ç­¾ï¼Œè¦ä¹ˆæ— æ³•é€‚åº”åŸºäºLLMçš„æ¶æ„ã€‚æˆ‘ä»¬æå‡ºäº†EMORL-TTSï¼ˆåŸºäºå¼ºåŒ–å­¦ä¹ çš„ç²¾ç»†æƒ…ç»ªæ§åˆ¶TTSï¼‰ï¼Œä¸€ä¸ªç»Ÿä¸€äº†VADç©ºé—´ä¸­çš„å…¨å±€å¼ºåº¦æ§åˆ¶ä¸å±€éƒ¨é‡ç‚¹è°ƒèŠ‚çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±æ¥è¿›è¡Œæƒ…ç»ªç±»åˆ«ã€å¼ºåº¦å’Œé‡ç‚¹çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥ç ”ç©¶äº†é‡ç‚¹æ”¾ç½®å¦‚ä½•è°ƒèŠ‚ç²¾ç»†çš„æƒ…ç»ªå¼ºåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒEMORL-TTSæé«˜äº†æƒ…ç»ªå‡†ç¡®æ€§ã€å¼ºåº¦åŒºåˆ†åº¦å’Œé‡ç‚¹æ¸…æ™°åº¦ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å¼ºå¤§çš„åŸºäºLLMçš„åŸºçº¿ç›¸å½“çš„ç»¼åˆè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05758v1">PDF</a> Under review for ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿè™½ç„¶å…·æœ‰è‰¯å¥½çš„è´¨é‡å’Œé›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä½†ç”±äºä¾èµ–äºç¦»æ•£è¯­éŸ³æ ‡è®°ï¼Œå®ƒä»¬åœ¨ç²¾ç»†æƒ…æ„Ÿæ§åˆ¶æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆé™åˆ¶æƒ…æ„Ÿä¸ºåˆ†ç±»æ ‡ç­¾ï¼Œè¦ä¹ˆæ— æ³•æ¨å¹¿åˆ°åŸºäºLLMçš„æ¶æ„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†EMORL-TTSï¼ˆä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ çš„ç²¾ç»†æƒ…æ„Ÿæ§åˆ¶TTSï¼‰ï¼Œå®ƒç»Ÿä¸€äº†VADç©ºé—´ä¸­çš„å…¨å±€å¼ºåº¦æ§åˆ¶ä¸å±€éƒ¨é‡ç‚¹è°ƒèŠ‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç›‘ç£å¾®è°ƒï¼Œå¹¶ä½¿ç”¨é’ˆå¯¹æƒ…æ„Ÿç±»åˆ«ã€å¼ºåº¦å’Œé‡ç‚¹çš„ä»»åŠ¡ç‰¹å®šå¥–åŠ±æ¥å¼•å¯¼å¼ºåŒ–å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ·±å…¥ç ”ç©¶äº†é‡ç‚¹æ”¾ç½®å¦‚ä½•è°ƒèŠ‚ç²¾ç»†æƒ…æ„Ÿå¼ºåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒEMORL-TTSæé«˜äº†æƒ…æ„Ÿå‡†ç¡®æ€§ã€å¼ºåº¦åŒºåˆ†åº¦å’Œé‡ç‚¹æ¸…æ™°åº¦ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å¼ºå¤§çš„åŸºäºLLMçš„åŸºçº¿ç›¸å½“çš„åˆæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based TTSç³»ç»Ÿè™½å…·æœ‰é«˜è´¨é‡å’Œé›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä½†åœ¨ç²¾ç»†æƒ…æ„Ÿæ§åˆ¶æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ç°æœ‰TTSæ–¹æ³•åœ¨æƒ…æ„Ÿæ§åˆ¶æ–¹é¢è¦ä¹ˆå±€é™äºåˆ†ç±»æ ‡ç­¾ï¼Œè¦ä¹ˆæ— æ³•é€‚åº”LLMæ¶æ„ã€‚</li>
<li>EMORL-TTSç»“åˆå¼ºåŒ–å­¦ä¹ ï¼Œå®ç°äº†å…¨å±€å¼ºåº¦æ§åˆ¶ä¸å±€éƒ¨é‡ç‚¹è°ƒèŠ‚çš„ç»Ÿä¸€ã€‚</li>
<li>EMORL-TTSé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œé’ˆå¯¹æƒ…æ„Ÿç±»åˆ«ã€å¼ºåº¦å’Œé‡ç‚¹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>é‡ç‚¹æ”¾ç½®å¯¹ç²¾ç»†æƒ…æ„Ÿå¼ºåº¦å…·æœ‰è°ƒèŠ‚ä½œç”¨ã€‚</li>
<li>EMORL-TTSæé«˜äº†æƒ…æ„Ÿå‡†ç¡®æ€§ã€å¼ºåº¦åŒºåˆ†åº¦å’Œé‡ç‚¹æ¸…æ™°åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6496d4ac17fa82897e6908990964a4bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973531&auth_key=1759973531-0-0-7a1fe4bfd5db1b844abf1c2247e209d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ccc1bb0a66e53ab370baebef6409b083~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973538&auth_key=1759973538-0-0-0a7670e6ced4d057463135e1d29068d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-8dae5b3fb815777931bb6ddbfa20b809.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-2566372d815cafadaa8c4595987faf16~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973552&auth_key=1759973552-0-0-e9791b4c5f6adb39d051ba12d544959b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e911eabc1a312f31bc3035e85effe1c7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973558&auth_key=1759973558-0-0-4350558350a6d399da0ccfe41b98a0fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-5fcc42178249e542b17e61d5bdbf9385.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ARM-Discovering-Agentic-Reasoning-Modules-for-Generalizable-Multi-Agent-Systems"><a href="#ARM-Discovering-Agentic-Reasoning-Modules-for-Generalizable-Multi-Agent-Systems" class="headerlink" title="ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent   Systems"></a>ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent   Systems</h2><p><strong>Authors:Bohan Yao, Shiva Krishna Reddy Malay, Vikas Yadav</strong></p>
<p>Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved state-of-the-art results on various complex reasoning tasks. Recent works have proposed techniques to automate the design of MASes, eliminating the need for manual engineering. However, these techniques perform poorly, often achieving similar or inferior performance to simple baselines. Furthermore, they require computationally expensive re-discovery of architectures for each new task domain and expensive data annotation on domains without existing labeled validation sets. A critical insight is that simple Chain of Thought (CoT) reasoning often performs competitively with these complex systems, suggesting that the fundamental reasoning unit of MASes, CoT, warrants further investigation. To this end, we present a new paradigm for automatic MAS design that pivots the focus to optimizing CoT reasoning. We introduce the Agentic Reasoning Module (ARM), an agentic generalization of CoT where each granular reasoning step is executed by a specialized reasoning module. This module is discovered through a tree search over the code space, starting from a simple CoT module and evolved using mutations informed by reflection on execution traces. The resulting ARM acts as a versatile reasoning building block which can be utilized as a direct recursive loop or as a subroutine in a learned meta-orchestrator. Our approach significantly outperforms both manually designed MASes and state-of-the-art automatic MAS design methods. Crucially, MASes built with ARM exhibit superb generalization, maintaining high performance across different foundation models and task domains without further optimization. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰åœ¨å„ç§å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚è¿‘æœŸçš„ç ”ç©¶æå‡ºäº†è‡ªåŠ¨åŒ–è®¾è®¡MASçš„æŠ€æœ¯ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹äººå·¥å·¥ç¨‹çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯çš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œå¾€å¾€è¾¾åˆ°ä¸ç®€å•åŸºå‡†çº¿ç›¸ä¼¼æˆ–æ›´ä½çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¿˜éœ€è¦ä¸ºæ¯ä¸ªæ–°çš„ä»»åŠ¡é¢†åŸŸé‡æ–°å‘ç°æ¶æ„ï¼Œå¹¶åœ¨æ²¡æœ‰ç°æœ‰æ ‡è®°éªŒè¯é›†çš„é¢†åŸŸè¿›è¡Œæ˜‚è´µçš„æ•°æ®æ³¨é‡Šã€‚ä¸€ä¸ªé‡è¦çš„è§è§£æ˜¯ï¼Œç®€å•çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†é€šå¸¸ä¸è¿™äº›å¤æ‚ç³»ç»Ÿçš„è¡¨ç°ç›¸ç«äº‰ï¼Œè¿™è¡¨æ˜MASçš„åŸºæœ¬æ¨ç†å•å…ƒâ€”â€”æ€ç»´é“¾éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨MASè®¾è®¡èŒƒå¼ï¼Œå°†é‡ç‚¹è½¬å‘ä¼˜åŒ–æ€ç»´é“¾æ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†Agenticæ¨ç†æ¨¡å—ï¼ˆARMï¼‰ï¼Œè¿™æ˜¯æ€ç»´é“¾çš„agenticæ¦‚æ‹¬ï¼Œå…¶ä¸­æ¯ä¸ªç²’åº¦çš„æ¨ç†æ­¥éª¤éƒ½ç”±ä¸“é—¨çš„æ¨ç†æ¨¡å—æ‰§è¡Œã€‚è¯¥æ¨¡å—æ˜¯é€šè¿‡ä»£ç ç©ºé—´ä¸­çš„æ ‘æœç´¢å‘ç°çš„ï¼Œä»ç®€å•çš„æ€ç»´é“¾æ¨¡å—å¼€å§‹ï¼Œå¹¶é€šè¿‡æ ¹æ®æ‰§è¡Œè½¨è¿¹çš„åæ€æ¥è¿›è¡Œå˜å¼‚è¿›åŒ–ã€‚æ‰€å¾—çš„ARMå……å½“é€šç”¨çš„æ¨ç†æ„å»ºå—ï¼Œå¯ä»¥ä½œä¸ºç›´æ¥çš„é€’å½’å¾ªç¯ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºå­¦ä¹ å…ƒç¼–æ’å™¨ä¸­çš„å­ç¨‹åºä½¿ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæ‰‹åŠ¨è®¾è®¡çš„MASå’Œæœ€æ–°çš„è‡ªåŠ¨MASè®¾è®¡æ–¹æ³•ã€‚å…³é”®çš„æ˜¯ï¼Œä½¿ç”¨ARMæ„å»ºçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¡¨ç°å‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¸åŒçš„åŸºç¡€æ¨¡å‹å’Œä»»åŠ¡é¢†åŸŸä¸Šéƒ½èƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œæ— éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05746v1">PDF</a> 29 pages, 2 figures</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå·²åœ¨å„ç§å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šå–å¾—æœ€æ–°æˆæœã€‚æ–°çš„æŠ€æœ¯å¯ä»¥è‡ªåŠ¨è®¾è®¡æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå‡å°‘äº†å¯¹äººå·¥å·¥ç¨‹çš„ä¾èµ–ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯åœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¸ä½³ï¼Œç»å¸¸ä¸ç®€å•åŸºçº¿ç›¸ä¼¼æˆ–è¾ƒå·®ã€‚å®ƒä»¬è¿˜éœ€è¦ä¸ºæ¯ä¸ªæ–°ä»»åŠ¡åŸŸé‡æ–°å‘ç°æ¶æ„ï¼Œå¹¶åœ¨æ²¡æœ‰ç°æœ‰æ ‡è®°éªŒè¯é›†çš„åŸŸä¸Šè¿›è¡Œæ˜‚è´µçš„æ•°æ®æ³¨é‡Šã€‚å…³é”®çš„è§è§£æ˜¯ï¼Œç®€å•çš„é“¾å¼æ€ç»´æ¨ç†å¾€å¾€èƒ½ä¸è¿™äº›å¤æ‚ç³»ç»Ÿç«äº‰ï¼Œè¿™è¡¨æ˜æ™ºèƒ½ä½“ç³»ç»Ÿçš„åŸºæœ¬æ¨ç†å•å…ƒâ€”â€”é“¾å¼æ€ç»´å€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨æ™ºèƒ½ä½“è®¾è®¡èŒƒå¼ï¼Œä»¥ä¼˜åŒ–é“¾å¼æ€ç»´æ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†Agenticæ¨ç†æ¨¡å—ï¼ˆARMï¼‰ï¼Œè¿™æ˜¯é“¾å¼æ€ç»´çš„ä¸€ç§æ™ºèƒ½ä½“æ¦‚æ‹¬ï¼Œæ¯ä¸ªé¢—ç²’çŠ¶æ¨ç†æ­¥éª¤éƒ½ç”±ä¸€ä¸ªä¸“é—¨çš„æ¨ç†æ¨¡å—æ‰§è¡Œã€‚è¯¥æ¨¡å—é€šè¿‡ä»£ç ç©ºé—´ä¸­çš„æ ‘æœç´¢å‘ç°ï¼Œä»ç®€å•çš„é“¾å¼æ€ç»´æ¨¡å—å¼€å§‹ï¼Œå¹¶é€šè¿‡æ ¹æ®æ‰§è¡Œè½¨è¿¹çš„åæ€è¿›è¡Œå˜å¼‚è€Œè¿›åŒ–ã€‚äº§ç”Ÿçš„ARMå……å½“ä¸€ä¸ªé€šç”¨çš„æ¨ç†æ„å»ºå—ï¼Œå¯ä»¥ä½œä¸ºç›´æ¥çš„é€’å½’å¾ªç¯ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºå­¦ä¹ å…ƒç¼–æ’å™¨ä¸­çš„å­ç¨‹åºä½¿ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæ‰‹åŠ¨è®¾è®¡æ™ºèƒ½ä½“ç³»ç»Ÿå’Œæœ€æ–°çš„è‡ªåŠ¨æ™ºèƒ½ä½“è®¾è®¡æ–¹æ³•ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä½¿ç”¨ARMæ„å»ºçš„æ™ºèƒ½ä½“è¡¨ç°å‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¸åŒåŸºç¡€æ¨¡å‹å’Œä»»åŠ¡åŸŸä¸Šæ— éœ€è¿›ä¸€æ­¥ä¼˜åŒ–å³å¯ç»´æŒé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è‡ªåŠ¨è®¾è®¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„æŠ€æœ¯è™½ç„¶å‡å°‘äº†å¯¹äººå·¥å·¥ç¨‹çš„ä¾èµ–ï¼Œä½†æ€§èƒ½æœ‰å¾…æé«˜ã€‚</li>
<li>ç®€å•çš„é“¾å¼æ€ç»´æ¨ç†ä¸å¤æ‚ç³»ç»Ÿç«äº‰åŠ›ç›¸å½“ï¼Œå¼•å‘å¯¹æ™ºèƒ½ä½“ç³»ç»ŸåŸºæœ¬æ¨ç†å•å…ƒçš„ç ”ç©¶ã€‚</li>
<li>æå‡ºä¸€ç§æ–°çš„è‡ªåŠ¨æ™ºèƒ½ä½“è®¾è®¡èŒƒå¼ï¼Œä»¥ä¼˜åŒ–é“¾å¼æ€ç»´æ¨ç†ï¼Œå¼•å…¥Agenticæ¨ç†æ¨¡å—ï¼ˆARMï¼‰ã€‚</li>
<li>ARMé€šè¿‡æ ‘æœç´¢å‘ç°ï¼Œä»ç®€å•çš„é“¾å¼æ€ç»´æ¨¡å—è¿›åŒ–è€Œæ¥ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ARMå¯ä»¥ä½œä¸ºé€’å½’å¾ªç¯æˆ–å­ç¨‹åºåœ¨æ™ºèƒ½ä½“ç³»ç»Ÿä¸­ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd7dd02e8a448c97e9bd9d5f36a3544c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24727a28e45b3e09ac9e195e1b8d8a7a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FinReflectKG-EvalBench-Benchmarking-Financial-KG-with-Multi-Dimensional-Evaluation"><a href="#FinReflectKG-EvalBench-Benchmarking-Financial-KG-with-Multi-Dimensional-Evaluation" class="headerlink" title="FinReflectKG - EvalBench: Benchmarking Financial KG with   Multi-Dimensional Evaluation"></a>FinReflectKG - EvalBench: Benchmarking Financial KG with   Multi-Dimensional Evaluation</h2><p><strong>Authors:Fabrizio Dimino, Abhinav Arun, Bhaskarjit Sarmah, Stefano Pasquali</strong></p>
<p>Large language models (LLMs) are increasingly being used to extract structured knowledge from unstructured financial text. Although prior studies have explored various extraction methods, there is no universal benchmark or unified evaluation framework for the construction of financial knowledge graphs (KG). We introduce FinReflectKG - EvalBench, a benchmark and evaluation framework for KG extraction from SEC 10-K filings. Building on the agentic and holistic evaluation principles of FinReflectKG - a financial KG linking audited triples to source chunks from S&amp;P 100 filings and supporting single-pass, multi-pass, and reflection-agent-based extraction modes - EvalBench implements a deterministic commit-then-justify judging protocol with explicit bias controls, mitigating position effects, leniency, verbosity and world-knowledge reliance. Each candidate triple is evaluated with binary judgments of faithfulness, precision, and relevance, while comprehensiveness is assessed on a three-level ordinal scale (good, partial, bad) at the chunk level. Our findings suggest that, when equipped with explicit bias controls, LLM-as-Judge protocols provide a reliable and cost-efficient alternative to human annotation, while also enabling structured error analysis. Reflection-based extraction emerges as the superior approach, achieving best performance in comprehensiveness, precision, and relevance, while single-pass extraction maintains the highest faithfulness. By aggregating these complementary dimensions, FinReflectKG - EvalBench enables fine-grained benchmarking and bias-aware evaluation, advancing transparency and governance in financial AI applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºä»éç»“æ„åŒ–é‡‘èæ–‡æœ¬ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†ã€‚å°½ç®¡å…ˆå‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†å„ç§æå–æ–¹æ³•ï¼Œä½†åœ¨æ„å»ºé‡‘èçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰æ–¹é¢ï¼Œå°šæ— é€šç”¨çš„åŸºå‡†æµ‹è¯•æˆ–ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬ä»‹ç»äº†FinReflectKG-EvalBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä»SEC 10-Kæ–‡ä»¶ä¸­æå–KGçš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ã€‚FinReflectKGåŸºäºä»£ç†å’Œæ•´ä½“è¯„ä¼°åŸåˆ™ï¼Œå°†ç»è¿‡å®¡æ ¸çš„ä¸‰é‡é“¾æ¥åˆ°æ ‡å‡†æ™®å°”100æ–‡ä»¶çš„æºå—ï¼Œå¹¶æ”¯æŒå•é€šé“ã€å¤šé€šé“å’ŒåŸºäºåå°„ä»£ç†çš„æå–æ¨¡å¼ã€‚EvalBenchå®ç°äº†å…·æœ‰æ˜ç¡®åè§æ§åˆ¶çš„ç¡®å®šæ€§å…ˆæäº¤åéªŒè¯åˆ¤æ–­åè®®ï¼Œå‡è½»äº†ä½ç½®æ•ˆåº”ã€å®½å®¹åº¦ã€å†—é•¿å’Œä¾èµ–ä¸–ç•ŒçŸ¥è¯†çš„é—®é¢˜ã€‚æ¯ä¸ªå€™é€‰ä¸‰é‡æ€§éƒ½æ˜¯é€šè¿‡å¿ å®åº¦ã€ç²¾ç¡®åº¦å’Œç›¸å…³æ€§çš„äºŒå…ƒåˆ¤æ–­æ¥è¯„ä¼°çš„ï¼Œè€Œç»¼åˆè¯„ä¼°åˆ™æ˜¯åœ¨å—çº§åˆ«ä¸Šè¿›è¡Œä¸‰çº§åºæ•°é‡è¡¨è¯„ä¼°ï¼ˆè‰¯å¥½ã€éƒ¨åˆ†ã€ä¸è‰¯ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé…å¤‡æ˜ç¡®åè§æ§åˆ¶æ—¶ï¼ŒLLMä½œä¸ºæ³•å®˜çš„åè®®æä¾›äº†ä¸€ç§å¯é ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£äººå·¥æ³¨é‡Šçš„æ–¹æ³•ï¼ŒåŒæ—¶èƒ½å¤Ÿè¿›è¡Œç»“æ„åŒ–è¯¯å·®åˆ†æã€‚åŸºäºåå°„çš„æå–æ–¹æ³•æ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ï¼Œåœ¨ç»¼åˆæ€§ã€ç²¾ç¡®åº¦å’Œç›¸å…³æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€Œå•é€šé“æå–ä¿æŒäº†æœ€é«˜çš„å¿ å®åº¦ã€‚é€šè¿‡èšåˆè¿™äº›äº’è¡¥ç»´åº¦ï¼ŒFinReflectKG-EvalBenchèƒ½å¤Ÿå®ç°ç²¾ç»†çš„åŸºå‡†æµ‹è¯•å’Œåè§æ„ŸçŸ¥è¯„ä¼°ï¼Œæé«˜é‡‘èäººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„é€æ˜åº¦å’Œæ²»ç†æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05710v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é‡‘èé¢†åŸŸçŸ¥è¯†å›¾è°±æ„å»ºçš„æ–°åŸºå‡†è¯„ä»·ä½“ç³»â€”â€”FinReflectKG-EvalBenchã€‚è¯¥æ¡†æ¶ä»SEC 10-Kæ–‡ä»¶ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨ç¡®å®šæ€§è¯„åˆ¤åè®®ï¼Œå¯¹å€™é€‰ä¸‰å…ƒç»„è¿›è¡Œå¿ å®åº¦ã€ç²¾ç¡®åº¦å’Œç›¸å…³æ€§çš„äºŒå…ƒåˆ¤æ–­ï¼ŒåŒæ—¶å¯¹ç‰‡æ®µçº§åˆ«çš„å®Œæ•´æ€§è¿›è¡Œä¸‰çº§è¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æ˜ç¡®çš„åè§æ§åˆ¶ä¸‹ï¼ŒLLM-as-Judgeåè®®ä¸ºå¯é ä¸”ç»æµçš„æ›¿ä»£äººå·¥æ ‡æ³¨æ–¹æ³•ï¼Œå¹¶å¯è¿›è¡Œç»“æ„åŒ–é”™è¯¯åˆ†æã€‚å…¶ä¸­ï¼Œåå°„å‹æå–æ–¹å¼å±•ç°å‡ºæœ€ä½³çš„å®Œæ•´æ€§ã€ç²¾ç¡®åº¦å’Œç›¸å…³æ€§ï¼Œè€Œå•éæå–ä¿æŒæœ€é«˜å¿ å®åº¦ã€‚é€šè¿‡èšåˆè¿™äº›äº’è¡¥ç»´åº¦ï¼ŒFinReflectKG-EvalBenchæ¨åŠ¨äº†é‡‘èAIåº”ç”¨çš„é€æ˜æ€§å’Œæ²»ç†è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FinReflectKG-EvalBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èçŸ¥è¯†å›¾è°±æ„å»ºçš„åŸºå‡†è¯„ä»·ä½“ç³»ï¼Œç”¨äºä»SEC 10-Kæ–‡ä»¶ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ç¡®å®šæ€§è¯„åˆ¤åè®®ï¼Œå¯¹å€™é€‰ä¸‰å…ƒç»„è¿›è¡Œå¿ å®åº¦ã€ç²¾ç¡®åº¦å’Œç›¸å…³æ€§çš„è¯„ä¼°ã€‚</li>
<li>LLM-as-Judgeåè®®åœ¨æ˜ç¡®çš„åè§æ§åˆ¶ä¸‹ï¼Œä½œä¸ºå¯é ä¸”ç»æµçš„æ›¿ä»£äººå·¥æ ‡æ³¨æ–¹æ³•ï¼Œå¹¶å¯è¿›è¡Œç»“æ„åŒ–é”™è¯¯åˆ†æã€‚</li>
<li>åå°„å‹æå–æ–¹å¼åœ¨å®Œæ•´æ€§ã€ç²¾ç¡®åº¦å’Œç›¸å…³æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€Œå•éæå–ä¿æŒæœ€é«˜å¿ å®åº¦ã€‚</li>
<li>FinReflectKG-EvalBenché€šè¿‡èšåˆå¤šç§è¯„ä¼°ç»´åº¦ï¼Œæ¨åŠ¨äº†é‡‘èAIåº”ç”¨çš„é€æ˜æ€§å’Œæ²»ç†è¿›æ­¥ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒå¤šç§æå–æ¨¡å¼ï¼ŒåŒ…æ‹¬å•éæå–ã€å¤šéæå–å’Œåå°„ä»£ç†åŸºæå–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-091fb3737b99ceb741bd670a631b0ca8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a21c9461618165be2c54d016dd4459b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-14be2453d97f219d364709c4bb7bbc1b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973601&auth_key=1759973601-0-0-8ca3f99c959e865863ef25f015b28ab3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-2933869ed77abc432033d2bdaf3d5a4f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Joint-Communication-Scheduling-and-Velocity-Control-for-Multi-UAV-Assisted-Post-Disaster-Monitoring-An-Attention-Based-In-Context-Learning-Approach"><a href="#Joint-Communication-Scheduling-and-Velocity-Control-for-Multi-UAV-Assisted-Post-Disaster-Monitoring-An-Attention-Based-In-Context-Learning-Approach" class="headerlink" title="Joint Communication Scheduling and Velocity Control for   Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context   Learning Approach"></a>Joint Communication Scheduling and Velocity Control for   Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context   Learning Approach</h2><p><strong>Authors:Yousef Emami, Seyedsina Nabavirazavi, Jingjing Zheng, Hao Zhou, Miguel Gutierrez Gaitan, Kai Li, Luis Almeida</strong></p>
<p>Recently, Unmanned Aerial Vehicles (UAVs) are increasingly being investigated to collect sensory data in post-disaster monitoring scenarios, such as tsunamis, where early actions are critical to limit coastal damage. A major challenge is to design the data collection schedules and flight velocities, as unfavorable schedules and velocities can lead to transmission errors and buffer overflows of the ground sensors, ultimately resulting in significant packet loss. Meanwhile, online Deep Reinforcement Learning (DRL) solutions have a complex training process and a mismatch between simulation and reality that does not meet the urgent requirements of tsunami monitoring. Recent advances in Large Language Models (LLMs) offer a compelling alternative. With their strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation through natural language prompts and example-based guidance without retraining. However, LLM models have input data limitations and thus require customized approaches. In this paper, a joint optimization of data collection schedules and velocities control for multiple UAVs is proposed to minimize data loss. The battery level of the ground sensors, the length of the queues, and the channel conditions, as well as the trajectories of the UAVs, are taken into account. Attention-Based In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS) is proposed as an alternative to DRL in emergencies. The simulation results show that the proposed AIC-VDS outperforms both the Deep-Q-Network (DQN) and maximum channel gain baselines. </p>
<blockquote>
<p>è¿‘æœŸï¼Œæ— äººé£è¡Œå™¨ï¼ˆUAVsï¼‰åœ¨ç¾åç›‘æµ‹åœºæ™¯ä¸­çš„æ„Ÿå®˜æ•°æ®æ”¶é›†è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œä¾‹å¦‚åœ¨æµ·å•¸ç­‰ç¾éš¾ä¸­ï¼Œæ—©æœŸè¡ŒåŠ¨å¯¹äºé™åˆ¶æ²¿æµ·ç ´åè‡³å…³é‡è¦ã€‚ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯è®¾è®¡æ•°æ®æ”¶é›†è®¡åˆ’å’Œé£è¡Œé€Ÿåº¦ï¼Œå› ä¸ºä¸åˆ©çš„æ—¶é—´å’Œé€Ÿåº¦è®¾ç½®ä¼šå¯¼è‡´åœ°é¢ä¼ æ„Ÿå™¨çš„ä¼ è¾“é”™è¯¯å’Œç¼“å†²åŒºæº¢å‡ºï¼Œæœ€ç»ˆé€ æˆé‡å¤§æ•°æ®åŒ…ä¸¢å¤±ã€‚åŒæ—¶ï¼Œåœ¨çº¿æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰è§£å†³æ–¹æ¡ˆå­˜åœ¨è®­ç»ƒè¿‡ç¨‹å¤æ‚ä»¥åŠåœ¨ä»¿çœŸä¸ç°å®ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…çš„é—®é¢˜ï¼Œæ— æ³•æ»¡è¶³æµ·å•¸ç›‘æµ‹çš„ç´§è¿«éœ€æ±‚ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æä¾›äº†ä¸€ä¸ªå¸å¼•äººçš„æ›¿ä»£æ–¹æ¡ˆã€‚å‡­å€Ÿå¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒLLMå¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰é€‚åº”æ–°ä»»åŠ¡ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå’ŒåŸºäºç¤ºä¾‹çš„æŒ‡å¯¼æ¥é€‚åº”ä»»åŠ¡ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼ŒLLMæ¨¡å‹å­˜åœ¨è¾“å…¥æ•°æ®é™åˆ¶ï¼Œå› æ­¤éœ€è¦å®šåˆ¶çš„æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæå‡ºäº†ä¸€ç§å¯¹å¤šä¸ªæ— äººé£è¡Œå™¨çš„æ•°æ®æ”¶é›†è®¡åˆ’å’Œé€Ÿåº¦æ§åˆ¶çš„è”åˆä¼˜åŒ–æ–¹æ¡ˆï¼Œä»¥æœ€å°åŒ–æ•°æ®ä¸¢å¤±ã€‚è€ƒè™‘åˆ°åœ°é¢ä¼ æ„Ÿå™¨çš„ç”µæ± æ°´å¹³ã€é˜Ÿåˆ—é•¿åº¦ã€ä¿¡é“æ¡ä»¶ä»¥åŠæ— äººé£è¡Œå™¨çš„è½¨è¿¹ã€‚æå‡ºåŸºäºæ³¨æ„åŠ›çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç”¨äºé€Ÿåº¦æ§åˆ¶å’Œæ•°æ®æ”¶é›†è®¡åˆ’ï¼ˆAIC-VDSï¼‰ä½œä¸ºç´§æ€¥æƒ…å†µä¸­DRLçš„æ›¿ä»£æ–¹æ¡ˆã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„AIC-VDSåœ¨æ€§èƒ½ä¸Šä¼˜äºæ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰å’Œæœ€å¤§ä¿¡é“å¢ç›ŠåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05698v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ— äººæœºåœ¨ç¾åç›‘æµ‹åœºæ™¯ä¸­æ”¶é›†æ„Ÿå®˜æ•°æ®è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œå¦‚æµ·å•¸ç­‰ç¾å®³ã€‚æ•°æ®æ”¶é›†æ—¥ç¨‹å’Œé£è¡Œé€Ÿåº¦çš„è®¾è®¡æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œä¸åˆç†çš„è®¾ç½®ä¼šå¯¼è‡´ä¼ è¾“é”™è¯¯å’Œåœ°é¢ä¼ æ„Ÿå™¨ç¼“å†²åŒºæº¢å‡ºï¼Œé€ æˆæ•°æ®åŒ…ä¸¢å¤±ã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ è§£å†³æ–¹æ¡ˆè™½å¯ç”¨äºæ­¤åœºæ™¯ï¼Œä½†å…¶å¤æ‚çš„è®­ç»ƒè¿‡ç¨‹å’Œæ¨¡æ‹Ÿä¸ç°å®çš„å·®å¼‚æ— æ³•æ»¡è¶³æµ·å•¸ç›‘æµ‹çš„ç´§æ€¥éœ€æ±‚ã€‚å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ é€‚åº”æ–°ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è”åˆä¼˜åŒ–æ— äººæœºæ•°æ®æ”¶é›†æ—¥ç¨‹å’Œé€Ÿåº¦æ§åˆ¶çš„æ–¹æ³•ï¼Œä»¥æœ€å°åŒ–æ•°æ®æŸå¤±ï¼Œè€ƒè™‘åœ°é¢ä¼ æ„Ÿå™¨ç”µæ± æ°´å¹³ã€é˜Ÿåˆ—é•¿åº¦ã€é€šé“æ¡ä»¶ä»¥åŠæ— äººæœºè½¨è¿¹ç­‰å› ç´ ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œæå‡ºçš„åŸºäºæ³¨æ„åŠ›çš„ä¸Šä¸‹æ–‡å­¦ä¹ åœ¨é€Ÿåº¦æ§åˆ¶å’Œæ•°æ®æ”¶é›†æ—¥ç¨‹ä¸Šçš„è¡¨ç°ä¼˜äºæ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰å’Œæœ€å¤§é€šé“å¢ç›ŠåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UAVsåœ¨ç¾åç›‘æµ‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨æµ·å•¸ç­‰ç¾å®³çš„æ—©æœŸè¡ŒåŠ¨ä¸­ã€‚</li>
<li>æ•°æ®æ”¶é›†æ—¥ç¨‹å’Œé£è¡Œé€Ÿåº¦è®¾è®¡æ˜¯ç¡®ä¿æ•°æ®å‡†ç¡®æ€§çš„å…³é”®ï¼Œä»¥é¿å…ä¼ è¾“é”™è¯¯å’Œæ•°æ®åŒ…ä¸¢å¤±ã€‚</li>
<li>DRLè§£å†³æ­¤é—®é¢˜é¢ä¸´è®­ç»ƒå¤æ‚æ€§å’Œæ¨¡æ‹Ÿä¸ç°å®çš„å·®å¼‚æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ é€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>æå‡ºçš„è”åˆä¼˜åŒ–æ–¹æ³•è€ƒè™‘äº†å¤šç§å› ç´ ï¼Œå¦‚åœ°é¢ä¼ æ„Ÿå™¨ç”µæ± æ°´å¹³ã€é˜Ÿåˆ—é•¿åº¦å’Œé€šé“æ¡ä»¶ç­‰ã€‚</li>
<li>åŸºäºæ³¨æ„åŠ›çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•åœ¨é€Ÿåº¦æ§åˆ¶å’Œæ•°æ®æ”¶é›†æ—¥ç¨‹ä¸Šçš„è¡¨ç°ä¼˜äºDQNå’Œæœ€å¤§é€šé“å¢ç›ŠåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d71f729dc1295d3127ef052dc661bb46~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973616&auth_key=1759973616-0-0-ea2c686e9089d53729bfa4afeb44e9e5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1768dd1ff0d3c532fd7fe082ecd6542b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973624&auth_key=1759973624-0-0-b9900a51fca5f9c9c99039e8c2b31ba3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-bd27f714edb7e5d67d6664fb05aaba04.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-910de383070d6a66cc01ab66f7fb537d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973637&auth_key=1759973637-0-0-eef5c1844add694d84314dfb9e7d7902&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-fe935ce55c026d32a70d68f3f1559a8a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-09/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-09/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-2d6f0849da3a5124ffc267efb99c7524~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973651&auth_key=1759973651-0-0-9f95c946ef91eb301263ffdcdb1dca9b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-09  EgoNight Towards Egocentric Vision Understanding at Night with a   Challenging Benchmark
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ca2dbdae06818516195f71f6e3eef2ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944504&auth_key=1759944504-0-0-df2fa6986fe7d8a907c7f8552a135c4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-07  Role of universal function of the nuclear proximity potential A   systematic study on the alpha-decay of heavy/super-heavy nuclei and   Î±-induced reactions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
