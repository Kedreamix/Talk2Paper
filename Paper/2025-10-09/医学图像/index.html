<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-09  Multimodal Feature Prototype Learning for Interpretable and   Discriminative Cancer Survival Prediction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b4e31bf6e5ba629e965c7206a07726bb')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-09-æ›´æ–°"><a href="#2025-10-09-æ›´æ–°" class="headerlink" title="2025-10-09 æ›´æ–°"></a>2025-10-09 æ›´æ–°</h1><h2 id="Multimodal-Feature-Prototype-Learning-for-Interpretable-and-Discriminative-Cancer-Survival-Prediction"><a href="#Multimodal-Feature-Prototype-Learning-for-Interpretable-and-Discriminative-Cancer-Survival-Prediction" class="headerlink" title="Multimodal Feature Prototype Learning for Interpretable and   Discriminative Cancer Survival Prediction"></a>Multimodal Feature Prototype Learning for Interpretable and   Discriminative Cancer Survival Prediction</h2><p><strong>Authors:Shuo Jiang, Zhuwen Chen, Liaoman Xu, Yanming Zhu, Changmiao Wang, Jiong Zhang, Feiwei Qin, Yifei Chen, Zhu Zhu</strong></p>
<p>Survival analysis plays a vital role in making clinical decisions. However, the models currently in use are often difficult to interpret, which reduces their usefulness in clinical settings. Prototype learning presents a potential solution, yet traditional methods focus on local similarities and static matching, neglecting the broader tumor context and lacking strong semantic alignment with genomic data. To overcome these issues, we introduce an innovative prototype-based multimodal framework, FeatProto, aimed at enhancing cancer survival prediction by addressing significant limitations in current prototype learning methodologies within pathology. Our framework establishes a unified feature prototype space that integrates both global and local features of whole slide images (WSI) with genomic profiles. This integration facilitates traceable and interpretable decision-making processes. Our approach includes three main innovations: (1) A robust phenotype representation that merges critical patches with global context, harmonized with genomic data to minimize local bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) that sustains stable cross-modal associations and employs a wandering mechanism to adapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototype matching scheme designed to capture global centrality, local typicality, and cohort-level trends, thereby refining prototype inference. Comprehensive evaluations on four publicly available cancer datasets indicate that our method surpasses current leading unimodal and multimodal survival prediction techniques in both accuracy and interoperability, providing a new perspective on prototype learning for critical medical applications. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/JSLiam94/FeatProto">https://github.com/JSLiam94/FeatProto</a>. </p>
<blockquote>
<p>ç”Ÿå­˜åˆ†æåœ¨ä¸´åºŠå†³ç­–ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ä½¿ç”¨çš„æ¨¡å‹å¾€å¾€éš¾ä»¥è§£é‡Šï¼Œä»è€Œé™ä½äº†å®ƒä»¬åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚è™½ç„¶åŸå‹å­¦ä¹ æä¾›äº†ä¸€ç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ä¼ ç»Ÿçš„æ–¹æ³•ä¾§é‡äºå±€éƒ¨ç›¸ä¼¼æ€§å’Œé™æ€åŒ¹é…ï¼Œå¿½ç•¥äº†æ›´å¹¿æ³›çš„è‚¿ç˜¤ä¸Šä¸‹æ–‡ï¼Œä»¥åŠä¸åŸºå› ç»„æ•°æ®çš„å¼ºçƒˆè¯­ä¹‰å¯¹é½ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„åŸºäºåŸå‹çš„å¤šæ¨¡å¼æ¡†æ¶â€”â€”FeatProtoï¼Œæ—¨åœ¨é€šè¿‡è§£å†³ç—…ç†å­¦é¢†åŸŸå½“å‰åŸå‹å­¦ä¹ æ–¹æ³•ä¸­çš„é‡å¤§å±€é™æ€§ï¼Œå¢å¼ºç™Œç—‡ç”Ÿå­˜é¢„æµ‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„ç‰¹å¾åŸå‹ç©ºé—´ï¼Œè¯¥ç©ºé—´æ•´åˆäº†å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ä»¥åŠåŸºå› ç»„å›¾è°±ã€‚è¿™ç§æ•´åˆä¿ƒè¿›äº†å¯è¿½æº¯å’Œå¯è§£é‡Šæ€§çš„å†³ç­–è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€ç§å¼ºå¤§çš„è¡¨å‹è¡¨ç¤ºï¼Œå°†å…³é”®è¡¥ä¸ä¸å…¨å±€ä¸Šä¸‹æ–‡åˆå¹¶ï¼Œä¸åŸºå› ç»„æ•°æ®åè°ƒï¼Œä»¥æœ€å°åŒ–å±€éƒ¨åè§ã€‚ï¼ˆ2ï¼‰ä¸€ç§æŒ‡æ•°åŸå‹æ›´æ–°ç­–ç•¥ï¼ˆEMA ProtoUpï¼‰ï¼Œå®ƒç»´æŒç¨³å®šçš„è·¨æ¨¡æ€å…³è”ï¼Œå¹¶é‡‡ç”¨æ¼«æ¸¸æœºåˆ¶ä½¿åŸå‹èƒ½å¤Ÿçµæ´»åœ°é€‚åº”è‚¿ç˜¤å¼‚è´¨æ€§ã€‚ï¼ˆ3ï¼‰ä¸€ç§åˆ†å±‚åŸå‹åŒ¹é…æ–¹æ¡ˆï¼Œæ—¨åœ¨æ•æ‰å…¨å±€ä¸­å¿ƒæ€§ã€å±€éƒ¨å…¸å‹æ€§å’Œç¾¤ä½“æ°´å¹³è¶‹åŠ¿ï¼Œä»è€Œç»†åŒ–åŸå‹æ¨ç†ã€‚åœ¨å››ä¸ªå…¬å¼€çš„ç™Œç—‡æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œå¯æ“ä½œæ€§æ–¹é¢è¶…è¶Šäº†å½“å‰é¢†å…ˆçš„å•æ¨¡å¼å’Œå¤šæ¨¡å¼ç”Ÿå­˜é¢„æµ‹æŠ€æœ¯ï¼Œä¸ºåŸå‹å­¦ä¹ åœ¨å…³é”®åŒ»ç–—åº”ç”¨æ–¹é¢æä¾›äº†æ–°çš„è§†è§’ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JSLiam94/FeatProto">https://github.com/JSLiam94/FeatProto</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06113v1">PDF</a> 12 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºåŸå‹çš„å¤šæ¨¡æ€æ¡†æ¶FeatProtoï¼Œç”¨äºæé«˜ç™Œç—‡ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡æ•´åˆå…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œå»ºç«‹ç»Ÿä¸€ç‰¹å¾åŸå‹ç©ºé—´ï¼Œå°†å…¨è§†é‡å›¾åƒï¼ˆWSIï¼‰çš„åŸºå› ç»„æ•°æ®ä¸è‚¿ç˜¤æ•´ä½“èƒŒæ™¯ç›¸ç»“åˆï¼Œä»¥å®ç°å¯è¿½è¸ªå’Œå¯è§£é‡Šæ€§çš„å†³ç­–è¿‡ç¨‹ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŒ…æ‹¬å¼ºå¤§çš„è¡¨å‹è¡¨ç¤ºã€æŒ‡æ•°åŸå‹æ›´æ–°ç­–ç•¥ï¼ˆEMA ProtoUpï¼‰å’Œå±‚æ¬¡åŸå‹åŒ¹é…æ–¹æ¡ˆï¼Œå¯åœ¨å››ä¸ªå…¬å¼€ç™Œç—‡æ•°æ®é›†ä¸Šå®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯æ“ä½œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç”Ÿå­˜åˆ†ææ¨¡å‹åœ¨ä¸´åºŠåŒ»å­¦ä¸­çš„è§£é‡Šæ€§è¾ƒå·®ï¼Œå½±å“äº†å…¶åœ¨ä¸´åºŠå†³ç­–ä¸­çš„åº”ç”¨ã€‚</li>
<li>ä¼ ç»ŸåŸå‹å­¦ä¹ æ–¹æ³•å…³æ³¨å±€éƒ¨ç›¸ä¼¼æ€§å’Œé™æ€åŒ¹é…ï¼Œå¿½ç•¥äº†è‚¿ç˜¤æ•´ä½“èƒŒæ™¯å’ŒåŸºå› ç»„æ•°æ®çš„è¯­ä¹‰å¯¹é½ã€‚</li>
<li>FeatProtoæ¡†æ¶é€šè¿‡æ•´åˆå…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œå»ºç«‹ç»Ÿä¸€ç‰¹å¾åŸå‹ç©ºé—´ï¼Œæé«˜äº†ç™Œç—‡ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨å¼ºå¤§çš„è¡¨å‹è¡¨ç¤ºï¼Œç»“åˆåŸºå› ç»„æ•°æ®ï¼Œå‡å°‘å±€éƒ¨åè§ã€‚</li>
<li>é‡‡ç”¨æŒ‡æ•°åŸå‹æ›´æ–°ç­–ç•¥ï¼ˆEMA ProtoUpï¼‰ç»´æŒè·¨æ¨¡æ€å…³è”çš„ç¨³å®šæ€§ï¼Œå¹¶é€šè¿‡æ¼«æ¸¸æœºåˆ¶çµæ´»é€‚åº”è‚¿ç˜¤å¼‚è´¨æ€§ã€‚</li>
<li>å±‚æ¬¡åŸå‹åŒ¹é…æ–¹æ¡ˆèƒ½å¤Ÿæ•æ‰å…¨å±€ä¸­å¿ƒæ€§ã€å±€éƒ¨å…¸å‹æ€§å’Œç¾¤ä½“æ°´å¹³è¶‹åŠ¿ï¼Œä»è€Œä¼˜åŒ–åŸå‹æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5716bca3fa12ced74d1ac3186540ce62" align="middle">
<img src="https://picx.zhimg.com/v2-54c934b0a5a82c8a745678a717eff7e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2a637ae7ea1981189d0c67ee2e84b28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b628474b8128f3d7129426fa31019dc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-public-cardiac-CT-dataset-featuring-the-left-atrial-appendage"><a href="#A-public-cardiac-CT-dataset-featuring-the-left-atrial-appendage" class="headerlink" title="A public cardiac CT dataset featuring the left atrial appendage"></a>A public cardiac CT dataset featuring the left atrial appendage</h2><p><strong>Authors:Bjoern Hansen, Jonas Pedersen, Klaus F. Kofoed, Oscar Camara, Rasmus R. Paulsen, Kristine Soerensen</strong></p>
<p>Despite the success of advanced segmentation frameworks such as TotalSegmentator (TS), accurate segmentations of the left atrial appendage (LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant challenge in medical imaging. In this work, we present the first open-source, anatomically coherent dataset of curated, high-resolution segmentations for these structures, supplemented with whole-heart labels produced by TS on the publicly available ImageCAS dataset consisting of 1000 cardiac computed tomography angiography (CCTA) scans. One purpose of the data set is to foster novel approaches to the analysis of LAA morphology.   LAA segmentations on ImageCAS were generated using a state-of-the-art segmentation framework developed specifically for high resolution LAA segmentation. We trained the network on a large private dataset with manual annotations provided by medical readers guided by a trained cardiologist and transferred the model to ImageCAS data. CA labels were improved from the original ImageCAS annotations, while PV segmentations were refined from TS outputs. In addition, we provide a list of scans from ImageCAS that contains common data flaws such as step artefacts, LAAs extending beyond the scannerâ€™s field of view, and other types of data defects. </p>
<blockquote>
<p>å°½ç®¡TotalSegmentatorï¼ˆTSï¼‰ç­‰å…ˆè¿›åˆ†å‰²æ¡†æ¶å–å¾—äº†æˆåŠŸï¼Œä½†å·¦å¿ƒè€³ï¼ˆLAAï¼‰ã€å† çŠ¶åŠ¨è„‰ï¼ˆCAï¼‰å’Œè‚ºé™è„‰ï¼ˆPVï¼‰çš„ç²¾ç¡®åˆ†å‰²åœ¨åŒ»å­¦æˆåƒä¸­ä»æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ç¬¬ä¸€ä¸ªå¼€æºçš„ã€è§£å‰–ç»“æ„è¿è´¯çš„æ•°æ®é›†ï¼ŒåŒ…å«è¿™äº›ç»“æ„çš„ç²¾é€‰é«˜åˆ†è¾¨ç‡åˆ†å‰²ï¼Œä»¥åŠç”±TSåœ¨å…¬å¼€å¯ç”¨çš„ImageCASæ•°æ®é›†ä¸Šç”Ÿæˆçš„æ•´ä¸ªå¿ƒè„æ ‡ç­¾ã€‚è¯¥æ•°æ®é›†çš„ä¸€ä¸ªç›®çš„æ˜¯ä¿ƒè¿›LAAå½¢æ€åˆ†æçš„æ–°æ–¹æ³•ã€‚ImageCASä¸Šçš„LAAåˆ†å‰²æ˜¯ä½¿ç”¨ä¸“ä¸ºé«˜åˆ†è¾¨ç‡LAAåˆ†å‰²å¼€å‘çš„æœ€å…ˆè¿›åˆ†å‰²æ¡†æ¶ç”Ÿæˆçš„ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªå¤§å‹ç§æœ‰æ•°æ®é›†ä¸Šè®­ç»ƒäº†ç½‘ç»œï¼Œè¯¥æ•°æ®é›†çš„æ ‡æ³¨ç”±å—è®­çš„å¿ƒè„ç—…åŒ»ç”ŸæŒ‡å¯¼çš„åŒ»å­¦é˜…è¯»è€…æä¾›ï¼Œå¹¶å°†æ¨¡å‹è½¬ç§»åˆ°ImageCASæ•°æ®ä¸Šã€‚CAæ ‡ç­¾æ˜¯å¯¹åŸå§‹ImageCASæ³¨é‡Šçš„æ”¹è¿›ï¼Œè€ŒPVåˆ†å‰²æ˜¯å¯¹TSè¾“å‡ºçš„ç»†åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ—å‡ºäº†ImageCASä¸­çš„ä¸€äº›æ‰«æç»“æœï¼Œå…¶ä¸­åŒ…å«å¸¸è§çš„æ•°æ®ç¼ºé™·ï¼Œå¦‚é˜¶æ¢¯æ•ˆåº”ã€LAAè¶…å‡ºæ‰«æä»ªçš„è§†é‡ä»¥åŠå…¶ä»–ç±»å‹çš„æ•°æ®ç¼ºé™·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06090v1">PDF</a> 8 pages, 5 figures, published at STACOM2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»¥å…¬å¼€å¯ç”¨çš„ImageCASæ•°æ®é›†ä¸ºåŸºç¡€ï¼Œä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹å·¦å¿ƒæˆ¿é™„è‚¢ï¼ˆLAAï¼‰ã€å† çŠ¶åŠ¨è„‰ï¼ˆCAï¼‰å’Œè‚ºé™è„‰ï¼ˆPVï¼‰çš„é«˜åˆ†è¾¨ç‡ã€è§£å‰–è¿è´¯æ€§çš„æ•°æ®é›†ã€‚æ•°æ®é›†åŒ…å«ä½¿ç”¨å…ˆè¿›çš„åˆ†å‰²æ¡†æ¶ï¼ˆå¦‚TotalSegmentatorï¼‰ç”Ÿæˆçš„æ ‡ç­¾ï¼Œæ—¨åœ¨ä¿ƒè¿›å¯¹è¿™äº›ç»“æ„åˆ†æçš„æ–°æ–¹æ³•çš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨LAAå½¢æ€åˆ†ææ–¹é¢ã€‚åŒæ—¶ï¼Œä¹Ÿæä¾›äº†åŒ…å«å¸¸è§æ•°æ®ç¼ºé™·çš„æ‰«æåˆ—è¡¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†é’ˆå¯¹å·¦å¿ƒæˆ¿é™„è‚¢ï¼ˆLAAï¼‰ã€å† çŠ¶åŠ¨è„‰ï¼ˆCAï¼‰å’Œè‚ºé™è„‰ï¼ˆPVï¼‰çš„é«˜åˆ†è¾¨ç‡è§£å‰–è¿è´¯æ€§æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†æ˜¯å…¬å¼€å¯ç”¨çš„ï¼Œå¹¶ä¸”æ˜¯é¦–ä¸ªå¼€æºçš„æ­¤ç±»æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ä½¿ç”¨å…ˆè¿›çš„åˆ†å‰²æ¡†æ¶ï¼ˆå¦‚TotalSegmentatorï¼‰ç”Ÿæˆçš„æ ‡ç­¾ã€‚</li>
<li>æ•°æ®é›†æ—¨åœ¨ä¿ƒè¿›å¯¹è¿™äº›ç»“æ„åˆ†æçš„æ–°æ–¹æ³•çš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨LAAå½¢æ€åˆ†ææ–¹é¢ã€‚</li>
<li>LAAåˆ†å‰²æ˜¯åœ¨ImageCASæ•°æ®é›†ä¸Šç”Ÿæˆçš„ï¼Œä½¿ç”¨äº†ä¸“é—¨ä¸ºé«˜åˆ†è¾¨ç‡LAAåˆ†å‰²å¼€å‘çš„å…ˆè¿›åˆ†å‰²æ¡†æ¶ã€‚</li>
<li>å† çŠ¶åŠ¨è„‰ï¼ˆCAï¼‰çš„æ ‡ç­¾åœ¨åŸå§‹ImageCASæ³¨é‡Šçš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œè€Œè‚ºé™è„‰ï¼ˆPVï¼‰çš„åˆ†å‰²åˆ™åŸºäºTSè¾“å‡ºè¿›è¡Œäº†ç»†åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f06047ba07428368f4356d460adce00" align="middle">
<img src="https://picx.zhimg.com/v2-e5def5475328d0efbaf9a30b0a22c65c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d0a407ef089cdfd8e0d8ec6541771b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5fb654ffb9be4318ec723289274bbd1" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Efficient-Universal-Models-for-Medical-Image-Segmentation-via-Weakly-Supervised-In-Context-Learning"><a href="#Efficient-Universal-Models-for-Medical-Image-Segmentation-via-Weakly-Supervised-In-Context-Learning" class="headerlink" title="Efficient Universal Models for Medical Image Segmentation via Weakly   Supervised In-Context Learning"></a>Efficient Universal Models for Medical Image Segmentation via Weakly   Supervised In-Context Learning</h2><p><strong>Authors:Jiesi Hu, Yanwu Yang, Zhiyu Ye, Jinyan Zhou, Jianfeng Cao, Hanyang Peng, Ting Ma</strong></p>
<p>Universal models for medical image segmentation, such as interactive and in-context learning (ICL) models, offer strong generalization but require extensive annotations. Interactive models need repeated user prompts for each image, while ICL relies on dense, pixel-level labels. To address this, we propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that leverages weak prompts (e.g., bounding boxes or points) instead of dense labels for context. This approach significantly reduces annotation effort by eliminating the need for fine-grained masks and repeated user prompting for all images. We evaluated the proposed WS-ICL model on three held-out benchmarks. Experimental results demonstrate that WS-ICL achieves performance comparable to regular ICL models at a significantly lower annotation cost. In addition, WS-ICL is highly competitive even under the interactive paradigm. These findings establish WS-ICL as a promising step toward more efficient and unified universal models for medical image segmentation. Our code and model are publicly available at <a target="_blank" rel="noopener" href="https://github.com/jiesihu/Weak-ICL">https://github.com/jiesihu/Weak-ICL</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²çš„é€šç”¨æ¨¡å‹ï¼Œå¦‚äº¤äº’å¼å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¨¡å‹ï¼Œè™½ç„¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†éœ€è¦å¤§é‡çš„æ ‡æ³¨ã€‚äº¤äº’å¼æ¨¡å‹éœ€è¦å¯¹æ¯å¼ å›¾åƒè¿›è¡Œå¤šæ¬¡ç”¨æˆ·æç¤ºï¼Œè€ŒICLåˆ™ä¾èµ–äºå¯†é›†çš„åƒç´ çº§æ ‡ç­¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¼±ç›‘ç£ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆWS-ICLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ICLèŒƒå¼ï¼Œå®ƒåˆ©ç”¨å¼±æç¤ºï¼ˆå¦‚è¾¹ç•Œæ¡†æˆ–ç‚¹ï¼‰è€Œä¸æ˜¯å¯†é›†çš„æ ‡ç­¾æ¥è¿›è¡Œä¸Šä¸‹æ–‡ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ¶ˆé™¤å¯¹ç²¾ç»†é®ç½©å’Œæ‰€æœ‰å›¾åƒé‡å¤ç”¨æˆ·æç¤ºçš„éœ€æ±‚ï¼Œæ˜¾è‘—å‡å°‘äº†æ ‡æ³¨å·¥ä½œé‡ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç‹¬ç«‹çš„æ ‡å‡†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„WS-ICLæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWS-ICLåœ¨æ ‡æ³¨æˆæœ¬æ˜¾è‘—é™ä½çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸å¸¸è§„ICLæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨äº¤äº’å¼æ¨¡å¼ä¸‹ï¼ŒWS-ICLä¹Ÿå…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒWS-ICLæ˜¯æœç€æ›´é«˜æ•ˆå’Œç»Ÿä¸€çš„åŒ»å­¦å›¾åƒåˆ†å‰²é€šç”¨æ¨¡å‹è¿ˆå‡ºçš„æœ‰å¸Œæœ›çš„ä¸€æ­¥ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiesihu/Weak-ICL%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/jiesihu/Weak-ICLå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05899v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²çš„é€šç”¨æ¨¡å‹ï¼Œå¦‚äº¤äº’å¼å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¨¡å‹ï¼Œè™½ç„¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†éœ€è¦å¤§é‡çš„æ ‡æ³¨ã€‚äº¤äº’å¼æ¨¡å‹éœ€è¦é’ˆå¯¹æ¯å¼ å›¾åƒè¿›è¡Œå¤šæ¬¡ç”¨æˆ·æç¤ºï¼Œè€ŒICLåˆ™ä¾èµ–äºå¯†é›†çš„åƒç´ çº§æ ‡ç­¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¼±ç›‘ç£ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆWS-ICLï¼‰è¿™ä¸€æ–°çš„ICLæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å¼±æç¤ºï¼ˆå¦‚è¾¹ç•Œæ¡†æˆ–ç‚¹ï¼‰ä»£æ›¿å¯†é›†çš„æ ‡ç­¾æ¥æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ¶ˆé™¤å¯¹ç²¾ç»†æ©è†œå’Œæ‰€æœ‰å›¾åƒé‡å¤ç”¨æˆ·æç¤ºçš„éœ€æ±‚ï¼Œå¤§å¤§é™ä½äº†æ ‡æ³¨å·¥ä½œé‡ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç‹¬ç«‹çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå¯¹WS-ICLæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWS-ICLåœ¨æ€§èƒ½ä¸Šå¯ä¸å¸¸è§„çš„ICLæ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨äº¤äº’å¼æ¨¡å¼ä¸‹ï¼ŒWS-ICLä¹Ÿè¡¨ç°å‡ºé«˜åº¦çš„ç«äº‰åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜WS-ICLæ˜¯æœç€æ›´é«˜æ•ˆå’Œç»Ÿä¸€çš„åŒ»å­¦å›¾åƒåˆ†å‰²é€šç”¨æ¨¡å‹çš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICLæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å…·æœ‰è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œä½†éœ€å¤§é‡æ ‡æ³¨ã€‚</li>
<li>WS-ICLæ˜¯ä¸€ç§æ–°çš„ICLæ–¹æ³•ï¼Œåˆ©ç”¨å¼±æç¤ºæä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé™ä½æ ‡æ³¨å·¥ä½œé‡ã€‚</li>
<li>WS-ICLä¸éœ€è¦ç²¾ç»†æ©è†œå’Œæ‰€æœ‰å›¾åƒçš„é‡å¤ç”¨æˆ·æç¤ºã€‚</li>
<li>WS-ICLåœ¨æ€§èƒ½ä¸Šå¯ä¸å¸¸è§„ICLæ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶é™ä½æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>WS-ICLåœ¨äº¤äº’å¼æ¨¡å¼ä¸‹ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>WS-ICLæ˜¯æœç€æ›´é«˜æ•ˆå’Œç»Ÿä¸€çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„é‡è¦ä¸€æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05899">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f1270b1ef8e120fcf21438271332bef" align="middle">
<img src="https://picx.zhimg.com/v2-fa2728f5c35854d7b405a05c088e983d" align="middle">
<img src="https://picx.zhimg.com/v2-d89f0df1fdd20cb9352454f853113f21" align="middle">
<img src="https://picx.zhimg.com/v2-21ae282cdcb7aba599d67777e0895fb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8ca4c56aa90618025d35ca8fd8a9dd6" align="middle">
<img src="https://picx.zhimg.com/v2-5d54a5c54ad19d544ce079aa2f80fdce" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="acia-workflows-Automated-Single-cell-Imaging-Analysis-for-Scalable-and-Deep-Learning-based-Live-cell-Imaging-Analysis-Workflows"><a href="#acia-workflows-Automated-Single-cell-Imaging-Analysis-for-Scalable-and-Deep-Learning-based-Live-cell-Imaging-Analysis-Workflows" class="headerlink" title="acia-workflows: Automated Single-cell Imaging Analysis for Scalable and   Deep Learning-based Live-cell Imaging Analysis Workflows"></a>acia-workflows: Automated Single-cell Imaging Analysis for Scalable and   Deep Learning-based Live-cell Imaging Analysis Workflows</h2><p><strong>Authors:Johannes Seiffarth, Keitaro Kasahara, Michelle Bund, Benita LÃ¼ckel, Richard D. Paul, Mathias Pesch, Lennart Witting, Michael Bott, Dietrich Kohlheyer, Katharina NÃ¶h</strong></p>
<p>Live-cell imaging (LCI) technology enables the detailed spatio-temporal characterization of living cells at the single-cell level, which is critical for advancing research in the life sciences, from biomedical applications to bioprocessing. High-throughput setups with tens to hundreds of parallel cell cultivations offer the potential for robust and reproducible insights. However, these insights are obscured by the large amount of LCI data recorded per experiment. Recent advances in state-of-the-art deep learning methods for cell segmentation and tracking now enable the automated analysis of such large data volumes, offering unprecedented opportunities to systematically study single-cell dynamics. The next key challenge lies in integrating these powerful tools into accessible, flexible, and user-friendly workflows that support routine application in biological research. In this work, we present acia-workflows, a platform that combines three key components: (1) the Automated live-Cell Imaging Analysis (acia) Python library, which supports the modular design of image analysis pipelines offering eight deep learning segmentation and tracking approaches; (2) workflows that assemble the image analysis pipeline, its software dependencies, documentation, and visualizations into a single Jupyter Notebook, leading to accessible, reproducible and scalable analysis workflows; and (3) a collection of application workflows showcasing the analysis and customization capabilities in real-world applications. Specifically, we present three workflows to investigate various types of microfluidic LCI experiments ranging from growth rate comparisons to precise, minute-resolution quantitative analyses of individual dynamic cells responses to changing oxygen conditions. Our collection of more than ten application workflows is open source and publicly available at <a target="_blank" rel="noopener" href="https://github.com/JuBiotech/acia-workflows">https://github.com/JuBiotech/acia-workflows</a>. </p>
<blockquote>
<p>æ´»ç»†èƒæˆåƒï¼ˆLCIï¼‰æŠ€æœ¯èƒ½å¤Ÿåœ¨å•ç»†èƒæ°´å¹³ä¸Šå¯¹æ´»ç»†èƒè¿›è¡Œè¯¦ç»†çš„æ—¶ç©ºç‰¹å¾è¡¨å¾ï¼Œè¿™å¯¹äºæ¨åŠ¨ç”Ÿå‘½ç§‘å­¦é¢†åŸŸçš„ç ”ç©¶ï¼Œä»ç”Ÿç‰©åŒ»å­¦åº”ç”¨åˆ°ç”Ÿç‰©åŠ å·¥éƒ½è‡³å…³é‡è¦ã€‚å…·æœ‰æ•°ååˆ°æ•°ç™¾ä¸ªå¹¶è¡Œç»†èƒåŸ¹å…»çš„é«˜é€šé‡è®¾ç½®æä¾›äº†ç¨³å¥ä¸”å¯é‡å¤çš„æ´å¯ŸåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ´å¯Ÿè¢«æ¯æ¬¡å®éªŒè®°å½•çš„å¤§é‡LCIæ•°æ®æ‰€æ©ç›–ã€‚æœ€è¿‘å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨ç»†èƒåˆ†å‰²å’Œè·Ÿè¸ªæ–¹é¢çš„æœ€æ–°è¿›å±•ç°åœ¨èƒ½å¤Ÿå®ç°è¿™äº›å¤§æ•°æ®é‡çš„è‡ªåŠ¨åˆ†æï¼Œä¸ºç³»ç»Ÿåœ°ç ”ç©¶å•ç»†èƒåŠ¨æ€æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šã€‚ä¸‹ä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºå°†è¿™äº›å¼ºå¤§å·¥å…·é›†æˆåˆ°å¯è®¿é—®ã€çµæ´»å’Œç”¨æˆ·å‹å¥½çš„å·¥ä½œæµç¨‹ä¸­ï¼Œä»¥æ”¯æŒç”Ÿç‰©å­¦ç ”ç©¶ä¸­çš„å¸¸è§„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†acia-workflowså¹³å°ï¼Œè¯¥å¹³å°ç»“åˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰Automated live-Cell Imaging Analysisï¼ˆaciaï¼‰Pythonåº“ï¼Œæ”¯æŒå›¾åƒåˆ†æç®¡é“æ¨¡å—åŒ–è®¾è®¡ï¼Œæä¾›å…«ç§æ·±åº¦å­¦ä¹ åˆ†å‰²å’Œè·Ÿè¸ªæ–¹æ³•ï¼›ï¼ˆ2ï¼‰å°†å›¾åƒåˆ†æç®¡é“ã€å…¶è½¯ä»¶ä¾èµ–é¡¹ã€æ–‡æ¡£å’Œå¯è§†åŒ–ç»„è£…æˆå•ä¸ªJupyter Notebookçš„å·¥ä½œæµç¨‹ï¼Œä»è€Œå®ç°å¯è®¿é—®ã€å¯é‡å¤å’Œå¯æ‰©å±•çš„åˆ†æå·¥ä½œæµç¨‹ï¼›ï¼ˆ3ï¼‰ä¸€ç³»åˆ—åº”ç”¨ç¨‹åºå·¥ä½œæµç¨‹ï¼Œå±•ç¤ºå®é™…åº”ç”¨ç¨‹åºä¸­çš„åˆ†æå’Œå®šåˆ¶åŠŸèƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸‰ä¸ªå·¥ä½œæµç¨‹æ¥ç ”ç©¶å„ç§å¾®æµä½“LCIå®éªŒï¼Œä»ç”Ÿé•¿ç‡æ¯”è¾ƒåˆ°å¯¹å˜åŒ–æ°§æ°”æ¡ä»¶ä¸‹å•ä¸ªåŠ¨æ€ç»†èƒçš„ç²¾ç¡®åˆ†é’Ÿåˆ†è¾¨ç‡å®šé‡åˆ†æã€‚æˆ‘ä»¬æä¾›çš„åå¤šä¸ªåº”ç”¨ç¨‹åºå·¥ä½œæµç¨‹æ˜¯å¼€æºçš„ï¼Œå¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/JuBiotech/acia-workflows">https://github.com/JuBiotech/acia-workflows</a> å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05886v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ´»ä½“ç»†èƒæˆåƒæŠ€æœ¯å¯å®ç°å•ç»†èƒæ°´å¹³çš„æ—¶ç©ºç‰¹å¾è¯¦ç»†è¡¨å¾ï¼Œå¯¹ç”Ÿå‘½ç§‘å­¦é¢†åŸŸçš„ç ”ç©¶è‡³å…³é‡è¦ã€‚é€šè¿‡æ•°ååˆ°æ•°ç™¾ä¸ªå¹¶è¡Œç»†èƒåŸ¹å…»çš„é«˜é€šé‡è®¾ç½®ï¼Œå¯è·å¾—ç¨³å¥ä¸”å¯é‡å¤çš„è§è§£ã€‚ç„¶è€Œï¼Œè¿™äº›è§è§£è¢«å¤§é‡æˆåƒæ•°æ®æ‰€æ©ç›–ã€‚æœ€è¿‘å…ˆè¿›çš„æ·±åº¦å­¦ä¹ ç»†èƒåˆ†å‰²å’Œè·Ÿè¸ªæ–¹æ³•ä¸ºè‡ªåŠ¨åˆ†æå¤§é‡æ•°æ®æä¾›äº†å¯èƒ½ï¼Œä¸ºç ”ç©¶å•ç»†èƒåŠ¨æ€æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šã€‚ä¸‹ä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºå°†è¿™äº›å¼ºå¤§å·¥å…·é›†æˆåˆ°å¯è®¿é—®ã€çµæ´»å’Œç”¨æˆ·å‹å¥½çš„å·¥ä½œæµä¸­ï¼Œä»¥æ”¯æŒç”Ÿç‰©å­¦ç ”ç©¶çš„å¸¸è§„åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†acia-workflowså¹³å°ï¼Œè¯¥å¹³å°ç»“åˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ”¯æŒå›¾åƒåˆ†æç®¡é“æ¨¡å—åŒ–è®¾è®¡çš„Automated live-Cell Imaging Analysisï¼ˆaciaï¼‰Pythonåº“ã€è£…é…å›¾åƒåˆ†æç®¡é“ã€å…¶è½¯ä»¶ä¾èµ–é¡¹ã€æ–‡æ¡£å’Œå¯è§†åŒ–çš„å·¥ä½œæµï¼Œä»¥åŠå±•ç¤ºå®é™…åº”ç”¨ä¸­åˆ†æå’Œå®šåˆ¶åŠŸèƒ½çš„åº”ç”¨ç¨‹åºå·¥ä½œæµé›†åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ´»ä½“ç»†èƒæˆåƒæŠ€æœ¯å¯¹äºç”Ÿå‘½ç§‘å­¦é¢†åŸŸçš„ç ”ç©¶è‡³å…³é‡è¦ï¼Œå¯å®ç°å•ç»†èƒæ°´å¹³çš„è¯¦ç»†æ—¶ç©ºè¡¨å¾ã€‚</li>
<li>é«˜é€šé‡è®¾ç½®ä¸ºè·å–ç¨³å¥å’Œå¯é‡å¤çš„è§è§£æä¾›äº†æœºä¼šï¼Œä½†å¤§é‡æ•°æ®æ©ç›–äº†è¿™äº›è§è§£ã€‚</li>
<li>å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å®ç°äº†å¯¹å¤§é‡æ•°æ®çš„è‡ªåŠ¨åˆ†æï¼Œä¸ºç ”ç©¶å•ç»†èƒåŠ¨æ€æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šã€‚</li>
<li>å½“å‰æŒ‘æˆ˜åœ¨äºå°†å¼ºå¤§å·¥å…·é›†æˆåˆ°æ˜“äºè®¿é—®ã€çµæ´»å’Œç”¨æˆ·å‹å¥½çš„å·¥ä½œæµä¸­ã€‚</li>
<li>acia-workflowså¹³å°ç»“åˆäº†å›¾åƒåˆ†æç®¡é“çš„å…³é”®ç»„ä»¶ï¼Œæ”¯æŒæ¨¡å—åŒ–çš„è®¾è®¡ã€‚</li>
<li>è¯¥å¹³å°æä¾›äº†åŒ…æ‹¬ç”Ÿé•¿é€Ÿç‡æ¯”è¾ƒå’ŒåŠ¨æ€ç»†èƒå¯¹å˜åŒ–æ°§æ°”æ¡ä»¶çš„å“åº”åœ¨å†…çš„å¤šç§å¾®æµä½“æˆåƒå®éªŒçš„åˆ†æå·¥ä½œæµã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11e51f31bf3e0433b601ea5c6298e207" align="middle">
<img src="https://picx.zhimg.com/v2-b4e31bf6e5ba629e965c7206a07726bb" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Deformable-Image-Registration-for-Self-supervised-Cardiac-Phase-Detection-in-Multi-View-Multi-Disease-Cardiac-Magnetic-Resonance-Images"><a href="#Deformable-Image-Registration-for-Self-supervised-Cardiac-Phase-Detection-in-Multi-View-Multi-Disease-Cardiac-Magnetic-Resonance-Images" class="headerlink" title="Deformable Image Registration for Self-supervised Cardiac Phase   Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images"></a>Deformable Image Registration for Self-supervised Cardiac Phase   Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images</h2><p><strong>Authors:Sven Koehler, Sarah Kaye Mueller, Jonathan Kiekenap, Gerald Greil, Tarique Hussain, Samir Sarikouch, Florian AndrÃ©, Norbert Frey, Sandy Engelhardt</strong></p>
<p>Cardiovascular magnetic resonance (CMR) is the gold standard for assessing cardiac function, but individual cardiac cycles complicate automatic temporal comparison or sub-phase analysis. Accurate cardiac keyframe detection can eliminate this problem. However, automatic methods solely derive end-systole (ES) and end-diastole (ED) frames from left ventricular volume curves, which do not provide a deeper insight into myocardial motion. We propose a self-supervised deep learning method detecting five keyframes in short-axis (SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable registration fields are derived from the images and used to compute a 1D motion descriptor, which provides valuable insights into global cardiac contraction and relaxation patterns. From these characteristic curves, keyframes are determined using a simple set of rules. The method was independently evaluated for both views using three public, multicentre, multidisease datasets. M&amp;Ms-2 (n&#x3D;360) dataset was used for training and evaluation, and M&amp;Ms (n&#x3D;345) and ACDC (n&#x3D;100) datasets for repeatability control. Furthermore, generalisability to patients with rare congenital heart defects was tested using the German Competence Network (GCN) dataset. Our self-supervised approach achieved improved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED and ES, as measured by cyclic frame difference (cFD), compared with the volume-based approach. We can detect ED and ES, as well as three additional keyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for SAX and 1.73 for LAX. Our approach enables temporally aligned inter- and intra-patient analysis of cardiac dynamics, irrespective of cycle or phase lengths. GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git">https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git</a> </p>
<blockquote>
<p>å¿ƒè¡€ç®¡ç£å…±æŒ¯æˆåƒï¼ˆCMRï¼‰æ˜¯è¯„ä¼°å¿ƒè„åŠŸèƒ½çš„é‡‘æ ‡å‡†ï¼Œä½†ä¸ªä½“å¿ƒè„å‘¨æœŸä½¿å¾—è‡ªåŠ¨æ—¶é—´æ¯”è¾ƒæˆ–äºšé˜¶æ®µåˆ†æå¤æ‚åŒ–ã€‚å‡†ç¡®çš„å¿ƒè„å…³é”®å¸§æ£€æµ‹å¯ä»¥æ¶ˆé™¤è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè‡ªåŠ¨æ–¹æ³•ä»…ä»å·¦å¿ƒå®¤ä½“ç§¯æ›²çº¿ä¸­å¾—å‡ºæ”¶ç¼©æœ«æœŸï¼ˆESï¼‰å’Œèˆ’å¼ æœ«æœŸï¼ˆEDï¼‰å¸§ï¼Œæ— æ³•æ·±å…¥äº†è§£å¿ƒè‚Œè¿åŠ¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£æ·±åº¦å­¦ä¹ çš„æ£€æµ‹çŸ­è½´ï¼ˆSAXï¼‰äº”ä¸ªå…³é”®å¸§å’Œå››è…”é•¿è½´ï¼ˆ4CHï¼‰ç”µå½±CMRå››ä¸ªå…³é”®å¸§çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œä»å›¾åƒä¸­å¾—å‡ºå¯†é›†çš„å¯å˜å½¢æ³¨å†Œå­—æ®µï¼Œç”¨äºè®¡ç®—ä¸€ç»´è¿åŠ¨æè¿°ç¬¦ï¼Œè¯¥æè¿°ç¬¦æä¾›äº†å¯¹å…¨å±€å¿ƒè„æ”¶ç¼©å’Œèˆ’å¼ æ¨¡å¼çš„å®è´µè§è§£ã€‚ä»è¿™äº›ç‰¹å¾æ›²çº¿ä¸­ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™é›†ç¡®å®šå…³é”®å¸§ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä¸‰ç§å…¬å…±å¤šä¸­å¿ƒå¤šç–¾ç—…æ•°æ®é›†å¯¹ä¸¤ç§è§†å›¾è¿›è¡Œäº†ç‹¬ç«‹è¯„ä¼°ã€‚M&amp;Ms-2ï¼ˆn&#x3D;360ï¼‰æ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°ï¼ŒM&amp;Msï¼ˆn&#x3D;345ï¼‰å’ŒACDCï¼ˆn&#x3D;100ï¼‰æ•°æ®é›†ç”¨äºé‡å¤æ€§æ§åˆ¶ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¾·å›½ä¸“é•¿ç½‘ç»œï¼ˆGCNï¼‰æ•°æ®é›†æµ‹è¯•äº†å¯¹æ‚£æœ‰ç½•è§å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·çš„æ‚£è€…çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„è‡ªç›‘ç£æ–¹æ³•é€šè¿‡å¾ªç¯å¸§å·®å¼‚ï¼ˆcFDï¼‰æµ‹é‡ï¼Œåœ¨SAXæ–¹é¢æé«˜äº†30%~51%çš„æ£€æµ‹ç²¾åº¦ï¼Œåœ¨4CHçš„EDå’ŒESæ–¹é¢æé«˜äº†11%~47%çš„ç²¾åº¦ã€‚æˆ‘ä»¬å¯ä»¥æ£€æµ‹EDå’ŒESä»¥åŠå¿ƒè„å‘¨æœŸä¸­çš„å¦å¤–ä¸‰ä¸ªå…³é”®å¸§ï¼ŒSAXçš„å¹³å‡cFDä½äº1.31å¸§ï¼ŒLAXä½äº1.73å¸§ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸è€ƒè™‘å‘¨æœŸæˆ–ç›¸ä½é•¿åº¦çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œæ—¶é—´å’Œç©ºé—´ä¸Šå¯¹é½çš„æ‚£è€…çš„å¿ƒè„åŠ¨æ€åˆ†æã€‚GitHubä»“åº“åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git">https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05819v1">PDF</a> Main 30 pages, 6 figures</p>
<p><strong>Summary</strong><br>     å¿ƒè„ç£å…±æŒ¯æˆåƒï¼ˆCMRï¼‰æ˜¯è¯„ä¼°å¿ƒè„åŠŸèƒ½çš„é‡‘æ ‡å‡†ï¼Œä½†è‡ªåŠ¨æ—¶é—´æ¯”è¾ƒæˆ–äºšç›¸ä½åˆ†æå› ä¸ªä½“å¿ƒè„å‘¨æœŸçš„å·®å¼‚è€Œå¤æ‚åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç›‘ç£æ–¹æ³•ï¼Œå¯ä»¥æ£€æµ‹çŸ­è½´å’Œé•¿è½´å¿ƒè„ç£å…±æŒ¯æˆåƒä¸­çš„å…³é”®å¸§ï¼Œå¹¶æä¾›å¯¹å¿ƒè„æ”¶ç¼©å’Œèˆ’å¼ æ¨¡å¼çš„æ·±å…¥äº†è§£ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†ç‹¬ç«‹è¯„ä¼°ï¼Œå®ç°äº†è¾ƒé«˜çš„æ£€æµ‹ç²¾åº¦ã€‚GitHubåœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git%E3%80%82%E6%AD%A4%E7%A0%94%E7%A9%B6%E6%8E%A8%E5%8A%A8%E4%BA%86%E6%9B%B4%E7%B2%BE%E7%A1%AE%E7%9A%84CM%E5%85%A8%E5%B1%80%E8%AF%84%E4%BC%B0%E5%92%8C%E5%BF%83%E8%84%8F%E7%97%85%E6%82%A3%E8%80%85%E7%9A%84%E4%B8%AA%E4%BD%93%E5%B7%AE%E5%BC%82%E8%AF%84%E4%BC%B0%E7%A0%94%E7%A9%B6%E5%8F%91%E5%B1%95%E3%80%82%E6%AD%A4%E6%96%B9%E6%B3%95%E4%B8%BA%E5%AE%9E%E7%8E%B0%E7%B2%BE%E7%A1%AE%E7%9A%84%E8%B7%A8%E5%AD%A6%E7%A7%91%E5%BF%83%E8%A1%80%E7%AE%A1%E8%AF%8A%E6%96%AD%E5%A5%A0%E5%AE%9A%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/Cardio-AI/cmr-multi-view-phase-detection.gitã€‚æ­¤ç ”ç©¶æ¨åŠ¨äº†æ›´ç²¾ç¡®çš„CMå…¨å±€è¯„ä¼°å’Œå¿ƒè„ç—…æ‚£è€…çš„ä¸ªä½“å·®å¼‚è¯„ä¼°ç ”ç©¶å‘å±•ã€‚æ­¤æ–¹æ³•ä¸ºå®ç°ç²¾ç¡®çš„è·¨å­¦ç§‘å¿ƒè¡€ç®¡è¯Šæ–­å¥ å®šäº†åŸºç¡€ã€‚</a>  æ­¤æŠ€æœ¯æœ‰æœ›æˆä¸ºåŒ»ç–—é¢†åŸŸçš„é‡Œç¨‹ç¢‘ã€‚åŒæ—¶é€šè¿‡å¯¹å¤šç§æ•°æ®é›†çš„åº”ç”¨å’Œæµ‹è¯•è¯æ˜äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„å¯è¡Œæ€§å’Œé€‚ç”¨æ€§ã€‚è¿™é¡¹ç ”ç©¶å°†ä¿ƒè¿›æ›´å‡†ç¡®çš„å…¨çƒå¿ƒè„è¯„ä¼°å’Œä¸ªä½“å·®å¼‚è¯„ä¼°ï¼Œå¹¶æœ‰æœ›æ¨åŠ¨å¿ƒè¡€ç®¡ç–¾ç—…è¯Šç–—çš„ç²¾å‡†åŒ–ã€‚è¯¥æ–¹æ³•çš„å¼€æºå®ç°ä¹Ÿä¸ºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨æä¾›äº†ä¾¿åˆ©ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹ç ”ç©¶å°†ä¸ºæœªæ¥çš„åŒ»ç–—é¢†åŸŸå¸¦æ¥é‡è¦çš„çªç ´å’Œè¿›æ­¥ã€‚è¯¥æ–¹æ³•æé«˜äº†æ£€æµ‹å‡†ç¡®æ€§ï¼Œä¸ºè·¨é¢†åŸŸå¿ƒè¡€ç®¡ç ”ç©¶æä¾›äº†é‡è¦å·¥å…·ï¼Œå¯èƒ½æœ‰åŠ©äºæé«˜ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å…³äºå¼€æºå­˜å‚¨åº“çš„è¯¦ç»†ä¿¡æ¯å°†åœ¨æ–‡æœ«ç»™å‡ºï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºæ­¤å¼€æºä»£ç åšæ›´å¤šæ·±å…¥çš„æ¢ç´¢å’Œè¯•éªŒä»¥è§£å†³å®é™…é—®é¢˜å¹¶æ‰©å¤§å…¶åœ¨ä¸åŒé¢†åŸŸçš„å®é™…åº”ç”¨èŒƒå›´ï¼Œç›¸ä¿¡è¿™å°†å¯¹åŒ»ç–—è¡Œä¸šäº§ç”Ÿç§¯æå½±å“ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæœ¬æ–¹æ³•æ”¹è¿›äº†ç°æœ‰å¿ƒè„ç£å…±æŒ¯æˆåƒçš„è¯„ä¼°æ–¹æ³•ï¼Œä½¿æ›´ç²¾ç¡®çš„è¯„ä¼°å’Œè·¨æ‚£è€…æ¯”è¾ƒæˆä¸ºå¯èƒ½ã€‚å¯¹äºå¿ƒè„ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—å…·æœ‰é‡å¤§æ„ä¹‰ã€‚é€šè¿‡ç²¾ç¡®çš„å…³é”®å¸§æ£€æµ‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¿ƒè„çš„åŠ¨æ€è¡Œä¸ºï¼Œä»è€Œä¸ºåŒ»ç”Ÿæä¾›æ›´å‡†ç¡®çš„è¯Šæ–­ä¾æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚    åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­è¿˜å¯ä»¥æå–ç–¾ç—…è¯†åˆ«ç‰¹å¾çš„å…¨é¢è®­ç»ƒæµç¨‹æŠ¥å‘Šç ”ç©¶å·²åœ¨GitHubå¹³å°ä¸Šå…¬å¼€å‘å¸ƒå±•ç¤ºåŠç›¸å…³å‚æ•°çš„å®æ—¶è°ƒä¼˜å°†ä¼šæ”¹è¿›ä»¥è§£é‡Šæ€§ç ”ç©¶åŠŸèƒ½ä»¥åŠæ½œåœ¨çš„æœªæ¥åº”ç”¨ä¸ºåŸºå‡†æ¥æ„å»ºæ–°çš„ç®—æ³•æ¨¡å‹æ¥æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„å‘å±•å¹¶ä¿ƒè¿›è·¨å­¦ç§‘åˆä½œä»¥å¼€å‘æ›´åŠ æ™ºèƒ½é«˜æ•ˆçš„åŒ»ç–—è¯Šæ–­å·¥å…·ä»¥è§£å†³å½“å‰åŒ»å­¦é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ç–¾ç—…çš„æ—©æœŸå‘ç°é¢„æµ‹ä»¥åŠç²¾å‡†æ²»ç–—ç­‰æœ¬æ–¹æ³•å°†åœ¨æœªæ¥ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ä¸­å‘æŒ¥é‡è¦ä½œç”¨æå‡æ‚£è€…çš„ç”Ÿæ´»è´¨é‡ä»¥åŠç–¾ç—…çš„æ²»æ„ˆç‡é’ˆå¯¹å…ˆå¤©æ€§å¿ƒè„ç—…ç­‰ç–¾ç—…ä¹Ÿå°†å®ç°ç²¾å‡†è¯Šæ–­å’Œä¸ªä½“åŒ–æ²»ç–—æä¾›æ›´æœ‰æ•ˆçš„è¯Šæ–­æ–¹æ³•å’Œæ”¹å–„æ²»ç–—ç»“å±€çš„æ‰‹æ®µå¯ä»¥å¤§å¤§æé«˜åŒ»ç–—ä¿å¥çš„è´¨é‡æ¨åŠ¨å…¨çƒåŒ»ç–—å¥åº·äº‹ä¸šçš„å‘å±•å’Œè¿›æ­¥è®©äººç±»å¥åº·äº‹ä¸šå¾—ä»¥æ›´è¿›ä¸€æ­¥æ¨åŠ¨ç²¾ç»†åŒ–åŒ»ç–—çš„æ™®åŠå’Œå‘å±•æå‡å…¨çƒåŒ»ç–—æ°´å¹³ä¸ºåŒ»å­¦é¢†åŸŸå¸¦æ¥é©å‘½æ€§çš„å˜é©ä¸ºåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„å‘å±•æ³¨å…¥æ–°çš„æ´»åŠ›ã€‚<strong>Key Takeaways</strong>ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2878b91c1530a9a5fa553d713dcb30d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d6e21fa4eb32e6c165db70cd793a9d3" align="middle">
<img src="https://picx.zhimg.com/v2-0c5380c2ab032a4dc15c533b2d0386ee" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Modulated-INR-with-Prior-Embeddings-for-Ultrasound-Imaging-Reconstruction"><a href="#Modulated-INR-with-Prior-Embeddings-for-Ultrasound-Imaging-Reconstruction" class="headerlink" title="Modulated INR with Prior Embeddings for Ultrasound Imaging   Reconstruction"></a>Modulated INR with Prior Embeddings for Ultrasound Imaging   Reconstruction</h2><p><strong>Authors:RÃ©mi Delaunay, Christoph Hennersperger, Stefan WÃ¶rz</strong></p>
<p>Ultrafast ultrasound imaging enables visualization of rapid physiological dynamics by acquiring data at exceptionally high frame rates. However, this speed often comes at the cost of spatial resolution and image quality due to unfocused wave transmissions and associated artifacts. In this work, we propose a novel modulated Implicit Neural Representation (INR) framework that leverages a coordinate-based neural network conditioned on latent embeddings extracted from time-delayed I&#x2F;Q channel data for high-quality ultrasound image reconstruction. Our method integrates complex Gabor wavelet activation and a conditioner network to capture the oscillatory and phase-sensitive nature of I&#x2F;Q ultrasound signals. We evaluate the framework on an in vivo intracardiac echocardiography (ICE) dataset and demonstrate that it outperforms the compared state-of-the-art methods. We believe these findings not only highlight the advantages of INR-based modeling for ultrasound image reconstruction, but also point to broader opportunities for applying INR frameworks across other medical imaging modalities. </p>
<blockquote>
<p>è¶…é«˜é€Ÿè¶…å£°æˆåƒèƒ½å¤Ÿé€šè¿‡ä»¥æé«˜å¸§ç‡è·å–æ•°æ®ï¼Œå®ç°å¿«é€Ÿç”Ÿç†åŠ¨æ€çš„å¯è§†åŒ–ã€‚ç„¶è€Œï¼Œç”±äºæ³¢çš„ä¸èšç„¦ä¼ è¾“å’Œç›¸å…³ä¼ªå½±ï¼Œè¿™ç§é€Ÿåº¦å¾€å¾€ä¼šç‰ºç‰²ç©ºé—´åˆ†è¾¨ç‡å’Œå›¾åƒè´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è°ƒåˆ¶éšå¼ç¥ç»ç½‘ç»œï¼ˆINRï¼‰æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨åŸºäºåæ ‡çš„ç¥ç»ç½‘ç»œå¯¹ä»æ—¶å»¶I&#x2F;Qé€šé“æ•°æ®ä¸­æå–çš„æ½œåœ¨åµŒå…¥è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»¥å®ç°é«˜è´¨é‡çš„è¶…å£°å›¾åƒé‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¤æ‚çš„Gaborå°æ³¢æ¿€æ´»å’Œæ¡ä»¶ç½‘ç»œï¼Œä»¥æ•æ‰I&#x2F;Qè¶…å£°ä¿¡å·çš„æŒ¯è¡å’Œç›¸ä½æ•æ„Ÿç‰¹æ€§ã€‚æˆ‘ä»¬åœ¨ä½“å†…å¿ƒå†…è¶…å£°å¿ƒåŠ¨å›¾ï¼ˆICEï¼‰æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ¡†æ¶ï¼Œå¹¶è¯æ˜å…¶ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å¯¹æ¯”æ–¹æ³•ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™äº›å‘ç°ä¸ä»…çªå‡ºäº†åŸºäºINRå»ºæ¨¡åœ¨è¶…å£°å›¾åƒé‡å»ºæ–¹é¢çš„ä¼˜åŠ¿ï¼Œè¿˜æŒ‡å‡ºäº†å°†INRæ¡†æ¶åº”ç”¨äºå…¶ä»–åŒ»å­¦æˆåƒæ¨¡å¼çš„æ›´å¹¿é˜”æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05731v1">PDF</a> Accepted to International Workshop on Advances in Simplifying Medical   Ultrasound (ASMUS 2025)</p>
<p><strong>Summary</strong><br>é«˜é€Ÿè¶…å£°æˆåƒå¯å®ç°å¿«é€Ÿç”Ÿç†åŠ¨æ€çš„å¯è§†åŒ–ï¼Œä½†å…¶é«˜å¸§ç‡å¸¦æ¥çš„ç©ºé—´åˆ†è¾¨ç‡å’Œå›¾åƒè´¨é‡æŸå¤±é—®é¢˜äºŸå¾…è§£å†³ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹è°ƒåˆ¶éšå¼ç¥ç»ç½‘ç»œï¼ˆINRï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºåæ ‡çš„ç¥ç»ç½‘ç»œå¤„ç†å»¶è¿Ÿçš„I&#x2F;Qé€šé“æ•°æ®ä»¥è¿›è¡Œé«˜è´¨é‡è¶…å£°å›¾åƒé‡å»ºã€‚è¯¥ç ”ç©¶é›†æˆäº†å¤æ‚Gaborå°æ³¢æ¿€æ´»å™¨å’Œæ¡ä»¶ç½‘ç»œï¼Œä»¥æ•æ‰I&#x2F;Qè¶…å£°ä¿¡å·çš„æŒ¯è¡å’Œç›¸ä½æ•æ„Ÿæ€§ã€‚åœ¨æ´»ä½“å¿ƒè„å†…è¶…å£°ï¼ˆICEï¼‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¿™äº›å‘ç°ä¸ä»…çªå‡ºäº†INRæ¨¡å‹åœ¨è¶…å£°å›¾åƒé‡å»ºä¸­çš„ä¼˜åŠ¿ï¼Œä¹Ÿæš—ç¤ºäº†INRæ¡†æ¶åœ¨å…¶ä»–åŒ»å­¦æˆåƒæ¨¡æ€ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ä½¿ç”¨éšå¼ç¥ç»ç½‘ç»œï¼ˆINRï¼‰æ¡†æ¶è¿›è¡Œé«˜è´¨é‡è¶…å£°å›¾åƒé‡å»ºã€‚</li>
<li>INRæ¡†æ¶ç»“åˆäº†åæ ‡ç¥ç»ç½‘ç»œå’Œå»¶è¿Ÿçš„I&#x2F;Qé€šé“æ•°æ®ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨äº†å¤æ‚Gaborå°æ³¢æ¿€æ´»å™¨å’Œæ¡ä»¶ç½‘ç»œæ¥æ•æ‰è¶…å£°ä¿¡å·çš„æŒ¯è¡å’Œç›¸ä½æ•æ„Ÿæ€§ã€‚</li>
<li>åœ¨æ´»ä½“å¿ƒè„å†…è¶…å£°ï¼ˆICEï¼‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æ­¤ç ”ç©¶å±•ç¤ºäº†INRæ¨¡å‹åœ¨è¶…å£°å›¾åƒé‡å»ºä¸­çš„ä¼˜åŠ¿ã€‚</li>
<li>è°ƒåˆ¶éšå¼ç¥ç»ç½‘ç»œï¼ˆINRï¼‰æ¡†æ¶å¯åº”ç”¨äºå…¶ä»–åŒ»å­¦æˆåƒæ¨¡æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9947eba00df9ea2a2d8f25a3025d7f5" align="middle">
<img src="https://picx.zhimg.com/v2-99bdcb9139afd260ebd36413ef5767e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cc233457c40edcc5b0c472eb7dfc8f3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Hierarchical-Geometry-guided-Transformer-for-Histological-Subtyping-of-Primary-Liver-Cancer"><a href="#A-Hierarchical-Geometry-guided-Transformer-for-Histological-Subtyping-of-Primary-Liver-Cancer" class="headerlink" title="A Hierarchical Geometry-guided Transformer for Histological Subtyping of   Primary Liver Cancer"></a>A Hierarchical Geometry-guided Transformer for Histological Subtyping of   Primary Liver Cancer</h2><p><strong>Authors:Anwen Lu, Mingxin Liu, Yiping Jiao, Hongyi Gong, Geyang Xu, Jun Chen, Jun Xu</strong></p>
<p>Primary liver malignancies are widely recognized as the most heterogeneous and prognostically diverse cancers of the digestive system. Among these, hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (ICC) emerge as the two principal histological subtypes, demonstrating significantly greater complexity in tissue morphology and cellular architecture than other common tumors. The intricate representation of features in Whole Slide Images (WSIs) encompasses abundant crucial information for liver cancer histological subtyping, regarding hierarchical pyramid structure, tumor microenvironment (TME), and geometric representation. However, recent approaches have not adequately exploited these indispensable effective descriptors, resulting in a limited understanding of histological representation and suboptimal subtyping performance. To mitigate these limitations, ARGUS is proposed to advance histological subtyping in liver cancer by capturing the macro-meso-micro hierarchical information within the TME. Specifically, we first construct a micro-geometry feature to represent fine-grained cell-level pattern via a geometric structure across nuclei, thereby providing a more refined and precise perspective for delineating pathological images. Then, a Hierarchical Field-of-Views (FoVs) Alignment module is designed to model macro- and meso-level hierarchical interactions inherent in WSIs. Finally, the augmented micro-geometry and FoVs features are fused into a joint representation via present Geometry Prior Guided Fusion strategy for modeling holistic phenotype interactions. Extensive experiments on public and private cohorts demonstrate that our ARGUS achieves state-of-the-art (SOTA) performance in histological subtyping of liver cancer, which provide an effective diagnostic tool for primary liver malignancies in clinical practice. </p>
<blockquote>
<p>åŸå‘æ€§è‚è„æ¶æ€§è‚¿ç˜¤è¢«å…¬è®¤ä¸ºæ˜¯æ¶ˆåŒ–ç³»ç»Ÿå†…æœ€å…·æœ‰å¼‚è´¨æ€§å’Œé¢„åå¤šæ ·æ€§çš„ç™Œç—‡ã€‚å…¶ä¸­ï¼Œè‚ç»†èƒç™Œï¼ˆHCCï¼‰å’Œè‚å†…èƒ†ç®¡ç™Œï¼ˆICCï¼‰æ˜¯ä¸¤ç§ä¸»è¦çš„ç»„ç»‡äºšå‹ï¼Œå®ƒä»¬åœ¨ç»„ç»‡å½¢æ€å’Œç»†èƒç»“æ„æ–¹é¢æ¯”å…¶ä»–å¸¸è§è‚¿ç˜¤è¡¨ç°å‡ºæ›´å¤§çš„å¤æ‚æ€§ã€‚å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰ä¸­ç‰¹å¾çš„å¤æ‚è¡¨ç¤ºåŒ…å«äº†å…³äºè‚è„ç™Œç—‡ç»„ç»‡äºšå‹çš„å±‚æ¬¡é‡‘å­—å¡”ç»“æ„ã€è‚¿ç˜¤å¾®ç¯å¢ƒï¼ˆTMEï¼‰å’Œå‡ ä½•è¡¨ç¤ºçš„å¤§é‡å…³é”®ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„æ–¹æ³•å¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨è¿™äº›ä¸å¯æˆ–ç¼ºçš„æœ‰æ•ˆæè¿°ç¬¦ï¼Œå¯¼è‡´å¯¹ç»„ç»‡è¡¨å¾çš„ç†è§£æœ‰é™ï¼Œä»¥åŠäºšå‹åˆ†ç±»æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é™åˆ¶ï¼ŒARGUSè¢«æå‡ºæ¥é€šè¿‡æ•æ‰è‚¿ç˜¤å¾®ç¯å¢ƒä¸­å®è§‚åˆ°å¾®è§‚çš„å±‚æ¬¡ä¿¡æ¯ï¼Œæ¨è¿›è‚è„ç™Œç—‡çš„ç»„ç»‡äºšå‹åˆ†ç±»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºä¸€ä¸ªå¾®è§‚å‡ ä½•ç‰¹å¾æ¥è¡¨ç¤ºç»†èƒæ ¸ä¹‹é—´ç²¾ç»†çš„ç»†èƒçº§æ¨¡å¼ï¼Œä»è€Œä¸ºç—…ç†å›¾åƒåˆ†å‰²æä¾›ä¸€ä¸ªæ›´ç²¾ç»†å’Œç²¾ç¡®çš„è§’åº¦ã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚è§†é‡ï¼ˆFoVsï¼‰å¯¹é½æ¨¡å—ï¼Œä»¥æ¨¡æ‹Ÿå…¨å¹»ç¯ç‰‡å›¾åƒä¸­å›ºæœ‰çš„å®è§‚å’Œä¸­é—´å±‚æ¬¡çš„äº¤äº’ä½œç”¨ã€‚æœ€åï¼Œå¢å¼ºå‹å¾®è§‚å‡ ä½•å’Œè§†é‡ç‰¹å¾é€šè¿‡å½“å‰çš„å‡ ä½•å…ˆéªŒå¼•å¯¼èåˆç­–ç•¥ï¼Œè¢«èåˆæˆä¸€ä¸ªè”åˆè¡¨ç¤ºï¼Œä»¥æ¨¡æ‹Ÿæ•´ä½“è¡¨å‹äº¤äº’ã€‚åœ¨å…¬å…±å’Œç§äººé˜Ÿåˆ—çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ARGUSåœ¨è‚è„ç™Œç—‡çš„ç»„ç»‡äºšå‹åˆ†ç±»ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºä¸´åºŠå®è·µä¸­çš„åŸå‘æ€§è‚è„æ¶æ€§è‚¿ç˜¤æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è¯Šæ–­å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05657v1">PDF</a> 7 pages, 2 figures, accepted by IEEE BIBM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‚ç™Œä¸­æœ€å…·å¼‚è´¨æ€§å’Œé¢„åå¤šæ ·æ€§çš„ä¸¤ç§ä¸»è¦ç»„ç»‡äºšå‹â€”â€”è‚ç»†èƒç™Œå’Œè‚å†…èƒ†ç®¡ç™Œã€‚æ–‡ç« æŒ‡å‡ºå…¨å¹»ç¯ç‰‡å›¾åƒä¸­åŒ…å«ä¸°å¯Œçš„å…³é”®ä¿¡æ¯ï¼Œå¯¹äºè‚ç™Œç»„ç»‡äºšå‹åˆ†ç±»å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è¿™äº›æœ‰æ•ˆçš„æè¿°ç¬¦å·ï¼Œå¯¼è‡´ç»„ç»‡è¡¨å¾ç†è§£æœ‰é™å’Œåˆ†å‹æ€§èƒ½ä¸ä½³ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ARGUSæ–¹æ³•ï¼Œé€šè¿‡æ•æ‰è‚¿ç˜¤å¾®ç¯å¢ƒä¸­å®è§‚ã€ä¸­è§‚å’Œå¾®è§‚çš„å±‚æ¬¡ä¿¡æ¯ï¼Œæ”¹è¿›è‚ç™Œç»„ç»‡åˆ†å‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‚ç™Œç»„ç»‡åˆ†å‹ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä¸ºä¸´åºŠå®è·µä¸­çš„åŸå‘æ€§è‚ç™Œè¯Šæ–­æä¾›äº†æœ‰æ•ˆçš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ç™Œå…·æœ‰é«˜åº¦çš„å¼‚è´¨æ€§å’Œé¢„åå¤šæ ·æ€§ï¼Œå…¶ä¸­è‚ç»†èƒç™Œå’Œè‚å†…èƒ†ç®¡ç™Œæ˜¯ä¸»è¦äºšå‹ã€‚</li>
<li>å…¨å¹»ç¯ç‰‡å›¾åƒåŒ…å«ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå¯¹äºè‚ç™Œç»„ç»‡äºšå‹åˆ†ç±»è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æè¿°ç¬¦å·ï¼Œå¯¼è‡´ç»„ç»‡è¡¨å¾ç†è§£å’Œåˆ†å‹æ€§èƒ½æœ‰é™ã€‚</li>
<li>ARGUSæ–¹æ³•é€šè¿‡æ•æ‰è‚¿ç˜¤å¾®ç¯å¢ƒä¸­çš„å®è§‚ã€ä¸­è§‚å’Œå¾®è§‚å±‚æ¬¡ä¿¡æ¯ï¼Œæ”¹è¿›è‚ç™Œç»„ç»‡åˆ†å‹ã€‚</li>
<li>ARGUSæ–¹æ³•åŒ…æ‹¬æ„å»ºå¾®è§‚å‡ ä½•ç‰¹å¾ã€è®¾è®¡è§†é‡å¯¹é½æ¨¡å—ä»¥åŠèåˆç‰¹å¾çš„ç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒARGUSæ–¹æ³•åœ¨è‚ç™Œç»„ç»‡åˆ†å‹ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9e3a95c307d638abe7afd7ec090f202" align="middle">
<img src="https://picx.zhimg.com/v2-f1fbc0eb0428fec26ef4231f431dc29b" align="middle">
<img src="https://picx.zhimg.com/v2-17e2a455465badb64802ed65c03667e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e9e25fb6ab8cc4185501c03bfa5057b" align="middle">
<img src="https://picx.zhimg.com/v2-bb6236f2135dafc012346138bb689ece" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TFM-Dataset-A-Novel-Multi-task-Dataset-and-Integrated-Pipeline-for-Automated-Tear-Film-Break-Up-Segmentation"><a href="#TFM-Dataset-A-Novel-Multi-task-Dataset-and-Integrated-Pipeline-for-Automated-Tear-Film-Break-Up-Segmentation" class="headerlink" title="TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for   Automated Tear Film Break-Up Segmentation"></a>TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for   Automated Tear Film Break-Up Segmentation</h2><p><strong>Authors:Guangrong Wan, Jun liu, Tang tang, Lianghao Shi, Wenjun Luo, TingTing Xu</strong></p>
<p>Tear film break-up (TFBU) analysis is critical for diagnosing dry eye syndrome, but automated TFBU segmentation remains challenging due to the lack of annotated datasets and integrated solutions. This paper introduces the Tear Film Multi-task (TFM) Dataset, the first comprehensive dataset for multi-task tear film analysis, comprising 15 high-resolution videos (totaling 6,247 frames) annotated with three vision tasks: frame-level classification (â€˜clearâ€™, â€˜closedâ€™, â€˜brokenâ€™, â€˜blurâ€™), Placido Ring detection, and pixel-wise TFBU area segmentation. Leveraging this dataset, we first propose TF-Net, a novel and efficient baseline segmentation model. TF-Net incorporates a MobileOne-mini backbone with re-parameterization techniques and an enhanced feature pyramid network to achieve a favorable balance between accuracy and computational efficiency for real-time clinical applications. We further establish benchmark performance on the TFM segmentation subset by comparing TF-Net against several state-of-the-art medical image segmentation models. Furthermore, we design TF-Collab, a novel integrated real-time pipeline that synergistically leverages models trained on all three tasks of the TFM dataset. By sequentially orchestrating frame classification for BUT determination, pupil region localization for input standardization, and TFBU segmentation, TF-Collab fully automates the analysis. Experimental results demonstrate the effectiveness of the proposed TF-Net and TF-Collab, providing a foundation for future research in ocular surface diagnostics. Our code and the TFM datasets are available at <a target="_blank" rel="noopener" href="https://github.com/glory-wan/TF-Net">https://github.com/glory-wan/TF-Net</a> </p>
<blockquote>
<p>æ³ªè†œç ´è£‚åˆ†æï¼ˆTFBUï¼‰å¯¹äºå¹²çœ¼ç»¼åˆå¾çš„è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ç”±äºç¼ºä¹æ ‡æ³¨æ•°æ®é›†å’Œé›†æˆè§£å†³æ–¹æ¡ˆï¼Œè‡ªåŠ¨åŒ–TFBUåˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†æ³ªè†œå¤šä»»åŠ¡ï¼ˆTFMï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯ç”¨äºå¤šä»»åŠ¡æ³ªè†œåˆ†æçš„é¦–ä¸ªç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«15ä¸ªé«˜æ¸…è§†é¢‘ï¼ˆæ€»è®¡6247å¸§ï¼‰ï¼Œæ ‡æ³¨äº†ä¸‰ä¸ªè§†è§‰ä»»åŠ¡ï¼šå¸§çº§åˆ«åˆ†ç±»ï¼ˆæ¸…æ™°ã€é—­åˆã€ç ´è£‚ã€æ¨¡ç³Šï¼‰ã€Placidoç¯æ£€æµ‹å’Œåƒç´ çº§TFBUåŒºåŸŸåˆ†å‰²ã€‚åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†TF-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„åˆ†å‰²åŸºçº¿æ¨¡å‹ã€‚TF-Neté‡‡ç”¨MobileOne-miniéª¨å¹²ç½‘ä¸é‡æ–°å‚æ•°åŒ–æŠ€æœ¯ï¼Œå¹¶å¢å¼ºç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼Œä»¥å®ç°å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡çš„å¹³è¡¡ï¼Œé€‚ç”¨äºå®æ—¶ä¸´åºŠåº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†TF-Netä¸å‡ ç§æœ€å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œåœ¨TFMåˆ†å‰²å­é›†ä¸Šå»ºç«‹äº†åŸºå‡†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†TF-Collabï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ä¸€ä½“åŒ–å®æ—¶ç®¡é“ï¼ŒååŒåˆ©ç”¨åœ¨TFMæ•°æ®é›†æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚é€šè¿‡æŒ‰é¡ºåºåè°ƒå¸§åˆ†ç±»ä»¥æµ‹å®šBUTã€ç³å­”åŒºåŸŸå®šä½ä»¥å®ç°è¾“å…¥æ ‡å‡†åŒ–å’ŒTFBUåˆ†å‰²ï¼ŒTF-Collabå®ç°äº†å…¨è‡ªåŠ¨åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜TF-Netå’ŒTF-Collabçš„æœ‰æ•ˆæ€§ï¼Œä¸ºçœ¼è¡¨è¯Šæ–­çš„æœªæ¥ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç å’ŒTFMæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/glory-wan/TF-Net%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/glory-wan/TF-Netè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05615v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¹²çœ¼ç»¼åˆå¾è¯Šæ–­ä¸­çš„æ³ªè†œç ´è£‚åˆ†æçš„é‡è¦æ€§ï¼Œä»¥åŠç°æœ‰è‡ªåŠ¨åŒ–TFBUåˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†Tear Film Multi-taskï¼ˆTFMï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºå¤šä»»åŠ¡æ³ªè†œåˆ†æçš„ç»¼åˆæ•°æ®é›†ã€‚åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œæå‡ºäº†TF-Netï¼Œä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„åˆ†å‰²æ¨¡å‹ï¼Œç”¨äºå®æ—¶ä¸´åºŠåº”ç”¨ã€‚æ­¤å¤–ï¼Œè¿˜å»ºç«‹äº†TF-Collabï¼Œä¸€ä¸ªå…¨æ–°çš„å®æ—¶é›†æˆæµç¨‹ï¼Œé€šè¿‡ååŒåˆ©ç”¨åœ¨TFMæ•°æ®é›†ä¸Šè®­ç»ƒçš„ä¸‰ä¸ªä»»åŠ¡æ¨¡å‹ï¼Œå®ç°å…¨è‡ªåŠ¨åŒ–åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tear Film Break-up (TFBU) åˆ†æå¯¹å¹²çœ¼ç»¼åˆå¾è¯Šæ–­è‡³å…³é‡è¦ã€‚</li>
<li>è‡ªåŠ¨åŒ–TFBUåˆ†å‰²é¢ä¸´ç¼ºä¹æ ‡æ³¨æ•°æ®é›†å’Œç»¼åˆè§£å†³æ–¹æ¡ˆçš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥Tear Film Multi-task (TFM) æ•°æ®é›†ï¼ŒåŒ…å«é«˜åˆ†è¾¨ç‡è§†é¢‘å’Œå¤šç§è§†è§‰ä»»åŠ¡æ ‡æ³¨ã€‚</li>
<li>æå‡ºTF-Netæ¨¡å‹ï¼Œç»“åˆMobileOne-mini backboneå’Œé‡å‚æ•°åŒ–æŠ€æœ¯ï¼Œå®ç°å‡†ç¡®å’Œé«˜æ•ˆçš„åˆ†å‰²ã€‚</li>
<li>TF-Netåœ¨TFMåˆ†å‰²å­é›†ä¸Šå»ºç«‹äº†åŸºå‡†æ€§èƒ½ã€‚</li>
<li>è®¾è®¡TF-Collabï¼Œä¸€ä¸ªå…¨æ–°çš„å®æ—¶é›†æˆæµç¨‹ï¼Œé€šè¿‡ååŒä¸‰ä¸ªä»»åŠ¡æ¨¡å‹å®ç°å…¨è‡ªåŠ¨åŒ–åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18c61b46f71f7dae6c313e0a303350ef" align="middle">
<img src="https://picx.zhimg.com/v2-45f18c3a2836b05e44aed922f31236bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b60f84ef2d07c0c0724c6055654c7847" align="middle">
<img src="https://picx.zhimg.com/v2-3b920b779ce1c14250dc3e6a9bc0229b" align="middle">
<img src="https://picx.zhimg.com/v2-5c8b505a2f5b3a59bb2983809f628f3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b08565542fdbe064d72ddf3b4de8999" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="nnSAM2-nnUNet-Enhanced-One-Prompt-SAM2-for-Few-shot-Multi-Modality-Segmentation-and-Composition-Analysis-of-Lumbar-Paraspinal-Muscles"><a href="#nnSAM2-nnUNet-Enhanced-One-Prompt-SAM2-for-Few-shot-Multi-Modality-Segmentation-and-Composition-Analysis-of-Lumbar-Paraspinal-Muscles" class="headerlink" title="nnSAM2: nnUNet-Enhanced One-Prompt SAM2 for Few-shot Multi-Modality   Segmentation and Composition Analysis of Lumbar Paraspinal Muscles"></a>nnSAM2: nnUNet-Enhanced One-Prompt SAM2 for Few-shot Multi-Modality   Segmentation and Composition Analysis of Lumbar Paraspinal Muscles</h2><p><strong>Authors:Zhongyi Zhang, Julie A. Hides, Enrico De Martino, Abdul Joseph Fofanah, Gervase Tuxworth</strong></p>
<p>Purpose: To develop and validate No-New SAM2 (nnsam2) for few-shot segmentation of lumbar paraspinal muscles using only a single annotated slice per dataset, and to assess its statistical comparability with expert measurements across multi-sequence MRI and multi-protocol CT.   Methods: We retrospectively analyzed 1,219 scans (19,439 slices) from 762 participants across six datasets. Six slices (one per dataset) served as labeled examples, while the remaining 19,433 slices were used for testing. In this minimal-supervision setting, nnsam2 used single-slice SAM2 prompts to generate pseudo-labels, which were pooled across datasets and refined through three sequential, independent nnU-Net models. Segmentation performance was evaluated using the Dice similarity coefficient (DSC), and automated measurements-including muscle volume, fat ratio, and CT attenuation-were assessed with two one-sided tests (TOST) and intraclass correlation coefficients (ICC).   Results: nnsam2 outperformed vanilla SAM2, its medical variants, TotalSegmentator, and the leading few-shot method, achieving DSCs of 0.94-0.96 on MR images and 0.92-0.93 on CT. Automated and expert measurements were statistically equivalent for muscle volume (MRI&#x2F;CT), CT attenuation, and Dixon fat ratio (TOST, P &lt; 0.05), with consistently high ICCs (0.86-1.00).   Conclusion: We developed nnsam2, a state-of-the-art few-shot framework for multi-modality LPM segmentation, producing muscle volume (MRI&#x2F;CT), attenuation (CT), and fat ratio (Dixon MRI) measurements that were statistically comparable to expert references. Validated across multimodal, multicenter, and multinational cohorts, and released with open code and data, nnsam2 demonstrated high annotation efficiency, robust generalizability, and reproducibility. </p>
<blockquote>
<p>ç›®çš„ï¼šå¼€å‘å¹¶éªŒè¯No-New SAM2ï¼ˆnnsam2ï¼‰æ–¹æ³•ï¼Œç”¨äºä»…ä½¿ç”¨æ¯ä¸ªæ•°æ®é›†çš„ä¸€ä¸ªå·²æ ‡æ³¨åˆ‡ç‰‡å¯¹è…°æ¤æ—è‚Œè‚‰è¿›è¡Œå°æ ·æœ¬åˆ†å‰²ï¼Œå¹¶è¯„ä¼°å…¶åœ¨å¤šåºåˆ—MRIå’Œå¤šåè®®CTä¸‹ä¸ä¸“å®¶æµ‹é‡çš„ç»Ÿè®¡å¯æ¯”æ€§ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬å›é¡¾æ€§åˆ†æäº†æ¥è‡ªå…­ä¸ªæ•°æ®é›†çš„762åå‚ä¸è€…çš„1,219æ¬¡æ‰«æï¼ˆå…±19,439å¼ åˆ‡ç‰‡ï¼‰ã€‚å…¶ä¸­å…­å¼ åˆ‡ç‰‡ï¼ˆæ¯å¼ æ•°æ®é›†ä¸€å¼ ï¼‰ä½œä¸ºæ ‡æ³¨ç¤ºä¾‹ï¼Œå…¶ä½™19,433å¼ åˆ‡ç‰‡ç”¨äºæµ‹è¯•ã€‚åœ¨è¿™ç§æœ€å°ç›‘ç£è®¾ç½®ä¸‹ï¼Œnnsam2ä½¿ç”¨å•åˆ‡ç‰‡SAM2æç¤ºç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè¿™äº›ä¼ªæ ‡ç­¾åœ¨æ•°æ®é›†ä¹‹é—´è¿›è¡Œæ±‡æ€»ï¼Œå¹¶é€šè¿‡ä¸‰ä¸ªç‹¬ç«‹ä¸”é¡ºåºçš„nnU-Netæ¨¡å‹è¿›è¡Œæ”¹è¿›ã€‚ä½¿ç”¨Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰è¯„ä¼°åˆ†å‰²æ€§èƒ½ï¼Œå¹¶ä½¿ç”¨ä¸¤ä¸ªå•ä¾§æ£€éªŒï¼ˆTOSTï¼‰å’Œç»„å†…ç›¸å…³ç³»æ•°ï¼ˆICCï¼‰è¯„ä¼°è‡ªåŠ¨åŒ–æµ‹é‡ï¼ŒåŒ…æ‹¬è‚Œè‚‰ä½“ç§¯ã€è„‚è‚ªæ¯”å’ŒCTè¡°å‡ã€‚ç»“æœï¼šnnsam2çš„è¡¨ç°ä¼˜äºæ™®é€šSAM2ã€å…¶åŒ»å­¦å˜ä½“ã€TotalSegmentatorä»¥åŠä¸»æµçš„å°‘æ ·æœ¬æ–¹æ³•ï¼Œåœ¨MRå›¾åƒä¸Šè¾¾åˆ°DSC 0.94-0.96ï¼Œåœ¨CTä¸Šè¾¾åˆ°DSC 0.92-0.93ã€‚è‡ªåŠ¨åŒ–å’Œä¸“å®¶æµ‹é‡çš„è‚Œè‚‰ä½“ç§¯ï¼ˆMRI&#x2F;CTï¼‰ã€CTè¡°å‡å’ŒDixonè„‚è‚ªæ¯”ï¼ˆTOSTï¼ŒP &lt; 0.05ï¼‰åœ¨ç»Ÿè®¡ä¸Šæ˜¯ç­‰æ•ˆçš„ï¼Œä¸”ICCå§‹ç»ˆå¾ˆé«˜ï¼ˆ0.86-1.00ï¼‰ã€‚ç»“è®ºï¼šæˆ‘ä»¬å¼€å‘äº†å…ˆè¿›çš„å°‘æ ·æœ¬å¤šæ¨¡æ€LPMåˆ†å‰²æ¡†æ¶nnsam2ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸ä¸“å®¶å‚è€ƒç»Ÿè®¡ç›¸å½“çš„è‚Œè‚‰ä½“ç§¯ï¼ˆMRI&#x2F;CTï¼‰ã€è¡°å‡ï¼ˆCTï¼‰å’Œè„‚è‚ªæ¯”ï¼ˆDixon MRIï¼‰æµ‹é‡å€¼ã€‚ç»è¿‡å¤šæ¨¡æ€ã€å¤šä¸­å¿ƒå’Œè·¨å›½é˜Ÿåˆ—çš„éªŒè¯ï¼Œå¹¶ä»¥å¼€æ”¾ä»£ç å’Œæ•°æ®å‘å¸ƒï¼Œnnsam2è¡¨ç°å‡ºé«˜æ ‡æ³¨æ•ˆç‡ã€ç¨³å¥çš„é€šç”¨æ€§å’Œå¯é‡å¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05555v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå•ä¸€æ ‡æ³¨åˆ‡ç‰‡å¼€å‘å¹¶éªŒè¯äº†No-New SAM2ï¼ˆnnsam2ï¼‰ç”¨äºè…°æ¤æ—è‚Œè‚‰ç»„ç»‡çš„å°‘æ ·æœ¬åˆ†å‰²æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨å¤šåºåˆ—MRIå’Œå¤šåè®®CTä¸Šå®ç°ä¸ä¸“å®¶æµ‹é‡ç»“æœçš„ç»Ÿè®¡å¯æ¯”æ€§ã€‚é€šè¿‡å¯¹æ¥è‡ªä¸åŒæ•°æ®é›†çš„å¤§é‡æ‰«ææ•°æ®è¿›è¡Œå›é¡¾æ€§åˆ†æå’Œæ¨¡å‹è®­ç»ƒï¼ŒéªŒè¯äº†nnsam2çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>nnsam2è¢«å¼€å‘ç”¨äºè…°æ¤æ—è‚Œè‚‰ç»„ç»‡çš„å°‘æ ·æœ¬åˆ†å‰²ï¼Œä»…éœ€è¦ä¸€ä¸ªæ ‡æ³¨åˆ‡ç‰‡å³å¯è¿›è¡Œè®­ç»ƒã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šåºåˆ—MRIå’Œå¤šåè®®CTä¸Šå®ç°äº†ä¸ä¸“å®¶æµ‹é‡ç»“æœçš„ç»Ÿè®¡å¯æ¯”æ€§ã€‚</li>
<li>é€šè¿‡åˆ†æå¤§é‡æ‰«ææ•°æ®éªŒè¯äº†nnsam2çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚</li>
<li>nnsam2ä½¿ç”¨å•ä¸€åˆ‡ç‰‡SAM2æç¤ºç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶é€šè¿‡ä¸‰ä¸ªç‹¬ç«‹çš„nnU-Netæ¨¡å‹è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚</li>
<li>åˆ†å‰²æ€§èƒ½é€šè¿‡Diceç›¸ä¼¼ç³»æ•°è¿›è¡Œè¯„ä¼°ï¼Œå®ç°äº†é«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>è‡ªåŠ¨æµ‹é‡ç»“æœä¸ä¸“å®¶æµ‹é‡ç»“æœç›¸æ¯”å…·æœ‰ç»Ÿè®¡å­¦ç­‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è‚Œè‚‰ä½“ç§¯ã€è„‚è‚ªæ¯”å’ŒCTè¡°å‡ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e3d78d43314b93194234bf3cb4c9483.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Comparing-LSTM-Based-Sequence-to-Sequence-Forecasting-Strategies-for-24-Hour-Solar-Proton-Flux-Profiles-Using-GOES-Data"><a href="#Comparing-LSTM-Based-Sequence-to-Sequence-Forecasting-Strategies-for-24-Hour-Solar-Proton-Flux-Profiles-Using-GOES-Data" class="headerlink" title="Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for   24-Hour Solar Proton Flux Profiles Using GOES Data"></a>Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for   24-Hour Solar Proton Flux Profiles Using GOES Data</h2><p><strong>Authors:Kangwoo Yi, Bo Shen, Qin Li, Haimin Wang, Yong-Jae Moon, Jaewon Lee, Hwanhee Lee</strong></p>
<p>Solar Proton Events (SPEs) cause significant radiation hazards to satellites, astronauts, and technological systems. Accurate forecasting of their proton flux time profiles is crucial for early warnings and mitigation. This paper explores deep learning sequence-to-sequence (seq2seq) models based on Long Short-Term Memory networks to predict 24-hour proton flux profiles following SPE onsets. We used a dataset of 40 well-connected SPEs (1997-2017) observed by NOAA GOES, each associated with a &gt;&#x3D;M-class western-hemisphere solar flare and undisturbed proton flux profiles. Using 4-fold stratified cross-validation, we evaluate seq2seq model configurations (varying hidden units and embedding dimensions) under multiple forecasting scenarios: (i) proton-only input vs. combined proton+X-ray input, (ii) original flux data vs. trend-smoothed data, and (iii) autoregressive vs. one-shot forecasting. Our major results are as follows: First, one-shot forecasting consistently yields lower error than autoregressive prediction, avoiding the error accumulation seen in iterative approaches. Second, on the original data, proton-only models outperform proton+X-ray models. However, with trend-smoothed data, this gap narrows or reverses in proton+X-ray models. Third, trend-smoothing significantly enhances the performance of proton+X-ray models by mitigating fluctuations in the X-ray channel. Fourth, while models trained on trendsmoothed data perform best on average, the best-performing model was trained on original data, suggesting that architectural choices can sometimes outweigh the benefits of data preprocessing. </p>
<blockquote>
<p>å¤ªé˜³è´¨å­äº‹ä»¶ï¼ˆSPEsï¼‰å¯¹å«æ˜Ÿã€å®‡èˆªå‘˜å’ŒæŠ€æœ¯ç³»ç»Ÿé€ æˆé‡å¤§è¾å°„å±å®³ã€‚å‡†ç¡®åœ°é¢„æµ‹å…¶è´¨å­æµé‡æ—¶é—´åˆ†å¸ƒå¯¹äºæå‰é¢„è­¦å’Œç¼“è§£è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¢è®¨äº†åŸºäºé•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„æ·±åº¦å­¦ä¹ åºåˆ—åˆ°åºåˆ—ï¼ˆseq2seqï¼‰æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹SPEå‘ç”Ÿå24å°æ—¶çš„è´¨å­æµé‡åˆ†å¸ƒã€‚æˆ‘ä»¬ä½¿ç”¨äº†NOAA GOESè§‚æµ‹åˆ°çš„40ä¸ªè”ç³»ç´§å¯†çš„SPEsæ•°æ®é›†ï¼ˆ1997-2017å¹´ï¼‰ï¼Œæ¯ä¸ªäº‹ä»¶éƒ½ä¸è¥¿éƒ¨åŠçƒå¤ªé˜³è€€æ–‘çš„Mçº§æˆ–ä»¥ä¸Šçº§åˆ«ä»¥åŠæœªå—å¹²æ‰°çš„è´¨å­æµé‡åˆ†å¸ƒç›¸å…³ã€‚é‡‡ç”¨4æŠ˜åˆ†å±‚äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸åŒé…ç½®çš„seq2seqæ¨¡å‹ï¼ˆå˜åŒ–éšè—å•å…ƒå’ŒåµŒå…¥ç»´åº¦ï¼‰åœ¨å¤šç§é¢„æµ‹åœºæ™¯ä¸‹çš„è¡¨ç°ï¼šï¼ˆiï¼‰ä»…è´¨å­è¾“å…¥ä¸ç»„åˆè´¨å­+Xå°„çº¿è¾“å…¥ï¼Œï¼ˆiiï¼‰åŸå§‹æµé‡æ•°æ®ä¸è¶‹åŠ¿å¹³æ»‘æ•°æ®ï¼Œä»¥åŠï¼ˆiiiï¼‰è‡ªå›å½’ä¸ä¸€æ¬¡æ€§é¢„æµ‹ã€‚æˆ‘ä»¬çš„ä¸»è¦ç»“æœå¦‚ä¸‹ï¼šé¦–å…ˆï¼Œä¸€æ¬¡æ€§é¢„æµ‹äº§ç”Ÿçš„è¯¯å·®å§‹ç»ˆä½äºè‡ªå›å½’é¢„æµ‹ï¼Œé¿å…äº†è¿­ä»£æ–¹æ³•ä¸­å‡ºç°çš„è¯¯å·®ç´¯ç§¯ã€‚å…¶æ¬¡ï¼Œåœ¨åŸå§‹æ•°æ®ä¸Šï¼Œä»…è´¨å­æ¨¡å‹çš„è¡¨ç°ä¼˜äºè´¨å­+Xå°„çº¿æ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨è¶‹åŠ¿å¹³æ»‘çš„æ•°æ®ä¸Šï¼Œè¿™ä¸€å·®è·ç¼©å°æˆ–ç”šè‡³é€†è½¬äº†ã€‚ç¬¬ä¸‰ï¼Œè¶‹åŠ¿å¹³æ»‘ä¼šæ˜¾è‘—æ”¹å–„è´¨å­+Xå°„çº¿æ¨¡å‹çš„è¡¨ç°ï¼Œé€šè¿‡å‡è½»Xå°„çº¿é€šé“çš„æ³¢åŠ¨æ¥å®ç°ã€‚ç¬¬å››ï¼Œè™½ç„¶åœ¨è¶‹åŠ¿å¹³æ»‘æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹å¹³å‡è¡¨ç°æœ€ä½³ï¼Œä½†è¡¨ç°æœ€å¥½çš„æ¨¡å‹æ˜¯åœ¨åŸå§‹æ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œè¿™è¡¨æ˜æ¶æ„çš„é€‰æ‹©æœ‰æ—¶å¯èƒ½ä¼šè¶…è¿‡æ•°æ®é¢„å¤„ç†çš„ä¼˜ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05399v1">PDF</a> 7 pages; accepted as a workshop paper at ICDM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨æ·±åº¦å­¦ä¹ åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼ˆseq2seqï¼‰ç»“åˆé•¿æœŸçŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ï¼Œå¯¹å¤ªé˜³è´¨å­äº‹ä»¶ï¼ˆSPEsï¼‰å‘ç”Ÿåçš„24å°æ—¶è´¨å­æµé‡è¿›è¡Œé¢„æµ‹ã€‚æ–‡ç« é€šè¿‡å¤šé¡¹å®éªŒéªŒè¯äº†ä¸åŒçš„æ¨¡å‹é…ç½®ï¼Œå‘ç°è¶‹åŠ¿å¹³æ»‘å¤„ç†èƒ½å¤Ÿæé«˜æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«è´¨å­åŠXå°„çº¿è¾“å…¥çš„æ¨¡å‹ä¸­è¡¨ç°æ˜æ˜¾ã€‚æœ€ä½³æ¨¡å‹åŸºäºåŸå§‹æ•°æ®è®­ç»ƒå¾—åˆ°ï¼Œè¯´æ˜æ¨¡å‹æ¶æ„çš„é€‰æ‹©æœ‰æ—¶èƒ½å¤ŸæŠµæ¶ˆæ•°æ®é¢„å¤„ç†çš„æ½œåœ¨ä¼˜åŠ¿ã€‚æœ¬æ–‡ç ”ç©¶æˆæœå¯¹æå‰é¢„è­¦å’Œç¼“è§£SPEå¼•å‘çš„è¾å°„å±å®³å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±å®å­¦ä¹ åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼ˆseq2seqï¼‰ç»“åˆLSTMç½‘ç»œç”¨äºé¢„æµ‹SPEå‘ç”Ÿåçš„24å°æ—¶è´¨å­æµé‡ã€‚</li>
<li>å®éªŒéªŒè¯äº†ä¸åŒçš„æ¨¡å‹é…ç½®ï¼ŒåŒ…æ‹¬éšè—å•å…ƒå’ŒåµŒå…¥ç»´åº¦çš„å˜åŒ–ã€‚</li>
<li>å‘ç°è¶‹åŠ¿å¹³æ»‘å¤„ç†èƒ½å¤Ÿæé«˜æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«è´¨å­åŠXå°„çº¿è¾“å…¥çš„æ¨¡å‹ä¸­ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åŸºäºåŸå§‹æ•°æ®è®­ç»ƒå¾—åˆ°ï¼Œå¼ºè°ƒæ¨¡å‹æ¶æ„é€‰æ‹©çš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a75c69c9e6d8c21daab0c541170b3ca7" align="middle">
<img src="https://picx.zhimg.com/v2-0d6e93177fe718b8021ee3569d562692.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a945e22312e9067bb1b2da09013487de" align="middle">
<img src="https://picx.zhimg.com/v2-7af6049e45b345197cfe385552bd9f06" align="middle">
<img src="https://pic1.zhimg.com/v2-fa0d9301c9e938b6c411e9f8f99c298b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8323cbdd55d4b4da9a327989ac87e67" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Fine-Tuned-CNN-Based-Approach-for-Multi-Class-Mango-Leaf-Disease-Detection"><a href="#Fine-Tuned-CNN-Based-Approach-for-Multi-Class-Mango-Leaf-Disease-Detection" class="headerlink" title="Fine-Tuned CNN-Based Approach for Multi-Class Mango Leaf Disease   Detection"></a>Fine-Tuned CNN-Based Approach for Multi-Class Mango Leaf Disease   Detection</h2><p><strong>Authors:Jalal Ahmmed, Faruk Ahmed, Rashedul Hasan Shohan, Md. Mahabub Rana, Mahdi Hasan</strong></p>
<p>Mango is an important fruit crop in South Asia, but its cultivation is frequently hampered by leaf diseases that greatly impact yield and quality. This research examines the performance of five pre-trained convolutional neural networks, DenseNet201, InceptionV3, ResNet152V2, SeResNet152, and Xception, for multi-class identification of mango leaf diseases across eight classes using a transfer learning strategy with fine-tuning. The models were assessed through standard evaluation metrics, such as accuracy, precision, recall, F1-score, and confusion matrices. Among the architectures tested, DenseNet201 delivered the best results, achieving 99.33% accuracy with consistently strong metrics for individual classes, particularly excelling in identifying Cutting Weevil and Bacterial Canker. Moreover, ResNet152V2 and SeResNet152 provided strong outcomes, whereas InceptionV3 and Xception exhibited lower performance in visually similar categories like Sooty Mould and Powdery Mildew. The training and validation plots demonstrated stable convergence for the highest-performing models. The capability of fine-tuned transfer learning models, for precise and dependable multi-class mango leaf disease detection in intelligent agricultural applications. </p>
<blockquote>
<p>èŠ’æœæ˜¯å—äºšé‡è¦çš„æœæ ‘ä½œç‰©ï¼Œä½†å…¶æ ½åŸ¹ç»å¸¸å—åˆ°å¶ç‰‡ç–¾ç—…çš„å›°æ‰°ï¼Œå¯¹äº§é‡å’Œè´¨é‡äº§ç”Ÿå¾ˆå¤§å½±å“ã€‚æœ¬ç ”ç©¶é‡‡ç”¨è¿ç§»å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å¾®è°ƒï¼Œå¯¹äº”ç§é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆDenseNet201ã€InceptionV3ã€ResNet152V2ã€SeResNet152å’ŒXceptionï¼‰è¿›è¡Œäº†å¤šç±»èŠ’æœå¶ç—…è¯†åˆ«æ€§èƒ½çš„ç ”ç©¶ã€‚è¿™äº›æ¨¡å‹é€šè¿‡å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’Œæ··æ·†çŸ©é˜µç­‰æ ‡å‡†è¯„ä»·æŒ‡æ ‡è¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨æµ‹è¯•çš„æ¶æ„ä¸­ï¼ŒDenseNet201å–å¾—äº†æœ€ä½³ç»“æœï¼Œå…¶å‡†ç¡®ç‡è¾¾åˆ°99.33%ï¼Œå¹¶ä¸”å„ç±»åˆ«çš„æŒ‡æ ‡è¡¨ç°ä¸€ç›´å¼ºåŠ²ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«åˆ‡å‰²é”¯æœ¨è™«å’Œç»†èŒæ€§æºƒç–¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒResNet152V2å’ŒSeResNet152ä¹Ÿå–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼Œè€ŒInceptionV3å’ŒXceptionåœ¨è§†è§‰ä¸Šç›¸ä¼¼çš„ç±»åˆ«ï¼ˆå¦‚ç…¤çƒŸç—…å’Œç²‰çŠ¶ç™½ç²‰ç—…ï¼‰ä¸­è¡¨ç°è¾ƒå·®ã€‚è®­ç»ƒå›¾å’ŒéªŒè¯å›¾æ˜¾ç¤ºäº†é«˜æ€§èƒ½æ¨¡å‹çš„ç¨³å®šæ”¶æ•›æ€§ã€‚å¾®è°ƒè¿ç§»å­¦ä¹ æ¨¡å‹åœ¨æ™ºèƒ½å†œä¸šåº”ç”¨ä¸­ç²¾ç¡®å¯é çš„å¤šç±»èŠ’æœå¶ç—…æ£€æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05326v1">PDF</a> Double column 6 pages, 10 figures, ieee conference style</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶åˆ©ç”¨è¿ç§»å­¦ä¹ ç­–ç•¥å¾®è°ƒäº”ç§é¢„è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬DenseNet201ã€InceptionV3ã€ResNet152V2ã€SeResNet152å’ŒXceptionï¼Œå¯¹å—äºšçƒ­å¸¦åœ°åŒºèŠ’æœå¶ç—…å®³è¿›è¡Œå¤šç±»è¯†åˆ«ã€‚ç ”ç©¶ä¸­é€šè¿‡å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’Œæ··æ·†çŸ©é˜µç­‰æ ‡å‡†è¯„ä»·æŒ‡æ ‡å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒDenseNet201è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡é«˜è¾¾99.33%ï¼Œå¯¹åˆ‡å‰²é’»å¿ƒç—‡å’Œç»†èŒæ€§æºƒç–¡ç—…è¯†åˆ«å°¤ä¸ºå‡ºè‰²ã€‚å…¶ä»–æ¨¡å‹å¦‚ResNet152V2å’ŒSeResNet152è¡¨ç°è‰¯å¥½ï¼Œè€ŒInceptionV3å’ŒXceptionåœ¨è§†è§‰ä¸Šç›¸ä¼¼çš„ç±»åˆ«å¦‚éœ‰æ–‘ç—…å’Œç™½ç²‰ç—…ä¸­çš„è¡¨ç°è¾ƒå·®ã€‚é«˜æ€§èƒ½æ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯å›¾æ˜¾ç¤ºç¨³å®šæ”¶æ•›ã€‚è¿™äº›å¾®è°ƒåçš„è¿ç§»å­¦ä¹ æ¨¡å‹å¯ç”¨äºæ™ºèƒ½å†œä¸šåº”ç”¨ä¸­çš„ç²¾å‡†å¯é èŠ’æœå¶å¤šç—…å®³æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨äº”ç§é¢„è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œå¯¹èŠ’æœå¶ç—…å®³è¿›è¡Œå¤šç±»è¯†åˆ«ã€‚</li>
<li>DenseNet201åœ¨èŠ’æœå¶ç—…å®³è¯†åˆ«ä¸­è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡é«˜ã€‚</li>
<li>ResNet152V2å’ŒSeResNet152åœ¨èŠ’æœå¶ç—…å®³è¯†åˆ«ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>InceptionV3å’ŒXceptionåœ¨è§†è§‰ä¸Šç›¸ä¼¼çš„ç±»åˆ«è¯†åˆ«ä¸­è¡¨ç°è¾ƒå·®ã€‚</li>
<li>è¿ç§»å­¦ä¹ ç­–ç•¥èƒ½æœ‰æ•ˆåº”ç”¨äºèŠ’æœå¶ç—…å®³çš„ç²¾å‡†æ£€æµ‹ã€‚</li>
<li>ç ”ç©¶é€šè¿‡æ ‡å‡†è¯„ä»·æŒ‡æ ‡å…¨é¢è¯„ä¼°äº†å„æ¨¡å‹çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b2ed3044d49130b73623838d4ad16be" align="middle">
<img src="https://pic1.zhimg.com/v2-25c5bc04b98f25ce0b802d88de678127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f36e028b229bfb4318076d75caa7dbd" align="middle">
<img src="https://picx.zhimg.com/v2-c44c37e2f7312eec432754f91cae582b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10e83b917e91ce7d54862049c82c5df4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bb07e60a4ec490b70dc4fff16ce1109" align="middle">
<img src="https://picx.zhimg.com/v2-d944cc160b466ab6addfb2a57e2d4b5d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Study-of-Lobster-and-Kirkpatrick-Baez-Designs-for-a-Small-Mission-dedicated-to-Gravitational-Wave-Transient-Localization"><a href="#Study-of-Lobster-and-Kirkpatrick-Baez-Designs-for-a-Small-Mission-dedicated-to-Gravitational-Wave-Transient-Localization" class="headerlink" title="Study of Lobster and Kirkpatrick-Baez Designs for a Small Mission   dedicated to Gravitational Wave Transient Localization"></a>Study of Lobster and Kirkpatrick-Baez Designs for a Small Mission   dedicated to Gravitational Wave Transient Localization</h2><p><strong>Authors:John Rankin, Sergio Campana, Giovanni Pareschi, Daniele Spiga, Stefano Basso, Marta Maria Civitani, Paolo Conconi, Vincenzo Cotroneo</strong></p>
<p>The localization of X-ray counterparts to gravitational wave events requires a telescope with accurate localization capability in a field of view comparable to the region constrained by the gravitational wave detectors. In the context of a small, dedicated, mission, we investigate which optical design could satisfy this capability. We compare the possible optical designs that have been proposed for X-rays: the Lobster Eye design (both in the Angel and Schmidt variant) - inspired by the eyes of crustaceans - consisting of many small capillaries where grazing incidence reflection occurs, the Kirkpatrick-Baez design, where double reflection occurs on two orthogonal parabolic mirrors, and the standard Wolter-I design. We find that the first two designs, compared to the latter, can achieve a significantly larger field of view, and have a good localization capability if the focal length is longer than existing Lobster Eye designs. The Kirkpatrick-Baez design presents the best angular resolution, but the best overall field of view is obtained with a Lobster system: we present a small optical module able to achieve an effective area $&gt;$100 cm$^2$ at 1 keV in a field of view of 10 deg$^2$. </p>
<blockquote>
<p>å¯¹å¼•åŠ›æ³¢äº‹ä»¶çš„Xå°„çº¿å¯¹åº”ç‰©çš„å®šä½è¦æ±‚æœ‰ä¸€ç§æœ›è¿œé•œï¼Œå…¶å®šä½ç²¾åº¦è¦ç›¸å½“é«˜ï¼Œä¸”åœ¨å…¶è§†åœºèŒƒå›´éœ€è¦ä¸ç”±å¼•åŠ›æ³¢æ¢æµ‹å™¨æ‰€çº¦æŸçš„åŒºåŸŸç›¸å½“ã€‚åœ¨ä¸€ä¸ªå°å‹çš„ä¸“é—¨ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å“ªç§å…‰å­¦è®¾è®¡å¯ä»¥æ»¡è¶³è¿™ä¸€èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å·²æå‡ºçš„ç”¨äºXå°„çº¿çš„å¯èƒ½å…‰å­¦è®¾è®¡è¿›è¡Œäº†æ¯”è¾ƒï¼šé¾™è™¾çœ¼è®¾è®¡ï¼ˆåŒ…æ‹¬å¤©ä½¿å’Œæ–½å¯†ç‰¹å˜ä½“ï¼‰â€”â€”è¿™æ˜¯å—ç”²å£³åŠ¨ç‰©çœ¼ç›çš„å¯å‘ï¼Œç”±è®¸å¤šå‘ç”Ÿæ å°„åå°„çš„å°æ¯›ç»†è¡€ç®¡ç»„æˆï¼ŒæŸ¯å…‹å¸•ç‰¹é‡Œå…‹-è´å…¹è®¾è®¡ï¼Œå…¶ä¸­ä¸¤æ¬¡åå°„å‘ç”Ÿåœ¨ä¸¤ä¸ªæ­£äº¤çš„æŠ›ç‰©é¢é•œä¸Šï¼Œä»¥åŠæ ‡å‡†çš„æ²ƒå°”ç‰¹Iå‹è®¾è®¡ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸åè€…ç›¸æ¯”ï¼Œå‰ä¸¤ç§è®¾è®¡å¯ä»¥å®ç°æ›´å¤§çš„è§†åœºï¼Œå¦‚æœç„¦è·é•¿äºç°æœ‰çš„é¾™è™¾çœ¼è®¾è®¡ï¼Œåˆ™å…·æœ‰è‰¯å¥½çš„å®šä½èƒ½åŠ›ã€‚æŸ¯å…‹å¸•ç‰¹é‡Œå…‹-è´å…¹è®¾è®¡å…·æœ‰æœ€ä½³çš„è§’åˆ†è¾¨ç‡ï¼Œä½†æœ€ä½³æ€»ä½“è§†åœºæ˜¯ç”±é¾™è™¾ç³»ç»Ÿè·å¾—çš„ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°å‹å…‰å­¦æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨10å¹³æ–¹åº¦çš„è§†åœºèŒƒå›´å†…å®ç°å¤§äº100å¹³æ–¹å˜ç±³çš„æœ‰æ•ˆé¢ç§¯åœ¨åªæœ‰åªæœ‰å‡ åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05002v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¯¹äºXå°„çº¿å¯¹åº”å¼•åŠ›æ³¢äº‹ä»¶å®šä½çš„éœ€æ±‚ï¼Œéœ€è¦æœ›è¿œé•œå…·æœ‰å‡†ç¡®çš„å®šä½èƒ½åŠ›ï¼Œå…¶è§†åœºéœ€ä¸å¼•åŠ›æ³¢æ¢æµ‹å™¨æ‰€é™å®šçš„åŒºåŸŸç›¸å½“ã€‚åœ¨å°å‹ä¸“ç”¨ä»»åŠ¡çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å“ªäº›å…‰å­¦è®¾è®¡å¯ä»¥æ»¡è¶³è¿™ç§èƒ½åŠ›ã€‚æ¯”è¾ƒäº†ä¸ºXå°„çº¿æå‡ºçš„å…‰å­¦è®¾è®¡æ–¹æ¡ˆï¼ŒåŒ…æ‹¬é¾™è™¾çœ¼è®¾è®¡ï¼ˆå¤©ä½¿å’Œæ–½å¯†ç‰¹å˜ä½“ï¼‰ï¼Œå…¶ç”±è®¸å¤šå°æ¯›ç»†è¡€ç®¡ç»„æˆï¼Œå‘ç”Ÿå€¾æ–œå…¥å°„åå°„ï¼›åŸºå°”å…‹å¸•ç‰¹é‡Œå…‹-è´å…¹è®¾è®¡ï¼Œå…¶ä¸­ä¸¤æ¬¡åå°„å‘ç”Ÿåœ¨ä¸¤ä¸ªæ­£äº¤æŠ›ç‰©é•œä¸Šï¼›ä»¥åŠæ ‡å‡†çš„æ²ƒå°”ç‰¹-Iè®¾è®¡ã€‚æˆ‘ä»¬å‘ç°å‰ä¸¤ç§è®¾è®¡ä¸åè€…ç›¸æ¯”ï¼Œå¯ä»¥å®ç°æ›´å¤§çš„è§†åœºï¼Œå¦‚æœç„¦è·é•¿äºç°æœ‰é¾™è™¾çœ¼è®¾è®¡ï¼Œåˆ™å…·æœ‰è‰¯å¥½çš„å®šä½èƒ½åŠ›ã€‚åŸºå°”å…‹å¸•ç‰¹é‡Œå…‹-è´å…¹è®¾è®¡å…·æœ‰æœ€ä½³è§’åˆ†è¾¨ç‡ï¼Œä½†æ•´ä½“æœ€ä½³è§†åœºæ˜¯ç”±é¾™è™¾ç³»ç»Ÿè·å¾—çš„ï¼šæˆ‘ä»¬æå‡ºä¸€ç§å°å‹å…‰å­¦æ¨¡å—ï¼Œèƒ½åœ¨10 deg$^2$çš„è§†åœºä¸Šå®ç°å¤§äº100 cm$^2$çš„æœ‰æ•ˆé¢ç§¯ï¼Œåœ¨1 keVæ—¶è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹äºå®šä½Xå°„çº¿å¯¹åº”å¼•åŠ›æ³¢äº‹ä»¶ï¼Œéœ€è¦æœ›è¿œé•œå…·å¤‡åœ¨è¾ƒå¤§è§†åœºä¸Šè¿›è¡Œå‡†ç¡®å®šä½çš„èƒ½åŠ›ã€‚</li>
<li>é¾™è™¾çœ¼è®¾è®¡å’ŒåŸºå°”å…‹å¸•ç‰¹é‡Œå…‹-è´å…¹è®¾è®¡ç›¸è¾ƒäºæ ‡å‡†æ²ƒå°”ç‰¹-Iè®¾è®¡ï¼Œèƒ½å¤Ÿå®ç°æ›´å¤§çš„è§†åœºã€‚</li>
<li>é¾™è™¾çœ¼è®¾è®¡çš„ä¼˜åŠ¿åœ¨äºå…¶ç‹¬ç‰¹çš„ç»“æ„èƒ½å¤Ÿå®ç°è‰¯å¥½çš„å®šä½èƒ½åŠ›ï¼Œå°¤å…¶åœ¨ç„¦è·è¾ƒé•¿çš„æƒ…å†µä¸‹ã€‚</li>
<li>åŸºå°”å…‹å¸•ç‰¹é‡Œå…‹-è´å…¹è®¾è®¡åœ¨è§’åˆ†è¾¨ç‡ä¸Šå…·æœ‰æœ€ä½³è¡¨ç°ã€‚</li>
<li>æœ€ä½³çš„æ•´ä½“è§†åœºæ˜¯é€šè¿‡é¾™è™¾ç³»ç»Ÿå®ç°çš„ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å°å‹å…‰å­¦æ¨¡å—ï¼Œèƒ½åœ¨è¾ƒå¤§çš„è§†åœºä¸Šå®ç°è¾ƒå¤§çš„æœ‰æ•ˆé¢ç§¯ï¼Œå°¤å…¶åœ¨1 keVæ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95c10f385e78cf5f3e449cb32b6f4a70" align="middle">
<img src="https://picx.zhimg.com/v2-28b8da9b80fa9e46f19c42b31abfad2b" align="middle">
<img src="https://picx.zhimg.com/v2-59e54d7884c078af5723c382c317666b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b310ab873f9491c68ba8895a0aedf19f" align="middle">
<img src="https://pic1.zhimg.com/v2-4797f91519eff6674710cd896e9a4489.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Bridging-Clinical-Narratives-and-ACR-Appropriateness-Guidelines-A-Multi-Agent-RAG-System-for-Medical-Imaging-Decisions"><a href="#Bridging-Clinical-Narratives-and-ACR-Appropriateness-Guidelines-A-Multi-Agent-RAG-System-for-Medical-Imaging-Decisions" class="headerlink" title="Bridging Clinical Narratives and ACR Appropriateness Guidelines: A   Multi-Agent RAG System for Medical Imaging Decisions"></a>Bridging Clinical Narratives and ACR Appropriateness Guidelines: A   Multi-Agent RAG System for Medical Imaging Decisions</h2><p><strong>Authors:Satrio Pambudi, Filippo Menolascina</strong></p>
<p>The selection of appropriate medical imaging procedures is a critical and complex clinical decision, guided by extensive evidence-based standards such as the ACR Appropriateness Criteria (ACR-AC). However, the underutilization of these guidelines, stemming from the difficulty of mapping unstructured patient narratives to structured criteria, contributes to suboptimal patient outcomes and increased healthcare costs. To bridge this gap, we introduce a multi-agent cognitive architecture that automates the translation of free-text clinical scenarios into specific, guideline-adherent imaging recommendations. Our system leverages a novel, domain-adapted dense retrieval model, ColBERT, fine-tuned on a synthetically generated dataset of 8,840 clinical scenario-recommendation pairs to achieve highly accurate information retrieval from the ACR-AC knowledge base. This retriever identifies candidate guidelines with a 93.9% top-10 recall, which are then processed by a sequence of LLM-based agents for selection and evidence-based synthesis. We evaluate our architecture using GPT-4.1 and MedGemma agents, demonstrating a state-of-the-art exact match accuracy of 81%, meaning that in 81% of test cases the predicted procedure set was identical to the guidelineâ€™s reference set, and an F1-score of 0.879. This represents a 67-percentage-point absolute improvement in accuracy over a strong standalone GPT-4.1 baseline, underscoring the contribution that our architecture makes to a frontier model. These results were obtained on a challenging test set with substantial lexical divergence from the source guidelines. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/demo-iclr-B567/">https://anonymous.4open.science/r/demo-iclr-B567/</a> </p>
<blockquote>
<p>é€‰æ‹©é€‚å½“çš„åŒ»å­¦æˆåƒç¨‹åºæ˜¯ä¸€é¡¹é‡è¦è€Œå¤æ‚çš„ä¸´åºŠå†³ç­–ï¼Œç”±è¯¸å¦‚ACRé€‚å®œæ€§æ ‡å‡†ï¼ˆACR-ACï¼‰ç­‰åŸºäºå¤§é‡è¯æ®çš„æ ‡å‡†æŒ‡å¯¼ã€‚ç„¶è€Œï¼Œç”±äºå°†éç»“æ„åŒ–æ‚£è€…å™è¿°æ˜ å°„åˆ°ç»“æ„åŒ–æ ‡å‡†ä¸Šçš„éš¾åº¦ï¼Œè¿™äº›æŒ‡å—çš„åˆ©ç”¨ä¸è¶³å¯¼è‡´äº†æ‚£è€…ç»“æœä¸ç†æƒ³å’ŒåŒ»ç–—ä¿å¥æˆæœ¬å¢åŠ ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ™ºèƒ½ä½“è®¤çŸ¥æ¶æ„ï¼Œè¯¥æ¶æ„å¯ä»¥è‡ªåŠ¨å°†è‡ªç”±æ–‡æœ¬ä¸´åºŠæƒ…æ™¯è½¬åŒ–ä¸ºç‰¹å®šçš„ã€ç¬¦åˆæŒ‡å—çš„æˆåƒå»ºè®®ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåˆ©ç”¨äº†ä¸€ç§æ–°å‹çš„ã€é€‚åº”é¢†åŸŸçš„å¯†é›†æ£€ç´¢æ¨¡å‹ColBERTï¼Œå®ƒåœ¨8840ä¸ªä¸´åºŠæƒ…æ™¯å»ºè®®å¯¹ä¸Šåˆæˆçš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥ä»ACR-ACçŸ¥è¯†åº“ä¸­å®ç°é«˜åº¦å‡†ç¡®çš„ä¿¡æ¯æ£€ç´¢ã€‚è¯¥æ£€ç´¢å™¨ä»¥93.9%çš„å‰åå¬å›ç‡è¯†åˆ«å€™é€‰æŒ‡å—ï¼Œç„¶åç»ç”±ä¸€ç³»åˆ—åŸºäºLLMçš„æ™ºèƒ½ä½“è¿›è¡Œé€‰æ‹©å’ŒåŸºäºè¯æ®çš„ç»¼åˆå¤„ç†ã€‚æˆ‘ä»¬ä½¿ç”¨GPT 4.1å’ŒMedGemmaæ™ºèƒ½ä½“è¯„ä¼°æˆ‘ä»¬çš„æ¶æ„ï¼Œè¡¨ç°å‡ºäº†æœ€å…ˆè¿›çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡81%ï¼Œè¿™æ„å‘³ç€åœ¨81%çš„æµ‹è¯•æ¡ˆä¾‹ä¸­ï¼Œé¢„æµ‹çš„ç¨‹åºé›†ä¸æŒ‡å—çš„å‚è€ƒé›†ç›¸åŒï¼Œä»¥åŠF1åˆ†æ•°ä¸º0.879ã€‚ç›¸è¾ƒäºå¼ºå¤§çš„ç‹¬ç«‹GPT 4.1åŸºçº¿ï¼Œæˆ‘ä»¬çš„æ¶æ„åœ¨å‡†ç¡®æ€§ä¸Šå®ç°äº†é«˜è¾¾67ä¸ªç™¾åˆ†ç‚¹çš„ç»å¯¹æå‡ï¼Œçªæ˜¾äº†æˆ‘ä»¬çš„æ¶æ„å¯¹å‰æ²¿æ¨¡å‹çš„è´¡çŒ®ã€‚è¿™äº›ç»“æœæ˜¯åœ¨ä¸€ä¸ªå…·æœ‰ä¸æ¥æºæŒ‡å—æ˜¾è‘—è¯æ±‡å·®å¼‚çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•é›†ä¸Šè·å¾—çš„ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/demo-iclr-B567/">https://anonymous.4open.science/r/demo-iclr-B567/</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04969v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šæ™ºèƒ½ä½“è®¤çŸ¥æ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿå°†è‡ªç”±æ–‡æœ¬çš„ä¸´åºŠåœºæ™¯è‡ªåŠ¨è½¬åŒ–ä¸ºç¬¦åˆæŒ‡å—çš„æˆåƒå»ºè®®ã€‚é€šè¿‡ä½¿ç”¨åŸŸé€‚åº”å¯†é›†æ£€ç´¢æ¨¡å‹ColBERTï¼Œç»“åˆåˆæˆæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œç³»ç»Ÿå®ç°äº†ä»ACR-ACçŸ¥è¯†åº“ä¸­é«˜åº¦å‡†ç¡®çš„ä¿¡æ¯æ£€ç´¢ã€‚è¯¥æ¶æ„ä½¿ç”¨GPT-4.1å’ŒMedGemmaæ™ºèƒ½ä½“è¿›è¡Œè¯„ä¼°ï¼Œåœ¨æŒ‘æˆ˜æµ‹è¯•é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒç¨‹åºçš„é€‰æ‹©æ˜¯ä¸€ä¸ªå—è¯æ®æ ‡å‡†ï¼ˆå¦‚ACRé€‚å®œæ€§æ ‡å‡†ï¼‰æŒ‡å¯¼çš„é‡è¦ä¸”å¤æ‚çš„ä¸´åºŠå†³ç­–ã€‚</li>
<li>æœªå……åˆ†åˆ©ç”¨è¿™äº›æŒ‡å—æºäºå°†éç»“æ„åŒ–æ‚£è€…å™äº‹æ˜ å°„åˆ°ç»“æ„åŒ–æ ‡å‡†çš„å›°éš¾ï¼Œå¯èƒ½å¯¼è‡´æ‚£è€…ç»“æœä¸ä½³å’ŒåŒ»ç–—ä¿å¥æˆæœ¬å¢åŠ ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“è®¤çŸ¥æ¶æ„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†è‡ªç”±æ–‡æœ¬ä¸´åºŠåœºæ™¯è½¬åŒ–ä¸ºå…·ä½“çš„ã€ç¬¦åˆæŒ‡å—çš„æˆåƒå»ºè®®ã€‚</li>
<li>ä½¿ç”¨åŸŸé€‚åº”å¯†é›†æ£€ç´¢æ¨¡å‹ColBERTï¼Œåœ¨åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒï¼Œå®ç°é«˜åº¦å‡†ç¡®çš„ä¿¡æ¯ä»ACR-ACçŸ¥è¯†åº“ä¸­æ£€ç´¢ã€‚</li>
<li>æ£€ç´¢æ¨¡å‹èƒ½å¤Ÿåœ¨å‰åä¸ªå€™é€‰æŒ‡å—ä¸­è¾¾åˆ°93.9%çš„å¬å›ç‡ã€‚</li>
<li>ä½¿ç”¨GPT-4.1å’ŒMedGemmaæ™ºèƒ½ä½“è¯„ä¼°æ¶æ„ï¼Œè·å¾—ä¸šç•Œæœ€ä½³ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡81%ï¼ŒF1åˆ†æ•°ä¸º0.879ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04969">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90bf4784b060fe2b3e222d44a79c821e" align="middle">
<img src="https://picx.zhimg.com/v2-e0d2c7931fc2e93dcae5d32ea0a7ab35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d4976ba0f383e316f026851486f161b" align="middle">
<img src="https://picx.zhimg.com/v2-6cb359ed2cabc3ac8ad76c24c750a05b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Bidirectional-Mammogram-View-Translation-with-Column-Aware-and-Implicit-3D-Conditional-Diffusion"><a href="#Bidirectional-Mammogram-View-Translation-with-Column-Aware-and-Implicit-3D-Conditional-Diffusion" class="headerlink" title="Bidirectional Mammogram View Translation with Column-Aware and Implicit   3D Conditional Diffusion"></a>Bidirectional Mammogram View Translation with Column-Aware and Implicit   3D Conditional Diffusion</h2><p><strong>Authors:Xin Li, Kaixiang Yang, Qiang Li, Zhiwei Wang</strong></p>
<p>Dual-view mammography, including craniocaudal (CC) and mediolateral oblique (MLO) projections, offers complementary anatomical views crucial for breast cancer diagnosis. However, in real-world clinical workflows, one view may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting the effectiveness of downstream analysis. View-to-view translation can help recover missing views and improve lesion alignment. Unlike natural images, this task in mammography is highly challenging due to large non-rigid deformations and severe tissue overlap in X-ray projections, which obscure pixel-level correspondences. In this paper, we propose Column-Aware and Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view translation framework based on conditional diffusion model. To address cross-view structural misalignment, we first design a column-aware cross-attention mechanism that leverages the geometric property that anatomically corresponding regions tend to lie in similar column positions across views. A Gaussian-decayed bias is applied to emphasize local column-wise correlations while suppressing distant mismatches. Furthermore, we introduce an implicit 3D structure reconstruction module that back-projects noisy 2D latents into a coarse 3D feature volume based on breast-view projection geometry. The reconstructed 3D structure is refined and injected into the denoising UNet to guide cross-view generation with enhanced anatomical awareness. Extensive experiments demonstrate that CA3D-Diff achieves superior performance in bidirectional tasks, outperforming state-of-the-art methods in visual fidelity and structural consistency. Furthermore, the synthesized views effectively improve single-view malignancy classification in screening settings, demonstrating the practical value of our method in real-world diagnostics. </p>
<blockquote>
<p>åŒè§†è§’ä¹³è…ºé€ å½±æœ¯åŒ…æ‹¬é¢…å°¾ï¼ˆCCï¼‰å’Œå†…å¤–æ–œï¼ˆMLOï¼‰æŠ•å½±ï¼Œæä¾›äº†ç›¸äº’è¡¥å……çš„è§£å‰–å­¦è§†è§’ï¼Œå¯¹ä¹³è…ºç™Œè¯Šæ–­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨ç°å®çš„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ï¼Œç”±äºé‡‡é›†é”™è¯¯æˆ–å‹ç¼©ä¼ªå½±ç­‰åŸå› ï¼ŒæŸä¸€è§†è§’å¯èƒ½ä¼šç¼ºå¤±ã€æŸåæˆ–é€€åŒ–ï¼Œé™åˆ¶äº†åç»­åˆ†æçš„æœ‰æ•ˆæ€§ã€‚è§†è§’åˆ°è§†è§’çš„ç¿»è¯‘å¯ä»¥å¸®åŠ©æ¢å¤ç¼ºå¤±çš„è§†è§’å¹¶æé«˜ç—…å˜å¯¹é½ã€‚ç„¶è€Œï¼Œä¸å¤©ç„¶å›¾åƒç›¸æ¯”ï¼Œè¿™ä¸€ä»»åŠ¡åœ¨ä¹³è…ºé€ å½±ä¸­éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨å¤§çš„éåˆšæ€§å˜å½¢å’Œä¸¥é‡çš„ç»„ç»‡é‡å åœ¨Xå°„çº¿æŠ•å½±ä¸­ï¼Œè¿™æ©ç›–äº†åƒç´ çº§çš„å¯¹åº”å…³ç³»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åˆ—æ„ŸçŸ¥å’Œéšå¼ä¸‰ç»´æ‰©æ•£ï¼ˆCA3D-Diffï¼‰åŒå‘ä¹³è…ºé€ å½±è§†å›¾ç¿»è¯‘æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è·¨è§†å›¾çš„ç»“æ„ä¸åŒ¹é…é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§åˆ—æ„ŸçŸ¥äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨å‡ ä½•å±æ€§ï¼Œå³è§£å‰–å¯¹åº”åŒºåŸŸå€¾å‘äºä½äºä¸åŒè§†å›¾ä¸­çš„ç›¸ä¼¼åˆ—ä½ç½®ã€‚åº”ç”¨é«˜æ–¯è¡°å‡åå·®ä»¥å¼ºè°ƒå±€éƒ¨åˆ—ç›¸å…³æ€§ï¼ŒåŒæ—¶æŠ‘åˆ¶è¿œè·ç¦»ä¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéšå¼ä¸‰ç»´ç»“æ„é‡å»ºæ¨¡å—ï¼Œè¯¥æ¨¡å—å°†å™ªå£°äºŒç»´æ½œåœ¨å˜é‡åæŠ•å½±åˆ°åŸºäºä¹³è…ºè§†å›¾æŠ•å½±å‡ ä½•çš„ç²—ç³™ä¸‰ç»´ç‰¹å¾ä½“ç§¯ä¸­ã€‚é‡å»ºçš„ä¸‰ç»´ç»“æ„ç»è¿‡ç»†åŒ–å¹¶æ³¨å…¥å»å™ªUNetä¸­ï¼Œä»¥æŒ‡å¯¼å…·æœ‰å¢å¼ºè§£å‰–å­¦æ„è¯†çš„è·¨è§†å›¾ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCA3D-Diffåœ¨åŒå‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨è§†è§‰ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆæˆçš„è§†å›¾æœ‰æ•ˆåœ°æé«˜äº†å•è§†è§’æ¶æ€§è‚¿ç˜¤åˆ†ç±»åœ¨ç­›æŸ¥ç¯å¢ƒä¸­çš„æ€§èƒ½ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œè¯Šæ–­ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04947v1">PDF</a> BIBM2025 accept, 8 pages, 4 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒè§†è§’ä¹³è…ºæ‘„å½±ï¼ˆåŒ…æ‹¬é¢…å°¾æŠ•å½±å’Œå†…å¤–æ–œä½æŠ•å½±ï¼‰ä¸ºä¹³è…ºç™Œè¯Šæ–­æä¾›äº†äº’è¡¥çš„è§£å‰–è§†å›¾ã€‚ç„¶è€Œï¼Œåœ¨å®é™…çš„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ï¼ŒæŸä¸€è§†è§’å¯èƒ½ç¼ºå¤±ã€æŸåæˆ–é€€åŒ–ï¼Œè¿™ä¼šå½±å“åç»­åˆ†æçš„æœ‰æ•ˆæ€§ã€‚è§†å›¾åˆ°è§†å›¾çš„è½¬æ¢å¯ä»¥å¸®åŠ©æ¢å¤ç¼ºå¤±çš„è§†å›¾å¹¶æé«˜ç—…å˜å¯¹é½çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åŒå‘ä¹³è…ºæ‘„å½±è§†å›¾è½¬æ¢æ¡†æ¶â€”â€”åˆ—æ„ŸçŸ¥ä¸éšå¼ä¸‰ç»´æ‰©æ•£ï¼ˆCA3D-Diffï¼‰ã€‚ä¸ºè§£å†³è·¨è§†å›¾ç»“æ„é”™ä½é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ—æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨å‡ ä½•å±æ€§ä¸­è§£å‰–å¯¹åº”åŒºåŸŸå€¾å‘äºä½äºç›¸ä¼¼åˆ—ä½ç½®çš„ç‰¹ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§éšå¼ä¸‰ç»´ç»“æ„é‡å»ºæ¨¡å—ï¼Œæ ¹æ®ä¹³æˆ¿è§†å›¾æŠ•å½±å‡ ä½•å°†å™ªå£°äºŒç»´æ½œåœ¨å˜é‡æŠ•å½±åˆ°ç²—ç³™çš„ä¸‰ç»´ç‰¹å¾ä½“ç§¯ä¸­ã€‚é‡å»ºçš„ä¸‰ç»´ç»“æ„ç»è¿‡ä¼˜åŒ–åæ³¨å…¥å»å™ªUNetä¸­ï¼Œä»¥æŒ‡å¯¼å…·æœ‰å¢å¼ºè§£å‰–æ„è¯†çš„è·¨è§†å›¾ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒCA3D-Diffåœ¨åŒå‘ä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œåœ¨è§†è§‰ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆæˆçš„è§†å›¾æœ‰æ•ˆæé«˜å•è§†å›¾æ¶æ€§è‚¿ç˜¤åˆ†ç±»çš„ç­›æŸ¥æ•ˆæœï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œè¯Šæ–­ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒè§†è§’ä¹³è…ºæ‘„å½±å¯¹äºä¹³è…ºç™Œè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†å®é™…åº”ç”¨ä¸­å¯èƒ½å­˜åœ¨è§†è§’ç¼ºå¤±ã€æŸåæˆ–é€€åŒ–çš„é—®é¢˜ã€‚</li>
<li>è§†å›¾åˆ°è§†å›¾çš„è½¬æ¢æœ‰åŠ©äºæ¢å¤ç¼ºå¤±çš„è§†å›¾å¹¶æé«˜ç—…å˜å¯¹é½çš„å‡†ç¡®æ€§ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒå‘ä¹³è…ºæ‘„å½±è§†å›¾è½¬æ¢æ¡†æ¶â€”â€”CA3D-Diffï¼ŒåŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>CA3D-Diffé‡‡ç”¨åˆ—æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è§£å†³è·¨è§†å›¾ç»“æ„é”™ä½é—®é¢˜ï¼Œåˆ©ç”¨è§£å‰–å¯¹åº”åŒºåŸŸçš„å‡ ä½•å±æ€§ã€‚</li>
<li>éšå¼ä¸‰ç»´ç»“æ„é‡å»ºæ¨¡å—ç”¨äºå°†å™ªå£°äºŒç»´æ½œåœ¨å˜é‡æŠ•å½±åˆ°ä¸‰ç»´ç‰¹å¾ä½“ç§¯ä¸­ï¼Œå¢å¼ºè§£å‰–æ„è¯†ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºCA3D-Diffåœ¨è§†è§‰ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œåˆæˆçš„è§†å›¾åœ¨å•è§†å›¾æ¶æ€§è‚¿ç˜¤åˆ†ç±»ä¸­å…·æœ‰å®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a75e458e6fc579d5c492765f859b03b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93a208d897fe77db372e7cdabe294d5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9671e24e77be6b552fd67430d0cc95ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-965f1a926fc511d961f8b05cad77b4b7" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Do-Superpixel-Segmentation-Methods-Influence-Deforestation-Image-Classification"><a href="#Do-Superpixel-Segmentation-Methods-Influence-Deforestation-Image-Classification" class="headerlink" title="Do Superpixel Segmentation Methods Influence Deforestation Image   Classification?"></a>Do Superpixel Segmentation Methods Influence Deforestation Image   Classification?</h2><p><strong>Authors:Hugo Resende, Fabio A. Faria, Eduardo B. Neto, Isabela Borlido, Victor Sundermann, Silvio Jamil F. GuimarÃ£es, Ãlvaro L. Fazenda</strong></p>
<p>Image segmentation is a crucial step in various visual applications, including environmental monitoring through remote sensing. In the context of the ForestEyes project, which combines citizen science and machine learning to detect deforestation in tropical forests, image segments are used for labeling by volunteers and subsequent model training. Traditionally, the Simple Linear Iterative Clustering (SLIC) algorithm is adopted as the segmentation method. However, recent studies have indicated that other superpixel-based methods outperform SLIC in remote sensing image segmentation, and might suggest that they are more suitable for the task of detecting deforested areas. In this sense, this study investigated the impact of the four best segmentation methods, together with SLIC, on the training of classifiers for the target application. Initially, the results showed little variation in performance among segmentation methods, even when selecting the top five classifiers using the PyCaret AutoML library. However, by applying a classifier fusion approach (ensemble of classifiers), noticeable improvements in balanced accuracy were observed, highlighting the importance of both the choice of segmentation method and the combination of machine learning-based models for deforestation detection tasks. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²æ˜¯å„ç§è§†è§‰åº”ç”¨ä¸­çš„å…³é”®æ­¥éª¤ï¼ŒåŒ…æ‹¬é€šè¿‡é¥æ„Ÿè¿›è¡Œç¯å¢ƒç›‘æµ‹ã€‚åœ¨ForestEyesé¡¹ç›®çš„èƒŒæ™¯ä¸‹ï¼Œè¯¥é¡¹ç›®ç»“åˆå…¬æ°‘ç§‘å­¦å’Œæœºå™¨å­¦ä¹ æ¥æ£€æµ‹çƒ­å¸¦æ£®æ—çš„ç ä¼æ´»åŠ¨ï¼Œå›¾åƒæ®µè½è¢«å¿—æ„¿è€…ç”¨äºæ ‡æ³¨å’Œéšåçš„æ¨¡å‹è®­ç»ƒã€‚ä¼ ç»Ÿä¸Šï¼Œé‡‡ç”¨Simple Linear Iterative Clusteringï¼ˆSLICï¼‰ç®—æ³•ä½œä¸ºåˆ†å‰²æ–¹æ³•ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå…¶ä»–åŸºäºè¶…åƒç´ çš„æ–¹æ³•åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­ä¼˜äºSLICï¼Œå¹¶ä¸”å¯èƒ½æ›´é€‚åˆæ£€æµ‹ç ä¼åŒºçš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶è°ƒæŸ¥äº†å››ç§æœ€ä½³åˆ†å‰²æ–¹æ³•ä»¥åŠSLICå¯¹ç›®æ ‡åº”ç”¨åˆ†ç±»å™¨è®­ç»ƒçš„å½±å“ã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œåˆ†å‰²æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å˜åŒ–ä¸å¤§ï¼Œå³ä½¿åœ¨ä½¿ç”¨PyCaret AutoMLåº“é€‰æ‹©å‰äº”ä¸ªåˆ†ç±»å™¨æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç„¶è€Œï¼Œé€šè¿‡åº”ç”¨åˆ†ç±»å™¨èåˆæ–¹æ³•ï¼ˆåˆ†ç±»å™¨ç»„åˆï¼‰ï¼Œè§‚å¯Ÿåˆ°å¹³è¡¡ç²¾åº¦çš„æ˜æ˜¾æé«˜ï¼Œè¿™çªå‡ºäº†é€‰æ‹©åˆ†å‰²æ–¹æ³•å’Œç»“åˆæœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œç ä¼æ£€æµ‹ä»»åŠ¡çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04645v1">PDF</a> 15 pages, 3 figures, paper accepted to present at CIARP 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨ForestEyesé¡¹ç›®ä¸­ï¼Œå›¾åƒåˆ†å‰²æ˜¯æ£€æµ‹çƒ­å¸¦æ£®æ—ç ä¼å·¥ä½œçš„é‡è¦æ­¥éª¤ä¹‹ä¸€ã€‚ä¼ ç»Ÿä¸Šé‡‡ç”¨Simple Linear Iterative Clustering (SLIC)ç®—æ³•è¿›è¡Œå›¾åƒåˆ†å‰²ï¼Œä½†æœ¬ç ”ç©¶æ¢è®¨äº†å››ç§æœ€ä½³åˆ†å‰²æ–¹æ³•å’ŒSLICå¯¹ç›®æ ‡åº”ç”¨åˆ†ç±»å™¨è®­ç»ƒçš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨åˆ†ç±»å™¨èåˆæ–¹æ³•ï¼ˆåˆ†ç±»å™¨é›†åˆï¼‰èƒ½å¤Ÿæé«˜å¹³è¡¡ç²¾åº¦ï¼Œè¿™è¡¨æ˜åˆ†å‰²æ–¹æ³•çš„é€‰æ‹©ä»¥åŠæœºå™¨å­¦ä¹ æ¨¡å‹çš„ç»„åˆå¯¹ç ä¼æ£€æµ‹ä»»åŠ¡è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²åœ¨åŒ…æ‹¬ç¯å¢ƒé¥æ„Ÿç›‘æµ‹åœ¨å†…çš„å„ç§è§†è§‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ForestEyesé¡¹ç›®ç»“åˆå…¬æ°‘ç§‘å­¦å’Œæœºå™¨å­¦ä¹ æ£€æµ‹çƒ­å¸¦æ£®æ—ç ä¼æƒ…å†µã€‚</li>
<li>ä¼ ç»Ÿä¸Šé‡‡ç”¨SLICç®—æ³•è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚</li>
<li>æœ‰ç ”ç©¶è¡¨æ˜å…¶ä»–è¶…åƒç´ åˆ†å‰²æ–¹æ³•åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸Šä¼˜äºSLICã€‚</li>
<li>ä¸åŒåˆ†å‰²æ–¹æ³•å¯¹åˆ†ç±»å™¨è®­ç»ƒå½±å“è¾ƒå°ï¼Œä½†é‡‡ç”¨åˆ†ç±»å™¨èåˆæ–¹æ³•èƒ½æ˜¾è‘—æé«˜å¹³è¡¡ç²¾åº¦ã€‚</li>
<li>åˆ†å‰²æ–¹æ³•çš„é€‰æ‹©å¯¹ç ä¼æ£€æµ‹ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-79ca37ce152effa5a185e3a64ba771fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05c7d03cfad3ed9655f04dd8e861291f" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="The-best-performance-in-the-CARE-2025-â€“-Liver-Task-LiSeg-Contrast-Contrast-Aware-Semi-Supervised-Segmentation-with-Domain-Generalization-and-Test-Time-Adaptation"><a href="#The-best-performance-in-the-CARE-2025-â€“-Liver-Task-LiSeg-Contrast-Contrast-Aware-Semi-Supervised-Segmentation-with-Domain-Generalization-and-Test-Time-Adaptation" class="headerlink" title="The best performance in the CARE 2025 â€“ Liver Task (LiSeg-Contrast):   Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and   Test-Time Adaptation"></a>The best performance in the CARE 2025 â€“ Liver Task (LiSeg-Contrast):   Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and   Test-Time Adaptation</h2><p><strong>Authors:Jincan Lou, Jingkun Chen, Haoquan Li, Hang Li, Wenjian Huang, Weihua Chen, Fan Wang, Jianguo Zhang</strong></p>
<p>Accurate liver segmentation from contrast-enhanced MRI is essential for diagnosis, treatment planning, and disease monitoring. However, it remains challenging due to limited annotated data, heterogeneous enhancement protocols, and significant domain shifts across scanners and institutions. Traditional image-to-image translation frameworks have made great progress in domain generalization, but their application is not straightforward. For example, Pix2Pix requires image registration, and cycle-GAN cannot be integrated seamlessly into segmentation pipelines. Meanwhile, these methods are originally used to deal with cross-modality scenarios, and often introduce structural distortions and suffer from unstable training, which may pose drawbacks in our single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised mean teacher scheme to exploit large amounts of unlabeled volumes. A domain adaptation module, incorporating a randomized histogram-based style appearance transfer function and a trainable contrast-aware network, enriches domain diversity and mitigates cross-center variability. Furthermore, a continual test-time adaptation strategy is employed to improve robustness during inference. Extensive experiments demonstrate that our framework consistently outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff Distance while exhibiting strong generalization to unseen domains under low-annotation conditions. </p>
<blockquote>
<p>ä»å¯¹æ¯”å¢å¼ºMRIè¿›è¡Œå‡†ç¡®çš„è‚è„åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ ‡æ³¨æ•°æ®æœ‰é™ã€å¢å¼ºåè®®å¼‚è´¨åŒ–ä»¥åŠæ‰«æä»ªå’Œæœºæ„é—´çš„é¢†åŸŸæ¼‚ç§»ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¡†æ¶åœ¨é¢†åŸŸé€šç”¨åŒ–æ–¹é¢å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œä½†å…¶åº”ç”¨å¹¶ä¸ç›´æ¥ã€‚ä¾‹å¦‚ï¼ŒPix2Pixéœ€è¦è¿›è¡Œå›¾åƒæ³¨å†Œï¼Œè€Œå¾ªç¯GANæ— æ³•æ— ç¼é›†æˆåˆ°åˆ†å‰²ç®¡é“ä¸­ã€‚åŒæ—¶ï¼Œè¿™äº›æ–¹æ³•æœ€åˆæ˜¯ç”¨äºå¤„ç†è·¨æ¨¡æ€åœºæ™¯çš„ï¼Œç»å¸¸ä¼šå¼•å…¥ç»“æ„å¤±çœŸå¹¶é¢ä¸´è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œè¿™å¯èƒ½ä¼šåœ¨æˆ‘ä»¬çš„å•æ¨¡æ€åœºæ™¯ä¸­é€ æˆç¼ºé™·ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoSSeg-TTAï¼Œè¿™æ˜¯ä¸€ä¸ªç´§å‡‘çš„åˆ†å‰²æ¡†æ¶ï¼Œç”¨äºGED4ï¼ˆGd-EOB-DTPAå¢å¼ºè‚èƒ†æœŸMRIï¼‰æ¨¡æ€ï¼ŒåŸºäºnnU-Netv2æ„å»ºï¼Œå¹¶ä½¿ç”¨åŠç›‘ç£å‡å€¼æ•™å¸ˆæ–¹æ¡ˆæ¥åˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„ä½“ç§¯æ•°æ®ã€‚ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å—ï¼Œç»“åˆåŸºäºéšæœºç›´æ–¹å›¾çš„é£æ ¼å¤–è§‚è½¬ç§»å‡½æ•°å’Œå¯è®­ç»ƒçš„å¯¹æ¯”æ„ŸçŸ¥ç½‘ç»œï¼Œä¸°å¯Œäº†é¢†åŸŸå¤šæ ·æ€§å¹¶å‡è½»äº†è·¨ä¸­å¿ƒå˜å¼‚æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨è¿ç»­æµ‹è¯•æ—¶é—´é€‚åº”ç­–ç•¥ï¼Œä»¥æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å§‹ç»ˆä¼˜äºnnU-Netv2åŸºçº¿ï¼Œåœ¨è¾¾åˆ°è¾ƒé«˜çš„Diceåˆ†æ•°å’ŒHausdorffè·ç¦»çš„åŒæ—¶ï¼Œåœ¨ä½æ³¨é‡Šæ¡ä»¶ä¸‹å¯¹æœªè§é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04243v1">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹GED4æ¨¡æ€çš„ç´§å‡‘åˆ†å‰²æ¡†æ¶CoSSeg-TTAï¼Œè¯¥æ¡†æ¶åŸºäºnnU-Netv2ï¼Œå¹¶é‡‡ç”¨åŠç›‘ç£å‡å€¼æ•™å¸ˆæ–¹æ¡ˆï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡è®°ä½“ç§¯æ•°æ®ã€‚é€šè¿‡é¢†åŸŸé€‚åº”æ¨¡å—å’ŒæŒç»­æµ‹è¯•æ—¶é€‚åº”ç­–ç•¥ï¼Œè¯¥æ¡†æ¶æé«˜äº†é¢†åŸŸå¤šæ ·æ€§å’Œè·¨ä¸­å¿ƒå˜å¼‚çš„åº”å¯¹èƒ½åŠ›ï¼Œä¸”åœ¨ä½æ ‡æ³¨æ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œç›¸è¾ƒäºnnU-Netv2åŸºå‡†æ¡†æ¶æœ‰æ›´ä¼˜ç§€çš„Diceåˆ†æ•°å’ŒHausdorffè·ç¦»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚è„åˆ†å‰²åœ¨å¯¹æ¯”å¢å¼ºMRIä¸­éå¸¸é‡è¦ï¼Œä½†å­˜åœ¨æ ‡æ³¨æ•°æ®æœ‰é™ã€å¢å¼ºåè®®å¼‚è´¨æ€§å’Œè·¨æ‰«æä»ªå’Œæœºæ„é¢†åŸŸæ¼‚ç§»ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¡†æ¶åœ¨é¢†åŸŸé€šç”¨åŒ–æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†åœ¨å•æ¨¡æ€åœºæ™¯ä¸­åº”ç”¨å­˜åœ¨å›°éš¾ï¼Œå¦‚Pix2Pixéœ€è¦å›¾åƒé…å‡†ï¼Œcycle-GANæ— æ³•æ— ç¼é›†æˆåˆ°åˆ†å‰²ç®¡é“ä¸­ã€‚</li>
<li>æå‡ºçš„CoSSeg-TTAæ¡†æ¶é’ˆå¯¹GED4æ¨¡æ€ï¼ŒåŸºäºnnU-Netv2å¹¶å¢å¼ºåŠç›‘ç£å‡å€¼æ•™å¸ˆæ–¹æ¡ˆï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡è®°ä½“ç§¯æ•°æ®ã€‚</li>
<li>é¢†åŸŸé€‚åº”æ¨¡å—é€šè¿‡éšæœºç›´æ–¹å›¾æ ·å¼çš„é£æ ¼è½¬æ¢åŠŸèƒ½å’Œå¯è®­ç»ƒçš„å¯¹æ¯”æ„ŸçŸ¥ç½‘ç»œï¼Œä¸°å¯Œäº†é¢†åŸŸå¤šæ ·æ€§å’Œå‡è½»äº†è·¨ä¸­å¿ƒå˜å¼‚ã€‚</li>
<li>é‡‡ç”¨æŒç»­æµ‹è¯•æ—¶é€‚åº”ç­–ç•¥ï¼Œæé«˜æ¨ç†æ—¶çš„ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCoSSeg-TTAæ¡†æ¶æ¯”nnU-Netv2åŸºçº¿æ›´ä¼˜ç§€ï¼Œå…·æœ‰æ›´é«˜çš„Diceåˆ†æ•°å’Œæ›´å°çš„Hausdorffè·ç¦»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3a3d7e77135245aaacf48d35f7b8cdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd93b6d3ead1a4ebf9ee116bf4377edf" align="middle">
<img src="https://picx.zhimg.com/v2-4759b31fe7fc13d067ccf4222b0acdc1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Fit-Pixels-Get-Labels-Meta-learned-Implicit-Networks-for-Image-Segmentation"><a href="#Fit-Pixels-Get-Labels-Meta-learned-Implicit-Networks-for-Image-Segmentation" class="headerlink" title="Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image   Segmentation"></a>Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image   Segmentation</h2><p><strong>Authors:Kushal Vyas, Ashok Veeraraghavan, Guha Balakrishnan</strong></p>
<p>Implicit neural representations (INRs) have achieved remarkable successes in learning expressive yet compact signal representations. However, they are not naturally amenable to predictive tasks such as segmentation, where they must learn semantic structures over a distribution of signals. In this study, we introduce MetaSeg, a meta-learning framework to train INRs for medical image segmentation. MetaSeg uses an underlying INR that simultaneously predicts per pixel intensity values and class labels. It then uses a meta-learning procedure to find optimal initial parameters for this INR over a training dataset of images and segmentation maps, such that the INR can simply be fine-tuned to fit pixels of an unseen test image, and automatically decode its class labels. We evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice scores comparable to commonly used U-Net models, but with $90%$ fewer parameters. MetaSeg offers a fresh, scalable alternative to traditional resource-heavy architectures such as U-Nets and vision transformers for medical image segmentation. Our project is available at <a target="_blank" rel="noopener" href="https://kushalvyas.github.io/metaseg.html">https://kushalvyas.github.io/metaseg.html</a> . </p>
<blockquote>
<p>éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰åœ¨å­¦ä¹ è¡¨è¾¾æ€§å¼ºä¸”ç´§å‡‘çš„ä¿¡å·è¡¨ç¤ºæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆå°±ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¹¶ä¸é€‚åˆç”¨äºé¢„æµ‹ä»»åŠ¡ï¼Œå¦‚åˆ†å‰²ä»»åŠ¡ï¼Œå¿…é¡»åœ¨å¤šç§ä¿¡å·åˆ†å¸ƒä¸Šå­¦ä¹ è¯­ä¹‰ç»“æ„ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MetaSegï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„å…ƒå­¦ä¹ æ¡†æ¶ã€‚MetaSegä½¿ç”¨ä¸€ä¸ªåŸºç¡€INRï¼ŒåŒæ—¶é¢„æµ‹åƒç´ å¼ºåº¦å€¼å’Œç±»åˆ«æ ‡ç­¾ã€‚ç„¶åå®ƒä½¿ç”¨ä¸€ä¸ªå…ƒå­¦ä¹ ç¨‹åºæ¥æ‰¾åˆ°å›¾åƒå’Œåˆ†å‰²å›¾è®­ç»ƒæ•°æ®é›†ä¸ŠINRçš„æœ€ä½³åˆå§‹å‚æ•°ï¼Œä»¥ä¾¿ç®€å•åœ°å¯¹çœ‹ä¸è§çš„æµ‹è¯•å›¾åƒè¿›è¡Œå¾®è°ƒï¼Œå¹¶è‡ªåŠ¨è§£ç å…¶ç±»åˆ«æ ‡ç­¾ã€‚æˆ‘ä»¬åœ¨äºŒç»´å’Œä¸‰ç»´è„‘éƒ¨MRIåˆ†å‰²ä»»åŠ¡ä¸Šè¯„ä¼°äº†MetaSegï¼ŒæŠ¥å‘Šçš„Diceå¾—åˆ†ä¸å¸¸ç”¨çš„U-Netæ¨¡å‹ç›¸å½“ï¼Œä½†å‚æ•°å‡å°‘äº†90%ã€‚MetaSegä¸ºä¼ ç»Ÿçš„èµ„æºå¯†é›†å‹æ¶æ„ï¼ˆå¦‚U-Netå’Œè§†è§‰è½¬æ¢å™¨ï¼‰æä¾›äº†ä¸€ç§æ–°çš„ã€å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://kushalvyas.github.io/metaseg.html%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://kushalvyas.github.io/metaseg.htmlä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04021v1">PDF</a> MICCAI 2025 (oral). Final peer-reviewed copy accessible at publisher   DOI <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-032-04947-6_19">https://link.springer.com/chapter/10.1007/978-3-032-04947-6_19</a> . Project   page, <a target="_blank" rel="noopener" href="https://kushalvyas.github.io/metaseg.html">https://kushalvyas.github.io/metaseg.html</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„å…ƒå­¦ä¹ æ¡†æ¶MetaSegï¼Œç»“åˆäº†éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰å’Œå…ƒå­¦ä¹ æŠ€æœ¯ã€‚MetaSegå¯è®­ç»ƒINRé¢„æµ‹åƒç´ å¼ºåº¦å’Œç±»åˆ«æ ‡ç­¾ï¼Œå¹¶åˆ©ç”¨å…ƒå­¦ä¹ æ‰¾åˆ°æœ€ä½³åˆå§‹å‚æ•°ï¼Œä½¿å¾—æ¨¡å‹èƒ½åœ¨æœªè§è¿‡çš„æµ‹è¯•å›¾åƒä¸Šè¿›è¡Œå¾®è°ƒå¹¶è‡ªåŠ¨è§£ç ç±»åˆ«æ ‡ç­¾ã€‚åœ¨2Då’Œ3Dè„‘MRIåˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒMetaSegè¡¨ç°ä¸U-Netæ¨¡å‹ç›¸å½“ï¼Œä½†å‚æ•°å‡å°‘äº†90%ã€‚å®ƒä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†ä¼ ç»Ÿèµ„æºå¯†é›†å‹æ¶æ„çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaSegç»“åˆäº†éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰å’Œå…ƒå­¦ä¹ ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>INRå¯ä»¥åŒæ—¶é¢„æµ‹åƒç´ å¼ºåº¦å’Œç±»åˆ«æ ‡ç­¾ã€‚</li>
<li>å…ƒå­¦ä¹ ç”¨äºæ‰¾åˆ°æœ€ä½³åˆå§‹å‚æ•°ï¼Œä½¿æ¨¡å‹èƒ½åœ¨æœªè§è¿‡çš„æµ‹è¯•å›¾åƒä¸Šå¿«é€Ÿé€‚åº”ã€‚</li>
<li>MetaSegåœ¨2Då’Œ3Dè„‘MRIåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸U-Netæ¨¡å‹ç›¸æ¯”ï¼ŒDiceå¾—åˆ†ç›¸å½“ã€‚</li>
<li>MetaSegçš„å‚æ•°æ•°é‡å‡å°‘äº†90%ï¼Œç›¸æ¯”ä¼ ç»Ÿæ¶æ„æ›´ä¸ºé«˜æ•ˆã€‚</li>
<li>MetaSegæä¾›äº†ä¸€ä¸ªå¯è¡Œä¸”å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºæ›¿ä»£èµ„æºå¯†é›†å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„ï¼Œå¦‚U-Netså’Œè§†è§‰è½¬æ¢å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ec583efee53041ea19ad8944d183a6e" align="middle">
<img src="https://picx.zhimg.com/v2-143bfb182088c930530db6b636b0d0c0" align="middle">
<img src="https://picx.zhimg.com/v2-7f5b56f1876a891ee8b8d875bccf68dd" align="middle">
<img src="https://picx.zhimg.com/v2-470ab63b187ecbaf2d320c02d4451294.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MambaCAFU-Hybrid-Multi-Scale-and-Multi-Attention-Model-with-Mamba-Based-Fusion-for-Medical-Image-Segmentation"><a href="#MambaCAFU-Hybrid-Multi-Scale-and-Multi-Attention-Model-with-Mamba-Based-Fusion-for-Medical-Image-Segmentation" class="headerlink" title="MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based   Fusion for Medical Image Segmentation"></a>MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based   Fusion for Medical Image Segmentation</h2><p><strong>Authors:T-Mai Bui, Fares Bougourzi, Fadi Dornaika, Vinh Truong Hoang</strong></p>
<p>In recent years, deep learning has shown near-expert performance in segmenting complex medical tissues and tumors. However, existing models are often task-specific, with performance varying across modalities and anatomical regions. Balancing model complexity and performance remains challenging, particularly in clinical settings where both accuracy and efficiency are critical. To address these issues, we propose a hybrid segmentation architecture featuring a three-branch encoder that integrates CNNs, Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture local, global, and long-range dependencies. A multi-scale attention-based CNN decoder reconstructs fine-grained segmentation maps while preserving contextual consistency. Additionally, a co-attention gate enhances feature selection by emphasizing relevant spatial and semantic information across scales during both encoding and decoding, improving feature interaction and cross-scale communication. Extensive experiments on multiple benchmark datasets show that our approach outperforms state-of-the-art methods in accuracy and generalization, while maintaining comparable computational complexity. By effectively balancing efficiency and effectiveness, our architecture offers a practical and scalable solution for diverse medical imaging tasks. Source code and trained models will be publicly released upon acceptance to support reproducibility and further research. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ åœ¨åˆ†å‰²å¤æ‚åŒ»å­¦ç»„ç»‡å’Œè‚¿ç˜¤æ–¹é¢è¡¨ç°å‡ºäº†æ¥è¿‘ä¸“å®¶çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å¾€å¾€æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ï¼Œåœ¨ä¸åŒæ¨¡æ€å’Œè§£å‰–åŒºåŸŸçš„æ€§èƒ½è¡¨ç°ä¸ä¸€ã€‚åœ¨ä¸´åºŠåŒ»å­¦ç¯å¢ƒä¸­ï¼Œå¹³è¡¡æ¨¡å‹çš„å¤æ‚æ€§å’Œæ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå‡†ç¡®æ€§å’Œæ•ˆç‡éƒ½è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆåˆ†å‰²æ¶æ„ï¼Œå®ƒé‡‡ç”¨ä¸‰åˆ†æ”¯ç¼–ç å™¨ï¼Œé›†æˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€Transformerå’ŒåŸºäºMambaçš„æ³¨æ„åŠ›èåˆï¼ˆMAFï¼‰æœºåˆ¶ï¼Œä»¥æ•æ‰å±€éƒ¨ã€å…¨å±€å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚åŸºäºå¤šå°ºåº¦æ³¨æ„åŠ›æœºåˆ¶çš„CNNè§£ç å™¨èƒ½å¤Ÿé‡å»ºç²¾ç»†çš„åˆ†å‰²å›¾ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒååŒæ³¨æ„åŠ›é—¨é€šè¿‡åœ¨ç¼–ç å’Œè§£ç è¿‡ç¨‹ä¸­å¼ºè°ƒè·¨å°ºåº¦çš„ç›¸å…³ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¢å¼ºäº†ç‰¹å¾é€‰æ‹©ï¼Œæé«˜äº†ç‰¹å¾äº¤äº’å’Œè·¨å°ºåº¦é€šä¿¡ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„è®¡ç®—å¤æ‚åº¦ã€‚é€šè¿‡æœ‰æ•ˆåœ°å¹³è¡¡æ•ˆç‡å’Œæ•ˆæœï¼Œæˆ‘ä»¬çš„æ¶æ„ä¸ºå„ç§åŒ»å­¦æˆåƒä»»åŠ¡æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚è®ºæ–‡è¢«æ¥å—åï¼Œæˆ‘ä»¬å°†å…¬å¼€æºä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä»¥æ”¯æŒå¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03786v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°æ¥è¿‘ä¸“å®¶æ°´å¹³ï¼Œä½†ä»å­˜åœ¨æ¨¡å‹ä»»åŠ¡ç‰¹å®šã€è·¨æ¨¡æ€å’Œè§£å‰–åŒºåŸŸæ€§èƒ½å·®å¼‚ç­‰é—®é¢˜ã€‚æå‡ºä¸€ç§æ··åˆåˆ†å‰²æ¶æ„ï¼ŒåŒ…å«ä¸‰åˆ†æ”¯ç¼–ç å™¨ï¼Œç»“åˆCNNã€Transformerå’ŒMambaåŸºäºæ³¨æ„åŠ›çš„èåˆæœºåˆ¶ï¼Œæ•è·å±€éƒ¨ã€å…¨å±€å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚å¤šå°ºåº¦æ³¨æ„åŠ›CNNè§£ç å™¨é‡å»ºç²¾ç»†åˆ†å‰²å›¾å¹¶ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸æ³›åŒ–æ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„è®¡ç®—å¤æ‚åº¦ã€‚æœ‰æ•ˆå¹³è¡¡æ•ˆç‡ä¸æ•ˆæœï¼Œä¸ºåŒ»å­¦æˆåƒä»»åŠ¡æä¾›å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†ä»å­˜åœ¨æ¨¡å‹ä»»åŠ¡ç‰¹å®šå’Œè·¨æ¨¡æ€æ€§èƒ½å·®å¼‚é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§æ··åˆåˆ†å‰²æ¶æ„ï¼ŒåŒ…å«ä¸‰åˆ†æ”¯ç¼–ç å™¨ï¼Œç»“åˆCNNã€Transformerå’ŒMambaåŸºäºæ³¨æ„åŠ›çš„èåˆæœºåˆ¶ã€‚</li>
<li>è¯¥æ¶æ„èƒ½å¤Ÿæ•è·å±€éƒ¨ã€å…¨å±€å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>å¤šå°ºåº¦æ³¨æ„åŠ›CNNè§£ç å™¨å¯é‡å»ºç²¾ç»†åˆ†å‰²å›¾å¹¶ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸æ³›åŒ–æ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è®¡ç®—å¤æ‚åº¦ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2be84782f7b8e783e8d46b238075c6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38f4688a9c090da15b0f35737b1d1c55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea9d8e0aa8fbcf1e9ce87d0440895779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b00abf05fa781fc028c0b45b1b8632e" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="How-We-Won-BraTS-SSA-2025-Brain-Tumor-Segmentation-in-the-Sub-Saharan-African-Population-Using-Segmentation-Aware-Data-Augmentation-and-Model-Ensembling"><a href="#How-We-Won-BraTS-SSA-2025-Brain-Tumor-Segmentation-in-the-Sub-Saharan-African-Population-Using-Segmentation-Aware-Data-Augmentation-and-Model-Ensembling" class="headerlink" title="How We Won BraTS-SSA 2025: Brain Tumor Segmentation in the Sub-Saharan   African Population Using Segmentation-Aware Data Augmentation and Model   Ensembling"></a>How We Won BraTS-SSA 2025: Brain Tumor Segmentation in the Sub-Saharan   African Population Using Segmentation-Aware Data Augmentation and Model   Ensembling</h2><p><strong>Authors:Claudia Takyi Ankomah, Livingstone Eli Ayivor, Ireneaus Nyame, Leslie Wambo, Patrick Yeboah Bonsu, Aondona Moses Iorumbur, Raymond Confidence, Toufiq Musah</strong></p>
<p>Brain tumors, particularly gliomas, pose significant chall-enges due to their complex growth patterns, infiltrative nature, and the variability in brain structure across individuals, which makes accurate diagnosis and monitoring difficult. Deep learning models have been developed to accurately delineate these tumors. However, most of these models were trained on relatively homogenous high-resource datasets, limiting their robustness when deployed in underserved regions. In this study, we performed segmentation-aware offline data augmentation on the BraTS-Africa dataset to increase the data sample size and diversity to enhance generalization. We further constructed an ensemble of three distinct architectures, MedNeXt, SegMamba, and Residual-Encoder U-Net, to leverage their complementary strengths. Our best-performing model, MedNeXt, was trained on 1000 epochs and achieved the highest average lesion-wise dice and normalized surface distance scores of 0.86 and 0.81 respectively. However, the ensemble model trained for 500 epochs produced the most balanced segmentation performance across the tumour subregions. This work demonstrates that a combination of advanced augmentation and model ensembling can improve segmentation accuracy and robustness on diverse and underrepresented datasets. Code available at: <a target="_blank" rel="noopener" href="https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti">https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti</a> </p>
<blockquote>
<p>è„‘è‚¿ç˜¤ï¼Œç‰¹åˆ«æ˜¯èƒ¶è´¨ç˜¤ï¼Œç”±äºå…¶å¤æ‚çš„ç”Ÿé•¿æ¨¡å¼ã€æµ¸æ¶¦æ€§å’Œä¸ªä½“é—´è„‘ç»“æ„çš„å˜åŒ–ï¼Œç»™å‡†ç¡®è¯Šæ–­å’Œæ²»ç–—å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹å·²è¢«å¼€å‘å‡ºæ¥ç²¾å‡†åœ°å‹¾ç”»è¿™äº›è‚¿ç˜¤ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹æ˜¯åœ¨ç›¸å¯¹å‡åŒ€ä¸”èµ„æºå……è¶³çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¿™åœ¨éƒ¨ç½²åˆ°æœåŠ¡ä¸è¶³çš„åœ°åŒºæ—¶é™åˆ¶äº†å…¶ç¨³å¥æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹BraTS-Africaæ•°æ®é›†è¿›è¡Œäº†åˆ†å‰²æ„ŸçŸ¥çš„ç¦»çº¿æ•°æ®å¢å¼ºï¼Œä»¥å¢åŠ æ•°æ®æ ·æœ¬é‡å’Œå¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ„å»ºäº†ä¸‰ç§ä¸åŒæ¶æ„çš„é›†åˆï¼šMedNeXtã€SegMambaå’ŒResidual-Encoder U-Netï¼Œä»¥åˆ©ç”¨å®ƒä»¬çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¡¨ç°æœ€ä½³çš„æ¨¡å‹MedNeXtè®­ç»ƒäº†1000ä¸ªå‘¨æœŸï¼Œå¹¶è·å¾—äº†æœ€é«˜çš„å¹³å‡ç—…å˜çº§diceè¯„åˆ†å’Œå½’ä¸€åŒ–è¡¨é¢è·ç¦»è¯„åˆ†ï¼Œåˆ†åˆ«ä¸º0.86å’Œ0.81ã€‚ç„¶è€Œï¼Œè®­ç»ƒäº†500ä¸ªå‘¨æœŸçš„é›†æˆæ¨¡å‹åœ¨è‚¿ç˜¤äºšåŒºäº§ç”Ÿäº†æœ€å¹³è¡¡çš„åˆ†å‰²æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå…ˆè¿›çš„å¢å¼ºæŠ€æœ¯å’Œæ¨¡å‹é›†æˆç›¸ç»“åˆå¯ä»¥æé«˜å¤šæ ·åŒ–å’Œä»£è¡¨æ€§ä¸è¶³çš„æ•°æ®é›†çš„åˆ†å‰²ç²¾åº¦å’Œç¨³å¥æ€§ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti">https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03568v1">PDF</a> Brain Tumor Segmentation Challenge, Medical Image Computing and   Computer Assisted Intervention (MICCAI) Conference, 11 Pages, 2 Figures, 2   Tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é’ˆå¯¹éæ´²æ•°æ®é›†BraTS-Africaè¿›è¡Œçº¿ä¸‹æ•°æ®å¢å¼ºï¼Œä»¥å¢åŠ æ ·æœ¬æ•°é‡å’Œå¤šæ ·æ€§ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºä¸‰ä¸ªä¸åŒæ¶æ„çš„æ¨¡å‹èåˆï¼Œå®ç°è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§æå‡ã€‚å…¶ä¸­ï¼ŒMedNeXtæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå¹³å‡ç—…ç¶çº§åˆ«çš„Diceç³»æ•°å’Œå½’ä¸€åŒ–è¡¨é¢è·ç¦»å¾—åˆ†åˆ†åˆ«ä¸º0.86å’Œ0.81ã€‚åŒæ—¶ï¼Œè®­ç»ƒ500è½®æ¬¡çš„èåˆæ¨¡å‹åœ¨è‚¿ç˜¤å„å­åŒºåŸŸåˆ†å‰²æ€§èƒ½ä¸Šè¡¨ç°æœ€å‡è¡¡ã€‚æœ¬ç ”ç©¶è¯æ˜äº†é«˜çº§æ•°æ®å¢å¼ºå’Œæ¨¡å‹èåˆçš„ç»„åˆå¯æœ‰æ•ˆæå‡åˆ†å‰²å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ ·æ€§å’Œä»£è¡¨æ€§ä¸è¶³çš„æ•°æ®é›†ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é’ˆå¯¹è„‘è‚¿ç˜¤ï¼Œç‰¹åˆ«æ˜¯èƒ¶è´¨ç˜¤çš„å¤æ‚ç”Ÿé•¿æ¨¡å¼å’Œä¸ªä½“å·®å¼‚å¸¦æ¥çš„è¯Šæ–­å›°éš¾ï¼Œé‡‡ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œè‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>ç ”ç©¶ä¸­å¯¹éæ´²æ•°æ®é›†BraTS-Africaè¿›è¡Œçº¿ä¸‹æ•°æ®å¢å¼ºï¼Œä»¥å¢åŠ æ ·æœ¬æ•°é‡å’Œå¤šæ ·æ€§ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ„å»ºäº†ä¸‰ä¸ªä¸åŒçš„æ¨¡å‹æ¶æ„å¹¶èåˆï¼Œä»¥æé«˜è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>MedNeXtæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œä½†èåˆæ¨¡å‹åœ¨è®­ç»ƒ500è½®æ¬¡æ—¶åœ¨è‚¿ç˜¤å„å­åŒºåŸŸåˆ†å‰²æ€§èƒ½ä¸Šè¡¨ç°æ›´å‡è¡¡ã€‚</li>
<li>èåˆæ¨¡å‹åœ¨å¤šæ ·æ€§å’Œä»£è¡¨æ€§ä¸è¶³çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè¾ƒé«˜çš„åˆ†å‰²å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶è¯æ˜äº†é«˜çº§æ•°æ®å¢å¼ºå’Œæ¨¡å‹èåˆçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a477cd74f96987e54bf378d5bbeb07fd" align="middle">
<img src="https://pic1.zhimg.com/v2-b6d6c24fe0734fb5bf2d5b163a639868.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a549fbdcb3d06a028aca374ff8655a2a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DuPLUS-Dual-Prompt-Vision-Language-Framework-for-Universal-Medical-Image-Segmentation-and-Prognosis"><a href="#DuPLUS-Dual-Prompt-Vision-Language-Framework-for-Universal-Medical-Image-Segmentation-and-Prognosis" class="headerlink" title="DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical   Image Segmentation and Prognosis"></a>DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical   Image Segmentation and Prognosis</h2><p><strong>Authors:Numan Saeed, Tausifa Jan Saleem, Fadillah Maani, Muhammad Ridzuan, Hu Wang, Mohammad Yaqub</strong></p>
<p>Deep learning for medical imaging is hampered by task-specific models that lack generalizability and prognostic capabilities, while existing â€˜universalâ€™ approaches suffer from simplistic conditioning and poor medical semantic understanding. To address these limitations, we introduce DuPLUS, a deep learning framework for efficient multi-modal medical image analysis. DuPLUS introduces a novel vision-language framework that leverages hierarchical semantic prompts for fine-grained control over the analysis task, a capability absent in prior universal models. To enable extensibility to other medical tasks, it includes a hierarchical, text-controlled architecture driven by a unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize across three imaging modalities, ten different anatomically various medical datasets, encompassing more than 30 organs and tumor types. It outperforms the state-of-the-art task specific and universal models on 8 out of 10 datasets. We demonstrate extensibility of its text-controlled architecture by seamless integration of electronic health record (EHR) data for prognosis prediction, and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI) of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks and modalities from varying centers, establishing DuPLUS as a versatile and clinically relevant solution for medical image analysis. The code for this work is made available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DuPLUS-6C52">https://anonymous.4open.science/r/DuPLUS-6C52</a> </p>
<blockquote>
<p>åŒ»ç–—å½±åƒæ·±åº¦å­¦ä¹ å—é™äºç¼ºä¹é€šç”¨æ€§å’Œé¢„åèƒ½åŠ›çš„ç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼Œè€Œç°æœ‰çš„â€œé€šç”¨â€æ–¹æ³•åˆ™å­˜åœ¨æ¡ä»¶è¿‡äºç®€å•å’ŒåŒ»ç–—è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†DuPLUSï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆå¤šæ¨¡å¼åŒ»ç–—å½±åƒåˆ†æçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚DuPLUSå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åˆ†å±‚è¯­ä¹‰æç¤ºå¯¹åˆ†æä»»åŠ¡è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œè¿™æ˜¯å…ˆå‰é€šç”¨æ¨¡å‹æ‰€ä¸å…·å¤‡çš„åŠŸèƒ½ã€‚ä¸ºäº†å®ç°æ‰©å±•åˆ°å…¶ä»–åŒ»ç–—ä»»åŠ¡ï¼Œå®ƒåŒ…æ‹¬ä¸€ç§åˆ†å±‚çš„æ–‡æœ¬æ§åˆ¶æ¶æ„ï¼Œè¯¥æ¶æ„ç”±ç‹¬ç‰¹çš„åŒé‡æç¤ºæœºåˆ¶é©±åŠ¨ã€‚å¯¹äºåˆ†å‰²ä»»åŠ¡ï¼ŒDuPLUSèƒ½å¤Ÿåœ¨ä¸‰ç§æˆåƒæ¨¡å¼ã€åä¸ªä¸åŒçš„è§£å‰–ç»“æ„åŒ»ç–—æ•°æ®é›†ä¸­è¿›è¡Œæ¨å¹¿ï¼Œæ¶µç›–äº†è¶…è¿‡30ä¸ªå™¨å®˜å’Œè‚¿ç˜¤ç±»å‹ã€‚å®ƒåœ¨8ä¸ªæ•°æ®é›†ä¸­çš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„ç‰¹å®šä»»åŠ¡å’Œé€šç”¨æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡æ— ç¼é›†æˆç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®æ¥è¿›è¡Œé¢„åé¢„æµ‹ï¼Œå±•ç¤ºäº†å…¶æ–‡æœ¬æ§åˆ¶æ¶æ„çš„æ‰©å±•æ€§ã€‚åœ¨å¤´é¢ˆç™Œæ•°æ®é›†ä¸­ï¼ŒDuPLUSçš„Concordance Indexï¼ˆCIï¼‰è¾¾åˆ°äº†0.69ã€‚å‚æ•°é«˜æ•ˆçš„å¾®è°ƒä½¿å¾—èƒ½å¤Ÿé€‚åº”æ–°ä»»åŠ¡å’Œæ¨¡å¼ä¸­å¿ƒçš„å„ç§å˜åŒ–ï¼Œç¡®ç«‹äº†DuPLUSåœ¨åŒ»ç–—å½±åƒåˆ†æé¢†åŸŸçš„é€šç”¨æ€§å’Œä¸´åºŠç›¸å…³æ€§è§£å†³æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œçš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DuPLUS-6C52">https://anonymous.4open.science/r/DuPLUS-6C52</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03483v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†ææ·±åº¦å­¦ä¹ æ¨¡å‹çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶DuPLUSã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è§†è¯­è¨€æ¡†æ¶ï¼Œåˆ©ç”¨å±‚æ¬¡è¯­ä¹‰æç¤ºå¯¹åˆ†æä»»åŠ¡è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œå¹¶åœ¨å¤šç§åŒ»å­¦æ•°æ®é›†ä¸Šå®ç°äº†è·¨æ¨¡æ€ã€è·¨è§£å‰–ç»“æ„çš„æ¨å¹¿ã€‚åŒæ—¶ï¼Œé€šè¿‡ç»“åˆç”µå­å¥åº·è®°å½•æ•°æ®ï¼Œå®ç°äº†é¢„åé¢„æµ‹ç­‰åŠŸèƒ½ï¼Œå±•ç°äº†å…¶é€šç”¨æ€§å’Œä¸´åºŠå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ·±åº¦å­¦ä¹ çš„å±€é™æ€§ï¼šç°æœ‰æ¨¡å‹ç¼ºä¹é€šç”¨æ€§å’Œé¢„åèƒ½åŠ›ï¼Œè€Œé€šç”¨æ¨¡å‹åˆ™å­˜åœ¨ç®€å•çš„æ¡ä»¶è®¾ç½®å’ŒåŒ»å­¦è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>DuPLUSæ¡†æ¶çš„å¼•å…¥ï¼šä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†DuPLUSæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆçš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†æã€‚</li>
<li>è§†è¯­è¨€æ¡†æ¶å’Œå±‚æ¬¡è¯­ä¹‰æç¤ºï¼šDuPLUSå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è§†è¯­è¨€æ¡†æ¶ï¼Œåˆ©ç”¨å±‚æ¬¡è¯­ä¹‰æç¤ºè¿›è¡Œç²¾ç»†çš„ä»»åŠ¡æ§åˆ¶ï¼Œè¿™æ˜¯å…ˆå‰é€šç”¨æ¨¡å‹æ‰€ç¼ºä¹çš„åŠŸèƒ½ã€‚</li>
<li>æ¨¡å‹çš„æ¨å¹¿å’Œæ€§èƒ½ï¼šDuPLUSèƒ½å¤Ÿåœ¨ä¸‰ç§æˆåƒæ¨¡æ€ã€åä¸ªä¸åŒçš„è§£å‰–ç»“æ„æ•°æ®é›†ä¸Šæ¨å¹¿ï¼Œæ¶µç›–è¶…è¿‡30ä¸ªå™¨å®˜å’Œè‚¿ç˜¤ç±»å‹ï¼Œå¹¶åœ¨8ä¸ªæ•°æ®é›†ä¸Šä¼˜äºæœ€æ–°çš„ä»»åŠ¡ç‰¹å®šå’Œé€šç”¨æ¨¡å‹ã€‚</li>
<li>ç”µå­å¥åº·è®°å½•æ•°æ®çš„æ•´åˆï¼šé€šè¿‡æ— ç¼é›†æˆç”µå­å¥åº·è®°å½•æ•°æ®ï¼ŒDuPLUSå®ç°äº†é¢„åé¢„æµ‹ç­‰åŠŸèƒ½ï¼Œå±•ç¤ºäº†å…¶æ–‡æœ¬æ§åˆ¶æ¶æ„çš„æ‰©å±•æ€§ã€‚</li>
<li>åœ¨å¤´é¢ˆç™Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼šåœ¨å¤´é¢ˆç™Œæ•°æ®é›†ä¸Šï¼ŒDuPLUSè¾¾åˆ°äº†0.69çš„åå’ŒæŒ‡æ•°ï¼Œæ˜¾ç¤ºäº†å…¶ä¸´åºŠå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f25244c1db868ea5c12acc3678de6a69" align="middle">
<img src="https://picx.zhimg.com/v2-d4bad778822430361e8f093aede1f941" align="middle">
<img src="https://picx.zhimg.com/v2-b5ad01b154fc6b721849fb3e6551a47b" align="middle">
<img src="https://picx.zhimg.com/v2-a754fd34bc4c23ae30558af09b2a2273" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-09/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ccc1bb0a66e53ab370baebef6409b083" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-09  TokenChain A Discrete Speech Chain via Semantic Token Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-09/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b25471fc451c86d9a2c2f4d32d007346.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-09  Periodontal Bone Loss Analysis via Keypoint Detection With Heuristic   Post-Processing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32298.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
