<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-10-09  EgoNight Towards Egocentric Vision Understanding at Night with a   Challenging Benchmark">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-2d6f0849da3a5124ffc267efb99c7524~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973651&auth_key=1759973651-0-0-9f95c946ef91eb301263ffdcdb1dca9b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-09-更新"><a href="#2025-10-09-更新" class="headerlink" title="2025-10-09 更新"></a>2025-10-09 更新</h1><h2 id="EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark"><a href="#EgoNight-Towards-Egocentric-Vision-Understanding-at-Night-with-a-Challenging-Benchmark" class="headerlink" title="EgoNight: Towards Egocentric Vision Understanding at Night with a   Challenging Benchmark"></a>EgoNight: Towards Egocentric Vision Understanding at Night with a   Challenging Benchmark</h2><p><strong>Authors:Deheng Zhang, Yuqian Fu, Runyi Yang, Yang Miao, Tianwen Qian, Xu Zheng, Guolei Sun, Ajad Chhatkuli, Xuanjing Huang, Yu-Gang Jiang, Luc Van Gool, Danda Pani Paudel</strong></p>
<p>Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance. </p>
<blockquote>
<p>现有针对以自我为中心的视觉理解基准测试大多侧重于白天场景，忽视了真实世界应用中不可避免的弱光条件。为了研究这一差距，我们推出了EgoNight，这是首个针对夜间以自我为中心的视觉的基准测试，核心任务为视觉问答（VQA）。EgoNight的关键特点是引入了日夜对齐的视频，这些视频利用白天数据提高了夜间注释质量，并揭示了不同照明条件之间的性能差距。为了实现这一点，我们收集了由Blender渲染的合成视频和真实世界录制，确保场景和动作在视觉和时间上的对齐。通过这些配对视频，我们构建了EgoNight-VQA，它由一个新型日间增强夜间自动标注引擎支持，并通过大量的人工验证进行完善。每个问答对都由注释者进行双重检查，以确保可靠性。总共有EgoNight-VQA包含3658个问答对，跨越90个视频，涵盖12种多样的问答类型，需要超过300小时的人工工作。对最先进的跨模态大型语言模型（MLLMs）的评估显示，从白天到夜晚的迁移导致性能大幅下降，这强调了在低光照条件下进行推理的挑战。除了VQA之外，EgoNight还引入了另外两个辅助任务，即日夜对应性检索和夜间以自我为中心的深度估计，进一步探索了现有模型的边界。我们相信EgoNight-VQA为以应用为导向的以自我为中心的视觉研究以及开发能够跨照明领域推广的模型提供了坚实的基础。所有数据代码将在接受后公开。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06218v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文关注现有以自我为中心的视觉理解基准测试主要关注白天场景，忽略了实际应用中不可避免的夜间低光条件的问题。因此，提出了EgoNight，这是首个针对夜间以自我为中心的视觉的全面基准测试，以视觉问答（VQA）为核心任务。EgoNight的关键在于引入了昼夜对齐的视频，利用白天数据提高夜间注释质量，并揭示了照明条件之间的性能差距。通过收集由Blender渲染的合成视频和真实世界录制，确保场景和动作在视觉和时间上的对齐。借助这些配对视频，构建了EgoNight-VQA，由新颖的白天增强夜间自动标签引擎支持，并通过大量的人工验证进行完善。每个问答对都经过注释者的双重检查以确保可靠性。EgoNight-VQA包含跨越12种不同问答类型的3658个问答对，跨越90个视频，总计超过300小时的人工工作。对最先进的跨模态大型语言模型的评估显示，从白天转移到夜晚时性能大幅下降，突显了低光条件下推理的挑战性。除了VQA之外，EgoNight还引入了昼夜对应检索和夜间以自我为中心的深度估计两个辅助任务，进一步探索现有模型的边界。我们相信EgoNight-VQA为以应用为导向的以自我为中心的研究提供了坚实的基础，并为开发能够适应照明域变化的模型铺平了道路。所有数据与代码将在审核通过后公布。</p>
<p><strong>要点总结</strong></p>
<ol>
<li>提出首个针对夜间以自我为中心的视觉基准测试——EgoNight。</li>
<li>专注于解决现有基准测试忽视夜间低光条件的问题。</li>
<li>以视觉问答（VQA）为核心任务，强调在夜间理解场景的重要性。</li>
<li>创新引入昼夜对齐的视频，以提高夜间注释质量和揭示照明条件的性能差距。</li>
<li>构建EgoNight-VQA数据集，包含合成视频和真实世界录制的视频，确保视觉和时间上的对齐。</li>
<li>利用白天数据增强夜间自动标签引擎，并通过大量人工验证完善数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06218">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c7de0e851ba614db66769530d9f5aae4.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-1065169af3f0d378c997fd83c63d76fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973666&auth_key=1759973666-0-0-b767541b46a20234c180c2b388048ae5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-e1eeb60f04f6d503bfd9024c6ba50046.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Stratified-GRPO-Handling-Structural-Heterogeneity-in-Reinforcement-Learning-of-LLM-Search-Agents"><a href="#Stratified-GRPO-Handling-Structural-Heterogeneity-in-Reinforcement-Learning-of-LLM-Search-Agents" class="headerlink" title="Stratified GRPO: Handling Structural Heterogeneity in Reinforcement   Learning of LLM Search Agents"></a>Stratified GRPO: Handling Structural Heterogeneity in Reinforcement   Learning of LLM Search Agents</h2><p><strong>Authors:Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia</strong></p>
<p>Large language model (LLM) agents increasingly rely on external tools such as search engines to solve complex, multi-step problems, and reinforcement learning (RL) has become a key paradigm for training them. However, the trajectories of search agents are structurally heterogeneous, where variations in the number, placement, and outcomes of search calls lead to fundamentally different answer directions and reward distributions. Standard policy gradient methods, which use a single global baseline, suffer from what we identify and formalize as cross-stratum bias-an “apples-to-oranges” comparison of heterogeneous trajectories. This cross-stratum bias distorts credit assignment and hinders exploration of complex, multi-step search strategies. To address this, we propose Stratified GRPO, whose central component, Stratified Advantage Normalization (SAN), partitions trajectories into homogeneous strata based on their structural properties and computes advantages locally within each stratum. This ensures that trajectories are evaluated only against their true peers. Our analysis proves that SAN eliminates cross-stratum bias, yields conditionally unbiased unit-variance estimates inside each stratum, and retains the global unbiasedness and unit-variance properties enjoyed by standard normalization, resulting in a more pure and scale-stable learning signal. To improve practical stability under finite-sample regimes, we further linearly blend SAN with the global estimator. Extensive experiments on diverse single-hop and multi-hop question-answering benchmarks demonstrate that Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3 points, achieving higher training rewards, greater training stability, and more effective search policies. These results establish stratification as a principled remedy for structural heterogeneity in RL for LLM search agents. </p>
<blockquote>
<p>大型语言模型（LLM）代理越来越依赖搜索引擎等外部工具来解决复杂的多步骤问题，强化学习（RL）已成为训练它们的关键范式。然而，搜索代理的轨迹在结构上具有异质性，搜索呼叫的数量、位置和结果的变化导致答案方向和奖励分布根本不同。标准策略梯度方法使用单一的全球基线，遭受我们确定和形式化的跨阶层偏见的影响，这是对异质轨迹的“苹果与橙子”的比较。这种跨阶层偏见扭曲了信用分配并阻碍了复杂的多步骤搜索策略的探索。</p>
</blockquote>
<p>为了解决这一问题，我们提出了分层GRPO，其核心组件分层优势归一化（SAN）根据结构特性将轨迹划分为同质的阶层，并在每个阶层内部进行局部优势计算。这确保了对轨迹的评估只针对其真正的同行。我们的分析证明，SAN消除了跨阶层偏见，在每个阶层内部产生条件无偏的单位方差估计，并保留了标准归一化所享有的全局无偏性和单位方差属性，从而产生更纯净和更稳定的学习信号。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06214v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）代理人越来越多地依赖外部工具，如搜索引擎来解决复杂的多步骤问题，强化学习（RL）已成为训练它们的关键范式。然而，搜索代理的轨迹在结构上具有异质性，搜索调用的数量、位置和结果的不同导致答案方向和奖励分布的根本差异。标准策略梯度方法使用单一全局基准线，存在跨层次偏差的问题，即异质轨迹的“苹果和橙子”的比较。这种跨层次偏差扭曲了信用分配并阻碍了复杂多步骤搜索策略的探索。为解决此问题，我们提出分层GRPO方法，其核心组件分层优势归一化（SAN）根据结构特性将轨迹划分为均匀的层次，并在每个层次内局部计算优势。确保轨迹只与其真正的同行进行评估。我们的分析证明SAN消除了跨层次偏差，在每个层次内产生条件无偏的单位方差估计，并保留了全局无偏性和单位方差属性，从而产生了更纯净和稳定的学习信号。为提高有限样本下的实际稳定性，我们将SAN与全局估计器进行线性混合。在多样的单跳和多跳问答基准测试上的实验表明，分层GRPO在各个方面均显著且持续优于GRPO，提高了训练奖励、训练稳定性和搜索策略的有效性。这些结果确立了分层作为解决RL中LLM搜索代理结构异质性的有效方法。</p>
<p><strong>关键发现</strong></p>
<ol>
<li>LLM代理人越来越多依赖外部工具如搜索引擎解决复杂问题，强化学习是训练关键。</li>
<li>搜索代理轨迹存在结构性异质性，影响答案方向和奖励分布。</li>
<li>标准策略梯度方法存在跨层次偏差问题。</li>
<li>提出分层GRPO方法，通过分层优势归一化（SAN）解决跨层次偏差问题。</li>
<li>SAN确保轨迹评估与其真实同行进行，消除跨层次偏差并产生更纯净稳定的学习信号。</li>
<li>SAN与全局估计器结合，提高有限样本下的实际稳定性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06214">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-994d2e85b586aaf3a1e6afaaf7fd091a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973680&auth_key=1759973680-0-0-311d1fba9937342c9e29c23f1d727e65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VecInfer-Efficient-LLM-Inference-with-Low-Bit-KV-Cache-via-Outlier-Suppressed-Vector-Quantization"><a href="#VecInfer-Efficient-LLM-Inference-with-Low-Bit-KV-Cache-via-Outlier-Suppressed-Vector-Quantization" class="headerlink" title="VecInfer: Efficient LLM Inference with Low-Bit KV Cache via   Outlier-Suppressed Vector Quantization"></a>VecInfer: Efficient LLM Inference with Low-Bit KV Cache via   Outlier-Suppressed Vector Quantization</h2><p><strong>Authors:Dingyu Yao, Chenxu Yang, Zhengyang Tong, Zheng Lin, Wei Liu, Jian Luan, Weiping Wang</strong></p>
<p>The Key-Value (KV) cache introduces substantial memory overhead during large language model (LLM) inference. Although existing vector quantization (VQ) methods reduce KV cache usage and provide flexible representational capacity across bit-widths, they suffer severe performance degradation at ultra-low bit-widths due to key cache outliers that hinder effective codebook utilization. To address this challenge, we propose VecInfer, a novel VQ method for aggressive KV cache compression while enabling efficient inference. By applying smooth and Hadamard transformations, VecInfer suppresses outliers in the key cache, enabling the codebook to comprehensively cover the original data distribution and thereby reducing quantization difficulty. To facilitate efficient deployment, we design an optimized CUDA kernel that fuses computation with dequantization to minimize memory access overhead. Extensive evaluations demonstrate that VecInfer consistently outperforms existing quantization baselines across both long-context understanding and mathematical reasoning tasks. With only 2-bit quantization, VecInfer achieves performance comparable to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length. </p>
<blockquote>
<p>键值（KV）缓存在大语言模型（LLM）推理过程中引入了较大的内存开销。尽管现有的向量量化（VQ）方法减少了KV缓存的使用，并在不同位宽下提供了灵活的表现能力，但在超低位宽下，由于关键缓存异常值阻碍了有效代码本的使用，它们遭受了严重的性能下降。为了解决这一挑战，我们提出了VecInfer，这是一种新型的VQ方法，用于激烈的KV缓存压缩，同时实现高效推理。通过应用平滑和哈达玛变换，VecInfer抑制了键缓存中的异常值，使代码本能够全面覆盖原始数据分布，从而降低量化难度。为了促进高效部署，我们设计了一个优化的CUDA内核，融合了计算与反量化，以最小化内存访问开销。广泛评估表明，VecInfer在长短文理解在数学推理任务中均持续优于现有量化基线。仅使用2位量化，VecInfer即可实现与全精度相当的性能，同时在Llama-3.1-8B的大批次自注意力计算中实现高达2.7倍的速度提升，在单批次端到端延迟上实现高达8.3倍的减少，序列长度达到19.6万。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06175v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在大型语言模型（LLM）推理过程中，键值（KV）缓存会带来巨大的内存开销。现有向量量化（VQ）方法虽能减少KV缓存使用并提供灵活的表示能力，但在超低位宽下却因关键缓存异常值而性能严重下降。为此，我们提出VecInfer，一种用于激烈KV缓存压缩的新型VQ方法，可实现高效推理。VecInfer通过应用平滑和哈达玛变换，抑制关键缓存中的异常值，使代码本能够全面覆盖原始数据分布，从而降低量化难度。为高效部署，我们设计了一个优化的CUDA内核，将计算与反量化相结合，以最小化内存访问开销。评估表明，VecInfer在长短文理解和数学推理任务上均优于现有量化基线。仅使用2位量化，VecInfer即可实现与全精度相当的性能，同时在Llama-3.1-8B的大型批次自注意力计算中实现了高达2.7倍的加速，并在单批次端到端延迟上实现了高达8.3倍的减少。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KV缓存在大规模语言模型（LLM）推理中引入显著内存开销。</li>
<li>现有VQ方法在超低位宽下性能严重下降，因为关键缓存异常值阻碍代码本有效利用。</li>
<li>VecInfer是一种新型VQ方法，旨在解决KV缓存压缩问题并实现高效推理。</li>
<li>VecInfer通过平滑和哈达玛变换抑制关键缓存中的异常值。</li>
<li>VecInfer优化CUDA内核，结合计算与反量化，降低内存访问开销。</li>
<li>VecInfer在长短文理解和数学推理任务上表现优于现有量化方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06175">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b35ecb4613a325d1f41e440afb4c87f2.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-12f2ead011b7ec49f76c70b25b4cf958~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973698&auth_key=1759973698-0-0-80a9a2a177a27c67c48edf713fff85e2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46606fe0e5b1c5a9a6014adfac818268~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973705&auth_key=1759973705-0-0-06c62af10ecb3c015dbdd7e12ba433c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-012bf2c170e4675b7203f96a759ad3d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973711&auth_key=1759973711-0-0-55a0d417e8bb0c0df774bd2e6a1cf35b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8128787773989d54ebe6cd71b7f624f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973718&auth_key=1759973718-0-0-1a850fb005345090b21b930272f8f2ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-914d7039bb79cade388f763719724501.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7003e762dc205e06ec45e1a1522293b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLMs-as-Policy-Agnostic-Teammates-A-Case-Study-in-Human-Proxy-Design-for-Heterogeneous-Agent-Teams"><a href="#LLMs-as-Policy-Agnostic-Teammates-A-Case-Study-in-Human-Proxy-Design-for-Heterogeneous-Agent-Teams" class="headerlink" title="LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design   for Heterogeneous Agent Teams"></a>LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design   for Heterogeneous Agent Teams</h2><p><strong>Authors:Aju Ani Justus, Chris Baber</strong></p>
<p>A critical challenge in modelling Heterogeneous-Agent Teams is training agents to collaborate with teammates whose policies are inaccessible or non-stationary, such as humans. Traditional approaches rely on expensive human-in-the-loop data, which limits scalability. We propose using Large Language Models (LLMs) as policy-agnostic human proxies to generate synthetic data that mimics human decision-making. To evaluate this, we conduct three experiments in a grid-world capture game inspired by Stag Hunt, a game theory paradigm that balances risk and reward. In Experiment 1, we compare decisions from 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and Mixtral 8x22B models. LLMs, prompted with game-state observations and reward structures, align more closely with experts than participants, demonstrating consistency in applying underlying decision criteria. Experiment 2 modifies prompts to induce risk-sensitive strategies (e.g. “be risk averse”). LLM outputs mirror human participants’ variability, shifting between risk-averse and risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic grid-world where the LLM agents generate movement actions. LLMs produce trajectories resembling human participants’ paths. While LLMs cannot yet fully replicate human adaptability, their prompt-guided diversity offers a scalable foundation for simulating policy-agnostic teammates. </p>
<blockquote>
<p>在模拟异质代理团队时，一个关键的挑战是训练代理与策略不可访问或非固定（如人类）的队友进行协作。传统的方法依赖于昂贵的人力闭环数据，这限制了可扩展性。我们建议使用大型语言模型（LLM）作为策略无关的人类代理，生成模仿人类决策的合成数据。为了评估这一点，我们在一个网格世界捕捉游戏中进行了三项实验，该游戏受到博弈论范式Stag Hunt的启发，平衡了风险和奖励。在第一个实验中，我们将来自30名人类参与者和2名专家评委的决策与LLaMA 3.1和Mixtral 8x22B模型的输出进行比较。使用游戏状态观察和奖励结构来提示LLM，其结果与专家相比更接近于人类参与者的决策，显示出在运用基本决策标准方面的一致性。第二个实验修改了提示来诱导风险敏感策略（例如，“规避风险”）。LLM的输出反映了人类参与者的变化性，在规避风险和寻求风险的行为之间转变。最后，第三个实验在动态网格世界中测试LLM，其中LLM代理生成移动动作。LLM产生的轨迹类似于人类参与者的路径。虽然LLM还不能完全复制人类的适应能力，但其提示引导的多样性为模拟策略无关队友提供了可扩展的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06151v1">PDF</a> This is a preprint of a paper presented at the \textit{European   Conference on Artificial Intelligence (ECAI 2025)}. It is made publicly   available for the benefit of the research community and should be regarded as   a preprint rather than a formally reviewed publication</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）被用于模拟人类决策，解决异质代理团队协作中的挑战。通过生成合成数据，模拟人类行为，并进行了三项实验验证。实验表明，LLMs能够在游戏理论范式中平衡风险与奖励，与专家决策更为一致，并展现出一定的风险敏感性。尽管无法完全复制人类的适应性，但LLMs的引导多样性为模拟政策无关的队友提供了可扩展的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）被应用于模拟人类决策，解决异质代理团队协作中的挑战。</li>
<li>LLMs通过生成合成数据，模拟人类行为。</li>
<li>在游戏理论范式中，LLMs展现出平衡风险与奖励的能力。</li>
<li>LLMs的决策与专家更为一致，相比人类参与者更具一致性。</li>
<li>通过调整提示，LLMs能够展现出风险敏感性。</li>
<li>LLMs的引导多样性为模拟政策无关的队友提供了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06151">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c0b2ac0e54c775bd4c3603e072e4d9c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973739&auth_key=1759973739-0-0-1dbe464935cd63e621b8582761c155f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-41c1bc8c3b42a014f1fc5092e20874ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973746&auth_key=1759973746-0-0-62579eaf454cc71ce9492def1e9262e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5168212b4c0f47ea237a16efce18ee2e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973753&auth_key=1759973753-0-0-afd8cca5532de16da175684d47f22ee4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da54d1de712f5ce47cfee04646ab01c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973759&auth_key=1759973759-0-0-d8b383fef5e38cddcd6d6f38c015be8e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-d2abc579267790fd97876b5423ca1392.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-568c1d2dc160a0160787f7fda0d129e2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RoSE-Round-robin-Synthetic-Data-Evaluation-for-Selecting-LLM-Generators-without-Human-Test-Sets"><a href="#RoSE-Round-robin-Synthetic-Data-Evaluation-for-Selecting-LLM-Generators-without-Human-Test-Sets" class="headerlink" title="RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators   without Human Test Sets"></a>RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators   without Human Test Sets</h2><p><strong>Authors:Jan Cegin, Branislav Pecher, Ivan Srba, Jakub Simko</strong></p>
<p>LLMs are powerful generators of synthetic data, which are used for training smaller, specific models. This is especially valuable for low-resource languages, where human-labelled data is scarce but LLMs can still produce high-quality text. However, LLMs differ in how useful their outputs are for training. Selecting the best LLM as a generator is challenging because extrinsic evaluation requires costly human annotations (which are often unavailable for low-resource languages), while intrinsic metrics correlate poorly with downstream performance. We introduce Round robin Synthetic data Evaluation (RoSE), a proxy metric for selecting the best LLM generator without human test sets. RoSE trains a small model on the outputs of a candidate generator (LLM) and then evaluates it on generated synthetic examples from all other candidate LLMs. The final RoSE score is the mean performance of this small model. Across six LLMs, eleven languages, and three tasks (sentiment, topic, intent), RoSE identifies the optimal generator more often than any other intrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within 0.76 percentage points of the optimal generator baseline. This result is measured in terms of downstream performance, obtained by training a small model on the chosen generator’s outputs (optimal vs. proxy metric selected) and evaluating it on human-labelled test data. Additionally, RoSE is the only metric to achieve a positive correlation with performance on human test data. </p>
<blockquote>
<p>大型语言模型（LLM）是强大的合成数据生成器，用于训练更小、更具体的模型。这在资源贫乏的语言中尤其有价值，因为这类语言的人类标注数据稀缺，但LLM仍然可以生成高质量的文本。然而，LLM在训练方面的输出效用存在差异。选择最佳LLM作为生成器是一项挑战，因为外在评估需要昂贵的人力标注（对于低资源语言通常无法获得），而内在指标与下游性能相关性很差。我们引入了轮转合成数据评估（RoSE），这是一种无需人工测试集的代理指标，用于选择最佳的LLM生成器。RoSE在一个候选生成器（LLM）的输出上训练一个小模型，然后在所有其他候选LLM生成的合成示例上对其进行评估。最终RoSE得分是这个小模型的平均性能。在六个LLM、十一种语言和三个任务（情感、主题、意图）中，RoSE比任何其他内在启发式方法更频繁地识别出最佳生成器。RoSE的表现优于内在启发式方法，并且与最佳生成器基准之间的差距缩小了0.76个百分点。这一结果是通过在所选生成器的输出上训练一个小模型（最佳与代理指标选择）并在人工测试数据上对其进行评估来衡量的。此外，RoSE是唯一与人类测试数据性能呈正相关性的指标。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06143v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong><br>    LLMs可生成用于训练小型特定模型的高质量合成数据，对低资源语言尤为有价值。然而，在选择最佳LLM生成器时面临挑战，因为外在评估需要昂贵的人力标注，而内在指标与下游性能相关性差。为此，我们提出了Round robin Synthetic data Evaluation（RoSE）方法，这是一种无需人工测试集的LLM生成器选择代理指标。RoSE在候选生成器（LLM）的输出上训练一个小模型，然后在所有其他候选LLM生成的示例上对其进行评估。最终RoSE得分是这个小模型的平均性能。在六个LLM、十一种语言和三个任务上，RoSE比任何其他内在启发式方法更能识别出最佳生成器。在下游性能上，RoSE的性能接近于最佳生成器基线。此外，RoSE是唯一与人类测试数据性能呈正相关性的指标。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs能够生成高质量合成数据，对低资源语言尤为有价值。</li>
<li>选择最佳LLM生成器具有挑战性，因为外在评估需要大量人力标注，而内在指标与下游性能相关性不强。</li>
<li>提出了RoSE方法作为无需人工测试集的LLM生成器选择代理指标。</li>
<li>RoSE通过在候选LLM的输出上训练小模型，并在其他候选LLM生成的示例上评估其性能来选择最佳生成器。</li>
<li>RoSE在多个LLM、语言和任务上的表现优于其他内在启发式方法，并接近最佳生成器的下游性能。</li>
<li>RoSE是唯一与人类测试数据性能呈正相关性的指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06143">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f92ac46e8e3ba3e5c870d593f7456a4b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973780&auth_key=1759973780-0-0-b652843ec716a6103fc5a3123de23d8c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-073b760b2f264e17eb0af6086d2f40fc~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973788&auth_key=1759973788-0-0-029a858a3a23b882bf96bb976e01c992&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-379cffdbaf1bfda3cac030946b77f65c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cc3fec8461926c055b8f4005f3f2ffc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="lm-Meter-Unveiling-Runtime-Inference-Latency-for-On-Device-Language-Models"><a href="#lm-Meter-Unveiling-Runtime-Inference-Latency-for-On-Device-Language-Models" class="headerlink" title="lm-Meter: Unveiling Runtime Inference Latency for On-Device Language   Models"></a>lm-Meter: Unveiling Runtime Inference Latency for On-Device Language   Models</h2><p><strong>Authors:Haoxin Wang, Xiaolong Tu, Hongyu Ke, Huirong Chai, Dawei Chen, Kyungtae Han</strong></p>
<p>Large Language Models (LLMs) are increasingly integrated into everyday applications, but their prevalent cloud-based deployment raises growing concerns around data privacy and long-term sustainability. Running LLMs locally on mobile and edge devices (on-device LLMs) offers the promise of enhanced privacy, reliability, and reduced communication costs. However, realizing this vision remains challenging due to substantial memory and compute demands, as well as limited visibility into performance-efficiency trade-offs on resource-constrained hardware. We propose lm-Meter, the first lightweight, online latency profiler tailored for on-device LLM inference. lm-Meter captures fine-grained, real-time latency at both phase (e.g., embedding, prefill, decode, softmax, sampling) and kernel levels without auxiliary devices. We implement lm-Meter on commercial mobile platforms and demonstrate its high profiling accuracy with minimal system overhead, e.g., only 2.58% throughput reduction in prefill and 0.99% in decode under the most constrained Powersave governor. Leveraging lm-Meter, we conduct comprehensive empirical studies revealing phase- and kernel-level bottlenecks in on-device LLM inference, quantifying accuracy-efficiency trade-offs, and identifying systematic optimization opportunities. lm-Meter provides unprecedented visibility into the runtime behavior of LLMs on constrained platforms, laying the foundation for informed optimization and accelerating the democratization of on-device LLM systems. Code and tutorials are available at <a target="_blank" rel="noopener" href="https://github.com/amai-gsu/LM-Meter">https://github.com/amai-gsu/LM-Meter</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在日常应用中的集成度越来越高，但它们普遍采用云部署方式引发了人们对数据隐私和长期可持续性的日益关注。在移动设备和边缘设备上本地运行LLM（即设备上的LLM）有望增强隐私、可靠性和降低通信成本。然而，由于巨大的内存和计算需求，以及在资源受限的硬件上性能效率权衡的可见性有限，实现这一愿景仍然具有挑战性。我们提出了lm-Meter，这是一款专为设备上的LLM推理量身定制的轻便型在线延迟分析器。lm-Meter能够捕获精细的实时延迟，包括阶段延迟（例如嵌入、预填充、解码、softmax、采样）和核心级别延迟，而无需额外的辅助设备。我们在商业移动平台上实现了lm-Meter，并展示了其高分析精度和极低的系统开销，例如在性能受限的Powersave管理器的管理下，预填充的吞吐量仅减少2.58%，解码减少0.99%。利用lm-Meter，我们进行了全面的实证研究，揭示了设备上LLM推理的阶段和核心级别瓶颈，量化了精度效率权衡，并发现了系统化的优化机会。lm-Meter提供了对受限平台上LLM运行时行为的前所未有的可见性，为优化提供了信息基础，并加速了设备上的LLM系统的普及。代码和教程可在<a target="_blank" rel="noopener" href="https://github.com/amai-gsu/LM-Meter%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/amai-gsu/LM-Meter获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06126v1">PDF</a> This is the preprint version of the paper accepted to The 10th   ACM&#x2F;IEEE Symposium on Edge Computing (SEC 2025)</p>
<p><strong>Summary</strong>：随着大型语言模型（LLM）在日常应用中的集成度不断提高，其基于云端的部署方式引发了关于数据隐私和长期可持续性的日益担忧。在移动设备和边缘设备上本地运行LLM提供了增强隐私、可靠性和降低通信成本的承诺。然而，由于巨大的内存和计算需求以及资源受限硬件上性能效率权衡的可见度有限，实现这一愿景仍然具有挑战性。为此，本文提出了lm-Meter，这是一款专为设备端LLM推理量身定制的轻量级在线延迟分析器。lm-Meter可以在不依赖辅助设备的情况下捕获精细的实时延迟数据，包括阶段层面（如嵌入、预填充、解码、softmax、采样）和内核层面。本文在商用移动平台上实现了lm-Meter，并展示了其高分析精度和低系统开销。利用lm-Meter进行的实证研究表明了设备端LLM推理中阶段和内核级别的瓶颈问题，量化了精度效率权衡，并发现了系统优化机会。lm-Meter提供了对受限平台上LLM运行时行为的前所未有的可见性，为优化提供了信息基础并加速了设备端LLM系统的民主化。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLMs）在日常应用中的集成度不断提高，但其云部署方式引发数据隐私和长期可持续性的担忧。</li>
<li>在移动和边缘设备上本地运行LLM（即on-device LLMs）具有增强隐私、可靠性和降低通信成本的潜力。</li>
<li>实现on-device LLMs面临巨大的内存和计算需求以及硬件资源限制的挑战。</li>
<li>lm-Meter是专为on-device LLM推理设计的轻量级在线延迟分析器，可捕获精细的实时延迟数据。</li>
<li>lm-Meter在商用移动平台上的实现展示了其高分析精度和低的系统开销。</li>
<li>利用lm-Meter进行的实证研究表明了阶段和内核级别的瓶颈问题，并发现了系统优化机会。</li>
<li>lm-Meter提供了对受限平台上LLM运行时行为的可见性，有助于优化并加速设备端LLM系统的民主化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06126">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-539fed4ac62438a900d749a97bfad6b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973809&auth_key=1759973809-0-0-2895e09ca564e498590fea86b1f3d55b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eed0c187b76784e34b506724be3962d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973816&auth_key=1759973816-0-0-0bef0f18537610a4534a4bcc61a607c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d17ea9bba095f626ee5b05827a17c09~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973823&auth_key=1759973823-0-0-018ba34a169f7b780a1c65f89063aea2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-8558177d0ace1e8fb66c85145454cc7c.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-71a8b345f461e41f3437c39bb7f8e34f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973836&auth_key=1759973836-0-0-522a48abfdce10396d92683c6e3f8d09&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Influence-Functions-for-Efficient-Data-Selection-in-Reasoning"><a href="#Influence-Functions-for-Efficient-Data-Selection-in-Reasoning" class="headerlink" title="Influence Functions for Efficient Data Selection in Reasoning"></a>Influence Functions for Efficient Data Selection in Reasoning</h2><p><strong>Authors:Prateek Humane, Paolo Cudrano, Daniel Z. Kaplan, Matteo Matteucci, Supriyo Chakraborty, Irina Rish</strong></p>
<p>Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows that a small amount of high-quality data can outperform massive datasets. Yet, what constitutes “quality” remains ill-defined. Existing reasoning methods rely on indirect heuristics such as problem difficulty or trace length, while instruction-tuning has explored a broader range of automated selection strategies, but rarely in the context of reasoning. We propose to define reasoning data quality using influence functions, which measure the causal effect of individual CoT examples on downstream accuracy, and introduce influence-based pruning, which consistently outperforms perplexity and embedding-based baselines on math reasoning within a model family. </p>
<blockquote>
<p>对思维链（CoT）数据微调大型语言模型（LLM）表明，少量高质量数据的表现可能优于大规模数据集。然而，“质量”的定义仍然不明确。现有的推理方法依赖于间接的启发式方法，如问题难度或跟踪长度，而指令微调已经探索了更广泛的自动选择策略，但很少在推理的语境下使用。我们提议使用影响函数来定义推理数据质量，影响函数可以衡量个别思维链示例对下游准确性的因果效应，并引入基于影响的修剪方法，该方法在模型家族内的数学推理上，表现优于困惑度和基于嵌入的基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06108v1">PDF</a> </p>
<p><strong>Summary</strong>：通过微调大型语言模型（LLM）在思维链（CoT）数据上，发现少量高质量数据的表现可能优于大规模数据集。然而，“质量”的定义仍然不明确。现有推理方法依赖于间接的启发式方法，如问题难度或跟踪长度，而指令调整则探索了更广泛的自动化选择策略，但很少在推理的情境中应用。我们提议使用影响函数来定义推理数据质量，它衡量单个思维链示例对下游准确性的因果效应，并引入基于影响力的修剪，它在模型家族的数学推理上始终优于困惑度和基于嵌入的基线。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>少量高质量数据在微调大型语言模型时可能表现优于大规模数据集。</li>
<li>现有推理方法在定义数据质量时主要依赖间接启发式方法。</li>
<li>推理数据质量可以通过影响函数来衡量，它反映了单个思维链示例对下游准确性的因果效应。</li>
<li>基于影响力的修剪方法在模型家族的数学推理任务上表现优异。</li>
<li>与困惑度和基于嵌入的基线相比，基于影响力的修剪具有显著优势。</li>
<li>这种方法为如何选择和优化用于微调大型语言模型的推理数据提供了新的视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06108">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2d6f0849da3a5124ffc267efb99c7524.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-61523c9da54f31dd9649892535ddbaf6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973851&auth_key=1759973851-0-0-f113841bf5ca20f4c8e0cad8784a34ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-60c397e43ef47adf17af649dc38c6d0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-303e6109dc43918b5665f65ab713d07e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Valley-of-Code-Reasoning-Scaling-Knowledge-Distillation-of-Large-Language-Models"><a href="#The-Valley-of-Code-Reasoning-Scaling-Knowledge-Distillation-of-Large-Language-Models" class="headerlink" title="The Valley of Code Reasoning: Scaling Knowledge Distillation of Large   Language Models"></a>The Valley of Code Reasoning: Scaling Knowledge Distillation of Large   Language Models</h2><p><strong>Authors:Muyu He, Muhammad Ali Shafique, Anand Kumar, Tsach Mackey, Nazneen Rajani</strong></p>
<p>Distilling the thinking traces of a Large Language Model (LLM) with reasoning capabilities into a smaller model has been proven effective. Yet, there is a scarcity of work done on how model performances scale with the quantity of distillation data. In this work, we study the scaling trend of distilling competitive coding skills on two small non-reasoning LLMs. We validate the hypothesis that there is a $\textit{valley of code reasoning}$: downstream performance on competitive coding first drops as data quantity increases, then it steadily increases in a sharper-than-log-linear fashion. Having identified the trend, we further fine-tune the models at two different distillation stages on the same data to ground conclusions on their respective learning phases. We learn that across stages in the low and medium-low data regimes, small models benefit significantly from easier coding questions than from harder ones. We also find that, surprisingly, the correctness of outputs in training data makes no difference to distillation outcomes. Our work represents a step forward in understanding the training dynamics of code reasoning distillation outside intuition </p>
<blockquote>
<p>将具有推理能力的大型语言模型（LLM）的思考轨迹蒸馏为较小的模型已经被证明是有效的。然而，关于模型性能如何随蒸馏数据量而变化的研究工作还很少。在这项工作中，我们研究了在两个小型的非推理型LLM上蒸馏竞争编程技能的规模趋势。我们验证了存在一个“代码推理的谷区”的假设：随着数据量的增加，下游竞争编程的性能首先会下降，然后会以比对数线性更快的方式稳定增长。确定了这一趋势后，我们在相同的数据上对两个不同蒸馏阶段的模型进行了微调，以对其各自的学习阶段得出相应的结论。我们发现，在低数据和中等低数据阶段，小型模型从更容易的编程问题中受益显著，而不是从更难的编程问题中受益。我们还惊讶地发现，训练数据中输出的正确性对蒸馏结果没有任何影响。我们的工作代表了理解代码推理蒸馏训练动力学的一个进步，超越了直觉的认知范围。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06101v1">PDF</a> NeurIPS 2025 Workshop on Deep Learning for Code (DL4C), Project page:   <a target="_blank" rel="noopener" href="https://collinear.ai/valley-of-reasoning">https://collinear.ai/valley-of-reasoning</a></p>
<p><strong>Summary</strong><br>大语言模型（LLM）的推理能力蒸馏到小型模型中被证明是有效的。本研究关注蒸馏数据量对模型性能的影响，发现在竞争编码任务中存在一个“代码推理谷”：随着数据量增加，下游性能先下降后呈快于对数线性的趋势上升。通过在不同蒸馏阶段使用相同数据进行微调，本研究确立了学习阶段对模型性能的影响。本研究发现，在低和中等低数据阶段，小模型更容易从简单的编码问题中受益，而非复杂问题。同时，令人惊讶的是，训练数据中输出的正确性对蒸馏结果没有影响。本研究是理解代码推理蒸馏训练动态方面的一个进步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型的推理能力可以通过蒸馏技术转移到小型模型。</li>
<li>在竞争编码任务中，存在一个“代码推理谷”，即随着数据量增加，模型性能先下降后上升。</li>
<li>在不同的蒸馏阶段使用相同数据进行微调有助于确立学习阶段对模型性能的影响。</li>
<li>在低和中等低数据阶段，小模型更容易从简单的编码问题中受益。</li>
<li>训练数据中输出的正确性对蒸馏结果没有显著影响。</li>
<li>本研究为理解代码推理蒸馏的训练动态提供了有价值的见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06101">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fb2250f968741cb505b80f1f00d80134~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973872&auth_key=1759973872-0-0-132f5f4e5c9bd641c1ddfd6b2bd9c2c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-586d4651e53e92a6c0134b2f19505edc~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973879&auth_key=1759973879-0-0-6bbce4fc52c44197ce5e32fa049a5c74&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-76bcdadcd6b0a7fdbb08951b6c63ceb0.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-692cdaa79b3c12339d2ecb76bab9d948~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973892&auth_key=1759973892-0-0-f593072f06acb462bb7c9e777a24ff90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Learning-from-Failures-Understanding-LLM-Alignment-through-Failure-Aware-Inverse-RL"><a href="#Learning-from-Failures-Understanding-LLM-Alignment-through-Failure-Aware-Inverse-RL" class="headerlink" title="Learning from Failures: Understanding LLM Alignment through   Failure-Aware Inverse RL"></a>Learning from Failures: Understanding LLM Alignment through   Failure-Aware Inverse RL</h2><p><strong>Authors:Nyal Patel, Matthieu Bou, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with human preferences, yet the underlying reward signals they internalize remain hidden, posing a critical challenge for interpretability and safety. Existing approaches attempt to extract these latent incentives using Inverse Reinforcement Learning (IRL), but treat all preference pairs equally, often overlooking the most informative signals: those examples the extracted reward model misclassifies or assigns nearly equal scores, which we term \emph{failures}. We introduce a novel \emph{failure-aware} IRL algorithm that focuses on misclassified or difficult examples to recover the latent rewards defining model behaviors. By learning from these failures, our failure-aware IRL extracts reward functions that better reflect the true objectives behind RLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines across multiple metrics when applied to LLM detoxification, without requiring external classifiers or supervision. Crucially, failure-aware IRL yields rewards that better capture the true incentives learned during RLHF, enabling more effective re-RLHF training than standard IRL. This establishes failure-aware IRL as a robust, scalable method for auditing model alignment and reducing ambiguity in the IRL process. </p>
<blockquote>
<p>强化学习从人类反馈（RLHF）使大型语言模型（LLM）符合人类偏好，但它们所内化的潜在奖励信号仍然保持隐藏，这为可解释性和安全性带来了重大挑战。现有方法试图使用逆向强化学习（IRL）提取这些潜在激励，但它们平等对待所有偏好对，往往忽略了最具有信息量的信号：即那些提取的奖励模型误分类或分配几乎相等分数的例子，我们称之为“失败”。我们引入了一种新型的“失败感知”IRL算法，该算法专注于误分类或难以分类的例子，以恢复定义模型行为的潜在奖励。通过从这些失败中学习，我们的失败感知IRL提取的奖励函数更好地反映了RLHF背后的真正目标。我们在LLM净化应用上展示了失败感知IRL在多个指标上的表现优于现有IRL基线，且无需外部分类器或监督。关键的是，失败感知IRL产生的奖励能更好地捕捉RLHF期间学习的真正激励，使标准RLHF训练更加有效。这证明了失败感知IRL是一种稳健且可扩展的方法，可用于审核模型对齐度并减少IRL过程中的模糊性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06092v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>强化学习从人类反馈（RLHF）使大型语言模型（LLM）符合人类偏好，但其内部化的奖励信号仍然隐藏，给解释性和安全性带来挑战。现有方法尝试使用逆向强化学习（IRL）提取潜在激励，但平等对待所有偏好对，往往忽略了最具有信息量的信号：那些提取的奖励模型误分类或分配近似分数的例子，我们称之为“失败”。我们引入了一种新型的失败感知的IRL算法，该算法专注于误分类或难以识别的例子，以恢复定义模型行为的潜在奖励。从失败中学习，我们的失败感知的IRL提取的奖励函数更好地反映了RLHF背后的真正目标。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLHF使LLM符合人类偏好，但奖励信号隐藏，带来解释性和安全性挑战。</li>
<li>现有方法平等对待所有偏好对，忽略最具信息量的信号——失败。</li>
<li>失败感知的IRL算法专注于误分类或难以识别的例子以恢复潜在奖励。</li>
<li>失败感知的IRL能从失败中学习，更好地反映RLHF背后的真正目标。</li>
<li>在LLM净化过程中，失败感知的IRL在多个指标上优于现有IRL基线。</li>
<li>失败感知的IRL不需要外部分类器或监督，具有鲁棒性和可扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06092">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-cc05a99efa3a2c9dc3b7e437b78a1e50~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973899&auth_key=1759973899-0-0-5f6d4502c6ec99f7d82f1a79903dd4c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-8cfdb2c0eaa2bd02210bf919e5897ed9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-09/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-141163230bd6dbbefd5a1ee5335ee6e7.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-10-09  ArchitectHead Continuous Level of Detail Control for 3D Gaussian Head   Avatars
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-02305d22bafeeeab946b271855424e24~resize:0:q75.jpg?source=1f5c5e47&expiration=1759945745&auth_key=1759945745-0-0-dcdd100f5695792c8d161590b5f6ed9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-10-09  Stratified GRPO Handling Structural Heterogeneity in Reinforcement   Learning of LLM Search Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
