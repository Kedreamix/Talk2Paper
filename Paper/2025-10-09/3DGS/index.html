<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-10-09  ArchitectHead Continuous Level of Detail Control for 3D Gaussian Head   Avatars">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-1123f4645c75e2d4591477e51bbec850~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967849&auth_key=1759967849-0-0-b5e3504b58e392d625b3e0afd5ea1daf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-09-更新"><a href="#2025-10-09-更新" class="headerlink" title="2025-10-09 更新"></a>2025-10-09 更新</h1><h2 id="ArchitectHead-Continuous-Level-of-Detail-Control-for-3D-Gaussian-Head-Avatars"><a href="#ArchitectHead-Continuous-Level-of-Detail-Control-for-3D-Gaussian-Head-Avatars" class="headerlink" title="ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head   Avatars"></a>ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head   Avatars</h2><p><strong>Authors:Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du</strong></p>
<p>3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on tens of thousands of 3D Gaussian points (Gaussians), with the number of Gaussians fixed after training. However, many practical applications require adjustable levels of detail (LOD) to balance rendering efficiency and visual quality. In this work, we propose “ArchitectHead”, the first framework for creating 3D Gaussian head avatars that support continuous control over LOD. Our key idea is to parameterize the Gaussians in a 2D UV feature space and propose a UV feature field composed of multi-level learnable feature maps to encode their latent features. A lightweight neural network-based decoder then transforms these latent features into 3D Gaussian attributes for rendering. ArchitectHead controls the number of Gaussians by dynamically resampling feature maps from the UV feature field at the desired resolutions. This method enables efficient and continuous control of LOD without retraining. Experimental results show that ArchitectHead achieves state-of-the-art (SOTA) quality in self and cross-identity reenactment tasks at the highest LOD, while maintaining near SOTA performance at lower LODs. At the lowest LOD, our method uses only 6.2% of the Gaussians while the quality degrades moderately (L1 Loss +7.9%, PSNR –0.97%, SSIM –0.6%, LPIPS Loss +24.1%), and the rendering speed nearly doubles. </p>
<blockquote>
<p>基于三维高斯理论（3DGS）的研究，已成功实现了三维头像的光照现实即时渲染技术。现有的基于3DGS的头像通常需要数万三维高斯点来完成渲染效果，且一旦训练完成，高斯点的数量就无法更改。然而在实际应用中，为了满足渲染效率和视觉质量的平衡，往往需要对细节层次（LOD）进行灵活调整。在此研究中，我们提出了“ArchitectHead”这一全新的框架，用以创建支持连续LOD控制的三维高斯头像。我们的核心理念在于在二维UV特征空间内对高斯参数进行设定，并提出由多层可学习特征图构成的UV特征场以编码其潜在特征。然后，一个轻量级的神经网络解码器将这些潜在特征转化为用于渲染的三维高斯属性。ArchitectHead通过动态从UV特征场重新采样特征图并调整至所需分辨率来控制高斯点的数量。这种方法能够在无需重新训练的情况下，实现对LOD的高效连续控制。实验结果表明，在最高LOD下，ArchitectHead在自我和跨身份重建任务中达到了领先水平（SOTA），同时在较低LOD下也保持了接近SOTA的性能表现。在最低LOD状态下，我们的方法仅使用6.2%的高斯点便能维持适中的质量下降（L1损失增加7.9%，PSNR下降0.97%，SSIM下降0.6%，LPIPS损失增加24.1%），同时渲染速度几乎翻倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05488v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于3D高斯Splatting（3DGS）的头像渲染技术。现有技术通常使用固定数量的高斯点，无法实现细节层次的调整（LOD）。本文提出的”ArchitectHead”框架首次实现了在3D高斯头像渲染中对LOD的连续控制。通过参数化二维UV特征空间的高斯，并使用多层可学习特征图编码潜在特征，结合轻量级神经网络解码器，实现了从潜在特征到3D高斯属性的转换。ArchitectHead通过动态重采样UV特征场的特征图来实现对高斯点数目的控制，从而在不同细节层次之间实现高效且连续的调整，无需重新训练。实验结果表明，ArchitectHead在最高细节层次上实现了先进的自我和跨身份重塑任务性能，同时在较低细节层次上保持了接近最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGS技术已用于创建具有真实感的实时渲染的3D头像。</li>
<li>现有技术使用固定数量的高斯点，难以实现细节层次的调整（LOD）。</li>
<li>ArchitectHead框架首次实现了基于3DGS的头像渲染中的LOD连续控制。</li>
<li>ArchitectHead通过参数化二维UV特征空间的高斯并使用多层可学习特征图进行编码。</li>
<li>使用轻量级神经网络解码器将潜在特征转换为3D高斯属性以实现渲染。</li>
<li>通过动态重采样UV特征场的特征图来控制Gaussians的数量，实现不同LOD之间的调整，无需重新训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05488">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f71fee7328486026e2d78f1ed78434ea.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-141163230bd6dbbefd5a1ee5335ee6e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967786&auth_key=1759967786-0-0-094f177dcc89b630a33252916ec5afb3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dfbb2b0f6d50a6565557fd32fd33bc8b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967794&auth_key=1759967794-0-0-64723229d2dfe693359be4461d44034f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Progressive-Gaussian-Transformer-with-Anisotropy-aware-Sampling-for-Open-Vocabulary-Occupancy-Prediction"><a href="#Progressive-Gaussian-Transformer-with-Anisotropy-aware-Sampling-for-Open-Vocabulary-Occupancy-Prediction" class="headerlink" title="Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open   Vocabulary Occupancy Prediction"></a>Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open   Vocabulary Occupancy Prediction</h2><p><strong>Authors:Chi Yan, Dan Xu</strong></p>
<p>The 3D occupancy prediction task has witnessed remarkable progress in recent years, playing a crucial role in vision-based autonomous driving systems. While traditional methods are limited to fixed semantic categories, recent approaches have moved towards predicting text-aligned features to enable open-vocabulary text queries in real-world scenes. However, there exists a trade-off in text-aligned scene modeling: sparse Gaussian representation struggles to capture small objects in the scene, while dense representation incurs significant computational overhead. To address these limitations, we present PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables open-vocabulary 3D occupancy prediction. Our framework employs progressive online densification, a feed-forward strategy that gradually enhances the 3D Gaussian representation to capture fine-grained scene details. By iteratively enhancing the representation, the framework achieves increasingly precise and detailed scene understanding. Another key contribution is the introduction of an anisotropy-aware sampling strategy with spatio-temporal fusion, which adaptively assigns receptive fields to Gaussians at different scales and stages, enabling more effective feature aggregation and richer scene information capture. Through extensive evaluations, we demonstrate that PG-Occ achieves state-of-the-art performance with a relative 14.3% mIoU improvement over the previous best performing method. Code and pretrained models will be released upon publication on our project page: <a target="_blank" rel="noopener" href="https://yanchi-3dv.github.io/PG-Occ">https://yanchi-3dv.github.io/PG-Occ</a> </p>
<blockquote>
<p>近年来，三维占用预测任务取得了显著进展，在基于视觉的自动驾驶系统中发挥了至关重要的作用。虽然传统方法局限于固定的语义类别，但最近的方法已经转向预测文本对齐的特征，以便在真实世界场景中实现开放式词汇文本查询。然而，文本对齐场景建模存在权衡：稀疏的高斯表示很难捕捉场景中的小物体，而密集表示则会产生巨大的计算开销。为了解决这些局限性，我们提出了PG-Occ，一种创新的渐进式高斯变换框架，可实现开放式词汇三维占用预测。我们的框架采用渐进式在线致密化策略，这是一种前馈策略，逐步增强三维高斯表示以捕捉场景的精细细节。通过迭代增强表示，该框架实现了越来越精确和详细的场景理解。另一个关键贡献是引入了具有时空融合的各向异性感知采样策略，该策略自适应地为不同尺度和阶段的高斯分配感受野，从而实现更有效的特征聚合和更丰富的场景信息捕获。通过广泛评估，我们证明PG-Occ达到了最先进的性能，相对于之前性能最佳的方法，相对提高了14.3%的mIoU。代码和预训练模型将在我们的项目页面发布时公布：<a target="_blank" rel="noopener" href="https://yanchi-3dv.github.io/PG-Occ%E3%80%82">https://yanchi-3dv.github.io/PG-Occ。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04759v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://yanchi-3dv.github.io/PG-Occ">https://yanchi-3dv.github.io/PG-Occ</a></p>
<p><strong>Summary</strong></p>
<p>近年来，三维占用预测任务在基于视觉的自动驾驶系统中取得了显著进展。传统方法受限于固定语义类别，而新方法趋向于预测文本对齐特征，以实现现实世界场景中的开放词汇文本查询。但文本对齐场景建模存在权衡：稀疏高斯表示难以捕捉场景中的小物体，而密集表示则带来显著的计算开销。为解决这些限制，我们提出了PG-Occ，一种创新的渐进式高斯变换框架，可实现开放词汇表的三维占用预测。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D occupancy prediction task has seen remarkable progress in recent years, crucial for autonomous driving systems.</li>
<li>传统方法受限于固定语义类别，而新方法预测文本对齐特征以实现开放词汇查询。</li>
<li>文本对齐场景建模存在稀疏与密集表示之间的权衡。</li>
<li>PG-Occ采用渐进式在线密化策略，逐步增强三维高斯表示以捕捉精细场景细节。</li>
<li>框架引入了一种具有时空融合功能的各向异性感知采样策略，能自适应地为不同尺度和阶段的高斯分配感受野。</li>
<li>通过评估，PG-Occ相较于之前最佳方法实现了相对14.3%的mIoU改进。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04759">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-dc2cf285e87d5b6a24da50dcce36452b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967801&auth_key=1759967801-0-0-cb4def01756b472ecf118904a8242e50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c8ea682333bd111afe81936beab9c491~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967809&auth_key=1759967809-0-0-93edb95fef10a2efe8ebf50df2cac0ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-622b45cd15e19db1451508809c2ee4a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967815&auth_key=1759967815-0-0-bb3962632c0c287ae5f577a1ea6de275&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-675e4946777a78abd34f7e5dc08b6293~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967822&auth_key=1759967822-0-0-0e4c95089b4476da6fb8edbbc873efec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Optimized-Minimal-4D-Gaussian-Splatting"><a href="#Optimized-Minimal-4D-Gaussian-Splatting" class="headerlink" title="Optimized Minimal 4D Gaussian Splatting"></a>Optimized Minimal 4D Gaussian Splatting</h2><p><strong>Authors:Minseo Lee, Byeonghyeon Lee, Lucas Yunkyu Lee, Eunsoo Lee, Sangmin Kim, Seunghyeon Song, Joo Chan Lee, Jong Hwan Ko, Jaesik Park, Eunbyung Park</strong></p>
<p>4D Gaussian Splatting has emerged as a new paradigm for dynamic scene representation, enabling real-time rendering of scenes with complex motions. However, it faces a major challenge of storage overhead, as millions of Gaussians are required for high-fidelity reconstruction. While several studies have attempted to alleviate this memory burden, they still face limitations in compression ratio or visual quality. In this work, we present OMG4 (Optimized Minimal 4D Gaussian Splatting), a framework that constructs a compact set of salient Gaussians capable of faithfully representing 4D Gaussian models. Our method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning to remove redundancies, and (3) Gaussian Merging to fuse primitives with similar characteristics. In addition, we integrate implicit appearance compression and generalize Sub-Vector Quantization (SVQ) to 4D representations, further reducing storage while preserving quality. Extensive experiments on standard benchmark datasets demonstrate that OMG4 significantly outperforms recent state-of-the-art methods, reducing model sizes by over 60% while maintaining reconstruction quality. These results position OMG4 as a significant step forward in compact 4D scene representation, opening new possibilities for a wide range of applications. Our source code is available at <a target="_blank" rel="noopener" href="https://minshirley.github.io/OMG4/">https://minshirley.github.io/OMG4/</a>. </p>
<blockquote>
<p>4D高斯贴图技术已经成为动态场景表示的新范式，能够实现复杂运动的实时渲染。然而，它面临存储开销的主要挑战，因为高保真重建需要数百万个高斯。尽管已有若干研究试图减轻这种内存负担，但它们仍在压缩率和视觉质量方面存在局限性。在这项工作中，我们提出了OMG4（优化最小4D高斯贴图），这是一种构建紧凑显著高斯集合的框架，能够忠实地代表4D高斯模型。我们的方法分三个阶段逐步删除高斯：（1）高斯采样，以识别对重建保真度至关重要的基本元素；（2）高斯修剪，以消除冗余；（3）高斯合并，以融合具有相似特征的基本元素。此外，我们整合了隐式外观压缩并将子向量量化（SVQ）推广到4D表示，进一步减少了存储量同时保持了质量。在标准基准数据集上的广泛实验表明，OMG4显著优于最新的先进技术，在保持重建质量的同时将模型大小减少了60%以上。这些成果标志着OMG4在紧凑4D场景表示方面取得了重大进展，为广泛的应用程序打开了新的可能性。我们的源代码可在<a target="_blank" rel="noopener" href="https://minshirley.github.io/OMG4/">https://minshirley.github.io/OMG4/</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03857v1">PDF</a> 17 pages, 8 figures</p>
<p><strong>Summary</strong><br>     优化后的最小四维高斯采样技术OMG4成功解决了存储冗余问题，在保持重建质量的同时显著减少了模型大小。该技术通过关键高斯采样、去除冗余和高斯融合等三个阶段逐步优化四维高斯模型，集成了隐式外观压缩技术并扩展了四维表示的子向量量化技术。实验结果证明了OMG4的显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OMG4是一种针对四维高斯模型的有效框架，通过构建紧凑的关键高斯集来忠实表示四维场景。</li>
<li>OMG4采用三个阶段的优化方法：关键高斯采样、去除冗余和高斯融合。</li>
<li>集成隐式外观压缩技术以进一步减少存储需求。</li>
<li>扩展了子向量量化技术至四维表示，以在保持质量的同时降低存储需求。</li>
<li>实验结果表明，OMG4显著优于最新的先进技术，在减少模型大小的同时保持重建质量。</li>
<li>OMG4技术对于紧凑四维场景表示具有重大意义，为广泛的应用提供了新的可能性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-60cef81d78634eb1c04273250d235020~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967829&auth_key=1759967829-0-0-95360030b2365b1120609658bde1b170&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-9c11e08bf10ca056f912b99ee17eef48.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-fc56bb5f6125901e31e4397ce0ef357f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967843&auth_key=1759967843-0-0-247b274aa20572fa43aaec9b1d0c6207&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1123f4645c75e2d4591477e51bbec850~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967849&auth_key=1759967849-0-0-b5e3504b58e392d625b3e0afd5ea1daf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HBSplat-Robust-Sparse-View-Gaussian-Reconstruction-with-Hybrid-Loss-Guided-Depth-and-Bidirectional-Warping"><a href="#HBSplat-Robust-Sparse-View-Gaussian-Reconstruction-with-Hybrid-Loss-Guided-Depth-and-Bidirectional-Warping" class="headerlink" title="HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss   Guided Depth and Bidirectional Warping"></a>HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss   Guided Depth and Bidirectional Warping</h2><p><strong>Authors:Yu Ma, Guoliang Wei, Yue Cheng</strong></p>
<p>Novel View Synthesis (NVS) from sparse views presents a formidable challenge in 3D reconstruction, where limited multi-view constraints lead to severe overfitting, geometric distortion, and fragmented scenes. While 3D Gaussian Splatting (3DGS) delivers real-time, high-fidelity rendering, its performance drastically deteriorates under sparse inputs, plagued by floating artifacts and structural failures. To address these challenges, we introduce HBSplat, a unified framework that elevates 3DGS by seamlessly integrating robust structural cues, virtual view constraints, and occluded region completion. Our core contributions are threefold: a Hybrid-Loss Depth Estimation module that ensures multi-view consistency by leveraging dense matching priors and integrating reprojection, point propagation, and smoothness constraints; a Bidirectional Warping Virtual View Synthesis method that enforces substantially stronger constraints by creating high-fidelity virtual views through bidirectional depth-image warping and multi-view fusion; and an Occlusion-Aware Reconstruction component that recovers occluded areas using a depth-difference mask and a learning-based inpainting model. Extensive evaluations on LLFF, Blender, and DTU benchmarks validate that HBSplat sets a new state-of-the-art, achieving up to 21.13 dB PSNR and 0.189 LPIPS, while maintaining real-time inference. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/eternalland/HBSplat">https://github.com/eternalland/HBSplat</a>. </p>
<blockquote>
<p>在3D重建中，从稀疏视角进行Novel View Synthesis（NVS）是一项巨大的挑战。有限的多视角约束会导致严重的过度拟合、几何失真和场景碎片化。虽然3D Gaussian Splatting（3DGS）可以实现实时、高保真渲染，但在稀疏输入的情况下，其性能会急剧下降，受到浮动伪影和结构失败的困扰。为了解决这些挑战，我们引入了HBSplat这一统一框架，它通过无缝集成稳健的结构线索、虚拟视图约束和遮挡区域完成来提升3DGS。我们的核心贡献有三点：一是Hybrid-Loss深度估计模块，它通过利用密集匹配先验知识并整合再投影、点传播和平滑约束来确保多视角一致性；二是双向扭曲虚拟视图合成方法，它通过双向深度图像扭曲和多视角融合来创建高保真虚拟视图，从而实施更严格的约束；三是遮挡感知重建组件，它使用深度差异掩膜和基于学习的修复模型来恢复遮挡区域。在LLFF、Blender和DTU基准测试上的广泛评估证明，HBSplat达到了最新技术水准，实现了高达21.13 dB的PSNR和0.189的LPIPS指标，同时保持实时推理。相关代码可访问：<a target="_blank" rel="noopener" href="https://github.com/eternalland/HBSplat%E3%80%82">https://github.com/eternalland/HBSplat。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24893v2">PDF</a> 14 pages, 21 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对稀疏视角下的Novel View Synthesis（NVS）的挑战，通过引入HBSplat框架来提升3DGS性能的方法。该方法结合了结构线索、虚拟视图约束和遮挡区域完成技术，通过Hybrid-Loss深度估计模块、双向扭曲虚拟视图合成方法和遮挡感知重建组件等技术手段，实现了高保真度的虚拟视图合成和遮挡区域的恢复。在LLFF、Blender和DTU等多个数据集上的实验结果表明，HBSplat达到了实时性能的新水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了稀疏视角下的Novel View Synthesis（NVS）的挑战性问题，如过拟合、几何失真和场景碎片化等。</li>
<li>指出3D Gaussian Splatting（3DGS）在稀疏输入下的性能问题，包括浮动伪影和结构失败等问题。</li>
<li>提出了一种新的统一框架HBSplat，通过集成结构线索、虚拟视图约束和遮挡区域完成技术，解决上述问题。</li>
<li>Hybrid-Loss深度估计模块实现了多视图一致性，利用密集匹配先验，并整合重投影、点传播和平滑约束等技术。</li>
<li>Bidirectional Warping Virtual View Synthesis方法创建高质量虚拟视图，通过双向深度图像扭曲和多视图融合施加更强的约束。</li>
<li>Occlusion-Aware Reconstruction组件使用深度差异掩膜和学习型修复模型恢复遮挡区域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24893">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-34729b811312e45a02d8d42112aa1971.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4047354873e66c55bfc6f29fdb6fe8d.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6787e0ade10462ef56890f1ed8d0a10~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967871&auth_key=1759967871-0-0-d38f2536a18c55d8a28c67ef853de80b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dea8bb9fc95c252b0455e730b8d9c4a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967878&auth_key=1759967878-0-0-0aef9f4059c714c8b133d8b47307fc5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-234fba4d90b57f316fe4080445eb7e76.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ExGS-Extreme-3D-Gaussian-Compression-with-Diffusion-Priors"><a href="#ExGS-Extreme-3D-Gaussian-Compression-with-Diffusion-Priors" class="headerlink" title="ExGS: Extreme 3D Gaussian Compression with Diffusion Priors"></a>ExGS: Extreme 3D Gaussian Compression with Diffusion Priors</h2><p><strong>Authors:Jiaqi Chen, Xinhao Ji, Yuanyuan Gao, Hao Li, Yuning Gong, Yifei Liu, Dan Xu, Zhihang Zhong, Dingwen Zhang, Xiao Sun</strong></p>
<p>Neural scene representations, such as 3D Gaussian Splatting (3DGS), have enabled high-quality neural rendering; however, their large storage and transmission costs hinder deployment in resource-constrained environments. Existing compression methods either rely on costly optimization, which is slow and scene-specific, or adopt training-free pruning and quantization, which degrade rendering quality under high compression ratios. In contrast, recent data-driven approaches provide a promising direction to overcome this trade-off, enabling efficient compression while preserving high rendering quality. We introduce ExGS, a novel feed-forward framework that unifies Universal Gaussian Compression (UGC) with GaussPainter for Extreme 3DGS compression. UGC performs re-optimization-free pruning to aggressively reduce Gaussian primitives while retaining only essential information, whereas GaussPainter leverages powerful diffusion priors with mask-guided refinement to restore high-quality renderings from heavily pruned Gaussian scenes. Unlike conventional inpainting, GaussPainter not only fills in missing regions but also enhances visible pixels, yielding substantial improvements in degraded renderings. To ensure practicality, it adopts a lightweight VAE and a one-step diffusion design, enabling real-time restoration. Our framework can even achieve over 100X compression (reducing a typical 354.77 MB model to about 3.31 MB) while preserving fidelity and significantly improving image quality under challenging conditions. These results highlight the central role of diffusion priors in bridging the gap between extreme compression and high-quality neural rendering. Our code repository will be released at: <a target="_blank" rel="noopener" href="https://github.com/chenttt2001/ExGS">https://github.com/chenttt2001/ExGS</a> </p>
<blockquote>
<p>神经场景表示，如3D高斯插值（3DGS），已经实现了高质量神经渲染；然而，其较大的存储和传输成本阻碍了其在资源受限环境中的部署。现有的压缩方法要么依赖于昂贵且耗时的优化，要么是特定场景的，或者采用无训练裁剪和量化，这在高压缩比下会降低渲染质量。相比之下，最近的数据驱动方法为解决这一权衡提供了有前景的方向，能够在保持高质量渲染的同时实现有效的压缩。我们介绍了ExGS，这是一种新型前馈框架，它将通用高斯压缩（UGC）与GaussPainter相结合，用于极端3DGS压缩。UGC通过无优化再处理的裁剪方法大幅度减少高斯原始数据，同时仅保留必要信息；而GaussPainter则利用强大的扩散先验和掩膜导向细化，从高度裁剪的高斯场景中恢复高质量渲染。不同于传统的补全技术，GaussPainter不仅填补缺失区域，还增强可见像素，在渲染质量下降的情况下实现显著改进。为确保实用性，它采用了轻量级的VAE和一步扩散设计，实现实时恢复。我们的框架甚至可以在保持保真度的同时实现超过100倍的压缩（将一个典型的354.77 MB模型缩减至约3.31 MB），并在具有挑战性的条件下显著提高图像质量。这些结果突显了扩散先验在极端压缩与高质量神经渲染之间的桥梁作用。我们的代码仓库将在：<a target="_blank" rel="noopener" href="https://github.com/chenttt2001/ExGS%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/chenttt2001/ExGS发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24758v4">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>神经场景表示（如3D高斯拼贴技术）实现了高质量神经渲染，但其存储和传输成本较高，限制了其在资源受限环境中的部署。现有压缩方法要么依赖于耗时且场景特定的优化，要么采用无训练裁剪和量化，这在高压缩比下会降低渲染质量。相反，最近的数据驱动方法为平衡压缩效率和渲染质量提供了希望。本研究介绍了一种新型前馈框架ExGS，它将通用高斯压缩（UGC）与GaussPainter相结合，用于实现极端3DGS压缩。UGC通过无优化裁剪大幅减少高斯原始数据，同时保留必要信息；而GaussPainter则利用强大的扩散先验和遮罩引导细化，从高度裁剪的高斯场景中恢复高质量渲染。与常规补全不同，GaussPainter不仅填补缺失区域，还增强可见像素，显著改善退化渲染质量。为确保实用性，它采用轻量级VAE和一步扩散设计，实现实时恢复。本框架甚至能在保持保真度的同时实现超过100倍的压缩（将典型的354.77MB模型缩减至约3.31MB），并在挑战条件下显著提高图像质量。这些结果突显扩散先验在弥合极端压缩与高质量神经渲染之间的差距中的关键作用。我们的代码仓库将在<a target="_blank" rel="noopener" href="https://github.com/chenttt2001/ExGS%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/chenttt2001/ExGS发布。</a></p>
<p><strong>关键发现</strong></p>
<ol>
<li>神经场景表示（如3DGS）实现了高质量神经渲染，但存在存储和传输成本高的挑战。</li>
<li>现有压缩方法面临优化成本高昂或渲染质量下降的权衡问题。</li>
<li>数据驱动方法能在保持高质量渲染的同时实现高效压缩。</li>
<li>新型前馈框架ExGS结合了通用高斯压缩（UGC）和GaussPainter技术，实现极端3DGS压缩。</li>
<li>UGC通过无优化裁剪保留必要信息，而GaussPainter利用扩散先验进行高质量渲染恢复。</li>
<li>GaussPainter不仅填补缺失区域，还增强可见像素，改善退化渲染质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24758">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b6de9aa0b0cb3a3807cc62feb7b2cf4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6529708a509e023a787c68d2b7c1600d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58c8c2375758fc99b6352ca92e698ffb.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c88ffa60e1b77e3bc2c14f7f135ced27~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967911&auth_key=1759967911-0-0-af42279d2eb6cc5c961c6e20fa54b23c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OracleGS-Grounding-Generative-Priors-for-Sparse-View-Gaussian-Splatting"><a href="#OracleGS-Grounding-Generative-Priors-for-Sparse-View-Gaussian-Splatting" class="headerlink" title="OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting"></a>OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting</h2><p><strong>Authors:Atakan Topaloglu, Kunyi Li, Michael Niemeyer, Nassir Navab, A. Murat Tekalp, Federico Tombari</strong></p>
<p>Sparse-view novel view synthesis is fundamentally ill-posed due to severe geometric ambiguity. Current methods are caught in a trade-off: regressive models are geometrically faithful but incomplete, whereas generative models can complete scenes but often introduce structural inconsistencies. We propose OracleGS, a novel framework that reconciles generative completeness with regressive fidelity for sparse view Gaussian Splatting. Instead of using generative models to patch incomplete reconstructions, our “propose-and-validate” framework first leverages a pre-trained 3D-aware diffusion model to synthesize novel views to propose a complete scene. We then repurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the 3D uncertainties of generated views, using its attention maps to reveal regions where the generated views are well-supported by multi-view evidence versus where they fall into regions of high uncertainty due to occlusion, lack of texture, or direct inconsistency. This uncertainty signal directly guides the optimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss. Our approach conditions the powerful generative prior on multi-view geometric evidence, filtering hallucinatory artifacts while preserving plausible completions in under-constrained regions, outperforming state-of-the-art methods on datasets including Mip-NeRF 360 and NeRF Synthetic. </p>
<blockquote>
<p>稀疏视角下的新型视图合成因严重的几何模糊性而本质上是不适定的。当前的方法陷入了权衡之中：回归模型在几何上忠实但不完整，而生成模型可以完成场景但经常引入结构不一致性。我们提出了OracleGS，这是一个新的框架，旨在调和生成模型的完整性与回归模型的忠实性，用于稀疏视角的高斯拼贴。我们并不使用生成模型来修复不完整的重建，而是采用“提出并验证”的框架。首先，我们利用预训练的3D感知扩散模型合成新型视图来提出一个完整的场景。然后，我们将多视角立体（MVS）模型重新用作一个3D感知的“专家”，以验证生成视图的3D不确定性，并使用其注意力图来揭示生成视图在多视角证据支持良好的区域，以及因遮挡、缺乏纹理或直接不一致而陷入高度不确定性的区域。这种不确定性信号直接指导了通过不确定性加权损失对3D高斯拼贴模型的优化。我们的方法以多视角几何证据为条件，对生成模型进行强大的先验约束，过滤掉幻觉伪影，同时在约束不足的区域内保留合理的完成度，在包括Mip-NeRF 360和NeRF Synthetic在内的数据集上的表现优于当前的主流方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23258v2">PDF</a> Project page available at:   <a target="_blank" rel="noopener" href="https://atakan-topaloglu.github.io/oraclegs/">https://atakan-topaloglu.github.io/oraclegs/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了OracleGS框架，融合了生成模型的完整性与回归模型的忠实性，用于稀疏视角的高斯Splatting。该框架采用预训练的3D感知扩散模型合成新视角以提出完整场景，并利用多视角立体（MVS）模型作为3D感知的oracle验证生成的视角的3D不确定性。这种不确定性信号直接指导了通过不确定性加权损失优化的3D高斯Splatting模型。该框架以多视角几何证据为条件，在生成模型中融入合理的先验知识，有效过滤了虚构的伪像，同时保留了可能完成未约束区域的场景，超越了包括Mip-NeRF 360和NeRF合成数据集在内的现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OracleGS解决了稀疏视角下的视图合成问题，结合了生成模型的完整性和回归模型的忠实性。</li>
<li>提出了一种“propose-and-validate”框架，利用预训练的3D感知扩散模型提出完整场景，并使用多视角立体模型验证其不确定性。</li>
<li>利用注意力图揭示生成视图与多视角证据之间的对应关系，以及因遮挡、缺乏纹理或直接不一致导致的高不确定性区域。</li>
<li>不确定性信号指导了通过不确定性加权损失优化的3D高斯Splatting模型。</li>
<li>该方法以多视角几何证据为条件，将强大的生成先验融入其中，过滤了虚构的伪像。</li>
<li>该方法在包括Mip-NeRF 360和NeRF合成数据集在内的数据集上超越了现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-93e879e85b4aa1a111e729bd943e9284.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21090328a2b5b6ea458ec253a1247bb9.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa794c0d0af6b17a06cafe9084a461a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967933&auth_key=1759967933-0-0-a66df15b5d2f15df66e6d2619a66bd5b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-367441bcab795e72f6f6bb0db1f8356a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967940&auth_key=1759967940-0-0-d9f90b8c4bc79a2403ef33007bde3f4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-6ddfa89b953479a371b0872ea4600326.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Robust-Neural-Rendering-in-the-Wild-with-Asymmetric-Dual-3D-Gaussian-Splatting"><a href="#Robust-Neural-Rendering-in-the-Wild-with-Asymmetric-Dual-3D-Gaussian-Splatting" class="headerlink" title="Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian   Splatting"></a>Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian   Splatting</h2><p><strong>Authors:Chengqi Li, Zhihao Shi, Yangdi Lu, Wenbo He, Xiangyu Xu</strong></p>
<p>3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts.In this work, we propose \modelname{}, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. See the project website at <a target="_blank" rel="noopener" href="https://steveli88.github.io/AsymGS">https://steveli88.github.io/AsymGS</a>. </p>
<blockquote>
<p>从野生图像中进行3D重建仍然是一项具有挑战性的任务，因为存在光照条件不一致和短暂干扰物等问题。现有方法通常依赖于启发式策略来处理低质量训练数据，这往往难以产生稳定和一致的重建结果，经常导致视觉伪影。在这项工作中，我们提出了名为“模型名称”的新框架，该框架利用这些伪影的随机性：它们倾向于在不同的训练运行中有所不同，这是由于存在轻微随机性。具体来说，我们的方法并行训练两个3D高斯展布（3DGS）模型，实施一致性约束，以鼓励可靠的场景几何收敛，同时抑制不一致的伪影。为了防止两个模型因确认偏见而陷入相似的失败模式，我们引入了一种发散掩蔽策略，该策略应用两种互补掩蔽：多线索自适应掩蔽和自我监督软掩蔽，这导致两个模型的不对称训练过程，减少共享错误模式。此外，为了提高模型训练的效率，我们引入了一个轻量级变体，称为动态EMA代理，它用动态更新的指数移动平均（EMA）代理替换其中一个模型，并采用交替掩蔽策略来保持发散。在具有挑战性的真实世界数据集上的大量实验表明，我们的方法始终优于现有方法，同时实现了高效率。请参阅项目网站：<a target="_blank" rel="noopener" href="https://steveli88.github.io/AsymGS%E3%80%82">https://steveli88.github.io/AsymGS。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03538v2">PDF</a> NeurIPS 2025 Spotlight; Project page:   <a target="_blank" rel="noopener" href="https://steveli88.github.io/AsymGS/">https://steveli88.github.io/AsymGS/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的框架模型名来解决野外图像3D重建中的挑战性问题，该框架通过利用伪随机艺术的特点进行并行训练两个3DGS模型并引入一致性约束来提高重建的稳定性和一致性。此外，为了优化模型训练效率，引入了动态EMA代理的轻量级变体，并在交替掩蔽策略的帮助下保持差异性。实验证明，该方法在真实世界数据集上的表现优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>模型利用伪随机艺术的特点进行训练，以提高重建的稳定性和一致性。</li>
<li>通过并行训练两个模型并引入一致性约束来增强模型性能。</li>
<li>通过引入两种互补的掩蔽策略来防止模型陷入相同的错误模式。</li>
<li>动态EMA代理和交替掩蔽策略用于提高模型训练效率。</li>
<li>在真实世界数据集上的实验证明，该方法在性能上优于现有方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03538">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-39c4a1cb11772c34bc9112caba37b5c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967954&auth_key=1759967954-0-0-589eb7502b5486cd0728601c0ea7be1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5e0c79b9e1125566226b09e8a8afdaa~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967961&auth_key=1759967961-0-0-ba2e02f16e566cf5005bca5f88e0a831&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-3dcef1a00bf6bb44d2b950786877b10e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26168208c3106be501141bd448e6f9be.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Motion-Blender-Gaussian-Splatting-for-Dynamic-Scene-Reconstruction"><a href="#Motion-Blender-Gaussian-Splatting-for-Dynamic-Scene-Reconstruction" class="headerlink" title="Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction"></a>Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction</h2><p><strong>Authors:Xinyu Zhang, Haonan Chang, Yuhan Liu, Abdeslam Boularias</strong></p>
<p>Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application in robotics. To address this, we propose Motion Blender Gaussian Splatting (MBGS), a novel framework that uses motion graphs as an explicit and sparse motion representation. The motion of a graph’s links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions that determine the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MBGS achieves state-of-the-art performance on the highly challenging iPhone dataset while being competitive on HyperNeRF. We demonstrate the application potential of our method in animating novel object poses, synthesizing real robot demonstrations, and predicting robot actions through visual planning. The source code, models, video demonstrations can be found at <a target="_blank" rel="noopener" href="http://mlzxy.github.io/motion-blender-gs">http://mlzxy.github.io/motion-blender-gs</a>. </p>
<blockquote>
<p>高斯描图法已经成为重建动态场景的一种强大工具。然而，现有的方法主要依赖于隐式运动表示，例如将运动编码到神经网络或每个高斯参数中，这使得进一步操纵重建的运动变得困难。这种缺乏明确的可控性限制了现有方法仅能回放记录的运动，阻碍了其在机器人领域的更广泛应用。为了解决这一问题，我们提出了运动混合器高斯描图法（MBGS），这是一种使用运动图作为明确且稀疏的运动表示的新型框架。图连接的运动通过双重四元数蒙皮传播到各个高斯分布上，通过可学习的权重绘制函数来确定每个连接的影响。运动图和3D高斯分布通过可微分渲染从输入视频中进行联合优化。实验表明，MBGS在极具挑战性的iPhone数据集上达到了最先进的性能，同时在HyperNeRF上表现良好。我们展示了该方法在动画新物体姿态、合成真实机器人演示以及通过视觉规划预测机器人动作方面的应用潜力。源代码、模型和视频演示可在<a target="_blank" rel="noopener" href="http://mlzxy.github.io/motion-blender-gs%E6%89%BE%E5%88%B0%E3%80%82">http://mlzxy.github.io/motion-blender-gs找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09040v4">PDF</a> CoRL 2025</p>
<p><strong>Summary</strong></p>
<p>高斯涂斑技术已成为重建动态场景的高保真工具。然而，现有方法主要依赖隐式运动表示，如将运动编码到神经网络或每个高斯参数中，这使得进一步操纵重建运动变得困难。这种缺乏明确可控性的限制使现有方法仅限于回放记录的运动，阻碍了其在机器人领域的更广泛应用。为解决此问题，我们提出使用运动图作为显式且稀疏的运动表示的全新框架——Motion Blender Gaussian Splatting（MBGS）。图链接的运动通过双重四元数蒙皮传播到各个高斯分布，可通过学习权重绘制函数来确定每个链接的影响。运动图和3D高斯分布通过可微分渲染从输入视频联合优化。实验表明，MBGS在极具挑战性的iPhone数据集上达到了最先进的性能，同时在HyperNeRF上具有很强的竞争力。我们展示了该方法在动画新物体姿态、合成真实机器人演示以及通过视觉规划预测机器人动作方面的应用潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高斯涂斑技术已用于高保真重建动态场景。</li>
<li>现有方法主要依赖隐式运动表示，限制了运动的进一步操纵和广泛应用。</li>
<li>提出了一种新型框架MBGS，使用运动图作为显式且稀疏的运动表示。</li>
<li>通过双重四元数蒙皮将运动图的链接运动传播到各个高斯分布。</li>
<li>运动图和3D高斯分布通过可微分渲染联合优化。</li>
<li>MBGS在iPhone数据集上表现先进，同时在HyperNeRF上具有很强的竞争力。</li>
<li>MBGS在动画新物体姿态、合成机器人演示以及预测机器人动作方面有潜在应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1d15f706821c48d9b7af1356a4823490.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-f96ac7a883fba0bdf31e8212b6e0e8dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759967991&auth_key=1759967991-0-0-3246b94e3b3873e7ebf906f37b716ef3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-85fa1c6aaf0391e79234b8f40421f2ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968018&auth_key=1759968018-0-0-1a5ec8e7f84b05c18015a3d7e059cebb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ad68a36f31622b7103ae8919b9fb8c39~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968025&auth_key=1759968025-0-0-0a8b2a744ba174a73e3a7711383482af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b609da3f025571159826c0adde7e717~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968032&auth_key=1759968032-0-0-8b8a72210ec910bb394e327cb8707627&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a5117ee97a31b97ba19d05c2415eba49~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968039&auth_key=1759968039-0-0-daf87f2cef475faee6a02da6a543dff7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-094b93d208f4fe53c6ddbdf271e12306~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968045&auth_key=1759968045-0-0-32045cd67a1584259e3854b6d0c777c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GSRF-Complex-Valued-3D-Gaussian-Splatting-for-Efficient-Radio-Frequency-Data-Synthesis"><a href="#GSRF-Complex-Valued-3D-Gaussian-Splatting-for-Efficient-Radio-Frequency-Data-Synthesis" class="headerlink" title="GSRF: Complex-Valued 3D Gaussian Splatting for Efficient Radio-Frequency   Data Synthesis"></a>GSRF: Complex-Valued 3D Gaussian Splatting for Efficient Radio-Frequency   Data Synthesis</h2><p><strong>Authors:Kang Yang, Gaofeng Dong, Sijie Ji, Wan Du, Mani Srivastava</strong></p>
<p>Synthesizing radio-frequency (RF) data given the transmitter and receiver positions, e.g., received signal strength indicator (RSSI), is critical for wireless networking and sensing applications, such as indoor localization. However, it remains challenging due to complex propagation interactions, including reflection, diffraction, and scattering. State-of-the-art neural radiance field (NeRF)-based methods achieve high-fidelity RF data synthesis but are limited by long training times and high inference latency. We introduce GSRF, a framework that extends 3D Gaussian Splatting (3DGS) from the optical domain to the RF domain, enabling efficient RF data synthesis. GSRF realizes this adaptation through three key innovations: First, it introduces complex-valued 3D Gaussians with a hybrid Fourier-Legendre basis to model directional and phase-dependent radiance. Second, it employs orthographic splatting for efficient ray-Gaussian intersection identification. Third, it incorporates a complex-valued ray tracing algorithm, executed on RF-customized CUDA kernels and grounded in wavefront propagation principles, to synthesize RF data in real time. Evaluated across various RF technologies, GSRF preserves high-fidelity RF data synthesis while achieving significant improvements in training efficiency, shorter training time, and reduced inference latency. </p>
<blockquote>
<p>根据发射器和接收器位置（例如接收信号强度指示器RSSI）合成无线电频率（RF）数据对于无线联网和感应应用（如室内定位）至关重要。然而，由于复杂的传播交互（包括反射、衍射和散射），这仍然是一个挑战。最先进的基于神经辐射场（NeRF）的方法能够实现高保真RF数据合成，但由于训练时间长和推理延迟高而受到限制。我们引入了GSRF，一个将三维高斯拼贴（3DGS）从光学领域扩展到射频领域的框架，以实现高效的射频数据合成。GSRF通过三个关键创新实现了这一适应：首先，它引入了具有混合傅里叶-勒让德基数的复数三维高斯值，以模拟方向和相位相关的辐射率。其次，它采用正交拼贴技术进行高效的射线-高斯交集识别。第三，它结合了实时执行的复数射线追踪算法，该算法基于射频定制CUDA内核和波前传播原理，以合成射频数据。经过对各种射频技术的评估，GSRF保持了高保真射频数据合成，同时在训练效率、缩短训练时间和降低推理延迟方面取得了显著改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01826v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了基于高斯合成器扩展射频数据的框架GSRF。它将光学领域的三维高斯分块扩展到射频领域，实现了高效的射频数据合成。通过引入复数三维高斯混合傅里叶-勒让德基模型、正交分块高效射线-高斯交点识别和基于波前传播原理的复杂射线追踪算法，GSRF在高保真射频数据合成的同时，提高了训练效率，缩短了训练时间和推理延迟。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GSRF框架扩展了三维高斯合成（3DGS）从光学领域到射频领域，实现高效射频数据合成。</li>
<li>GSRF引入复数三维高斯混合傅里叶-勒让德基模型，以模拟方向性和相位依赖的辐射强度。</li>
<li>通过正交分块技术，GSRF能高效识别射线和高斯交点的交互。</li>
<li>GSRF采用基于波前传播原理的复杂射线追踪算法，合成射频数据。</li>
<li>GSRF在高保真射频数据合成的同时，提高了训练效率，缩短了训练时间和推理延迟。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01826">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a40eb1ad5cecb78772ed99f8ad07a9fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00c2956f93d76afbd784c1695882ba59.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TimeFormer-Capturing-Temporal-Relationships-of-Deformable-3D-Gaussians-for-Robust-Reconstruction"><a href="#TimeFormer-Capturing-Temporal-Relationships-of-Deformable-3D-Gaussians-for-Robust-Reconstruction" class="headerlink" title="TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians   for Robust Reconstruction"></a>TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians   for Robust Reconstruction</h2><p><strong>Authors:DaDong Jiang, Zhihui Ke, Xiaobo Zhou, Zhi Hou, Xianghui Yang, Wenbo Hu, Tie Qiu, Chunchao Guo</strong></p>
<p>Dynamic scene reconstruction is a long-term challenge in 3D vision. Recent methods extend 3D Gaussian Splatting to dynamic scenes via additional deformation fields and apply explicit constraints like motion flow to guide the deformation. However, they learn motion changes from individual timestamps independently, making it challenging to reconstruct complex scenes, particularly when dealing with violent movement, extreme-shaped geometries, or reflective surfaces. To address the above issue, we design a plug-and-play module called TimeFormer to enable existing deformable 3D Gaussians reconstruction methods with the ability to implicitly model motion patterns from a learning perspective. Specifically, TimeFormer includes a Cross-Temporal Transformer Encoder, which adaptively learns the temporal relationships of deformable 3D Gaussians. Furthermore, we propose a two-stream optimization strategy that transfers the motion knowledge learned from TimeFormer to the base stream during the training phase. This allows us to remove TimeFormer during inference, thereby preserving the original rendering speed. Extensive experiments in the multi-view and monocular dynamic scenes validate qualitative and quantitative improvement brought by TimeFormer. Project Page: <a target="_blank" rel="noopener" href="https://patrickddj.github.io/TimeFormer/">https://patrickddj.github.io/TimeFormer/</a> </p>
<blockquote>
<p>动态场景重建是计算机三维视觉领域的一个长期挑战。最近的方法通过将三维高斯拼贴技术扩展到动态场景，并利用运动流等显式约束来引导变形，来解决这个问题。然而，这些方法独立地从各个时间戳学习运动变化，使得重建复杂场景变得具有挑战性，尤其是在处理剧烈的运动、极端形状的几何结构或反射表面时更是如此。为了解决上述问题，我们设计了一个即插即用的模块，名为TimeFormer，使现有的可变形三维高斯重建方法能够从学习角度隐式地模拟运动模式。具体来说，TimeFormer包括一个跨时间Transformer编码器，该编码器可以自适应地学习可变形三维高斯的时间关系。此外，我们还提出了一种双流优化策略，将TimeFormer在训练阶段学到的运动知识转移到基础流中。这允许我们在推理阶段移除TimeFormer，从而保持原始渲染速度。在多角度和单眼动态场景的大量实验验证了TimeFormer带来的定性和定量改进。项目页面：<a target="_blank" rel="noopener" href="https://patrickddj.github.io/TimeFormer/">https://patrickddj.github.io/TimeFormer/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11941v2">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>动态场景重建是3D视觉领域的一个长期挑战。针对现有方法在复杂动态场景重建中的不足，如无法处理剧烈运动、极端形状几何或反射表面等，本文设计了一个名为TimeFormer的即插即用模块。该模块能增强现有可变形3D高斯重建方法的能力，使其能够隐式地从学习角度对运动模式进行建模。TimeFormer包括一个跨时间Transformer编码器，可自适应地学习可变形3D高斯值的临时关系。此外，还提出了一个两流优化策略，将在训练阶段从TimeFormer中学到的运动知识转移到基础流中。在推理阶段，可移除TimeFormer，从而保持原始渲染速度。通过多视角和单目动态场景的广泛实验验证了TimeFormer带来的定性和定量改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动态场景重建是3D视觉领域的长期挑战。</li>
<li>现有方法通过添加变形场和应用运动流等显式约束来扩展3D高斯贴片以适应动态场景。</li>
<li>现有方法独立学习各个时间戳的运动变化，对于复杂场景、剧烈运动、极端形状几何或反射表面的处理存在挑战。</li>
<li>TimeFormer被设计为一个即插即用的模块，旨在增强现有可变形3D高斯重建方法的能力，使其能够隐式建模运动模式。</li>
<li>TimeFormer包括一个跨时间Transformer编码器，可以自适应地学习可变形3D高斯值的临时关系。</li>
<li>提出了一个两流优化策略，将运动知识从TimeFormer转移到基础流中，并在训练阶段应用。</li>
<li>在推理阶段，可以移除TimeFormer以保持原始渲染速度，经过多视角和单目动态场景的广泛实验验证，TimeFormer能够提高定性和定量的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11941">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-11b380c777bb9afc109f451f8d30e171.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ae4bc5d369fcd293cc944bf0e01673c.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ddc75711d702397d7ccc360a823b67b7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968082&auth_key=1759968082-0-0-2101c5fd5dcdd3ae9f32a664bad63b4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-58f958b240591ba07a22c9f6b0d57c44~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968088&auth_key=1759968088-0-0-36e7b6c65ce512c97a72c592c55a76b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-aa25f5d635b8d9225c039d6c5067b5d0.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-17d34356d88c5dda0139b7c048096dae~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968136&auth_key=1759968136-0-0-8f980cdd69e83ee492da56506acf9228&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RT-GuIDE-Real-Time-Gaussian-Splatting-for-Information-Driven-Exploration"><a href="#RT-GuIDE-Real-Time-Gaussian-Splatting-for-Information-Driven-Exploration" class="headerlink" title="RT-GuIDE: Real-Time Gaussian Splatting for Information-Driven   Exploration"></a>RT-GuIDE: Real-Time Gaussian Splatting for Information-Driven   Exploration</h2><p><strong>Authors:Yuezhan Tao, Dexter Ong, Varun Murali, Igor Spasojevic, Pratik Chaudhari, Vijay Kumar</strong></p>
<p>We propose a framework for active mapping and exploration that leverages Gaussian splatting for constructing dense maps. Further, we develop a GPU-accelerated motion planning algorithm that can exploit the Gaussian map for real-time navigation. The Gaussian map constructed onboard the robot is optimized for both photometric and geometric quality while enabling real-time situational awareness for autonomy. We show through viewpoint selection experiments that our method yields comparable Peak Signal-to-Noise Ratio (PSNR) and similar reconstruction error to state-of-the-art approaches, while being orders of magnitude faster to compute. In closed-loop physics-based simulation and real-world experiments, our algorithm achieves better map quality (at least 0.8dB higher PSNR and more than 16% higher geometric reconstruction accuracy) than maps constructed by a state-of-the-art method, enabling semantic segmentation using off-the-shelf open-set models. Experiment videos and more details can be found on our project page: <a target="_blank" rel="noopener" href="https://tyuezhan.github.io/RT">https://tyuezhan.github.io/RT</a> GuIDE&#x2F; </p>
<blockquote>
<p>我们提出一个利用高斯喷溅技术进行密集地图构建的主动映射和探索框架。此外，我们开发了一个GPU加速的运动规划算法，该算法可以利用高斯地图进行实时导航。在机器人内部构建的高斯地图针对光度和几何质量进行了优化，同时提高了自主性的实时情境意识。我们通过视点选择实验表明，我们的方法在峰值信噪比（PSNR）上产生了与最新技术相当的结果，重建误差相似，但计算速度却快得多。在闭环物理仿真和真实世界实验中，我们的算法在地图质量方面表现更好（至少高出0.8dB的PSNR和超过16%的几何重建精度），超越了当前最先进的地图构建方法，并使用现成的开放集模型实现了语义分割。实验视频和更多细节可以在我们的项目页面找到：<a target="_blank" rel="noopener" href="https://tyuezhan.github.io/RT%E6%8C%87%E5%BC%95/%E3%80%82">https://tyuezhan.github.io/RT指引/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18122v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种利用高斯映射进行主动地图构建和探索的框架，并开发了一种GPU加速的运动规划算法，用于实时导航。该高斯地图在机器人上构建，优化光度和几何质量，提高实时态势感知能力。实验显示，该方法在峰值信噪比（PSNR）和重建误差方面与最新方法相当，但计算速度更快。在闭环物理仿真和真实实验中，该算法构建的地图质量优于现有方法，提高了至少0.8dB的PSNR和超过16%的几何重建精度，并实现了语义分割。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于高斯映射的主动地图构建和探索框架。</li>
<li>开发了一种GPU加速的运动规划算法，用于实时导航。</li>
<li>高斯地图在机器人上构建，同时优化光度和几何质量。</li>
<li>高斯地图能提高实时态势感知能力。</li>
<li>实验显示，该方法在计算速度和地图质量方面优于现有方法。</li>
<li>该算法构建的地图质量提高至少0.8dB的PSNR和超过16%的几何重建精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-44f0de2f4755bede412f3cfd34c16491~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968143&auth_key=1759968143-0-0-04586b2ca94cbfed08b0ccbac438d2d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-b864947a9b582434215ccbcc9cdfe4cb.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c5ae517b47d380008172b1cf4ff3a580~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968157&auth_key=1759968157-0-0-00df6ef3c34bbacfd24ffbfdca045e80&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d163fd1d2202449d596f3aad94b8a70~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968164&auth_key=1759968164-0-0-206617e08368a9dea6cd1d395038a811&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79f0b66d7a51443736c97232f6b17c49~resize:0:q75.jpg?source=1f5c5e47&expiration=1759968170&auth_key=1759968170-0-0-bdd9d83f2bdeeb72a45a49d59dbaa3d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-09/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-09/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-09/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-21090328a2b5b6ea458ec253a1247bb9.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-10-09  NGGAN Noise Generation GAN Based on the Practical Measurement Dataset   for Narrowband Powerline Communications
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-09/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-141163230bd6dbbefd5a1ee5335ee6e7.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-10-09  ArchitectHead Continuous Level of Detail Control for 3D Gaussian Head   Avatars
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
