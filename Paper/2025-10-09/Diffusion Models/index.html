<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-09  Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-c93ba0d3466ed2538277f53209df7a53.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    64 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-09-æ›´æ–°"><a href="#2025-10-09-æ›´æ–°" class="headerlink" title="2025-10-09 æ›´æ–°"></a>2025-10-09 æ›´æ–°</h1><h2 id="Bimanual-3D-Hand-Motion-and-Articulation-Forecasting-in-Everyday-Images"><a href="#Bimanual-3D-Hand-Motion-and-Articulation-Forecasting-in-Everyday-Images" class="headerlink" title="Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images"></a>Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images</h2><p><strong>Authors:Aditya Prakash, David Forsyth, Saurabh Gupta</strong></p>
<p>We tackle the problem of forecasting bimanual 3D hand motion &amp; articulation from a single image in everyday settings. To address the lack of 3D hand annotations in diverse settings, we design an annotation pipeline consisting of a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the forecasting model, we adopt a diffusion loss to account for the multimodality in hand motion distribution. Extensive experiments across 6 datasets show the benefits of training on diverse data with imputed labels (14% improvement) and effectiveness of our lifting (42% better) &amp; forecasting (16.4% gain) models, over the best baselines, especially in zero-shot generalization to everyday images. </p>
<blockquote>
<p>æˆ‘ä»¬è§£å†³äº†åœ¨æ—¥å¸¸ç¯å¢ƒä¸­ä»å•å¹…å›¾åƒé¢„æµ‹åŒæ‰‹3DåŠ¨ä½œå’Œå…³èŠ‚æ´»åŠ¨çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³å¤šæ ·ç¯å¢ƒä¸­ç¼ºä¹3Dæ‰‹éƒ¨æ ‡æ³¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ ‡æ³¨æµç¨‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹å°†2Dæ‰‹éƒ¨å…³é”®ç‚¹åºåˆ—æå‡åˆ°4Dæ‰‹éƒ¨åŠ¨ä½œã€‚å¯¹äºé¢„æµ‹æ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ‰©æ•£æŸå¤±æ¥æ¨¡æ‹Ÿæ‰‹éƒ¨è¿åŠ¨åˆ†å¸ƒä¸­çš„å¤šæ¨¡æ€æ€§ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨å…·æœ‰æ’å€¼æ ‡ç­¾çš„å¤šæ ·æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„å¥½å¤„ï¼ˆæé«˜äº†14%ï¼‰ï¼Œä»¥åŠæˆ‘ä»¬çš„æå‡æ¨¡å‹ï¼ˆæé«˜äº†42%ï¼‰å’Œé¢„æµ‹æ¨¡å‹ï¼ˆæé«˜äº†16.4%ï¼‰ç›¸è¾ƒäºæœ€ä½³åŸºçº¿æ¨¡å‹çš„ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬æ¨å¹¿åˆ°æ—¥å¸¸å›¾åƒä¸­çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06145v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ap229997.github.io/projects/forehand4d">https://ap229997.github.io/projects/forehand4d</a></p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹æ—¥å¸¸ç¯å¢ƒä¸‹çš„å•å›¾åƒåŒæ‰‹åŠ¨ä½œé¢„æµ‹å’Œå§¿æ€ä¼°è®¡é—®é¢˜ï¼Œæœ¬ç ”ç©¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹è®¾è®¡äº†ä¸€ç§æ ‡æ³¨ç®¡é“ï¼Œå®ç°ç”±äºŒç»´æ‰‹éƒ¨å…³é”®ç‚¹åºåˆ—åˆ°å››ç»´æ‰‹éƒ¨è¿åŠ¨çš„æå‡ã€‚é€šè¿‡é‡‡ç”¨æ‰©æ•£æŸå¤±ä»¥åº”å¯¹æ‰‹éƒ¨è¿åŠ¨åˆ†å¸ƒçš„å¤šæ¨¡æ€é—®é¢˜ã€‚å®éªŒè¯æ˜åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è®­ç»ƒæ•°æ®å¢å¼ºï¼ˆé‡‡ç”¨å¡«è¡¥æ ‡ç­¾æ–¹æ³•æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼‰çš„ä¼˜åŠ¿ä»¥åŠç›¸æ¯”åŸºå‡†æ¨¡å‹çš„æå‡ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬å›¾åƒä¸Šå–å¾—äº†æ˜¾è‘—çš„é¢„æµ‹ç»“æœæ”¹è¿›ã€‚é€šè¿‡è®­ç»ƒå’Œä½¿ç”¨æ­¤æŠ€æœ¯ï¼Œæœ‰æœ›å®ç°æ‰‹éƒ¨åŠ¨ä½œçš„å‡†ç¡®é¢„æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>ç ”ç©¶è§£å†³äº†ä»å•å›¾åƒé¢„æµ‹åŒæ‰‹åœ¨ç¯å¢ƒä¸­çš„ä¸‰ç»´åŠ¨ä½œå’Œå§¿æ€çš„é—®é¢˜ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ ‡æ³¨ç®¡é“ï¼Œç”¨äºå°†äºŒç»´æ‰‹éƒ¨å…³é”®ç‚¹åºåˆ—æå‡åˆ°å››ç»´æ‰‹éƒ¨è¿åŠ¨ã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£æŸå¤±æ¥åº”å¯¹æ‰‹éƒ¨è¿åŠ¨åˆ†å¸ƒçš„å¤šæ¨¡æ€é—®é¢˜ï¼Œä»¥æ”¹å–„é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†é€šè¿‡å¡«è¡¥æ ‡ç­¾çš„è®­ç»ƒæ•°æ®å¢å¼ºèƒ½å¤Ÿæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶çš„æ‰‹éƒ¨æå‡å’Œé¢„æµ‹æ¨¡å‹æ€§èƒ½æœ‰æ‰€æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å›¾åƒä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06145">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-440dcc972e08d225650c792d5b865847~resize:0:q75.jpg?source=1f5c5e47&expiration=1759969992&auth_key=1759969992-0-0-f2169db1e0ed61acf2aefc0f59b0f8c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b6d801b730ab35423be72d1d26cc7c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1759969999&auth_key=1759969999-0-0-3a4eae499a6a80923154691944743e34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-2714d56d1ada482cdc54bed49b95870f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba193f3d035051da4ad54737be5165f2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970012&auth_key=1759970012-0-0-5d040b463677ae2767c408c4e37ec4ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-ec0cdf520de8ef022601910d6bde7445.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-522d70d0f1dbc971e9c9b513c8a846bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970027&auth_key=1759970027-0-0-05079cd1956fd92838bf03fe451a7ce5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-Image-Editing-for-Breaking-Robust-Watermarks"><a href="#Diffusion-Based-Image-Editing-for-Breaking-Robust-Watermarks" class="headerlink" title="Diffusion-Based Image Editing for Breaking Robust Watermarks"></a>Diffusion-Based Image Editing for Breaking Robust Watermarks</h2><p><strong>Authors:Yunyi Ni, Finn Carter, Ze Niu, Emily Davis, Bo Zhang</strong></p>
<p>Robust invisible watermarking aims to embed hidden information into images such that the watermark can survive various image manipulations. However, the rise of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we present a theoretical study and method demonstrating that diffusion models can effectively break robust image watermarks that were designed to resist conventional perturbations. We show that a diffusion-driven &#96;&#96;image regenerationâ€™â€™ process can erase embedded watermarks while preserving perceptual image content. We further introduce a novel guided diffusion attack that explicitly targets the watermark signal during generation, significantly degrading watermark detectability. Theoretically, we prove that as an image undergoes sufficient diffusion-based transformation, the mutual information between the watermarked image and the embedded watermark payload vanishes, resulting in decoding failure. Experimentally, we evaluate our approach on multiple state-of-the-art watermarking schemes (including the deep learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Our findings highlight a fundamental vulnerability in current robust watermarking techniques against generative model-based attacks, underscoring the need for new watermarking strategies in the era of generative AI. </p>
<blockquote>
<p>é²æ£’æ€§éšå½¢æ°´å°æŠ€æœ¯æ—¨åœ¨å°†éšè—ä¿¡æ¯åµŒå…¥å›¾åƒä¸­ï¼Œä½¿æ°´å°èƒ½å¤ŸæŠµå¾¡å„ç§å›¾åƒæ“ä½œè€Œä¿æŒä¸å˜ã€‚ç„¶è€Œï¼ŒåŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æŠ€æœ¯çš„å…´èµ·ï¼Œç»™è¿™äº›æ°´å°æ–¹æ¡ˆå¸¦æ¥äº†æ–°çš„å¨èƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç†è®ºç ”ç©¶å¹¶å±•ç¤ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯æ˜æ‰©æ•£æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°ç ´åä¸ºæŠµæŠ—ä¼ ç»Ÿæ‰°åŠ¨è€Œè®¾è®¡çš„ç¨³å¥å›¾åƒæ°´å°ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæ‰©æ•£é©±åŠ¨çš„â€œå›¾åƒå†ç”Ÿâ€è¿‡ç¨‹å¯ä»¥æ¶ˆé™¤åµŒå…¥çš„æ°´å°ï¼ŒåŒæ—¶ä¿ç•™æ„ŸçŸ¥å›¾åƒå†…å®¹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§æ–°çš„å¼•å¯¼æ‰©æ•£æ”»å‡»ï¼Œæ˜ç¡®é’ˆå¯¹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ°´å°ä¿¡å·ï¼Œæ˜¾è‘—é™ä½äº†æ°´å°çš„å¯æ£€æµ‹æ€§ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬è¯æ˜äº†å½“å›¾åƒç»å†è¶³å¤Ÿçš„åŸºäºæ‰©æ•£çš„å˜æ¢æ—¶ï¼Œæ°´å°å›¾åƒä¸åµŒå…¥çš„æ°´å°è´Ÿè½½ä¹‹é—´çš„äº’ä¿¡æ¯æ¶ˆå¤±ï¼Œå¯¼è‡´è§£ç å¤±è´¥ã€‚å®éªŒä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•å¯¹äºå¤šç§æœ€å…ˆè¿›çš„æ°´å°æ–¹æ¡ˆï¼ˆåŒ…æ‹¬åŸºäºæ·±åº¦å­¦ä¹ çš„StegaStampã€TrustMarkå’ŒVINEæ–¹æ³•ï¼‰çš„å½±å“ï¼Œå¹¶å±•ç¤ºäº†åœ¨æ”»å‡»åè¿‘ä¹ä¸ºé›¶çš„æ°´å°æ¢å¤ç‡ï¼ŒåŒæ—¶ä¿æŒå†ç”Ÿå›¾åƒçš„é«˜è§†è§‰ä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‡¸æ˜¾äº†å½“å‰ç¨³å¥æ°´å°æŠ€æœ¯åœ¨é¢å¯¹ç”Ÿæˆæ¨¡å‹æ”»å‡»æ—¶çš„åŸºæœ¬è„†å¼±æ€§ï¼Œå¼ºè°ƒäº†ç”Ÿæˆäººå·¥æ™ºèƒ½æ—¶ä»£éœ€è¦æ–°çš„æ°´å°ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05978v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ‰©æ•£æ¨¡å‹å¯¹é²æ£’æ€§å›¾åƒæ°´å°çš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹å¯ä»¥æœ‰æ•ˆç ´åè®¾è®¡ç”¨äºæŠµæŠ—å¸¸è§„å¹²æ‰°çš„é²æ£’å›¾åƒæ°´å°ã€‚é€šè¿‡æ‰©æ•£é©±åŠ¨çš„â€œå›¾åƒå†ç”Ÿâ€è¿‡ç¨‹ï¼Œå¯ä»¥åœ¨ä¿ç•™æ„ŸçŸ¥å›¾åƒå†…å®¹çš„åŒæ—¶æ¶ˆé™¤åµŒå…¥çš„æ°´å°ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ä¸€ç§é’ˆå¯¹æ°´å°ä¿¡å·çš„æ–°å‹å¼•å¯¼æ‰©æ•£æ”»å‡»ï¼Œè¯¥æ”»å‡»åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ˜ç¡®ç„å‡†æ°´å°ä¿¡å·ï¼Œå¤§å¤§é™ä½äº†æ°´å°çš„å¯æ£€æµ‹æ€§ã€‚ç†è®ºä¸Šè¯æ˜ï¼Œå½“å›¾åƒç»å†è¶³å¤Ÿçš„åŸºäºæ‰©æ•£çš„å˜æ¢æ—¶ï¼Œæ°´å°å›¾åƒä¸åµŒå…¥çš„æ°´å°è´Ÿè½½ä¹‹é—´çš„äº’ä¿¡æ¯æ¶ˆå¤±ï¼Œå¯¼è‡´è§£ç å¤±è´¥ã€‚å®éªŒä¸Šï¼Œæˆ‘ä»¬å¯¹å¤šç§å…ˆè¿›çš„æ°´å°æ–¹æ¡ˆè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†åœ¨å—åˆ°æ”»å‡»åè¿‘é›¶çš„æ°´å°æ¢å¤ç‡ï¼ŒåŒæ—¶ä¿æŒå†ç”Ÿå›¾åƒçš„é«˜è§†è§‰ä¿çœŸåº¦ã€‚æœ¬æ–‡å¼ºè°ƒäº†å½“å‰é²æ£’æ°´å°æŠ€æœ¯åœ¨é¢å¯¹ç”Ÿæˆæ¨¡å‹æ”»å‡»æ—¶çš„åŸºæœ¬è„†å¼±æ€§ï¼Œå¼ºè°ƒäº†ç”Ÿæˆäººå·¥æ™ºèƒ½æ—¶ä»£éœ€è¦æ–°çš„æ°´å°ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç ´åè®¾è®¡ç”¨äºæŠµæŠ—å¸¸è§„å¹²æ‰°çš„é²æ£’å›¾åƒæ°´å°ã€‚</li>
<li>é€šè¿‡æ‰©æ•£é©±åŠ¨çš„â€œå›¾åƒå†ç”Ÿâ€è¿‡ç¨‹å¯ä»¥æ¶ˆé™¤åµŒå…¥çš„æ°´å°è€Œä¸å½±å“å›¾åƒçš„æ„ŸçŸ¥å†…å®¹ã€‚</li>
<li>å¼•å¯¼æ‰©æ•£æ”»å‡»èƒ½å¤Ÿé’ˆå¯¹æ°´å°ä¿¡å·è¿›è¡Œæ”»å‡»ï¼Œæ˜¾è‘—é™ä½æ°´å°çš„å¯æ£€æµ‹æ€§ã€‚</li>
<li>å½“å›¾åƒç»å†è¶³å¤Ÿçš„åŸºäºæ‰©æ•£çš„å˜æ¢æ—¶ï¼Œæ°´å°ä¸å›¾åƒå†…å®¹ä¹‹é—´çš„äº’ä¿¡æ¯ä¼šæ¶ˆå¤±ï¼Œå¯¼è‡´è§£ç å¤±è´¥ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œå¤šç§å…ˆè¿›çš„æ°´å°æ–¹æ¡ˆåœ¨å—åˆ°æ”»å‡»åéš¾ä»¥å®ç°æ°´å°æ¢å¤ã€‚</li>
<li>ç°æœ‰çš„é²æ£’æ°´å°æŠ€æœ¯åœ¨é¢å¯¹ç”Ÿæˆæ¨¡å‹æ”»å‡»æ—¶å­˜åœ¨åŸºæœ¬è„†å¼±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-34e98c941281b8ede2caadf3f6c306b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970034&auth_key=1759970034-0-0-e642fac5ea809f81534b0cfb636f97e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-for-Low-Light-Image-Enhancement-A-Multi-Perspective-Taxonomy-and-Performance-Analysis"><a href="#Diffusion-Models-for-Low-Light-Image-Enhancement-A-Multi-Perspective-Taxonomy-and-Performance-Analysis" class="headerlink" title="Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective   Taxonomy and Performance Analysis"></a>Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective   Taxonomy and Performance Analysis</h2><p><strong>Authors:Eashan Adhikarla, Yixin Liu, Brian D. Davison</strong></p>
<p>Low-light image enhancement (LLIE) is vital for safety-critical applications such as surveillance, autonomous navigation, and medical imaging, where visibility degradation can impair downstream task performance. Recently, diffusion models have emerged as a promising generative paradigm for LLIE due to their capacity to model complex image distributions via iterative denoising. This survey provides an up-to-date critical analysis of diffusion models for LLIE, distinctively featuring an in-depth comparative performance evaluation against Generative Adversarial Network and Transformer-based state-of-the-art methods, a thorough examination of practical deployment challenges, and a forward-looking perspective on the role of emerging paradigms like foundation models. We propose a multi-perspective taxonomy encompassing six categories: Intrinsic Decomposition, Spectral &amp; Latent, Accelerated, Guided, Multimodal, and Autonomous; that map enhancement methods across physical priors, conditioning schemes, and computational efficiency. Our taxonomy is grounded in a hybrid view of both the model mechanism and the conditioning signals. We evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs between interpretability, generalization, and inference efficiency. We also discuss real-world deployment constraints (e.g., memory, energy use) and ethical considerations. This survey aims to guide the next generation of diffusion-based LLIE research by highlighting trends and surfacing open research questions, including novel conditioning, real-time adaptation, and the potential of foundation models. </p>
<blockquote>
<p>ä½å…‰å›¾åƒå¢å¼ºï¼ˆLLIEï¼‰å¯¹äºç›‘æ§ã€è‡ªä¸»å¯¼èˆªå’ŒåŒ»å­¦å½±åƒç­‰å®‰å…¨å…³é”®åº”ç”¨è‡³å…³é‡è¦ï¼Œåœ¨è¿™äº›åº”ç”¨ä¸­ï¼Œèƒ½è§åº¦ä¸‹é™ä¼šæŸå®³ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æœ€è¿‘ï¼Œç”±äºèƒ½å¤Ÿé€šè¿‡è¿­ä»£å»å™ªå¯¹å¤æ‚å›¾åƒåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºLLIEçš„ä¸€ç§æœ‰å‰æ™¯çš„ç”ŸæˆèŒƒå¼ã€‚è¿™ç¯‡ç»¼è¿°æä¾›äº†å…³äºæ‰©æ•£æ¨¡å‹åœ¨LLIEä¸­çš„æœ€æ–°æ‰¹åˆ¤æ€§åˆ†æï¼Œå…¶ç‰¹è‰²åŒ…æ‹¬ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’ŒåŸºäºTransformerçš„æœ€å…ˆè¿›æ–¹æ³•çš„æ·±å…¥æ€§èƒ½æ¯”è¾ƒè¯„ä¼°ã€å®é™…éƒ¨ç½²æŒ‘æˆ˜çš„å½»åº•æ£€æŸ¥ä»¥åŠæ–°å…´èŒƒå¼ï¼ˆå¦‚åŸºç¡€æ¨¡å‹ï¼‰ä½œç”¨çš„å‰ç»æ€§è§†è§’ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šç»´åº¦çš„åˆ†ç±»ä½“ç³»ï¼ŒåŒ…æ‹¬å…­å¤§ç±»åˆ«ï¼šå†…åœ¨åˆ†è§£ã€å…‰è°±ä¸æ½œåœ¨ã€åŠ é€Ÿã€æŒ‡å¯¼ã€å¤šæ¨¡æ€å’Œè‡ªä¸»ï¼Œè¿™äº›æ–¹æ³•æ¶µç›–äº†ç‰©ç†å…ˆéªŒã€æ¡ä»¶æ–¹æ¡ˆå’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„å¢å¼ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆ†ç±»ä½“ç³»æ—¢è€ƒè™‘äº†æ¨¡å‹æœºåˆ¶ä¹Ÿè€ƒè™‘äº†æ¡ä»¶ä¿¡å·çš„æ··åˆè§†å›¾ã€‚æˆ‘ä»¬è¯„ä¼°äº†å®šæ€§å¤±è´¥æ¨¡å¼ã€åŸºå‡†ä¸ä¸€è‡´æ€§ä»¥åŠåœ¨è§£é‡Šæ€§ã€æ³›åŒ–å’Œæ¨ç†æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†ç°å®ä¸–ç•Œçš„éƒ¨ç½²çº¦æŸï¼ˆä¾‹å¦‚å†…å­˜ã€èƒ½æºä½¿ç”¨ï¼‰å’Œä¼¦ç†è€ƒé‡ã€‚æœ¬ç»¼è¿°æ—¨åœ¨é€šè¿‡çªå‡ºè¶‹åŠ¿å’Œæå‡ºå¼€æ”¾çš„ç ”ç©¶é—®é¢˜æ¥æŒ‡å¯¼ä¸‹ä¸€ä»£åŸºäºæ‰©æ•£çš„LLIEç ”ç©¶ï¼ŒåŒ…æ‹¬æ–°é¢–çš„æ¡ä»¶ã€å®æ—¶é€‚åº”å’Œæ½œåœ¨çš„åŸºç¡€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05976v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨ä½å…‰ç…§å›¾åƒå¢å¼ºï¼ˆLLIEï¼‰é¢†åŸŸè¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå·²æˆä¸ºå½“å‰çš„ç ”ç©¶çƒ­ç‚¹ã€‚æœ¬æ–‡æä¾›äº†å¯¹æ‰©æ•£æ¨¡å‹åœ¨LLIEé¢†åŸŸçš„æœ€æ–°æ‰¹åˆ¤æ€§åˆ†æï¼Œä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’ŒåŸºäºTransformerçš„å…ˆè¿›æ–¹æ³•è¿›è¡Œäº†æ·±å…¥çš„æ€§èƒ½æ¯”è¾ƒï¼Œæ¢è®¨äº†å®é™…éƒ¨ç½²æŒ‘æˆ˜ï¼Œå¹¶å±•æœ›äº†åŸºç¡€æ¨¡å‹ç­‰æ–°å…´èŒƒå¼çš„ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨LLIEé¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå·²æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•ï¼ˆå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’ŒåŸºäºTransformerçš„æ–¹æ³•ï¼‰è¿›è¡Œäº†æ€§èƒ½æ¯”è¾ƒã€‚</li>
<li>æ·±å…¥æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåŒ…å«å…­ç±»çš„å¤šè§†è§’åˆ†ç±»æ³•ï¼Œä»¥æ˜ å°„å¢å¼ºæ–¹æ³•ã€ç‰©ç†å…ˆéªŒã€æ¡ä»¶æ–¹æ¡ˆå’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>è¯„ä¼°äº†å®šæ€§å¤±è´¥æ¨¡å¼ã€åŸºå‡†ä¸ä¸€è‡´æ€§ï¼Œä»¥åŠè§£é‡Šæ€§ã€é€šç”¨æ€§å’Œæ¨ç†æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>è®¨è®ºäº†ç°å®ä¸–ç•Œçš„éƒ¨ç½²çº¦æŸï¼Œå¦‚å†…å­˜ã€èƒ½æºä½¿ç”¨ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-12d11c936daf827a8daa63ed15ccc0c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970043&auth_key=1759970043-0-0-66e9c736281d1694e7d36a9da6eec0dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-07fb0f5753112e85ba396180b835a4fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e36d6f08e2287c38968ec307d31eb92.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AgeBooth-Controllable-Facial-Aging-and-Rejuvenation-via-Diffusion-Models"><a href="#AgeBooth-Controllable-Facial-Aging-and-Rejuvenation-via-Diffusion-Models" class="headerlink" title="AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion   Models"></a>AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion   Models</h2><p><strong>Authors:Shihao Zhu, Bohan Cao, Ziheng Ouyang, Zhen Li, Peng-Tao Jiang, Qibin Hou</strong></p>
<p>Recent diffusion model research focuses on generating identity-consistent images from a reference photo, but they struggle to accurately control age while preserving identity, and fine-tuning such models often requires costly paired images across ages. In this paper, we propose AgeBooth, a novel age-specific finetuning approach that can effectively enhance the age control capability of adapterbased identity personalization models without the need for expensive age-varied datasets. To reduce dependence on a large amount of age-labeled data, we exploit the linear nature of aging by introducing age-conditioned prompt blending and an age-specific LoRA fusion strategy that leverages SVDMix, a matrix fusion technique. These techniques enable high-quality generation of intermediate-age portraits. Our AgeBooth produces realistic and identity-consistent face images across different ages from a single reference image. Experiments show that AgeBooth achieves superior age control and visual quality compared to previous state-of-the-art editing-based methods. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„ç ”ç©¶é‡ç‚¹æ˜¯ä»å‚è€ƒç…§ç‰‡ç”Ÿæˆèº«ä»½ä¸€è‡´çš„å›¾åƒï¼Œä½†å®ƒä»¬åœ¨ä¿æŒèº«ä»½çš„åŒæ—¶ï¼Œå¾ˆéš¾å‡†ç¡®æ§åˆ¶å¹´é¾„ï¼Œå¹¶ä¸”å¾®è°ƒæ­¤ç±»æ¨¡å‹é€šå¸¸éœ€è¦è·¨å¹´é¾„çš„é…å¯¹å›¾åƒï¼Œæˆæœ¬è¾ƒé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AgeBoothï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¹´é¾„ç‰¹å®šå¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜åŸºäºé€‚é…å™¨çš„ä¸ªæ€§åŒ–æ¨¡å‹çš„å¹´é¾„æ§åˆ¶èƒ½åŠ›ï¼Œè€Œæ— éœ€æ˜‚è´µçš„å¹´é¾„å˜åŒ–æ•°æ®é›†ã€‚ä¸ºäº†å‡å°‘å¯¹å¤§é‡å¹´é¾„æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥å¹´é¾„æ¡ä»¶æç¤ºæ··åˆå’ŒåŸºäºSVDMixçŸ©é˜µèåˆæŠ€æœ¯çš„å¹´é¾„ç‰¹å®šLoRAèåˆç­–ç•¥ï¼Œåˆ©ç”¨è¡°è€çš„çº¿æ€§ç‰¹æ€§ã€‚è¿™äº›æŠ€æœ¯èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ä¸­é—´å¹´é¾„è‚–åƒã€‚æˆ‘ä»¬çš„AgeBoothèƒ½å¤Ÿä»å•ä¸€å‚è€ƒå›¾åƒç”Ÿæˆä¸åŒå¹´é¾„æ®µçœŸå®ä¸”èº«ä»½ä¸€è‡´çš„é¢éƒ¨å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºç¼–è¾‘çš„ç°æœ‰å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒAgeBoothåœ¨å¹´é¾„æ§åˆ¶å’Œè§†è§‰è´¨é‡æ–¹é¢è¾¾åˆ°äº†æ›´ä¼˜è¶Šçš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05715v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAgeBoothçš„æ–°å‹å¹´é¾„ç‰¹å®šå¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯æœ‰æ•ˆæé«˜åŸºäºé€‚é…å™¨çš„èº«ä»½ä¸ªæ€§åŒ–æ¨¡å‹çš„å¹´é¾„æ§åˆ¶èƒ½åŠ›ï¼Œè€Œæ— éœ€æ˜‚è´µçš„å¹´é¾„å˜åŒ–æ•°æ®é›†ã€‚é€šè¿‡å¼•å…¥å¹´é¾„æ¡ä»¶æç¤ºæ··åˆå’ŒåŸºäºSVDMixçŸ©é˜µèåˆæŠ€æœ¯çš„å¹´é¾„ç‰¹å®šLoRAèåˆç­–ç•¥ï¼Œå‡å°‘äº†å¤§é‡å¹´é¾„æ ‡ç­¾æ•°æ®çš„ä¾èµ–ã€‚AgeBoothèƒ½å¤Ÿä»å•ä¸€å‚è€ƒå›¾åƒç”Ÿæˆä¸åŒå¹´é¾„æ®µçœŸå®ä¸”èº«ä»½ä¸€è‡´çš„äººè„¸å›¾åƒï¼Œå¹¶åœ¨å®éªŒä¸Šå®ç°äº†æ¯”åŸºäºç¼–è¾‘çš„æ–¹æ³•æ›´ä¼˜ç§€çš„å¹´é¾„æ§åˆ¶å’Œè§†è§‰å“è´¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AgeBoothæ˜¯ä¸€ç§æ–°å‹å¹´é¾„ç‰¹å®šå¾®è°ƒæ–¹æ³•ï¼Œå¯æé«˜æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆèº«ä»½ä¸€è‡´å›¾åƒæ—¶çš„å¹´é¾„æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>AgeBoothå¯åœ¨æ— éœ€æ˜‚è´µçš„å¹´é¾„å˜åŒ–æ•°æ®é›†çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆæ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¹´é¾„æ¡ä»¶æç¤ºæ··åˆç­–ç•¥ï¼Œå‡å°‘äº†å¤§é‡æ ‡ç­¾æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>åˆ©ç”¨SVDMixçŸ©é˜µèåˆæŠ€æœ¯çš„å¹´é¾„ç‰¹å®šLoRAèåˆç­–ç•¥æ˜¯æé«˜æ¨¡å‹æ€§èƒ½çš„å…³é”®ã€‚</li>
<li>AgeBoothèƒ½å¤Ÿä»å•ä¸€å‚è€ƒå›¾åƒç”Ÿæˆä¸åŒå¹´é¾„æ®µçœŸå®ä¸”èº«ä»½ä¸€è‡´çš„äººè„¸å›¾åƒã€‚</li>
<li>AgeBoothåœ¨å¹´é¾„æ§åˆ¶å’Œè§†è§‰å“è´¨æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäºç¼–è¾‘çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-df508e5b0a8b8937649c300ca460e9aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970064&auth_key=1759970064-0-0-ad6aef212caffbe4474f703d7db6c537&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf0680e087b303255344c2affddadebb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970071&auth_key=1759970071-0-0-7965de948c9e6e4281e68b2e0ffcf55f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b8822fcf2ef2ea454597096773a61345~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970078&auth_key=1759970078-0-0-706fd5c7a96ad81107f41948d7c389e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Teleportraits-Training-Free-People-Insertion-into-Any-Scene"><a href="#Teleportraits-Training-Free-People-Insertion-into-Any-Scene" class="headerlink" title="Teleportraits: Training-Free People Insertion into Any Scene"></a>Teleportraits: Training-Free People Insertion into Any Scene</h2><p><strong>Authors:Jialu Gao, K J Joseph, Fernando De La Torre</strong></p>
<p>The task of realistically inserting a human from a reference image into a background scene is highly challenging, requiring the model to (1) determine the correct location and poses of the person and (2) perform high-quality personalization conditioned on the background. Previous approaches often treat them as separate problems, overlooking their interconnections, and typically rely on training to achieve high performance. In this work, we introduce a unified training-free pipeline that leverages pre-trained text-to-image diffusion models. We show that diffusion models inherently possess the knowledge to place people in complex scenes without requiring task-specific training. By combining inversion techniques with classifier-free guidance, our method achieves affordance-aware global editing, seamlessly inserting people into scenes. Furthermore, our proposed mask-guided self-attention mechanism ensures high-quality personalization, preserving the subjectâ€™s identity, clothing, and body features from just a single reference image. To the best of our knowledge, we are the first to perform realistic human insertions into scenes in a training-free manner and achieve state-of-the-art results in diverse composite scene images with excellent identity preservation in backgrounds and subjects. </p>
<blockquote>
<p>å°†æ¥è‡ªå‚è€ƒå›¾åƒçš„äººç±»ç°å®åœ°æ’å…¥èƒŒæ™¯åœºæ™¯ä¸­çš„ä»»åŠ¡æå…·æŒ‘æˆ˜æ€§ï¼Œéœ€è¦æ¨¡å‹ï¼ˆ1ï¼‰ç¡®å®šäººçš„æ­£ç¡®ä½ç½®å’Œå§¿åŠ¿ï¼Œï¼ˆ2ï¼‰æ ¹æ®èƒŒæ™¯è¿›è¡Œé«˜è´¨é‡ä¸ªæ€§åŒ–æ“ä½œã€‚ä»¥å‰çš„æ–¹æ³•é€šå¸¸å°†å®ƒä»¬è§†ä¸ºå•ç‹¬çš„é—®é¢˜ï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œé€šå¸¸ä¾èµ–äºè®­ç»ƒæ¥å®ç°é«˜æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ— è®­ç»ƒç®¡é“ã€‚æˆ‘ä»¬è¯æ˜æ‰©æ•£æ¨¡å‹æœ¬èº«å…·å¤‡åœ¨å¤æ‚åœºæ™¯ä¸­æ”¾ç½®äººçš„çŸ¥è¯†ï¼Œè€Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚é€šè¿‡ç»“åˆåæ¼”æŠ€æœ¯ä¸æ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ„ŸçŸ¥èƒ½åŠ›çš„å…¨å±€ç¼–è¾‘ï¼Œæ— ç¼åœ°å°†äººæ’å…¥åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„æ©è†œå¼•å¯¼è‡ªæ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿äº†é«˜è´¨é‡ä¸ªæ€§åŒ–ï¼Œä»…ä»å•å¼ å‚è€ƒå›¾åƒä¸­ä¿ç•™ä¸»ä½“çš„èº«ä»½ã€æœè£…å’Œèº«ä½“ç‰¹å¾ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡ä»¥æ— è®­ç»ƒçš„æ–¹å¼åœ¨åœºæ™¯ä¸­æ‰§è¡Œé€¼çœŸçš„äººç±»æ’å…¥ï¼Œå¹¶åœ¨å…·æœ‰å¤šç§å¤åˆåœºæ™¯å›¾åƒçš„èƒŒæ™¯å’Œä¸»ä½“ä¸­å®ç°äº†å“è¶Šçš„èº«ä»½ä¿ç•™ï¼Œè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05660v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å‚è€ƒå›¾åƒä¸­çš„äººç‰©çœŸå®æ’å…¥èƒŒæ™¯åœºæ™¯æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦æ¨¡å‹ç¡®å®šäººçš„æ­£ç¡®ä½ç½®å’Œå§¿æ€ï¼Œä»¥åŠæ ¹æ®èƒŒæ™¯è¿›è¡Œé«˜è´¨é‡ä¸ªæ€§åŒ–è°ƒæ•´ã€‚ä»¥å‰çš„æ–¹æ³•å¸¸å¸¸å°†å…¶è§†ä¸ºä¸¤ä¸ªé—®é¢˜åˆ†å¼€å¤„ç†ï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„è”ç³»ï¼Œå¹¶é€šå¸¸ä¾èµ–äºè®­ç»ƒæ¥å®ç°é«˜æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ç»Ÿä¸€æµç¨‹ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹æœ¬èº«å°±å…·å¤‡åœ¨å¤æ‚åœºæ™¯ä¸­æ”¾ç½®äººç‰©çš„çŸ¥è¯†ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚é€šè¿‡ç»“åˆåæ¼”æŠ€æœ¯ä¸æ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è´Ÿæ‹…æ„ŸçŸ¥å…¨å±€ç¼–è¾‘ï¼Œæ— ç¼åœ°å°†äººç‰©æ’å…¥åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„æ©è†œå¼•å¯¼è‡ªæ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿äº†é«˜è´¨é‡çš„ä¸ªäººåŒ–è°ƒæ•´ï¼Œä»…ä»ä¸€å¼ å‚è€ƒå›¾åƒä¸­ä¿ç•™ä¸»ä½“çš„èº«ä»½ã€æœè£…å’Œèº«ä½“ç‰¹å¾ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªä»¥æ— éœ€è®­ç»ƒçš„æ–¹å¼åœ¨åœºæ™¯ä¸­å®ç°é€¼çœŸçš„äººç‰©æ’å…¥ï¼Œå¹¶åœ¨å¤åˆåœºæ™¯å›¾åƒä¸­å–å¾—äº†å‡ºè‰²çš„èº«ä»½ä¿ç•™ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ’å…¥äººç‰©åˆ°èƒŒæ™¯åœºæ™¯æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜çš„ä»»åŠ¡ï¼Œéœ€è¦æ¨¡å‹ç¡®å®šäººçš„æ­£ç¡®ä½ç½®å’Œå§¿æ€ï¼Œå¹¶è¦æ±‚é«˜è´¨é‡ä¸ªæ€§åŒ–è°ƒæ•´ã€‚</li>
<li>ä»¥å‰çš„å¤„ç†æ–¹æ³•å¸¸å¸¸å°†è¿™ä¸ªé—®é¢˜åˆ†ä¸ºä¸¤éƒ¨åˆ†å¤„ç†ï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ç»Ÿä¸€æµç¨‹æ¥å¤„ç†è¿™ä¸ªä»»åŠ¡ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æœ¬èº«å…·å¤‡åœ¨å¤æ‚åœºæ™¯ä¸­æ”¾ç½®äººç‰©çš„çŸ¥è¯†ã€‚</li>
<li>ç»“åˆåæ¼”æŠ€æœ¯å’Œæ— åˆ†ç±»å™¨å¼•å¯¼çš„æ–¹æ³•å®ç°äº†è´Ÿæ‹…æ„ŸçŸ¥å…¨å±€ç¼–è¾‘ã€‚</li>
<li>æå‡ºçš„æ©è†œå¼•å¯¼è‡ªæ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿äº†é«˜è´¨é‡çš„ä¸ªäººåŒ–è°ƒæ•´ï¼Œä¿ç•™äº†ä¸»ä½“çš„èº«ä»½ã€æœè£…å’Œèº«ä½“ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-596484c7b157fb3e836eb9b27def42a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e006bd4bcd7fcb9eeffdf5fa36e394fd.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c97da041844417420981ee0f60b416f2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970100&auth_key=1759970100-0-0-f3f906efe9f2e79371e7883bf44d7cb3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e9e42cea37f61ce330343383dbb9d5bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970107&auth_key=1759970107-0-0-06512efbec61336476619969cb558f6b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Bidirectional-Mammogram-View-Translation-with-Column-Aware-and-Implicit-3D-Conditional-Diffusion"><a href="#Bidirectional-Mammogram-View-Translation-with-Column-Aware-and-Implicit-3D-Conditional-Diffusion" class="headerlink" title="Bidirectional Mammogram View Translation with Column-Aware and Implicit   3D Conditional Diffusion"></a>Bidirectional Mammogram View Translation with Column-Aware and Implicit   3D Conditional Diffusion</h2><p><strong>Authors:Xin Li, Kaixiang Yang, Qiang Li, Zhiwei Wang</strong></p>
<p>Dual-view mammography, including craniocaudal (CC) and mediolateral oblique (MLO) projections, offers complementary anatomical views crucial for breast cancer diagnosis. However, in real-world clinical workflows, one view may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting the effectiveness of downstream analysis. View-to-view translation can help recover missing views and improve lesion alignment. Unlike natural images, this task in mammography is highly challenging due to large non-rigid deformations and severe tissue overlap in X-ray projections, which obscure pixel-level correspondences. In this paper, we propose Column-Aware and Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view translation framework based on conditional diffusion model. To address cross-view structural misalignment, we first design a column-aware cross-attention mechanism that leverages the geometric property that anatomically corresponding regions tend to lie in similar column positions across views. A Gaussian-decayed bias is applied to emphasize local column-wise correlations while suppressing distant mismatches. Furthermore, we introduce an implicit 3D structure reconstruction module that back-projects noisy 2D latents into a coarse 3D feature volume based on breast-view projection geometry. The reconstructed 3D structure is refined and injected into the denoising UNet to guide cross-view generation with enhanced anatomical awareness. Extensive experiments demonstrate that CA3D-Diff achieves superior performance in bidirectional tasks, outperforming state-of-the-art methods in visual fidelity and structural consistency. Furthermore, the synthesized views effectively improve single-view malignancy classification in screening settings, demonstrating the practical value of our method in real-world diagnostics. </p>
<blockquote>
<p>åŒè§†è§’ä¹³è…ºæ‘„å½±ï¼ˆåŒ…æ‹¬é¢…å°¾å‘ï¼ˆCCï¼‰å’Œå†…å¤–æ–œä½ï¼ˆMLOï¼‰æŠ•å½±ï¼‰ä¸ºä¹³è…ºç™Œè¯Šæ–­æä¾›äº†é‡è¦çš„äº’è¡¥è§£å‰–è§†è§’ã€‚ç„¶è€Œï¼Œåœ¨ç°å®çš„ä¸´åºŠå·¥ä½œä¸­ï¼ŒæŸä¸€è§†è§’å¯èƒ½ä¼šä¸¢å¤±ã€æŸåæˆ–é€€åŒ–ï¼Œå¯èƒ½æ˜¯ç”±äºé‡‡é›†é”™è¯¯æˆ–å‹ç¼©é€ æˆçš„ä¼ªå½±ç­‰åŸå› æ‰€è‡´ï¼Œè¿™ä¼šé™åˆ¶ä¸‹æ¸¸åˆ†æçš„æœ‰æ•ˆæ€§ã€‚è§†è§’é—´çš„ç¿»è¯‘ï¼ˆView-to-view translationï¼‰æœ‰åŠ©äºæ¢å¤ä¸¢å¤±çš„è§†è§’å¹¶æ”¹å–„ç—…ç¶çš„å¯¹é½ã€‚ç„¶è€Œï¼Œä¸è‡ªç„¶å›¾åƒä¸åŒï¼Œè¿™ä¸€ä»»åŠ¡åœ¨ä¹³è…ºæ‘„å½±ä¸­å…·æœ‰æå¤§çš„æŒ‘æˆ˜æ€§ï¼ŒåŸå› åœ¨äºXå°„çº¿æŠ•å½±ä¸­å­˜åœ¨è¾ƒå¤§çš„éåˆšæ€§å˜å½¢å’Œä¸¥é‡çš„ç»„ç»‡é‡å ï¼Œå¯¼è‡´åƒç´ çº§å¯¹åº”å…³ç³»è¢«é®è”½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åˆ—æ„ŸçŸ¥å’Œéšå¼ä¸‰ç»´æ‰©æ•£ï¼ˆCA3D-Diffï¼‰æ–°å‹åŒå‘ä¹³è…ºæ‘„å½±è§†è§’ç¿»è¯‘æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è·¨è§†è§’çš„ç»“æ„ä¸å¯¹é½é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§åˆ—æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨å‡ ä½•å±æ€§ï¼Œå³è§£å‰–ä¸Šå¯¹åº”çš„åŒºåŸŸå€¾å‘äºä½äºä¸åŒè§†è§’çš„ç›¸ä¼¼åˆ—ä½ç½®ã€‚åº”ç”¨é«˜æ–¯è¡°å‡åå·®æ¥å¼ºè°ƒå±€éƒ¨åˆ—ç›¸å…³æ€§ï¼ŒåŒæ—¶æŠ‘åˆ¶è¿œè·ç¦»ä¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéšå¼ä¸‰ç»´ç»“æ„é‡å»ºæ¨¡å—ï¼Œè¯¥æ¨¡å—å°†å™ªå£°çš„äºŒç»´æ½œåœ¨ç‰¹å¾åå‘æŠ•å½±åˆ°åŸºäºä¹³è…ºè§†å›¾æŠ•å½±å‡ ä½•çš„ç²—ç³™ä¸‰ç»´ç‰¹å¾ä½“ç§¯ä¸­ã€‚é‡å»ºçš„ä¸‰ç»´ç»“æ„ç»è¿‡ä¼˜åŒ–åæ³¨å…¥å»å™ªUNetä¸­ï¼Œä»¥æŒ‡å¯¼è·¨è§†è§’ç”Ÿæˆå¢å¼ºäº†è§£å‰–ç»“æ„æ„è¯†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCA3D-Diffåœ¨åŒå‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨è§†è§‰ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆæˆçš„è§†å›¾æœ‰æ•ˆåœ°æé«˜äº†å•è§†è§’æ¶æ€§åˆ†ç±»åœ¨ç­›æŸ¥è®¾ç½®ä¸­çš„æ€§èƒ½ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œè¯Šæ–­ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04947v1">PDF</a> BIBM2025 accept, 8 pages, 4 figures</p>
<p><strong>Summary</strong><br>     è®ºæ–‡æå‡ºä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åŒå‘ä¹³è…ºé’¼é¶æ‘„å½±è§†å›¾è½¬æ¢æ¡†æ¶ï¼Œåä¸ºColumn-Aware and Implicit 3D Diffusionï¼ˆCA3D-Diffï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ—æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œéšå¼ä¸‰ç»´ç»“æ„é‡å»ºæ¨¡å—ï¼Œè§£å†³äº†è·¨è§†å›¾ç»“æ„ä¸åŒ¹é…çš„é—®é¢˜ï¼Œå®ç°äº†åŒå‘çš„ä¹³è…ºé’¼é¶æ‘„å½±è§†å›¾è½¬æ¢ã€‚CA3D-Diffåœ¨åŒå‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œåˆæˆçš„è§†å›¾æœ‰æ•ˆæé«˜äº†å•è§†å›¾æ¶æ€§åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒè§†è§’ä¹³è…ºé’¼é¶æ‘„å½±ï¼ˆåŒ…æ‹¬é¢…å°¾ä½å’Œå†…å¤–æ–œä½æŠ•å½±ï¼‰å¯¹ä¹³è…ºç™Œè¯Šæ–­è‡³å…³é‡è¦ã€‚</li>
<li>å®é™…ä¸´åºŠå·¥ä½œä¸­ï¼ŒæŸä¸€è§†è§’çš„ç¼ºå¤±ã€æŸåæˆ–é€€åŒ–ä¼šå½±å“åç»­åˆ†æçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è§†å›¾åˆ°è§†å›¾çš„è½¬æ¢å¯ä»¥æ¢å¤ç¼ºå¤±çš„è§†å›¾å¹¶æ”¹è¿›ç—…å˜å¯¹å‡†ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†CA3D-Diffæ¡†æ¶ï¼ŒåŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡ŒåŒå‘ä¹³è…ºé’¼é¶æ‘„å½±è§†å›¾è½¬æ¢ã€‚</li>
<li>CA3D-Diffé‡‡ç”¨åˆ—æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨è§£å‰–ä¸Šå¯¹åº”åŒºåŸŸåœ¨è·¨è§†å›¾ä¸­çš„åˆ—ä½ç½®ç›¸ä¼¼æ€§ã€‚</li>
<li>éšå¼ä¸‰ç»´ç»“æ„é‡å»ºæ¨¡å—å°†å™ªå£°çš„äºŒç»´æ½œåœ¨ç‰¹å¾æŠ•å½±åˆ°åŸºäºä¹³è…ºè§†å›¾å‡ ä½•çš„ç²—ç³™ä¸‰ç»´ç‰¹å¾ä½“ç§¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a75e458e6fc579d5c492765f859b03b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970114&auth_key=1759970114-0-0-c8164ca2dfbdeb5568d5bafba0abed75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-93a208d897fe77db372e7cdabe294d5b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-9671e24e77be6b552fd67430d0cc95ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970129&auth_key=1759970129-0-0-b441a42ddee973192c3d51416e7b4edc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-965f1a926fc511d961f8b05cad77b4b7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970136&auth_key=1759970136-0-0-df7b21ccc171a26c7415dc97b9d93f6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ConceptSplit-Decoupled-Multi-Concept-Personalization-of-Diffusion-Models-via-Token-wise-Adaptation-and-Attention-Disentanglement"><a href="#ConceptSplit-Decoupled-Multi-Concept-Personalization-of-Diffusion-Models-via-Token-wise-Adaptation-and-Attention-Disentanglement" class="headerlink" title="ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion   Models via Token-wise Adaptation and Attention Disentanglement"></a>ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion   Models via Token-wise Adaptation and Attention Disentanglement</h2><p><strong>Authors:Habin Lim, Yeongseob Won, Juwon Seo, Gyeong-Moon Park</strong></p>
<p>In recent years, multi-concept personalization for text-to-image (T2I) diffusion models to represent several subjects in an image has gained much more attention. The main challenge of this task is â€œconcept mixingâ€, where multiple learned concepts interfere or blend undesirably in the output image. To address this issue, in this paper, we present ConceptSplit, a novel framework to split the individual concepts through training and inference. Our framework comprises two key components. First, we introduce Token-wise Value Adaptation (ToVA), a merging-free training method that focuses exclusively on adapting the value projection in cross-attention. Based on our empirical analysis, we found that modifying the key projection, a common approach in existing methods, can disrupt the attention mechanism and lead to concept mixing. Second, we propose Latent Optimization for Disentangled Attention (LODA), which alleviates attention entanglement during inference by optimizing the input latent. Through extensive qualitative and quantitative experiments, we demonstrate that ConceptSplit achieves robust multi-concept personalization, mitigating unintended concept interference. Code is available at <a target="_blank" rel="noopener" href="https://github.com/KU-VGI/ConceptSplit">https://github.com/KU-VGI/ConceptSplit</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„å¤šæ¦‚å¿µä¸ªæ€§åŒ–ï¼ˆå³åœ¨ä¸€ä¸ªå›¾åƒä¸­è¡¨ç¤ºå¤šä¸ªä¸»é¢˜ï¼‰å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚è¿™é¡¹ä»»åŠ¡çš„ä¸»è¦æŒ‘æˆ˜æ˜¯â€œæ¦‚å¿µæ··åˆâ€ï¼Œå³å¤šä¸ªå­¦ä¹ åˆ°çš„æ¦‚å¿µåœ¨è¾“å‡ºå›¾åƒä¸­äº§ç”Ÿäº†å¹²æ‰°æˆ–äº§ç”Ÿäº†ä¸å¸Œæœ›çœ‹åˆ°çš„æ··åˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ConceptSplitï¼Œé€šè¿‡è®­ç»ƒå’Œæ¨ç†æ¥æ‹†åˆ†ä¸ªäººæ¦‚å¿µã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†Tokençº§åˆ«çš„ä»·å€¼é€‚åº”ï¼ˆToVAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— åˆå¹¶çš„è®­ç»ƒæ–¹æ³•ï¼Œä¸“æ³¨äºé€‚åº”è·¨æ³¨æ„åŠ›çš„å€¼æŠ•å½±ã€‚åŸºäºæˆ‘ä»¬çš„ç»éªŒåˆ†æï¼Œæˆ‘ä»¬å‘ç°ä¿®æ”¹å…³é”®æŠ•å½±ï¼ˆç°æœ‰æ–¹æ³•ä¸­çš„å¸¸è§åšæ³•ï¼‰å¯èƒ½ä¼šç ´åæ³¨æ„åŠ›æœºåˆ¶å¹¶å¯¼è‡´æ¦‚å¿µæ··åˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºè§£çº ç¼ æ³¨æ„åŠ›çš„æ½œåœ¨ä¼˜åŒ–ï¼ˆLODAï¼‰ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼˜åŒ–è¾“å…¥æ½œåœ¨é‡ï¼Œå‡è½»æ³¨æ„åŠ›çº ç¼ ã€‚é€šè¿‡å¹¿æ³›çš„è´¨é‡å’Œæ•°é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ConceptSplitå®ç°äº†ç¨³å¥çš„å¤šæ¦‚å¿µä¸ªæ€§åŒ–ï¼Œå‡è½»äº†æ„å¤–çš„æ¦‚å¿µå¹²æ‰°ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KU-VGI/ConceptSplit%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KU-VGI/ConceptSplitæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04668v1">PDF</a> 14 pages, 13 figures, to be published in ICCV 2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„å¤šæ¦‚å¿µä¸ªæ€§åŒ–è¡¨ç¤ºçš„æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆã€‚æå‡ºä¸€ç§åä¸ºConceptSplitçš„æ–°æ¡†æ¶ï¼ŒåŒ…å«Token-wise Value Adaptationï¼ˆToVAï¼‰å’ŒLatent Optimization for Disentangled Attentionï¼ˆLODAï¼‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œä»¥åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­åˆ†ç¦»ä¸ªäººæ¦‚å¿µï¼Œå®ç°ç¨³å¥çš„å¤šæ¦‚å¿µä¸ªæ€§åŒ–ï¼Œç¼“è§£æ— æ„ä¸­çš„æ¦‚å¿µå¹²æ‰°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¦‚å¿µä¸ªæ€§åŒ–åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­å—åˆ°å…³æ³¨ï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºæ¦‚å¿µæ··åˆã€‚</li>
<li>ConceptSplitæ¡†æ¶è¢«æå‡ºä»¥è§£å†³æ¦‚å¿µæ··åˆé—®é¢˜ï¼ŒåŒ…å«Token-wise Value Adaptationï¼ˆToVAï¼‰å’ŒLatent Optimization for Disentangled Attentionï¼ˆLODAï¼‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>ToVAæ˜¯ä¸€ç§æ— åˆå¹¶çš„è®­ç»ƒæ–¹æ³•ï¼Œä¸“æ³¨äºè·¨æ³¨æ„åŠ›ä¸­çš„å€¼æŠ•å½±é€‚åº”ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸­çš„å…³é”®æŠ•å½±ä¿®æ”¹å¯èƒ½ä¼šç ´åæ³¨æ„åŠ›æœºåˆ¶å¹¶å¯¼è‡´æ¦‚å¿µæ··åˆã€‚</li>
<li>LODAé€šè¿‡ä¼˜åŒ–è¾“å…¥æ½œåœ¨å˜é‡æ¥ç¼“è§£æ¨ç†è¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›çº ç¼ ã€‚</li>
<li>é€šè¿‡å®šæ€§å’Œå®šé‡å®éªŒï¼Œè¯æ˜ConceptSplitèƒ½å¤Ÿå®ç°ç¨³å¥çš„å¤šæ¦‚å¿µä¸ªæ€§åŒ–ï¼Œå¹¶å‡è½»æ— æ„ä¸­çš„æ¦‚å¿µå¹²æ‰°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef6eeef36852ddaeeef5af095bf56b6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12a2384c55eca445ca261662c077b600.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d42a9c26228e55ed4b8cc4e49dd35e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970158&auth_key=1759970158-0-0-40b1dd7df3f4161f2b44bb5f89f33a18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-e6c049bc158ffff4fd08083b9432e583.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0328128c6c792a383a57b51d417dff37.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-773e688d4dc40bf69538757067df2365~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970178&auth_key=1759970178-0-0-623d96700fcb486139450d900b20c950&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TAG-Tangential-Amplifying-Guidance-for-Hallucination-Resistant-Diffusion-Sampling"><a href="#TAG-Tangential-Amplifying-Guidance-for-Hallucination-Resistant-Diffusion-Sampling" class="headerlink" title="TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion   Sampling"></a>TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion   Sampling</h2><p><strong>Authors:Hyunmin Cho, Donghoon Ahn, Susung Hong, Jee Eun Kim, Seungryong Kim, Kyong Hwan Jin</strong></p>
<p>Recent diffusion models achieve the state-of-the-art performance in image generation, but often suffer from semantic inconsistencies or hallucinations. While various inference-time guidance methods can enhance generation, they often operate indirectly by relying on external signals or architectural modifications, which introduces additional computational overhead. In this paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model. TAG leverages an intermediate sample as a projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory. We formalize this guidance process by leveraging a first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions, thereby reducing inconsistencies and enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module that improves diffusion sampling fidelity with minimal computational addition, offering a new perspective on diffusion guidance. </p>
<blockquote>
<p>æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†ç»å¸¸é­å—è¯­ä¹‰ä¸ä¸€è‡´æˆ–å¹»è§‰çš„å›°æ‰°ã€‚è™½ç„¶å„ç§æ¨ç†æ—¶é—´å¼•å¯¼æ–¹æ³•å¯ä»¥å¢å¼ºç”Ÿæˆï¼Œä½†å®ƒä»¬é€šå¸¸é—´æ¥åœ°ä¾èµ–äºå¤–éƒ¨ä¿¡å·æˆ–æ¶æ„ä¿®æ”¹ï¼Œè¿™å¢åŠ äº†é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œåˆ‡çº¿æ”¾å¤§å¼•å¯¼â€ï¼ˆTAGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ›´é«˜æ•ˆå’Œç›´æ¥çš„å¼•å¯¼æ–¹æ³•ï¼Œå®ƒä»…ä½œç”¨äºè½¨è¿¹ä¿¡å·ï¼Œæ— éœ€ä¿®æ”¹åŸºç¡€æ‰©æ•£æ¨¡å‹ã€‚TAGåˆ©ç”¨ä¸­é—´æ ·æœ¬ä½œä¸ºæŠ•å½±åŸºç¡€ï¼Œæ”¾å¤§ä¼°è®¡åˆ†æ•°ç›¸å¯¹äºæ­¤åŸºç¡€çš„åˆ‡çº¿åˆ†é‡ï¼Œä»¥æ ¡æ­£é‡‡æ ·è½¨è¿¹ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨ä¸€é˜¶æ³°å‹’å±•å¼€æ¥æ­£å¼åŒ–è¿™ä¸€å¼•å¯¼è¿‡ç¨‹ï¼Œè¿™è¯æ˜æ”¾å¤§åˆ‡çº¿åˆ†é‡å¯ä»¥ä½¿çŠ¶æ€è½¬å‘é«˜æ¦‚ç‡åŒºåŸŸï¼Œä»è€Œå‡å°‘ä¸ä¸€è‡´æ€§ï¼Œæé«˜æ ·æœ¬è´¨é‡ã€‚TAGæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„ã€ä¸æ¶æ„æ— å…³çš„æ¨¡å—ï¼Œä»¥æœ€å°çš„è®¡ç®—å¢åŠ æé«˜äº†æ‰©æ•£é‡‡æ ·çš„ä¿çœŸåº¦ï¼Œä¸ºæ‰©æ•£å¼•å¯¼æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04533v1">PDF</a> 16 pages, 9 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Tangential Amplifying Guidanceï¼ˆTAGï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ“ä½œé‡‡æ ·è½¨è¿¹æ¥ç›´æ¥æé«˜æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œæ— éœ€ä¾èµ–å¤–éƒ¨ä¿¡å·æˆ–ä¿®æ”¹æ¨¡å‹æ¶æ„ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸­é—´æ ·æœ¬ä½œä¸ºæŠ•å½±åŸºç¡€ï¼Œæ”¾å¤§ä¼°è®¡åˆ†æ•°çš„åˆ‡çº¿åˆ†é‡æ¥æ ¡æ­£é‡‡æ ·è½¨è¿¹ã€‚é€šè¿‡ä½¿ç”¨ä¸€é˜¶æ³°å‹’å±•å¼€å¼ï¼Œè¯æ˜æ”¾å¤§åˆ‡çº¿åˆ†é‡å¯ä»¥ä½¿çŠ¶æ€è½¬å‘é«˜æ¦‚ç‡åŒºåŸŸï¼Œä»è€Œå‡å°‘ä¸ä¸€è‡´æ€§å¹¶æé«˜æ ·æœ¬è´¨é‡ã€‚TAGæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„ã€ä¸æ¶æ„æ— å…³çš„æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨å¢åŠ æœ€å°‘è®¡ç®—é‡çš„æƒ…å†µä¸‹æé«˜æ‰©æ•£é‡‡æ ·çš„ä¿çœŸåº¦ï¼Œä¸ºæ‰©æ•£æŒ‡å¯¼æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TAGæ–¹æ³•é€šè¿‡æ“ä½œé‡‡æ ·è½¨è¿¹ç›´æ¥æé«˜æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>TAGåˆ©ç”¨ä¸­é—´æ ·æœ¬ä½œä¸ºæŠ•å½±åŸºç¡€ï¼Œæ”¾å¤§ä¼°è®¡åˆ†æ•°çš„åˆ‡çº¿åˆ†é‡æ¥æ ¡æ­£é‡‡æ ·è½¨è¿¹ã€‚</li>
<li>ä¸€é˜¶æ³°å‹’å±•å¼€å¼è¢«ç”¨äºå½¢å¼åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œè¯æ˜æ”¾å¤§åˆ‡çº¿åˆ†é‡æœ‰åŠ©äºæé«˜é‡‡æ ·è´¨é‡ã€‚</li>
<li>TAGæ–¹æ³•æ— éœ€ä¾èµ–å¤–éƒ¨ä¿¡å·æˆ–ä¿®æ”¹æ¨¡å‹æ¶æ„ã€‚</li>
<li>TAGæ˜¯ä¸€ä¸ªä¸æ¶æ„æ— å…³çš„æ¨¡å—ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°é›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li>
<li>TAGåœ¨æé«˜æ‰©æ•£é‡‡æ ·ä¿çœŸåº¦çš„åŒæ—¶ï¼Œåªå¢åŠ äº†æå°‘çš„è®¡ç®—é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a60409e345534db8a14d00c203d254d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970186&auth_key=1759970186-0-0-940228f9ba5a579b36e6026d176ab447&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-c93ba0d3466ed2538277f53209df7a53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-751f4d4bf46172330a32fbf2e2f5ced5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a606c5da58026be19d8e41727ee3a74c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Asynchronous-Denoising-Diffusion-Models-for-Aligning-Text-to-Image-Generation"><a href="#Asynchronous-Denoising-Diffusion-Models-for-Aligning-Text-to-Image-Generation" class="headerlink" title="Asynchronous Denoising Diffusion Models for Aligning Text-to-Image   Generation"></a>Asynchronous Denoising Diffusion Models for Aligning Text-to-Image   Generation</h2><p><strong>Authors:Zijing Hu, Yunze Tong, Fengda Zhang, Junkun Yuan, Jun Xiao, Kun Kuang</strong></p>
<p>Diffusion models have achieved impressive results in generating high-quality images. Yet, they often struggle to faithfully align the generated images with the input prompts. This limitation arises from synchronous denoising, where all pixels simultaneously evolve from random noise to clear images. As a result, during generation, the prompt-related regions can only reference the unrelated regions at the same noise level, failing to obtain clear context and ultimately impairing text-to-image alignment. To address this issue, we propose asynchronous diffusion models â€“ a novel framework that allocates distinct timesteps to different pixels and reformulates the pixel-wise denoising process. By dynamically modulating the timestep schedules of individual pixels, prompt-related regions are denoised more gradually than unrelated regions, thereby allowing them to leverage clearer inter-pixel context. Consequently, these prompt-related regions achieve better alignment in the final images. Extensive experiments demonstrate that our asynchronous diffusion models can significantly improve text-to-image alignment across diverse prompts. The code repository for this work is available at <a target="_blank" rel="noopener" href="https://github.com/hu-zijing/AsynDM">https://github.com/hu-zijing/AsynDM</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éš¾ä»¥å¿ å®åœ°å°†ç”Ÿæˆçš„å›¾åƒä¸è¾“å…¥æç¤ºå¯¹é½ã€‚è¿™ä¸€é™åˆ¶æºäºåŒæ­¥å»å™ªï¼Œå…¶ä¸­æ‰€æœ‰åƒç´ åŒæ—¶ä»éšæœºå™ªå£°æ¼”åŒ–æˆæ¸…æ™°å›¾åƒã€‚å› æ­¤ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæç¤ºç›¸å…³åŒºåŸŸåªèƒ½å¼•ç”¨åŒä¸€å™ªå£°æ°´å¹³çš„æ— å…³åŒºåŸŸï¼Œæ— æ³•è·å¾—æ¸…æ™°çš„ä¸Šä¸‹æ–‡ï¼Œæœ€ç»ˆæŸå®³æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¼‚æ­¥æ‰©æ•£æ¨¡å‹â€”â€”ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œä¸ºä¸åŒçš„åƒç´ åˆ†é…ä¸åŒçš„æ—¶é—´æ­¥é•¿ï¼Œå¹¶é‡æ–°åˆ¶å®šåƒç´ çº§çš„å»å™ªè¿‡ç¨‹ã€‚é€šè¿‡åŠ¨æ€è°ƒåˆ¶ä¸ªåˆ«åƒç´ çš„æ—¶é—´æ­¥é•¿è°ƒåº¦ï¼Œæç¤ºç›¸å…³åŒºåŸŸçš„å»å™ªé€Ÿåº¦æ¯”æ— å…³åŒºåŸŸæ›´ç¼“æ…¢ï¼Œä»è€Œå…è®¸å®ƒä»¬åˆ©ç”¨æ›´æ¸…æ™°çš„åƒç´ é—´ä¸Šä¸‹æ–‡ã€‚å› æ­¤ï¼Œè¿™äº›æç¤ºç›¸å…³åŒºåŸŸåœ¨æœ€ç»ˆå›¾åƒä¸­å®ç°äº†æ›´å¥½çš„å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¼‚æ­¥æ‰©æ•£æ¨¡å‹å¯ä»¥åœ¨å„ç§æç¤ºä¸­æ˜¾è‘—æé«˜æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹é½æ•ˆæœã€‚è¯¥å·¥ä½œçš„ä»£ç ä»“åº“å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/hu-zijing/AsynDM">https://github.com/hu-zijing/AsynDM</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04504v1">PDF</a> 22 pages, 11 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å°†ç”Ÿæˆå›¾åƒä¸è¾“å…¥æç¤ºå¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼‚æ­¥æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œä¸ºä¸åŒåƒç´ åˆ†é…ä¸åŒçš„æ—¶é—´æ­¥é•¿å¹¶é‡æ–°åˆ¶å®šåƒç´ çº§çš„å»å™ªè¿‡ç¨‹ã€‚é€šè¿‡åŠ¨æ€è°ƒåˆ¶ä¸ªåˆ«åƒç´ çš„æ—¶é—´æ­¥é•¿è¡¨ï¼Œä¸æç¤ºç›¸å…³çš„åŒºåŸŸå»å™ªæ¯”æ— å…³åŒºåŸŸæ›´ç¼“æ…¢ï¼Œä»è€Œå¯ä»¥åˆ©ç”¨æ›´æ¸…æ™°çš„åƒç´ é—´ä¸Šä¸‹æ–‡ã€‚å› æ­¤ï¼Œè¿™äº›ä¸æç¤ºç›¸å…³çš„åŒºåŸŸåœ¨æœ€ç»ˆå›¾åƒä¸­å®ç°äº†æ›´å¥½çš„å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ–‡æœ¬åˆ°å›¾åƒå¯¹é½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>åŒæ­¥å»å™ªæ˜¯å¯¼è‡´è¯¥æŒ‘æˆ˜çš„ä¸»è¦åŸå› ï¼Œå…¶ä¸­æ‰€æœ‰åƒç´ åŒæ—¶ä»éšæœºå™ªå£°æ¼”å˜ä¸ºæ¸…æ™°å›¾åƒã€‚</li>
<li>å¼‚æ­¥æ‰©æ•£æ¨¡å‹æ¡†æ¶è¢«æå‡ºä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ¡†æ¶ä¸ºä¸åŒåƒç´ åˆ†é…ä¸åŒçš„æ—¶é—´æ­¥é•¿ã€‚</li>
<li>ä¸æç¤ºç›¸å…³çš„åŒºåŸŸåœ¨å»å™ªè¿‡ç¨‹ä¸­æ¯”æ— å…³åŒºåŸŸæ›´ç¼“æ…¢ï¼Œä»è€Œåˆ©ç”¨æ›´æ¸…æ™°çš„åƒç´ é—´ä¸Šä¸‹æ–‡ã€‚</li>
<li>è¿™ç§ç­–ç•¥å…è®¸ä¸æç¤ºç›¸å…³çš„åŒºåŸŸåœ¨æœ€ç»ˆå›¾åƒä¸­å®ç°æ›´å¥½çš„å¯¹é½ã€‚</li>
<li>å¼‚æ­¥æ‰©æ•£æ¨¡å‹åœ¨å¤šç§æç¤ºä¸‹æ˜¾è‘—æ”¹å–„äº†æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹é½ã€‚</li>
<li>ç›¸å…³ä»£ç ä»“åº“å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8f56a6e15dd271190a0b4fec3115549.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-8103d479f57c72e6f1cc7e692baee579~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970222&auth_key=1759970222-0-0-0c3aeced04476a2e38d5fa81203b3bc7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ed6068dd5d3cdee2c696ecc1cdbbb26~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970229&auth_key=1759970229-0-0-0b0e1f326d5745f585cf9fbb0d0db361&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AutoEdit-Automatic-Hyperparameter-Tuning-for-Image-Editing"><a href="#AutoEdit-Automatic-Hyperparameter-Tuning-for-Image-Editing" class="headerlink" title="AutoEdit: Automatic Hyperparameter Tuning for Image Editing"></a>AutoEdit: Automatic Hyperparameter Tuning for Image Editing</h2><p><strong>Authors:Chau Pham, Quan Dao, Mahesh Bhosale, Yunjie Tian, Dimitris Metaxas, David Doermann</strong></p>
<p>Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification. This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editingâ€™s hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world. Codes can be found at <a target="_blank" rel="noopener" href="https://github.com/chaupham1709/AutoEdit.git">https://github.com/chaupham1709/AutoEdit.git</a>. </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹æ–¹é¢çš„è¿›å±•å½»åº•æ”¹å˜äº†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ï¼Œä½†ç°æœ‰çš„ç¼–è¾‘æ–¹æ³•åœ¨è¶…å‚æ•°è¯†åˆ«æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è·å¾—åˆç†çš„ç¼–è¾‘æ€§èƒ½ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸è¦æ±‚ç”¨æˆ·æš´åŠ›è°ƒæ•´å¤šä¸ªç›¸äº’ä¾èµ–çš„è¶…å‚æ•°ï¼Œå¦‚åè½¬æ—¶é—´æ­¥é•¿å’Œæ³¨æ„åŠ›ä¿®æ”¹ã€‚ç”±äºè¶…å‚æ•°æœç´¢ç©ºé—´å·¨å¤§ï¼Œè¿™ä¸€è¿‡ç¨‹äº§ç”Ÿäº†é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å°†å¯»æ‰¾æœ€ä½³ç¼–è¾‘è¶…å‚æ•°è§†ä¸ºæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­çš„ä¸€ä¸ªé¡ºåºå†³ç­–ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å»ºç«‹äº†ä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹åœ¨å»å™ªæ­¥éª¤ä¸­åŠ¨æ€è°ƒæ•´è¶…å‚æ•°ï¼Œå¹¶å°†ç¼–è¾‘ç›®æ ‡æ•´åˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–å®ç°äº†æ—¶é—´æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†æœ€ä½³è¶…å‚æ•°é…ç½®ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æš´åŠ›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æœç´¢æ—¶é—´å’Œè®¡ç®—å¼€é”€æ–¹é¢å¤§å¤§å‡å°‘äº†ï¼Œè¿™æ¨åŠ¨äº†åŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘æ¡†æ¶åœ¨ç°å®ä¸–ç•Œä¸­çš„å®é™…åº”ç”¨ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chaupham1709/AutoEdit.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/chaupham1709/AutoEdit.gitæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15031v2">PDF</a> Provided code link</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æå‡ºä¸€ç§æ–°çš„åŸºäºå¼ºåŒ–å­¦ä¹ æ¡†æ¶çš„æ‰©æ•£æ¨¡å‹ç¼–è¾‘è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å»ºç«‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ¥åŠ¨æ€è°ƒæ•´ç¼–è¾‘è¿‡ç¨‹ä¸­çš„è¶…å‚æ•°ï¼Œå¹¶å°†ç¼–è¾‘ç›®æ ‡æ•´åˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡å’Œè¶…å‚æ•°é…ç½®çš„ä¼˜åŒ–æ°´å¹³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æš´åŠ›æœç´¢æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æœç´¢æ—¶é—´å’Œè®¡ç®—å¼€é”€æ–¹é¢æ˜¾è‘—å‡å°‘ï¼Œæ¨åŠ¨äº†æ‰©æ•£æ¨¡å‹å›¾åƒç¼–è¾‘æ¡†æ¶åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ä¸­çš„æœ€æ–°è¿›å±•ã€‚</li>
<li>ç°æœ‰ç¼–è¾‘æ–¹æ³•é¢ä¸´è¶…å‚æ•°è¯†åˆ«æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éœ€è¦ç”¨æˆ·é€šè¿‡æš´åŠ›æ–¹å¼è°ƒæ•´å¤šä¸ªç›¸äº’ä¾èµ–çš„è¶…å‚æ•°ï¼Œå¦‚åæ¼”æ—¶é—´æ­¥é•¿å’Œæ³¨æ„åŠ›ä¿®æ”¹ç­‰ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶æ¥è§£å†³æœ€ä¼˜ç¼–è¾‘è¶…å‚æ•°çš„æœç´¢é—®é¢˜ã€‚</li>
<li>å»ºç«‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œåœ¨é™å™ªæ­¥éª¤ä¸­åŠ¨æ€è°ƒæ•´è¶…å‚æ•°ã€‚</li>
<li>å°†ç¼–è¾‘ç›®æ ‡æ•´åˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ï¼Œä»¥æé«˜æ—¶é—´æ•ˆç‡å’Œä¿æŒè¶…å‚æ•°é…ç½®çš„ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-155c5355769851beae8889749ab451cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970236&auth_key=1759970236-0-0-fff1ce168e8007f69f9d66a3030abe8e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-63792a45d4a15c475eab45d97a23100a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970243&auth_key=1759970243-0-0-1914b5573906fb5206a3495751328596&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-877e5ff48e41ff61055c0f5b360025b9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Robust-Concept-Erasure-in-Diffusion-Models-A-Theoretical-Perspective-on-Security-and-Robustness"><a href="#Robust-Concept-Erasure-in-Diffusion-Models-A-Theoretical-Perspective-on-Security-and-Robustness" class="headerlink" title="Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on   Security and Robustness"></a>Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on   Security and Robustness</h2><p><strong>Authors:Zixuan Fu, Yan Ren, Finn Carter, Chenyue Wen, Le Ku, Daheng Yu, Emily Davis, Bo Zhang</strong></p>
<p>Diffusion models have achieved unprecedented success in image generation but pose increasing risks in terms of privacy, fairness, and security. A growing demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW content, private individuals, artistic styles) from these models while preserving their overall generative capabilities. We introduce \textbf{SCORE} (Secure and Concept-Oriented Robust Erasure), a novel framework for robust concept removal in diffusion models. SCORE formulates concept erasure as an \emph{adversarial independence} problem, theoretically guaranteeing that the modelâ€™s outputs become statistically independent of the erased concept. Unlike prior heuristic methods, SCORE minimizes the mutual information between a target concept and generated outputs, yielding provable erasure guarantees. We provide formal proofs establishing convergence properties and derive upper bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW removal, celebrity face suppression, and artistic style unlearning. SCORE consistently outperforms state-of-the-art methods including EraseAnything, ANT, MACE, ESD, and UCE, achieving up to \textbf{12.5%} higher erasure efficacy while maintaining comparable or superior image quality. By integrating adversarial optimization, trajectory consistency, and saliency-driven fine-tuning, SCORE sets a new standard for secure and robust concept erasure in diffusion models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸï¼Œä½†åŒæ—¶åœ¨éšç§ã€å…¬å¹³å’Œå®‰å…¨æ–¹é¢å¸¦æ¥äº†æ—¥ç›Šå¢åŠ çš„é£é™©ã€‚å› æ­¤ï¼Œå­˜åœ¨ä¸€ç§æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œéœ€è¦ä»è¿™äº›æ¨¡å‹ä¸­åˆ é™¤æ•æ„Ÿæˆ–æœ‰å®³çš„æ¦‚å¿µï¼ˆä¾‹å¦‚ï¼ŒNSFWå†…å®¹ã€ä¸ªäººéšç§é—®é¢˜ã€è‰ºæœ¯é£æ ¼ç­‰ï¼‰ï¼ŒåŒæ—¶ä¿æŒå…¶æ•´ä½“çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>SCORE</strong>ï¼ˆå®‰å…¨å’Œé¢å‘æ¦‚å¿µç¨³å¥æ“¦é™¤ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ‰©æ•£æ¨¡å‹ä¸­ç¨³å¥æ¦‚å¿µåˆ é™¤çš„æ–°å‹æ¡†æ¶ã€‚SCOREå°†æ¦‚å¿µæ“¦é™¤è¡¨è¿°ä¸ºå¯¹æŠ—ç‹¬ç«‹æ€§é—®é¢˜ï¼Œä»ç†è®ºä¸Šä¿è¯äº†æ¨¡å‹çš„è¾“å‡ºä¸æ“¦é™¤çš„æ¦‚å¿µåœ¨ç»Ÿè®¡ä¸Šç‹¬ç«‹ã€‚ä¸ä¹‹å‰çš„å¯å‘å¼æ–¹æ³•ä¸åŒï¼ŒSCOREæœ€å°åŒ–ç›®æ ‡æ¦‚å¿µä¸ç”Ÿæˆè¾“å‡ºä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œä»è€Œæä¾›å¯è¯æ˜çš„æ“¦é™¤ä¿è¯ã€‚æˆ‘ä»¬æä¾›äº†æ­£å¼è¯æ˜æ¥å»ºç«‹æ”¶æ•›å±æ€§å¹¶æ¨å¯¼å‡ºå‰©ä½™æ¦‚å¿µæ³„æ¼çš„ä¸Šé™ã€‚åœ¨å®è¯æ–¹é¢ï¼Œæˆ‘ä»¬å¯¹Stable Diffusionå’ŒFLUXåœ¨å››é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†SCOREï¼šå¯¹è±¡æ“¦é™¤ã€NSFWåˆ é™¤ã€åäººé¢éƒ¨æŠ‘åˆ¶å’Œè‰ºæœ¯é£æ ¼å»é™¤ã€‚SCOREåœ¨å„é¡¹æ€§èƒ½æŒ‡æ ‡ä¸Šå‡è¡¨ç°æœ€ä½³ï¼Œè¶…è¶Šäº†åŒ…æ‹¬EraseAnythingã€ANTã€MACEã€ESDå’ŒUCEç­‰æœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾**12.5%**çš„æ›´é«˜çš„æ“¦é™¤æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†å›¾åƒè´¨é‡ã€‚é€šè¿‡é›†æˆå¯¹æŠ—ä¼˜åŒ–ã€è½¨è¿¹ä¸€è‡´æ€§å’Œæ˜¾è‘—æ€§é©±åŠ¨å¾®è°ƒï¼ŒSCOREä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„å®‰å…¨ç¨³å¥æ¦‚å¿µæ“¦é™¤è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12024v2">PDF</a> updated version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—çš„å·¨å¤§æˆåŠŸï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†å…¶åœ¨éšç§ã€å…¬å¹³æ€§å’Œå®‰å…¨æ€§æ–¹é¢æ—¥ç›Šå¢é•¿çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSCOREçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºåœ¨æ‰©æ•£æ¨¡å‹ä¸­å®ç°ç¨³å¥çš„æ¦‚å¿µå»é™¤ã€‚SCOREå°†æ¦‚å¿µåˆ é™¤è¡¨è¿°ä¸ºå¯¹æŠ—æ€§ç‹¬ç«‹é—®é¢˜ï¼Œç†è®ºä¸Šä¿è¯äº†æ¨¡å‹è¾“å‡ºä¸åˆ é™¤æ¦‚å¿µä¹‹é—´çš„ç»Ÿè®¡ç‹¬ç«‹æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSCOREè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå®ç°äº†é«˜è¾¾12.5%çš„åˆ é™¤æ•ˆç‡æå‡ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†å›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„æˆåŠŸå¸¦æ¥äº†éšç§ã€å…¬å¹³æ€§å’Œå®‰å…¨æ€§æ–¹é¢çš„é£é™©ã€‚</li>
<li>æå‡ºäº†åä¸ºSCOREçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºç¨³å¥åœ°åˆ é™¤æ‰©æ•£æ¨¡å‹ä¸­çš„ç‰¹å®šæ¦‚å¿µã€‚</li>
<li>SCOREå°†æ¦‚å¿µåˆ é™¤è¡¨è¿°ä¸ºå¯¹æŠ—æ€§ç‹¬ç«‹é—®é¢˜ï¼Œç¡®ä¿æ¨¡å‹è¾“å‡ºä¸åˆ é™¤æ¦‚å¿µä¹‹é—´çš„ç»Ÿè®¡ç‹¬ç«‹æ€§ã€‚</li>
<li>SCOREé€šè¿‡æ•´åˆå¯¹æŠ—æ€§ä¼˜åŒ–ã€è½¨è¿¹ä¸€è‡´æ€§å’Œæ˜¾è‘—æ€§é©±åŠ¨å¾®è°ƒï¼Œå®ç°äº†é«˜æ•ˆçš„æ¦‚å¿µåˆ é™¤ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSCOREæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬EraseAnythingã€ANTã€MACEã€ESDå’ŒUCEã€‚</li>
<li>SCOREåœ¨åˆ é™¤æ•ˆç‡ä¸Šæé«˜äº†é«˜è¾¾12.5%ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9375737f91d112c238f4d8c6b4b52ad6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970257&auth_key=1759970257-0-0-31b727a8672b71e6ac193c57ec0c4890&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Divergence-Minimization-Preference-Optimization-for-Diffusion-Model-Alignment"><a href="#Divergence-Minimization-Preference-Optimization-for-Diffusion-Model-Alignment" class="headerlink" title="Divergence Minimization Preference Optimization for Diffusion Model   Alignment"></a>Divergence Minimization Preference Optimization for Diffusion Model   Alignment</h2><p><strong>Authors:Binxu Li, Minkai Xu, Jiaqi Han, Meihua Dang, Stefano Ermon</strong></p>
<p>Diffusion models have achieved remarkable success in generating realistic and versatile images from text prompts. Inspired by the recent advancements of language models, there is an increasing interest in further improving the models by aligning with human preferences. However, we investigate alignment from a divergence minimization perspective and reveal that existing preference optimization methods are typically trapped in suboptimal mean-seeking optimization. In this paper, we introduce Divergence Minimization Preference Optimization (DMPO), a novel and principled method for aligning diffusion models by minimizing reverse KL divergence, which asymptotically enjoys the same optimization direction as original RL. We provide rigorous analysis to justify the effectiveness of DMPO and conduct comprehensive experiments to validate its empirical strength across both human evaluations and automatic metrics. Our extensive results show that diffusion models fine-tuned with DMPO can consistently outperform or match existing techniques, specifically consistently outperforming all baseline models across different base models and test sets, achieving the best PickScore in every case, demonstrating the methodâ€™s superiority in aligning generative behavior with desired outputs. Overall, DMPO unlocks a robust and elegant pathway for preference alignment, bridging principled theory with practical performance in diffusion models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆç°å®ä¸”å¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚å—è¯­è¨€æ¨¡å‹æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œäººä»¬è¶Šæ¥è¶Šæ„Ÿå…´è¶£é€šè¿‡ç¬¦åˆäººç±»åå¥½æ¥è¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä»åˆ†æ­§æœ€å°åŒ–è§’åº¦ç ”ç©¶å¯¹é½é—®é¢˜ï¼Œå¹¶æ­ç¤ºç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•é€šå¸¸é™·å…¥æ¬¡ä¼˜çš„å¹³å‡å€¼å¯»æ±‚ä¼˜åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åˆ†æ­§æœ€å°åŒ–åå¥½ä¼˜åŒ–ï¼ˆDMPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æœ€å°åŒ–åå‘KLåˆ†æ­§æ¥å¯¹é½æ‰©æ•£æ¨¡å‹çš„æ–°å‹æœ‰åŸåˆ™çš„æ–¹æ³•ï¼Œå®ƒåœ¨æ¸è¿‘æƒ…å†µä¸‹ä¸åŸå§‹å¼ºåŒ–å­¦ä¹ å…·æœ‰ç›¸åŒçš„ä¼˜åŒ–æ–¹å‘ã€‚æˆ‘ä»¬æä¾›äº†ä¸¥æ ¼çš„åˆ†ææ¥è¯æ˜DMPOçš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡å®éªŒå…¨é¢éªŒè¯äº†å…¶åœ¨äººç±»è¯„ä¼°å’Œè‡ªåŠ¨æŒ‡æ ‡æ–¹é¢çš„å®è¯å®åŠ›ã€‚æˆ‘ä»¬çš„å¹¿æ³›ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DMPOå¾®è°ƒè¿‡çš„æ‰©æ•£æ¨¡å‹å¯ä»¥æŒç»­è¶…è¶Šæˆ–åŒ¹é…ç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒçš„åŸºå‡†æ¨¡å‹å’Œæµ‹è¯•é›†ä¸ŠæŒç»­è¶…è¶Šæ‰€æœ‰åŸºå‡†æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªæ¡ˆä¾‹ä¸­å‡è·å¾—æœ€ä½³çš„PickScoreï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å°†ç”Ÿæˆè¡Œä¸ºä¸æœŸæœ›è¾“å‡ºå¯¹é½æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æ€»çš„æ¥è¯´ï¼ŒDMPOä¸ºåå¥½å¯¹é½è§£é”äº†ç¨³å¥è€Œä¼˜é›…çš„é€”å¾„ï¼Œåœ¨åŸåˆ™ç†è®ºä¸æ‰©æ•£æ¨¡å‹çš„å®è·µæ€§èƒ½ä¹‹é—´æ¶èµ·äº†ä¸€åº§æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07510v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬æç¤ºç”ŸæˆçœŸå®ä¸”å¤šæ ·åŒ–çš„å›¾åƒï¼Œå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚éšç€è¯­è¨€æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œäººä»¬è¶Šæ¥è¶Šæœ‰å…´è¶£é€šè¿‡ä¸äººç±»åå¥½å¯¹é½æ¥æ”¹è¿›è¿™äº›æ¨¡å‹ã€‚æœ¬æ–‡ä»åˆ†æ­§æœ€å°åŒ–è§’åº¦ç ”ç©¶å¯¹é½é—®é¢˜ï¼Œå¹¶æ­ç¤ºç°æœ‰åå¥½ä¼˜åŒ–æ–¹æ³•é€šå¸¸é™·å…¥æ¬¡ä¼˜å‡å€¼å¯»æ±‚ä¼˜åŒ–ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ã€åŸºäºåˆ†æ­§æœ€å°åŒ–åå¥½ä¼˜åŒ–ï¼ˆDMPOï¼‰çš„æ‰©æ•£æ¨¡å‹å¯¹é½æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–åå‘KLåˆ†æ­§ï¼Œè¯¥æ–¹æ³•åœ¨ä¼˜åŒ–æ–¹å‘ä¸Šä¸åŸå¼ºåŒ–å­¦ä¹ ç›¸åŒã€‚æœ¬æ–‡è¿›è¡Œäº†ä¸¥æ ¼çš„åˆ†ææ¥è¯æ˜DMPOçš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡ç»¼åˆå®éªŒéªŒè¯äº†å…¶åœ¨äººç±»è¯„ä»·å’Œè‡ªåŠ¨æŒ‡æ ‡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚é€šè¿‡DMPOå¾®è°ƒæ‰©æ•£æ¨¡å‹èƒ½åœ¨ä¸åŒåŸºå‡†æ¨¡å‹å’Œæµ‹è¯•é›†ä¸Šå§‹ç»ˆè¶…è¶Šæˆ–åŒ¹é…ç°æœ‰æŠ€æœ¯ï¼Œä¸”åœ¨æ¯ä¸ªæ¡ˆä¾‹ä¸­å‡è·å¾—æœ€ä½³PickScoreï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å°†ç”Ÿæˆè¡Œä¸ºä¸æœŸæœ›è¾“å‡ºå¯¹é½æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒDMPOä¸ºåå¥½å¯¹é½æä¾›äº†ç¨³å¥è€Œä¼˜é›…çš„é€”å¾„ï¼Œå°†åŸåˆ™ç†è®ºä¸å®é™…æ€§èƒ½ç›¸ç»“åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚</li>
<li>é€šè¿‡ä¸è¯­è¨€æ¨¡å‹çš„ç»“åˆï¼Œäººä»¬æ­£åŠªåŠ›æ”¹è¿›æ‰©æ•£æ¨¡å‹ä»¥æ›´å¥½åœ°ä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>ç°æœ‰åå¥½ä¼˜åŒ–æ–¹æ³•å­˜åœ¨æ¬¡ä¼˜å‡å€¼å¯»æ±‚ä¼˜åŒ–çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„åŸºäºåˆ†æ­§æœ€å°åŒ–åå¥½ä¼˜åŒ–ï¼ˆDMPOï¼‰çš„æ–¹æ³•æ¥å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>DMPOé€šè¿‡æœ€å°åŒ–åå‘KLåˆ†æ­§æ¥ä¼˜åŒ–æ¨¡å‹ï¼Œå…¶ä¼˜åŒ–æ–¹å‘ä¸åŸå§‹å¼ºåŒ–å­¦ä¹ ç›¸åŒã€‚</li>
<li>DMPOçš„æœ‰æ•ˆæ€§å¾—åˆ°äº†ä¸¥æ ¼çš„åˆ†æå’Œå®éªŒéªŒè¯ï¼Œå…¶åœ¨å¤šä¸ªåŸºå‡†æ¨¡å‹å’Œæµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed9f397769edb4ec3b4bf1528dcfa0d9.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a65244dcadce7206ecc3e373c4bfe1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970273&auth_key=1759970273-0-0-5438654d8643d99a6785c320c565e416&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7f6c8f72a48fff710589833fbdf0a8ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970280&auth_key=1759970280-0-0-39e95f510e3591439d1754d1f385697c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Do-We-Need-All-the-Synthetic-Data-Targeted-Synthetic-Image-Augmentation-via-Diffusion-Models"><a href="#Do-We-Need-All-the-Synthetic-Data-Targeted-Synthetic-Image-Augmentation-via-Diffusion-Models" class="headerlink" title="Do We Need All the Synthetic Data? Targeted Synthetic Image Augmentation   via Diffusion Models"></a>Do We Need All the Synthetic Data? Targeted Synthetic Image Augmentation   via Diffusion Models</h2><p><strong>Authors:Dang Nguyen, Jiping Li, Jinghao Zheng, Baharan Mirzasoleiman</strong></p>
<p>Synthetically augmenting training datasets with diffusion models has been an effective strategy for improving generalization of image classifiers. However, existing techniques struggle to ensure the diversity of generation and increase the size of the data by up to 10-30x to improve the in-distribution performance. In this work, we show that synthetically augmenting part of the data that is not learned early in training with faithful images-containing same features but different noise-outperforms augmenting the entire dataset. By analyzing a two-layer CNN, we prove that this strategy improves generalization by promoting homogeneity in feature learning speed without amplifying noise. Our extensive experiments show that by augmenting only 30%-40% of the data, our method boosts generalization by up to 2.8% in a variety of scenarios, including training ResNet, ViT, ConvNeXt, and Swin Transformer on CIFAR-10&#x2F;100, and TinyImageNet, with various optimizers including SGD and SAM. Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on CIFAR-100 and TinyImageNet. </p>
<blockquote>
<p>åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œåˆæˆå¢å¼ºï¼Œæ˜¯æé«˜å›¾åƒåˆ†ç±»å™¨æ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯åœ¨ç¡®ä¿ç”Ÿæˆå¤šæ ·æ€§å’Œé€šè¿‡æ‰©å¤§æ•°æ®è§„æ¨¡ï¼ˆæœ€å¤šè¾¾10-30å€ï¼‰ä»¥æé«˜åˆ†å¸ƒå†…æ€§èƒ½æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§åˆæˆå¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¸“é—¨é’ˆå¯¹è®­ç»ƒè¿‡ç¨‹ä¸­æ—©æœŸæœªå­¦ä¼šçš„éƒ¨åˆ†æ•°æ®è¿›è¡Œå¢å¼ºï¼Œé€šè¿‡åŠ å…¥åŒ…å«ç›¸åŒç‰¹å¾ä½†å™ªå£°ä¸åŒçš„å¿ å®å›¾åƒï¼Œå…¶æ•ˆæœä¼˜äºå¢å¼ºæ•´ä¸ªæ•°æ®é›†ã€‚é€šè¿‡åˆ†æä¸¤å±‚å·ç§¯ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥ç­–ç•¥é€šè¿‡ä¿ƒè¿›ç‰¹å¾å­¦ä¹ é€Ÿåº¦çš„å‡ä¸€æ€§ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¸ä¼šæ”¾å¤§å™ªå£°ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ä»…å¢å¼º30%-40%çš„æ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸‹æé«˜äº†æ³›åŒ–æ€§èƒ½é«˜è¾¾2.8%ã€‚è¿™äº›åœºæ™¯åŒ…æ‹¬åœ¨CIFAR-10&#x2F;100å’ŒTinyImageNetä¸Šè®­ç»ƒResNetã€ViTã€ConvNeXtå’ŒSwin Transformeræ¨¡å‹ï¼Œä»¥åŠä½¿ç”¨SGDå’ŒSAMç­‰å¤šç§ä¼˜åŒ–å™¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸SGDç›¸ç»“åˆï¼Œåœ¨CIFAR-100å’ŒTinyImageNetä¸Šçš„æ€§èƒ½ä¼˜äºå½“å‰æœ€ä½³ä¼˜åŒ–å™¨SAMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21574v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆæˆæ‰©å……è®­ç»ƒæ•°æ®é›†æ˜¯æé«˜å›¾åƒåˆ†ç±»å™¨æ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯åœ¨ç¡®ä¿ç”Ÿæˆå¤šæ ·æ€§å’Œé€šè¿‡æ‰©å¤§æ•°æ®é›†è§„æ¨¡ï¼ˆé«˜è¾¾10-30å€ï¼‰ä»¥æé«˜å†…éƒ¨æ€§èƒ½åˆ†å¸ƒæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ˜¾ç¤ºï¼Œä»…å¯¹è®­ç»ƒæ—©æœŸæœªå­¦ä¼šçš„éƒ¨åˆ†æ•°æ®è¿›è¡Œåˆæˆæ‰©å……ï¼ŒåŒæ—¶å¼•å…¥åŒ…å«ç›¸åŒç‰¹å¾ä½†ä¸åŒå™ªå£°çš„å¿ å®å›¾åƒï¼Œå…¶æ•ˆæœä¼˜äºæ‰©å……æ•´ä¸ªæ•°æ®é›†ã€‚é€šè¿‡åˆ†æä¸¤å±‚å·ç§¯ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬è¯æ˜è¯¥ç­–ç•¥é€šè¿‡ä¿ƒè¿›ç‰¹å¾å­¦ä¹ é€Ÿåº¦çš„åŒè´¨æ€§ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¸ä¼šæ”¾å¤§å™ªå£°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä»…æ‰©å……30%-40%çš„æ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸‹å°†æ³›åŒ–èƒ½åŠ›æé«˜äº†2.8%ã€‚è¿™äº›åœºæ™¯åŒ…æ‹¬åœ¨CIFAR-10&#x2F;100ã€TinyImageNetä¸Šä½¿ç”¨ResNetã€ViTã€ConvNeXtå’ŒSwin Transformerè¿›è¡Œè®­ç»ƒï¼Œä»¥åŠä½¿ç”¨SGDå’ŒSAMç­‰å¤šç§ä¼˜åŒ–å™¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸SGDä¸€èµ·åº”ç”¨åœ¨CIFAR-100å’ŒTinyImageNetä¸Šçš„è¡¨ç°ä¼˜äºSOTAä¼˜åŒ–å™¨SAMã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åˆæˆæ‰©å……è®­ç»ƒæ•°æ®é›†æœ‰åŠ©äºæé«˜å›¾åƒåˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜åœ¨äºç¡®ä¿ç”Ÿæˆçš„å¤šæ ·æ€§å’Œæ‰©å¤§æ•°æ®é›†è§„æ¨¡ä»¥æé«˜å†…éƒ¨æ€§èƒ½åˆ†å¸ƒã€‚</li>
<li>ä»…å¯¹è®­ç»ƒæ—©æœŸæœªå­¦ä¼šçš„éƒ¨åˆ†æ•°æ®è¿›è¡Œåˆæˆæ‰©å……ï¼Œå¼•å…¥å¿ å®å›¾åƒï¼Œå…¶æ•ˆæœæ›´ä½³ã€‚</li>
<li>è¯¥ç­–ç•¥é€šè¿‡ä¿ƒè¿›ç‰¹å¾å­¦ä¹ é€Ÿåº¦çš„åŒè´¨æ€§å’Œé™ä½å™ªå£°ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä»…æ‰©å……30%-40%çš„æ•°æ®ï¼Œå°±èƒ½åœ¨å¤šç§åœºæ™¯ä¸‹æ˜¾è‘—æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šç§å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚CIFAR-10&#x2F;100ã€TinyImageNetï¼‰å’Œå¤šç§ä¼˜åŒ–å™¨ï¼ˆå¦‚SGDå’ŒSAMï¼‰ä¸‹éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸å…¶ä»–ä¼˜åŒ–å™¨ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸SGDç»“åˆä½¿ç”¨æ—¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-04de6d40e9c5666bd77378c765d013f1.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-539e21db637b6646d5a73008b15be342~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970295&auth_key=1759970295-0-0-77bdfc56698121549aa4107a3210bbee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="HOG-Diff-Higher-Order-Guided-Diffusion-for-Graph-Generation"><a href="#HOG-Diff-Higher-Order-Guided-Diffusion-for-Graph-Generation" class="headerlink" title="HOG-Diff: Higher-Order Guided Diffusion for Graph Generation"></a>HOG-Diff: Higher-Order Guided Diffusion for Graph Generation</h2><p><strong>Authors:Yiming Huang, Tolga Birdal</strong></p>
<p>Graph generation is a critical yet challenging task as empirical analyses require a deep understanding of complex, non-Euclidean structures. Diffusion models have recently made significant achievements in graph generation, but these models are typically adapted from image generation frameworks and overlook inherent higher-order topology, leaving them ill-suited for capturing the topological properties of graphs. In this work, we propose Higher-order Guided Diffusion (HOG-Diff), a principled framework that progressively generates plausible graphs with inherent topological structures. HOG-Diff follows a coarse-to-fine generation curriculum guided by higher-order topology and implemented via diffusion bridges. We further prove that our model exhibits a stronger theoretical guarantee than classical diffusion frameworks. Extensive experiments on both molecular and generic graph generation tasks demonstrate that our method consistently outperforms or remains competitive with state-of-the-art baselines. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Yiminghh/HOG-Diff">https://github.com/Yiminghh/HOG-Diff</a>. </p>
<blockquote>
<p>å›¾çš„ç”Ÿæˆæ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒéœ€è¦å¯¹å¤æ‚çš„éæ¬§å‡ é‡Œå¾—ç»“æ„è¿›è¡Œæ·±å…¥ç†è§£ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨å›¾çš„ç”Ÿæˆæ–¹é¢æœ€è¿‘å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹é€šå¸¸æ˜¯ä»å›¾åƒç”Ÿæˆæ¡†æ¶ä¸­æ”¹ç¼–è€Œæ¥ï¼Œå¿½ç•¥äº†å›ºæœ‰çš„é«˜é˜¶æ‹“æ‰‘ç»“æ„ï¼Œå¯¼è‡´å®ƒä»¬ä¸é€‚åˆæ•æ‰å›¾çš„æ‹“æ‰‘å±æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜é˜¶å¼•å¯¼æ‰©æ•£ï¼ˆHOG-Diffï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåŸç†çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé€æ­¥ç”Ÿæˆå…·æœ‰å›ºæœ‰æ‹“æ‰‘ç»“æ„çš„åˆç†å›¾ã€‚HOG-Difféµå¾ªç”±é«˜é˜¶æ‹“æ‰‘å¼•å¯¼çš„ç”±ç²—åˆ°ç»†çš„ç”Ÿæˆè¯¾ç¨‹ï¼Œå¹¶é€šè¿‡æ‰©æ•£æ¡¥å®ç°ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç›¸æ¯”ä¼ ç»Ÿçš„æ‰©æ•£æ¡†æ¶å…·æœ‰æ›´å¼ºçš„ç†è®ºä¿è¯ã€‚åœ¨åˆ†å­å’Œé€šç”¨å›¾ç”Ÿæˆä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæˆ–ä¿æŒä¸æœ€æ–°åŸºçº¿æ–¹æ³•çš„ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yiminghh/HOG-Diff">https://github.com/Yiminghh/HOG-Diff</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04308v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é«˜é˜¶å¼•å¯¼æ‰©æ•£ï¼ˆHOG-Diffï¼‰æ¡†æ¶ï¼Œç”¨äºé€æ­¥ç”Ÿæˆå…·æœ‰å†…åœ¨æ‹“æ‰‘ç»“æ„çš„å›¾ã€‚è¯¥æ¡†æ¶é€šè¿‡é«˜é˜¶æ‹“æ‰‘å¼•å¯¼ç²—åˆ°ç»†çš„ç”Ÿæˆè¯¾ç¨‹ï¼Œå¹¶é€šè¿‡æ‰©æ•£æ¡¥å®ç°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å­å’Œé€šç”¨å›¾ç”Ÿæˆä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸”è¾ƒä¼ ç»Ÿæ‰©æ•£æ¡†æ¶æœ‰æ›´å¼ºçš„ç†è®ºä¿è¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾å½¢ç”Ÿæˆä¸­å…·æœ‰æ˜¾è‘—æˆæœï¼Œä½†ç°æœ‰æ¨¡å‹å¿½ç•¥é«˜é˜¶æ‹“æ‰‘ç»“æ„ï¼Œéš¾ä»¥æ•æ‰å›¾å½¢çš„æ‹“æ‰‘å±æ€§ã€‚</li>
<li>æå‡ºé«˜é˜¶å¼•å¯¼æ‰©æ•£ï¼ˆHOG-Diffï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€æ­¥ç”Ÿæˆå…·æœ‰å†…åœ¨æ‹“æ‰‘ç»“æ„çš„å›¾å½¢ã€‚</li>
<li>HOG-Diffé‡‡ç”¨ç²—åˆ°ç»†çš„ç”Ÿæˆè¯¾ç¨‹ï¼Œé€šè¿‡é«˜é˜¶æ‹“æ‰‘å¼•å¯¼ï¼Œå¹¶é€šè¿‡æ‰©æ•£æ¡¥å®ç°ã€‚</li>
<li>HOG-Diffå…·æœ‰æ¯”ä¼ ç»Ÿæ‰©æ•£æ¡†æ¶æ›´å¼ºçš„ç†è®ºä¿è¯ã€‚</li>
<li>åœ¨åˆ†å­å’Œé€šç”¨å›¾ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒHOG-Diffæ€§èƒ½å“è¶Šï¼Œè¾ƒç°æœ‰æ–¹æ³•è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-af416b92f6f07d169aa84e94054dc056~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970302&auth_key=1759970302-0-0-bb05026a786e1f44b4def421fb3c399e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SEE-DPO-Self-Entropy-Enhanced-Direct-Preference-Optimization"><a href="#SEE-DPO-Self-Entropy-Enhanced-Direct-Preference-Optimization" class="headerlink" title="SEE-DPO: Self Entropy Enhanced Direct Preference Optimization"></a>SEE-DPO: Self Entropy Enhanced Direct Preference Optimization</h2><p><strong>Authors:Shivanshu Shekhar, Shreyas Singh, Tong Zhang</strong></p>
<p>Direct Preference Optimization (DPO) has been successfully used to align large language models (LLMs) according to human preferences, and more recently it has also been applied to improving the quality of text-to-image diffusion models. However, DPO-based methods such as SPO, Diffusion-DPO, and D3PO are highly susceptible to overfitting and reward hacking, especially when the generative model is optimized to fit out-of-distribution during prolonged training. To overcome these challenges and stabilize the training of diffusion models, we introduce a self-entropy regularization mechanism in reinforcement learning from human feedback. This enhancement improves DPO training by encouraging broader exploration and greater robustness. Our regularization technique effectively mitigates reward hacking, leading to improved stability and enhanced image quality across the latent space. Extensive experiments demonstrate that integrating human feedback with self-entropy regularization can significantly boost image diversity and specificity, achieving state-of-the-art results on key image generation metrics. </p>
<blockquote>
<p>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å·²æˆåŠŸç”¨äºæ ¹æ®äººç±»åå¥½å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæœ€è¿‘ä¹Ÿåº”ç”¨äºæé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è´¨é‡ã€‚ç„¶è€Œï¼ŒåŸºäºDPOçš„æ–¹æ³•ï¼Œå¦‚SPOã€Diffusion-DPOå’ŒD3POï¼Œéå¸¸å®¹æ˜“è¿‡åº¦æ‹Ÿåˆå’Œå¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œç‰¹åˆ«æ˜¯å½“ç”Ÿæˆæ¨¡å‹åœ¨é•¿æ—¶é—´çš„è®­ç»ƒè¿‡ç¨‹ä¸­è¢«ä¼˜åŒ–ä»¥é€‚åº”éåˆ†å¸ƒæ•°æ®æ—¶ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜å¹¶ç¨³å®šæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ä¸­çš„è‡ªæˆ‘ç†µæ­£åˆ™åŒ–æœºåˆ¶ã€‚è¿™ç§å¢å¼ºé€šè¿‡é¼“åŠ±æ›´å¹¿æ³›çš„æ¢ç´¢å’Œæ›´å¤§çš„ç¨³å¥æ€§ï¼Œæ”¹è¿›äº†DPOè®­ç»ƒã€‚æˆ‘ä»¬çš„æ­£åˆ™åŒ–æŠ€æœ¯æœ‰æ•ˆåœ°å‡è½»äº†å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œæé«˜äº†æ½œä¼ç©ºé—´ä¸­çš„ç¨³å®šæ€§å’Œå›¾åƒè´¨é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå°†äººç±»åé¦ˆä¸è‡ªæˆ‘ç†µæ­£åˆ™åŒ–ç›¸ç»“åˆï¼Œå¯ä»¥æ˜¾è‘—æé«˜å›¾åƒçš„å¤šæ ·æ€§å’Œç‰¹å¼‚æ€§ï¼Œå¹¶åœ¨å…³é”®å›¾åƒç”ŸæˆæŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04712v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­åº”ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚è¿‡åº¦æ‹Ÿåˆå’Œå¥–åŠ±æ»¥ç”¨ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜å¹¶ç¨³å®šæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªæˆ‘ç†µæ­£åˆ™åŒ–æœºåˆ¶ã€‚è¿™ç§æœºåˆ¶èƒ½å¤Ÿæ”¹å–„DPOè®­ç»ƒï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œå›¾åƒè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œç»“åˆäººç±»åé¦ˆçš„è‡ªæˆ‘ç†µæ­£åˆ™åŒ–å¯ä»¥æ˜¾è‘—æé«˜å›¾åƒçš„å¤šæ ·æ€§å’Œç‰¹å¼‚æ€§ï¼Œè¾¾åˆ°å›¾åƒç”Ÿæˆçš„å…³é”®æŒ‡æ ‡çš„æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å·²æˆåŠŸåº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¼˜åŒ–ã€‚</li>
<li>DPOæ–¹æ³•é¢ä¸´è¿‡åº¦æ‹Ÿåˆå’Œå¥–åŠ±æ»¥ç”¨çš„é—®é¢˜ã€‚</li>
<li>è‡ªæˆ‘ç†µæ­£åˆ™åŒ–æœºåˆ¶è¢«å¼•å…¥ä»¥è§£å†³DPOæ–¹æ³•çš„æŒ‘æˆ˜å¹¶ç¨³å®šæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>è‡ªæˆ‘ç†µæ­£åˆ™åŒ–æœºåˆ¶é¼“åŠ±æ¨¡å‹è¿›è¡Œæ›´å¹¿æ³›çš„æ¢ç´¢ï¼Œæé«˜ç¨³å¥æ€§ã€‚</li>
<li>ç»“åˆäººç±»åé¦ˆçš„è‡ªæˆ‘ç†µæ­£åˆ™åŒ–èƒ½æ˜¾è‘—æé«˜å›¾åƒçš„å¤šæ ·æ€§å’Œç‰¹å¼‚æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•è¾¾åˆ°å›¾åƒç”Ÿæˆçš„å…³é”®æŒ‡æ ‡çš„æœ€å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-554204357a9c8c611953e0f0b8d4d7f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac67299929fb6c306bcb53170cb48ab8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Fast-constrained-sampling-in-pre-trained-diffusion-models"><a href="#Fast-constrained-sampling-in-pre-trained-diffusion-models" class="headerlink" title="Fast constrained sampling in pre-trained diffusion models"></a>Fast constrained sampling in pre-trained diffusion models</h2><p><strong>Authors:Alexandros Graikos, Nebojsa Jojic, Dimitris Samaras</strong></p>
<p>Large denoising diffusion models, such as Stable Diffusion, have been trained on billions of image-caption pairs to perform text-conditioned image generation. As a byproduct of this training, these models have acquired general knowledge about image statistics, which can be useful for other inference tasks. However, when confronted with sampling an image under new constraints, e.g. generating the missing parts of an image, using large pre-trained text-to-image diffusion models is inefficient and often unreliable. Previous approaches either utilized backpropagation through the denoiser network, making them significantly slower and more memory-demanding than simple text-to-image generation, or only enforced the constraint locally, failing to capture critical long-range correlations in the sampled image. In this work, we propose an algorithm that enables fast, high-quality generation under arbitrary constraints. We show that in denoising diffusion models, we can employ an approximation to Newtonâ€™s optimization method that allows us to speed up inference and avoid the expensive backpropagation operations. Our approach produces results that rival or surpass the state-of-the-art training-free inference methods while requiring a fraction of the time. We demonstrate the effectiveness of our algorithm under both linear (inpainting, super-resolution) and non-linear (style-guided generation) constraints. An implementation is provided at <a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/fast-constrained-sampling">https://github.com/cvlab-stonybrook/fast-constrained-sampling</a>. </p>
<blockquote>
<p>å¤§å‹å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰å·²ç»æ¥å—äº†æ•°åäº¿å¼ å›¾åƒæ ‡é¢˜å¯¹çš„è®­ç»ƒï¼Œç”¨äºæ‰§è¡Œæ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆã€‚ä½œä¸ºè¿™ç§è®­ç»ƒçš„å‰¯äº§å“ï¼Œè¿™äº›æ¨¡å‹å·²ç»è·å¾—äº†å…³äºå›¾åƒç»Ÿè®¡çš„é€šç”¨çŸ¥è¯†ï¼Œè¿™å¯¹äºå…¶ä»–æ¨ç†ä»»åŠ¡å¯èƒ½æ˜¯æœ‰ç”¨çš„ã€‚ç„¶è€Œï¼Œå½“é¢ä¸´åœ¨æ–°çš„çº¦æŸä¸‹é‡‡æ ·å›¾åƒæ—¶ï¼Œä¾‹å¦‚ç”Ÿæˆå›¾åƒç¼ºå¤±çš„éƒ¨åˆ†ï¼Œä½¿ç”¨å¤§å‹é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ•ˆç‡ä½ä¸‹ä¸”å¾€å¾€ä¸å¯é ã€‚ä»¥å‰çš„æ–¹æ³•è¦ä¹ˆé€šè¿‡å»å™ªå™¨ç½‘ç»œè¿›è¡Œåå‘ä¼ æ’­ï¼Œè¿™ä½¿å¾—å®ƒä»¬æ¯”ç®€å•çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ…¢å¾—å¤šä¸”æ›´å å†…å­˜ï¼Œè¦ä¹ˆåªå±€éƒ¨å¼ºåˆ¶æ‰§è¡Œçº¦æŸï¼Œæ— æ³•æ•è·é‡‡æ ·å›¾åƒä¸­å…³é”®çš„é•¿ç¨‹å…³è”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„çº¦æŸä¸‹å®ç°å¿«é€Ÿã€é«˜è´¨é‡ç”Ÿæˆã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨å»å™ªæ‰©æ•£æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ç‰›é¡¿ä¼˜åŒ–æ–¹æ³•çš„è¿‘ä¼¼å€¼ï¼Œè¿™å…è®¸æˆ‘ä»¬åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶é¿å…æ˜‚è´µçš„åå‘ä¼ æ’­æ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿçš„ç»“æœå¯ä¸æœ€å…ˆè¿›çš„æ— è®­ç»ƒæ¨ç†æ–¹æ³•ç›¸åŒ¹æ•Œæˆ–è¶…è¶Šï¼ŒåŒæ—¶æ‰€éœ€æ—¶é—´å¤§å¤§å‡å°‘ã€‚æˆ‘ä»¬åœ¨çº¿æ€§ï¼ˆå¡«å……ã€è¶…åˆ†è¾¨ç‡ï¼‰å’Œéçº¿æ€§ï¼ˆé£æ ¼å¼•å¯¼ç”Ÿæˆï¼‰çº¦æŸä¸‹å±•ç¤ºäº†ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³å®ç°å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/fast-constrained-sampling">https://github.com/cvlab-stonybrook/fast-constrained-sampling</a> äº†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18804v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œå¦‚Stable Diffusionï¼Œå·²è®­ç»ƒäºæ•°åäº¿å›¾åƒ-æ ‡é¢˜å¯¹ä¸Šï¼Œæ‰§è¡Œæ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆã€‚æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—äº†å…³äºå›¾åƒç»Ÿè®¡çš„é€šç”¨çŸ¥è¯†ï¼Œå¯ç”¨äºå…¶ä»–æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“é¢ä¸´åœ¨æ–°çš„çº¦æŸä¸‹é‡‡æ ·å›¾åƒæ—¶ï¼Œå¦‚ç”Ÿæˆå›¾åƒçš„ç¼ºå¤±éƒ¨åˆ†ï¼Œä½¿ç”¨å¤§å‹é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ•ˆç‡ä½ä¸‹ä¸”å¸¸ä¸å¯é ã€‚ä»¥å‰çš„æ–¹æ³•è¦ä¹ˆé€šè¿‡å»å™ªå™¨ç½‘ç»œè¿›è¡Œåå‘ä¼ æ’­ï¼Œä½¿å…¶æ¯”ç®€å•æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ›´æ…¢ã€æ›´å å†…å­˜ï¼Œè¦ä¹ˆä»…å±€éƒ¨å¼ºåˆ¶æ‰§è¡Œçº¦æŸï¼Œæ— æ³•æ•è·é‡‡æ ·å›¾åƒä¸­å…³é”®çš„é•¿ç¨‹å…³è”ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œå¯åœ¨ä»»æ„çº¦æŸä¸‹å®ç°å¿«é€Ÿã€é«˜è´¨é‡ç”Ÿæˆã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨æ‰©æ•£æ¨¡å‹ä¸­é‡‡ç”¨ç‰›é¡¿ä¼˜åŒ–æ–¹æ³•çš„è¿‘ä¼¼ï¼Œå¯ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶é¿å…æ˜‚è´µçš„åå‘ä¼ æ’­æ“ä½œã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨ä¸æœ€æ–°æ— è®­ç»ƒæ¨ç†æ–¹æ³•æ¯”è¾ƒæ—¶æ•ˆæœæ˜¾è‘—ï¼Œæ‰€éœ€æ—¶é—´æ›´å°‘ã€‚æˆ‘ä»¬åœ¨çº¿æ€§ï¼ˆå¦‚è¡¥å…¨ã€è¶…åˆ†è¾¨ç‡ï¼‰å’Œéçº¿æ€§ï¼ˆå¦‚é£æ ¼å¼•å¯¼ç”Ÿæˆï¼‰çº¦æŸä¸‹éªŒè¯äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“å®ç°è¯·å‚è§<a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/fast-constrained-sampling%E3%80%82">https://github.com/cvlab-stonybrook/fast-constrained-samplingã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å»å™ªæ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionå·²è®­ç»ƒäºæ•°åäº¿å›¾åƒ-æ ‡é¢˜å¯¹ï¼Œå…·å¤‡æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶è·å¾—äº†å›¾åƒç»Ÿè®¡çš„é€šç”¨çŸ¥è¯†ã€‚</li>
<li>é¢å¯¹æ–°çš„çº¦æŸï¼ˆå¦‚ç”Ÿæˆå›¾åƒçš„ç¼ºå¤±éƒ¨åˆ†ï¼‰ï¼Œç›´æ¥ä½¿ç”¨å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ•ˆç‡ä½ä¸‹ä¸”ä¸ç¨³å®šã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼šé€šè¿‡å»å™ªå™¨ç½‘ç»œè¿›è¡Œåå‘ä¼ æ’­å¯¼è‡´é€Ÿåº¦æ…¢ã€å†…å­˜éœ€æ±‚å¤§ï¼›ä»…å±€éƒ¨çº¦æŸæ— æ³•æ•è·é•¿ç¨‹å…³è”ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œèƒ½åœ¨ä»»æ„çº¦æŸä¸‹å®ç°å¿«é€Ÿã€é«˜è´¨é‡ç”Ÿæˆï¼Œé‡‡ç”¨ç‰›é¡¿ä¼˜åŒ–æ–¹æ³•çš„è¿‘ä¼¼ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶é¿å…åå‘ä¼ æ’­æ“ä½œã€‚</li>
<li>è¯¥ç®—æ³•åœ¨å¤šç§çº¿æ€§ä¸éçº¿æ€§çº¦æŸä¸‹å‡è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœï¼ŒåŒ…æ‹¬è¡¥å…¨ã€è¶…åˆ†è¾¨ç‡å’Œé£æ ¼å¼•å¯¼ç”Ÿæˆç­‰ä»»åŠ¡ã€‚</li>
<li>è¯¥ç®—æ³•ç›¸è¾ƒäºç°æœ‰æ–¹æ³•æ›´é«˜æ•ˆï¼Œæ‰€éœ€æ—¶é—´æ›´å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-700ce00cd60f896cf0be6206c396ebf2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f13361333dc9af8950aed9be199143e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38fe64e645c690067577d760b7883777.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-2f53e005bf22c398472fd6a17cc258c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970345&auth_key=1759970345-0-0-419e63b2e24b29ac76795a5429af3d94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Cross-Domain-Graph-Data-Scaling-A-Showcase-with-Diffusion-Models"><a href="#Cross-Domain-Graph-Data-Scaling-A-Showcase-with-Diffusion-Models" class="headerlink" title="Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models"></a>Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models</h2><p><strong>Authors:Wenzhuo Tang, Haitao Mao, Danial Dervovic, Ivan Brugere, Saumitra Mishra, Yuying Xie, Jiliang Tang</strong></p>
<p>Models for natural language and images benefit from data scaling behavior: the more data fed into the model, the better they perform. This â€˜better with moreâ€™ phenomenon enables the effectiveness of large-scale pre-training on vast amounts of data. However, current graph pre-training methods struggle to scale up data due to heterogeneity across graphs. To achieve effective data scaling, we aim to develop a general model that is able to capture diverse data patterns of graphs and can be utilized to adaptively help the downstream tasks. To this end, we propose UniAug, a universal graph structure augmentor built on a diffusion model. We first pre-train a discrete diffusion model on thousands of graphs across domains to learn the graph structural patterns. In the downstream phase, we provide adaptive enhancement by conducting graph structure augmentation with the help of the pre-trained diffusion model via guided generation. By leveraging the pre-trained diffusion model for structure augmentation, we consistently achieve performance improvements across various downstream tasks in a plug-and-play manner. To the best of our knowledge, this study represents the first demonstration of a data-scaling graph structure augmentor on graphs across domains. </p>
<blockquote>
<p>æ¨¡å‹çš„è‡ªç„¶è¯­è¨€å’Œå›¾åƒå—ç›Šäºæ•°æ®ç¼©æ”¾è¡Œä¸ºï¼šè¾“å…¥æ¨¡å‹çš„æ•°æ®è¶Šå¤šï¼Œå…¶æ€§èƒ½å°±è¶Šå¥½ã€‚â€œè¶Šå¤šè¶Šå¥½â€çš„ç°è±¡è¯æ˜äº†å¤§è§„æ¨¡é¢„è®­ç»ƒåœ¨å¤§é‡æ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œç”±äºå›¾ä¹‹é—´çš„å¼‚è´¨æ€§ï¼Œå½“å‰çš„å›¾é¢„è®­ç»ƒæ–¹æ³•éš¾ä»¥æ‰©å¤§æ•°æ®è§„æ¨¡ã€‚ä¸ºäº†å®ç°æœ‰æ•ˆçš„æ•°æ®ç¼©æ”¾ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿæ•è·å›¾çš„å¤šæ ·æ•°æ®æ¨¡å¼å¹¶å¯ç”¨äºè‡ªé€‚åº”å¸®åŠ©ä¸‹æ¸¸ä»»åŠ¡çš„é€šç”¨æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é€šç”¨å›¾ç»“æ„å¢å¼ºå™¨UniAugã€‚æˆ‘ä»¬é¦–å…ˆåœ¨è·¨åŸŸçš„æ•°åƒä¸ªå›¾ä¸Šå¯¹ç¦»æ•£æ‰©æ•£æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ å›¾ç»“æ„æ¨¡å¼ã€‚åœ¨ä¸‹æ¸¸é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡å€ŸåŠ©é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¼•å¯¼ç”Ÿæˆæ¥è¿›è¡Œå›¾ç»“æ„å¢å¼ºï¼Œæä¾›è‡ªé€‚åº”å¢å¼ºã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œç»“æ„å¢å¼ºï¼Œæˆ‘ä»¬åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­é€šè¿‡å³æ’å³ç”¨æ–¹å¼å§‹ç»ˆå®ç°äº†æ€§èƒ½æ”¹è¿›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹ç ”ç©¶ä»£è¡¨äº†è·¨åŸŸå›¾ä¸Šæ•°æ®ç¼©æ”¾å›¾ç»“æ„å¢å¼ºå™¨çš„é¦–æ¬¡å±•ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01899v3">PDF</a> NeurIPSâ€™25</p>
<p><strong>Summary</strong><br>     æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ä¸å›¾åƒå—ç›Šäºæ•°æ®è§„æ¨¡åŒ–çš„è¡Œä¸ºï¼šè¾“å…¥æ¨¡å‹çš„æ•°æ®è¶Šå¤šï¼Œå…¶è¡¨ç°æ€§èƒ½è¶Šå¥½ã€‚è¿™ç§ç°è±¡ä½¿å¾—å¤§è§„æ¨¡é¢„è®­ç»ƒåœ¨å¤§é‡æ•°æ®ä¸Šéå¸¸æœ‰æ•ˆã€‚ç„¶è€Œï¼Œç”±äºå›¾çš„å¼‚è´¨æ€§ï¼Œå½“å‰å›¾é¢„è®­ç»ƒæ–¹æ³•åœ¨æ‰©å¤§æ•°æ®è§„æ¨¡æ—¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†å®ç°æœ‰æ•ˆçš„æ•°æ®è§„æ¨¡åŒ–ï¼Œæˆ‘ä»¬æ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿæ•æ‰å›¾çš„å¤šæ ·æ•°æ®æ¨¡å¼å¹¶èƒ½è‡ªé€‚åº”åœ°å¸®åŠ©ä¸‹æ¸¸ä»»åŠ¡çš„é€šç”¨æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é€šç”¨å›¾ç»“æ„å¢å¼ºå™¨UniAugã€‚æˆ‘ä»¬é¦–å…ˆåœ¨ä¸åŒçš„åŸŸä¸Šå¯¹æˆåƒä¸Šä¸‡çš„å›¾è¿›è¡Œç¦»æ•£æ‰©æ•£æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ å›¾ç»“æ„æ¨¡å¼ã€‚åœ¨ä¸‹æ¸¸é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾ç»“æ„å¢å¼ºï¼Œé€šè¿‡å¼•å¯¼ç”Ÿæˆå®ç°è‡ªé€‚åº”å¢å¼ºã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œç»“æ„å¢å¼ºï¼Œæˆ‘ä»¬åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†æ€§èƒ½çš„æå‡ï¼Œæ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ–¹å¼ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬ç ”ç©¶æ˜¯é¦–æ¬¡åœ¨è·¨åŸŸå›¾ä¸Šè¿›è¡Œæ•°æ®è§„æ¨¡åŒ–çš„å›¾ç»“æ„å¢å¼ºå™¨çš„å±•ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®è§„æ¨¡åŒ–å¯¹äºæå‡æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒå¤„ç†é¢†åŸŸã€‚</li>
<li>å½“å‰å›¾é¢„è®­ç»ƒæ–¹æ³•é¢ä¸´å¼‚è´¨æ€§é—®é¢˜ï¼Œé™åˆ¶äº†æ•°æ®è§„æ¨¡çš„æ‰©å¤§ã€‚</li>
<li>UniAugæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„é€šç”¨å›¾ç»“æ„å¢å¼ºå™¨ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>UniAugé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ å›¾ç»“æ„æ¨¡å¼ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿›è¡Œè‡ªé€‚åº”å¢å¼ºã€‚</li>
<li>UniAugé€šè¿‡å¼•å¯¼ç”Ÿæˆå®ç°å›¾ç»“æ„å¢å¼ºï¼Œæå‡æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>UniAugæ˜¯é¦–ä¸ªè·¨åŸŸå›¾æ•°æ®è§„æ¨¡åŒ–çš„å›¾ç»“æ„å¢å¼ºå™¨çš„å±•ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01899">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2112ebe7a20aa1c18fad28c3200307a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970352&auth_key=1759970352-0-0-9692609a58c79177cc06ba4b43de114b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d8a49b9ad7cf1379fb79885c342297e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970359&auth_key=1759970359-0-0-76a55f7762fb76e166d85ddf5b7043d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d719f61c2aec191574bedda4c034879b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759970366&auth_key=1759970366-0-0-2d57f879b33cd5ba981d31d30857ac2f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-9f78c76d80bb6d2c472a34bc7e62d6a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6fb8c21ba8482b417c5e599c9dff2bf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-09/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-09/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-09/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b25471fc451c86d9a2c2f4d32d007346.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-09  Periodontal Bone Loss Analysis via Keypoint Detection With Heuristic   Post-Processing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-09/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-21090328a2b5b6ea458ec253a1247bb9.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-09  NGGAN Noise Generation GAN Based on the Practical Measurement Dataset   for Narrowband Powerline Communications
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
