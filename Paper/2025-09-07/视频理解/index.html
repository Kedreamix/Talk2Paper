<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  VideoRewardBench Comprehensive Evaluation of Multimodal Reward Models   for Video Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6f1b4bd471705c36ac62904a918f4210.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    46 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="VideoRewardBench-Comprehensive-Evaluation-of-Multimodal-Reward-Models-for-Video-Understanding"><a href="#VideoRewardBench-Comprehensive-Evaluation-of-Multimodal-Reward-Models-for-Video-Understanding" class="headerlink" title="VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models   for Video Understanding"></a>VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models   for Video Understanding</h2><p><strong>Authors:Zhihong Zhang, Xiaojian Huang, Jin Xu, Zhuodong Luo, Xinzhi Wang, Jiansheng Wei, Xuejin Chen</strong></p>
<p>Multimodal reward models (MRMs) play a crucial role in the training, inference, and evaluation of Large Vision Language Models (LVLMs) by assessing response quality. However, existing benchmarks for evaluating MRMs in the video domain suffer from a limited number and diversity of questions, a lack of comprehensive evaluation dimensions, and inadequate evaluation of diverse types of MRMs. To address these gaps, we introduce VideoRewardBench, the first comprehensive benchmark covering four core aspects of video understanding: perception, knowledge, reasoning, and safety. Through our AI-assisted data pipeline, we curate a high-quality preference dataset of 1,563 annotated samples, including 1,482 unique videos and 1,559 distinct questionsâ€“15 times the number found in the most question-rich prior benchmark. Each sample is a triplet consisting of a video-text prompt, a chosen response, and a rejected response. We also conduct a comprehensive evaluation across 28 multimodal reward models spanning three categories: generative, discriminative, and semi-scalar. Results show that even the top-performing model GPT-4o achieves only 57.0% overall accuracy, and the state-of-the-art open-source model Qwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three key insights: (i) MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization than those trained without RL; (ii) except for discriminative MRMs, other types of MRMs across varying model capacities can benefit from inference-time scaling; and (iii) variations in input video frame count have different effects on different types of MRMs. We believe VideoRewardBench offers a challenging and valuable benchmark for advancing the evaluation and development of MRMs in the video domain. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è®­ç»ƒã€æ¨ç†å’Œè¯„ä¼°ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œé€šè¿‡å¯¹å“åº”è´¨é‡è¿›è¡Œè¯„ä¼°æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰è§†é¢‘é¢†åŸŸçš„MRMè¯„ä¼°åŸºå‡†æµ‹è¯•å­˜åœ¨è®¸å¤šé—®é¢˜ï¼Œå¦‚é—®é¢˜æ•°é‡å’Œå¤šæ ·æ€§æœ‰é™ã€è¯„ä¼°ç»´åº¦ä¸å…¨é¢ä»¥åŠå¯¹å¤šç§MRMçš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoRewardBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢æ¶µç›–è§†é¢‘ç†è§£çš„å››ä¸ªæ ¸å¿ƒæ–¹é¢çš„åŸºå‡†æµ‹è¯•ï¼šæ„ŸçŸ¥ã€çŸ¥è¯†ã€æ¨ç†å’Œå®‰å…¨ã€‚é€šè¿‡æˆ‘ä»¬çš„äººå·¥æ™ºèƒ½è¾…åŠ©æ•°æ®ç®¡é“ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªé«˜è´¨é‡çš„é¦–é€‰æ•°æ®é›†ï¼ŒåŒ…å«1563ä¸ªæ³¨é‡Šæ ·æœ¬ï¼Œå…¶ä¸­åŒ…æ‹¬1482ä¸ªå”¯ä¸€è§†é¢‘å’Œ1559ä¸ªä¸åŒçš„é—®é¢˜ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢é—®é¢˜æœ€ä¸°å¯Œçš„åŸºå‡†æµ‹è¯•çš„15å€ã€‚æ¯ä¸ªæ ·æœ¬éƒ½ç”±ä¸€ä¸ªè§†é¢‘æ–‡æœ¬æç¤ºã€ä¸€ä¸ªé€‰å®šå“åº”å’Œä¸€ä¸ªè¢«æ‹’ç»å“åº”ç»„æˆçš„ä¸‰å…ƒç»„ã€‚æˆ‘ä»¬è¿˜å¯¹28ç§è·¨ä¸‰ç±»åˆ«çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼šç”Ÿæˆå¼ã€åˆ¤åˆ«å¼å’ŒåŠæ ‡é‡ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿è¡¨ç°æœ€ä½³çš„GPT-4oæ¨¡å‹æ€»ä½“å‡†ç¡®ç‡ä¹Ÿåªæœ‰57.0%ï¼Œè€Œæœ€å…ˆè¿›çš„å¼€æºæ¨¡å‹Qwen2.5-VL-72Bä»…è¾¾åˆ°53.3%ã€‚æˆ‘ä»¬çš„è¿›ä¸€æ­¥åˆ†æè¿˜æ­ç¤ºäº†ä¸‰ä¸ªå…³é”®è§è§£ï¼šï¼ˆiï¼‰ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹å¹¶ä¸ä¸€å®šæ¯”æœªç”¨RLè®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ï¼›ï¼ˆiiï¼‰é™¤äº†åˆ¤åˆ«å¼MRMå¤–ï¼Œå…¶ä»–ç±»å‹çš„MRMåœ¨å„ç§æ¨¡å‹å®¹é‡ä¸Šéƒ½å¯ä»¥ä»æ¨ç†æ—¶é—´ç¼©æ”¾ä¸­å—ç›Šï¼›ï¼ˆiiiï¼‰è¾“å…¥è§†é¢‘å¸§è®¡æ•°çš„å˜åŒ–å¯¹ä¸åŒç±»å‹çš„MRMæœ‰ä¸åŒçš„å½±å“ã€‚æˆ‘ä»¬ç›¸ä¿¡VideoRewardBenchä¸ºæ¨è¿›è§†é¢‘é¢†åŸŸMRMçš„è¯„ä¼°å’Œå¼€å‘æä¾›äº†ä¸€ä¸ªæœ‰æŒ‘æˆ˜æ€§å’Œä»·å€¼çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00484v1">PDF</a> <a target="_blank" rel="noopener" href="https://videorewardbench.github.io/">https://videorewardbench.github.io/</a></p>
<p><strong>Summary</strong></p>
<pre><code>å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è®­ç»ƒã€æ¨ç†å’Œè¯„ä¼°ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘é¢†åŸŸMRMsè¯„ä¼°åŸºå‡†å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¦‚é—®é¢˜æ•°é‡æœ‰é™ã€ç¼ºä¹å…¨é¢è¯„ä¼°ç»´åº¦ä»¥åŠå¯¹ä¸åŒç±»å‹MRMsçš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VideoRewardBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¦†ç›–è§†é¢‘ç†è§£çš„å››ä¸ªæ ¸å¿ƒæ–¹é¢çš„å…¨é¢åŸºå‡†ï¼šæ„ŸçŸ¥ã€çŸ¥è¯†ã€æ¨ç†å’Œå®‰å…¨ã€‚é€šè¿‡äººå·¥æ™ºèƒ½è¾…åŠ©çš„æ•°æ®ç®¡é“ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªé«˜è´¨é‡çš„é¦–é€‰æ•°æ®é›†ï¼ŒåŒ…å«1563ä¸ªæ³¨é‡Šæ ·æœ¬ï¼ŒåŒ…æ‹¬1482ä¸ªç‹¬ç‰¹è§†é¢‘å’Œ1559ä¸ªä¸åŒé—®é¢˜ï¼Œæ•°é‡æ˜¯æœ€å¯Œæœ‰çš„å…ˆå‰åŸºå‡†çš„åäº”å€ã€‚æˆ‘ä»¬è¿˜å¯¹è·¨è¶Šä¸‰ä¸ªç±»åˆ«çš„28ä¸ªå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼šç”Ÿæˆå¼ã€åˆ¤åˆ«å¼å’ŒåŠæ ‡é‡å¼ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„GPT-4oæ¨¡å‹ä¹Ÿä»…è¾¾åˆ°ç™¾åˆ†ä¹‹äº”åä¸ƒçš„æ•´ä½“å‡†ç¡®ç‡ï¼Œæœ€å…ˆè¿›çš„å¼€æºæ¨¡å‹Qwen2.5-VL-72Bä¹Ÿä»…è¾¾åˆ°ç™¾åˆ†ä¹‹äº”åä¸‰ç‚¹ä¸‰ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸‰ä¸ªå…³é”®è§è§£ã€‚æˆ‘ä»¬ç›¸ä¿¡VideoRewardBenchä¸ºæ¨è¿›è§†é¢‘é¢†åŸŸMRMsçš„è¯„ä¼°å’Œå‘å±•æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œä»·å€¼çš„åŸºå‡†ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰åœ¨è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å“åº”è´¨é‡ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰è§†é¢‘é¢†åŸŸMRMsè¯„ä¼°åŸºå‡†å­˜åœ¨å±€é™æ€§ï¼Œå¦‚é—®é¢˜æ•°é‡å°‘ã€ç¼ºä¹å…¨é¢è¯„ä¼°ç»´åº¦ä»¥åŠå¯¹ä¸åŒç±»å‹MRMsçš„è¯„ä¼°ä¸è¶³ã€‚</li>
<li>VideoRewardBenchæ˜¯é¦–ä¸ªè¦†ç›–è§†é¢‘ç†è§£çš„å››ä¸ªæ ¸å¿ƒæ–¹é¢çš„å…¨é¢åŸºå‡†ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€çŸ¥è¯†ã€æ¨ç†å’Œå®‰å…¨ã€‚</li>
<li>VideoRewardBenché€šè¿‡AIè¾…åŠ©çš„æ•°æ®ç®¡é“æ•´ç†äº†ä¸€ä¸ªé«˜è´¨é‡çš„é¦–é€‰æ•°æ®é›†ï¼Œæ ·æœ¬åŒ…å«è§†é¢‘æ–‡æœ¬æç¤ºã€é€‰å®šå›åº”å’Œæ‹’ç»å›åº”çš„ä¸‰å…ƒç»„ã€‚</li>
<li>ç°æœ‰é¡¶å°–æ¨¡å‹å¦‚GPT-4oå’ŒQwen2.5-VL-72Båœ¨VideoRewardBenchä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œæ•´ä½“å‡†ç¡®ç‡æœ‰å¾…æé«˜ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„MRMså¹¶ä¸ä¸€å®šèƒ½è¡¨ç°å‡ºæ›´å¼ºçš„è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00484">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7763401f591442d9b03f793555e10def.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f1b4bd471705c36ac62904a918f4210.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aaa64bd8fa2c7d393c96cb5e8898737d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06aa474a6012853ecab9060b8951a27b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-130c41deda181854e90f7cab81a612f3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SurgLLM-A-Versatile-Large-Multimodal-Model-with-Spatial-Focus-and-Temporal-Awareness-for-Surgical-Video-Understanding"><a href="#SurgLLM-A-Versatile-Large-Multimodal-Model-with-Spatial-Focus-and-Temporal-Awareness-for-Surgical-Video-Understanding" class="headerlink" title="SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and   Temporal Awareness for Surgical Video Understanding"></a>SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and   Temporal Awareness for Surgical Video Understanding</h2><p><strong>Authors:Zhen Chen, Xingjian Luo, Kun Yuan, Jinlin Wu, Danny T. M. Chan, Nassir Navab, Hongbin Liu, Zhen Lei, Jiebo Luo</strong></p>
<p>Surgical video understanding is crucial for facilitating Computer-Assisted Surgery (CAS) systems. Despite significant progress in existing studies, two major limitations persist, including inadequate visual content perception and insufficient temporal awareness in surgical videos, and hinder the development of versatile CAS solutions. In this work, we propose the SurgLLM framework, an effective large multimodal model tailored for versatile surgical video understanding tasks with enhanced spatial focus and temporal awareness. Specifically, to empower the spatial focus of surgical videos, we first devise Surgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the video encoder of SurgLLM, by performing instrument-centric Masked Video Reconstruction (MV-Recon) and subsequent multimodal alignment. To incorporate surgical temporal knowledge into SurgLLM, we further propose Temporal-aware Multimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleaved multimodal embeddings. Moreover, to accommodate various understanding tasks of surgical videos without conflicts, we devise a Surgical Task Dynamic Ensemble to efficiently triage a query with optimal learnable parameters in our SurgLLM. Extensive experiments performed on diverse surgical video understanding tasks, including captioning, general VQA, and temporal VQA, demonstrate significant improvements over the state-of-the-art approaches, validating the effectiveness of our SurgLLM in versatile surgical video understanding. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/franciszchen/SurgLLM">https://github.com/franciszchen/SurgLLM</a>. </p>
<blockquote>
<p>æ‰‹æœ¯è§†é¢‘ç†è§£å¯¹äºä¿ƒè¿›è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ï¼ˆCASï¼‰ç³»ç»Ÿçš„å‘å±•è‡³å…³é‡è¦ã€‚å°½ç®¡ç°æœ‰ç ”ç©¶å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¸¤å¤§å±€é™ï¼ŒåŒ…æ‹¬æ‰‹æœ¯è§†é¢‘ä¸­è§†è§‰å†…å®¹æ„ŸçŸ¥ä¸è¶³å’Œæ—¶é—´æ„ŸçŸ¥ä¸è¶³ï¼Œè¿™é˜»ç¢äº†å¤šåŠŸèƒ½CASè§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SurgLLMæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œé’ˆå¯¹é€šç”¨çš„æ‰‹æœ¯è§†é¢‘ç†è§£ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå…·æœ‰å¢å¼ºçš„ç©ºé—´ç„¦ç‚¹å’Œæ—¶é—´æ„ŸçŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å¢å¼ºæ‰‹æœ¯è§†é¢‘çš„ç©ºé—´ç„¦ç‚¹ï¼Œæˆ‘ä»¬ä¸ºSurgLLMçš„è§†é¢‘ç¼–ç å™¨è®¾è®¡äº†æ‰‹æœ¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ¨¡æ€é¢„è®­ç»ƒï¼ˆSurg-Pretrainï¼‰ï¼Œé€šè¿‡æ‰§è¡Œä»¥ä»ªå™¨ä¸ºä¸­å¿ƒçš„æ©ç è§†é¢‘é‡å»ºï¼ˆMV-Reconï¼‰å’Œéšåçš„å¤šæ¨¡æ€å¯¹é½ã€‚ä¸ºäº†å°†æ‰‹æœ¯æ—¶é—´çŸ¥è¯†èå…¥SurgLLMï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†æ—¶é—´æ„ŸçŸ¥å¤šæ¨¡æ€è°ƒæ•´ï¼ˆTM-Tuningï¼‰ï¼Œä»¥å¢å¼ºå…·æœ‰äº¤é”™å¤šæ¨¡æ€åµŒå…¥çš„æ—¶é—´æ¨ç†ã€‚æ­¤å¤–ï¼Œä¸ºäº†å®¹çº³å„ç§æ‰‹æœ¯è§†é¢‘ç†è§£ä»»åŠ¡è€Œä¸äº§ç”Ÿå†²çªï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ‰‹æœ¯ä»»åŠ¡åŠ¨æ€é›†æˆï¼Œä»¥åœ¨æˆ‘ä»¬çš„SurgLLMä¸­ä»¥æœ€ä½³çš„å¯å­¦ä¹ å‚æ•°æœ‰æ•ˆåœ°å¤„ç†æŸ¥è¯¢ã€‚åœ¨å¤šç§æ‰‹æœ¯è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬æè¿°ã€ä¸€èˆ¬è§†é¢‘é—®ç­”å’Œæ—¶é—´è§†é¢‘é—®ç­”ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„SurgLLMç›¸è¾ƒäºæœ€æ–°æ–¹æ³•æœ‰æ˜æ˜¾çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶åœ¨é€šç”¨æ‰‹æœ¯è§†é¢‘ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/franciszchen/SurgLLM">https://github.com/franciszchen/SurgLLM</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00357v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶é’ˆå¯¹è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ç³»ç»Ÿä¸­çš„æ‰‹æœ¯è§†é¢‘ç†è§£é—®é¢˜ï¼Œæå‡ºäº†SurgLLMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·å¤‡å¢å¼ºçš„ç©ºé—´èšç„¦å’Œæ—¶åºæ„ŸçŸ¥èƒ½åŠ›ï¼Œç”¨äºå®Œæˆå¤šæ ·åŒ–çš„æ‰‹æœ¯è§†é¢‘ç†è§£ä»»åŠ¡ã€‚é€šè¿‡Surgical Context-aware Multimodal Pretrainingï¼ˆSurg-Pretrainï¼‰å’ŒTemporal-aware Multimodal Tuningï¼ˆTM-Tuningï¼‰æŠ€æœ¯ï¼Œæé«˜äº†è§†é¢‘çš„ç©ºé—´èšç„¦å’Œæ—¶åºæ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé€šè¿‡Surgical Task Dynamic Ensembleç­–ç•¥ï¼Œå®ç°ä¸åŒç†è§£ä»»åŠ¡çš„çµæ´»å¤„ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSurgLLMåœ¨æ‰‹æœ¯è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹æœ¯è§†é¢‘ç†è§£åœ¨è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨è§†è§‰å†…å®¹æ„ŸçŸ¥å’Œæ—¶åºæ„ŸçŸ¥ä¸è¶³ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>SurgLLMæ¡†æ¶è¢«æå‡ºä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå…·å¤‡å¢å¼ºçš„ç©ºé—´èšç„¦å’Œæ—¶åºæ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>SurgLLMé€šè¿‡Surgical Context-aware Multimodal Pretrainingï¼ˆSurg-Pretrainï¼‰æé«˜è§†é¢‘çš„ç©ºé—´èšç„¦èƒ½åŠ›ï¼Œé€šè¿‡MV-Reconå’Œéšåçš„å¤šæ¨¡å¼å¯¹é½å®ç°ã€‚</li>
<li>Temporal-aware Multimodal Tuningï¼ˆTM-Tuningï¼‰è¢«ç”¨äºå¢å¼ºSurgLLMçš„æ—¶åºæ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡äº¤æ›¿å¤šæ¨¡å¼åµŒå…¥å®ç°ã€‚</li>
<li>Surgical Task Dynamic Ensembleç­–ç•¥å…è®¸SurgLLMé€‚åº”ä¸åŒçš„æ‰‹æœ¯è§†é¢‘ç†è§£ä»»åŠ¡ï¼ŒåŒæ—¶é¿å…å†²çªã€‚</li>
<li>åœ¨å¤šä¸ªæ‰‹æœ¯è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSurgLLMæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e15aaa0ebd40fc2a977c6c5131cdf73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d538c872c4f8ecdc69bd0fcb4a995c4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e2f9061ee82d823d6d119d4cfa6f871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ed95784a4217472e0748017774d1a54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-954e51f4d59639f98226c887eaf8b75d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Looking-Beyond-the-Obvious-A-Survey-on-Abstract-Concept-Recognition-for-Video-Understanding"><a href="#Looking-Beyond-the-Obvious-A-Survey-on-Abstract-Concept-Recognition-for-Video-Understanding" class="headerlink" title="Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for   Video Understanding"></a>Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for   Video Understanding</h2><p><strong>Authors:Gowreesh Mago, Pascal Mettes, Stevan Rudinac</strong></p>
<p>The automatic understanding of video content is advancing rapidly. Empowered by deeper neural networks and large datasets, machines are increasingly capable of understanding what is concretely visible in video frames, whether it be objects, actions, events, or scenes. In comparison, humans retain a unique ability to also look beyond concrete entities and recognize abstract concepts like justice, freedom, and togetherness. Abstract concept recognition forms a crucial open challenge in video understanding, where reasoning on multiple semantic levels based on contextual information is key. In this paper, we argue that the recent advances in foundation models make for an ideal setting to address abstract concept understanding in videos. Automated understanding of high-level abstract concepts is imperative as it enables models to be more aligned with human reasoning and values. In this survey, we study different tasks and datasets used to understand abstract concepts in video content. We observe that, periodically and over a long period, researchers have attempted to solve these tasks, making the best use of the tools available at their disposal. We advocate that drawing on decades of community experience will help us shed light on this important open grand challenge and avoid &#96;&#96;re-inventing the wheelâ€™â€™ as we start revisiting it in the era of multi-modal foundation models. </p>
<blockquote>
<p>è§†é¢‘å†…å®¹çš„è‡ªåŠ¨ç†è§£æ­£åœ¨è¿…é€Ÿå‘å±•ã€‚åœ¨æ›´æ·±çš„ç¥ç»ç½‘ç»œå’Œå¤§æ•°æ®çš„æ”¯æŒä¸‹ï¼Œæœºå™¨è¶Šæ¥è¶Šèƒ½ç†è§£è§†é¢‘å¸§ä¸­å…·ä½“å¯è§çš„å†…å®¹ï¼Œæ— è®ºæ˜¯ç‰©ä½“ã€åŠ¨ä½œã€äº‹ä»¶è¿˜æ˜¯åœºæ™¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»è¿˜æ‹¥æœ‰è¶…è¶Šå…·ä½“å®ä½“çš„ç‹¬ç‰¹èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¯†åˆ«æ­£ä¹‰ã€è‡ªç”±ã€å›¢ç»“ç­‰æŠ½è±¡æ¦‚å¿µã€‚æŠ½è±¡æ¦‚å¿µè¯†åˆ«æ˜¯è§†é¢‘ç†è§£ä¸­çš„ä¸€ä¸ªå…³é”®å¼€æ”¾æŒ‘æˆ˜ï¼Œå…¶ä¸­åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯åœ¨å¤šè¯­ä¹‰å±‚é¢è¿›è¡Œæ¨ç†æ˜¯å…³é”®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºæœ€è¿‘çš„è¿›å±•åŸºç¡€æ¨¡å‹ä¸ºè§£å†³è§†é¢‘ä¸­çš„æŠ½è±¡æ¦‚å¿µç†è§£æä¾›äº†ç†æƒ³çš„å¹³å°ã€‚å¯¹é«˜çº§æŠ½è±¡æ¦‚å¿µçš„è‡ªåŠ¨ç†è§£è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯ä»¥ä½¿æ¨¡å‹æ›´ç¬¦åˆäººç±»æ¨ç†å’Œä»·å€¼è§‚ã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”¨äºç†è§£è§†é¢‘å†…å®¹ä¸­æŠ½è±¡æ¦‚å¿µçš„ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ã€‚æˆ‘ä»¬å‘ç°ï¼Œç ”ç©¶äººå‘˜ä¼šå®šæœŸå¹¶é•¿æœŸå°è¯•è§£å†³è¿™äº›ä»»åŠ¡ï¼Œå……åˆ†åˆ©ç”¨ç°æœ‰å·¥å…·ã€‚æˆ‘ä»¬ä¸»å¼ å€Ÿé‰´ç¤¾åŒºå‡ åå¹´çš„ç»éªŒï¼Œè¿™å°†æœ‰åŠ©äºæˆ‘ä»¬è§£å†³è¿™ä¸€é‡è¦çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œé¿å…åœ¨å¤šåª’ä½“åŸºç¡€æ¨¡å‹çš„å¤å…´æ—¶ä»£â€œé‡è¹ˆè¦†è¾™â€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20765v1">PDF</a> Under Review for IJCV</p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘å†…å®¹è‡ªåŠ¨ç†è§£é¢†åŸŸæ­£è¿…é€Ÿå‘å±•ï¼Œå¾—ç›Šäºæ›´æ·±çš„ç¥ç»ç½‘ç»œå’Œå¤§æ•°æ®é›†çš„æ”¯æŒï¼Œæœºå™¨è¶Šæ¥è¶Šèƒ½ç†è§£è§†é¢‘å¸§ä¸­å…·ä½“å¯è§çš„å†…å®¹ï¼Œå¦‚ç‰©ä½“ã€åŠ¨ä½œã€äº‹ä»¶æˆ–åœºæ™¯ã€‚ç„¶è€Œï¼Œäººç±»ç‹¬æœ‰çš„èƒ½åŠ›ä¹Ÿèƒ½è¯†åˆ«æŠ½è±¡æ¦‚å¿µï¼Œå¦‚æ­£ä¹‰ã€è‡ªç”±å’Œå›¢ç»“ã€‚æŠ½è±¡æ¦‚å¿µè¯†åˆ«æ˜¯è§†é¢‘ç†è§£ä¸­çš„å…³é”®å¼€æ”¾æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨å¤šä¸ªè¯­ä¹‰å±‚æ¬¡ä¸Šæ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚æœ¬æ–‡ä¸»å¼ åˆ©ç”¨æœ€æ–°çš„åŸºç¡€æ¨¡å‹æ¥è§£å†³è§†é¢‘ä¸­çš„æŠ½è±¡æ¦‚å¿µç†è§£é—®é¢˜ã€‚å¯¹é«˜çº§æŠ½è±¡æ¦‚å¿µçš„è‡ªåŠ¨ç†è§£è‡³å…³é‡è¦ï¼Œå› ä¸ºè¿™èƒ½ä½¿æ¨¡å‹æ›´ç¬¦åˆäººç±»æ¨ç†å’Œä»·å€¼è§‚ã€‚æœ¬æ–‡è°ƒæŸ¥äº†ç†è§£è§†é¢‘å†…å®¹ä¸­æŠ½è±¡æ¦‚å¿µçš„ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ï¼Œå¹¶å‘ç°ç ”ç©¶è€…ä»¬å·²ç»å°è¯•è§£å†³è¿™äº›é—®é¢˜ï¼Œå¦‚ä½•åˆ©ç”¨ç°æœ‰çš„æœ€ä½³å·¥å…·å°¤ä¸ºé‡è¦ã€‚æˆ‘ä»¬ç›¸ä¿¡å€Ÿé‰´ç¤¾åŒºå¤šå¹´çš„ç»éªŒå°†æœ‰åŠ©äºè§£å†³è¿™ä¸€é‡è¦çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œé¿å…åœ¨å¤šåª’ä½“åŸºç¡€æ¨¡å‹æ—¶ä»£â€œé‡è¹ˆè¦†è¾™â€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å†…å®¹è‡ªåŠ¨ç†è§£é¢†åŸŸæ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œå¾—ç›ŠäºæŠ€æœ¯è¿›æ­¥å¦‚æ›´æ·±çš„ç¥ç»ç½‘ç»œå’Œå¤§æ•°æ®é›†çš„åº”ç”¨ã€‚</li>
<li>æœºå™¨èƒ½å¤Ÿç†è§£è§†é¢‘å¸§ä¸­çš„å…·ä½“å¯è§å†…å®¹ï¼ˆå¦‚ç‰©ä½“ã€åŠ¨ä½œç­‰ï¼‰ï¼Œä½†äººç±»è¿˜èƒ½è¯†åˆ«æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚æ­£ä¹‰ã€è‡ªç”±ç­‰ï¼‰ã€‚</li>
<li>æŠ½è±¡æ¦‚å¿µè¯†åˆ«æ˜¯è§†é¢‘ç†è§£é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ä¹‹ä¸€ï¼Œéœ€è¦åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯åœ¨å¤šä¸ªè¯­ä¹‰å±‚æ¬¡ä¸Šè¿›è¡Œæ¨ç†ã€‚</li>
<li>åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸ºè§£å†³è§†é¢‘ä¸­çš„æŠ½è±¡æ¦‚å¿µç†è§£é—®é¢˜æä¾›äº†ç†æƒ³å·¥å…·ã€‚</li>
<li>é«˜çº§æŠ½è±¡æ¦‚å¿µçš„è‡ªåŠ¨ç†è§£è‡³å…³é‡è¦ï¼Œå› ä¸ºè¿™æœ‰åŠ©äºæ¨¡å‹ä¸äººç±»æ¨ç†å’Œä»·å€¼è§‚çš„åŒ¹é…ã€‚</li>
<li>è§†é¢‘ç†è§£é¢†åŸŸçš„ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†è¢«å¹¿æ³›ç ”ç©¶ç”¨äºæŠ½è±¡æ¦‚å¿µçš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20765">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ffbeffc41bba50667bebf4be93e6eae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5996a70d3853e4fa565b193f05877f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-912d2bd475b76db4d0455a2ac18cf87e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ada419a022c2e786448b7ba1e703394.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb5386331f87643867a6d82f3d60c781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63d2019ed4ef142f8ce8045b50635595.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding"><a href="#Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding" class="headerlink" title="Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding"></a>Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding</h2><p><strong>Authors:Yuan Xie, Tianshui Chen, Zheng Ge, Lionel Ni</strong></p>
<p>Long-form video understanding, characterized by long-range temporal dependencies and multiple events, remains a challenge. Existing methods often rely on static reasoning or external visual-language models (VLMs), which face issues like complexity and sub-optimal performance due to the lack of end-to-end training. In this paper, we propose Video-MTR, a reinforced multi-turn reasoning framework designed to enable iterative key video segment selection and question comprehension. Unlike traditional video reasoning pipeline, which generate predictions in a single turn, Video-MTR performs reasoning in multiple turns, selecting video segments progressively based on the evolving understanding of previously processed segments and the current question. This iterative process allows for a more refined and contextually aware analysis of the video. To ensure intermediate reasoning process, we introduce a novel gated bi-level reward system, combining trajectory-level rewards based on answer correctness and turn-level rewards emphasizing frame-query relevance. This system optimizes both video segment selection and question comprehension, eliminating the need for external VLMs and allowing end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema demonstrate that Video-MTR outperforms existing methods in both accuracy and efficiency, advancing the state-of-the-art in long video understanding. </p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå…¶ç‰¹ç‚¹æ˜¯å…·æœ‰é•¿æ—¶ä¾èµ–æ€§ï¼Œå­˜åœ¨å¤šä¸ªäº‹ä»¶ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºé™æ€æ¨ç†æˆ–å¤–éƒ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œç”±äºç¼ºä¹ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œè¿™äº›æ–¹æ³•é¢ä¸´å¤æ‚æ€§å’Œæ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Video-MTRï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºçš„å¤šè½®æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è¿­ä»£çš„å…³é”®è§†é¢‘ç‰‡æ®µé€‰æ‹©å’Œé—®é¢˜ç†è§£ã€‚ä¸ä¼ ç»Ÿçš„è§†é¢‘æ¨ç†æµç¨‹ä¸åŒï¼ŒVideo-MTRåœ¨å¤šè½®ä¸­è¿›è¡Œæ¨ç†ï¼Œæ ¹æ®å·²å¤„ç†ç‰‡æ®µçš„æ¼”å˜ç†è§£å’Œå½“å‰é—®é¢˜ï¼Œé€æ­¥é€‰æ‹©è§†é¢‘ç‰‡æ®µã€‚è¿™ç§è¿­ä»£è¿‡ç¨‹å…è®¸å¯¹è§†é¢‘è¿›è¡Œæ›´ç²¾ç»†å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆ†æã€‚ä¸ºç¡®ä¿ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„é—¨æ§ä¸¤çº§å¥–åŠ±ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆåŸºäºç­”æ¡ˆæ­£ç¡®æ€§çš„è½¨è¿¹çº§å¥–åŠ±å’Œå¼ºè°ƒå¸§æŸ¥è¯¢ç›¸å…³æ€§çš„è½®æ¬¡çº§å¥–åŠ±ã€‚è¯¥ç³»ç»Ÿä¼˜åŒ–äº†è§†é¢‘ç‰‡æ®µçš„é€‰æ‹©å’Œé—®é¢˜ç†è§£ï¼Œæ— éœ€å¤–éƒ¨VLMsï¼Œå¹¶å®ç°äº†ç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚åœ¨VideoMMEã€MLVUå’ŒEgoSchemaç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVideo-MTRåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæ¨åŠ¨äº†é•¿è§†é¢‘ç†è§£çš„æœ€æ–°è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20478v1">PDF</a> 15 pages, 9 figures</p>
<p><strong>Summary</strong>ï¼š<br>é’ˆå¯¹é•¿è§†é¢‘ç†è§£ä¸­çš„é•¿æ—¶é—´åºåˆ—ä¾èµ–å’Œå¤šäº‹ä»¶é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Video-MTRæ¡†æ¶ï¼Œå®ç°äº†è¿­ä»£çš„å…³é”®è§†é¢‘ç‰‡æ®µé€‰æ‹©å’Œé—®é¢˜ç†è§£ã€‚ä¸ä¼ ç»Ÿçš„è§†é¢‘æ¨ç†æµç¨‹ä¸åŒï¼ŒVideo-MTRé‡‡ç”¨å¤šè½®æ¨ç†ï¼ŒåŸºäºå·²å¤„ç†ç‰‡æ®µçš„æ·±å…¥ç†è§£ä»¥åŠå½“å‰é—®é¢˜ï¼Œé€æ­¥é€‰æ‹©è§†é¢‘ç‰‡æ®µã€‚ä¸ºä¼˜åŒ–ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼Œå¼•å…¥äº†æ–°å‹çš„é—¨æ§åŒçº§å¥–åŠ±ç³»ç»Ÿï¼Œç»“åˆäº†åŸºäºç­”æ¡ˆæ­£ç¡®æ€§çš„è½¨è¿¹çº§å¥–åŠ±å’Œå¼ºè°ƒå¸§æŸ¥è¯¢ç›¸å…³æ€§çš„å›åˆçº§å¥–åŠ±ã€‚è¯¥æ¡†æ¶æ— éœ€å¤–éƒ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œå¹¶åœ¨VideoMMEã€MLVUå’ŒEgoSchemaç­‰åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é•¿è§†é¢‘ç†è§£å­˜åœ¨æŒ‘æˆ˜ï¼Œå› æ¶‰åŠé•¿æ—¶é—´åºåˆ—ä¾èµ–å’Œå¤šäº‹ä»¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–é™æ€æ¨ç†æˆ–å¤–éƒ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå­˜åœ¨å¤æ‚æ€§å’Œæ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚</li>
<li>Video-MTRæ¡†æ¶é‡‡ç”¨å¤šè½®æ¨ç†ï¼Œå®ç°å…³é”®è§†é¢‘ç‰‡æ®µçš„è¿­ä»£é€‰æ‹©å’Œé—®é¢˜ç†è§£ã€‚</li>
<li>Video-MTRåŸºäºå·²å¤„ç†ç‰‡æ®µçš„æ·±å…¥ç†è§£åŠå½“å‰é—®é¢˜é€æ­¥é€‰æ‹©è§†é¢‘ç‰‡æ®µï¼Œå®ç°æ›´ç²¾ç»†ã€æ›´å…·ä¸Šä¸‹æ–‡æ„è¯†çš„åˆ†æã€‚</li>
<li>å¼•å…¥æ–°å‹çš„é—¨æ§åŒçº§å¥–åŠ±ç³»ç»Ÿï¼Œä¼˜åŒ–ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼Œç»“åˆè½¨è¿¹çº§å’Œå›åˆçº§å¥–åŠ±ã€‚</li>
<li>Video-MTRæ¡†æ¶æ— éœ€å¤–éƒ¨VLMsï¼Œå®ç°ç«¯åˆ°ç«¯è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4c3e711164667eea8e35a846c1ee0a3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-391b1b9f7f50061c4f0f0545ff743986.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70270e476f8981b773a05b61029b4535.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f21dccf7473cd6b72ab825e87e02cdb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CVBench-Evaluating-Cross-Video-Synergies-for-Complex-Multimodal-Understanding-and-Reasoning"><a href="#CVBench-Evaluating-Cross-Video-Synergies-for-Complex-Multimodal-Understanding-and-Reasoning" class="headerlink" title="CVBench: Evaluating Cross-Video Synergies for Complex Multimodal   Understanding and Reasoning"></a>CVBench: Evaluating Cross-Video Synergies for Complex Multimodal   Understanding and Reasoning</h2><p><strong>Authors:Nannan Zhu, Yonghao Dong, Teng Wang, Xueqian Li, Shengjun Deng, Yijia Wang, Zheng Hong, Tiantian Geng, Guo Niu, Hanyan Huang, Xiongfei Yao, Shuaiwei Jiao</strong></p>
<p>While multimodal large language models (MLLMs) exhibit strong performance on single-video tasks (e.g., video question answering), their ability across multiple videos remains critically underexplored. However, this capability is essential for real-world applications, including multi-camera surveillance and cross-video procedural learning. To bridge this gap, we present CVBench, the first comprehensive benchmark designed to assess cross-video relational reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning three hierarchical tiers: cross-video object association (identifying shared entities), cross-video event association (linking temporal or causal event chains), and cross-video complex reasoning (integrating commonsense and domain knowledge). Built from five domain-diverse video clusters (e.g., sports, life records), the benchmark challenges models to synthesise information across dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought prompting paradigms. Key findings reveal stark performance gaps: even top models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks, compared to the 91% accuracy of human performance. Crucially, our analysis reveals fundamental bottlenecks inherent in current MLLM architectures, notably deficient inter-video context retention and poor disambiguation of overlapping entities. CVBench establishes a rigorous framework for diagnosing and advancing multi-video reasoning, offering architectural insights for next-generation MLLMs. The data and evaluation code are available at <a target="_blank" rel="noopener" href="https://github.com/Hokhim2/CVBench">https://github.com/Hokhim2/CVBench</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å•è§†é¢‘ä»»åŠ¡ï¼ˆä¾‹å¦‚è§†é¢‘é—®ç­”ï¼‰ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨å¤šä¸ªè§†é¢‘ä¸Šçš„èƒ½åŠ›ä»ç„¶è¢«ä¸¥é‡å¿½è§†ã€‚ç„¶è€Œï¼Œè¿™ç§èƒ½åŠ›å¯¹äºåŒ…æ‹¬å¤šæ‘„åƒå¤´ç›‘æ§å’Œè·¨è§†é¢‘ç¨‹åºå­¦ä¹ åœ¨å†…çš„å®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CVBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°è·¨è§†é¢‘å…³ç³»æ¨ç†çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚CVBenchåŒ…å«1000ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–ä¸‰ä¸ªå±‚æ¬¡ï¼šè·¨è§†é¢‘å¯¹è±¡å…³è”ï¼ˆè¯†åˆ«å…±äº«å®ä½“ï¼‰ã€è·¨è§†é¢‘äº‹ä»¶å…³è”ï¼ˆé“¾æ¥æ—¶é—´æˆ–å› æœäº‹ä»¶é“¾ï¼‰ï¼Œä»¥åŠè·¨è§†é¢‘å¤æ‚æ¨ç†ï¼ˆæ•´åˆå¸¸è¯†å’Œé¢†åŸŸçŸ¥è¯†ï¼‰ã€‚è¯¥åŸºå‡†æµ‹è¯•ç”±äº”ä¸ªé¢†åŸŸå¤šæ ·çš„è§†é¢‘é›†ç¾¤ï¼ˆä¾‹å¦‚ä½“è‚²ã€ç”Ÿæ´»è®°å½•ï¼‰ç»„æˆï¼ŒæŒ‘æˆ˜æ¨¡å‹åœ¨åŠ¨æ€è§†è§‰ç¯å¢ƒä¸­çš„ä¿¡æ¯åˆæˆèƒ½åŠ›ã€‚å¯¹10å¤šä¸ªé¢†å…ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oã€Gemini-2.0-flashã€Qwen2.5-VLï¼‰è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œé‡‡ç”¨é›¶æ ·æœ¬æˆ–æ€ç»´é“¾æç¤ºèŒƒå¼ã€‚å…³é”®ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯é¡¶çº§æ¨¡å‹å¦‚GPT-4oï¼Œåœ¨å› æœæ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰60%ï¼Œè€Œäººç±»çš„è¡¨ç°å‡†ç¡®ç‡ä¸º91%ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å½“å‰MLLMæ¶æ„çš„å†…åœ¨æ ¹æœ¬ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯ç¼ºä¹è·¨è§†é¢‘ä¸Šä¸‹æ–‡ä¿ç•™å’Œé‡å å®ä½“çš„è¾¨åˆ«èƒ½åŠ›ã€‚CVBenchä¸ºè¯Šæ–­å’Œæ¨è¿›å¤šè§†é¢‘æ¨ç†æä¾›äº†ä¸¥æ ¼çš„æ¡†æ¶ï¼Œå¹¶ä¸ºä¸‹ä¸€ä»£MLLMæä¾›äº†æ¶æ„è§è§£ã€‚æ•°æ®å’Œè¯„ä¼°ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hokhim2/CVBench">https://github.com/Hokhim2/CVBench</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19542v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å•è§†é¢‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå¦‚åœ¨è§†é¢‘é—®ç­”ä¸­ï¼Œä½†å®ƒä»¬åœ¨è·¨å¤šä¸ªè§†é¢‘çš„èƒ½åŠ›æ–¹é¢ä»å¾…å……åˆ†æ¢ç´¢ã€‚è¿™å¯¹ç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬å¤šæ‘„åƒå¤´ç›‘æ§å’Œè·¨è§†é¢‘è¿‡ç¨‹å­¦ä¹ ç­‰é¢†åŸŸã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºCVBenchï¼Œå®ƒæ˜¯é¦–ä¸ªæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°è·¨è§†é¢‘å…³ç³»æ¨ç†çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ã€‚CVBenchåŒ…å«è·¨è¶Šä¸‰ä¸ªå±‚æ¬¡çš„1000ä¸ªé—®ç­”å¯¹ï¼šè·¨è§†é¢‘å¯¹è±¡å…³è”ï¼ˆè¯†åˆ«å…±äº«å®ä½“ï¼‰ã€è·¨è§†é¢‘äº‹ä»¶å…³è”ï¼ˆé“¾æ¥æ—¶é—´æˆ–å› æœäº‹ä»¶é“¾ï¼‰ï¼Œä»¥åŠè·¨è§†é¢‘å¤æ‚æ¨ç†ï¼ˆæ•´åˆå¸¸è¯†å’Œé¢†åŸŸçŸ¥è¯†ï¼‰ã€‚è¯¥åŸºå‡†æµ‹è¯•ç”±äº”ä¸ªé¢†åŸŸå¤šæ ·çš„è§†é¢‘é›†ç¾¤æ„æˆï¼ˆå¦‚ä½“è‚²ã€ç”Ÿæ´»è®°å½•ï¼‰ï¼ŒæŒ‘æˆ˜æ¨¡å‹åœ¨åŠ¨æ€è§†è§‰ç¯å¢ƒä¸­çš„ä¿¡æ¯ç»¼åˆã€‚å¯¹åä½™æ¬¾é¢†å…ˆMLLMsçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼šå³ä½¿æ˜¯é¡¶å°–æ¨¡å‹å¦‚GPT-4oï¼Œåœ¨å› æœæ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰60%ï¼Œè€Œäººç±»å‡†ç¡®ç‡é«˜è¾¾91%ã€‚åˆ†ææ­ç¤ºäº†å½“å‰MLLMæ¶æ„çš„å†…åœ¨ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯è·¨è§†é¢‘ä¸Šä¸‹æ–‡ä¿ç•™ä¸è¶³å’Œé‡å å®ä½“è¾¨æèƒ½åŠ›å·®ã€‚CVBenchä¸ºè¯Šæ–­å’Œæ”¹è¿›å¤šè§†é¢‘æ¨ç†æä¾›äº†ä¸¥æ ¼æ¡†æ¶ï¼Œå¹¶ä¸ºä¸‹ä¸€ä»£MLLMsæä¾›äº†æ¶æ„è§è§£ã€‚æ•°æ®å’Œè¯„ä¼°ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hokhim2/CVBench">https://github.com/Hokhim2/CVBench</a>è·å¾—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è·¨å¤šä¸ªè§†é¢‘çš„èƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°½ç®¡åœ¨å•è§†é¢‘ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>è·¨è§†é¢‘å…³ç³»æ¨ç†èƒ½åŠ›å¯¹äºç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬å¤šæ‘„åƒå¤´ç›‘æ§å’Œè·¨è§†é¢‘è¿‡ç¨‹å­¦ä¹ ã€‚</li>
<li>CVBenchæ˜¯é¦–ä¸ªè¯„ä¼°è·¨è§†é¢‘å…³ç³»æ¨ç†çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸‰ä¸ªå±‚æ¬¡çš„é—®ç­”å¯¹ï¼šè·¨è§†é¢‘å¯¹è±¡å…³è”ã€è·¨è§†é¢‘äº‹ä»¶å…³è”å’Œè·¨è§†é¢‘å¤æ‚æ¨ç†ã€‚</li>
<li>CVBenchç”±äº”ä¸ªé¢†åŸŸå¤šæ ·çš„è§†é¢‘é›†ç¾¤æ„æˆï¼ŒæŒ‘æˆ˜æ¨¡å‹åœ¨åŠ¨æ€è§†è§‰ç¯å¢ƒä¸­çš„ä¿¡æ¯ç»¼åˆã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰MLLMsåœ¨è·¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šæ€§èƒ½æœ‰é™ï¼Œç‰¹åˆ«æ˜¯å› æœæ¨ç†ä»»åŠ¡ã€‚</li>
<li>åˆ†ææ­ç¤ºäº†MLLMæ¶æ„çš„å†…åœ¨ç“¶é¢ˆï¼Œå¦‚è·¨è§†é¢‘ä¸Šä¸‹æ–‡ä¿ç•™ä¸è¶³å’Œé‡å å®ä½“è¾¨æèƒ½åŠ›å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b83647e098cc0fbaa1ce3e56252cd8a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9a3737a923aed6de1b6e78c299b7284.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8349c3496fc3547179fa000c27b34ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3266e21dacf308ce281d05ee15a1b560.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e12f14860c26338bb80d1f47b277846b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61c3c061cd91b15acc7da2172640dcc2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Controllable-Hybrid-Captioner-for-Improved-Long-form-Video-Understanding"><a href="#Controllable-Hybrid-Captioner-for-Improved-Long-form-Video-Understanding" class="headerlink" title="Controllable Hybrid Captioner for Improved Long-form Video Understanding"></a>Controllable Hybrid Captioner for Improved Long-form Video Understanding</h2><p><strong>Authors:Kuleen Sasse, Efsun Sarioglu Kayi, Arun Reddy</strong></p>
<p>Video data, especially long-form video, is extremely dense and high-dimensional. Text-based summaries of video content offer a way to represent query-relevant content in a much more compact manner than raw video. In addition, textual representations are easily ingested by state-of-the-art large language models (LLMs), which enable reasoning over video content to answer complex natural language queries. To solve this issue, we rely on the progressive construction of a text-based memory by a video captioner operating on shorter chunks of the video, where spatio-temporal modeling is computationally feasible. We explore ways to improve the quality of the activity log comprised solely of short video captions. Because the video captions tend to be focused on human actions, and questions may pertain to other information in the scene, we seek to enrich the memory with static scene descriptions using Vision Language Models (VLMs). Our video understanding system relies on the LaViLa video captioner in combination with a LLM to answer questions about videos. We first explored different ways of partitioning the video into meaningful segments such that the textual descriptions more accurately reflect the structure of the video content. Furthermore, we incorporated static scene descriptions into the captioning pipeline using LLaVA VLM, resulting in a more detailed and complete caption log and expanding the space of questions that are answerable from the textual memory. Finally, we have successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline compared to using separate captioning models for the two tasks. Our model, controllable hybrid captioner, can alternate between different types of captions according to special input tokens that signals scene changes detected in the video. </p>
<blockquote>
<p>è§†é¢‘æ•°æ®ï¼Œç‰¹åˆ«æ˜¯é•¿è§†é¢‘ï¼Œæä¸ºå¯†é›†ä¸”é«˜ç»´ã€‚åŸºäºæ–‡æœ¬çš„è§†é¢‘å†…å®¹æ‘˜è¦æä¾›äº†ä¸€ç§æ¯”åŸå§‹è§†é¢‘æ›´ç´§å‡‘åœ°è¡¨ç¤ºæŸ¥è¯¢ç›¸å…³å†…å®¹çš„æ–¹å¼ã€‚æ­¤å¤–ï¼Œæ–‡æœ¬è¡¨ç¤ºå¾ˆå®¹æ˜“è¢«æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰€æ¥çº³ï¼Œè¿™ä½¿å¾—èƒ½å¤Ÿæ¨ç†è§†é¢‘å†…å®¹ä»¥å›ç­”å¤æ‚çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬ä¾èµ–äºç”±æ“ä½œè§†é¢‘è¾ƒçŸ­ç‰‡æ®µçš„è§†é¢‘å­—å¹•å™¨é€æ­¥æ„å»ºçš„åŸºäºæ–‡æœ¬çš„å­˜å‚¨å™¨ï¼Œå…¶ä¸­æ—¶ç©ºå»ºæ¨¡æ˜¯è®¡ç®—å¯è¡Œçš„ã€‚æˆ‘ä»¬æ¢ç´¢äº†æé«˜ä»…ç”±çŸ­è§†é¢‘å­—å¹•æ„æˆçš„æ´»åŠ¨æ—¥å¿—è´¨é‡çš„æ–¹æ³•ã€‚ç”±äºè§†é¢‘å­—å¹•å¾€å¾€ä¾§é‡äºäººç±»è¡Œä¸ºï¼Œè€Œé—®é¢˜å¯èƒ½ä¸åœºæ™¯ä¸­çš„å…¶ä»–ä¿¡æ¯æœ‰å…³ï¼Œæˆ‘ä»¬å¯»æ±‚ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸°å¯Œå­˜å‚¨å™¨ä¸­çš„é™æ€åœºæ™¯æè¿°ã€‚æˆ‘ä»¬çš„è§†é¢‘ç†è§£ç³»ç»Ÿä¾èµ–äºLaViLaè§†é¢‘å­—å¹•å™¨ç»“åˆLLMæ¥å›ç­”è§†é¢‘é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆæ¢ç´¢äº†å°†è§†é¢‘åˆ†å‰²æˆæœ‰æ„ä¹‰ç‰‡æ®µçš„ä¸åŒæ–¹å¼ï¼Œä»¥ä¾¿æ–‡æœ¬æè¿°æ›´å‡†ç¡®åœ°åæ˜ è§†é¢‘å†…å®¹çš„ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨LLaVA VLMå°†é™æ€åœºæ™¯æè¿°çº³å…¥å­—å¹•ç®¡é“ï¼Œä»è€Œå¾—åˆ°æ›´è¯¦ç»†å’Œå®Œæ•´çš„å­—å¹•æ—¥å¿—ï¼Œå¹¶æ‰©å¤§äº†å¯ä»æ–‡æœ¬å­˜å‚¨å™¨ä¸­å›ç­”çš„é—®é¢˜ç©ºé—´ã€‚æœ€åï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸå¾®è°ƒäº†LaViLaè§†é¢‘å­—å¹•å™¨ï¼Œä»¥äº§ç”ŸåŠ¨ä½œå’Œåœºæ™¯å­—å¹•ï¼Œä¸ä¸ºä¸¤é¡¹ä»»åŠ¡ä½¿ç”¨å•ç‹¬çš„å­—å¹•æ¨¡å‹ç›¸æ¯”ï¼Œè¿™å¤§å¤§æé«˜äº†å­—å¹•ç®¡é“çš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹â€”â€”å¯æ§æ··åˆå­—å¹•å™¨ï¼Œå¯ä»¥æ ¹æ®è§†é¢‘ä¸­çš„åœºæ™¯å˜åŒ–ç­‰ç‰¹æ®Šè¾“å…¥ä»¤ç‰Œåœ¨ä¸åŒç±»å‹çš„å­—å¹•ä¹‹é—´è¿›è¡Œåˆ‡æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17047v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¯¹è§†é¢‘å†…å®¹è¿›è¡Œæ–‡æœ¬æ‘˜è¦çš„æ–¹æ³•ã€‚é’ˆå¯¹é•¿è§†é¢‘æ•°æ®çš„é«˜å¯†åº¦å’Œé«˜ç»´åº¦ç‰¹æ€§ï¼Œé€šè¿‡è§†é¢‘å­—å¹•å‘˜å¯¹è§†é¢‘è¾ƒçŸ­ç‰‡æ®µçš„æ“ä½œï¼Œé€æ­¥æ„å»ºåŸºäºæ–‡æœ¬çš„è®°å¿†ã€‚ä¸ºæé«˜ä»…ç”±çŸ­è§†é¢‘å­—å¹•æ„æˆçš„æ´»åŠ¨æ—¥å¿—è´¨é‡ï¼Œç»“åˆä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸°å¯Œè®°å¿†å†…å®¹ï¼ŒåŒ…æ‹¬é™æ€åœºæ™¯æè¿°ã€‚è§†é¢‘ç†è§£ç³»ç»Ÿä¾èµ–LaViLaè§†é¢‘å­—å¹•å‘˜ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå›ç­”å…³äºè§†é¢‘çš„é—®é¢˜ã€‚å°è¯•ä¸åŒæ–¹å¼åˆ’åˆ†è§†é¢‘ç‰‡æ®µï¼Œæ›´å‡†ç¡®åæ˜ è§†é¢‘å†…å®¹ç»“æ„ã€‚é€šè¿‡LLaVA VLMèå…¥é™æ€åœºæ™¯æè¿°ï¼Œç”Ÿæˆæ›´è¯¦ç»†å’Œå®Œæ•´çš„å­—å¹•æ—¥å¿—ï¼Œæ‰©å±•å¯ä»æ–‡æœ¬è®°å¿†ä¸­æé—®çš„ç©ºé—´ã€‚æˆåŠŸè°ƒæ•´LaViLaè§†é¢‘å­—å¹•å‘˜ï¼Œå¯åŒæ—¶ç”ŸæˆåŠ¨ä½œå’Œåœºæ™¯å­—å¹•ï¼Œç›¸è¾ƒäºä¸ºä¸¤é¡¹ä»»åŠ¡åˆ†åˆ«ä½¿ç”¨å•ç‹¬çš„å­—å¹•æ¨¡å‹ï¼Œæé«˜äº†å­—å¹•ç”Ÿæˆæ•ˆç‡ã€‚æå‡ºçš„å¯æ§æ··åˆå­—å¹•å‘˜å¯æ ¹æ®è§†é¢‘ä¸­æ£€æµ‹åˆ°çš„åœºæ™¯å˜åŒ–äº§ç”Ÿçš„ç‰¹æ®Šè¾“å…¥ä»¤ç‰Œï¼Œäº¤æ›¿ç”Ÿæˆä¸åŒç±»å‹çš„å­—å¹•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ•°æ®çš„é«˜å¯†åº¦å’Œé«˜ç»´åº¦éœ€è¦é€šè¿‡æ–‡æœ¬æ‘˜è¦è¿›è¡Œç®€åŒ–è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡çŸ­è§†é¢‘ç‰‡æ®µçš„é€æ­¥æ„å»ºæ–‡æœ¬è®°å¿†æ¥è§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>ä¸ºæé«˜æ´»åŠ¨æ—¥å¿—è´¨é‡ï¼Œç»“åˆä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸°å¯Œæ–‡æœ¬è®°å¿†å†…å®¹ã€‚</li>
<li>LaViLaè§†é¢‘å­—å¹•å‘˜ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆç”¨äºå›ç­”å…³äºè§†é¢‘çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡åˆ’åˆ†è§†é¢‘ç‰‡æ®µä»¥æ›´å‡†ç¡®åæ˜ è§†é¢‘å†…å®¹ç»“æ„çš„æ–¹å¼è¿›è¡Œäº†æ¢ç´¢ã€‚</li>
<li>æˆåŠŸè°ƒæ•´LaViLaè§†é¢‘å­—å¹•å‘˜ï¼Œå¯ä»¥åŒæ—¶ç”ŸæˆåŠ¨ä½œå’Œåœºæ™¯å­—å¹•ï¼Œæé«˜å­—å¹•ç”Ÿæˆæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ba646b9b9bc870b96cf7ff06426546c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8a4dc65f4c11423ce7c47e052c695e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fc426551145b8cf0777a42c9e13bc61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0fa747d8aa62fb2a8dead0bddc8be2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9161ea8f77515579e23c737f29543ddb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-Understanding-Camera-Motions-in-Any-Video"><a href="#Towards-Understanding-Camera-Motions-in-Any-Video" class="headerlink" title="Towards Understanding Camera Motions in Any Video"></a>Towards Understanding Camera Motions in Any Video</h2><p><strong>Authors:Zhiqiu Lin, Siyuan Cen, Daniel Jiang, Jay Karhade, Hewei Wang, Chancharik Mitra, Tiffany Ling, Yuhan Huang, Sifan Liu, Mingyu Chen, Rushikesh Zawar, Xue Bai, Yilun Du, Chuang Gan, Deva Ramanan</strong></p>
<p>We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of camera motion primitives, designed in collaboration with cinematographers. We find, for example, that some motions like â€œfollowâ€ (or tracking) require understanding scene content like moving subjects. We conduct a large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy. For example, a novice may confuse zoom-in (a change of intrinsics) with translating forward (a change of extrinsics), but can be trained to differentiate the two. Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs), finding that SfM models struggle to capture semantic primitives that depend on scene content, while VLMs struggle to capture geometric primitives that require precise estimation of trajectories. We then fine-tune a generative VLM on CameraBench to achieve the best of both worlds and showcase its applications, including motion-augmented captioning, video question answering, and video-text retrieval. We hope our taxonomy, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motions in any video. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†CameraBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å’Œæé«˜æ‘„åƒæœºè¿åŠ¨ç†è§£çš„å¤§å‹æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚CameraBenchç”±å¤§çº¦3000ä¸ªå¤šæ ·åŒ–çš„äº’è”ç½‘è§†é¢‘ç»„æˆï¼Œè¿™äº›è§†é¢‘ç»è¿‡ä¸“å®¶é€šè¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µè´¨é‡æ§åˆ¶è¿‡ç¨‹è¿›è¡Œæ ‡æ³¨ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¹‹ä¸€æ˜¯ä¸ç”µå½±æ‘„å½±å¸ˆå…±åŒè®¾è®¡çš„æ‘„åƒæœºè¿åŠ¨åŸå§‹åˆ†ç±»ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¾‹å¦‚ï¼Œâ€œè·Ÿéšâ€ï¼ˆæˆ–è·Ÿè¸ªï¼‰ç­‰åŠ¨ä½œéœ€è¦ç†è§£åœºæ™¯å†…å®¹ï¼Œå¦‚ç§»åŠ¨ä¸»é¢˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡çš„äººç±»ç ”ç©¶ï¼Œä»¥è¡¡é‡äººç±»æ ‡æ³¨æ€§èƒ½ï¼Œå‘ç°é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’ŒåŸºäºæ•™ç¨‹çš„åŸ¹è®­å¯ä»¥æ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œæ–°æ‰‹å¯èƒ½ä¼šæ··æ·†ç¼©æ”¾ï¼ˆå†…åœ¨å˜åŒ–ï¼‰å’Œå‘å‰å¹³ç§»ï¼ˆå¤–åœ¨å˜åŒ–ï¼‰ï¼Œä½†å¯ä»¥é€šè¿‡è®­ç»ƒæ¥åŒºåˆ†è¿™ä¸¤è€…ã€‚ä½¿ç”¨CameraBenchï¼Œæˆ‘ä»¬è¯„ä¼°äº†ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰å’Œè§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå‘ç°SfMæ¨¡å‹åœ¨æ•è·ä¾èµ–äºåœºæ™¯å†…å®¹çš„è¯­ä¹‰åŸå§‹æ•°æ®æ—¶é‡åˆ°å›°éš¾ï¼Œè€ŒVLMåœ¨æ•è·éœ€è¦ç²¾ç¡®ä¼°è®¡è½¨è¿¹çš„å‡ ä½•åŸå§‹æ•°æ®æ—¶åˆ™é‡åˆ°å›°éš¾ã€‚ç„¶åæˆ‘ä»¬åœ¨CameraBenchä¸Šå¯¹ç”Ÿæˆå¼VLMè¿›è¡Œäº†å¾®è°ƒï¼Œå®ç°äº†ä¸¤è€…çš„ä¼˜ç‚¹ï¼Œå¹¶å±•ç¤ºäº†å…¶åº”ç”¨ï¼ŒåŒ…æ‹¬è¿åŠ¨å¢å¼ºæè¿°ã€è§†é¢‘é—®ç­”å’Œè§†é¢‘æ–‡æœ¬æ£€ç´¢ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åˆ†ç±»ã€åŸºå‡†æµ‹è¯•å’Œæ•™ç¨‹å°†æ¨åŠ¨æœªæ¥å‘ç€ç†è§£ä»»ä½•è§†é¢‘ä¸­çš„æ‘„åƒæœºåŠ¨ä½œè¿™ä¸€ç›®æ ‡å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15376v2">PDF</a> Project site: <a target="_blank" rel="noopener" href="https://linzhiqiu.github.io/papers/camerabench/">https://linzhiqiu.github.io/papers/camerabench/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CameraBenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å’Œæ”¹è¿›æ‘„åƒæœºåŠ¨ä½œç†è§£çš„å¤§å‹æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚CameraBenchåŒ…å«çº¦3000ä¸ªå¤šæ ·åŒ–çš„äº’è”ç½‘è§†é¢‘ï¼Œç»è¿‡ä¸“å®¶ä¸¥æ ¼çš„å¤šé˜¶æ®µè´¨é‡æ§åˆ¶è¿‡ç¨‹è¿›è¡Œæ ‡æ³¨ã€‚æœ¬æ–‡çš„è´¡çŒ®ä¹‹ä¸€æ˜¯ä¸ç”µå½±æ‘„å½±å¸ˆå…±åŒè®¾è®¡çš„æ‘„åƒæœºåŠ¨ä½œåŸå§‹åˆ†ç±»æ³•ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸€äº›åŠ¨ä½œå¦‚â€œè·Ÿè¸ªâ€éœ€è¦ç†è§£åœºæ™¯å†…å®¹å¦‚ç§»åŠ¨ä¸»ä½“ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡çš„äººæœºç ”ç©¶æ¥é‡åŒ–äººç±»æ ‡æ³¨æ€§èƒ½ï¼Œå‘ç°é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’ŒåŸºäºæ•™ç¨‹çš„åŸ¹è®­å¯ä»¥æ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€‚ä½¿ç”¨CameraBenchï¼Œæˆ‘ä»¬è¯„ä¼°äº†ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰å’Œè§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå‘ç°SfMæ¨¡å‹åœ¨ä¾èµ–äºåœºæ™¯å†…å®¹çš„è¯­ä¹‰åŸå§‹åŠ¨ä½œä¸Šè¡¨ç°æŒ£æ‰ï¼Œè€ŒVLMåœ¨éœ€è¦ç²¾ç¡®è½¨è¿¹ä¼°è®¡çš„å‡ ä½•åŸå§‹åŠ¨ä½œä¸Šè¡¨ç°æŒ£æ‰ã€‚æˆ‘ä»¬å¯¹åŸºå‡†æµ‹è¯•ã€åˆ†ç±»æ³•å’Œæ•™ç¨‹çš„æœŸæœ›å°†æ¨åŠ¨æœªæ¥å‘ç†è§£ä»»ä½•è§†é¢‘ä¸­çš„æ‘„åƒæœºåŠ¨ä½œè¿™ä¸€ç›®æ ‡åŠªåŠ›ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ul>
<li>CameraBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å’Œæ”¹è¿›æ‘„åƒæœºåŠ¨ä½œç†è§£çš„å¤§å‹æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>åŒ…å«çº¦3000ä¸ªå¤šæ ·åŒ–äº’è”ç½‘è§†é¢‘ï¼Œç»è¿‡ä¸“å®¶ä¸¥æ ¼æ ‡æ³¨ã€‚</li>
<li>ä¸ç”µå½±æ‘„å½±å¸ˆå…±åŒè®¾è®¡äº†ä¸€ç§æ‘„åƒæœºåŠ¨ä½œåŸå§‹åˆ†ç±»æ³•ã€‚</li>
<li>éƒ¨åˆ†åŠ¨ä½œå¦‚â€œè·Ÿè¸ªâ€éœ€è¦ç†è§£åœºæ™¯å†…å®¹ã€‚</li>
<li>é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’ŒåŸºäºæ•™ç¨‹çš„åŸ¹è®­å¯ä»¥æ˜¾è‘—æé«˜äººæœºæ ‡æ³¨æ€§èƒ½ã€‚</li>
<li>ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰å’Œè§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91bfb0753e185633f3515241ba1a4c21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3d81cf616dd1a8911a4f4b5f8026c7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a63cac2999a94de142c3fbfc6d47739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7621cd132df8ca25178abf6aea260951.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab71b3511732073e228b6150548ce0df.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="T-Re-thinking-Temporal-Search-for-Long-Form-Video-Understanding"><a href="#T-Re-thinking-Temporal-Search-for-Long-Form-Video-Understanding" class="headerlink" title="T*: Re-thinking Temporal Search for Long-Form Video Understanding"></a>T*: Re-thinking Temporal Search for Long-Form Video Understanding</h2><p><strong>Authors:Jinhui Ye, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, Jiajun Wu, Manling Li</strong></p>
<p>Efficiently understanding long-form videos remains a significant challenge in computer vision. In this work, we revisit temporal search paradigms for long-form video understanding and address a fundamental issue pertaining to all state-of-the-art (SOTA) long-context vision-language models (VLMs). Our contributions are twofold: First, we frame temporal search as a Long Video Haystack problem: finding a minimal set of relevant frames (e.g., one to five) from tens of thousands based on specific queries. Upon this formulation, we introduce LV-Haystack, the first dataset with 480 hours of videos, 15,092 human-annotated instances for both training and evaluation aiming to improve temporal search quality and efficiency. Results on LV-Haystack highlight a significant research gap in temporal search capabilities, with current SOTA search methods only achieving 2.1% temporal F1 score on the Longvideobench subset. Next, inspired by visual search in images, we propose a lightweight temporal search framework, T* that reframes costly temporal search as spatial search. T* leverages powerful visual localization techniques commonly used in images and introduces an adaptive zooming-in mechanism that operates across both temporal and spatial dimensions. Extensive experiments show that integrating T* with existing methods significantly improves SOTA long-form video understanding. Under an inference budget of 32 frames, T* improves GPT-4oâ€™s performance from 50.5% to 53.1% and LLaVA-OneVision-OV-72Bâ€™s performance from 56.5% to 62.4% on the Longvideobench XL subset. Our code, benchmark, and models are provided in the Supplementary material. </p>
<blockquote>
<p>åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œæœ‰æ•ˆåœ°ç†è§£é•¿è§†é¢‘ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°æ€è€ƒäº†ç”¨äºé•¿è§†é¢‘ç†è§£çš„æ—¶åºæœç´¢èŒƒå¼ï¼Œå¹¶è§£å†³äº†ä¸€ä¸ªä¸æ‰€æœ‰æœ€å…ˆè¿›çš„é•¿ä¸Šä¸‹æ–‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç›¸å…³çš„åŸºç¡€é—®é¢˜ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸¤æ–¹é¢ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å°†æ—¶åºæœç´¢é—®é¢˜å®šä½ä¸ºé•¿è§†é¢‘å †æ ˆé—®é¢˜ï¼šä»æ•°ä¸‡å¸§ä¸­æ‰¾å‡ºä¸ç‰¹å®šæŸ¥è¯¢ç›¸å…³çš„æœ€å°‘å¸§æ•°ï¼ˆä¾‹å¦‚ä¸€åˆ°äº”å¸§ï¼‰ã€‚åŸºäºæ­¤æ„æƒ³ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LV-Haystackæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«480å°æ—¶çš„è§†é¢‘å’Œé’ˆå¯¹è®­ç»ƒå’Œè¯„ä¼°çš„15,092ä¸ªäººå·¥æ³¨é‡Šå®ä¾‹ï¼Œæ—¨åœ¨æé«˜æ—¶åºæœç´¢çš„è´¨é‡å’Œæ•ˆç‡ã€‚åœ¨LV-Haystackä¸Šçš„ç»“æœæ˜¾ç¤ºå‡ºæ—¶åºæœç´¢èƒ½åŠ›å­˜åœ¨æ˜¾è‘—çš„ç ”ç©¶å·®è·ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•åœ¨Longvideobenchå­é›†ä¸Šä»…å®ç°äº†2.1%çš„æ—¶åºF1åˆ†æ•°ã€‚æ¥ä¸‹æ¥ï¼Œå—åˆ°å›¾åƒè§†è§‰æœç´¢çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ—¶åºæœç´¢æ¡†æ¶T<em>ï¼Œå®ƒå°†æ˜‚è´µçš„æ—¶åºæœç´¢é‡æ–°å®šä½ä¸ºç©ºé—´æœç´¢ã€‚T</em>åˆ©ç”¨å›¾åƒä¸­å¸¸ç”¨çš„å¼ºå¤§è§†è§‰å®šä½æŠ€æœ¯ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”ç¼©æ”¾æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨æ—¶ç©ºä¸¤ä¸ªç»´åº¦ä¸Šè¿è¡Œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå°†T<em>ä¸ç°æœ‰æ–¹æ³•ç›¸ç»“åˆï¼Œå¯ä»¥æ˜¾è‘—æé«˜æœ€å…ˆè¿›çš„é•¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚åœ¨æ¨ç†é¢„ç®—ä¸º32å¸§çš„æ¡ä»¶ä¸‹ï¼ŒT</em>å°†GPT-4oçš„æ€§èƒ½ä»50.5%æé«˜åˆ°53.1%ï¼Œå°†LLaVA-OneVision-OV-72Bçš„æ€§èƒ½ä»56.5%æé«˜åˆ°62.4%ï¼Œè¿™æ˜¯åœ¨Longvideobench XLå­é›†ä¸Šçš„ç»“æœã€‚æˆ‘ä»¬çš„ä»£ç ã€åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹éƒ½å·²åœ¨è¡¥å……ææ–™ä¸­æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02259v3">PDF</a> Accepted by CVPR 2025; A real-world long video needle-in-haystack   benchmark; long-video QA with human ref frames</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡é‡æ–°å®¡è§†äº†é•¿è§†é¢‘ç†è§£çš„æ—¶åºæœç´¢èŒƒå¼ï¼Œå¹¶é’ˆå¯¹å½“å‰ä¸»æµçš„é•¿ä¸Šä¸‹æ–‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å­˜åœ¨çš„ä¸€ä¸ªæ ¹æœ¬é—®é¢˜è¿›è¡Œäº†ç ”ç©¶ã€‚æ–‡ç« å°†æ—¶åºæœç´¢é—®é¢˜ç±»æ¯”ä¸ºé•¿è§†é¢‘çš„â€œé’ˆå°–æœå¯»â€é—®é¢˜ï¼Œå³éœ€è¦ä»æ•°ä¸‡å¸§ä¸­å¯»æ‰¾æå°‘é‡çš„å…³é”®å¸§ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æ¨å‡ºäº†LV-Haystackæ•°æ®é›†ï¼ŒåŒ…å«480å°æ—¶çš„è§†é¢‘å’Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„15,092ä¸ªäººç±»æ ‡æ³¨å®ä¾‹ã€‚å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•åœ¨LV-Haystackçš„å­é›†Longvideobenchä¸Šçš„è¡¨ç°ä»æœ‰æ˜¾è‘—å·®è·ï¼Œä»…å®ç°äº†2.1%çš„æ—¶é—´F1å¾—åˆ†ã€‚æ–‡ç« å—åˆ°å›¾åƒè§†è§‰æœç´¢çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªè½»é‡çº§çš„æ—¶åºæœç´¢æ¡†æ¶T<em>ï¼Œå°†æ˜‚è´µçš„æ—¶åºæœç´¢é‡æ–°å®šä½ä¸ºç©ºé—´æœç´¢ã€‚T</em>åˆ©ç”¨å›¾åƒä¸­å¸¸ç”¨çš„è§†è§‰å®šä½æŠ€æœ¯ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”ç¼©æ”¾æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨æ—¶é—´å’Œç©ºç‰ˆæœ¬ä¸Šéƒ½æœ‰æ•ˆã€‚å®éªŒè¡¨æ˜ï¼Œå°†T<em>ä¸ç°æœ‰æ–¹æ³•ç›¸ç»“åˆï¼Œå¯æ˜¾è‘—æé«˜é•¿è§†é¢‘ç†è§£çš„æœ€å…ˆè¿›æŠ€æœ¯æ°´å¹³ã€‚åœ¨æ¨ç†é¢„ç®—ä¸º32å¸§çš„æ¡ä»¶ä¸‹ï¼ŒT</em>å°†GPT-4oçš„æ€§èƒ½ä»50.5%æé«˜åˆ°53.1%ï¼Œå°†LLaVA-OneVision-OV-72Bçš„æ€§èƒ½ä»56.5%æé«˜åˆ°62.4%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºå°†æ—¶åºæœç´¢é—®é¢˜ç±»æ¯”ä¸ºé•¿è§†é¢‘çš„â€œé’ˆå°–æœå¯»â€é—®é¢˜ï¼Œå¹¶å¼•å…¥LV-Haystackæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ—¶åºæœç´¢æ¨¡å‹ã€‚</li>
<li>æ­ç¤ºäº†å½“å‰æœ€å…ˆè¿›æ–¹æ³•åœ¨LV-Haystackä¸Šçš„æ€§èƒ½å·®è·ï¼Œæ—¶é—´F1å¾—åˆ†ä»…ä¸º2.1%ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªè½»é‡çº§çš„æ—¶åºæœç´¢æ¡†æ¶T*ï¼Œå°†æ˜‚è´µçš„æ—¶åºæœç´¢é‡æ–°å®šä½ä¸ºç©ºé—´æœç´¢ã€‚</li>
<li>T*åˆ©ç”¨è§†è§‰å®šä½æŠ€æœ¯å’Œè‡ªé€‚åº”ç¼©æ”¾æœºåˆ¶ï¼Œåœ¨æ—¶é—´å’Œç©ºé—´ä¸¤ä¸ªç»´åº¦ä¸Šæ“ä½œã€‚</li>
<li>T*ä¸ç°æœ‰æ–¹æ³•çš„ç»“åˆæ˜¾è‘—æé«˜é•¿è§†é¢‘ç†è§£æ€§èƒ½ï¼Œæ”¹å–„GPT-4oå’ŒLLaVA-OneVision-OV-72Bçš„æ€§èƒ½ã€‚</li>
<li>åœ¨æ¨ç†é¢„ç®—ä¸º32å¸§çš„æ¡ä»¶ä¸‹ï¼ŒT*èƒ½æ˜¾è‘—æå‡æ€§èƒ½è¡¨ç°ã€‚</li>
<li>æä¾›äº†ä»£ç ã€åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-380601c28fc938ce5d8938334c48ef05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfb394513b8f8b45eb4777ee6680fc74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8911596cd19584c4c228931cedf0856.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98004a7f89d83363ebcef804d999f73e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33af6afef84e6c8e8f131e0403ef77dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb172dbc7af7c63f4789d4c17de07f5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8b4912e1b6be6428e31f40b3d21adc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Generative-Frame-Sampler-for-Long-Video-Understanding"><a href="#Generative-Frame-Sampler-for-Long-Video-Understanding" class="headerlink" title="Generative Frame Sampler for Long Video Understanding"></a>Generative Frame Sampler for Long Video Understanding</h2><p><strong>Authors:Linli Yao, Haoning Wu, Kun Ouyang, Yuanxing Zhang, Caiming Xiong, Bei Chen, Xu Sun, Junnan Li</strong></p>
<p>Despite recent advances in Video Large Language Models (VideoLLMs), effectively understanding long-form videos remains a significant challenge. Perceiving lengthy videos containing thousands of frames poses substantial computational burden. To mitigate this issue, this paper introduces Generative Frame Sampler (GenS), a plug-and-play module integrated with VideoLLMs to facilitate efficient lengthy video perception. Built upon a lightweight VideoLLM, GenS leverages its inherent vision-language capabilities to identify question-relevant frames. To facilitate effective retrieval, we construct GenS-Video-150K, a large-scale video instruction dataset with dense frame relevance annotations. Extensive experiments demonstrate that GenS consistently boosts the performance of various VideoLLMs, including open-source models (Qwen2-VL-7B, Aria-25B, VILA-40B, LLaVA-Video-7B&#x2F;72B) and proprietary assistants (GPT-4o, Gemini). When equipped with GenS, open-source VideoLLMs achieve impressive state-of-the-art results on long-form video benchmarks: LLaVA-Video-72B reaches 66.8 (+4.3) on LongVideoBench and 77.0 (+2.7) on MLVU, while Aria obtains 39.2 on HourVideo surpassing the Gemini-1.5-pro by 1.9 points. We will release all datasets and models at <a target="_blank" rel="noopener" href="https://generative-sampler.github.io/">https://generative-sampler.github.io</a>. </p>
<blockquote>
<p>å°½ç®¡è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰è¿‘æœŸå–å¾—äº†è¿›å±•ï¼Œä½†æœ‰æ•ˆç†è§£é•¿è§†é¢‘ä»ç„¶æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚æ„ŸçŸ¥åŒ…å«æ•°åƒå¸§çš„å†—é•¿è§†é¢‘å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†ç”Ÿæˆå¸§é‡‡æ ·å™¨ï¼ˆGenSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸VideoLLMsé›†æˆçš„å³æ’å³ç”¨æ¨¡å—ï¼Œå¯ä¿ƒè¿›é«˜æ•ˆçš„é•¿è§†é¢‘æ„ŸçŸ¥ã€‚åŸºäºè½»é‡çº§çš„VideoLLMï¼ŒGenSåˆ©ç”¨å…¶å›ºæœ‰çš„è§†è§‰è¯­è¨€åŠŸèƒ½æ¥è¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„å¸§ã€‚ä¸ºäº†ä¿ƒè¿›æœ‰æ•ˆæ£€ç´¢ï¼Œæˆ‘ä»¬æ„å»ºäº†GenS-Video-150Kï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è§†é¢‘æŒ‡ä»¤æ•°æ®é›†ï¼Œå…·æœ‰å¯†é›†çš„å¸§ç›¸å…³æ€§æ³¨é‡Šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGenSæŒç»­æå‡äº†å„ç§VideoLLMsçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¼€æºæ¨¡å‹ï¼ˆQwen2-VL-7Bã€Aria-25Bã€VILA-40Bã€LLaVA-Video-7B&#x2F;72Bï¼‰å’Œä¸“æœ‰åŠ©ç†ï¼ˆGPT-4oã€Geminiï¼‰ã€‚é…å¤‡GenSåï¼Œå¼€æºVideoLLMsåœ¨é•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æœ€å…ˆè¿›ç»“æœï¼šLLaVA-Video-72Båœ¨LongVideoBenchä¸Šè¾¾åˆ°66.8ï¼ˆ+4.3ï¼‰ï¼Œåœ¨MLVUä¸Šè¾¾åˆ°77.0ï¼ˆ+2.7ï¼‰ï¼Œè€ŒAriaåœ¨HourVideoä¸Šè·å¾—39.2åˆ†ï¼Œè¶…è¶Šäº†Gemini-1.5-pro 1.9åˆ†ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://generative-sampler.github.ioå‘å¸ƒæ‰€æœ‰æ•°æ®é›†å’Œæ¨¡å‹./">https://generative-sampler.github.ioå‘å¸ƒæ‰€æœ‰æ•°æ®é›†å’Œæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09146v2">PDF</a> ACL 2025 Findings. Code: <a target="_blank" rel="noopener" href="https://github.com/yaolinli/GenS">https://github.com/yaolinli/GenS</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹é•¿è§†é¢‘ç†è§£çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºGenerative Frame Samplerï¼ˆGenSï¼‰çš„æ’ä»¶æ¨¡å—ï¼Œå®ƒå¯ä»¥ä¸VideoLLMsé›†æˆï¼Œä»¥æé«˜å¯¹é•¿è§†é¢‘çš„é«˜æ•ˆæ„ŸçŸ¥ã€‚GenSåˆ©ç”¨è½»é‡çº§çš„VideoLLMæ¨¡å‹ï¼Œé€šè¿‡è¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„å¸§æ¥å‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚åŒæ—¶ï¼Œæ–‡ç« æ„å»ºäº†ä¸€ä¸ªå¤§å‹è§†é¢‘æŒ‡ä»¤æ•°æ®é›†GenS-Video-150Kï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºGenSèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤šç§VideoLLMsçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Generative Frame Samplerï¼ˆGenSï¼‰æ˜¯ä¸€ä¸ªæ’ä»¶æ¨¡å—ï¼Œæ—¨åœ¨è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„è®¡ç®—è´Ÿæ‹…é—®é¢˜ã€‚</li>
<li>GenSå¯ä»¥é›†æˆåˆ°ç°æœ‰çš„VideoLLMsä¸­ï¼Œæé«˜é•¿è§†é¢‘æ„ŸçŸ¥çš„æ•ˆç‡ã€‚</li>
<li>GenSåˆ©ç”¨è½»é‡çº§çš„VideoLLMæ¨¡å‹è¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„å¸§ã€‚</li>
<li>æ„å»ºäº†å¤§å‹è§†é¢‘æŒ‡ä»¤æ•°æ®é›†GenS-Video-150Kï¼Œç”¨äºæ”¯æŒæœ‰æ•ˆæ£€ç´¢ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒGenSå¯ä»¥æ˜¾è‘—æé«˜å¤šç§å¼€æºå’Œä¸“æœ‰VideoLLMsçš„æ€§èƒ½ã€‚</li>
<li>é…å¤‡GenSçš„å¼€æºVideoLLMsåœ¨é•¿ç¯‡è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3692fcac5bef9bbda7ed216243e3a1a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48749a9a770ddc8f0c4013bdff75d78d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67680dc17be53b65863d73317345b73a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4503e28780303676a358ac56f242cb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37eb1d8f0304676ac5e8267e85003291.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Temporal-Preference-Optimization-for-Long-Form-Video-Understanding"><a href="#Temporal-Preference-Optimization-for-Long-Form-Video-Understanding" class="headerlink" title="Temporal Preference Optimization for Long-Form Video Understanding"></a>Temporal Preference Optimization for Long-Form Video Understanding</h2><p><strong>Authors:Rui Li, Xiaohan Wang, Yuhui Zhang, Orr Zohar, Zeyu Wang, Serena Yeung-Levy</strong></p>
<p>Despite significant advancements in video large multimodal models (video-LMMs), achieving effective temporal grounding in long-form videos remains a challenge for existing models. To address this limitation, we propose Temporal Preference Optimization (TPO), a novel post-training framework designed to enhance the temporal grounding capabilities of video-LMMs through preference learning. TPO adopts a self-training approach that enables models to differentiate between well-grounded and less accurate temporal responses by leveraging curated preference datasets at two granularities: localized temporal grounding, which focuses on specific video segments, and comprehensive temporal grounding, which captures extended temporal dependencies across entire video sequences. By optimizing on these preference datasets, TPO significantly enhances temporal understanding while reducing reliance on manually annotated data. Extensive experiments on three long-form video understanding benchmarksâ€“LongVideoBench, MLVU, and Video-MMEâ€“demonstrate the effectiveness of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO establishes itself as the leading 7B model on the Video-MME benchmark, underscoring the potential of TPO as a scalable and efficient solution for advancing temporal reasoning in long-form video understanding. Project page: <a target="_blank" rel="noopener" href="https://ruili33.github.io/tpo_website">https://ruili33.github.io/tpo_website</a>. </p>
<blockquote>
<p>å°½ç®¡è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆvideo-LMMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é•¿è§†é¢‘ä¸­å®ç°æœ‰æ•ˆçš„æ—¶åºå®šä½ä»ç„¶æ˜¯ç°æœ‰æ¨¡å‹çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶åºåå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åå¥½å­¦ä¹ æé«˜è§†é¢‘-LMMsçš„æ—¶åºå®šä½èƒ½åŠ›ã€‚TPOé‡‡ç”¨è‡ªæˆ‘è®­ç»ƒçš„æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸¤ä¸ªç²’åº¦ä¸Šåˆ©ç”¨å®šåˆ¶çš„åå¥½æ•°æ®é›†æ¥åŒºåˆ†å‡†ç¡®çš„æ—¶åºå“åº”å’Œä¸é‚£ä¹ˆå‡†ç¡®çš„å“åº”ï¼šå±€éƒ¨æ—¶åºå®šä½ï¼Œä¾§é‡äºç‰¹å®šçš„è§†é¢‘ç‰‡æ®µï¼›å…¨é¢æ—¶åºå®šä½ï¼Œæ•æ‰æ•´ä¸ªè§†é¢‘åºåˆ—ä¸­çš„æ‰©å±•æ—¶é—´ä¾èµ–å…³ç³»ã€‚é€šè¿‡è¿™äº›åå¥½æ•°æ®é›†è¿›è¡Œä¼˜åŒ–ï¼ŒTPOåœ¨æé«˜æ—¶é—´ç†è§£çš„åŒæ—¶ï¼Œå‡å°‘äº†å¯¹æ‰‹åŠ¨æ³¨é‡Šæ•°æ®çš„ä¾èµ–ã€‚åœ¨LongVideoBenchã€MLVUå’ŒVideo-MMEä¸‰ä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTPOåœ¨ä¸¤ç§æœ€å…ˆè¿›çš„è§†é¢‘-LMMsä¸­éƒ½æœ‰æ•ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLLaVA-Video-TPOåœ¨Video-MMEåŸºå‡†æµ‹è¯•ä¸­æˆä¸ºé¢†å…ˆçš„7Bæ¨¡å‹ï¼Œçªæ˜¾äº†TPOä½œä¸ºæ¨è¿›é•¿è§†é¢‘ç†è§£ä¸­æ—¶åºæ¨ç†çš„å¯æ‰©å±•å’Œé«˜æ•ˆè§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ruili3.github.io/tpo_website%E3%80%82">https://ruili3.github.io/tpo_websiteã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13919v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå°½ç®¡è§†é¢‘å¤§æ¨¡æ€æ¨¡å‹ï¼ˆvideo-LMMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é•¿è§†é¢‘ä¸­å®ç°æœ‰æ•ˆçš„æ—¶åºå®šä½ä»æ˜¯ç°æœ‰æ¨¡å‹çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶åºåå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰è¿™ä¸€æ–°å‹åè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åå¥½å­¦ä¹ æé«˜è§†é¢‘æ¨¡å‹çš„æ—¶åºå®šä½èƒ½åŠ›ã€‚TPOé‡‡ç”¨è‡ªè®­ç»ƒæ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç²¾ç»†åŒ–çš„åå¥½æ•°æ®é›†ä¸ŠåŒºåˆ†å‡†ç¡®çš„æ—¶åºå“åº”å’Œä¸å‡†ç¡®çš„å“åº”ï¼Œåå¥½æ•°æ®é›†åˆ†ä¸ºå±€éƒ¨æ—¶åºå®šä½å’Œå…¨é¢æ—¶åºå®šä½ä¸¤ä¸ªç²’åº¦ã€‚é€šè¿‡ä¼˜åŒ–è¿™äº›æ•°æ®é›†ï¼ŒTPOæé«˜äº†æ—¶åºç†è§£èƒ½åŠ›ï¼Œå¹¶é™ä½äº†å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚åœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTPOåœ¨ä¸¤ç§æœ€å…ˆè¿›çš„è§†é¢‘å¤§æ¨¡æ€æ¨¡å‹ä¸­éƒ½æœ‰æ•ˆã€‚ç‰¹åˆ«æ˜¯LLaVA-Video-TPOåœ¨Video-MMEåŸºå‡†æµ‹è¯•ä¸­æˆä¸ºé¢†å…ˆçš„7Bæ¨¡å‹ï¼Œçªæ˜¾äº†TPOåœ¨æ¨è¿›é•¿è§†é¢‘ç†è§£ä¸­çš„æ—¶åºæ¨ç†çš„æ½œåŠ›å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†é¢‘å¤§æ¨¡æ€æ¨¡å‹ï¼ˆvideo-LMMsï¼‰åœ¨é•¿è§†é¢‘çš„æ—¶åºå®šä½ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†æ—¶åºåå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†é¢‘æ¨¡å‹çš„æ—¶åºå®šä½èƒ½åŠ›ã€‚</li>
<li>TPOé‡‡ç”¨è‡ªè®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨ç²¾ç»†åŒ–çš„åå¥½æ•°æ®é›†æ¥åŒºåˆ†å‡†ç¡®çš„æ—¶åºå“åº”å’Œä¸å‡†ç¡®çš„å“åº”ã€‚</li>
<li>åå¥½æ•°æ®é›†åŒ…æ‹¬å±€éƒ¨æ—¶åºå®šä½å’Œå…¨é¢æ—¶åºå®šä½ä¸¤ä¸ªç²’åº¦ã€‚</li>
<li>TPOæé«˜äº†æ¨¡å‹çš„æ—¶åºç†è§£èƒ½åŠ›ï¼Œå¹¶é™ä½äº†å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šï¼ŒTPOæ˜¾è‘—æå‡äº†è§†é¢‘å¤§æ¨¡æ€æ¨¡å‹çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a501c101fd12ca94dfe76b7ab9ea3f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcdf12334192646ea8a30d1ce8111418.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5caa1bc2b8d2fc09b7b83aa60bdade8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1a18a3a6abfb6c9ac649e6950b4fd7fe.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Aesthetic Image Captioning with Saliency Enhanced MLLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-650026a5c03df615d42f14e98ed1734a.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
