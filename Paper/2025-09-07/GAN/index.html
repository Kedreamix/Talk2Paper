<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN 方向最新论文已更新，请持续关注 Update in 2025-09-07  Enhancing Robustness in Post-Processing Watermarking An Ensemble Attack   Network Using CNNs and Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-2bc346488cc2f961ad5a020e9350b097.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-07-更新"><a href="#2025-09-07-更新" class="headerlink" title="2025-09-07 更新"></a>2025-09-07 更新</h1><h2 id="Enhancing-Robustness-in-Post-Processing-Watermarking-An-Ensemble-Attack-Network-Using-CNNs-and-Transformers"><a href="#Enhancing-Robustness-in-Post-Processing-Watermarking-An-Ensemble-Attack-Network-Using-CNNs-and-Transformers" class="headerlink" title="Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack   Network Using CNNs and Transformers"></a>Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack   Network Using CNNs and Transformers</h2><p><strong>Authors:Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen</strong></p>
<p>Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model’s internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:<a target="_blank" rel="noopener" href="https://github.com/aiiu-lab/DeepRobustWatermark">https://github.com/aiiu-lab/DeepRobustWatermark</a>. </p>
<blockquote>
<p>近期关于深度水印的研究主要集中于处理中的水印，即将水印过程集成到图像生成中。然而，后处理水印方法，即在图像生成后嵌入水印，提供了更大的灵活性。它可以应用于任何生成模型的输出（例如GANs、扩散模型），而无需访问模型的内部结构。它还允许用户将独特的水印嵌入到单独的图像中。因此，本研究专注于后处理水印，并通过在训练过程中结合集成攻击网络来提高其稳健性。我们构建了各种版本的攻击网络，使用CNN和Transformer在空间域和频域进行研究，以调查每种组合如何影响水印模型的稳健性。我们的结果表明，在空间域中使用基于CNN的攻击网络与频域中使用基于Transformer的攻击网络相结合，为水印模型提供了最高的稳健性。我们在WAVES基准测试集上进行了广泛评估，以平均位准确性作为度量标准，证明我们的集成攻击网络在各种压力测试下显著提高了基线水印方法的稳健性。特别是在WAVES中定义的再生攻击情况下，我们的方法将StegaStamp提高了18.743%。代码已发布在：<a target="_blank" rel="noopener" href="https://github.com/aiiu-lab/DeepRobustWatermark%E3%80%82">https://github.com/aiiu-lab/DeepRobustWatermark。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03006v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong><br>深度学习水印技术的新研究聚焦于在图像生成过程中的水印嵌入。然而，研究也表明，对生成图像进行后续处理的水印嵌入方法具有更大的灵活性。该方法可应用于任何生成模型的输出，无需了解模型的内部结构，并能为单幅图像嵌入独特的水印。本研究重点关注后续处理水印技术，并通过训练过程中结合集成攻击网络来提高其稳健性。研究发现结合CNN在空间和基于Transformer在频率域的攻击网络将大大提高水印模型的稳健性。在WAVES基准测试上的评估显示，集成攻击网络显著提高了基线水印方法在各种压力测试下的稳健性。特别是针对WAVES定义的再生攻击，我们的方法提高了StegaStamp的准确率高达18.743%。相关代码已发布在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前深度水印研究主要关注在图像生成过程中的水印嵌入方法。</li>
<li>另一种方法是采用后处理水印技术，该技术具有更大的灵活性，适用于各种生成模型的输出。</li>
<li>研究重点是通过结合集成攻击网络提高后处理水印技术的稳健性。</li>
<li>结合CNN在空间和基于Transformer在频率域的攻击网络能够显著提高水印模型的稳健性。</li>
<li>在WAVES基准测试上，集成攻击网络显著增强了基线水印方法的稳健性，尤其是在面对特定类型的攻击时。</li>
<li>相关代码已公开发布在GitHub上供研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03006">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-31d9a8eb646e4b9694a615a59ab71f4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa5648be02b7878b66a80855f977a8a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7946b3dbb9a0533fbf22ac7279e04e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2bc346488cc2f961ad5a020e9350b097.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3481b10ba88f480acfd4ac0a7e7dcccd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d14c8f238fa9a41bb14fb8640d88cf8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Revisiting-the-Privacy-Risks-of-Split-Inference-A-GAN-Based-Data-Reconstruction-Attack-via-Progressive-Feature-Optimization"><a href="#Revisiting-the-Privacy-Risks-of-Split-Inference-A-GAN-Based-Data-Reconstruction-Attack-via-Progressive-Feature-Optimization" class="headerlink" title="Revisiting the Privacy Risks of Split Inference: A GAN-Based Data   Reconstruction Attack via Progressive Feature Optimization"></a>Revisiting the Privacy Risks of Split Inference: A GAN-Based Data   Reconstruction Attack via Progressive Feature Optimization</h2><p><strong>Authors:Yixiang Qiu, Yanhan Liu, Hongyao Yu, Hao Fang, Bin Chen, Shu-Tao Xia, Ke Xu</strong></p>
<p>The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs. </p>
<blockquote>
<p>深度神经网络（DNNs）日益增长的复杂性导致采用Split Inference（SI）技术。作为一种合作型范式，它在边缘设备和云之间分配计算任务以减少延迟并保护用户隐私。然而，数据重建攻击（DRAs）的最新进展表明，中间特征交换在SI中可以用于恢复敏感输入数据，从而带来重大隐私风险。现有的DRAs通常仅在浅层模型上有效，未能充分利用语义先验信息，从而限制了其重建质量和跨数据集和模型架构的通用性。在本文中，我们提出了一种基于生成对抗网络（GAN）的新型DRA框架，其中包含渐进特征优化（PFO），该框架将生成器分解为分层块并逐层优化中间表示形式，以提高重建图像语义保真度。为了稳定优化并改善图像的真实性，我们在重建过程中引入了L1球约束。大量实验表明，我们的方法大大优于先前攻击，特别是在高分辨率场景、非内部分布设置以及针对更深和更复杂的DNNs时效果更佳。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20613v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong><br>深度神经网络（DNN）的复杂性增长推动了Split Inference（SI）的应用。但新的数据重建攻击（DRA）揭示了SI中交换的中间特征可被用来恢复敏感输入数据，带来隐私风险。现有的DRAs通常在浅层模型上有效，无法充分利用语义先验信息。本文提出一种基于GAN的DRA框架，结合渐进特征优化（PFO），分解生成器为层次块并逐步优化中间表示，提高重建图像语义保真度。引入L1球约束以优化稳定性和图像真实性。实验证明，该方法在高清场景、非分布设置和更复杂的DNNs上的攻击效果远超先前方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Split Inference（SI）被应用于深度神经网络（DNN），旨在减少延迟和保护用户隐私。</li>
<li>数据重建攻击（DRA）可以利用SI中的中间特征恢复敏感输入数据，带来隐私风险。</li>
<li>现有DRAs主要适用于浅层模型，且在利用语义先验信息方面存在局限性。</li>
<li>提出一种基于GAN的DRA框架，结合渐进特征优化（PFO），提高重建图像的语义保真度。</li>
<li>引入L1球约束以优化图像的真实性和稳定性。</li>
<li>该方法在多种实验场景下的攻击效果优于先前的攻击方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20613">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-54fff541b55d8fd7c223e0e571d9efec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7873759da6809fd4e60c464b2b06ccc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1750f769c8d6b44376df275ce5b43e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27e96681eaff16f54dca0604fc89986f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c78917b622581b741a7288fdacb454c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13679b502687e67a732c944c8a66abd0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GENRE-CMR-Generalizable-Deep-Learning-for-Diverse-Multi-Domain-Cardiac-MRI-Reconstruction"><a href="#GENRE-CMR-Generalizable-Deep-Learning-for-Diverse-Multi-Domain-Cardiac-MRI-Reconstruction" class="headerlink" title="GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac   MRI Reconstruction"></a>GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac   MRI Reconstruction</h2><p><strong>Authors:Kian Anvari Hamedani, Narges Razizadeh, Shahabedin Nabavi, Mohsen Ebrahimi Moghaddam</strong></p>
<p>Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols. </p>
<blockquote>
<p>加速心血管磁共振（CMR）图像重建仍然是一个关键挑战，因为在扫描时间和图像质量之间存在权衡，特别是在不同采集环境之间进行推广时。我们提出了GENRE-CMR，这是一种基于生成对抗网络（GAN）的架构，采用残差深度展开重建框架，以提高重建的保真度和通用性。该架构将迭代优化展开成卷积子网络的级联，通过残差连接丰富特征，实现从浅层到深层阶段的渐进特征传播。为了进一步提高性能，我们集成了两种损失函数：（1）边缘感知区域（EAR）损失，引导网络关注结构信息丰富的区域，有助于防止常见的重建模糊；（2）统计分布对齐（SDA）损失，通过对称KL散度公式，对特征空间进行不同数据分布的规范化。大量实验证实，GENRE-CMR在训练和未见数据上超越了最先进的方法，在不同加速因子和采样轨迹的未见分布上实现了0.9552的结构相似性度量（SSIM）和38.90分贝的峰值信噪比（PSNR）。消融研究证实了所提出组件对重建质量和通用性的贡献。我们的框架为高质量CMR重建提供了统一且稳健的解决方案，为在临床环境中适应不同采集协议部署铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20600v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于生成对抗网络（GAN）的架构GENRE-CMR，采用残差深度展开重建框架，提高重建保真度和通用性。它将迭代优化展开为一系列卷积子网络，借助残差连接实现从浅层到深层阶段的特征传播。同时集成Edge-Aware Region（EAR）损失和Statistical Distribution Alignment（SDA）损失，分别引导网络关注结构信息区域并规范化不同数据分布的特征空间。实验证明，GENRE-CMR在训练和未见数据上超越现有方法，实现0.9552的SSIM和38.90 dB的PSNR。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GENRE-CMR利用GAN架构解决心血管磁共振（CMR）图像重建的挑战。</li>
<li>采用残差深度展开重建框架，提高重建质量和通用性。</li>
<li>集成EAR损失，引导网络关注结构信息区域，减少重建模糊。</li>
<li>集成SDA损失，通过对称KL散度公式规范化不同数据分布的特征空间。</li>
<li>在各种加速因素和采样轨迹下，GENRE-CMR在未见数据上实现0.9552的SSIM和38.90 dB的PSNR，超越现有方法。</li>
<li>消融研究证实了每个提议组件对重建质量和通用性的贡献。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20600">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8ce62fecc6d670d575a4b873c8b9d965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-800e8bc408c699ca5c913370018114d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61873e291fbabdd3c28d3b6c687bbd4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SimShear-Sim-to-Real-Shear-based-Tactile-Servoing"><a href="#SimShear-Sim-to-Real-Shear-based-Tactile-Servoing" class="headerlink" title="SimShear: Sim-to-Real Shear-based Tactile Servoing"></a>SimShear: Sim-to-Real Shear-based Tactile Servoing</h2><p><strong>Authors:Kipp McAdam Freud, Yijiong Lin, Nathan F. Lepora</strong></p>
<p>We present SimShear, a sim-to-real pipeline for tactile control that enables the use of shear information without explicitly modeling shear dynamics in simulation. Shear, arising from lateral movements across contact surfaces, is critical for tasks involving dynamic object interactions but remains challenging to simulate. To address this, we introduce shPix2pix, a shear-conditioned U-Net GAN that transforms simulated tactile images absent of shear, together with a vector encoding shear information, into realistic equivalents with shear deformations. This method outperforms baseline pix2pix approaches in simulating tactile images and in pose&#x2F;shear prediction. We apply SimShear to two control tasks using a pair of low-cost desktop robotic arms equipped with a vision-based tactile sensor: (i) a tactile tracking task, where a follower arm tracks a surface moved by a leader arm, and (ii) a collaborative co-lifting task, where both arms jointly hold an object while the leader follows a prescribed trajectory. Our method maintains contact errors within 1 to 2 mm across varied trajectories where shear sensing is essential, validating the feasibility of sim-to-real shear modeling with rigid-body simulators and opening new directions for simulation in tactile robotics. </p>
<blockquote>
<p>我们提出了SimShear，这是一个用于触觉控制的模拟到现实管道，它能够在模拟过程中不使用明确的剪切动力学模型就使用剪切信息。剪切力产生于接触表面的横向运动，对于涉及动态对象交互的任务至关重要，但在模拟中仍然具有挑战性。为了解决这一问题，我们引入了shPix2pix，这是一个受剪切力条件约束的U-Net GAN，它能够将模拟的触觉图像（不含剪切力）与编码剪切信息的向量相结合，转化为具有剪切变形的现实等效图像。该方法在模拟触觉图像、姿态&#x2F;剪切预测方面优于基线pix2pix方法。我们将SimShear应用于两个控制任务中，使用一对配备有基于视觉的触觉传感器的低成本桌面机械臂：（i）触觉跟踪任务，其中跟随臂跟踪领导者臂移动的表面；（ii）协作协同提升任务，其中两个机械臂共同握住一个对象，领导者臂遵循预定的轨迹。我们的方法在各种轨迹上保持接触误差在1到2毫米之间，剪切感知至关重要，验证了使用刚体模拟器进行模拟到现实的剪切建模的可行性，并为触觉机器人的模拟打开了新的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20561v1">PDF</a> 2025 Conference on Robot Learning (CoRL)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SimShear，这是一种用于触觉控制的模拟到现实场景的管道，它能够在模拟过程中使用剪切信息，而无需显式建模剪切动力学。SimShear通过引入shPix2pix，一个受剪切力条件约束的U-Net GAN，将模拟的触觉图像（无剪切信息）与编码剪切信息的向量相结合，转化为具有剪切变形的真实图像。该方法在模拟触觉图像和姿态&#x2F;剪切预测方面优于基本的pix2pix方法。文章还展示了SimShear在两台低成本桌面机器人手臂上的两个控制任务的应用，包括触觉跟踪任务和协作协同升降任务。该方法在剪切感知至关重要的不同轨迹上保持接触误差在1至2毫米范围内，验证了使用刚体模拟器进行模拟到现实的剪切建模的可行性，为触觉机器人模拟提供了新的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SimShear是一种模拟到现实的管道，用于处理触觉控制中的剪切信息，无需显式模拟剪切动力学。</li>
<li>shPix2pix是一个受剪切条件约束的U-Net GAN，能够将模拟的触觉图像转化为具有剪切变形的真实图像。</li>
<li>SimShear在模拟触觉图像和姿态&#x2F;剪切预测方面的性能优于基本的pix2pix方法。</li>
<li>SimShear应用于两个机器人控制任务：触觉跟踪任务和协作协同升降任务。</li>
<li>SimShear在剪切感知重要的不同轨迹上保持低接触误差。</li>
<li>该方法验证了使用刚体模拟器进行模拟到现实的剪切建模的可行性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20561">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ea5579a607ce21c779c0e8699a749b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfba1606fb597bacc0ea88a88f894f74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-126954ee54801aad5684a4be1865d66e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-619ea2f073f7e7cb24261b5521056ac6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6699e99c3fe90ad9f883b433e189f5e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56d5539def0d08867930b29487cd695f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolution"><a href="#CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolution" class="headerlink" title="CATformer: Contrastive Adversarial Transformer for Image   Super-Resolution"></a>CATformer: Contrastive Adversarial Transformer for Image   Super-Resolution</h2><p><strong>Authors:Qinyi Tian, Spence Cox, Laura E. Dalton</strong></p>
<p>Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired transformer, which progressively refines latent representations, with an auxiliary transformer branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and decoded using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among transformer-, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired transformers in super-resolution. </p>
<blockquote>
<p>超分辨率技术依然是一种能够提升低分辨率图像质量的具有前景的技术。本研究介绍了CATformer（对比对抗性转换器），这是一种新型神经网络，融合了扩散启发特征细化、对抗性学习和对比学习。CATformer采用双分支架构，结合主要扩散启发转换器，逐步优化潜在表示，以及一个辅助转换器分支，通过学习潜在对比增强对噪声的鲁棒性。这些互补的表示融合并使用深度Residual-in-Residual Dense块进行解码，以提高重建质量。在基准数据集上的广泛实验表明，CATformer在效率和视觉图像质量上均优于最新的基于转换器和扩散的方法。这项工作缩小了基于转换器、扩散和GAN的方法之间的性能差距，为扩散启发转换器的实际应用在超分辨率领域奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17708v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为CATformer的新型神经网络，它结合了扩散启发特征细化、对抗性学习与对比学习。CATformer采用双分支架构，主分支为扩散启发变压器，用于逐步优化潜在表示，辅助分支为增强噪声鲁棒性的对比学习变压器。两者融合并使用深度Residual-in-Residual Dense Blocks进行解码，以提高重建质量。在基准数据集上的实验表明，CATformer在效率和视觉图像质量上均优于最近的基于变压器和扩散的方法。该研究缩小了基于变压器、扩散和GAN的方法之间的性能差距，为扩散启发变压器在超分辨率中的实际应用奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CATformer是一种结合扩散启发特征细化、对抗性学习和对比学习的新型神经网络。</li>
<li>CATformer采用双分支架构，主分支用于逐步优化潜在表示，辅助分支增强噪声鲁棒性。</li>
<li>通过对比学习，CATformer能够学习潜在对比，提高模型的性能。</li>
<li>CATformer使用深度Residual-in-Residual Dense Blocks进行解码，以提高图像重建质量。</li>
<li>在基准数据集上的实验表明，CATformer在效率和视觉质量上均表现优异。</li>
<li>CATformer的研究缩小了不同方法之间的性能差距，为扩散启发变压器在超分辨率中的应用奠定了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17708">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a98965b00ed9e65b5db306e9c4661723.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31ae8de490f71b23e3af54dcbc851cd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffcb7da9a2d9366c4dc6b83d57058c2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83842ff9321d1bf83aac6cf72964c1cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c6861ee19e892a19ee335d5bedf357d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Novel-Local-Focusing-Mechanism-for-Deepfake-Detection-Generalization"><a href="#A-Novel-Local-Focusing-Mechanism-for-Deepfake-Detection-Generalization" class="headerlink" title="A Novel Local Focusing Mechanism for Deepfake Detection Generalization"></a>A Novel Local Focusing Mechanism for Deepfake Detection Generalization</h2><p><strong>Authors:Mingliang Li, Lin Yuanbo Wu, Changhong Liu, Hanxi Li</strong></p>
<p>The rapid advancement of deepfake generation techniques has intensified the need for robust and generalizable detection methods. Existing approaches based on reconstruction learning typically leverage deep convolutional networks to extract differential features. However, these methods show poor generalization across object categories (e.g., from faces to cars) and generation domains (e.g., from GANs to Stable Diffusion), due to intrinsic limitations of deep CNNs. First, models trained on a specific category tend to overfit to semantic feature distributions, making them less transferable to other categories, especially as network depth increases. Second, Global Average Pooling (GAP) compresses critical local forgery cues into a single vector, thus discarding discriminative patterns vital for real-fake classification. To address these issues, we propose a novel Local Focus Mechanism (LFM) that explicitly attends to discriminative local features for differentiating fake from real images. LFM integrates a Salience Network (SNet) with a task-specific Top-K Pooling (TKP) module to select the K most informative local patterns. To mitigate potential overfitting introduced by Top-K pooling, we introduce two regularization techniques: Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS), which enhance the model’s robustness. LFM achieves a 3.7 improvement in accuracy and a 2.8 increase in average precision over the state-of-the-art Neighboring Pixel Relationships (NPR) method, while maintaining exceptional efficiency at 1789 FPS on a single NVIDIA A6000 GPU. Our approach sets a new benchmark for cross-domain deepfake detection. The source code are available in <a target="_blank" rel="noopener" href="https://github.com/lmlpy/LFM.git">https://github.com/lmlpy/LFM.git</a> </p>
<blockquote>
<p>随着深度伪造生成技术的快速发展，对稳健且通用的检测方法的需求愈发迫切。现有的基于重建学习的方法通常利用深度卷积网络来提取差异特征。然而，由于深度CNN的内在局限性，这些方法在对象类别（例如从人脸到汽车）和生成领域（例如从GAN到Stable Diffusion）之间的泛化能力较差。首先，针对特定类别训练的模型往往会对语义特征分布产生过度拟合，使其难以转移到其他类别，尤其是随着网络深度的增加。其次，全局平均池化（GAP）将关键的局部伪造线索压缩成一个单一的向量，从而丢弃了对于真实和伪造分类至关重要的判别模式。为了解决这些问题，我们提出了一种新颖的局部焦点机制（LFM），它专注于区分真实和伪造图像的判别局部特征。LFM将显著性网络（SNet）与特定任务的Top-K池化（TKP）模块相结合，选择K个最具信息量的局部模式。为了缓解Top-K池化可能引入的过度拟合问题，我们引入了两种正则化技术：基于排名的线性dropout（RBLD）和随机K采样（RKS），提高了模型的稳健性。与最先进的邻近像素关系（NPR）方法相比，LFM在准确性上提高了3.7%，平均精度提高了2.8%，同时在单个NVIDIA A6000 GPU上保持了出色的效率。我们的方法为跨域深度伪造检测设定了新的基准。源代码可在<a target="_blank" rel="noopener" href="https://github.com/lmlpy/LFM.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lmlpy/LFM.git找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17029v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型本地焦点机制（LFM），解决了深度伪造图像检测中跨对象类别和生成域泛化性能差的问题。该机制通过显著性网络（SNet）和任务特定Top-K池化模块（TKP）的结合，关注鉴别局部特征，以提高模型对真实和伪造图像的区分能力。通过引入两种正则化技术：基于排名的线性丢弃（RBLD）和随机K采样（RKS），增强了模型的稳健性。LFM在准确率和平均精度上取得了显著改进，相比现有方法有明显提升，同时在单一NVIDIA A6000 GPU上保持了高效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度伪造生成技术的快速发展加剧了需要寻找稳健且可泛化的检测办法。</li>
<li>当前基于重建学习的方法主要利用深度卷积网络提取特征，但在跨对象类别和生成域方面的泛化性能不佳。</li>
<li>本地焦点机制（LFM）通过关注鉴别局部特征来提高模型性能。</li>
<li>LFM结合显著性网络（SNet）和任务特定Top-K池化模块（TKP）进行选择性的特征提取。</li>
<li>引入两种正则化技术以增强模型的稳健性：基于排名的线性丢弃（RBLD）和随机K采样（RKS）。</li>
<li>LFM在准确率和平均精度上实现了显著改进，相比现有方法表现更优。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17029">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cb36237ab6313bb522aee0d2972c7d36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-499478081a0acaa27b842f673f25ea9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69afb54610cc3fd2db9ab6227faabf49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-739445d3faf16d4c49b926fe7b94f57f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Foundations-and-Models-in-Modern-Computer-Vision-Key-Building-Blocks-in-Landmark-Architectures"><a href="#Foundations-and-Models-in-Modern-Computer-Vision-Key-Building-Blocks-in-Landmark-Architectures" class="headerlink" title="Foundations and Models in Modern Computer Vision: Key Building Blocks in   Landmark Architectures"></a>Foundations and Models in Modern Computer Vision: Key Building Blocks in   Landmark Architectures</h2><p><strong>Authors:Radu-Andrei Bourceanu, Neil De La Fuente, Jan Grimm, Andrei Jardan, Andriy Manucharyan, Cornelius Weiss, Daniel Cremers, Roman Pflugfelder</strong></p>
<p>This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analysis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer architecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recognition. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models. </p>
<blockquote>
<p>本报告通过分析六篇有影响力的论文，分析了计算机视觉中关键设计模式的演变。分析从图像识别的基本架构开始。我们回顾了ResNet，它引入了残差连接，克服了梯度消失问题，使得训练更深层次的卷积网络变得有效。之后，我们研究了将Transformer架构应用于图像补丁序列的Vision Transformer（ViT），这奠定了新的范式，并证明了基于注意力的模型在大规模图像识别中的有效性。在这些视觉表示主干的基础上，我们研究了生成模型。分析了生成对抗网络（GANs）的新型对抗训练过程，该过程挑战生成器与鉴别器学习复杂的数据分布。然后介绍了潜在扩散模型（LDMs），通过对先前生成方法的改进，在感知压缩的潜在空间中执行序贯去噪过程，实现了高保真合成和更高的计算效率，代表了当前图像生成的最新技术。最后，我们探索了减少对比标签数据依赖性的自监督学习技术。DINO是一种自蒸馏框架，学生网络学习匹配动量更新后的教师输出，产生具有强大k-NN分类性能的特征。最后以Masked Autoencoders（MAE）为例，它采用对称的编码器-解码器设计来重建高度遮罩的输入，提供了一种可伸缩性高、有效的预训练大规模视觉模型的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23357v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇报告分析了计算机视觉中关键设计模式的演变，通过考察六篇有影响力的论文，从基础的图像识别架构到生成模型、自监督学习技术进行了深入探讨。报告强调了残差连接、Transformer架构、生成对抗网络（GANs）、潜在扩散模型（LDMs）以及自蒸馏框架（DINO）和掩码自动编码器（MAE）等关键技术和方法的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>报告分析了计算机视觉领域关键设计模式的演进，从图像识别的基础架构开始。</li>
<li>介绍了ResNet如何引入残差连接解决梯度消失问题，并有效训练更深的卷积网络。</li>
<li>Vision Transformer（ViT）首次将Transformer架构应用于图像补丁序列，展示了基于注意力的模型在大规模图像识别中的有效性。</li>
<li>报告探讨了生成模型，特别是生成对抗网络（GANs）的对抗性训练过程，以及潜在扩散模型（LDMs）在生成方法上的改进。</li>
<li>自监督学习技术减少了对面标签数据的依赖，其中DINO自蒸馏框架和Masked Autoencoders（MAE）是代表方法。</li>
<li>LDMs通过感知压缩的潜在空间中的连续去噪过程提高了生成图像的质量和计算效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23357">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1a18a3a6abfb6c9ac649e6950b4fd7fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb308890c31345f2aab07dd14fc1774c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c7b7523d1996c598c4c86034ca318b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-498ae1a3c68c14971d462838ba0e1fa5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a42b24e9ff5111ddf2f9f617b925344.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Pixel-Perfect-MegaMed-A-Megapixel-Scale-Vision-Language-Foundation-Model-for-Generating-High-Resolution-Medical-Images"><a href="#Pixel-Perfect-MegaMed-A-Megapixel-Scale-Vision-Language-Foundation-Model-for-Generating-High-Resolution-Medical-Images" class="headerlink" title="Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation   Model for Generating High Resolution Medical Images"></a>Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation   Model for Generating High Resolution Medical Images</h2><p><strong>Authors:Zahra TehraniNasab, Hujun Ni, Amar Kumar, Tal Arbel</strong></p>
<p>Medical image synthesis presents unique challenges due to the inherent complexity and high-resolution details required in clinical contexts. Traditional generative architectures such as Generative Adversarial Networks (GANs) or Variational Auto Encoder (VAEs) have shown great promise for high-resolution image generation but struggle with preserving fine-grained details that are key for accurate diagnosis. To address this issue, we introduce Pixel Perfect MegaMed, the first vision-language foundation model to synthesize images at resolutions of 1024x1024. Our method deploys a multi-scale transformer architecture designed specifically for ultra-high resolution medical image generation, enabling the preservation of both global anatomical context and local image-level details. By leveraging vision-language alignment techniques tailored to medical terminology and imaging modalities, Pixel Perfect MegaMed bridges the gap between textual descriptions and visual representations at unprecedented resolution levels. We apply our model to the CheXpert dataset and demonstrate its ability to generate clinically faithful chest X-rays from text prompts. Beyond visual quality, these high-resolution synthetic images prove valuable for downstream tasks such as classification, showing measurable performance gains when used for data augmentation, particularly in low-data regimes. Our code is accessible through the project website - <a target="_blank" rel="noopener" href="https://tehraninasab.github.io/pixelperfect-megamed">https://tehraninasab.github.io/pixelperfect-megamed</a>. </p>
<blockquote>
<p>医学影像合成面临独特的挑战，这是由于在临床环境中需要固有的复杂性和高分辨率细节。传统的生成架构，如生成对抗网络（GANs）或变分自动编码器（VAEs）在高分辨率图像生成方面显示出巨大潜力，但在保留对于准确诊断至关重要的精细细节方面却遇到困难。为解决这一问题，我们推出了Pixel Perfect MegaMed，这是第一个以视觉语言为基础、能够在1024x1024分辨率下合成图像的模型。我们的方法采用专门设计用于超高分辨率医学影像生成的多尺度变压器架构，能够同时保留全局解剖背景和局部图像级细节。通过利用针对医学术语和成像模式的视觉语言对齐技术，Pixel Perfect MegaMed能够在前所未有的高分辨率水平上弥合了文本描述和视觉表示之间的鸿沟。我们将其模型应用于CheXpert数据集，展示了根据文本提示生成临床真实的胸部X光片的能力。除了视觉质量外，这些高分辨率的合成图像对于下游任务（如分类）具有证明价值，在数据增强方面使用时表现出可衡量的性能提升，特别是在数据稀缺的情况下。我们的代码可通过项目网站访问：<a target="_blank" rel="noopener" href="https://tehraninasab.github.io/pixelperfect-megamed">https://tehraninasab.github.io/pixelperfect-megamed</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12698v2">PDF</a> </p>
<p><strong>Summary</strong><br>医疗图像合成面临独特的挑战，因为临床环境中需要复杂的固有性和高解析度细节。传统生成架构如生成对抗网络（GANs）或变分自动编码器（VAEs）在高解析度图像生成方面展现出巨大潜力，但在保持精细粒度细节方面存在困难，这对于准确诊断至关重要。为解决这一问题，我们推出Pixel Perfect MegaMed，首个用于合成1024x1024分辨率图像的视觉语言基础模型。该方法采用专为超高分辨率医疗图像生成设计的多尺度变压器架构，能够同时保留全局解剖背景和局部图像级细节。通过利用针对医疗术语和成像模式的视觉语言对齐技术，Pixel Perfect MegaMed在前所未有的高分辨率水平上搭建了文本描述和视觉表示之间的桥梁。我们在CheXpert数据集上应用该模型，并展示了从文本提示生成临床真实胸部X射线图像的能力。除了视觉质量外，这些高分辨率合成图像对于下游任务如分类也证明其价值，特别是在数据增强方面表现出可衡量的性能提升，尤其在低数据情况下更是如此。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗图像合成面临固有复杂性和高解析度细节的挑战。</li>
<li>传统生成架构如GANs和VAEs在高分辨率图像生成上表现出潜力，但难以保留精细细节。</li>
<li>Pixel Perfect MegaMed是首个用于超高分辨率医疗图像生成的视觉语言基础模型。</li>
<li>该模型采用多尺度变压器架构，能同时保留全局解剖背景和局部图像级细节。</li>
<li>Pixel Perfect MegaMed利用针对医疗术语和成像模式的视觉语言对齐技术。</li>
<li>该模型能在前所未有的高分辨率水平上生成临床真实的胸部X射线图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12698">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1ae8955fe252a9c571aaa9314e3f353e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57e817aa0dbbcf308bea5b3261f30dff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1a2176d3fa572acc0854816c8a72660.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d012fe2a81642ecec2f15be78edc072.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c42a57577e8cea47e099ec7775fa2046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-330f1d2c544ed40486dfb2bd3aede002.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a52c70c867007936d6d1befa915b616.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LumiNet-Latent-Intrinsics-Meets-Diffusion-Models-for-Indoor-Scene-Relighting"><a href="#LumiNet-Latent-Intrinsics-Meets-Diffusion-Models-for-Indoor-Scene-Relighting" class="headerlink" title="LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene   Relighting"></a>LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene   Relighting</h2><p><strong>Authors:Xiaoyan Xing, Konrad Groh, Sezer Karaoglu, Theo Gevers, Anand Bhattad</strong></p>
<p>We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the target’s lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the target’s latent extrinsic properties via cross-attention and fine-tuning.   Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input. </p>
<blockquote>
<p>我们介绍了LumiNet，这是一种新型架构，它利用生成模型和潜在内在表示来进行有效的光照转移。给定源图像和目标光照图像，LumiNet合成源场景的重照版本，该版本捕捉目标的光照。我们的方法做出了两个主要贡献：一是基于StyleGAN的重照模型的数据整理策略，用于我们的训练；二是对基于扩散的ControlNet进行修改，该网络处理来自源图像的潜在内在属性和来自目标图像潜在外在属性。我们进一步通过学到的适配器（MLP）改进光照转移，该适配器通过跨注意力和微调注入目标潜在外在属性。与传统的ControlNet不同，后者根据单个场景生成带有条件映射的图像，LumiNet处理来自两个不同图像的潜在表示——保留源场景的几何形状和反射率，同时转移目标场景的光照特性。实验表明，我们的方法在场景空间布局和材料变化的情况下成功转移复杂的照明现象，包括高光和间接照明，并且在仅使用图像作为输入的情况下，在室内场景的复杂挑战上表现优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00177v3">PDF</a> Corrects an evaluation bug in Table 1 due to a data normalization   error. Thanks to the Sony PlayStation team for discovering and reporting the   issue. The paper’s core contributions, qualitative results, and user study   are unaffected. We also include a minor update to the method to further   improve result quality. Project page: <a target="_blank" rel="noopener" href="https://luminet-relight.github.io/">https://luminet-relight.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>LumiNet是一个新型架构，利用生成模型和潜在内在表示进行高效的光线转移。给定源图像和目标光照图像，LumiNet合成源场景的重新照明版本，捕捉目标的光照。我们的方法做出了两个关键贡献：一是从StyleGAN的重新照明模型中为我们的训练进行数据整理策略，二是改进基于扩散的ControlNet，处理来自源图像的潜在内在属性和来自目标图像的外在潜在属性。我们进一步通过学习的适配器和交叉注意力及微调技术改进光线转移。不同于传统的ControlNet，LumiNet处理来自两个不同图像的潜在表示，保留源场景的几何和颜色信息，同时转移目标场景的光照特性。实验表明，我们的方法在复杂的照明现象转移方面表现出色，包括高光和间接照明，适用于不同空间布局和材料的不同室内场景，仅使用图像作为输入就超越了现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LumiNet是一个新型架构，利用生成模型和潜在表示进行光线转移。</li>
<li>LumiNet可以从源图像获取潜在内在属性，并从目标图像获取潜在外在属性。</li>
<li>通过数据整理策略、改进的ControlNet和学习的适配器改进光线转移。</li>
<li>LumiNet通过交叉注意力和微调技术实现目标的光照特性转移。</li>
<li>LumiNet不同于传统的ControlNet，因为它处理来自两个不同图像的潜在表示。</li>
<li>LumiNet保留了源场景的几何和颜色信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00177">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3ddfa93d603285b161310151d6eb7062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3149a0b30159a04a8702353c22367c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a0137817fc38ca27ba7c1dbc0c5dee7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9690f7f6d5f03951499d130dff2a8437.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Conditional-Wasserstein-Distances-with-Applications-in-Bayesian-OT-Flow-Matching"><a href="#Conditional-Wasserstein-Distances-with-Applications-in-Bayesian-OT-Flow-Matching" class="headerlink" title="Conditional Wasserstein Distances with Applications in Bayesian OT Flow   Matching"></a>Conditional Wasserstein Distances with Applications in Bayesian OT Flow   Matching</h2><p><strong>Authors:Jannis Chemseddine, Paul Hagemann, Gabriele Steidl, Christian Wald</strong></p>
<p>In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback–Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein distance. Based on this, we propose an extension of OT Flow Matching for solving Bayesian inverse problems and demonstrate its numerical advantages on an inverse problem and class-conditional image generation. </p>
<blockquote>
<p>在逆向问题中，许多条件生成模型通过最小化联合分布与其学习近似分布之间的距离来近似后验分布。虽然这种方法在Kullback-Leibler散度的情况下也控制了后验分布之间的距离，但对于Wasserstein距离来说，通常并不适用。在本文中，我们通过一组限制耦合引入条件Wasserstein距离，该距离等于后验的期望Wasserstein距离。有趣的是，条件Wasserstein-1流的双重形式与条件Wasserstein GAN文献中的损失非常自然地相似。我们推导出条件Wasserstein距离的理论性质，并描述了相应的测地线、速度场以及流ODEs的特性。随后，我们提出了通过放松条件Wasserstein距离来近似速度场的方法。基于此，我们提出了扩展的OT流匹配来解决贝叶斯逆向问题，并在一个逆向问题和类别条件图像生成上展示了其数值优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.18705v3">PDF</a> This paper supersedes arXiv:2310.13433, accepted at JMLR</p>
<p><strong>Summary</strong><br>本文介绍了基于条件Wasserstein距离的逆问题解决方案。文章引入了通过限制耦合集定义的条件Wasserstein距离，该距离等于后验分布的期望Wasserstein距离。此外，文章探讨了条件Wasserstein-1流的双重表述，它与条件Wasserstein GAN文献中的损失函数有自然联系。文章还推导了条件Wasserstein距离的理论属性，并展示了其解决贝叶斯逆问题的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>条件生成模型通过最小化联合分布与其学习近似之间的距离来逼近后验分布。</li>
<li>Wasserstein距离下的条件生成模型并不等同于在Kullback-Leibler散度下的模型。</li>
<li>本文引入了条件Wasserstein距离，该距离是通过一组限制耦合定义的，等于后验的期望Wasserstein距离。</li>
<li>条件Wasserstein-1流的双重表述与条件Wasserstein GAN文献中的损失函数有自然联系。</li>
<li>文章推导了条件Wasserstein距离的理论属性，包括对应的测地线、速度场和流动常微分方程。</li>
<li>提出通过放松条件Wasserstein距离来近似速度场的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.18705">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2f005d4e2b8b574b449e21711994e8c2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cf62327d584f9334a71420e5574fb1e5.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-09-07  Hyper Diffusion Avatars Dynamic Human Avatar Generation using Network   Weight Space Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6a0bdfdf63cac948d1a54a7ecf6398fa.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-09-07  Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
