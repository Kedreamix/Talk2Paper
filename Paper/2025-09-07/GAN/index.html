<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Enhancing Robustness in Post-Processing Watermarking An Ensemble Attack   Network Using CNNs and Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-2bc346488cc2f961ad5a020e9350b097.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    40 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Enhancing-Robustness-in-Post-Processing-Watermarking-An-Ensemble-Attack-Network-Using-CNNs-and-Transformers"><a href="#Enhancing-Robustness-in-Post-Processing-Watermarking-An-Ensemble-Attack-Network-Using-CNNs-and-Transformers" class="headerlink" title="Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack   Network Using CNNs and Transformers"></a>Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack   Network Using CNNs and Transformers</h2><p><strong>Authors:Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen</strong></p>
<p>Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the modelâ€™s internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:<a target="_blank" rel="noopener" href="https://github.com/aiiu-lab/DeepRobustWatermark">https://github.com/aiiu-lab/DeepRobustWatermark</a>. </p>
<blockquote>
<p>è¿‘æœŸå…³äºæ·±åº¦æ°´å°çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºå¤„ç†ä¸­çš„æ°´å°ï¼Œå³å°†æ°´å°è¿‡ç¨‹é›†æˆåˆ°å›¾åƒç”Ÿæˆä¸­ã€‚ç„¶è€Œï¼Œåå¤„ç†æ°´å°æ–¹æ³•ï¼Œå³åœ¨å›¾åƒç”ŸæˆååµŒå…¥æ°´å°ï¼Œæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚å®ƒå¯ä»¥åº”ç”¨äºä»»ä½•ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºï¼ˆä¾‹å¦‚GANsã€æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œè€Œæ— éœ€è®¿é—®æ¨¡å‹çš„å†…éƒ¨ç»“æ„ã€‚å®ƒè¿˜å…è®¸ç”¨æˆ·å°†ç‹¬ç‰¹çš„æ°´å°åµŒå…¥åˆ°å•ç‹¬çš„å›¾åƒä¸­ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶ä¸“æ³¨äºåå¤„ç†æ°´å°ï¼Œå¹¶é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆé›†æˆæ”»å‡»ç½‘ç»œæ¥æé«˜å…¶ç¨³å¥æ€§ã€‚æˆ‘ä»¬æ„å»ºäº†å„ç§ç‰ˆæœ¬çš„æ”»å‡»ç½‘ç»œï¼Œä½¿ç”¨CNNå’ŒTransformeråœ¨ç©ºé—´åŸŸå’Œé¢‘åŸŸè¿›è¡Œç ”ç©¶ï¼Œä»¥è°ƒæŸ¥æ¯ç§ç»„åˆå¦‚ä½•å½±å“æ°´å°æ¨¡å‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ç©ºé—´åŸŸä¸­ä½¿ç”¨åŸºäºCNNçš„æ”»å‡»ç½‘ç»œä¸é¢‘åŸŸä¸­ä½¿ç”¨åŸºäºTransformerçš„æ”»å‡»ç½‘ç»œç›¸ç»“åˆï¼Œä¸ºæ°´å°æ¨¡å‹æä¾›äº†æœ€é«˜çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨WAVESåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œä»¥å¹³å‡ä½å‡†ç¡®æ€§ä½œä¸ºåº¦é‡æ ‡å‡†ï¼Œè¯æ˜æˆ‘ä»¬çš„é›†æˆæ”»å‡»ç½‘ç»œåœ¨å„ç§å‹åŠ›æµ‹è¯•ä¸‹æ˜¾è‘—æé«˜äº†åŸºçº¿æ°´å°æ–¹æ³•çš„ç¨³å¥æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨WAVESä¸­å®šä¹‰çš„å†ç”Ÿæ”»å‡»æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†StegaStampæé«˜äº†18.743%ã€‚ä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/aiiu-lab/DeepRobustWatermark%E3%80%82">https://github.com/aiiu-lab/DeepRobustWatermarkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03006v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ°´å°æŠ€æœ¯çš„æ–°ç ”ç©¶èšç„¦äºåœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ°´å°åµŒå…¥ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿè¡¨æ˜ï¼Œå¯¹ç”Ÿæˆå›¾åƒè¿›è¡Œåç»­å¤„ç†çš„æ°´å°åµŒå…¥æ–¹æ³•å…·æœ‰æ›´å¤§çš„çµæ´»æ€§ã€‚è¯¥æ–¹æ³•å¯åº”ç”¨äºä»»ä½•ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºï¼Œæ— éœ€äº†è§£æ¨¡å‹çš„å†…éƒ¨ç»“æ„ï¼Œå¹¶èƒ½ä¸ºå•å¹…å›¾åƒåµŒå…¥ç‹¬ç‰¹çš„æ°´å°ã€‚æœ¬ç ”ç©¶é‡ç‚¹å…³æ³¨åç»­å¤„ç†æ°´å°æŠ€æœ¯ï¼Œå¹¶é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆé›†æˆæ”»å‡»ç½‘ç»œæ¥æé«˜å…¶ç¨³å¥æ€§ã€‚ç ”ç©¶å‘ç°ç»“åˆCNNåœ¨ç©ºé—´å’ŒåŸºäºTransformeråœ¨é¢‘ç‡åŸŸçš„æ”»å‡»ç½‘ç»œå°†å¤§å¤§æé«˜æ°´å°æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨WAVESåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œé›†æˆæ”»å‡»ç½‘ç»œæ˜¾è‘—æé«˜äº†åŸºçº¿æ°´å°æ–¹æ³•åœ¨å„ç§å‹åŠ›æµ‹è¯•ä¸‹çš„ç¨³å¥æ€§ã€‚ç‰¹åˆ«æ˜¯é’ˆå¯¹WAVESå®šä¹‰çš„å†ç”Ÿæ”»å‡»ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†StegaStampçš„å‡†ç¡®ç‡é«˜è¾¾18.743%ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ·±åº¦æ°´å°ç ”ç©¶ä¸»è¦å…³æ³¨åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ°´å°åµŒå…¥æ–¹æ³•ã€‚</li>
<li>å¦ä¸€ç§æ–¹æ³•æ˜¯é‡‡ç”¨åå¤„ç†æ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å…·æœ‰æ›´å¤§çš„çµæ´»æ€§ï¼Œé€‚ç”¨äºå„ç§ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºã€‚</li>
<li>ç ”ç©¶é‡ç‚¹æ˜¯é€šè¿‡ç»“åˆé›†æˆæ”»å‡»ç½‘ç»œæé«˜åå¤„ç†æ°´å°æŠ€æœ¯çš„ç¨³å¥æ€§ã€‚</li>
<li>ç»“åˆCNNåœ¨ç©ºé—´å’ŒåŸºäºTransformeråœ¨é¢‘ç‡åŸŸçš„æ”»å‡»ç½‘ç»œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ°´å°æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨WAVESåŸºå‡†æµ‹è¯•ä¸Šï¼Œé›†æˆæ”»å‡»ç½‘ç»œæ˜¾è‘—å¢å¼ºäº†åŸºçº¿æ°´å°æ–¹æ³•çš„ç¨³å¥æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹ç‰¹å®šç±»å‹çš„æ”»å‡»æ—¶ã€‚</li>
<li>ç›¸å…³ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31d9a8eb646e4b9694a615a59ab71f4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa5648be02b7878b66a80855f977a8a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7946b3dbb9a0533fbf22ac7279e04e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2bc346488cc2f961ad5a020e9350b097.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3481b10ba88f480acfd4ac0a7e7dcccd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d14c8f238fa9a41bb14fb8640d88cf8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Revisiting-the-Privacy-Risks-of-Split-Inference-A-GAN-Based-Data-Reconstruction-Attack-via-Progressive-Feature-Optimization"><a href="#Revisiting-the-Privacy-Risks-of-Split-Inference-A-GAN-Based-Data-Reconstruction-Attack-via-Progressive-Feature-Optimization" class="headerlink" title="Revisiting the Privacy Risks of Split Inference: A GAN-Based Data   Reconstruction Attack via Progressive Feature Optimization"></a>Revisiting the Privacy Risks of Split Inference: A GAN-Based Data   Reconstruction Attack via Progressive Feature Optimization</h2><p><strong>Authors:Yixiang Qiu, Yanhan Liu, Hongyao Yu, Hao Fang, Bin Chen, Shu-Tao Xia, Ke Xu</strong></p>
<p>The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§å¯¼è‡´é‡‡ç”¨Split Inferenceï¼ˆSIï¼‰æŠ€æœ¯ã€‚ä½œä¸ºä¸€ç§åˆä½œå‹èŒƒå¼ï¼Œå®ƒåœ¨è¾¹ç¼˜è®¾å¤‡å’Œäº‘ä¹‹é—´åˆ†é…è®¡ç®—ä»»åŠ¡ä»¥å‡å°‘å»¶è¿Ÿå¹¶ä¿æŠ¤ç”¨æˆ·éšç§ã€‚ç„¶è€Œï¼Œæ•°æ®é‡å»ºæ”»å‡»ï¼ˆDRAsï¼‰çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œä¸­é—´ç‰¹å¾äº¤æ¢åœ¨SIä¸­å¯ä»¥ç”¨äºæ¢å¤æ•æ„Ÿè¾“å…¥æ•°æ®ï¼Œä»è€Œå¸¦æ¥é‡å¤§éšç§é£é™©ã€‚ç°æœ‰çš„DRAsé€šå¸¸ä»…åœ¨æµ…å±‚æ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è¯­ä¹‰å…ˆéªŒä¿¡æ¯ï¼Œä»è€Œé™åˆ¶äº†å…¶é‡å»ºè´¨é‡å’Œè·¨æ•°æ®é›†å’Œæ¨¡å‹æ¶æ„çš„é€šç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–°å‹DRAæ¡†æ¶ï¼Œå…¶ä¸­åŒ…å«æ¸è¿›ç‰¹å¾ä¼˜åŒ–ï¼ˆPFOï¼‰ï¼Œè¯¥æ¡†æ¶å°†ç”Ÿæˆå™¨åˆ†è§£ä¸ºåˆ†å±‚å—å¹¶é€å±‚ä¼˜åŒ–ä¸­é—´è¡¨ç¤ºå½¢å¼ï¼Œä»¥æé«˜é‡å»ºå›¾åƒè¯­ä¹‰ä¿çœŸåº¦ã€‚ä¸ºäº†ç¨³å®šä¼˜åŒ–å¹¶æ”¹å–„å›¾åƒçš„çœŸå®æ€§ï¼Œæˆ‘ä»¬åœ¨é‡å»ºè¿‡ç¨‹ä¸­å¼•å…¥äº†L1çƒçº¦æŸã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ä¼˜äºå…ˆå‰æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡åœºæ™¯ã€éå†…éƒ¨åˆ†å¸ƒè®¾ç½®ä»¥åŠé’ˆå¯¹æ›´æ·±å’Œæ›´å¤æ‚çš„DNNsæ—¶æ•ˆæœæ›´ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20613v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong><br>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„å¤æ‚æ€§å¢é•¿æ¨åŠ¨äº†Split Inferenceï¼ˆSIï¼‰çš„åº”ç”¨ã€‚ä½†æ–°çš„æ•°æ®é‡å»ºæ”»å‡»ï¼ˆDRAï¼‰æ­ç¤ºäº†SIä¸­äº¤æ¢çš„ä¸­é—´ç‰¹å¾å¯è¢«ç”¨æ¥æ¢å¤æ•æ„Ÿè¾“å…¥æ•°æ®ï¼Œå¸¦æ¥éšç§é£é™©ã€‚ç°æœ‰çš„DRAsé€šå¸¸åœ¨æµ…å±‚æ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œæ— æ³•å……åˆ†åˆ©ç”¨è¯­ä¹‰å…ˆéªŒä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºGANçš„DRAæ¡†æ¶ï¼Œç»“åˆæ¸è¿›ç‰¹å¾ä¼˜åŒ–ï¼ˆPFOï¼‰ï¼Œåˆ†è§£ç”Ÿæˆå™¨ä¸ºå±‚æ¬¡å—å¹¶é€æ­¥ä¼˜åŒ–ä¸­é—´è¡¨ç¤ºï¼Œæé«˜é‡å»ºå›¾åƒè¯­ä¹‰ä¿çœŸåº¦ã€‚å¼•å…¥L1çƒçº¦æŸä»¥ä¼˜åŒ–ç¨³å®šæ€§å’Œå›¾åƒçœŸå®æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é«˜æ¸…åœºæ™¯ã€éåˆ†å¸ƒè®¾ç½®å’Œæ›´å¤æ‚çš„DNNsä¸Šçš„æ”»å‡»æ•ˆæœè¿œè¶…å…ˆå‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Split Inferenceï¼ˆSIï¼‰è¢«åº”ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ï¼Œæ—¨åœ¨å‡å°‘å»¶è¿Ÿå’Œä¿æŠ¤ç”¨æˆ·éšç§ã€‚</li>
<li>æ•°æ®é‡å»ºæ”»å‡»ï¼ˆDRAï¼‰å¯ä»¥åˆ©ç”¨SIä¸­çš„ä¸­é—´ç‰¹å¾æ¢å¤æ•æ„Ÿè¾“å…¥æ•°æ®ï¼Œå¸¦æ¥éšç§é£é™©ã€‚</li>
<li>ç°æœ‰DRAsä¸»è¦é€‚ç”¨äºæµ…å±‚æ¨¡å‹ï¼Œä¸”åœ¨åˆ©ç”¨è¯­ä¹‰å…ˆéªŒä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºGANçš„DRAæ¡†æ¶ï¼Œç»“åˆæ¸è¿›ç‰¹å¾ä¼˜åŒ–ï¼ˆPFOï¼‰ï¼Œæé«˜é‡å»ºå›¾åƒçš„è¯­ä¹‰ä¿çœŸåº¦ã€‚</li>
<li>å¼•å…¥L1çƒçº¦æŸä»¥ä¼˜åŒ–å›¾åƒçš„çœŸå®æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§å®éªŒåœºæ™¯ä¸‹çš„æ”»å‡»æ•ˆæœä¼˜äºå…ˆå‰çš„æ”»å‡»æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-54fff541b55d8fd7c223e0e571d9efec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7873759da6809fd4e60c464b2b06ccc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1750f769c8d6b44376df275ce5b43e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27e96681eaff16f54dca0604fc89986f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c78917b622581b741a7288fdacb454c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13679b502687e67a732c944c8a66abd0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GENRE-CMR-Generalizable-Deep-Learning-for-Diverse-Multi-Domain-Cardiac-MRI-Reconstruction"><a href="#GENRE-CMR-Generalizable-Deep-Learning-for-Diverse-Multi-Domain-Cardiac-MRI-Reconstruction" class="headerlink" title="GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac   MRI Reconstruction"></a>GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac   MRI Reconstruction</h2><p><strong>Authors:Kian Anvari Hamedani, Narges Razizadeh, Shahabedin Nabavi, Mohsen Ebrahimi Moghaddam</strong></p>
<p>Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols. </p>
<blockquote>
<p>åŠ é€Ÿå¿ƒè¡€ç®¡ç£å…±æŒ¯ï¼ˆCMRï¼‰å›¾åƒé‡å»ºä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨æ‰«ææ—¶é—´å’Œå›¾åƒè´¨é‡ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒé‡‡é›†ç¯å¢ƒä¹‹é—´è¿›è¡Œæ¨å¹¿æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†GENRE-CMRï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¶æ„ï¼Œé‡‡ç”¨æ®‹å·®æ·±åº¦å±•å¼€é‡å»ºæ¡†æ¶ï¼Œä»¥æé«˜é‡å»ºçš„ä¿çœŸåº¦å’Œé€šç”¨æ€§ã€‚è¯¥æ¶æ„å°†è¿­ä»£ä¼˜åŒ–å±•å¼€æˆå·ç§¯å­ç½‘ç»œçš„çº§è”ï¼Œé€šè¿‡æ®‹å·®è¿æ¥ä¸°å¯Œç‰¹å¾ï¼Œå®ç°ä»æµ…å±‚åˆ°æ·±å±‚é˜¶æ®µçš„æ¸è¿›ç‰¹å¾ä¼ æ’­ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬é›†æˆäº†ä¸¤ç§æŸå¤±å‡½æ•°ï¼šï¼ˆ1ï¼‰è¾¹ç¼˜æ„ŸçŸ¥åŒºåŸŸï¼ˆEARï¼‰æŸå¤±ï¼Œå¼•å¯¼ç½‘ç»œå…³æ³¨ç»“æ„ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸï¼Œæœ‰åŠ©äºé˜²æ­¢å¸¸è§çš„é‡å»ºæ¨¡ç³Šï¼›ï¼ˆ2ï¼‰ç»Ÿè®¡åˆ†å¸ƒå¯¹é½ï¼ˆSDAï¼‰æŸå¤±ï¼Œé€šè¿‡å¯¹ç§°KLæ•£åº¦å…¬å¼ï¼Œå¯¹ç‰¹å¾ç©ºé—´è¿›è¡Œä¸åŒæ•°æ®åˆ†å¸ƒçš„è§„èŒƒåŒ–ã€‚å¤§é‡å®éªŒè¯å®ï¼ŒGENRE-CMRåœ¨è®­ç»ƒå’Œæœªè§æ•°æ®ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨ä¸åŒåŠ é€Ÿå› å­å’Œé‡‡æ ·è½¨è¿¹çš„æœªè§åˆ†å¸ƒä¸Šå®ç°äº†0.9552çš„ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰å’Œ38.90åˆ†è´çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€‚æ¶ˆèç ”ç©¶è¯å®äº†æ‰€æå‡ºç»„ä»¶å¯¹é‡å»ºè´¨é‡å’Œé€šç”¨æ€§çš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºé«˜è´¨é‡CMRé‡å»ºæä¾›äº†ç»Ÿä¸€ä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºåœ¨ä¸´åºŠç¯å¢ƒä¸­é€‚åº”ä¸åŒé‡‡é›†åè®®éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20600v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¶æ„GENRE-CMRï¼Œé‡‡ç”¨æ®‹å·®æ·±åº¦å±•å¼€é‡å»ºæ¡†æ¶ï¼Œæé«˜é‡å»ºä¿çœŸåº¦å’Œé€šç”¨æ€§ã€‚å®ƒå°†è¿­ä»£ä¼˜åŒ–å±•å¼€ä¸ºä¸€ç³»åˆ—å·ç§¯å­ç½‘ç»œï¼Œå€ŸåŠ©æ®‹å·®è¿æ¥å®ç°ä»æµ…å±‚åˆ°æ·±å±‚é˜¶æ®µçš„ç‰¹å¾ä¼ æ’­ã€‚åŒæ—¶é›†æˆEdge-Aware Regionï¼ˆEARï¼‰æŸå¤±å’ŒStatistical Distribution Alignmentï¼ˆSDAï¼‰æŸå¤±ï¼Œåˆ†åˆ«å¼•å¯¼ç½‘ç»œå…³æ³¨ç»“æ„ä¿¡æ¯åŒºåŸŸå¹¶è§„èŒƒåŒ–ä¸åŒæ•°æ®åˆ†å¸ƒçš„ç‰¹å¾ç©ºé—´ã€‚å®éªŒè¯æ˜ï¼ŒGENRE-CMRåœ¨è®­ç»ƒå’Œæœªè§æ•°æ®ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå®ç°0.9552çš„SSIMå’Œ38.90 dBçš„PSNRã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GENRE-CMRåˆ©ç”¨GANæ¶æ„è§£å†³å¿ƒè¡€ç®¡ç£å…±æŒ¯ï¼ˆCMRï¼‰å›¾åƒé‡å»ºçš„æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨æ®‹å·®æ·±åº¦å±•å¼€é‡å»ºæ¡†æ¶ï¼Œæé«˜é‡å»ºè´¨é‡å’Œé€šç”¨æ€§ã€‚</li>
<li>é›†æˆEARæŸå¤±ï¼Œå¼•å¯¼ç½‘ç»œå…³æ³¨ç»“æ„ä¿¡æ¯åŒºåŸŸï¼Œå‡å°‘é‡å»ºæ¨¡ç³Šã€‚</li>
<li>é›†æˆSDAæŸå¤±ï¼Œé€šè¿‡å¯¹ç§°KLæ•£åº¦å…¬å¼è§„èŒƒåŒ–ä¸åŒæ•°æ®åˆ†å¸ƒçš„ç‰¹å¾ç©ºé—´ã€‚</li>
<li>åœ¨å„ç§åŠ é€Ÿå› ç´ å’Œé‡‡æ ·è½¨è¿¹ä¸‹ï¼ŒGENRE-CMRåœ¨æœªè§æ•°æ®ä¸Šå®ç°0.9552çš„SSIMå’Œ38.90 dBçš„PSNRï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†æ¯ä¸ªæè®®ç»„ä»¶å¯¹é‡å»ºè´¨é‡å’Œé€šç”¨æ€§çš„è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ce62fecc6d670d575a4b873c8b9d965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-800e8bc408c699ca5c913370018114d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61873e291fbabdd3c28d3b6c687bbd4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SimShear-Sim-to-Real-Shear-based-Tactile-Servoing"><a href="#SimShear-Sim-to-Real-Shear-based-Tactile-Servoing" class="headerlink" title="SimShear: Sim-to-Real Shear-based Tactile Servoing"></a>SimShear: Sim-to-Real Shear-based Tactile Servoing</h2><p><strong>Authors:Kipp McAdam Freud, Yijiong Lin, Nathan F. Lepora</strong></p>
<p>We present SimShear, a sim-to-real pipeline for tactile control that enables the use of shear information without explicitly modeling shear dynamics in simulation. Shear, arising from lateral movements across contact surfaces, is critical for tasks involving dynamic object interactions but remains challenging to simulate. To address this, we introduce shPix2pix, a shear-conditioned U-Net GAN that transforms simulated tactile images absent of shear, together with a vector encoding shear information, into realistic equivalents with shear deformations. This method outperforms baseline pix2pix approaches in simulating tactile images and in pose&#x2F;shear prediction. We apply SimShear to two control tasks using a pair of low-cost desktop robotic arms equipped with a vision-based tactile sensor: (i) a tactile tracking task, where a follower arm tracks a surface moved by a leader arm, and (ii) a collaborative co-lifting task, where both arms jointly hold an object while the leader follows a prescribed trajectory. Our method maintains contact errors within 1 to 2 mm across varied trajectories where shear sensing is essential, validating the feasibility of sim-to-real shear modeling with rigid-body simulators and opening new directions for simulation in tactile robotics. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SimShearï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè§¦è§‰æ§åˆ¶çš„æ¨¡æ‹Ÿåˆ°ç°å®ç®¡é“ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­ä¸ä½¿ç”¨æ˜ç¡®çš„å‰ªåˆ‡åŠ¨åŠ›å­¦æ¨¡å‹å°±ä½¿ç”¨å‰ªåˆ‡ä¿¡æ¯ã€‚å‰ªåˆ‡åŠ›äº§ç”Ÿäºæ¥è§¦è¡¨é¢çš„æ¨ªå‘è¿åŠ¨ï¼Œå¯¹äºæ¶‰åŠåŠ¨æ€å¯¹è±¡äº¤äº’çš„ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†åœ¨æ¨¡æ‹Ÿä¸­ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†shPix2pixï¼Œè¿™æ˜¯ä¸€ä¸ªå—å‰ªåˆ‡åŠ›æ¡ä»¶çº¦æŸçš„U-Net GANï¼Œå®ƒèƒ½å¤Ÿå°†æ¨¡æ‹Ÿçš„è§¦è§‰å›¾åƒï¼ˆä¸å«å‰ªåˆ‡åŠ›ï¼‰ä¸ç¼–ç å‰ªåˆ‡ä¿¡æ¯çš„å‘é‡ç›¸ç»“åˆï¼Œè½¬åŒ–ä¸ºå…·æœ‰å‰ªåˆ‡å˜å½¢çš„ç°å®ç­‰æ•ˆå›¾åƒã€‚è¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿè§¦è§‰å›¾åƒã€å§¿æ€&#x2F;å‰ªåˆ‡é¢„æµ‹æ–¹é¢ä¼˜äºåŸºçº¿pix2pixæ–¹æ³•ã€‚æˆ‘ä»¬å°†SimShearåº”ç”¨äºä¸¤ä¸ªæ§åˆ¶ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨ä¸€å¯¹é…å¤‡æœ‰åŸºäºè§†è§‰çš„è§¦è§‰ä¼ æ„Ÿå™¨çš„ä½æˆæœ¬æ¡Œé¢æœºæ¢°è‡‚ï¼šï¼ˆiï¼‰è§¦è§‰è·Ÿè¸ªä»»åŠ¡ï¼Œå…¶ä¸­è·Ÿéšè‡‚è·Ÿè¸ªé¢†å¯¼è€…è‡‚ç§»åŠ¨çš„è¡¨é¢ï¼›ï¼ˆiiï¼‰åä½œååŒæå‡ä»»åŠ¡ï¼Œå…¶ä¸­ä¸¤ä¸ªæœºæ¢°è‡‚å…±åŒæ¡ä½ä¸€ä¸ªå¯¹è±¡ï¼Œé¢†å¯¼è€…è‡‚éµå¾ªé¢„å®šçš„è½¨è¿¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è½¨è¿¹ä¸Šä¿æŒæ¥è§¦è¯¯å·®åœ¨1åˆ°2æ¯«ç±³ä¹‹é—´ï¼Œå‰ªåˆ‡æ„ŸçŸ¥è‡³å…³é‡è¦ï¼ŒéªŒè¯äº†ä½¿ç”¨åˆšä½“æ¨¡æ‹Ÿå™¨è¿›è¡Œæ¨¡æ‹Ÿåˆ°ç°å®çš„å‰ªåˆ‡å»ºæ¨¡çš„å¯è¡Œæ€§ï¼Œå¹¶ä¸ºè§¦è§‰æœºå™¨äººçš„æ¨¡æ‹Ÿæ‰“å¼€äº†æ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20561v1">PDF</a> 2025 Conference on Robot Learning (CoRL)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SimShearï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§¦è§‰æ§åˆ¶çš„æ¨¡æ‹Ÿåˆ°ç°å®åœºæ™¯çš„ç®¡é“ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­ä½¿ç”¨å‰ªåˆ‡ä¿¡æ¯ï¼Œè€Œæ— éœ€æ˜¾å¼å»ºæ¨¡å‰ªåˆ‡åŠ¨åŠ›å­¦ã€‚SimShearé€šè¿‡å¼•å…¥shPix2pixï¼Œä¸€ä¸ªå—å‰ªåˆ‡åŠ›æ¡ä»¶çº¦æŸçš„U-Net GANï¼Œå°†æ¨¡æ‹Ÿçš„è§¦è§‰å›¾åƒï¼ˆæ— å‰ªåˆ‡ä¿¡æ¯ï¼‰ä¸ç¼–ç å‰ªåˆ‡ä¿¡æ¯çš„å‘é‡ç›¸ç»“åˆï¼Œè½¬åŒ–ä¸ºå…·æœ‰å‰ªåˆ‡å˜å½¢çš„çœŸå®å›¾åƒã€‚è¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿè§¦è§‰å›¾åƒå’Œå§¿æ€&#x2F;å‰ªåˆ‡é¢„æµ‹æ–¹é¢ä¼˜äºåŸºæœ¬çš„pix2pixæ–¹æ³•ã€‚æ–‡ç« è¿˜å±•ç¤ºäº†SimShearåœ¨ä¸¤å°ä½æˆæœ¬æ¡Œé¢æœºå™¨äººæ‰‹è‡‚ä¸Šçš„ä¸¤ä¸ªæ§åˆ¶ä»»åŠ¡çš„åº”ç”¨ï¼ŒåŒ…æ‹¬è§¦è§‰è·Ÿè¸ªä»»åŠ¡å’Œåä½œååŒå‡é™ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åœ¨å‰ªåˆ‡æ„ŸçŸ¥è‡³å…³é‡è¦çš„ä¸åŒè½¨è¿¹ä¸Šä¿æŒæ¥è§¦è¯¯å·®åœ¨1è‡³2æ¯«ç±³èŒƒå›´å†…ï¼ŒéªŒè¯äº†ä½¿ç”¨åˆšä½“æ¨¡æ‹Ÿå™¨è¿›è¡Œæ¨¡æ‹Ÿåˆ°ç°å®çš„å‰ªåˆ‡å»ºæ¨¡çš„å¯è¡Œæ€§ï¼Œä¸ºè§¦è§‰æœºå™¨äººæ¨¡æ‹Ÿæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SimShearæ˜¯ä¸€ç§æ¨¡æ‹Ÿåˆ°ç°å®çš„ç®¡é“ï¼Œç”¨äºå¤„ç†è§¦è§‰æ§åˆ¶ä¸­çš„å‰ªåˆ‡ä¿¡æ¯ï¼Œæ— éœ€æ˜¾å¼æ¨¡æ‹Ÿå‰ªåˆ‡åŠ¨åŠ›å­¦ã€‚</li>
<li>shPix2pixæ˜¯ä¸€ä¸ªå—å‰ªåˆ‡æ¡ä»¶çº¦æŸçš„U-Net GANï¼Œèƒ½å¤Ÿå°†æ¨¡æ‹Ÿçš„è§¦è§‰å›¾åƒè½¬åŒ–ä¸ºå…·æœ‰å‰ªåˆ‡å˜å½¢çš„çœŸå®å›¾åƒã€‚</li>
<li>SimShearåœ¨æ¨¡æ‹Ÿè§¦è§‰å›¾åƒå’Œå§¿æ€&#x2F;å‰ªåˆ‡é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºåŸºæœ¬çš„pix2pixæ–¹æ³•ã€‚</li>
<li>SimShearåº”ç”¨äºä¸¤ä¸ªæœºå™¨äººæ§åˆ¶ä»»åŠ¡ï¼šè§¦è§‰è·Ÿè¸ªä»»åŠ¡å’Œåä½œååŒå‡é™ä»»åŠ¡ã€‚</li>
<li>SimShearåœ¨å‰ªåˆ‡æ„ŸçŸ¥é‡è¦çš„ä¸åŒè½¨è¿¹ä¸Šä¿æŒä½æ¥è§¦è¯¯å·®ã€‚</li>
<li>è¯¥æ–¹æ³•éªŒè¯äº†ä½¿ç”¨åˆšä½“æ¨¡æ‹Ÿå™¨è¿›è¡Œæ¨¡æ‹Ÿåˆ°ç°å®çš„å‰ªåˆ‡å»ºæ¨¡çš„å¯è¡Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea5579a607ce21c779c0e8699a749b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfba1606fb597bacc0ea88a88f894f74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-126954ee54801aad5684a4be1865d66e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-619ea2f073f7e7cb24261b5521056ac6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6699e99c3fe90ad9f883b433e189f5e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56d5539def0d08867930b29487cd695f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolution"><a href="#CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolution" class="headerlink" title="CATformer: Contrastive Adversarial Transformer for Image   Super-Resolution"></a>CATformer: Contrastive Adversarial Transformer for Image   Super-Resolution</h2><p><strong>Authors:Qinyi Tian, Spence Cox, Laura E. Dalton</strong></p>
<p>Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired transformer, which progressively refines latent representations, with an auxiliary transformer branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and decoded using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among transformer-, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired transformers in super-resolution. </p>
<blockquote>
<p>è¶…åˆ†è¾¨ç‡æŠ€æœ¯ä¾ç„¶æ˜¯ä¸€ç§èƒ½å¤Ÿæå‡ä½åˆ†è¾¨ç‡å›¾åƒè´¨é‡çš„å…·æœ‰å‰æ™¯çš„æŠ€æœ¯ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†CATformerï¼ˆå¯¹æ¯”å¯¹æŠ—æ€§è½¬æ¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œï¼Œèåˆäº†æ‰©æ•£å¯å‘ç‰¹å¾ç»†åŒ–ã€å¯¹æŠ—æ€§å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ ã€‚CATformeré‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œç»“åˆä¸»è¦æ‰©æ•£å¯å‘è½¬æ¢å™¨ï¼Œé€æ­¥ä¼˜åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œä»¥åŠä¸€ä¸ªè¾…åŠ©è½¬æ¢å™¨åˆ†æ”¯ï¼Œé€šè¿‡å­¦ä¹ æ½œåœ¨å¯¹æ¯”å¢å¼ºå¯¹å™ªå£°çš„é²æ£’æ€§ã€‚è¿™äº›äº’è¡¥çš„è¡¨ç¤ºèåˆå¹¶ä½¿ç”¨æ·±åº¦Residual-in-Residual Denseå—è¿›è¡Œè§£ç ï¼Œä»¥æé«˜é‡å»ºè´¨é‡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCATformeråœ¨æ•ˆç‡å’Œè§†è§‰å›¾åƒè´¨é‡ä¸Šå‡ä¼˜äºæœ€æ–°çš„åŸºäºè½¬æ¢å™¨å’Œæ‰©æ•£çš„æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œç¼©å°äº†åŸºäºè½¬æ¢å™¨ã€æ‰©æ•£å’ŒGANçš„æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºæ‰©æ•£å¯å‘è½¬æ¢å™¨çš„å®é™…åº”ç”¨åœ¨è¶…åˆ†è¾¨ç‡é¢†åŸŸå¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17708v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCATformerçš„æ–°å‹ç¥ç»ç½‘ç»œï¼Œå®ƒç»“åˆäº†æ‰©æ•£å¯å‘ç‰¹å¾ç»†åŒ–ã€å¯¹æŠ—æ€§å­¦ä¹ ä¸å¯¹æ¯”å­¦ä¹ ã€‚CATformeré‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œä¸»åˆ†æ”¯ä¸ºæ‰©æ•£å¯å‘å˜å‹å™¨ï¼Œç”¨äºé€æ­¥ä¼˜åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œè¾…åŠ©åˆ†æ”¯ä¸ºå¢å¼ºå™ªå£°é²æ£’æ€§çš„å¯¹æ¯”å­¦ä¹ å˜å‹å™¨ã€‚ä¸¤è€…èåˆå¹¶ä½¿ç”¨æ·±åº¦Residual-in-Residual Dense Blocksè¿›è¡Œè§£ç ï¼Œä»¥æé«˜é‡å»ºè´¨é‡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCATformeråœ¨æ•ˆç‡å’Œè§†è§‰å›¾åƒè´¨é‡ä¸Šå‡ä¼˜äºæœ€è¿‘çš„åŸºäºå˜å‹å™¨å’Œæ‰©æ•£çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶ç¼©å°äº†åŸºäºå˜å‹å™¨ã€æ‰©æ•£å’ŒGANçš„æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºæ‰©æ•£å¯å‘å˜å‹å™¨åœ¨è¶…åˆ†è¾¨ç‡ä¸­çš„å®é™…åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CATformeræ˜¯ä¸€ç§ç»“åˆæ‰©æ•£å¯å‘ç‰¹å¾ç»†åŒ–ã€å¯¹æŠ—æ€§å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ çš„æ–°å‹ç¥ç»ç½‘ç»œã€‚</li>
<li>CATformeré‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œä¸»åˆ†æ”¯ç”¨äºé€æ­¥ä¼˜åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œè¾…åŠ©åˆ†æ”¯å¢å¼ºå™ªå£°é²æ£’æ€§ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼ŒCATformerèƒ½å¤Ÿå­¦ä¹ æ½œåœ¨å¯¹æ¯”ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>CATformerä½¿ç”¨æ·±åº¦Residual-in-Residual Dense Blocksè¿›è¡Œè§£ç ï¼Œä»¥æé«˜å›¾åƒé‡å»ºè´¨é‡ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCATformeråœ¨æ•ˆç‡å’Œè§†è§‰è´¨é‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>CATformerçš„ç ”ç©¶ç¼©å°äº†ä¸åŒæ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºæ‰©æ•£å¯å‘å˜å‹å™¨åœ¨è¶…åˆ†è¾¨ç‡ä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17708">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a98965b00ed9e65b5db306e9c4661723.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31ae8de490f71b23e3af54dcbc851cd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffcb7da9a2d9366c4dc6b83d57058c2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83842ff9321d1bf83aac6cf72964c1cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c6861ee19e892a19ee335d5bedf357d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Novel-Local-Focusing-Mechanism-for-Deepfake-Detection-Generalization"><a href="#A-Novel-Local-Focusing-Mechanism-for-Deepfake-Detection-Generalization" class="headerlink" title="A Novel Local Focusing Mechanism for Deepfake Detection Generalization"></a>A Novel Local Focusing Mechanism for Deepfake Detection Generalization</h2><p><strong>Authors:Mingliang Li, Lin Yuanbo Wu, Changhong Liu, Hanxi Li</strong></p>
<p>The rapid advancement of deepfake generation techniques has intensified the need for robust and generalizable detection methods. Existing approaches based on reconstruction learning typically leverage deep convolutional networks to extract differential features. However, these methods show poor generalization across object categories (e.g., from faces to cars) and generation domains (e.g., from GANs to Stable Diffusion), due to intrinsic limitations of deep CNNs. First, models trained on a specific category tend to overfit to semantic feature distributions, making them less transferable to other categories, especially as network depth increases. Second, Global Average Pooling (GAP) compresses critical local forgery cues into a single vector, thus discarding discriminative patterns vital for real-fake classification. To address these issues, we propose a novel Local Focus Mechanism (LFM) that explicitly attends to discriminative local features for differentiating fake from real images. LFM integrates a Salience Network (SNet) with a task-specific Top-K Pooling (TKP) module to select the K most informative local patterns. To mitigate potential overfitting introduced by Top-K pooling, we introduce two regularization techniques: Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS), which enhance the modelâ€™s robustness. LFM achieves a 3.7 improvement in accuracy and a 2.8 increase in average precision over the state-of-the-art Neighboring Pixel Relationships (NPR) method, while maintaining exceptional efficiency at 1789 FPS on a single NVIDIA A6000 GPU. Our approach sets a new benchmark for cross-domain deepfake detection. The source code are available in <a target="_blank" rel="noopener" href="https://github.com/lmlpy/LFM.git">https://github.com/lmlpy/LFM.git</a> </p>
<blockquote>
<p>éšç€æ·±åº¦ä¼ªé€ ç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹ç¨³å¥ä¸”é€šç”¨çš„æ£€æµ‹æ–¹æ³•çš„éœ€æ±‚æ„ˆå‘è¿«åˆ‡ã€‚ç°æœ‰çš„åŸºäºé‡å»ºå­¦ä¹ çš„æ–¹æ³•é€šå¸¸åˆ©ç”¨æ·±åº¦å·ç§¯ç½‘ç»œæ¥æå–å·®å¼‚ç‰¹å¾ã€‚ç„¶è€Œï¼Œç”±äºæ·±åº¦CNNçš„å†…åœ¨å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•åœ¨å¯¹è±¡ç±»åˆ«ï¼ˆä¾‹å¦‚ä»äººè„¸åˆ°æ±½è½¦ï¼‰å’Œç”Ÿæˆé¢†åŸŸï¼ˆä¾‹å¦‚ä»GANåˆ°Stable Diffusionï¼‰ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚é¦–å…ˆï¼Œé’ˆå¯¹ç‰¹å®šç±»åˆ«è®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¼šå¯¹è¯­ä¹‰ç‰¹å¾åˆ†å¸ƒäº§ç”Ÿè¿‡åº¦æ‹Ÿåˆï¼Œä½¿å…¶éš¾ä»¥è½¬ç§»åˆ°å…¶ä»–ç±»åˆ«ï¼Œå°¤å…¶æ˜¯éšç€ç½‘ç»œæ·±åº¦çš„å¢åŠ ã€‚å…¶æ¬¡ï¼Œå…¨å±€å¹³å‡æ± åŒ–ï¼ˆGAPï¼‰å°†å…³é”®çš„å±€éƒ¨ä¼ªé€ çº¿ç´¢å‹ç¼©æˆä¸€ä¸ªå•ä¸€çš„å‘é‡ï¼Œä»è€Œä¸¢å¼ƒäº†å¯¹äºçœŸå®å’Œä¼ªé€ åˆ†ç±»è‡³å…³é‡è¦çš„åˆ¤åˆ«æ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å±€éƒ¨ç„¦ç‚¹æœºåˆ¶ï¼ˆLFMï¼‰ï¼Œå®ƒä¸“æ³¨äºåŒºåˆ†çœŸå®å’Œä¼ªé€ å›¾åƒçš„åˆ¤åˆ«å±€éƒ¨ç‰¹å¾ã€‚LFMå°†æ˜¾è‘—æ€§ç½‘ç»œï¼ˆSNetï¼‰ä¸ç‰¹å®šä»»åŠ¡çš„Top-Kæ± åŒ–ï¼ˆTKPï¼‰æ¨¡å—ç›¸ç»“åˆï¼Œé€‰æ‹©Kä¸ªæœ€å…·ä¿¡æ¯é‡çš„å±€éƒ¨æ¨¡å¼ã€‚ä¸ºäº†ç¼“è§£Top-Kæ± åŒ–å¯èƒ½å¼•å…¥çš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼šåŸºäºæ’åçš„çº¿æ€§dropoutï¼ˆRBLDï¼‰å’ŒéšæœºKé‡‡æ ·ï¼ˆRKSï¼‰ï¼Œæé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚ä¸æœ€å…ˆè¿›çš„é‚»è¿‘åƒç´ å…³ç³»ï¼ˆNPRï¼‰æ–¹æ³•ç›¸æ¯”ï¼ŒLFMåœ¨å‡†ç¡®æ€§ä¸Šæé«˜äº†3.7%ï¼Œå¹³å‡ç²¾åº¦æé«˜äº†2.8%ï¼ŒåŒæ—¶åœ¨å•ä¸ªNVIDIA A6000 GPUä¸Šä¿æŒäº†å‡ºè‰²çš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºè·¨åŸŸæ·±åº¦ä¼ªé€ æ£€æµ‹è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lmlpy/LFM.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lmlpy/LFM.gitæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17029v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æœ¬åœ°ç„¦ç‚¹æœºåˆ¶ï¼ˆLFMï¼‰ï¼Œè§£å†³äº†æ·±åº¦ä¼ªé€ å›¾åƒæ£€æµ‹ä¸­è·¨å¯¹è±¡ç±»åˆ«å’Œç”ŸæˆåŸŸæ³›åŒ–æ€§èƒ½å·®çš„é—®é¢˜ã€‚è¯¥æœºåˆ¶é€šè¿‡æ˜¾è‘—æ€§ç½‘ç»œï¼ˆSNetï¼‰å’Œä»»åŠ¡ç‰¹å®šTop-Kæ± åŒ–æ¨¡å—ï¼ˆTKPï¼‰çš„ç»“åˆï¼Œå…³æ³¨é‰´åˆ«å±€éƒ¨ç‰¹å¾ï¼Œä»¥æé«˜æ¨¡å‹å¯¹çœŸå®å’Œä¼ªé€ å›¾åƒçš„åŒºåˆ†èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ä¸¤ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼šåŸºäºæ’åçš„çº¿æ€§ä¸¢å¼ƒï¼ˆRBLDï¼‰å’ŒéšæœºKé‡‡æ ·ï¼ˆRKSï¼‰ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚LFMåœ¨å‡†ç¡®ç‡å’Œå¹³å‡ç²¾åº¦ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æå‡ï¼ŒåŒæ—¶åœ¨å•ä¸€NVIDIA A6000 GPUä¸Šä¿æŒäº†é«˜æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ä¼ªé€ ç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†éœ€è¦å¯»æ‰¾ç¨³å¥ä¸”å¯æ³›åŒ–çš„æ£€æµ‹åŠæ³•ã€‚</li>
<li>å½“å‰åŸºäºé‡å»ºå­¦ä¹ çš„æ–¹æ³•ä¸»è¦åˆ©ç”¨æ·±åº¦å·ç§¯ç½‘ç»œæå–ç‰¹å¾ï¼Œä½†åœ¨è·¨å¯¹è±¡ç±»åˆ«å’Œç”ŸæˆåŸŸæ–¹é¢çš„æ³›åŒ–æ€§èƒ½ä¸ä½³ã€‚</li>
<li>æœ¬åœ°ç„¦ç‚¹æœºåˆ¶ï¼ˆLFMï¼‰é€šè¿‡å…³æ³¨é‰´åˆ«å±€éƒ¨ç‰¹å¾æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>LFMç»“åˆæ˜¾è‘—æ€§ç½‘ç»œï¼ˆSNetï¼‰å’Œä»»åŠ¡ç‰¹å®šTop-Kæ± åŒ–æ¨¡å—ï¼ˆTKPï¼‰è¿›è¡Œé€‰æ‹©æ€§çš„ç‰¹å¾æå–ã€‚</li>
<li>å¼•å…¥ä¸¤ç§æ­£åˆ™åŒ–æŠ€æœ¯ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ï¼šåŸºäºæ’åçš„çº¿æ€§ä¸¢å¼ƒï¼ˆRBLDï¼‰å’ŒéšæœºKé‡‡æ ·ï¼ˆRKSï¼‰ã€‚</li>
<li>LFMåœ¨å‡†ç¡®ç‡å’Œå¹³å‡ç²¾åº¦ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17029">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb36237ab6313bb522aee0d2972c7d36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-499478081a0acaa27b842f673f25ea9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69afb54610cc3fd2db9ab6227faabf49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-739445d3faf16d4c49b926fe7b94f57f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Foundations-and-Models-in-Modern-Computer-Vision-Key-Building-Blocks-in-Landmark-Architectures"><a href="#Foundations-and-Models-in-Modern-Computer-Vision-Key-Building-Blocks-in-Landmark-Architectures" class="headerlink" title="Foundations and Models in Modern Computer Vision: Key Building Blocks in   Landmark Architectures"></a>Foundations and Models in Modern Computer Vision: Key Building Blocks in   Landmark Architectures</h2><p><strong>Authors:Radu-Andrei Bourceanu, Neil De La Fuente, Jan Grimm, Andrei Jardan, Andriy Manucharyan, Cornelius Weiss, Daniel Cremers, Roman Pflugfelder</strong></p>
<p>This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analysis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer architecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recognition. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models. </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šé€šè¿‡åˆ†æå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡ï¼Œåˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ã€‚åˆ†æä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„å¼€å§‹ã€‚æˆ‘ä»¬å›é¡¾äº†ResNetï¼Œå®ƒå¼•å…¥äº†æ®‹å·®è¿æ¥ï¼Œå…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿å¾—è®­ç»ƒæ›´æ·±å±‚æ¬¡çš„å·ç§¯ç½‘ç»œå˜å¾—æœ‰æ•ˆã€‚ä¹‹åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—çš„Vision Transformerï¼ˆViTï¼‰ï¼Œè¿™å¥ å®šäº†æ–°çš„èŒƒå¼ï¼Œå¹¶è¯æ˜äº†åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨è¿™äº›è§†è§‰è¡¨ç¤ºä¸»å¹²çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”Ÿæˆæ¨¡å‹ã€‚åˆ†æäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ–°å‹å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹æŒ‘æˆ˜ç”Ÿæˆå™¨ä¸é‰´åˆ«å™¨å­¦ä¹ å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚ç„¶åä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ï¼Œé€šè¿‡å¯¹å…ˆå‰ç”Ÿæˆæ–¹æ³•çš„æ”¹è¿›ï¼Œåœ¨æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œåºè´¯å»å™ªè¿‡ç¨‹ï¼Œå®ç°äº†é«˜ä¿çœŸåˆæˆå’Œæ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œä»£è¡¨äº†å½“å‰å›¾åƒç”Ÿæˆçš„æœ€æ–°æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡å°‘å¯¹æ¯”æ ‡ç­¾æ•°æ®ä¾èµ–æ€§çš„è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚DINOæ˜¯ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ åŒ¹é…åŠ¨é‡æ›´æ–°åçš„æ•™å¸ˆè¾“å‡ºï¼Œäº§ç”Ÿå…·æœ‰å¼ºå¤§k-NNåˆ†ç±»æ€§èƒ½çš„ç‰¹å¾ã€‚æœ€åä»¥Masked Autoencodersï¼ˆMAEï¼‰ä¸ºä¾‹ï¼Œå®ƒé‡‡ç”¨å¯¹ç§°çš„ç¼–ç å™¨-è§£ç å™¨è®¾è®¡æ¥é‡å»ºé«˜åº¦é®ç½©çš„è¾“å…¥ï¼Œæä¾›äº†ä¸€ç§å¯ä¼¸ç¼©æ€§é«˜ã€æœ‰æ•ˆçš„é¢„è®­ç»ƒå¤§è§„æ¨¡è§†è§‰æ¨¡å‹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23357v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æŠ¥å‘Šåˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ï¼Œé€šè¿‡è€ƒå¯Ÿå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡ï¼Œä»åŸºç¡€çš„å›¾åƒè¯†åˆ«æ¶æ„åˆ°ç”Ÿæˆæ¨¡å‹ã€è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚æŠ¥å‘Šå¼ºè°ƒäº†æ®‹å·®è¿æ¥ã€Transformeræ¶æ„ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ä»¥åŠè‡ªè’¸é¦æ¡†æ¶ï¼ˆDINOï¼‰å’Œæ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰ç­‰å…³é”®æŠ€æœ¯å’Œæ–¹æ³•çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ¥å‘Šåˆ†æäº†è®¡ç®—æœºè§†è§‰é¢†åŸŸå…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”è¿›ï¼Œä»å›¾åƒè¯†åˆ«çš„åŸºç¡€æ¶æ„å¼€å§‹ã€‚</li>
<li>ä»‹ç»äº†ResNetå¦‚ä½•å¼•å…¥æ®‹å·®è¿æ¥è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå¹¶æœ‰æ•ˆè®­ç»ƒæ›´æ·±çš„å·ç§¯ç½‘ç»œã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰é¦–æ¬¡å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—ï¼Œå±•ç¤ºäº†åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æŠ¥å‘Šæ¢è®¨äº†ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å¯¹æŠ—æ€§è®­ç»ƒè¿‡ç¨‹ï¼Œä»¥åŠæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨ç”Ÿæˆæ–¹æ³•ä¸Šçš„æ”¹è¿›ã€‚</li>
<li>è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯å‡å°‘äº†å¯¹é¢æ ‡ç­¾æ•°æ®çš„ä¾èµ–ï¼Œå…¶ä¸­DINOè‡ªè’¸é¦æ¡†æ¶å’ŒMasked Autoencodersï¼ˆMAEï¼‰æ˜¯ä»£è¡¨æ–¹æ³•ã€‚</li>
<li>LDMsé€šè¿‡æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­çš„è¿ç»­å»å™ªè¿‡ç¨‹æé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a18a3a6abfb6c9ac649e6950b4fd7fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb308890c31345f2aab07dd14fc1774c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c7b7523d1996c598c4c86034ca318b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-498ae1a3c68c14971d462838ba0e1fa5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a42b24e9ff5111ddf2f9f617b925344.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Pixel-Perfect-MegaMed-A-Megapixel-Scale-Vision-Language-Foundation-Model-for-Generating-High-Resolution-Medical-Images"><a href="#Pixel-Perfect-MegaMed-A-Megapixel-Scale-Vision-Language-Foundation-Model-for-Generating-High-Resolution-Medical-Images" class="headerlink" title="Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation   Model for Generating High Resolution Medical Images"></a>Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation   Model for Generating High Resolution Medical Images</h2><p><strong>Authors:Zahra TehraniNasab, Hujun Ni, Amar Kumar, Tal Arbel</strong></p>
<p>Medical image synthesis presents unique challenges due to the inherent complexity and high-resolution details required in clinical contexts. Traditional generative architectures such as Generative Adversarial Networks (GANs) or Variational Auto Encoder (VAEs) have shown great promise for high-resolution image generation but struggle with preserving fine-grained details that are key for accurate diagnosis. To address this issue, we introduce Pixel Perfect MegaMed, the first vision-language foundation model to synthesize images at resolutions of 1024x1024. Our method deploys a multi-scale transformer architecture designed specifically for ultra-high resolution medical image generation, enabling the preservation of both global anatomical context and local image-level details. By leveraging vision-language alignment techniques tailored to medical terminology and imaging modalities, Pixel Perfect MegaMed bridges the gap between textual descriptions and visual representations at unprecedented resolution levels. We apply our model to the CheXpert dataset and demonstrate its ability to generate clinically faithful chest X-rays from text prompts. Beyond visual quality, these high-resolution synthetic images prove valuable for downstream tasks such as classification, showing measurable performance gains when used for data augmentation, particularly in low-data regimes. Our code is accessible through the project website - <a target="_blank" rel="noopener" href="https://tehraninasab.github.io/pixelperfect-megamed">https://tehraninasab.github.io/pixelperfect-megamed</a>. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒåˆæˆé¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºåœ¨ä¸´åºŠç¯å¢ƒä¸­éœ€è¦å›ºæœ‰çš„å¤æ‚æ€§å’Œé«˜åˆ†è¾¨ç‡ç»†èŠ‚ã€‚ä¼ ç»Ÿçš„ç”Ÿæˆæ¶æ„ï¼Œå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æˆ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨ä¿ç•™å¯¹äºå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦çš„ç²¾ç»†ç»†èŠ‚æ–¹é¢å´é‡åˆ°å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Pixel Perfect MegaMedï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»¥è§†è§‰è¯­è¨€ä¸ºåŸºç¡€ã€èƒ½å¤Ÿåœ¨1024x1024åˆ†è¾¨ç‡ä¸‹åˆæˆå›¾åƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸“é—¨è®¾è®¡ç”¨äºè¶…é«˜åˆ†è¾¨ç‡åŒ»å­¦å½±åƒç”Ÿæˆçš„å¤šå°ºåº¦å˜å‹å™¨æ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶ä¿ç•™å…¨å±€è§£å‰–èƒŒæ™¯å’Œå±€éƒ¨å›¾åƒçº§ç»†èŠ‚ã€‚é€šè¿‡åˆ©ç”¨é’ˆå¯¹åŒ»å­¦æœ¯è¯­å’Œæˆåƒæ¨¡å¼çš„è§†è§‰è¯­è¨€å¯¹é½æŠ€æœ¯ï¼ŒPixel Perfect MegaMedèƒ½å¤Ÿåœ¨å‰æ‰€æœªæœ‰çš„é«˜åˆ†è¾¨ç‡æ°´å¹³ä¸Šå¼¥åˆäº†æ–‡æœ¬æè¿°å’Œè§†è§‰è¡¨ç¤ºä¹‹é—´çš„é¸¿æ²Ÿã€‚æˆ‘ä»¬å°†å…¶æ¨¡å‹åº”ç”¨äºCheXpertæ•°æ®é›†ï¼Œå±•ç¤ºäº†æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆä¸´åºŠçœŸå®çš„èƒ¸éƒ¨Xå…‰ç‰‡çš„èƒ½åŠ›ã€‚é™¤äº†è§†è§‰è´¨é‡å¤–ï¼Œè¿™äº›é«˜åˆ†è¾¨ç‡çš„åˆæˆå›¾åƒå¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ï¼‰å…·æœ‰è¯æ˜ä»·å€¼ï¼Œåœ¨æ•°æ®å¢å¼ºæ–¹é¢ä½¿ç”¨æ—¶è¡¨ç°å‡ºå¯è¡¡é‡çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡é¡¹ç›®ç½‘ç«™è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://tehraninasab.github.io/pixelperfect-megamed">https://tehraninasab.github.io/pixelperfect-megamed</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12698v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»ç–—å›¾åƒåˆæˆé¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºä¸´åºŠç¯å¢ƒä¸­éœ€è¦å¤æ‚çš„å›ºæœ‰æ€§å’Œé«˜è§£æåº¦ç»†èŠ‚ã€‚ä¼ ç»Ÿç”Ÿæˆæ¶æ„å¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æˆ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰åœ¨é«˜è§£æåº¦å›¾åƒç”Ÿæˆæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨ä¿æŒç²¾ç»†ç²’åº¦ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™å¯¹äºå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºPixel Perfect MegaMedï¼Œé¦–ä¸ªç”¨äºåˆæˆ1024x1024åˆ†è¾¨ç‡å›¾åƒçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸“ä¸ºè¶…é«˜åˆ†è¾¨ç‡åŒ»ç–—å›¾åƒç”Ÿæˆè®¾è®¡çš„å¤šå°ºåº¦å˜å‹å™¨æ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶ä¿ç•™å…¨å±€è§£å‰–èƒŒæ™¯å’Œå±€éƒ¨å›¾åƒçº§ç»†èŠ‚ã€‚é€šè¿‡åˆ©ç”¨é’ˆå¯¹åŒ»ç–—æœ¯è¯­å’Œæˆåƒæ¨¡å¼çš„è§†è§‰è¯­è¨€å¯¹é½æŠ€æœ¯ï¼ŒPixel Perfect MegaMedåœ¨å‰æ‰€æœªæœ‰çš„é«˜åˆ†è¾¨ç‡æ°´å¹³ä¸Šæ­å»ºäº†æ–‡æœ¬æè¿°å’Œè§†è§‰è¡¨ç¤ºä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬åœ¨CheXpertæ•°æ®é›†ä¸Šåº”ç”¨è¯¥æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†ä»æ–‡æœ¬æç¤ºç”Ÿæˆä¸´åºŠçœŸå®èƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„èƒ½åŠ›ã€‚é™¤äº†è§†è§‰è´¨é‡å¤–ï¼Œè¿™äº›é«˜åˆ†è¾¨ç‡åˆæˆå›¾åƒå¯¹äºä¸‹æ¸¸ä»»åŠ¡å¦‚åˆ†ç±»ä¹Ÿè¯æ˜å…¶ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®å¢å¼ºæ–¹é¢è¡¨ç°å‡ºå¯è¡¡é‡çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨ä½æ•°æ®æƒ…å†µä¸‹æ›´æ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆæˆé¢ä¸´å›ºæœ‰å¤æ‚æ€§å’Œé«˜è§£æåº¦ç»†èŠ‚çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿç”Ÿæˆæ¶æ„å¦‚GANså’ŒVAEsåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸Šè¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†éš¾ä»¥ä¿ç•™ç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>Pixel Perfect MegaMedæ˜¯é¦–ä¸ªç”¨äºè¶…é«˜åˆ†è¾¨ç‡åŒ»ç–—å›¾åƒç”Ÿæˆçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨å¤šå°ºåº¦å˜å‹å™¨æ¶æ„ï¼Œèƒ½åŒæ—¶ä¿ç•™å…¨å±€è§£å‰–èƒŒæ™¯å’Œå±€éƒ¨å›¾åƒçº§ç»†èŠ‚ã€‚</li>
<li>Pixel Perfect MegaMedåˆ©ç”¨é’ˆå¯¹åŒ»ç–—æœ¯è¯­å’Œæˆåƒæ¨¡å¼çš„è§†è§‰è¯­è¨€å¯¹é½æŠ€æœ¯ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½åœ¨å‰æ‰€æœªæœ‰çš„é«˜åˆ†è¾¨ç‡æ°´å¹³ä¸Šç”Ÿæˆä¸´åºŠçœŸå®çš„èƒ¸éƒ¨Xå°„çº¿å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1ae8955fe252a9c571aaa9314e3f353e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57e817aa0dbbcf308bea5b3261f30dff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1a2176d3fa572acc0854816c8a72660.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d012fe2a81642ecec2f15be78edc072.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c42a57577e8cea47e099ec7775fa2046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-330f1d2c544ed40486dfb2bd3aede002.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a52c70c867007936d6d1befa915b616.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LumiNet-Latent-Intrinsics-Meets-Diffusion-Models-for-Indoor-Scene-Relighting"><a href="#LumiNet-Latent-Intrinsics-Meets-Diffusion-Models-for-Indoor-Scene-Relighting" class="headerlink" title="LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene   Relighting"></a>LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene   Relighting</h2><p><strong>Authors:Xiaoyan Xing, Konrad Groh, Sezer Karaoglu, Theo Gevers, Anand Bhattad</strong></p>
<p>We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the targetâ€™s lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the targetâ€™s latent extrinsic properties via cross-attention and fine-tuning.   Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†LumiNetï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å’Œæ½œåœ¨å†…åœ¨è¡¨ç¤ºæ¥è¿›è¡Œæœ‰æ•ˆçš„å…‰ç…§è½¬ç§»ã€‚ç»™å®šæºå›¾åƒå’Œç›®æ ‡å…‰ç…§å›¾åƒï¼ŒLumiNetåˆæˆæºåœºæ™¯çš„é‡ç…§ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬æ•æ‰ç›®æ ‡çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åšå‡ºäº†ä¸¤ä¸ªä¸»è¦è´¡çŒ®ï¼šä¸€æ˜¯åŸºäºStyleGANçš„é‡ç…§æ¨¡å‹çš„æ•°æ®æ•´ç†ç­–ç•¥ï¼Œç”¨äºæˆ‘ä»¬çš„è®­ç»ƒï¼›äºŒæ˜¯å¯¹åŸºäºæ‰©æ•£çš„ControlNetè¿›è¡Œä¿®æ”¹ï¼Œè¯¥ç½‘ç»œå¤„ç†æ¥è‡ªæºå›¾åƒçš„æ½œåœ¨å†…åœ¨å±æ€§å’Œæ¥è‡ªç›®æ ‡å›¾åƒæ½œåœ¨å¤–åœ¨å±æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å­¦åˆ°çš„é€‚é…å™¨ï¼ˆMLPï¼‰æ”¹è¿›å…‰ç…§è½¬ç§»ï¼Œè¯¥é€‚é…å™¨é€šè¿‡è·¨æ³¨æ„åŠ›å’Œå¾®è°ƒæ³¨å…¥ç›®æ ‡æ½œåœ¨å¤–åœ¨å±æ€§ã€‚ä¸ä¼ ç»Ÿçš„ControlNetä¸åŒï¼Œåè€…æ ¹æ®å•ä¸ªåœºæ™¯ç”Ÿæˆå¸¦æœ‰æ¡ä»¶æ˜ å°„çš„å›¾åƒï¼ŒLumiNetå¤„ç†æ¥è‡ªä¸¤ä¸ªä¸åŒå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºâ€”â€”ä¿ç•™æºåœºæ™¯çš„å‡ ä½•å½¢çŠ¶å’Œåå°„ç‡ï¼ŒåŒæ—¶è½¬ç§»ç›®æ ‡åœºæ™¯çš„å…‰ç…§ç‰¹æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åœºæ™¯ç©ºé—´å¸ƒå±€å’Œææ–™å˜åŒ–çš„æƒ…å†µä¸‹æˆåŠŸè½¬ç§»å¤æ‚çš„ç…§æ˜ç°è±¡ï¼ŒåŒ…æ‹¬é«˜å…‰å’Œé—´æ¥ç…§æ˜ï¼Œå¹¶ä¸”åœ¨ä»…ä½¿ç”¨å›¾åƒä½œä¸ºè¾“å…¥çš„æƒ…å†µä¸‹ï¼Œåœ¨å®¤å†…åœºæ™¯çš„å¤æ‚æŒ‘æˆ˜ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00177v3">PDF</a> Corrects an evaluation bug in Table 1 due to a data normalization   error. Thanks to the Sony PlayStation team for discovering and reporting the   issue. The paperâ€™s core contributions, qualitative results, and user study   are unaffected. We also include a minor update to the method to further   improve result quality. Project page: <a target="_blank" rel="noopener" href="https://luminet-relight.github.io/">https://luminet-relight.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>LumiNetæ˜¯ä¸€ä¸ªæ–°å‹æ¶æ„ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å’Œæ½œåœ¨å†…åœ¨è¡¨ç¤ºè¿›è¡Œé«˜æ•ˆçš„å…‰çº¿è½¬ç§»ã€‚ç»™å®šæºå›¾åƒå’Œç›®æ ‡å…‰ç…§å›¾åƒï¼ŒLumiNetåˆæˆæºåœºæ™¯çš„é‡æ–°ç…§æ˜ç‰ˆæœ¬ï¼Œæ•æ‰ç›®æ ‡çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åšå‡ºäº†ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šä¸€æ˜¯ä»StyleGANçš„é‡æ–°ç…§æ˜æ¨¡å‹ä¸­ä¸ºæˆ‘ä»¬çš„è®­ç»ƒè¿›è¡Œæ•°æ®æ•´ç†ç­–ç•¥ï¼ŒäºŒæ˜¯æ”¹è¿›åŸºäºæ‰©æ•£çš„ControlNetï¼Œå¤„ç†æ¥è‡ªæºå›¾åƒçš„æ½œåœ¨å†…åœ¨å±æ€§å’Œæ¥è‡ªç›®æ ‡å›¾åƒçš„å¤–åœ¨æ½œåœ¨å±æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å­¦ä¹ çš„é€‚é…å™¨å’Œäº¤å‰æ³¨æ„åŠ›åŠå¾®è°ƒæŠ€æœ¯æ”¹è¿›å…‰çº¿è½¬ç§»ã€‚ä¸åŒäºä¼ ç»Ÿçš„ControlNetï¼ŒLumiNetå¤„ç†æ¥è‡ªä¸¤ä¸ªä¸åŒå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œä¿ç•™æºåœºæ™¯çš„å‡ ä½•å’Œé¢œè‰²ä¿¡æ¯ï¼ŒåŒæ—¶è½¬ç§»ç›®æ ‡åœºæ™¯çš„å…‰ç…§ç‰¹æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚çš„ç…§æ˜ç°è±¡è½¬ç§»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬é«˜å…‰å’Œé—´æ¥ç…§æ˜ï¼Œé€‚ç”¨äºä¸åŒç©ºé—´å¸ƒå±€å’Œææ–™çš„ä¸åŒå®¤å†…åœºæ™¯ï¼Œä»…ä½¿ç”¨å›¾åƒä½œä¸ºè¾“å…¥å°±è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LumiNetæ˜¯ä¸€ä¸ªæ–°å‹æ¶æ„ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å’Œæ½œåœ¨è¡¨ç¤ºè¿›è¡Œå…‰çº¿è½¬ç§»ã€‚</li>
<li>LumiNetå¯ä»¥ä»æºå›¾åƒè·å–æ½œåœ¨å†…åœ¨å±æ€§ï¼Œå¹¶ä»ç›®æ ‡å›¾åƒè·å–æ½œåœ¨å¤–åœ¨å±æ€§ã€‚</li>
<li>é€šè¿‡æ•°æ®æ•´ç†ç­–ç•¥ã€æ”¹è¿›çš„ControlNetå’Œå­¦ä¹ çš„é€‚é…å™¨æ”¹è¿›å…‰çº¿è½¬ç§»ã€‚</li>
<li>LumiNeté€šè¿‡äº¤å‰æ³¨æ„åŠ›å’Œå¾®è°ƒæŠ€æœ¯å®ç°ç›®æ ‡çš„å…‰ç…§ç‰¹æ€§è½¬ç§»ã€‚</li>
<li>LumiNetä¸åŒäºä¼ ç»Ÿçš„ControlNetï¼Œå› ä¸ºå®ƒå¤„ç†æ¥è‡ªä¸¤ä¸ªä¸åŒå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>LumiNetä¿ç•™äº†æºåœºæ™¯çš„å‡ ä½•å’Œé¢œè‰²ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ddfa93d603285b161310151d6eb7062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3149a0b30159a04a8702353c22367c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a0137817fc38ca27ba7c1dbc0c5dee7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9690f7f6d5f03951499d130dff2a8437.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Conditional-Wasserstein-Distances-with-Applications-in-Bayesian-OT-Flow-Matching"><a href="#Conditional-Wasserstein-Distances-with-Applications-in-Bayesian-OT-Flow-Matching" class="headerlink" title="Conditional Wasserstein Distances with Applications in Bayesian OT Flow   Matching"></a>Conditional Wasserstein Distances with Applications in Bayesian OT Flow   Matching</h2><p><strong>Authors:Jannis Chemseddine, Paul Hagemann, Gabriele Steidl, Christian Wald</strong></p>
<p>In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullbackâ€“Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein distance. Based on this, we propose an extension of OT Flow Matching for solving Bayesian inverse problems and demonstrate its numerical advantages on an inverse problem and class-conditional image generation. </p>
<blockquote>
<p>åœ¨é€†å‘é—®é¢˜ä¸­ï¼Œè®¸å¤šæ¡ä»¶ç”Ÿæˆæ¨¡å‹é€šè¿‡æœ€å°åŒ–è”åˆåˆ†å¸ƒä¸å…¶å­¦ä¹ è¿‘ä¼¼åˆ†å¸ƒä¹‹é—´çš„è·ç¦»æ¥è¿‘ä¼¼åéªŒåˆ†å¸ƒã€‚è™½ç„¶è¿™ç§æ–¹æ³•åœ¨Kullback-Leibleræ•£åº¦çš„æƒ…å†µä¸‹ä¹Ÿæ§åˆ¶äº†åéªŒåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œä½†å¯¹äºWassersteinè·ç¦»æ¥è¯´ï¼Œé€šå¸¸å¹¶ä¸é€‚ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç»„é™åˆ¶è€¦åˆå¼•å…¥æ¡ä»¶Wassersteinè·ç¦»ï¼Œè¯¥è·ç¦»ç­‰äºåéªŒçš„æœŸæœ›Wassersteinè·ç¦»ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæ¡ä»¶Wasserstein-1æµçš„åŒé‡å½¢å¼ä¸æ¡ä»¶Wasserstein GANæ–‡çŒ®ä¸­çš„æŸå¤±éå¸¸è‡ªç„¶åœ°ç›¸ä¼¼ã€‚æˆ‘ä»¬æ¨å¯¼å‡ºæ¡ä»¶Wassersteinè·ç¦»çš„ç†è®ºæ€§è´¨ï¼Œå¹¶æè¿°äº†ç›¸åº”çš„æµ‹åœ°çº¿ã€é€Ÿåº¦åœºä»¥åŠæµODEsçš„ç‰¹æ€§ã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡æ”¾æ¾æ¡ä»¶Wassersteinè·ç¦»æ¥è¿‘ä¼¼é€Ÿåº¦åœºçš„æ–¹æ³•ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©å±•çš„OTæµåŒ¹é…æ¥è§£å†³è´å¶æ–¯é€†å‘é—®é¢˜ï¼Œå¹¶åœ¨ä¸€ä¸ªé€†å‘é—®é¢˜å’Œç±»åˆ«æ¡ä»¶å›¾åƒç”Ÿæˆä¸Šå±•ç¤ºäº†å…¶æ•°å€¼ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.18705v3">PDF</a> This paper supersedes arXiv:2310.13433, accepted at JMLR</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ¡ä»¶Wassersteinè·ç¦»çš„é€†é—®é¢˜è§£å†³æ–¹æ¡ˆã€‚æ–‡ç« å¼•å…¥äº†é€šè¿‡é™åˆ¶è€¦åˆé›†å®šä¹‰çš„æ¡ä»¶Wassersteinè·ç¦»ï¼Œè¯¥è·ç¦»ç­‰äºåéªŒåˆ†å¸ƒçš„æœŸæœ›Wassersteinè·ç¦»ã€‚æ­¤å¤–ï¼Œæ–‡ç« æ¢è®¨äº†æ¡ä»¶Wasserstein-1æµçš„åŒé‡è¡¨è¿°ï¼Œå®ƒä¸æ¡ä»¶Wasserstein GANæ–‡çŒ®ä¸­çš„æŸå¤±å‡½æ•°æœ‰è‡ªç„¶è”ç³»ã€‚æ–‡ç« è¿˜æ¨å¯¼äº†æ¡ä»¶Wassersteinè·ç¦»çš„ç†è®ºå±æ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶è§£å†³è´å¶æ–¯é€†é—®é¢˜çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¡ä»¶ç”Ÿæˆæ¨¡å‹é€šè¿‡æœ€å°åŒ–è”åˆåˆ†å¸ƒä¸å…¶å­¦ä¹ è¿‘ä¼¼ä¹‹é—´çš„è·ç¦»æ¥é€¼è¿‘åéªŒåˆ†å¸ƒã€‚</li>
<li>Wassersteinè·ç¦»ä¸‹çš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹å¹¶ä¸ç­‰åŒäºåœ¨Kullback-Leibleræ•£åº¦ä¸‹çš„æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥äº†æ¡ä»¶Wassersteinè·ç¦»ï¼Œè¯¥è·ç¦»æ˜¯é€šè¿‡ä¸€ç»„é™åˆ¶è€¦åˆå®šä¹‰çš„ï¼Œç­‰äºåéªŒçš„æœŸæœ›Wassersteinè·ç¦»ã€‚</li>
<li>æ¡ä»¶Wasserstein-1æµçš„åŒé‡è¡¨è¿°ä¸æ¡ä»¶Wasserstein GANæ–‡çŒ®ä¸­çš„æŸå¤±å‡½æ•°æœ‰è‡ªç„¶è”ç³»ã€‚</li>
<li>æ–‡ç« æ¨å¯¼äº†æ¡ä»¶Wassersteinè·ç¦»çš„ç†è®ºå±æ€§ï¼ŒåŒ…æ‹¬å¯¹åº”çš„æµ‹åœ°çº¿ã€é€Ÿåº¦åœºå’ŒæµåŠ¨å¸¸å¾®åˆ†æ–¹ç¨‹ã€‚</li>
<li>æå‡ºé€šè¿‡æ”¾æ¾æ¡ä»¶Wassersteinè·ç¦»æ¥è¿‘ä¼¼é€Ÿåº¦åœºçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.18705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2f005d4e2b8b574b449e21711994e8c2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cf62327d584f9334a71420e5574fb1e5.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Hyper Diffusion Avatars Dynamic Human Avatar Generation using Network   Weight Space Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6a0bdfdf63cac948d1a54a7ecf6398fa.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
