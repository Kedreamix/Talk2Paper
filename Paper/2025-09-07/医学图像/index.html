<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  A Primer on Causal and Statistical Dataset Biases for Fair and Robust   Image Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7314b58fae444ebe507db6dc94c2adc1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="A-Primer-on-Causal-and-Statistical-Dataset-Biases-for-Fair-and-Robust-Image-Analysis"><a href="#A-Primer-on-Causal-and-Statistical-Dataset-Biases-for-Fair-and-Robust-Image-Analysis" class="headerlink" title="A Primer on Causal and Statistical Dataset Biases for Fair and Robust   Image Analysis"></a>A Primer on Causal and Statistical Dataset Biases for Fair and Robust   Image Analysis</h2><p><strong>Authors:Charles Jones, Ben Glocker</strong></p>
<p>Machine learning methods often fail when deployed in the real world. Worse still, they fail in high-stakes situations and across socially sensitive lines. These issues have a chilling effect on the adoption of machine learning methods in settings such as medical diagnosis, where they are arguably best-placed to provide benefits if safely deployed. In this primer, we introduce the causal and statistical structures which induce failure in machine learning methods for image analysis. We highlight two previously overlooked problems, which we call the \textit{no fair lunch} problem and the \textit{subgroup separability} problem. We elucidate why todayâ€™s fair representation learning methods fail to adequately solve them and propose potential paths forward for the field. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ åœ¨å®é™…éƒ¨ç½²æ—¶ç»å¸¸å‡ºç°å¤±æ•ˆçš„æƒ…å†µã€‚æ›´ä¸ºä¸¥é‡çš„æ˜¯ï¼Œå®ƒä»¬åœ¨å…³é”®æ—¶åˆ»å’Œæ¶‰åŠç¤¾ä¼šæ•æ„Ÿé—®é¢˜çš„æƒ…å¢ƒä¸­ä¹Ÿä¼šå‡ºç°å¤±è¯¯ã€‚è¿™äº›é—®é¢˜ä¸¥é‡å½±å“äº†æœºå™¨å­¦ä¹ åœ¨åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸçš„åº”ç”¨æ¨å¹¿ï¼Œåœ¨é‚£äº›å¦‚æœèƒ½å®‰å…¨éƒ¨ç½²çš„è¯ï¼Œæœºå™¨å­¦ä¹ å¯å‘æŒ¥å‡ºæœ€å¥½çš„ä¼˜åŠ¿ã€‚åœ¨è¿™ç¯‡åŸºç¡€æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¯¼è‡´å›¾åƒåˆ†ææœºå™¨å­¦ä¹ æ–¹æ³•å¤±è´¥çš„å› æœå’Œç»Ÿè®¡ç»“æ„ã€‚æˆ‘ä»¬é‡ç‚¹ä»‹ç»äº†ä¸¤ä¸ªä¹‹å‰è¢«å¿½è§†çš„é—®é¢˜ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ²¡æœ‰å…è´¹åˆé¤â€é—®é¢˜å’Œâ€œå­ç»„å¯åˆ†æ€§â€é—®é¢˜ã€‚æˆ‘ä»¬é˜è¿°äº†ç›®å‰çš„å…¬å¹³è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ä¸ºä½•æ— æ³•å……åˆ†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œå¹¶æå‡ºäº†æœªæ¥è§£å†³è¯¥é¢†åŸŸé—®é¢˜çš„æ½œåœ¨é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04295v1">PDF</a> Excerpt from C. Jonesâ€™ PhD thesis. Winner of the G-Research PhD prize   2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœºå™¨å­¦ä¹ åœ¨å›¾åƒåˆ†æä¸­çš„å¤±è´¥åŸå› ï¼Œå¼ºè°ƒäº†å½“å‰å…¬å¹³è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ— æ³•è§£å†³çš„ä¸¤ä¸ªé—®é¢˜ï¼šâ€œæ— å…¬å¹³åˆé¤â€é—®é¢˜å’Œâ€œå­ç»„å¯åˆ†æ€§â€é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºè¿™äº›é—®é¢˜åœ¨é«˜é£é™©å’Œç¤¾ä¼šæ•æ„Ÿé¢†åŸŸå°¤ä¸ºçªå‡ºï¼Œå¹¶å»ºè®®æœªæ¥è§£å†³æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨å®é™…éƒ¨ç½²ä¸­ç»å¸¸å¤±è´¥ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©å’Œç¤¾ä¼šæ•æ„Ÿé¢†åŸŸã€‚</li>
<li>åœ¨åŒ»ç–—è¯Šæ–­ç­‰åœºæ™¯ä¸­ï¼Œæœºå™¨å­¦ä¹ æ–¹æ³•çš„å¤±è´¥å¯¹å…¶åº”ç”¨äº§ç”Ÿäº†è´Ÿé¢å½±å“ã€‚</li>
<li>ä»‹ç»äº†å¯¼è‡´æœºå™¨å­¦ä¹ åœ¨å›¾åƒåˆ†æå¤±è´¥çš„åŸå› ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªè¢«å¿½è§†çš„é—®é¢˜ï¼šâ€œæ— å…¬å¹³åˆé¤â€å’Œâ€œå­ç»„å¯åˆ†æ€§â€ã€‚</li>
<li>ç°æœ‰çš„å…¬å¹³è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ— æ³•è§£å†³è¿™ä¸¤ä¸ªæ–°é—®é¢˜ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºäº†è¿™ä¸¤ä¸ªé—®é¢˜çš„ä¸¥é‡æ€§ï¼Œå¹¶å¼ºè°ƒäº†è§£å†³å®ƒä»¬çš„å¿…è¦æ€§ã€‚</li>
<li>æ–‡ç« ä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸçš„æœªæ¥å‘å±•æå‡ºäº†æ½œåœ¨çš„æ–¹å‘å’Œå»ºè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76d79e1c20a4bdc8f0e5319e1cb5ed24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83450006803913b523e7f3afc75af0f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5f410f393626f6bf8ecd8476d27ac48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-872f4a985c1cba0d7dc3582661d02720.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a04a3a7155314da774d9583d9a4dc07.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dual-Scale-Volume-Priors-with-Wasserstein-Based-Consistency-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#Dual-Scale-Volume-Priors-with-Wasserstein-Based-Consistency-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Dual-Scale Volume Priors with Wasserstein-Based Consistency for   Semi-Supervised Medical Image Segmentation"></a>Dual-Scale Volume Priors with Wasserstein-Based Consistency for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Junying Meng, Gangxuan Zhou, Jun Liu, Weihong Guo</strong></p>
<p>Despite signi cant progress in semi-supervised medical image segmentation, most existing segmentation networks overlook e ective methodological guidance for feature extraction and important prior information from   datasets. In this paper, we develop a semi-supervised medical image segmentation framework that e ectively integrates spatial regularization methods and volume priors. Speci cally, our approach integrates a strong explicit volume prior at the image scale and Threshold Dynamics spatial regularization, both derived from variational models, into the backbone segmentation network. The target region volumes for each unlabeled image are estimated by a regression network, which e ectively regularizes the backbone segmentation network through an image-scale Wasserstein distance constraint, ensuring that the class ratios in the segmentation results for each unlabeled image match those predicted by the regression network. Additionally, we design a dataset-scale Wasserstein distance loss function based on a weak implicit volume prior, which enforces that the volume distribution predicted for the unlabeled dataset is similar to that of labeled dataset. Experimental results on the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset show the superiority of the proposed method. </p>
<blockquote>
<p>å°½ç®¡åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¤§å¤šæ•°ç°æœ‰çš„åˆ†å‰²ç½‘ç»œå¿½è§†äº†ç‰¹å¾æå–çš„æœ‰æ•ˆæ–¹æ³•æŒ‡å¯¼ä»¥åŠæ•°æ®é›†ä¸­é‡è¦çš„å…ˆéªŒä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°é›†æˆäº†ç©ºé—´æ­£åˆ™åŒ–æ–¹æ³•å’Œä½“ç§¯å…ˆéªŒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å›¾åƒå°ºåº¦çš„å¼ºæ˜¾å¼ä½“ç§¯å…ˆéªŒå’Œé˜ˆå€¼åŠ¨æ€ç©ºé—´æ­£åˆ™åŒ–ï¼ˆä¸¤è€…å‡æ¥è‡ªå˜åˆ†æ¨¡å‹ï¼‰é›†æˆåˆ°ä¸»å¹²åˆ†å‰²ç½‘ç»œä¸­ã€‚å›å½’ç½‘ç»œä¼°è®¡æ¯å¼ æœªæ ‡è®°å›¾åƒçš„ç›®æ ‡åŒºåŸŸä½“ç§¯ï¼Œé€šè¿‡å›¾åƒå°ºåº¦çš„Wassersteinè·ç¦»çº¦æŸæœ‰æ•ˆåœ°å¯¹ä¸»å¹²åˆ†å‰²ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–ï¼Œç¡®ä¿æ¯å¼ æœªæ ‡è®°å›¾åƒçš„åˆ†å‰²ç»“æœä¸­çš„ç±»åˆ«æ¯”ä¾‹ä¸å›å½’ç½‘ç»œçš„é¢„æµ‹ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºå¼±éšå¼ä½“ç§¯å…ˆéªŒè®¾è®¡äº†ä¸€ä¸ªæ•°æ®é›†å°ºåº¦çš„Wassersteinè·ç¦»æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°å¼ºåˆ¶æœªæ ‡è®°æ•°æ®é›†çš„ä½“ç§¯åˆ†å¸ƒä¸æ ‡è®°æ•°æ®é›†çš„ä½“ç§¯åˆ†å¸ƒç›¸ä¼¼ã€‚åœ¨2017å¹´ACDCæ•°æ®é›†ã€PROMISE12æ•°æ®é›†å’Œå¤§è…¿è‚Œè‚‰MRå›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04273v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸç°æœ‰ç½‘ç»œå¿½ç•¥æœ‰æ•ˆçš„ç‰¹å¾æå–å’Œå…ˆéªŒä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œæ•´åˆç©ºé—´æ­£åˆ™åŒ–æ–¹æ³•å’Œä½“ç§¯å…ˆéªŒã€‚é€šè¿‡å›å½’ç½‘ç»œä¼°è®¡æœªæ ‡è®°å›¾åƒçš„ç›®æ ‡åŒºåŸŸä½“ç§¯ï¼Œé€šè¿‡å›¾åƒå°ºåº¦çš„Wassersteinè·ç¦»çº¦æŸæœ‰æ•ˆæ­£åˆ™åŒ–åˆ†å‰²ç½‘ç»œã€‚åŒæ—¶è®¾è®¡åŸºäºå¼±éšå¼ä½“ç§¯å…ˆéªŒçš„æ•°æ®é›†å°ºåº¦Wassersteinè·ç¦»æŸå¤±å‡½æ•°ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œå¿½ç•¥ç‰¹å¾æå–å’Œå…ˆéªŒä¿¡æ¯çš„é‡è¦æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œæ•´åˆç©ºé—´æ­£åˆ™åŒ–æ–¹æ³•å’Œä½“ç§¯å…ˆéªŒã€‚</li>
<li>é€šè¿‡å›å½’ç½‘ç»œä¼°è®¡æœªæ ‡è®°å›¾åƒçš„ç›®æ ‡åŒºåŸŸä½“ç§¯ã€‚</li>
<li>é€šè¿‡å›¾åƒå°ºåº¦çš„Wassersteinè·ç¦»çº¦æŸæ­£åˆ™åŒ–åˆ†å‰²ç½‘ç»œã€‚</li>
<li>è®¾è®¡åŸºäºå¼±éšå¼ä½“ç§¯å…ˆéªŒçš„æ•°æ®é›†å°ºåº¦Wassersteinè·ç¦»æŸå¤±å‡½æ•°ã€‚</li>
<li>æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9ea14450e68f9f45e52035c6cc03725f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Error-Detection-Schemes-for-Barrett-Reduction-of-CT-BU-on-FPGA-in-Post-Quantum-Cryptography"><a href="#Error-Detection-Schemes-for-Barrett-Reduction-of-CT-BU-on-FPGA-in-Post-Quantum-Cryptography" class="headerlink" title="Error Detection Schemes for Barrett Reduction of CT-BU on FPGA in Post   Quantum Cryptography"></a>Error Detection Schemes for Barrett Reduction of CT-BU on FPGA in Post   Quantum Cryptography</h2><p><strong>Authors:Paresh Baidya, Rourab Paul, Vikas Srivastava, Sumit Kumar Debnath</strong></p>
<p>A fault can occur naturally or intentionally. However, intentionally injecting faults into hardware accelerators of Post-Quantum Cryptographic (PQC) algorithms may leak sensitive information. This intentional fault injection in side-channel attacks compromises the reliability of PQC implementations. The recently NIST-standardized key encapsulation mechanism (KEM), Kyber may also leak information at the hardware implementation level. This work proposes three efficient and lightweight recomputation-based fault detection methods for Barrett Reduction in the Cooley-Tukey Butterfly Unit (CT-BU) of Kyber on a Field Programmable Gate Array (FPGA). The CT-BU and Barrett Reduction are fundamental components in structured lattice-based PQC algorithms, including Kyber, NTRU, Falcon, CRYSTALS-Dilithium, etc. This paper introduces a new algorithm, Recomputation with Swapped Operand (RESWO), for fault detection. While Recomputation with Negated Operand (RENO) and Recomputation with Shifted Operand (RESO) are existing methods used in other PQC hardware algorithms. To the best of our knowledge, RENO and RESO have never been used in Barrett Reduction before. The proposed RESWO method consumes a similar number of slices compared to RENO and RESO. However, RESWO shows lesser delay compared to both RENO and RESO. The fault detection efficiency of RESWO, RENO, and RESO is nearly 100%. </p>
<blockquote>
<p>æ•…éšœå¯èƒ½è‡ªç„¶å‘ç”Ÿæˆ–äººä¸ºé€ æˆã€‚ç„¶è€Œï¼Œæ•…æ„å‘åé‡å­å¯†ç ï¼ˆPQCï¼‰ç®—æ³•çš„ç¡¬ä»¶åŠ é€Ÿå™¨æ³¨å…¥æ•…éšœå¯èƒ½ä¼šæ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚è¿™ç§åœ¨ä¾§ä¿¡é“æ”»å‡»ä¸­çš„æ•…æ„æ•…éšœæ³¨å…¥ä¼šæŸå®³PQCå®ç°çš„å¯é æ€§ã€‚æœ€è¿‘è¢«NISTæ ‡å‡†åŒ–çš„å¯†é’¥å°è£…æœºåˆ¶ï¼ˆKEMï¼‰Kyberåœ¨ç¡¬ä»¶å®ç°å±‚é¢ä¹Ÿå¯èƒ½æ³„éœ²ä¿¡æ¯ã€‚é’ˆå¯¹Kyberçš„Cooley-Tukeyè¶å½¢è¿ç®—å•å…ƒï¼ˆCT-BUï¼‰ä¸­çš„Barrettè§„çº¦ï¼Œæœ¬æ–‡æå‡ºäº†ä¸‰ç§é«˜æ•ˆä¸”è½»é‡çº§çš„åŸºäºé‡æ–°è®¡ç®—çš„æ•…éšœæ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ£€æµ‹æ‰‹æ®µå¯åœ¨ç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼ˆFPGAï¼‰ä¸Šå®æ–½ã€‚CT-BUå’ŒBarrettè§„çº¦æ˜¯åŸºäºç»“æ„æ ¼çš„åé‡å­å¯†ç ç®—æ³•ï¼ˆåŒ…æ‹¬Kyberã€NTRUã€Falconã€CRYSTALS-Dilithiumç­‰ï¼‰çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç”¨äºæ•…éšœæ£€æµ‹çš„ç®—æ³•â€”â€”äº¤æ¢æ“ä½œæ•°é‡æ–°è®¡ç®—ï¼ˆRESWOï¼‰ã€‚è€Œå¦å®šæ“ä½œæ•°é‡æ–°è®¡ç®—ï¼ˆRENOï¼‰å’Œç§»ä½æ“ä½œæ•°é‡æ–°è®¡ç®—ï¼ˆRESOï¼‰æ˜¯å·²åº”ç”¨äºå…¶ä»–PQCç¡¬ä»¶ç®—æ³•çš„æ–¹æ³•ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œåœ¨Barrettè§„çº¦ä¸­ä»æœªä½¿ç”¨è¿‡RENOå’ŒRESOã€‚æ‰€æå‡ºçš„RESWOæ–¹æ³•ä¸RENOå’ŒRESOç›¸æ¯”ï¼Œæ¶ˆè€—ç›¸ä¼¼çš„åˆ‡ç‰‡æ•°é‡ã€‚ä½†RESWOç›¸å¯¹äºRENOå’ŒRESOè¡¨ç°å‡ºæ›´çŸ­çš„å»¶è¿Ÿæ—¶é—´ã€‚RESWOã€RENOå’ŒRESOçš„æ•…éšœæ£€æµ‹æ•ˆç‡æ¥è¿‘100%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04070v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨åŠ å¯†ç³»ç»Ÿä¸­å­˜åœ¨å¤©ç„¶æˆ–äººä¸ºæ³¨å…¥çš„é”™è¯¯ã€‚äººä¸ºå°†é”™è¯¯æ³¨å…¥åˆ°é‡å­å¯†ç ç®—æ³•çš„åé‡å­å¯†ç å­¦ç¡¬ä»¶åŠ é€Ÿå™¨ä¸­å¯èƒ½ä¼šæ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚è¿™äº›ä¾§ä¿¡é“æ”»å‡»ä¸­çš„æœ‰æ„é”™è¯¯æ³¨å…¥ç ´åäº†åé‡å­å¯†ç å­¦å®ç°çš„å¯é æ€§ã€‚æœ€è¿‘æ ‡å‡†åŒ–çš„å¯†é’¥å°è£…æœºåˆ¶Kyberåœ¨ç¡¬ä»¶å®ç°å±‚é¢ä¹Ÿå¯èƒ½æ³„éœ²ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸‰ç§é«˜æ•ˆè½»é‡çº§çš„åŸºäºé‡æ„çš„é”™è¯¯æ£€æµ‹æ–¹æ³•ï¼Œç”¨äºKyberä¸­çš„åº“ä¼Š-å›¾åŸºè´è¶å•ä½ä¸­çš„å·´é›·ç‰¹è§„çº¦ç®—æ³•ï¼ˆFPGAï¼‰ã€‚è¯¥å•ä½å’Œå·´é›·ç‰¹è§„çº¦ç®—æ³•æ˜¯åŸºäºç»“æ„åŒ–ç½‘æ ¼çš„åé‡å­å¯†ç ç®—æ³•çš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬Kyberã€NTRUã€Falconã€CRYSTALS-Dilithiumç­‰ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„é”™è¯¯æ£€æµ‹ç®—æ³•Recomputation with Swapped Operand (RESWO)ã€‚è™½ç„¶ç°æœ‰çš„æ–¹æ³•åŒ…æ‹¬Recomputation with Negated Operand (RENO)å’ŒRecomputation with Shifted Operand (RESO)ï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒä»¬ä»¥å‰ä»æœªåœ¨å·´é›·ç‰¹è§„çº¦ä¸­ä½¿ç”¨è¿‡ã€‚æ–°çš„RESWOæ–¹æ³•æ¶ˆè€—çš„åˆ‡ç‰‡æ•°é‡ä¸RENOå’ŒRESOç›¸ä¼¼ï¼Œä½†å»¶è¿Ÿæ›´å°‘ã€‚è¿™ä¸‰ç§æ–¹æ³•çš„é”™è¯¯æ£€æµ‹æ•ˆç‡å‡ ä¹è¾¾åˆ°äº†ç™¾åˆ†ä¹‹ç™¾ã€‚é‡ç‚¹åœ¨äºç®€åŒ–è¿™äº›æœºåˆ¶å¹¶æé«˜æ•ˆç‡å’Œè¿è¡Œé€Ÿåº¦ï¼Œä»è€Œå¢å¼ºå…¶åœ¨å®é™…ç¡¬ä»¶éƒ¨ç½²ä¸­çš„å®ç”¨æ€§ã€‚è¿™åœ¨è®¾è®¡å’Œå®ç°å¯é çš„åé‡å­å¯†ç å­¦ç¡¬ä»¶è§£å†³æ–¹æ¡ˆä¸­è‡³å…³é‡è¦ã€‚å¯¹äºä¿éšœé‡å­å¯†ç å­¦å®‰å…¨æ€§å’Œå®Œæ•´æ€§å…·æœ‰é‡å¤§æ„ä¹‰ã€‚ </p>
<p><strong>å…³é”®è¦ç‚¹</strong>ï¼š</p>
<ul>
<li>æœ‰æ„å°†é”™è¯¯æ³¨å…¥åˆ°é‡å­å¯†ç ç¡¬ä»¶åŠ é€Ÿå™¨ä¸­ä¼šå¼•å‘æ•æ„Ÿä¿¡æ¯çš„æ³„éœ²é—®é¢˜ï¼Œå½±å“åˆ°åé‡å­å¯†ç ç®—æ³•çš„å¯é æ€§ã€‚å¯¹äºåŠ å¯†é¢†åŸŸæ„æˆäº†é‡å¤§çš„æ½œåœ¨å¨èƒå’Œé£é™©æŒ‘æˆ˜ã€‚åº”å¯¹ç›¸å…³è®¾è®¡ç¯èŠ‚å’Œåç»­å®Œå–„æ”¹è¿›æªæ–½å®æ–½æå‡ºäº†éœ€æ±‚å’ŒæŒ‘æˆ˜åº”å¯¹çš„å®¢è§‚å¿…è¦ã€‚å…·å¤‡æ¨è¿›å…·ä½“å®‰å…¨ä¿éšœæœºåˆ¶ç ”ç©¶çš„é‡è¦æ„ä¹‰ã€‚ </li>
<li>Kyberå¯†é’¥å°è£…æœºåˆ¶åœ¨ç¡¬ä»¶å®ç°å±‚é¢ä¹Ÿå­˜åœ¨ä¿¡æ¯æ³„éœ²é£é™©ï¼ŒäºŸéœ€è§£å†³ç›¸å…³æ¼æ´é—®é¢˜ä»¥ä¿éšœæ•°æ®å®‰å…¨æ€§å’Œå¯é æ€§ã€‚ </li>
<li>é’ˆå¯¹åº“ä¼Š-å›¾åŸºè´è¶å•ä½ä¸­çš„å·´é›·ç‰¹è§„çº¦ç®—æ³•æå‡ºäº†ä¸‰ç§æ–°çš„é”™è¯¯æ£€æµ‹æ–¹æ³•ï¼ˆRESWOã€RENOå’ŒRESOï¼‰ï¼Œå…·å¤‡é«˜æ•ˆä¸”è½»é‡çº§çš„ç‰¹ç‚¹ï¼Œå¯¹æ•…éšœçš„æ£€æµ‹ç‡è¿‘ç™¾åˆ†ä¹‹ç™¾å‡†ç¡®ç‡é«˜èƒ½è¿›ä¸€æ­¥æå‡å·´é›·ç‰¹å½’çº¦çš„æ€§èƒ½å¯é æ€§é˜²æ­¢å› è¯¯å¼•å‘çš„æ•…éšœå±å®³å½±å“åˆ°å…¶ä»–é‡è¦çš„PQCç®—æ³•çš„è½åœ°ä½¿ç”¨å¥ å®šäº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚ </li>
<li>ä¸Šè¿°ä¸‰ç§æ–¹æ³•å…·å¤‡ç›¸ä¼¼çš„è®¡ç®—èµ„æºæ¶ˆè€—æ°´å¹³ï¼Œä½†RESWOåœ¨å»¶è¿Ÿæ–¹é¢è¡¨ç°æ›´ä¼˜ï¼Œè¡¨æ˜å…¶åœ¨å®æ—¶ç³»ç»Ÿä¸­çš„åº”ç”¨æ½œåŠ›æ›´å¤§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-22155524112e3cf05f4d9a5814bc7b39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f9849cb7e1deaa696b47c4a1de7b3ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0033f48227f44834a1078a5379595f3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b920e01aebc825ac6ebc54e0e1ccaf40.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning"><a href="#Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning" class="headerlink" title="Learning from Majority Label: A Novel Problem in Multi-class   Multiple-Instance Learning"></a>Learning from Majority Label: A Novel Problem in Multi-class   Multiple-Instance Learning</h2><p><strong>Authors:Shiku Kaito, Shinnosuke Matsuo, Daiki Suehiro, Ryoma Bise</strong></p>
<p>The paper proposes a novel multi-class Multiple-Instance Learning (MIL) problem called Learning from Majority Label (LML). In LML, the majority class of instances in a bag is assigned as the bag-level label. The goal of LML is to train a classification model that estimates the class of each instance using the majority label. This problem is valuable in a variety of applications, including pathology image segmentation, political voting prediction, customer sentiment analysis, and environmental monitoring. To solve LML, we propose a Counting Network trained to produce bag-level majority labels, estimated by counting the number of instances in each class. Furthermore, analysis experiments on the characteristics of LML revealed that bags with a high proportion of the majority class facilitate learning. Based on this result, we developed a Majority Proportion Enhancement Module (MPEM) that increases the proportion of the majority class by removing minority class instances within the bags. Experiments demonstrate the superiority of the proposed method on four datasets compared to conventional MIL methods. Moreover, ablation studies confirmed the effectiveness of each module. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning%7D%7Bhere%7D">https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šç±»å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰é—®é¢˜ï¼Œç§°ä¸ºä»å¤šæ•°æ ‡ç­¾å­¦ä¹ ï¼ˆLMLï¼‰ã€‚åœ¨LMLä¸­ï¼Œå°†ä¸€ä¸ªåŒ…ä¸­çš„å¤šæ•°å®ä¾‹ç±»åˆ†é…ä¸ºåŒ…çº§æ ‡ç­¾ã€‚LMLçš„ç›®æ ‡æ˜¯åˆ©ç”¨å¤šæ•°æ ‡ç­¾è®­ç»ƒä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œä»¥ä¼°è®¡æ¯ä¸ªå®ä¾‹çš„ç±»åˆ«ã€‚è¯¥é—®é¢˜åœ¨å„ç§åº”ç”¨ä¸­éƒ½å…·æœ‰ä»·å€¼ï¼ŒåŒ…æ‹¬ç—…ç†å­¦å›¾åƒåˆ†å‰²ã€æ”¿æ²»æŠ•ç¥¨é¢„æµ‹ã€å®¢æˆ·æƒ…æ„Ÿåˆ†æå’Œç¯å¢ƒç›‘æµ‹ã€‚ä¸ºäº†è§£å†³LMLé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è®¡æ•°ç½‘ç»œï¼Œè¯¥ç½‘ç»œç»è¿‡è®­ç»ƒä»¥è®¡ç®—æ¯ä¸ªç±»çš„å®ä¾‹æ•°é‡æ¥äº§ç”ŸåŒ…çº§å¤šæ•°æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œå¯¹LMLç‰¹æ€§çš„åˆ†æå®éªŒè¡¨æ˜ï¼ŒåŒ…å«å¤šæ•°ç±»å®ä¾‹æ¯”ä¾‹è¾ƒé«˜çš„è¢‹å­æœ‰åŠ©äºå­¦ä¹ ã€‚åŸºäºè¿™ä¸€ç»“æœï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ•°æ¯”ä¾‹å¢å¼ºæ¨¡å—ï¼ˆMPEMï¼‰ï¼Œé€šè¿‡ç§»é™¤è¢‹å­å†…çš„å°‘æ•°ç±»å®ä¾‹æ¥å¢åŠ å¤šæ•°ç±»çš„æ¯”ä¾‹ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å¸¸è§„MILæ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¯å®äº†æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/Shiku-Kaito/Learning-from-Majority-label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning">https://github.com/Shiku-Kaito/Learning-from-Majority-label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04023v1">PDF</a> 35 pages, 9 figures, Accepted in Pattern recognition</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šç±»å¤šå®ä¾‹å­¦ä¹ é—®é¢˜ï¼Œç§°ä¸ºåŸºäºå¤šæ•°æ ‡ç­¾çš„å­¦ä¹ ï¼ˆLMLï¼‰ã€‚LMLå°†è¢‹å­ä¸­çš„å¤šæ•°ç±»å®ä¾‹ä½œä¸ºè¢‹çº§æ ‡ç­¾ï¼Œæ—¨åœ¨è®­ç»ƒä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œåˆ©ç”¨å¤šæ•°æ ‡ç­¾æ¥ä¼°è®¡æ¯ä¸ªå®ä¾‹çš„ç±»åˆ«ã€‚æ­¤æ–¹æ³•åœ¨ç—…ç†å­¦å›¾åƒåˆ†å‰²ã€æ”¿æ²»æŠ•ç¥¨é¢„æµ‹ã€å®¢æˆ·æƒ…æ„Ÿåˆ†æå’Œç¯å¢ƒç›‘æµ‹ç­‰å¤šç§åº”ç”¨ä¸­å…·æœ‰åº”ç”¨ä»·å€¼ã€‚ä¸ºè§£å†³LMLé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è®¡æ•°ç½‘ç»œï¼Œé€šè¿‡è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å®ä¾‹æ•°é‡æ¥äº§ç”Ÿè¢‹çº§å¤šæ•°æ ‡ç­¾ã€‚åˆ†æå®éªŒè¡¨æ˜ï¼Œå¤šæ•°ç±»å®ä¾‹æ¯”ä¾‹è¾ƒé«˜çš„è¢‹å­æœ‰åŠ©äºå­¦ä¹ ã€‚åŸºäºæ­¤ï¼Œå¼€å‘äº†ä¸€ä¸ªå¤šæ•°æ¯”ä¾‹å¢å¼ºæ¨¡å—ï¼ˆMPEMï¼‰ï¼Œé€šè¿‡ç§»é™¤è¢‹å­ä¸­çš„å°‘æ•°ç±»å®ä¾‹æ¥å¢åŠ å¤šæ•°ç±»çš„æ¯”ä¾‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„MILæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æ–°å‹çš„å¤šç±»å¤šå®ä¾‹å­¦ä¹ é—®é¢˜â€”â€”åŸºäºå¤šæ•°æ ‡ç­¾çš„å­¦ä¹ ï¼ˆLMLï¼‰ã€‚</li>
<li>LMLæ–¹æ³•å°†è¢‹å­ä¸­çš„å¤šæ•°ç±»å®ä¾‹ä½œä¸ºè¢‹çº§æ ‡ç­¾ï¼Œç”¨äºè®­ç»ƒåˆ†ç±»æ¨¡å‹ã€‚</li>
<li>LMLåœ¨å¤šç§åº”ç”¨é¢†åŸŸä¸­å…·æœ‰å®ç”¨æ€§ï¼Œå¦‚ç—…ç†å­¦å›¾åƒåˆ†å‰²ã€æ”¿æ²»æŠ•ç¥¨é¢„æµ‹ç­‰ã€‚</li>
<li>è®¡æ•°ç½‘ç»œç”¨äºç”Ÿæˆè¢‹çº§å¤šæ•°æ ‡ç­¾ï¼Œé€šè¿‡è®¡ç®—å„ç±»çš„å®ä¾‹æ•°é‡æ¥å®ç°ã€‚</li>
<li>å¤šæ•°ç±»å®ä¾‹æ¯”ä¾‹è¾ƒé«˜çš„è¢‹å­æœ‰åŠ©äºå­¦ä¹ ï¼ŒåŸºäºæ­¤å¼€å‘äº†MPEMæ¨¡å—ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»ŸMILæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8828bf13fc082b2eabe426a2802fc63b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3018a224e905707ab92276db81cd513c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5428d56d857a94c8873f48761cef338f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b0f56767e98ab7705cadb54c7e6439d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Foundation-Model-for-Chest-X-ray-Interpretation-with-Grounded-Reasoning-via-Online-Reinforcement-Learning"><a href="#A-Foundation-Model-for-Chest-X-ray-Interpretation-with-Grounded-Reasoning-via-Online-Reinforcement-Learning" class="headerlink" title="A Foundation Model for Chest X-ray Interpretation with Grounded   Reasoning via Online Reinforcement Learning"></a>A Foundation Model for Chest X-ray Interpretation with Grounded   Reasoning via Online Reinforcement Learning</h2><p><strong>Authors:Qika Lin, Yifan Zhu, Bin Pu, Ling Huang, Haoran Luo, Jingying Ma, Zhen Peng, Tianzhe Zhao, Fangzhi Xu, Jian Zhang, Kai He, Zhonghong Ou, Swapnil Mishra, Mengling Feng</strong></p>
<p>Medical foundation models (FMs) have shown tremendous promise amid the rapid advancements in artificial intelligence (AI) technologies. However, current medical FMs typically generate answers in a black-box manner, lacking transparent reasoning processes and locally grounded interpretability, which hinders their practical clinical deployments. To this end, we introduce DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It leverages a sequential training pipeline: initially fine-tuned on curated CXR instruction data to equip with fundamental CXR interpretation capabilities, then exposed to high-quality synthetic reasoning samples to enable cold-start reasoning, and finally refined via online reinforcement learning to enhance both grounded reasoning quality and generation performance. Thus, the model produces both an answer and reasoning steps tied to the imageâ€™s local regions for each query. Quantitative evaluation demonstrates substantial improvements in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent) tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking framework using advanced language models to evaluate answer quality, further highlighting the superiority of DeepMedix-R1. Expert review of generated reasoning steps reveals greater interpretability and clinical plausibility compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall preference). Collectively, our work advances medical FM development toward holistic, transparent, and clinically actionable modeling for CXR interpretation. </p>
<blockquote>
<p>åŒ»ç–—åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŒ»ç–—FMé€šå¸¸ä»¥é»‘ç®±æ–¹å¼ç”Ÿæˆç­”æ¡ˆï¼Œç¼ºä¹é€æ˜çš„æ¨ç†è¿‡ç¨‹å’ŒåŸºäºæœ¬åœ°çš„å¯è§£é‡Šæ€§ï¼Œè¿™é˜»ç¢äº†å…¶åœ¨å®é™…ä¸´åºŠéƒ¨ç½²ä¸­çš„åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeepMedix-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ç”¨äºèƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰è§£è¯»çš„åŒ»ç–—FMã€‚å®ƒåˆ©ç”¨äº†ä¸€ä¸ªåºåˆ—è®­ç»ƒç®¡é“ï¼šé¦–å…ˆï¼Œåœ¨ç²¾é€‰çš„CXRæŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥å…·å¤‡åŸºæœ¬çš„CXRè§£è¯»èƒ½åŠ›ï¼›ç„¶åï¼Œæš´éœ²äºé«˜è´¨é‡åˆæˆæ¨ç†æ ·æœ¬ä¸­ä»¥å¯åŠ¨å†·å¯åŠ¨æ¨ç†ï¼›æœ€åï¼Œé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ è¿›è¡Œç²¾ç‚¼ï¼Œä»¥æé«˜åŸºäºæœ¬åœ°çš„æ¨ç†è´¨é‡å’Œç”Ÿæˆæ€§èƒ½ã€‚å› æ­¤ï¼Œè¯¥æ¨¡å‹ä¸ºæ¯ä¸€ä¸ªæŸ¥è¯¢ç”Ÿæˆä¸å›¾åƒå±€éƒ¨åŒºåŸŸç›¸å…³è”çš„ç­”æ¡ˆå’Œæ¨ç†æ­¥éª¤ã€‚å®šé‡è¯„ä¼°è¡¨æ˜ï¼Œåœ¨æŠ¥å‘Šç”Ÿæˆï¼ˆä¾‹å¦‚ï¼Œç›¸è¾ƒäºLLaVA-Radå’ŒMedGemmaåˆ†åˆ«æé«˜14.54%å’Œ31.32%ï¼‰å’Œè§†è§‰é—®ç­”ï¼ˆä¾‹å¦‚ï¼Œç›¸è¾ƒäºMedGemmaå’ŒCheXagentåˆ†åˆ«æé«˜57.75%å’Œ23.06%ï¼‰çš„ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¸ºäº†è¿›è¡Œç¨³å¥çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†Report Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨å…ˆè¿›è¯­è¨€æ¨¡å‹æ¥è¯„ä¼°ç­”æ¡ˆè´¨é‡çš„åŸºå‡†æ¡†æ¶ï¼Œè¿›ä¸€æ­¥çªæ˜¾äº†DeepMedix-R1çš„ä¼˜è¶Šæ€§ã€‚å¯¹ç”Ÿæˆæ¨ç†æ­¥éª¤çš„ä¸“å®¶è¯„å®¡æ˜¾ç¤ºï¼Œä¸å·²å»ºç«‹çš„Qwen2.5-VL-7Bæ¨¡å‹ç›¸æ¯”ï¼Œå…¶è§£é‡Šæ€§å’Œä¸´åºŠå¯ä¿¡åº¦æ›´é«˜ï¼ˆæ•´ä½“åå¥½ä¸º0.7416å¯¹0.2584ï¼‰ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ¨åŠ¨äº†åŒ»ç–—FMå‘å…¨é¢ã€é€æ˜ä¸”ä¸´åºŠå¯è¡Œçš„CXRè§£è¯»æ¨¡å‹å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03906v1">PDF</a> 15 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒåŒ»ç–—åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨åŒ»ç–—å›¾åƒè§£è¯»æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰åŒ»ç–—FMsé€šå¸¸é‡‡ç”¨é»‘ç›’æ–¹å¼ç”Ÿæˆç­”æ¡ˆï¼Œç¼ºä¹é€æ˜çš„æ¨ç†è¿‡ç¨‹å’Œæœ¬åœ°åŒ–çš„è§£é‡Šæ€§ï¼Œè¿™é˜»ç¢äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DeepMedix-R1ï¼Œä¸€ä¸ªå…¨é¢çš„ç”¨äºèƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰è§£è¯»çš„åŒ»ç–—FMã€‚å®ƒé‡‡ç”¨åºè´¯è®­ç»ƒç®¡é“ï¼šé¦–å…ˆï¼Œåœ¨ç²¾é€‰çš„CXRæŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥å…·å¤‡åŸºæœ¬çš„CXRè§£è¯»èƒ½åŠ›ï¼›ç„¶åï¼Œé€šè¿‡é«˜è´¨é‡åˆæˆæ¨ç†æ ·æœ¬è¿›è¡Œå†·å¯åŠ¨æ¨ç†ï¼›æœ€åï¼Œé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ è¿›è¡Œç»†åŒ–ï¼Œæé«˜åŸºäºå›¾åƒçš„æœ¬åœ°æ¨ç†è´¨é‡å’Œç”Ÿæˆæ€§èƒ½ã€‚å› æ­¤ï¼Œè¯¥æ¨¡å‹ä¸ºæ¯ä¸€ä¸ªæŸ¥è¯¢ç”Ÿæˆç­”æ¡ˆå’Œä¸å›¾åƒå±€éƒ¨åŒºåŸŸç›¸å…³çš„æ¨ç†æ­¥éª¤ã€‚å®šé‡è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ä»»åŠ¡æ–¹é¢ï¼ŒDeepMedix-R1è¾ƒLLaVA-Radå’ŒMedGemmaæœ‰æ˜¾è‘—æ”¹å–„ã€‚ä¸ºä¾¿äºè¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†Report Arenaï¼Œä¸€ä¸ªä½¿ç”¨å…ˆè¿›è¯­è¨€æ¨¡å‹è¯„ä¼°ç­”æ¡ˆè´¨é‡çš„åŸºå‡†æ¡†æ¶ï¼Œè¿›ä¸€æ­¥çªæ˜¾DeepMedix-R1çš„ä¼˜è¶Šæ€§ã€‚ä¸å·²å»ºç«‹çš„Qwen2.5-VL-7Bæ¨¡å‹ç›¸æ¯”ï¼Œä¸“å®¶å¯¹ç”Ÿæˆçš„æ¨ç†æ­¥éª¤çš„å®¡æŸ¥æ˜¾ç¤ºå‡ºæ›´é«˜çš„å¯è§£é‡Šæ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚æ€»ä½“ä¸Šï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¨åŠ¨äº†åŒ»ç–—FMåœ¨CXRè§£è¯»æ–¹é¢æœç€å…¨é¢ã€é€æ˜å’Œä¸´åºŠå¯æ“ä½œçš„æ–¹å‘å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»ç–—åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰è§£è¯»ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰åŒ»ç–—FMså­˜åœ¨ç¼ºä¹é€æ˜æ¨ç†å’Œæœ¬åœ°è§£é‡Šæ€§çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚</li>
<li>DeepMedix-R1æ˜¯ä¸€ä¸ªå…¨é¢çš„ç”¨äºCXRè§£è¯»çš„åŒ»ç–—FMï¼Œé‡‡ç”¨åºè´¯è®­ç»ƒç®¡é“æé«˜æ€§èƒ½ã€‚</li>
<li>DeepMedix-R1åœ¨æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šè¾ƒå…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>ä¸ºè¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼Œæå‡ºäº†Report ArenaåŸºå‡†æ¡†æ¶ã€‚</li>
<li>DeepMedix-R1çš„æ¨ç†æ­¥éª¤å…·æœ‰æ›´é«˜çš„å¯è§£é‡Šæ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cbbcc1af58efb135a2de8ece3d2debbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ffed779ba67c21b42c1ebdf11dd9290.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df3755f147d5d908de4cb6244d59c196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a49c6a256c7b4859869d3cd6355b82b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MedVista3D-Vision-Language-Modeling-for-Reducing-Diagnostic-Errors-in-3D-CT-Disease-Detection-Understanding-and-Reporting"><a href="#MedVista3D-Vision-Language-Modeling-for-Reducing-Diagnostic-Errors-in-3D-CT-Disease-Detection-Understanding-and-Reporting" class="headerlink" title="MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in   3D CT Disease Detection, Understanding and Reporting"></a>MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in   3D CT Disease Detection, Understanding and Reporting</h2><p><strong>Authors:Yuheng Li, Yenho Chen, Yuxiang Lai, Jike Zhong, Vanessa Wildman, Xiaofeng Yang</strong></p>
<p>Radiologic diagnostic errors-under-reading errors, inattentional blindness, and communication failures-remain prevalent in clinical practice. These issues often stem from missed localized abnormalities, limited global context, and variability in report language. These challenges are amplified in 3D imaging, where clinicians must examine hundreds of slices per scan. Addressing them requires systems with precise localized detection, global volume-level reasoning, and semantically consistent natural language reporting. However, existing 3D vision-language models are unable to meet all three needs jointly, lacking local-global understanding for spatial reasoning and struggling with the variability and noise of uncurated radiology reports. We present MedVista3D, a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis. To enable joint disease detection and holistic interpretation, MedVista3D performs local and global image-text alignment for fine-grained representation learning within full-volume context. To address report variability, we apply language model rewrites and introduce a Radiology Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering, while transferring well to organ segmentation and prognosis prediction. Code and datasets will be released. </p>
<blockquote>
<p>åœ¨ä¸´åºŠå®è·µä¸­ï¼Œæ”¾å°„å­¦è¯Šæ–­é”™è¯¯ï¼ˆåŒ…æ‹¬æ¼è¯»é”™è¯¯ã€æ³¨æ„åŠ›ç¼ºå¤±æ€§å¤±æ˜å’Œæ²Ÿé€šå¤±è´¥ï¼‰ä»ç„¶æ™®éå­˜åœ¨ã€‚è¿™äº›é—®é¢˜é€šå¸¸æºäºå±€éƒ¨å¼‚å¸¸çš„é—æ¼ã€å…¨å±€æƒ…å¢ƒçš„å±€é™ä»¥åŠæŠ¥å‘Šè¯­è¨€çš„å·®å¼‚ã€‚åœ¨ä¸‰ç»´æˆåƒä¸­ï¼ŒåŒ»ç”Ÿæ¯æ¬¡æ‰«æéœ€è¦æ£€æŸ¥æ•°ç™¾ä¸ªåˆ‡ç‰‡ï¼Œè¿™äº›æŒ‘æˆ˜è¢«è¿›ä¸€æ­¥æ”¾å¤§ã€‚è¦è§£å†³è¿™äº›é—®é¢˜ï¼Œéœ€è¦æ‹¥æœ‰ç²¾ç¡®å±€éƒ¨æ£€æµ‹ã€å…¨å±€ä½“ç§¯çº§æ¨ç†å’Œè¯­ä¹‰è¿è´¯çš„è‡ªç„¶è¯­è¨€æŠ¥å‘Šçš„ç³»ç»Ÿã€‚ç„¶è€Œï¼Œç°æœ‰çš„3Dè§†è§‰è¯­è¨€æ¨¡å‹æ— æ³•åŒæ—¶æ»¡è¶³æ‰€æœ‰ä¸‰ä¸ªéœ€æ±‚ï¼Œç¼ºä¹å±€éƒ¨å…¨å±€ç†è§£æ¥è¿›è¡Œç©ºé—´æ¨ç†ï¼Œå¹¶ä¸”éš¾ä»¥åº”å¯¹æœªæ•´ç†çš„æ”¾å°„å­¦æŠ¥å‘Šçš„å˜ä½“å’Œå™ªå£°ã€‚æˆ‘ä»¬æ¨å‡ºäº†MedVista3Dï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº3D CTåˆ†æçš„å¤šå°ºåº¦è¯­ä¹‰ä¸°å¯Œè§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ã€‚ä¸ºäº†å®ç°è”åˆç–¾ç—…æ£€æµ‹å’Œæ•´ä½“è§£é‡Šï¼ŒMedVista3Dæ‰§è¡Œå±€éƒ¨å’Œå…¨å±€å›¾åƒæ–‡æœ¬å¯¹é½ï¼Œä»¥åœ¨å…¨å·ç§¯èƒŒæ™¯ä¸‹è¿›è¡Œç²¾ç»†ç²’åº¦è¡¨ç¤ºå­¦ä¹ ã€‚ä¸ºäº†è§£å†³æŠ¥å‘Šå·®å¼‚é—®é¢˜ï¼Œæˆ‘ä»¬åº”ç”¨äº†è¯­è¨€æ¨¡å‹é‡å†™ï¼Œå¹¶å¼•å…¥æ”¾å°„å­¦è¯­ä¹‰åŒ¹é…é“¶è¡Œè¿›è¡Œè¯­ä¹‰æ„ŸçŸ¥å¯¹é½ã€‚MedVista3Dåœ¨é›¶æ ·æœ¬ç–¾ç—…åˆ†ç±»ã€æŠ¥å‘Šæ£€ç´¢å’ŒåŒ»å­¦è§†è§‰é—®ç­”æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å¾ˆå¥½åœ°åº”ç”¨äºå™¨å®˜åˆ†å‰²å’Œé¢„åé¢„æµ‹ã€‚ä»£ç å’Œæ•°æ®é›†å°†äºˆä»¥å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å½±åƒè¯Šæ–­ä¸­å¸¸è§çš„ä¸€äº›é”™è¯¯å’ŒæŒ‘æˆ˜ï¼Œå¦‚å¿½ç•¥å±€éƒ¨å¼‚å¸¸ã€æ³¨æ„åŠ›ç¼ºå¤±ã€æ²Ÿé€šéšœç¢ç­‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMedVista3Dçš„å¤šå°ºåº¦è¯­ä¹‰å¢å¼ºè§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºè¿›è¡Œä¸‰ç»´CTåˆ†æã€‚è¯¥æ¡†æ¶é€šè¿‡å±€éƒ¨å’Œå…¨å±€å›¾åƒæ–‡æœ¬å¯¹é½ï¼Œå®ç°ç²¾ç»†ç²’åº¦è¡¨ç¤ºå­¦ä¹ ï¼Œåœ¨æ•´ä½“è¯­å¢ƒä¸‹è¾¾åˆ°è”åˆç–¾ç—…æ£€æµ‹å’Œæ•´ä½“è§£é‡Šçš„ç›®æ ‡ã€‚åŒæ—¶å¼•å…¥è¯­è¨€æ¨¡å‹é‡å†™å’Œæ”¾å°„å­¦è¯­ä¹‰åŒ¹é…é“¶è¡Œï¼Œä»¥è§£å†³æŠ¥å‘Šå·®å¼‚å’Œè¯­ä¹‰å¯¹é½é—®é¢˜ã€‚MedVista3Dåœ¨é›¶æ ·æœ¬ç–¾ç—…åˆ†ç±»ã€æŠ¥å‘Šæ£€ç´¢ã€åŒ»å­¦å½±åƒé—®ç­”ç­‰æ–¹é¢å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°åº”ç”¨äºå™¨å®˜åˆ†å‰²å’Œé¢„åé¢„æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å½±åƒè¯Šæ–­ä¸­å­˜åœ¨å¸¸è§çš„é”™è¯¯å’ŒæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¿½ç•¥å±€éƒ¨å¼‚å¸¸ã€æ³¨æ„åŠ›ç¼ºå¤±å’Œæ²Ÿé€šéšœç¢ç­‰ã€‚</li>
<li>è¿™äº›æŒ‘æˆ˜åœ¨3Dæˆåƒä¸­æ›´åŠ çªå‡ºï¼Œå› ä¸ºä¸´åºŠåŒ»ç”Ÿéœ€è¦æ£€æŸ¥çš„åˆ‡ç‰‡æ•°é‡éå¸¸å¤šã€‚</li>
<li>MedVista3Dæ˜¯ä¸€ä¸ªå¤šå°ºåº¦çš„è¯­ä¹‰å¢å¼ºè§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>MedVista3Dé€šè¿‡å±€éƒ¨å’Œå…¨å±€å›¾åƒæ–‡æœ¬å¯¹é½ï¼Œå®ç°ç²¾ç»†ç²’åº¦è¡¨ç¤ºå­¦ä¹ ï¼Œåœ¨æ•´ä½“è¯­å¢ƒä¸‹è¾¾åˆ°è”åˆç–¾ç—…æ£€æµ‹å’Œæ•´ä½“è§£é‡Šçš„ç›®æ ‡ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†æŠ¥å‘Šå·®å¼‚é—®é¢˜ï¼Œé€šè¿‡è¯­è¨€æ¨¡å‹é‡å†™å’Œæ”¾å°„å­¦è¯­ä¹‰åŒ¹é…é“¶è¡Œå®ç°è¯­ä¹‰å¯¹é½ã€‚</li>
<li>MedVista3Dåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ç–¾ç—…åˆ†ç±»ã€æŠ¥å‘Šæ£€ç´¢ã€åŒ»å­¦å½±åƒé—®ç­”ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e147cd293f36496eccdea8fb6175c341.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-507953e1ebbbc369516ffa2a1bb2fb29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1954e2934f254357518bb5145abd582b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Learning-functions-through-Diffusion-Maps"><a href="#Learning-functions-through-Diffusion-Maps" class="headerlink" title="Learning functions through Diffusion Maps"></a>Learning functions through Diffusion Maps</h2><p><strong>Authors:Alvaro Almeida Gomez</strong></p>
<p>We propose a data-driven method for approximating real-valued functions on smooth manifolds, building on the Diffusion Maps framework under the manifold hypothesis. Given pointwise evaluations of a function, the method constructs a smooth extension to the ambient space by exploiting diffusion geometry and its connection to the heat equation and the Laplace-Beltrami operator.   To address the computational challenges of high-dimensional data, we introduce a dimensionality reduction strategy based on the low-rank structure of the distance matrix, revealed via singular value decomposition (SVD). In addition, we develop an online updating mechanism that enables efficient incorporation of new data, thereby improving scalability and reducing computational cost.   Numerical experiments, including applications to sparse CT reconstruction, demonstrate that the proposed methodology outperforms classical feedforward neural networks and interpolation methods in terms of both accuracy and efficiency. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¹³æ»‘æµå½¢ä¸Šè¿‘ä¼¼å®å€¼å‡½æ•°ã€‚è¯¥æ–¹æ³•å»ºç«‹åœ¨æµå½¢å‡è®¾ä¸‹çš„æ‰©æ•£å›¾æ¡†æ¶ä¹‹ä¸Šã€‚ç»™å®šå‡½æ•°çš„ç‚¹æ€è¯„ä¼°ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ‰©æ•£å‡ ä½•åŠå…¶ä¸çƒ­æ–¹ç¨‹å’ŒLaplace-Beltramiç®—å­çš„è”ç³»ï¼Œåœ¨ç¯å¢ƒç©ºé—´ä¸­æ„å»ºå¹³æ»‘æ‰©å±•ã€‚ä¸ºäº†è§£å†³é«˜ç»´æ•°æ®çš„è®¡ç®—æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ­ç¤ºçš„è·ç¦»çŸ©é˜µä½ç§©ç»“æ„çš„é™ç»´ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åœ¨çº¿æ›´æ–°æœºåˆ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°èå…¥æ–°æ•°æ®ï¼Œä»è€Œæé«˜å¯æ‰©å±•æ€§å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼ŒåŒ…æ‹¬åœ¨ç¨€ç–CTé‡å»ºä¸­çš„åº”ç”¨åœ¨å†…ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç»å…¸çš„å‰é¦ˆç¥ç»ç½‘ç»œå’Œæ’å€¼æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03758v1">PDF</a> Comments are welcome</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¹³æ»‘æµå½¢ä¸Šè¿‘ä¼¼å®å€¼å‡½æ•°ã€‚è¯¥æ–¹æ³•åœ¨æµå½¢å‡è®¾ä¸‹æ„å»ºåœ¨æ‰©æ•£æ˜ å°„æ¡†æ¶ä¸Šçš„æ‰©æ•£å‡ ä½•ç»“æ„ï¼Œå¹¶åˆ©ç”¨å…¶ä¸çƒ­æ–¹ç¨‹å’ŒLaplace-Beltramiç®—å­çš„è”ç³»è¿›è¡Œå¹³æ»‘æ‰©å±•ã€‚ä¸ºäº†åº”å¯¹é«˜ç»´æ•°æ®çš„è®¡ç®—æŒ‘æˆ˜ï¼Œå¼•å…¥åŸºäºè·ç¦»çŸ©é˜µä½ç§©ç»“æ„çš„é™ç»´ç­–ç•¥ï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ­ç¤ºã€‚æ­¤å¤–ï¼Œå¼€å‘äº†åœ¨çº¿æ›´æ–°æœºåˆ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆèå…¥æ–°æ•°æ®ï¼Œä»è€Œæé«˜æ–¹æ³•çš„å¯æ‰©å±•æ€§å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚æ•°å€¼å®éªŒåŒ…æ‹¬ç¨€ç–CTé‡å»ºçš„åº”ç”¨ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç»å…¸çš„å‰é¦ˆç¥ç»ç½‘ç»œå’Œæ’å€¼æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ˜ å°„æ¡†æ¶åœ¨å¹³æ»‘æµå½¢ä¸Šè¿‘ä¼¼å®å€¼å‡½æ•°ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£å‡ ä½•ç»“æ„ï¼Œç»“åˆçƒ­æ–¹ç¨‹å’ŒLaplace-Beltramiç®—å­çš„è”ç³»è¿›è¡Œå¹³æ»‘æ‰©å±•ã€‚</li>
<li>å¼•å…¥åŸºäºè·ç¦»çŸ©é˜µä½ç§©ç»“æ„çš„é™ç»´ç­–ç•¥ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>é€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ­ç¤ºè·ç¦»çŸ©é˜µçš„ä½ç§©ç»“æ„ã€‚</li>
<li>å¼€å‘åœ¨çº¿æ›´æ–°æœºåˆ¶ï¼Œä»¥ä¾¿é«˜æ•ˆèå…¥æ–°æ•°æ®ã€‚</li>
<li>æ–¹æ³•å…·æœ‰ä¼˜ç§€çš„å¯æ‰©å±•æ€§ï¼Œå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83d54e6a78cb30b57866887db930a34a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Scalable-and-Loosely-Coupled-Multimodal-Deep-Learning-for-Breast-Cancer-Subtyping"><a href="#Scalable-and-Loosely-Coupled-Multimodal-Deep-Learning-for-Breast-Cancer-Subtyping" class="headerlink" title="Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer   Subtyping"></a>Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer   Subtyping</h2><p><strong>Authors:Mohammed Amer, Mohamed A. Suliman, Tu Bui, Nuria Garcia, Serban Georgescu</strong></p>
<p>Healthcare applications are inherently multimodal, benefiting greatly from the integration of diverse data sources. However, the modalities available in clinical settings can vary across different locations and patients. A key area that stands to gain from multimodal integration is breast cancer molecular subtyping, an important clinical task that can facilitate personalized treatment and improve patient prognosis. In this work, we propose a scalable and loosely-coupled multimodal framework that seamlessly integrates data from various modalities, including copy number variation (CNV), clinical records, and histopathology images, to enhance breast cancer subtyping. While our primary focus is on breast cancer, our framework is designed to easily accommodate additional modalities, offering the flexibility to scale up or down with minimal overhead without requiring re-training of existing modalities, making it applicable to other types of cancers as well. We introduce a dual-based representation for whole slide images (WSIs), combining traditional image-based and graph-based WSI representations. This novel dual approach results in significant performance improvements. Moreover, we present a new multimodal fusion strategy, demonstrating its ability to enhance performance across a range of multimodal conditions. Our comprehensive results show that integrating our dual-based WSI representation with CNV and clinical health records, along with our pipeline and fusion strategy, outperforms state-of-the-art methods in breast cancer subtyping. </p>
<blockquote>
<p>åŒ»ç–—å¥åº·åº”ç”¨ç¨‹åºæœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡å¼çš„ï¼Œå¤§å¤§å—ç›Šäºä¸åŒæ•°æ®æºçš„é›†æˆã€‚ç„¶è€Œï¼Œä¸´åºŠç¯å¢ƒä¸­å¯ç”¨çš„æ¨¡å¼å¯ä»¥åœ¨ä¸åŒçš„åœ°ç‚¹å’Œæ‚£è€…ä¹‹é—´å˜åŒ–ã€‚ä»å¤šæ¨¡å¼é›†æˆä¸­å—ç›Šçš„å…³é”®é¢†åŸŸä¹‹ä¸€æ˜¯ä¹³è…ºç™Œåˆ†å­åˆ†å‹ï¼Œè¿™æ˜¯ä¸€é¡¹é‡è¦çš„ä¸´åºŠä»»åŠ¡ï¼Œå¯ä»¥ä¿ƒè¿›ä¸ªæ€§åŒ–æ²»ç–—å¹¶æ”¹å–„æ‚£è€…é¢„åã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•ä¸”æ¾æ•£è€¦åˆçš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— ç¼é›†æˆäº†å„ç§æ¨¡å¼çš„æ•°æ®ï¼ŒåŒ…æ‹¬æ‹·è´æ•°å˜å¼‚ï¼ˆCNVï¼‰ã€ä¸´åºŠè®°å½•å’Œç»„ç—…ç†å›¾åƒï¼Œä»¥æé«˜ä¹³è…ºç™Œçš„åˆ†å‹èƒ½åŠ›ã€‚è™½ç„¶æˆ‘ä»¬çš„é‡ç‚¹æ˜¯åœ¨ä¹³è…ºç™Œä¸Šï¼Œä½†æˆ‘ä»¬çš„æ¡†æ¶è®¾è®¡æ˜“äºå®¹çº³å…¶ä»–æ¨¡å¼ï¼Œæä¾›çµæ´»çš„å¯ä¼¸ç¼©æ€§ï¼Œæ— éœ€å¯¹ç°æœ‰æ¨¡å¼è¿›è¡Œé‡æ–°è®­ç»ƒï¼Œä½¿å…¶ä¹Ÿé€‚ç”¨äºå…¶ä»–ç±»å‹çš„ç™Œç—‡ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºåŒè¡¨ç¤ºçš„æ•´å¼ å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰ï¼Œç»“åˆäº†ä¼ ç»Ÿçš„å›¾åƒå’ŒåŸºäºå›¾çš„WSIè¡¨ç¤ºã€‚è¿™ç§æ–°å‹åŒè·¯å¾„æ–¹æ³•å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å¼èåˆç­–ç•¥ï¼Œå±•ç¤ºäº†å…¶åœ¨å„ç§å¤šæ¨¡å¼æ¡ä»¶ä¸‹çš„å¢å¼ºæ€§èƒ½çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»¼åˆç»“æœè¡¨æ˜ï¼Œå°†æˆ‘ä»¬çš„åŸºäºåŒè·¯å¾„çš„WSIè¡¨ç¤ºä¸CNVå’Œä¸´åºŠå¥åº·è®°å½•ç›¸ç»“åˆï¼Œä»¥åŠæˆ‘ä»¬çš„ç®¡é“å’Œèåˆç­–ç•¥ï¼Œåœ¨ä¹³è…ºç™Œåˆ†å‹æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03408v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªå¯æ‰©å±•çš„ã€æ¾æ•£è€¦åˆçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— ç¼é›†æˆäº†æ‹·è´æ•°å˜å¼‚ã€ä¸´åºŠè®°å½•å’Œç—…ç†å›¾åƒç­‰å¤šç§æ•°æ®ï¼Œä»¥æ”¹è¿›ä¹³è…ºç™Œåˆ†å­åˆ†å‹ã€‚é‡‡ç”¨åŒé‡åŸºäºè¡¨ç¤ºçš„å…¨å¹»ç¯ç‰‡å›¾åƒæŠ€æœ¯ï¼Œç»“åˆä¼ ç»Ÿå›¾åƒå’ŒåŸºäºå›¾çš„å¹»ç¯ç‰‡è¡¨ç¤ºæ³•ï¼Œå–å¾—æ˜¾è‘—æ€§èƒ½æå‡ã€‚æ–°çš„å¤šæ¨¡æ€èåˆç­–ç•¥èƒ½å¢å¼ºå„ç§å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚æ•´åˆåŒé‡åŸºäºå…¨å¹»ç¯ç‰‡å›¾åƒè¡¨ç¤ºçš„æ–¹æ³•ä¸æ‹·è´æ•°å˜å¼‚å’Œä¸´åºŠå¥åº·è®°å½•ï¼Œé…åˆç®¡é“å’Œèåˆç­–ç•¥ï¼Œåœ¨ä¹³è…ºç™Œåˆ†å‹æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—ä¿å¥åº”ç”¨å…·æœ‰å¤šæ¨¡æ€ç‰¹æ€§ï¼Œä»æ•´åˆå¤šæ ·æ•°æ®æºä¸­è·ç›Šå·¨å¤§ã€‚</li>
<li>ä¹³è…ºç™Œåˆ†å­åˆ†å‹æ˜¯å¤šæ¨¡æ€æ•´åˆçš„å…³é”®åº”ç”¨é¢†åŸŸä¹‹ä¸€ï¼Œæœ‰åŠ©äºä¸ªæ€§åŒ–æ²»ç–—å’Œæ”¹å–„æ‚£è€…é¢„åã€‚</li>
<li>æå‡ºä¸€ç§å¯æ‰©å±•çš„ã€æ¾æ•£è€¦åˆçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œèƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒæ¨¡æ€çš„å¢å‡ï¼Œæ— éœ€å¯¹ç°æœ‰æ¨¡æ€è¿›è¡Œé‡æ–°è®­ç»ƒã€‚</li>
<li>é‡‡ç”¨åŒé‡åŸºäºè¡¨ç¤ºçš„å…¨å¹»ç¯ç‰‡å›¾åƒæŠ€æœ¯ï¼Œç»“åˆä¼ ç»Ÿå›¾åƒå’ŒåŸºäºå›¾çš„è¡¨ç¤ºæ³•ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>å¼•å…¥æ–°çš„å¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œèƒ½å¤Ÿå¢å¼ºåœ¨å„ç§å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ä¹³è…ºç™Œåˆ†å‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86452aceb31ad4bb7421cf3c0fdc5b4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26bd189085a0e98ec813730e7fa49474.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd1255089e958b3dfd874b5b9abbe8ea.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SynBT-High-quality-Tumor-Synthesis-for-Breast-Tumor-Segmentation-by-3D-Diffusion-Model"><a href="#SynBT-High-quality-Tumor-Synthesis-for-Breast-Tumor-Segmentation-by-3D-Diffusion-Model" class="headerlink" title="SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D   Diffusion Model"></a>SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D   Diffusion Model</h2><p><strong>Authors:Hongxu Yang, Edina Timko, Levente Lippenszky, Vanda Czipczer, Lehel Ferenczi</strong></p>
<p>Synthetic tumors in medical images offer controllable characteristics that facilitate the training of machine learning models, leading to an improved segmentation performance. However, the existing methods of tumor synthesis yield suboptimal performances when tumor occupies a large spatial volume, such as breast tumor segmentation in MRI with a large field-of-view (FOV), while commonly used tumor generation methods are based on small patches. In this paper, we propose a 3D medical diffusion model, called SynBT, to generate high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed model consists of a patch-to-volume autoencoder, which is able to compress the high-resolution MRIs into compact latent space, while preserving the resolution of volumes with large FOV. Using the obtained latent space feature vector, a mask-conditioned diffusion model is used to synthesize breast tumors within selected regions of breast tissue, resulting in realistic tumor appearances. We evaluated the proposed method for a tumor segmentation task, which demonstrated the proposed high-quality tumor synthesis method can facilitate the common segmentation models with performance improvement of 2-3% Dice Score on a large public dataset, and therefore provides benefits for tumor segmentation in MRI images. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä¸­çš„åˆæˆè‚¿ç˜¤å…·æœ‰å¯æ§ç‰¹æ€§ï¼Œæœ‰åŠ©äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‚¿ç˜¤åˆæˆæ–¹æ³•åœ¨è‚¿ç˜¤å æ®è¾ƒå¤§ç©ºé—´ä½“ç§¯æ—¶ï¼Œè¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œä¾‹å¦‚åœ¨MRIä¸­å…·æœ‰å¤§è§†é‡ï¼ˆFOVï¼‰çš„ä¹³è…ºç™Œåˆ†å‰²ã€‚é€šå¸¸ä½¿ç”¨çš„è‚¿ç˜¤ç”Ÿæˆæ–¹æ³•åŸºäºå°è¡¥ä¸ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSynBTçš„3DåŒ»å­¦æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡å¯¹æ¯”å¢å¼ºMRIå›¾åƒä¸­çš„ä¹³è…ºç™Œï¼ˆBTï¼‰ã€‚æ‰€æå‡ºæ¨¡å‹åŒ…æ‹¬ä¸€ä¸ªç”±è¡¥ä¸åˆ°ä½“ç§¯çš„è‡ªç¼–ç å™¨ï¼Œèƒ½å¤Ÿå°†é«˜åˆ†è¾¨ç‡MRIå‹ç¼©æˆç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤§è§†é‡ä½“ç§¯çš„åˆ†è¾¨ç‡ã€‚åˆ©ç”¨è·å¾—çš„æ½œåœ¨ç©ºé—´ç‰¹å¾å‘é‡ï¼Œä½¿ç”¨æ©æ¨¡æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨é€‰å®šä¹³è…ºç»„ç»‡åŒºåŸŸå†…åˆæˆä¹³è…ºç™Œï¼Œäº§ç”Ÿé€¼çœŸçš„è‚¿ç˜¤å¤–è§‚ã€‚æˆ‘ä»¬å¯¹æ‰€æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œé«˜è´¨é‡è‚¿ç˜¤åˆæˆæ–¹æ³•èƒ½å¤Ÿä¿ƒè¿›å…¬å…±å¤§å‹æ•°æ®é›†ä¸Šçš„å¸¸è§åˆ†å‰²æ¨¡å‹ï¼Œæé«˜2-3%çš„Diceå¾—åˆ†ã€‚å› æ­¤ï¼Œè¯¥æ–¹æ³•å¯¹MRIå›¾åƒçš„è‚¿ç˜¤åˆ†å‰²å…·æœ‰ç›Šå¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03267v1">PDF</a> Accepted by MICCAI 2025 Deep-Breath Workshop. Supported by IHI   SYNTHIA project</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒä¸­åˆæˆè‚¿ç˜¤å…·æœ‰å¯æ§ç‰¹æ€§ï¼Œæœ‰åŠ©äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰è‚¿ç˜¤åˆæˆæ–¹æ³•åœ¨è‚¿ç˜¤å æ®è¾ƒå¤§ç©ºé—´ä½“ç§¯æ—¶æ€§èƒ½ä¸ä½³çš„é—®é¢˜ï¼Œå¦‚MRIä¸­è§†é‡è¾ƒå¤§çš„ä¹³è…ºç™Œåˆ†å‰²ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSynBTçš„3DåŒ»å­¦æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„ä¹³è…ºç™Œå¯¹æ¯”å¢å¼ºMRIå›¾åƒã€‚è¯¥æ¨¡å‹ç”±patch-to-volumeè‡ªç¼–ç å™¨ç»„æˆï¼Œèƒ½å¤Ÿå‹ç¼©é«˜åˆ†è¾¨ç‡MRIå›¾åƒåˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤§è§†é‡çš„ä½“ç§¯åˆ†è¾¨ç‡ã€‚ä½¿ç”¨è·å¾—çš„æ½œåœ¨ç©ºé—´ç‰¹å¾å‘é‡ï¼Œé€šè¿‡æ©è†œæ§åˆ¶çš„æ‰©æ•£æ¨¡å‹åœ¨é€‰å®šä¹³è…ºç»„ç»‡åŒºåŸŸåˆæˆè‚¿ç˜¤ï¼Œäº§ç”Ÿé€¼çœŸçš„è‚¿ç˜¤å¤–è§‚ã€‚è¯„ä¼°ç”¨äºè‚¿ç˜¤åˆ†å‰²ä»»åŠ¡çš„æ–¹æ³•è¡¨æ˜ï¼Œé«˜è´¨é‡è‚¿ç˜¤åˆæˆæ–¹æ³•èƒ½ä¿ƒè¿›å…¬å…±æ•°æ®é›†ä¸Šçš„åˆ†å‰²æ¨¡å‹æ€§èƒ½æé«˜2-3%çš„Diceå¾—åˆ†ï¼Œå› æ­¤åœ¨MRIå›¾åƒè‚¿ç˜¤åˆ†å‰²ä¸­æä¾›ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆè‚¿ç˜¤åœ¨åŒ»å­¦å›¾åƒä¸­å…·æœ‰å¯æ§ç‰¹æ€§ï¼Œæœ‰åŠ©äºæœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒï¼Œæå‡åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>ç°æœ‰è‚¿ç˜¤åˆæˆæ–¹æ³•åœ¨å¤„ç†å¤§ç©ºé—´ä½“ç§¯çš„è‚¿ç˜¤æ—¶å­˜åœ¨æ€§èƒ½é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨MRIä¸­çš„å¤§è§†é‡ä¹³è…ºç™Œåˆ†å‰²ã€‚</li>
<li>æå‡ºäº†åä¸ºSynBTçš„3DåŒ»å­¦æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å¯¹æ¯”å¢å¼ºMRIä¹³è…ºç™Œå›¾åƒã€‚</li>
<li>è¯¥æ¨¡å‹åŒ…å«patch-to-volumeè‡ªç¼–ç å™¨ï¼Œèƒ½å¤Ÿå‹ç¼©é«˜åˆ†è¾¨ç‡MRIåˆ°ç´§å‡‘æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒå¤§è§†é‡çš„ä½“ç§¯åˆ†è¾¨ç‡ã€‚</li>
<li>ä½¿ç”¨æ½œåœ¨ç©ºé—´ç‰¹å¾å‘é‡ï¼Œé€šè¿‡æ©è†œæ§åˆ¶çš„æ‰©æ•£æ¨¡å‹åœ¨é€‰å®šä¹³è…ºç»„ç»‡åŒºåŸŸåˆæˆè‚¿ç˜¤ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½æé«˜è‚¿ç˜¤åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„Diceå¾—åˆ†æé«˜2-3%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f50b5c6253aeef5d14d0b484228a94ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44ae15c4d6dcc8cd75fa20b5af59f954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c63b9af8e8178353b459f3842e27ebe8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SPENet-Self-guided-Prototype-Enhancement-Network-for-Few-shot-Medical-Image-Segmentation"><a href="#SPENet-Self-guided-Prototype-Enhancement-Network-for-Few-shot-Medical-Image-Segmentation" class="headerlink" title="SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical   Image Segmentation"></a>SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical   Image Segmentation</h2><p><strong>Authors:Chao Fan, Xibin Jia, Anqi Xiao, Hongyuan Yu, Zhenghan Yang, Dawei Yang, Hui Xu, Yan Huang, Liang Wang</strong></p>
<p>Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of medical objects using only a few labeled images. Prototype-based methods have made significant progress in addressing FSMIS. However, they typically generate a single global prototype for the support image to match with the query image, overlooking intra-class variations. To address this issue, we propose a Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce a Multi-level Prototype Generation (MPG) module, which enables multi-granularity measurement between the support and query images by simultaneously generating a global prototype and an adaptive number of local prototypes. Additionally, we observe that not all local prototypes in the support image are beneficial for matching, especially when there are substantial discrepancies between the support and query images. To alleviate this issue, we propose a Query-guided Local Prototype Enhancement (QLPE) module, which adaptively refines support prototypes by incorporating guidance from the query image, thus mitigating the negative effects of such discrepancies. Extensive experiments on three public medical datasets demonstrate that SPENet outperforms existing state-of-the-art methods, achieving superior performance. </p>
<blockquote>
<p>å°‘é‡åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ—¨åœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡è®°å›¾åƒå¯¹æ–°å‹åŒ»å­¦å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚åŸºäºåŸå‹çš„æ–¹æ³•åœ¨è§£å†³FSMISæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸åªä¸ºæ”¯æŒå›¾åƒç”Ÿæˆä¸€ä¸ªå…¨å±€åŸå‹ï¼Œä»¥ä¸æŸ¥è¯¢å›¾åƒç›¸åŒ¹é…ï¼Œä»è€Œå¿½ç•¥äº†ç±»å†…å˜åŒ–ã€‚ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªå¼•å¯¼åŸå‹å¢å¼ºç½‘ç»œï¼ˆSPENetï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šçº§åŸå‹ç”Ÿæˆï¼ˆMPGï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡åŒæ—¶ç”Ÿæˆå…¨å±€åŸå‹å’Œè‡ªé€‚åº”æ•°é‡çš„å±€éƒ¨åŸå‹ï¼Œå®ç°äº†æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å¤šç²’åº¦åº¦é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ”¯æŒå›¾åƒä¸­çš„æ‰€æœ‰å±€éƒ¨åŸå‹å¹¶ä¸éƒ½æœ‰åˆ©äºåŒ¹é…ï¼Œå°¤å…¶æ˜¯åœ¨æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚æ—¶ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æŸ¥è¯¢å¼•å¯¼å±€éƒ¨åŸå‹å¢å¼ºï¼ˆQLPEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡èå…¥æŸ¥è¯¢å›¾åƒçš„æŒ‡å¯¼ä¿¡æ¯è‡ªé€‚åº”åœ°ä¼˜åŒ–æ”¯æŒåŸå‹ï¼Œä»è€Œå‡è½»äº†æ­¤ç±»å·®å¼‚å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSPENetä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02993v1">PDF</a> Accepted by MICCAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°æ–¹æ³•â€”â€”è‡ªæˆ‘å¼•å¯¼åŸå‹å¢å¼ºç½‘ç»œï¼ˆSPENetï¼‰ã€‚é’ˆå¯¹ç°æœ‰åŸå‹æ–¹æ³•å¿½ç•¥ç±»å†…å˜åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†å¤šçº§åˆ«åŸå‹ç”Ÿæˆæ¨¡å—ï¼Œèƒ½å¤Ÿç”Ÿæˆå…¨å±€åŸå‹å’Œè‡ªé€‚åº”æ•°é‡çš„å±€éƒ¨åŸå‹ï¼Œä»¥å®ç°å¤šç²’åº¦æµ‹é‡ã€‚åŒæ—¶ï¼Œä¸ºè§£å†³æ”¯æŒå›¾åƒä¸æŸ¥è¯¢å›¾åƒé—´å·®å¼‚å¯¼è‡´çš„é—®é¢˜ï¼Œæå‡ºäº†æŸ¥è¯¢å¼•å¯¼å±€éƒ¨åŸå‹å¢å¼ºæ¨¡å—ï¼Œé€šè¿‡èå…¥æŸ¥è¯¢å›¾åƒçš„å¼•å¯¼ä¿¡æ¯æ¥ä¼˜åŒ–æ”¯æŒåŸå‹ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSPENetä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå–å¾—äº†ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Medical Image Segmentation (FSMIS)çš„ç›®æ ‡æ˜¯ä»…ä½¿ç”¨å°‘é‡æ ‡è®°å›¾åƒå¯¹æ–°çš„åŒ»å­¦å¯¹è±¡è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>åŸå‹æ–¹æ³•å·²åœ¨FSMISä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†é€šå¸¸åªç”Ÿæˆå•ä¸€å…¨å±€åŸå‹ï¼Œå¿½ç•¥äº†ç±»å†…å˜åŒ–ã€‚</li>
<li>SPENeté€šè¿‡å¼•å…¥å¤šçº§åˆ«åŸå‹ç”Ÿæˆæ¨¡å—æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ¨¡å—å¯ä»¥åŒæ—¶ç”Ÿæˆå…¨å±€åŸå‹å’Œè‡ªé€‚åº”æ•°é‡çš„å±€éƒ¨åŸå‹ã€‚</li>
<li>åœ¨æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´å­˜åœ¨å·®å¼‚æ—¶ï¼Œå¹¶éæ‰€æœ‰å±€éƒ¨åŸå‹éƒ½æœ‰ç›ŠäºåŒ¹é…ã€‚</li>
<li>SPENeté€šè¿‡æŸ¥è¯¢å¼•å¯¼å±€éƒ¨åŸå‹å¢å¼ºæ¨¡å—æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ¨¡å—é€šè¿‡èå…¥æŸ¥è¯¢å›¾åƒçš„å¼•å¯¼ä¿¡æ¯æ¥ä¼˜åŒ–æ”¯æŒåŸå‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSPENetåœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>SPENetçš„å®ç°ç»“åˆäº†å…¨å±€å’Œå±€éƒ¨åŸå‹ï¼Œæœ‰æ•ˆåº”å¯¹äº†åŒ»å­¦å›¾åƒä¸­çš„å¤æ‚æ€§å’Œç±»å†…å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7314b58fae444ebe507db6dc94c2adc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a15f586450735b44153dc7c6a07eaa95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9ad05ab04be4c65e5d7b741a3b94e6a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Digital-Twin-for-Robotic-Post-Mortem-Tissue-Sampling-using-Virtual-Reality"><a href="#A-Digital-Twin-for-Robotic-Post-Mortem-Tissue-Sampling-using-Virtual-Reality" class="headerlink" title="A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual   Reality"></a>A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual   Reality</h2><p><strong>Authors:Maximilian Neidhardt, Ludwig Bosse, Vidas Raudonis, Kristina Allgoewer, Axel Heinemann, Benjamin Ondruschka, Alexander Schlaefer</strong></p>
<p>Studying tissue samples obtained during autopsies is the gold standard when diagnosing the cause of death and for understanding disease pathophysiology. Recently, the interest in post mortem minimally invasive biopsies has grown which is a less destructive approach in comparison to an open autopsy and reduces the risk of infection. While manual biopsies under ultrasound guidance are more widely performed, robotic post mortem biopsies have been recently proposed. This approach can further reduce the risk of infection for physicians. However, planning of the procedure and control of the robot need to be efficient and usable. We explore a virtual reality setup with a digital twin to realize fully remote planning and control of robotic post mortem biopsies. The setup is evaluated with forensic pathologists in a usability study for three interaction methods. Furthermore, we evaluate clinical feasibility and evaluate the system with three human cadavers. Overall, 132 needle insertions were performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue samples were successfully biopsied and histopathologically verified. Users reported a very intuitive needle placement approach, indicating that the system is a promising, precise, and low-risk alternative to conventional approaches. </p>
<blockquote>
<p>ç ”ç©¶åœ¨å°¸æ£€è¿‡ç¨‹ä¸­è·å¾—çš„ç»„ç»‡æ ·æœ¬æ˜¯é‡‘æ ‡å‡†ï¼Œåœ¨è¯Šæ–­æ­»äº¡åŸå› å’Œäº†è§£ç–¾ç—…ç—…ç†ç”Ÿç†å­¦æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ€è¿‘ï¼Œå¯¹æ­»åå¾®åˆ›æ´»æ£€çš„å…´è¶£æœ‰æ‰€å¢åŠ ï¼Œè¿™æ˜¯ä¸€ç§ä¸å¼€æ”¾å°¸æ£€ç›¸æ¯”ç ´åæ€§è¾ƒå°çš„æ–¹æ³•ï¼Œå¹¶é™ä½äº†æ„ŸæŸ“çš„é£é™©ã€‚è™½ç„¶è¶…å£°å¼•å¯¼ä¸‹æ‰‹åŠ¨æ´»æ£€æ›´ä¸ºæ™®éï¼Œä½†æ­»åæœºå™¨äººæ´»æ£€æœ€è¿‘å·²è¢«æå‡ºã€‚è¿™ç§æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥é™ä½åŒ»ç”Ÿæ„ŸæŸ“çš„é£é™©ã€‚ç„¶è€Œï¼Œç¨‹åºçš„è§„åˆ’å’Œæœºå™¨äººçš„æ§åˆ¶å¿…é¡»é«˜æ•ˆä¸”æ˜“äºä½¿ç”¨ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨æ•°å­—åŒèƒèƒçš„è™šæ‹Ÿç°å®è®¾ç½®ï¼Œä»¥å®ç°è¿œç¨‹è§„åˆ’å’Œæ§åˆ¶æ­»åæœºå™¨äººæ´»æ£€ã€‚è¯¥è®¾ç½®é€šè¿‡æ³•åŒ»ç—…ç†å­¦å®¶å¯¹ä¸‰ç§äº¤äº’æ–¹æ³•è¿›è¡Œäº†å¯ç”¨æ€§è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ä¸´åºŠå¯è¡Œæ€§ï¼Œå¹¶ç”¨ä¸‰å…·äººç±»é—ä½“å¯¹ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿›è¡Œäº†132æ¬¡é’ˆåˆºæ’å…¥ï¼Œç¦»è½´é’ˆæ”¾ç½®è¯¯å·®ä¸º5.30+-3.25æ¯«ç±³ã€‚æˆåŠŸè¿›è¡Œäº†ç»„ç»‡æ ·æœ¬æ´»æ£€å¹¶è¿›è¡Œç—…ç†ç»„ç»‡å­¦éªŒè¯ã€‚ç”¨æˆ·æŠ¥å‘Šäº†ä¸€ç§éå¸¸ç›´è§‚çš„é’ˆæ”¾ç½®æ–¹æ³•ï¼Œè¡¨æ˜è¯¥ç³»ç»Ÿæ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„ã€ç²¾ç¡®çš„ã€ä½é£é™©çš„æ›¿ä»£ä¼ ç»Ÿæ–¹æ³•çš„é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02760v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å°¸æ£€è¿‡ç¨‹ä¸­è·å–ç»„ç»‡æ ·æœ¬è¿›è¡Œç ”ç©¶æ˜¯è¯Šæ–­æ­»äº¡åŸå› å’Œäº†è§£ç–¾ç—…ç—…ç†ç”Ÿç†å­¦çš„é‡‘æ ‡å‡†ã€‚è¿‘æœŸï¼Œå¯¹å°¸æ£€å¾®åˆ›æ´»æ£€æŠ€æœ¯çš„å…´è¶£æ—¥ç›Šæµ“åšï¼Œä¸å¼€æ”¾æ€§å°¸æ£€ç›¸æ¯”ï¼Œè¿™æ˜¯ä¸€ç§ç ´åç¨‹åº¦è¾ƒä½çš„æ–¹æ³•ï¼Œé™ä½äº†æ„ŸæŸ“é£é™©ã€‚è™½ç„¶è¶…å£°å¼•å¯¼ä¸‹æ‰‹åŠ¨æ´»æ£€æ›´æ™®éï¼Œä½†æœºå™¨äººè¿›è¡Œå°¸æ£€æ´»æ£€çš„æ–¹æ³•ä¹Ÿå·²è¢«æå‡ºã€‚ä¸ºé™ä½åŒ»å¸ˆæ„ŸæŸ“é£é™©ï¼Œæœ¬æ–‡æ¢ç´¢äº†ä½¿ç”¨è™šæ‹Ÿç°å®è®¾ç½®å’Œæ•°å­—å­ªç”ŸæŠ€æœ¯å®ç°è¿œç¨‹è§„åˆ’å’Œæ§åˆ¶æœºå™¨äººè¿›è¡Œå°¸æ£€æ´»æ£€ã€‚é€šè¿‡æ³•åŒ»ç—…ç†å­¦å®¶è¿›è¡Œå¯ç”¨æ€§è¯„ä¼°ï¼Œå¹¶å¯¹ä¸‰ç§äº¤äº’æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¿˜è¯„ä¼°äº†ç³»ç»Ÿçš„ä¸´åºŠå¯è¡Œæ€§ï¼Œå¹¶åœ¨ä¸‰å…·äººç±»å°¸ä½“ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚æ€»ä½“ä¸Šï¼Œè¿›è¡Œäº†132æ¬¡é’ˆå¤´æ’å…¥æ“ä½œï¼Œç¦»è½´é’ˆå¤´æ”¾ç½®è¯¯å·®ä¸º5.30Â±3.25æ¯«ç±³ã€‚æˆåŠŸè¿›è¡Œäº†ç»„ç»‡æ ·æœ¬æ´»æ£€å¹¶è¿›è¡Œç»„ç»‡ç—…ç†å­¦éªŒè¯ã€‚ç”¨æˆ·æŠ¥å‘Šé’ˆå¤´æ”¾ç½®æ–¹æ³•éå¸¸ç›´è§‚ï¼Œè¡¨æ˜è¯¥ç³»ç»Ÿæ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„ã€ç²¾ç¡®çš„ã€ä½é£é™©çš„æ›¿ä»£ä¼ ç»Ÿæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°¸æ£€ä¸­ç ”ç©¶ç»„ç»‡æ ·æœ¬æ˜¯è¯Šæ–­æ­»äº¡åŸå› å’Œäº†è§£ç–¾ç—…ç—…ç†ç”Ÿç†å­¦çš„é‡‘æ ‡å‡†ã€‚</li>
<li>å¾®åˆ›æ´»æ£€æŠ€æœ¯æˆä¸ºå°¸æ£€çš„æ–°è¶‹åŠ¿ï¼Œä»¥é™ä½æ„ŸæŸ“é£é™©ã€‚</li>
<li>æœºå™¨äººè¿›è¡Œå°¸æ£€æ´»æ£€èƒ½å¤Ÿè¿›ä¸€æ­¥é™ä½åŒ»å¸ˆæ„ŸæŸ“é£é™©ã€‚</li>
<li>ä½¿ç”¨è™šæ‹Ÿç°å®è®¾ç½®å’Œæ•°å­—å­ªç”ŸæŠ€æœ¯å¯ä»¥å®ç°è¿œç¨‹è§„åˆ’å’Œæ§åˆ¶æœºå™¨äººè¿›è¡Œå°¸æ£€æ´»æ£€ã€‚</li>
<li>ç³»ç»Ÿå¯ç”¨æ€§å¾—åˆ°äº†æ³•åŒ»ç—…ç†å­¦å®¶çš„è¯„ä¼°ã€‚</li>
<li>åœ¨ä¸‰å…·äººç±»å°¸ä½“ä¸Šè¿›è¡Œäº†ç³»ç»Ÿçš„ä¸´åºŠå¯è¡Œæ€§è¯„ä¼°ï¼Œæ€»ä½“ä¸Šçš„é’ˆå¤´æ”¾ç½®è¯¯å·®è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-295b39693cdcaa11c3c00f81c7ed89b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-485ea2f663edfff7e4ea843d47c4963a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d6d2162a8bbc57872267168646acd0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b81e1d2cb759a74e3f6f6289ce12638.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e2bce3f6e3215a90e5427c8fce1fffc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0725d28f7b711dd6a57a7af7e4e7758e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f22a4970a9781d6c3dc73c915037a5ef.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Mix-modal-Federated-Learning-for-MRI-Image-Segmentation"><a href="#Mix-modal-Federated-Learning-for-MRI-Image-Segmentation" class="headerlink" title="Mix-modal Federated Learning for MRI Image Segmentation"></a>Mix-modal Federated Learning for MRI Image Segmentation</h2><p><strong>Authors:Guyue Hu, Siyuan Song, Jingpeng Sun, Zhe Jin, Chenglong Li, Jin Tang</strong></p>
<p>Magnetic resonance imaging (MRI) image segmentation is crucial in diagnosing and treating many diseases, such as brain tumors. Existing MRI image segmentation methods mainly fall into a centralized multimodal paradigm, which is inapplicable in engineering non-centralized mix-modal medical scenarios. In this situation, each distributed client (hospital) processes multiple mixed MRI modalities, and the modality set and image data for each client are diverse, suffering from extensive client-wise modality heterogeneity and data heterogeneity. In this paper, we first formulate non-centralized mix-modal MRI image segmentation as a new paradigm for federated learning (FL) that involves multiple modalities, called mix-modal federated learning (MixMFL). It distinguishes from existing multimodal federating learning (MulMFL) and cross-modal federating learning (CroMFL) paradigms. Then, we proposed a novel modality decoupling and memorizing mix-modal federated learning framework (MDM-MixMFL) for MRI image segmentation, which is characterized by a modality decoupling strategy and a modality memorizing mechanism. Specifically, the modality decoupling strategy disentangles each modality into modality-tailored and modality-shared information. During mix-modal federated updating, corresponding modality encoders undergo tailored and shared updating, respectively. It facilitates stable and adaptive federating aggregation of heterogeneous data and modalities from distributed clients. Besides, the modality memorizing mechanism stores client-shared modality prototypes dynamically refreshed from every modality-tailored encoder to compensate for incomplete modalities in each local client. It further benefits modality aggregation and fusion processes during mixmodal federated learning. Extensive experiments on two public datasets for MRI image segmentation demonstrate the effectiveness and superiority of our methods. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒåˆ†å‰²åœ¨è¯Šæ–­å’Œæ²»ç–—è®¸å¤šç–¾ç—…ï¼ˆå¦‚è„‘è‚¿ç˜¤ï¼‰ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç°æœ‰çš„MRIå›¾åƒåˆ†å‰²æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¸€ä¸ªé›†ä¸­çš„å¤šæ¨¡æ€èŒƒå¼ä¸Šï¼Œè¿™åœ¨å·¥ç¨‹éé›†ä¸­æ··åˆæ¨¡æ€åŒ»ç–—åœºæ™¯ä¸­å¹¶ä¸é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªåˆ†å¸ƒå¼å®¢æˆ·ç«¯ï¼ˆåŒ»é™¢ï¼‰å¤„ç†å¤šç§æ··åˆMRIæ¨¡æ€ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯çš„æ¨¡æ€é›†å’Œå›¾åƒæ•°æ®å„ä¸ç›¸åŒï¼Œé¢ä¸´ç€å¹¿æ³›çš„å®¢æˆ·ç«¯æ¨¡æ€å¼‚è´¨æ€§å’Œæ•°æ®å¼‚è´¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ¶å®šäº†ä¸€ç§æ–°çš„è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰èŒƒå¼ï¼Œç”¨äºéé›†ä¸­æ··åˆæ¨¡æ€MRIå›¾åƒåˆ†å‰²ï¼Œç§°ä¸ºæ··åˆæ¨¡æ€è”é‚¦å­¦ä¹ ï¼ˆMixMFLï¼‰ã€‚å®ƒä¸ç°æœ‰çš„å¤šæ¨¡æ€è”é‚¦å­¦ä¹ ï¼ˆMulMFLï¼‰å’Œè·¨æ¨¡æ€è”é‚¦å­¦ä¹ ï¼ˆCroMFLï¼‰èŒƒå¼æœ‰æ‰€ä¸åŒã€‚ç„¶åï¼Œæˆ‘ä»¬é’ˆå¯¹MRIå›¾åƒåˆ†å‰²æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨¡æ€è§£è€¦å’Œè®°å¿†æ··åˆæ¨¡æ€è”é‚¦å­¦ä¹ æ¡†æ¶ï¼ˆMDM-MixMFLï¼‰ï¼Œå…¶ç‰¹ç‚¹æ˜¯å…·æœ‰æ¨¡æ€è§£è€¦ç­–ç•¥å’Œæ¨¡æ€è®°å¿†æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡æ€è§£è€¦ç­–ç•¥å°†æ¯ä¸ªæ¨¡æ€åˆ†è§£ä¸ºæ¨¡æ€å®šåˆ¶å’Œæ¨¡æ€å…±äº«çš„ä¿¡æ¯ã€‚åœ¨æ··åˆæ¨¡æ€è”é‚¦æ›´æ–°è¿‡ç¨‹ä¸­ï¼Œç›¸åº”çš„æ¨¡æ€ç¼–ç å™¨åˆ†åˆ«è¿›è¡Œå®šåˆ¶å’Œå…±äº«æ›´æ–°ã€‚å®ƒä¿ƒè¿›äº†æ¥è‡ªåˆ†å¸ƒå¼å®¢æˆ·ç«¯çš„å¼‚æ„æ•°æ®å’Œæ¨¡æ€çš„ç¨³å®šå’Œè‡ªé€‚åº”è”é‚¦èšåˆã€‚æ­¤å¤–ï¼Œæ¨¡æ€è®°å¿†æœºåˆ¶å­˜å‚¨æ¥è‡ªæ¯ä¸ªæ¨¡æ€å®šåˆ¶ç¼–ç å™¨çš„åŠ¨æ€åˆ·æ–°å®¢æˆ·ç«¯å…±äº«æ¨¡æ€åŸå‹ï¼Œä»¥è¡¥å¿æ¯ä¸ªæœ¬åœ°å®¢æˆ·ç«¯çš„ä¸å®Œæ•´æ¨¡æ€ã€‚è¿™è¿›ä¸€æ­¥æœ‰åˆ©äºæ··åˆæ¨¡æ€è”é‚¦å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¨¡æ€èšåˆå’Œèåˆè¿‡ç¨‹ã€‚åœ¨MRIå›¾åƒåˆ†å‰²é¢†åŸŸçš„ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02541v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åœ¨éé›†ä¸­å¼çš„æ··åˆæ¨¡æ€åŒ»ç–—åœºæ™¯ä¸­ï¼ŒMRIå›¾åƒåˆ†å‰²é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„æ··åˆæ¨¡æ€è”é‚¦å­¦ä¹ ï¼ˆMixMFLï¼‰èŒƒå¼ï¼Œç”¨äºè§£å†³éé›†ä¸­å¼çš„æ··åˆæ¨¡æ€MRIå›¾åƒåˆ†å‰²é—®é¢˜ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨¡æ€è§£è€¦å’Œè®°å¿†æ··åˆæ¨¡æ€è”é‚¦å­¦ä¹ æ¡†æ¶ï¼ˆMDM-MixMFLï¼‰ï¼Œè¯¥æ¡†æ¶å…·æœ‰æ¨¡æ€è§£è€¦ç­–ç•¥å’Œæ¨¡æ€è®°å¿†æœºåˆ¶ï¼Œå¯æœ‰æ•ˆå¤„ç†æ¥è‡ªåˆ†å¸ƒå¼å®¢æˆ·ç«¯çš„å¼‚æ„æ•°æ®å’Œæ¨¡æ€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIå›¾åƒåˆ†å‰²åœ¨è®¸å¤šç–¾ç—…è¯Šæ–­å’Œæ²»ç–—ä¸­è‡³å…³é‡è¦ï¼Œå¦‚è„‘è‚¿ç˜¤ã€‚</li>
<li>ç°æœ‰MRIå›¾åƒåˆ†å‰²æ–¹æ³•ä¸»è¦åŸºäºé›†ä¸­åŒ–çš„å¤šæ¨¡æ€èŒƒå¼ï¼Œä¸é€‚ç”¨äºå·¥ç¨‹ä¸­çš„éé›†ä¸­åŒ–æ··åˆæ¨¡æ€åŒ»ç–—åœºæ™¯ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„æ··åˆæ¨¡æ€è”é‚¦å­¦ä¹ ï¼ˆMixMFLï¼‰èŒƒå¼ï¼Œç”¨äºå¤„ç†éé›†ä¸­åŒ–çš„æ··åˆæ¨¡æ€MRIå›¾åƒåˆ†å‰²ã€‚</li>
<li>MDM-MixMFLæ¡†æ¶å…·æœ‰æ¨¡æ€è§£è€¦ç­–ç•¥å’Œæ¨¡æ€è®°å¿†æœºåˆ¶ï¼Œå¯å¤„ç†æ¥è‡ªåˆ†å¸ƒå¼å®¢æˆ·ç«¯çš„å¼‚æ„æ•°æ®å’Œæ¨¡æ€ã€‚</li>
<li>æ¨¡æ€è§£è€¦ç­–ç•¥å°†æ¯ä¸ªæ¨¡æ€åˆ†è§£ä¸ºæ¨¡æ€ç‰¹å®šå’Œæ¨¡æ€å…±äº«çš„ä¿¡æ¯ï¼Œä¾¿äºåœ¨æ··åˆæ¨¡æ€è”é‚¦æ›´æ–°ä¸­è¿›è¡Œé’ˆå¯¹æ€§çš„æ›´æ–°ã€‚</li>
<li>æ¨¡æ€è®°å¿†æœºåˆ¶æœ‰åŠ©äºè¡¥å¿æœ¬åœ°å®¢æˆ·ç«¯çš„ä¸å®Œæ•´æ¨¡æ€ï¼Œå¹¶è¿›ä¸€æ­¥ä¿ƒè¿›æ¨¡æ€èšåˆå’Œèåˆè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-36fa0129b0c9d72d8dc4dfee8dbec1db.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86e51d50fcbc64b0259b395730f10d7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de7f0908d950ab3c136a755ec063725c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa6db68a56223817ff7b9e4e5000d6ac.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Anisotropic-Fourier-Features-for-Positional-Encoding-in-Medical-Imaging"><a href="#Anisotropic-Fourier-Features-for-Positional-Encoding-in-Medical-Imaging" class="headerlink" title="Anisotropic Fourier Features for Positional Encoding in Medical Imaging"></a>Anisotropic Fourier Features for Positional Encoding in Medical Imaging</h2><p><strong>Authors:Nabil Jabareen, Dongsheng Yuan, Dingming Liu, Foo-Wei Ten, SÃ¶ren Lukassen</strong></p>
<p>The adoption of Transformer-based architectures in the medical domain is growing rapidly. In medical imaging, the analysis of complex shapes - such as organs, tissues, or other anatomical structures - combined with the often anisotropic nature of high-dimensional images complicates these adaptations. In this study, we critically examine the role of Positional Encodings (PEs), arguing that commonly used approaches may be suboptimal for the specific challenges of medical imaging. Sinusoidal Positional Encodings (SPEs) have proven effective in vision tasks, but they struggle to preserve Euclidean distances in higher-dimensional spaces. Isotropic Fourier Feature Positional Encodings (IFPEs) have been proposed to better preserve Euclidean distances, but they lack the ability to account for anisotropy in images. To address these limitations, we propose Anisotropic Fourier Feature Positional Encoding (AFPE), a generalization of IFPE that incorporates anisotropic, class-specific, and domain-specific spatial dependencies. We systematically benchmark AFPE against commonly used PEs on multi-label classification in chest X-rays, organ classification in CT images, and ejection fraction regression in echocardiography. Our results demonstrate that choosing the correct PE can significantly improve model performance. We show that the optimal PE depends on the shape of the structure of interest and the anisotropy of the data. Finally, our proposed AFPE significantly outperforms state-of-the-art PEs in all tested anisotropic settings. We conclude that, in anisotropic medical images and videos, it is of paramount importance to choose an anisotropic PE that fits the data and the shape of interest. </p>
<blockquote>
<p>åœ¨åŒ»å­¦é¢†åŸŸé‡‡ç”¨åŸºäºTransformerçš„æ¶æ„æ­£åœ¨è¿…é€Ÿå¢é•¿ã€‚åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œå¯¹å™¨å®˜ã€ç»„ç»‡æˆ–å…¶ä»–è§£å‰–ç»“æ„ç­‰å¤æ‚å½¢çŠ¶çš„åˆ†æï¼Œä»¥åŠé«˜ç»´å›¾åƒçš„å¸¸å„å‘å¼‚æ€§æ€§è´¨ï¼Œä½¿è¿™äº›é€‚åº”å¤æ‚åŒ–ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†ä½ç½®ç¼–ç ï¼ˆPEsï¼‰çš„ä½œç”¨ï¼Œè®¤ä¸ºå¸¸ç”¨çš„æ–¹æ³•å¯èƒ½ä¸é€‚ç”¨äºåŒ»å­¦æˆåƒæ‰€é¢ä¸´çš„ç‰¹å®šæŒ‘æˆ˜ã€‚æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆSPEsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†åœ¨é«˜ç»´ç©ºé—´ä¸­éš¾ä»¥ä¿æŒæ¬§å‡ é‡Œå¾—è·ç¦»ã€‚å„å‘åŒæ€§å‚…é‡Œå¶ç‰¹å¾ä½ç½®ç¼–ç ï¼ˆIFPEsï¼‰èƒ½æ›´å¥½åœ°ä¿æŒæ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œä½†å®ƒä»¬æ— æ³•è€ƒè™‘å›¾åƒçš„å„å‘å¼‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å„å‘å¼‚æ€§å‚…é‡Œå¶ç‰¹å¾ä½ç½®ç¼–ç ï¼ˆAFPEï¼‰ï¼Œå®ƒæ˜¯IFPEçš„ä¸€ç§æ¨å¹¿ï¼Œç»“åˆäº†å„å‘å¼‚æ€§ã€ç‰¹å®šç±»åˆ«å’Œç‰¹å®šé¢†åŸŸçš„ç©ºé—´ä¾èµ–æ€§ã€‚æˆ‘ä»¬åœ¨èƒ¸éƒ¨Xå°„çº¿çš„å¤šæ ‡ç­¾åˆ†ç±»ã€CTå›¾åƒçš„å™¨å®˜åˆ†ç±»å’Œè¶…å£°å¿ƒåŠ¨å›¾çš„å°„è¡€åˆ†æ•°å›å½’ç­‰æ–¹é¢ç³»ç»Ÿåœ°è¯„ä¼°äº†AFPEä¸å¸¸ç”¨PEsçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€‰æ‹©æ­£ç¡®çš„PEå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬è¯æ˜äº†æœ€ä½³PEå–å†³äºæ‰€å…³æ³¨ç»“æ„çš„å¤–å½¢å’Œæ•°æ®çš„å„å‘å¼‚æ€§ã€‚æœ€åï¼Œåœ¨æ‰€æœ‰çš„å„å‘å¼‚æ€§æµ‹è¯•ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„AFPEæ˜¾è‘—ä¼˜äºæœ€æ–°çš„PEsã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œåœ¨å„å‘å¼‚æ€§çš„åŒ»å­¦å›¾åƒå’Œè§†é¢‘ä¸­ï¼Œé€‰æ‹©é€‚åˆæ•°æ®å’Œå…³æ³¨å½¢çŠ¶çš„å„å‘å¼‚æ€§PEè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02488v1">PDF</a> 13 pages, 3 figures, 2 tables, to be published in ShapeMI MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨åŒ»å­¦é¢†åŸŸï¼Œé‡‡ç”¨åŸºäºTransformerçš„æ¶æ„è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œåˆ†æå¤æ‚çš„å½¢çŠ¶ï¼ˆå¦‚å™¨å®˜ã€ç»„ç»‡æˆ–å…¶ä»–è§£å‰–ç»“æ„ï¼‰ä¸å›¾åƒçš„é«˜ç»´æ€§ç»“åˆä½¿å¾—è¿™äº›é€‚åº”æ€§é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¯¹ä½ç½®ç¼–ç ï¼ˆPEsï¼‰çš„ä½œç”¨è¿›è¡Œäº†æ‰¹åˆ¤æ€§è¯„ä¼°ï¼Œè®¤ä¸ºå¸¸ç”¨æ–¹æ³•å¯èƒ½ä¸é€‚ç”¨äºåŒ»å­¦æˆåƒæ‰€é¢ä¸´çš„ç‰¹å®šæŒ‘æˆ˜ã€‚æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆSPEsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é«˜ç»´ç©ºé—´ä¸­éš¾ä»¥ä¿æŒæ¬§å‡ é‡Œå¾—è·ç¦»ã€‚ç­‰è·å‚…é‡Œå¶ç‰¹å¾ä½ç½®ç¼–ç ï¼ˆIFPEsï¼‰èƒ½æ›´å¥½åœ°ä¿æŒæ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œä½†å¿½ç•¥äº†å›¾åƒçš„å„å‘å¼‚æ€§ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å„å‘å¼‚æ€§å‚…é‡Œå¶ç‰¹å¾ä½ç½®ç¼–ç ï¼ˆAFPEï¼‰ï¼Œå®ƒæ˜¯IFPEçš„ä¸€ç§æ¨å¹¿ï¼Œç»“åˆäº†å„å‘å¼‚æ€§ã€ç±»åˆ«ç‰¹å®šå’Œé¢†åŸŸç‰¹å®šçš„ç©ºé—´ä¾èµ–æ€§ã€‚æˆ‘ä»¬åœ¨èƒ¸éƒ¨Xå°„çº¿çš„å¤šæ ‡ç­¾åˆ†ç±»ã€CTå›¾åƒçš„å™¨å®˜åˆ†ç±»å’Œè¶…å£°å¿ƒåŠ¨å›¾çš„å¿ƒåŠŸèƒ½åˆ†çº§ç­‰æ–¹é¢ç³»ç»Ÿåœ°è¯„ä¼°äº†AFPEä¸å¸¸ç”¨PEçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œé€‰æ‹©æ­£ç¡®çš„PEå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚æœ€ä½³PEçš„é€‰æ‹©å–å†³äºå…³æ³¨ç»“æ„çš„å½¢çŠ¶å’Œæ•°æ®å„å‘å¼‚æ€§ã€‚æœ€åï¼Œåœ¨æ‰€æœ‰çš„æµ‹è¯•å„å‘å¼‚æ€§è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„AFPEå‡æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„PEsã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œåœ¨å„å‘å¼‚æ€§çš„åŒ»å­¦å›¾åƒå’Œè§†é¢‘ä¸­ï¼Œé€‰æ‹©é€‚åˆæ•°æ®å’Œå…³æ³¨å½¢çŠ¶çš„å„å‘å¼‚æ€§PEè‡³å…³é‡è¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Transformeræ¶æ„åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨æ­£åœ¨å¿«é€Ÿå¢é•¿ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦æˆåƒé¢†åŸŸã€‚</li>
<li>ä½ç½®ç¼–ç ï¼ˆPEsï¼‰åœ¨åŒ»å­¦æˆåƒä¸­çš„é€‰æ‹©è‡³å…³é‡è¦ï¼Œå› ä¸ºå›¾åƒå¤æ‚æ€§å¯¹æ¨¡å‹æ€§èƒ½æœ‰å½±å“ã€‚</li>
<li>å¸¸è§ä½ç½®ç¼–ç æ–¹æ³•å¯èƒ½åœ¨å¤„ç†åŒ»å­¦å›¾åƒçš„ç‰¹å®šæŒ‘æˆ˜æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>å„å‘å¼‚æ€§å‚…é‡Œå¶ç‰¹å¾ä½ç½®ç¼–ç ï¼ˆAFPEï¼‰æ˜¯ä¸€ç§æ–°å‹ç¼–ç æ–¹æ³•ï¼Œèƒ½å¤Ÿç»“åˆå„å‘å¼‚æ€§ã€ç±»åˆ«ç‰¹å®šå’Œé¢†åŸŸç‰¹å®šçš„ç©ºé—´ä¾èµ–æ€§ã€‚</li>
<li>AFPEåœ¨å¤šç§åŒ»å­¦æˆåƒä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–ä½ç½®ç¼–ç æ–¹æ³•ã€‚</li>
<li>é€‰æ‹©æœ€ä½³çš„ä½ç½®ç¼–ç å–å†³äºå…³æ³¨çš„ç»“æ„å½¢çŠ¶å’Œæ•°æ®çš„å„å‘å¼‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5e596c25f03f0c938911f88f341ad5be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d37bf795532a87ab879f7ba446bae2e1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="From-Noisy-Labels-to-Intrinsic-Structure-A-Geometric-Structural-Dual-Guided-Framework-for-Noise-Robust-Medical-Image-Segmentation"><a href="#From-Noisy-Labels-to-Intrinsic-Structure-A-Geometric-Structural-Dual-Guided-Framework-for-Noise-Robust-Medical-Image-Segmentation" class="headerlink" title="From Noisy Labels to Intrinsic Structure: A Geometric-Structural   Dual-Guided Framework for Noise-Robust Medical Image Segmentation"></a>From Noisy Labels to Intrinsic Structure: A Geometric-Structural   Dual-Guided Framework for Noise-Robust Medical Image Segmentation</h2><p><strong>Authors:Tao Wang, Zhenxuan Zhang, Yuanbo Zhou, Xinlin Zhang, Yuanbin Chen, Tao Tan, Guang Yang, Tong Tong</strong></p>
<p>The effectiveness of convolutional neural networks in medical image segmentation relies on large-scale, high-quality annotations, which are costly and time-consuming to obtain. Even expert-labeled datasets inevitably contain noise arising from subjectivity and coarse delineations, which disrupt feature learning and adversely impact model performance. To address these challenges, this study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which integrates geometric and structural cues to improve robustness against noisy annotations. It incorporates a Geometric Distance-Aware module that dynamically adjusts pixel-level weights using geometric features, thereby strengthening supervision in reliable regions while suppressing noise. A Structure-Guided Label Refinement module further refines labels with structural priors, and a Knowledge Transfer module enriches supervision and improves sensitivity to local details. To comprehensively assess its effectiveness, we evaluated GSD-Net on six publicly available datasets: four containing three types of simulated label noise, and two with multi-expert annotations that reflect real-world subjectivity and labeling inconsistencies. Experimental results demonstrate that GSD-Net achieves state-of-the-art performance under noisy annotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen, 8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of this study are available at <a target="_blank" rel="noopener" href="https://github.com/ortonwang/GSD-Net">https://github.com/ortonwang/GSD-Net</a>. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ä¾èµ–äºå¤§è§„æ¨¡çš„é«˜è´¨é‡æ ‡æ³¨ï¼Œè€Œè¿™äº›æ ‡æ³¨çš„è·å–æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚å³ä½¿æ˜¯ä¸“å®¶æ ‡æ³¨çš„æ•°æ®é›†ä¹Ÿä¸å¯é¿å…åœ°åŒ…å«ç”±äºä¸»è§‚æ€§å’Œç²—ç•¥æç»˜è€Œäº§ç”Ÿçš„å™ªå£°ï¼Œè¿™äº›å™ªå£°ä¼šç ´åç‰¹å¾å­¦ä¹ å¹¶å¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å‡ ä½•ç»“æ„åŒé‡å¼•å¯¼ç½‘ç»œï¼ˆGSD-Netï¼‰ï¼Œå®ƒèåˆäº†å‡ ä½•å’Œç»“æ„çº¿ç´¢ï¼Œä»¥æé«˜å¯¹å™ªå£°æ ‡æ³¨çš„ç¨³å¥æ€§ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªå‡ ä½•è·ç¦»æ„ŸçŸ¥æ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨å‡ ä½•ç‰¹å¾åŠ¨æ€è°ƒæ•´åƒç´ çº§æƒé‡ï¼Œä»è€Œåœ¨å¯é çš„åŒºåŸŸåŠ å¼ºç›‘ç£ï¼ŒåŒæ—¶æŠ‘åˆ¶å™ªå£°ã€‚ç»“æ„å¼•å¯¼æ ‡ç­¾ç»†åŒ–æ¨¡å—è¿›ä¸€æ­¥ä½¿ç”¨ç»“æ„å…ˆéªŒæ¥ç»†åŒ–æ ‡ç­¾ï¼ŒçŸ¥è¯†ä¼ é€’æ¨¡å—ä¸°å¯Œäº†ç›‘ç£å¹¶æé«˜äº†å¯¹å±€éƒ¨ç»†èŠ‚çš„æ•æ„Ÿåº¦ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°å…¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨å…­ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¯„ä¼°äº†GSD-Netçš„æ€§èƒ½ï¼šå››ä¸ªæ•°æ®é›†åŒ…å«ä¸‰ç§æ¨¡æ‹Ÿæ ‡ç­¾å™ªå£°ï¼Œä¸¤ä¸ªæ•°æ®é›†å…·æœ‰å¤šä¸“å®¶æ³¨é‡Šï¼Œåæ˜ äº†ç°å®ä¸–ç•Œä¸­çš„ä¸»è§‚æ€§å’Œæ ‡ç­¾ä¸ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å™ªå£°æ ‡æ³¨ä¸‹ï¼ŒGSD-Netè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨Kvasirä¸Šæé«˜äº†2.52%ï¼Œåœ¨Shenzhenä¸Šæé«˜äº†22.76%ï¼Œåœ¨BU-SUCä¸Šæé«˜äº†8.87%ï¼Œåœ¨BraTS2020çš„SRæ¨¡æ‹Ÿå™ªå£°ä¸‹æé«˜äº†4.59%ã€‚è¯¥ç ”ç©¶çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ortonwang/GSD-Net%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ortonwang/GSD-Netè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02419v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡é«˜è´¨é‡æ³¨é‡Šçš„å·ç§¯ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§æ˜¯å»ºç«‹åœ¨å…¶æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶çš„è·å–è¿‡ç¨‹ä¹‹ä¸Šçš„ã€‚ä¸“å®¶æ ‡æ³¨çš„æ•°æ®é›†ä¸å¯é¿å…åœ°å­˜åœ¨ç”±äºä¸»è§‚æ€§å’Œç²—ç•¥æç»˜è€Œäº§ç”Ÿçš„å™ªå£°ï¼Œè¿™ç ´åäº†ç‰¹å¾å­¦ä¹ å¹¶å¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§å‡ ä½•ç»“æ„åŒé‡å¼•å¯¼ç½‘ç»œï¼ˆGSD-Netï¼‰ï¼Œå®ƒæ•´åˆå‡ ä½•å’Œç»“æ„çº¿ç´¢ï¼Œæé«˜å¯¹æŠ—å™ªå£°æ³¨é‡Šçš„ç¨³å¥æ€§ã€‚é€šè¿‡å‡ ä½•è·ç¦»æ„ŸçŸ¥æ¨¡å—åŠ¨æ€è°ƒæ•´åƒç´ çº§æƒé‡ï¼Œåˆ©ç”¨ç»“æ„å…ˆéªŒè¿›ä¸€æ­¥ç»†åŒ–æ ‡ç­¾ï¼Œå¹¶ä¸°å¯Œç›‘ç£å’Œæé«˜å¯¹å±€éƒ¨ç»†èŠ‚çš„æ•æ„Ÿæ€§ã€‚åœ¨å…­ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒGSD-Netåœ¨å™ªå£°æ ‡æ³¨ä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œåœ¨Kvasirã€Shenzhenã€BU-SUCå’ŒBraTS2020ç­‰æ•°æ®é›†ä¸Šçš„æ”¹è¿›ç‡åˆ†åˆ«ä¸º2.52%ã€22.76%ã€8.87%å’Œ4.59%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·ç§¯ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ä¾èµ–äºå¤§è§„æ¨¡é«˜è´¨é‡æ³¨é‡Šï¼Œä½†è·å–è¿™äº›æ³¨é‡Šæˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>ä¸“å®¶æ ‡æ³¨çš„æ•°æ®é›†å­˜åœ¨ç”±äºä¸»è§‚æ€§å’Œç²—ç•¥æç»˜è€Œäº§ç”Ÿçš„å™ªå£°ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºGSD-Netï¼Œé€šè¿‡æ•´åˆå‡ ä½•å’Œç»“æ„çº¿ç´¢ï¼Œæé«˜å¯¹æŠ—å™ªå£°æ³¨é‡Šçš„ç¨³å¥æ€§ã€‚</li>
<li>GSD-NetåŒ…å«å‡ ä½•è·ç¦»æ„ŸçŸ¥æ¨¡å—ã€ç»“æ„å¼•å¯¼æ ‡ç­¾ç»†åŒ–æ¨¡å—å’ŒçŸ¥è¯†è½¬ç§»æ¨¡å—ã€‚</li>
<li>å‡ ä½•è·ç¦»æ„ŸçŸ¥æ¨¡å—èƒ½åŠ¨æ€è°ƒæ•´åƒç´ çº§æƒé‡ï¼Œå¼ºåŒ–å¯é åŒºåŸŸçš„ç›‘ç£ï¼ŒæŠ‘åˆ¶å™ªå£°ã€‚</li>
<li>ç»“æ„å¼•å¯¼æ ‡ç­¾ç»†åŒ–æ¨¡å—åˆ©ç”¨ç»“æ„å…ˆéªŒç»†åŒ–æ ‡ç­¾ã€‚</li>
<li>GSD-Netåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œå±•ç°å‡ºå¯¹å™ªå£°æ ‡æ³¨çš„é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-67021406430bc13562f9fb4655692ead.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c625bb6432f39e5f2696e96087fdc329.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-098dbfa0508b135062a80ed6e8ee1392.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbeaf2f742c9f5c92ea20147aa829954.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56f215570a5dcfa786670a6e992f4c95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03c5d4d1e1a9e72552e48267679ae2b4.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation"><a href="#MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation" class="headerlink" title="MedDINOv3: How to adapt vision foundation models for medical image   segmentation?"></a>MedDINOv3: How to adapt vision foundation models for medical image   segmentation?</h2><p><strong>Authors:Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang</strong></p>
<p>Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ricklisz/MedDINOv3">https://github.com/ricklisz/MedDINOv3</a>. </p>
<blockquote>
<p>åœ¨CTå’ŒMRIæ‰«æä¸­ï¼Œå™¨å®˜å’Œè‚¿ç˜¤çš„ç²¾ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ è™½ç„¶æ¨åŠ¨äº†è‡ªåŠ¨åŒ–åˆ†å‰²æŠ€æœ¯çš„å‘å±•ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä»ç„¶æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼Œåœ¨ä¸åŒæ¨¡æ€å’Œæœºæ„ä¹‹é—´ç¼ºä¹é€šç”¨æ€§ã€‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰ï¼Œåœ¨ç™¾äº¿çº§è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæä¾›äº†å¼ºå¤§ä¸”å¯è¿ç§»çš„è¡¨ç¤ºèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†å…¶é€‚åº”åŒ»å­¦æˆåƒé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¤§å¤šæ•°åŸºç¡€æ¨¡å‹çš„ViTéª¨å¹²ç½‘åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ä»ä¸å¦‚ä¸“ä¸šåŒ–çš„CNNè¡¨ç°è‰¯å¥½ï¼›ï¼ˆ2ï¼‰è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·é™åˆ¶äº†å¯è¿ç§»æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MedDINOv3ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºå°†DINOv3é€‚åº”åŒ»å­¦åˆ†å‰²ã€‚æˆ‘ä»¬é¦–å…ˆå›é¡¾äº†ç®€å•çš„ViTsï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å…·æœ‰å¤šå°ºåº¦ä»¤ç‰Œèšåˆçš„æ¶æ„ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨CT-3Mï¼ˆä¸€ä¸ªç²¾é€‰çš„åŒ…å«387ä¸‡å¼ è½´å‘CTåˆ‡ç‰‡çš„é›†åˆï¼‰ä¸Šè¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œé‡‡ç”¨å¤šé˜¶æ®µçš„DINOv3é…æ–¹æ¥å­¦ä¹ ç¨³å¥çš„å¯†é›†ç‰¹å¾ã€‚MedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æˆ–è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ç»Ÿä¸€éª¨å¹²çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ricklisz/MedDINOv3%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ricklisz/MedDINOv3è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02379v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MedDINOv3æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†DINOv3é€‚åº”äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚é€šè¿‡é‡æ–°å®¡è§†æ™®é€šViTså¹¶è®¾è®¡ç®€æ´æœ‰æ•ˆçš„å¤šå°ºåº¦ä»¤ç‰Œèšåˆæ¶æ„ï¼Œä»¥åŠä½¿ç”¨CT-3Mä¸Šçš„åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼ŒMedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æˆ–è¶…è¶Šäº†æœ€æ–°æ€§èƒ½ï¼Œè¯æ˜äº†è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ç»Ÿä¸€éª¨å¹²ç½‘çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä¸­å™¨å®˜å’Œè‚¿ç˜¤çš„ç²¾ç¡®åˆ†å‰²å¯¹è¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å·²æ¨è¿›äº†è‡ªåŠ¨åŒ–åˆ†å‰²ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä»ç¼ºä¹è·¨æ¨¡æ€å’Œæœºæ„çš„é€šç”¨æ€§ã€‚</li>
<li>è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨åŒ»å­¦æˆåƒä¸­é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šViTä¸»å¹²åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šæ€§èƒ½ä¸è¶³ä»¥åŠè‡ªç„¶ä¸åŒ»å­¦å›¾åƒä¹‹é—´çš„åŸŸå·®è·é™åˆ¶äº†å¯è½¬ç§»æ€§ã€‚</li>
<li>MedDINOv3æ¡†æ¶æ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡é€‚åº”DINOv3è‡³åŒ»å­¦åˆ†å‰²é¢†åŸŸã€‚</li>
<li>MedDINOv3è®¾è®¡äº†ä¸€ç§ç®€æ´æœ‰æ•ˆçš„å¤šå°ºåº¦ä»¤ç‰Œèšåˆæ¶æ„ï¼Œå¹¶ä½¿ç”¨äº†CT-3Mæ•°æ®é›†ä¸Šçš„åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒã€‚</li>
<li>MedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fb0243cd38ff90ab236674e41771cda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69eb7b8f98c17f7f1a653c65dd53dc9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2e2cb37fabb471dedd94f68ca9e27b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f3f2b82643e7fec0a1229c41ce5fb1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfe3be72dd8124edfca2d135d61d1066.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Continuous-Encoding-Based-Representation-for-Efficient-Multi-Fidelity-Multi-Objective-Neural-Architecture-Search"><a href="#A-Continuous-Encoding-Based-Representation-for-Efficient-Multi-Fidelity-Multi-Objective-Neural-Architecture-Search" class="headerlink" title="A Continuous Encoding-Based Representation for Efficient Multi-Fidelity   Multi-Objective Neural Architecture Search"></a>A Continuous Encoding-Based Representation for Efficient Multi-Fidelity   Multi-Objective Neural Architecture Search</h2><p><strong>Authors:Zhao Wei, Chin Chun Ooi, Yew-Soon Ong</strong></p>
<p>Neural architecture search (NAS) is an attractive approach to automate the design of optimized architectures but is constrained by high computational budget, especially when optimizing for multiple, important conflicting objectives. To address this, an adaptive Co-Kriging-assisted multi-fidelity multi-objective NAS algorithm is proposed to further reduce the computational cost of NAS by incorporating a clustering-based local multi-fidelity infill sampling strategy, enabling efficient exploration of the search space for faster convergence. This algorithm is further accelerated by the use of a novel continuous encoding method to represent the connections of nodes in each cell within a generalized cell-based U-Net backbone, thereby decreasing the search dimension (number of variables). Results indicate that the proposed NAS algorithm outperforms previously published state-of-the-art methods under limited computational budget on three numerical benchmarks, a 2D Darcy flow regression problem and a CHASE_DB1 biomedical image segmentation problem. The proposed method is subsequently used to create a wind velocity regression model with application in urban modelling, with the found model able to achieve good prediction with less computational complexity. Further analysis revealed that the NAS algorithm independently identified principles undergirding superior U-Net architectures in other literature, such as the importance of allowing each cell to incorporate information from prior cells. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æ˜¯ä¸€ç§å¸å¼•äººçš„è‡ªåŠ¨åŒ–è®¾è®¡ä¼˜åŒ–æ¶æ„çš„æ–¹æ³•ï¼Œä½†å—åˆ°é«˜è®¡ç®—é¢„ç®—çš„é™åˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨é’ˆå¯¹å¤šä¸ªé‡è¦ä¸”ç›¸äº’å†²çªçš„ç›®æ ‡è¿›è¡Œä¼˜åŒ–æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”ååŒå…‹é‡Œæ ¼è¾…åŠ©å¤šä¿çœŸå¤šç›®æ ‡NASç®—æ³•ï¼Œé€šè¿‡å¼•å…¥åŸºäºèšç±»çš„å±€éƒ¨å¤šä¿çœŸå¡«å……é‡‡æ ·ç­–ç•¥ï¼Œè¿›ä¸€æ­¥é™ä½äº†NASçš„è®¡ç®—æˆæœ¬ï¼Œå®ç°äº†æœç´¢ç©ºé—´çš„é«˜æ•ˆæ¢ç´¢ï¼Œä»è€ŒåŠ å¿«äº†æ”¶æ•›é€Ÿåº¦ã€‚è¯¥ç®—æ³•é€šè¿‡ä½¿ç”¨ä¸€ç§æ–°çš„è¿ç»­ç¼–ç æ–¹æ³•æ¥è¡¨ç¤ºåŸºäºé€šç”¨å•å…ƒU-Netéª¨å¹²ç½‘ä¸­æ¯ä¸ªå•å…ƒçš„è¿æ¥ï¼Œä»è€Œé™ä½äº†æœç´¢ç»´åº¦ï¼ˆå˜é‡æ•°ï¼‰ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿäº†ç®—æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æœ‰é™çš„è®¡ç®—é¢„ç®—ä¸‹ï¼Œæ‰€æå‡ºçš„NASç®—æ³•åœ¨ä¸‰ä¸ªæ•°å€¼åŸºå‡†æµ‹è¯•ã€ä¸€ä¸ª2D Darcyæµå›å½’é—®é¢˜å’Œä¸€ä¸ªCHASE_DB1ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜ä¸Šä¼˜äºä¹‹å‰å‘è¡¨çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚éšåï¼Œè¯¥æ–¹æ³•è¢«ç”¨äºåˆ›å»ºé£é€Ÿå›å½’æ¨¡å‹ï¼Œåº”ç”¨äºåŸå¸‚å»ºæ¨¡ï¼Œæ‰€å‘ç°çš„æ¨¡å‹èƒ½å¤Ÿä»¥è¾ƒä½çš„è®¡ç®—å¤æ‚åº¦å®ç°è‰¯å¥½çš„é¢„æµ‹ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒNASç®—æ³•ç‹¬ç«‹åœ°å‘ç°äº†å…¶ä»–æ–‡çŒ®ä¸­æ”¯æ’‘ä¼˜è¶ŠU-Netæ¶æ„çš„åŸåˆ™ï¼Œä¾‹å¦‚å…è®¸æ¯ä¸ªå•å…ƒä»å‰åºå•å…ƒä¸­è·å–ä¿¡æ¯çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01943v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰åœ¨è®¡ç®—é¢„ç®—é«˜ä¸”éœ€è¦ä¼˜åŒ–å¤šä¸ªé‡è¦å†²çªç›®æ ‡æ—¶å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”ååŒå…‹é‡Œæ ¼è¾…åŠ©çš„å¤šä¿çœŸå¤šç›®æ ‡NASç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡å¼•å…¥åŸºäºèšç±»çš„å±€éƒ¨å¤šä¿çœŸå¡«å……é‡‡æ ·ç­–ç•¥ï¼Œå®ç°äº†æœç´¢ç©ºé—´çš„é«˜æ•ˆæ¢ç´¢ï¼Œä»è€ŒåŠ å¿«äº†æ”¶æ•›é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä½¿ç”¨æ–°å‹è¿ç»­ç¼–ç æ–¹æ³•è¡¨ç¤ºé€šç”¨å•å…ƒåŸºU-Netéª¨æ¶ä¸­æ¯ä¸ªå•å…ƒçš„è¿æ¥ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿäº†ç®—æ³•ï¼Œå¹¶é™ä½äº†æœç´¢ç»´åº¦ï¼ˆå˜é‡æ•°ï¼‰ã€‚åœ¨ä¸‰ä¸ªæ•°å€¼åŸºå‡†æµ‹è¯•ã€ä¸€ä¸ªäºŒç»´è¾¾è¥¿æµå›å½’é—®é¢˜å’ŒCHASE_DB1ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜ä¸Šï¼Œæ‰€æNASç®—æ³•åœ¨æœ‰é™è®¡ç®—é¢„ç®—ä¸‹è¡¨ç°å‡ºä¼˜äºå…ˆå‰æœ€å…ˆè¿›æ–¹æ³•çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œåˆ©ç”¨è¯¥æ–¹æ³•åˆ›å»ºäº†ç”¨äºåŸå¸‚å»ºæ¨¡çš„é£é€Ÿå›å½’æ¨¡å‹ï¼Œæ‰€å»ºæ¨¡å‹åœ¨é™ä½è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ï¼Œå…·æœ‰è‰¯å¥½çš„é¢„æµ‹èƒ½åŠ›ã€‚ç ”ç©¶è¿˜å‘ç°ï¼ŒNASç®—æ³•ç‹¬ç«‹åœ°æ­ç¤ºäº†å…¶ä»–æ–‡çŒ®ä¸­æ”¯æ’‘ä¼˜ç§€U-Netæ¶æ„çš„åŸåˆ™ï¼Œå¦‚å…è®¸æ¯ä¸ªå•å…ƒä»å‰å•å…ƒè·å–ä¿¡æ¯çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NASç®—æ³•å¯ä»¥é€šè¿‡è‡ªé€‚åº”ååŒå…‹é‡Œæ ¼è¾…åŠ©çš„å¤šä¿çœŸå¤šç›®æ ‡ä¼˜åŒ–ç­–ç•¥æ¥é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŸºäºèšç±»çš„å±€éƒ¨å¤šä¿çœŸå¡«å……é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç®—æ³•èƒ½é«˜æ•ˆæ¢ç´¢æœç´¢ç©ºé—´å¹¶åŠ å¿«æ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>ä½¿ç”¨æ–°å‹è¿ç»­ç¼–ç æ–¹æ³•è¡¨ç¤ºå•å…ƒè¿æ¥ï¼Œé™ä½æœç´¢ç»´åº¦å¹¶æé«˜ç®—æ³•æ•ˆç‡ã€‚</li>
<li>æ‰€æNASç®—æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™çš„è®¡ç®—é¢„ç®—ä¸‹ã€‚</li>
<li>è¯¥ç®—æ³•å¯åº”ç”¨äºåˆ›å»ºåŸå¸‚æ¨¡å‹ä¸­çš„é£é€Ÿå›å½’æ¨¡å‹ï¼Œå…·æœ‰è‰¯å¥½çš„é¢„æµ‹èƒ½åŠ›ä¸”è®¡ç®—å¤æ‚æ€§è¾ƒä½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-059a232b2d004c3da263ee1bcc3552e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6aa85721ce4004e128869f72c195401a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0218b912f1af62ca34b4dba29ed89f43.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DroneSR-Rethinking-Few-shot-Thermal-Image-Super-Resolution-from-Drone-based-Perspective"><a href="#DroneSR-Rethinking-Few-shot-Thermal-Image-Super-Resolution-from-Drone-based-Perspective" class="headerlink" title="DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from   Drone-based Perspective"></a>DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from   Drone-based Perspective</h2><p><strong>Authors:Zhipeng Weng, Xiaopeng Liu, Ce Liu, Xingyuan Guo, Yukai Shi, Liang Lin</strong></p>
<p>Although large scale models achieve significant improvements in performance, the overfitting challenge still frequently undermines their generalization ability. In super resolution tasks on images, diffusion models as representatives of generative models typically adopt large scale architectures. However, few-shot drone-captured infrared training data frequently induces severe overfitting in large-scale architectures. To address this key challenge, our method proposes a new Gaussian quantization representation learning method oriented to diffusion models that alleviates overfitting and enhances robustness. At the same time, an effective monitoring mechanism tracks large scale architectures during training to detect signs of overfitting. By introducing Gaussian quantization representation learning, our method effectively reduces overfitting while maintaining architecture complexity. On this basis, we construct a multi source drone-based infrared image benchmark dataset for detection and use it to emphasize overfitting issues of large scale architectures in few sample, drone-based diverse drone-based image reconstruction scenarios. To verify the efficacy of the method in mitigating overfitting, experiments are conducted on the constructed benchmark. Experimental results demonstrate that our method outperforms existing super resolution approaches and significantly mitigates overfitting of large scale architectures under complex conditions. The code and DroneSR dataset will be available at: <a target="_blank" rel="noopener" href="https://github.com/wengzp1/GARLSR">https://github.com/wengzp1/GARLSR</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§è§„æ¨¡æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä½†è¿‡æ‹Ÿåˆé—®é¢˜ä»ç„¶ç»å¸¸ç ´åå®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„ä»£è¡¨é€šå¸¸é‡‡ç”¨å¤§è§„æ¨¡æ¶æ„ã€‚ç„¶è€Œï¼Œå¯¹äºå°‘é‡æ— äººæœºæ•è·çš„çº¢å¤–è®­ç»ƒæ•°æ®ï¼Œè¿™å¾€å¾€å¯¼è‡´å¤§è§„æ¨¡æ¶æ„ä¸­çš„ä¸¥é‡è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æå‡ºäº†ä¸€ç§é¢å‘æ‰©æ•£æ¨¡å‹çš„æ–°å‹é«˜æ–¯é‡åŒ–è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜å¹¶å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚åŒæ—¶ï¼Œæœ‰æ•ˆçš„ç›‘æ§æœºåˆ¶å¯ä»¥è·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¤§è§„æ¨¡æ¶æ„ï¼Œä»¥æ£€æµ‹è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚é€šè¿‡å¼•å…¥é«˜æ–¯é‡åŒ–è¡¨ç¤ºå­¦ä¹ ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒæ¶æ„å¤æ‚æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†è¿‡æ‹Ÿåˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç”¨äºæ£€æµ‹çš„å¤šæºæ— äººæœºçº¢å¤–å›¾åƒåŸºå‡†æ•°æ®é›†ï¼Œå¹¶ç”¨æ¥å¼ºè°ƒåœ¨å°‘é‡æ ·æœ¬ã€åŸºäºæ— äººæœºçš„å¤šæ ·åŒ–å›¾åƒé‡å»ºåœºæ™¯ä¸­å¤§è§„æ¨¡æ¶æ„çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ä¸ºäº†éªŒè¯è¯¥æ–¹æ³•åœ¨ç¼“è§£è¿‡æ‹Ÿåˆæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨æ„å»ºçš„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œå¹¶åœ¨å¤æ‚æ¡ä»¶ä¸‹æ˜¾è‘—å‡è½»äº†å¤§è§„æ¨¡æ¶æ„çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ä»£ç å’ŒDroneSRæ•°æ®é›†å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/wengzp1/GARLSR">https://github.com/wengzp1/GARLSR</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01898v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œä½†è¿‡æ‹Ÿåˆé—®é¢˜ä»ç„¶é¢‘ç¹åœ°å½±å“äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­ï¼Œæ‰©æ•£æ¨¡å‹ç­‰ç”Ÿæˆæ¨¡å‹é€šå¸¸é‡‡ç”¨å¤§è§„æ¨¡æ¶æ„ï¼Œä½†å¯¹äºæ— äººæœºæ•è·çš„çº¢å¤–è®­ç»ƒæ•°æ®ï¼Œè¿™ç§æ¶æ„å¾€å¾€ä¼šå‡ºç°ä¸¥é‡çš„è¿‡æ‹Ÿåˆç°è±¡ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘æ‰©æ•£æ¨¡å‹çš„é«˜æ–¯é‡åŒ–è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜å¹¶å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥æœ‰æ•ˆçš„ç›‘æ§æœºåˆ¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·Ÿè¸ªå¤§è§„æ¨¡æ¶æ„ä»¥æ£€æµ‹è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚æ¡ä»¶ä¸‹ä¼˜äºç°æœ‰çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—ç¼“è§£äº†å¤§è§„æ¨¡æ¶æ„çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œä½†è¿‡æ‹Ÿåˆé—®é¢˜ä»å½±å“å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­å¸¸é‡‡ç”¨å¤§è§„æ¨¡æ¶æ„ã€‚</li>
<li>æ— äººæœºæ•è·çš„çº¢å¤–è®­ç»ƒæ•°æ®å¯¹å¤§è§„æ¨¡æ¶æ„å®¹æ˜“äº§ç”Ÿè¿‡æ‹Ÿåˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§é¢å‘æ‰©æ•£æ¨¡å‹çš„é«˜æ–¯é‡åŒ–è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä»¥ç¼“è§£è¿‡æ‹Ÿåˆå¹¶å¢å¼ºæ¨¡å‹ç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥ç›‘æ§æœºåˆ¶æ¥è·Ÿè¸ªå¤§è§„æ¨¡æ¶æ„çš„è®­ç»ƒè¿‡ç¨‹ï¼Œä»¥æ£€æµ‹è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚</li>
<li>æ„å»ºçš„åŸºäºå¤šæºæ— äººæœºçš„çº¢å¤–å›¾åƒåŸºå‡†æ•°æ®é›†ç”¨äºå¼ºè°ƒå¤§è§„æ¨¡æ¶æ„åœ¨å°‘é‡æ ·æœ¬ä¸‹çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6967de0026cfc3866db2656f597604cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b5279beb21342041c6028560656f9b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39aba19ae0c8fc1c6a66420102f15b72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52a2aa7d3e418ccc93b3986ff85d6ad9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe7ab5324116328929562ccfcdcfbf45.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision"><a href="#M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision" class="headerlink" title="M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via   Self-Supervision"></a>M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via   Self-Supervision</h2><p><strong>Authors:Che Liu, Zheng Jiang, Chengyu Fang, Heng Guo, Yan-Jie Zhou, Jiaqi Qu, Le Lu, Minfeng Xu</strong></p>
<p>Medical image retrieval is essential for clinical decision-making and translational research, relying on discriminative visual representations. Yet, current methods remain fragmented, relying on separate architectures and training strategies for 2D, 3D, and video-based medical data. This modality-specific design hampers scalability and inhibits the development of unified representations. To enable unified learning, we curate a large-scale hybrid-modality dataset comprising 867,653 medical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging this dataset, we train M3Ret, a unified visual encoder without any modality-specific customization. It successfully learns transferable representations using both generative (MAE) and contrastive (SimDINO) self-supervised learning (SSL) paradigms. Our approach sets a new state-of-the-art in zero-shot image-to-image retrieval across all individual modalities, surpassing strong baselines such as DINOv3 and the text-supervised BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired data, and the model generalizes to unseen MRI tasks, despite never observing MRI during pretraining, demonstrating the generalizability of purely visual self-supervision to unseen modalities. Comprehensive analyses further validate the scalability of our framework across model and data sizes. These findings deliver a promising signal to the medical imaging community, positioning M3Ret as a step toward foundation models for visual SSL in multimodal medical image understanding. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ£€ç´¢å¯¹ä¸´åºŠå†³ç­–å’Œè½¬åŒ–ç ”ç©¶è‡³å…³é‡è¦ï¼Œä¾èµ–äºåŒºåˆ†æ€§çš„è§†è§‰è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä»ç„¶æ”¯ç¦»ç ´ç¢ï¼Œé’ˆå¯¹2Dã€3Då’ŒåŸºäºè§†é¢‘çš„åŒ»ç–—æ•°æ®ï¼Œä¾èµ–äºç‹¬ç«‹çš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥ã€‚è¿™ç§é’ˆå¯¹ç‰¹å®šæ¨¡æ€çš„è®¾è®¡é˜»ç¢äº†å¯æ‰©å±•æ€§ï¼Œå¹¶æŠ‘åˆ¶äº†ç»Ÿä¸€è¡¨ç¤ºçš„å‘å±•ã€‚ä¸ºäº†å®ç°ç»Ÿä¸€å­¦ä¹ ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ··åˆæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«867,653ä¸ªåŒ»å­¦æˆåƒæ ·æœ¬ï¼ŒåŒ…æ‹¬2D Xå…‰ç‰‡å’Œè¶…å£°æ³¢ã€RGBå†…çª¥é•œè§†é¢‘å’Œ3D CTæ‰«æã€‚åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†M3Retï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰ç¼–ç å™¨ï¼Œæ— éœ€ä»»ä½•é’ˆå¯¹ç‰¹å®šæ¨¡æ€çš„å®šåˆ¶ã€‚å®ƒæˆåŠŸåœ°åˆ©ç”¨ç”Ÿæˆå¼ï¼ˆMAEï¼‰å’Œå¯¹æ¯”å¼ï¼ˆSimDINOï¼‰è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰èŒƒå¼å­¦ä¹ å¯è¿ç§»çš„è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰å•ä¸ªæ¨¡æ€çš„é›¶æ ·æœ¬å›¾åƒåˆ°å›¾åƒæ£€ç´¢ä¸­æ ‘ç«‹äº†æ–°çš„æ ‡æ†ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå¦‚DINOv3å’Œæ–‡æœ¬ç›‘ç£çš„BMC-CLIPã€‚æ›´å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå°½ç®¡åœ¨é¢„è®­ç»ƒé˜¶æ®µä»æœªæ¥è§¦è¿‡MRIï¼Œä½†åœ¨æ²¡æœ‰é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹å‡ºç°äº†å¼ºå¤§çš„è·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶ä¸”è¯¥æ¨¡å‹èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„MRIä»»åŠ¡ï¼Œè¯æ˜äº†çº¯è§†è§‰è‡ªç›‘ç£åœ¨æœªè§æ¨¡æ€ä¸­çš„é€šç”¨æ€§ã€‚å…¨é¢çš„åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ¨¡å‹å’Œæ•°æ®è§„æ¨¡æ–¹é¢çš„å¯æ‰©å±•æ€§ã€‚è¿™äº›å‘ç°å‘åŒ»å­¦æˆåƒé¢†åŸŸä¼ é€’äº†ç§¯æçš„ä¿¡å·ï¼Œè¡¨æ˜M3Retæ˜¯æœç€è§†è§‰SSLå¤šæ¨¡æ€åŒ»å­¦å›¾åƒç†è§£åŸºç¡€æ¨¡å‹è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01360v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦å›¾åƒæ£€ç´¢åœ¨ä¸´åºŠå†³ç­–å’Œè½¬åŒ–ç ”ç©¶ä¸­çš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•å› é’ˆå¯¹2Dã€3Då’Œè§†é¢‘åŒ»å­¦æ•°æ®çš„ç‹¬ç«‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥è€Œå­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡æ··åˆæ¨¡æ€æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒäº†M3Retç»Ÿä¸€è§†è§‰ç¼–ç å™¨ï¼Œæ— éœ€ä»»ä½•ç‰¹å®šæ¨¡æ€çš„å®šåˆ¶ï¼Œé€šè¿‡ç”Ÿæˆå¼ï¼ˆMAEï¼‰å’Œå¯¹æ¯”å¼ï¼ˆSimDINOï¼‰è‡ªç›‘ç£å­¦ä¹ èŒƒå¼æˆåŠŸå­¦ä¹ å¯è¿ç§»è¡¨ç¤ºã€‚è¯¥æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒæ£€ç´¢æ–¹é¢è¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ï¼Œå…·æœ‰è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡ç ”ç©¶ç»“æœæ ‡å¿—ç€å‘å¤šæ¨¡æ€åŒ»å­¦å›¾åƒç†è§£çš„è§†è§‰è‡ªç›‘ç£åŸºç¡€æ¨¡å‹è¿ˆè¿›äº†é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ£€ç´¢åœ¨ä¸´åºŠå†³ç­–å’Œè½¬åŒ–ç ”ç©¶ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¾èµ–äºåˆ¤åˆ«æ€§è§†è§‰è¡¨ç¤ºã€‚</li>
<li>å½“å‰åŒ»å­¦å›¾åƒå¤„ç†æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹ç»Ÿä¸€çš„å¤šæ¨¡æ€è¡¨ç¤ºæ–¹æ³•ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡æ··åˆæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§åŒ»å­¦æˆåƒæ ·æœ¬ã€‚</li>
<li>è®­ç»ƒäº†M3Retç»Ÿä¸€è§†è§‰ç¼–ç å™¨ï¼Œæ— éœ€ç‰¹å®šæ¨¡æ€å®šåˆ¶ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆå¼å’Œå¯¹æ¯”å¼è‡ªç›‘ç£å­¦ä¹ èŒƒå¼æˆåŠŸè®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒæ£€ç´¢æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œå…·æœ‰è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aed23e09619f8c8f8887f48b4e894db0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dfc2e23005fc5962fbaa1878cf47127.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SegAssess-Panoramic-quality-mapping-for-robust-and-transferable-unsupervised-segmentation-assessment"><a href="#SegAssess-Panoramic-quality-mapping-for-robust-and-transferable-unsupervised-segmentation-assessment" class="headerlink" title="SegAssess: Panoramic quality mapping for robust and transferable   unsupervised segmentation assessment"></a>SegAssess: Panoramic quality mapping for robust and transferable   unsupervised segmentation assessment</h2><p><strong>Authors:Bingnan Yang, Mi Zhang, Zhili Zhang, Zhan Zhang, Yuanxin Zhao, Xiangyun Hu, Jianya Gong</strong></p>
<p>High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments across 32 datasets derived from 6 sources demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks, establishing PQM via SegAssess as a robust and transferable solution for unsupervised SQA. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Yangbn97/SegAssess">https://github.com/Yangbn97/SegAssess</a>. </p>
<blockquote>
<p>é«˜è´¨é‡å›¾åƒåˆ†å‰²å¯¹äºé¥æ„Ÿä¸­çš„åƒç´ çº§åœ°ç†ç©ºé—´åˆ†æè‡³å…³é‡è¦ï¼Œéœ€è¦è¿›è¡Œç¨³å¥çš„åˆ†å‰²è´¨é‡è¯„ä¼°ï¼ˆSQAï¼‰ã€‚ç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹çœŸå®åœ°é¢ä¿¡æ¯çš„æƒ…å†µä¸‹è¿›è¡Œæ— ç›‘ç£è®¾ç½®æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚å°½ç®¡æœ€è¿‘åŸºäºæ·±åº¦å­¦ä¹ çš„æ— ç›‘ç£SQAæ–¹æ³•æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸å­˜åœ¨è¯„ä¼°ç²’åº¦ç²—ç³™ã€è¯„ä¼°ä¸å®Œæ•´ä»¥åŠè¿ç§»æ€§å·®ç­‰å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†å…¨æ™¯è´¨é‡æ˜ å°„ï¼ˆPQMï¼‰ä½œä¸ºç”¨äºå…¨é¢åƒç´ çº§SQAçš„æ–°èŒƒå¼ï¼Œå¹¶æå‡ºäº†SegAssessï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç°è¯¥æ–¹æ³•çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚SegAssessç‹¬ç‰¹åœ°å°†SQAåˆ¶å®šä¸ºç²¾ç»†ç²’åº¦çš„å››åˆ†ç±»å…¨æ™¯åˆ†å‰²ä»»åŠ¡ï¼Œå°†è¯„ä¼°ä¸­çš„åˆ†å‰²æ©è†œå†…çš„åƒç´ åˆ†ç±»ä¸ºçœŸæ­£é˜³æ€§ï¼ˆTPï¼‰ã€å‡é˜³æ€§ï¼ˆFPï¼‰ã€çœŸé˜´æ€§ï¼ˆTNï¼‰å’Œå‡é˜´æ€§ï¼ˆFNï¼‰ç±»åˆ«ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„è´¨é‡å›¾ã€‚SegAssessåˆ©ç”¨å¢å¼ºçš„Segment Anything Modelï¼ˆSAMï¼‰æ¶æ„ï¼Œé‡‡ç”¨è¾“å…¥æ©è†œä½œä¸ºé€šè¿‡äº¤å‰æ³¨æ„è¿›è¡Œæœ‰æ•ˆç‰¹å¾é›†æˆçš„æç¤ºã€‚å…³é”®åˆ›æ–°åŒ…æ‹¬å¸¦æœ‰èšåˆè¯­ä¹‰è¿‡æ»¤å™¨ï¼ˆASFï¼‰æ¨¡å—çš„è¾¹ç¼˜å¼•å¯¼å‹å®ï¼ˆEGCï¼‰åˆ†æ”¯ï¼Œä»¥æ”¹è¿›åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¯¹è±¡è¾¹ç¼˜é™„è¿‘çš„é¢„æµ‹ï¼Œä»¥åŠç»“åˆäº†å¤šæºæ©è†œçš„å¢å¼ºæ··åˆé‡‡æ ·ï¼ˆAMSï¼‰è®­ç»ƒç­–ç•¥ï¼Œä»¥æ˜¾è‘—æé«˜è·¨åŸŸé²æ£’æ€§å’Œé›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚åœ¨æ¥è‡ªå…­ä¸ªæºçš„32ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒSegAssessè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶å¯¹æœªè§è¿‡çš„æ©è†œè¡¨ç°å‡ºæ˜¾è‘—çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œè¯æ˜äº†é€šè¿‡SegAssesså®ç°çš„PQMæ˜¯ä¸€ç§ç¨³å¥ä¸”å¯è¿ç§»çš„æ— ç›‘ç£SQAè§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/Yangbn97/SegAssess%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Yangbn97/SegAssessè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01183v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„é¥æ„Ÿå›¾åƒåˆ†å‰²è´¨é‡è¯„ä¼°æ–¹æ³•â€”â€”å…¨æ™¯è´¨é‡æ˜ å°„ï¼ˆPQMï¼‰ï¼Œå¹¶å®ç°äº†åŸºäºæ­¤æ–¹æ³•çš„æ·±åº¦å­¦ä¹ æ¡†æ¶SegAssessã€‚SegAssesså°†åˆ†å‰²è´¨é‡è¯„ä¼°ç²¾ç»†åŒ–ä¸ºå››åˆ†ç±»å…¨æ™¯åˆ†å‰²ä»»åŠ¡ï¼Œç”Ÿæˆå®Œæ•´çš„è´¨é‡å›¾ã€‚é€šè¿‡å¢å¼ºå‹Segment Anything Modelæ¶æ„ï¼Œå®ç°æœ‰æ•ˆç‰¹å¾æ•´åˆã€‚å…³é”®åˆ›æ–°åŒ…æ‹¬å¸¦æœ‰èšåˆè¯­ä¹‰è¿‡æ»¤å™¨çš„è¾¹ç¼˜å¼•å¯¼ç´§ç¼©åˆ†æ”¯ï¼Œä»¥åŠé€šè¿‡æ··åˆé‡‡æ ·ç­–ç•¥æå‡è·¨åŸŸç¨³å¥æ€§å’Œé›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚å®éªŒè¯æ˜SegAssessè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨æœªè§è¿‡çš„æ©è†œä¸Šå±•ç°å‡ºæ˜¾è‘—çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥å…¨æ™¯è´¨é‡æ˜ å°„ï¼ˆPQMï¼‰ä½œä¸ºå…¨é¢çš„åƒç´ çº§åˆ†å‰²è´¨é‡è¯„ä¼°æ–°æ–¹æ³•ã€‚</li>
<li>SegAssessæ¡†æ¶å®ç°äº†ç²¾ç»†ç²’åº¦çš„å››åˆ†ç±»å…¨æ™¯åˆ†å‰²è´¨é‡è¯„ä¼°ã€‚</li>
<li>SegAssessåˆ©ç”¨å¢å¼ºçš„Segment Anything Modelæ¶æ„ï¼Œé€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶å®ç°æœ‰æ•ˆç‰¹å¾æ•´åˆã€‚</li>
<li>è¾¹ç¼˜å¼•å¯¼ç´§ç¼©åˆ†æ”¯å’Œèšåˆè¯­ä¹‰è¿‡æ»¤å™¨ç”¨äºæ”¹è¿›åœ¨æŒ‘æˆ˜æ€§è¾¹ç¼˜é™„è¿‘çš„é¢„æµ‹ã€‚</li>
<li>é‡‡ç”¨å¢å¼ºæ··åˆé‡‡æ ·ç­–ç•¥æå‡æ¨¡å‹è·¨åŸŸç¨³å¥æ€§å’Œé›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜SegAssessè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-04032c07dbe2efabcbc3f70a0cf941e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fed1469b2cb73387c473ae6e471f43b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-565ae47a0afcb25fd15f0e973a13290c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MetaSSL-A-General-Heterogeneous-Loss-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#MetaSSL-A-General-Heterogeneous-Loss-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="MetaSSL: A General Heterogeneous Loss for Semi-Supervised Medical Image   Segmentation"></a>MetaSSL: A General Heterogeneous Loss for Semi-Supervised Medical Image   Segmentation</h2><p><strong>Authors:Weiren Zhao, Lanfeng Zhong, Xin Liao, Wenjun Liao, Sichuan Zhang, Shaoting Zhang, Guotai Wang</strong></p>
<p>Semi-Supervised Learning (SSL) is important for reducing the annotation cost for medical image segmentation models. State-of-the-art SSL methods such as Mean Teacher, FixMatch and Cross Pseudo Supervision (CPS) are mainly based on consistency regularization or pseudo-label supervision between a reference prediction and a supervised prediction. Despite the effectiveness, they have overlooked the potential noise in the labeled data, and mainly focus on strategies to generate the reference prediction, while ignoring the heterogeneous values of different unlabeled pixels. We argue that effectively mining the rich information contained by the two predictions in the loss function, instead of the specific strategy to obtain a reference prediction, is more essential for SSL, and propose a universal framework MetaSSL based on a spatially heterogeneous loss that assigns different weights to pixels by simultaneously leveraging the uncertainty and consistency information between the reference and supervised predictions. Specifically, we split the predictions on unlabeled data into four regions with decreasing weights in the loss: Unanimous and Confident (UC), Unanimous and Suspicious (US), Discrepant and Confident (DC), and Discrepant and Suspicious (DS), where an adaptive threshold is proposed to distinguish confident predictions from suspicious ones. The heterogeneous loss is also applied to labeled images for robust learning considering the potential annotation noise. Our method is plug-and-play and general to most existing SSL methods. The experimental results showed that it improved the segmentation performance significantly when integrated with existing SSL frameworks on different datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HiLab-git/MetaSSL">https://github.com/HiLab-git/MetaSSL</a>. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å¯¹äºé™ä½åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„æ ‡æ³¨æˆæœ¬éå¸¸é‡è¦ã€‚æœ€å…ˆè¿›çš„SSLæ–¹æ³•ï¼Œå¦‚Mean Teacherã€FixMatchå’ŒCross Pseudo Supervisionï¼ˆCPSï¼‰ï¼Œä¸»è¦åŸºäºå‚è€ƒé¢„æµ‹å’Œæœ‰ç›‘ç£é¢„æµ‹ä¹‹é—´çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–æˆ–ä¼ªæ ‡ç­¾ç›‘ç£ã€‚å°½ç®¡è¿™äº›æ–¹æ³•æœ‰æ•ˆï¼Œä½†å®ƒä»¬å¿½ç•¥äº†æ ‡è®°æ•°æ®ä¸­çš„æ½œåœ¨å™ªå£°ï¼Œä¸»è¦å…³æ³¨ç”Ÿæˆå‚è€ƒé¢„æµ‹çš„ç­–ç•¥ï¼Œè€Œå¿½ç•¥äº†ä¸åŒæœªæ ‡è®°åƒç´ çš„å¼‚è´¨å€¼ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæœ‰æ•ˆæŒ–æ˜æŸå¤±å‡½æ•°ä¸­ä¸¤ä¸ªé¢„æµ‹æ‰€åŒ…å«çš„ä¸°å¯Œä¿¡æ¯ï¼Œè€Œä¸æ˜¯è·å¾—å‚è€ƒé¢„æµ‹çš„å…·ä½“ç­–ç•¥ï¼Œå¯¹äºSSLæ›´ä¸ºé‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç©ºé—´å¼‚è´¨æŸå¤±çš„é€šç”¨æ¡†æ¶MetaSSLï¼Œè¯¥æ¡†æ¶é€šè¿‡åŒæ—¶åˆ©ç”¨å‚è€ƒå’Œç›‘ç£é¢„æµ‹ä¹‹é—´çš„ä¸ç¡®å®šæ€§å’Œä¸€è‡´æ€§ä¿¡æ¯ï¼Œä¸ºåƒç´ åˆ†é…ä¸åŒçš„æƒé‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æœªæ ‡è®°æ•°æ®ä¸Šçš„é¢„æµ‹åˆ†ä¸ºå››ä¸ªåŒºåŸŸï¼Œåœ¨æŸå¤±ä¸­çš„æƒé‡é€æ¸å‡å°ï¼šä¸€è‡´ä¸”è‡ªä¿¡ï¼ˆUCï¼‰ã€ä¸€è‡´ä¸”å¯ç–‘ï¼ˆUSï¼‰ã€ä¸ä¸€è‡´ä¸”è‡ªä¿¡ï¼ˆDCï¼‰å’Œä¸ä¸€è‡´ä¸”å¯ç–‘ï¼ˆDSï¼‰ï¼Œå…¶ä¸­æå‡ºè‡ªé€‚åº”é˜ˆå€¼æ¥åŒºåˆ†è‡ªä¿¡é¢„æµ‹å’Œå¯ç–‘é¢„æµ‹ã€‚å¼‚è´¨æŸå¤±ä¹Ÿåº”ç”¨äºå¸¦æ ‡ç­¾çš„å›¾åƒï¼Œä»¥å®ç°ç¨³å¥å­¦ä¹ ï¼Œè€ƒè™‘æ½œåœ¨çš„æ ‡æ³¨å™ªå£°ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å³æ’å³ç”¨å‹ï¼Œé€‚ç”¨äºå¤§å¤šæ•°ç°æœ‰çš„SSLæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸ä¸åŒæ•°æ®é›†çš„ç°æœ‰SSLæ¡†æ¶é›†æˆæ—¶ï¼Œå®ƒæ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HiLab-git/MetaSSL%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HiLab-git/MetaSSLä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01144v1">PDF</a> 13 pages, 12 figures. This work has been accepted by IEEE TMI</p>
<p><strong>Summary</strong><br>     åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨é™ä½åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„æ ‡æ³¨æˆæœ¬æ–¹é¢è‡³å…³é‡è¦ã€‚æœ€æ–°çš„SSLæ–¹æ³•ä¸»è¦åŸºäºä¸€è‡´æ€§æ­£åˆ™åŒ–æˆ–ä¼ªæ ‡ç­¾ç›‘ç£ç”Ÿæˆå‚è€ƒé¢„æµ‹å€¼ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¿½è§†äº†æ ‡ç­¾æ•°æ®ä¸­æ½œåœ¨çš„å™ªå£°ï¼Œå¹¶ä¸»è¦å…³æ³¨ç”Ÿæˆå‚è€ƒé¢„æµ‹çš„ç­–ç•¥ï¼Œè€Œå¿½ç•¥äº†æœªæ ‡è®°åƒç´ çš„å¼‚è´¨å€¼ã€‚æ–‡ç« å»ºè®®æœ‰æ•ˆæŒ–æ˜æŸå¤±å‡½æ•°ä¸­ä¸¤ä¸ªé¢„æµ‹å€¼ä¹‹é—´ä¸°å¯Œçš„ä¿¡æ¯æ¯”è·å¾—å‚è€ƒé¢„æµ‹å€¼çš„ç‰¹å®šç­–ç•¥æ›´é‡è¦ï¼Œå¹¶æå‡ºäº†åŸºäºç©ºé—´å¼‚è´¨æŸå¤±çš„é€šç”¨æ¡†æ¶MetaSSLã€‚è¯¥æ¡†æ¶é€šè¿‡åŒæ—¶åˆ©ç”¨å‚è€ƒå’Œç›‘ç£é¢„æµ‹ä¹‹é—´çš„ä¸ç¡®å®šæ€§å’Œä¸€è‡´æ€§ä¿¡æ¯ï¼Œå¯¹åƒç´ åˆ†é…ä¸åŒçš„æƒé‡ã€‚æ­¤å¤–ï¼Œè¯¥å¼‚è´¨æŸå¤±ä¹Ÿåº”ç”¨äºæ ‡è®°å›¾åƒï¼Œä»¥å®ç°ç¨³å¥å­¦ä¹ ï¼Œè€ƒè™‘æ½œåœ¨çš„æ ‡æ³¨å™ªå£°ã€‚å®éªŒç»“æœåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯æ˜äº†è¯¥æ–¹æ³•ä¸ç°æœ‰SSLæ¡†æ¶é›†æˆåï¼Œèƒ½æé«˜åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SSLåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¯¹äºé™ä½æ ‡æ³¨æˆæœ¬è‡³å…³é‡è¦ã€‚</li>
<li>æœ€æ–°çš„SSLæ–¹æ³•ä¸»è¦åŸºäºä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œä¼ªæ ‡ç­¾ç›‘ç£ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½è§†äº†æ ‡ç­¾æ•°æ®ä¸­çš„æ½œåœ¨å™ªå£°å’Œæœªæ ‡è®°åƒç´ çš„å¼‚è´¨å€¼ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒæœ‰æ•ˆæŒ–æ˜ä¸¤ç§é¢„æµ‹å€¼é—´çš„ä¿¡æ¯æ¯”ç”Ÿæˆå‚è€ƒé¢„æµ‹çš„ç­–ç•¥æ›´é‡è¦ã€‚</li>
<li>æå‡ºäº†åŸºäºç©ºé—´å¼‚è´¨æŸå¤±çš„é€šç”¨æ¡†æ¶MetaSSLï¼Œå¯¹åƒç´ åˆ†é…ä¸åŒæƒé‡ã€‚</li>
<li>MetaSSLæ¡†æ¶åŒæ—¶åˆ©ç”¨å‚è€ƒå’Œç›‘ç£é¢„æµ‹ä¹‹é—´çš„ä¸ç¡®å®šæ€§å’Œä¸€è‡´æ€§ä¿¡æ¯ã€‚</li>
<li>è¯¥å¼‚è´¨æŸå¤±ä¹Ÿåº”ç”¨äºæ ‡è®°å›¾åƒä»¥å®ç°ç¨³å¥å­¦ä¹ ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-abdb6cc3f9bc8bb1043ee64057330a76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aafd3abde6d8678220361e1e6275e63e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53eb655d074f856cf30c474c6c784e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67bf3c25224b98b4a4cc5ba387ea03e3.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-41abcaaf44cd5b6288db938b78329f6c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  AUDETER A Large-scale Dataset for Deepfake Audio Detection in Open   Worlds
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-97d2c1e109c4a374cf44348db26016a8.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  DeepSeek performs better than other Large Language Models in Dental   Cases
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
