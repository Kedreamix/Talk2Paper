<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-650026a5c03df615d42f14e98ed1734a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Robotic-3D-Flower-Pose-Estimation-for-Small-Scale-Urban-Farms"><a href="#Robotic-3D-Flower-Pose-Estimation-for-Small-Scale-Urban-Farms" class="headerlink" title="Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms"></a>Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms</h2><p><strong>Authors:Harsh Muriki, Hong Ray Teo, Ved Sengupta, Ai-Ping Hu</strong></p>
<p>The small scale of urban farms and the commercial availability of low-cost robots (such as the FarmBot) that automate simple tending tasks enable an accessible platform for plant phenotyping. We have used a FarmBot with a custom camera end-effector to estimate strawberry plant flower pose (for robotic pollination) from acquired 3D point cloud models. We describe a novel algorithm that translates individual occupancy grids along orthogonal axes of a point cloud to obtain 2D images corresponding to the six viewpoints. For each image, 2D object detection models for flowers are used to identify 2D bounding boxes which can be converted into the 3D space to extract flower point clouds. Pose estimation is performed by fitting three shapes (superellipsoids, paraboloids and planes) to the flower point clouds and compared with manually labeled ground truth. Our method successfully finds approximately 80% of flowers scanned using our customized FarmBot platform and has a mean flower pose error of 7.7 degrees, which is sufficient for robotic pollination and rivals previous results. All code will be made available at <a target="_blank" rel="noopener" href="https://github.com/harshmuriki/flowerPose.git">https://github.com/harshmuriki/flowerPose.git</a>. </p>
<blockquote>
<p>åŸå¸‚å†œåœºçš„è§„æ¨¡è¾ƒå°ï¼Œä¸”å¸‚é¢ä¸Šæœ‰ä½æˆæœ¬æœºå™¨äººï¼ˆå¦‚FarmBotï¼‰å¯ä¾›å•†ä¸šè´­ä¹°ï¼Œè¿™äº›æœºå™¨äººå¯ä»¥è‡ªåŠ¨å®Œæˆç®€å•çš„ç»´æŠ¤ä»»åŠ¡ï¼Œä»è€Œä¸ºæ¤ç‰©è¡¨å‹ç ”ç©¶æä¾›äº†ä¸€ä¸ªä¾¿æ·çš„å¹³å°ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€å°å¸¦æœ‰å®šåˆ¶ç›¸æœºæœ«ç«¯æ‰§è¡Œå™¨çš„FarmBotï¼Œæ ¹æ®è·å–çš„3Dç‚¹äº‘æ¨¡å‹æ¥ä¼°ç®—è‰è“æ¤æ ªçš„èŠ±å§¿æ€ï¼ˆç”¨äºæœºå™¨äººæˆç²‰ï¼‰ã€‚æˆ‘ä»¬æè¿°äº†ä¸€ç§æ–°å‹ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯å°†ç‚¹äº‘çš„æ­£äº¤è½´ä¸Šçš„ä¸ªä½“å ç”¨ç½‘æ ¼è½¬æ¢ä¸ºä¸å…­ä¸ªè§†ç‚¹ç›¸å¯¹åº”çš„2Då›¾åƒã€‚å¯¹äºæ¯å¼ å›¾åƒï¼Œæˆ‘ä»¬ä½¿ç”¨é’ˆå¯¹èŠ±æœµçš„2Dç›®æ ‡æ£€æµ‹æ¨¡å‹æ¥è¯†åˆ«2Dè¾¹ç•Œæ¡†ï¼Œè¿™äº›æ¡†å¯ä»¥è½¬æ¢ä¸º3Dç©ºé—´ä»¥æå–èŠ±æœµç‚¹äº‘ã€‚å§¿æ€ä¼°è®¡åˆ™æ˜¯é€šè¿‡å°†ä¸‰ç§å½¢çŠ¶ï¼ˆè¶…æ¤­åœ†ã€æŠ›ç‰©é¢å’Œå¹³é¢ï¼‰æ‹Ÿåˆåˆ°èŠ±æœµç‚¹äº‘ä¸Šï¼Œå¹¶ä¸æ‰‹åŠ¨æ ‡è®°çš„åœ°é¢å®å†µè¿›è¡Œæ¯”è¾ƒæ¥å®Œæˆçš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å®šåˆ¶çš„FarmBotå¹³å°æˆåŠŸæ‰¾åˆ°äº†å¤§çº¦80%çš„èŠ±æœµï¼ŒèŠ±æœµå§¿æ€çš„å¹³å‡è¯¯å·®ä¸º7.7åº¦ï¼Œè¿™å¯¹äºæœºå™¨äººæˆç²‰å·²ç»è¶³å¤Ÿï¼Œå¹¶ä¸”ä¸ä»¥å‰çš„ç»“æœç›¸å½“ã€‚æ‰€æœ‰ä»£ç éƒ½å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/harshmuriki/flowerPose.git%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/harshmuriki/flowerPose.gitä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02870v1">PDF</a> 7 pages, 7 figures</p>
<p><strong>Summary</strong><br>åŸå¸‚å†œåœºçš„å°å‹åŒ–å’Œä½æˆæœ¬æœºå™¨äººçš„å•†ä¸šåŒ–ï¼Œå¦‚è‡ªåŠ¨åŒ–ç®€å•ç»´æŠ¤ä»»åŠ¡çš„FarmBotï¼Œä¸ºæ¤ç‰©è¡¨å‹ç ”ç©¶æä¾›äº†ä¸€ä¸ªä¾¿æ·çš„å¹³å°ã€‚ç ”ç©¶è€…ä½¿ç”¨é…å¤‡å®šåˆ¶æ‘„åƒå¤´çš„FarmBotæ¥ä¼°è®¡è‰è“æ¤ç‰©çš„èŠ±å§¿æ€ï¼ˆç”¨äºæœºå™¨äººæˆç²‰ï¼‰ï¼ŒåŸºäºè·å–çš„3Dç‚¹äº‘æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§æ–°ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯å°†ç‚¹äº‘çš„ä¸ªåˆ«å ç”¨ç½‘æ ¼æ²¿æ­£äº¤è½´è½¬æ¢ä¸ºå¯¹åº”çš„å…­ä¸ªè§†è§’çš„äºŒç»´å›¾åƒã€‚å¯¹äºæ¯ä¸ªå›¾åƒï¼Œä½¿ç”¨äºŒç»´ç›®æ ‡æ£€æµ‹æ¨¡å‹è¯†åˆ«èŠ±æœµçš„è¾¹ç•Œæ¡†ï¼Œå¯è½¬æ¢ä¸ºä¸‰ç»´ç©ºé—´ä»¥æå–èŠ±æœµçš„ç‚¹äº‘ã€‚é€šè¿‡æ‹Ÿåˆä¸‰ç§å½¢çŠ¶ï¼ˆè¶…æ¤­åœ†ä½“ã€æŠ›ç‰©é¢å’Œå¹³é¢ï¼‰è¿›è¡Œå§¿æ€ä¼°è®¡ï¼Œå¹¶ä¸æ‰‹åŠ¨æ ‡è®°çš„çœŸå®å€¼è¿›è¡Œæ¯”è¾ƒã€‚ç ”ç©¶æˆåŠŸæ‰¾åˆ°äº†å¤§çº¦80%çš„èŠ±å‰ä¿¡æ¯ï¼Œé‡‡ç”¨å®šåˆ¶çš„FarmBotå¹³å°ï¼›ä¸”å…¶èŠ±æœµå§¿æ€çš„å¹³å‡è¯¯å·®ä¸º7.7åº¦ï¼Œé€‚ç”¨äºæœºå™¨äººæˆç²‰ä¸”æœ‰æœ›ä¸å‰äººç ”ç©¶ç›¸æå¹¶è®ºã€‚ç›¸å…³ç ”ç©¶ä»£ç å°†åœ¨ <a target="_blank" rel="noopener" href="https://github.com/harshmuriki/flowerPose.git%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/harshmuriki/flowerPose.gitä¸Šå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚å†œåœºå°å‹åŒ–å’Œä½æˆæœ¬æœºå™¨äººçš„å‘å±•ä¿ƒè¿›äº†æ¤ç‰©è¡¨å‹ç ”ç©¶çš„ä¾¿æ·æ€§ã€‚</li>
<li>ä½¿ç”¨FarmBotç»“åˆå®šåˆ¶æ‘„åƒå¤´è¿›è¡Œè‰è“æ¤ç‰©èŠ±å‰å§¿æ€ä¼°è®¡ï¼Œç”¨äºæœºå™¨äººæˆç²‰ã€‚</li>
<li>æ–°ç®—æ³•å°†ç‚¹äº‘å ç”¨ç½‘æ ¼è½¬æ¢ä¸ºå¯¹åº”å…­ä¸ªè§†è§’çš„äºŒç»´å›¾åƒï¼Œå®ç°èŠ±å‰çš„è¯†åˆ«å’Œåˆ†æã€‚</li>
<li>åˆ©ç”¨äºŒç»´ç›®æ ‡æ£€æµ‹æ¨¡å‹è¯†åˆ«èŠ±æœµè¾¹ç•Œæ¡†ï¼Œå¹¶è½¬æ¢ä¸ºä¸‰ç»´ç©ºé—´è¿›è¡Œå§¿æ€ä¼°è®¡ã€‚</li>
<li>é€šè¿‡æ‹Ÿåˆä¸åŒå½¢çŠ¶è¿›è¡Œå§¿æ€ä¼°è®¡ï¼Œä¸æ‰‹åŠ¨æ ‡è®°çš„çœŸå®å€¼ç›¸æ¯”ï¼Œå…·æœ‰è¾ƒä½è¯¯å·®ã€‚</li>
<li>ç ”ç©¶æˆåŠŸè¯†åˆ«äº†å¤§çº¦80%çš„èŠ±å‰ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨å®šåˆ¶FarmBotå¹³å°å®Œæˆå®éªŒã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨èŠ±å‰å§¿æ€ä¼°è®¡æ–¹é¢çš„å¹³å‡è¯¯å·®ä¸º7.7åº¦ï¼Œé€‚åˆæœºå™¨äººæˆç²‰å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6f9c1f77ca0273ac6747f0a721bc3d39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ced3c2c0b4eaad7fe3b0244177b034e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d121ec6a1e67c44cf3644ac75b4c691c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae1228387926de54150acbc15a49b50d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04b9227e6b44de79157a6f0e65a70153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82a9e576fcce2a55116dd4b07e953306.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-atomic-force-microscopy-structure-discovery-via-style-translation"><a href="#Improving-atomic-force-microscopy-structure-discovery-via-style-translation" class="headerlink" title="Improving atomic force microscopy structure discovery via   style-translation"></a>Improving atomic force microscopy structure discovery via   style-translation</h2><p><strong>Authors:Jie Huang, Niko Oinonen, Fabio Priante, Filippo Federici Canova, Lauri Kurki, Chen Xu, Adam S. Foster</strong></p>
<p>Atomic force microscopy (AFM) is a key tool for characterising nanoscale structures, with functionalised tips now offering detailed images of the atomic structure. In parallel, AFM simulations using the particle probe model provide a cost-effective approach for rapid AFM image generation. Using state-of-the-art machine learning models and substantial simulated datasets, properties such as molecular structure, electrostatic potential, and molecular graph can be predicted from AFM images. However, transferring model performance from simulated to experimental AFM images poses challenges due to the subtle variations in real experimental data compared to the seemingly flawless simulations. In this study, we explore style translation to augment simulated images and improve the predictive performance of machine learning models in surface property analysis. We reduce the style gap between simulated and experimental AFM images and demonstrate the methodâ€™s effectiveness in enhancing structure discovery models through local structural property distribution comparisons. This research presents a novel approach to improving the efficiency of machine learning models in the absence of labelled experimental data. </p>
<blockquote>
<p>åŸå­åŠ›æ˜¾å¾®é•œï¼ˆAFMï¼‰æ˜¯è¡¨å¾çº³ç±³ç»“æ„çš„å…³é”®å·¥å…·ï¼Œå…¶åŠŸèƒ½åŒ–çš„å°–ç«¯ç°åœ¨èƒ½å¤Ÿæä¾›åŸå­ç»“æ„çš„è¯¦ç»†å›¾åƒã€‚åŒæ—¶ï¼Œä½¿ç”¨ç²’å­æ¢é’ˆæ¨¡å‹çš„AFMæ¨¡æ‹Ÿä¸ºå¿«é€Ÿç”ŸæˆAFMå›¾åƒæä¾›äº†ä¸€ç§ç»æµé«˜æ•ˆçš„æ–¹æ³•ã€‚åˆ©ç”¨æœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ¨¡å‹å’Œå¤§é‡çš„æ¨¡æ‹Ÿæ•°æ®é›†ï¼Œå¯ä»¥ä»AFMå›¾åƒé¢„æµ‹åˆ†å­ç»“æ„ã€é™ç”µåŠ¿å’Œåˆ†å­å›¾ç­‰å±æ€§ã€‚ç„¶è€Œï¼Œå°†ä»æ¨¡æ‹Ÿåˆ°å®éªŒAFMå›¾åƒçš„æ¨¡å‹æ€§èƒ½è½¬ç§»å´é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºä¸çœ‹ä¼¼å®Œç¾çš„æ¨¡æ‹Ÿç›¸æ¯”ï¼Œå®é™…å®éªŒæ•°æ®å­˜åœ¨ç»†å¾®å˜åŒ–ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢é£æ ¼ç¿»è¯‘ä»¥æ‰©å……æ¨¡æ‹Ÿå›¾åƒï¼Œæé«˜æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åˆ†æè¡¨é¢å±æ€§æ–¹é¢çš„é¢„æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬ç¼©å°äº†æ¨¡æ‹ŸAFMå›¾åƒä¸å®éªŒAFMå›¾åƒä¹‹é—´çš„é£æ ¼å·®è·ï¼Œé€šè¿‡å±€éƒ¨ç»“æ„å±æ€§åˆ†å¸ƒæ¯”è¾ƒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æé«˜ç»“æ„å‘ç°æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨ç¼ºä¹æ ‡è®°å®éªŒæ•°æ®çš„æƒ…å†µä¸‹æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹æ•ˆç‡çš„æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02240v1">PDF</a> 14 pages, 7 figures</p>
<p><strong>Summary</strong><br>     åŸå­åŠ›æ˜¾å¾®é•œï¼ˆAFMï¼‰æ˜¯è¡¨å¾çº³ç±³ç»“æ„çš„å…³é”®å·¥å…·ï¼ŒåŠŸèƒ½åŒ–æ¢é’ˆå°–å¯ä»¥æä¾›åŸå­ç»“æ„çš„è¯¦ç»†å›¾åƒã€‚åŒæ—¶ï¼Œä½¿ç”¨ç²’å­æ¢é’ˆæ¨¡å‹çš„AFMæ¨¡æ‹Ÿä¸ºå¿«é€ŸAFMå›¾åƒç”Ÿæˆæä¾›äº†ç»æµé«˜æ•ˆçš„æ–¹æ³•ã€‚å€ŸåŠ©æœ€æ–°æœºå™¨æ¨¡å‹æ¨¡æ‹Ÿæ•°æ®é›†ï¼Œå¯ä»¥ä»AFMå›¾åƒé¢„æµ‹åˆ†å­ç»“æ„ã€é™ç”µåŠ¿ç­‰å±æ€§ã€‚ç„¶è€Œï¼Œå°†æ¨¡å‹æ€§èƒ½ä»æ¨¡æ‹Ÿè½¬ç§»åˆ°å®éªŒAFMå›¾åƒé¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºç°å®å®éªŒæ•°æ®ä¸çœ‹ä¼¼å®Œç¾çš„æ¨¡æ‹Ÿä¹‹é—´å­˜åœ¨å¾®å¦™å·®å¼‚ã€‚æœ¬ç ”ç©¶æ¢è®¨é£æ ¼ç¿»è¯‘æŠ€æœ¯ï¼Œå¢å¼ºæ¨¡æ‹Ÿå›¾åƒä»¥æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è¡¨é¢æ€§è´¨åˆ†æä¸­çš„é¢„æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬å‡å°‘äº†æ¨¡æ‹Ÿå’Œå®éªŒAFMå›¾åƒä¹‹é—´çš„é£æ ¼å·®å¼‚ï¼Œé€šè¿‡å±€éƒ¨ç»“æ„å±æ€§åˆ†å¸ƒæ¯”è¾ƒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æå‡ç»“æ„å‘ç°æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ”¹è¿›æœºå™¨å­¦ä¹ æ¨¡å‹æ•ˆç‡çš„æ–°æ–¹æ³•ï¼Œå³ä½¿åœ¨æ²¡æœ‰æ ‡è®°å®éªŒæ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å‘æŒ¥ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AFMæ˜¯è¡¨å¾çº³ç±³ç»“æ„çš„é‡è¦å·¥å…·ï¼ŒåŠŸèƒ½åŒ–æ¢é’ˆå°–èƒ½æä¾›åŸå­ç»“æ„çš„è¯¦ç»†å›¾åƒã€‚</li>
<li>AFMæ¨¡æ‹Ÿåˆ©ç”¨ç²’å­æ¢é’ˆæ¨¡å‹è¿›è¡Œå¿«é€Ÿå›¾åƒç”Ÿæˆã€‚</li>
<li>åˆ©ç”¨æœ€æ–°æœºå™¨å­¦ä¹ å’Œæ¨¡æ‹Ÿæ•°æ®é›†ï¼Œå¯ä»¥ä»AFMå›¾åƒé¢„æµ‹åˆ†å­ç»“æ„ç­‰å±æ€§ã€‚</li>
<li>å°†æ¨¡å‹æ€§èƒ½ä»æ¨¡æ‹Ÿè½¬ç§»åˆ°å®éªŒAFMå›¾åƒå­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºç°å®ä¸æ¨¡æ‹Ÿä¹‹é—´å­˜åœ¨å¾®å¦™å·®å¼‚ã€‚</li>
<li>é£æ ¼ç¿»è¯‘æŠ€æœ¯ç”¨äºå¢å¼ºæ¨¡æ‹Ÿå›¾åƒï¼Œæé«˜æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è¡¨é¢æ€§è´¨åˆ†æä¸­çš„é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•é€šè¿‡å‡å°‘æ¨¡æ‹Ÿå’Œå®éªŒAFMå›¾åƒä¹‹é—´çš„é£æ ¼å·®å¼‚æ¥æé«˜ç»“æ„å‘ç°æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fcd137fd83836836289b41aba97a4c69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e20f901b5845b56272f9d54fb406087.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4fc00ac4fd48834002f018d67ac39a7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec89d88263d234b32084b8985e9c3832.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Draw-In-Mind-Learning-Precise-Image-Editing-via-Chain-of-Thought-Imagination"><a href="#Draw-In-Mind-Learning-Precise-Image-Editing-via-Chain-of-Thought-Imagination" class="headerlink" title="Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought   Imagination"></a>Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought   Imagination</h2><p><strong>Authors:Ziyun Zeng, Junhao Zhang, Wei Li, Mike Zheng Shou</strong></p>
<p>In recent years, integrating multimodal understanding and generation into a single unified model has emerged as a promising paradigm. While this approach achieves strong results in text-to-image (T2I) generation, it still struggles with precise image editing. We attribute this limitation to an imbalanced division of responsibilities. The understanding module primarily functions as a translator that encodes user instructions into semantic conditions, while the generation module must simultaneously act as designer and painter, inferring the original layout, identifying the target editing region, and rendering the new content. This imbalance is counterintuitive because the understanding module is typically trained with several times more data on complex reasoning tasks than the generation module. To address this issue, we introduce Draw-In-Mind (DIM), a dataset comprising two complementary subsets: (i) DIM-T2I, containing 14M long-context image-text pairs to enhance complex instruction comprehension; and (ii) DIM-Edit, consisting of 233K chain-of-thought imaginations generated by GPT-4o, serving as explicit design blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM dataset, resulting in DIM-4.6B-T2I&#x2F;Edit. Despite its modest parameter scale, DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1 and Step1X-Edit. These findings demonstrate that explicitly assigning the design responsibility to the understanding module provides significant benefits for image editing. Our dataset and models will be available at <a target="_blank" rel="noopener" href="https://github.com/showlab/DIM">https://github.com/showlab/DIM</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå°†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆé›†æˆåˆ°ä¸€ä¸ªå•ä¸€ç»Ÿä¸€æ¨¡å‹ä¸­å·²æˆä¸ºä¸€ç§å‰æ™¯çœ‹å¥½çš„èŒƒå¼ã€‚è™½ç„¶è¿™ä¸€æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ–¹é¢å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œä½†åœ¨ç²¾ç¡®å›¾åƒç¼–è¾‘æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬å°†è¿™ä¸€å±€é™æ€§å½’å› äºèŒè´£åˆ’åˆ†çš„ä¸å¹³è¡¡ã€‚ç†è§£æ¨¡å—ä¸»è¦å……å½“å°†ç”¨æˆ·æŒ‡ä»¤ç¼–ç ä¸ºè¯­ä¹‰æ¡ä»¶çš„ç¿»è¯‘å™¨ï¼Œè€Œç”Ÿæˆæ¨¡å—å¿…é¡»åŒæ—¶å……å½“è®¾è®¡å¸ˆå’Œç”»å®¶ï¼Œæ¨æ–­åŸå§‹å¸ƒå±€ï¼Œè¯†åˆ«ç›®æ ‡ç¼–è¾‘åŒºåŸŸï¼Œå¹¶å‘ˆç°æ–°å†…å®¹ã€‚è¿™ç§ä¸å¹³è¡¡æ˜¯ä»¤äººå›°æƒ‘çš„ï¼Œå› ä¸ºç†è§£æ¨¡å—é€šå¸¸ä½¿ç”¨å¤§é‡æ•°æ®è¿›è¡Œå¤æ‚æ¨ç†ä»»åŠ¡çš„è®­ç»ƒï¼Œè€Œç”Ÿæˆæ¨¡å—çš„å¯ç”¨æ•°æ®ç›¸å¯¹è¾ƒå°‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Draw-In-Mindï¼ˆDIMï¼‰æ•°æ®é›†ï¼Œå®ƒç”±ä¸¤ä¸ªäº’è¡¥çš„å­é›†ç»„æˆï¼šï¼ˆiï¼‰DIM-T2IåŒ…å«ç”¨äºå¢å¼ºå¤æ‚æŒ‡ä»¤ç†è§£çš„14Mé•¿ä¸Šä¸‹æ–‡å›¾åƒæ–‡æœ¬å¯¹ï¼›ï¼ˆiiï¼‰DIM-Editç”±GPT-4oç”Ÿæˆçš„æ€ç»´é“¾å›¾åƒç»„æˆï¼ŒåŒ…å«233Kå¼ å›¾åƒï¼Œä½œä¸ºå›¾åƒç¼–è¾‘çš„æ˜ç¡®è®¾è®¡è“å›¾ã€‚æˆ‘ä»¬é€šè¿‡è½»é‡çº§ä¸¤å±‚MLPå°†å†·å†»çš„Qwen2.5-VL-3Bä¸å¯è®­ç»ƒçš„SANA1.5-1.6Bè¿æ¥èµ·æ¥ï¼Œå¹¶åœ¨æå‡ºçš„DIMæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°DIM-4.6B-T2I&#x2F;Editæ¨¡å‹ã€‚å°½ç®¡å…¶å‚æ•°è§„æ¨¡é€‚ä¸­ï¼ŒDIM-4.6B-Editåœ¨ImgEditå’ŒGEdit-BenchåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æˆ–å…·æœ‰ç«äº‰åŠ›ï¼Œè¶…è¿‡äº†è¯¸å¦‚UniWorld-V1å’ŒStep1X-Editç­‰å¤§å‹æ¨¡å‹ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ˜ç¡®å°†è®¾è®¡è´£ä»»åˆ†é…ç»™ç†è§£æ¨¡å—å¯¹äºå›¾åƒç¼–è¾‘å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/showlab/DIM%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/showlab/DIMä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01986v1">PDF</a> Tech Report</p>
<p><strong>Summary</strong><br>     è¿‘å¹´ï¼Œå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆèåˆè¿›å•ä¸€æ¨¡å‹æˆä¸ºæœ‰å‰æ™¯çš„ç ”ç©¶èŒƒå¼ï¼Œå°½ç®¡åœ¨æ–‡æœ¬å›¾åƒç”Ÿæˆä¸Šå–å¾—è‰¯å¥½æ•ˆæœï¼Œä½†åœ¨å›¾åƒç²¾ç¡®ç¼–è¾‘ä¸Šä»æœ‰å±€é™ã€‚æœ¬æ–‡æå‡ºDraw-In-Mindæ•°æ®é›†ï¼ŒåŒ…å«ä¸¤ä¸ªäº’è¡¥å­é›†ï¼Œæ—¨åœ¨è§£å†³ç†è§£æ¨¡å—ä¸ç”Ÿæˆæ¨¡å—é—´çš„èŒè´£ä¸å¹³è¡¡é—®é¢˜ã€‚è®­ç»ƒåŸºäºDIMæ•°æ®é›†çš„æ–°æ¨¡å‹DIM-4.6B-T2I&#x2F;Editå±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç¼–è¾‘æ€§èƒ½ï¼Œè¾¾åˆ°äº†ç›®å‰é¡¶å°–æˆ–ç«äº‰åŠ›æ°´å¹³ã€‚è¿™æ˜¾ç¤ºæ˜ç¡®è®¾è®¡è´£ä»»åˆ†é…å¯¹äºå›¾åƒç¼–è¾‘çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æ¨¡å‹å’Œæ•°æ®é›†å°†åœ¨ç½‘ä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆç»Ÿä¸€æ¨¡å‹æˆä¸ºç ”ç©¶çƒ­ç‚¹ï¼Œå°¤å…¶åœ¨æ–‡æœ¬å›¾åƒç”Ÿæˆé¢†åŸŸè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨å›¾åƒç²¾ç¡®ç¼–è¾‘æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç†è§£æ¨¡å—ä¸ç”Ÿæˆæ¨¡å—èŒè´£ä¸å¹³è¡¡ã€‚</li>
<li>Draw-In-Mindæ•°æ®é›†çš„å¼•å…¥æ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒåŒ…å«ä¸¤ä¸ªäº’è¡¥å­é›†ï¼Œç”¨äºå¢å¼ºå¤æ‚æŒ‡ä»¤ç†è§£å’Œå›¾åƒç¼–è¾‘è®¾è®¡è“å›¾ã€‚</li>
<li>åŸºäºDIMæ•°æ®é›†çš„æ–°æ¨¡å‹DIM-4.6B-T2I&#x2F;Editå±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç¼–è¾‘æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å³ä¾¿å‚æ•°è§„æ¨¡ä¸å¤§ï¼Œä¹Ÿèƒ½è¾¾åˆ°é¡¶å°–æˆ–ç«äº‰åŠ›æ°´å¹³ï¼Œæ˜¾ç¤ºæ˜ç¡®åˆ†é…è®¾è®¡è´£ä»»çš„ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ImgEditå’ŒGEdit-BenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¸€äº›æ›´å¤§çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-be068aa5afaa205d4321784e9bca1409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84013a0ecfabdabcf6d94509b11921b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89d2b87c932d94ee51fbae1265712eb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98c84d9dbe48fe9083043b753a13d3bf.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision"><a href="#M3Ret-Unleashing-Zero-shot-Multimodal-Medical-Image-Retrieval-via-Self-Supervision" class="headerlink" title="M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via   Self-Supervision"></a>M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via   Self-Supervision</h2><p><strong>Authors:Che Liu, Zheng Jiang, Chengyu Fang, Heng Guo, Yan-Jie Zhou, Jiaqi Qu, Le Lu, Minfeng Xu</strong></p>
<p>Medical image retrieval is essential for clinical decision-making and translational research, relying on discriminative visual representations. Yet, current methods remain fragmented, relying on separate architectures and training strategies for 2D, 3D, and video-based medical data. This modality-specific design hampers scalability and inhibits the development of unified representations. To enable unified learning, we curate a large-scale hybrid-modality dataset comprising 867,653 medical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging this dataset, we train M3Ret, a unified visual encoder without any modality-specific customization. It successfully learns transferable representations using both generative (MAE) and contrastive (SimDINO) self-supervised learning (SSL) paradigms. Our approach sets a new state-of-the-art in zero-shot image-to-image retrieval across all individual modalities, surpassing strong baselines such as DINOv3 and the text-supervised BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired data, and the model generalizes to unseen MRI tasks, despite never observing MRI during pretraining, demonstrating the generalizability of purely visual self-supervision to unseen modalities. Comprehensive analyses further validate the scalability of our framework across model and data sizes. These findings deliver a promising signal to the medical imaging community, positioning M3Ret as a step toward foundation models for visual SSL in multimodal medical image understanding. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ£€ç´¢å¯¹äºä¸´åºŠå†³ç­–å’Œè½¬åŒ–ç ”ç©¶è‡³å…³é‡è¦ï¼Œä¾èµ–äºåŒºåˆ†æ€§çš„è§†è§‰è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä»ç„¶æ”¯ç¦»ç ´ç¢ï¼Œé’ˆå¯¹2Dã€3Då’ŒåŸºäºè§†é¢‘çš„åŒ»ç–—æ•°æ®ï¼Œä¾èµ–ä¸åŒçš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥ã€‚è¿™ç§æ¨¡æ€ç‰¹å®šçš„è®¾è®¡é˜»ç¢äº†å¯æ‰©å±•æ€§ï¼Œå¹¶æŠ‘åˆ¶äº†ç»Ÿä¸€è¡¨ç¤ºçš„å‘å±•ã€‚ä¸ºäº†æ”¯æŒç»Ÿä¸€å­¦ä¹ ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ··åˆæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«867,653ä¸ªåŒ»å­¦æˆåƒæ ·æœ¬ï¼ŒåŒ…æ‹¬2D Xå…‰ç‰‡å’Œè¶…å£°æ³¢ã€RGBå†…çª¥é•œè§†é¢‘å’Œ3D CTæ‰«æã€‚åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†M3Retï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰ç¼–ç å™¨ï¼Œæ— éœ€ä»»ä½•æ¨¡æ€ç‰¹å®šçš„å®šåˆ¶ã€‚å®ƒæˆåŠŸåœ°åˆ©ç”¨ç”Ÿæˆå¼ï¼ˆMAEï¼‰å’Œå¯¹æ¯”å¼ï¼ˆSimDINOï¼‰è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰èŒƒå¼å­¦ä¹ å¯è¿ç§»çš„è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬å›¾åƒåˆ°å›¾åƒæ£€ç´¢æ–¹é¢ä¸ºæ‰€æœ‰ä¸ªä½“æ¨¡æ€è®¾å®šäº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå¦‚DINOv3å’Œæ–‡æœ¬ç›‘ç£çš„BMC-CLIPã€‚æ›´å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå³ä½¿æ²¡æœ‰é…å¯¹æ•°æ®ä¹Ÿå‡ºç°äº†å¼ºå¤§çš„è·¨æ¨¡æ€å¯¹é½ï¼Œè€Œä¸”è¯¥æ¨¡å‹å¯ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„MRIä»»åŠ¡ï¼Œå°½ç®¡åœ¨é¢„è®­ç»ƒé˜¶æ®µä»æœªæ¥è§¦è¿‡MRIæ•°æ®ï¼Œè¿™è¯æ˜äº†çº¯è§†è§‰è‡ªç›‘ç£å¯¹æœªè§æ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚å…¨é¢çš„åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®è§„æ¨¡ä¸Šçš„å¯æ‰©å±•æ€§ã€‚è¿™äº›å‘ç°å‘åŒ»å­¦æˆåƒç•Œä¼ é€’äº†ç§¯æçš„ä¿¡å·ï¼Œå¹¶å°†M3Retå®šä½ä¸ºé¢å‘å¤šæ¨¡æ€åŒ»å­¦å›¾åƒç†è§£çš„è§†è§‰SSLåŸºç¡€æ¨¡å‹çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01360v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬ç ”ç©¶é’ˆå¯¹åŒ»ç–—å›¾åƒæ£€ç´¢åœ¨ä¸´åºŠå†³ç­–å’Œè½¬åŒ–ç ”ç©¶ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€è§†è§‰ç¼–ç å™¨M3Retã€‚è¯¥ç¼–ç å™¨æ— éœ€é’ˆå¯¹ç‰¹å®šæ¨¡æ€è¿›è¡Œå®šåˆ¶ï¼Œèƒ½å¤Ÿåœ¨å¤§å‹æ··åˆæ¨¡æ€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶æˆåŠŸåˆ©ç”¨ç”Ÿæˆå¼ï¼ˆMAEï¼‰å’Œå¯¹æ¯”å¼ï¼ˆSimDINOï¼‰è‡ªç›‘ç£å­¦ä¹ èŒƒå¼å­¦ä¹ å¯è¿ç§»è¡¨ç¤ºã€‚M3Retåœ¨é›¶æ ·æœ¬å›¾åƒæ£€ç´¢æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†DINOv3å’Œæ–‡æœ¬ç›‘ç£çš„BMC-CLIPç­‰å¼ºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æœªè§è¿‡MRIä»»åŠ¡çš„æƒ…å†µä¸‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†çº¯ç²¹è§†è§‰è‡ªç›‘ç£åœ¨æœªè§æ¨¡æ€ä¸­çš„é€šç”¨æ€§ã€‚ç»¼åˆåˆ†æéªŒè¯äº†è¯¥æ¡†æ¶åœ¨æ¨¡å‹å’Œæ•°æ®è§„æ¨¡æ–¹é¢çš„å¯æ‰©å±•æ€§ã€‚è¿™äº›å‘ç°å¯¹åŒ»ç–—æˆåƒé¢†åŸŸå…·æœ‰ç§¯ææ„ä¹‰ï¼Œä¸ºè§†è§‰è‡ªç›‘ç£åœ¨å¤šç§æ¨¡æ€åŒ»ç–—å›¾åƒç†è§£ä¸­çš„åŸºç¡€æ¨¡å‹å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŒ»ç–—å›¾åƒæ£€ç´¢åœ¨ä¸´åºŠå†³ç­–å’Œè½¬åŒ–ç ”ç©¶ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¾èµ–äºåˆ¤åˆ«æ€§è§†è§‰è¡¨ç¤ºã€‚</li>
<li>å½“å‰æ–¹æ³•å› é’ˆå¯¹2Dã€3Då’Œè§†é¢‘åŸºç¡€åŒ»ç–—æ•°æ®çš„ç‹¬ç«‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥è€Œæ˜¾å¾—é›¶æ•£ï¼Œé˜»ç¢äº†å¯æ‰©å±•æ€§å’Œç»Ÿä¸€è¡¨ç¤ºçš„å‘å±•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤§å‹æ··åˆæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«867,653ä¸ªåŒ»ç–—æˆåƒæ ·æœ¬ï¼Œç”¨äºè®­ç»ƒç»Ÿä¸€çš„è§†è§‰ç¼–ç å™¨M3Retã€‚</li>
<li>M3Retæ— éœ€ç‰¹å®šæ¨¡æ€å®šåˆ¶ï¼Œåˆ©ç”¨ç”Ÿæˆå¼å’Œå¯¹æ¯”å¼è‡ªç›‘ç£å­¦ä¹ èŒƒå¼æˆåŠŸå­¦ä¹ å¯è¿ç§»è¡¨ç¤ºã€‚</li>
<li>M3Retåœ¨é›¶æ ·æœ¬å›¾åƒæ£€ç´¢æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå…¶ä»–å¼ºåŸºçº¿æ¨¡å‹ï¼Œå¹¶æ˜¾ç¤ºå‡ºåœ¨ä¸åŒæ¨¡æ€é—´çš„è‰¯å¥½å¯¹é½æ•ˆæœã€‚</li>
<li>æ¨¡å‹åœ¨æœªè§è¿‡çš„MRIä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†çº¯ç²¹è§†è§‰è‡ªç›‘ç£çš„é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aed23e09619f8c8f8887f48b4e894db0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dfc2e23005fc5962fbaa1878cf47127.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Unified-Low-level-Foundation-Model-for-Enhancing-Pathology-Image-Quality"><a href="#A-Unified-Low-level-Foundation-Model-for-Enhancing-Pathology-Image-Quality" class="headerlink" title="A Unified Low-level Foundation Model for Enhancing Pathology Image   Quality"></a>A Unified Low-level Foundation Model for Enhancing Pathology Image   Quality</h2><p><strong>Authors:Ziyi Liu, Zhe Xu, Jiabo Ma, Wenqaing Li, Junlin Hou, Fuxiang Huang, Xi Wang, Ronald Cheong Kin Chan, Terence Tsz Wai Wong, Hao Chen</strong></p>
<p>Foundation models have revolutionized computational pathology by achieving remarkable success in high-level diagnostic tasks, yet the critical challenge of low-level image enhancement remains largely unaddressed. Real-world pathology images frequently suffer from degradations such as noise, blur, and low resolution due to slide preparation artifacts, staining variability, and imaging constraints, while the reliance on physical staining introduces significant costs, delays, and inconsistency. Although existing methods target individual problems like denoising or super-resolution, their task-specific designs lack the versatility to handle the diverse low-level vision challenges encountered in practice. To bridge this gap, we propose the first unified Low-level Pathology Foundation Model (LPFM), capable of enhancing image quality in restoration tasks, including super-resolution, deblurring, and denoising, as well as facilitating image translation tasks like virtual staining (H&amp;E and special stains), all through a single adaptable architecture. Our approach introduces a contrastive pre-trained encoder that learns transferable, stain-invariant feature representations from 190 million unlabeled pathology images, enabling robust identification of degradation patterns. A unified conditional diffusion process dynamically adapts to specific tasks via textual prompts, ensuring precise control over output quality. Trained on a curated dataset of 87,810 whole slied images (WSIs) across 34 tissue types and 5 staining protocols, LPFM demonstrates statistically significant improvements (p&lt;0.01) over state-of-the-art methods in most tasks (56&#x2F;66), achieving Peak Signal-to-Noise Ratio (PSNR) gains of 10-15% for image restoration and Structural Similarity Index Measure (SSIM) improvements of 12-18% for virtual staining. </p>
<blockquote>
<p>æ¨¡å‹åœ¨é«˜çº§è¯Šæ–­ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä¸ºè®¡ç®—ç—…ç†å­¦å¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ã€‚ç„¶è€Œï¼Œä½å±‚æ¬¡å›¾åƒå¢å¼ºçš„å…³é”®æŒ‘æˆ˜ä»æœªå¾—åˆ°å¾ˆå¥½çš„è§£å†³ã€‚ç°å®ä¸–ç•Œä¸­çš„ç—…ç†å›¾åƒç”±äºåˆ‡ç‰‡åˆ¶å¤‡è¿‡ç¨‹ä¸­çš„ä¼ªå½±ã€æŸ“è‰²å·®å¼‚å’Œæˆåƒé™åˆ¶ç­‰å› ç´ ï¼Œç»å¸¸å‡ºç°å™ªå£°ã€æ¨¡ç³Šå’Œä½åˆ†è¾¨ç‡ç­‰é€€åŒ–ç°è±¡ã€‚è€Œä¾èµ–äºç‰©ç†æŸ“è‰²ä¼šå¼•å…¥æˆæœ¬é«˜æ˜‚ã€å»¶è¿Ÿå’Œä¸ä¸€è‡´æ€§ç­‰é—®é¢˜ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•é’ˆå¯¹å»å™ªæˆ–è¶…åˆ†è¾¨ç‡ç­‰å•ä¸ªé—®é¢˜è¿›è¡Œä¼˜åŒ–ï¼Œä½†å®ƒä»¬å…·æœ‰ç‰¹å®šçš„ä»»åŠ¡å¯¼å‘è®¾è®¡ï¼Œç¼ºä¹å¤„ç†å®è·µä¸­é‡åˆ°çš„å¤šæ ·åŒ–ä½å±‚æ¬¡è§†è§‰æŒ‘æˆ˜çš„é€šç”¨æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ç»Ÿä¸€çš„ä½å±‚æ¬¡ç—…ç†åŸºç¡€æ¨¡å‹ï¼ˆLPFMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ¢å¤ä»»åŠ¡ä¸­å¢å¼ºå›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šå’Œå»å™ªç­‰ä»»åŠ¡ï¼Œå¹¶ä¸”èƒ½ä¿ƒè¿›å›¾åƒç¿»è¯‘ä»»åŠ¡ï¼Œå¦‚è™šæ‹ŸæŸ“è‰²ï¼ˆH&amp;EæŸ“è‰²å’Œç‰¹æ®ŠæŸ“è‰²ï¼‰ï¼Œæ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½é€šè¿‡å•ä¸€çš„é€‚åº”æ¶æ„å®ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå¯¹æ¯”é¢„è®­ç»ƒç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨ä»1.9äº¿å¼ æœªæ ‡è®°çš„ç—…ç†å›¾åƒä¸­å­¦ä¹ å¯è¿ç§»çš„ã€æŸ“è‰²ä¸å˜çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œèƒ½å¤Ÿç¨³å¥åœ°è¯†åˆ«é€€åŒ–æ¨¡å¼ã€‚ä¸€ä¸ªç»Ÿä¸€çš„æ¡ä»¶æ‰©æ•£è¿‡ç¨‹é€šè¿‡æ–‡æœ¬æç¤ºåŠ¨æ€é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œç¡®ä¿å¯¹è¾“å‡ºè´¨é‡çš„ç²¾ç¡®æ§åˆ¶ã€‚åœ¨åŒ…å«å¤šç§ç»„ç»‡å’ŒæŸ“è‰²åè®®çš„87,810å¼ å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„æ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒLPFMåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šå‡å®ç°äº†æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯çš„æ”¹è¿›ï¼ˆp&lt;0.01ï¼‰ï¼Œåœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸­å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜äº†10-15%ï¼Œè™šæ‹ŸæŸ“è‰²ä»»åŠ¡çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰æé«˜äº†12-18%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01071v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹å·²ç»åœ¨é«˜çº§è¯Šæ–­ä»»åŠ¡ä¸­å–å¾—äº†é©å‘½æ€§çš„æˆæœï¼Œä½†åœ¨ä½çº§å›¾åƒå¢å¼ºæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç—…ç†å›¾åƒç»å¸¸å—åˆ°å™ªå£°ã€æ¨¡ç³Šå’Œä½åˆ†è¾¨ç‡ç­‰é€€åŒ–é—®é¢˜çš„å½±å“ï¼Œè€Œä¾èµ–äºç‰©ç†æŸ“è‰²å¸¦æ¥çš„æˆæœ¬ã€å»¶è¿Ÿå’Œä¸ä¸€è‡´æ€§ä¹ŸåŠ å‰§äº†è¿™äº›é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦è§£å†³å»å™ªæˆ–è¶…åˆ†è¾¨ç‡ç­‰å•ä¸€é—®é¢˜ï¼Œç¼ºä¹åº”å¯¹å®è·µä¸­é‡åˆ°çš„å„ç§ä½çº§è§†è§‰æŒ‘æˆ˜çš„èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªç»Ÿä¸€çš„ä½çº§ç—…ç†åŸºç¡€æ¨¡å‹ï¼ˆLPFMï¼‰ï¼Œèƒ½å¤Ÿåœ¨æ¢å¤ä»»åŠ¡ä¸­æé«˜å›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šå’Œå»å™ªï¼Œå¹¶ä¿ƒè¿›å›¾åƒç¿»è¯‘ä»»åŠ¡ï¼Œå¦‚è™šæ‹ŸæŸ“è‰²ï¼ˆH&amp;Eå’Œç‰¹æ®ŠæŸ“è‰²ï¼‰ï¼Œé€šè¿‡ä¸€ä¸ªå•ä¸€çš„å¯é€‚åº”æ¶æ„å®ç°ã€‚é€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒç¼–ç å™¨å­¦ä¹ å¯è½¬ç§»ã€æŸ“è‰²ä¸å˜çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶å¼•å…¥ä¸€ä¸ªç»Ÿä¸€çš„æ¡ä»¶æ‰©æ•£è¿‡ç¨‹æ¥åŠ¨æ€é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œç¡®ä¿è¾“å‡ºè´¨é‡çš„ç²¾ç¡®æ§åˆ¶ã€‚åœ¨æ¶µç›–34ç§ç»„ç»‡å’Œ5ç§æŸ“è‰²åè®®çš„87810å¼ å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰æ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒLPFMåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ˆp&lt;0.01ï¼‰ï¼Œå›¾åƒæ¢å¤çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜äº†10-15%ï¼Œè™šæ‹ŸæŸ“è‰²çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰æé«˜äº†12-18%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ¨¡å‹åœ¨é«˜å±‚æ¬¡è¯Šæ–­ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨ä½å±‚æ¬¡å›¾åƒå¢å¼ºæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç—…ç†å›¾åƒé¢ä¸´å™ªå£°ã€æ¨¡ç³Šå’Œä½åˆ†è¾¨ç‡ç­‰é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹åº”å¯¹å¤šç§é—®é¢˜çš„é€šç”¨æ€§ã€‚</li>
<li>æå‡ºçš„ä½çº§ç—…ç†åŸºç¡€æ¨¡å‹ï¼ˆLPFMï¼‰èƒ½å¤Ÿç»Ÿä¸€å¤„ç†å›¾åƒæ¢å¤å’Œè™šæ‹ŸæŸ“è‰²ä»»åŠ¡ã€‚</li>
<li>LPFMé€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒç¼–ç å™¨å­¦ä¹ æŸ“è‰²ä¸å˜çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>LPFMå¼•å…¥ç»Ÿä¸€çš„æ¡ä»¶æ‰©æ•£è¿‡ç¨‹ï¼Œå¯åŠ¨æ€é€‚åº”ä¸åŒä»»åŠ¡ã€‚</li>
<li>LPFMåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬å›¾åƒæ¢å¤çš„PSNRå’Œè™šæ‹ŸæŸ“è‰²çš„SSIMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d20cdddb2abb4730756f79741880a6c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97759328abe853efc3b7f37717a7d882.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aed6f770d902f18fff2c364badcaa54f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b949e0d3274e79cc3b1cfa611452914f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TRUST-Token-dRiven-Ultrasound-Style-Transfer-for-Cross-Device-Adaptation"><a href="#TRUST-Token-dRiven-Ultrasound-Style-Transfer-for-Cross-Device-Adaptation" class="headerlink" title="TRUST: Token-dRiven Ultrasound Style Transfer for Cross-Device   Adaptation"></a>TRUST: Token-dRiven Ultrasound Style Transfer for Cross-Device   Adaptation</h2><p><strong>Authors:Nhat-Tuong Do-Tran, Ngoc-Hoang-Lam Le, Ian Chiu, Po-Tsun Paul Kuo, Ching-Chun Huang</strong></p>
<p>Ultrasound images acquired from different devices exhibit diverse styles, resulting in decreased performance of downstream tasks. To mitigate the style gap, unpaired image-to-image (UI2I) translation methods aim to transfer images from a source domain, corresponding to new device acquisitions, to a target domain where a frozen task model has been trained for downstream applications. However, existing UI2I methods have not explicitly considered filtering the most relevant style features, which may result in translated images misaligned with the needs of downstream tasks. In this work, we propose TRUST, a token-driven dual-stream framework that preserves source content while transferring the common style of the target domain, ensuring that content and style remain unblended. Given multiple styles in the target domain, we introduce a Token-dRiven (TR) module that operates from two perspectives: (1) a data viewâ€“selecting â€œsuitableâ€ target tokens corresponding to each source token, and (2) a model viewâ€“identifying &#96;&#96;optimalâ€ target tokens for the downstream model, guided by a behavior mirror loss. Additionally, we inject auxiliary prompts into the source encoder to match content representation with downstream behavior. Experimental results on ultrasound datasets demonstrate that TRUST outperforms existing UI2I methods in both visual quality and downstream task performance. </p>
<blockquote>
<p>ä»ä¸åŒè®¾å¤‡è·å–çš„è¶…å£°å›¾åƒå‘ˆç°å‡ºå„ç§é£æ ¼ï¼Œå¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†ç¼©å°é£æ ¼å·®è·ï¼Œæ— é…å¯¹å›¾åƒåˆ°å›¾åƒï¼ˆUI2Iï¼‰è½¬æ¢æ–¹æ³•æ—¨åœ¨å°†å›¾åƒä»æºåŸŸï¼ˆå¯¹åº”æ–°è®¾å¤‡é‡‡é›†ï¼‰è½¬ç§»åˆ°ç›®æ ‡åŸŸï¼Œå…¶ä¸­å†»ç»“çš„ä»»åŠ¡æ¨¡å‹å·²é’ˆå¯¹ä¸‹æ¸¸åº”ç”¨è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„UI2Iæ–¹æ³•æ²¡æœ‰æ˜ç¡®è€ƒè™‘è¿‡æ»¤æœ€ç›¸å…³çš„é£æ ¼ç‰¹å¾ï¼Œè¿™å¯èƒ½å¯¼è‡´ç¿»è¯‘åçš„å›¾åƒä¸ä¸‹æ¸¸ä»»åŠ¡çš„éœ€æ±‚ä¸åŒ¹é…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TRUSTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ ‡è®°çš„åŒå‘æµæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæºå†…å®¹çš„åŒæ—¶è½¬ç§»ç›®æ ‡åŸŸçš„å…±åŒé£æ ¼ï¼Œç¡®ä¿å†…å®¹å’Œé£æ ¼ä¿æŒç»Ÿä¸€ã€‚å¯¹äºç›®æ ‡åŸŸä¸­çš„å¤šç§é£æ ¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†Token-dRivenï¼ˆTRï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ä»ä¸¤ä¸ªè§’åº¦è¿›è¡Œæ“ä½œï¼šï¼ˆ1ï¼‰æ•°æ®è§†å›¾â€“é€‰æ‹©æ¯ä¸ªæºæ ‡è®°å¯¹åº”çš„â€œåˆé€‚â€ç›®æ ‡æ ‡è®°ï¼›ï¼ˆ2ï¼‰æ¨¡å‹è§†å›¾â€“ç”±è¡Œä¸ºé•œåƒæŸå¤±å¼•å¯¼ï¼Œä¸ºä¸‹æ¸¸æ¨¡å‹è¯†åˆ«â€œæœ€ä½³â€ç›®æ ‡æ ‡è®°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘æºç¼–ç å™¨æ³¨å…¥è¾…åŠ©æç¤ºï¼Œä»¥åŒ¹é…å†…å®¹è¡¨ç¤ºå’Œä¸‹æ¸¸è¡Œä¸ºã€‚åœ¨è¶…å£°æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTRUSTåœ¨è§†è§‰è´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„UI2Iæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00508v1">PDF</a> Accepted to APSIPA ASC 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¶…å£°å›¾åƒåœ¨ä¸åŒè®¾å¤‡é‡‡é›†æ—¶äº§ç”Ÿçš„é£æ ¼å·®å¼‚å¯¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŸºäºä»¤ç‰Œé©±åŠ¨çš„åŒæµæ¡†æ¶TRUSTï¼Œè¯¥æ¡†æ¶åœ¨è¿ç§»å›¾åƒé£æ ¼æ—¶èƒ½å¤Ÿä¿ç•™æºå†…å®¹ï¼Œå¹¶ç¡®ä¿å†…å®¹å’Œé£æ ¼ä¸æ··æ·†ã€‚é€šè¿‡å¼•å…¥Token-dRivenï¼ˆTRï¼‰æ¨¡å—ï¼Œä»æ•°æ®è§†è§’å’Œæ¨¡å‹è§†è§’é€‰æ‹©é€‚åˆçš„ä»¤ç‰Œï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚åŒæ—¶ï¼Œé€šè¿‡æ³¨å…¥è¾…åŠ©æç¤ºåŒ¹é…å†…å®¹è¡¨ç¤ºä¸ä¸‹æ¸¸è¡Œä¸ºã€‚åœ¨è¶…å£°æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTRUSTåœ¨è§†è§‰è´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰UI2Iæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°å›¾åƒåœ¨ä¸åŒè®¾å¤‡é‡‡é›†æ—¶å­˜åœ¨é£æ ¼å·®å¼‚ï¼Œå½±å“ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ç°è¡Œçš„UI2Iæ–¹æ³•æ²¡æœ‰æ˜ç¡®åœ°è€ƒè™‘è¿‡æ»¤æœ€ç›¸å…³çš„é£æ ¼ç‰¹å¾ï¼Œå¯èƒ½å¯¼è‡´ç¿»è¯‘å›¾åƒä¸ä¸‹æ¸¸ä»»åŠ¡éœ€æ±‚ä¸åŒ¹é…ã€‚</li>
<li>æå‡ºåŸºäºä»¤ç‰Œé©±åŠ¨çš„åŒæµæ¡†æ¶TRUSTï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è§£å†³UI2Iä¸­çš„é£æ ¼è½¬æ¢é—®é¢˜ã€‚</li>
<li>TRUSTèƒ½å¤Ÿä¿ç•™æºå†…å®¹å¹¶è¿ç§»ç›®æ ‡åŸŸçš„å…±åŒé£æ ¼ï¼Œç¡®ä¿å†…å®¹å’Œé£æ ¼çš„åˆ†ç¦»ã€‚</li>
<li>å¼•å…¥Token-dRivenï¼ˆTRï¼‰æ¨¡å—ï¼Œä»æ•°æ®è§†è§’å’Œæ¨¡å‹è§†è§’é€‰æ‹©é€‚åˆçš„ä»¤ç‰Œï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ³¨å…¥è¾…åŠ©æç¤ºåŒ¹é…å†…å®¹è¡¨ç¤ºä¸ä¸‹æ¸¸è¡Œä¸ºï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce5ac8193abea0fcff916c1650b68f50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bee8e39a3dc38b6abcb15878c28af0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1deaf608d1e3a585b255e1c4982125d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e9d910eaa702bda0abdc20ad87a93bf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Application-of-Super-Sampling-to-Microscopy-Images-Produces-Image-Resolution-below-Optical-Diffraction-Limit"><a href="#Application-of-Super-Sampling-to-Microscopy-Images-Produces-Image-Resolution-below-Optical-Diffraction-Limit" class="headerlink" title="Application of Super-Sampling to Microscopy Images Produces Image   Resolution below Optical Diffraction Limit"></a>Application of Super-Sampling to Microscopy Images Produces Image   Resolution below Optical Diffraction Limit</h2><p><strong>Authors:James N. Caron</strong></p>
<p>Image Phase Alignment Super-Sampling (ImPASS) is a computational imaging algorithm for converting a sequence of displaced low-resolution images into a single high-resolution image. The method consists of a unique combination of Phase Correlation image registration and SeDDaRA blind deconvolution. The method has previously been validated in simulations and applied successfully to images captured in a laboratory setting. As discussed here, the performance of ImPASS surpasses similar methods that provide quantitative results. ImPASS is applied for the first time to images taken by a widefield microscope, requiring no customization other than a translation stage, to determine if this approach can subceed the diffraction limit for this application. The 80-frame image sets had as targets a slide with a slice of Porcine Cornea, and a standard US Air Force resolution chart, providing quantitative and quantitative assessments. The sets were up-sampled by a factor of eight, aligned, combined, and processed. The measurement revealed that image resolution improved by a factor of 2.68 and subceeded the diffraction limit by a factor of 1.79. </p>
<blockquote>
<p>å›¾åƒç›¸ä½å¯¹å‡†è¶…é‡‡æ ·ï¼ˆImPASSï¼‰æ˜¯ä¸€ç§è®¡ç®—æˆåƒç®—æ³•ï¼Œå¯å°†ä¸€ç³»åˆ—ä½ç§»çš„ä½åˆ†è¾¨ç‡å›¾åƒè½¬æ¢ä¸ºå•ä¸€çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚è¯¥æ–¹æ³•ç”±ç›¸ä½ç›¸å…³å›¾åƒé…å‡†å’ŒSeDDaRAç›²è§£å·ç§¯çš„ç‹¬ç‰¹ç»„åˆæ„æˆã€‚è¯¥æ–¹æ³•å·²åœ¨æ¨¡æ‹Ÿä¸­å¾—åˆ°éªŒè¯ï¼Œå¹¶æˆåŠŸåº”ç”¨äºå®éªŒå®¤æ•è·çš„å›¾åƒã€‚æ­£å¦‚è¿™é‡Œè®¨è®ºçš„ï¼ŒImPASSçš„æ€§èƒ½è¶…è¿‡äº†æä¾›å®šé‡ç»“æœçš„å…¶ä»–ç±»ä¼¼æ–¹æ³•ã€‚ImPASSé¦–æ¬¡åº”ç”¨äºå®½åœºæ˜¾å¾®é•œæ‹æ‘„çš„å›¾åƒï¼Œé™¤å¹³ç§»å°å¤–ï¼Œæ— éœ€å…¶ä»–å®šåˆ¶ï¼Œä»¥ç¡®å®šæ­¤æ–¹æ³•æ˜¯å¦å¯ä»¥è¶…è¶Šæ­¤åº”ç”¨ç¨‹åºçš„è¡å°„æé™ã€‚80å¸§å›¾åƒé›†çš„ç›®æ ‡æ˜¯ä¸€å—å¸¦æœ‰çŒªè§’è†œåˆ‡ç‰‡å’Œæ ‡å‡†çš„ç¾å›½ç©ºå†›åˆ†è¾¨ç‡å›¾çš„è½½ç»ç‰‡ï¼Œæä¾›äº†å®šé‡å’Œå®šæ€§è¯„ä¼°ã€‚è¿™äº›å›¾åƒé›†ä»¥å…«å€çš„ä¸Šé‡‡æ ·ç‡è¿›è¡Œå¤„ç†ï¼Œå¯¹é½ã€ç»„åˆå’ŒåŠ å·¥ã€‚æµ‹é‡ç»“æœæ˜¾ç¤ºï¼Œå›¾åƒåˆ†è¾¨ç‡æé«˜äº†2.68å€ï¼Œè¶…è¶Šäº†è¡å°„æé™çš„1.79å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21714v1">PDF</a> 12 pages, 8 figures. Extended version first presented at Optica   Biophotonics Congress: Optics in the Life Sciences, 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾åƒç›¸ä½å¯¹é½è¶…é‡‡æ ·ï¼ˆImPASSï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡ä¸€ç³»åˆ—ä½ç§»çš„ä½åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆå•ä¸€çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚ç»“åˆç›¸ä½ç›¸å…³å›¾åƒé…å‡†å’Œç›²å»å·ç§¯æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•æˆåŠŸåº”ç”¨äºå®éªŒå®¤è®¾ç½®çš„å›¾åƒä¸­ï¼Œå¹¶ä¸”åœ¨ä»¿çœŸå’Œå®šé‡ç»“æœä¸Šå‡éªŒè¯äº†å…¶æ€§èƒ½ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é¦–æ¬¡å°†ImPASSåº”ç”¨äºå®½åœºæ˜¾å¾®é•œæ‹æ‘„çš„å›¾åƒï¼Œä¸éœ€è¦å®šåˆ¶åŒ–ï¼Œåªéœ€é€šè¿‡å¹³ç§»é˜¶æ®µå³å¯åº”ç”¨ã€‚é€šè¿‡å¯¹æ¯”è¯•éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•æé«˜äº†å›¾åƒåˆ†è¾¨ç‡å¹¶è¶…è¶Šäº†è¡å°„æé™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ImPASSç®—æ³•å¯ä»¥å°†ä¸€ç³»åˆ—ä½ç§»çš„ä½åˆ†è¾¨ç‡å›¾åƒè½¬åŒ–ä¸ºå•ä¸€é«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li>
<li>è¯¥ç®—æ³•ç»“åˆäº†ç›¸ä½ç›¸å…³å›¾åƒé…å‡†å’Œç›²å»å·ç§¯æŠ€æœ¯ã€‚</li>
<li>ImPASSåœ¨ä»¿çœŸå’Œå®éªŒå®¤è®¾ç½®ä¸­éƒ½å¾—åˆ°äº†éªŒè¯ï¼Œå¹¶ä¸”åœ¨å®šé‡ç»“æœä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>ImPASSé¦–æ¬¡åº”ç”¨äºå®½åœºæ˜¾å¾®é•œæ‹æ‘„çš„å›¾åƒã€‚</li>
<li>åº”ç”¨ImPASSåï¼Œå›¾åƒåˆ†è¾¨ç‡æé«˜äº†2.68å€ã€‚</li>
<li>ImPASSè¶…è¶Šäº†è¡å°„æé™ï¼Œæé«˜äº†æˆåƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b32444b995dd056fc4c301a8d2db624.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78e706eface25a688dcf2d38eecd98f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdf5c6a303ea67e64503a1e7bdb76d9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8ab65cccaf69931af7bd840dcb90ca1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-773acb141b688c0d17dcf3a8b8efb386.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Why-Stop-at-Words-Unveiling-the-Bigger-Picture-through-Line-Level-OCR"><a href="#Why-Stop-at-Words-Unveiling-the-Bigger-Picture-through-Line-Level-OCR" class="headerlink" title="Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR"></a>Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR</h2><p><strong>Authors:Shashank Vempati, Nishit Anand, Gaurav Talebailkar, Arpan Garai, Chetan Arora</strong></p>
<p>Conventional optical character recognition (OCR) techniques segmented each character and then recognized. This made them prone to error in character segmentation, and devoid of context to exploit language models. Advances in sequence to sequence translation in last decade led to modern techniques first detecting words and then inputting one word at a time to a model to directly output full words as sequence of characters. This allowed better utilization of language models and bypass error-prone character segmentation step. We observe that the above transition in style has moved the bottleneck in accuracy to word segmentation. Hence, in this paper, we propose a natural and logical progression from word level OCR to line-level OCR. The proposal allows to bypass errors in word detection, and provides larger sentence context for better utilization of language models. We show that the proposed technique not only improves the accuracy but also efficiency of OCR. Despite our thorough literature survey, we did not find any public dataset to train and benchmark such shift from word to line-level OCR. Hence, we also contribute a meticulously curated dataset of 251 English page images with line-level annotations. Our experimentation revealed a notable end-to-end accuracy improvement of 5.4%, underscoring the potential benefits of transitioning towards line-level OCR, especially for document images. We also report a 4 times improvement in efficiency compared to word-based pipelines. With continuous improvements in large language models, our methodology also holds potential to exploit such advances. Project Website: <a target="_blank" rel="noopener" href="https://nishitanand.github.io/line-level-ocr-website">https://nishitanand.github.io/line-level-ocr-website</a> </p>
<blockquote>
<p>ä¼ ç»Ÿçš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æŠ€æœ¯é€šè¿‡å¯¹æ¯ä¸ªå­—ç¬¦è¿›è¡Œåˆ†å‰²ç„¶åå†è¿›è¡Œè¯†åˆ«ã€‚è¿™ä½¿å¾—å®ƒä»¬åœ¨å­—ç¬¦åˆ†å‰²æ—¶å®¹æ˜“å‡ºé”™ï¼Œå¹¶ä¸”æ— æ³•åˆ©ç”¨ä¸Šä¸‹æ–‡æ¥åˆ©ç”¨è¯­è¨€æ¨¡å‹ã€‚è¿‡å»åå¹´ä¸­åºåˆ—åˆ°åºåˆ—ç¿»è¯‘çš„è¿›å±•å¯¼è‡´äº†ç°ä»£æŠ€æœ¯é¦–å…ˆæ£€æµ‹å•è¯ï¼Œç„¶åä¸€æ¬¡è¾“å…¥ä¸€ä¸ªå•è¯åˆ°æ¨¡å‹ä¸­ï¼Œç›´æ¥è¾“å‡ºç”±å­—ç¬¦åºåˆ—ç»„æˆçš„å®Œæ•´å•è¯ã€‚è¿™å…è®¸æ›´å¥½åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹ï¼Œå¹¶ç»•è¿‡å®¹æ˜“å‡ºç°é”™è¯¯çš„å­—ç¬¦åˆ†å‰²æ­¥éª¤ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸Šè¿°é£æ ¼çš„å˜åŒ–å·²å°†å‡†ç¡®æ€§çš„ç“¶é¢ˆè½¬ç§»åˆ°å•è¯åˆ†å‰²ä¸Šã€‚å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»å•è¯çº§OCRåˆ°è¡Œçº§OCRçš„è‡ªç„¶ä¸”é€»è¾‘æ€§çš„è¿›æ­¥ã€‚è¯¥æè®®å…è®¸ç»•è¿‡å•è¯æ£€æµ‹ä¸­çš„é”™è¯¯ï¼Œå¹¶ä¸ºæ›´å¥½åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹æä¾›äº†æ›´å¤§çš„å¥å­ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æŠ€æœ¯ä¸ä»…æé«˜äº†OCRçš„å‡†ç¡®æ€§ï¼Œè¿˜æé«˜äº†å…¶æ•ˆç‡ã€‚å°½ç®¡æˆ‘ä»¬è¿›è¡Œäº†å½»åº•çš„æ–‡çŒ®è°ƒæŸ¥ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…¬å…±æ•°æ®é›†æ¥è®­ç»ƒå’Œè¯„ä¼°ä»å•è¯åˆ°è¡Œçº§OCRçš„è¿™ç§è½¬å˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿˜ç²¾å¿ƒåˆ¶ä½œäº†ä¸€ä¸ªåŒ…å«251ä¸ªè‹±è¯­é¡µé¢å›¾åƒçš„è¡Œçº§æ³¨é‡Šæ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºç«¯åˆ°ç«¯å‡†ç¡®ç‡æé«˜äº†5.4%ï¼Œè¿™çªæ˜¾äº†è½¬å‘è¡Œçº§OCRçš„æ½œåœ¨æ•ˆç›Šï¼Œå°¤å…¶æ˜¯å¯¹äºæ–‡æ¡£å›¾åƒã€‚æˆ‘ä»¬è¿˜æŠ¥å‘Šäº†ä¸åŸºäºå•è¯çš„ç®¡é“ç›¸æ¯”ï¼Œæ•ˆç‡æé«˜äº†4å€ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸æ–­æ”¹è¿›ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿå…·æœ‰åˆ©ç”¨è¿™äº›è¿›å±•çš„æ½œåŠ›ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://nishitanand.github.io/line-level-ocr-website">https://nishitanand.github.io/line-level-ocr-website</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21693v1">PDF</a> 11 pages. Project Website:   <a target="_blank" rel="noopener" href="https://nishitanand.github.io/line-level-ocr-website">https://nishitanand.github.io/line-level-ocr-website</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¼ ç»Ÿå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æŠ€æœ¯çš„å±€é™ï¼ŒåŒ…æ‹¬å­—ç¬¦åˆ†å‰²æ˜“å‡ºé”™å’Œç¼ºä¹è¯­å¢ƒåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„é—®é¢˜ã€‚éšç€åºåˆ—åˆ°åºåˆ—ç¿»è¯‘æŠ€æœ¯çš„è¿›æ­¥ï¼Œç°ä»£OCRæŠ€æœ¯å¼€å§‹ç›´æ¥æ£€æµ‹å•è¯å¹¶è¾“å…¥æ¨¡å‹è¿›è¡Œè¯†åˆ«ï¼Œä»è€Œæé«˜äº†è¯­è¨€æ¨¡å‹çš„åˆ©ç”¨ç‡å¹¶é¿å…äº†æ˜“é”™çš„å­—ç¬¦åˆ†å‰²æ­¥éª¤ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°è¿™ä¸€è½¬å˜æé«˜äº†è¯†åˆ«å‡†ç¡®æ€§ï¼Œå¹¶å°†ç“¶é¢ˆè½¬ç§»åˆ°å•è¯çº§åˆ«çš„åˆ†å‰²ä¸Šã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä»å•è¯çº§åˆ«OCRåˆ°è¡Œçº§åˆ«OCRçš„è‡ªç„¶é€»è¾‘å‘å±•ï¼Œé€šè¿‡ç»•è¿‡å•è¯æ£€æµ‹ä¸­çš„é”™è¯¯å¹¶æä¾›æ›´å¤§çš„å¥å­è¯­å¢ƒæ¥æ›´å¥½åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè¿˜æé«˜äº†OCRçš„æ•ˆç‡ã€‚åŒæ—¶ï¼Œç”±äºæ²¡æœ‰æ‰¾åˆ°ç›¸åº”çš„å…¬å…±æ•°æ®é›†æ¥è®­ç»ƒå’Œè¯„ä¼°è¿™ç§ä»å•è¯åˆ°è¡Œçº§åˆ«çš„OCRè½¬å˜ï¼Œæœ¬æ–‡è¿˜è´¡çŒ®äº†ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„åŒ…å«251é¡µè‹±æ–‡å›¾åƒçš„è¡Œçº§åˆ«æ³¨é‡Šæ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç«¯åˆ°ç«¯çš„å‡†ç¡®ç‡æé«˜äº†5.4%ï¼Œä½“ç°äº†å‘è¡Œçº§åˆ«OCRè¿‡æ¸¡çš„æ½œåœ¨ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æ¡£å›¾åƒæ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»ŸOCRæŠ€æœ¯é€šè¿‡å­—ç¬¦åˆ†å‰²è¿›è¡Œè¯†åˆ«ï¼Œå­˜åœ¨æ˜“é”™å’Œç¼ºä¹è¯­å¢ƒçš„é—®é¢˜ã€‚</li>
<li>ç°ä»£OCRæŠ€æœ¯é€šè¿‡æ£€æµ‹å•è¯è¿›è¡Œè¯†åˆ«ï¼Œæé«˜äº†è¯­è¨€æ¨¡å‹çš„åˆ©ç”¨ç‡å¹¶é¿å…äº†å­—ç¬¦åˆ†å‰²çš„é”™è¯¯ã€‚</li>
<li>OCRæŠ€æœ¯çš„ç“¶é¢ˆå·²ä»å­—ç¬¦çº§åˆ«è½¬ç§»åˆ°å•è¯çº§åˆ«åˆ†å‰²ã€‚</li>
<li>æœ¬æ–‡æå‡ºä»å•è¯çº§åˆ«OCRåˆ°è¡Œçº§åˆ«OCRçš„è‡ªç„¶è¿‡æ¸¡ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>è¡Œçº§åˆ«OCRèƒ½å¤Ÿç»•è¿‡å•è¯æ£€æµ‹ä¸­çš„é”™è¯¯ï¼Œå¹¶æä¾›æ›´å¤§çš„å¥å­è¯­å¢ƒä»¥æ›´å¥½åœ°åˆ©ç”¨è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ç¼ºä¹å…¬å…±æ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°è¿™ç§ä»å•è¯åˆ°è¡Œçº§åˆ«çš„OCRè½¬å˜ã€‚</li>
<li>æœ¬æ–‡è´¡çŒ®äº†ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„è¡Œçº§åˆ«æ³¨é‡Šæ•°æ®é›†ï¼Œå¹¶æŠ¥å‘Šäº†æ˜¾è‘—çš„ç«¯åˆ°ç«¯å‡†ç¡®ç‡å’Œæ•ˆç‡æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-01c60fe73a706cd191be6d178f73a2de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-650026a5c03df615d42f14e98ed1734a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-294647ee33c086f27c7b3e13310a5c38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86e4b8e4f422f39091c4c7ef3cffb89d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-864593d183016a652903d0632f514a35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ef327fe1af98f482fa5cf0fc1822355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbbba33a26574603c5db846c5da8f6a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab36555f61f5c791131b2e9fc9e6c11b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MedShift-Implicit-Conditional-Transport-for-X-Ray-Domain-Adaptation"><a href="#MedShift-Implicit-Conditional-Transport-for-X-Ray-Domain-Adaptation" class="headerlink" title="MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation"></a>MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation</h2><p><strong>Authors:Francisco Caetano, Christiaan Viviers, Peter H. H. de With, Fons van der Sommen</strong></p>
<p>Synthetic medical data offers a scalable solution for training robust models, but significant domain gaps limit its generalizability to real-world clinical settings. This paper addresses the challenge of cross-domain translation between synthetic and real X-ray images of the head, focusing on bridging discrepancies in attenuation behavior, noise characteristics, and soft tissue representation. We propose MedShift, a unified class-conditional generative model based on Flow Matching and Schrodinger Bridges, which enables high-fidelity, unpaired image translation across multiple domains. Unlike prior approaches that require domain-specific training or rely on paired data, MedShift learns a shared domain-agnostic latent space and supports seamless translation between any pair of domains seen during training. We introduce X-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays under varying radiation doses, to benchmark domain translation models. Experimental results demonstrate that, despite its smaller model size compared to diffusion-based approaches, MedShift offers strong performance and remains flexible at inference time, as it can be tuned to prioritize either perceptual fidelity or structural consistency, making it a scalable and generalizable solution for domain adaptation in medical imaging. The code and dataset are available at <a target="_blank" rel="noopener" href="https://caetas.github.io/medshift.html">https://caetas.github.io/medshift.html</a> </p>
<blockquote>
<p>åˆæˆåŒ»ç–—æ•°æ®ä¸ºè®­ç»ƒç¨³å¥æ¨¡å‹æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºé¢†åŸŸå·®è·è¾ƒå¤§ï¼Œå…¶åœ¨çœŸå®ä¸–ç•Œä¸´åºŠç¯å¢ƒä¸­çš„é€šç”¨æ€§å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡é’ˆå¯¹åˆæˆå’ŒçœŸå®å¤´éƒ¨Xå°„çº¿å›¾åƒè·¨åŸŸç¿»è¯‘çš„æŒ‘æˆ˜ï¼Œä¸“æ³¨äºè§£å†³è¡°å‡è¡Œä¸ºã€å™ªå£°ç‰¹å¾å’Œè½¯ç»„ç»‡è¡¨ç¤ºæ–¹é¢çš„å·®å¼‚ã€‚æˆ‘ä»¬æå‡ºäº†MedShiftï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæµåŒ¹é…å’Œè–›å®šè°”æ¡¥çš„ç»Ÿä¸€çš„ç±»æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°å¤šä¸ªé¢†åŸŸä¹‹é—´çš„é«˜ä¿çœŸã€æ— é…å¯¹å›¾åƒç¿»è¯‘ã€‚ä¸éœ€è¦ç‰¹å®šé¢†åŸŸè®­ç»ƒæˆ–ä¾èµ–é…å¯¹æ•°æ®çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒMedShiftå­¦ä¹ å…±äº«çš„é¢†åŸŸæ— å…³æ½œåœ¨ç©ºé—´ï¼Œå¹¶æ”¯æŒåœ¨è®­ç»ƒæœŸé—´çœ‹åˆ°çš„ä»»ä½•ä¸€å¯¹é¢†åŸŸä¹‹é—´è¿›è¡Œæ— ç¼ç¿»è¯‘ã€‚æˆ‘ä»¬å¼•å…¥äº†X-DigiSkullï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«ä¸åŒè¾å°„å‰‚é‡ä¸‹çš„åˆæˆå’ŒçœŸå®é¢…éª¨Xå°„çº¿çš„å¯¹é½å›¾åƒï¼Œä»¥è¯„ä¼°é¢†åŸŸç¿»è¯‘æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ä¸åŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼ŒMedShiftçš„æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä½†åœ¨æ¨ç†æ—¶é—´å´è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½å’Œçµæ´»æ€§ï¼Œå› ä¸ºå®ƒå¯ä»¥è°ƒæ•´ä»¥ä¼˜å…ˆæ³¨é‡æ„ŸçŸ¥ä¿çœŸåº¦æˆ–ç»“æ„ä¸€è‡´æ€§ï¼Œä½¿å…¶æˆä¸ºåŒ»å­¦å½±åƒé¢†åŸŸè‡ªé€‚åº”çš„å¯æ‰©å±•å’Œé€šç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://caetas.github.io/medshift.html%E6%89%BE%E5%88%B0%E3%80%82">https://caetas.github.io/medshift.htmlæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21435v1">PDF</a> Accepted at the ICCV 2025 AIM Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆæˆåŒ»ç–—æ•°æ®åœ¨è®­ç»ƒç¨³å¥æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œä½†ç”±äºé¢†åŸŸå·®è·è¾ƒå¤§ï¼Œå…¶åœ¨çœŸå®ä¸´åºŠç¯å¢ƒä¸­çš„é€šç”¨æ€§å—åˆ°é™åˆ¶ã€‚é’ˆå¯¹å¤´éƒ¨åˆæˆXå°„çº¿å›¾åƒä¸çœŸå®Xå°„çº¿å›¾åƒä¹‹é—´çš„è·¨åŸŸç¿»è¯‘é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…å’Œè–›å®šè°”æ¡¥çš„é€šç”¨ç±»åˆ«æ¡ä»¶ç”Ÿæˆæ¨¡å‹MedShiftã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸä¹‹é—´å®ç°é«˜ä¿çœŸã€æ— é…å¯¹çš„å›¾åƒç¿»è¯‘ã€‚ç›¸è¾ƒäºéœ€è¦ç‰¹å®šé¢†åŸŸè®­ç»ƒæˆ–ä¾èµ–é…å¯¹æ•°æ®çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒMedShiftå­¦ä¹ äº†ä¸€ä¸ªé€šç”¨çš„é¢†åŸŸæ— å…³æ½œåœ¨ç©ºé—´ï¼Œå¹¶æ”¯æŒåœ¨è®­ç»ƒæœŸé—´çœ‹åˆ°çš„ä»»æ„ä¸¤ä¸ªé¢†åŸŸé—´çš„æ— ç¼ç¿»è¯‘ã€‚ä¸ºè¯„ä¼°é¢†åŸŸç¿»è¯‘æ¨¡å‹ï¼Œæœ¬æ–‡å¼•å…¥äº†åŒ…å«ä¸åŒè¾å°„å‰‚é‡ä¸‹çš„åˆæˆä¸çœŸå®é¢…éª¨Xå°„çº¿å¯¹é½æ•°æ®çš„æ–°æ•°æ®é›†X-DigiSkullã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶MedShiftçš„æ¨¡å‹å°ºå¯¸å°äºåŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œä½†å…¶æ€§èƒ½å¼ºå¤§ä¸”æ¨ç†çµæ´»ï¼Œå¯è°ƒæ•´ä»¥ä¼˜å…ˆä¿è¯æ„ŸçŸ¥ä¿çœŸåº¦æˆ–ç»“æ„ä¸€è‡´æ€§ï¼Œæˆä¸ºåŒ»å­¦æˆåƒé¢†åŸŸè‡ªé€‚åº”çš„å¯æ‰©å±•å’Œé€šç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆåŒ»ç–—æ•°æ®ä¸ºè®­ç»ƒç¨³å¥æ¨¡å‹æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å…¶åœ¨çœŸå®ä¸´åºŠç¯å¢ƒä¸­çš„é€šç”¨æ€§å—é™ã€‚</li>
<li>è®ºæ–‡èšç„¦äºå¤´éƒ¨åˆæˆXå°„çº¿å›¾åƒå’ŒçœŸå®Xå°„çº¿å›¾åƒä¹‹é—´çš„è·¨åŸŸç¿»è¯‘æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åŸºäºæµåŒ¹é…å’Œè–›å®šè°”æ¡¥çš„MedShiftæ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸã€æ— é…å¯¹çš„è·¨é¢†åŸŸå›¾åƒç¿»è¯‘ã€‚</li>
<li>MedShiftå­¦ä¹ é€šç”¨çš„é¢†åŸŸæ— å…³æ½œåœ¨ç©ºé—´ï¼Œæ”¯æŒä»»æ„ä¸¤ä¸ªé¢†åŸŸé—´çš„æ— ç¼ç¿»è¯‘ã€‚</li>
<li>å¼•å…¥X-DigiSkullæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°åˆæˆä¸çœŸå®é¢…éª¨Xå°„çº¿çš„é¢†åŸŸç¿»è¯‘æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMedShiftè™½å°ä½†æ€§èƒ½å¼ºå¤§ï¼Œå¹¶èƒ½åœ¨æ¨ç†æ—¶çµæ´»è°ƒæ•´ä»¥å¹³è¡¡æ„ŸçŸ¥ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-712a63af933f870734388ceaa442a30b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca863af09d2e7594cae3e7430785df5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38d112750126d342946be03774bcab53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9411ead24bd2e786add19b724748f51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c708db506e663cb14f91a73cd973daa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8817a585424fc2da53a0f4129835a36e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2793e6a9ea7abfd2b8a88af1e5aea4a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HERMES-Human-to-Robot-Embodied-Learning-from-Multi-Source-Motion-Data-for-Mobile-Dexterous-Manipulation"><a href="#HERMES-Human-to-Robot-Embodied-Learning-from-Multi-Source-Motion-Data-for-Mobile-Dexterous-Manipulation" class="headerlink" title="HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data   for Mobile Dexterous Manipulation"></a>HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data   for Mobile Dexterous Manipulation</h2><p><strong>Authors:Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu</strong></p>
<p>Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:<a target="_blank" rel="noopener" href="https://gemcollector.github.io/HERMES/">https://gemcollector.github.io/HERMES/</a>. </p>
<blockquote>
<p>åˆ©ç”¨äººç±»è¿åŠ¨æ•°æ®èµ‹äºˆæœºå™¨äººå¤šç§æ“ä½œæŠ€èƒ½å·²æˆä¸ºæœºå™¨äººæ“ä½œä¸­çš„ä¸€é¡¹æœ‰å‰é€”çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œå°†å¤šæºäººç±»æ‰‹åŠ¿åŠ¨ä½œè½¬åŒ–ä¸ºå¯è¡Œçš„æœºå™¨äººè¡Œä¸ºä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé…å¤‡å¤šæŒ‡çµå·§æ‰‹ä¸”åŠ¨ä½œç©ºé—´å¤æ‚ã€é«˜ç»´åº¦çš„æœºå™¨äººã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥äº§ç”Ÿèƒ½å¤Ÿé€‚åº”å¤šç§ç¯å¢ƒæ¡ä»¶çš„ç­–ç•¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HERMESï¼Œè¿™æ˜¯ä¸€ç§äººç±»åˆ°æœºå™¨äººçš„å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºç§»åŠ¨åŒæ‰‹åŠ¨ä½œçµå·§æ“ä½œã€‚é¦–å…ˆï¼ŒHERMESåˆ¶å®šäº†ä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿæ— ç¼åœ°å°†æ¥è‡ªå¤šä¸ªæºå¤´çš„ä¸åŒäººç±»æ‰‹åŠ¿åŠ¨ä½œè½¬åŒ–ä¸ºç‰©ç†ä¸Šå¯è¡Œçš„æœºå™¨äººè¡Œä¸ºã€‚å…¶æ¬¡ï¼Œä¸ºäº†ç¼“è§£æ¨¡æ‹Ÿåˆ°ç°å®çš„å·®è·ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæ·±åº¦å›¾åƒçš„ç«¯åˆ°ç«¯æ¨¡æ‹Ÿåˆ°ç°å®çš„è½¬ç§»æ–¹æ³•ï¼Œä»¥æé«˜å¯¹çœŸå®åœºæ™¯çš„æ€»æ‹¬èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†åœ¨å¤šå˜å’Œéç»“æ„åŒ–çš„ç¯å¢ƒä¸­å®ç°è‡ªä¸»æ“ä½œï¼Œæˆ‘ä»¬å¢åŠ äº†åŸºäºé—­ç¯é€è§†nç‚¹ï¼ˆPnPï¼‰å®šä½æœºåˆ¶çš„å¯¼èˆªåŸºç¡€æ¨¡å‹ï¼Œç¡®ä¿è§†è§‰ç›®æ ‡çš„ç²¾ç¡®å¯¹é½ï¼Œæœ‰æ•ˆæ¡¥æ¥è‡ªä¸»å¯¼èˆªå’Œçµå·§æ“ä½œã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHERMESåœ¨å¤šç§é‡å¤–åœºæ™¯ä¸­è¡¨ç°å‡ºæŒç»­çš„æ€»æ‹¬èƒ½åŠ›ï¼ŒæˆåŠŸå®Œæˆä¼—å¤šå¤æ‚çš„ç§»åŠ¨åŒæ‰‹çµå·§æ“ä½œä»»åŠ¡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://gemcollector.github.io/HERMES/%E3%80%82">https://gemcollector.github.io/HERMES/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20085v3">PDF</a> </p>
<p><strong>Summary</strong><br>äººç±»åŠ¨ä½œæ•°æ®èµ‹äºˆæœºå™¨äººé€šç”¨æ“æ§æŠ€èƒ½å·²æˆä¸ºæœºå™¨äººæ“æ§é¢†åŸŸçš„ä¸€ç§æœ‰å‰é€”çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œå°†å¤šæºäººç±»æ‰‹éƒ¨åŠ¨ä½œè½¬åŒ–ä¸ºå¯è¡Œçš„æœºå™¨äººè¡Œä¸ºä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé…å¤‡å¤æ‚ã€é«˜ç»´åº¦åŠ¨ä½œç©ºé—´çš„çµå·§å¤šæŒ‡æœºå™¨äººã€‚æœ¬æ–‡ä»‹ç»HERMESï¼Œä¸€ç§ç”¨äºç§»åŠ¨çµå·§åŒæ‰‹åŠ¨ä½œæ“æ§çš„äººç±»åˆ°æœºå™¨äººå­¦ä¹ æ¡†æ¶ã€‚å®ƒä½¿ç”¨ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ— ç¼è½¬åŒ–æ¥è‡ªå¤šä¸ªæ¥æºçš„å¼‚è´¨äººç±»æ‰‹éƒ¨åŠ¨ä½œä¸ºç‰©ç†åˆç†çš„æœºå™¨äººè¡Œä¸ºã€‚åŒæ—¶ï¼Œé‡‡ç”¨ç«¯åˆ°ç«¯çš„æ·±åº¦å›¾åƒsim2realè½¬ç§»æ–¹æ³•å‡å°‘æ¨¡æ‹Ÿåˆ°ç°å®çš„å·®è·ï¼Œå¢å¼ºåœ¨ç°å®ä¸–ç•Œçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºåœ¨å¤šå˜å’Œéç»“æ„åŒ–ç¯å¢ƒä¸­å®ç°è‡ªä¸»æ“ä½œï¼Œæˆ‘ä»¬æ•´åˆå¯¼èˆªåŸºç¡€æ¨¡å‹ä¸é—­ç¯Perspective-n-Pointï¼ˆPnPï¼‰å®šä½æœºåˆ¶ï¼Œç¡®ä¿è§†è§‰ç›®æ ‡çš„ç²¾ç¡®å¯¹é½ï¼Œæœ‰æ•ˆæ¡¥æ¥è‡ªä¸»å¯¼èˆªä¸çµå·§æ“æ§ã€‚å®éªŒç»“æœè¯æ˜HERMESåœ¨ä¸åŒåœºæ™¯çš„é€šç”¨è¡Œä¸ºè¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸæ‰§è¡Œå¤šç§å¤æ‚ç§»åŠ¨çµå·§æ“æ§ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨äººç±»åŠ¨ä½œæ•°æ®èµ‹äºˆæœºå™¨äººé€šç”¨æ“æ§æŠ€èƒ½å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„èŒƒå¼ã€‚</li>
<li>å°†å¤šæºäººç±»æ‰‹éƒ¨åŠ¨ä½œè½¬åŒ–ä¸ºæœºå™¨äººè¡Œä¸ºå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯é«˜ç»´åº¦åŠ¨ä½œç©ºé—´çš„å¤æ‚æœºå™¨äººã€‚</li>
<li>HERMESæ¡†æ¶ä½¿ç”¨ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ–¹æ³•è½¬åŒ–äººç±»æ‰‹éƒ¨åŠ¨ä½œä¸ºæœºå™¨äººè¡Œä¸ºã€‚</li>
<li>HERMESé‡‡ç”¨æ·±åº¦å›¾åƒsim2realè½¬ç§»æ–¹æ³•å‡å°‘æ¨¡æ‹Ÿä¸ç°å®çš„å·®è·ã€‚</li>
<li>ç»“åˆå¯¼èˆªæ¨¡å‹ä¸é—­ç¯PnPå®šä½æœºåˆ¶ï¼Œæå‡æœºå™¨äººåœ¨å¤šå˜ç¯å¢ƒä¸­çš„è‡ªä¸»æ“ä½œèƒ½åŠ›ã€‚</li>
<li>HERMESåœ¨å¤šç§å¤æ‚ç§»åŠ¨çµå·§æ“æ§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3555804c83dceb9a1e786da9c40bc51c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-665bb438802a8ab0fef6157ec49a6342.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66cedcf16373d760151c4fe828b53bfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8127ba2976f06d756e0ca28c98c82843.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-624d5b415ad7081af3fa36c76bc5ec89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac42df3af75915cec6c10b1da2b59309.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a3e0108eeb9f34de4a2205aa7caae5d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Is-Uncertainty-Quantification-a-Viable-Alternative-to-Learned-Deferral"><a href="#Is-Uncertainty-Quantification-a-Viable-Alternative-to-Learned-Deferral" class="headerlink" title="Is Uncertainty Quantification a Viable Alternative to Learned Deferral?"></a>Is Uncertainty Quantification a Viable Alternative to Learned Deferral?</h2><p><strong>Authors:Anna M. Wundram, Christian F. Baumgartner</strong></p>
<p>Artificial Intelligence (AI) holds the potential to dramatically improve patient care. However, it is not infallible, necessitating human-AI-collaboration to ensure safe implementation. One aspect of AI safety is the modelsâ€™ ability to defer decisions to a human expert when they are likely to misclassify autonomously. Recent research has focused on methods that learn to defer by optimising a surrogate loss function that finds the optimal trade-off between predicting a class label or deferring. However, during clinical translation, models often face challenges such as data shift. Uncertainty quantification methods aim to estimate a modelâ€™s confidence in its predictions. However, they may also be used as a deferral strategy which does not rely on learning from specific training distribution. We hypothesise that models developed to quantify uncertainty are more robust to out-of-distribution (OOD) input than learned deferral models that have been trained in a supervised fashion. To investigate this hypothesis, we constructed an extensive evaluation study on a large ophthalmology dataset, examining both learned deferral models and established uncertainty quantification methods, assessing their performance in- and out-of-distribution. Specifically, we evaluate their ability to accurately classify glaucoma from fundus images while deferring cases with a high likelihood of error. We find that uncertainty quantification methods may be a promising choice for AI deferral. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æœ‰æ½œåŠ›å¤§å¹…æ”¹å–„æ‚£è€…æŠ¤ç†çš„è´¨é‡ã€‚ç„¶è€Œï¼Œå®ƒå¹¶éæ— æ‰€ä¸èƒ½ï¼Œå› æ­¤éœ€è¦äººç±»-AIåä½œä»¥ç¡®ä¿å®‰å…¨å®æ–½ã€‚äººå·¥æ™ºèƒ½å®‰å…¨çš„ä¸€ä¸ªæ–¹é¢æ˜¯ï¼Œå½“å®ƒä»¬æœ‰å¯èƒ½è‡ªä¸»è¯¯åˆ†ç±»æ—¶ï¼Œæ¨¡å‹èƒ½å¤Ÿå°†å†³ç­–æƒè½¬äº¤ç»™äººç±»ä¸“å®¶ã€‚æœ€è¿‘çš„ç ”ç©¶é›†ä¸­åœ¨é€šè¿‡ä¼˜åŒ–ä»£ç†æŸå¤±å‡½æ•°æ¥å­¦ä¹ æ¨è¿Ÿå†³ç­–çš„æ–¹æ³•ï¼Œè¯¥æŸå¤±å‡½æ•°æ—¨åœ¨æ‰¾åˆ°é¢„æµ‹ç±»åˆ«æ ‡ç­¾æˆ–æ¨è¿Ÿä¹‹é—´çš„æœ€ä½³æƒè¡¡ã€‚ç„¶è€Œï¼Œåœ¨ä¸´åºŠç¿»è¯‘è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ç»å¸¸é¢ä¸´æ•°æ®è¿ç§»ç­‰æŒ‘æˆ˜ã€‚ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•æ—¨åœ¨ä¼°è®¡æ¨¡å‹å¯¹å…¶é¢„æµ‹çš„ä¿¡å¿ƒã€‚ä½†å®ƒä»¬ä¹Ÿå¯ä»¥ä½œä¸ºä¸€ç§ä¸ä¾èµ–äºä»ç‰¹å®šè®­ç»ƒåˆ†å¸ƒä¸­å­¦ä¹ çš„æ¨è¿Ÿç­–ç•¥ã€‚æˆ‘ä»¬å‡è®¾ï¼Œä¸ç»è¿‡ç›‘ç£è®­ç»ƒçš„å­¦ä¹ æ¨è¿Ÿæ¨¡å‹ç›¸æ¯”ï¼Œå¼€å‘ç”¨äºé‡åŒ–ä¸ç¡®å®šæ€§çš„æ¨¡å‹å¯¹è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰çš„è¾“å…¥æ›´å…·é²æ£’æ€§ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬åœ¨å¤§å‹çœ¼ç§‘æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„è¯„ä¼°ç ”ç©¶ï¼Œç ”ç©¶äº†å­¦ä¹ æ¨è¿Ÿæ¨¡å‹å’Œå·²å»ºç«‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•çš„è¡¨ç°ï¼Œè¯„ä¼°å…¶åœ¨å†…éƒ¨å’Œå¤–éƒ¨çš„åˆ†å¸ƒä¸­çš„è¡¨ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å®ƒä»¬ä»çœ¼åº•å›¾åƒä¸­å‡†ç¡®åˆ†ç±»é’å…‰çœ¼çš„èƒ½åŠ›ï¼ŒåŒæ—¶æ¨è¿Ÿå¯èƒ½å‡ºç°é”™è¯¯çš„æƒ…å†µã€‚æˆ‘ä»¬å‘ç°ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•å¯èƒ½æ˜¯AIæ¨è¿Ÿå†³ç­–çš„ä¸€ä¸ªæœ‰å‰é€”çš„é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02319v2">PDF</a> Accepted as an oral presentation at MICCAI UNSURE 2025</p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æœ‰æ½œåŠ›å¤§å¹…æ”¹å–„æ‚£è€…æŠ¤ç†æ°´å¹³ï¼Œä½†å…¶å¹¶éä¸‡èƒ½ï¼Œéœ€è¦äººç±»-AIåä½œä»¥ç¡®ä¿å®‰å…¨å®æ–½ã€‚AIæ¨¡å‹åœ¨è‡ªä¸»å†³ç­–æ—¶ï¼Œå¦‚é¢„æ–™ä¼šè¯¯åˆ¤ï¼Œéœ€å…·å¤‡è½¬å‘äººç±»ä¸“å®¶å†³ç­–çš„èƒ½åŠ›ã€‚æœ€æ–°ç ”ç©¶èšç„¦äºé€šè¿‡ä¼˜åŒ–æ›¿ä»£æŸå¤±å‡½æ•°æ¥å­¦ä¹ è½¬å‘çš„æ–¹æ³•ï¼Œä»¥æ‰¾åˆ°é¢„æµ‹ç±»åˆ«æˆ–è½¬å‘ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚ä½†åœ¨ä¸´åºŠç¿»è¯‘ä¸­ï¼Œæ¨¡å‹å¸¸é¢ä¸´æ•°æ®è¿ç§»ç­‰æŒ‘æˆ˜ã€‚ç ”ç©¶å‡è®¾ï¼Œå¼€å‘ç”¨äºé‡åŒ–ä¸ç¡®å®šæ€§çš„æ¨¡å‹ï¼Œç›¸è¾ƒäºåœ¨ç›‘ç£æ¨¡å¼ä¸‹è®­ç»ƒå¾—åˆ°çš„å­¦ä¼šè½¬å‘æ¨¡å‹ï¼Œæ›´èƒ½ç¨³å¥åº”å¯¹ç¦»ç¾¤è¾“å…¥ã€‚ä¸ºéªŒè¯æ­¤å‡è®¾ï¼Œæˆ‘ä»¬åœ¨å¤§å‹çœ¼ç§‘æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸€é¡¹å…¨é¢è¯„ä¼°ç ”ç©¶ï¼Œå¯¹æ¯”å­¦ä¼šè½¬å‘æ¨¡å‹å’Œå·²å»ºç«‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œè¯„ä¼°å…¶åœ¨å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ä¸­çš„è¡¨ç°ã€‚ç‰¹åˆ«æ˜¯åœ¨ä»çœ¼åº•å›¾åƒå‡†ç¡®åˆ†ç±»é’å…‰çœ¼æ—¶ï¼Œæˆ‘ä»¬å‘ç°ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•å¯èƒ½æˆä¸ºAIè½¬å‘çš„ä¸€ä¸ªæœ‰å‰é€”çš„é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨æ”¹å–„ç—…äººæŠ¤ç†æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†éœ€è¦äººç±»-AIåä½œä»¥ç¡®ä¿å®‰å…¨å®æ–½ã€‚</li>
<li>AIæ¨¡å‹åº”å…·å¤‡åœ¨é¢„æµ‹é”™è¯¯æ—¶è½¬å‘äººç±»ä¸“å®¶å†³ç­–çš„èƒ½åŠ›ã€‚</li>
<li>æœ€æ–°ç ”ç©¶é€šè¿‡ä¼˜åŒ–æ›¿ä»£æŸå¤±å‡½æ•°æ¥å­¦ä¹ æ¨¡å‹ä½•æ—¶åº”è½¬å‘äººç±»å†³ç­–ã€‚</li>
<li>åœ¨ä¸´åºŠç¿»è¯‘ä¸­ï¼Œæ¨¡å‹é¢ä¸´æ•°æ®è¿ç§»ç­‰æŒ‘æˆ˜ã€‚</li>
<li>é‡åŒ–ä¸ç¡®å®šæ€§çš„æ¨¡å‹å¯èƒ½æ›´ç¨³å¥åœ°åº”å¯¹ç¦»ç¾¤è¾“å…¥ï¼Œç›¸è¾ƒäºå­¦ä¼šè½¬å‘æ¨¡å‹ã€‚</li>
<li>åœ¨çœ¼ç§‘æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç ”ç©¶æ˜¾ç¤ºï¼Œä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•åœ¨AIå†³ç­–æ—¶å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c176cb762498572b8a6301bf6611cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b592644dd1dab37ccbfa84f1e669c99d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5fbc9faac9acc970fd293dacaa034b5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Visual-Perturbation-and-Adaptive-Hard-Negative-Contrastive-Learning-for-Compositional-Reasoning-in-Vision-Language-Models"><a href="#Visual-Perturbation-and-Adaptive-Hard-Negative-Contrastive-Learning-for-Compositional-Reasoning-in-Vision-Language-Models" class="headerlink" title="Visual Perturbation and Adaptive Hard Negative Contrastive Learning for   Compositional Reasoning in Vision-Language Models"></a>Visual Perturbation and Adaptive Hard Negative Contrastive Learning for   Compositional Reasoning in Vision-Language Models</h2><p><strong>Authors:Xin Huang, Ruibin Li, Tong Jia, Wei Zheng, Ya Wang</strong></p>
<p>Vision-Language Models (VLMs) are essential for multimodal tasks, especially compositional reasoning (CR) tasks, which require distinguishing fine-grained semantic differences between visual and textual embeddings. However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model. Moreover, negative samples are typically treated uniformly, without considering their difficulty levels, and the alignment of positive samples is insufficient, which leads to challenges in aligning difficult sample pairs. To address these issues, we propose Adaptive Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard negatives into the visual domain to generate semantically disturbed image-based negatives for training the model, thereby enhancing its overall performance. AHNPL also introduces a contrastive learning approach using a multimodal hard negative loss to improve the modelâ€™s discrimination of hard negatives within each modality and a dynamic margin loss that adjusts the contrastive margin according to sample difficulty to enhance the distinction of challenging sample pairs. Experiments on three public datasets demonstrate that our method effectively boosts VLMsâ€™ performance on complex CR tasks. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/nynu-BDAI/AHNPL">https://github.com/nynu-BDAI/AHNPL</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹äºå¤šæ¨¡æ€ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯éœ€è¦åŒºåˆ†è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´ç»†å¾®è¯­ä¹‰å·®å¼‚çš„ç»„åˆæ¨ç†ï¼ˆCRï¼‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ç”ŸæˆåŸºäºæ–‡æœ¬çš„ç¡¬è´Ÿæ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¿½ç•¥äº†åŸºäºå›¾åƒçš„è´Ÿæ ·æœ¬çš„é‡è¦æ€§ï¼Œå¯¼è‡´è§†è§‰ç¼–ç å™¨è®­ç»ƒä¸è¶³ï¼Œæœ€ç»ˆå½±å“æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè´Ÿæ ·æœ¬é€šå¸¸è¢«ä¸€è§†åŒä»ï¼Œæ²¡æœ‰è€ƒè™‘å…¶éš¾åº¦æ°´å¹³ï¼Œæ­£æ ·æœ¬çš„å¯¹é½ä¹Ÿä¸è¶³ï¼Œè¿™å¯¼è‡´éš¾ä»¥å¯¹é½å›°éš¾æ ·æœ¬å¯¹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ç¡¬è´Ÿæ‰°åŠ¨å­¦ä¹ ï¼ˆAHNPLï¼‰ã€‚AHNPLå°†åŸºäºæ–‡æœ¬çš„ç¡¬è´Ÿæ ·æœ¬è½¬æ¢ä¸ºè§†è§‰åŸŸï¼Œä»¥ç”Ÿæˆç”¨äºè®­ç»ƒæ¨¡å‹çš„åœ¨è¯­ä¹‰ä¸Šå—åˆ°å¹²æ‰°çš„åŸºäºå›¾åƒçš„è´Ÿæ ·æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚AHNPLè¿˜å¼•å…¥äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨å¤šæ¨¡æ€ç¡¬è´ŸæŸå¤±æ¥æé«˜æ¨¡å‹åœ¨æ¯ä¸ªæ¨¡æ€å†…å¯¹ç¡¬è´Ÿæ ·æœ¬çš„è¾¨åˆ«èƒ½åŠ›ï¼Œä»¥åŠåŠ¨æ€è¾¹ç•ŒæŸå¤±ä¼šæ ¹æ®æ ·æœ¬éš¾åº¦è°ƒæ•´å¯¹æ¯”è¾¹ç•Œï¼Œæé«˜å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬å¯¹çš„åŒºåˆ†åº¦ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æé«˜äº†VLMsåœ¨å¤æ‚çš„CRä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nynu-BDAI/AHNPL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nynu-BDAI/AHNPLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15576v2">PDF</a> Accepted at the International Joint Conference on Artificial   Intelligence (IJCAI 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾§é‡äºåŸºäºæ–‡æœ¬ç”Ÿæˆç¡¬è´Ÿæ ·æœ¬è¿›è¡Œå¾®è°ƒï¼Œå¿½ç•¥äº†åŸºäºå›¾åƒçš„è´Ÿæ ·æœ¬çš„é‡è¦æ€§ï¼Œå¯¼è‡´è§†è§‰ç¼–ç å™¨è®­ç»ƒä¸è¶³ï¼Œå½±å“æ¨¡å‹æ•´ä½“æ€§èƒ½ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”ç¡¬è´Ÿæ‰°åŠ¨å­¦ä¹ ï¼ˆAHNPLï¼‰æ–¹æ³•ã€‚AHNPLå°†æ–‡æœ¬ç¡¬è´Ÿæ ·æœ¬è½¬æ¢ä¸ºå›¾åƒé¢†åŸŸï¼Œç”Ÿæˆè¯­ä¹‰å¹²æ‰°çš„å›¾åƒè´Ÿæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚åŒæ—¶ï¼ŒAHNPLå¼•å…¥äº†å¤šæ¨¡æ€ç¡¬è´ŸæŸå¤±å’ŒåŠ¨æ€è¾¹è·æŸå¤±æœºåˆ¶æ¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMsåœ¨è¿›è¡Œå¤šæ¨¡æ€ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯ç»„åˆæ¨ç†ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è¯†åˆ«è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„ç»†å¾®è¯­ä¹‰å·®å¼‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾§é‡äºé€šè¿‡ç”ŸæˆåŸºäºæ–‡æœ¬çš„ç¡¬è´Ÿæ ·æœ¬æ¥å¾®è°ƒæ¨¡å‹ï¼Œå¿½ç•¥äº†åŸºäºå›¾åƒçš„è´Ÿæ ·æœ¬çš„é‡è¦æ€§ã€‚</li>
<li>AHNPLé€šè¿‡å°†æ–‡æœ¬ç¡¬è´Ÿæ ·æœ¬è½¬æ¢ä¸ºå›¾åƒé¢†åŸŸç”Ÿæˆè¯­ä¹‰å¹²æ‰°çš„å›¾åƒè´Ÿæ ·æœ¬ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>AHNPLå¼•å…¥äº†å¤šæ¨¡æ€ç¡¬è´ŸæŸå¤±æœºåˆ¶ï¼Œæé«˜äº†æ¨¡å‹å¯¹æ¯ç§æ¨¡æ€å†…ç¡¬è´Ÿæ ·æœ¬çš„è¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>AHNPLè¿˜ä½¿ç”¨åŠ¨æ€è¾¹è·æŸå¤±æœºåˆ¶ï¼Œæ ¹æ®æ ·æœ¬éš¾åº¦è°ƒæ•´å¯¹æ¯”è¾¹è·ï¼Œä»¥æé«˜å¯¹å›°éš¾æ ·æœ¬å¯¹çš„åŒºåˆ†èƒ½åŠ›ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAHNPLæ–¹æ³•èƒ½æœ‰æ•ˆæå‡VLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e23ae3721211e250347b5b4ed2cec71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1004ca344ee68838ce925e68fa95898f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bad1755fdea4b3fceaefd7ff9aa8ef7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92e9a61c76361a95438b820942e15985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-736ebbecd410d8af99ec91aa7ee509fc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Pixel-Motion-as-Universal-Representation-for-Robot-Control"><a href="#Pixel-Motion-as-Universal-Representation-for-Robot-Control" class="headerlink" title="Pixel Motion as Universal Representation for Robot Control"></a>Pixel Motion as Universal Representation for Robot Control</h2><p><strong>Authors:Kanchana Ranasinghe, Xiang Li, E-Ro Nguyen, Cristina Mata, Jongwoo Park, Michael S Ryoo</strong></p>
<p>We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a weakly-supervised manner, enabling diffusion model training on any video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout <a target="_blank" rel="noopener" href="https://kahnchana.github.io/LangToMo">https://kahnchana.github.io/LangToMo</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†LangToMoï¼Œè¿™æ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡ç³»ç»Ÿæ¶æ„ï¼Œä»¥åƒç´ è¿åŠ¨é¢„æµ‹ä½œä¸ºä¸­é—´è¡¨ç¤ºã€‚æˆ‘ä»¬çš„é«˜çº§ç³»ç»Ÿ2æ˜¯ä¸€ä¸ªå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿä»å•å¸§ç”Ÿæˆæ–‡æœ¬æ§åˆ¶çš„åƒç´ è¿åŠ¨åºåˆ—ï¼Œä»è€Œå¼•å¯¼æœºå™¨äººæ§åˆ¶ã€‚åƒç´ è¿åŠ¨æ˜¯ä¸€ç§é€šç”¨ã€å¯è§£é‡Šã€ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„è¡¨ç¤ºï¼Œå¯ä»¥ä»¥ä¸€ç§å¼±ç›‘ç£çš„æ–¹å¼ä»è§†é¢‘ä¸­æå–ï¼Œä»è€Œå®ç°åœ¨ä»»ä½•è§†é¢‘å­—å¹•æ•°æ®ä¸Šè¿›è¡Œæ‰©æ•£æ¨¡å‹è®­ç»ƒã€‚å°†ç”Ÿæˆçš„åƒç´ è¿åŠ¨è§†ä¸ºå­¦ä¹ çš„é€šç”¨è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„ä½çº§ç³»ç»Ÿ1æ¨¡å—é€šè¿‡è¿åŠ¨åˆ°åŠ¨ä½œçš„æ˜ å°„å‡½æ•°å°†è¿™äº›è¡¨ç¤ºè½¬æ¢ä¸ºæœºå™¨äººåŠ¨ä½œï¼Œè¿™äº›æ˜ å°„å‡½æ•°å¯ä»¥æ˜¯æ‰‹å·¥åˆ¶ä½œçš„ï¼Œä¹Ÿå¯ä»¥åœ¨æœ€å°‘çš„ç›‘ç£ä¸‹å­¦ä¹ ã€‚ç³»ç»Ÿ2ä½œä¸ºé«˜çº§ç­–ç•¥ï¼Œåœ¨ç¨€ç–çš„æ—¶é—´é—´éš”å†…è¿è¡Œï¼Œè€Œç³»ç»Ÿ1åˆ™åœ¨å¯†é›†çš„æ—¶é—´é—´éš”å†…ä½œä¸ºä½çº§ç­–ç•¥è¿è¡Œã€‚è¿™ç§åˆ†å±‚è§£è€¦å®ç°äº†çµæ´»çš„ã€å¯æ‰©å±•çš„å’Œé€šç”¨çš„æœºå™¨äººæ§åˆ¶ï¼Œæ— è®ºåœ¨æ²¡æœ‰ç›‘ç£è¿˜æ˜¯æœ‰ç›‘ç£çš„ç¯å¢ƒä¸‹éƒ½èƒ½å¾ˆå¥½åœ°å·¥ä½œï¼Œä»è€Œç¼©å°äº†è¯­è¨€ã€è¿åŠ¨å’ŒåŠ¨ä½œä¹‹é—´çš„å·®è·ã€‚è¯·è®¿é—® <a target="_blank" rel="noopener" href="https://kahnchana.github.io/LangToMo">https://kahnchana.github.io/LangToMo</a> ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07817v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LangToMoæ˜¯ä¸€ä¸ªè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œé‡‡ç”¨åŒç³»ç»Ÿæ¶æ„ï¼Œä»¥åƒç´ è¿åŠ¨é¢„æµ‹ä½œä¸ºä¸­é—´è¡¨ç¤ºã€‚è¯¥ç³»ç»Ÿçš„é«˜å±‚æ¬¡æ¨¡å—ï¼ˆSystem 2ï¼‰æ˜¯ä¸€ä¸ªå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å•å¸§ç”Ÿæˆæ–‡æœ¬æ¡ä»¶ä¸‹çš„åƒç´ è¿åŠ¨åºåˆ—ï¼ŒæŒ‡å¯¼æœºå™¨äººæ§åˆ¶ã€‚åƒç´ è¿åŠ¨æ˜¯ä¸€ç§é€šç”¨ã€å¯è§£é‡Šã€ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„è¡¨ç°å½¢å¼ï¼Œå¯ä»è§†é¢‘ä¸­ä»¥å¼±ç›‘ç£æ–¹å¼æå–ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨ä»»ä½•è§†é¢‘å­—å¹•æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚å°†ç”Ÿæˆçš„åƒç´ è¿åŠ¨è§†ä¸ºå­¦ä¹ çš„é€šç”¨è¡¨ç¤ºï¼Œç³»ç»Ÿçš„ä½å±‚æ¬¡æ¨¡å—ï¼ˆSystem 1ï¼‰å°†è¿™äº›è¡¨ç¤ºè½¬åŒ–ä¸ºæœºå™¨äººåŠ¨ä½œï¼Œé€šè¿‡æ‰‹åŠ¨æˆ–æœ€å°ç›‘ç£å­¦ä¹ å®ç°è¿åŠ¨åˆ°åŠ¨ä½œçš„æ˜ å°„åŠŸèƒ½ã€‚System 2åœ¨ç¨€ç–æ—¶é—´é—´éš”å†…ä½œä¸ºé«˜çº§ç­–ç•¥åº”ç”¨ï¼Œè€ŒSystem 1åœ¨å¯†é›†æ—¶é—´é—´éš”å†…ä½œä¸ºä½çº§ç­–ç•¥æ‰§è¡Œã€‚è¿™ç§å±‚æ¬¡åŒ–çš„è§£è€¦å®ç°äº†çµæ´»çš„ã€å¯æ‰©å±•çš„ã€é€šç”¨çš„æœºå™¨äººæ§åˆ¶ï¼Œæ— è®ºåœ¨æ— ç›‘ç£è¿˜æ˜¯ç›‘ç£è®¾ç½®ä¸‹éƒ½èƒ½å¾ˆå¥½åœ°å·¥ä½œï¼Œç¼©å°äº†è¯­è¨€ã€è¿åŠ¨å’ŒåŠ¨ä½œä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LangToMoæ˜¯ä¸€ä¸ªè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œå…·æœ‰åŒç³»ç»Ÿæ¶æ„ã€‚</li>
<li>åƒç´ è¿åŠ¨é¢„æµ‹ä½œä¸ºä¸­é—´è¡¨ç¤ºåœ¨è¯¥æ¡†æ¶ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>System 2æ˜¯ä¸€ä¸ªå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¯ä»å•å¸§ç”Ÿæˆæ–‡æœ¬æ¡ä»¶ä¸‹çš„åƒç´ è¿åŠ¨åºåˆ—ï¼ŒæŒ‡å¯¼æœºå™¨äººæ§åˆ¶ã€‚</li>
<li>åƒç´ è¿åŠ¨æ˜¯ä¸€ç§é€šç”¨ã€å¯è§£é‡Šã€ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„è¡¨ç°å½¢å¼ã€‚</li>
<li>System 1å°†åƒç´ è¿åŠ¨è½¬åŒ–ä¸ºæœºå™¨äººåŠ¨ä½œï¼Œå®ç°è¿åŠ¨åˆ°åŠ¨ä½œçš„æ˜ å°„ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†å±‚æ¬¡åŒ–çš„è§£è€¦ï¼Œä½¿æœºå™¨äººæ§åˆ¶æ›´åŠ çµæ´»ã€å¯æ‰©å±•å’Œé€šç”¨ã€‚</li>
<li>LangToMoç¼©å°äº†è¯­è¨€ã€è¿åŠ¨å’ŒåŠ¨ä½œä¹‹é—´çš„å·®è·ï¼Œæ— è®ºæ˜¯åœ¨æ— ç›‘ç£è¿˜æ˜¯ç›‘ç£è®¾ç½®ä¸‹éƒ½èƒ½å¾ˆå¥½åœ°å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dcbb1bb0ffafbad3c5aa01298d33cef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f6458e7279e276ddd1d82fdb125c6ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f673da46e25a628fdf5a62b572a24c7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df5ef56546fe1f9d67ba61c050941f0c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Making-Physical-Objects-with-Generative-AI-and-Robotic-Assembly-Considering-Fabrication-Constraints-Sustainability-Time-Functionality-and-Accessibility"><a href="#Making-Physical-Objects-with-Generative-AI-and-Robotic-Assembly-Considering-Fabrication-Constraints-Sustainability-Time-Functionality-and-Accessibility" class="headerlink" title="Making Physical Objects with Generative AI and Robotic Assembly:   Considering Fabrication Constraints, Sustainability, Time, Functionality, and   Accessibility"></a>Making Physical Objects with Generative AI and Robotic Assembly:   Considering Fabrication Constraints, Sustainability, Time, Functionality, and   Accessibility</h2><p><strong>Authors:Alexander Htet Kyaw, Se Hwan Jeon, Miana Smith, Neil Gershenfeld</strong></p>
<p>3D generative AI enables rapid and accessible creation of 3D models from text or image inputs. However, translating these outputs into physical objects remains a challenge due to the constraints in the physical world. Recent studies have focused on improving the capabilities of 3D generative AI to produce fabricable outputs, with 3D printing as the main fabrication method. However, this workshop paper calls for a broader perspective by considering how fabrication methods align with the capabilities of 3D generative AI. As a case study, we present a novel system using discrete robotic assembly and 3D generative AI to make physical objects. Through this work, we identified five key aspects to consider in a physical making process based on the capabilities of 3D generative AI. 1) Fabrication Constraints: Current text-to-3D models can generate a wide range of 3D designs, requiring fabrication methods that can adapt to the variability of generative AI outputs. 2) Time: While generative AI can generate 3D models in seconds, fabricating physical objects can take hours or even days. Faster production could enable a closer iterative design loop between humans and AI in the making process. 3) Sustainability: Although text-to-3D models can generate thousands of models in the digital world, extending this capability to the real world would be resource-intensive, unsustainable and irresponsible. 4) Functionality: Unlike digital outputs from 3D generative AI models, the fabrication method plays a crucial role in the usability of physical objects. 5) Accessibility: While generative AI simplifies 3D model creation, the need for fabrication equipment can limit participation, making AI-assisted creation less inclusive. These five key aspects provide a framework for assessing how well a physical making process aligns with the capabilities of 3D generative AI and values in the world. </p>
<blockquote>
<p>3Dç”Ÿæˆå¼AIèƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æˆ–å›¾åƒè¾“å…¥å¿«é€Ÿä¸”ä¾¿æ·åœ°åˆ›å»º3Dæ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºç‰©ç†ä¸–ç•Œçš„é™åˆ¶ï¼Œå°†è¿™äº›è¾“å‡ºç‰©è½¬åŒ–ä¸ºå®ä½“ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿‘æœŸçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æé«˜3Dç”Ÿæˆå¼AIäº§ç”Ÿå¯åˆ¶ä½œè¾“å‡ºçš„èƒ½åŠ›ï¼Œè€Œ3Dæ‰“å°æ˜¯ä¸»è¦åˆ¶ä½œæ–¹æ³•ã€‚ç„¶è€Œï¼Œæœ¬ç ”è®¨ä¼šè®ºæ–‡å‘¼åä»æ›´å¹¿æ³›çš„è§†è§’è€ƒè™‘åˆ¶ä½œæ–¹æ³•å¦‚ä½•ä¸3Dç”Ÿæˆå¼AIçš„èƒ½åŠ›ç›¸åŒ¹é…ã€‚ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨ç¦»æ•£æœºå™¨äººè£…é…å’Œ3Dç”Ÿæˆå¼AIåˆ¶ä½œå®ä½“ç‰©ä½“çš„æ–°å‹ç³»ç»Ÿã€‚é€šè¿‡è¿™é¡¹å·¥ä½œï¼Œæˆ‘ä»¬ç¡®å®šäº†åŸºäº3Dç”Ÿæˆå¼AIçš„èƒ½åŠ›çš„ç‰©ç†åˆ¶é€ è¿‡ç¨‹ä¸­éœ€è¦è€ƒè™‘çš„äº”ä¸ªå…³é”®å› ç´ ã€‚</p>
</blockquote>
<ol>
<li>åˆ¶ä½œçº¦æŸï¼šç›®å‰çš„æ–‡æœ¬åˆ°3Dæ¨¡å‹å¯ä»¥ç”Ÿæˆå„ç§3Dè®¾è®¡ï¼Œéœ€è¦èƒ½å¤Ÿé€‚åº”ç”Ÿæˆå¼AIè¾“å‡ºå¤šå˜æ€§çš„åˆ¶ä½œæ–¹æ³•ã€‚</li>
<li>æ—¶é—´ï¼šè™½ç„¶ç”Ÿæˆå¼AIå¯ä»¥åœ¨å‡ ç§’å†…ç”Ÿæˆ3Dæ¨¡å‹ï¼Œä½†åˆ¶ä½œå®ä½“ç‰©ä½“å¯èƒ½éœ€è¦æ•°å°æ—¶ç”šè‡³æ•°å¤©ã€‚æ›´å¿«çš„ç”Ÿäº§é€Ÿåº¦å¯ä»¥è®©äººå·¥æ™ºèƒ½å’Œäººç±»åœ¨åˆ¶é€ è¿‡ç¨‹ä¸­æœ‰æ›´ç´§å¯†çš„è¿­ä»£è®¾è®¡å¾ªç¯ã€‚</li>
<li>å¯æŒç»­æ€§ï¼šè™½ç„¶æ–‡æœ¬åˆ°3Dæ¨¡å‹å¯ä»¥åœ¨æ•°å­—ä¸–ç•Œä¸­ç”Ÿæˆæ•°åƒä¸ªæ¨¡å‹ï¼Œä½†å°†è¿™ç§èƒ½åŠ›æ‰©å±•åˆ°ç°å®ä¸–ç•Œå°†æ˜¯èµ„æºå¯†é›†å‹çš„ï¼Œä¸å¯æŒç»­ä¸”ä¸è´Ÿè´£ä»»ã€‚</li>
<li>åŠŸèƒ½ï¼šä¸æ¥è‡ª3Dç”Ÿæˆå¼AIæ¨¡å‹çš„æ•°å­—è¾“å‡ºä¸åŒï¼Œåˆ¶ä½œæ–¹æ³•å¯¹äºå®ä½“ç‰©ä½“çš„å®ç”¨æ€§èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚</li>
</ol>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19131v3">PDF</a> ACM CHI Conference on Human Factors in Computing Systems (CHI 2025),   Workshop on Generative AI and Human-Computer Interaction, Yokohama, Japan,   April 26 to May 1, 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºäº†åŸºäºæ–‡æœ¬æˆ–å›¾åƒè¾“å…¥çš„3Dç”ŸæˆAIæŠ€æœ¯çš„å¿«é€Ÿå‘å±•åŠå…¶åœ¨åˆ›å»º3Dæ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚å°½ç®¡å®ƒèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆæ¨¡å‹ï¼Œä½†åœ¨å°†è¿™äº›è¾“å‡ºè½¬åŒ–ä¸ºå®é™…ç‰©ä½“æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼ºè°ƒäº†è€ƒè™‘åˆ¶é€ æ–¹æ³•ä¸3Dç”ŸæˆAIèƒ½åŠ›ç›¸ç»“åˆçš„é‡è¦æ€§ï¼Œå¹¶æå‡ºä¸€ä¸ªç»“åˆç¦»æ•£æœºå™¨äººç»„è£…å’Œ3Dç”ŸæˆAIçš„åˆ›æ–°ç³»ç»Ÿä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ã€‚ä½œè€…æŒ‡å‡ºäº†åœ¨åŸºäºç‰©ç†åˆ¶é€ è¿‡ç¨‹ä¸­è€ƒè™‘äº”ä¸ªå…³é”®æ–¹é¢çš„é‡è¦æ€§ï¼šåˆ¶é€ çº¦æŸã€æ—¶é—´ã€å¯æŒç»­æ€§ã€åŠŸèƒ½æ€§å’Œå¯è®¿é—®æ€§ã€‚æ­¤ç ”ç©¶æœ‰åŠ©äºè¯„ä¼°åˆ¶é€ è¿‡ç¨‹ä¸3Dç”ŸæˆAIçš„èƒ½åŠ›ä¹‹é—´çš„å¥‘åˆåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dç”ŸæˆAIå¯ä»æ–‡æœ¬æˆ–å›¾åƒè¾“å…¥ä¸­å¿«é€Ÿåˆ›å»ºå¤šæ ·åŒ–çš„3Dæ¨¡å‹ã€‚</li>
<li>åˆ¶é€ å°†è¿™äº›æ¨¡å‹è½¬åŒ–ä¸ºç‰©ç†å¯¹è±¡ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦é€‚åº”AIè¾“å‡ºçš„å¤šå˜æ€§å’Œæ»¡è¶³ç‰©ç†ä¸–ç•Œçš„çº¦æŸã€‚</li>
<li>ç¦»æ•£æœºå™¨äººç»„è£…æŠ€æœ¯è¢«ç”¨ä½œæ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†ä¸3Dç”ŸæˆAIç»“åˆåˆ¶é€ ç‰©ä½“çš„æ½œåŠ›ã€‚</li>
<li>åœ¨ç‰©ç†åˆ¶é€ è¿‡ç¨‹ä¸­éœ€è¦è€ƒè™‘äº”ä¸ªå…³é”®æ–¹é¢ï¼šåˆ¶é€ çº¦æŸã€æ—¶é—´æ•ˆç‡ã€å¯æŒç»­æ€§ã€ç‰©ä½“åŠŸèƒ½æ€§å’Œå¯è®¿é—®æ€§ã€‚</li>
<li>åº”è¯„ä¼°åˆ¶é€ è¿‡ç¨‹ä¸3Dç”ŸæˆAIçš„èƒ½åŠ›æ˜¯å¦å¥‘åˆï¼Œä»¥ç¡®ä¿æœ‰æ•ˆå’Œè´Ÿè´£ä»»åœ°åˆ©ç”¨èµ„æºã€‚</li>
<li>å®ç°å¿«é€Ÿè¿­ä»£è®¾è®¡å’Œç”Ÿäº§ä¸­äººç±»ä¸AIä¹‹é—´çš„ç´§å¯†åˆä½œè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-412bb26512372e26201c5a04d56b33c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14e8cfa060ec00336da46edd7995f14c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af8b8fc73f66cc3e78a8d0dccdd7a3e6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Impoola-The-Power-of-Average-Pooling-for-Image-Based-Deep-Reinforcement-Learning"><a href="#Impoola-The-Power-of-Average-Pooling-for-Image-Based-Deep-Reinforcement-Learning" class="headerlink" title="Impoola: The Power of Average Pooling for Image-Based Deep Reinforcement   Learning"></a>Impoola: The Power of Average Pooling for Image-Based Deep Reinforcement   Learning</h2><p><strong>Authors:Raphael Trumpp, Ansgar SchÃ¤fftlein, Mirco Theile, Marco Caccamo</strong></p>
<p>As image-based deep reinforcement learning tackles more challenging tasks, increasing model size has become an important factor in improving performance. Recent studies achieved this by focusing on the parameter efficiency of scaled networks, typically using Impala-CNN, a 15-layer ResNet-inspired network, as the image encoder. However, while Impala-CNN evidently outperforms older CNN architectures, potential advancements in network design for deep reinforcement learning-specific image encoders remain largely unexplored. We find that replacing the flattening of output feature maps in Impala-CNN with global average pooling leads to a notable performance improvement. This approach outperforms larger and more complex models in the Procgen Benchmark, particularly in terms of generalization. We call our proposed encoder model Impoola-CNN. A decrease in the networkâ€™s translation sensitivity may be central to this improvement, as we observe the most significant gains in games without agent-centered observations. Our results demonstrate that network scaling is not just about increasing model size - efficient network design is also an essential factor. We make our code available at <a target="_blank" rel="noopener" href="https://github.com/raphajaner/impoola">https://github.com/raphajaner/impoola</a>. </p>
<blockquote>
<p>éšç€åŸºäºå›¾åƒçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ å¤„ç†æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ¨¡å‹è§„æ¨¡çš„å¢åŠ å·²æˆä¸ºæé«˜æ€§èƒ½çš„é‡è¦å› ç´ ã€‚è¿‘æœŸçš„ç ”ç©¶é€šè¿‡å…³æ³¨æ‰©å±•ç½‘ç»œçš„å‚æ•°æ•ˆç‡æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œé€šå¸¸ä½¿ç”¨å—ResNetå¯å‘çš„15å±‚Impala-CNNä½œä¸ºå›¾åƒç¼–ç å™¨ã€‚ç„¶è€Œï¼Œè™½ç„¶Impala-CNNæ˜æ˜¾ä¼˜äºè¾ƒæ—§çš„CNNæ¶æ„ï¼Œä½†é’ˆå¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ ç‰¹å®šå›¾åƒç¼–ç å™¨çš„ç½‘ç»œè®¾è®¡æ½œåœ¨è¿›å±•å´é²œæœ‰æ¢ç´¢ã€‚æˆ‘ä»¬å‘ç°ï¼Œç”¨å…¨å±€å¹³å‡æ± åŒ–ä»£æ›¿Impala-CNNä¸­è¾“å‡ºç‰¹å¾å›¾çš„å¹³é“ºï¼Œä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—æé«˜ã€‚è¿™ç§æ–¹æ³•åœ¨Procgen Benchmarkä¸Šçš„è¡¨ç°ä¼˜äºæ›´å¤§ã€æ›´å¤æ‚çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³›åŒ–æ–¹é¢ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬æå‡ºçš„ç¼–ç å™¨æ¨¡å‹ç§°ä¸ºImpoola-CNNã€‚è¿™ç§æ”¹è¿›çš„æ ¸å¿ƒå¯èƒ½æ˜¯ç½‘ç»œç¿»è¯‘æ•æ„Ÿåº¦çš„é™ä½ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨éä»¥ä»£ç†ä¸ºä¸­å¿ƒçš„æ¸¸æˆä¸­è§‚å¯Ÿåˆ°æœ€å¤§çš„æ”¶ç›Šã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç½‘ç»œæ‰©å±•ä¸ä»…ä»…å…³ä¹å¢åŠ æ¨¡å‹è§„æ¨¡â€”â€”é«˜æ•ˆçš„ç½‘ç»œè®¾è®¡ä¹Ÿæ˜¯ä¸€ä¸ªå…³é”®å› ç´ ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/raphajaner/impoola%E6%8F%90%E4%BE%9B%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/raphajaner/impoolaæä¾›æˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05546v2">PDF</a> Reinforcement Learning Conference 2025</p>
<p><strong>Summary</strong></p>
<p>å›¾åƒæ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨å¤„ç†æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡æ—¶ï¼Œæ¨¡å‹è§„æ¨¡çš„å¢åŠ å·²æˆä¸ºæé«˜æ€§èƒ½çš„é‡è¦å› ç´ ã€‚æœ€æ–°ç ”ç©¶é€šè¿‡å…³æ³¨æ‰©å±•ç½‘ç»œçš„å‚æ•°æ•ˆç‡æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œé€šå¸¸ä½¿ç”¨Impala-CNNä½œä¸ºå›¾åƒç¼–ç å™¨ã€‚ç„¶è€Œï¼Œå°½ç®¡Impala-CNNæ˜æ˜¾ä¼˜äºæ—§CNNæ¶æ„ï¼Œä½†åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸“ç”¨å›¾åƒç¼–ç å™¨çš„ç½‘ç»œè®¾è®¡æ–¹é¢ä»å­˜åœ¨æ½œåœ¨çš„è¿›æ­¥ç©ºé—´ã€‚ç ”ç©¶å‘ç°ï¼Œç”¨å…¨å±€å¹³å‡æ± åŒ–æ›¿æ¢Impala-CNNä¸­çš„è¾“å‡ºç‰¹å¾æ˜ å°„å¹³å±•ï¼Œèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚åœ¨Procgen Benchmarkä¸­ï¼Œè¿™ç§æ–¹æ³•ä¼˜äºæ›´å¤§ã€æ›´å¤æ‚çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é€šç”¨åŒ–æ–¹é¢ã€‚æ‰€æå‡ºçš„ç¼–ç å™¨æ¨¡å‹è¢«ç§°ä¸ºImpoola-CNNã€‚ç½‘ç»œç¿»è¯‘çµæ•åº¦çš„é™ä½å¯èƒ½æ˜¯è¿™ä¸€æ”¹è¿›çš„æ ¸å¿ƒï¼Œå› ä¸ºåœ¨æ²¡æœ‰ä»¥ä»£ç†ä¸ºä¸­å¿ƒçš„è§‚å¯Ÿçš„æ¸¸æˆä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æœ€å¤§çš„æ”¶ç›Šã€‚ç»“æœè¡¨æ˜ï¼Œç½‘ç»œè§„æ¨¡æ‰©å±•ä¸ä»…ä»…æ˜¯å…³äºå¢åŠ æ¨¡å‹å¤§å°çš„é—®é¢˜â€”â€”é«˜æ•ˆçš„ç½‘ç»œè®¾è®¡ä¹Ÿæ˜¯ä¸€ä¸ªå…³é”®å› ç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹è§„æ¨¡å¢åŠ æ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ å¤„ç†å¤æ‚ä»»åŠ¡æ—¶æé«˜æ€§èƒ½çš„é‡è¦å› ç´ ã€‚</li>
<li>æœ€æ–°ç ”ç©¶å…³æ³¨æ‰©å±•ç½‘ç»œçš„å‚æ•°æ•ˆç‡ï¼Œé€šå¸¸ä½¿ç”¨Impala-CNNä½œä¸ºå›¾åƒç¼–ç å™¨ã€‚</li>
<li>Impala-CNNæ€§èƒ½æ˜¾è‘—ä¼˜äºæ—§CNNæ¶æ„ï¼Œä½†ä»å­˜åœ¨ç½‘ç»œè®¾è®¡æ–¹é¢çš„æ½œåœ¨è¿›æ­¥ç©ºé—´ã€‚</li>
<li>é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–æ›¿æ¢Impala-CNNä¸­çš„ç‰¹å¾æ˜ å°„å¹³å±•ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
<li>Impoola-CNNåœ¨Procgen Benchmarkä¸­çš„è¡¨ç°ä¼˜äºæ›´å¤§ã€æ›´å¤æ‚çš„æ¨¡å‹ï¼Œå°¤å…¶åœ¨é€šç”¨åŒ–æ–¹é¢ã€‚</li>
<li>ç½‘ç»œç¿»è¯‘çµæ•åº¦çš„é™ä½å¯èƒ½æ˜¯è¿™ä¸€æ”¹è¿›çš„æ ¸å¿ƒï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰ä»¥ä»£ç†ä¸ºä¸­å¿ƒçš„è§‚å¯Ÿçš„æ¸¸æˆä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35256ed6ddcdbfa74d4029c69b36ec64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9197f9bf7cf6548f5bc6d6635f7244cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9465d209fb399648510a1e4235695bfd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a0a7e0f48f2721e6ee2835af9839600.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6f1b4bd471705c36ac62904a918f4210.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  VideoRewardBench Comprehensive Evaluation of Multimodal Reward Models   for Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-585ab68f8b79809c9b58c75ffb4f525b.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Delta Activations A Representation for Finetuned Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
