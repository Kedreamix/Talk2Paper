<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Aesthetic Image Captioning with Saliency Enhanced MLLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1a18a3a6abfb6c9ac649e6950b4fd7fe.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Aesthetic-Image-Captioning-with-Saliency-Enhanced-MLLMs"><a href="#Aesthetic-Image-Captioning-with-Saliency-Enhanced-MLLMs" class="headerlink" title="Aesthetic Image Captioning with Saliency Enhanced MLLMs"></a>Aesthetic Image Captioning with Saliency Enhanced MLLMs</h2><p><strong>Authors:Yilin Tao, Jiashui Huang, Huaze Xu, Ling Shao</strong></p>
<p>Aesthetic Image Captioning (AIC) aims to generate textual descriptions of image aesthetics, becoming a key research direction in the field of computational aesthetics. In recent years, pretrained Multimodal Large Language Models (MLLMs) have advanced rapidly, leading to a significant increase in image aesthetics research that integrates both visual and textual modalities. However, most existing studies on image aesthetics primarily focus on predicting aesthetic ratings and have shown limited application in AIC. Existing AIC works leveraging MLLMs predominantly rely on fine-tuning methods without specifically adapting MLLMs to focus on target aesthetic content. To address this limitation, we propose the Aesthetic Saliency Enhanced Multimodal Large Language Model (ASE-MLLM), an end-to-end framework that explicitly incorporates aesthetic saliency into MLLMs. Within this framework, we introduce the Image Aesthetic Saliency Module (IASM), which efficiently and effectively extracts aesthetic saliency features from images. Additionally, we design IAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency features with original image features via a cross-attention mechanism. To the best of our knowledge, ASE-MLLM is the first framework to integrate image aesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments demonstrated that our approach significantly outperformed traditional methods and generic MLLMs on current mainstream AIC benchmarks, achieving state-of-the-art (SOTA) performance. </p>
<blockquote>
<p>ç¾å­¦å›¾åƒæ ‡é¢˜ç”Ÿæˆï¼ˆAICï¼‰æ—¨åœ¨ç”Ÿæˆå›¾åƒç¾å­¦å±æ€§çš„æ–‡æœ¬æè¿°ï¼Œå·²æˆä¸ºè®¡ç®—ç¾å­¦é¢†åŸŸçš„å…³é”®ç ”ç©¶æ–¹å‘ã€‚è¿‘å¹´æ¥ï¼Œé¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å‘å±•è¿…é€Ÿï¼Œæ¨åŠ¨äº†èåˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„å›¾åƒç¾å­¦ç ”ç©¶æ˜¾è‘—å¢åŠ ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„å›¾åƒç¾å­¦ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é¢„æµ‹ç¾å­¦è¯„åˆ†ä¸Šï¼Œåœ¨AICä¸­çš„åº”ç”¨æœ‰é™ã€‚ç°æœ‰çš„åˆ©ç”¨MLLMçš„AICå·¥ä½œä¸»è¦ä¾èµ–äºå¾®è°ƒæ–¹æ³•ï¼Œè€Œæ²¡æœ‰ä¸“é—¨è°ƒæ•´MLLMä»¥å…³æ³¨ç›®æ ‡ç¾å­¦å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç¾å­¦æ˜¾è‘—æ€§å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆASE-MLLMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œæ˜¾å¼åœ°å°†ç¾å­¦æ˜¾è‘—æ€§çº³å…¥MLLMã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å›¾åƒç¾å­¦æ˜¾è‘—æ€§æ¨¡å—ï¼ˆIASMï¼‰ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿé«˜æ•ˆåœ°ä»å›¾åƒä¸­æå–ç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†IAS-ViTä½œä¸ºMLLMçš„å›¾åƒç¼–ç å™¨ï¼Œè¯¥æ¨¡å—é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å°†ç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾ä¸åŸå§‹å›¾åƒç‰¹å¾èåˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒASE-MLLMæ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨å°†å›¾åƒç¾å­¦æ˜¾è‘—æ€§é›†æˆåˆ°MLLMä¸­çš„AICä»»åŠ¡æ¡†æ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å½“å‰ä¸»æµçš„AICåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œé€šç”¨MLLMï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04378v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºé¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒç¾å­¦é¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä¸­å­˜åœ¨çš„å¯¹å›¾åƒç¾å­¦è¯„ä»·åº”ç”¨æœ‰é™çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºASE-MLLMçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å›¾åƒç¾å­¦æ˜¾è‘—æ€§æ¨¡å—ï¼ˆIASMï¼‰å’ŒIAS-ViTå›¾åƒç¼–ç å™¨ï¼Œæœ‰æ•ˆåœ°æå–å¹¶èåˆå›¾åƒçš„ç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾ï¼Œå®ç°äº†å¯¹å›¾åƒç¾å­¦çš„ç²¾å‡†æè¿°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»æµç¾å­¦å›¾åƒæ ‡æ³¨ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œé€šç”¨MLLMsï¼Œè¾¾åˆ°äº†ç›®å‰çš„æœ€ä¼˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¾å­¦å›¾åƒæ ‡æ³¨ï¼ˆAICï¼‰æ˜¯è®¡ç®—ç¾å­¦é¢†åŸŸçš„ä¸€ä¸ªé‡è¦ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ç”Ÿæˆå¯¹å›¾åƒç¾å­¦çš„æ–‡æœ¬æè¿°ã€‚</li>
<li>è¿‘å¹´å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å›¾åƒç¾å­¦ç ”ç©¶çš„æ–°è¿›å±•ã€‚</li>
<li>ç°æœ‰å›¾åƒç¾å­¦ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é¢„æµ‹ç¾å­¦è¯„åˆ†ä¸Šï¼Œä½†åœ¨AICåº”ç”¨ä¸Šè¡¨ç°æœ‰é™ã€‚</li>
<li>ASE-MLLMæ¡†æ¶é€šè¿‡å¼•å…¥å›¾åƒç¾å­¦æ˜¾è‘—æ€§æ¨¡å—ï¼ˆIASMï¼‰å’ŒIAS-ViTå›¾åƒç¼–ç å™¨ï¼Œè§£å†³äº†ç°æœ‰ç ”ç©¶çš„å±€é™æ€§ã€‚</li>
<li>IASMæ¨¡å—èƒ½å¤Ÿé«˜æ•ˆåœ°ä»å›¾åƒä¸­æå–ç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾ã€‚</li>
<li>IAS-ViTå›¾åƒç¼–ç å™¨é€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶èåˆç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾å’ŒåŸå§‹å›¾åƒç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a67da6ce682ab685d0dddb84d7d9cbd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-035bc289a0d85b0f35a4d5cce242da4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28adab153a4bfea00172e9611afe2960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebd4e555d0e3e86a2f228c42901fcd51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08ec77594dbb633c31f79e76cde9f5a9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Attn-Adapter-Attention-Is-All-You-Need-for-Online-Few-shot-Learner-of-Vision-Language-Model"><a href="#Attn-Adapter-Attention-Is-All-You-Need-for-Online-Few-shot-Learner-of-Vision-Language-Model" class="headerlink" title="Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of   Vision-Language Model"></a>Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of   Vision-Language Model</h2><p><strong>Authors:Phuoc-Nguyen Bui, Khanh-Binh Nguyen, Hyunseung Choo</strong></p>
<p>Contrastive vision-language models excel in zero-shot image recognition but face challenges in few-shot scenarios due to computationally intensive offline fine-tuning using prompt learning, which risks overfitting. To overcome these limitations, we propose Attn-Adapter, a novel online few-shot learning framework that enhances CLIPâ€™s adaptability via a dual attention mechanism. Our design incorporates dataset-specific information through two components: the Memory Attn-Adapter, which refines category embeddings using support examples, and the Local-Global Attn-Adapter, which enriches image embeddings by integrating local and global features. This architecture enables dynamic adaptation from a few labeled samples without retraining the base model. Attn-Adapter outperforms state-of-the-art methods in cross-category and cross-dataset generalization, maintaining efficient inference and scaling across CLIP backbones. </p>
<blockquote>
<p>å¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒè¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å°æ ·æœ¬åœºæ™¯ä¸‹é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºä½¿ç”¨æç¤ºå­¦ä¹ è¿›è¡Œç¦»çº¿å¾®è°ƒè®¡ç®—é‡å¤§ï¼Œå­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Attn-Adapterï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åœ¨çº¿å°æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åŒæ³¨æ„åŠ›æœºåˆ¶æé«˜CLIPçš„é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„è®¾è®¡é€šè¿‡ä¸¤ä¸ªç»„ä»¶èå…¥äº†æ•°æ®é›†ç‰¹å®šä¿¡æ¯ï¼šMemory Attn-Adapterä½¿ç”¨æ”¯æŒæ ·ä¾‹ç»†åŒ–ç±»åˆ«åµŒå…¥ï¼Œè€ŒLocal-Global Attn-Adapteré€šè¿‡èåˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ä¸°å¯Œå›¾åƒåµŒå…¥ã€‚è¿™ç§æ¶æ„èƒ½å¤Ÿä»å°‘é‡æ ‡è®°æ ·æœ¬ä¸­è¿›è¡ŒåŠ¨æ€é€‚åº”ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚Attn-Adapteråœ¨è·¨ç±»åˆ«å’Œè·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆçš„æ¨ç†å’Œåœ¨ä¸åŒCLIPä¸»å¹²ä¹‹é—´çš„æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03895v1">PDF</a> ICCV 2025 - LIMIT Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºAttn-Adapterçš„æ–°å‹åœ¨çº¿å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŒæ³¨æ„åŠ›æœºåˆ¶æå‡CLIPæ¨¡å‹çš„é€‚åº”æ€§ã€‚è¯¥è®¾è®¡é€šè¿‡ä¸¤ä¸ªç»„ä»¶â€”â€”Memory Attn-Adapterå’ŒLocal-Global Attn-Adapterâ€”â€”èå…¥ç‰¹å®šæ•°æ®é›†ä¿¡æ¯ï¼Œåˆ†åˆ«é€šè¿‡æ”¯æŒæ ·æœ¬ç²¾ç‚¼ç±»åˆ«åµŒå…¥å’Œèåˆå±€éƒ¨ä¸å…¨å±€ç‰¹å¾æ¥ä¸°å¯Œå›¾åƒåµŒå…¥ã€‚è¿™ä¸€æ¶æ„èƒ½å¤Ÿå®ç°ä»å°‘é‡æ ‡è®°æ ·æœ¬çš„åŠ¨æ€é€‚åº”ï¼Œæ— éœ€é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œä¸”åœ¨è·¨ç±»åˆ«å’Œè·¨æ•°æ®é›†æ¨å¹¿æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆæ¨ç†å’Œè·¨CLIPéª¨å¹²çš„æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Attn-Adapteræ˜¯ä¸€ç§æ–°å‹çš„åœ¨çº¿å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡CLIPæ¨¡å‹åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹çš„é€‚åº”æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡åŒæ³¨æ„åŠ›æœºåˆ¶å®ç°åŠ¨æ€é€‚åº”ï¼Œæ— éœ€é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚</li>
<li>Memory Attn-Adapteré€šè¿‡æ”¯æŒæ ·æœ¬ç²¾ç‚¼ç±»åˆ«åµŒå…¥ã€‚</li>
<li>Local-Global Attn-Adapteré€šè¿‡èåˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾æ¥ä¸°å¯Œå›¾åƒåµŒå…¥ã€‚</li>
<li>Attn-Adapteråœ¨è·¨ç±»åˆ«å’Œè·¨æ•°æ®é›†æ¨å¹¿æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿä¿æŒé«˜æ•ˆæ¨ç†ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯åº”ç”¨äºä¸åŒçš„CLIPéª¨å¹²ç½‘ç»œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79ae95c4abbcc9efe11c35a7e0780247.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93a47a414ed1ecb392dfc28de146c644.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59383881004df2cd1a5975c76c313b26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-130bb3e97b54c3cc1dfa1d48b5aaead0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af1841fa5efa596d34a2a6e4e075c547.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a2af71c28bf2d92928c741053f975c8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Single-Domain-Generalization-in-Diabetic-Retinopathy-A-Neuro-Symbolic-Learning-Approach"><a href="#Single-Domain-Generalization-in-Diabetic-Retinopathy-A-Neuro-Symbolic-Learning-Approach" class="headerlink" title="Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic   Learning Approach"></a>Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic   Learning Approach</h2><p><strong>Authors:Midhat Urooj, Ayan Banerjee, Farhat Shaikh, Kuntal Thakur, Sandeep Gupta</strong></p>
<p>Domain generalization remains a critical challenge in medical imaging, where models trained on single sources often fail under real-world distribution shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy (DR) classification that integrates vision transformers with expert-guided symbolic reasoning to enable robust generalization across unseen domains. Our approach leverages clinical lesion ontologies through structured, rule-based features and retinal vessel segmentation, fusing them with deep visual representations via a confidence-weighted integration strategy. The framework addresses both single-domain generalization (SDG) and multi-domain generalization (MDG) by minimizing the KL divergence between domain embeddings, thereby enforcing alignment of high-level clinical semantics. Extensive experiments across four public datasets (APTOS, EyePACS, Messidor-1, Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in cross-domain settings and a 6% improvement over baseline ViT models. Notably, our symbolic-only model achieves a 63.67% average accuracy in MDG, while the complete neuro-symbolic integration achieves the highest accuracy compared to existing published baselines and benchmarks in challenging SDG scenarios. Ablation studies reveal that lesion-based features (84.65% accuracy) substantially outperform purely neural approaches, confirming that symbolic components act as effective regularizers beyond merely enhancing interpretability. Our findings establish neuro-symbolic integration as a promising paradigm for building clinically robust, and domain-invariant medical AI systems. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å½±åƒé¢†åŸŸï¼Œè·¨åŸŸæ³›åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æ¨¡å‹åœ¨å•ä¸€æºä¸Šè®­ç»ƒå¾€å¾€ä¼šåœ¨çœŸå®ä¸–ç•Œåˆ†å¸ƒå˜åŒ–çš„æƒ…å†µä¸‹å¤±æ•ˆã€‚æˆ‘ä»¬æå‡ºäº†KG-DGï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç³–å°¿ç—…è§†ç½‘è†œç—…å˜ï¼ˆDRï¼‰åˆ†ç±»çš„ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œå®ƒå°†è§†è§‰å˜å‹å™¨ä¸ä¸“å®¶å¼•å¯¼çš„è±¡å¾æ¨ç†ç›¸ç»“åˆï¼Œå®ç°äº†è·¨æœªè§åŸŸçš„ç¨³å¥æ³›åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç»“æ„åŒ–ã€åŸºäºè§„åˆ™çš„ç‰¹å¾å’Œè§†ç½‘è†œè¡€ç®¡åˆ†å‰²ï¼Œåˆ©ç”¨ä¸´åºŠç—…å˜æœ¬ä½“ï¼Œå°†å®ƒä»¬ä¸æ·±åº¦è§†è§‰è¡¨ç¤ºé€šè¿‡ç½®ä¿¡åº¦åŠ æƒé›†æˆç­–ç•¥ç›¸èåˆã€‚è¯¥æ¡†æ¶é€šè¿‡æœ€å°åŒ–åŸŸåµŒå…¥ä¹‹é—´çš„KLæ•£åº¦ï¼Œè§£å†³äº†å•åŸŸæ³›åŒ–ï¼ˆSDGï¼‰å’Œå¤šåŸŸæ³›åŒ–ï¼ˆMDGï¼‰é—®é¢˜ï¼Œä»è€Œå¼ºåˆ¶é«˜çº§ä¸´åºŠè¯­ä¹‰å¯¹é½ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆAPTOSã€EyePACSã€Messidor-1ã€Messidor-2ï¼‰ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿ViTæ¨¡å‹ç›¸æ¯”ï¼Œè·¨åŸŸè®¾ç½®ä¸­çš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾5.2%ï¼Œå¹¶ä¸”åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„SDGåœºæ™¯ä¸­å¹³å‡æé«˜äº†6%çš„å‡†ç¡®ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„çº¯ç¬¦å·æ¨¡å‹åœ¨MDGä¸­è¾¾åˆ°äº†63.67%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè€Œå®Œæ•´çš„ç¥ç»ç¬¦å·é›†æˆåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„SDGåœºæ™¯ä¸­ä¸ç°æœ‰çš„å·²å‘å¸ƒåŸºå‡†å€¼å’Œé‡Œç¨‹ç¢‘ç›¸æ¯”å®ç°äº†æœ€é«˜å‡†ç¡®ç‡ã€‚æ¶ˆèç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºç—…å˜çš„ç‰¹å¾ï¼ˆå‡†ç¡®ç‡ä¸º84.65%ï¼‰å¤§å¤§ä¼˜äºçº¯ç¥ç»æ–¹æ³•ï¼Œè¯å®äº†ç¬¦å·ç»„ä»¶ä½œä¸ºæœ‰æ•ˆæ­£åˆ™åŒ–å™¨çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯å¢å¼ºå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœç¡®ç«‹äº†ç¥ç»ç¬¦å·èåˆä½œä¸ºä¸€ä¸ªæœ‰å‰é€”çš„èŒƒå¼æ„å»ºä¸´åºŠç¨³å¥ã€è·¨åŸŸä¸å˜åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æœ‰å‰æ™¯çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02918v1">PDF</a> Accepted in ANSyA 2025: 1st International Workshop on Advanced   Neuro-Symbolic Applications</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ç³–å°¿ç—…è§†ç½‘è†œç—…å˜åˆ†ç±»çš„ç¥ç»ç¬¦å·æ¡†æ¶KG-DGï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è§†è§‰å˜å‹å™¨å’Œä¸“å®¶å¼•å¯¼çš„ç¬¦å·æ¨ç†ï¼Œä»¥å®ç°è·¨æœªè§é¢†åŸŸçš„ç¨³å¥æ³›åŒ–ã€‚é€šè¿‡ä¸´åºŠç—…ç¶æœ¬ä½“ã€ç»“æ„åŒ–è§„åˆ™ç‰¹å¾å’Œè§†ç½‘è†œè¡€ç®¡åˆ†å‰²çš„ç»“åˆï¼Œä¸æ·±åº¦è§†è§‰è¡¨ç¤ºé€šè¿‡ç½®ä¿¡åº¦åŠ æƒé›†æˆç­–ç•¥ç›¸èåˆã€‚è¯¥æ¡†æ¶é€šè¿‡æœ€å°åŒ–åŸŸåµŒå…¥ä¹‹é—´çš„KLæ•£åº¦æ¥è§£å†³å•åŸŸæ³›åŒ–ï¼ˆSDGï¼‰å’Œå¤šåŸŸæ³›åŒ–ï¼ˆMDGï¼‰é—®é¢˜ï¼Œä»è€Œå¼ºåˆ¶æ‰§è¡Œé«˜çº§ä¸´åºŠè¯­ä¹‰çš„å¯¹é½ã€‚åœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è·¨åŸŸè®¾ç½®ä¸­å®ç°äº†é«˜è¾¾5.2%çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶åœ¨åŸºçº¿ViTæ¨¡å‹ä¸Šå®ç°äº†6%çš„æ”¹è¿›ã€‚ç¬¦å·æ¨¡å‹åœ¨MDGä¸­å®ç°äº†63.67%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè€Œå®Œæ•´çš„ç¥ç»ç¬¦å·é›†æˆåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„SDGåœºæ™¯ä¸­è¾¾åˆ°äº†æœ€é«˜å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒé¢†åŸŸå­˜åœ¨æ¨¡å‹æ³›åŒ–é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨æœªè§é¢†åŸŸæƒ…å†µä¸‹ï¼Œæ¨¡å‹è®­ç»ƒçš„å•æºæ•°æ®ç»å¸¸æ— æ³•é€‚åº”çœŸå®ä¸–ç•Œçš„åˆ†å¸ƒå˜åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºKG-DGçš„ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œç”¨äºç³–å°¿ç—…è§†ç½‘è†œç—…å˜åˆ†ç±»ï¼Œç»“åˆäº†è§†è§‰å˜å‹å™¨å’Œä¸“å®¶å¼•å¯¼çš„ç¬¦å·æ¨ç†ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨ä¸´åºŠç—…ç¶æœ¬ä½“å’Œç»“æ„åŒ–è§„åˆ™ç‰¹å¾è¿›è¡Œè§†ç½‘è†œè¡€ç®¡åˆ†å‰²ï¼Œå¹¶é€šè¿‡ç½®ä¿¡åº¦åŠ æƒé›†æˆç­–ç•¥ä¸æ·±åº¦è§†è§‰è¡¨ç¤ºç›¸èåˆã€‚</li>
<li>é€šè¿‡æœ€å°åŒ–åŸŸåµŒå…¥ä¹‹é—´çš„KLæ•£åº¦æ¥è§£å†³å•åŸŸå’Œå¤šåŸŸæ³›åŒ–é—®é¢˜ï¼Œå¼ºè°ƒé«˜çº§ä¸´åºŠè¯­ä¹‰çš„å¯¹é½ã€‚</li>
<li>åœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨åŸŸè®¾ç½®ä¸­å…·æœ‰æŒ‘æˆ˜æ€§çš„æƒ…å†µã€‚</li>
<li>ç¬¦å·æ¨¡å‹åœ¨å¤šåŸŸæ³›åŒ–ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè€Œå®Œæ•´çš„ç¥ç»ç¬¦å·é›†æˆåœ¨å•åŸŸæ³›åŒ–åœºæ™¯ä¸­è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c89674a64b3d9066c643cb341668c5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9cfc0509b84d18424c61ef2c47ffb6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0640e669c820b2df87780f2f2648e77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4eca8e745c1b4bbca91aea40ce1fc252.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25234fe7e31c109830fe3fbf4cff8e45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e4e454dad5071c5166732628921f184.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4ba4481343c37f98ce4ab028dbf3101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7114554281ef4726dfd15eb006e3f71b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Is-Synthetic-Image-Augmentation-Useful-for-Imbalanced-Classification-Problems-Case-Study-on-the-MIDOG2025-Atypical-Cell-Detection-Competition"><a href="#Is-Synthetic-Image-Augmentation-Useful-for-Imbalanced-Classification-Problems-Case-Study-on-the-MIDOG2025-Atypical-Cell-Detection-Competition" class="headerlink" title="Is Synthetic Image Augmentation Useful for Imbalanced Classification   Problems? Case-Study on the MIDOG2025 Atypical Cell Detection Competition"></a>Is Synthetic Image Augmentation Useful for Imbalanced Classification   Problems? Case-Study on the MIDOG2025 Atypical Cell Detection Competition</h2><p><strong>Authors:Leire Benito-Del-Valle, Pedro A. Moreno-SÃ¡nchez, Itziar Egusquiza, Itsaso Vitoria, Artzai PicÃ³n, Cristina LÃ³pez-Saratxaga, Adrian Galdran</strong></p>
<p>The MIDOG 2025 challenge extends prior work on mitotic figure detection by introducing a new Track 2 on atypical mitosis classification. This task aims to distinguish normal from atypical mitotic figures in histopathology images, a clinically relevant but highly imbalanced and cross-domain problem. We investigated two complementary backbones: (i) ConvNeXt-Small, pretrained on ImageNet, and (ii) a histopathology-specific ViT from Lunit trained via self-supervision. To address the strong prevalence imbalance (9408 normal vs. 1741 atypical), we synthesized additional atypical examples to approximate class balance and compared models trained with real-only vs. real+synthetic data. Using five-fold cross-validation, both backbones reached strong performance (mean AUROC approximately 95 percent), with ConvNeXt achieving slightly higher peaks while Lunit exhibited greater fold-to-fold stability. Synthetic balancing, however, did not lead to consistent improvements. On the organizersâ€™ preliminary hidden test set, explicitly designed as an out-of-distribution debug subset, ConvNeXt attained the highest AUROC (95.4 percent), whereas Lunit remained competitive on balanced accuracy. These findings suggest that both ImageNet and domain-pretrained backbones are viable for atypical mitosis classification, with domain-pretraining conferring robustness and ImageNet pretraining reaching higher peaks, while naive synthetic balancing has limited benefit. Full hidden test set results will be reported upon challenge completion. </p>
<blockquote>
<p>MIDOG 2025æŒ‘æˆ˜èµ›åœ¨å‰æœŸç»†èƒåˆ†è£‚å›¾åƒæ£€æµ‹å·¥ä½œçš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†æ–°çš„è½¨é“2â€”â€”ä¸å…¸å‹ç»†èƒåˆ†è£‚åˆ†ç±»ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡æ—¨åœ¨åŒºåˆ†ç—…ç†å›¾åƒä¸­çš„æ­£å¸¸ç»†èƒåˆ†è£‚ä¸å¼‚å¸¸ç»†èƒåˆ†è£‚ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ä¸´åºŠæ„ä¹‰ä½†é«˜åº¦ä¸å¹³è¡¡ã€è·¨é¢†åŸŸçš„é—®é¢˜ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸¤ç§äº’è¡¥çš„éª¨å¹²ç½‘ç»œï¼š(i)åœ¨ImageNetä¸Šé¢„è®­ç»ƒçš„ConvNeXt-Smallï¼›(ii)é€šè¿‡è‡ªæˆ‘ç›‘ç£è®­ç»ƒé’ˆå¯¹ç—…ç†å­¦çš„ViTï¼ˆæ¥è‡ªLunitï¼‰ã€‚ä¸ºäº†è§£å†³ä¸¥é‡çš„ä¸å¹³è¡¡é—®é¢˜ï¼ˆæ­£å¸¸æ ·æœ¬9408ä¸å¼‚å¸¸æ ·æœ¬ä»…1741ï¼‰ï¼Œæˆ‘ä»¬é€šè¿‡åˆæˆé¢å¤–çš„å¼‚å¸¸æ ·æœ¬è¿‘ä¼¼åœ°å¹³è¡¡äº†ç±»åˆ«åˆ†å¸ƒï¼Œå¹¶æ¯”è¾ƒäº†ä»…ç”¨çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹ä¸ç»“åˆçœŸå®æ•°æ®å’Œåˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½ã€‚é€šè¿‡äº”æŠ˜äº¤å‰éªŒè¯ï¼Œè¿™ä¸¤ç§éª¨å¹²ç½‘ç»œéƒ½è¾¾åˆ°äº†è¾ƒå¼ºçš„æ€§èƒ½ï¼ˆå¹³å‡AUROCçº¦95%ï¼‰ï¼Œå…¶ä¸­ConvNeXtå–å¾—äº†ç•¥é«˜çš„å³°å€¼ï¼Œè€ŒLunitå±•ç°å‡ºæ›´å¼ºçš„è·¨æŠ˜ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œåˆæˆå¹³è¡¡ç­–ç•¥å¹¶æ²¡æœ‰å¸¦æ¥æŒç»­çš„æ”¹è¿›ã€‚åœ¨ç»„ç»‡è€…ç‰¹æ„è®¾è®¡çš„éšè—æµ‹è¯•é›†ä¸Šï¼Œä½œä¸ºç¦»ç¾¤è°ƒè¯•å­é›†ï¼ŒConvNeXtå–å¾—äº†æœ€é«˜çš„AUROCï¼ˆ95.4%ï¼‰ï¼Œè€ŒLunitåœ¨å¹³è¡¡ç²¾åº¦ä¸Šä¿æŒäº†ç«äº‰åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒImageNetå’Œé¢†åŸŸé¢„è®­ç»ƒçš„éª¨å¹²ç½‘ç»œå¯¹äºä¸å…¸å‹ç»†èƒåˆ†è£‚åˆ†ç±»éƒ½æ˜¯å¯è¡Œçš„é€‰æ‹©ï¼Œé¢†åŸŸé¢„è®­ç»ƒèµ‹äºˆæ¨¡å‹ç¨³å¥æ€§ï¼Œè€ŒImageNeté¢„è®­ç»ƒåˆ™èƒ½è¾¾åˆ°æ›´é«˜çš„å³°å€¼æ€§èƒ½ï¼Œè€Œç®€å•çš„åˆæˆå¹³è¡¡ç­–ç•¥å¸¦æ¥çš„ç›Šå¤„æœ‰é™ã€‚å®Œæ•´çš„éšè—æµ‹è¯•é›†ç»“æœå°†åœ¨æŒ‘æˆ˜èµ›ç»“æŸåå…¬å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02612v1">PDF</a> version 0, to be updated; submitted to midog 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MIDOG 2025æŒ‘æˆ˜èµ›åœ¨ç»†èƒåˆ†è£‚æ£€æµ‹æ–¹é¢çš„æ–°ä»»åŠ¡â€”â€”ä¸å…¸å‹ç»†èƒåˆ†è£‚åˆ†ç±»ã€‚è¯¥ä»»åŠ¡æ—¨åœ¨åŒºåˆ†ç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­çš„æ­£å¸¸ç»†èƒå’Œä¸å…¸å‹ç»†èƒåˆ†è£‚ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†ä¸¤ç§äº’è¡¥æ¨¡å‹éª¨æ¶ï¼šä¸€ç§æ˜¯åŸºäºImageNeté¢„è®­ç»ƒçš„ConvNeXt-Smallæ¨¡å‹ï¼Œå¦ä¸€ç§æ˜¯åŸºäºè‡ªæˆ‘ç›‘ç£è®­ç»ƒçš„ç‰¹å®šäºç»„ç»‡ç—…ç†å­¦çš„ViTæ¨¡å‹ã€‚ä¸ºäº†è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç ”ç©¶é€šè¿‡åˆæˆé¢å¤–çš„å¼‚å¸¸æ ·æœ¬æ¥è¿‘ä¼¼å¹³è¡¡ç±»åˆ«æ¯”ä¾‹ï¼Œå¹¶æ¯”è¾ƒäº†ä»…ä½¿ç”¨çœŸå®æ•°æ®å’ŒçœŸå®æ•°æ®ç»“åˆåˆæˆæ•°æ®çš„æ¨¡å‹è®­ç»ƒæ•ˆæœã€‚é€šè¿‡äº”æŠ˜äº¤å‰éªŒè¯ï¼Œä¸¤ç§æ¨¡å‹éª¨æ¶å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ˆå¹³å‡AUROCçº¦ä¸º95%ï¼‰ï¼Œå…¶ä¸­ConvNeXtåœ¨æŸäº›æƒ…å†µä¸‹è¾¾åˆ°æ›´é«˜çš„å³°å€¼ï¼Œè€ŒLunitåˆ™å±•ç°å‡ºæ›´å¼ºçš„ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œåˆæˆå¹³è¡¡æ–¹æ³•å¹¶æœªå¸¦æ¥ä¸€è‡´çš„æ”¹è¿›ã€‚åœ¨ç»„ç»‡è€…è®¾è®¡çš„ä½œä¸ºåˆ†å¸ƒå¤–è°ƒè¯•å­é›†çš„éšè—æµ‹è¯•é›†ä¸Šï¼ŒConvNeXtè·å¾—äº†æœ€é«˜çš„AUROCï¼ˆ95.4%ï¼‰ï¼Œè€ŒLunitåœ¨å¹³è¡¡ç²¾åº¦ä¸Šä¿æŒäº†ç«äº‰åŠ›ã€‚è¿™è¡¨æ˜ImageNeté¢„è®­ç»ƒå’Œé¢†åŸŸé¢„è®­ç»ƒéª¨æ¶å¯¹äºä¸å…¸å‹ç»†èƒåˆ†è£‚åˆ†ç±»éƒ½æ˜¯å¯è¡Œçš„ï¼Œè€Œåˆæˆå¹³è¡¡æ–¹æ³•çš„æ•ˆç›Šæœ‰é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIDOG 2025æŒ‘æˆ˜èµ›å¼•å…¥äº†æ–°çš„Task 2ï¼šä¸å…¸å‹ç»†èƒåˆ†è£‚åˆ†ç±»ã€‚è¿™é¡¹ä»»åŠ¡çš„ç›®æ ‡æ˜¯åœ¨ç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­åŒºåˆ†æ­£å¸¸å’Œä¸å…¸å‹çš„ç»†èƒåˆ†è£‚ã€‚è¿™æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†ä¸¤ç§äº’è¡¥æ¨¡å‹éª¨æ¶è¿›è¡Œæ¢ç©¶ï¼šåŸºäºImageNeté¢„è®­ç»ƒçš„ConvNeXtæ¨¡å‹å’ŒåŸºäºè‡ªæˆ‘ç›‘ç£è®­ç»ƒçš„ç‰¹å®šäºç»„ç»‡ç—…ç†å­¦çš„ViTæ¨¡å‹ï¼ˆæ¥è‡ªLunitï¼‰ã€‚ä¸¤è€…éƒ½å–å¾—äº†å¾ˆå¥½çš„æ€§èƒ½ã€‚</li>
<li>ç”±äºæ•°æ®é›†å­˜åœ¨ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼ˆæ­£å¸¸æ ·æœ¬æ•°é‡è¿œå¤§äºå¼‚å¸¸æ ·æœ¬ï¼‰ï¼Œç ”ç©¶è€…å°è¯•é€šè¿‡åˆæˆé¢å¤–çš„å¼‚å¸¸æ ·æœ¬æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†æ•ˆæœå¹¶ä¸æ˜æ˜¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1e36282c52c227df1e205bb9dd2a1af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac51723cb6f4ac0fdd5d6c29c43e8ee2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0d0c456c5275aac1b633a827e84ba99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21d4120d27f2d269ac328178872a15ee.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation"><a href="#MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation" class="headerlink" title="MedDINOv3: How to adapt vision foundation models for medical image   segmentation?"></a>MedDINOv3: How to adapt vision foundation models for medical image   segmentation?</h2><p><strong>Authors:Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang</strong></p>
<p>Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ricklisz/MedDINOv3">https://github.com/ricklisz/MedDINOv3</a>. </p>
<blockquote>
<p>åœ¨CTå’ŒMRIæ‰«æä¸­ï¼Œå™¨å®˜å’Œè‚¿ç˜¤çš„ç²¾ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ å·²ç»æ¨åŠ¨äº†è‡ªåŠ¨åŒ–åˆ†å‰²çš„å‘å±•ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä»ç„¶å…·æœ‰ç‰¹å®šæ€§ï¼Œç¼ºä¹è·¨ä¸åŒæ¨¡æ€å’Œæœºæ„ä¹‹é—´çš„é€šç”¨æ€§ã€‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨ç™¾äº¿çº§è‡ªç„¶å›¾åƒä¸Šçš„é¢„è®­ç»ƒæä¾›äº†å¼ºå¤§ä¸”å¯è¿ç§»çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå°†å…¶é€‚åº”åŒ»å­¦æˆåƒé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¤§å¤šæ•°åŸºç¡€æ¨¡å‹çš„ViTä¸»å¹²åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ä»ç„¶è¡¨ç°ä¸ä½³ï¼Œä¸åŠä¸“ä¸šåŒ–çš„CNNï¼›ï¼ˆ2ï¼‰è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´çš„å·¨å¤§é¢†åŸŸå·®è·é™åˆ¶äº†å¯è¿ç§»æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MedDINOv3ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºå°†DINOv3é€‚åº”åŒ»å­¦åˆ†å‰²ã€‚æˆ‘ä»¬é¦–å…ˆå›é¡¾äº†ç®€å•çš„ViTsï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¶æ„è¿›è¡Œå¤šå°ºåº¦ä»¤ç‰Œèšåˆã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨CT-3Mä¸Šè¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾é€‰çš„åŒ…å«387ä¸‡å¼ è½´å‘CTåˆ‡ç‰‡çš„é›†åˆï¼Œä½¿ç”¨å¤šé˜¶æ®µçš„DINOv3é…æ–¹æ¥å­¦ä¹ ç¨³å¥çš„å¯†é›†ç‰¹å¾ã€‚MedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æˆ–è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ç»Ÿä¸€ä¸»å¹²çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ricklisz/MedDINOv3">https://github.com/ricklisz/MedDINOv3</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02379v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MedDINOv3æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å°†DINOv3é€‚åº”äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚é€šè¿‡é‡æ–°å®¡è§†æ™®é€šViTå¹¶è®¾è®¡å…·æœ‰å¤šå°ºåº¦ä»¤ç‰Œèšåˆçš„ç®€å•æœ‰æ•ˆæ¶æ„ï¼Œä»¥åŠä½¿ç”¨å¤šé˜¶æ®µDINOv3é…æ–¹åœ¨CT-3Mä¸Šè¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œå­¦ä¹ ç¨³å¥çš„å¯†é›†ç‰¹å¾ï¼ŒMedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¿‡æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ç»Ÿä¸€éª¨æ¶çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å™¨å®˜å’Œè‚¿ç˜¤çš„å‡†ç¡®åˆ†å‰²åœ¨CTå’ŒMRIæ‰«æä¸­å¯¹è¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>è™½ç„¶æ·±åº¦å­¦ä¹ å·²ç»æ¨è¿›äº†è‡ªåŠ¨åˆ†å‰²ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä»ç„¶ç¼ºä¹è·¨æ¨¡æ€å’Œæœºæ„çš„é€šç”¨æ€§ã€‚</li>
<li>è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨è‡ªç„¶å›¾åƒä¸Šçš„é¢„è®­ç»ƒæä¾›äº†å¼ºå¤§çš„å¯è½¬ç§»è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>å°†è¿™äº›æ¨¡å‹é€‚åº”äºåŒ»å­¦æˆåƒé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šViTä¸»å¹²åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šæ€§èƒ½ä¸è¶³ï¼Œä»¥åŠè‡ªç„¶ä¸åŒ»å­¦å›¾åƒä¹‹é—´çš„åŸŸå·®è·é™åˆ¶äº†å¯è½¬ç§»æ€§ã€‚</li>
<li>MedDINOv3æ¡†æ¶é€šè¿‡è®¾è®¡ç®€å•æœ‰æ•ˆçš„æ¶æ„å’Œè¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>MedDINOv3ä½¿ç”¨å¤šå°ºåº¦ä»¤ç‰Œèšåˆçš„æ™®é€šViTï¼Œå¹¶åœ¨CT-3Mæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9fb0243cd38ff90ab236674e41771cda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69eb7b8f98c17f7f1a653c65dd53dc9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2e2cb37fabb471dedd94f68ca9e27b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f3f2b82643e7fec0a1229c41ce5fb1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfe3be72dd8124edfca2d135d61d1066.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Unified-Low-level-Foundation-Model-for-Enhancing-Pathology-Image-Quality"><a href="#A-Unified-Low-level-Foundation-Model-for-Enhancing-Pathology-Image-Quality" class="headerlink" title="A Unified Low-level Foundation Model for Enhancing Pathology Image   Quality"></a>A Unified Low-level Foundation Model for Enhancing Pathology Image   Quality</h2><p><strong>Authors:Ziyi Liu, Zhe Xu, Jiabo Ma, Wenqaing Li, Junlin Hou, Fuxiang Huang, Xi Wang, Ronald Cheong Kin Chan, Terence Tsz Wai Wong, Hao Chen</strong></p>
<p>Foundation models have revolutionized computational pathology by achieving remarkable success in high-level diagnostic tasks, yet the critical challenge of low-level image enhancement remains largely unaddressed. Real-world pathology images frequently suffer from degradations such as noise, blur, and low resolution due to slide preparation artifacts, staining variability, and imaging constraints, while the reliance on physical staining introduces significant costs, delays, and inconsistency. Although existing methods target individual problems like denoising or super-resolution, their task-specific designs lack the versatility to handle the diverse low-level vision challenges encountered in practice. To bridge this gap, we propose the first unified Low-level Pathology Foundation Model (LPFM), capable of enhancing image quality in restoration tasks, including super-resolution, deblurring, and denoising, as well as facilitating image translation tasks like virtual staining (H&amp;E and special stains), all through a single adaptable architecture. Our approach introduces a contrastive pre-trained encoder that learns transferable, stain-invariant feature representations from 190 million unlabeled pathology images, enabling robust identification of degradation patterns. A unified conditional diffusion process dynamically adapts to specific tasks via textual prompts, ensuring precise control over output quality. Trained on a curated dataset of 87,810 whole slied images (WSIs) across 34 tissue types and 5 staining protocols, LPFM demonstrates statistically significant improvements (p&lt;0.01) over state-of-the-art methods in most tasks (56&#x2F;66), achieving Peak Signal-to-Noise Ratio (PSNR) gains of 10-15% for image restoration and Structural Similarity Index Measure (SSIM) improvements of 12-18% for virtual staining. </p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹é€šè¿‡é«˜çº§è¯Šæ–­ä»»åŠ¡å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä»è€Œå½»åº•æ”¹å˜äº†è®¡ç®—ç—…ç†å­¦ã€‚ç„¶è€Œï¼Œä½å±‚æ¬¡å›¾åƒå¢å¼ºçš„å…³é”®æŒ‘æˆ˜ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªå¾—åˆ°è§£å†³ã€‚ç°å®ä¸–ç•Œä¸­çš„ç—…ç†å›¾åƒç”±äºå¹»ç¯ç‰‡åˆ¶ä½œè¿‡ç¨‹ä¸­çš„ä¼ªè¿¹ã€æŸ“è‰²å˜é‡ä»¥åŠæˆåƒé™åˆ¶ç­‰å› ç´ ï¼Œç»å¸¸å‡ºç°è¯¸å¦‚å™ªå£°ã€æ¨¡ç³Šå’Œåˆ†è¾¨ç‡ä½ç­‰é—®é¢˜ã€‚è€Œå¯¹ç‰©ç†æŸ“è‰²çš„ä¾èµ–åˆ™å¼•å…¥äº†é‡å¤§æˆæœ¬ã€å»¶è¿Ÿå’Œä¸ä¸€è‡´æ€§ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•é’ˆå¯¹å»å™ªæˆ–è¶…åˆ†è¾¨ç‡ç­‰å•ä¸ªé—®é¢˜è¿›è¡Œäº†ç ”ç©¶ï¼Œä½†å…¶ä»»åŠ¡ç‰¹å®šè®¾è®¡ç¼ºä¹åº”å¯¹å®è·µä¸­é‡åˆ°çš„å„ç§ä½å±‚æ¬¡è§†è§‰æŒ‘æˆ˜çš„èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ä½å±‚æ¬¡ç—…ç†åŸºç¡€æ¨¡å‹ï¼ˆLPFMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ¢å¤ä»»åŠ¡ä¸­æé«˜å›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šå’Œå»å™ªï¼Œå¹¶èƒ½å¤Ÿä¿ƒè¿›å›¾åƒç¿»è¯‘ä»»åŠ¡ï¼Œå¦‚è™šæ‹ŸæŸ“è‰²ï¼ˆH&amp;Eå’Œç‰¹æ®ŠæŸ“è‰²ï¼‰ï¼Œæ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½é€šè¿‡å•ä¸€çš„é€‚åº”æ€§æ¶æ„å®ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå¯¹æ¯”é¢„è®­ç»ƒç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨ä»1.9äº¿å¼ æœªæ ‡è®°çš„ç—…ç†å›¾åƒä¸­å­¦ä¹ å¯è½¬ç§»ã€æŸ“è‰²ä¸å˜çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œèƒ½å¤Ÿç¨³å¥åœ°è¯†åˆ«é€€åŒ–æ¨¡å¼ã€‚ä¸€ä¸ªç»Ÿä¸€çš„æ¡ä»¶æ‰©æ•£è¿‡ç¨‹é€šè¿‡æ–‡æœ¬æç¤ºåŠ¨æ€é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œç¡®ä¿å¯¹è¾“å‡ºè´¨é‡çš„ç²¾ç¡®æ§åˆ¶ã€‚LPFMæ˜¯åœ¨34ç§ç»„ç»‡ç±»å‹å’Œ5ç§æŸ“è‰²åè®®çš„87810å¼ å…¨æ™¯å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œåœ¨å¤§å¤šæ•°ä»»åŠ¡ï¼ˆ56&#x2F;66ï¼‰ä¸­éƒ½å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ˆp&lt;0.01ï¼‰ï¼Œåœ¨å›¾åƒæ¢å¤æ–¹é¢ï¼Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜äº†ç™¾åˆ†ä¹‹åè‡³ç™¾åˆ†ä¹‹åäº”ï¼Œåœ¨è™šæ‹ŸæŸ“è‰²æ–¹é¢ï¼Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰æé«˜äº†ç™¾åˆ†ä¹‹åäºŒè‡³ç™¾åˆ†ä¹‹åå…«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01071v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨åŸºç¡€æ¨¡å‹å·²ç»ä¸ºè®¡ç®—ç—…ç†å­¦å¸¦æ¥æ˜¾è‘—æˆåŠŸçš„èƒŒæ™¯ä¸‹ï¼Œä½å±‚æ¬¡å›¾åƒå¢å¼ºé—®é¢˜ä¾ç„¶æœªå¾—åˆ°æœ‰æ•ˆè§£å†³ã€‚ç°å®ç—…ç†å­¦å›¾åƒå­˜åœ¨å™ªå£°ã€æ¨¡ç³Šå’Œä½åˆ†è¾¨ç‡ç­‰ç¼ºé™·ï¼Œä¾èµ–äºç‰©ç†æŸ“è‰²æŠ€æœ¯ï¼Œå¯¼è‡´äº†æˆæœ¬å¢åŠ ã€å»¶è¿Ÿå’Œä¸ä¸€è‡´æ€§ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªç»Ÿä¸€çš„ä½å±‚æ¬¡ç—…ç†å­¦åŸºç¡€æ¨¡å‹ï¼ˆLPFMï¼‰ï¼Œèƒ½å¤Ÿåœ¨æ¢å¤ä»»åŠ¡ä¸­æé«˜å›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šå’Œå»å™ªç­‰ï¼Œå¹¶ä¿ƒè¿›å›¾åƒç¿»è¯‘ä»»åŠ¡å¦‚è™šæ‹ŸæŸ“è‰²ã€‚è¯¥æ¨¡å‹é€šè¿‡å•ä¸€çš„å¯é€‚åº”æ¶æ„å®ç°å¤šç§åŠŸèƒ½ã€‚é€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒç¼–ç å™¨å­¦ä¹ å¯è½¬ç§»ã€æŸ“è‰²ä¸å˜çš„ç‰¹æ€§è¡¨ç¤ºï¼Œä»å¤§é‡æœªæ ‡è®°çš„ç—…ç†å›¾åƒä¸­å­¦ä¹ å¯è½¬ç§»çš„ç‰¹æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¯é åœ°è¯†åˆ«é€€åŒ–æ¨¡å¼ã€‚è®­ç»ƒäºè·¨è¶Šä¸åŒç»„ç»‡å’ŒæŸ“è‰²åè®®çš„æ•´ä¸ªåˆ‡ç‰‡å›¾åƒæ•°æ®é›†ä¸Šï¼ŒLPFMåœ¨å¤šæ•°ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œæé«˜äº†å³°å€¼ä¿¡å™ªæ¯”å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡å€¼ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ¨¡å‹æé«˜äº†ç—…ç†å›¾åƒçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä½å±‚æ¬¡å›¾åƒå¢å¼ºé—®é¢˜ä»ç„¶å­˜åœ¨ã€‚</li>
<li>ç°å®ç—…ç†å­¦å›¾åƒå­˜åœ¨å™ªå£°ã€æ¨¡ç³Šå’Œä½åˆ†è¾¨ç‡ç­‰é—®é¢˜ï¼Œè¿™é™åˆ¶äº†è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç°æœ‰çš„æ–¹æ³•é’ˆå¯¹ç‰¹å®šé—®é¢˜å¦‚å»å™ªæˆ–è¶…åˆ†è¾¨ç‡è¿›è¡Œè®¾è®¡ï¼Œç¼ºä¹å¤„ç†å®è·µä¸­é‡åˆ°çš„å„ç§ä½å±‚æ¬¡è§†è§‰æŒ‘æˆ˜çš„èƒ½åŠ›ã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€çš„ä½å±‚æ¬¡ç—…ç†å­¦åŸºç¡€æ¨¡å‹ï¼ˆLPFMï¼‰ï¼Œç”¨äºå¢å¼ºå›¾åƒè´¨é‡å¹¶ä¿ƒè¿›å›¾åƒç¿»è¯‘ä»»åŠ¡å¦‚è™šæ‹ŸæŸ“è‰²ã€‚</li>
<li>LPFMé€šè¿‡ä¸€ä¸ªå•ä¸€çš„å¯é€‚åº”æ¶æ„å®ç°å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šå’Œå»å™ªç­‰ä»»åŠ¡ã€‚</li>
<li>LPFMé€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒç¼–ç å™¨å­¦ä¹ å¯è½¬ç§»å’ŒæŸ“è‰²ä¸å˜çš„ç‰¹æ€§è¡¨ç¤ºï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d20cdddb2abb4730756f79741880a6c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97759328abe853efc3b7f37717a7d882.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aed6f770d902f18fff2c364badcaa54f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b949e0d3274e79cc3b1cfa611452914f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Causal-Interpretation-of-Sparse-Autoencoder-Features-in-Vision"><a href="#Causal-Interpretation-of-Sparse-Autoencoder-Features-in-Vision" class="headerlink" title="Causal Interpretation of Sparse Autoencoder Features in Vision"></a>Causal Interpretation of Sparse Autoencoder Features in Vision</h2><p><strong>Authors:Sangyu Han, Yearim Kim, Nojun Kwak</strong></p>
<p>Understanding what sparse auto-encoder (SAE) features in vision transformers truly represent is usually done by inspecting the patches where a featureâ€™s activation is highest. However, self-attention mixes information across the entire image, so an activated patch often co-occurs with-but does not cause-the featureâ€™s firing. We propose Causal Feature Explanation (CaFE), which leverages Effective Receptive Field (ERF). We consider each activation of an SAE feature to be a target and apply input-attribution methods to identify the image patches that causally drive that activation. Across CLIP-ViT features, ERF maps frequently diverge from naive activation maps, revealing hidden context dependencies (e.g., a â€œroaring faceâ€ feature that requires the co-occurrence of eyes and nose, rather than merely an open mouth). Patch insertion tests confirm that CaFE more effectively recovers or suppresses feature activations than activation-ranked patches. Our results show that CaFE yields more faithful and semantically precise explanations of vision-SAE features, highlighting the risk of misinterpretation when relying solely on activation location. </p>
<blockquote>
<p>ç†è§£è§†è§‰è½¬æ¢å™¨ä¸­çš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ç‰¹å¾çœŸæ­£ä»£è¡¨ä»€ä¹ˆï¼Œé€šå¸¸æ˜¯é€šè¿‡æ£€æŸ¥ç‰¹å¾æ¿€æ´»æœ€é«˜çš„æ–‘å—æ¥å®ç°çš„ã€‚ç„¶è€Œï¼Œè‡ªæ³¨æ„åŠ›ä¼šæ··åˆæ•´ä¸ªå›¾åƒçš„ä¿¡æ¯ï¼Œå› æ­¤æ¿€æ´»çš„æ–‘å—é€šå¸¸ä¸ç‰¹å¾çš„è§¦å‘åŒæ—¶å‘ç”Ÿï¼Œä½†å¹¶ä¸å¼•èµ·ç‰¹å¾çš„è§¦å‘ã€‚æˆ‘ä»¬æå‡ºäº†å› æœç‰¹å¾è§£é‡Šï¼ˆCaFEï¼‰ï¼Œå®ƒåˆ©ç”¨æœ‰æ•ˆæ„Ÿå—é‡ï¼ˆERFï¼‰ã€‚æˆ‘ä»¬è®¤ä¸ºSAEç‰¹å¾çš„æ¯æ¬¡æ¿€æ´»éƒ½æ˜¯ä¸€ä¸ªç›®æ ‡ï¼Œå¹¶åº”ç”¨è¾“å…¥å½’å› æ–¹æ³•æ¥è¯†åˆ«é‚£äº›å› æœé©±åŠ¨è¯¥æ¿€æ´»çš„å›¾åƒæ–‘å—ã€‚åœ¨CLIP-ViTç‰¹å¾ä¸­ï¼ŒERFæ˜ å°„ç»å¸¸ä¸ç®€å•çš„æ¿€æ´»æ˜ å°„ç›¸åç¦»ï¼Œæ­ç¤ºäº†éšè—çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œâ€œå’†å“®çš„è„¸â€ç‰¹å¾éœ€è¦çœ¼ç›å’Œé¼»å­çš„å…±å­˜ï¼Œè€Œä¸ä»…ä»…æ˜¯å¼ å¼€çš„å˜´å·´ï¼‰ã€‚æ–‘å—æ’å…¥æµ‹è¯•è¯å®ï¼Œç›¸æ¯”æ¿€æ´»æ’åºæ–‘å—ï¼ŒCaFEæ›´èƒ½æ¢å¤æˆ–æŠ‘åˆ¶ç‰¹å¾æ¿€æ´»ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒCaFEå¯¹è§†è§‰SAEç‰¹å¾æä¾›äº†æ›´å¿ å®ã€è¯­ä¹‰æ›´ç²¾ç¡®çš„è§£é‡Šï¼Œå¼ºè°ƒä»…ä¾èµ–æ¿€æ´»ä½ç½®è¿›è¡Œè§£é‡Šçš„é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00749v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é€šè¿‡å¯¹ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰åœ¨è§†è§‰è½¬æ¢å™¨ä¸­çš„ç‰¹æ€§è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ä»…é€šè¿‡è§‚å¯Ÿç‰¹å¾æ¿€æ´»æœ€é«˜çš„æ–‘å—æ¥ç†è§£å…¶çœŸæ­£å«ä¹‰æ˜¯ä¸å¤Ÿå‡†ç¡®çš„ã€‚ç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¼šæ··åˆå›¾åƒä¸­æ‰€æœ‰ä¿¡æ¯ï¼Œå› æ­¤ç‰¹å¾æ¿€æ´»é€šå¸¸ä¼´éšç€æŸäº›æ–‘å—çš„å‡ºç°ï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€è¿™äº›æ–‘å—ç›´æ¥å¯¼è‡´äº†ç‰¹å¾çš„æ¿€æ´»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å› æœç‰¹å¾è§£é‡Šï¼ˆCaFEï¼‰æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æœ‰æ•ˆæ„Ÿå—é‡ï¼ˆERFï¼‰æŠ€æœ¯ã€‚æˆ‘ä»¬å°†æ¯ä¸ªSAEç‰¹å¾çš„æ¿€æ´»è§†ä¸ºç›®æ ‡ï¼Œå¹¶è¿ç”¨è¾“å…¥å½’å› æ–¹æ³•æ¥è¯†åˆ«é‚£äº›çœŸæ­£é©±åŠ¨è¯¥ç‰¹å¾æ¿€æ´»çš„å›¾åƒæ–‘å—ã€‚åœ¨CLIP-ViTç‰¹å¾ä¸­ï¼Œæˆ‘ä»¬å‘ç°ERFæ˜ å°„ä¸ç®€å•çš„æ¿€æ´»æ˜ å°„å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæ­ç¤ºäº†éšè—ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œâ€œå’†å“®é¢å­”â€ç‰¹å¾éœ€è¦çœ¼ç›å’Œé¼»å­çš„åŒæ—¶å‡ºç°ï¼Œè€Œä¸ä»…ä»…æ˜¯å˜´å·´çš„å¼ å¼€ï¼‰ã€‚é€šè¿‡æ–‘å—æ’å…¥æµ‹è¯•è¯å®ï¼Œç›¸è¾ƒäºåŸºäºæ¿€æ´»çš„æ–‘å—æ’åæ–¹æ³•ï¼ŒCaFEæ›´èƒ½æœ‰æ•ˆåœ°æ¢å¤æˆ–æŠ‘åˆ¶ç‰¹å¾æ¿€æ´»ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒCaFEæä¾›äº†å¯¹è§†è§‰SAEç‰¹å¾æ›´çœŸå®ã€è¯­ä¹‰æ›´ç²¾ç¡®çš„è§£é‡Šï¼Œå¹¶å¼ºè°ƒäº†ä»…ä¾èµ–æ¿€æ´»ä½ç½®è¿›è¡Œè§£é‡Šçš„é£é™©ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰åœ¨è§†è§‰è½¬æ¢å™¨ä¸­çš„ç‰¹æ€§ç†è§£éœ€è¦è¶…è¶Šç®€å•çš„æ¿€æ´»æ–‘å—è§‚å¯Ÿã€‚</li>
<li>è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¶‰åŠæ•´ä¸ªå›¾åƒçš„ä¿¡æ¯æ··åˆï¼Œå¯¼è‡´ç‰¹å¾æ¿€æ´»ä¸å…¶ç›´è§‚è§£é‡Šä¹‹é—´å¯èƒ½å­˜åœ¨åå·®ã€‚</li>
<li>å¼•å…¥å› æœç‰¹å¾è§£é‡Šï¼ˆCaFEï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æœ‰æ•ˆæ„Ÿå—é‡ï¼ˆERFï¼‰æŠ€æœ¯æ¥è¯†åˆ«çœŸæ­£é©±åŠ¨ç‰¹å¾æ¿€æ´»çš„å›¾åƒæ–‘å—ã€‚</li>
<li>ERFæ˜ å°„ä¸ç®€å•æ¿€æ´»æ˜ å°„å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæ­ç¤ºäº†ç‰¹å¾çš„éšè—ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚</li>
<li>é€šè¿‡æ–‘å—æ’å…¥æµ‹è¯•è¯å®ï¼ŒCaFEåœ¨æ¢å¤æˆ–æŠ‘åˆ¶ç‰¹å¾æ¿€æ´»æ–¹é¢æ¯”åŸºäºæ¿€æ´»çš„æ–‘å—æ’åæ–¹æ³•æ›´æœ‰æ•ˆã€‚</li>
<li>CaFEæä¾›äº†å¯¹è§†è§‰SAEç‰¹å¾æ›´çœŸå®ã€è¯­ä¹‰æ›´ç²¾ç¡®çš„è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ebd4b001202b2b5537287e03bdf7859.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaabc56306772ff777f40270ae1b4487.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18cddb9b26fe98049c1c9e5f604f47b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ad467a46d6cf04b1f035ac007d597ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b15ed7e78ffab9da95084ba026a605d7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Generalizable-Object-Re-Identification-via-Visual-In-Context-Prompting"><a href="#Generalizable-Object-Re-Identification-via-Visual-In-Context-Prompting" class="headerlink" title="Generalizable Object Re-Identification via Visual In-Context Prompting"></a>Generalizable Object Re-Identification via Visual In-Context Prompting</h2><p><strong>Authors:Zhizhong Huang, Xiaoming Liu</strong></p>
<p>Current object re-identification (ReID) methods train domain-specific models (e.g., for persons or vehicles), which lack generalization and demand costly labeled data for new categories. While self-supervised learning reduces annotation needs by learning instance-wise invariance, it struggles to capture \textit{identity-sensitive} features critical for ReID. This paper proposes Visual In-Context Prompting<del>(VICP), a novel framework where models trained on seen categories can directly generalize to unseen novel categories using only \textit{in-context examples} as prompts, without requiring parameter adaptation. VICP synergizes LLMs and vision foundation models</del>(VFM): LLMs infer semantic identity rules from few-shot positive&#x2F;negative pairs through task-specific prompting, which then guides a VFM (\eg, DINO) to extract ID-discriminative features via \textit{dynamic visual prompts}. By aligning LLM-derived semantic concepts with the VFMâ€™s pre-trained prior, VICP enables generalization to novel categories, eliminating the need for dataset-specific retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K object instances from e-commerce platforms, featuring multi-view images and cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks demonstrate that VICP outperforms baselines by a clear margin on unseen categories. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Hzzone/VICP">https://github.com/Hzzone/VICP</a>. </p>
<blockquote>
<p>å½“å‰çš„å¯¹è±¡å†è¯†åˆ«ï¼ˆReIDï¼‰æ–¹æ³•ä¸»è¦è®­ç»ƒç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ï¼ˆä¾‹å¦‚é’ˆå¯¹äººæˆ–è½¦è¾†ï¼‰ï¼Œè¿™æ ·çš„æ¨¡å‹ç¼ºä¹é€šç”¨æ€§ï¼Œå¹¶ä¸”éœ€è¦ä¸ºæ–°ç±»åˆ«æ ‡æ³¨æ˜‚è´µçš„æ ‡ç­¾æ•°æ®ã€‚è™½ç„¶è‡ªç›‘ç£å­¦ä¹ é€šè¿‡å®ä¾‹ä¸å˜æ€§å­¦ä¹ å‡å°‘äº†æ ‡æ³¨éœ€æ±‚ï¼Œä½†å®ƒéš¾ä»¥æ•è·å¯¹äºReIDè‡³å…³é‡è¦çš„â€œèº«ä»½æ•æ„Ÿâ€ç‰¹å¾ã€‚æœ¬æ–‡æå‡ºäº†Visual In-Context Promptingï¼ˆVICPï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥è®©åœ¨å¯è§ç±»åˆ«ä¸Šè®­ç»ƒçš„æ¨¡å‹é€šè¿‡ä»…ä½¿ç”¨â€œä¸Šä¸‹æ–‡å®ä¾‹â€ä½œä¸ºæç¤ºæ¥ç›´æ¥æ¨å¹¿åˆ°æœªè§çš„æ–°ç±»åˆ«ï¼Œæ— éœ€è¿›è¡Œå‚æ•°è°ƒæ•´ã€‚VICPååŒLLMsï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å’Œè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å‘æŒ¥ä½œç”¨ï¼šLLMsé€šè¿‡ä»»åŠ¡ç‰¹å®šæç¤ºä»å°‘æ•°æ­£å‘&#x2F;è´Ÿå‘å¯¹ä¸­æå–è¯­ä¹‰èº«ä»½è§„åˆ™ï¼Œç„¶åæŒ‡å¯¼VFMï¼ˆä¾‹å¦‚DINOï¼‰é€šè¿‡â€œåŠ¨æ€è§†è§‰æç¤ºâ€æå–èº«ä»½åˆ¤åˆ«ç‰¹å¾ã€‚é€šè¿‡å°†LLMè¡ç”Ÿçš„è¯­ä¹‰æ¦‚å¿µä¸VFMçš„é¢„è®­ç»ƒå…ˆéªŒçŸ¥è¯†å¯¹é½ï¼ŒVICPèƒ½å¤Ÿå®ç°å¯¹æ–°ç±»åˆ«çš„é€šç”¨åŒ–ï¼Œæ¶ˆé™¤äº†å¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡Œå†è®­ç»ƒçš„éœ€æ±‚ã€‚ä¸ºäº†æ”¯æŒè¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ShopID10Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªç”µå­å•†åŠ¡å¹³å°çš„1ä¸‡å¤šä¸ªå¯¹è±¡å®ä¾‹ï¼Œå…·æœ‰å¤šè§†è§’å›¾åƒå’Œè·¨åŸŸæµ‹è¯•åŠŸèƒ½ã€‚åœ¨ShopID10Kå’Œå„ç§ReIDåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVICPåœ¨æœªè§ç±»åˆ«ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hzzone/VICP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Hzzone/VICPä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21222v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong><br>åŸºäºè§†è§‰ä¸Šä¸‹æ–‡æç¤ºï¼ˆVICPï¼‰çš„æ–°æ¡†æ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœªè§è¿‡çš„ç±»åˆ«ä¸Šè¿›è¡Œç›´æ¥æ¨å¹¿ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„ååŒä½œç”¨ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„æç¤ºæ¥æ¨æ–­è¯­ä¹‰èº«ä»½è§„åˆ™ï¼Œå¹¶æŒ‡å¯¼VFMé€šè¿‡åŠ¨æ€è§†è§‰æç¤ºæå–èº«ä»½åˆ¤åˆ«ç‰¹å¾ã€‚é€šè¿‡è¯­ä¹‰æ¦‚å¿µå’ŒVFMé¢„è®­ç»ƒå…ˆéªŒçš„å¯¹é½ï¼ŒVICPå®ç°äº†å¯¹æ–°ç±»åˆ«çš„æ¨å¹¿ï¼Œæ— éœ€é’ˆå¯¹æ•°æ®é›†è¿›è¡Œç‰¹å®šå†è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¯¹è±¡å†è¯†åˆ«ï¼ˆReIDï¼‰æ–¹æ³•ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦æ˜‚è´µçš„æ–°ç±»åˆ«æ ‡æ³¨æ•°æ®ã€‚</li>
<li>è‡ªæˆ‘ç›‘ç£å­¦ä¹ å¯ä»¥å‡å°‘å¯¹æ³¨é‡Šçš„éœ€æ±‚ï¼Œä½†éš¾ä»¥æ•è·å¯¹äºReIDè‡³å…³é‡è¦çš„èº«ä»½æ•æ„Ÿç‰¹å¾ã€‚</li>
<li>æå‡ºçš„VICPæ¡†æ¶å…è®¸æ¨¡å‹åœ¨æœªè§è¿‡çš„ç±»åˆ«ä¸Šè¿›è¡Œç›´æ¥æ¨å¹¿ï¼Œä»…ä½¿ç”¨ä¸Šä¸‹æ–‡å®ä¾‹ä½œä¸ºæç¤ºã€‚</li>
<li>VICPç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„ååŒä½œç”¨ã€‚</li>
<li>LLMsé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„æç¤ºæ¥æ¨æ–­è¯­ä¹‰èº«ä»½è§„åˆ™ï¼ŒæŒ‡å¯¼VFMæå–èº«ä»½åˆ¤åˆ«ç‰¹å¾ã€‚</li>
<li>VICPé€šè¿‡å¯¹é½LLMè¯­ä¹‰æ¦‚å¿µå’ŒVFMé¢„è®­ç»ƒå…ˆéªŒï¼Œå®ç°æ–°ç±»åˆ«çš„æ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b737955ede91f1f6f7a46039a7b25b24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd0ccf4672ddc19ebffbb1d36a33ca8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5437fb6ab722b1a20f2df48b7da2f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e9bedb2cda94cabab963dc1fb4ffd049.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Disentangling-Latent-Embeddings-with-Sparse-Linear-Concept-Subspaces-SLiCS"><a href="#Disentangling-Latent-Embeddings-with-Sparse-Linear-Concept-Subspaces-SLiCS" class="headerlink" title="Disentangling Latent Embeddings with Sparse Linear Concept Subspaces   (SLiCS)"></a>Disentangling Latent Embeddings with Sparse Linear Concept Subspaces   (SLiCS)</h2><p><strong>Authors:Zhi Li, Hau Phan, Matthew Emigh, Austin J. Brockmeier</strong></p>
<p>Vision-language co-embedding networks, such as CLIP, provide a latent embedding space with semantic information that is useful for downstream tasks. We hypothesize that the embedding space can be disentangled to separate the information on the content of complex scenes by decomposing the embedding into multiple concept-specific component vectors that lie in different subspaces. We propose a supervised dictionary learning approach to estimate a linear synthesis model consisting of sparse, non-negative combinations of groups of vectors in the dictionary (atoms), whose group-wise activity matches the multi-label information. Each concept-specific component is a non-negative combination of atoms associated to a label. The group-structured dictionary is optimized through a novel alternating optimization with guaranteed convergence. Exploiting the text co-embeddings, we detail how semantically meaningful descriptions can be found based on text embeddings of words best approximated by a conceptâ€™s group of atoms, and unsupervised dictionary learning can exploit zero-shot classification of training set images using the text embeddings of concept labels to provide instance-wise multi-labels. We show that the disentangled embeddings provided by our sparse linear concept subspaces (SLiCS) enable concept-filtered image retrieval (and conditional generation using image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed autoencoder embeddings from TiTok and the latent embedding from self-supervised DINOv2. Quantitative and qualitative results highlight the improved precision of the concept-filtered image retrieval for all embeddings. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€ååŒåµŒå…¥ç½‘ç»œï¼ˆå¦‚CLIPï¼‰æä¾›äº†ä¸€ä¸ªåŒ…å«è¯­ä¹‰ä¿¡æ¯çš„æ½œåœ¨åµŒå…¥ç©ºé—´ï¼Œå¯¹äºä¸‹æ¸¸ä»»åŠ¡éå¸¸æœ‰ç”¨ã€‚æˆ‘ä»¬å‡è®¾åµŒå…¥ç©ºé—´å¯ä»¥è¢«åˆ†è§£ï¼Œä»¥åˆ†ç¦»å¤æ‚åœºæ™¯å†…å®¹çš„ä¿¡æ¯ï¼Œé€šè¿‡å°†åµŒå…¥åˆ†è§£æˆä½äºä¸åŒå­ç©ºé—´çš„å¤šä¸ªç‰¹å®šæ¦‚å¿µç»„ä»¶å‘é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰ç›‘ç£çš„å­—å…¸å­¦ä¹ æ–¹æ³•ï¼Œä»¥ä¼°è®¡ç”±å­—å…¸ï¼ˆåŸå­ï¼‰ä¸­çš„å‘é‡ç»„ç¨€ç–éè´Ÿç»„åˆæ„æˆçš„çº¿æ€§åˆæˆæ¨¡å‹ï¼Œå…¶åˆ†ç»„æ´»åŠ¨ä¸å¤šæ ‡ç­¾ä¿¡æ¯ç›¸åŒ¹é…ã€‚æ¯ä¸ªç‰¹å®šæ¦‚å¿µç»„ä»¶æ˜¯ä¸æ ‡ç­¾ç›¸å…³çš„åŸå­çš„éè´Ÿç»„åˆã€‚é€šè¿‡ä¸€ç§æ–°å‹äº¤æ›¿ä¼˜åŒ–æ–¹æ³•ä¼˜åŒ–ç»“æ„åŒ–å­—å…¸ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¿è¯çš„æ”¶æ•›æ€§ã€‚æˆ‘ä»¬è¯¦ç»†è¯´æ˜äº†å¦‚ä½•åˆ©ç”¨æ–‡æœ¬ååŒåµŒå…¥ï¼Œæ ¹æ®ä¸æ¦‚å¿µåŸå­ç¾¤ä½“æœ€åŒ¹é…çš„æ–‡æœ¬åµŒå…¥è¯æ±‡æ¥æ‰¾åˆ°è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æè¿°ï¼Œå¹¶ä¸”æ— ç›‘ç£çš„å­—å…¸å­¦ä¹ å¯ä»¥åˆ©ç”¨æ¦‚å¿µæ ‡ç­¾çš„æ–‡æœ¬åµŒå…¥æ¥å¯¹è®­ç»ƒé›†å›¾åƒè¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»ï¼Œä»¥æä¾›å®ä¾‹çº§çš„å¤šæ ‡ç­¾ã€‚æˆ‘ä»¬è¯æ˜ï¼Œç”±æˆ‘ä»¬çš„ç¨€ç–çº¿æ€§æ¦‚å¿µå­ç©ºé—´ï¼ˆSLiCSï¼‰æä¾›çš„è§£è€¦åµŒå…¥ï¼Œèƒ½å¤Ÿå®ç°æ›´ç²¾ç¡®çš„æ¦‚å¿µè¿‡æ»¤å›¾åƒæ£€ç´¢ï¼ˆä»¥åŠä½¿ç”¨å›¾åƒåˆ°æç¤ºçš„æ¡ä»¶ç”Ÿæˆï¼‰ã€‚æˆ‘ä»¬è¿˜å°†SLiCSåº”ç”¨äºTiTokçš„é«˜åº¦å‹ç¼©è‡ªç¼–ç å™¨åµŒå…¥å’Œè‡ªç›‘ç£DINOv2çš„æ½œåœ¨åµŒå…¥ã€‚å®šé‡å’Œå®šæ€§ç»“æœéƒ½çªå‡ºäº†æ¦‚å¿µè¿‡æ»¤å›¾åƒæ£€ç´¢çš„æ”¹è¿›ç²¾åº¦ï¼Œé€‚ç”¨äºæ‰€æœ‰åµŒå…¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20322v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€è”åˆåµŒå…¥ç½‘ç»œï¼ˆå¦‚CLIPï¼‰çš„æ½œåœ¨åµŒå…¥ç©ºé—´ï¼Œå¹¶æå‡ºä¸€ç§ç›‘ç£å­—å…¸å­¦ä¹ çš„æ–¹æ³•ï¼Œå°†åµŒå…¥ç©ºé—´åˆ†è§£ä¸ºå¤šä¸ªæ¦‚å¿µç‰¹å®šçš„ç»„ä»¶å‘é‡ã€‚é€šè¿‡ä¼˜åŒ–åˆ†ç»„ç»“æ„çš„å­—å…¸ï¼Œå®ç°æ¦‚å¿µè¿‡æ»¤çš„å›¾åƒæ£€ç´¢å’Œæ¡ä»¶ç”Ÿæˆã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œç¨€ç–çº¿æ€§æ¦‚å¿µå­ç©ºé—´ï¼ˆSLiCSï¼‰æä¾›çš„è§£çº ç¼ åµŒå…¥å¯æé«˜å›¾åƒæ£€ç´¢çš„ç²¾ç¡®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€è”åˆåµŒå…¥ç½‘ç»œï¼ˆå¦‚CLIPï¼‰æä¾›åŒ…å«è¯­ä¹‰ä¿¡æ¯çš„æ½œåœ¨åµŒå…¥ç©ºé—´ï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨ã€‚</li>
<li>åµŒå…¥ç©ºé—´å¯ä»¥è¢«åˆ†è§£ä¸ºå¤šä¸ªæ¦‚å¿µç‰¹å®šçš„ç»„ä»¶å‘é‡ï¼Œè¿™äº›å‘é‡ä½äºä¸åŒçš„å­ç©ºé—´ä¸­ã€‚</li>
<li>æå‡ºä¸€ç§ç›‘ç£å­—å…¸å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–åˆ†ç»„ç»“æ„çš„å­—å…¸æ¥å®ç°æ¦‚å¿µè¿‡æ»¤çš„å›¾åƒæ£€ç´¢å’Œæ¡ä»¶ç”Ÿæˆã€‚</li>
<li>ç¨€ç–çº¿æ€§æ¦‚å¿µå­ç©ºé—´ï¼ˆSLiCSï¼‰èƒ½æé«˜å›¾åƒæ£€ç´¢çš„ç²¾ç¡®åº¦ã€‚</li>
<li>åˆ©ç”¨æ–‡æœ¬è”åˆåµŒå…¥ï¼Œå¯ä»¥æ‰¾åˆ°ä¸æ¦‚å¿µæ ‡ç­¾æœ€åŒ¹é…çš„è¯­ä¹‰æè¿°ã€‚</li>
<li>æ— ç›‘ç£å­—å…¸å­¦ä¹ å¯ä»¥åˆ©ç”¨é›¶æ ·æœ¬åˆ†ç±»æ³•ï¼Œä½¿ç”¨æ–‡æœ¬æ¦‚å¿µæ ‡ç­¾å¯¹è®­ç»ƒé›†å›¾åƒè¿›è¡Œå®ä¾‹çº§å¤šæ ‡ç­¾åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce118eb3b856351221287f5f43575289.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0fc779aa46c1638f741af0020c22fe7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c8d54daccc33b2e422690bad0ccb3b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fc48c9562f428d74a2e295ba51f19bc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OpenM3D-Open-Vocabulary-Multi-view-Indoor-3D-Object-Detection-without-Human-Annotations"><a href="#OpenM3D-Open-Vocabulary-Multi-view-Indoor-3D-Object-Detection-without-Human-Annotations" class="headerlink" title="OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without   Human Annotations"></a>OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without   Human Annotations</h2><p><strong>Authors:Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo</strong></p>
<p>Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡ï¼ˆOVï¼‰3Då¯¹è±¡æ£€æµ‹æ˜¯ä¸€ä¸ªæ–°å…´é¢†åŸŸï¼Œç„¶è€Œä¸åŸºäº3Dç‚¹äº‘çš„æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºå›¾åƒçš„æ–¹æ³•å¯¹å…¶æ¢ç´¢ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬å¼•å…¥äº†OpenM3Dï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼€æ”¾è¯æ±‡çš„å¤šè§†è§’å®¤å†…3Då¯¹è±¡æ£€æµ‹å™¨ï¼Œæ— éœ€äººå·¥æ³¨é‡Šå³å¯è¿›è¡Œè®­ç»ƒã€‚ç‰¹åˆ«æ˜¯ï¼ŒOpenM3Dæ˜¯ä¸€ä¸ªå•é˜¶æ®µæ£€æµ‹å™¨ï¼Œé€‚åº”äºä»ImGeoNetæ¨¡å‹è¯±å¯¼çš„2Dä½“ç´ ç‰¹å¾ã€‚ä¸ºäº†æ”¯æŒå¼€æ”¾è¯æ±‡ï¼Œå®ƒä¸ç±»æ— å…³çš„3Då®šä½æŸå¤±å…±åŒè®­ç»ƒï¼Œéœ€è¦é«˜è´¨é‡3Dä¼ªæ¡†å’Œä½“ç´ è¯­ä¹‰å¯¹é½æŸå¤±ï¼Œè¿™éœ€è¦å¤šæ ·çš„é¢„è®­ç»ƒCLIPç‰¹å¾ã€‚æˆ‘ä»¬éµå¾ªOV-3DETçš„è®­ç»ƒè®¾ç½®ï¼Œæä¾›æœ‰å§¿æ€çš„RGB-Då›¾åƒï¼Œä½†ä¸æä¾›3Dæ¡†æˆ–ç±»çš„æ‰‹å·¥æ³¨é‡Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å›¾åµŒå…¥æŠ€æœ¯çš„3Dä¼ªæ¡†ç”Ÿæˆæ–¹æ³•ï¼Œå°†2Dç‰‡æ®µç»„åˆæˆè¿è´¯çš„3Dç»“æ„ã€‚æˆ‘ä»¬çš„ä¼ªæ¡†åœ¨ç²¾åº¦å’Œå¬å›ç‡æ–¹é¢è¶…è¿‡äº†å…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬OV-3DETä¸­æå‡ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä»ä¸æ¯ä¸ªè¿è´¯çš„3Dç»“æ„ç›¸å…³çš„2Dç‰‡æ®µä¸­é‡‡æ ·ä¸åŒçš„CLIPç‰¹å¾ï¼Œä»¥ä¸ç›¸åº”çš„ä½“ç´ ç‰¹å¾å¯¹é½ã€‚è®­ç»ƒé«˜åº¦å‡†ç¡®çš„å•é˜¶æ®µæ£€æµ‹å™¨çš„å…³é”®åœ¨äºï¼Œä¸¤ç§æŸå¤±éƒ½éœ€è¦æœç€é«˜è´¨é‡ç›®æ ‡å­¦ä¹ ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒOpenM3Dæ˜¯ä¸€ç§é«˜æ•ˆçš„æ£€æµ‹å™¨ï¼Œä»…éœ€è¦å¤šè§†è§’å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨ScanNet200å’ŒARKitSceneså®¤å†…åŸºå‡†æµ‹è¯•ä¸Šæ˜¾ç¤ºå‡ºå“è¶Šçš„æ€§èƒ½å’Œé€Ÿåº¦ï¼ˆæ¯ç§’0.3ç§’ï¼‰ï¼Œè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨æˆ‘ä»¬çš„ç±»æ— å…³æ£€æµ‹å™¨ä¸åŸºäºViT CLIPçš„å¼€æ”¾è¯æ±‡åˆ†ç±»å™¨ä»¥åŠç»“åˆå¤šè§†è§’æ·±åº¦ä¼°è®¡å™¨çš„åŸºå‡†çº¿ï¼Œåœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¸Šéƒ½è¶…è¶Šäº†ä¸€ç§å¼ºå¤§çš„ä¸¤é˜¶æ®µæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20063v1">PDF</a> ICCV2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼€æ”¾è¯æ±‡ï¼ˆOVï¼‰3Dç›®æ ‡æ£€æµ‹æ˜¯ä¸€ä¸ªæ–°å…´é¢†åŸŸï¼ŒåŸºäºå›¾åƒçš„æ–¹æ³•ç›¸è¾ƒäºåŸºäº3Dç‚¹äº‘çš„æ–¹æ³•ï¼Œå…¶æ¢ç´¢ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬ä»‹ç»äº†OpenM3Dï¼Œä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨çš„æ–°å¼€æ”¾è¯æ±‡å¤šè§†è§’å®¤å†…3Dç›®æ ‡æ£€æµ‹å™¨ã€‚ç‰¹åˆ«æ˜¯ï¼ŒOpenM3Dæ˜¯ä¸€ç§å•é˜¶æ®µæ£€æµ‹å™¨ï¼Œé€‚åº”äºä»ImGeoNetæ¨¡å‹è¯±å¯¼çš„2Dç«‹ä½“ç‰¹å¾ã€‚ä¸ºäº†æ”¯æŒå¼€æ”¾è¯æ±‡ï¼Œå®ƒä¸ç±»æ— å…³çš„3Då®šä½æŸå¤±ä¸€èµ·è¿›è¡Œè®­ç»ƒï¼Œéœ€è¦é«˜è´¨é‡çš„3Dä¼ªæ¡†å’Œç«‹ä½“è¯­ä¹‰å¯¹é½æŸå¤±ä»¥åŠå¤šæ ·åŒ–çš„é¢„è®­ç»ƒCLIPç‰¹å¾ã€‚æˆ‘ä»¬éµå¾ªOV-3DETçš„è®­ç»ƒè®¾ç½®ï¼Œæä¾›å§¿æ€RGB-Då›¾åƒï¼Œä½†æ— éœ€3Dæ¡†æˆ–ç±»çš„æ‰‹å·¥æ ‡æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å›¾åµŒå…¥æŠ€æœ¯çš„3Dä¼ªæ¡†ç”Ÿæˆæ–¹æ³•ï¼Œå°†2Dç‰‡æ®µç»„åˆæˆè¿è´¯çš„3Dç»“æ„ã€‚æˆ‘ä»¬çš„ä¼ªæ¡†å®ç°äº†æ¯”å…¶å®ƒæ–¹æ³•æ›´é«˜çš„ç²¾åº¦å’Œå¬å›ç‡ï¼ŒåŒ…æ‹¬OV-3DETä¸­æå‡ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä»ä¸æ¯ä¸ªè¿è´¯çš„3Dç»“æ„å…³è”çš„2Dç‰‡æ®µä¸­é‡‡æ ·å¤šæ ·åŒ–çš„CLIPç‰¹å¾ï¼Œä»¥ä¸ç›¸åº”çš„ç«‹ä½“ç‰¹å¾å¯¹é½ã€‚è®­ç»ƒé«˜åº¦ç²¾ç¡®çš„å•é˜¶æ®µæ£€æµ‹å™¨çš„å…³é”®æ˜¯éœ€è¦ä½¿ä¸¤ç§æŸå¤±æœå‘é«˜è´¨é‡ç›®æ ‡å­¦ä¹ ã€‚åœ¨æ¨æ–­æ—¶ï¼ŒOpenM3Dæ˜¯ä¸€ç§é«˜æ•ˆçš„æ£€æµ‹å™¨ï¼Œä»…éœ€è¦å¤šè§†è§’å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨ScanNet200å’ŒARKitSceneså®¤å†…åŸºå‡†æµ‹è¯•ä¸Šå±•ç°å‡ºå“è¶Šçš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ï¼ˆæ¯ç§’0.3ç§’ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç±»æ— å…³æ£€æµ‹å™¨ä¸Šç»“åˆViT CLIPçš„OVåˆ†ç±»å™¨ä»¥åŠèåˆå¤šè§†è§’æ·±åº¦ä¼°è®¡å™¨çš„åŸºç¡€çº¿æ–¹æ³•ï¼Œåœ¨å‡†ç¡®æ€§ä¸é€Ÿåº¦ä¸Šå‡è¡¨ç°å‡ºè¶…è¶Šå¼ºä¸¤é˜¶æ®µæ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡ï¼ˆOVï¼‰3Dç›®æ ‡æ£€æµ‹æ˜¯ä¸€ä¸ªæ–°å…´é¢†åŸŸï¼ŒåŸºäºå›¾åƒçš„æ–¹æ³•ç›¸è¾ƒäºåŸºäºç‚¹äº‘çš„æ–¹æ³•æ¢ç´¢å—é™ã€‚</li>
<li>å¼•å…¥OpenM3Dï¼Œä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨çš„å¼€æ”¾è¯æ±‡å¤šè§†è§’å®¤å†…3Dç›®æ ‡æ£€æµ‹å™¨ã€‚</li>
<li>OpenM3Dæ˜¯å•é˜¶æ®µæ£€æµ‹å™¨ï¼Œé‡‡ç”¨ImGeoNetæ¨¡å‹çš„2Dè¯±å¯¼ç«‹ä½“ç‰¹å¾ï¼Œå¹¶è”åˆè®­ç»ƒä»¥æ”¯æŒå¼€æ”¾è¯æ±‡ã€‚</li>
<li>æå‡ºä¸€ç§åˆ©ç”¨å›¾åµŒå…¥æŠ€æœ¯ç”Ÿæˆé«˜è´¨é‡3Dä¼ªæ¡†çš„æ–¹æ³•ï¼Œæé«˜äº†ç²¾åº¦å’Œå¬å›ç‡ã€‚</li>
<li>åˆ©ç”¨å¤šæ ·åŒ–çš„é¢„è®­ç»ƒCLIPç‰¹å¾è¿›è¡Œè®­ç»ƒä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨æ¨æ–­æ—¶ï¼ŒOpenM3Dä»…éœ€è¦å¤šè§†è§’å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå±•ç°å‡ºå“è¶Šçš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2f34544b513221d73b41c0822ad2ab95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3321507207d7beac680f3c8953b9836.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-332864918e26e244a60f12897ad0276c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multimodal-Prototype-Alignment-for-Semi-supervised-Pathology-Image-Segmentation"><a href="#Multimodal-Prototype-Alignment-for-Semi-supervised-Pathology-Image-Segmentation" class="headerlink" title="Multimodal Prototype Alignment for Semi-supervised Pathology Image   Segmentation"></a>Multimodal Prototype Alignment for Semi-supervised Pathology Image   Segmentation</h2><p><strong>Authors:Mingxi Fu, Fanglei Fu, Xitong Ling, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu</strong></p>
<p>Pathological image segmentation faces numerous challenges, particularly due to ambiguous semantic boundaries and the high cost of pixel-level annotations. Although recent semi-supervised methods based on consistency regularization (e.g., UniMatch) have made notable progress, they mainly rely on perturbation-based consistency within the image modality, making it difficult to capture high-level semantic priors, especially in structurally complex pathology images. To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. The core innovation of MPAMatch lies in the dual contrastive learning scheme between image prototypes and pixel labels, and between text prototypes and pixel labels, providing supervision at both structural and semantic levels. This coarse-to-fine supervisory strategy not only enhances the discriminative capability on unlabeled samples but also introduces the text prototype supervision into segmentation for the first time, significantly improving semantic boundary modeling. In addition, we reconstruct the classic segmentation architecture (TransUNet) by replacing its ViT backbone with a pathology-pretrained foundation model (Uni), enabling more effective extraction of pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatchâ€™s superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling. </p>
<blockquote>
<p>ç—…ç†å›¾åƒåˆ†å‰²é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å› ä¸ºè¯­ä¹‰è¾¹ç•Œæ¨¡ç³Šå’Œåƒç´ çº§æ ‡æ³¨çš„æˆæœ¬é«˜æ˜‚ã€‚å°½ç®¡æœ€è¿‘åŸºäºä¸€è‡´æ€§æ­£åˆ™åŒ–çš„åŠç›‘ç£æ–¹æ³•ï¼ˆä¾‹å¦‚UniMatchï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬ä¸»è¦ä¾èµ–äºå›¾åƒæ¨¡æ€å†…çš„åŸºäºæ‰°åŠ¨çš„ä¸€è‡´æ€§ï¼Œå› æ­¤åœ¨æ•è·é«˜çº§è¯­ä¹‰å…ˆéªŒæ—¶é¢ä¸´å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ç»“æ„å¤æ‚çš„ç—…ç†å›¾åƒä¸­ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MPAMatchâ€”â€”ä¸€ç§åœ¨å¤šåª’ä½“åŸå‹å¼•å¯¼ç›‘ç£èŒƒå¼ä¸‹è¿›è¡Œåƒç´ çº§å¯¹æ¯”å­¦ä¹ çš„æ–°å‹åˆ†å‰²æ¡†æ¶ã€‚MPAMatchçš„æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºå›¾åƒåŸå‹ä¸åƒç´ æ ‡ç­¾ä¹‹é—´ä»¥åŠæ–‡æœ¬åŸå‹ä¸åƒç´ æ ‡ç­¾ä¹‹é—´çš„åŒé‡å¯¹æ¯”å­¦ä¹ æ–¹æ¡ˆï¼Œåœ¨ç»“æ„å’Œè¯­ä¹‰çº§åˆ«ä¸Šæä¾›ç›‘ç£ã€‚è¿™ç§ç”±ç²—åˆ°ç»†çš„ç›‘ç£ç­–ç•¥ä¸ä»…æé«˜äº†å¯¹æœªæ ‡è®°æ ·æœ¬çš„è¾¨åˆ«èƒ½åŠ›ï¼Œè€Œä¸”é¦–æ¬¡å°†æ–‡æœ¬åŸå‹ç›‘ç£å¼•å…¥åˆ†å‰²ï¼Œä»è€Œæ˜¾ç€æ”¹è¿›äº†è¯­ä¹‰è¾¹ç•Œå»ºæ¨¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç”¨ç—…ç†é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆUniï¼‰æ›¿æ¢ç»å…¸åˆ†å‰²æ¶æ„ï¼ˆTransUNetï¼‰çš„ViTéª¨å¹²ç½‘ï¼Œä»è€Œé‡å»ºäº†æ›´é«˜æ•ˆçš„ç—…ç†ç›¸å…³ç‰¹å¾æå–ã€‚åœ¨GLASã€EBHI-SEG-GLANDã€EBHI-SEG-CANCERå’ŒKPIä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMPAMatchåœ¨ç»“æ„å’Œè¯­ä¹‰å»ºæ¨¡æ–¹é¢å…·æœ‰åŒé‡ä¼˜åŠ¿ï¼Œä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19574v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºä¸€è‡´æ€§æ­£åˆ™åŒ–çš„åŠç›‘ç£æ–¹æ³•é¢ä¸´è¯­ä¹‰è¾¹ç•Œæ¨¡ç³Šå’Œå¤æ‚ç»“æ„å›¾åƒä¸­çš„é«˜æˆæœ¬æ ‡æ³¨é—®é¢˜ã€‚æå‡ºMPAMatchæ¡†æ¶ï¼Œé‡‡ç”¨åƒç´ çº§å¯¹æ¯”å­¦ä¹ ï¼Œåœ¨å¤šæ¨¡æ€åŸå‹å¼•å¯¼çš„ç›‘ç£ä¸‹è¿›è¡Œåˆ†å‰²ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºå›¾åƒåŸå‹ä¸åƒç´ æ ‡ç­¾ä¹‹é—´ä»¥åŠæ–‡æœ¬åŸå‹ä¸åƒç´ æ ‡ç­¾ä¹‹é—´çš„åŒé‡å¯¹æ¯”å­¦ä¹ æ–¹æ¡ˆï¼Œå®ç°äº†ç»“æ„çº§å’Œè¯­ä¹‰çº§çš„ç›‘ç£ã€‚é‡æ„ç»å…¸åˆ†å‰²æ¶æ„TransUNetï¼Œé‡‡ç”¨ç—…ç†é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹Uniæ›¿æ¢ViTä¸»å¹²ï¼Œæ›´æœ‰æ•ˆæå–ç—…ç†ç›¸å…³ç‰¹å¾ã€‚åœ¨GLASã€EBHI-SEG-GLANDç­‰æ•°æ®é›†ä¸ŠéªŒè¯MPAMatchçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MPAMatchè§£å†³äº†åŸºäºä¸€è‡´æ€§æ­£åˆ™åŒ–çš„åŠç›‘ç£æ–¹æ³•åœ¨ç—…ç†å›¾åƒåˆ†å‰²ä¸­é¢ä¸´çš„è¯­ä¹‰è¾¹ç•Œæ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>MPAMatché‡‡ç”¨å¤šæ¨¡æ€åŸå‹å¼•å¯¼çš„ç›‘ç£æ–¹å¼ï¼Œå®ç°äº†åƒç´ çº§çš„å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>åŒé‡å¯¹æ¯”å­¦ä¹ æ–¹æ¡ˆç»“åˆäº†å›¾åƒåŸå‹ä¸åƒç´ æ ‡ç­¾ã€æ–‡æœ¬åŸå‹ä¸åƒç´ æ ‡ç­¾çš„ç›‘ç£ï¼Œæå‡äº†æœªæ ‡æ³¨æ ·æœ¬çš„åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>MPAMatché¦–æ¬¡å°†æ–‡æœ¬åŸå‹ç›‘ç£å¼•å…¥åˆ†å‰²ä»»åŠ¡ï¼Œæ”¹è¿›äº†è¯­ä¹‰è¾¹ç•Œå»ºæ¨¡ã€‚</li>
<li>MPAMatché‡æ„äº†TransUNetæ¶æ„ï¼Œä½¿ç”¨ç—…ç†é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹Uniæ›¿æ¢ViTä¸»å¹²ï¼Œå¢å¼ºäº†ç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>MPAMatchåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒéªŒè¯äº†å…¶åœ¨ç»“æ„å’Œè¯­ä¹‰å»ºæ¨¡ä¸Šçš„åŒé‡ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57582b15f91667c387164c26a24ea837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e516b2b5d77134af10b42a8246c3e40d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b8dd2d8536092cc2b310cb0782bcc26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5d6ea3dbf0bdcfa8156e48859941abe.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Toward-Robust-Medical-Fairness-Debiased-Dual-Modal-Alignment-via-Text-Guided-Attribute-Disentangled-Prompt-Learning-for-Vision-Language-Models"><a href="#Toward-Robust-Medical-Fairness-Debiased-Dual-Modal-Alignment-via-Text-Guided-Attribute-Disentangled-Prompt-Learning-for-Vision-Language-Models" class="headerlink" title="Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via   Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models"></a>Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via   Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models</h2><p><strong>Authors:Yuexuan Xia, Benteng Ma, Jiang He, Zhiyong Wang, Qi Dou, Yong Xia</strong></p>
<p>Ensuring fairness across demographic groups in medical diagnosis is essential for equitable healthcare, particularly under distribution shifts caused by variations in imaging equipment and clinical practice. Vision-language models (VLMs) exhibit strong generalization, and text prompts encode identity attributes, enabling explicit identification and removal of sensitive directions. However, existing debiasing approaches typically address vision and text modalities independently, leaving residual cross-modal misalignment and fairness gaps. To address this challenge, we propose DualFairVL, a multimodal prompt-learning framework that jointly debiases and aligns cross-modal representations. DualFairVL employs a parallel dual-branch architecture that separates sensitive and target attributes, enabling disentangled yet aligned representations across modalities. Approximately orthogonal text anchors are constructed via linear projections, guiding cross-attention mechanisms to produce fused features. A hypernetwork further disentangles attribute-related information and generates instance-aware visual prompts, which encode dual-modal cues for fairness and robustness. Prototype-based regularization is applied in the visual branch to enforce separation of sensitive features and strengthen alignment with textual anchors. Extensive experiments on eight medical imaging datasets across four modalities show that DualFairVL achieves state-of-the-art fairness and accuracy under both in- and out-of-distribution settings, outperforming full fine-tuning and parameter-efficient baselines with only 3.6M trainable parameters. Code will be released upon publication. </p>
<blockquote>
<p>åœ¨åŒ»ç–—è¯Šæ–­ä¸­ï¼Œç¡®ä¿ä¸åŒäººç¾¤ä¹‹é—´çš„å…¬å¹³æ€§æ˜¯å…¬å¹³åŒ»ç–—çš„å…³é”®ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”±æˆåƒè®¾å¤‡å’Œä¸´åºŠå®è·µå˜åŒ–å¼•èµ·çš„åˆ†å¸ƒå˜åŒ–ä¸‹å°¤ä¸ºé‡è¦ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ–‡æœ¬æç¤ºèƒ½å¤Ÿç¼–ç èº«ä»½å±æ€§ï¼Œä»è€Œèƒ½å¤Ÿæ˜ç¡®è¯†åˆ«å’Œæ¶ˆé™¤æ•æ„Ÿæ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å»åæ–¹æ³•é€šå¸¸ç‹¬ç«‹åœ°å¤„ç†è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œå¯¼è‡´å‰©ä½™çš„è·¨æ¨¡æ€ä¸åŒ¹é…å’Œå…¬å¹³æ€§å·®è·ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DualFairVLï¼Œä¸€ä¸ªè”åˆå»åå’Œè·¨æ¨¡æ€å¯¹é½çš„å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¡†æ¶ã€‚DualFairVLé‡‡ç”¨å¹¶è¡ŒåŒåˆ†æ”¯æ¶æ„ï¼Œå°†æ•æ„Ÿå±æ€§å’Œç›®æ ‡å±æ€§åˆ†ç¦»ï¼Œå®ç°è·¨æ¨¡æ€çš„è§£è€¦ä¸”å¯¹é½çš„è¡¨ç¤ºã€‚é€šè¿‡çº¿æ€§æŠ•å½±æ„å»ºè¿‘ä¼¼æ­£äº¤çš„æ–‡æœ¬é”šç‚¹ï¼Œå¼•å¯¼è·¨æ³¨æ„åŠ›æœºåˆ¶ç”Ÿæˆèåˆç‰¹å¾ã€‚è¶…ç½‘ç»œè¿›ä¸€æ­¥è§£è€¦å±æ€§ç›¸å…³ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆå®ä¾‹æ„ŸçŸ¥çš„è§†è§‰æç¤ºï¼Œç¼–ç åŒæ¨¡æ€çº¿ç´¢ä»¥å®ç°å…¬å¹³æ€§å’Œç¨³å¥æ€§ã€‚åŸºäºåŸå‹çš„æ­£åˆ™åŒ–åº”ç”¨äºè§†è§‰åˆ†æ”¯ï¼Œä»¥å¼ºåˆ¶æ•æ„Ÿç‰¹å¾çš„åˆ†ç¦»å¹¶åŠ å¼ºæ–‡æœ¬é”šç‚¹çš„å¯¹é½ã€‚åœ¨å››ç§æ¨¡æ€çš„å…«ä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDualFairVLåœ¨å†…å¤–åˆ†å¸ƒè®¾ç½®ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„å…¬å¹³æ€§å’Œå‡†ç¡®æ€§ï¼Œä»…ä½¿ç”¨360ä¸‡å¯è®­ç»ƒå‚æ•°å°±è¶…è¿‡äº†å…¨å¾®è°ƒå‚æ•°é«˜æ•ˆçš„åŸºçº¿ã€‚ä»£ç å°†åœ¨å‘å¸ƒæ—¶å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18886v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†åŒ»ç–—è¯Šæ–­ä¸­çš„è·¨æ¨¡æ€åè§é—®é¢˜æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDualFairVLçš„å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè”åˆæ¶ˆé™¤åè§å’Œè·¨æ¨¡æ€å¯¹é½è¡¨ç¤ºã€‚DualFairVLé‡‡ç”¨å¹³è¡ŒåŒåˆ†æ”¯æ¶æ„åˆ†ç¦»æ•æ„Ÿå±æ€§å’Œç›®æ ‡å±æ€§ï¼Œç”Ÿæˆå®ä¾‹æ„ŸçŸ¥çš„è§†è§‰æç¤ºä»¥å¢å¼ºå…¬å¹³æ€§å’Œç¨³å¥æ€§ã€‚å®éªŒè¯æ˜å…¶åœ¨ä¸åŒæ¨¡æ€çš„å…«ä¸ªåŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å…¬å¹³æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¬å¹³åŒ»ç–—è¯Šæ–­çš„é‡è¦æ€§ï¼šå¼ºè°ƒåœ¨åŒ»ç–—è¯Šæ–­ä¸­ç¡®ä¿è·¨äººå£ç¾¤ä½“å…¬å¹³çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯å—åˆ°æˆåƒè®¾å¤‡å’Œä¸´åºŠå®è·µå˜åŒ–çš„å½±å“ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ï¼šè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è·¨æ¨¡æ€åè§é—®é¢˜ä¸Šå­˜åœ¨éš¾é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‹¬ç«‹å¤„ç†è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€æ—¶æ˜“å‡ºç°å‰©ä½™è·¨æ¨¡æ€ä¸å¯¹é½å’Œå…¬å¹³å·®è·ã€‚</li>
<li>DualFairVLæ¡†æ¶çš„æå‡ºï¼šä»‹ç»äº†ä¸€ç§åä¸ºDualFairVLçš„å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è”åˆæ¶ˆé™¤åè§å¹¶è·¨æ¨¡æ€å¯¹é½è¡¨ç¤ºã€‚</li>
<li>å¹³è¡ŒåŒåˆ†æ”¯æ¶æ„ï¼šDualFairVLé‡‡ç”¨å¹³è¡ŒåŒåˆ†æ”¯æ¶æ„ï¼Œèƒ½å¤Ÿåˆ†ç¦»æ•æ„Ÿå±æ€§å’Œç›®æ ‡å±æ€§ï¼Œå®ç°è·¨æ¨¡æ€çš„è§£è€¦è¡¨ç¤ºã€‚</li>
<li>æ–‡æœ¬é”šç‚¹çš„ä½œç”¨ï¼šé€šè¿‡çº¿æ€§æŠ•å½±æ„å»ºè¿‘ä¼¼æ­£äº¤çš„æ–‡æœ¬é”šç‚¹ï¼Œå¼•å¯¼è·¨æ³¨æ„åŠ›æœºåˆ¶äº§ç”Ÿèåˆç‰¹å¾ã€‚</li>
<li>å®ä¾‹æ„ŸçŸ¥çš„è§†è§‰æç¤ºï¼šä½¿ç”¨è¶…ç½‘ç»œç”Ÿæˆå®ä¾‹æ„ŸçŸ¥çš„è§†è§‰æç¤ºï¼Œè¯¥æç¤ºç»“åˆäº†åŒæ¨¡æ€çº¿ç´¢ä»¥æé«˜å…¬å¹³æ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76e328c84db82ca8046e724172331825.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef8f7b5eced416be467ac565c0ed67cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80b1785abc81bc5689b21ff982addbdb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LPLC-A-Dataset-for-License-Plate-Legibility-Classification"><a href="#LPLC-A-Dataset-for-License-Plate-Legibility-Classification" class="headerlink" title="LPLC: A Dataset for License Plate Legibility Classification"></a>LPLC: A Dataset for License Plate Legibility Classification</h2><p><strong>Authors:Lucas Wojcik, Gabriel E. Lima, Valfride Nascimento, Eduil Nascimento Jr., Rayson Laroca, David Menotti</strong></p>
<p>Automatic License Plate Recognition (ALPR) faces a major challenge when dealing with illegible license plates (LPs). While reconstruction methods such as super-resolution (SR) have emerged, the core issue of recognizing these low-quality LPs remains unresolved. To optimize model performance and computational efficiency, image pre-processing should be applied selectively to cases that require enhanced legibility. To support research in this area, we introduce a novel dataset comprising 10,210 images of vehicles with 12,687 annotated LPs for legibility classification (the LPLC dataset). The images span a wide range of vehicle types, lighting conditions, and camera&#x2F;image quality levels. We adopt a fine-grained annotation strategy that includes vehicle- and LP-level occlusions, four legibility categories (perfect, good, poor, and illegible), and character labels for three categories (excluding illegible LPs). As a benchmark, we propose a classification task using three image recognition networks to determine whether an LP image is good enough, requires super-resolution, or is completely unrecoverable. The overall F1 score, which remained below 80% for all three baseline models (ViT, ResNet, and YOLO), together with the analyses of SR and LP recognition methods, highlights the difficulty of the task and reinforces the need for further research. The proposed dataset is publicly available at <a target="_blank" rel="noopener" href="https://github.com/lmlwojcik/lplc-dataset">https://github.com/lmlwojcik/lplc-dataset</a>. </p>
<blockquote>
<p>è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿï¼ˆALPRï¼‰åœ¨å¤„ç†éš¾ä»¥è¾¨è®¤çš„è½¦ç‰Œï¼ˆLPsï¼‰æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶è¶…åˆ†è¾¨ç‡é‡å»ºæ–¹æ³•ï¼ˆSRï¼‰å·²ç»å‡ºç°ï¼Œä½†è¯†åˆ«è¿™äº›ä½è´¨é‡è½¦ç‰Œçš„æ ¸å¿ƒé—®é¢˜ä»æœªè§£å†³ã€‚ä¸ºäº†ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œåº”å¯¹éœ€è¦æé«˜æ¸…æ™°åº¦çš„æ¡ˆä¾‹é€‰æ‹©æ€§åº”ç”¨å›¾åƒé¢„å¤„ç†ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«è½¦è¾†å›¾åƒçš„æ–°æ•°æ®é›†ï¼Œå…¶ä¸­å«æœ‰è½¦è¾†å›¾åƒå…±è®¡æœ‰è½¦è¾†10,210å¼ ï¼Œæ ‡æ³¨è½¦ç‰ŒLPsæ•°é‡ä¸ºæœ‰æ•°å­—ä¿¡æ¯çš„LPæœ‰è¶³è¶³è¾¾åˆ°äº†å°†åŒ…å«è¿‘è¿‘æ–‡å­—ç»„åˆè€Œæœ‰æ•°å­—å’Œç¼–å·ä½†å¤–è§‚ç”±äºç‰©ç†æ±¡è¿¹æˆ–è¢«é£æ²™æ©åŸ‹æˆ–æ˜¯å¤–è§‚ä¸ä½³ä»¥è‡´è¤ªè‰²å®Œå…¨è¾¨è®¤åŸºæœ¬æ— æ³•è¾¨è®¤è½¦ç‰Œå·ç çš„è½¦è¾†å›¾åƒå…±è®¡æœ‰æ ‡æ³¨è½¦ç‰Œå…±è®¡æœ‰æ ‡æ³¨è½¦ç‰Œæ•°å­—ä¿¡æ¯çš„LPå…±è®¡æœ‰å…±è®¡æœ‰æ ‡æ³¨è½¦ç‰Œæ•°å­—ä¿¡æ¯çš„æœ‰é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é«˜è¾¾é€šè¿‡ä¸åŒçš„æ ‡è¯†æ¸…æ™°çš„å¯¹æ¯”å°±å¯ä»¥è¾¨è®¤å¹¶æ‰¾å‡ºå‘ç°éš¾åº¦ç»†å¾®ä¹‹åˆ†æ•°å­—ç»†å·®åˆ«æ— æ³•å®Œå…¨è¿›è¡Œè¾¨å¹¶ä¸”æ ‡è®°å…¶åˆ†åˆ«ç±»åˆ«å¯ä»¥å°†æ ¹æ®ä¸åŒç±»è¢«åˆ†ç±»ä¸ºå®Œç¾ã€è‰¯å¥½ã€æ¶åŠ£å’Œæ— æ³•è¾¨è®¤å››ä¸ªæ¸…æ™°åº¦ç±»åˆ«åˆ«åŒ…æ‹¬è½¦è¾†çº§åˆ«å’Œè½¦ç‰Œçº§åˆ«çš„é®æŒ¡ç±»åˆ«è¿˜åŒ…æ‹¬é™¤äº†æ— æ³•è¾¨è®¤çš„è½¦ç‰Œå¤–å…¶ä½™ä¸‰ç§ç±»åˆ«çš„å­—ç¬¦æ ‡ç­¾æœ€ç»ˆå»ºç«‹ä¸€ä¸ªæ¯”è¾ƒå…¨å®Œå¤‡çš„æ ·æœ¬é›†è¯¥æ ·æœ¬é›†å…±ç”¨æ¥å¯ä»¥è®­ç»ƒå’Œè¯„ä¼°ç›¸å…³çš„æ·±åº¦å­¦ä¹ ç®—æ³•ç½‘ç»œè¯¥æ ·æœ¬é›†å›Šæ‹¬äº†å„ç§å„æ ·çš„è½¦è¾†ç±»å‹å…‰ç…§æ¡ä»¶ä»¥åŠç›¸æœºæˆ–å›¾åƒè´¨é‡æ°´å¹³ä½œä¸ºä¸€ä¸ªåŸºå‡†æµ‹è¯•æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ä½¿ç”¨ä¸‰ä¸ªå›¾åƒè¯†åˆ«ç½‘ç»œæ¥ç¡®å®šè½¦ç‰Œå›¾åƒæ˜¯å¦è¶³å¤Ÿå¥½éœ€è¦è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†è¿˜æ˜¯å®Œå…¨æ— æ³•æ¢å¤æ€»ä½“è€Œè¨€F1å¾—åˆ†å¯¹æ‰€æœ‰ä¸‰ä¸ªåŸºå‡†æ¨¡å‹ï¼ˆViTã€ResNetå’ŒYOLOï¼‰éƒ½ä½äºç™¾åˆ†ä¹‹å…«åè¿™å‡¸æ˜¾äº†ä»»åŠ¡çš„éš¾åº¦ä»¥åŠå¯¹è¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§æ­¤å¤–è¿˜å¯¹SRå’Œè½¦ç‰Œè¯†åˆ«æ–¹æ³•è¿›è¡Œäº†åˆ†ææ‰€æå‡ºçš„æ•°æ®é›†å¯åœ¨å…¬å¼€æ¸ é“ä¸‹è½½è®¿é—®å…¶ç½‘å€ä¸º<a target="_blank" rel="noopener" href="https://github.com/lmlwojcik/lplc-dataset">https://github.com/lmlwojcik/lplc-dataset</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18425v1">PDF</a> Accepted for presentation at the Conference on Graphics, Patterns and   Images (SIBGRAPI) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è½¦ç‰Œè¯†åˆ«ï¼ˆALPRï¼‰ä¸­é‡åˆ°çš„éš¾ä»¥è¯†åˆ«çš„è½¦ç‰Œï¼ˆLPsï¼‰é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°å‹æ•°æ®é›†LPLCã€‚è¯¥æ•°æ®é›†åŒ…å«å¤šç§è½¦è¾†ç±»å‹ã€å…‰ç…§æ¡ä»¶å’Œæ‘„åƒå¤´&#x2F;å›¾åƒè´¨é‡æ°´å¹³çš„å›¾åƒï¼Œå¹¶å¯¹è½¦ç‰Œçš„å¯è¯»æ€§è¿›è¡Œåˆ†ç±»æ ‡æ³¨ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡åŸºå‡†çº¿ï¼Œä½¿ç”¨ä¸‰ç§å›¾åƒè¯†åˆ«ç½‘ç»œåˆ¤æ–­è½¦ç‰Œå›¾åƒæ˜¯å¦è¶³å¤Ÿå¥½ã€æ˜¯å¦éœ€è¦è¶…åˆ†è¾¨ç‡å¤„ç†æˆ–å®Œå…¨æ— æ³•æ¢å¤ã€‚ç„¶è€Œï¼Œæ‰€æœ‰åŸºå‡†æ¨¡å‹çš„F1å¾—åˆ†å‡ä½äº80%ï¼Œè¡¨æ˜ä»»åŠ¡éš¾åº¦å¤§ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚æ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ALPRåœ¨å¤„ç†æ¨¡ç³Šè½¦ç‰Œæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ•°æ®é›†LPLCï¼Œç”¨äºè½¦ç‰Œçš„å¯è¯»æ€§åˆ†ç±»ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤šç§è½¦è¾†ç±»å‹ã€å…‰ç…§æ¡ä»¶å’Œæ‘„åƒå¤´&#x2F;å›¾åƒè´¨é‡æ°´å¹³çš„å›¾åƒã€‚</li>
<li>é‡‡ç”¨ç²¾ç»†æ ‡æ³¨ç­–ç•¥ï¼ŒåŒ…æ‹¬è½¦è¾†å’Œè½¦ç‰Œçº§åˆ«çš„é®æŒ¡ä¿¡æ¯ä»¥åŠå››ä¸ªæ¸…æ™°åº¦ç±»åˆ«ï¼ˆå®Œç¾ã€è‰¯å¥½ã€è¾ƒå·®å’Œæ— æ³•è¯†åˆ«ï¼‰ã€‚</li>
<li>æå‡ºä¸€ä¸ªåˆ†ç±»ä»»åŠ¡åŸºå‡†çº¿ï¼Œä½¿ç”¨ä¸‰ç§å›¾åƒè¯†åˆ«ç½‘ç»œåˆ¤æ–­è½¦ç‰Œå›¾åƒçš„å¯è¯»æ€§ã€‚</li>
<li>æ‰€æœ‰åŸºå‡†æ¨¡å‹çš„F1å¾—åˆ†å‡ä½äº80%ï¼Œè¡¨æ˜ä»»åŠ¡éš¾åº¦å¤§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-753715737b37f001561f1eb1f63259fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec0393cff386dcf618e24f575fdbdb70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-792f9139163cef5af33c561e55a968bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ac0264929a127a0179a139b0a5fe620.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-274314aeb819744610584dd63281ea96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-271876844524c2d3bf2be9cb56c93377.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-491c905347b2b63477b95266640fb9ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-493980e950969fe2c704ec28ac68ebfa.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Explain-and-Monitor-Deep-Learning-Models-for-Computer-Vision-using-Obz-AI"><a href="#Explain-and-Monitor-Deep-Learning-Models-for-Computer-Vision-using-Obz-AI" class="headerlink" title="Explain and Monitor Deep Learning Models for Computer Vision using Obz   AI"></a>Explain and Monitor Deep Learning Models for Computer Vision using Obz   AI</h2><p><strong>Authors:Neo Christopher Chung, Jakub Binda</strong></p>
<p>Deep learning has transformed computer vision (CV), achieving outstanding performance in classification, segmentation, and related tasks. Such AI-based CV systems are becoming prevalent, with applications spanning from medical imaging to surveillance. State of the art models such as convolutional neural networks (CNNs) and vision transformers (ViTs) are often regarded as &#96;&#96;black boxes,â€™â€™ offering limited transparency into their decision-making processes. Despite a recent advancement in explainable AI (XAI), explainability remains underutilized in practical CV deployments. A primary obstacle is the absence of integrated software solutions that connect XAI techniques with robust knowledge management and monitoring frameworks. To close this gap, we have developed Obz AI, a comprehensive software ecosystem designed to facilitate state-of-the-art explainability and observability for vision AI systems. Obz AI provides a seamless integration pipeline, from a Python client library to a full-stack analytics dashboard. With Obz AI, a machine learning engineer can easily incorporate advanced XAI methodologies, extract and analyze features for outlier detection, and continuously monitor AI models in real time. By making the decision-making mechanisms of deep models interpretable, Obz AI promotes observability and responsible deployment of computer vision systems. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²ç»æ”¹å˜äº†è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰é¢†åŸŸï¼Œåœ¨åˆ†ç±»ã€åˆ†å‰²å’Œç›¸å…³ä»»åŠ¡ä¸­å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™ç§åŸºäºäººå·¥æ™ºèƒ½çš„è®¡ç®—æœºè§†è§‰ç³»ç»Ÿå˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œå…¶åº”ç”¨ä»åŒ»å­¦å½±åƒåˆ°ç›‘æ§æ— æ‰€ä¸åœ¨ã€‚æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ï¼Œé€šå¸¸è¢«è§†ä¸ºâ€œé»‘ç®±â€ï¼Œå¯¹å…¶å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦æœ‰é™ã€‚å°½ç®¡æœ€è¿‘è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æœ‰æ‰€å‘å±•ï¼Œä½†åœ¨å®é™…è®¡ç®—æœºè§†è§‰éƒ¨ç½²ä¸­ï¼Œå¯è§£é‡Šæ€§ä»ç„¶è¢«åˆ©ç”¨ä¸è¶³ã€‚ä¸»è¦éšœç¢åœ¨äºç¼ºä¹å°†XAIæŠ€æœ¯ä¸ç¨³å¥çš„çŸ¥è¯†ç®¡ç†å’Œç›‘æ§æ¡†æ¶è¿æ¥èµ·æ¥çš„ç»¼åˆè½¯ä»¶è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼€å‘äº†Obz AIï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿï¼Œæ—¨åœ¨ä¿ƒè¿›å…ˆè¿›çš„è§†è§‰äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå¯è§‚å¯Ÿæ€§ã€‚Obz AIæä¾›äº†ä¸€ä¸ªæ— ç¼é›†æˆç®¡é“ï¼Œä»Pythonå®¢æˆ·ç«¯åº“åˆ°å…¨æ ˆåˆ†æä»ªè¡¨æ¿ã€‚ä½¿ç”¨Obz AIï¼Œæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆå¯ä»¥è½»æ¾åœ°é‡‡ç”¨å…ˆè¿›çš„XAIæ–¹æ³•ï¼Œæå–å’Œåˆ†æç‰¹å¾ä»¥è¿›è¡Œå¼‚å¸¸æ£€æµ‹ï¼Œå¹¶å®æ—¶æŒç»­ç›‘æ§AIæ¨¡å‹ã€‚é€šè¿‡ä½¿æ·±åº¦æ¨¡å‹çš„å†³ç­–æœºåˆ¶å¯è§£é‡Šï¼ŒObz AIä¿ƒè¿›äº†è®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„å¯è§‚æ€§å’Œè´Ÿè´£ä»»çš„éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18188v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å“è¶Šè¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†ç±»ã€åˆ†å‰²å’Œç›¸å…³ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚å°½ç®¡å­˜åœ¨å¦‚å·ç§¯ç¥ç»ç½‘ç»œå’Œè§†è§‰å˜å‹å™¨ç­‰å…ˆè¿›æ¨¡å‹ï¼Œä½†äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å†³ç­–è¿‡ç¨‹ä»ç„¶ç¼ºä¹é€æ˜åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼€å‘äº†ä¸€ä¸ªåä¸ºObz AIçš„ç»¼åˆè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿï¼Œæ—¨åœ¨ä¿ƒè¿›è®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„å…ˆè¿›è§£é‡Šæ€§å’Œå¯è§‚æµ‹æ€§ã€‚Obz AIæä¾›äº†ä¸€ä¸ªæ— ç¼é›†æˆç®¡é“ï¼Œä»Pythonå®¢æˆ·ç«¯åº“åˆ°å…¨æ ˆåˆ†æä»ªè¡¨æ¿ï¼Œä½¿å¾—æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆå¯ä»¥è½»æ¾åœ°èå…¥å…ˆè¿›çš„XAIæ–¹æ³•ï¼Œå®æ—¶åˆ†æå’Œç›‘æ§AIæ¨¡å‹ã€‚é€šè¿‡è§£é‡Šæ·±åº¦æ¨¡å‹çš„å†³ç­–æœºåˆ¶ï¼ŒObz AIä¿ƒè¿›äº†è®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„å¯è§‚æµ‹æ€§å’Œè´Ÿè´£ä»»éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè¡¨ç°å“è¶Šï¼Œå°¤å…¶åœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„æ¨¡å‹å¦‚CNNå’ŒViTè¢«è§†ä¸ºâ€œé»‘ç®±â€ï¼Œå†³ç­–è¿‡ç¨‹ç¼ºä¹é€æ˜åº¦ã€‚</li>
<li>ç°æœ‰çš„è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æŠ€æœ¯åœ¨å®ç”¨éƒ¨ç½²ä¸­çš„åˆ©ç”¨ç‡ä¸é«˜ã€‚</li>
<li>Obz AIæ˜¯ä¸€ä¸ªç»¼åˆè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿï¼Œæ—¨åœ¨ä¿ƒè¿›è®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„å…ˆè¿›è§£é‡Šæ€§å’Œå¯è§‚æµ‹æ€§ã€‚</li>
<li>Obz AIæä¾›äº†ä¸€ä¸ªæ— ç¼é›†æˆç®¡é“ï¼ŒåŒ…æ‹¬Pythonå®¢æˆ·ç«¯åº“å’Œå…¨æ ˆåˆ†æä»ªè¡¨æ¿ã€‚</li>
<li>æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆå¯ä»¥æ–¹ä¾¿åœ°åˆ©ç”¨Obz AIèå…¥å…ˆè¿›çš„XAIæ–¹æ³•ï¼Œå®æ—¶åˆ†æå’Œç›‘æ§AIæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bfd429fa7019ea945b5ad2dd22a75195.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9254ce3fdc9243861678a3165dc519b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e83c2b2e8299e3a0e761bdc0f0035a7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7993e5a42ad065e5fac65dcf01454a75.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Edge-Enhanced-Vision-Transformer-Framework-for-Accurate-AI-Generated-Image-Detection"><a href="#Edge-Enhanced-Vision-Transformer-Framework-for-Accurate-AI-Generated-Image-Detection" class="headerlink" title="Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated   Image Detection"></a>Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated   Image Detection</h2><p><strong>Authors:Dabbrata Das, Mahshar Yahan, Md Tareq Zaman, Md Rishadul Bayesh</strong></p>
<p>The rapid advancement of generative models has led to a growing prevalence of highly realistic AI-generated images, posing significant challenges for digital forensics and content authentication. Conventional detection methods mainly rely on deep learning models that extract global features, which often overlook subtle structural inconsistencies and demand substantial computational resources. To address these limitations, we propose a hybrid detection framework that combines a fine-tuned Vision Transformer (ViT) with a novel edge-based image processing module. The edge-based module computes variance from edge-difference maps generated before and after smoothing, exploiting the observation that AI-generated images typically exhibit smoother textures, weaker edges, and reduced noise compared to real images. When applied as a post-processing step on ViT predictions, this module enhances sensitivity to fine-grained structural cues while maintaining computational efficiency. Extensive experiments on the CIFAKE, Artistic, and Custom Curated datasets demonstrate that the proposed framework achieves superior detection performance across all benchmarks, attaining 97.75% accuracy and a 97.77% F1-score on CIFAKE, surpassing widely adopted state-of-the-art models. These results establish the proposed method as a lightweight, interpretable, and effective solution for both still images and video frames, making it highly suitable for real-world applications in automated content verification and digital forensics. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´äº†é«˜åº¦é€¼çœŸçš„AIç”Ÿæˆå›¾åƒçš„æ™®åŠï¼Œè¿™ç»™æ•°å­—å–è¯å’Œå†…å®¹è®¤è¯å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æå–å…¨å±€ç‰¹å¾ï¼Œå¾€å¾€ä¼šå¿½ç•¥ç»†å¾®çš„ç»“æ„ä¸ä¸€è‡´æ€§ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ£€æµ‹æ¡†æ¶ï¼Œå®ƒå°†ç»è¿‡å¾®è°ƒè¿‡çš„è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ä¸ä¸€ç§æ–°å‹çš„è¾¹ç¼˜å›¾åƒå¤„ç†æ¨¡å—ç›¸ç»“åˆã€‚è¾¹ç¼˜æ¨¡å—è®¡ç®—å¹³æ»‘å‰åçš„è¾¹ç¼˜å·®å¼‚å›¾ç”Ÿæˆçš„æ–¹å·®ï¼Œåˆ©ç”¨è§‚å¯Ÿåˆ°AIç”Ÿæˆçš„å›¾åƒé€šå¸¸å…·æœ‰æ›´å¹³æ»‘çš„çº¹ç†ã€è¾ƒå¼±çš„è¾¹ç¼˜å’Œä¸çœŸå®å›¾åƒç›¸æ¯”å‡å°‘çš„å™ªå£°ã€‚å½“ä½œä¸ºViTé¢„æµ‹çš„åæœŸå¤„ç†æ­¥éª¤åº”ç”¨æ—¶ï¼Œæ­¤æ¨¡å—åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæé«˜äº†å¯¹ç»†å¾®ç»“æ„çº¿ç´¢çš„æ•æ„Ÿæ€§ã€‚åœ¨CIFAKEã€è‰ºæœ¯å®šåˆ¶å’Œè‡ªå®šä¹‰ç²¾é€‰æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†å“è¶Šçš„æ£€æµ‹æ€§èƒ½ï¼Œåœ¨CIFAKEä¸Šè¾¾åˆ°äº†97.75%çš„å‡†ç¡®ç‡å’Œ97.77%çš„F1åˆ†æ•°ï¼Œè¶…è¿‡äº†å¹¿æ³›é‡‡ç”¨çš„æœ€æ–°æ¨¡å‹ã€‚è¿™äº›ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•ä½œä¸ºä¸€ç§è½»é‡çº§ã€å¯è§£é‡Šã€æœ‰æ•ˆçš„é™æ­¢å›¾åƒå’Œè§†é¢‘å¸§è§£å†³æ–¹æ¡ˆï¼Œéå¸¸é€‚åˆç”¨äºè‡ªåŠ¨åŒ–å†…å®¹éªŒè¯å’Œæ•°å­—å–è¯ç­‰å®é™…åº”ç”¨åœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17877v1">PDF</a> 19 pages, 14 figures</p>
<p><strong>Summary</strong><br>åœ¨ç”Ÿæˆæ¨¡å‹å¿«é€Ÿå‘å±•çš„èƒŒæ™¯ä¸‹ï¼ŒAIç”Ÿæˆçš„å›¾åƒè¶Šæ¥è¶Šé€¼çœŸï¼Œç»™æ•°å­—å–è¯å’Œå†…å®¹è®¤è¯å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–æ·±åº¦å­¦ä¹ æ¨¡å‹æå–å…¨å±€ç‰¹å¾ï¼Œå®¹æ˜“å¿½ç•¥ç»†å¾®çš„ç»“æ„ä¸ä¸€è‡´ï¼Œå¹¶ä¸”è®¡ç®—èµ„æºéœ€æ±‚å¤§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“åˆå¾®è°ƒåçš„Vision Transformerï¼ˆViTï¼‰å’Œæ–°å‹è¾¹ç¼˜å›¾åƒå¤„ç†çš„æ··åˆæ£€æµ‹æ¡†æ¶ã€‚è¾¹ç¼˜å¤„ç†æ¨¡å—è®¡ç®—å¹³æ»‘å‰åçš„è¾¹ç¼˜å·®å¼‚å›¾æ–¹å·®ï¼Œåˆ©ç”¨AIç”Ÿæˆçš„å›¾åƒé€šå¸¸çº¹ç†æ›´å¹³æ»‘ã€è¾¹ç¼˜æ›´å¼±ã€å™ªå£°å‡å°‘çš„ç‰¹æ€§ã€‚ä½œä¸ºViTé¢„æµ‹çš„åæœŸå¤„ç†æ­¥éª¤ï¼Œè¯¥æ¨¡å—æé«˜äº†å¯¹ç»†å¾®ç»“æ„çº¿ç´¢çš„æ•æ„Ÿæ€§ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚åœ¨CIFAKEã€Artisticå’ŒCustom Curatedæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ£€æµ‹æ€§èƒ½ï¼Œåœ¨CIFAKEä¸Šè¾¾åˆ°äº†97.75%çš„å‡†ç¡®ç‡å’Œ97.77%çš„F1åˆ†æ•°ï¼Œè¶…è¶Šäº†å¹¿æ³›é‡‡ç”¨çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚è¯¥æ–¹æ³•çš„è½»é‡åŒ–ã€å¯è§£é‡Šæ€§å’Œå¯¹é™æ€å›¾åƒå’Œè§†é¢‘å¸§çš„æœ‰æ•ˆæ€§ï¼Œä½¿å…¶æˆä¸ºè‡ªåŠ¨åŒ–å†…å®¹éªŒè¯å’Œæ•°å­—å–è¯é¢†åŸŸçš„ç†æƒ³é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆçš„å›¾åƒçš„å¢é•¿ç»™æ•°å­—å–è¯å’Œå†…å®¹è®¤è¯å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•ä¾èµ–æ·±åº¦å­¦ä¹ æ¨¡å‹æå–å…¨å±€ç‰¹å¾ï¼Œå­˜åœ¨è®¡ç®—èµ„æºéœ€æ±‚å¤§å’Œå¿½ç•¥ç»†å¾®ç»“æ„ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„æ··åˆæ£€æµ‹æ¡†æ¶ç»“åˆäº†Vision Transformerï¼ˆViTï¼‰å’Œè¾¹ç¼˜å›¾åƒå¤„ç†æ–¹æ³•ã€‚</li>
<li>è¾¹ç¼˜å¤„ç†æ¨¡å—è®¡ç®—å¹³æ»‘å‰åçš„è¾¹ç¼˜å·®å¼‚å›¾æ–¹å·®æ¥è¯†åˆ«AIç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†å¯¹ç»†å¾®ç»“æ„çº¿ç´¢çš„æ•æ„Ÿæ€§å¹¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜è¯¥æ¡†æ¶æ€§èƒ½å“è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17877">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ab7a309b813eea6846c820260d42abc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40d985d776d1710e770a788851e5fd66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4dda706f808011d30de18cd9fac7513.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4fbe4c958f1252dda3cc377863f0146.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="First-Place-Solution-to-the-MLCAS-2025-GWFSS-Challenge-The-Devil-is-in-the-Detail-and-Minority"><a href="#First-Place-Solution-to-the-MLCAS-2025-GWFSS-Challenge-The-Devil-is-in-the-Detail-and-Minority" class="headerlink" title="First Place Solution to the MLCAS 2025 GWFSS Challenge: The Devil is in   the Detail and Minority"></a>First Place Solution to the MLCAS 2025 GWFSS Challenge: The Devil is in   the Detail and Minority</h2><p><strong>Authors:Songliang Cao, Tianqi Hu, Hao Lu</strong></p>
<p>In this report, we present our solution during the participation of the MLCAS 2025 GWFSS Challenge. This challenge hosts a semantic segmentation competition specific to wheat plants, which requires to segment three wheat organs including the head, leaf, and stem, and another background class. In 2025, participating a segmentation competition is significantly different from that in previous years where many tricks can play important roles. Nowadays most segmentation tricks have been well integrated into existing codebases such that our naive ViT-Adapter baseline has already achieved sufficiently good performance. Hence, we believe the key to stand out among other competitors is to focus on the problem nature of wheat per se. By probing visualizations, we identify the key â€“ the stem matters. In contrast to heads and leaves, stems exhibit fine structure and occupy only few pixels, which suffers from fragile predictions and class imbalance. Building on our baseline, we present three technical improvements tailored to stems: i) incorporating a dynamic upsampler SAPA used to enhance detail delineation; ii) leveraging semi-supervised guided distillation with stem-aware sample selection to mine the treasure beneath unlabeled data; and iii) applying a test-time scaling strategy to zoom in and segment twice the image. Despite being simple, the three improvements bring us to the first place of the competition, outperforming the second place by clear margins. Code and models will be released at <a target="_blank" rel="noopener" href="https://github.com/tiny-smart/gwfss25">https://github.com/tiny-smart/gwfss25</a>. </p>
<blockquote>
<p>åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å‚åŠ MLCAS 2025 GWFSSæŒ‘æˆ˜æ—¶çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ¬¡æŒ‘æˆ˜ä¸¾åŠäº†ä¸€åœºé’ˆå¯¹å°éº¦æ¤æ ªçš„è¯­ä¹‰åˆ†å‰²æ¯”èµ›ï¼Œè¦æ±‚å¯¹å°æ‰çš„å¤´éƒ¨ã€å¶ç‰‡å’ŒèŒéƒ¨è¿™ä¸‰ä¸ªå™¨å®˜ä»¥åŠå¦ä¸€ä¸ªèƒŒæ™¯ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚åœ¨2025å¹´ï¼Œå‚åŠ åˆ†å‰²æ¯”èµ›ä¸å¾€å¹´æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œå› ä¸ºè®¸å¤šæŠ€å·§éƒ½èƒ½èµ·åˆ°é‡è¦ä½œç”¨ã€‚å¦‚ä»Šï¼Œå¤§å¤šæ•°åˆ†å‰²æŠ€å·§å·²ç»å¾ˆå¥½åœ°é›†æˆåˆ°ç°æœ‰çš„ä»£ç åº“ä¸­ï¼Œä½¿å¾—æˆ‘ä»¬ç®€å•çš„ViT-AdapteråŸºçº¿å·²ç»å–å¾—äº†è¶³å¤Ÿå¥½çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºè¦åœ¨å…¶ä»–ç«äº‰è€…ä¸­è„±é¢–è€Œå‡ºçš„å…³é”®åœ¨äºå…³æ³¨å°éº¦æœ¬èº«çš„é—®é¢˜ç‰¹æ€§ã€‚é€šè¿‡å¯è§†åŒ–æ¢æµ‹ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†å…³é”®â€”â€”èŒéƒ¨å¾ˆé‡è¦ã€‚ä¸å¤´éƒ¨å’Œå¶ç‰‡ç›¸æ¯”ï¼ŒèŒéƒ¨å‘ˆç°å‡ºç²¾ç»†çš„ç»“æ„ï¼Œåªå ç”¨äº†å¾ˆå°‘çš„åƒç´ ï¼Œè¿™ä¼šå¯¼è‡´é¢„æµ‹ç»“æœä¸ç¨³å¥å’Œç±»åˆ«ä¸å¹³è¡¡ã€‚åŸºäºæˆ‘ä»¬çš„åŸºçº¿ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰é¡¹é’ˆå¯¹èŒéƒ¨çš„æŠ€æœ¯æ”¹è¿›ï¼šä¸€ï¼‰åŠ å…¥ä¸€ä¸ªåŠ¨æ€ä¸Šé‡‡æ ·å™¨SAPAï¼Œç”¨äºå¢å¼ºç»†èŠ‚æç»˜ï¼›äºŒï¼‰åˆ©ç”¨åŠç›‘ç£å¼•å¯¼è’¸é¦å’Œå¸¦èŒæ„ŸçŸ¥çš„æ ·æœ¬é€‰æ‹©æ¥æŒ–æ˜æœªæ ‡è®°æ•°æ®ä¸­çš„å®è—ï¼›ä¸‰ï¼‰é‡‡ç”¨æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥æ¥ä¸¤æ¬¡æ”¾å¤§å¹¶åˆ†å‰²å›¾åƒã€‚å°½ç®¡è¿™äº›æ”¹è¿›å¾ˆç®€å•ï¼Œä½†å®ƒä»¬å¸®åŠ©æˆ‘ä»¬è·å¾—äº†æ¯”èµ›çš„ç¬¬ä¸€åï¼Œå¹¶æ˜¾è‘—ä¼˜äºç¬¬äºŒåã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tiny-smart/gwfss25%E4%B8%8A%E5%8F%91%E8%A1%A8%E3%80%82">https://github.com/tiny-smart/gwfss25ä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17305v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥æŠ¥å‘Šä¸»è¦ä»‹ç»äº†å›¢é˜Ÿå‚ä¸MLCAS 2025å°éº¦åˆ†å‰²æŒ‘æˆ˜èµ›çš„ç»éªŒã€‚è¯¥æŠ¥å‘Šè¯¦ç»†ä»‹ç»äº†ä»–ä»¬åœ¨å°éº¦å¤´éƒ¨ã€å¶ç‰‡å’ŒèŒéƒ¨çš„è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­æ‰€é‡‡å–çš„æŠ€æœ¯ç­–ç•¥ï¼Œå¹¶å¼ºè°ƒäº†èŒéƒ¨ç»†èŠ‚å¯¹é¢„æµ‹å’Œç±»åˆ«å¹³è¡¡çš„é‡è¦æ€§ã€‚å›¢é˜Ÿæå‡ºäº†ä¸‰é¡¹é’ˆå¯¹èŒéƒ¨çš„æŠ€æœ¯æ”¹è¿›ï¼Œå¹¶åœ¨æ¯”èµ›ä¸­å–å¾—äº†ç¬¬ä¸€åçš„æˆç»©ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥æŠ¥å‘Šå‚ä¸äº†ä¸€ä¸ªä¸“æ³¨äºå°éº¦æ¤ç‰©è¯­ä¹‰åˆ†å‰²çš„æŒ‘æˆ˜èµ›ï¼Œé‡ç‚¹å¯¹å°éº¦å¤´éƒ¨ã€å¶ç‰‡å’ŒèŒéƒ¨è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>ä¸å¾€å¹´ä¸åŒï¼Œå½“å‰å¤§å¤šæ•°åˆ†å‰²æŠ€å·§å·²ç»å¾ˆå¥½åœ°é›†æˆåˆ°ç°æœ‰ä»£ç åº“ä¸­ï¼Œå› æ­¤å…³é”®åœ¨äºç†è§£å°éº¦æœ¬èº«çš„é—®é¢˜ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡å¯è§†åŒ–åˆ†æï¼Œå‘ç°èŒéƒ¨æ˜¯é—®é¢˜çš„å…³é”®ï¼Œå…¶ç²¾ç»†ç»“æ„å’Œå°‘é‡åƒç´ å¯¼è‡´é¢„æµ‹æ˜“å‡ºé”™å’Œç±»åˆ«ä¸å¹³è¡¡ã€‚</li>
<li>é’ˆå¯¹èŒéƒ¨æå‡ºäº†ä¸‰é¡¹æŠ€æœ¯æ”¹è¿›ï¼šé‡‡ç”¨åŠ¨æ€ä¸Šé‡‡æ ·å™¨SAPAä»¥å¢å¼ºç»†èŠ‚æç»˜ï¼›åˆ©ç”¨åŠç›‘ç£å¼•å¯¼è’¸é¦ä¸èŒéƒ¨æ„ŸçŸ¥æ ·æœ¬é€‰æ‹©æ¥æŒ–æ˜æœªæ ‡è®°æ•°æ®ä¸­çš„ä¿¡æ¯ï¼›åº”ç”¨æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥å¯¹å›¾åƒè¿›è¡Œä¸¤æ¬¡åˆ†å‰²ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17305">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9993fdb9cf68dc539ad8e6d8afafea06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ebfe3ac2071418ad5283cc73dff860e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13a3667b019ffad4cadfa8539cefa6d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb58b59da71d560285672dd61bb46322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89d0dcc40a5e766f4488e433ff053852.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Foundations-and-Models-in-Modern-Computer-Vision-Key-Building-Blocks-in-Landmark-Architectures"><a href="#Foundations-and-Models-in-Modern-Computer-Vision-Key-Building-Blocks-in-Landmark-Architectures" class="headerlink" title="Foundations and Models in Modern Computer Vision: Key Building Blocks in   Landmark Architectures"></a>Foundations and Models in Modern Computer Vision: Key Building Blocks in   Landmark Architectures</h2><p><strong>Authors:Radu-Andrei Bourceanu, Neil De La Fuente, Jan Grimm, Andrei Jardan, Andriy Manucharyan, Cornelius Weiss, Daniel Cremers, Roman Pflugfelder</strong></p>
<p>This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analysis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer architecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recognition. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models. </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šé€šè¿‡åˆ†æå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡ï¼Œåˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ã€‚åˆ†æä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„å¼€å§‹ã€‚æˆ‘ä»¬å›é¡¾äº†ResNetï¼Œå®ƒå¼•å…¥äº†æ®‹å·®è¿æ¥ï¼Œå…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå®ç°äº†å¯¹æ·±åº¦å·ç§¯ç½‘ç»œçš„æœ‰æ•ˆè®­ç»ƒã€‚ä¹‹åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—çš„Vision Transformerï¼ˆViTï¼‰ï¼Œè¿™å¼€åˆ›äº†æ–°çš„èŒƒå¼ï¼Œè¯æ˜äº†æ³¨æ„åŠ›æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨è¿™äº›è§†è§‰è¡¨ç¤ºéª¨å¹²çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”Ÿæˆæ¨¡å‹ã€‚åˆ†æäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ–°å‹å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹é€šè¿‡ç”Ÿæˆå™¨ä¸é‰´åˆ«å™¨çš„å¯¹æŠ—æ¥å­¦ä¹ å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚ç„¶åä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ï¼Œé€šè¿‡æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­çš„è¿ç»­å»å™ªè¿‡ç¨‹æ”¹è¿›äº†å…ˆå‰çš„ç”Ÿæˆæ–¹æ³•ã€‚LDMså®ç°äº†é«˜ä¿çœŸåˆæˆï¼Œå…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œä»£è¡¨äº†å½“å‰å›¾åƒç”Ÿæˆçš„æœ€æ–°æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡å°‘å¯¹æ ‡å®šæ•°æ®ä¾èµ–æ€§çš„è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚DINOæ˜¯ä¸€ä¸ªè‡ªè’¸é¦æ¡†æ¶ï¼Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ åŒ¹é…åŠ¨é‡æ›´æ–°çš„æ•™å¸ˆè¾“å‡ºï¼Œäº§ç”Ÿå…·æœ‰å¼ºå¤§k-NNåˆ†ç±»æ€§èƒ½çš„ç‰¹å¾ã€‚æœ€ç»ˆæˆ‘ä»¬ä»‹ç»äº†Masked Autoencodersï¼ˆMAEï¼‰ï¼Œå®ƒé‡‡ç”¨å¯¹ç§°çš„ç¼–ç å™¨-è§£ç å™¨è®¾è®¡æ¥é‡å»ºé«˜åº¦é®æŒ¡çš„è¾“å…¥ï¼Œæä¾›äº†ä¸€ç§é«˜åº¦å¯æ‰©å±•å’Œæœ‰æ•ˆçš„é¢„è®­ç»ƒå¤§è§„æ¨¡è§†è§‰æ¨¡å‹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23357v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æŠ¥å‘Šåˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ï¼Œé€šè¿‡è€ƒå¯Ÿå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡è¿›è¡Œæ·±å…¥æ¢è®¨ã€‚æŠ¥å‘Šä»å›¾åƒè¯†åˆ«çš„åŸºç¡€æ¶æ„å¼€å§‹ï¼Œä»‹ç»äº†ResNetã€Vision Transformerï¼ˆViTï¼‰ã€ç”Ÿæˆæ¨¡å‹å¦‚Generative Adversarial Networksï¼ˆGANsï¼‰å’ŒLatent Diffusion Modelsï¼ˆLDMsï¼‰ï¼Œä»¥åŠè‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯å¦‚DINOå’ŒMasked Autoencodersï¼ˆMAEï¼‰ã€‚è¿™äº›æŠ€æœ¯ä»£è¡¨äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ¥å‘Šåˆ†æäº†è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å…³é”®æŠ€æœ¯è¿›å±•ï¼ŒåŒ…æ‹¬åŸºç¡€æ¶æ„ã€ç”Ÿæˆæ¨¡å‹å’Œè‡ªç›‘ç£å­¦ä¹ ã€‚</li>
<li>ResNeté€šè¿‡å¼•å…¥æ®‹å·®è¿æ¥å…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿è®­ç»ƒæ›´æ·±çš„å·ç§¯ç½‘ç»œæˆä¸ºå¯èƒ½ã€‚</li>
<li>Vision Transformer (ViT) å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒåºåˆ—ï¼Œå±•ç¤ºäº†æ³¨æ„åŠ›æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>Generative Adversarial Networks (GANs) é€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒè¿‡ç¨‹å­¦ä¹ å¤æ‚çš„æ•°æ®åˆ†å¸ƒï¼Œç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
<li>Latent Diffusion Models (LDMs) åœ¨æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œåºè´¯å»å™ªè¿‡ç¨‹ï¼Œå®ç°äº†é«˜ä¿çœŸåˆæˆå’Œæ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚</li>
<li>DINOæ˜¯ä¸€ä¸ªè‡ªè’¸é¦æ¡†æ¶ï¼Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ åŒ¹é…åŠ¨é‡æ›´æ–°çš„æ•™å¸ˆè¾“å‡ºï¼Œäº§ç”Ÿå¼ºå¤§çš„k-NNåˆ†ç±»æ€§èƒ½ã€‚</li>
<li>Masked Autoencoders (MAE) åˆ©ç”¨å¯¹ç§°çš„ç¼–ç å™¨-è§£ç å™¨è®¾è®¡æ¥é‡å»ºé«˜åº¦é®æŒ¡çš„è¾“å…¥ï¼Œä¸ºå¤§è§„æ¨¡è§†è§‰æ¨¡å‹é¢„è®­ç»ƒæä¾›äº†å¯ä¼¸ç¼©å’Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a18a3a6abfb6c9ac649e6950b4fd7fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb308890c31345f2aab07dd14fc1774c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c7b7523d1996c598c4c86034ca318b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-498ae1a3c68c14971d462838ba0e1fa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a42b24e9ff5111ddf2f9f617b925344.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Prompt-based-Dynamic-Token-Pruning-for-Efficient-Segmentation-of-Medical-Images"><a href="#Prompt-based-Dynamic-Token-Pruning-for-Efficient-Segmentation-of-Medical-Images" class="headerlink" title="Prompt-based Dynamic Token Pruning for Efficient Segmentation of Medical   Images"></a>Prompt-based Dynamic Token Pruning for Efficient Segmentation of Medical   Images</h2><p><strong>Authors:Pallabi Dutta, Anubhab Maity, Sushmita Mitra</strong></p>
<p>The high computational demands of Vision Transformers (ViTs) in processing a large number of tokens often constrain their practical application in analyzing medical images. This research proposes a Prompt-driven Adaptive Token ({\it PrATo}) pruning method to selectively reduce the processing of irrelevant tokens in the segmentation pipeline. The prompt-based spatial prior helps to rank the tokens according to their relevance. Tokens with low-relevance scores are down-weighted, ensuring that only the relevant ones are propagated for processing across subsequent stages. This data-driven pruning strategy improves segmentation accuracy and inference speed by allocating computational resources to essential regions. The proposed framework is integrated with several state-of-the-art models to facilitate the elimination of irrelevant tokens, thereby enhancing computational efficiency while preserving segmentation accuracy. The experimental results show a reduction of $\sim$ 35-55% tokens; thus reducing the computational costs relative to baselines. Cost-effective medical image processing, using our framework, facilitates real-time diagnosis by expanding its applicability in resource-constrained environments. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTï¼‰åœ¨å¤„ç†å¤§é‡æ ‡è®°ç¬¦æ—¶è®¡ç®—éœ€æ±‚è¾ƒé«˜ï¼Œè¿™ç»å¸¸é™åˆ¶å…¶åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å®é™…åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„è‡ªé€‚åº”æ ‡è®°ç¬¦ï¼ˆPrAToï¼‰ä¿®å‰ªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰é€‰æ‹©åœ°å‡å°‘åˆ†å‰²ç®¡é“ä¸­ä¸ç›¸å…³æ ‡è®°ç¬¦çš„å¤„ç†ã€‚åŸºäºæç¤ºçš„ç©ºé—´å…ˆéªŒæœ‰åŠ©äºæ ¹æ®ç›¸å…³æ€§å¯¹æ ‡è®°ç¬¦è¿›è¡Œæ’åã€‚ä½ç›¸å…³æ€§å¾—åˆ†çš„æ ‡è®°ç¬¦è¢«é™æƒå¤„ç†ï¼Œç¡®ä¿ä»…å°†ç›¸å…³çš„æ ‡è®°ç¬¦ä¼ æ’­åˆ°åç»­é˜¶æ®µè¿›è¡Œå¤„ç†ã€‚è¿™ç§æ•°æ®é©±åŠ¨çš„ä¿®å‰ªç­–ç•¥é€šè¿‡å‘å…³é”®åŒºåŸŸåˆ†é…è®¡ç®—èµ„æºæ¥æé«˜åˆ†å‰²ç²¾åº¦å’Œæ¨ç†é€Ÿåº¦ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸å¤šç§æœ€æ–°æ¨¡å‹é›†æˆï¼Œä»¥æ¶ˆé™¤ä¸ç›¸å…³çš„æ ‡è®°ç¬¦ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡å¹¶ä¿æŒåˆ†å‰²ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‡å°‘äº†çº¦35-55%çš„æ ‡è®°ç¬¦ï¼Œä¸åŸºçº¿ç›¸æ¯”é™ä½äº†è®¡ç®—æˆæœ¬ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶è¿›è¡Œç»æµå®æƒ çš„åŒ»å­¦å›¾åƒå¤„ç†ï¼Œé€šè¿‡åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ‰©å¤§å…¶é€‚ç”¨æ€§ï¼Œä¿ƒè¿›äº†å®æ—¶è¯Šæ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16369v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæç¤ºçš„è‡ªé€‚åº”ä»¤ç‰Œï¼ˆPrAToï¼‰å‰ªææ–¹æ³•ï¼Œç”¨äºé€‰æ‹©æ€§å‡å°‘åŒ»å­¦å›¾åƒå¤„ç†ä¸­æ— å…³ä»¤ç‰Œçš„å¤„ç†ã€‚è¯¥æ–¹æ³•é€šè¿‡æç¤ºç©ºé—´å…ˆéªŒçŸ¥è¯†å¯¹ä»¤ç‰Œè¿›è¡Œæ’åï¼Œé™ä½ä½ç›¸å…³æ€§ä»¤ç‰Œçš„æƒé‡ï¼Œç¡®ä¿åªå¤„ç†ç›¸å…³ä»¤ç‰Œã€‚æ­¤æ•°æ®é©±åŠ¨çš„ç­–ç•¥æé«˜äº†åˆ†å‰²å‡†ç¡®æ€§å’Œæ¨æ–­é€Ÿåº¦ï¼Œé€šè¿‡å¯¹é‡è¦åŒºåŸŸåˆ†é…è®¡ç®—èµ„æºæ¥æå‡è®¡ç®—æ•ˆç‡å¹¶ç»´æŒåˆ†å‰²ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å‡å°‘äº†çº¦35%~55%çš„ä»¤ç‰Œå¤„ç†é‡ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚ä½¿ç”¨æ­¤æ¡†æ¶è¿›è¡ŒåŒ»å­¦å›¾åƒå¤„ç†å¯å®ç°å®æ—¶è¯Šæ–­ï¼Œå¹¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ‰©å¤§å…¶é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs)åœ¨å¤„ç†å¤§é‡ä»¤ç‰Œæ—¶å­˜åœ¨é«˜è®¡ç®—éœ€æ±‚ï¼Œé™åˆ¶äº†å…¶åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å®é™…åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„è‡ªé€‚åº”ä»¤ç‰Œï¼ˆPrAToï¼‰å‰ªææ–¹æ³•ï¼Œä»¥é€‰æ‹©æ€§å‡å°‘æ— å…³ä»¤ç‰Œçš„å¤„ç†ã€‚</li>
<li>æç¤ºç©ºé—´å…ˆéªŒçŸ¥è¯†ç”¨äºæ’åä»¤ç‰Œï¼Œæ ¹æ®ç›¸å…³æ€§é™ä½ä½ç›¸å…³æ€§ä»¤ç‰Œçš„æƒé‡ã€‚</li>
<li>æ•°æ®é©±åŠ¨çš„å‰ªæç­–ç•¥é€šè¿‡åˆ†é…è®¡ç®—èµ„æºç»™é‡è¦åŒºåŸŸï¼Œæé«˜äº†åˆ†å‰²å‡†ç¡®æ€§å’Œæ¨æ–­é€Ÿåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸å¤šç§æœ€æ–°æ¨¡å‹é›†æˆï¼Œå‡å°‘äº†æ— å…³ä»¤ç‰Œçš„å¤„ç†ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å‡å°‘äº†çº¦35%~55%çš„è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7c1960f08075e1faadee65bfea5d825c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d69c0dbc629cd47f57716b69cb8c868c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0947cacd4adefee6f58ebae3d3b53d81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ec905cb77de64418e451f263c6ce74f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DART-Differentiable-Dynamic-Adaptive-Region-Tokenizer-for-Vision-Transformer-and-Mamba"><a href="#DART-Differentiable-Dynamic-Adaptive-Region-Tokenizer-for-Vision-Transformer-and-Mamba" class="headerlink" title="DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision   Transformer and Mamba"></a>DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision   Transformer and Mamba</h2><p><strong>Authors:Shicheng Yin, Kaixuan Yin, Yang Liu, Weixing Chen, Liang Lin</strong></p>
<p>Recently, non-convolutional models such as the Vision Transformer (ViT) and Vision Mamba (Vim) have achieved remarkable performance in computer vision tasks. However, their reliance on fixed-size patches often results in excessive encoding of background regions and omission of critical local details, especially when informative objects are sparsely distributed. To address this, we introduce a fully differentiable Dynamic Adaptive Region Tokenizer (DART), which adaptively partitions images into content-dependent patches of varying sizes. DART combines learnable region scores with piecewise differentiable quantile operations to allocate denser tokens to information-rich areas. Despite introducing only approximately 1 million (1M) additional parameters, DART improves accuracy by 2.1% on DeiT (ImageNet-1K). Unlike methods that uniformly increase token density to capture fine-grained details, DART offers a more efficient alternative, achieving 45% FLOPs reduction with superior performance. Extensive experiments on DeiT, Vim, and VideoMamba confirm that DART consistently enhances accuracy while incurring minimal or even reduced computational overhead. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HCPLab-SYSU/DART">https://github.com/HCPLab-SYSU/DART</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè¯¸å¦‚Vision Transformerï¼ˆViTï¼‰å’ŒVision Mambaï¼ˆVimï¼‰ç­‰éå·ç§¯æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹å›ºå®šå¤§å°å›¾å—çš„ä¾èµ–å¾€å¾€å¯¼è‡´èƒŒæ™¯åŒºåŸŸçš„è¿‡åº¦ç¼–ç ä»¥åŠå…³é”®å±€éƒ¨ç»†èŠ‚çš„é—æ¼ï¼Œå°¤å…¶æ˜¯åœ¨ä¿¡æ¯å¯¹è±¡ç¨€ç–åˆ†å¸ƒçš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å®Œå…¨å¯å¾®åˆ†çš„åŠ¨æ€è‡ªé€‚åº”åŒºåŸŸæ ‡è®°å™¨ï¼ˆDARTï¼‰ï¼Œå®ƒèƒ½å¤Ÿè‡ªé€‚åº”åœ°å°†å›¾åƒåˆ†å‰²æˆå¤§å°ä¸åŒçš„å†…å®¹ç›¸å…³å›¾å—ã€‚DARTé€šè¿‡ç»“åˆå¯å­¦ä¹ çš„åŒºåŸŸåˆ†æ•°å’Œåˆ†æ®µå¯å¾®åˆ†çš„ä¸­ä½æ•°æ“ä½œï¼Œå°†æ›´å¯†é›†çš„æ ‡è®°åˆ†é…ç»™ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸã€‚å°½ç®¡åªå¼•å…¥äº†å¤§çº¦ä¸€ç™¾ä¸‡ï¼ˆ1Mï¼‰ä¸ªé¢å¤–çš„å‚æ•°ï¼Œä½†DARTåœ¨DeiTï¼ˆImageNet-1Kï¼‰ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†2.1%ã€‚ä¸é€šè¿‡å‡åŒ€å¢åŠ æ ‡è®°å¯†åº¦æ¥æ•æ‰ç»†èŠ‚çš„æ–¹æ³•ä¸åŒï¼ŒDARTæä¾›äº†ä¸€ç§æ›´æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨æ€§èƒ½ä¼˜è¶Šçš„åŒæ—¶å®ç°äº†45%çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°å‡å°‘ã€‚åœ¨DeiTã€Vimå’ŒVideoMambaä¸Šçš„å¤§é‡å®éªŒè¯å®ï¼ŒDARTåœ¨ä¿æŒå‡†ç¡®æ€§æé«˜çš„åŒæ—¶ï¼Œè®¡ç®—å¼€é”€æå°ç”šè‡³æœ‰æ‰€å‡å°‘ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/HCPLab-SYSU/DART%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HCPLab-SYSU/DARTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10390v2">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/HCPLab-SYSU/DART">https://github.com/HCPLab-SYSU/DART</a></p>
<p><strong>Summary</strong><br>     éå·ç§¯æ¨¡å‹å¦‚Vision Transformerï¼ˆViTï¼‰å’ŒVision Mambaï¼ˆVimï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å®ƒä»¬å›ºå®šå¤§å°çš„æ–‘å—å¯¼è‡´èƒŒæ™¯åŒºåŸŸè¿‡åº¦ç¼–ç ï¼Œå¿½ç•¥å…³é”®å±€éƒ¨ç»†èŠ‚ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºå…¨å¾®åˆ†åŠ¨æ€è‡ªé€‚åº”åŒºåŸŸä»¤ç‰ŒåŒ–å™¨ï¼ˆDARTï¼‰ï¼Œå…¶å¯è‡ªé€‚åº”åœ°å°†å›¾åƒåˆ†å‰²æˆå†…å®¹ç›¸å…³çš„å¯å˜å¤§å°æ–‘å—ã€‚DARTç»“åˆå­¦ä¹ åŒºåŸŸåˆ†æ•°å’Œåˆ†æ®µå¾®åˆ†å®šé‡æ“ä½œï¼Œå°†æ›´å¯†é›†çš„ä»¤ç‰Œåˆ†é…ç»™ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸã€‚ä»…å¢åŠ çº¦1ç™¾ä¸‡ï¼ˆ1Mï¼‰å‚æ•°ï¼ŒDARTå³å¯æé«˜DeiTï¼ˆImageNet-1Kï¼‰çš„å‡†ç¡®ç‡2.1%ã€‚ä¸å‡åŒ€å¢åŠ ä»¤ç‰Œå¯†åº¦ä»¥æ•è·ç»†èŠ‚çš„æ–¹æ³•ä¸åŒï¼ŒDARTæä¾›æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ç°45%æµ®ç‚¹è¿ç®—å‡å°‘ä¸”æ€§èƒ½æ›´ä½³ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜ï¼ŒDARTåœ¨å¢å¼ºç²¾åº¦çš„åŒæ—¶ï¼Œè®¡ç®—å¼€é”€æœ€å°æˆ–ç”šè‡³å‡å°‘ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/HCPLab-SYSU/DART%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HCPLab-SYSU/DARTè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éå·ç§¯æ¨¡å‹å¦‚ViTå’ŒVimåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†å­˜åœ¨å›ºå®šæ–‘å—å¯¼è‡´çš„èƒŒæ™¯åŒºåŸŸè¿‡åº¦ç¼–ç å’Œå±€éƒ¨ç»†èŠ‚å¿½ç•¥é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå¼•å…¥å…¨å¾®åˆ†åŠ¨æ€è‡ªé€‚åº”åŒºåŸŸä»¤ç‰ŒåŒ–å™¨ï¼ˆDARTï¼‰ï¼Œå…¶èƒ½è‡ªé€‚åº”åœ°æ ¹æ®å†…å®¹å°†å›¾åƒåˆ†å‰²æˆä¸åŒå¤§å°çš„æ–‘å—ã€‚</li>
<li>DARTé€šè¿‡ç»“åˆå­¦ä¹ åŒºåŸŸåˆ†æ•°å’Œåˆ†æ®µå¾®åˆ†å®šé‡æ“ä½œï¼Œå®ç°ä¿¡æ¯ä¸°å¯ŒåŒºåŸŸçš„æ›´å¯†é›†ä»¤ç‰Œåˆ†é…ã€‚</li>
<li>DARTåœ¨ä»…å¢åŠ çº¦1ç™¾ä¸‡å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†DeiTåœ¨ImageNet-1Kä¸Šçš„å‡†ç¡®ç‡2.1%ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒDARTæä¾›æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½åœ¨å‡å°‘æµ®ç‚¹è¿ç®—çš„åŒæ—¶ä¿æŒæˆ–æé«˜æ€§èƒ½ã€‚</li>
<li>å¹¿æ³›çš„å®éªŒè¯æ˜ï¼ŒDARTåœ¨å¤šç§æ¨¡å‹å’Œä»»åŠ¡ä¸­å‡èƒ½æœ‰æ•ˆå¢å¼ºç²¾åº¦ï¼ŒåŒæ—¶è®¡ç®—å¼€é”€è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e597942d15560a81eba9410c9ac8551b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0af1031bbc22130525bbc0ebd3221f59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c77a362038403378099d858d1c7becf4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a30ab3f137d23f240dade9c71dbf5ce3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3446f548e7ca2a262b5d88f9da3e44b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8abd033004876f692989c123292705ec.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Advancing-Marine-Research-UWSAM-Framework-and-UIIS10K-Dataset-for-Precise-Underwater-Instance-Segmentation"><a href="#Advancing-Marine-Research-UWSAM-Framework-and-UIIS10K-Dataset-for-Precise-Underwater-Instance-Segmentation" class="headerlink" title="Advancing Marine Research: UWSAM Framework and UIIS10K Dataset for   Precise Underwater Instance Segmentation"></a>Advancing Marine Research: UWSAM Framework and UIIS10K Dataset for   Precise Underwater Instance Segmentation</h2><p><strong>Authors:Hua Li, Shijie Lian, Zhiyuan Li, Runmin Cong, Chongyi Li, Laurence T. Yang, Weidong Zhang, Sam Kwong</strong></p>
<p>With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at <a target="_blank" rel="noopener" href="https://github.com/LiamLian0727/UIIS10K">https://github.com/LiamLian0727/UIIS10K</a>. </p>
<blockquote>
<p>éšç€å¤§è§„æ¨¡å»ºæ¨¡çš„æœ€æ–°çªç ´ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨å„ç§è§†è§‰åº”ç”¨ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ°´ä¸‹é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ŒSAMåŠå…¶å˜ä½“åœ¨ç«¯åˆ°ç«¯çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­é¢ä¸´æ€§èƒ½å±€é™ï¼Œè€Œå®ƒä»¬è¾ƒé«˜çš„è®¡ç®—è¦æ±‚è¿›ä¸€æ­¥é˜»ç¢äº†åœ¨æ°´ä¸‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼Œå…¶ä¸­åŒ…æ‹¬10,048å¼ å…·æœ‰10ç±»åƒç´ çº§æ³¨é‡Šçš„å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬ä»‹ç»äº†UWSAMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨å‡†ç¡®åˆ†å‰²è€Œè®¾è®¡çš„é«˜æ•ˆæ¨¡å‹ã€‚UWSAMé€šè¿‡åŸºäºMask GATçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä»SAMçš„ViT-Hugeå›¾åƒç¼–ç å™¨è’¸é¦çŸ¥è¯†åˆ°è¾ƒå°çš„ViT-Smallå›¾åƒç¼–ç å™¨ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºUWSAMè®¾è®¡äº†End-to-endæ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œå®ƒä¼šè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œè€Œä¸æ˜¯æ˜¾å¼æä¾›å‰æ™¯ç‚¹æˆ–æ¡†ä½œä¸ºæç¤ºï¼Œä»è€Œä½¿ç½‘ç»œèƒ½å¤Ÿå‡†ç¡®å®šä½æ°´ä¸‹å®ä¾‹ï¼Œå®ç°é«˜æ•ˆåˆ†å‰²ã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¯æœ‰æ•ˆçš„ï¼Œåœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šå®ç°äº†å¯¹æœ€æ–°æŠ€æœ¯çš„æ˜¾è‘—æ€§èƒ½æ”¹è¿›ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/LiamLian0">https://github.com/LiamLian0</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15581v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å¤§å‹æ¨¡å‹çš„æ–°çªç ´ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨å„ç§è§†è§‰åº”ç”¨ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ°´ä¸‹é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ŒSAMåŠå…¶å˜ä½“åœ¨ç«¯åˆ°ç«¯çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­æ€§èƒ½å—é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§è§„æ¨¡æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼ŒåŒ…å«10,048å¼ å¸¦æœ‰åƒç´ çº§æ³¨é‡Šçš„10ç±»å›¾åƒã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨å‡†ç¡®åˆ†å‰²è€Œè®¾è®¡çš„UWSAMæ¨¡å‹ã€‚UWSAMé€šè¿‡Mask GATåŸºç¡€ä¸Šçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä»SAMçš„ViT-Hugeå›¾åƒç¼–ç å™¨æç‚¼çŸ¥è¯†ï¼Œç”¨äºViT-Smallå›¾åƒç¼–ç å™¨çš„æœ‰æ•ˆè§†è§‰è¡¨å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºUWSAMè®¾è®¡äº†ç«¯åˆ°ç«¯çš„æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œè€Œæ— éœ€æ˜ç¡®æä¾›å‰æ™¯ç‚¹æˆ–æ¡†ä½œä¸ºæç¤ºï¼Œä»è€Œå¸®åŠ©ç½‘ç»œå‡†ç¡®å®šä½æ°´ä¸‹å®ä¾‹ï¼Œå®ç°é«˜æ•ˆåˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Segment Anything Model (SAM) åœ¨è§†è§‰åº”ç”¨ä¸­æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†åœ¨æ°´ä¸‹å®ä¾‹åˆ†å‰²æ–¹é¢å­˜åœ¨æ€§èƒ½é™åˆ¶ã€‚</li>
<li>ç¼ºä¹æ°´ä¸‹é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†æ˜¯SAMåŠå…¶å˜ä½“åœ¨æ°´ä¸‹ä»»åŠ¡ä¸­æ€§èƒ½å—é™çš„ä¸»è¦åŸå› ã€‚</li>
<li>UIIS10Kæ•°æ®é›†æ˜¯ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜è€Œæå‡ºçš„å¤§è§„æ¨¡æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†ï¼ŒåŒ…å«å¸¦åƒç´ çº§æ³¨é‡Šçš„10,048å¼ å›¾åƒã€‚</li>
<li>UWSAMæ¨¡å‹ä¸“ä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨å‡†ç¡®åˆ†å‰²è€Œè®¾è®¡ã€‚</li>
<li>UWSAMé€šè¿‡Mask GATåŸºç¡€ä¸Šçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•æç‚¼çŸ¥è¯†ï¼Œå®ç°æœ‰æ•ˆè§†è§‰è¡¨å¾å­¦ä¹ ã€‚</li>
<li>ç«¯åˆ°ç«¯çš„æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œæé«˜ç½‘ç»œå®šä½æ°´ä¸‹å®ä¾‹çš„å‡†ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-957435f96e126abe8ed1099bbe43f5c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e91ca0fa49ab6cc348f37002c35a8f86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92266d6fab62f08423ef24225b28f52f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09817e9bda876567c999a938e069877d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a91b39393a16a3d55d80e35c78ca89a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6a0d5b5490b8a8af3e9b72fac8e335e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-28758a3572f844aeef49743a5127ccca.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Differential Morphological Profile Neural Networks for Semantic   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6f1b4bd471705c36ac62904a918f4210.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  VideoRewardBench Comprehensive Evaluation of Multimodal Reward Models   for Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
