<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-09-07  Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c4a5bc59031c8c4fec5e3de832ba5d56.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    80 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-07-更新"><a href="#2025-09-07-更新" class="headerlink" title="2025-09-07 更新"></a>2025-09-07 更新</h1><h2 id="Test-Time-Adaptation-for-Speech-Enhancement-via-Domain-Invariant-Embedding-Transformation"><a href="#Test-Time-Adaptation-for-Speech-Enhancement-via-Domain-Invariant-Embedding-Transformation" class="headerlink" title="Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation"></a>Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation</h2><p><strong>Authors:Tobias Raichle, Niels Edinger, Bin Yang</strong></p>
<p>Deep learning-based speech enhancement models achieve remarkable performance when test distributions match training conditions, but often degrade when deployed in unpredictable real-world environments with domain shifts. To address this challenge, we present LaDen (latent denoising), the first test-time adaptation method specifically designed for speech enhancement. Our approach leverages powerful pre-trained speech representations to perform latent denoising, approximating clean speech representations through a linear transformation of noisy embeddings. We show that this transformation generalizes well across domains, enabling effective pseudo-labeling for target domains without labeled target data. The resulting pseudo-labels enable effective test-time adaptation of speech enhancement models across diverse acoustic environments. We propose a comprehensive benchmark spanning multiple datasets with various domain shifts, including changes in noise types, speaker characteristics, and languages. Our extensive experiments demonstrate that LaDen consistently outperforms baseline methods across perceptual metrics, particularly for speaker and language domain shifts. </p>
<blockquote>
<p>基于深度学习的语音增强模型在测试分布与训练条件相匹配时表现出卓越的性能，但在部署在不可预测的现实世界环境中，由于领域偏移，性能往往会下降。为了解决这一挑战，我们提出了LaDen（潜在去噪），这是专门为语音增强设计的第一个测试时间适应方法。我们的方法利用强大的预训练语音表示来执行潜在去噪，通过噪声嵌入的线性变换来近似清洁语音表示。我们表明，这种变换在不同的领域之间具有很好的通用性，能够在没有标记目标数据的情况下，对目标领域进行有效的伪标签标注。这些生成的伪标签能够在各种声学环境中有效地适应语音增强模型的测试时间。我们提出了一个全面的基准测试，涵盖多个数据集，包括噪声类型、说话人特征和语言方面的各种领域偏移。我们的广泛实验表明，LaDen在感知指标上始终优于基准方法，特别是在说话人和语言领域偏移方面。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04280v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>摘要</strong></p>
<p>基于深度学习的语音增强模型在测试分布与训练条件相匹配时表现出卓越的性能，但在部署到具有领域差异性的不可预测的现实世界环境中时，性能往往会下降。为了应对这一挑战，我们提出了LaDen（潜在去噪），这是一种专门为语音增强设计的测试时间适应方法。我们的方法利用强大的预训练语音表示来执行潜在去噪，通过噪声嵌入的线性变换来近似清洁语音表示。我们表明，这种变换在不同的领域之间具有很好的通用性，能够在没有标记目标数据的情况下，对目标域进行有效的伪标签。伪标签使得在多种声学环境中的语音增强模型的测试时间适应变得有效。我们提出了一个全面的基准测试，涵盖多个具有各种领域差异性的数据集，包括噪声类型、说话人特征和语言的变化。我们的广泛实验表明，LaDen在感知指标上始终优于基准方法，特别是在说话人和语言领域的变化方面。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LaDen是一种测试时间适应方法，专为语音增强设计，用于处理现实世界中不可预测的声学环境。</li>
<li>利用预训练语音表示的潜在去噪技术，通过线性变换噪声嵌入来近似清洁语音。</li>
<li>该方法在不同领域间具有良好的通用性，并能在无标签目标数据的情况下进行有效的伪标签。</li>
<li>LaDen的伪标签使语音增强模型在多种声学环境中的测试时间适应变得有效。</li>
<li>提出了一个全面的基准测试，涵盖多个数据集，反映各种领域差异，如噪声类型、说话人特征和语言变化。</li>
<li>LaDen在感知指标上超越了其他方法，特别是在处理说话人和语言领域的差异方面表现出色。</li>
<li>LaDen的优越性能表明它在处理现实世界的语音增强任务时的实用性和有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04280">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9caed9dc433b0a900fa90f005c3763ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99ef586c2ae488498167235f58182b80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d679a2e4b13e239a074ed22e7250e0a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0fff68cf086b086368aae42689ff911.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37e872d34af63f1aceef049bf8b611d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbc29eb7e9affe8939906473e8b132b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09d56deab92e004e34d59f15783d710f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WenetSpeech-Yue-A-Large-scale-Cantonese-Speech-Corpus-with-Multi-dimensional-Annotation"><a href="#WenetSpeech-Yue-A-Large-scale-Cantonese-Speech-Corpus-with-Multi-dimensional-Annotation" class="headerlink" title="WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with   Multi-dimensional Annotation"></a>WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with   Multi-dimensional Annotation</h2><p><strong>Authors:Longhao Li, Zhao Guo, Hongjie Chen, Yuhang Dai, Ziyu Zhang, Hongfei Xue, Tianlun Zuo, Chengyou Wang, Shuiyuan Wang, Jie Li, Xin Xu, Hui Bu, Binbin Zhang, Ruibin Yuan, Ziya Zhou, Wei Xue, Lei Xie</strong></p>
<p>The development of speech understanding and generation has been significantly accelerated by the availability of large-scale, high-quality speech datasets. Among these, ASR and TTS are regarded as the most established and fundamental tasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9 million native speakers worldwide, limited annotated resources have hindered progress and resulted in suboptimal ASR and TTS performance. To address this challenge, we propose WenetSpeech-Pipe, an integrated pipeline for building large-scale speech corpus with multi-dimensional annotation tailored for speech understanding and generation. It comprises six modules: Audio Collection, Speaker Attributes Annotation, Speech Quality Annotation, Automatic Speech Recognition, Text Postprocessing and Recognizer Output Voting, enabling rich and high-quality annotations. Based on this pipeline, we release WenetSpeech-Yue, the first large-scale Cantonese speech corpus with multi-dimensional annotation for ASR and TTS, covering 21,800 hours across 10 domains with annotations including ASR transcription, text confidence, speaker identity, age, gender, speech quality scores, among other annotations. We also release WSYue-eval, a comprehensive Cantonese benchmark with two components: WSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long utterances, code-switching, and diverse acoustic conditions, and WSYue-TTS-eval, with base and coverage subsets for standard and generalization testing. Experimental results show that models trained on WenetSpeech-Yue achieve competitive results against state-of-the-art (SOTA) Cantonese ASR and TTS systems, including commercial and LLM-based models, highlighting the value of our dataset and pipeline. </p>
<blockquote>
<p>语音理解和生成的发展得益于大规模高质量语音数据集的可用性而得到显著加速。其中，语音识别和文本转语音被认为是最成熟、最基本的任务。然而，对于全球约有8490万使用人口的粤语（粤系中文），标注资源有限阻碍了其发展，导致语音识别和文本转语音的性能不尽如人意。为了解决这一挑战，我们提出了WenetSpeech-Pipe，这是一个为语音理解和生成量身定制的大规模语音语料库构建的综合管道，具有多维度注释。它包含六个模块：音频收集、说话人属性注释、语音质量注释、自动语音识别、文本后处理和识别输出投票，能够实现丰富且高质量注释。基于这一管道，我们发布了WenetSpeech-Yue，这是首个具有多维度注释的粤语语音识别和文本转语音大规模语音语料库，涵盖10个领域的21800小时音频，注释包括语音识别转录、文本置信度、说话人身份、年龄、性别、语音质量评分等。我们还发布了WSYue-eval，这是一个全面的粤语基准测试，包含两个组成部分：WSYue-ASR-eval，用于评估短句和长句、语言切换和各种声学条件下的语音识别；以及WSYue-TTS-eval，包含基本和覆盖子集用于标准测试和泛化测试。实验结果表明，在WenetSpeech-Yue上训练的模型与最新的粤语语音识别和文本转语音系统相比具有竞争力，包括商业模型和基于大型语言模型的模型，这凸显了我们数据集和管道的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03959v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>针对粤语（一种全球约有8490万母语者的语言），现有的语音理解和生成技术发展受限于标注资源的稀缺。为此，研究团队提出了WenetSpeech-Pipe这一集成管道，旨在构建大规模粤语语音语料库，并配备多维度标注，用于加速语音理解和生成的研究进展。该管道包含六个模块，能够实现丰富且高质量的标注。基于该管道，团队发布了首个大规模粤语语音语料库——WenetSpeech-Yue，包含超过21800小时的语音数据，覆盖十大领域，并配备多维度标注。同时发布的还有WSYue-eval基准测试集，用于评估粤语自动语音识别和文本转语音系统性能。实验结果表明，在WenetSpeech-Yue训练得到的模型表现具有竞争力，证明其价值。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>语音理解和生成的发展受益于大规模高质量语音数据集的可用性。</li>
<li>对于粤语（Yue Chinese），资源限制阻碍了语音理解和生成的进步。</li>
<li>WenetSpeech-Pipe是一个为粤语设计的集成管道，用于构建大规模语音语料库并提供多维度标注。</li>
<li>WenetSpeech-Yue是基于该管道的第一个大规模粤语语音语料库，包含丰富的标注数据。</li>
<li>WSYue-eval基准测试集用于评估粤语自动语音识别（ASR）和文本转语音（TTS）系统的性能。</li>
<li>实验证明在WenetSpeech-Yue训练的模型具有竞争力，体现了数据集的价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03959">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-62b44d42a53b42a895f853468d4d8f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d68bb830dc32bd1cafaf09ed5da72c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5287fc63fb61101fb566f76721a794e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08191cf1e6dd2e03e06c98efa514d89a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd6b2f995a79a167332bc1604a4ff929.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="NE-PADD-Leveraging-Named-Entity-Knowledge-for-Robust-Partial-Audio-Deepfake-Detection-via-Attention-Aggregation"><a href="#NE-PADD-Leveraging-Named-Entity-Knowledge-for-Robust-Partial-Audio-Deepfake-Detection-via-Attention-Aggregation" class="headerlink" title="NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio   Deepfake Detection via Attention Aggregation"></a>NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio   Deepfake Detection via Attention Aggregation</h2><p><strong>Authors:Huhong Xian, Rui Liu, Berrak Sisman, Haizhou Li</strong></p>
<p>Different from traditional sentence-level audio deepfake detection (ADD), partial audio deepfake detection (PADD) requires frame-level positioning of the location of fake speech. While some progress has been made in this area, leveraging semantic information from audio, especially named entities, remains an underexplored aspect. To this end, we propose NE-PADD, a novel method for Partial Audio Deepfake Detection (PADD) that leverages named entity knowledge through two parallel branches: Speech Name Entity Recognition (SpeechNER) and PADD. The approach incorporates two attention aggregation mechanisms: Attention Fusion (AF) for combining attention weights and Attention Transfer (AT) for guiding PADD with named entity semantics using an auxiliary loss. Built on the PartialSpoof-NER dataset, experiments show our method outperforms existing baselines, proving the effectiveness of integrating named entity knowledge in PADD. The code is available at <a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/NE-PADD">https://github.com/AI-S2-Lab/NE-PADD</a>. </p>
<blockquote>
<p>与传统的句子级音频深度伪造检测（ADD）不同，部分音频深度伪造检测（PADD）需要在帧级别定位虚假语音的位置。虽然该领域已经取得了一些进展，但利用音频中的语义信息，特别是命名实体，仍然是一个被忽视的方面。为此，我们提出了NE-PADD，这是一种利用命名实体知识的新型部分音频深度伪造检测方法（PADD）。该方法通过两个并行分支：语音名称实体识别（SpeechNER）和PADD来实现。该方法结合了两种注意力聚合机制：用于结合注意力权重的注意力融合（AF）和用于使用辅助损失引导PADD与命名实体语义的注意力转移（AT）。在PartialSpoof-NER数据集上进行的实验表明，我们的方法优于现有基线，证明了在PADD中整合命名实体知识的有效性。代码可在<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/NE-PADD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/NE-PADD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03829v1">PDF</a> </p>
<p><strong>Summary</strong><br>这篇文本提出了一种新型的基于命名实体知识的部分音频深度伪造检测（NE-PADD）方法。它采用两个并行分支进行语音名称实体识别（SpeechNER）和PADD，通过注意力融合和注意力转移机制整合命名实体知识，以提高检测性能。实验证明，该方法在PartialSpoof-NER数据集上的表现优于现有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NE-PADD是一种新型的部分音频深度伪造检测方法，通过利用命名实体知识来提高检测性能。</li>
<li>该方法采用两个并行分支：SpeechNER和PADD。</li>
<li>注意力融合（AF）和注意力转移（AT）机制被用于整合命名实体知识。</li>
<li>实验在PartialSpoof-NER数据集上进行，证明NE-PADD方法的性能优于现有基线。</li>
<li>NE-PADD方法可以有效地定位虚假语音的帧级别位置。</li>
<li>语义信息，特别是命名实体在信息检测中起到了重要作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03829">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f588b10342d72ecaf58cfc42a0fc7803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a18c36214766c252e17c60e0495e8177.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-104dc16f7d8b9d9ec8c2b0c96002244c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6e16547218f3228d59092970bf9e847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7fd05ed7d120e779d8fff4ebdcbc133.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06285d604239cf2e00f1a5c971d811e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-663f253d099dda553d2da78c506f3124.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Continuous-Saudi-Sign-Language-Recognition-A-Vision-Transformer-Approach"><a href="#Continuous-Saudi-Sign-Language-Recognition-A-Vision-Transformer-Approach" class="headerlink" title="Continuous Saudi Sign Language Recognition: A Vision Transformer   Approach"></a>Continuous Saudi Sign Language Recognition: A Vision Transformer   Approach</h2><p><strong>Authors:Soukeina Elhassen, Lama Al Khuzayem, Areej Alhothali, Ohoud Alzamzami, Nahed Alowaidi</strong></p>
<p>Sign language (SL) is an essential communication form for hearing-impaired and deaf people, enabling engagement within the broader society. Despite its significance, limited public awareness of SL often leads to inequitable access to educational and professional opportunities, thereby contributing to social exclusion, particularly in Saudi Arabia, where over 84,000 individuals depend on Saudi Sign Language (SSL) as their primary form of communication. Although certain technological approaches have helped to improve communication for individuals with hearing impairments, there continues to be an urgent requirement for more precise and dependable translation techniques, especially for Arabic sign language variants like SSL. Most state-of-the-art solutions have primarily focused on non-Arabic sign languages, resulting in a considerable absence of resources dedicated to Arabic sign language, specifically SSL. The complexity of the Arabic language and the prevalence of isolated sign language datasets that concentrate on individual words instead of continuous speech contribute to this issue. To address this gap, our research represents an important step in developing SSL resources. To address this, we introduce the first continuous Saudi Sign Language dataset called KAU-CSSL, focusing on complete sentences to facilitate further research and enable sophisticated recognition systems for SSL recognition and translation. Additionally, we propose a transformer-based model, utilizing a pretrained ResNet-18 for spatial feature extraction and a Transformer Encoder with Bidirectional LSTM for temporal dependencies, achieving 99.02% accuracy at signer dependent mode and 77.71% accuracy at signer independent mode. This development leads the way to not only improving communication tools for the SSL community but also making a substantial contribution to the wider field of sign language. </p>
<blockquote>
<p>手语（SL）是听障人士的一种重要的沟通方式，能够让他们融入更广阔的社会。尽管手语具有重要性，但公众对其的有限认知往往导致教育和职业机会的不公平分配，从而导致社会排斥，特别是在沙特阿拉伯。在沙特阿拉伯，有超过84，000人依赖沙特手语（SSL）作为他们的主要沟通方式。虽然某些技术方法已经帮助听障人士改善了沟通，但仍然存在对更精确和可靠的翻译技术的迫切需求，特别是对于像SSL这样的阿拉伯语手语变体。大多数最新解决方案主要关注非阿拉伯语手语，导致缺乏专门针对阿拉伯语手语的资源。阿拉伯语语言的复杂性以及孤立的手语数据集普遍关注单个单词而不是连续语是导致这一问题的原因之一。为了解决这个问题，我们的研究在开发SSL资源方面迈出了重要的一步。为了解决这个问题，我们推出了第一个连续的沙特手语数据集，名为KAU-CSSL，专注于完整的句子，以促进进一步的研究并启用SSL识别和翻译的先进识别系统。此外，我们提出了一个基于transformer的模型，利用预训练的ResNet-18进行空间特征提取和带有双向LSTM的Transformer编码器处理时间依赖性，在签约人依赖模式下达到99.02％的准确率，签约人独立模式下达到77.71％的准确率。这一发展不仅为SSL社区改善了沟通工具，而且为更广泛的手语领域做出了重大贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03467v1">PDF</a> 23 pages, 13 figures, 5 tables</p>
<p><strong>摘要</strong></p>
<p>手势语言（SL）对于听障人士至关重要，使其能够在更广泛的社会中参与交流。然而，公众对SL的认识有限，导致听障人士在教育及职业机会方面遭受不公平待遇，加剧了社会排斥现象。在沙特阿拉伯等国家，超过84,000人依赖阿拉伯语手语（SSL）作为主要交流方式，因此更需关注阿拉伯语手语的精确翻译技术。当前主流解决方案主要关注非阿拉伯语手语，缺乏针对阿拉伯语手语的资源，尤其是SSL的资源。我们的研究致力于开发SSL资源，填补了这一空白。为此，我们推出了首个连续的沙特阿拉伯手语数据集KAU-CSSL，专注于完整的句子，以促进进一步的研究，并为SSL识别和翻译启用高级识别系统。此外，我们提出了一个基于Transformer的模型，利用预训练的ResNet-18进行空间特征提取和Transformer Encoder与双向LSTM处理时序依赖关系，在签名者依赖模式下达到了99.02%的准确率，在签名者独立模式下达到了77.71%的准确率。这不仅为SSL社区改进交流工具铺平了道路，而且为更广泛的符号语言领域做出了重大贡献。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>手势语言（SL）是听障人士的重要沟通方式，但公众对其认识有限，导致社会机会的不公平分配。</li>
<li>在沙特阿拉伯，超过84,000人依赖阿拉伯语手语（SSL）作为主要交流手段，凸显了对精确翻译技术的迫切需求。</li>
<li>当前的技术解决方案主要关注非阿拉伯语手语，导致阿拉伯语手语资源匮乏，尤其是SSL资源。</li>
<li>推出首个连续的沙特阿拉伯手语数据集KAU-CSSL，专注于完整的句子，以促进SSL研究。</li>
<li>提出的基于Transformer的模型结合了空间特征提取和时序依赖处理，表现出高准确率。</li>
<li>研究成果为听障人士改进交流工具铺平了道路，特别是在SSL社区。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03467">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-24feeaecabc76ac9906185bf0e998888.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad87b00d0cc82c0e4cb3bf3255c10a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cee333334c9458fb131b826e58afd3cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efe927fc7dcd9e36b9cf5cfa5ce9f8d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c0828ce638aade094a1bf98868a197b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb7a2044ccd753d6019f5c53c362fe4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Exploring-persuasive-Interactions-with-generative-social-robots-An-experimental-framework"><a href="#Exploring-persuasive-Interactions-with-generative-social-robots-An-experimental-framework" class="headerlink" title="Exploring persuasive Interactions with generative social robots: An   experimental framework"></a>Exploring persuasive Interactions with generative social robots: An   experimental framework</h2><p><strong>Authors:Stephan Vonschallen, Larissa Julia Corina Finsler, Theresa Schmiedel, Friederike Eyssel</strong></p>
<p>Integrating generative AI such as large language models into social robots has improved their ability to engage in natural, human-like communication. This study presents a method to examine their persuasive capabilities. We designed an experimental framework focused on decision making and tested it in a pilot that varied robot appearance and self-knowledge. Using qualitative analysis, we evaluated interaction quality, persuasion effectiveness, and the robot’s communicative strategies. Participants generally experienced the interaction positively, describing the robot as competent, friendly, and supportive, while noting practical limits such as delayed responses and occasional speech-recognition errors. Persuasiveness was highly context dependent and shaped by robot behavior: participants responded well to polite, reasoned suggestions and expressive gestures, but emphasized the need for more personalized, context-aware arguments and clearer social roles. These findings suggest that generative social robots can influence user decisions, but their effectiveness depends on communicative nuance and contextual relevance. We propose refinements to the framework to further study persuasive dynamics between robots and human users. </p>
<blockquote>
<p>将生成式人工智能（如大型语言模型）整合到社会机器人中，增强了它们进行自然、拟人化通信的能力。本研究提出了一种方法来检验它们的劝说能力。我们设计了一个以决策制定为重点的实验框架，并在机器人外观和自我知识各异的试点中进行了测试。通过定性分析，我们评估了交互质量、劝说有效性和机器人的通信策略。参与者普遍对交互体验持积极态度，认为机器人有能力、友好、支持，同时注意到实际限制，如响应延迟和偶尔的语音识别错误。劝说性高度依赖于上下文，受机器人行为影响：参与者对礼貌、合理的建议和表达性手势反应良好，但强调需要更多个性化、基于上下文的论证和更明确的社会角色。这些结果表明，生成式社会机器人可以影响用户的决策，但其有效性取决于通信的细微差别和上下文相关性。我们提议对框架进行改进，以进一步研究机器人和人类用户之间的劝说动态。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03231v1">PDF</a> A shortened version of this paper was accepted as poster for the   Thirteenth International Conference on Human-Agent Interaction (HAI2025)</p>
<p><strong>摘要</strong></p>
<p>集成大型语言模型等生成式人工智能的社会机器人，提升了其进行自然、人类般交流的能力。本研究提出了一种方法来研究他们的说服力。我们设计了一个以决策制定为重点的实验框架，并在试点中测试了不同机器人外观和自我认知的影响。通过定性分析，我们评估了互动质量、说服效果和机器人的沟通策略。参与者普遍对互动体验持积极态度，认为机器人有能力、友好和支持性，同时也注意到一些实际限制，如反应延迟和偶尔的语音识别错误。说服力在很大程度上取决于上下文和机器人行为：参与者对礼貌、理性的建议和表达性手势反应良好，但强调需要更多个性化、上下文感知的论证和明确的社会角色。这些发现表明，生成的社会机器人可以影响用户的决策，但其有效性取决于沟通的细微差别和上下文的相关性。我们提出了对该框架的改进，以进一步研究机器人和人类用户之间的说服动态。</p>
<p><strong>要点</strong></p>
<ol>
<li>生成式AI增强了社会机器人的自然交流能力，使其更类似于人类交流。</li>
<li>通过实验框架评估了机器人在决策场景中的说服力。</li>
<li>参与者对机器人互动持积极态度，认为机器人具有能力和友好性，但也指出了实际限制。</li>
<li>说服力受上下文和机器人行为影响，需要机器人具备礼貌、理性的建议和表达性手势等沟通技巧。</li>
<li>参与者强调需要更多个性化、上下文感知的论证和明确的社会角色。</li>
<li>社会机器人的有效性在于其沟通的细微差别和上下文相关性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03231">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3ec46c21269250573d5761b9fb7b8254.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a672fc17df5417291900bb15bb34097.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Speech-DF-Arena-A-Leaderboard-for-Speech-DeepFake-Detection-Models"><a href="#Speech-DF-Arena-A-Leaderboard-for-Speech-DeepFake-Detection-Models" class="headerlink" title="Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models"></a>Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models</h2><p><strong>Authors:Sandipana Dowerah, Atharva Kulkarni, Ajinkya Kulkarni, Hoan My Tran, Joonas Kalda, Artem Fedorchenko, Benoit Fauve, Damien Lolive, Tanel Alumäe, Matthew Magimai Doss</strong></p>
<p>Parallel to the development of advanced deepfake audio generation, audio deepfake detection has also seen significant progress. However, a standardized and comprehensive benchmark is still missing. To address this, we introduce Speech DeepFake (DF) Arena, the first comprehensive benchmark for audio deepfake detection. Speech DF Arena provides a toolkit to uniformly evaluate detection systems, currently across 14 diverse datasets and attack scenarios, standardized evaluation metrics and protocols for reproducibility and transparency. It also includes a leaderboard to compare and rank the systems to help researchers and developers enhance their reliability and robustness. We include 14 evaluation sets, 12 state-of-the-art open-source and 3 proprietary detection systems. Our study presents many systems exhibiting high EER in out-of-domain scenarios, highlighting the need for extensive cross-domain evaluation. The leaderboard is hosted on Huggingface1 and a toolkit for reproducing results across the listed datasets is available on GitHub. </p>
<blockquote>
<p>随着先进的深度伪造音频生成技术的发展，音频深度伪造检测也取得了显著进展。然而，目前仍缺乏标准化和全面的基准测试。为了解决这一问题，我们推出了语音深度伪造（DF）竞技场，这是音频深度伪造检测的首个全面基准测试。语音DF竞技场提供了一个工具包，用于统一评估检测系统，目前涵盖14个不同的数据集和攻击场景，提供可重复性和透明度的标准化评估指标和协议。它还包括一个排行榜，用于比较和排名系统，帮助研究者和开发人员提高系统的可靠性和稳健性。我们包含了14个评估集，包括最新的、开源的检测系统中有主流的音频生成对抗模型的攻击场景数据集共包含有12个开源数据集和3个专有检测系统。我们的研究表明，在跨域场景中有很多系统的等错误率（EER）很高，这凸显了需要进行广泛跨域评估的必要性。排行榜托管在Huggingface上，一个用于在所列数据集上复制结果的工具包可在GitHub上获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02859v1">PDF</a> </p>
<p><strong>Summary</strong>：随着深度伪造音频技术的发展，音频深度伪造检测也取得了重要进展，但仍缺少标准化的全面基准。因此，我们引入了Speech DeepFake Arena，这是一个全面的音频深度伪造检测基准。它提供了一个工具包，可以统一评估检测系统，包括14个不同的数据集和攻击场景，标准化的评估指标和协议，以提高可重复性和透明度。此外，还包括一个排行榜，可帮助研究人员和开发人员提高系统的可靠性和鲁棒性。研究发现在跨域场景中存在高误识率，强调需要进行广泛的跨域评估。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Speech DeepFake Arena是首个全面的音频深度伪造检测基准，填补了该领域的标准化缺失。</li>
<li>提供了一个工具包，用于统一评估检测系统，涵盖多种数据集和攻击场景。</li>
<li>提供了标准化的评估指标和协议，以提高研究的可重复性和透明度。</li>
<li>包含一个排行榜，以比较和排名检测系统，帮助提高其可靠性和鲁棒性。</li>
<li>研究发现跨域场景中系统表现出较高的误识率，强调需要更多跨域评估。</li>
<li>Speech DeepFake Arena在Huggingface上托管了排行榜。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02859">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-31fd0a69f31843e975babb415e6d6f2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d221b8ffaf3f74db6ec7378b31b077bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15c35bf7485e44e3d79490e9ebda30f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35301340ab54f63372a1955149151b6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38d84b3d143aad2b619b416552d2cb05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21db6bf92b57650068cadde02fe91312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de7ed38807a2fd9dabb8e62e6c61783c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SSVD-Structured-SVD-for-Parameter-Efficient-Fine-Tuning-and-Benchmarking-under-Domain-Shift-in-ASR"><a href="#SSVD-Structured-SVD-for-Parameter-Efficient-Fine-Tuning-and-Benchmarking-under-Domain-Shift-in-ASR" class="headerlink" title="SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and   Benchmarking under Domain Shift in ASR"></a>SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and   Benchmarking under Domain Shift in ASR</h2><p><strong>Authors:Pu Wang, Shinji Watanabe, Hugo Van hamme</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for adapting large foundation models. While low-rank adaptation (LoRA) is widely used in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA, PiSSA, and SVFT, are developed mainly for language and vision tasks, with limited validation in speech. This work presents the first comprehensive integration and benchmarking of these PEFT methods within ESPnet. We further introduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates input-associated right singular vectors while keeping output-associated vectors fixed to preserve semantic mappings. This design enables robust domain adaptation with minimal trainable parameters and improved efficiency. We evaluate all methods on domain-shifted speech recognition tasks, including child speech and dialectal variation, across model scales from 0.1B to 2B. All implementations are released in ESPnet to support reproducibility and future work. </p>
<blockquote>
<p>参数高效微调（PEFT）已成为适应大型基础模型的可扩展解决方案。虽然低秩适应（LoRA）在语音应用中广泛使用，但其最新变体，例如VeRA、DoRA、PiSSA和SVFT，主要针对语言和视觉任务开发，在语音方面的验证有限。本研究首次在ESPnet中全面集成和评估了这些PEFT方法。我们还引入了结构化SVD引导（SSVD）微调，它选择性地旋转与输入相关的右奇异向量，同时保持与输出相关的向量固定，以保留语义映射。这种设计能够在保持可训练参数最少的情况下，实现稳健的域适应，提高效率。我们在模型规模从0.1B到2B的情况下，对所有方法在域转移的语音识别任务中进行了评估，包括儿童语音和方言变化。所有实现都在ESPnet中发布，以支持可重复性和未来的工作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02830v1">PDF</a> Accepted by IEEE ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了参数高效微调（PEFT）在语音应用中的首次全面集成和基准测试。文章重点介绍了低秩适应（LoRA）及其最新变种（如VeRA、DoRA、PiSSA和SVFT），并首次将其应用于ESPnet中的语音任务。此外，文章还提出了结构化的SVD引导（SSVD）微调方法，该方法能够选择性地旋转输入相关的右奇异向量，同时保持输出相关向量不变，以保留语义映射，实现具有较少可训练参数的稳健域适应并提高了效率。在所有方法上，作者在儿童语音和方言变化等跨模型规模的领域迁移语音识别任务上进行了评估。所有实现均已发布在ESPnet上，以支持可重复性和未来的工作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>参数高效微调（PEFT）已成为适应大型基础模型的可扩展解决方案。</li>
<li>低秩适应（LoRA）及其最新变种在语音任务中的应用尚待充分验证。</li>
<li>本文首次全面集成和基准测试了PEFT方法在ESPnet中的语音应用。</li>
<li>提出了结构化的SVD引导（SSVD）微调方法，通过选择性地旋转输入相关的右奇异向量，实现稳健的域适应并提高了效率。</li>
<li>所有方法在儿童语音和方言变化的领域迁移语音识别任务上进行了评估。</li>
<li>所有方法的实现均已发布在ESPnet上，以支持研究的可重复性和未来工作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02830">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5879b29de518afcf0da654365465fea2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d42725ec401cb09217c70792145c21e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36ac551726f21d8fb8366ec9f45fc734.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36da36ed6a4303c6652db3ce90034b25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95b45aa563750c5d6bc0b20a41396d75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-191cb3b74357e796dc38e1621f0858e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7cadc93bbbd6e58a39d0ab254141bfbf.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Flavors-of-Moonshine-Tiny-Specialized-ASR-Models-for-Edge-Devices"><a href="#Flavors-of-Moonshine-Tiny-Specialized-ASR-Models-for-Edge-Devices" class="headerlink" title="Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices"></a>Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices</h2><p><strong>Authors:Evan King, Adam Sabra, Manjunath Kudlur, James Wang, Pete Warden</strong></p>
<p>We present the Flavors of Moonshine, a suite of tiny automatic speech recognition (ASR) models specialized for a range of underrepresented languages. Prevailing wisdom suggests that multilingual ASR models outperform monolingual counterparts by exploiting cross-lingual phonetic similarities. We challenge this assumption, showing that for sufficiently small models (27M parameters), training monolingual systems on a carefully balanced mix of high-quality human-labeled, pseudo-labeled, and synthetic data yields substantially superior performance. On average, our models achieve error rates 48% lower than the comparably sized Whisper Tiny model, outperform the 9x larger Whisper Small model, and in most cases match or outperform the 28x larger Whisper Medium model. These results advance the state of the art for models of this size, enabling accurate on-device ASR for languages that previously had limited support. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and Vietnamese Moonshine models under a permissive open-source license. </p>
<blockquote>
<p>我们推出了“月光风味”系列，这是一套专为一系列代表性不足的语种设计的超小型自动语音识别（ASR）模型。普遍的看法是，跨语种语音相似性的利用使得多语种ASR模型的性能优于单语种模型。我们对此假设提出质疑，并证明对于足够小的模型（27M参数），在高质量人工标注、伪标注和合成数据的平衡混合上训练单语种系统会产生显著优越的性能。平均而言，我们的模型错误率比同类大小的whisper tiny模型低48%，在大多数情况下与whisper medium模型相当或更优，甚至超过了9倍更大的whisper small模型。这些结果代表了这一规模的模型的最新技术，使得之前支持有限的语言的设备端ASR更加准确。我们发布了阿拉伯语、中文、日语、韩语、乌克兰语和越南语的月光风味模型，并采用了宽松的开源许可协议。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02523v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Flavors of Moonshine——一套专为多种代表性不足的语种设计的自动语音识别（ASR）模型套件。研究表明，对于足够小的模型（如具有27M参数的模型），在高质量人工标注、伪标注和合成数据的平衡混合上训练单语种系统，可以获得优于跨语种模型的性能表现。这些模型在错误率上平均降低了48%，超过了类似规模的Whisper Tiny模型，并在多数情况下与或超越了28倍大的Whisper Medium模型。这一成果推进了小规模模型的技术前沿，为之前支持有限的语种提供了准确的设备端ASR。现已发布包括阿拉伯语、中文、日语、韩语、乌克兰语和越南语在内的Moonshine模型，并采用了宽松的开源许可。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Flavors of Moonshine是一系列专为多种代表性不足的语种设计的自动语音识别（ASR）模型。</li>
<li>研究挑战了现有的假设，即在某些小规模模型中，单语种ASR模型的性能表现可能优于跨语种模型。</li>
<li>训练数据包括高质量人工标注、伪标注和合成数据的平衡混合。</li>
<li>模型性能显著提升，错误率平均降低了48%，超过了相同规模的Whisper Tiny模型和更大的Whisper Small与Medium模型。</li>
<li>该成果推进了小规模ASR模型的技术发展，并为之前支持有限的语种提供了更准确的解决方案。</li>
<li>模型已在多种语言上进行了测试，包括阿拉伯语、中文、日语、韩语、乌克兰语和越南语。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02523">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bd3af707feec7aa5473fffa6d6ab7150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-714c81b485b348ddd28c3d225b405537.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1095a6408d899d58c1892a23f821f971.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08f5cfe6549ced8d13100e03a66cc414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9fe28988ac11fb64069af1380bd0f64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-345b5ad4fb5b0a1a40f688da6f12089f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57e6b6632a2275c246c1c317eaaef9c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e01bf397d18b1ee3ae787f7d2a005050.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="NADI-2025-The-First-Multidialectal-Arabic-Speech-Processing-Shared-Task"><a href="#NADI-2025-The-First-Multidialectal-Arabic-Speech-Processing-Shared-Task" class="headerlink" title="NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task"></a>NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task</h2><p><strong>Authors:Bashar Talafha, Hawau Olamide Toyin, Peter Sullivan, AbdelRahim Elmadany, Abdurrahman Juma, Amirbek Djanibekov, Chiyu Zhang, Hamad Alshehhi, Hanan Aldarmaki, Mustafa Jarrar, Nizar Habash, Muhammad Abdul-Mageed</strong></p>
<p>We present the findings of the sixth Nuanced Arabic Dialect Identification (NADI 2025) Shared Task, which focused on Arabic speech dialect processing across three subtasks: spoken dialect identification (Subtask 1), speech recognition (Subtask 2), and diacritic restoration for spoken dialects (Subtask 3). A total of 44 teams registered, and during the testing phase, 100 valid submissions were received from eight unique teams. The distribution was as follows: 34 submissions for Subtask 1 “five teams{\ae}, 47 submissions for Subtask 2 “six teams”, and 19 submissions for Subtask 3 “two teams”. The best-performing systems achieved 79.8% accuracy on Subtask 1, 35.68&#x2F;12.20 WER&#x2F;CER (overall average) on Subtask 2, and 55&#x2F;13 WER&#x2F;CER on Subtask 3. These results highlight the ongoing challenges of Arabic dialect speech processing, particularly in dialect identification, recognition, and diacritic restoration. We also summarize the methods adopted by participating teams and briefly outline directions for future editions of NADI. </p>
<blockquote>
<p>我们呈现第六次委婉阿拉伯语方言识别（NADI 2025）共享任务的发现结果，该任务聚焦于三个子任务的阿拉伯语语音方言处理：口语方言识别（子任务1）、语音识别（子任务2）以及口语方言的变音符恢复（子任务3）。共有44支队伍注册参与，测试阶段共收到来自8支不同队伍的100份有效提交。分布如下：子任务1有34份提交（“五支队伍”），子任务2有47份提交（“六支队伍”），子任务3有19份提交（“两支队伍”）。表现最佳的系统的准确率在子任务1上达到79.8%，子任务2的整体平均WER&#x2F;CER为35.68&#x2F;12.20，子任务3的WER&#x2F;CER为55&#x2F;13。这些结果突显了阿拉伯语方言语音处理，尤其是方言识别、识别和变音符恢复的持续挑战。我们还总结了参与团队采用的方法，并简要概述了未来NADI版本的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02038v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>阿拉伯语音方言处理的最新研究成果公布，涉及方言识别、语音识别和方言变音恢复三个子任务。共有44支队伍参与，最终有八支队伍提交有效成果。最佳系统的表现显示，方言识别准确率为79.8%，语音识别平均WER&#x2F;CER为35.68&#x2F;12.20，方言变音恢复平均WER&#x2F;CER为55&#x2F;13。这些结果突显了阿拉伯方言语音处理的挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NADI 2025共享任务的目的是研究阿拉伯语音方言处理。</li>
<li>任务包括三个子任务：方言识别、语音识别和方言变音恢复。</li>
<li>共有44支队伍参与，测试阶段有八支队伍提交有效成果。</li>
<li>最佳系统在方言识别上达到79.8%准确率，在语音识别方面，平均WER&#x2F;CER为35.68&#x2F;12.20，方言变音恢复平均为WER&#x2F;CER为55&#x2F;13。这些结果显示了不小的挑战和进步空间。 </li>
<li>这些成果提供了对阿拉伯方言语音处理的深入了解。 </li>
<li>参与团队采用的方法总结为未来研究提供了方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02038">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6225d62dad8397935288da3cac838d24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-145288a9c9c3e42c22b2d63b0e23f07d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f9d710042804c86e9885f053953813e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8a548b15de93802543d082d69c2ab2f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Group-Relative-Policy-Optimization-for-Speech-Recognition"><a href="#Group-Relative-Policy-Optimization-for-Speech-Recognition" class="headerlink" title="Group Relative Policy Optimization for Speech Recognition"></a>Group Relative Policy Optimization for Speech Recognition</h2><p><strong>Authors:Prashanth Gurunath Shivakumar, Yile Gu, Ankur Gandhe, Ivan Bulyko</strong></p>
<p>Speech Recognition has seen a dramatic shift towards adopting Large Language Models (LLMs). This shift is partly driven by good scalability properties demonstrated by LLMs, ability to leverage large amounts of labelled, unlabelled speech and text data, streaming capabilities with auto-regressive framework and multi-tasking with instruction following characteristics of LLMs. However, simple next-token prediction objective, typically employed with LLMs, have certain limitations in performance and challenges with hallucinations. In this paper, we propose application of Group Relative Policy Optimization (GRPO) to enable reinforcement learning from human feedback for automatic speech recognition (ASR). We design simple rule based reward functions to guide the policy updates. We demonstrate significant improvements in word error rate (upto 18.4% relative), reduction in hallucinations, increased robustness on out-of-domain datasets and effectiveness in domain adaptation. </p>
<blockquote>
<p>语音识别领域已经发生了向采用大型语言模型（LLM）的重大转变。这一转变部分是由LLM展示的良好的可扩展性、利用大量有标签和无标签的语音和文本数据的能力、与自回归框架的流式传输能力以及LLM的遵循指令的多任务处理特性所驱动的。然而，通常与LLM一起使用的简单下一个令牌预测目标在性能和幻视方面具有一定的局限性。在本文中，我们提出了应用组相对策略优化（GRPO）来实现基于人类反馈的强化学习，用于自动语音识别（ASR）。我们设计了基于简单规则的奖励函数来指导策略更新。我们在词错误率方面取得了显著改进（相对降低了18.4%），幻视减少，对域外数据集的稳健性增强，以及域适应的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01939v1">PDF</a> Accepted for ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的语音识别技术在诸多领域表现出优越性能，但其性能表现仍然受限于单一的应用模型和算法的局限性和挑战。为了改善性能和提高适应能力，本研究将引入Group Relative Policy Optimization（GRPO）算法和强化学习技术，旨在通过人类反馈进行自动语音识别（ASR）。本研究通过设计基于规则的奖励函数来引导策略更新，并在实验中实现了显著的改进，如词错误率降低了高达百分之十八点四的相对误差，减少“假想”（hallucinations）现象，增强了在领域外的数据集上的稳健性，以及有效的领域适应性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）已成为语音识别领域的重要技术趋势。</li>
<li>LLM具备可扩展性、利用大量标签和无标签语音和文本数据的能力、流式处理和多任务处理能力等关键优势。</li>
<li>简单下一个词预测目标（next-token prediction objective）在性能和假想现象方面存在局限性。</li>
<li>本研究引入了Group Relative Policy Optimization（GRPO）算法以改善语音识别性能。</li>
<li>通过设计基于规则的奖励函数来引导策略更新，显著提高了性能。</li>
<li>实验结果显示，GRPO在词错误率、假想现象、稳健性和领域适应性方面取得了显著改进。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01939">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8fbe04b4e481d23ff04ec4b859fa6fce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4bb96b44c84b17fb927377e80c9991bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb27fd8bb207a547872b6d833a1eb9d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-964c41566b5dcf22d5252a7f4f2ffd20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd071b593e87409c657b433a0ba57ab7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multilingual-Speech-Recognition-Using-Discrete-Tokens-with-a-Two-step-Training-Strategy"><a href="#Multilingual-Speech-Recognition-Using-Discrete-Tokens-with-a-Two-step-Training-Strategy" class="headerlink" title="Multilingual Speech Recognition Using Discrete Tokens with a Two-step   Training Strategy"></a>Multilingual Speech Recognition Using Discrete Tokens with a Two-step   Training Strategy</h2><p><strong>Authors:Zehan Li, Yan Yang, Xueqing Li, Jian Kang, Xiao-Lei Zhang, Jie Li</strong></p>
<p>Pre-trained models, especially self-supervised learning (SSL) models, have demonstrated impressive results in automatic speech recognition (ASR) task. While most applications of SSL models focus on leveraging continuous representations as features for training downstream tasks, the utilization of discrete units has gained increasing attention in recent years owing to its lower storage requirements and broader range of applications. In multilingual ASR tasks, representations at different layers of the model contribute differently to various languages, complicating the unification of discrete unit modeling. In this paper, we propose a two-stage training strategy to improve the discrete token performance of pre-trained models and narrow the gap with continuous representation performance. We validate our method on the XLS-R model following the settings of Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge. Our method demonstrates a significant improvement on the ML-SUPERB dataset, achieving a 44% relative reduction on CER for the XLS-R model. This surpasses the previous baseline set by the WavLM model, which achieves a 26% relative reduction on CER. Furthermore, our method achieves the first place among all the single-system results on the leaderboard. </p>
<blockquote>
<p>预训练模型，特别是自监督学习（SSL）模型，在自动语音识别（ASR）任务中取得了令人印象深刻的结果。虽然大多数SSL模型的应用主要集中在利用连续表示为下游任务的特征，但由于其较低的存储需求和更广泛的应用范围，离散单元的使用近年来受到了越来越多的关注。在多语种ASR任务中，模型不同层级的表示对不同语言的影响不同，这使得离散单元建模的统一变得复杂。在本文中，我们提出了一种两阶段训练策略，以提高预训练模型的离散令牌性能，并缩小与连续表示性能的差距。我们在遵循Interspeech2024使用离散语音单元挑战设置的条件下，对XLS-R模型进行了方法验证。我们的方法在ML-SUPERB数据集上实现了显著改进，相对误差率（CER）降低了44%，超越了之前由WavLM模型设定的基线，后者实现了26%的CER相对降低。此外，我们的方法在排行榜上获得了单系统第一名。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01900v1">PDF</a> Accepted by NCMMSC 2024</p>
<p><strong>Summary</strong></p>
<p>预训练模型，特别是自监督学习模型，在自动语音识别任务中取得了显著成果。本文关注自监督学习模型在离散单元建模方面的应用，提出一种两阶段训练策略来提升预训练模型的离散令牌性能，并缩小其与连续表示性能之间的差距。在Multilingual ASR任务中，模型不同层对不同语言的表示贡献不同，我们通过在XLS-R模型上验证该方法，并在Interspeech2024离散语音单元挑战的设置中取得显著成果。我们的方法在ML-SUPERB数据集上实现了44%的相对错误率降低，超越了WavLM模型的基线表现，同时在排行榜上获得了第一名。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自监督学习模型在自动语音识别任务中表现优异。</li>
<li>离散单元建模在近年来受到越来越多的关注，因其具有较低的存储需求和更广泛的应用范围。</li>
<li>在多语种自动语音识别任务中，模型不同层对不同语言的表示贡献不同。</li>
<li>提出的两阶段训练策略能提升预训练模型的离散令牌性能。</li>
<li>该方法在XLS-R模型上进行了验证，并在Interspeech2024挑战中取得显著成果。</li>
<li>方法在ML-SUPERB数据集上实现了44%的相对错误率降低，超越了WavLM模型的基线表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01900">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ec7d24b8e42865dfe831dd14f8c2d936.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70e5a92c17972d7a166d348d3adecddf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-619cccd67b18bf0775cc359b651d2650.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9468f9061cd8e9df8a9c5da04386f907.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629fa0dd87a6aa9b7b1a3d7afe12c4d0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Characterization-of-Speech-Similarity-Between-Australian-Aboriginal-and-High-Resource-Languages-A-Case-Study-on-Dharawal"><a href="#Characterization-of-Speech-Similarity-Between-Australian-Aboriginal-and-High-Resource-Languages-A-Case-Study-on-Dharawal" class="headerlink" title="Characterization of Speech Similarity Between Australian Aboriginal and   High-Resource Languages: A Case Study on Dharawal"></a>Characterization of Speech Similarity Between Australian Aboriginal and   High-Resource Languages: A Case Study on Dharawal</h2><p><strong>Authors:Ting Dang, Trini Manoj Jeyaseelan, Eliathamby Ambikairajah, Vidhyasaharan Sethu</strong></p>
<p>Australian Aboriginal languages are of significant cultural and linguistic value but remain severely underrepresented in modern speech AI systems. While state-of-the-art speech foundation models and automatic speech recognition excel in high-resource settings, they often struggle to generalize to low-resource languages, especially those lacking clean, annotated speech data. In this work, we collect and clean a speech dataset for Dharawal, a low-resource Australian Aboriginal language, by carefully sourcing and processing publicly available recordings. Using this dataset, we analyze the speech similarity between Dharawal and 107 high-resource languages using a pre-trained multilingual speech encoder. Our approach combines (1) misclassification rate analysis to assess language confusability, and (2) fine-grained similarity measurements using cosine similarity and Fr&#39;echet Inception Distance (FID) in the embedding space. Experimental results reveal that Dharawal shares strong speech similarity with languages such as Latin, M=aori, Korean, Thai, and Welsh. These findings offer practical guidance for future transfer learning and model adaptation efforts, and underscore the importance of data collection and embedding-based analysis in supporting speech technologies for endangered language communities. </p>
<blockquote>
<p>澳大利亚原住民语言在文化及语言上具有重大价值，但在现代语音人工智能系统中却严重缺乏代表性。虽然最先进的语音基础模型和自动语音识别技术在资源丰富的情况下表现出色，但它们往往难以推广到资源匮乏的语言，尤其是那些缺乏干净、注释的语音数据。在这项工作中，我们通过仔细搜索和处理公开可用的录音，收集和清理了一个用于达拉瓦（一种资源匮乏的澳大利亚原住民语言）的语音数据集。使用该数据集，我们使用预训练的多语言语音编码器分析达拉瓦语与107种资源丰富语言的语音相似性。我们的方法结合了（1）通过误分类率分析来评估语言混淆性，（2）使用余弦相似度和Fr’echet Inception Distance（FID）在嵌入空间中进行精细相似性测量。实验结果表明，达拉瓦语与拉丁语、毛利语、韩语、泰语和威尔士语等语言有很强的语音相似性。这些发现为未来迁移学习和模型适应工作提供了实际指导，并强调了数据采集和基于嵌入的分析对于支持濒危语言社区的语音技术的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01419v1">PDF</a> Accepted at APSIPA ASC 2025</p>
<p><strong>总结</strong></p>
<p>针对澳大利亚原住民语言在现代语音识别系统中的严重缺乏代表性问题，本文收集并清理了Dharawal（一种低资源的澳大利亚原住民语言）的语音数据集。通过预训练的多语言语音编码器，对Dharawal与107种高资源语言之间的语音相似性进行了分析。研究方法包括误分类率分析来评估语言混淆性，以及使用余弦相似度和Fr’echet Inception Distance（FID）进行精细相似性测量。实验结果显示，Dharawal与拉丁、毛利、韩语、泰语和威尔士语等语言具有较强的语音相似性。这为未来的迁移学习和模型适应工作提供了实际指导，并强调了数据收集和基于嵌入的分析对于支持濒危语言社区的语音识别技术的重要性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>澳大利亚原住民语言在现代语音识别系统中严重缺乏代表性。</li>
<li>Dharawal语言是一种低资源的澳大利亚原住民语言，其语音数据集被收集并清理。</li>
<li>通过预训练的多语言语音编码器，对Dharawal与多种高资源语言进行了语音相似性分析。</li>
<li>误分类率分析用于评估语言的混淆性。</li>
<li>通过余弦相似度和Fr’echet Inception Distance（FID）进行了精细的语音相似性测量。</li>
<li>实验结果显示，Dharawal与某些语言（如拉丁、毛利、韩语、泰语和威尔士语）有强语音相似性。</li>
<li>这为迁移学习和模型适应提供了指导，并强调了数据收集和基于嵌入的分析在支持濒危语言社区的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-997a5163ab5db2be4dfb0136959e09eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2aa54f0780a289eb69809b37e7e64ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8628c58167f1b9641ee7e7a7989c3604.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c7f60dd295e694b54fb2e729fd3d2c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80ed81d24ce56c369ce927a6af622d18.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ArabEmoNet-A-Lightweight-Hybrid-2D-CNN-BiLSTM-Model-with-Attention-for-Robust-Arabic-Speech-Emotion-Recognition"><a href="#ArabEmoNet-A-Lightweight-Hybrid-2D-CNN-BiLSTM-Model-with-Attention-for-Robust-Arabic-Speech-Emotion-Recognition" class="headerlink" title="ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for   Robust Arabic Speech Emotion Recognition"></a>ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for   Robust Arabic Speech Emotion Recognition</h2><p><strong>Authors:Ali Abouzeid, Bilal Elbouardi, Mohamed Maged, Shady Shehata</strong></p>
<p>Speech emotion recognition is vital for human-computer interaction, particularly for low-resource languages like Arabic, which face challenges due to limited data and research. We introduce ArabEmoNet, a lightweight architecture designed to overcome these limitations and deliver state-of-the-art performance. Unlike previous systems relying on discrete MFCC features and 1D convolutions, which miss nuanced spectro-temporal patterns, ArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving critical emotional cues often lost in traditional methods.   While recent models favor large-scale architectures with millions of parameters, ArabEmoNet achieves superior results with just 1 million parameters, 90 times smaller than HuBERT base and 74 times smaller than Whisper. This efficiency makes it ideal for resource-constrained environments. ArabEmoNet advances Arabic speech emotion recognition, offering exceptional performance and accessibility for real-world applications. </p>
<blockquote>
<p>语音情绪识别对于人机交互至关重要，特别是对于像阿拉伯语这样的低资源语言来说，由于数据和研究的局限性，它们面临着诸多挑战。我们推出了ArabEmoNet，这是一个轻量级架构，旨在克服这些限制并带来最先进的性能。与以往依赖于离散MFCC特征和1D卷积的系统不同，这些系统会忽略细微的谱时模式，ArabEmoNet使用通过2D卷积处理的梅尔频谱图，保留了传统方法中经常丢失的关键情感线索。虽然最近的模型倾向于采用具有数百万参数的大规模架构，但ArabEmoNet仅使用100万个参数就取得了优越的结果，比HuBERT基础版本小90倍，比whisper小74倍。这种效率使其非常适合资源受限的环境。ArabEmoNet在阿拉伯语音情绪识别方面取得了进展，为现实世界应用提供了卓越的性能和可访问性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01401v1">PDF</a> Accepted (The Third Arabic Natural Language Processing Conference)</p>
<p><strong>Summary</strong></p>
<p>阿拉伯语的语音情绪识别对于人机交互至关重要，尤其是面临数据有限和研究资源不足等挑战。我们推出了ArabEmoNet，这是一种轻便架构，旨在克服这些限制并提供最先进的性能。与传统的基于离散MFCC特征和一维卷积的系统相比，ArabEmoNet使用经过二维卷积处理的梅尔频谱图，能够捕捉到微妙的谱时模式，保留了关键的情感线索。尽管最近的模型倾向于大规模架构，拥有数百万个参数，但ArabEmoNet仅使用一百万个参数就取得了优越的结果，比HuBERT基础版本小90倍，比whisper小74倍。这种效率使其成为资源受限环境的理想选择。ArabEmoNet为阿拉伯语的语音情绪识别提供了出色的性能和实际应用的可访问性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>阿拉伯语的语音情绪识别在人机交互中至关重要。</li>
<li>有限的资源和数据是阿拉伯语语音情绪识别的挑战。</li>
<li>ArabEmoNet是一种轻便的架构，旨在解决这些挑战并提供先进的性能。</li>
<li>ArabEmoNet使用梅尔频谱图通过二维卷积处理，能够捕捉到微妙的谱时模式。</li>
<li>与其他大型模型相比，ArabEmoNet使用较少的参数取得了优越的性能。</li>
<li>ArabEmoNet的效率和性能使其成为资源受限环境的理想选择。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01401">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ec9914afd66ed29457b4e5e215f95106.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7bdcfcb2e14c12da375447484fcafb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3204fde3b4918cd3472e9e7eceeb963.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-495b8f6f6e9aaceb8454139befafdb48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a87d1d07d99758bce286b74858afdfe3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CabinSep-IR-Augmented-Mask-Based-MVDR-for-Real-Time-In-Car-Speech-Separation-with-Distributed-Heterogeneous-Arrays"><a href="#CabinSep-IR-Augmented-Mask-Based-MVDR-for-Real-Time-In-Car-Speech-Separation-with-Distributed-Heterogeneous-Arrays" class="headerlink" title="CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech   Separation with Distributed Heterogeneous Arrays"></a>CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech   Separation with Distributed Heterogeneous Arrays</h2><p><strong>Authors:Runduo Han, Yanxin Hu, Yihui Fu, Zihan Zhang, Yukai Jv, Li Chen, Lei Xie</strong></p>
<p>Separating overlapping speech from multiple speakers is crucial for effective human-vehicle interaction. This paper proposes CabinSep, a lightweight neural mask-based minimum variance distortionless response (MVDR) speech separation approach, to reduce speech recognition errors in back-end automatic speech recognition (ASR) models. Our contributions are threefold: First, we utilize channel information to extract spatial features, which improves the estimation of speech and noise masks. Second, we employ MVDR during inference, reducing speech distortion to make it more ASR-friendly. Third, we introduce a data augmentation method combining simulated and real-recorded impulse responses (IRs), improving speaker localization at zone boundaries and further reducing speech recognition errors. With a computational complexity of only 0.4 GMACs, CabinSep achieves a 17.5% relative reduction in speech recognition error rate in a real-recorded dataset compared to the state-of-the-art DualSep model. Demos are available at: <a target="_blank" rel="noopener" href="https://cabinsep.github.io/cabinsep/">https://cabinsep.github.io/cabinsep/</a>. </p>
<blockquote>
<p>在多人对话场景中，从重叠的语音中分离出各个说话人的声音对于实现有效的人车交互至关重要。本文提出了一种基于轻量级神经掩码的最低方差无失真响应（MVDR）语音分离方法CabinSep，旨在降低后端自动语音识别（ASR）模型的语音识别错误。我们的贡献主要体现在三个方面：首先，我们利用通道信息提取空间特征，提高了语音和噪声掩码的估计效果。其次，我们在推理过程中采用MVDR，减少语音失真，使其更适应ASR。最后，我们结合模拟和真实录制的脉冲响应（IRs）引入了一种数据增强方法，提高了扬声器在区域边界的定位精度，并进一步降低了语音识别错误。CabinSep的计算复杂度只有0.4GMACs，在真实数据集上与最新的DualSep模型相比，语音识别错误率降低了17.5%。相关演示可通过以下网址查看：<a target="_blank" rel="noopener" href="https://cabinsep.github.io/cabinsep/">https://cabinsep.github.io/cabinsep/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01399v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为CabinSep的轻量级神经网络掩码基于最小方差无失真响应（MVDR）的语音分离方法，用于减少后端自动语音识别（ASR）模型中的语音识别错误。通过利用通道信息提取空间特征、在推理过程中采用MVDR以及结合模拟和真实录制的冲击响应进行数据增强，CabinSep提高了语音识别的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CabinSep是一种轻量级的神经网络掩码基于MVDR的语音分离方法，旨在减少ASR模型中的语音识别错误。</li>
<li>通过利用通道信息提取空间特征，提高了语音和噪声掩码的估计。</li>
<li>在推理过程中采用MVDR，减少语音失真，使其更适应ASR。</li>
<li>引入结合模拟和真实录制的冲击响应的数据增强方法，改善了在区域边界的说话人定位，并进一步减少语音识别错误。</li>
<li>CabinSep的计算复杂度仅为0.4GMACs。</li>
<li>与当前先进的DualSep模型相比，CabinSep在真实录制的数据集上实现了17.5%的相对语音识别错误率降低。</li>
<li>演示地址：<a target="_blank" rel="noopener" href="https://cabinsep.github.io/cabinsep/%E3%80%82">https://cabinsep.github.io/cabinsep/。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01399">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c4a5bc59031c8c4fec5e3de832ba5d56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55085014391f835dc25749519e1aa827.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e83d735eb11180fe3c1e140e64d99b60.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SimulMEGA-MoE-Routers-are-Advanced-Policy-Makers-for-Simultaneous-Speech-Translation"><a href="#SimulMEGA-MoE-Routers-are-Advanced-Policy-Makers-for-Simultaneous-Speech-Translation" class="headerlink" title="SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation"></a>SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation</h2><p><strong>Authors:Chenyang Le, Bing Han, Jinshun Li, Songyong Chen, Yanmin Qian</strong></p>
<p>Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs. </p>
<blockquote>
<p>同步语音识别翻译（SimulST）通过联合优化语音识别和机器翻译，在严格的延迟限制下实现实时跨语言交流。现有系统在平衡翻译质量、延迟和语义连贯性方面存在困难，特别是在多语种多对多的场景中，不同的读写策略阻碍了统一策略学习。在本文中，我们提出了SimulMEGA（基于专家混合门控的同步生成），这是一个无监督的策略学习框架，它将前缀训练与专家混合精炼器相结合，以隐式的方式学习有效的读写决策，而不会增加推理时间开销。我们的设计只需对标准变压器架构进行最小修改，就可以应用于语音到文本和文本到语音的流式任务。通过对六种语言对的全面评估，我们的5亿参数语音到文本模型优于无缝基线，在平均延迟1.5秒的情况下，BLEU值降低不到7%，在3秒时降低不到3%。我们还将SimulMEGA扩展到具有单向主干的流式TTS，进一步展示了其通用性，产生了优越的延迟质量权衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01200v1">PDF</a> </p>
<p><strong>Summary</strong><br>同步语音识别翻译（SimulST）在严格的延迟限制下联合优化语音识别和机器翻译，实现实时跨语言沟通。现有系统在多语言多对多的场景中难以平衡翻译质量、延迟和语义连贯性。本文提出SimulMEGA（基于专家门控的同步生成），采用无监督策略学习框架，结合前缀训练与专家混合精炼器，隐式学习有效的读写决策，不增加推理时间开销。适用于标准转换器架构，并适用于语音到文本和文本到语音的流式传输任务。在六种语言对上的全面评估中，我们的5亿参数语音到文本模型在平均延迟1.5秒的情况下实现了低于7%的BLEU退化，在3秒内低于3%。我们还通过将其扩展到具有单向主干流的TTS来展示SimulMEGA的通用性，获得了出色的延迟质量权衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Simultaneous Speech Translation (SimulST) 旨在实现实时跨语言沟通，需要优化翻译质量、延迟和语义连贯性。</li>
<li>现有系统在多语言多对多的场景中面临挑战，需要平衡各项性能指标。</li>
<li>SimulMEGA是一个无监督策略学习框架，通过前缀训练和专家混合精炼器隐式学习读写决策。</li>
<li>SimulMEGA适用于标准转换器架构，可应用于语音到文本和文本到语音的流式传输任务。</li>
<li>在多种语言对的评估中，SimulMEGA表现出优秀的性能，在较短延迟下实现较高的翻译质量。</li>
<li>SimulMEGA通过扩展到文本到语音的流式传输任务，展示了其通用性和灵活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01200">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-26489addf658b993d6290d1f1ae21778.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce5b5111ae557683a6ee2e36b7ca4e35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38e5b98d73926dee6d1adfcf8a4fd647.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Noisy-Disentanglement-with-Tri-stage-Training-for-Noise-Robust-Speech-Recognition"><a href="#Noisy-Disentanglement-with-Tri-stage-Training-for-Noise-Robust-Speech-Recognition" class="headerlink" title="Noisy Disentanglement with Tri-stage Training for Noise-Robust Speech   Recognition"></a>Noisy Disentanglement with Tri-stage Training for Noise-Robust Speech   Recognition</h2><p><strong>Authors:Shuangyuan Chen, Shuang Wei, Dongxing Xu, Yanhua Long</strong></p>
<p>To enhance the performance of end-to-end (E2E) speech recognition systems in noisy or low signal-to-noise ratio (SNR) conditions, this paper introduces NoisyD-CT, a novel tri-stage training framework built on the Conformer-Transducer architecture. The core of NoisyD-CT is a especially designed compact noisy disentanglement (NoisyD) module (adding only 1.71M parameters), integrated between the Conformer blocks and Transducer Decoder to perform deep noise suppression and improve ASR robustness in challenging acoustic noise environments. To fully exploit the noise suppression capability of the NoisyD-CT, we further propose a clean representation consistency loss to align high-level representations derived from noisy speech with those obtained from corresponding clean speech. Together with a noisy reconstruction loss, this consistency alignment enables the NoisyD module to effectively suppress noise while preserving essential acoustic and linguistic features consistent across both clean and noisy conditions, thereby producing cleaner internal representations that enhance ASR performance. Moreover, our tri-stage training strategy is designed to fully leverage the functionalities of both the noisy disentanglement and speech recognition modules throughout the model training process, ultimately maximizing performance gains under noisy conditions. Our experiments are performed on the LibriSpeech and CHiME-4 datasets, extensive results demonstrate that our proposed NoisyD-CT significantly outperforms the competitive Conformer-Transducer baseline, achieving up to 25.7% and 10.6% relative word error rate reductions on simulated and real-world noisy test sets, respectively, while maintaining or even improving performance on clean speech test sets. The source code, model checkpoint and data simulation scripts will be available at <a target="_blank" rel="noopener" href="https://github.com/litchimo/NoisyD-CT">https://github.com/litchimo/NoisyD-CT</a>. </p>
<blockquote>
<p>为了提升端到端（E2E）语音识别系统在噪声环境或低信噪比（SNR）条件下的性能，本文引入了基于Conformer-Transducer架构的新型三阶段训练框架NoisyD-CT。NoisyD-CT的核心是一个特别设计的紧凑噪声分解（NoisyD）模块（仅增加1.71M参数），它位于Conformer块和转换器解码器之间，执行深度噪声抑制，提高ASR在具有挑战性的声学噪声环境中的稳健性。为了充分利用NoisyD-CT的噪声抑制能力，我们进一步提出了清洁表示一致性损失，以使从带噪声的语音中派生出的高级表示与从相应的清洁语音中获得的表示对齐。与噪声重建损失相结合，这种一致性对齐使NoisyD模块能够在抑制噪声的同时保留跨清洁和噪声条件下的基本声学和语言特征的一致性，从而产生更清洁的内部表示，提高ASR性能。此外，我们的三阶段训练策略旨在充分利用噪声分解和语音识别模块的功能，通过模型训练过程，最大限度地提高噪声条件下的性能。我们在LibriSpeech和CHiME-4数据集上进行了实验，大量结果表明，我们提出的NoisyD-CT显著优于竞争性的Conformer-Transducer基线，在模拟和真实世界噪声测试集上相对字词错误率分别降低了25.7%和10.6%，同时在清洁语音测试集上保持或提高了性能。源代码、模型检查点和数据模拟脚本将发布在<a target="_blank" rel="noopener" href="https://github.com/litchimo/NoisyD-CT%E3%80%82">https://github.com/litchimo/NoisyD-CT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01087v1">PDF</a> 11 pages,4 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于Conformer-Transducer架构的新型三阶段训练框架NoisyD-CT，旨在提高端到端语音识别系统在噪声或低信噪比条件下的性能。核心在于紧凑的噪声分解（NoisyD）模块，该模块在Conformer块和Transducer解码器之间进行集成，以执行深度噪声抑制并改善在具有挑战性的声学噪声环境中的ASR稳健性。通过引入清晰表示一致性损失和噪声重建损失，NoisyD-CT实现了噪声抑制的同时保留跨清洁和噪声条件下的关键声学和语言特征。在LibriSpeech和CHiME-4数据集上的实验表明，与竞争的Conformer-Transducer基线相比，NoisyD-CT显著提高了性能，在模拟和真实世界噪声测试集上分别实现了相对字词错误率降低25.7%和10.6%，同时在清洁语音测试集上保持或提高了性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NoisyD-CT是基于Conformer-Transducer架构的新型三阶段训练框架，旨在提高端到端语音识别系统在噪声或低信噪比环境中的性能。</li>
<li>NoisyD模块用于深度噪声抑制，并改善ASR在噪声环境中的稳健性。</li>
<li>通过清晰表示一致性损失和噪声重建损失，NoisyD模块实现噪声抑制的同时保留关键声学特征。</li>
<li>NoisyD-CT在LibriSpeech和CHiME-4数据集上表现出优异的性能。</li>
<li>与基线相比，NoisyD-CT在模拟和真实世界噪声测试集上显著降低了相对字词错误率。</li>
<li>NoisyD-CT在清洁语音测试集上保持或提高了性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01087">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-736d06dbb5ad91b401aaf5b1d54b628e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aee76bc71b7c400d1d57ce21677b1cde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70363a741ecf80d59a7aa278939b052f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2129a9c64b294656359d9858173d2f2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Unified-Denoising-and-Adaptation-Framework-for-Self-Supervised-Bengali-Dialectal-ASR"><a href="#A-Unified-Denoising-and-Adaptation-Framework-for-Self-Supervised-Bengali-Dialectal-ASR" class="headerlink" title="A Unified Denoising and Adaptation Framework for Self-Supervised Bengali   Dialectal ASR"></a>A Unified Denoising and Adaptation Framework for Self-Supervised Bengali   Dialectal ASR</h2><p><strong>Authors:Swadhin Biswas,  Imran, Tuhin Sheikh</strong></p>
<p>Automatic Speech Recognition (ASR) for Bengali, the world’s fifth most spoken language, remains a significant challenge, critically hindering technological accessibility for its over 270 million speakers. This challenge is compounded by two persistent and intertwined factors: the language’s vast dialectal diversity and the prevalence of acoustic noise in real-world environments. While state-of-the-art self-supervised learning (SSL) models have advanced ASR for low-resource languages, they often lack explicit mechanisms to handle environmental noise during pre-training or specialized adaptation strategies for the complex phonetic and lexical variations across Bengali dialects. This paper introduces a novel, unified framework designed to address these dual challenges simultaneously. Our approach is founded on the WavLM model, which is uniquely pre-trained with a masked speech denoising objective, making it inherently robust to acoustic distortions. We propose a specialized multi-stage fine-tuning strategy that first adapts the model to general-domain standard Bengali to establish a strong linguistic foundation and subsequently specializes it for noise-robust dialectal recognition through targeted data augmentation. The framework is rigorously evaluated on a comprehensive benchmark comprising multiple Bengali dialects under a wide range of simulated noisy conditions, from clean audio to low Signal-to-Noise Ratio (SNR) levels.   Experimental results demonstrate that the proposed framework significantly outperforms strong baselines, including standard fine-tuned wav2vec 2.0 and the large-scale multilingual Whisper model. This work establishes a new state-of-the-art for this task and provides a scalable, effective blueprint for developing practical ASR systems for other low-resource, high-variation languages globally. </p>
<blockquote>
<p>孟加拉语作为世界上第五大使用语言，其自动语音识别（ASR）仍然是一个重大挑战，严重影响了超过2.7亿孟加拉语使用者的技术可及性。这一挑战由两个持续交织的因素加剧：孟加拉语的巨大方言多样性和现实环境中普遍存在的声音噪音。虽然最新的自监督学习（SSL）模型已经推动了低资源语言的ASR发展，但它们通常缺乏处理环境噪声的明确机制，或者在应对孟加拉语方言的复杂语音和词汇变化方面的专门适应策略。本文介绍了一种新的统一框架，旨在同时应对这两个挑战。我们的方法基于WavLM模型，该模型具有独特的预训练机制，使用掩蔽语音去噪目标，使其对声音失真具有固有的稳健性。我们提出了一种专门的分阶段微调策略，首先使模型适应通用领域的标准孟加拉语，以建立坚实的语言基础，然后通过有针对性的数据增强使其适应噪声鲁棒的方言识别。该框架在包含多种孟加拉语方言的综合性基准测试上进行了严格评估，测试环境模拟了各种噪声条件，从干净音频到低信噪比（SNR）。实验结果表明，所提出的框架显著优于强大的基线模型，包括标准微调wav2vec 2.0和大规模多语言Whisper模型。这项工作为该任务建立了新的最新技术状态，为开发适用于其他低资源、高变化语言的实用ASR系统提供了可伸缩和有效的蓝图。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00988v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>针对孟加拉语——世界上第五大语言——的自动语音识别（ASR）仍然是一项挑战，这严重影响了超过2.7亿孟加拉语使用者的技术可访问性。该挑战由两个持久且交织的因素加剧：孟加拉语的巨大方言多样性和现实环境中存在的声音噪音的普及。本文提出了一种新颖的统一框架，旨在同时解决这两个挑战。我们的方法基于WavLM模型，该模型采用掩蔽语音降噪目标进行预训练，使其固有地适应声音失真。我们提出了一种专门的多阶段微调策略，首先使模型适应一般领域的标准孟加拉语，以建立坚实的语言基础，然后通过有针对性的数据增强使其适应噪声鲁棒的方言识别。该框架在包含多种孟加拉方言的广泛基准测试下进行了严格评估，测试环境涵盖从干净音频到低信噪比（SNR）水平的各种模拟噪声条件。实验结果表明，所提出的框架显著优于强大的基线，包括标准微调wav2vec 2.0和大规模多语言Whisper模型。这项工作为该任务建立了新的最先进的性能，并为全球其他资源匮乏、变化性高的语言开发实用的ASR系统提供了可伸缩、有效的蓝图。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>孟加拉语的自动语音识别（ASR）仍然是一项挑战，主要是由于其方言多样性和现实环境中的声音噪音问题。</li>
<li>现有技术如SSL模型在处理环境噪声和孟加拉语方言的复杂语音和词汇变化方面存在不足。</li>
<li>本文提出了一个基于WavLM模型的新框架，该模型通过掩蔽语音降噪目标进行预训练，以增强对声音失真的适应性。</li>
<li>引入了一种多阶段微调策略，先建立语言基础，然后通过数据增强适应噪声鲁棒的方言识别。</li>
<li>在广泛的基准测试中，新框架表现出显著优于其他先进技术的性能。</li>
<li>该工作为其他资源匮乏、变化性高的语言开发ASR系统提供了有效的蓝图。</li>
<li>该框架的建立为孟加拉语使用者提高了技术可访问性，推动了ASR技术的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00988">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ab1aeacf707a9bc0c2dfb3efda9459c5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MPO-Multidimensional-Preference-Optimization-for-Language-Model-based-Text-to-Speech"><a href="#MPO-Multidimensional-Preference-Optimization-for-Language-Model-based-Text-to-Speech" class="headerlink" title="MPO: Multidimensional Preference Optimization for Language Model-based   Text-to-Speech"></a>MPO: Multidimensional Preference Optimization for Language Model-based   Text-to-Speech</h2><p><strong>Authors:Kangxiang Xia, Xinfa Zhu, Jixun Yao, Lei Xie</strong></p>
<p>In recent years, text-to-speech (TTS) has seen impressive advancements through large-scale language models, achieving human-level speech quality. Integrating human feedback has proven effective for enhancing robustness in these systems. However, current approaches face challenges in optimizing TTS with preference data across multiple dimensions and often suffer from performance degradation due to overconfidence in rewards. We propose Multidimensional Preference Optimization (MPO) to better align TTS systems with human preferences. MPO introduces a preference set that streamlines the construction of data for multidimensional preference optimization, enabling alignment with multiple dimensions. Additionally, we incorporate regularization during training to address the typical degradation issues in DPO-based approaches. Our experiments demonstrate MPO’s effectiveness, showing significant improvements in intelligibility, speaker similarity, and prosody compared to baseline systems. </p>
<blockquote>
<p>近年来，通过大规模语言模型，文本到语音（TTS）取得了令人印象深刻的进步，达到了人类水平的语音质量。集成人类反馈已被证明可以提高这些系统的稳健性。然而，当前的方法在优化具有跨多个维度的偏好数据的TTS时面临挑战，并且由于过度依赖奖励而导致性能下降。我们提出多维偏好优化（MPO），以更好地将TTS系统与人类偏好对齐。MPO引入了一个偏好集，简化了用于多维偏好优化的数据构建，能够实现与多个维度的对齐。此外，我们在训练过程中加入了正则化，以解决DPO方法常见的退化问题。我们的实验证明了MPO的有效性，与基线系统相比，在清晰度、发音人相似度和语调方面都有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00685v1">PDF</a> Accepted by NCMMSC2025</p>
<p><strong>总结</strong></p>
<p>随着近年文本转语音（TTS）技术的显著进步，特别是借助大规模语言模型，已实现了近乎人类水平的语音质量。尽管集成人类反馈已被证明可以提高系统的稳健性，但当前方法在面对跨多个维度的偏好数据优化时仍面临挑战，且由于过度依赖奖励而导致性能下降。为此，我们提出了多维度偏好优化（MPO）方法，以更好地将TTS系统与人类偏好对齐。MPO通过引入偏好集简化了多维偏好优化数据的构建，并解决了DPO方法常见的退化问题。实验表明，与基线系统相比，MPO在清晰度、说话人相似性和语调方面都有显著提高。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本转语音（TTS）技术近年来取得显著进展，已接近人类水平的语音质量。</li>
<li>集成人类反馈可以提高TTS系统的稳健性。</li>
<li>当前TTS优化方法面临跨多个维度优化时的挑战，并可能出现因过度依赖奖励而导致的性能下降。</li>
<li>提出了多维度偏好优化（MPO）方法，以更好地将TTS系统与人类偏好对齐。</li>
<li>MPO通过引入偏好集简化了数据构建过程。</li>
<li>MPO解决了DPO方法常见的退化问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00685">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3226177ccf1b07cde45a65581c5a1810.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9add0f64f43031e99e9f2c65f001d5e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef58a5db1a63afde397c64847af0e07e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee6b852101aeb9ee9ab9f1477c85cd3c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SaD-A-Scenario-Aware-Discriminator-for-Speech-Enhancement"><a href="#SaD-A-Scenario-Aware-Discriminator-for-Speech-Enhancement" class="headerlink" title="SaD: A Scenario-Aware Discriminator for Speech Enhancement"></a>SaD: A Scenario-Aware Discriminator for Speech Enhancement</h2><p><strong>Authors:Xihao Yuan, Siqi Liu, Yan Chen, Hang Zhou, Chang Liu, Hanting Chen, Jie Hu</strong></p>
<p>Generative adversarial network-based models have shown remarkable performance in the field of speech enhancement. However, the current optimization strategies for these models predominantly focus on refining the architecture of the generator or enhancing the quality evaluation metrics of the discriminator. This approach often overlooks the rich contextual information inherent in diverse scenarios. In this paper, we propose a scenario-aware discriminator that captures scene-specific features and performs frequency-domain division, thereby enabling a more accurate quality assessment of the enhanced speech generated by the generator. We conducted comprehensive experiments on three representative models using two publicly available datasets. The results demonstrate that our method can effectively adapt to various generator architectures without altering their structure, thereby unlocking further performance gains in speech enhancement across different scenarios. </p>
<blockquote>
<p>基于生成对抗网络（GAN）的模型在语音增强领域表现出了显著的性能。然而，当前针对这些模型的优化策略主要集中在优化生成器的架构或提高判别器的质量评估指标上。这种方法往往会忽略不同场景中丰富的上下文信息。在本文中，我们提出了一种场景感知判别器，它捕获特定场景的特征并执行频域划分，从而能够对生成器生成的增强语音进行更准确的质量评估。我们在三个代表性模型上使用了两个公开数据集进行了全面的实验。结果表明，我们的方法可以有效地适应各种生成器架构，而无需改变其结构，从而在不同场景的语音增强中实现了进一步的性能提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00405v1">PDF</a> 5 pages, 2 figures.Accepted by InterSpeech2025</p>
<p><strong>总结</strong></p>
<p>在语音增强领域，基于生成对抗网络（GAN）的模型表现出卓越的性能。然而，当前对这些模型的优化策略主要集中在改进生成器的架构或提高判别器的质量评估指标上，忽略了不同场景中丰富的上下文信息。本文提出了一种场景感知判别器，它能够捕捉场景特定特征并进行频域分割，从而更准确地评估生成器生成的增强语音质量。我们在三个代表性模型上使用两个公开数据集进行了综合实验。结果表明，我们的方法可以有效适应各种生成器架构，而无需改变其结构，从而在不同场景中实现语音增强的进一步性能提升。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>当前GAN在语音增强领域的优化策略主要集中在改进生成器的架构和判别器的质量评估指标上。</li>
<li>提出的场景感知判别器能捕捉场景特定特征。</li>
<li>场景感知判别器进行频域分割，提高了对增强语音质量的评估准确性。</li>
<li>综合实验证明，该方法能有效适应各种生成器架构，无需改变其结构。</li>
<li>该方法在不同场景中实现了语音增强的进一步性能提升。</li>
<li>公开数据集的综合实验验证了该方法的有效性和实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00405">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2daf3905efd50b63c3288f5e0b9a3c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eafcf80d595db3223e892251cdaad63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b824d5da87f48a5995af722d91b929c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4209e86f8dd3e365f5ab3472ab0f517a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eced143a285d1eb8e6ad7415a815fdaf.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="AHELM-A-Holistic-Evaluation-of-Audio-Language-Models"><a href="#AHELM-A-Holistic-Evaluation-of-Audio-Language-Models" class="headerlink" title="AHELM: A Holistic Evaluation of Audio-Language Models"></a>AHELM: A Holistic Evaluation of Audio-Language Models</h2><p><strong>Authors:Tony Lee, Haoqin Tu, Chi Heem Wong, Zijun Wang, Siwei Yang, Yifan Mai, Yuyin Zhou, Cihang Xie, Percy Liang</strong></p>
<p>Evaluations of audio-language models (ALMs) – multimodal models that take interleaved audio and text as input and output text – are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets – including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering – to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness ($p&#x3D;0.01$) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 6th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at <a target="_blank" rel="noopener" href="https://crfm.stanford.edu/helm/audio/v1.0.0">https://crfm.stanford.edu/helm/audio/v1.0.0</a>. AHELM is intended to be a living benchmark and new datasets and models will be added over time. </p>
<blockquote>
<p>对于音频语言模型（ALM）的评估——这种多模态模型以交替的音频和文本作为输入并输出文本——由于缺乏标准化基准测试而受到阻碍。大多数基准测试只衡量一种或两种能力，并忽略了评估方面，如公平性或安全性。此外，由于不同的模型使用了有限的评估方法和提示方法以及推理参数，因此很难对这些模型进行比较。为了解决这个问题，我们引入了AHELM基准测试，它聚合了各种数据集，包括两个新的合成音频文本数据集PARADE和CoRe-Bench。PARADE旨在评估ALM在避免刻板印象方面的表现，而CoRe-Bench则通过推理多回合问答来衡量对话音频的推理能力。我们全面衡量了ALM在我们已确定的10个方面的性能，这些方面对于ALM的发展和使用很重要：音频感知、知识、推理、情感检测、偏见、公平性、多语言能力、稳健性、毒性和安全性。我们还对提示、推理参数和评估指标进行了标准化，以确保模型之间的公平比较。我们测试了来自三个开发者的14个公开权重和封闭API的ALM以及三个额外的简单基线系统（每个系统由自动语音识别器和语言模型组成）。我们的结果表明，虽然Gemini 2.5 Pro在十个方面中的五个方面排名第一，但在ASR任务上表现出群体不公平性（p&#x3D;0.01），而其他大多数模型则没有。我们还发现基线系统在AHELM上的表现相当不错，其中一个系统在整体排名中位列第六，尽管它只有语音到文本的功能。为了透明起见，所有原始提示、模型生成和输出都可在我们的网站上找到：<a target="_blank" rel="noopener" href="https://crfm.stanford.edu/helm/audio/v1.0.0%E3%80%82AHELM%E6%9C%AF%E8%AF%B4%E6%98%AF%E4%B8%BA%E4%BA%86%E6%88%90%E4%B8%BA%E4%B8%80%E4%B8%AA%E7%BB%B4%E6%BB%BE%E7%9A%84%E5%9F%BA%E6%A1%86%E6%B5%8B%E8%AF%95%EF%BC%8C%E5%B9%B6%E4%BC%9A%E9%9A%8F%E7%9D%A1%E6%8E%A8%E7%A7%BB%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%A8%A1%E5%9E%8B%E3%80%82">https://crfm.stanford.edu/helm/audio/v1.0.0。AHELM旨在成为一个常设的基准测试，并将随着时间的推移添加新的数据集和模型。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21376v2">PDF</a> </p>
<p><strong>摘要</strong><br>音频语言模型（ALM）的评价因缺乏标准化基准测试而受到阻碍。大多数基准测试只衡量一种或两种能力，并忽略了公平性或安全性等评价方面。此外，由于缺乏统一的评价和不同模型的测试采用不同的提示方法和推理参数，导致难以进行跨模型比较。为了弥补这些不足，我们推出了AHELM基准测试，该测试汇集了多个数据集，包括两个新的合成音频文本数据集PARADE和CoRe-Bench，以全面衡量ALM在音频感知、知识、推理、情感检测、偏见、公平性等方面的性能。我们标准化了提示、推理参数和评价指标，以确保模型之间的公平比较。我们对来自三个开发者的十四种开放权重和封闭API的ALM以及由自动语音识别器和语言模型组成的三个附加简单基准系统进行了测试。结果表明，虽然Gemini 2.5 Pro在十个方面中的五个方面排名第一，但在语音识别任务上表现出群体不公平性。我们还发现基线系统表现良好，其中一个系统排名第六，尽管它只有语音到文本的功能。为确保透明度，所有原始提示、模型生成和输出都可在我们的网站上找到：<a target="_blank" rel="noopener" href="https://crfm.stanford.edu/helm/audio/v1.0.0%E3%80%82AHELM%E6%97%A8%E5%9C%A8%E6%88%90%E4%B8%BA%E4%B8%80%E4%B8%AA%E6%8C%81%E7%BB%AD%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%EF%BC%8C%E9%9A%8F%E7%9D%80%E6%97%B6%E9%97%B4%E7%9A%84%E6%8E%A8%E7%A7%BB%EF%BC%8C%E6%96%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%B0%86%E8%A2%AB%E6%B7%BB%E5%8A%A0%E8%BF%9B%E6%9D%A5%E3%80%82">https://crfm.stanford.edu/helm/audio/v1.0.0。AHELM旨在成为一个持续的基准测试，随着时间的推移，新的数据集和模型将被添加进来。</a></p>
<p><strong>要点摘要</strong></p>
<ol>
<li>缺乏标准化基准测试限制了音频语言模型（ALM）的评价。</li>
<li>现有基准测试往往只关注一种或两种能力，忽略评价模型的公平性、安全性等方面。</li>
<li>引入AHELM基准测试以全面衡量ALM的性能，涵盖音频感知、知识等十个方面。</li>
<li>AHELM使用标准化提示、推理参数和评价指标以确保模型之间的公平比较。</li>
<li>测试了多种ALM和基线系统，发现Gemini 2.5 Pro在多个方面表现优秀但存在群体不公平性。</li>
<li>基线系统表现良好，证明简单系统的实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21376">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-75d788d32e08447d8ce829be3ddcb7ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7aac0542c74c865ec7cfacbb955f60d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5feda23c048d7c169d9fd8a44c62b0d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcea1d680f2a3c394b73d1cf9a831a70.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6a0bdfdf63cac948d1a54a7ecf6398fa.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-09-07  Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1bbbb934850718bf94cc89a9bb6bcd1f.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-09-07  Ultrasound-based detection and malignancy prediction of breast lesions   eligible for biopsy A multi-center clinical-scenario study using nomograms,   large language models, and radiologist evaluation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
