<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c4a5bc59031c8c4fec5e3de832ba5d56.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Test-Time-Adaptation-for-Speech-Enhancement-via-Domain-Invariant-Embedding-Transformation"><a href="#Test-Time-Adaptation-for-Speech-Enhancement-via-Domain-Invariant-Embedding-Transformation" class="headerlink" title="Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation"></a>Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation</h2><p><strong>Authors:Tobias Raichle, Niels Edinger, Bin Yang</strong></p>
<p>Deep learning-based speech enhancement models achieve remarkable performance when test distributions match training conditions, but often degrade when deployed in unpredictable real-world environments with domain shifts. To address this challenge, we present LaDen (latent denoising), the first test-time adaptation method specifically designed for speech enhancement. Our approach leverages powerful pre-trained speech representations to perform latent denoising, approximating clean speech representations through a linear transformation of noisy embeddings. We show that this transformation generalizes well across domains, enabling effective pseudo-labeling for target domains without labeled target data. The resulting pseudo-labels enable effective test-time adaptation of speech enhancement models across diverse acoustic environments. We propose a comprehensive benchmark spanning multiple datasets with various domain shifts, including changes in noise types, speaker characteristics, and languages. Our extensive experiments demonstrate that LaDen consistently outperforms baseline methods across perceptual metrics, particularly for speaker and language domain shifts. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨æµ‹è¯•åˆ†å¸ƒä¸è®­ç»ƒæ¡ä»¶ç›¸åŒ¹é…æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨éƒ¨ç½²åœ¨ä¸å¯é¢„æµ‹çš„ç°å®ä¸–ç•Œç¯å¢ƒä¸­ï¼Œç”±äºé¢†åŸŸåç§»ï¼Œæ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LaDenï¼ˆæ½œåœ¨å»å™ªï¼‰ï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºè¯­éŸ³å¢å¼ºè®¾è®¡çš„ç¬¬ä¸€ä¸ªæµ‹è¯•æ—¶é—´é€‚åº”æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒè¯­éŸ³è¡¨ç¤ºæ¥æ‰§è¡Œæ½œåœ¨å»å™ªï¼Œé€šè¿‡å™ªå£°åµŒå…¥çš„çº¿æ€§å˜æ¢æ¥è¿‘ä¼¼æ¸…æ´è¯­éŸ³è¡¨ç¤ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§å˜æ¢åœ¨ä¸åŒçš„é¢†åŸŸä¹‹é—´å…·æœ‰å¾ˆå¥½çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡è®°ç›®æ ‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯¹ç›®æ ‡é¢†åŸŸè¿›è¡Œæœ‰æ•ˆçš„ä¼ªæ ‡ç­¾æ ‡æ³¨ã€‚è¿™äº›ç”Ÿæˆçš„ä¼ªæ ‡ç­¾èƒ½å¤Ÿåœ¨å„ç§å£°å­¦ç¯å¢ƒä¸­æœ‰æ•ˆåœ°é€‚åº”è¯­éŸ³å¢å¼ºæ¨¡å‹çš„æµ‹è¯•æ—¶é—´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šä¸ªæ•°æ®é›†ï¼ŒåŒ…æ‹¬å™ªå£°ç±»å‹ã€è¯´è¯äººç‰¹å¾å’Œè¯­è¨€æ–¹é¢çš„å„ç§é¢†åŸŸåç§»ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLaDenåœ¨æ„ŸçŸ¥æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯´è¯äººå’Œè¯­è¨€é¢†åŸŸåç§»æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04280v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨æµ‹è¯•åˆ†å¸ƒä¸è®­ç»ƒæ¡ä»¶ç›¸åŒ¹é…æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨éƒ¨ç½²åˆ°å…·æœ‰é¢†åŸŸå·®å¼‚æ€§çš„ä¸å¯é¢„æµ‹çš„ç°å®ä¸–ç•Œç¯å¢ƒä¸­æ—¶ï¼Œæ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LaDenï¼ˆæ½œåœ¨å»å™ªï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºè¯­éŸ³å¢å¼ºè®¾è®¡çš„æµ‹è¯•æ—¶é—´é€‚åº”æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒè¯­éŸ³è¡¨ç¤ºæ¥æ‰§è¡Œæ½œåœ¨å»å™ªï¼Œé€šè¿‡å™ªå£°åµŒå…¥çš„çº¿æ€§å˜æ¢æ¥è¿‘ä¼¼æ¸…æ´è¯­éŸ³è¡¨ç¤ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§å˜æ¢åœ¨ä¸åŒçš„é¢†åŸŸä¹‹é—´å…·æœ‰å¾ˆå¥½çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡è®°ç›®æ ‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯¹ç›®æ ‡åŸŸè¿›è¡Œæœ‰æ•ˆçš„ä¼ªæ ‡ç­¾ã€‚ä¼ªæ ‡ç­¾ä½¿å¾—åœ¨å¤šç§å£°å­¦ç¯å¢ƒä¸­çš„è¯­éŸ³å¢å¼ºæ¨¡å‹çš„æµ‹è¯•æ—¶é—´é€‚åº”å˜å¾—æœ‰æ•ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šä¸ªå…·æœ‰å„ç§é¢†åŸŸå·®å¼‚æ€§çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬å™ªå£°ç±»å‹ã€è¯´è¯äººç‰¹å¾å’Œè¯­è¨€çš„å˜åŒ–ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLaDenåœ¨æ„ŸçŸ¥æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯´è¯äººå’Œè¯­è¨€é¢†åŸŸçš„å˜åŒ–æ–¹é¢ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LaDenæ˜¯ä¸€ç§æµ‹è¯•æ—¶é—´é€‚åº”æ–¹æ³•ï¼Œä¸“ä¸ºè¯­éŸ³å¢å¼ºè®¾è®¡ï¼Œç”¨äºå¤„ç†ç°å®ä¸–ç•Œä¸­ä¸å¯é¢„æµ‹çš„å£°å­¦ç¯å¢ƒã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒè¯­éŸ³è¡¨ç¤ºçš„æ½œåœ¨å»å™ªæŠ€æœ¯ï¼Œé€šè¿‡çº¿æ€§å˜æ¢å™ªå£°åµŒå…¥æ¥è¿‘ä¼¼æ¸…æ´è¯­éŸ³ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸé—´å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¹¶èƒ½åœ¨æ— æ ‡ç­¾ç›®æ ‡æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„ä¼ªæ ‡ç­¾ã€‚</li>
<li>LaDençš„ä¼ªæ ‡ç­¾ä½¿è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨å¤šç§å£°å­¦ç¯å¢ƒä¸­çš„æµ‹è¯•æ—¶é—´é€‚åº”å˜å¾—æœ‰æ•ˆã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šä¸ªæ•°æ®é›†ï¼Œåæ˜ å„ç§é¢†åŸŸå·®å¼‚ï¼Œå¦‚å™ªå£°ç±»å‹ã€è¯´è¯äººç‰¹å¾å’Œè¯­è¨€å˜åŒ–ã€‚</li>
<li>LaDenåœ¨æ„ŸçŸ¥æŒ‡æ ‡ä¸Šè¶…è¶Šäº†å…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¯´è¯äººå’Œè¯­è¨€é¢†åŸŸçš„å·®å¼‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>LaDençš„ä¼˜è¶Šæ€§èƒ½è¡¨æ˜å®ƒåœ¨å¤„ç†ç°å®ä¸–ç•Œçš„è¯­éŸ³å¢å¼ºä»»åŠ¡æ—¶çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04280">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9caed9dc433b0a900fa90f005c3763ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99ef586c2ae488498167235f58182b80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d679a2e4b13e239a074ed22e7250e0a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0fff68cf086b086368aae42689ff911.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37e872d34af63f1aceef049bf8b611d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbc29eb7e9affe8939906473e8b132b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09d56deab92e004e34d59f15783d710f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WenetSpeech-Yue-A-Large-scale-Cantonese-Speech-Corpus-with-Multi-dimensional-Annotation"><a href="#WenetSpeech-Yue-A-Large-scale-Cantonese-Speech-Corpus-with-Multi-dimensional-Annotation" class="headerlink" title="WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with   Multi-dimensional Annotation"></a>WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with   Multi-dimensional Annotation</h2><p><strong>Authors:Longhao Li, Zhao Guo, Hongjie Chen, Yuhang Dai, Ziyu Zhang, Hongfei Xue, Tianlun Zuo, Chengyou Wang, Shuiyuan Wang, Jie Li, Xin Xu, Hui Bu, Binbin Zhang, Ruibin Yuan, Ziya Zhou, Wei Xue, Lei Xie</strong></p>
<p>The development of speech understanding and generation has been significantly accelerated by the availability of large-scale, high-quality speech datasets. Among these, ASR and TTS are regarded as the most established and fundamental tasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9 million native speakers worldwide, limited annotated resources have hindered progress and resulted in suboptimal ASR and TTS performance. To address this challenge, we propose WenetSpeech-Pipe, an integrated pipeline for building large-scale speech corpus with multi-dimensional annotation tailored for speech understanding and generation. It comprises six modules: Audio Collection, Speaker Attributes Annotation, Speech Quality Annotation, Automatic Speech Recognition, Text Postprocessing and Recognizer Output Voting, enabling rich and high-quality annotations. Based on this pipeline, we release WenetSpeech-Yue, the first large-scale Cantonese speech corpus with multi-dimensional annotation for ASR and TTS, covering 21,800 hours across 10 domains with annotations including ASR transcription, text confidence, speaker identity, age, gender, speech quality scores, among other annotations. We also release WSYue-eval, a comprehensive Cantonese benchmark with two components: WSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long utterances, code-switching, and diverse acoustic conditions, and WSYue-TTS-eval, with base and coverage subsets for standard and generalization testing. Experimental results show that models trained on WenetSpeech-Yue achieve competitive results against state-of-the-art (SOTA) Cantonese ASR and TTS systems, including commercial and LLM-based models, highlighting the value of our dataset and pipeline. </p>
<blockquote>
<p>è¯­éŸ³ç†è§£å’Œç”Ÿæˆçš„å‘å±•å¾—ç›Šäºå¤§è§„æ¨¡é«˜è´¨é‡è¯­éŸ³æ•°æ®é›†çš„å¯ç”¨æ€§è€Œå¾—åˆ°æ˜¾è‘—åŠ é€Ÿã€‚å…¶ä¸­ï¼Œè¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³è¢«è®¤ä¸ºæ˜¯æœ€æˆç†Ÿã€æœ€åŸºæœ¬çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¯¹äºå…¨çƒçº¦æœ‰8490ä¸‡ä½¿ç”¨äººå£çš„ç²¤è¯­ï¼ˆç²¤ç³»ä¸­æ–‡ï¼‰ï¼Œæ ‡æ³¨èµ„æºæœ‰é™é˜»ç¢äº†å…¶å‘å±•ï¼Œå¯¼è‡´è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³çš„æ€§èƒ½ä¸å°½å¦‚äººæ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†WenetSpeech-Pipeï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºè¯­éŸ³ç†è§£å’Œç”Ÿæˆé‡èº«å®šåˆ¶çš„å¤§è§„æ¨¡è¯­éŸ³è¯­æ–™åº“æ„å»ºçš„ç»¼åˆç®¡é“ï¼Œå…·æœ‰å¤šç»´åº¦æ³¨é‡Šã€‚å®ƒåŒ…å«å…­ä¸ªæ¨¡å—ï¼šéŸ³é¢‘æ”¶é›†ã€è¯´è¯äººå±æ€§æ³¨é‡Šã€è¯­éŸ³è´¨é‡æ³¨é‡Šã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åå¤„ç†å’Œè¯†åˆ«è¾“å‡ºæŠ•ç¥¨ï¼Œèƒ½å¤Ÿå®ç°ä¸°å¯Œä¸”é«˜è´¨é‡æ³¨é‡Šã€‚åŸºäºè¿™ä¸€ç®¡é“ï¼Œæˆ‘ä»¬å‘å¸ƒäº†WenetSpeech-Yueï¼Œè¿™æ˜¯é¦–ä¸ªå…·æœ‰å¤šç»´åº¦æ³¨é‡Šçš„ç²¤è¯­è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³å¤§è§„æ¨¡è¯­éŸ³è¯­æ–™åº“ï¼Œæ¶µç›–10ä¸ªé¢†åŸŸçš„21800å°æ—¶éŸ³é¢‘ï¼Œæ³¨é‡ŠåŒ…æ‹¬è¯­éŸ³è¯†åˆ«è½¬å½•ã€æ–‡æœ¬ç½®ä¿¡åº¦ã€è¯´è¯äººèº«ä»½ã€å¹´é¾„ã€æ€§åˆ«ã€è¯­éŸ³è´¨é‡è¯„åˆ†ç­‰ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†WSYue-evalï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ç²¤è¯­åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šWSYue-ASR-evalï¼Œç”¨äºè¯„ä¼°çŸ­å¥å’Œé•¿å¥ã€è¯­è¨€åˆ‡æ¢å’Œå„ç§å£°å­¦æ¡ä»¶ä¸‹çš„è¯­éŸ³è¯†åˆ«ï¼›ä»¥åŠWSYue-TTS-evalï¼ŒåŒ…å«åŸºæœ¬å’Œè¦†ç›–å­é›†ç”¨äºæ ‡å‡†æµ‹è¯•å’Œæ³›åŒ–æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨WenetSpeech-Yueä¸Šè®­ç»ƒçš„æ¨¡å‹ä¸æœ€æ–°çš„ç²¤è¯­è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬å•†ä¸šæ¨¡å‹å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨¡å‹ï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬æ•°æ®é›†å’Œç®¡é“çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03959v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹ç²¤è¯­ï¼ˆä¸€ç§å…¨çƒçº¦æœ‰8490ä¸‡æ¯è¯­è€…çš„è¯­è¨€ï¼‰ï¼Œç°æœ‰çš„è¯­éŸ³ç†è§£å’Œç”ŸæˆæŠ€æœ¯å‘å±•å—é™äºæ ‡æ³¨èµ„æºçš„ç¨€ç¼ºã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†WenetSpeech-Pipeè¿™ä¸€é›†æˆç®¡é“ï¼Œæ—¨åœ¨æ„å»ºå¤§è§„æ¨¡ç²¤è¯­è¯­éŸ³è¯­æ–™åº“ï¼Œå¹¶é…å¤‡å¤šç»´åº¦æ ‡æ³¨ï¼Œç”¨äºåŠ é€Ÿè¯­éŸ³ç†è§£å’Œç”Ÿæˆçš„ç ”ç©¶è¿›å±•ã€‚è¯¥ç®¡é“åŒ…å«å…­ä¸ªæ¨¡å—ï¼Œèƒ½å¤Ÿå®ç°ä¸°å¯Œä¸”é«˜è´¨é‡çš„æ ‡æ³¨ã€‚åŸºäºè¯¥ç®¡é“ï¼Œå›¢é˜Ÿå‘å¸ƒäº†é¦–ä¸ªå¤§è§„æ¨¡ç²¤è¯­è¯­éŸ³è¯­æ–™åº“â€”â€”WenetSpeech-Yueï¼ŒåŒ…å«è¶…è¿‡21800å°æ—¶çš„è¯­éŸ³æ•°æ®ï¼Œè¦†ç›–åå¤§é¢†åŸŸï¼Œå¹¶é…å¤‡å¤šç»´åº¦æ ‡æ³¨ã€‚åŒæ—¶å‘å¸ƒçš„è¿˜æœ‰WSYue-evalåŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°ç²¤è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨WenetSpeech-Yueè®­ç»ƒå¾—åˆ°çš„æ¨¡å‹è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œè¯æ˜å…¶ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­éŸ³ç†è§£å’Œç”Ÿæˆçš„å‘å±•å—ç›Šäºå¤§è§„æ¨¡é«˜è´¨é‡è¯­éŸ³æ•°æ®é›†çš„å¯ç”¨æ€§ã€‚</li>
<li>å¯¹äºç²¤è¯­ï¼ˆYue Chineseï¼‰ï¼Œèµ„æºé™åˆ¶é˜»ç¢äº†è¯­éŸ³ç†è§£å’Œç”Ÿæˆçš„è¿›æ­¥ã€‚</li>
<li>WenetSpeech-Pipeæ˜¯ä¸€ä¸ªä¸ºç²¤è¯­è®¾è®¡çš„é›†æˆç®¡é“ï¼Œç”¨äºæ„å»ºå¤§è§„æ¨¡è¯­éŸ³è¯­æ–™åº“å¹¶æä¾›å¤šç»´åº¦æ ‡æ³¨ã€‚</li>
<li>WenetSpeech-Yueæ˜¯åŸºäºè¯¥ç®¡é“çš„ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡ç²¤è¯­è¯­éŸ³è¯­æ–™åº“ï¼ŒåŒ…å«ä¸°å¯Œçš„æ ‡æ³¨æ•°æ®ã€‚</li>
<li>WSYue-evalåŸºå‡†æµ‹è¯•é›†ç”¨äºè¯„ä¼°ç²¤è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜åœ¨WenetSpeech-Yueè®­ç»ƒçš„æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œä½“ç°äº†æ•°æ®é›†çš„ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62b44d42a53b42a895f853468d4d8f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d68bb830dc32bd1cafaf09ed5da72c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5287fc63fb61101fb566f76721a794e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08191cf1e6dd2e03e06c98efa514d89a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd6b2f995a79a167332bc1604a4ff929.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="NE-PADD-Leveraging-Named-Entity-Knowledge-for-Robust-Partial-Audio-Deepfake-Detection-via-Attention-Aggregation"><a href="#NE-PADD-Leveraging-Named-Entity-Knowledge-for-Robust-Partial-Audio-Deepfake-Detection-via-Attention-Aggregation" class="headerlink" title="NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio   Deepfake Detection via Attention Aggregation"></a>NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio   Deepfake Detection via Attention Aggregation</h2><p><strong>Authors:Huhong Xian, Rui Liu, Berrak Sisman, Haizhou Li</strong></p>
<p>Different from traditional sentence-level audio deepfake detection (ADD), partial audio deepfake detection (PADD) requires frame-level positioning of the location of fake speech. While some progress has been made in this area, leveraging semantic information from audio, especially named entities, remains an underexplored aspect. To this end, we propose NE-PADD, a novel method for Partial Audio Deepfake Detection (PADD) that leverages named entity knowledge through two parallel branches: Speech Name Entity Recognition (SpeechNER) and PADD. The approach incorporates two attention aggregation mechanisms: Attention Fusion (AF) for combining attention weights and Attention Transfer (AT) for guiding PADD with named entity semantics using an auxiliary loss. Built on the PartialSpoof-NER dataset, experiments show our method outperforms existing baselines, proving the effectiveness of integrating named entity knowledge in PADD. The code is available at <a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/NE-PADD">https://github.com/AI-S2-Lab/NE-PADD</a>. </p>
<blockquote>
<p>ä¸ä¼ ç»Ÿçš„å¥å­çº§éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ï¼ˆADDï¼‰ä¸åŒï¼Œéƒ¨åˆ†éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ï¼ˆPADDï¼‰éœ€è¦åœ¨å¸§çº§åˆ«å®šä½è™šå‡è¯­éŸ³çš„ä½ç½®ã€‚è™½ç„¶è¯¥é¢†åŸŸå·²ç»å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†åˆ©ç”¨éŸ³é¢‘ä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å‘½åå®ä½“ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªè¢«å¿½è§†çš„æ–¹é¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†NE-PADDï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å‘½åå®ä½“çŸ¥è¯†çš„æ–°å‹éƒ¨åˆ†éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•ï¼ˆPADDï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯ï¼šè¯­éŸ³åç§°å®ä½“è¯†åˆ«ï¼ˆSpeechNERï¼‰å’ŒPADDæ¥å®ç°ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸¤ç§æ³¨æ„åŠ›èšåˆæœºåˆ¶ï¼šç”¨äºç»“åˆæ³¨æ„åŠ›æƒé‡çš„æ³¨æ„åŠ›èåˆï¼ˆAFï¼‰å’Œç”¨äºä½¿ç”¨è¾…åŠ©æŸå¤±å¼•å¯¼PADDä¸å‘½åå®ä½“è¯­ä¹‰çš„æ³¨æ„åŠ›è½¬ç§»ï¼ˆATï¼‰ã€‚åœ¨PartialSpoof-NERæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰åŸºçº¿ï¼Œè¯æ˜äº†åœ¨PADDä¸­æ•´åˆå‘½åå®ä½“çŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/NE-PADD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/NE-PADDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03829v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿™ç¯‡æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºå‘½åå®ä½“çŸ¥è¯†çš„éƒ¨åˆ†éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ï¼ˆNE-PADDï¼‰æ–¹æ³•ã€‚å®ƒé‡‡ç”¨ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯è¿›è¡Œè¯­éŸ³åç§°å®ä½“è¯†åˆ«ï¼ˆSpeechNERï¼‰å’ŒPADDï¼Œé€šè¿‡æ³¨æ„åŠ›èåˆå’Œæ³¨æ„åŠ›è½¬ç§»æœºåˆ¶æ•´åˆå‘½åå®ä½“çŸ¥è¯†ï¼Œä»¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨PartialSpoof-NERæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NE-PADDæ˜¯ä¸€ç§æ–°å‹çš„éƒ¨åˆ†éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å‘½åå®ä½“çŸ¥è¯†æ¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯ï¼šSpeechNERå’ŒPADDã€‚</li>
<li>æ³¨æ„åŠ›èåˆï¼ˆAFï¼‰å’Œæ³¨æ„åŠ›è½¬ç§»ï¼ˆATï¼‰æœºåˆ¶è¢«ç”¨äºæ•´åˆå‘½åå®ä½“çŸ¥è¯†ã€‚</li>
<li>å®éªŒåœ¨PartialSpoof-NERæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œè¯æ˜NE-PADDæ–¹æ³•çš„æ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>NE-PADDæ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å®šä½è™šå‡è¯­éŸ³çš„å¸§çº§åˆ«ä½ç½®ã€‚</li>
<li>è¯­ä¹‰ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å‘½åå®ä½“åœ¨ä¿¡æ¯æ£€æµ‹ä¸­èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03829">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f588b10342d72ecaf58cfc42a0fc7803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a18c36214766c252e17c60e0495e8177.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-104dc16f7d8b9d9ec8c2b0c96002244c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6e16547218f3228d59092970bf9e847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7fd05ed7d120e779d8fff4ebdcbc133.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06285d604239cf2e00f1a5c971d811e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-663f253d099dda553d2da78c506f3124.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Continuous-Saudi-Sign-Language-Recognition-A-Vision-Transformer-Approach"><a href="#Continuous-Saudi-Sign-Language-Recognition-A-Vision-Transformer-Approach" class="headerlink" title="Continuous Saudi Sign Language Recognition: A Vision Transformer   Approach"></a>Continuous Saudi Sign Language Recognition: A Vision Transformer   Approach</h2><p><strong>Authors:Soukeina Elhassen, Lama Al Khuzayem, Areej Alhothali, Ohoud Alzamzami, Nahed Alowaidi</strong></p>
<p>Sign language (SL) is an essential communication form for hearing-impaired and deaf people, enabling engagement within the broader society. Despite its significance, limited public awareness of SL often leads to inequitable access to educational and professional opportunities, thereby contributing to social exclusion, particularly in Saudi Arabia, where over 84,000 individuals depend on Saudi Sign Language (SSL) as their primary form of communication. Although certain technological approaches have helped to improve communication for individuals with hearing impairments, there continues to be an urgent requirement for more precise and dependable translation techniques, especially for Arabic sign language variants like SSL. Most state-of-the-art solutions have primarily focused on non-Arabic sign languages, resulting in a considerable absence of resources dedicated to Arabic sign language, specifically SSL. The complexity of the Arabic language and the prevalence of isolated sign language datasets that concentrate on individual words instead of continuous speech contribute to this issue. To address this gap, our research represents an important step in developing SSL resources. To address this, we introduce the first continuous Saudi Sign Language dataset called KAU-CSSL, focusing on complete sentences to facilitate further research and enable sophisticated recognition systems for SSL recognition and translation. Additionally, we propose a transformer-based model, utilizing a pretrained ResNet-18 for spatial feature extraction and a Transformer Encoder with Bidirectional LSTM for temporal dependencies, achieving 99.02% accuracy at signer dependent mode and 77.71% accuracy at signer independent mode. This development leads the way to not only improving communication tools for the SSL community but also making a substantial contribution to the wider field of sign language. </p>
<blockquote>
<p>æ‰‹è¯­ï¼ˆSLï¼‰æ˜¯å¬éšœäººå£«çš„ä¸€ç§é‡è¦çš„æ²Ÿé€šæ–¹å¼ï¼Œèƒ½å¤Ÿè®©ä»–ä»¬èå…¥æ›´å¹¿é˜”çš„ç¤¾ä¼šã€‚å°½ç®¡æ‰‹è¯­å…·æœ‰é‡è¦æ€§ï¼Œä½†å…¬ä¼—å¯¹å…¶çš„æœ‰é™è®¤çŸ¥å¾€å¾€å¯¼è‡´æ•™è‚²å’ŒèŒä¸šæœºä¼šçš„ä¸å…¬å¹³åˆ†é…ï¼Œä»è€Œå¯¼è‡´ç¤¾ä¼šæ’æ–¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²™ç‰¹é˜¿æ‹‰ä¼¯ã€‚åœ¨æ²™ç‰¹é˜¿æ‹‰ä¼¯ï¼Œæœ‰è¶…è¿‡84ï¼Œ000äººä¾èµ–æ²™ç‰¹æ‰‹è¯­ï¼ˆSSLï¼‰ä½œä¸ºä»–ä»¬çš„ä¸»è¦æ²Ÿé€šæ–¹å¼ã€‚è™½ç„¶æŸäº›æŠ€æœ¯æ–¹æ³•å·²ç»å¸®åŠ©å¬éšœäººå£«æ”¹å–„äº†æ²Ÿé€šï¼Œä½†ä»ç„¶å­˜åœ¨å¯¹æ›´ç²¾ç¡®å’Œå¯é çš„ç¿»è¯‘æŠ€æœ¯çš„è¿«åˆ‡éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåƒSSLè¿™æ ·çš„é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­å˜ä½“ã€‚å¤§å¤šæ•°æœ€æ–°è§£å†³æ–¹æ¡ˆä¸»è¦å…³æ³¨éé˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­ï¼Œå¯¼è‡´ç¼ºä¹ä¸“é—¨é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­çš„èµ„æºã€‚é˜¿æ‹‰ä¼¯è¯­è¯­è¨€çš„å¤æ‚æ€§ä»¥åŠå­¤ç«‹çš„æ‰‹è¯­æ•°æ®é›†æ™®éå…³æ³¨å•ä¸ªå•è¯è€Œä¸æ˜¯è¿ç»­è¯­æ˜¯å¯¼è‡´è¿™ä¸€é—®é¢˜çš„åŸå› ä¹‹ä¸€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶åœ¨å¼€å‘SSLèµ„æºæ–¹é¢è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ç¬¬ä¸€ä¸ªè¿ç»­çš„æ²™ç‰¹æ‰‹è¯­æ•°æ®é›†ï¼Œåä¸ºKAU-CSSLï¼Œä¸“æ³¨äºå®Œæ•´çš„å¥å­ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å¹¶å¯ç”¨SSLè¯†åˆ«å’Œç¿»è¯‘çš„å…ˆè¿›è¯†åˆ«ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºtransformerçš„æ¨¡å‹ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ResNet-18è¿›è¡Œç©ºé—´ç‰¹å¾æå–å’Œå¸¦æœ‰åŒå‘LSTMçš„Transformerç¼–ç å™¨å¤„ç†æ—¶é—´ä¾èµ–æ€§ï¼Œåœ¨ç­¾çº¦äººä¾èµ–æ¨¡å¼ä¸‹è¾¾åˆ°99.02ï¼…çš„å‡†ç¡®ç‡ï¼Œç­¾çº¦äººç‹¬ç«‹æ¨¡å¼ä¸‹è¾¾åˆ°77.71ï¼…çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€å‘å±•ä¸ä»…ä¸ºSSLç¤¾åŒºæ”¹å–„äº†æ²Ÿé€šå·¥å…·ï¼Œè€Œä¸”ä¸ºæ›´å¹¿æ³›çš„æ‰‹è¯­é¢†åŸŸåšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03467v1">PDF</a> 23 pages, 13 figures, 5 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰‹åŠ¿è¯­è¨€ï¼ˆSLï¼‰å¯¹äºå¬éšœäººå£«è‡³å…³é‡è¦ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´å¹¿æ³›çš„ç¤¾ä¼šä¸­å‚ä¸äº¤æµã€‚ç„¶è€Œï¼Œå…¬ä¼—å¯¹SLçš„è®¤è¯†æœ‰é™ï¼Œå¯¼è‡´å¬éšœäººå£«åœ¨æ•™è‚²åŠèŒä¸šæœºä¼šæ–¹é¢é­å—ä¸å…¬å¹³å¾…é‡ï¼ŒåŠ å‰§äº†ç¤¾ä¼šæ’æ–¥ç°è±¡ã€‚åœ¨æ²™ç‰¹é˜¿æ‹‰ä¼¯ç­‰å›½å®¶ï¼Œè¶…è¿‡84,000äººä¾èµ–é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­ï¼ˆSSLï¼‰ä½œä¸ºä¸»è¦äº¤æµæ–¹å¼ï¼Œå› æ­¤æ›´éœ€å…³æ³¨é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­çš„ç²¾ç¡®ç¿»è¯‘æŠ€æœ¯ã€‚å½“å‰ä¸»æµè§£å†³æ–¹æ¡ˆä¸»è¦å…³æ³¨éé˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­ï¼Œç¼ºä¹é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­çš„èµ„æºï¼Œå°¤å…¶æ˜¯SSLçš„èµ„æºã€‚æˆ‘ä»¬çš„ç ”ç©¶è‡´åŠ›äºå¼€å‘SSLèµ„æºï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é¦–ä¸ªè¿ç»­çš„æ²™ç‰¹é˜¿æ‹‰ä¼¯æ‰‹è¯­æ•°æ®é›†KAU-CSSLï¼Œä¸“æ³¨äºå®Œæ•´çš„å¥å­ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œå¹¶ä¸ºSSLè¯†åˆ«å’Œç¿»è¯‘å¯ç”¨é«˜çº§è¯†åˆ«ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºTransformerçš„æ¨¡å‹ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ResNet-18è¿›è¡Œç©ºé—´ç‰¹å¾æå–å’ŒTransformer Encoderä¸åŒå‘LSTMå¤„ç†æ—¶åºä¾èµ–å…³ç³»ï¼Œåœ¨ç­¾åè€…ä¾èµ–æ¨¡å¼ä¸‹è¾¾åˆ°äº†99.02%çš„å‡†ç¡®ç‡ï¼Œåœ¨ç­¾åè€…ç‹¬ç«‹æ¨¡å¼ä¸‹è¾¾åˆ°äº†77.71%çš„å‡†ç¡®ç‡ã€‚è¿™ä¸ä»…ä¸ºSSLç¤¾åŒºæ”¹è¿›äº¤æµå·¥å…·é“ºå¹³äº†é“è·¯ï¼Œè€Œä¸”ä¸ºæ›´å¹¿æ³›çš„ç¬¦å·è¯­è¨€é¢†åŸŸåšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰‹åŠ¿è¯­è¨€ï¼ˆSLï¼‰æ˜¯å¬éšœäººå£«çš„é‡è¦æ²Ÿé€šæ–¹å¼ï¼Œä½†å…¬ä¼—å¯¹å…¶è®¤è¯†æœ‰é™ï¼Œå¯¼è‡´ç¤¾ä¼šæœºä¼šçš„ä¸å…¬å¹³åˆ†é…ã€‚</li>
<li>åœ¨æ²™ç‰¹é˜¿æ‹‰ä¼¯ï¼Œè¶…è¿‡84,000äººä¾èµ–é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­ï¼ˆSSLï¼‰ä½œä¸ºä¸»è¦äº¤æµæ‰‹æ®µï¼Œå‡¸æ˜¾äº†å¯¹ç²¾ç¡®ç¿»è¯‘æŠ€æœ¯çš„è¿«åˆ‡éœ€æ±‚ã€‚</li>
<li>å½“å‰çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆä¸»è¦å…³æ³¨éé˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­ï¼Œå¯¼è‡´é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­èµ„æºåŒ®ä¹ï¼Œå°¤å…¶æ˜¯SSLèµ„æºã€‚</li>
<li>æ¨å‡ºé¦–ä¸ªè¿ç»­çš„æ²™ç‰¹é˜¿æ‹‰ä¼¯æ‰‹è¯­æ•°æ®é›†KAU-CSSLï¼Œä¸“æ³¨äºå®Œæ•´çš„å¥å­ï¼Œä»¥ä¿ƒè¿›SSLç ”ç©¶ã€‚</li>
<li>æå‡ºçš„åŸºäºTransformerçš„æ¨¡å‹ç»“åˆäº†ç©ºé—´ç‰¹å¾æå–å’Œæ—¶åºä¾èµ–å¤„ç†ï¼Œè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚</li>
<li>ç ”ç©¶æˆæœä¸ºå¬éšœäººå£«æ”¹è¿›äº¤æµå·¥å…·é“ºå¹³äº†é“è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨SSLç¤¾åŒºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24feeaecabc76ac9906185bf0e998888.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad87b00d0cc82c0e4cb3bf3255c10a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cee333334c9458fb131b826e58afd3cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efe927fc7dcd9e36b9cf5cfa5ce9f8d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c0828ce638aade094a1bf98868a197b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb7a2044ccd753d6019f5c53c362fe4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Exploring-persuasive-Interactions-with-generative-social-robots-An-experimental-framework"><a href="#Exploring-persuasive-Interactions-with-generative-social-robots-An-experimental-framework" class="headerlink" title="Exploring persuasive Interactions with generative social robots: An   experimental framework"></a>Exploring persuasive Interactions with generative social robots: An   experimental framework</h2><p><strong>Authors:Stephan Vonschallen, Larissa Julia Corina Finsler, Theresa Schmiedel, Friederike Eyssel</strong></p>
<p>Integrating generative AI such as large language models into social robots has improved their ability to engage in natural, human-like communication. This study presents a method to examine their persuasive capabilities. We designed an experimental framework focused on decision making and tested it in a pilot that varied robot appearance and self-knowledge. Using qualitative analysis, we evaluated interaction quality, persuasion effectiveness, and the robotâ€™s communicative strategies. Participants generally experienced the interaction positively, describing the robot as competent, friendly, and supportive, while noting practical limits such as delayed responses and occasional speech-recognition errors. Persuasiveness was highly context dependent and shaped by robot behavior: participants responded well to polite, reasoned suggestions and expressive gestures, but emphasized the need for more personalized, context-aware arguments and clearer social roles. These findings suggest that generative social robots can influence user decisions, but their effectiveness depends on communicative nuance and contextual relevance. We propose refinements to the framework to further study persuasive dynamics between robots and human users. </p>
<blockquote>
<p>å°†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰æ•´åˆåˆ°ç¤¾ä¼šæœºå™¨äººä¸­ï¼Œå¢å¼ºäº†å®ƒä»¬è¿›è¡Œè‡ªç„¶ã€æ‹ŸäººåŒ–é€šä¿¡çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–¹æ³•æ¥æ£€éªŒå®ƒä»¬çš„åŠè¯´èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä»¥å†³ç­–åˆ¶å®šä¸ºé‡ç‚¹çš„å®éªŒæ¡†æ¶ï¼Œå¹¶åœ¨æœºå™¨äººå¤–è§‚å’Œè‡ªæˆ‘çŸ¥è¯†å„å¼‚çš„è¯•ç‚¹ä¸­è¿›è¡Œäº†æµ‹è¯•ã€‚é€šè¿‡å®šæ€§åˆ†æï¼Œæˆ‘ä»¬è¯„ä¼°äº†äº¤äº’è´¨é‡ã€åŠè¯´æœ‰æ•ˆæ€§å’Œæœºå™¨äººçš„é€šä¿¡ç­–ç•¥ã€‚å‚ä¸è€…æ™®éå¯¹äº¤äº’ä½“éªŒæŒç§¯ææ€åº¦ï¼Œè®¤ä¸ºæœºå™¨äººæœ‰èƒ½åŠ›ã€å‹å¥½ã€æ”¯æŒï¼ŒåŒæ—¶æ³¨æ„åˆ°å®é™…é™åˆ¶ï¼Œå¦‚å“åº”å»¶è¿Ÿå’Œå¶å°”çš„è¯­éŸ³è¯†åˆ«é”™è¯¯ã€‚åŠè¯´æ€§é«˜åº¦ä¾èµ–äºä¸Šä¸‹æ–‡ï¼Œå—æœºå™¨äººè¡Œä¸ºå½±å“ï¼šå‚ä¸è€…å¯¹ç¤¼è²Œã€åˆç†çš„å»ºè®®å’Œè¡¨è¾¾æ€§æ‰‹åŠ¿ååº”è‰¯å¥½ï¼Œä½†å¼ºè°ƒéœ€è¦æ›´å¤šä¸ªæ€§åŒ–ã€åŸºäºä¸Šä¸‹æ–‡çš„è®ºè¯å’Œæ›´æ˜ç¡®çš„ç¤¾ä¼šè§’è‰²ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç”Ÿæˆå¼ç¤¾ä¼šæœºå™¨äººå¯ä»¥å½±å“ç”¨æˆ·çš„å†³ç­–ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºé€šä¿¡çš„ç»†å¾®å·®åˆ«å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚æˆ‘ä»¬æè®®å¯¹æ¡†æ¶è¿›è¡Œæ”¹è¿›ï¼Œä»¥è¿›ä¸€æ­¥ç ”ç©¶æœºå™¨äººå’Œäººç±»ç”¨æˆ·ä¹‹é—´çš„åŠè¯´åŠ¨æ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03231v1">PDF</a> A shortened version of this paper was accepted as poster for the   Thirteenth International Conference on Human-Agent Interaction (HAI2025)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ç­‰ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„ç¤¾ä¼šæœºå™¨äººï¼Œæå‡äº†å…¶è¿›è¡Œè‡ªç„¶ã€äººç±»èˆ¬äº¤æµçš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–¹æ³•æ¥ç ”ç©¶ä»–ä»¬çš„è¯´æœåŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä»¥å†³ç­–åˆ¶å®šä¸ºé‡ç‚¹çš„å®éªŒæ¡†æ¶ï¼Œå¹¶åœ¨è¯•ç‚¹ä¸­æµ‹è¯•äº†ä¸åŒæœºå™¨äººå¤–è§‚å’Œè‡ªæˆ‘è®¤çŸ¥çš„å½±å“ã€‚é€šè¿‡å®šæ€§åˆ†æï¼Œæˆ‘ä»¬è¯„ä¼°äº†äº’åŠ¨è´¨é‡ã€è¯´æœæ•ˆæœå’Œæœºå™¨äººçš„æ²Ÿé€šç­–ç•¥ã€‚å‚ä¸è€…æ™®éå¯¹äº’åŠ¨ä½“éªŒæŒç§¯ææ€åº¦ï¼Œè®¤ä¸ºæœºå™¨äººæœ‰èƒ½åŠ›ã€å‹å¥½å’Œæ”¯æŒæ€§ï¼ŒåŒæ—¶ä¹Ÿæ³¨æ„åˆ°ä¸€äº›å®é™…é™åˆ¶ï¼Œå¦‚ååº”å»¶è¿Ÿå’Œå¶å°”çš„è¯­éŸ³è¯†åˆ«é”™è¯¯ã€‚è¯´æœåŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºä¸Šä¸‹æ–‡å’Œæœºå™¨äººè¡Œä¸ºï¼šå‚ä¸è€…å¯¹ç¤¼è²Œã€ç†æ€§çš„å»ºè®®å’Œè¡¨è¾¾æ€§æ‰‹åŠ¿ååº”è‰¯å¥½ï¼Œä½†å¼ºè°ƒéœ€è¦æ›´å¤šä¸ªæ€§åŒ–ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è®ºè¯å’Œæ˜ç¡®çš„ç¤¾ä¼šè§’è‰²ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç”Ÿæˆçš„ç¤¾ä¼šæœºå™¨äººå¯ä»¥å½±å“ç”¨æˆ·çš„å†³ç­–ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºæ²Ÿé€šçš„ç»†å¾®å·®åˆ«å’Œä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬æå‡ºäº†å¯¹è¯¥æ¡†æ¶çš„æ”¹è¿›ï¼Œä»¥è¿›ä¸€æ­¥ç ”ç©¶æœºå™¨äººå’Œäººç±»ç”¨æˆ·ä¹‹é—´çš„è¯´æœåŠ¨æ€ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIå¢å¼ºäº†ç¤¾ä¼šæœºå™¨äººçš„è‡ªç„¶äº¤æµèƒ½åŠ›ï¼Œä½¿å…¶æ›´ç±»ä¼¼äºäººç±»äº¤æµã€‚</li>
<li>é€šè¿‡å®éªŒæ¡†æ¶è¯„ä¼°äº†æœºå™¨äººåœ¨å†³ç­–åœºæ™¯ä¸­çš„è¯´æœåŠ›ã€‚</li>
<li>å‚ä¸è€…å¯¹æœºå™¨äººäº’åŠ¨æŒç§¯ææ€åº¦ï¼Œè®¤ä¸ºæœºå™¨äººå…·æœ‰èƒ½åŠ›å’Œå‹å¥½æ€§ï¼Œä½†ä¹ŸæŒ‡å‡ºäº†å®é™…é™åˆ¶ã€‚</li>
<li>è¯´æœåŠ›å—ä¸Šä¸‹æ–‡å’Œæœºå™¨äººè¡Œä¸ºå½±å“ï¼Œéœ€è¦æœºå™¨äººå…·å¤‡ç¤¼è²Œã€ç†æ€§çš„å»ºè®®å’Œè¡¨è¾¾æ€§æ‰‹åŠ¿ç­‰æ²Ÿé€šæŠ€å·§ã€‚</li>
<li>å‚ä¸è€…å¼ºè°ƒéœ€è¦æ›´å¤šä¸ªæ€§åŒ–ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è®ºè¯å’Œæ˜ç¡®çš„ç¤¾ä¼šè§’è‰²ã€‚</li>
<li>ç¤¾ä¼šæœºå™¨äººçš„æœ‰æ•ˆæ€§åœ¨äºå…¶æ²Ÿé€šçš„ç»†å¾®å·®åˆ«å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ec46c21269250573d5761b9fb7b8254.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a672fc17df5417291900bb15bb34097.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Speech-DF-Arena-A-Leaderboard-for-Speech-DeepFake-Detection-Models"><a href="#Speech-DF-Arena-A-Leaderboard-for-Speech-DeepFake-Detection-Models" class="headerlink" title="Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models"></a>Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models</h2><p><strong>Authors:Sandipana Dowerah, Atharva Kulkarni, Ajinkya Kulkarni, Hoan My Tran, Joonas Kalda, Artem Fedorchenko, Benoit Fauve, Damien Lolive, Tanel AlumÃ¤e, Matthew Magimai Doss</strong></p>
<p>Parallel to the development of advanced deepfake audio generation, audio deepfake detection has also seen significant progress. However, a standardized and comprehensive benchmark is still missing. To address this, we introduce Speech DeepFake (DF) Arena, the first comprehensive benchmark for audio deepfake detection. Speech DF Arena provides a toolkit to uniformly evaluate detection systems, currently across 14 diverse datasets and attack scenarios, standardized evaluation metrics and protocols for reproducibility and transparency. It also includes a leaderboard to compare and rank the systems to help researchers and developers enhance their reliability and robustness. We include 14 evaluation sets, 12 state-of-the-art open-source and 3 proprietary detection systems. Our study presents many systems exhibiting high EER in out-of-domain scenarios, highlighting the need for extensive cross-domain evaluation. The leaderboard is hosted on Huggingface1 and a toolkit for reproducing results across the listed datasets is available on GitHub. </p>
<blockquote>
<p>éšç€å…ˆè¿›çš„æ·±åº¦ä¼ªé€ éŸ³é¢‘ç”ŸæˆæŠ€æœ¯çš„å‘å±•ï¼ŒéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç›®å‰ä»ç¼ºä¹æ ‡å‡†åŒ–å’Œå…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†è¯­éŸ³æ·±åº¦ä¼ªé€ ï¼ˆDFï¼‰ç«æŠ€åœºï¼Œè¿™æ˜¯éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹çš„é¦–ä¸ªå…¨é¢åŸºå‡†æµ‹è¯•ã€‚è¯­éŸ³DFç«æŠ€åœºæä¾›äº†ä¸€ä¸ªå·¥å…·åŒ…ï¼Œç”¨äºç»Ÿä¸€è¯„ä¼°æ£€æµ‹ç³»ç»Ÿï¼Œç›®å‰æ¶µç›–14ä¸ªä¸åŒçš„æ•°æ®é›†å’Œæ”»å‡»åœºæ™¯ï¼Œæä¾›å¯é‡å¤æ€§å’Œé€æ˜åº¦çš„æ ‡å‡†åŒ–è¯„ä¼°æŒ‡æ ‡å’Œåè®®ã€‚å®ƒè¿˜åŒ…æ‹¬ä¸€ä¸ªæ’è¡Œæ¦œï¼Œç”¨äºæ¯”è¾ƒå’Œæ’åç³»ç»Ÿï¼Œå¸®åŠ©ç ”ç©¶è€…å’Œå¼€å‘äººå‘˜æé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬åŒ…å«äº†14ä¸ªè¯„ä¼°é›†ï¼ŒåŒ…æ‹¬æœ€æ–°çš„ã€å¼€æºçš„æ£€æµ‹ç³»ç»Ÿä¸­æœ‰ä¸»æµçš„éŸ³é¢‘ç”Ÿæˆå¯¹æŠ—æ¨¡å‹çš„æ”»å‡»åœºæ™¯æ•°æ®é›†å…±åŒ…å«æœ‰12ä¸ªå¼€æºæ•°æ®é›†å’Œ3ä¸ªä¸“æœ‰æ£€æµ‹ç³»ç»Ÿã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è·¨åŸŸåœºæ™¯ä¸­æœ‰å¾ˆå¤šç³»ç»Ÿçš„ç­‰é”™è¯¯ç‡ï¼ˆEERï¼‰å¾ˆé«˜ï¼Œè¿™å‡¸æ˜¾äº†éœ€è¦è¿›è¡Œå¹¿æ³›è·¨åŸŸè¯„ä¼°çš„å¿…è¦æ€§ã€‚æ’è¡Œæ¦œæ‰˜ç®¡åœ¨Huggingfaceä¸Šï¼Œä¸€ä¸ªç”¨äºåœ¨æ‰€åˆ—æ•°æ®é›†ä¸Šå¤åˆ¶ç»“æœçš„å·¥å…·åŒ…å¯åœ¨GitHubä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02859v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€æ·±åº¦ä¼ªé€ éŸ³é¢‘æŠ€æœ¯çš„å‘å±•ï¼ŒéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ä¹Ÿå–å¾—äº†é‡è¦è¿›å±•ï¼Œä½†ä»ç¼ºå°‘æ ‡å‡†åŒ–çš„å…¨é¢åŸºå‡†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Speech DeepFake Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹åŸºå‡†ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå·¥å…·åŒ…ï¼Œå¯ä»¥ç»Ÿä¸€è¯„ä¼°æ£€æµ‹ç³»ç»Ÿï¼ŒåŒ…æ‹¬14ä¸ªä¸åŒçš„æ•°æ®é›†å’Œæ”»å‡»åœºæ™¯ï¼Œæ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡å’Œåè®®ï¼Œä»¥æé«˜å¯é‡å¤æ€§å’Œé€æ˜åº¦ã€‚æ­¤å¤–ï¼Œè¿˜åŒ…æ‹¬ä¸€ä¸ªæ’è¡Œæ¦œï¼Œå¯å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜æé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œé²æ£’æ€§ã€‚ç ”ç©¶å‘ç°åœ¨è·¨åŸŸåœºæ™¯ä¸­å­˜åœ¨é«˜è¯¯è¯†ç‡ï¼Œå¼ºè°ƒéœ€è¦è¿›è¡Œå¹¿æ³›çš„è·¨åŸŸè¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Speech DeepFake Arenaæ˜¯é¦–ä¸ªå…¨é¢çš„éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹åŸºå‡†ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„æ ‡å‡†åŒ–ç¼ºå¤±ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªå·¥å…·åŒ…ï¼Œç”¨äºç»Ÿä¸€è¯„ä¼°æ£€æµ‹ç³»ç»Ÿï¼Œæ¶µç›–å¤šç§æ•°æ®é›†å’Œæ”»å‡»åœºæ™¯ã€‚</li>
<li>æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡å’Œåè®®ï¼Œä»¥æé«˜ç ”ç©¶çš„å¯é‡å¤æ€§å’Œé€æ˜åº¦ã€‚</li>
<li>åŒ…å«ä¸€ä¸ªæ’è¡Œæ¦œï¼Œä»¥æ¯”è¾ƒå’Œæ’åæ£€æµ‹ç³»ç»Ÿï¼Œå¸®åŠ©æé«˜å…¶å¯é æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>ç ”ç©¶å‘ç°è·¨åŸŸåœºæ™¯ä¸­ç³»ç»Ÿè¡¨ç°å‡ºè¾ƒé«˜çš„è¯¯è¯†ç‡ï¼Œå¼ºè°ƒéœ€è¦æ›´å¤šè·¨åŸŸè¯„ä¼°ã€‚</li>
<li>Speech DeepFake Arenaåœ¨Huggingfaceä¸Šæ‰˜ç®¡äº†æ’è¡Œæ¦œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-31fd0a69f31843e975babb415e6d6f2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d221b8ffaf3f74db6ec7378b31b077bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15c35bf7485e44e3d79490e9ebda30f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35301340ab54f63372a1955149151b6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38d84b3d143aad2b619b416552d2cb05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21db6bf92b57650068cadde02fe91312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de7ed38807a2fd9dabb8e62e6c61783c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SSVD-Structured-SVD-for-Parameter-Efficient-Fine-Tuning-and-Benchmarking-under-Domain-Shift-in-ASR"><a href="#SSVD-Structured-SVD-for-Parameter-Efficient-Fine-Tuning-and-Benchmarking-under-Domain-Shift-in-ASR" class="headerlink" title="SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and   Benchmarking under Domain Shift in ASR"></a>SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and   Benchmarking under Domain Shift in ASR</h2><p><strong>Authors:Pu Wang, Shinji Watanabe, Hugo Van hamme</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for adapting large foundation models. While low-rank adaptation (LoRA) is widely used in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA, PiSSA, and SVFT, are developed mainly for language and vision tasks, with limited validation in speech. This work presents the first comprehensive integration and benchmarking of these PEFT methods within ESPnet. We further introduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates input-associated right singular vectors while keeping output-associated vectors fixed to preserve semantic mappings. This design enables robust domain adaptation with minimal trainable parameters and improved efficiency. We evaluate all methods on domain-shifted speech recognition tasks, including child speech and dialectal variation, across model scales from 0.1B to 2B. All implementations are released in ESPnet to support reproducibility and future work. </p>
<blockquote>
<p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å·²æˆä¸ºé€‚åº”å¤§å‹åŸºç¡€æ¨¡å‹çš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åœ¨è¯­éŸ³åº”ç”¨ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä½†å…¶æœ€æ–°å˜ä½“ï¼Œä¾‹å¦‚VeRAã€DoRAã€PiSSAå’ŒSVFTï¼Œä¸»è¦é’ˆå¯¹è¯­è¨€å’Œè§†è§‰ä»»åŠ¡å¼€å‘ï¼Œåœ¨è¯­éŸ³æ–¹é¢çš„éªŒè¯æœ‰é™ã€‚æœ¬ç ”ç©¶é¦–æ¬¡åœ¨ESPnetä¸­å…¨é¢é›†æˆå’Œè¯„ä¼°äº†è¿™äº›PEFTæ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç»“æ„åŒ–SVDå¼•å¯¼ï¼ˆSSVDï¼‰å¾®è°ƒï¼Œå®ƒé€‰æ‹©æ€§åœ°æ—‹è½¬ä¸è¾“å…¥ç›¸å…³çš„å³å¥‡å¼‚å‘é‡ï¼ŒåŒæ—¶ä¿æŒä¸è¾“å‡ºç›¸å…³çš„å‘é‡å›ºå®šï¼Œä»¥ä¿ç•™è¯­ä¹‰æ˜ å°„ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿåœ¨ä¿æŒå¯è®­ç»ƒå‚æ•°æœ€å°‘çš„æƒ…å†µä¸‹ï¼Œå®ç°ç¨³å¥çš„åŸŸé€‚åº”ï¼Œæé«˜æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨æ¨¡å‹è§„æ¨¡ä»0.1Båˆ°2Bçš„æƒ…å†µä¸‹ï¼Œå¯¹æ‰€æœ‰æ–¹æ³•åœ¨åŸŸè½¬ç§»çš„è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å„¿ç«¥è¯­éŸ³å’Œæ–¹è¨€å˜åŒ–ã€‚æ‰€æœ‰å®ç°éƒ½åœ¨ESPnetä¸­å‘å¸ƒï¼Œä»¥æ”¯æŒå¯é‡å¤æ€§å’Œæœªæ¥çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02830v1">PDF</a> Accepted by IEEE ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰åœ¨è¯­éŸ³åº”ç”¨ä¸­çš„é¦–æ¬¡å…¨é¢é›†æˆå’ŒåŸºå‡†æµ‹è¯•ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åŠå…¶æœ€æ–°å˜ç§ï¼ˆå¦‚VeRAã€DoRAã€PiSSAå’ŒSVFTï¼‰ï¼Œå¹¶é¦–æ¬¡å°†å…¶åº”ç”¨äºESPnetä¸­çš„è¯­éŸ³ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†ç»“æ„åŒ–çš„SVDå¼•å¯¼ï¼ˆSSVDï¼‰å¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€‰æ‹©æ€§åœ°æ—‹è½¬è¾“å…¥ç›¸å…³çš„å³å¥‡å¼‚å‘é‡ï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºç›¸å…³å‘é‡ä¸å˜ï¼Œä»¥ä¿ç•™è¯­ä¹‰æ˜ å°„ï¼Œå®ç°å…·æœ‰è¾ƒå°‘å¯è®­ç»ƒå‚æ•°çš„ç¨³å¥åŸŸé€‚åº”å¹¶æé«˜äº†æ•ˆç‡ã€‚åœ¨æ‰€æœ‰æ–¹æ³•ä¸Šï¼Œä½œè€…åœ¨å„¿ç«¥è¯­éŸ³å’Œæ–¹è¨€å˜åŒ–ç­‰è·¨æ¨¡å‹è§„æ¨¡çš„é¢†åŸŸè¿ç§»è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æ‰€æœ‰å®ç°å‡å·²å‘å¸ƒåœ¨ESPnetä¸Šï¼Œä»¥æ”¯æŒå¯é‡å¤æ€§å’Œæœªæ¥çš„å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å·²æˆä¸ºé€‚åº”å¤§å‹åŸºç¡€æ¨¡å‹çš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åŠå…¶æœ€æ–°å˜ç§åœ¨è¯­éŸ³ä»»åŠ¡ä¸­çš„åº”ç”¨å°šå¾…å……åˆ†éªŒè¯ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡å…¨é¢é›†æˆå’ŒåŸºå‡†æµ‹è¯•äº†PEFTæ–¹æ³•åœ¨ESPnetä¸­çš„è¯­éŸ³åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ç»“æ„åŒ–çš„SVDå¼•å¯¼ï¼ˆSSVDï¼‰å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°æ—‹è½¬è¾“å…¥ç›¸å…³çš„å³å¥‡å¼‚å‘é‡ï¼Œå®ç°ç¨³å¥çš„åŸŸé€‚åº”å¹¶æé«˜äº†æ•ˆç‡ã€‚</li>
<li>æ‰€æœ‰æ–¹æ³•åœ¨å„¿ç«¥è¯­éŸ³å’Œæ–¹è¨€å˜åŒ–çš„é¢†åŸŸè¿ç§»è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>æ‰€æœ‰æ–¹æ³•çš„å®ç°å‡å·²å‘å¸ƒåœ¨ESPnetä¸Šï¼Œä»¥æ”¯æŒç ”ç©¶çš„å¯é‡å¤æ€§å’Œæœªæ¥å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5879b29de518afcf0da654365465fea2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d42725ec401cb09217c70792145c21e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36ac551726f21d8fb8366ec9f45fc734.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36da36ed6a4303c6652db3ce90034b25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95b45aa563750c5d6bc0b20a41396d75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-191cb3b74357e796dc38e1621f0858e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7cadc93bbbd6e58a39d0ab254141bfbf.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Flavors-of-Moonshine-Tiny-Specialized-ASR-Models-for-Edge-Devices"><a href="#Flavors-of-Moonshine-Tiny-Specialized-ASR-Models-for-Edge-Devices" class="headerlink" title="Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices"></a>Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices</h2><p><strong>Authors:Evan King, Adam Sabra, Manjunath Kudlur, James Wang, Pete Warden</strong></p>
<p>We present the Flavors of Moonshine, a suite of tiny automatic speech recognition (ASR) models specialized for a range of underrepresented languages. Prevailing wisdom suggests that multilingual ASR models outperform monolingual counterparts by exploiting cross-lingual phonetic similarities. We challenge this assumption, showing that for sufficiently small models (27M parameters), training monolingual systems on a carefully balanced mix of high-quality human-labeled, pseudo-labeled, and synthetic data yields substantially superior performance. On average, our models achieve error rates 48% lower than the comparably sized Whisper Tiny model, outperform the 9x larger Whisper Small model, and in most cases match or outperform the 28x larger Whisper Medium model. These results advance the state of the art for models of this size, enabling accurate on-device ASR for languages that previously had limited support. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and Vietnamese Moonshine models under a permissive open-source license. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†â€œæœˆå…‰é£å‘³â€ç³»åˆ—ï¼Œè¿™æ˜¯ä¸€å¥—ä¸“ä¸ºä¸€ç³»åˆ—ä»£è¡¨æ€§ä¸è¶³çš„è¯­ç§è®¾è®¡çš„è¶…å°å‹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚æ™®éçš„çœ‹æ³•æ˜¯ï¼Œè·¨è¯­ç§è¯­éŸ³ç›¸ä¼¼æ€§çš„åˆ©ç”¨ä½¿å¾—å¤šè¯­ç§ASRæ¨¡å‹çš„æ€§èƒ½ä¼˜äºå•è¯­ç§æ¨¡å‹ã€‚æˆ‘ä»¬å¯¹æ­¤å‡è®¾æå‡ºè´¨ç–‘ï¼Œå¹¶è¯æ˜å¯¹äºè¶³å¤Ÿå°çš„æ¨¡å‹ï¼ˆ27Må‚æ•°ï¼‰ï¼Œåœ¨é«˜è´¨é‡äººå·¥æ ‡æ³¨ã€ä¼ªæ ‡æ³¨å’Œåˆæˆæ•°æ®çš„å¹³è¡¡æ··åˆä¸Šè®­ç»ƒå•è¯­ç§ç³»ç»Ÿä¼šäº§ç”Ÿæ˜¾è‘—ä¼˜è¶Šçš„æ€§èƒ½ã€‚å¹³å‡è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é”™è¯¯ç‡æ¯”åŒç±»å¤§å°çš„whisper tinyæ¨¡å‹ä½48%ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸whisper mediumæ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ï¼Œç”šè‡³è¶…è¿‡äº†9å€æ›´å¤§çš„whisper smallæ¨¡å‹ã€‚è¿™äº›ç»“æœä»£è¡¨äº†è¿™ä¸€è§„æ¨¡çš„æ¨¡å‹çš„æœ€æ–°æŠ€æœ¯ï¼Œä½¿å¾—ä¹‹å‰æ”¯æŒæœ‰é™çš„è¯­è¨€çš„è®¾å¤‡ç«¯ASRæ›´åŠ å‡†ç¡®ã€‚æˆ‘ä»¬å‘å¸ƒäº†é˜¿æ‹‰ä¼¯è¯­ã€ä¸­æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ã€ä¹Œå…‹å…°è¯­å’Œè¶Šå—è¯­çš„æœˆå…‰é£å‘³æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨äº†å®½æ¾çš„å¼€æºè®¸å¯åè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02523v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Flavors of Moonshineâ€”â€”ä¸€å¥—ä¸“ä¸ºå¤šç§ä»£è¡¨æ€§ä¸è¶³çš„è¯­ç§è®¾è®¡çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹å¥—ä»¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºè¶³å¤Ÿå°çš„æ¨¡å‹ï¼ˆå¦‚å…·æœ‰27Må‚æ•°çš„æ¨¡å‹ï¼‰ï¼Œåœ¨é«˜è´¨é‡äººå·¥æ ‡æ³¨ã€ä¼ªæ ‡æ³¨å’Œåˆæˆæ•°æ®çš„å¹³è¡¡æ··åˆä¸Šè®­ç»ƒå•è¯­ç§ç³»ç»Ÿï¼Œå¯ä»¥è·å¾—ä¼˜äºè·¨è¯­ç§æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚è¿™äº›æ¨¡å‹åœ¨é”™è¯¯ç‡ä¸Šå¹³å‡é™ä½äº†48%ï¼Œè¶…è¿‡äº†ç±»ä¼¼è§„æ¨¡çš„Whisper Tinyæ¨¡å‹ï¼Œå¹¶åœ¨å¤šæ•°æƒ…å†µä¸‹ä¸æˆ–è¶…è¶Šäº†28å€å¤§çš„Whisper Mediumæ¨¡å‹ã€‚è¿™ä¸€æˆæœæ¨è¿›äº†å°è§„æ¨¡æ¨¡å‹çš„æŠ€æœ¯å‰æ²¿ï¼Œä¸ºä¹‹å‰æ”¯æŒæœ‰é™çš„è¯­ç§æä¾›äº†å‡†ç¡®çš„è®¾å¤‡ç«¯ASRã€‚ç°å·²å‘å¸ƒåŒ…æ‹¬é˜¿æ‹‰ä¼¯è¯­ã€ä¸­æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ã€ä¹Œå…‹å…°è¯­å’Œè¶Šå—è¯­åœ¨å†…çš„Moonshineæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨äº†å®½æ¾çš„å¼€æºè®¸å¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Flavors of Moonshineæ˜¯ä¸€ç³»åˆ—ä¸“ä¸ºå¤šç§ä»£è¡¨æ€§ä¸è¶³çš„è¯­ç§è®¾è®¡çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶æŒ‘æˆ˜äº†ç°æœ‰çš„å‡è®¾ï¼Œå³åœ¨æŸäº›å°è§„æ¨¡æ¨¡å‹ä¸­ï¼Œå•è¯­ç§ASRæ¨¡å‹çš„æ€§èƒ½è¡¨ç°å¯èƒ½ä¼˜äºè·¨è¯­ç§æ¨¡å‹ã€‚</li>
<li>è®­ç»ƒæ•°æ®åŒ…æ‹¬é«˜è´¨é‡äººå·¥æ ‡æ³¨ã€ä¼ªæ ‡æ³¨å’Œåˆæˆæ•°æ®çš„å¹³è¡¡æ··åˆã€‚</li>
<li>æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œé”™è¯¯ç‡å¹³å‡é™ä½äº†48%ï¼Œè¶…è¿‡äº†ç›¸åŒè§„æ¨¡çš„Whisper Tinyæ¨¡å‹å’Œæ›´å¤§çš„Whisper Smallä¸Mediumæ¨¡å‹ã€‚</li>
<li>è¯¥æˆæœæ¨è¿›äº†å°è§„æ¨¡ASRæ¨¡å‹çš„æŠ€æœ¯å‘å±•ï¼Œå¹¶ä¸ºä¹‹å‰æ”¯æŒæœ‰é™çš„è¯­ç§æä¾›äº†æ›´å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ¨¡å‹å·²åœ¨å¤šç§è¯­è¨€ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬é˜¿æ‹‰ä¼¯è¯­ã€ä¸­æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ã€ä¹Œå…‹å…°è¯­å’Œè¶Šå—è¯­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd3af707feec7aa5473fffa6d6ab7150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-714c81b485b348ddd28c3d225b405537.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1095a6408d899d58c1892a23f821f971.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08f5cfe6549ced8d13100e03a66cc414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9fe28988ac11fb64069af1380bd0f64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-345b5ad4fb5b0a1a40f688da6f12089f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57e6b6632a2275c246c1c317eaaef9c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e01bf397d18b1ee3ae787f7d2a005050.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="NADI-2025-The-First-Multidialectal-Arabic-Speech-Processing-Shared-Task"><a href="#NADI-2025-The-First-Multidialectal-Arabic-Speech-Processing-Shared-Task" class="headerlink" title="NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task"></a>NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task</h2><p><strong>Authors:Bashar Talafha, Hawau Olamide Toyin, Peter Sullivan, AbdelRahim Elmadany, Abdurrahman Juma, Amirbek Djanibekov, Chiyu Zhang, Hamad Alshehhi, Hanan Aldarmaki, Mustafa Jarrar, Nizar Habash, Muhammad Abdul-Mageed</strong></p>
<p>We present the findings of the sixth Nuanced Arabic Dialect Identification (NADI 2025) Shared Task, which focused on Arabic speech dialect processing across three subtasks: spoken dialect identification (Subtask 1), speech recognition (Subtask 2), and diacritic restoration for spoken dialects (Subtask 3). A total of 44 teams registered, and during the testing phase, 100 valid submissions were received from eight unique teams. The distribution was as follows: 34 submissions for Subtask 1 â€œfive teams{\ae}, 47 submissions for Subtask 2 â€œsix teamsâ€, and 19 submissions for Subtask 3 â€œtwo teamsâ€. The best-performing systems achieved 79.8% accuracy on Subtask 1, 35.68&#x2F;12.20 WER&#x2F;CER (overall average) on Subtask 2, and 55&#x2F;13 WER&#x2F;CER on Subtask 3. These results highlight the ongoing challenges of Arabic dialect speech processing, particularly in dialect identification, recognition, and diacritic restoration. We also summarize the methods adopted by participating teams and briefly outline directions for future editions of NADI. </p>
<blockquote>
<p>æˆ‘ä»¬å‘ˆç°ç¬¬å…­æ¬¡å§”å©‰é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€è¯†åˆ«ï¼ˆNADI 2025ï¼‰å…±äº«ä»»åŠ¡çš„å‘ç°ç»“æœï¼Œè¯¥ä»»åŠ¡èšç„¦äºä¸‰ä¸ªå­ä»»åŠ¡çš„é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³æ–¹è¨€å¤„ç†ï¼šå£è¯­æ–¹è¨€è¯†åˆ«ï¼ˆå­ä»»åŠ¡1ï¼‰ã€è¯­éŸ³è¯†åˆ«ï¼ˆå­ä»»åŠ¡2ï¼‰ä»¥åŠå£è¯­æ–¹è¨€çš„å˜éŸ³ç¬¦æ¢å¤ï¼ˆå­ä»»åŠ¡3ï¼‰ã€‚å…±æœ‰44æ”¯é˜Ÿä¼æ³¨å†Œå‚ä¸ï¼Œæµ‹è¯•é˜¶æ®µå…±æ”¶åˆ°æ¥è‡ª8æ”¯ä¸åŒé˜Ÿä¼çš„100ä»½æœ‰æ•ˆæäº¤ã€‚åˆ†å¸ƒå¦‚ä¸‹ï¼šå­ä»»åŠ¡1æœ‰34ä»½æäº¤ï¼ˆâ€œäº”æ”¯é˜Ÿä¼â€ï¼‰ï¼Œå­ä»»åŠ¡2æœ‰47ä»½æäº¤ï¼ˆâ€œå…­æ”¯é˜Ÿä¼â€ï¼‰ï¼Œå­ä»»åŠ¡3æœ‰19ä»½æäº¤ï¼ˆâ€œä¸¤æ”¯é˜Ÿä¼â€ï¼‰ã€‚è¡¨ç°æœ€ä½³çš„ç³»ç»Ÿçš„å‡†ç¡®ç‡åœ¨å­ä»»åŠ¡1ä¸Šè¾¾åˆ°79.8%ï¼Œå­ä»»åŠ¡2çš„æ•´ä½“å¹³å‡WER&#x2F;CERä¸º35.68&#x2F;12.20ï¼Œå­ä»»åŠ¡3çš„WER&#x2F;CERä¸º55&#x2F;13ã€‚è¿™äº›ç»“æœçªæ˜¾äº†é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€è¯­éŸ³å¤„ç†ï¼Œå°¤å…¶æ˜¯æ–¹è¨€è¯†åˆ«ã€è¯†åˆ«å’Œå˜éŸ³ç¬¦æ¢å¤çš„æŒç»­æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜æ€»ç»“äº†å‚ä¸å›¢é˜Ÿé‡‡ç”¨çš„æ–¹æ³•ï¼Œå¹¶ç®€è¦æ¦‚è¿°äº†æœªæ¥NADIç‰ˆæœ¬çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02038v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é˜¿æ‹‰ä¼¯è¯­éŸ³æ–¹è¨€å¤„ç†çš„æœ€æ–°ç ”ç©¶æˆæœå…¬å¸ƒï¼Œæ¶‰åŠæ–¹è¨€è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œæ–¹è¨€å˜éŸ³æ¢å¤ä¸‰ä¸ªå­ä»»åŠ¡ã€‚å…±æœ‰44æ”¯é˜Ÿä¼å‚ä¸ï¼Œæœ€ç»ˆæœ‰å…«æ”¯é˜Ÿä¼æäº¤æœ‰æ•ˆæˆæœã€‚æœ€ä½³ç³»ç»Ÿçš„è¡¨ç°æ˜¾ç¤ºï¼Œæ–¹è¨€è¯†åˆ«å‡†ç¡®ç‡ä¸º79.8%ï¼Œè¯­éŸ³è¯†åˆ«å¹³å‡WER&#x2F;CERä¸º35.68&#x2F;12.20ï¼Œæ–¹è¨€å˜éŸ³æ¢å¤å¹³å‡WER&#x2F;CERä¸º55&#x2F;13ã€‚è¿™äº›ç»“æœçªæ˜¾äº†é˜¿æ‹‰ä¼¯æ–¹è¨€è¯­éŸ³å¤„ç†çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NADI 2025å…±äº«ä»»åŠ¡çš„ç›®çš„æ˜¯ç ”ç©¶é˜¿æ‹‰ä¼¯è¯­éŸ³æ–¹è¨€å¤„ç†ã€‚</li>
<li>ä»»åŠ¡åŒ…æ‹¬ä¸‰ä¸ªå­ä»»åŠ¡ï¼šæ–¹è¨€è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œæ–¹è¨€å˜éŸ³æ¢å¤ã€‚</li>
<li>å…±æœ‰44æ”¯é˜Ÿä¼å‚ä¸ï¼Œæµ‹è¯•é˜¶æ®µæœ‰å…«æ”¯é˜Ÿä¼æäº¤æœ‰æ•ˆæˆæœã€‚</li>
<li>æœ€ä½³ç³»ç»Ÿåœ¨æ–¹è¨€è¯†åˆ«ä¸Šè¾¾åˆ°79.8%å‡†ç¡®ç‡ï¼Œåœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢ï¼Œå¹³å‡WER&#x2F;CERä¸º35.68&#x2F;12.20ï¼Œæ–¹è¨€å˜éŸ³æ¢å¤å¹³å‡ä¸ºWER&#x2F;CERä¸º55&#x2F;13ã€‚è¿™äº›ç»“æœæ˜¾ç¤ºäº†ä¸å°çš„æŒ‘æˆ˜å’Œè¿›æ­¥ç©ºé—´ã€‚ </li>
<li>è¿™äº›æˆæœæä¾›äº†å¯¹é˜¿æ‹‰ä¼¯æ–¹è¨€è¯­éŸ³å¤„ç†çš„æ·±å…¥äº†è§£ã€‚ </li>
<li>å‚ä¸å›¢é˜Ÿé‡‡ç”¨çš„æ–¹æ³•æ€»ç»“ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6225d62dad8397935288da3cac838d24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-145288a9c9c3e42c22b2d63b0e23f07d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f9d710042804c86e9885f053953813e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8a548b15de93802543d082d69c2ab2f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Group-Relative-Policy-Optimization-for-Speech-Recognition"><a href="#Group-Relative-Policy-Optimization-for-Speech-Recognition" class="headerlink" title="Group Relative Policy Optimization for Speech Recognition"></a>Group Relative Policy Optimization for Speech Recognition</h2><p><strong>Authors:Prashanth Gurunath Shivakumar, Yile Gu, Ankur Gandhe, Ivan Bulyko</strong></p>
<p>Speech Recognition has seen a dramatic shift towards adopting Large Language Models (LLMs). This shift is partly driven by good scalability properties demonstrated by LLMs, ability to leverage large amounts of labelled, unlabelled speech and text data, streaming capabilities with auto-regressive framework and multi-tasking with instruction following characteristics of LLMs. However, simple next-token prediction objective, typically employed with LLMs, have certain limitations in performance and challenges with hallucinations. In this paper, we propose application of Group Relative Policy Optimization (GRPO) to enable reinforcement learning from human feedback for automatic speech recognition (ASR). We design simple rule based reward functions to guide the policy updates. We demonstrate significant improvements in word error rate (upto 18.4% relative), reduction in hallucinations, increased robustness on out-of-domain datasets and effectiveness in domain adaptation. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«é¢†åŸŸå·²ç»å‘ç”Ÿäº†å‘é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡å¤§è½¬å˜ã€‚è¿™ä¸€è½¬å˜éƒ¨åˆ†æ˜¯ç”±LLMå±•ç¤ºçš„è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€åˆ©ç”¨å¤§é‡æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾çš„è¯­éŸ³å’Œæ–‡æœ¬æ•°æ®çš„èƒ½åŠ›ã€ä¸è‡ªå›å½’æ¡†æ¶çš„æµå¼ä¼ è¾“èƒ½åŠ›ä»¥åŠLLMçš„éµå¾ªæŒ‡ä»¤çš„å¤šä»»åŠ¡å¤„ç†ç‰¹æ€§æ‰€é©±åŠ¨çš„ã€‚ç„¶è€Œï¼Œé€šå¸¸ä¸LLMä¸€èµ·ä½¿ç”¨çš„ç®€å•ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ç›®æ ‡åœ¨æ€§èƒ½å’Œå¹»è§†æ–¹é¢å…·æœ‰ä¸€å®šçš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åº”ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥å®ç°åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼Œç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚æˆ‘ä»¬è®¾è®¡äº†åŸºäºç®€å•è§„åˆ™çš„å¥–åŠ±å‡½æ•°æ¥æŒ‡å¯¼ç­–ç•¥æ›´æ–°ã€‚æˆ‘ä»¬åœ¨è¯é”™è¯¯ç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ˆç›¸å¯¹é™ä½äº†18.4%ï¼‰ï¼Œå¹»è§†å‡å°‘ï¼Œå¯¹åŸŸå¤–æ•°æ®é›†çš„ç¨³å¥æ€§å¢å¼ºï¼Œä»¥åŠåŸŸé€‚åº”çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01939v1">PDF</a> Accepted for ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³è¯†åˆ«æŠ€æœ¯åœ¨è¯¸å¤šé¢†åŸŸè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œä½†å…¶æ€§èƒ½è¡¨ç°ä»ç„¶å—é™äºå•ä¸€çš„åº”ç”¨æ¨¡å‹å’Œç®—æ³•çš„å±€é™æ€§å’ŒæŒ‘æˆ˜ã€‚ä¸ºäº†æ”¹å–„æ€§èƒ½å’Œæé«˜é€‚åº”èƒ½åŠ›ï¼Œæœ¬ç ”ç©¶å°†å¼•å…¥Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•å’Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡äººç±»åé¦ˆè¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚æœ¬ç ”ç©¶é€šè¿‡è®¾è®¡åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°æ¥å¼•å¯¼ç­–ç•¥æ›´æ–°ï¼Œå¹¶åœ¨å®éªŒä¸­å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¦‚è¯é”™è¯¯ç‡é™ä½äº†é«˜è¾¾ç™¾åˆ†ä¹‹åå…«ç‚¹å››çš„ç›¸å¯¹è¯¯å·®ï¼Œå‡å°‘â€œå‡æƒ³â€ï¼ˆhallucinationsï¼‰ç°è±¡ï¼Œå¢å¼ºäº†åœ¨é¢†åŸŸå¤–çš„æ•°æ®é›†ä¸Šçš„ç¨³å¥æ€§ï¼Œä»¥åŠæœ‰æ•ˆçš„é¢†åŸŸé€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆä¸ºè¯­éŸ³è¯†åˆ«é¢†åŸŸçš„é‡è¦æŠ€æœ¯è¶‹åŠ¿ã€‚</li>
<li>LLMå…·å¤‡å¯æ‰©å±•æ€§ã€åˆ©ç”¨å¤§é‡æ ‡ç­¾å’Œæ— æ ‡ç­¾è¯­éŸ³å’Œæ–‡æœ¬æ•°æ®çš„èƒ½åŠ›ã€æµå¼å¤„ç†å’Œå¤šä»»åŠ¡å¤„ç†èƒ½åŠ›ç­‰å…³é”®ä¼˜åŠ¿ã€‚</li>
<li>ç®€å•ä¸‹ä¸€ä¸ªè¯é¢„æµ‹ç›®æ ‡ï¼ˆnext-token prediction objectiveï¼‰åœ¨æ€§èƒ½å’Œå‡æƒ³ç°è±¡æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶å¼•å…¥äº†Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•ä»¥æ”¹å–„è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>é€šè¿‡è®¾è®¡åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°æ¥å¼•å¯¼ç­–ç•¥æ›´æ–°ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRPOåœ¨è¯é”™è¯¯ç‡ã€å‡æƒ³ç°è±¡ã€ç¨³å¥æ€§å’Œé¢†åŸŸé€‚åº”æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8fbe04b4e481d23ff04ec4b859fa6fce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4bb96b44c84b17fb927377e80c9991bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb27fd8bb207a547872b6d833a1eb9d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-964c41566b5dcf22d5252a7f4f2ffd20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd071b593e87409c657b433a0ba57ab7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multilingual-Speech-Recognition-Using-Discrete-Tokens-with-a-Two-step-Training-Strategy"><a href="#Multilingual-Speech-Recognition-Using-Discrete-Tokens-with-a-Two-step-Training-Strategy" class="headerlink" title="Multilingual Speech Recognition Using Discrete Tokens with a Two-step   Training Strategy"></a>Multilingual Speech Recognition Using Discrete Tokens with a Two-step   Training Strategy</h2><p><strong>Authors:Zehan Li, Yan Yang, Xueqing Li, Jian Kang, Xiao-Lei Zhang, Jie Li</strong></p>
<p>Pre-trained models, especially self-supervised learning (SSL) models, have demonstrated impressive results in automatic speech recognition (ASR) task. While most applications of SSL models focus on leveraging continuous representations as features for training downstream tasks, the utilization of discrete units has gained increasing attention in recent years owing to its lower storage requirements and broader range of applications. In multilingual ASR tasks, representations at different layers of the model contribute differently to various languages, complicating the unification of discrete unit modeling. In this paper, we propose a two-stage training strategy to improve the discrete token performance of pre-trained models and narrow the gap with continuous representation performance. We validate our method on the XLS-R model following the settings of Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge. Our method demonstrates a significant improvement on the ML-SUPERB dataset, achieving a 44% relative reduction on CER for the XLS-R model. This surpasses the previous baseline set by the WavLM model, which achieves a 26% relative reduction on CER. Furthermore, our method achieves the first place among all the single-system results on the leaderboard. </p>
<blockquote>
<p>é¢„è®­ç»ƒæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹ï¼Œåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚è™½ç„¶å¤§å¤šæ•°SSLæ¨¡å‹çš„åº”ç”¨ä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨è¿ç»­è¡¨ç¤ºä¸ºä¸‹æ¸¸ä»»åŠ¡çš„ç‰¹å¾ï¼Œä½†ç”±äºå…¶è¾ƒä½çš„å­˜å‚¨éœ€æ±‚å’Œæ›´å¹¿æ³›çš„åº”ç”¨èŒƒå›´ï¼Œç¦»æ•£å•å…ƒçš„ä½¿ç”¨è¿‘å¹´æ¥å—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚åœ¨å¤šè¯­ç§ASRä»»åŠ¡ä¸­ï¼Œæ¨¡å‹ä¸åŒå±‚çº§çš„è¡¨ç¤ºå¯¹ä¸åŒè¯­è¨€çš„å½±å“ä¸åŒï¼Œè¿™ä½¿å¾—ç¦»æ•£å•å…ƒå»ºæ¨¡çš„ç»Ÿä¸€å˜å¾—å¤æ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜é¢„è®­ç»ƒæ¨¡å‹çš„ç¦»æ•£ä»¤ç‰Œæ€§èƒ½ï¼Œå¹¶ç¼©å°ä¸è¿ç»­è¡¨ç¤ºæ€§èƒ½çš„å·®è·ã€‚æˆ‘ä»¬åœ¨éµå¾ªInterspeech2024ä½¿ç”¨ç¦»æ•£è¯­éŸ³å•å…ƒæŒ‘æˆ˜è®¾ç½®çš„æ¡ä»¶ä¸‹ï¼Œå¯¹XLS-Ræ¨¡å‹è¿›è¡Œäº†æ–¹æ³•éªŒè¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ML-SUPERBæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œç›¸å¯¹è¯¯å·®ç‡ï¼ˆCERï¼‰é™ä½äº†44%ï¼Œè¶…è¶Šäº†ä¹‹å‰ç”±WavLMæ¨¡å‹è®¾å®šçš„åŸºçº¿ï¼Œåè€…å®ç°äº†26%çš„CERç›¸å¯¹é™ä½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ’è¡Œæ¦œä¸Šè·å¾—äº†å•ç³»ç»Ÿç¬¬ä¸€åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01900v1">PDF</a> Accepted by NCMMSC 2024</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚æœ¬æ–‡å…³æ³¨è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹åœ¨ç¦»æ•£å•å…ƒå»ºæ¨¡æ–¹é¢çš„åº”ç”¨ï¼Œæå‡ºä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥æå‡é¢„è®­ç»ƒæ¨¡å‹çš„ç¦»æ•£ä»¤ç‰Œæ€§èƒ½ï¼Œå¹¶ç¼©å°å…¶ä¸è¿ç»­è¡¨ç¤ºæ€§èƒ½ä¹‹é—´çš„å·®è·ã€‚åœ¨Multilingual ASRä»»åŠ¡ä¸­ï¼Œæ¨¡å‹ä¸åŒå±‚å¯¹ä¸åŒè¯­è¨€çš„è¡¨ç¤ºè´¡çŒ®ä¸åŒï¼Œæˆ‘ä»¬é€šè¿‡åœ¨XLS-Ræ¨¡å‹ä¸ŠéªŒè¯è¯¥æ–¹æ³•ï¼Œå¹¶åœ¨Interspeech2024ç¦»æ•£è¯­éŸ³å•å…ƒæŒ‘æˆ˜çš„è®¾ç½®ä¸­å–å¾—æ˜¾è‘—æˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ML-SUPERBæ•°æ®é›†ä¸Šå®ç°äº†44%çš„ç›¸å¯¹é”™è¯¯ç‡é™ä½ï¼Œè¶…è¶Šäº†WavLMæ¨¡å‹çš„åŸºçº¿è¡¨ç°ï¼ŒåŒæ—¶åœ¨æ’è¡Œæ¦œä¸Šè·å¾—äº†ç¬¬ä¸€åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç¦»æ•£å•å…ƒå»ºæ¨¡åœ¨è¿‘å¹´æ¥å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå› å…¶å…·æœ‰è¾ƒä½çš„å­˜å‚¨éœ€æ±‚å’Œæ›´å¹¿æ³›çš„åº”ç”¨èŒƒå›´ã€‚</li>
<li>åœ¨å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹ä¸åŒå±‚å¯¹ä¸åŒè¯­è¨€çš„è¡¨ç¤ºè´¡çŒ®ä¸åŒã€‚</li>
<li>æå‡ºçš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥èƒ½æå‡é¢„è®­ç»ƒæ¨¡å‹çš„ç¦»æ•£ä»¤ç‰Œæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨XLS-Ræ¨¡å‹ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶åœ¨Interspeech2024æŒ‘æˆ˜ä¸­å–å¾—æ˜¾è‘—æˆæœã€‚</li>
<li>æ–¹æ³•åœ¨ML-SUPERBæ•°æ®é›†ä¸Šå®ç°äº†44%çš„ç›¸å¯¹é”™è¯¯ç‡é™ä½ï¼Œè¶…è¶Šäº†WavLMæ¨¡å‹çš„åŸºçº¿è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ec7d24b8e42865dfe831dd14f8c2d936.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70e5a92c17972d7a166d348d3adecddf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-619cccd67b18bf0775cc359b651d2650.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9468f9061cd8e9df8a9c5da04386f907.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629fa0dd87a6aa9b7b1a3d7afe12c4d0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Characterization-of-Speech-Similarity-Between-Australian-Aboriginal-and-High-Resource-Languages-A-Case-Study-on-Dharawal"><a href="#Characterization-of-Speech-Similarity-Between-Australian-Aboriginal-and-High-Resource-Languages-A-Case-Study-on-Dharawal" class="headerlink" title="Characterization of Speech Similarity Between Australian Aboriginal and   High-Resource Languages: A Case Study on Dharawal"></a>Characterization of Speech Similarity Between Australian Aboriginal and   High-Resource Languages: A Case Study on Dharawal</h2><p><strong>Authors:Ting Dang, Trini Manoj Jeyaseelan, Eliathamby Ambikairajah, Vidhyasaharan Sethu</strong></p>
<p>Australian Aboriginal languages are of significant cultural and linguistic value but remain severely underrepresented in modern speech AI systems. While state-of-the-art speech foundation models and automatic speech recognition excel in high-resource settings, they often struggle to generalize to low-resource languages, especially those lacking clean, annotated speech data. In this work, we collect and clean a speech dataset for Dharawal, a low-resource Australian Aboriginal language, by carefully sourcing and processing publicly available recordings. Using this dataset, we analyze the speech similarity between Dharawal and 107 high-resource languages using a pre-trained multilingual speech encoder. Our approach combines (1) misclassification rate analysis to assess language confusability, and (2) fine-grained similarity measurements using cosine similarity and Fr&#39;echet Inception Distance (FID) in the embedding space. Experimental results reveal that Dharawal shares strong speech similarity with languages such as Latin, M=aori, Korean, Thai, and Welsh. These findings offer practical guidance for future transfer learning and model adaptation efforts, and underscore the importance of data collection and embedding-based analysis in supporting speech technologies for endangered language communities. </p>
<blockquote>
<p>æ¾³å¤§åˆ©äºšåŸä½æ°‘è¯­è¨€åœ¨æ–‡åŒ–åŠè¯­è¨€ä¸Šå…·æœ‰é‡å¤§ä»·å€¼ï¼Œä½†åœ¨ç°ä»£è¯­éŸ³äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­å´ä¸¥é‡ç¼ºä¹ä»£è¡¨æ€§ã€‚è™½ç„¶æœ€å…ˆè¿›çš„è¯­éŸ³åŸºç¡€æ¨¡å‹å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯åœ¨èµ„æºä¸°å¯Œçš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥æ¨å¹¿åˆ°èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œå°¤å…¶æ˜¯é‚£äº›ç¼ºä¹å¹²å‡€ã€æ³¨é‡Šçš„è¯­éŸ³æ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä»”ç»†æœç´¢å’Œå¤„ç†å…¬å¼€å¯ç”¨çš„å½•éŸ³ï¼Œæ”¶é›†å’Œæ¸…ç†äº†ä¸€ä¸ªç”¨äºè¾¾æ‹‰ç“¦ï¼ˆä¸€ç§èµ„æºåŒ®ä¹çš„æ¾³å¤§åˆ©äºšåŸä½æ°‘è¯­è¨€ï¼‰çš„è¯­éŸ³æ•°æ®é›†ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨åˆ†æè¾¾æ‹‰ç“¦è¯­ä¸107ç§èµ„æºä¸°å¯Œè¯­è¨€çš„è¯­éŸ³ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ï¼ˆ1ï¼‰é€šè¿‡è¯¯åˆ†ç±»ç‡åˆ†ææ¥è¯„ä¼°è¯­è¨€æ··æ·†æ€§ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å’ŒFrâ€™echet Inception Distanceï¼ˆFIDï¼‰åœ¨åµŒå…¥ç©ºé—´ä¸­è¿›è¡Œç²¾ç»†ç›¸ä¼¼æ€§æµ‹é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¾¾æ‹‰ç“¦è¯­ä¸æ‹‰ä¸è¯­ã€æ¯›åˆ©è¯­ã€éŸ©è¯­ã€æ³°è¯­å’Œå¨å°”å£«è¯­ç­‰è¯­è¨€æœ‰å¾ˆå¼ºçš„è¯­éŸ³ç›¸ä¼¼æ€§ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥è¿ç§»å­¦ä¹ å’Œæ¨¡å‹é€‚åº”å·¥ä½œæä¾›äº†å®é™…æŒ‡å¯¼ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®é‡‡é›†å’ŒåŸºäºåµŒå…¥çš„åˆ†æå¯¹äºæ”¯æŒæ¿’å±è¯­è¨€ç¤¾åŒºçš„è¯­éŸ³æŠ€æœ¯çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01419v1">PDF</a> Accepted at APSIPA ASC 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>é’ˆå¯¹æ¾³å¤§åˆ©äºšåŸä½æ°‘è¯­è¨€åœ¨ç°ä»£è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­çš„ä¸¥é‡ç¼ºä¹ä»£è¡¨æ€§é—®é¢˜ï¼Œæœ¬æ–‡æ”¶é›†å¹¶æ¸…ç†äº†Dharawalï¼ˆä¸€ç§ä½èµ„æºçš„æ¾³å¤§åˆ©äºšåŸä½æ°‘è¯­è¨€ï¼‰çš„è¯­éŸ³æ•°æ®é›†ã€‚é€šè¿‡é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ï¼Œå¯¹Dharawalä¸107ç§é«˜èµ„æºè¯­è¨€ä¹‹é—´çš„è¯­éŸ³ç›¸ä¼¼æ€§è¿›è¡Œäº†åˆ†æã€‚ç ”ç©¶æ–¹æ³•åŒ…æ‹¬è¯¯åˆ†ç±»ç‡åˆ†ææ¥è¯„ä¼°è¯­è¨€æ··æ·†æ€§ï¼Œä»¥åŠä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å’ŒFrâ€™echet Inception Distanceï¼ˆFIDï¼‰è¿›è¡Œç²¾ç»†ç›¸ä¼¼æ€§æµ‹é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDharawalä¸æ‹‰ä¸ã€æ¯›åˆ©ã€éŸ©è¯­ã€æ³°è¯­å’Œå¨å°”å£«è¯­ç­‰è¯­è¨€å…·æœ‰è¾ƒå¼ºçš„è¯­éŸ³ç›¸ä¼¼æ€§ã€‚è¿™ä¸ºæœªæ¥çš„è¿ç§»å­¦ä¹ å’Œæ¨¡å‹é€‚åº”å·¥ä½œæä¾›äº†å®é™…æŒ‡å¯¼ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®æ”¶é›†å’ŒåŸºäºåµŒå…¥çš„åˆ†æå¯¹äºæ”¯æŒæ¿’å±è¯­è¨€ç¤¾åŒºçš„è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„é‡è¦æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¾³å¤§åˆ©äºšåŸä½æ°‘è¯­è¨€åœ¨ç°ä»£è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­ä¸¥é‡ç¼ºä¹ä»£è¡¨æ€§ã€‚</li>
<li>Dharawalè¯­è¨€æ˜¯ä¸€ç§ä½èµ„æºçš„æ¾³å¤§åˆ©äºšåŸä½æ°‘è¯­è¨€ï¼Œå…¶è¯­éŸ³æ•°æ®é›†è¢«æ”¶é›†å¹¶æ¸…ç†ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ï¼Œå¯¹Dharawalä¸å¤šç§é«˜èµ„æºè¯­è¨€è¿›è¡Œäº†è¯­éŸ³ç›¸ä¼¼æ€§åˆ†æã€‚</li>
<li>è¯¯åˆ†ç±»ç‡åˆ†æç”¨äºè¯„ä¼°è¯­è¨€çš„æ··æ·†æ€§ã€‚</li>
<li>é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦å’ŒFrâ€™echet Inception Distanceï¼ˆFIDï¼‰è¿›è¡Œäº†ç²¾ç»†çš„è¯­éŸ³ç›¸ä¼¼æ€§æµ‹é‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDharawalä¸æŸäº›è¯­è¨€ï¼ˆå¦‚æ‹‰ä¸ã€æ¯›åˆ©ã€éŸ©è¯­ã€æ³°è¯­å’Œå¨å°”å£«è¯­ï¼‰æœ‰å¼ºè¯­éŸ³ç›¸ä¼¼æ€§ã€‚</li>
<li>è¿™ä¸ºè¿ç§»å­¦ä¹ å’Œæ¨¡å‹é€‚åº”æä¾›äº†æŒ‡å¯¼ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®æ”¶é›†å’ŒåŸºäºåµŒå…¥çš„åˆ†æåœ¨æ”¯æŒæ¿’å±è¯­è¨€ç¤¾åŒºçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-997a5163ab5db2be4dfb0136959e09eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2aa54f0780a289eb69809b37e7e64ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8628c58167f1b9641ee7e7a7989c3604.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c7f60dd295e694b54fb2e729fd3d2c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80ed81d24ce56c369ce927a6af622d18.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ArabEmoNet-A-Lightweight-Hybrid-2D-CNN-BiLSTM-Model-with-Attention-for-Robust-Arabic-Speech-Emotion-Recognition"><a href="#ArabEmoNet-A-Lightweight-Hybrid-2D-CNN-BiLSTM-Model-with-Attention-for-Robust-Arabic-Speech-Emotion-Recognition" class="headerlink" title="ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for   Robust Arabic Speech Emotion Recognition"></a>ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for   Robust Arabic Speech Emotion Recognition</h2><p><strong>Authors:Ali Abouzeid, Bilal Elbouardi, Mohamed Maged, Shady Shehata</strong></p>
<p>Speech emotion recognition is vital for human-computer interaction, particularly for low-resource languages like Arabic, which face challenges due to limited data and research. We introduce ArabEmoNet, a lightweight architecture designed to overcome these limitations and deliver state-of-the-art performance. Unlike previous systems relying on discrete MFCC features and 1D convolutions, which miss nuanced spectro-temporal patterns, ArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving critical emotional cues often lost in traditional methods.   While recent models favor large-scale architectures with millions of parameters, ArabEmoNet achieves superior results with just 1 million parameters, 90 times smaller than HuBERT base and 74 times smaller than Whisper. This efficiency makes it ideal for resource-constrained environments. ArabEmoNet advances Arabic speech emotion recognition, offering exceptional performance and accessibility for real-world applications. </p>
<blockquote>
<p>è¯­éŸ³æƒ…ç»ªè¯†åˆ«å¯¹äºäººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåƒé˜¿æ‹‰ä¼¯è¯­è¿™æ ·çš„ä½èµ„æºè¯­è¨€æ¥è¯´ï¼Œç”±äºæ•°æ®å’Œç ”ç©¶çš„å±€é™æ€§ï¼Œå®ƒä»¬é¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†ArabEmoNetï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§æ¶æ„ï¼Œæ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶å¹¶å¸¦æ¥æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ä»¥å¾€ä¾èµ–äºç¦»æ•£MFCCç‰¹å¾å’Œ1Då·ç§¯çš„ç³»ç»Ÿä¸åŒï¼Œè¿™äº›ç³»ç»Ÿä¼šå¿½ç•¥ç»†å¾®çš„è°±æ—¶æ¨¡å¼ï¼ŒArabEmoNetä½¿ç”¨é€šè¿‡2Då·ç§¯å¤„ç†çš„æ¢…å°”é¢‘è°±å›¾ï¼Œä¿ç•™äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç»å¸¸ä¸¢å¤±çš„å…³é”®æƒ…æ„Ÿçº¿ç´¢ã€‚è™½ç„¶æœ€è¿‘çš„æ¨¡å‹å€¾å‘äºé‡‡ç”¨å…·æœ‰æ•°ç™¾ä¸‡å‚æ•°çš„å¤§è§„æ¨¡æ¶æ„ï¼Œä½†ArabEmoNetä»…ä½¿ç”¨100ä¸‡ä¸ªå‚æ•°å°±å–å¾—äº†ä¼˜è¶Šçš„ç»“æœï¼Œæ¯”HuBERTåŸºç¡€ç‰ˆæœ¬å°90å€ï¼Œæ¯”whisperå°74å€ã€‚è¿™ç§æ•ˆç‡ä½¿å…¶éå¸¸é€‚åˆèµ„æºå—é™çš„ç¯å¢ƒã€‚ArabEmoNetåœ¨é˜¿æ‹‰ä¼¯è¯­éŸ³æƒ…ç»ªè¯†åˆ«æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä¸ºç°å®ä¸–ç•Œåº”ç”¨æä¾›äº†å“è¶Šçš„æ€§èƒ½å’Œå¯è®¿é—®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01401v1">PDF</a> Accepted (The Third Arabic Natural Language Processing Conference)</p>
<p><strong>Summary</strong></p>
<p>é˜¿æ‹‰ä¼¯è¯­çš„è¯­éŸ³æƒ…ç»ªè¯†åˆ«å¯¹äºäººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯é¢ä¸´æ•°æ®æœ‰é™å’Œç ”ç©¶èµ„æºä¸è¶³ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†ArabEmoNetï¼Œè¿™æ˜¯ä¸€ç§è½»ä¾¿æ¶æ„ï¼Œæ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶å¹¶æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºç¦»æ•£MFCCç‰¹å¾å’Œä¸€ç»´å·ç§¯çš„ç³»ç»Ÿç›¸æ¯”ï¼ŒArabEmoNetä½¿ç”¨ç»è¿‡äºŒç»´å·ç§¯å¤„ç†çš„æ¢…å°”é¢‘è°±å›¾ï¼Œèƒ½å¤Ÿæ•æ‰åˆ°å¾®å¦™çš„è°±æ—¶æ¨¡å¼ï¼Œä¿ç•™äº†å…³é”®çš„æƒ…æ„Ÿçº¿ç´¢ã€‚å°½ç®¡æœ€è¿‘çš„æ¨¡å‹å€¾å‘äºå¤§è§„æ¨¡æ¶æ„ï¼Œæ‹¥æœ‰æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œä½†ArabEmoNetä»…ä½¿ç”¨ä¸€ç™¾ä¸‡ä¸ªå‚æ•°å°±å–å¾—äº†ä¼˜è¶Šçš„ç»“æœï¼Œæ¯”HuBERTåŸºç¡€ç‰ˆæœ¬å°90å€ï¼Œæ¯”whisperå°74å€ã€‚è¿™ç§æ•ˆç‡ä½¿å…¶æˆä¸ºèµ„æºå—é™ç¯å¢ƒçš„ç†æƒ³é€‰æ‹©ã€‚ArabEmoNetä¸ºé˜¿æ‹‰ä¼¯è¯­çš„è¯­éŸ³æƒ…ç»ªè¯†åˆ«æä¾›äº†å‡ºè‰²çš„æ€§èƒ½å’Œå®é™…åº”ç”¨çš„å¯è®¿é—®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿æ‹‰ä¼¯è¯­çš„è¯­éŸ³æƒ…ç»ªè¯†åˆ«åœ¨äººæœºäº¤äº’ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>æœ‰é™çš„èµ„æºå’Œæ•°æ®æ˜¯é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³æƒ…ç»ªè¯†åˆ«çš„æŒ‘æˆ˜ã€‚</li>
<li>ArabEmoNetæ˜¯ä¸€ç§è½»ä¾¿çš„æ¶æ„ï¼Œæ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜å¹¶æä¾›å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ArabEmoNetä½¿ç”¨æ¢…å°”é¢‘è°±å›¾é€šè¿‡äºŒç»´å·ç§¯å¤„ç†ï¼Œèƒ½å¤Ÿæ•æ‰åˆ°å¾®å¦™çš„è°±æ—¶æ¨¡å¼ã€‚</li>
<li>ä¸å…¶ä»–å¤§å‹æ¨¡å‹ç›¸æ¯”ï¼ŒArabEmoNetä½¿ç”¨è¾ƒå°‘çš„å‚æ•°å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>ArabEmoNetçš„æ•ˆç‡å’Œæ€§èƒ½ä½¿å…¶æˆä¸ºèµ„æºå—é™ç¯å¢ƒçš„ç†æƒ³é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec9914afd66ed29457b4e5e215f95106.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7bdcfcb2e14c12da375447484fcafb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3204fde3b4918cd3472e9e7eceeb963.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-495b8f6f6e9aaceb8454139befafdb48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a87d1d07d99758bce286b74858afdfe3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CabinSep-IR-Augmented-Mask-Based-MVDR-for-Real-Time-In-Car-Speech-Separation-with-Distributed-Heterogeneous-Arrays"><a href="#CabinSep-IR-Augmented-Mask-Based-MVDR-for-Real-Time-In-Car-Speech-Separation-with-Distributed-Heterogeneous-Arrays" class="headerlink" title="CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech   Separation with Distributed Heterogeneous Arrays"></a>CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech   Separation with Distributed Heterogeneous Arrays</h2><p><strong>Authors:Runduo Han, Yanxin Hu, Yihui Fu, Zihan Zhang, Yukai Jv, Li Chen, Lei Xie</strong></p>
<p>Separating overlapping speech from multiple speakers is crucial for effective human-vehicle interaction. This paper proposes CabinSep, a lightweight neural mask-based minimum variance distortionless response (MVDR) speech separation approach, to reduce speech recognition errors in back-end automatic speech recognition (ASR) models. Our contributions are threefold: First, we utilize channel information to extract spatial features, which improves the estimation of speech and noise masks. Second, we employ MVDR during inference, reducing speech distortion to make it more ASR-friendly. Third, we introduce a data augmentation method combining simulated and real-recorded impulse responses (IRs), improving speaker localization at zone boundaries and further reducing speech recognition errors. With a computational complexity of only 0.4 GMACs, CabinSep achieves a 17.5% relative reduction in speech recognition error rate in a real-recorded dataset compared to the state-of-the-art DualSep model. Demos are available at: <a target="_blank" rel="noopener" href="https://cabinsep.github.io/cabinsep/">https://cabinsep.github.io/cabinsep/</a>. </p>
<blockquote>
<p>åœ¨å¤šäººå¯¹è¯åœºæ™¯ä¸­ï¼Œä»é‡å çš„è¯­éŸ³ä¸­åˆ†ç¦»å‡ºå„ä¸ªè¯´è¯äººçš„å£°éŸ³å¯¹äºå®ç°æœ‰æ•ˆçš„äººè½¦äº¤äº’è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè½»é‡çº§ç¥ç»æ©ç çš„æœ€ä½æ–¹å·®æ— å¤±çœŸå“åº”ï¼ˆMVDRï¼‰è¯­éŸ³åˆ†ç¦»æ–¹æ³•CabinSepï¼Œæ—¨åœ¨é™ä½åç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„è¯­éŸ³è¯†åˆ«é”™è¯¯ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸»è¦ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨é€šé“ä¿¡æ¯æå–ç©ºé—´ç‰¹å¾ï¼Œæé«˜äº†è¯­éŸ³å’Œå™ªå£°æ©ç çš„ä¼°è®¡æ•ˆæœã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨MVDRï¼Œå‡å°‘è¯­éŸ³å¤±çœŸï¼Œä½¿å…¶æ›´é€‚åº”ASRã€‚æœ€åï¼Œæˆ‘ä»¬ç»“åˆæ¨¡æ‹Ÿå’ŒçœŸå®å½•åˆ¶çš„è„‰å†²å“åº”ï¼ˆIRsï¼‰å¼•å…¥äº†ä¸€ç§æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæé«˜äº†æ‰¬å£°å™¨åœ¨åŒºåŸŸè¾¹ç•Œçš„å®šä½ç²¾åº¦ï¼Œå¹¶è¿›ä¸€æ­¥é™ä½äº†è¯­éŸ³è¯†åˆ«é”™è¯¯ã€‚CabinSepçš„è®¡ç®—å¤æ‚åº¦åªæœ‰0.4GMACsï¼Œåœ¨çœŸå®æ•°æ®é›†ä¸Šä¸æœ€æ–°çš„DualSepæ¨¡å‹ç›¸æ¯”ï¼Œè¯­éŸ³è¯†åˆ«é”™è¯¯ç‡é™ä½äº†17.5%ã€‚ç›¸å…³æ¼”ç¤ºå¯é€šè¿‡ä»¥ä¸‹ç½‘å€æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://cabinsep.github.io/cabinsep/">https://cabinsep.github.io/cabinsep/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01399v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCabinSepçš„è½»é‡çº§ç¥ç»ç½‘ç»œæ©ç åŸºäºæœ€å°æ–¹å·®æ— å¤±çœŸå“åº”ï¼ˆMVDRï¼‰çš„è¯­éŸ³åˆ†ç¦»æ–¹æ³•ï¼Œç”¨äºå‡å°‘åç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ä¸­çš„è¯­éŸ³è¯†åˆ«é”™è¯¯ã€‚é€šè¿‡åˆ©ç”¨é€šé“ä¿¡æ¯æå–ç©ºé—´ç‰¹å¾ã€åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨MVDRä»¥åŠç»“åˆæ¨¡æ‹Ÿå’ŒçœŸå®å½•åˆ¶çš„å†²å‡»å“åº”è¿›è¡Œæ•°æ®å¢å¼ºï¼ŒCabinSepæé«˜äº†è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CabinSepæ˜¯ä¸€ç§è½»é‡çº§çš„ç¥ç»ç½‘ç»œæ©ç åŸºäºMVDRçš„è¯­éŸ³åˆ†ç¦»æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘ASRæ¨¡å‹ä¸­çš„è¯­éŸ³è¯†åˆ«é”™è¯¯ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨é€šé“ä¿¡æ¯æå–ç©ºé—´ç‰¹å¾ï¼Œæé«˜äº†è¯­éŸ³å’Œå™ªå£°æ©ç çš„ä¼°è®¡ã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨MVDRï¼Œå‡å°‘è¯­éŸ³å¤±çœŸï¼Œä½¿å…¶æ›´é€‚åº”ASRã€‚</li>
<li>å¼•å…¥ç»“åˆæ¨¡æ‹Ÿå’ŒçœŸå®å½•åˆ¶çš„å†²å‡»å“åº”çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæ”¹å–„äº†åœ¨åŒºåŸŸè¾¹ç•Œçš„è¯´è¯äººå®šä½ï¼Œå¹¶è¿›ä¸€æ­¥å‡å°‘è¯­éŸ³è¯†åˆ«é”™è¯¯ã€‚</li>
<li>CabinSepçš„è®¡ç®—å¤æ‚åº¦ä»…ä¸º0.4GMACsã€‚</li>
<li>ä¸å½“å‰å…ˆè¿›çš„DualSepæ¨¡å‹ç›¸æ¯”ï¼ŒCabinSepåœ¨çœŸå®å½•åˆ¶çš„æ•°æ®é›†ä¸Šå®ç°äº†17.5%çš„ç›¸å¯¹è¯­éŸ³è¯†åˆ«é”™è¯¯ç‡é™ä½ã€‚</li>
<li>æ¼”ç¤ºåœ°å€ï¼š<a target="_blank" rel="noopener" href="https://cabinsep.github.io/cabinsep/%E3%80%82">https://cabinsep.github.io/cabinsep/ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4a5bc59031c8c4fec5e3de832ba5d56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55085014391f835dc25749519e1aa827.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e83d735eb11180fe3c1e140e64d99b60.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SimulMEGA-MoE-Routers-are-Advanced-Policy-Makers-for-Simultaneous-Speech-Translation"><a href="#SimulMEGA-MoE-Routers-are-Advanced-Policy-Makers-for-Simultaneous-Speech-Translation" class="headerlink" title="SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation"></a>SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation</h2><p><strong>Authors:Chenyang Le, Bing Han, Jinshun Li, Songyong Chen, Yanmin Qian</strong></p>
<p>Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs. </p>
<blockquote>
<p>åŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSimulSTï¼‰é€šè¿‡è”åˆä¼˜åŒ–è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘ï¼Œåœ¨ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶ä¸‹å®ç°å®æ—¶è·¨è¯­è¨€äº¤æµã€‚ç°æœ‰ç³»ç»Ÿåœ¨å¹³è¡¡ç¿»è¯‘è´¨é‡ã€å»¶è¿Ÿå’Œè¯­ä¹‰è¿è´¯æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯­ç§å¤šå¯¹å¤šçš„åœºæ™¯ä¸­ï¼Œä¸åŒçš„è¯»å†™ç­–ç•¥é˜»ç¢äº†ç»Ÿä¸€ç­–ç•¥å­¦ä¹ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SimulMEGAï¼ˆåŸºäºä¸“å®¶æ··åˆé—¨æ§çš„åŒæ­¥ç”Ÿæˆï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†å‰ç¼€è®­ç»ƒä¸ä¸“å®¶æ··åˆç²¾ç‚¼å™¨ç›¸ç»“åˆï¼Œä»¥éšå¼çš„æ–¹å¼å­¦ä¹ æœ‰æ•ˆçš„è¯»å†™å†³ç­–ï¼Œè€Œä¸ä¼šå¢åŠ æ¨ç†æ—¶é—´å¼€é”€ã€‚æˆ‘ä»¬çš„è®¾è®¡åªéœ€å¯¹æ ‡å‡†å˜å‹å™¨æ¶æ„è¿›è¡Œæœ€å°ä¿®æ”¹ï¼Œå°±å¯ä»¥åº”ç”¨äºè¯­éŸ³åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„æµå¼ä»»åŠ¡ã€‚é€šè¿‡å¯¹å…­ç§è¯­è¨€å¯¹çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬çš„5äº¿å‚æ•°è¯­éŸ³åˆ°æ–‡æœ¬æ¨¡å‹ä¼˜äºæ— ç¼åŸºçº¿ï¼Œåœ¨å¹³å‡å»¶è¿Ÿ1.5ç§’çš„æƒ…å†µä¸‹ï¼ŒBLEUå€¼é™ä½ä¸åˆ°7%ï¼Œåœ¨3ç§’æ—¶é™ä½ä¸åˆ°3%ã€‚æˆ‘ä»¬è¿˜å°†SimulMEGAæ‰©å±•åˆ°å…·æœ‰å•å‘ä¸»å¹²çš„æµå¼TTSï¼Œè¿›ä¸€æ­¥å±•ç¤ºäº†å…¶é€šç”¨æ€§ï¼Œäº§ç”Ÿäº†ä¼˜è¶Šçš„å»¶è¿Ÿè´¨é‡æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01200v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSimulSTï¼‰åœ¨ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶ä¸‹è”åˆä¼˜åŒ–è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘ï¼Œå®ç°å®æ—¶è·¨è¯­è¨€æ²Ÿé€šã€‚ç°æœ‰ç³»ç»Ÿåœ¨å¤šè¯­è¨€å¤šå¯¹å¤šçš„åœºæ™¯ä¸­éš¾ä»¥å¹³è¡¡ç¿»è¯‘è´¨é‡ã€å»¶è¿Ÿå’Œè¯­ä¹‰è¿è´¯æ€§ã€‚æœ¬æ–‡æå‡ºSimulMEGAï¼ˆåŸºäºä¸“å®¶é—¨æ§çš„åŒæ­¥ç”Ÿæˆï¼‰ï¼Œé‡‡ç”¨æ— ç›‘ç£ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆå‰ç¼€è®­ç»ƒä¸ä¸“å®¶æ··åˆç²¾ç‚¼å™¨ï¼Œéšå¼å­¦ä¹ æœ‰æ•ˆçš„è¯»å†™å†³ç­–ï¼Œä¸å¢åŠ æ¨ç†æ—¶é—´å¼€é”€ã€‚é€‚ç”¨äºæ ‡å‡†è½¬æ¢å™¨æ¶æ„ï¼Œå¹¶é€‚ç”¨äºè¯­éŸ³åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„æµå¼ä¼ è¾“ä»»åŠ¡ã€‚åœ¨å…­ç§è¯­è¨€å¯¹ä¸Šçš„å…¨é¢è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬çš„5äº¿å‚æ•°è¯­éŸ³åˆ°æ–‡æœ¬æ¨¡å‹åœ¨å¹³å‡å»¶è¿Ÿ1.5ç§’çš„æƒ…å†µä¸‹å®ç°äº†ä½äº7%çš„BLEUé€€åŒ–ï¼Œåœ¨3ç§’å†…ä½äº3%ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å°†å…¶æ‰©å±•åˆ°å…·æœ‰å•å‘ä¸»å¹²æµçš„TTSæ¥å±•ç¤ºSimulMEGAçš„é€šç”¨æ€§ï¼Œè·å¾—äº†å‡ºè‰²çš„å»¶è¿Ÿè´¨é‡æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Simultaneous Speech Translation (SimulST) æ—¨åœ¨å®ç°å®æ—¶è·¨è¯­è¨€æ²Ÿé€šï¼Œéœ€è¦ä¼˜åŒ–ç¿»è¯‘è´¨é‡ã€å»¶è¿Ÿå’Œè¯­ä¹‰è¿è´¯æ€§ã€‚</li>
<li>ç°æœ‰ç³»ç»Ÿåœ¨å¤šè¯­è¨€å¤šå¯¹å¤šçš„åœºæ™¯ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦å¹³è¡¡å„é¡¹æ€§èƒ½æŒ‡æ ‡ã€‚</li>
<li>SimulMEGAæ˜¯ä¸€ä¸ªæ— ç›‘ç£ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å‰ç¼€è®­ç»ƒå’Œä¸“å®¶æ··åˆç²¾ç‚¼å™¨éšå¼å­¦ä¹ è¯»å†™å†³ç­–ã€‚</li>
<li>SimulMEGAé€‚ç”¨äºæ ‡å‡†è½¬æ¢å™¨æ¶æ„ï¼Œå¯åº”ç”¨äºè¯­éŸ³åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„æµå¼ä¼ è¾“ä»»åŠ¡ã€‚</li>
<li>åœ¨å¤šç§è¯­è¨€å¯¹çš„è¯„ä¼°ä¸­ï¼ŒSimulMEGAè¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œåœ¨è¾ƒçŸ­å»¶è¿Ÿä¸‹å®ç°è¾ƒé«˜çš„ç¿»è¯‘è´¨é‡ã€‚</li>
<li>SimulMEGAé€šè¿‡æ‰©å±•åˆ°æ–‡æœ¬åˆ°è¯­éŸ³çš„æµå¼ä¼ è¾“ä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26489addf658b993d6290d1f1ae21778.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce5b5111ae557683a6ee2e36b7ca4e35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38e5b98d73926dee6d1adfcf8a4fd647.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Noisy-Disentanglement-with-Tri-stage-Training-for-Noise-Robust-Speech-Recognition"><a href="#Noisy-Disentanglement-with-Tri-stage-Training-for-Noise-Robust-Speech-Recognition" class="headerlink" title="Noisy Disentanglement with Tri-stage Training for Noise-Robust Speech   Recognition"></a>Noisy Disentanglement with Tri-stage Training for Noise-Robust Speech   Recognition</h2><p><strong>Authors:Shuangyuan Chen, Shuang Wei, Dongxing Xu, Yanhua Long</strong></p>
<p>To enhance the performance of end-to-end (E2E) speech recognition systems in noisy or low signal-to-noise ratio (SNR) conditions, this paper introduces NoisyD-CT, a novel tri-stage training framework built on the Conformer-Transducer architecture. The core of NoisyD-CT is a especially designed compact noisy disentanglement (NoisyD) module (adding only 1.71M parameters), integrated between the Conformer blocks and Transducer Decoder to perform deep noise suppression and improve ASR robustness in challenging acoustic noise environments. To fully exploit the noise suppression capability of the NoisyD-CT, we further propose a clean representation consistency loss to align high-level representations derived from noisy speech with those obtained from corresponding clean speech. Together with a noisy reconstruction loss, this consistency alignment enables the NoisyD module to effectively suppress noise while preserving essential acoustic and linguistic features consistent across both clean and noisy conditions, thereby producing cleaner internal representations that enhance ASR performance. Moreover, our tri-stage training strategy is designed to fully leverage the functionalities of both the noisy disentanglement and speech recognition modules throughout the model training process, ultimately maximizing performance gains under noisy conditions. Our experiments are performed on the LibriSpeech and CHiME-4 datasets, extensive results demonstrate that our proposed NoisyD-CT significantly outperforms the competitive Conformer-Transducer baseline, achieving up to 25.7% and 10.6% relative word error rate reductions on simulated and real-world noisy test sets, respectively, while maintaining or even improving performance on clean speech test sets. The source code, model checkpoint and data simulation scripts will be available at <a target="_blank" rel="noopener" href="https://github.com/litchimo/NoisyD-CT">https://github.com/litchimo/NoisyD-CT</a>. </p>
<blockquote>
<p>ä¸ºäº†æå‡ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨å™ªå£°ç¯å¢ƒæˆ–ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼Œæœ¬æ–‡å¼•å…¥äº†åŸºäºConformer-Transduceræ¶æ„çš„æ–°å‹ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶NoisyD-CTã€‚NoisyD-CTçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç‰¹åˆ«è®¾è®¡çš„ç´§å‡‘å™ªå£°åˆ†è§£ï¼ˆNoisyDï¼‰æ¨¡å—ï¼ˆä»…å¢åŠ 1.71Må‚æ•°ï¼‰ï¼Œå®ƒä½äºConformerå—å’Œè½¬æ¢å™¨è§£ç å™¨ä¹‹é—´ï¼Œæ‰§è¡Œæ·±åº¦å™ªå£°æŠ‘åˆ¶ï¼Œæé«˜ASRåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦å™ªå£°ç¯å¢ƒä¸­çš„ç¨³å¥æ€§ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨NoisyD-CTçš„å™ªå£°æŠ‘åˆ¶èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†æ¸…æ´è¡¨ç¤ºä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ä½¿ä»å¸¦å™ªå£°çš„è¯­éŸ³ä¸­æ´¾ç”Ÿå‡ºçš„é«˜çº§è¡¨ç¤ºä¸ä»ç›¸åº”çš„æ¸…æ´è¯­éŸ³ä¸­è·å¾—çš„è¡¨ç¤ºå¯¹é½ã€‚ä¸å™ªå£°é‡å»ºæŸå¤±ç›¸ç»“åˆï¼Œè¿™ç§ä¸€è‡´æ€§å¯¹é½ä½¿NoisyDæ¨¡å—èƒ½å¤Ÿåœ¨æŠ‘åˆ¶å™ªå£°çš„åŒæ—¶ä¿ç•™è·¨æ¸…æ´å’Œå™ªå£°æ¡ä»¶ä¸‹çš„åŸºæœ¬å£°å­¦å’Œè¯­è¨€ç‰¹å¾çš„ä¸€è‡´æ€§ï¼Œä»è€Œäº§ç”Ÿæ›´æ¸…æ´çš„å†…éƒ¨è¡¨ç¤ºï¼Œæé«˜ASRæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥æ—¨åœ¨å……åˆ†åˆ©ç”¨å™ªå£°åˆ†è§£å’Œè¯­éŸ³è¯†åˆ«æ¨¡å—çš„åŠŸèƒ½ï¼Œé€šè¿‡æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ï¼Œæœ€å¤§é™åº¦åœ°æé«˜å™ªå£°æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨LibriSpeechå’ŒCHiME-4æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¤§é‡ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„NoisyD-CTæ˜¾è‘—ä¼˜äºç«äº‰æ€§çš„Conformer-TransduceråŸºçº¿ï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå™ªå£°æµ‹è¯•é›†ä¸Šç›¸å¯¹å­—è¯é”™è¯¯ç‡åˆ†åˆ«é™ä½äº†25.7%å’Œ10.6%ï¼ŒåŒæ—¶åœ¨æ¸…æ´è¯­éŸ³æµ‹è¯•é›†ä¸Šä¿æŒæˆ–æé«˜äº†æ€§èƒ½ã€‚æºä»£ç ã€æ¨¡å‹æ£€æŸ¥ç‚¹å’Œæ•°æ®æ¨¡æ‹Ÿè„šæœ¬å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/litchimo/NoisyD-CT%E3%80%82">https://github.com/litchimo/NoisyD-CTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01087v1">PDF</a> 11 pages,4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºConformer-Transduceræ¶æ„çš„æ–°å‹ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶NoisyD-CTï¼Œæ—¨åœ¨æé«˜ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨å™ªå£°æˆ–ä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚æ ¸å¿ƒåœ¨äºç´§å‡‘çš„å™ªå£°åˆ†è§£ï¼ˆNoisyDï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨Conformerå—å’ŒTransducerè§£ç å™¨ä¹‹é—´è¿›è¡Œé›†æˆï¼Œä»¥æ‰§è¡Œæ·±åº¦å™ªå£°æŠ‘åˆ¶å¹¶æ”¹å–„åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦å™ªå£°ç¯å¢ƒä¸­çš„ASRç¨³å¥æ€§ã€‚é€šè¿‡å¼•å…¥æ¸…æ™°è¡¨ç¤ºä¸€è‡´æ€§æŸå¤±å’Œå™ªå£°é‡å»ºæŸå¤±ï¼ŒNoisyD-CTå®ç°äº†å™ªå£°æŠ‘åˆ¶çš„åŒæ—¶ä¿ç•™è·¨æ¸…æ´å’Œå™ªå£°æ¡ä»¶ä¸‹çš„å…³é”®å£°å­¦å’Œè¯­è¨€ç‰¹å¾ã€‚åœ¨LibriSpeechå’ŒCHiME-4æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç«äº‰çš„Conformer-TransduceråŸºçº¿ç›¸æ¯”ï¼ŒNoisyD-CTæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå™ªå£°æµ‹è¯•é›†ä¸Šåˆ†åˆ«å®ç°äº†ç›¸å¯¹å­—è¯é”™è¯¯ç‡é™ä½25.7%å’Œ10.6%ï¼ŒåŒæ—¶åœ¨æ¸…æ´è¯­éŸ³æµ‹è¯•é›†ä¸Šä¿æŒæˆ–æé«˜äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NoisyD-CTæ˜¯åŸºäºConformer-Transduceræ¶æ„çš„æ–°å‹ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨å™ªå£°æˆ–ä½ä¿¡å™ªæ¯”ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚</li>
<li>NoisyDæ¨¡å—ç”¨äºæ·±åº¦å™ªå£°æŠ‘åˆ¶ï¼Œå¹¶æ”¹å–„ASRåœ¨å™ªå£°ç¯å¢ƒä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡æ¸…æ™°è¡¨ç¤ºä¸€è‡´æ€§æŸå¤±å’Œå™ªå£°é‡å»ºæŸå¤±ï¼ŒNoisyDæ¨¡å—å®ç°å™ªå£°æŠ‘åˆ¶çš„åŒæ—¶ä¿ç•™å…³é”®å£°å­¦ç‰¹å¾ã€‚</li>
<li>NoisyD-CTåœ¨LibriSpeechå’ŒCHiME-4æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>ä¸åŸºçº¿ç›¸æ¯”ï¼ŒNoisyD-CTåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå™ªå£°æµ‹è¯•é›†ä¸Šæ˜¾è‘—é™ä½äº†ç›¸å¯¹å­—è¯é”™è¯¯ç‡ã€‚</li>
<li>NoisyD-CTåœ¨æ¸…æ´è¯­éŸ³æµ‹è¯•é›†ä¸Šä¿æŒæˆ–æé«˜äº†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-736d06dbb5ad91b401aaf5b1d54b628e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aee76bc71b7c400d1d57ce21677b1cde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70363a741ecf80d59a7aa278939b052f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2129a9c64b294656359d9858173d2f2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Unified-Denoising-and-Adaptation-Framework-for-Self-Supervised-Bengali-Dialectal-ASR"><a href="#A-Unified-Denoising-and-Adaptation-Framework-for-Self-Supervised-Bengali-Dialectal-ASR" class="headerlink" title="A Unified Denoising and Adaptation Framework for Self-Supervised Bengali   Dialectal ASR"></a>A Unified Denoising and Adaptation Framework for Self-Supervised Bengali   Dialectal ASR</h2><p><strong>Authors:Swadhin Biswas,  Imran, Tuhin Sheikh</strong></p>
<p>Automatic Speech Recognition (ASR) for Bengali, the worldâ€™s fifth most spoken language, remains a significant challenge, critically hindering technological accessibility for its over 270 million speakers. This challenge is compounded by two persistent and intertwined factors: the languageâ€™s vast dialectal diversity and the prevalence of acoustic noise in real-world environments. While state-of-the-art self-supervised learning (SSL) models have advanced ASR for low-resource languages, they often lack explicit mechanisms to handle environmental noise during pre-training or specialized adaptation strategies for the complex phonetic and lexical variations across Bengali dialects. This paper introduces a novel, unified framework designed to address these dual challenges simultaneously. Our approach is founded on the WavLM model, which is uniquely pre-trained with a masked speech denoising objective, making it inherently robust to acoustic distortions. We propose a specialized multi-stage fine-tuning strategy that first adapts the model to general-domain standard Bengali to establish a strong linguistic foundation and subsequently specializes it for noise-robust dialectal recognition through targeted data augmentation. The framework is rigorously evaluated on a comprehensive benchmark comprising multiple Bengali dialects under a wide range of simulated noisy conditions, from clean audio to low Signal-to-Noise Ratio (SNR) levels.   Experimental results demonstrate that the proposed framework significantly outperforms strong baselines, including standard fine-tuned wav2vec 2.0 and the large-scale multilingual Whisper model. This work establishes a new state-of-the-art for this task and provides a scalable, effective blueprint for developing practical ASR systems for other low-resource, high-variation languages globally. </p>
<blockquote>
<p>å­ŸåŠ æ‹‰è¯­ä½œä¸ºä¸–ç•Œä¸Šç¬¬äº”å¤§ä½¿ç”¨è¯­è¨€ï¼Œå…¶è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œä¸¥é‡å½±å“äº†è¶…è¿‡2.7äº¿å­ŸåŠ æ‹‰è¯­ä½¿ç”¨è€…çš„æŠ€æœ¯å¯åŠæ€§ã€‚è¿™ä¸€æŒ‘æˆ˜ç”±ä¸¤ä¸ªæŒç»­äº¤ç»‡çš„å› ç´ åŠ å‰§ï¼šå­ŸåŠ æ‹‰è¯­çš„å·¨å¤§æ–¹è¨€å¤šæ ·æ€§å’Œç°å®ç¯å¢ƒä¸­æ™®éå­˜åœ¨çš„å£°éŸ³å™ªéŸ³ã€‚è™½ç„¶æœ€æ–°çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹å·²ç»æ¨åŠ¨äº†ä½èµ„æºè¯­è¨€çš„ASRå‘å±•ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹å¤„ç†ç¯å¢ƒå™ªå£°çš„æ˜ç¡®æœºåˆ¶ï¼Œæˆ–è€…åœ¨åº”å¯¹å­ŸåŠ æ‹‰è¯­æ–¹è¨€çš„å¤æ‚è¯­éŸ³å’Œè¯æ±‡å˜åŒ–æ–¹é¢çš„ä¸“é—¨é€‚åº”ç­–ç•¥ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶åº”å¯¹è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºWavLMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰ç‹¬ç‰¹çš„é¢„è®­ç»ƒæœºåˆ¶ï¼Œä½¿ç”¨æ©è”½è¯­éŸ³å»å™ªç›®æ ‡ï¼Œä½¿å…¶å¯¹å£°éŸ³å¤±çœŸå…·æœ‰å›ºæœ‰çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨çš„åˆ†é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œé¦–å…ˆä½¿æ¨¡å‹é€‚åº”é€šç”¨é¢†åŸŸçš„æ ‡å‡†å­ŸåŠ æ‹‰è¯­ï¼Œä»¥å»ºç«‹åšå®çš„è¯­è¨€åŸºç¡€ï¼Œç„¶åé€šè¿‡æœ‰é’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºä½¿å…¶é€‚åº”å™ªå£°é²æ£’çš„æ–¹è¨€è¯†åˆ«ã€‚è¯¥æ¡†æ¶åœ¨åŒ…å«å¤šç§å­ŸåŠ æ‹‰è¯­æ–¹è¨€çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œæµ‹è¯•ç¯å¢ƒæ¨¡æ‹Ÿäº†å„ç§å™ªå£°æ¡ä»¶ï¼Œä»å¹²å‡€éŸ³é¢‘åˆ°ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬æ ‡å‡†å¾®è°ƒwav2vec 2.0å’Œå¤§è§„æ¨¡å¤šè¯­è¨€Whisperæ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œä¸ºè¯¥ä»»åŠ¡å»ºç«‹äº†æ–°çš„æœ€æ–°æŠ€æœ¯çŠ¶æ€ï¼Œä¸ºå¼€å‘é€‚ç”¨äºå…¶ä»–ä½èµ„æºã€é«˜å˜åŒ–è¯­è¨€çš„å®ç”¨ASRç³»ç»Ÿæä¾›äº†å¯ä¼¸ç¼©å’Œæœ‰æ•ˆçš„è“å›¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00988v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹å­ŸåŠ æ‹‰è¯­â€”â€”ä¸–ç•Œä¸Šç¬¬äº”å¤§è¯­è¨€â€”â€”çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»ç„¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œè¿™ä¸¥é‡å½±å“äº†è¶…è¿‡2.7äº¿å­ŸåŠ æ‹‰è¯­ä½¿ç”¨è€…çš„æŠ€æœ¯å¯è®¿é—®æ€§ã€‚è¯¥æŒ‘æˆ˜ç”±ä¸¤ä¸ªæŒä¹…ä¸”äº¤ç»‡çš„å› ç´ åŠ å‰§ï¼šå­ŸåŠ æ‹‰è¯­çš„å·¨å¤§æ–¹è¨€å¤šæ ·æ€§å’Œç°å®ç¯å¢ƒä¸­å­˜åœ¨çš„å£°éŸ³å™ªéŸ³çš„æ™®åŠã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºWavLMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨æ©è”½è¯­éŸ³é™å™ªç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶å›ºæœ‰åœ°é€‚åº”å£°éŸ³å¤±çœŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨çš„å¤šé˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œé¦–å…ˆä½¿æ¨¡å‹é€‚åº”ä¸€èˆ¬é¢†åŸŸçš„æ ‡å‡†å­ŸåŠ æ‹‰è¯­ï¼Œä»¥å»ºç«‹åšå®çš„è¯­è¨€åŸºç¡€ï¼Œç„¶åé€šè¿‡æœ‰é’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºä½¿å…¶é€‚åº”å™ªå£°é²æ£’çš„æ–¹è¨€è¯†åˆ«ã€‚è¯¥æ¡†æ¶åœ¨åŒ…å«å¤šç§å­ŸåŠ æ‹‰æ–¹è¨€çš„å¹¿æ³›åŸºå‡†æµ‹è¯•ä¸‹è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œæµ‹è¯•ç¯å¢ƒæ¶µç›–ä»å¹²å‡€éŸ³é¢‘åˆ°ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ°´å¹³çš„å„ç§æ¨¡æ‹Ÿå™ªå£°æ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ŒåŒ…æ‹¬æ ‡å‡†å¾®è°ƒwav2vec 2.0å’Œå¤§è§„æ¨¡å¤šè¯­è¨€Whisperæ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œä¸ºè¯¥ä»»åŠ¡å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸ºå…¨çƒå…¶ä»–èµ„æºåŒ®ä¹ã€å˜åŒ–æ€§é«˜çš„è¯­è¨€å¼€å‘å®ç”¨çš„ASRç³»ç»Ÿæä¾›äº†å¯ä¼¸ç¼©ã€æœ‰æ•ˆçš„è“å›¾ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å­ŸåŠ æ‹‰è¯­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»ç„¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºå…¶æ–¹è¨€å¤šæ ·æ€§å’Œç°å®ç¯å¢ƒä¸­çš„å£°éŸ³å™ªéŸ³é—®é¢˜ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯å¦‚SSLæ¨¡å‹åœ¨å¤„ç†ç¯å¢ƒå™ªå£°å’Œå­ŸåŠ æ‹‰è¯­æ–¹è¨€çš„å¤æ‚è¯­éŸ³å’Œè¯æ±‡å˜åŒ–æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºWavLMæ¨¡å‹çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ©è”½è¯­éŸ³é™å™ªç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å¢å¼ºå¯¹å£°éŸ³å¤±çœŸçš„é€‚åº”æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œå…ˆå»ºç«‹è¯­è¨€åŸºç¡€ï¼Œç„¶åé€šè¿‡æ•°æ®å¢å¼ºé€‚åº”å™ªå£°é²æ£’çš„æ–¹è¨€è¯†åˆ«ã€‚</li>
<li>åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ–°æ¡†æ¶è¡¨ç°å‡ºæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
<li>è¯¥å·¥ä½œä¸ºå…¶ä»–èµ„æºåŒ®ä¹ã€å˜åŒ–æ€§é«˜çš„è¯­è¨€å¼€å‘ASRç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„è“å›¾ã€‚</li>
<li>è¯¥æ¡†æ¶çš„å»ºç«‹ä¸ºå­ŸåŠ æ‹‰è¯­ä½¿ç”¨è€…æé«˜äº†æŠ€æœ¯å¯è®¿é—®æ€§ï¼Œæ¨åŠ¨äº†ASRæŠ€æœ¯çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ab1aeacf707a9bc0c2dfb3efda9459c5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MPO-Multidimensional-Preference-Optimization-for-Language-Model-based-Text-to-Speech"><a href="#MPO-Multidimensional-Preference-Optimization-for-Language-Model-based-Text-to-Speech" class="headerlink" title="MPO: Multidimensional Preference Optimization for Language Model-based   Text-to-Speech"></a>MPO: Multidimensional Preference Optimization for Language Model-based   Text-to-Speech</h2><p><strong>Authors:Kangxiang Xia, Xinfa Zhu, Jixun Yao, Lei Xie</strong></p>
<p>In recent years, text-to-speech (TTS) has seen impressive advancements through large-scale language models, achieving human-level speech quality. Integrating human feedback has proven effective for enhancing robustness in these systems. However, current approaches face challenges in optimizing TTS with preference data across multiple dimensions and often suffer from performance degradation due to overconfidence in rewards. We propose Multidimensional Preference Optimization (MPO) to better align TTS systems with human preferences. MPO introduces a preference set that streamlines the construction of data for multidimensional preference optimization, enabling alignment with multiple dimensions. Additionally, we incorporate regularization during training to address the typical degradation issues in DPO-based approaches. Our experiments demonstrate MPOâ€™s effectiveness, showing significant improvements in intelligibility, speaker similarity, and prosody compared to baseline systems. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œé€šè¿‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›æ­¥ï¼Œè¾¾åˆ°äº†äººç±»æ°´å¹³çš„è¯­éŸ³è´¨é‡ã€‚é›†æˆäººç±»åé¦ˆå·²è¢«è¯æ˜å¯ä»¥æé«˜è¿™äº›ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨ä¼˜åŒ–å…·æœ‰è·¨å¤šä¸ªç»´åº¦çš„åå¥½æ•°æ®çš„TTSæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”ç”±äºè¿‡åº¦ä¾èµ–å¥–åŠ±è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºå¤šç»´åå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰ï¼Œä»¥æ›´å¥½åœ°å°†TTSç³»ç»Ÿä¸äººç±»åå¥½å¯¹é½ã€‚MPOå¼•å…¥äº†ä¸€ä¸ªåå¥½é›†ï¼Œç®€åŒ–äº†ç”¨äºå¤šç»´åå¥½ä¼˜åŒ–çš„æ•°æ®æ„å»ºï¼Œèƒ½å¤Ÿå®ç°ä¸å¤šä¸ªç»´åº¦çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥äº†æ­£åˆ™åŒ–ï¼Œä»¥è§£å†³DPOæ–¹æ³•å¸¸è§çš„é€€åŒ–é—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†MPOçš„æœ‰æ•ˆæ€§ï¼Œä¸åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼Œåœ¨æ¸…æ™°åº¦ã€å‘éŸ³äººç›¸ä¼¼åº¦å’Œè¯­è°ƒæ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00685v1">PDF</a> Accepted by NCMMSC2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>éšç€è¿‘å¹´æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„æ˜¾è‘—è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯å€ŸåŠ©å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œå·²å®ç°äº†è¿‘ä¹äººç±»æ°´å¹³çš„è¯­éŸ³è´¨é‡ã€‚å°½ç®¡é›†æˆäººç±»åé¦ˆå·²è¢«è¯æ˜å¯ä»¥æé«˜ç³»ç»Ÿçš„ç¨³å¥æ€§ï¼Œä½†å½“å‰æ–¹æ³•åœ¨é¢å¯¹è·¨å¤šä¸ªç»´åº¦çš„åå¥½æ•°æ®ä¼˜åŒ–æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸”ç”±äºè¿‡åº¦ä¾èµ–å¥–åŠ±è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šç»´åº¦åå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°å°†TTSç³»ç»Ÿä¸äººç±»åå¥½å¯¹é½ã€‚MPOé€šè¿‡å¼•å…¥åå¥½é›†ç®€åŒ–äº†å¤šç»´åå¥½ä¼˜åŒ–æ•°æ®çš„æ„å»ºï¼Œå¹¶è§£å†³äº†DPOæ–¹æ³•å¸¸è§çš„é€€åŒ–é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼ŒMPOåœ¨æ¸…æ™°åº¦ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè¯­è°ƒæ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯è¿‘å¹´æ¥å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå·²æ¥è¿‘äººç±»æ°´å¹³çš„è¯­éŸ³è´¨é‡ã€‚</li>
<li>é›†æˆäººç±»åé¦ˆå¯ä»¥æé«˜TTSç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚</li>
<li>å½“å‰TTSä¼˜åŒ–æ–¹æ³•é¢ä¸´è·¨å¤šä¸ªç»´åº¦ä¼˜åŒ–æ—¶çš„æŒ‘æˆ˜ï¼Œå¹¶å¯èƒ½å‡ºç°å› è¿‡åº¦ä¾èµ–å¥–åŠ±è€Œå¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºäº†å¤šç»´åº¦åå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°å°†TTSç³»ç»Ÿä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>MPOé€šè¿‡å¼•å…¥åå¥½é›†ç®€åŒ–äº†æ•°æ®æ„å»ºè¿‡ç¨‹ã€‚</li>
<li>MPOè§£å†³äº†DPOæ–¹æ³•å¸¸è§çš„é€€åŒ–é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3226177ccf1b07cde45a65581c5a1810.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9add0f64f43031e99e9f2c65f001d5e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef58a5db1a63afde397c64847af0e07e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee6b852101aeb9ee9ab9f1477c85cd3c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SaD-A-Scenario-Aware-Discriminator-for-Speech-Enhancement"><a href="#SaD-A-Scenario-Aware-Discriminator-for-Speech-Enhancement" class="headerlink" title="SaD: A Scenario-Aware Discriminator for Speech Enhancement"></a>SaD: A Scenario-Aware Discriminator for Speech Enhancement</h2><p><strong>Authors:Xihao Yuan, Siqi Liu, Yan Chen, Hang Zhou, Chang Liu, Hanting Chen, Jie Hu</strong></p>
<p>Generative adversarial network-based models have shown remarkable performance in the field of speech enhancement. However, the current optimization strategies for these models predominantly focus on refining the architecture of the generator or enhancing the quality evaluation metrics of the discriminator. This approach often overlooks the rich contextual information inherent in diverse scenarios. In this paper, we propose a scenario-aware discriminator that captures scene-specific features and performs frequency-domain division, thereby enabling a more accurate quality assessment of the enhanced speech generated by the generator. We conducted comprehensive experiments on three representative models using two publicly available datasets. The results demonstrate that our method can effectively adapt to various generator architectures without altering their structure, thereby unlocking further performance gains in speech enhancement across different scenarios. </p>
<blockquote>
<p>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰é’ˆå¯¹è¿™äº›æ¨¡å‹çš„ä¼˜åŒ–ç­–ç•¥ä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ–ç”Ÿæˆå™¨çš„æ¶æ„æˆ–æé«˜åˆ¤åˆ«å™¨çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡ä¸Šã€‚è¿™ç§æ–¹æ³•å¾€å¾€ä¼šå¿½ç•¥ä¸åŒåœºæ™¯ä¸­ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœºæ™¯æ„ŸçŸ¥åˆ¤åˆ«å™¨ï¼Œå®ƒæ•è·ç‰¹å®šåœºæ™¯çš„ç‰¹å¾å¹¶æ‰§è¡Œé¢‘åŸŸåˆ’åˆ†ï¼Œä»è€Œèƒ½å¤Ÿå¯¹ç”Ÿæˆå™¨ç”Ÿæˆçš„å¢å¼ºè¯­éŸ³è¿›è¡Œæ›´å‡†ç¡®çš„è´¨é‡è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä»£è¡¨æ€§æ¨¡å‹ä¸Šä½¿ç”¨äº†ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†è¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”å„ç§ç”Ÿæˆå™¨æ¶æ„ï¼Œè€Œæ— éœ€æ”¹å˜å…¶ç»“æ„ï¼Œä»è€Œåœ¨ä¸åŒåœºæ™¯çš„è¯­éŸ³å¢å¼ºä¸­å®ç°äº†è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00405v1">PDF</a> 5 pages, 2 figures.Accepted by InterSpeech2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸï¼ŒåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¨¡å‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰å¯¹è¿™äº›æ¨¡å‹çš„ä¼˜åŒ–ç­–ç•¥ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›ç”Ÿæˆå™¨çš„æ¶æ„æˆ–æé«˜åˆ¤åˆ«å™¨çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡ä¸Šï¼Œå¿½ç•¥äº†ä¸åŒåœºæ™¯ä¸­ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åœºæ™¯æ„ŸçŸ¥åˆ¤åˆ«å™¨ï¼Œå®ƒèƒ½å¤Ÿæ•æ‰åœºæ™¯ç‰¹å®šç‰¹å¾å¹¶è¿›è¡Œé¢‘åŸŸåˆ†å‰²ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¯„ä¼°ç”Ÿæˆå™¨ç”Ÿæˆçš„å¢å¼ºè¯­éŸ³è´¨é‡ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä»£è¡¨æ€§æ¨¡å‹ä¸Šä½¿ç”¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†è¿›è¡Œäº†ç»¼åˆå®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆé€‚åº”å„ç§ç”Ÿæˆå™¨æ¶æ„ï¼Œè€Œæ— éœ€æ”¹å˜å…¶ç»“æ„ï¼Œä»è€Œåœ¨ä¸åŒåœºæ™¯ä¸­å®ç°è¯­éŸ³å¢å¼ºçš„è¿›ä¸€æ­¥æ€§èƒ½æå‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰GANåœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸçš„ä¼˜åŒ–ç­–ç•¥ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›ç”Ÿæˆå™¨çš„æ¶æ„å’Œåˆ¤åˆ«å™¨çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡ä¸Šã€‚</li>
<li>æå‡ºçš„åœºæ™¯æ„ŸçŸ¥åˆ¤åˆ«å™¨èƒ½æ•æ‰åœºæ™¯ç‰¹å®šç‰¹å¾ã€‚</li>
<li>åœºæ™¯æ„ŸçŸ¥åˆ¤åˆ«å™¨è¿›è¡Œé¢‘åŸŸåˆ†å‰²ï¼Œæé«˜äº†å¯¹å¢å¼ºè¯­éŸ³è´¨é‡çš„è¯„ä¼°å‡†ç¡®æ€§ã€‚</li>
<li>ç»¼åˆå®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé€‚åº”å„ç§ç”Ÿæˆå™¨æ¶æ„ï¼Œæ— éœ€æ”¹å˜å…¶ç»“æ„ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸­å®ç°äº†è¯­éŸ³å¢å¼ºçš„è¿›ä¸€æ­¥æ€§èƒ½æå‡ã€‚</li>
<li>å…¬å¼€æ•°æ®é›†çš„ç»¼åˆå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00405">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2daf3905efd50b63c3288f5e0b9a3c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eafcf80d595db3223e892251cdaad63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b824d5da87f48a5995af722d91b929c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4209e86f8dd3e365f5ab3472ab0f517a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eced143a285d1eb8e6ad7415a815fdaf.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="AHELM-A-Holistic-Evaluation-of-Audio-Language-Models"><a href="#AHELM-A-Holistic-Evaluation-of-Audio-Language-Models" class="headerlink" title="AHELM: A Holistic Evaluation of Audio-Language Models"></a>AHELM: A Holistic Evaluation of Audio-Language Models</h2><p><strong>Authors:Tony Lee, Haoqin Tu, Chi Heem Wong, Zijun Wang, Siwei Yang, Yifan Mai, Yuyin Zhou, Cihang Xie, Percy Liang</strong></p>
<p>Evaluations of audio-language models (ALMs) â€“ multimodal models that take interleaved audio and text as input and output text â€“ are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets â€“ including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering â€“ to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness ($p&#x3D;0.01$) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 6th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at <a target="_blank" rel="noopener" href="https://crfm.stanford.edu/helm/audio/v1.0.0">https://crfm.stanford.edu/helm/audio/v1.0.0</a>. AHELM is intended to be a living benchmark and new datasets and models will be added over time. </p>
<blockquote>
<p>å¯¹äºéŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰çš„è¯„ä¼°â€”â€”è¿™ç§å¤šæ¨¡æ€æ¨¡å‹ä»¥äº¤æ›¿çš„éŸ³é¢‘å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥å¹¶è¾“å‡ºæ–‡æœ¬â€”â€”ç”±äºç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•è€Œå—åˆ°é˜»ç¢ã€‚å¤§å¤šæ•°åŸºå‡†æµ‹è¯•åªè¡¡é‡ä¸€ç§æˆ–ä¸¤ç§èƒ½åŠ›ï¼Œå¹¶å¿½ç•¥äº†è¯„ä¼°æ–¹é¢ï¼Œå¦‚å…¬å¹³æ€§æˆ–å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œç”±äºä¸åŒçš„æ¨¡å‹ä½¿ç”¨äº†æœ‰é™çš„è¯„ä¼°æ–¹æ³•å’Œæç¤ºæ–¹æ³•ä»¥åŠæ¨ç†å‚æ•°ï¼Œå› æ­¤å¾ˆéš¾å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AHELMåŸºå‡†æµ‹è¯•ï¼Œå®ƒèšåˆäº†å„ç§æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–°çš„åˆæˆéŸ³é¢‘æ–‡æœ¬æ•°æ®é›†PARADEå’ŒCoRe-Benchã€‚PARADEæ—¨åœ¨è¯„ä¼°ALMåœ¨é¿å…åˆ»æ¿å°è±¡æ–¹é¢çš„è¡¨ç°ï¼Œè€ŒCoRe-Benchåˆ™é€šè¿‡æ¨ç†å¤šå›åˆé—®ç­”æ¥è¡¡é‡å¯¹è¯éŸ³é¢‘çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å…¨é¢è¡¡é‡äº†ALMåœ¨æˆ‘ä»¬å·²ç¡®å®šçš„10ä¸ªæ–¹é¢çš„æ€§èƒ½ï¼Œè¿™äº›æ–¹é¢å¯¹äºALMçš„å‘å±•å’Œä½¿ç”¨å¾ˆé‡è¦ï¼šéŸ³é¢‘æ„ŸçŸ¥ã€çŸ¥è¯†ã€æ¨ç†ã€æƒ…æ„Ÿæ£€æµ‹ã€åè§ã€å…¬å¹³æ€§ã€å¤šè¯­è¨€èƒ½åŠ›ã€ç¨³å¥æ€§ã€æ¯’æ€§å’Œå®‰å…¨æ€§ã€‚æˆ‘ä»¬è¿˜å¯¹æç¤ºã€æ¨ç†å‚æ•°å’Œè¯„ä¼°æŒ‡æ ‡è¿›è¡Œäº†æ ‡å‡†åŒ–ï¼Œä»¥ç¡®ä¿æ¨¡å‹ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚æˆ‘ä»¬æµ‹è¯•äº†æ¥è‡ªä¸‰ä¸ªå¼€å‘è€…çš„14ä¸ªå…¬å¼€æƒé‡å’Œå°é—­APIçš„ALMä»¥åŠä¸‰ä¸ªé¢å¤–çš„ç®€å•åŸºçº¿ç³»ç»Ÿï¼ˆæ¯ä¸ªç³»ç»Ÿç”±è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å™¨å’Œè¯­è¨€æ¨¡å‹ç»„æˆï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶Gemini 2.5 Proåœ¨åä¸ªæ–¹é¢ä¸­çš„äº”ä¸ªæ–¹é¢æ’åç¬¬ä¸€ï¼Œä½†åœ¨ASRä»»åŠ¡ä¸Šè¡¨ç°å‡ºç¾¤ä½“ä¸å…¬å¹³æ€§ï¼ˆp&#x3D;0.01ï¼‰ï¼Œè€Œå…¶ä»–å¤§å¤šæ•°æ¨¡å‹åˆ™æ²¡æœ‰ã€‚æˆ‘ä»¬è¿˜å‘ç°åŸºçº¿ç³»ç»Ÿåœ¨AHELMä¸Šçš„è¡¨ç°ç›¸å½“ä¸é”™ï¼Œå…¶ä¸­ä¸€ä¸ªç³»ç»Ÿåœ¨æ•´ä½“æ’åä¸­ä½åˆ—ç¬¬å…­ï¼Œå°½ç®¡å®ƒåªæœ‰è¯­éŸ³åˆ°æ–‡æœ¬çš„åŠŸèƒ½ã€‚ä¸ºäº†é€æ˜èµ·è§ï¼Œæ‰€æœ‰åŸå§‹æç¤ºã€æ¨¡å‹ç”Ÿæˆå’Œè¾“å‡ºéƒ½å¯åœ¨æˆ‘ä»¬çš„ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://crfm.stanford.edu/helm/audio/v1.0.0%E3%80%82AHELM%E6%9C%AF%E8%AF%B4%E6%98%AF%E4%B8%BA%E4%BA%86%E6%88%90%E4%B8%BA%E4%B8%80%E4%B8%AA%E7%BB%B4%E6%BB%BE%E7%9A%84%E5%9F%BA%E6%A1%86%E6%B5%8B%E8%AF%95%EF%BC%8C%E5%B9%B6%E4%BC%9A%E9%9A%8F%E7%9D%A1%E6%8E%A8%E7%A7%BB%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%A8%A1%E5%9E%8B%E3%80%82">https://crfm.stanford.edu/helm/audio/v1.0.0ã€‚AHELMæ—¨åœ¨æˆä¸ºä¸€ä¸ªå¸¸è®¾çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å°†éšç€æ—¶é—´çš„æ¨ç§»æ·»åŠ æ–°çš„æ•°æ®é›†å’Œæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21376v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰çš„è¯„ä»·å› ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•è€Œå—åˆ°é˜»ç¢ã€‚å¤§å¤šæ•°åŸºå‡†æµ‹è¯•åªè¡¡é‡ä¸€ç§æˆ–ä¸¤ç§èƒ½åŠ›ï¼Œå¹¶å¿½ç•¥äº†å…¬å¹³æ€§æˆ–å®‰å…¨æ€§ç­‰è¯„ä»·æ–¹é¢ã€‚æ­¤å¤–ï¼Œç”±äºç¼ºä¹ç»Ÿä¸€çš„è¯„ä»·å’Œä¸åŒæ¨¡å‹çš„æµ‹è¯•é‡‡ç”¨ä¸åŒçš„æç¤ºæ–¹æ³•å’Œæ¨ç†å‚æ•°ï¼Œå¯¼è‡´éš¾ä»¥è¿›è¡Œè·¨æ¨¡å‹æ¯”è¾ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AHELMåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ±‡é›†äº†å¤šä¸ªæ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–°çš„åˆæˆéŸ³é¢‘æ–‡æœ¬æ•°æ®é›†PARADEå’ŒCoRe-Benchï¼Œä»¥å…¨é¢è¡¡é‡ALMåœ¨éŸ³é¢‘æ„ŸçŸ¥ã€çŸ¥è¯†ã€æ¨ç†ã€æƒ…æ„Ÿæ£€æµ‹ã€åè§ã€å…¬å¹³æ€§ç­‰æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬æ ‡å‡†åŒ–äº†æç¤ºã€æ¨ç†å‚æ•°å’Œè¯„ä»·æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿æ¨¡å‹ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚æˆ‘ä»¬å¯¹æ¥è‡ªä¸‰ä¸ªå¼€å‘è€…çš„åå››ç§å¼€æ”¾æƒé‡å’Œå°é—­APIçš„ALMä»¥åŠç”±è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å™¨å’Œè¯­è¨€æ¨¡å‹ç»„æˆçš„ä¸‰ä¸ªé™„åŠ ç®€å•åŸºå‡†ç³»ç»Ÿè¿›è¡Œäº†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶Gemini 2.5 Proåœ¨åä¸ªæ–¹é¢ä¸­çš„äº”ä¸ªæ–¹é¢æ’åç¬¬ä¸€ï¼Œä½†åœ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç¾¤ä½“ä¸å…¬å¹³æ€§ã€‚æˆ‘ä»¬è¿˜å‘ç°åŸºçº¿ç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼Œå…¶ä¸­ä¸€ä¸ªç³»ç»Ÿæ’åç¬¬å…­ï¼Œå°½ç®¡å®ƒåªæœ‰è¯­éŸ³åˆ°æ–‡æœ¬çš„åŠŸèƒ½ã€‚ä¸ºç¡®ä¿é€æ˜åº¦ï¼Œæ‰€æœ‰åŸå§‹æç¤ºã€æ¨¡å‹ç”Ÿæˆå’Œè¾“å‡ºéƒ½å¯åœ¨æˆ‘ä»¬çš„ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://crfm.stanford.edu/helm/audio/v1.0.0%E3%80%82AHELM%E6%97%A8%E5%9C%A8%E6%88%90%E4%B8%BA%E4%B8%80%E4%B8%AA%E6%8C%81%E7%BB%AD%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%EF%BC%8C%E9%9A%8F%E7%9D%80%E6%97%B6%E9%97%B4%E7%9A%84%E6%8E%A8%E7%A7%BB%EF%BC%8C%E6%96%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%B0%86%E8%A2%AB%E6%B7%BB%E5%8A%A0%E8%BF%9B%E6%9D%A5%E3%80%82">https://crfm.stanford.edu/helm/audio/v1.0.0ã€‚AHELMæ—¨åœ¨æˆä¸ºä¸€ä¸ªæŒç»­çš„åŸºå‡†æµ‹è¯•ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œæ–°çš„æ•°æ®é›†å’Œæ¨¡å‹å°†è¢«æ·»åŠ è¿›æ¥ã€‚</a></p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•é™åˆ¶äº†éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰çš„è¯„ä»·ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€åªå…³æ³¨ä¸€ç§æˆ–ä¸¤ç§èƒ½åŠ›ï¼Œå¿½ç•¥è¯„ä»·æ¨¡å‹çš„å…¬å¹³æ€§ã€å®‰å…¨æ€§ç­‰æ–¹é¢ã€‚</li>
<li>å¼•å…¥AHELMåŸºå‡†æµ‹è¯•ä»¥å…¨é¢è¡¡é‡ALMçš„æ€§èƒ½ï¼Œæ¶µç›–éŸ³é¢‘æ„ŸçŸ¥ã€çŸ¥è¯†ç­‰åä¸ªæ–¹é¢ã€‚</li>
<li>AHELMä½¿ç”¨æ ‡å‡†åŒ–æç¤ºã€æ¨ç†å‚æ•°å’Œè¯„ä»·æŒ‡æ ‡ä»¥ç¡®ä¿æ¨¡å‹ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚</li>
<li>æµ‹è¯•äº†å¤šç§ALMå’ŒåŸºçº¿ç³»ç»Ÿï¼Œå‘ç°Gemini 2.5 Proåœ¨å¤šä¸ªæ–¹é¢è¡¨ç°ä¼˜ç§€ä½†å­˜åœ¨ç¾¤ä½“ä¸å…¬å¹³æ€§ã€‚</li>
<li>åŸºçº¿ç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼Œè¯æ˜ç®€å•ç³»ç»Ÿçš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75d788d32e08447d8ce829be3ddcb7ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7aac0542c74c865ec7cfacbb955f60d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5feda23c048d7c169d9fd8a44c62b0d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcea1d680f2a3c394b73d1cf9a831a70.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6a0bdfdf63cac948d1a54a7ecf6398fa.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1bbbb934850718bf94cc89a9bb6bcd1f.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Ultrasound-based detection and malignancy prediction of breast lesions   eligible for biopsy A multi-center clinical-scenario study using nomograms,   large language models, and radiologist evaluation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
