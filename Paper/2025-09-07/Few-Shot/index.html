<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Delta Activations A Representation for Finetuned Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-585ab68f8b79809c9b58c75ffb4f525b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models"><a href="#Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models" class="headerlink" title="Delta Activations: A Representation for Finetuned Large Language Models"></a>Delta Activations: A Representation for Finetuned Large Language Models</h2><p><strong>Authors:Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim</strong></p>
<p>The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/OscarXZQ/delta_activations">https://github.com/OscarXZQ/delta_activations</a>. </p>
<blockquote>
<p>å¼ºå¤§çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸä½¿å¾—ç¤¾åŒºèƒ½å¤Ÿåˆ›å»ºå¤§é‡é€‚åº”ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸçš„åè®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºå…ƒæ•°æ®ä¸ä¸€è‡´å’Œéç»“æ„åŒ–ä»“åº“ï¼Œæµè§ˆå’Œç†è§£è¿™äº›æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†Delta Activationsï¼Œä¸€ç§é€šè¿‡æµ‹é‡å¾®è°ƒæ¨¡å‹çš„å†…éƒ¨æ¿€æ´»ç›¸å¯¹äºåŸºç¡€æ¨¡å‹çš„ç§»åŠ¨æ¥å°†å…¶è¡¨ç¤ºä¸ºå‘é‡åµŒå…¥çš„æ–¹æ³•ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•å…è®¸æŒ‰é¢†åŸŸå’Œä»»åŠ¡è¿›è¡Œæœ‰æ•ˆèšç±»ï¼Œæ­ç¤ºæ¨¡å‹æ™¯è§‚ä¸­çš„ç»“æ„ã€‚Delta Activationsè¿˜æ˜¾ç¤ºå‡ºç†æƒ³çš„å±æ€§ï¼šå®ƒåœ¨å¾®è°ƒè®¾ç½®ä¸­æ˜¯ç¨³å¥çš„ï¼Œå¹¶ä¸”åœ¨æ··åˆå¾®è°ƒæ•°æ®é›†æ—¶è¡¨ç°å‡ºç´¯åŠ å±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†Delta Activationså¯ä»¥é€šè¿‡å‡ æ¬¡å¾®è°ƒåµŒå…¥ä»»åŠ¡ï¼Œå¹¶è¿›ä¸€æ­¥æ¢ç´¢å…¶åœ¨æ¨¡å‹é€‰æ‹©å’Œåˆå¹¶ä¸­çš„ä½¿ç”¨ã€‚æˆ‘ä»¬å¸Œæœ›Delta Activationsèƒ½å¤Ÿæ¨åŠ¨é‡æ–°ä½¿ç”¨å…¬å¼€å¯ç”¨æ¨¡å‹çš„å®è·µã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/OscarXZQ/delta_activations%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/OscarXZQ/delta_activationsè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04442v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸä¿ƒä½¿ç¤¾åŒºåˆ›å»ºäº†ä¼—å¤šé’ˆå¯¹ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸçš„åè®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºå…ƒæ•°æ®çš„ä¸ä¸€è‡´æ€§å’Œä»“åº“çš„ç»“æ„åŒ–ä¸è¶³ï¼Œç†è§£å’Œå¯¼èˆªè¿™äº›æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡ä»‹ç»Delta Activationsï¼Œä¸€ç§é€šè¿‡æµ‹é‡ç›¸å¯¹äºåŸºç¡€æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»çš„å˜åŒ–æ¥è¡¨ç¤ºå¾®è°ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œå°†å…¶è¡¨ç¤ºä¸ºå‘é‡åµŒå…¥ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•å…è®¸æŒ‰é¢†åŸŸå’Œä»»åŠ¡è¿›è¡Œæœ‰æ•ˆèšç±»ï¼Œæ­ç¤ºæ¨¡å‹æ™¯è§‚ä¸­çš„ç»“æ„ã€‚Delta Activationsè¿˜å…·æœ‰ä»¤äººæ»¡æ„çš„ç‰¹æ€§ï¼šå®ƒé€‚ç”¨äºå„ç§å¾®è°ƒè®¾ç½®ï¼Œå¹¶åœ¨æ··åˆå¾®è°ƒæ•°æ®é›†æ—¶è¡¨ç°å‡ºå¯åŠ æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å±•ç¤ºäº†Delta Activationså¯ä»¥é€šè¿‡å°‘æ ·æœ¬å¾®è°ƒåµŒå…¥ä»»åŠ¡ï¼Œå¹¶è¿›ä¸€æ­¥æ¢ç´¢å…¶åœ¨æ¨¡å‹é€‰æ‹©å’Œåˆå¹¶ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡å¸Œæœ›Delta Activationsèƒ½ä¿ƒè¿›å…¬å¼€æ¨¡å‹çš„å¤ç”¨å®è·µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¤¾åŒºå·²ç»åˆ›å»ºäº†ä¼—å¤šé’ˆå¯¹ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸçš„åè®­ç»ƒæ¨¡å‹ã€‚</li>
<li>ç†è§£å’Œå¯¼èˆªè¿™äº›æ¨¡å‹ç”±äºå…ƒæ•°æ®çš„ä¸ä¸€è‡´æ€§å’Œä»“åº“çš„ç»“æ„åŒ–ä¸è¶³è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>Delta Activationsæ˜¯ä¸€ç§é€šè¿‡æµ‹é‡å†…éƒ¨æ¿€æ´»çš„å˜åŒ–æ¥è¡¨ç¤ºå¾®è°ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œå…è®¸æŒ‰é¢†åŸŸå’Œä»»åŠ¡æœ‰æ•ˆèšç±»ã€‚</li>
<li>Delta Activationså…·æœ‰ç¨³å¥æ€§å’Œå¯åŠ æ€§ï¼Œé€‚ç”¨äºå„ç§å¾®è°ƒè®¾ç½®å’Œæ··åˆæ•°æ®é›†ã€‚</li>
<li>Delta Activationså¯ä»¥é€šè¿‡å°‘æ ·æœ¬å¾®è°ƒåµŒå…¥ä»»åŠ¡ï¼Œå¹¶ç”¨äºæ¨¡å‹é€‰æ‹©å’Œåˆå¹¶ã€‚</li>
<li>Delta Activationsæœ‰æœ›ä¿ƒè¿›å…¬å¼€æ¨¡å‹çš„å¤ç”¨å®è·µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8adf90d00afc453c7439301c7c3164d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da384e9c2f4887723b38eee8b93ecfcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ab378fb1d9c7de550bb06fdeeae4410.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19072fa45112b139700171bf76fb15fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e37f715a0d9b0171777c218e8cff9095.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78d74f1c2f2b010766714140f6023cb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0236bc1379c89c847bf7557d645ae2d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CANDY-Benchmarking-LLMsâ€™-Limitations-and-Assistive-Potential-in-Chinese-Misinformation-Fact-Checking"><a href="#CANDY-Benchmarking-LLMsâ€™-Limitations-and-Assistive-Potential-in-Chinese-Misinformation-Fact-Checking" class="headerlink" title="CANDY: Benchmarking LLMsâ€™ Limitations and Assistive Potential in Chinese   Misinformation Fact-Checking"></a>CANDY: Benchmarking LLMsâ€™ Limitations and Assistive Potential in Chinese   Misinformation Fact-Checking</h2><p><strong>Authors:Ruiling Guo, Xinwei Yang, Chen Huang, Tong Zhang, Yong Hu</strong></p>
<p>The effectiveness of large language models (LLMs) to fact-check misinformation remains uncertain, despite their growing use. To this end, we present CANDY, a benchmark designed to systematically evaluate the capabilities and limitations of LLMs in fact-checking Chinese misinformation. Specifically, we curate a carefully annotated dataset of ~20k instances. Our analysis shows that current LLMs exhibit limitations in generating accurate fact-checking conclusions, even when enhanced with chain-of-thought reasoning and few-shot prompting. To understand these limitations, we develop a taxonomy to categorize flawed LLM-generated explanations for their conclusions and identify factual fabrication as the most common failure mode. Although LLMs alone are unreliable for fact-checking, our findings indicate their considerable potential to augment human performance when deployed as assistive tools in scenarios. Our dataset and code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/SCUNLP/CANDY">https://github.com/SCUNLP/CANDY</a> </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä½¿ç”¨æ—¥ç›Šæ™®éï¼Œä½†å®ƒä»¬æ ¸å®è¯¯ä¿¡æ¯çš„æœ‰æ•ˆæ€§ä»ä¸ç¡®å®šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CANDYï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°LLMåœ¨æ ¸å®ä¸­æ–‡è¯¯ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›å’Œå±€é™æ€§çš„åŸºå‡†æµ‹è¯•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç²¾å¿ƒæ ‡æ³¨äº†ä¸€ä¸ªåŒ…å«çº¦20kå®ä¾‹çš„æ•°æ®é›†ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå³ä½¿åœ¨é‡‡ç”¨æ€ç»´é“¾æ¨ç†å’Œå°‘é‡æç¤ºçš„æƒ…å†µä¸‹ï¼Œå½“å‰çš„LLMåœ¨ç”Ÿæˆå‡†ç¡®çš„æ ¸å®ç»“è®ºæ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†äº†è§£è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåˆ†ç±»æ³•ï¼Œå¯¹LLMç”Ÿæˆçš„ç»“è®ºä¸­çš„é”™è¯¯è§£é‡Šè¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶ç¡®å®šäº†äº‹å®æé€ æ˜¯æœ€å¸¸è§çš„å¤±è´¥æ¨¡å¼ã€‚è™½ç„¶LLMå•ç‹¬ç”¨äºæ ¸å®ä¿¡æ¯å¹¶ä¸å¯é ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ä½œä¸ºè¾…åŠ©å·¥å…·éƒ¨ç½²çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬å¯¹æé«˜äººç±»æ€§èƒ½æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SCUNLP/CANDY%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/SCUNLP/CANDYè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03957v1">PDF</a> Findings of EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€æµ‹ä¸­æ–‡é”™è¯¯ä¿¡æ¯æ–¹é¢çš„æ•ˆèƒ½å°šæœªæ˜ç¡®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºCANDYåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMsåœ¨äº‹å®æ ¸æŸ¥æ–¹é¢çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚æˆ‘ä»¬ç²¾å¿ƒæ ‡æ³¨äº†ä¸€ä¸ªåŒ…å«çº¦2ä¸‡ä¸ªå®ä¾‹çš„æ•°æ®é›†ï¼Œåˆ†æè¡¨æ˜ï¼Œå³ä½¿åœ¨é‡‡ç”¨æ€ç»´é“¾æ¨ç†å’Œå°‘é‡æç¤ºçš„æƒ…å†µä¸‹ï¼Œå½“å‰çš„LLMsåœ¨ç”Ÿæˆå‡†ç¡®çš„äº‹å®æ ¸æŸ¥ç»“è®ºæ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€æµ‹é”™è¯¯ä¿¡æ¯æ–¹é¢çš„æ•ˆèƒ½å°šä¸ç¡®å®šã€‚</li>
<li>CANDYåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°LLMsåœ¨äº‹å®æ ¸æŸ¥ä¸­æ–‡é”™è¯¯ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æˆ‘ä»¬æ ‡æ³¨äº†ä¸€ä¸ªåŒ…å«çº¦2ä¸‡ä¸ªå®ä¾‹çš„æ•°æ®é›†ç”¨äºè¯„ä¼°ã€‚</li>
<li>å½“å‰LLMsåœ¨ç”Ÿæˆå‡†ç¡®äº‹å®æ ¸æŸ¥ç»“è®ºæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>LLMsç”Ÿæˆçš„è§£é‡Šå­˜åœ¨ç¼ºé™·ï¼Œå¹¶åˆ†ç±»äº†å¸¸è§çš„å¤±è´¥æ¨¡å¼ã€‚</li>
<li>äº‹å®ç¼–é€ æ˜¯æœ€å¸¸è§çš„å¤±è´¥æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6f11e4efc663e158ee9813c17500d95b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b995a2ea75a8143e1e4314d513ed9e81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c984f8d8f47f486a514c7ca0879fdce4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3df12fe804312e7f134b94ec1e3d6337.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bc49e94ecb35eef855a9f21cf92386b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Attn-Adapter-Attention-Is-All-You-Need-for-Online-Few-shot-Learner-of-Vision-Language-Model"><a href="#Attn-Adapter-Attention-Is-All-You-Need-for-Online-Few-shot-Learner-of-Vision-Language-Model" class="headerlink" title="Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of   Vision-Language Model"></a>Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of   Vision-Language Model</h2><p><strong>Authors:Phuoc-Nguyen Bui, Khanh-Binh Nguyen, Hyunseung Choo</strong></p>
<p>Contrastive vision-language models excel in zero-shot image recognition but face challenges in few-shot scenarios due to computationally intensive offline fine-tuning using prompt learning, which risks overfitting. To overcome these limitations, we propose Attn-Adapter, a novel online few-shot learning framework that enhances CLIPâ€™s adaptability via a dual attention mechanism. Our design incorporates dataset-specific information through two components: the Memory Attn-Adapter, which refines category embeddings using support examples, and the Local-Global Attn-Adapter, which enriches image embeddings by integrating local and global features. This architecture enables dynamic adaptation from a few labeled samples without retraining the base model. Attn-Adapter outperforms state-of-the-art methods in cross-category and cross-dataset generalization, maintaining efficient inference and scaling across CLIP backbones. </p>
<blockquote>
<p>å¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒè¯†åˆ«æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å°æ ·æœ¬åœºæ™¯ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºä½¿ç”¨æç¤ºå­¦ä¹ è¿›è¡Œç¦»çº¿ç²¾ç»†è°ƒæ•´è®¡ç®—é‡å¤§ï¼Œå­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Attn-Adapterï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åœ¨çº¿å°æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åŒæ³¨æ„åŠ›æœºåˆ¶æé«˜CLIPçš„é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„è®¾è®¡é€šè¿‡ä¸¤ä¸ªç»„ä»¶èå…¥äº†æ•°æ®é›†ç‰¹å®šä¿¡æ¯ï¼šMemory Attn-Adapterä½¿ç”¨æ”¯æŒæ ·ä¾‹ç»†åŒ–ç±»åˆ«åµŒå…¥ï¼Œè€ŒLocal-Global Attn-Adapteré€šè¿‡æ•´åˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ä¸°å¯Œå›¾åƒåµŒå…¥ã€‚è¯¥æ¶æ„èƒ½å¤Ÿä»æœªæ ‡æ³¨çš„æ ·æœ¬ä¸­åŠ¨æ€é€‚åº”ï¼Œæ— éœ€é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚Attn-Adapteråœ¨è·¨ç±»åˆ«å’Œè·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢ä¼˜äºç°æœ‰æœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆçš„æ¨ç†å’Œè·¨CLIPéª¨å¹²ç½‘çš„æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03895v1">PDF</a> ICCV 2025 - LIMIT Workshop</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒè¯†åˆ«ä¸­çš„å“è¶Šè¡¨ç°ï¼Œé’ˆå¯¹å…¶åœ¨å°æ ·æœ¬åœºæ™¯ä¸‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†Attn-Adapterè¿™ä¸€å…¨æ–°çš„åœ¨çº¿å°æ ·æœ¬å­¦ä¹ æ¡†æ¶ã€‚å®ƒé€šè¿‡åŒé‡æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºCLIPæ¨¡å‹çš„é€‚åº”æ€§ï¼Œé€šè¿‡Memory Attn-Adapterå’ŒLocal-Global Attn-Adapterä¸¤ä¸ªç»„ä»¶ï¼Œåˆ©ç”¨æ”¯æŒå®ä¾‹å’Œé›†æˆå±€éƒ¨ä¸å…¨å±€ç‰¹å¾çš„æ–¹å¼ï¼Œå®ç°æ•°æ®é›†ç‰¹å®šä¿¡æ¯çš„èå…¥ã€‚è¿™ä¸€æ¶æ„èƒ½å¤Ÿåœ¨å°‘é‡æ ‡æ³¨æ ·æœ¬çš„åŸºç¡€ä¸Šå®ç°åŠ¨æ€é€‚åº”ï¼Œæ— éœ€é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚Attn-Adapteråœ¨è·¨ç±»åˆ«å’Œè·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆæ¨ç†å’Œè·¨CLIPéª¨æ¶çš„æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒè¯†åˆ«ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨å°æ ·æœ¬åœºæ™¯ä¸‹é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>Attn-Adapteræ˜¯ä¸€ä¸ªåœ¨çº¿å°æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºCLIPæ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>Attn-Adapteré€šè¿‡åŒé‡æ³¨æ„åŠ›æœºåˆ¶å®ç°åŠ¨æ€é€‚åº”ï¼Œæ— éœ€é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚</li>
<li>Memory Attn-Adapteré€šè¿‡æ”¯æŒå®ä¾‹ä¼˜åŒ–ç±»åˆ«åµŒå…¥ã€‚</li>
<li>Local-Global Attn-Adapteré€šè¿‡é›†æˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ä¸°å¯Œå›¾åƒåµŒå…¥ã€‚</li>
<li>Attn-Adapteråœ¨è·¨ç±»åˆ«å’Œè·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79ae95c4abbcc9efe11c35a7e0780247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93a47a414ed1ecb392dfc28de146c644.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59383881004df2cd1a5975c76c313b26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-130bb3e97b54c3cc1dfa1d48b5aaead0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af1841fa5efa596d34a2a6e4e075c547.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a2af71c28bf2d92928c741053f975c8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Singular-Value-Few-shot-Adaptation-of-Vision-Language-Models"><a href="#Singular-Value-Few-shot-Adaptation-of-Vision-Language-Models" class="headerlink" title="Singular Value Few-shot Adaptation of Vision-Language Models"></a>Singular Value Few-shot Adaptation of Vision-Language Models</h2><p><strong>Authors:Taha Koleilat, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and \textit{parameter-efficient} adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only \textbf{0.04%} of the modelâ€™s total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/CLIP-SVD">https://github.com/HealthX-Lab/CLIP-SVD</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨å¤šç§åº”ç”¨ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹é€‚åº”åˆ°æ–°çš„ç»†ç²’åº¦é¢†åŸŸä»ç„¶å¾ˆå›°éš¾ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºæç¤ºå·¥ç¨‹å’Œé«˜æ˜‚çš„å®Œå…¨æ¨¡å‹å¾®è°ƒæˆæœ¬ã€‚ç°æœ‰çš„é€‚åº”æ–¹æ³•ä¾èµ–äºå¢å¼ºç»„ä»¶ï¼Œå¦‚æç¤ºä»¤ç‰Œå’Œé€‚é…å™¨æ¨¡å—ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶é€‚åº”è´¨é‡ï¼Œä½¿æ¨¡å‹ä¸ç¨³å®šï¼Œå¹¶å¯èƒ½ç ´ååœ¨é¢„è®­ç»ƒæœŸé—´å­¦åˆ°çš„ä¸°å¯ŒçŸ¥è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{CLIP-SVD}ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€å’Œå‚æ•°æœ‰æ•ˆçš„é€‚åº”æŠ€æœ¯ï¼Œå®ƒåˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ¥ä¿®æ”¹CLIPçš„å†…éƒ¨å‚æ•°ç©ºé—´ï¼Œè€Œæ— éœ€æ³¨å…¥é¢å¤–çš„æ¨¡å—ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åªå¾®è°ƒCLIPå‚æ•°çŸ©é˜µçš„å¥‡å¼‚å€¼ï¼Œä»¥é‡æ–°ç¼©æ”¾ç”¨äºé¢†åŸŸé€‚åº”çš„åŸºå‘é‡ï¼ŒåŒæ—¶ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™ç§è®¾è®¡ä»…ä½¿ç”¨æ¨¡å‹æ€»å‚æ•°çš„\textbf{0.04%}å°±èƒ½å®ç°å¢å¼ºçš„é€‚åº”æ€§èƒ½ï¼Œå¹¶æ›´å¥½åœ°ä¿æŒå…¶æ³›åŒ–èƒ½åŠ›ã€‚CLIP-SVDåœ¨11ä¸ªè‡ªç„¶å’Œ10ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç»“æœï¼Œåœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹åœ¨å‡†ç¡®æ€§å’Œæ³›åŒ–æ–¹é¢å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨åŸºäºè‡ªç„¶è¯­è¨€çš„æ–¹æ³•æ¥åˆ†æCLIPé€‚åº”çš„æœ‰æ•ˆæ€§å’ŒåŠ¨æ€æ€§ï¼Œä»¥å®ç°CLIP-SVDçš„å¯è§£é‡Šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/CLIP-SVD%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HealthX-Lab/CLIP-SVDä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03740v1">PDF</a> 10 pages, 2 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰çš„CLIPæ¨¡å‹é€‚åº”æ–°æ–¹æ³•â€”â€”CLIP-SVDã€‚è¯¥æ–¹æ³•åˆ©ç”¨SVDä¿®æ”¹CLIPçš„å†…éƒ¨å‚æ•°ç©ºé—´ï¼Œå®ç°å¤šæ¨¡æ€å’Œå‚æ•°é«˜æ•ˆçš„æ¨¡å‹é€‚åº”ã€‚é€šè¿‡å¾®è°ƒCLIPå‚æ•°çŸ©é˜µçš„å¥‡å¼‚å€¼æ¥è°ƒæ•´åŸºç¡€å‘é‡ï¼Œä»¥é€‚åº”æ–°é¢†åŸŸï¼ŒåŒæ—¶ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„ä¸°å¯ŒçŸ¥è¯†ã€‚è¯¥æ–¹æ³•ä»…ä½¿ç”¨æ¨¡å‹æ€»å‚æ•°çš„0.04%ï¼Œå®ç°ä¼˜ç§€çš„é€‚åº”æ€§èƒ½å¹¶ä¿æŒè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚CLIP-SVDåœ¨å¤šç§æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç»“æœï¼Œå¹¶åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é‡‡ç”¨è‡ªç„¶è¯­è¨€åˆ†ææ–¹æ³•ç ”ç©¶CLIPé€‚åº”çš„æœ‰æ•ˆæ€§åŠå…¶åŠ¨æ€å˜åŒ–ï¼Œä¸ºCLIP-SVDæä¾›å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIP-SVDæ˜¯ä¸€ç§æ–°å‹çš„CLIPæ¨¡å‹é€‚åº”æ–¹æ³•ï¼Œåˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ä¿®æ”¹æ¨¡å‹å†…éƒ¨å‚æ•°ç©ºé—´ï¼Œå®ç°å¤šæ¨¡æ€å’Œå‚æ•°é«˜æ•ˆçš„é€‚åº”ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒCLIPå‚æ•°çŸ©é˜µçš„å¥‡å¼‚å€¼æ¥é€‚åº”æ–°é¢†åŸŸï¼ŒåŒæ—¶ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„ä¸°å¯ŒçŸ¥è¯†ï¼Œå®ç°ä¼˜ç§€çš„é€‚åº”æ€§èƒ½å¹¶ä¿æŒè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>CLIP-SVDä½¿ç”¨ä»…æ¨¡å‹æ€»å‚æ•°çš„0.04%ï¼Œå¯å¢å¼ºé€‚åº”æ€§èƒ½åŒæ—¶ä¿æŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>CLIP-SVDåœ¨å¤šç§æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç»“æœï¼ŒåŒ…æ‹¬è‡ªç„¶å’Œç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-67d4ca625cfd58cf54373c460647595d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8994c8ff3e0fbec4bf1617ebb2045e5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5336d247e8ab3dd92c429852a35f3db.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Are-We-SOLID-Yet-An-Empirical-Study-on-Prompting-LLMs-to-Detect-Design-Principle-Violations"><a href="#Are-We-SOLID-Yet-An-Empirical-Study-on-Prompting-LLMs-to-Detect-Design-Principle-Violations" class="headerlink" title="Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design   Principle Violations"></a>Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design   Principle Violations</h2><p><strong>Authors:Fatih Pehlivan, ArÃ§in ÃœlkÃ¼ ErgÃ¼zen, Sahand Moslemi Yengejeh, Mayasah Lami, Anil Koyuncu</strong></p>
<p>Traditional static analysis methods struggle to detect semantic design flaws, such as violations of the SOLID principles, which require a strong understanding of object-oriented design patterns and principles. Existing solutions typically focus on individual SOLID principles or specific programming languages, leaving a gap in the ability to detect violations across all five principles in multi-language codebases. This paper presents a new approach: a methodology that leverages tailored prompt engineering to assess LLMs on their ability to detect SOLID violations across multiple languages. We present a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder, and GPT-4o Mini-on their ability to detect violations of all five SOLID principles. For this evaluation, we construct a new benchmark dataset of 240 manually validated code examples. Using this dataset, we test four distinct prompt strategies inspired by established zero-shot, few-shot, and chain-of-thought techniques to systematically measure their impact on detection accuracy. Our emerging results reveal a stark hierarchy among models, with GPT-4o Mini decisively outperforming others, yet even struggles with challenging principles like DIP. Crucially, we show that prompt strategy has a dramatic impact, but no single strategy is universally best; for instance, a deliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE prompt is superior for DIP violations. Across all experiments, detection accuracy is heavily influenced by language characteristics and degrades sharply with increasing code complexity. These initial findings demonstrate that effective, AI-driven design analysis requires not a single best model, but a tailored approach that matches the right model and prompt to the specific design context, highlighting the potential of LLMs to support maintainability through AI-assisted code analysis. </p>
<blockquote>
<p>ä¼ ç»Ÿé™æ€åˆ†ææ–¹æ³•åœ¨æ£€æµ‹è¯­ä¹‰è®¾è®¡ç¼ºé™·æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¦‚è¿åSOLIDåŸåˆ™ç­‰ï¼Œè¿™éœ€è¦æ·±å…¥ç†è§£é¢å‘å¯¹è±¡çš„è®¾è®¡æ¨¡å¼å’ŒåŸåˆ™ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸ä¸“æ³¨äºå•ä¸ªSOLIDåŸåˆ™æˆ–ç‰¹å®šç¼–ç¨‹è¯­è¨€ï¼Œå› æ­¤åœ¨å¤šè¯­è¨€ä»£ç åº“ä¸­çš„æ‰€æœ‰äº”ä¸ªåŸåˆ™è¿åæ£€æµ‹æ–¹é¢å­˜åœ¨ç©ºç™½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼šä¸€ç§åˆ©ç”¨å®šåˆ¶æç¤ºå·¥ç¨‹æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§è¯­è¨€ä¸‹æ£€æµ‹SOLIDè¿åèƒ½åŠ›çš„æ–¹æ³•ã€‚æˆ‘ä»¬å±•ç¤ºäº†å››ä¸ªé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹CodeLlamaã€DeepSeekCoderã€QwenCoderå’ŒGPT-4o Miniåœ¨æ£€æµ‹æ‰€æœ‰äº”ä¸ªSOLIDåŸåˆ™è¿åæ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºæ­¤è¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«240ä¸ªæ‰‹åŠ¨éªŒè¯çš„ä»£ç ç¤ºä¾‹çš„æ–°åŸºå‡†æ•°æ®é›†ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬æµ‹è¯•äº†å››ç§ä¸åŒçš„æç¤ºç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥çµæ„Ÿæ¥è‡ªäºå·²å»ºç«‹çš„é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ€ç»´é“¾æŠ€æœ¯ï¼Œä»¥ç³»ç»Ÿåœ°æµ‹é‡å®ƒä»¬å¯¹æ£€æµ‹å‡†ç¡®ç‡çš„å½±å“ã€‚æˆ‘ä»¬çš„åˆæ­¥ç»“æœæ­ç¤ºäº†æ¨¡å‹ä¹‹é—´çš„é²œæ˜å±‚æ¬¡ç»“æ„ï¼ŒGPT-4o Miniåœ¨æ€§èƒ½æ–¹é¢æ˜æ˜¾è¶…è¶Šäº†å…¶ä»–æ¨¡å‹ï¼Œä½†åœ¨DIPç­‰æŒ‘æˆ˜æ€§åŸç†æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜æç¤ºç­–ç•¥å…·æœ‰å·¨å¤§å½±å“ï¼Œä½†æ²¡æœ‰ä¸€ç§å•ä¸€ç­–ç•¥æ˜¯æ™®éé€‚ç”¨çš„æœ€ä½³ç­–ç•¥ï¼›ä¾‹å¦‚ï¼Œä¸€ä¸ªæ·±æ€ç†Ÿè™‘çš„ENSEMBLæç¤ºåœ¨OCPæ£€æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œä¸€ä¸ªåŸºäºæç¤ºçš„EXAMPLEæç¤ºåœ¨DIPè¿è§„æ–¹é¢æ›´èƒœä¸€ç­¹ã€‚åœ¨æ‰€æœ‰å®éªŒä¸­ï¼Œæ£€æµ‹å‡†ç¡®ç‡å—åˆ°è¯­è¨€ç‰¹æ€§çš„å¼ºçƒˆå½±å“ï¼Œéšç€ä»£ç å¤æ‚æ€§çš„å¢åŠ ï¼Œå‡†ç¡®ç‡æ€¥å‰§ä¸‹é™ã€‚è¿™äº›åˆæ­¥ç»“æœè¡¨æ˜ï¼Œæœ‰æ•ˆã€AIé©±åŠ¨çš„è®¾è®¡åˆ†æéœ€è¦ä¸€ç§å®šåˆ¶çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ­£ç¡®çš„æ¨¡å‹å’Œæç¤ºä¸ç‰¹å®šçš„è®¾è®¡ä¸Šä¸‹æ–‡ç›¸åŒ¹é…ï¼Œçªæ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šè¿‡AIè¾…åŠ©ä»£ç åˆ†ææ”¯æŒå¯ç»´æŠ¤æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03093v1">PDF</a> Accepted to ASE2025</p>
<p><strong>æ‘˜è¦</strong><br>æœ¬ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿé™æ€åˆ†ææ–¹æ³•åœ¨è¯­ä¹‰è®¾è®¡ç¼ºé™·æ£€æµ‹ä¸­çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å®šåˆ¶æç¤ºå·¥ç¨‹è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ä»£ç åº“ä¸­æ£€æµ‹SOLIDåŸåˆ™è¿åçš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å¯¹å››æ¬¾é¢†å…ˆçš„LLMs-CodeLlamaã€DeepSeekCoderã€QwenCoderå’ŒGPT-4o Miniè¿›è¡Œäº†è¯„ä¼°ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«240ä¸ªæ‰‹åŠ¨éªŒè¯çš„ä»£ç ç¤ºä¾‹çš„æ–°åŸºå‡†æ•°æ®é›†ï¼Œå¹¶æµ‹è¯•äº†å››ç§ä¸åŒçš„æç¤ºç­–ç•¥ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-4o Miniè¡¨ç°æœ€ä½³ï¼Œä½†ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚æç¤ºç­–ç•¥å¯¹æ£€æµ‹æ•ˆæœæœ‰å¾ˆå¤§å½±å“ï¼Œä½†æ²¡æœ‰ä¸€ç§ç­–ç•¥æ˜¯æ™®éé€‚ç”¨çš„ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯­è¨€ç‰¹æ€§å¯¹æ£€æµ‹ç»“æœæœ‰å¾ˆå¤§å½±å“ï¼Œéšç€ä»£ç å¤æ‚æ€§çš„å¢åŠ ï¼Œæ£€æµ‹ç²¾åº¦ä¼šæ€¥å‰§ä¸‹é™ã€‚è¿™äº›åˆæ­¥ç»“æœè¡¨æ˜ï¼Œæœ‰æ•ˆçš„AIé©±åŠ¨è®¾è®¡åˆ†æéœ€è¦åŒ¹é…ç‰¹å®šè®¾è®¡ä¸Šä¸‹æ–‡çš„å®šåˆ¶æ–¹æ³•ï¼Œçªæ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¯æŒå¯ç»´æŠ¤æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¼ ç»Ÿé™æ€åˆ†ææ–¹æ³•åœ¨æ£€æµ‹è¯­ä¹‰è®¾è®¡ç¼ºé™·ï¼ˆå¦‚SOLIDåŸåˆ™è¿åï¼‰æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œéœ€è¦æ·±å…¥ç†è§£é¢å‘å¯¹è±¡çš„è®¾è®¡æ¨¡å¼å’ŒåŸåˆ™ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆä¸»è¦å…³æ³¨å•ä¸ªSOLIDåŸåˆ™æˆ–ç‰¹å®šç¼–ç¨‹è¯­è¨€ï¼Œç¼ºä¹åœ¨å¤šè¯­è¨€ä»£ç åº“ä¸­æ£€æµ‹æ‰€æœ‰äº”ä¸ªåŸåˆ™è¿åçš„èƒ½åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å®šåˆ¶æç¤ºå·¥ç¨‹è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€æµ‹SOLIDåŸåˆ™è¿åæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å¯¹å››æ¬¾é¢†å…ˆçš„LLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°GPT-4o Miniåœ¨æ£€æµ‹SOLIDåŸåˆ™è¿åæ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä½†ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚</li>
<li>æç¤ºç­–ç•¥å¯¹æ£€æµ‹æ•ˆæœæœ‰å¾ˆå¤§å½±å“ï¼Œæ²¡æœ‰ä¸€ç§ç­–ç•¥æ˜¯æ™®éé€‚ç”¨çš„ã€‚</li>
<li>è¯­è¨€ç‰¹æ€§å’Œä»£ç å¤æ‚æ€§å¯¹æ£€æµ‹ç»“æœæœ‰å¾ˆå¤§å½±å“ï¼Œéšç€ä»£ç å¤æ‚æ€§çš„å¢åŠ ï¼Œæ£€æµ‹ç²¾åº¦ä¼šæ€¥å‰§ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0220c60f6fe1cf932df8129dcb96ba2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac763418a98aecf8fdb27d83137507e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62b65d76becc9481e5fd3b4091a5d370.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SPENet-Self-guided-Prototype-Enhancement-Network-for-Few-shot-Medical-Image-Segmentation"><a href="#SPENet-Self-guided-Prototype-Enhancement-Network-for-Few-shot-Medical-Image-Segmentation" class="headerlink" title="SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical   Image Segmentation"></a>SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical   Image Segmentation</h2><p><strong>Authors:Chao Fan, Xibin Jia, Anqi Xiao, Hongyuan Yu, Zhenghan Yang, Dawei Yang, Hui Xu, Yan Huang, Liang Wang</strong></p>
<p>Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of medical objects using only a few labeled images. Prototype-based methods have made significant progress in addressing FSMIS. However, they typically generate a single global prototype for the support image to match with the query image, overlooking intra-class variations. To address this issue, we propose a Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce a Multi-level Prototype Generation (MPG) module, which enables multi-granularity measurement between the support and query images by simultaneously generating a global prototype and an adaptive number of local prototypes. Additionally, we observe that not all local prototypes in the support image are beneficial for matching, especially when there are substantial discrepancies between the support and query images. To alleviate this issue, we propose a Query-guided Local Prototype Enhancement (QLPE) module, which adaptively refines support prototypes by incorporating guidance from the query image, thus mitigating the negative effects of such discrepancies. Extensive experiments on three public medical datasets demonstrate that SPENet outperforms existing state-of-the-art methods, achieving superior performance. </p>
<blockquote>
<p>å°‘é‡åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ—¨åœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡è®°å›¾åƒå¯¹æ–°å‹åŒ»ç–—å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚åŸºäºåŸå‹çš„æ–¹æ³•åœ¨è§£å†³FSMISæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸åªä¸ºæ”¯æŒå›¾åƒç”Ÿæˆä¸€ä¸ªå…¨å±€åŸå‹ï¼Œä»¥ä¸æŸ¥è¯¢å›¾åƒè¿›è¡ŒåŒ¹é…ï¼Œä»è€Œå¿½ç•¥äº†ç±»å†…å˜åŒ–ã€‚ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªå¼•å¯¼åŸå‹å¢å¼ºç½‘ç»œï¼ˆSPENetï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šå±‚æ¬¡åŸå‹ç”Ÿæˆï¼ˆMPGï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡åŒæ—¶ç”Ÿæˆå…¨å±€åŸå‹å’Œè‡ªé€‚åº”æ•°é‡çš„å±€éƒ¨åŸå‹ï¼Œå®ç°äº†æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å¤šç²’åº¦æµ‹é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ”¯æŒå›¾åƒä¸­çš„æ‰€æœ‰å±€éƒ¨åŸå‹å¹¶ä¸éƒ½æœ‰åˆ©äºåŒ¹é…ï¼Œå°¤å…¶æ˜¯åœ¨æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æŸ¥è¯¢å¼•å¯¼å±€éƒ¨åŸå‹å¢å¼ºï¼ˆQLPEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡èå…¥æŸ¥è¯¢å›¾åƒçš„æŒ‡å¯¼æ¥è‡ªé€‚åº”åœ°ä¼˜åŒ–æ”¯æŒåŸå‹ï¼Œä»è€Œå‡è½»äº†è¿™ç§å·®å¼‚å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚åœ¨ä¸‰ä¸ªå…¬å…±åŒ»ç–—æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSPENetä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02993v1">PDF</a> Accepted by MICCAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Few-Shot Medical Image Segmentationï¼ˆFSMISï¼‰çš„ç›®æ ‡æ˜¯åˆ©ç”¨å°‘é‡æ ‡æ³¨å›¾åƒå¯¹æ–°å‹åŒ»ç–—å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚é’ˆå¯¹åŸºäºåŸå‹çš„æ–¹æ³•åœ¨ç”Ÿæˆå•ä¸€å…¨å±€åŸå‹æ—¶å¿½ç•¥ç±»å†…å˜åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†Self-guided Prototype Enhancement Networkï¼ˆSPENetï¼‰ã€‚è¯¥ç½‘ç»œåŒ…æ‹¬Multi-level Prototype Generationï¼ˆMPGï¼‰æ¨¡å—å’ŒQuery-guided Local Prototype Enhancementï¼ˆQLPEï¼‰æ¨¡å—ï¼Œåˆ†åˆ«å®ç°å¤šç²’åº¦æµ‹é‡å’Œè‡ªé€‚åº”ä¼˜åŒ–æ”¯æŒåŸå‹ã€‚å®éªŒè¯æ˜ï¼ŒSPENetåœ¨ä¸‰ä¸ªå…¬å¼€åŒ»ç–—æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Medical Image Segmentation (FSMIS) æ—¨åœ¨ä½¿ç”¨å°‘é‡æ ‡æ³¨å›¾åƒå¯¹æ–°å‹åŒ»ç–—å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>ç°æœ‰åŸºäºåŸå‹çš„æ–¹æ³•å¿½ç•¥ç±»å†…å˜åŒ–ï¼Œç”Ÿæˆå•ä¸€å…¨å±€åŸå‹ã€‚</li>
<li>Self-guided Prototype Enhancement Network (SPENet) å¼•å…¥Multi-level Prototype Generation (MPG) æ¨¡å—ï¼Œå®ç°å¤šç²’åº¦æµ‹é‡ï¼ŒåŒæ—¶ç”Ÿæˆå…¨å±€å’Œå±€éƒ¨åŸå‹ã€‚</li>
<li>SPENeté€šè¿‡Query-guided Local Prototype Enhancement (QLPE) æ¨¡å—è‡ªé€‚åº”ä¼˜åŒ–æ”¯æŒåŸå‹ï¼Œå‡è½»æ”¯æŒå›¾åƒä¸æŸ¥è¯¢å›¾åƒä¹‹é—´çš„å·®å¼‚å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚</li>
<li>SPENetåœ¨ä¸‰ä¸ªå…¬å¼€åŒ»ç–—æ•°æ®é›†ä¸Šå®ç°ä¼˜è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>å¤šç²’åº¦æµ‹é‡å’Œè‡ªé€‚åº”ä¼˜åŒ–æ˜¯SPENetçš„ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7314b58fae444ebe507db6dc94c2adc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a15f586450735b44153dc7c6a07eaa95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9ad05ab04be4c65e5d7b741a3b94e6a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Advancing-Minority-Stress-Detection-with-Transformers-Insights-from-the-Social-Media-Datasets"><a href="#Advancing-Minority-Stress-Detection-with-Transformers-Insights-from-the-Social-Media-Datasets" class="headerlink" title="Advancing Minority Stress Detection with Transformers: Insights from the   Social Media Datasets"></a>Advancing Minority Stress Detection with Transformers: Insights from the   Social Media Datasets</h2><p><strong>Authors:Santosh Chapagain, Cory J Cascalheira, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi, Jillian R. Scheer</strong></p>
<p>Individuals from sexual and gender minority groups experience disproportionately high rates of poor health outcomes and mental disorders compared to their heterosexual and cisgender counterparts, largely as a consequence of minority stress as described by Meyerâ€™s (2003) model. This study presents the first comprehensive evaluation of transformer-based architectures for detecting minority stress in online discourse. We benchmark multiple transformer models including ELECTRA, BERT, RoBERTa, and BART against traditional machine learning baselines and graph-augmented variants. We further assess zero-shot and few-shot learning paradigms to assess their applicability on underrepresented datasets. Experiments are conducted on the two largest publicly available Reddit corpora for minority stress detection, comprising 12,645 and 5,789 posts, and are repeated over five random seeds to ensure robustness. Our results demonstrate that integrating graph structure consistently improves detection performance across transformer-only models and that supervised fine-tuning with relational context outperforms zero and few-shot approaches. Theoretical analysis reveals that modeling social connectivity and conversational context via graph augmentation sharpens the modelsâ€™ ability to identify key linguistic markers such as identity concealment, internalized stigma, and calls for support, suggesting that graph-enhanced transformers offer the most reliable foundation for digital health interventions and public health policy. </p>
<blockquote>
<p>æ¥è‡ªæ€§å°‘æ•°å’Œæ€§åˆ«å°‘æ•°ç¾¤ä½“çš„ä¸ªä½“åœ¨å¥åº·ç»“æœä¸è‰¯å’Œå¿ƒç†å¥åº·éšœç¢æ–¹é¢çš„æ¯”ä¾‹æ˜æ˜¾é«˜äºå…¶å¼‚æ€§æ‹å’Œé¡ºæ€§åˆ«ç¾¤ä½“ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºMeyerï¼ˆ2003ï¼‰æå‡ºçš„å°‘æ•°ç¾¤ä½“å‹åŠ›æ¨¡å‹æ‰€æè¿°çš„åŸå› ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å…¨é¢è¯„ä¼°äº†åŸºäºè½¬æ¢å™¨çš„æ¶æ„åœ¨æ£€æµ‹åœ¨çº¿å¯¹è¯ä¸­çš„å°‘æ•°ç¾¤ä½“å‹åŠ›æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬ELECTRAã€BERTã€RoBERTaå’ŒBARTåœ¨å†…çš„å¤šç§è½¬æ¢å™¨æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•å’Œå›¾å½¢å¢å¼ºå˜ä½“è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯„ä¼°äº†é›¶æ ·æœ¬å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨ä»£è¡¨æ€§ä¸è¶³çš„æ•°æ®é›†ä¸Šçš„é€‚ç”¨æ€§ã€‚å®éªŒæ˜¯åœ¨ä¸¤ä¸ªç”¨äºå°‘æ•°ç¾¤ä½“å‹åŠ›æ£€æµ‹çš„æœ€å¤§çš„å…¬å¼€å¯ç”¨çš„Redditè¯­æ–™åº“ä¸Šè¿›è¡Œçš„ï¼ŒåŒ…å«12645ä¸ªå’Œ5789ä¸ªå¸–å­ï¼Œå¹¶ä¸”ä¸ºäº†ç¡®ä¿ç¨³å¥æ€§ï¼Œå®éªŒé‡å¤äº†äº”æ¬¡éšæœºç§å­ã€‚ç»“æœè¡¨æ˜ï¼Œæ•´åˆå›¾å½¢ç»“æ„å¯ä»¥æŒç»­æé«˜æ£€æµ‹æ€§èƒ½ï¼Œå¹¶ä¸”ç›‘ç£å¾®è°ƒä¸å…³ç³»ä¸Šä¸‹æ–‡ç›¸æ¯”é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œé€šè¿‡å›¾å½¢å¢å¼ºå»ºæ¨¡ç¤¾ä¼šè¿é€šæ€§å’Œå¯¹è¯ä¸Šä¸‹æ–‡å¯ä»¥åŠ å¼ºæ¨¡å‹è¯†åˆ«å…³é”®è¯­è¨€æ ‡è®°çš„èƒ½åŠ›ï¼Œå¦‚èº«ä»½éšç’ã€å†…åœ¨è€»è¾±æ„Ÿå’Œå¯»æ±‚æ”¯æŒç­‰ï¼Œè¿™è¡¨æ˜å›¾å½¢å¢å¼ºçš„è½¬æ¢å™¨æä¾›äº†æœ€å¯é çš„æ•°å­—å¥åº·å¹²é¢„å’Œå…¬å…±å«ç”Ÿæ”¿ç­–çš„åŸºçŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02908v1">PDF</a> Accepted in Social Network Analysis and Mining Journal (SNAM)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºTransformeræ¶æ„çš„æ¨¡å‹åœ¨æ£€æµ‹ç½‘ç»œè¨€è®ºä¸­å°‘æ•°ç¾¤ä½“å‹åŠ›çš„åº”ç”¨ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å’Œå›¾å½¢å¢å¼ºå˜ä½“è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒåœ¨æœ€å¤§çš„Redditæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœè¡¨æ˜æ•´åˆå›¾å½¢ç»“æ„èƒ½æé«˜æ£€æµ‹æ€§èƒ½ï¼Œç›‘ç£å¾®è°ƒå…³ç³»ä¸Šä¸‹æ–‡ä¼˜äºé›¶å’Œå°‘é•œå¤´æ–¹æ³•ã€‚è¿™è¡¨æ˜å›¾å½¢å¢å¼ºTransformerä¸ºæ•°å­—å¥åº·å¹²é¢„å’Œå…¬å…±å«ç”Ÿæ”¿ç­–æä¾›äº†æœ€å¯é çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ•°ç¾¤ä½“ç»å†æ›´é«˜çš„ä¸è‰¯å¥åº·ç»“æœå’Œå¿ƒç†å¥åº·éšœç¢çš„æ¯”ä¾‹ã€‚</li>
<li>æ­¤ç ”ç©¶é¦–æ¬¡å…¨é¢è¯„ä¼°äº†åŸºäºTransformerçš„æ¶æ„æ¥æ£€æµ‹ç½‘ç»œè¨€è®ºä¸­çš„å°‘æ•°ç¾¤ä½“å‹åŠ›ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†ELECTRAã€BERTã€RoBERTaå’ŒBARTç­‰å¤šä¸ªTransformeræ¨¡å‹ã€‚</li>
<li>å®éªŒåœ¨æœ€å¤§çš„Redditæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œæ¶‰åŠ12,645å’Œ5,789ç¯‡å¸–å­ï¼Œä¸”é‡å¤äº†äº”æ¬¡éšæœºç§å­ä»¥ç¡®ä¿ç¨³å¥æ€§ã€‚</li>
<li>æ•´åˆå›¾å½¢ç»“æ„èƒ½æ˜¾è‘—æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>ç›‘ç£å¾®è°ƒå…³ç³»ä¸Šä¸‹æ–‡çš„æ–¹æ³•ä¼˜äºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f496cf576853b097625413a9100651a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9982b9128f81f8e214d7b2864a1f71c5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RS-OOD-A-Vision-Language-Augmented-Framework-for-Out-of-Distribution-Detection-in-Remote-Sensing"><a href="#RS-OOD-A-Vision-Language-Augmented-Framework-for-Out-of-Distribution-Detection-in-Remote-Sensing" class="headerlink" title="RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution   Detection in Remote Sensing"></a>RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution   Detection in Remote Sensing</h2><p><strong>Authors:Yingrui Ji, Jiansheng Chen, Jingbo Chen, Anzhi Yue, Chenhao Wang, Kai Li, Yao Zhu</strong></p>
<p>Out-of-distribution (OOD) detection represents a critical challenge in remote sensing applications, where reliable identification of novel or anomalous patterns is essential for autonomous monitoring, disaster response, and environmental assessment. Despite remarkable progress in OOD detection for natural images, existing methods and benchmarks remain poorly suited to remote sensing imagery due to data scarcity, complex multi-scale scene structures, and pronounced distribution shifts. To this end, we propose RS-OOD, a novel framework that leverages remote sensing-specific vision-language modeling to enable robust few-shot OOD detection. Our approach introduces three key innovations: spatial feature enhancement that improved scene discrimination, a dual-prompt alignment mechanism that cross-verifies scene context against fine-grained semantics for spatial-semantic consistency, and a confidence-guided self-training loop that dynamically mines pseudo-labels to expand training data without manual annotation. RS-OOD consistently outperforms existing methods across multiple remote sensing benchmarks and enables efficient adaptation with minimal labeled data, demonstrating the critical value of spatial-semantic integration. </p>
<blockquote>
<p>åœ¨é¥æ„Ÿåº”ç”¨ä¸­ï¼Œç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚åœ¨è‡ªä¸»ç›‘æ§ã€ç¾å®³å“åº”å’Œç¯å¢ƒè¯„ä¼°ä¸­ï¼Œå¯é åœ°è¯†åˆ«æ–°é¢–æˆ–å¼‚å¸¸æ¨¡å¼è‡³å…³é‡è¦ã€‚å°½ç®¡åœ¨è‡ªç„¶å›¾åƒä¸­çš„ç¦»ç¾¤åˆ†å¸ƒæ£€æµ‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºæ•°æ®ç¨€ç¼ºã€å¤æ‚çš„å¤šå°ºåº¦åœºæ™¯ç»“æ„å’Œæ˜æ˜¾çš„åˆ†å¸ƒåç§»ï¼Œç°æœ‰æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•å¯¹äºé¥æ„Ÿå›¾åƒä»ä¸å¤ªé€‚åˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RS-OODï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨é¥æ„Ÿç‰¹å®šè§†è§‰è¯­è¨€å»ºæ¨¡çš„æ–°æ¡†æ¶ï¼Œä»¥å®ç°å¥å£®çš„å°‘é•œå¤´ç¦»ç¾¤åˆ†å¸ƒæ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šç©ºé—´ç‰¹å¾å¢å¼ºï¼Œæé«˜åœºæ™¯é‰´åˆ«èƒ½åŠ›ï¼›åŒæç¤ºå¯¹é½æœºåˆ¶ï¼Œé€šè¿‡åœºæ™¯ä¸Šä¸‹æ–‡ä¸ç²¾ç»†ç²’åº¦è¯­ä¹‰çš„äº¤å‰éªŒè¯å®ç°ç©ºé—´è¯­ä¹‰ä¸€è‡´æ€§ï¼›ä»¥åŠç½®ä¿¡åº¦å¼•å¯¼çš„è‡ªè®­ç»ƒå¾ªç¯ï¼ŒåŠ¨æ€æŒ–æ˜ä¼ªæ ‡ç­¾ä»¥æ‰©å±•è®­ç»ƒæ•°æ®è€Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šã€‚RS-OODåœ¨å¤šä¸ªé¥æ„ŸåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å°‘é‡æ ‡æ³¨æ•°æ®ä¸‹å®ç°äº†æœ‰æ•ˆçš„é€‚åº”ï¼Œè¯æ˜äº†ç©ºé—´è¯­ä¹‰é›†æˆçš„å…³é”®ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02273v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿œç¨‹é¥æ„Ÿä¸­å¼‚å¸¸æ£€æµ‹æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹é¥æ„Ÿå½±åƒçš„å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–°æ¡†æ¶RS-OODï¼Œè¯¥æ¡†æ¶å…·æœ‰ä¸‰å¤§åˆ›æ–°ç‚¹ï¼šå¢å¼ºç©ºé—´ç‰¹å¾ä»¥æé«˜åœºæ™¯é‰´åˆ«èƒ½åŠ›ï¼Œé€šè¿‡åŒæç¤ºå¯¹é½æœºåˆ¶è¿›è¡Œåœºæ™¯ä¸Šä¸‹æ–‡ä¸ç²¾ç»†è¯­ä¹‰çš„ç©ºé—´è¯­ä¹‰ä¸€è‡´æ€§æ ¡éªŒï¼Œä»¥åŠé€šè¿‡ç½®ä¿¡åº¦å¼•å¯¼çš„è‡ªè®­ç»ƒå¾ªç¯åŠ¨æ€æŒ–æ˜ä¼ªæ ‡ç­¾æ¥æ‰©å……è®­ç»ƒæ•°æ®ä¸”æ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚è¯¥æ¡†æ¶åœ¨å¤šé¥æ„Ÿæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å°‘é‡æ ‡ç­¾æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿæœ‰è‰¯å¥½é€‚åº”èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨é¥æ„Ÿåº”ç”¨ä¸­ï¼Œæ–°å‹æˆ–å¼‚å¸¸æ¨¡å¼çš„å¯é è¯†åˆ«æ˜¯å…³é”®çš„æŒ‘æˆ˜ï¼Œå¯¹è‡ªåŠ¨ç›‘æ§ã€ç¾å®³å“åº”å’Œç¯å¢ƒè¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>RS-OODæ¡†æ¶é’ˆå¯¹é¥æ„Ÿå½±åƒçš„å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹æå‡ºäº†ä¸‰å¤§åˆ›æ–°ç‚¹ã€‚</li>
<li>ç©ºé—´ç‰¹å¾å¢å¼ºæé«˜äº†åœºæ™¯çš„é‰´åˆ«èƒ½åŠ›ã€‚</li>
<li>åŒæç¤ºå¯¹é½æœºåˆ¶å®ç°äº†åœºæ™¯ä¸Šä¸‹æ–‡ä¸ç²¾ç»†è¯­ä¹‰çš„ç©ºé—´è¯­ä¹‰ä¸€è‡´æ€§æ ¡éªŒã€‚</li>
<li>é€šè¿‡ç½®ä¿¡åº¦å¼•å¯¼çš„è‡ªè®­ç»ƒå¾ªç¯å¯åŠ¨æ€æŒ–æ˜ä¼ªæ ‡ç­¾ä»¥æ‰©å……è®­ç»ƒæ•°æ®ã€‚</li>
<li>RS-OODåœ¨å¤šä¸ªé¥æ„Ÿæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8c04774376edce187f94acb45865ab8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b40ebe706e98e50c192a25c5ce3dfd5c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DroneSR-Rethinking-Few-shot-Thermal-Image-Super-Resolution-from-Drone-based-Perspective"><a href="#DroneSR-Rethinking-Few-shot-Thermal-Image-Super-Resolution-from-Drone-based-Perspective" class="headerlink" title="DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from   Drone-based Perspective"></a>DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from   Drone-based Perspective</h2><p><strong>Authors:Zhipeng Weng, Xiaopeng Liu, Ce Liu, Xingyuan Guo, Yukai Shi, Liang Lin</strong></p>
<p>Although large scale models achieve significant improvements in performance, the overfitting challenge still frequently undermines their generalization ability. In super resolution tasks on images, diffusion models as representatives of generative models typically adopt large scale architectures. However, few-shot drone-captured infrared training data frequently induces severe overfitting in large-scale architectures. To address this key challenge, our method proposes a new Gaussian quantization representation learning method oriented to diffusion models that alleviates overfitting and enhances robustness. At the same time, an effective monitoring mechanism tracks large scale architectures during training to detect signs of overfitting. By introducing Gaussian quantization representation learning, our method effectively reduces overfitting while maintaining architecture complexity. On this basis, we construct a multi source drone-based infrared image benchmark dataset for detection and use it to emphasize overfitting issues of large scale architectures in few sample, drone-based diverse drone-based image reconstruction scenarios. To verify the efficacy of the method in mitigating overfitting, experiments are conducted on the constructed benchmark. Experimental results demonstrate that our method outperforms existing super resolution approaches and significantly mitigates overfitting of large scale architectures under complex conditions. The code and DroneSR dataset will be available at: <a target="_blank" rel="noopener" href="https://github.com/wengzp1/GARLSR">https://github.com/wengzp1/GARLSR</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§è§„æ¨¡æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä½†è¿‡æ‹ŸåˆæŒ‘æˆ˜ä»ç„¶ç»å¸¸ç ´åå…¶æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­ï¼Œä»¥æ‰©æ•£æ¨¡å‹ä¸ºä»£è¡¨çš„ç”Ÿæˆæ¨¡å‹é€šå¸¸é‡‡ç”¨å¤§è§„æ¨¡æ¶æ„ã€‚ç„¶è€Œï¼Œåœ¨æ— äººæœºæ•è·çš„çº¢å¤–è®­ç»ƒæ•°æ®ä¸­ï¼Œå°‘é‡æ•°æ®å¸¸å¸¸å¯¼è‡´å¤§è§„æ¨¡æ¶æ„å‡ºç°è¿‡æ‹Ÿåˆç°è±¡ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æå‡ºäº†ä¸€ç§é¢å‘æ‰©æ•£æ¨¡å‹çš„æ–°å‹é«˜æ–¯é‡åŒ–è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç¼“è§£è¿‡æ‹Ÿåˆç°è±¡å¹¶å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚åŒæ—¶ï¼Œæœ‰æ•ˆçš„ç›‘æ§æœºåˆ¶å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·Ÿè¸ªå¤§è§„æ¨¡æ¶æ„ï¼Œä»¥æ£€æµ‹è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚é€šè¿‡å¼•å…¥é«˜æ–¯é‡åŒ–è¡¨ç¤ºå­¦ä¹ ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒæ¶æ„å¤æ‚æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†è¿‡æ‹Ÿåˆç°è±¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºäºå¤šæºæ— äººæœºçš„çº¢å¤–å›¾åƒåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºæ£€æµ‹ï¼Œå¹¶å¼ºè°ƒåœ¨å°‘é‡æ ·æœ¬ã€åŸºäºæ— äººæœºçš„å¤šæ ·åŒ–æ— äººæœºå›¾åƒé‡å»ºåœºæ™¯ä¸­å¤§è§„æ¨¡æ¶æ„çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ä¸ºäº†éªŒè¯è¯¥æ–¹æ³•åœ¨ç¼“è§£è¿‡æ‹Ÿåˆæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨æ„å»ºçš„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œå¹¶åœ¨å¤æ‚æ¡ä»¶ä¸‹æ˜¾è‘—ç¼“è§£äº†å¤§è§„æ¨¡æ¶æ„çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ä»£ç å’Œæ•°æ®é›†å°†å…¬å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/wengzp1/GARLSR%E3%80%82">https://github.com/wengzp1/GARLSRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01898v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§æ¨¡å‹è™½ç„¶åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œä½†è¿‡æ‹Ÿåˆé—®é¢˜ä»ç„¶é¢‘ç¹åœ°å½±å“äº†å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹è¶…åˆ†è¾¨ç‡å›¾åƒä»»åŠ¡ä¸­ï¼Œå¤§å‹æ¶æ„åœ¨æ— äººæœºæ•è·çš„çº¢å¤–è®­ç»ƒæ•°æ®ä¸Šç»å¸¸å‡ºç°ä¸¥é‡è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘æ‰©æ•£æ¨¡å‹çš„é«˜æ–¯é‡åŒ–è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä»¥å‡è½»è¿‡æ‹Ÿåˆå¹¶å¢å¼ºç¨³å¥æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡æœ‰æ•ˆçš„ç›‘æ§æœºåˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·Ÿè¸ªå¤§å‹æ¶æ„ï¼Œä»¥æ£€æµ‹è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚å¼•å…¥é«˜æ–¯é‡åŒ–è¡¨ç¤ºå­¦ä¹ åï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ä¿æŒæ¶æ„å¤æ‚æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†è¿‡æ‹Ÿåˆã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç”¨äºæ£€æµ‹çš„å¤šæºæ— äººæœºçº¢å¤–å›¾åƒåŸºå‡†æ•°æ®é›†ï¼Œå¹¶å¼ºè°ƒåœ¨å°‘æ•°æ ·æœ¬ã€åŸºäºæ— äººæœºçš„å¤šæ ·åŒ–å›¾åƒé‡å»ºåœºæ™¯ä¸­å¤§å‹æ¶æ„çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡è½»è¿‡æ‹Ÿåˆæ–¹é¢ä¼˜äºç°æœ‰çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œå¹¶åœ¨å¤æ‚æ¡ä»¶ä¸‹æ˜¾è‘—å‡è½»äº†å¤§å‹æ¶æ„çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨¡å‹åœ¨æ€§èƒ½æå‡çš„åŒæ—¶é¢ä¸´è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå½±å“å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„ä»£è¡¨ï¼Œåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­å¸¸é‡‡ç”¨å¤§å‹æ¶æ„ã€‚</li>
<li>æ— äººæœºæ•è·çš„çº¢å¤–è®­ç»ƒæ•°æ®å¯¹å¤§å‹æ¶æ„ç»å¸¸å¯¼è‡´ä¸¥é‡çš„è¿‡æ‹Ÿåˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯é‡åŒ–è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œé¢å‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥å‡è½»è¿‡æ‹Ÿåˆå¹¶å¢å¼ºç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥æœ‰æ•ˆç›‘æ§æœºåˆ¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·Ÿè¸ªå¤§å‹æ¶æ„ï¼Œä»¥æ£€æµ‹è¿‡æ‹Ÿåˆè¿¹è±¡ã€‚</li>
<li>é«˜æ–¯é‡åŒ–è¡¨ç¤ºå­¦ä¹ æ–¹æ³•åœ¨å‡å°‘è¿‡æ‹Ÿåˆçš„åŒæ—¶ä¿æŒäº†æ¶æ„çš„å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6967de0026cfc3866db2656f597604cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b5279beb21342041c6028560656f9b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39aba19ae0c8fc1c6a66420102f15b72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a2aa7d3e418ccc93b3986ff85d6ad9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe7ab5324116328929562ccfcdcfbf45.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Cross-Domain-Few-Shot-Segmentation-via-Ordinary-Differential-Equations-over-Time-Intervals"><a href="#Cross-Domain-Few-Shot-Segmentation-via-Ordinary-Differential-Equations-over-Time-Intervals" class="headerlink" title="Cross-Domain Few-Shot Segmentation via Ordinary Differential Equations   over Time Intervals"></a>Cross-Domain Few-Shot Segmentation via Ordinary Differential Equations   over Time Intervals</h2><p><strong>Authors:Huan Ni, Qingshan Liu, Xiaonan Niu, Danfeng Hong, Lingli Zhao, Haiyan Guan</strong></p>
<p>Cross-domain few-shot segmentation (CD-FSS) not only enables the segmentation of unseen categories with very limited samples, but also improves cross-domain generalization ability within the few-shot segmentation framework. Currently, existing CD-FSS studies typically design multiple independent modules to enhance the cross-domain generalization ability of feature representations. However, the independence among these modules hinders the effective flow of knowledge, making it difficult to fully leverage their collective potential. In contrast, this paper proposes an all-in-one module based on ordinary differential equations and Fourier transform, resulting in a structurally concise methodâ€“Few-Shot Segmentation over Time Intervals (FSS-TIs). FSS-TIs assumes the existence of an ODE relationship between the spectra (including amplitude and phase spectra) of domain-specific features and domain-agnostic features. This ODE formulation yields an iterative transformation process along a sequence of time intervals, while simultaneously applying affine transformations with randomized perturbations to the spectra. In doing so, the exploration of domain-agnostic feature representation spaces and the simulation of diverse potential target-domain distributions are reformulated as an optimization process over the intrinsic parameters of the ODE. Moreover, we strictly constrain the support-sample selection during target-domain fine-tuning so that it is consistent with the requirements of real-world few-shot segmentation tasks. For evaluation, we introduce five datasets from substantially different domains and define two sets of cross-domain few-shot segmentation tasks to comprehensively analyze the performance of FSS-TIs. Experimental results demonstrate the superiority of FSS-TIs over existing CD-FSS methods, and in-depth ablation studies further validate the cross-domain adaptability of FSS-TIs. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰ä¸ä»…èƒ½å¤Ÿåœ¨éå¸¸æœ‰é™çš„æ ·æœ¬ä¸‹å®ç°å¯¹æœªè§ç±»åˆ«çš„åˆ†å‰²ï¼Œè€Œä¸”è¿˜æé«˜äº†å°æ ·æœ¬åˆ†å‰²æ¡†æ¶å†…çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚ç›®å‰ï¼Œç°æœ‰çš„CD-FSSç ”ç©¶é€šå¸¸è®¾è®¡å¤šä¸ªç‹¬ç«‹æ¨¡å—æ¥å¢å¼ºç‰¹å¾è¡¨ç¤ºçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å—çš„ç‹¬ç«‹æ€§é˜»ç¢äº†çŸ¥è¯†çš„æœ‰æ•ˆæµåŠ¨ï¼Œä½¿å¾—éš¾ä»¥å……åˆ†åˆ©ç”¨å®ƒä»¬çš„é›†ä½“æ½œåŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹å’Œå‚…é‡Œå¶å˜æ¢çš„å…¨æ–¹ä½æ¨¡å—ï¼Œä»è€Œå¾—åˆ°äº†ä¸€ç§ç»“æ„ç®€æ´çš„æ–¹æ³•â€”â€”åŸºäºæ—¶é—´é—´éš”çš„å°æ ·æœ¬åˆ†å‰²ï¼ˆFSS-TIsï¼‰ã€‚FSS-TIså‡è®¾ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾å…‰è°±ï¼ˆåŒ…æ‹¬æŒ¯å¹…å’Œç›¸ä½å…‰è°±ï¼‰ä¸é¢†åŸŸé€šç”¨çš„ç‰¹å¾ä¹‹é—´å­˜åœ¨å¸¸å¾®åˆ†æ–¹ç¨‹å…³ç³»ã€‚è¯¥å¸¸å¾®åˆ†æ–¹ç¨‹çš„å…¬å¼åŒ–äº§ç”Ÿäº†ä¸€ä¸ªæ²¿æ—¶é—´é—´éš”åºåˆ—çš„è¿­ä»£è½¬æ¢è¿‡ç¨‹ï¼ŒåŒæ—¶å…‰è°±åº”ç”¨äº†å¸¦æœ‰éšæœºæ‰°åŠ¨çš„ä»¿å°„å˜æ¢ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œé¢†åŸŸé€šç”¨çš„ç‰¹å¾è¡¨ç¤ºç©ºé—´çš„æ¢ç´¢å’Œå„ç§æ½œåœ¨ç›®æ ‡åŸŸåˆ†å¸ƒçš„æ¨¡æ‹Ÿè¢«é‡æ–°è¡¨è¿°ä¸ºå¸¸å¾®åˆ†æ–¹ç¨‹çš„å†…åœ¨å‚æ•°çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸¥æ ¼é™åˆ¶äº†ç›®æ ‡åŸŸå¾®è°ƒè¿‡ç¨‹ä¸­çš„æ”¯æŒæ ·æœ¬é€‰æ‹©ï¼Œä½¿å…¶ç¬¦åˆç°å®ä¸–ç•Œä¸­å°‘é‡æ ·æœ¬åˆ†å‰²ä»»åŠ¡çš„è¦æ±‚ã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œæˆ‘ä»¬ä»ä¸åŒé¢†åŸŸå¼•å…¥äº†äº”ä¸ªæ•°æ®é›†ï¼Œå¹¶å®šä¹‰äº†ä¸¤å¥—è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ä»»åŠ¡æ¥å…¨é¢åˆ†æFSS-TIsçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº†FSS-TIsç›¸è¾ƒäºç°æœ‰CD-FSSæ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œæ·±å…¥çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†FSS-TIsçš„è·¨åŸŸé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01299v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰çš„æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•å¤šé‡‡ç”¨å¤šä¸ªç‹¬ç«‹æ¨¡å—æå‡è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨çŸ¥è¯†æµé€šä¸ç•…çš„é—®é¢˜ã€‚é’ˆå¯¹æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹å’Œå‚…é‡Œå¶å˜æ¢çš„å…¨åˆä¸€æ¨¡å—æ–¹æ³•â€”â€”FSS-TIsã€‚è¯¥æ–¹æ³•é€šè¿‡å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰å»ºæ¨¡åŸŸç‰¹å®šç‰¹å¾ä¸åŸŸé€šç”¨ç‰¹å¾ä¹‹é—´çš„å…‰è°±å…³ç³»ï¼Œå¹¶æ²¿æ—¶é—´åºåˆ—è¿›è¡Œè¿­ä»£è½¬æ¢ã€‚å®éªŒè¯æ˜ï¼ŒFSS-TIsåœ¨è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSSé¢ä¸´é€šè¿‡æœ‰é™æ ·æœ¬è¿›è¡Œæœªè§ç±»åˆ«åˆ†å‰²ä»¥åŠæå‡è·¨åŸŸæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»ŸCD-FSSç ”ç©¶é€šè¿‡è®¾è®¡å¤šä¸ªç‹¬ç«‹æ¨¡å—å¢å¼ºè·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä½†å­˜åœ¨çŸ¥è¯†æµé€šä¸ç•…çš„é—®é¢˜ã€‚</li>
<li>FSS-TIsæ–¹æ³•åŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹å’Œå‚…é‡Œå¶å˜æ¢æå‡ºå…¨åˆä¸€æ¨¡å—ï¼Œä½¿æ–¹æ³•æ›´ç®€æ´æœ‰æ•ˆã€‚</li>
<li>FSS-TIså‡è®¾åŸŸç‰¹å®šç‰¹å¾å’ŒåŸŸé€šç”¨ç‰¹å¾ä¹‹é—´å­˜åœ¨ODEå…³ç³»ï¼Œå¹¶é€šè¿‡è¿­ä»£è½¬æ¢æ²¿æ—¶é—´åºåˆ—ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>ä¸¥æ ¼çº¦æŸç›®æ ‡åŸŸå¾®è°ƒæ—¶çš„æ”¯æŒæ ·æœ¬é€‰æ‹©ï¼Œä»¥ç¬¦åˆçœŸå®ä¸–ç•Œå°æ ·æœ¬åˆ†å‰²ä»»åŠ¡çš„è¦æ±‚ã€‚</li>
<li>åœ¨äº”ä¸ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®šä¹‰äº†ä¸¤å¥—è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ä»»åŠ¡ï¼Œè¯æ˜äº†FSS-TIsçš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2d5285ae779e90d4cc46594acb0d987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beac8e9981c3e633aa18b7f3114b4e80.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Analysis-of-Error-Sources-in-LLM-based-Hypothesis-Search-for-Few-Shot-Rule-Induction"><a href="#Analysis-of-Error-Sources-in-LLM-based-Hypothesis-Search-for-Few-Shot-Rule-Induction" class="headerlink" title="Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot   Rule Induction"></a>Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot   Rule Induction</h2><p><strong>Authors:Aishni Parab, Hongjing Lu, Ying Nian Wu, Sumit Gulwani</strong></p>
<p>Inductive reasoning enables humans to infer abstract rules from limited examples and apply them to novel situations. In this work, we compare an LLM-based hypothesis search framework with direct program generation approaches on few-shot rule induction tasks. Our findings show that hypothesis search achieves performance comparable to humans, while direct program generation falls notably behind. An error analysis reveals key bottlenecks in hypothesis generation and suggests directions for advancing program induction methods. Overall, this paper underscores the potential of LLM-based hypothesis search for modeling inductive reasoning and the challenges in building more efficient systems. </p>
<blockquote>
<p>å½’çº³æ¨ç†ä½¿äººç±»èƒ½å¤Ÿä»æœ‰é™çš„ä¾‹å­ä¸­æ¨æ–­å‡ºæŠ½è±¡è§„åˆ™ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°æ–°æƒ…å¢ƒä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºLLMçš„å‡è®¾æœç´¢æ¡†æ¶å’Œç›´æ¥ç¨‹åºç”Ÿæˆæ–¹æ³•åœ¨å°‘é‡å½’çº³æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå‡è®¾æœç´¢çš„è¡¨ç°ä¸äººç±»ç›¸å½“ï¼Œè€Œç›´æ¥ç¨‹åºç”Ÿæˆçš„è¡¨ç°åˆ™æ˜æ˜¾è½åã€‚é€šè¿‡é”™è¯¯åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†å‡è®¾ç”Ÿæˆçš„å…³é”®ç“¶é¢ˆï¼Œå¹¶ä¸ºæ¨è¿›ç¨‹åºå½’çº³æ–¹æ³•æä¾›äº†æ–¹å‘ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™ç¯‡è®ºæ–‡å¼ºè°ƒäº†åŸºäºLLMçš„å‡è®¾æœç´¢åœ¨æ¨¡æ‹Ÿå½’çº³æ¨ç†æ–¹é¢çš„æ½œåŠ›ï¼Œä»¥åŠæ„å»ºæ›´é«˜æ•ˆç³»ç»Ÿçš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01016v1">PDF</a> This is the preprint version corresponding to our NeurIPS 2025   Workshop on Multimodal Algorithmic Reasoning submission</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºLLMçš„å‡è®¾æœç´¢æ¡†æ¶ä¸ç›´æ¥ç¨‹åºç”Ÿæˆæ–¹æ³•åœ¨å°‘æ ·æœ¬è§„åˆ™å½’çº³ä»»åŠ¡ä¸­çš„æ¯”è¾ƒã€‚ç ”ç©¶å‘ç°ï¼Œå‡è®¾æœç´¢çš„æ€§èƒ½ä¸äººç±»ç›¸å½“ï¼Œè€Œç›´æ¥ç¨‹åºç”Ÿæˆåˆ™æ˜æ˜¾è½åã€‚è¯¯å·®åˆ†ææ­ç¤ºäº†å‡è®¾ç”Ÿæˆçš„å…³é”®ç“¶é¢ˆï¼Œä¸ºæ¨è¿›ç¨‹åºå½’çº³æ–¹æ³•æä¾›äº†æ–¹å‘ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡çªå‡ºäº†LLMå‡è®¾æœç´¢åœ¨æ¨¡æ‹Ÿå½’çº³æ¨ç†æ–¹é¢çš„æ½œåŠ›ä»¥åŠæ„å»ºæ›´é«˜æ•ˆç³»ç»Ÿæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½’çº³æ¨ç†èƒ½ä»æœ‰é™ä¾‹å­ä¸­æ¨æ–­å‡ºæŠ½è±¡è§„åˆ™å¹¶åº”ç”¨äºæ–°æƒ…å¢ƒã€‚</li>
<li>LLM-basedå‡è®¾æœç´¢æ¡†æ¶åœ¨å°‘æ ·æœ¬è§„åˆ™å½’çº³ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸äººç±»ç›¸å½“ã€‚</li>
<li>ç›´æ¥ç¨‹åºç”Ÿæˆæ–¹æ³•åœ¨å°‘æ ·æœ¬è§„åˆ™å½’çº³ä»»åŠ¡ä¸­è¡¨ç°è¾ƒå·®ã€‚</li>
<li>è¯¯å·®åˆ†ææ­ç¤ºäº†å‡è®¾ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…³é”®ç“¶é¢ˆã€‚</li>
<li>LLM-basedå‡è®¾æœç´¢å…·æœ‰å»ºæ¨¡å½’çº³æ¨ç†çš„æ½œåŠ›ã€‚</li>
<li>æ„å»ºæ›´é«˜æ•ˆçš„ç³»ç»Ÿæ˜¯å½“å‰çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01016">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49180707cc2963078c07f63162a87ed3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef567eb08520db5442e1a878f3c92ae8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b75474e520dfd3421658eb281d0234a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Spotlighter-Revisiting-Prompt-Tuning-from-a-Representative-Mining-View"><a href="#Spotlighter-Revisiting-Prompt-Tuning-from-a-Representative-Mining-View" class="headerlink" title="Spotlighter: Revisiting Prompt Tuning from a Representative Mining View"></a>Spotlighter: Revisiting Prompt Tuning from a Representative Mining View</h2><p><strong>Authors:Yutong Gao, Maoyuan Shao, Xinyang Huang, Chuang Zhu, Lijuan Sun, Yu Weng, Xuan Liu, Guoshun Nan</strong></p>
<p>CLIPâ€™s success has demonstrated that prompt tuning can achieve robust cross-modal semantic alignment for tasks ranging from open-domain recognition to fine-grained classification. However, redundant or weakly relevant feature components introduce noise and incur unnecessary computational costs. In this work, we propose Spotlighter, a lightweight token-selection framework that simultaneously enhances accuracy and efficiency in prompt tuning. Spotlighter evaluates each visual tokenâ€™s activation from both sample-wise and semantic-wise perspectives and retains only the top-scoring tokens for downstream prediction. A class-specific semantic memory bank of learned prototypes refines this selection, ensuring semantic representativeness and compensating for discarded features. To further prioritize informative signals, we introduce a two-level ranking mechanism that dynamically weights tokenâ€“prototype interactions. Across 11 few-shot benchmarks, Spotlighter outperforms CLIP by up to 11.19% in harmonic mean accuracy and achieves up to 0.8K additional FPS, with only 21 extra parameters. These results establish Spotlighter as an effective and scalable baseline for prompt tuning. Code for our method will be available at <a target="_blank" rel="noopener" href="https://github.com/greatest-gourmet/Spotlighter">https://github.com/greatest-gourmet/Spotlighter</a>. </p>
<blockquote>
<p>CLIPçš„æˆåŠŸè¡¨æ˜ï¼Œæç¤ºè°ƒæ•´å¯ä»¥å®ç°ä»å¼€æ”¾åŸŸè¯†åˆ«åˆ°ç²¾ç»†åˆ†ç±»ä»»åŠ¡çš„ç¨³å¥è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ã€‚ç„¶è€Œï¼Œå†—ä½™æˆ–å¼±ç›¸å…³çš„ç‰¹å¾æˆåˆ†ä¼šå¼•å…¥å™ªå£°å¹¶äº§ç”Ÿä¸å¿…è¦çš„è®¡ç®—æˆæœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Spotlighterï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ä»¤ç‰Œé€‰æ‹©æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚Spotlighterä»æ ·æœ¬å’Œè¯­ä¹‰ä¸¤ä¸ªè§’åº¦è¯„ä¼°æ¯ä¸ªè§†è§‰ä»¤ç‰Œçš„æ¿€æ´»ï¼Œä»…ä¿ç•™å¾—åˆ†æœ€é«˜çš„ä»¤ç‰Œè¿›è¡Œä¸‹æ¸¸é¢„æµ‹ã€‚ä¸€ä¸ªç‰¹å®šç±»åˆ«çš„è¯­ä¹‰è®°å¿†åº“ä¸­çš„å­¦ä¹ åŸå‹å¯ä»¥ä¼˜åŒ–æ­¤é€‰æ‹©ï¼Œç¡®ä¿è¯­ä¹‰ä»£è¡¨æ€§å¹¶è¡¥å¿ä¸¢å¼ƒçš„ç‰¹å¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–ä¿¡æ¯ä¿¡å·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸¤çº§æ’åæœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥åŠ¨æ€åœ°æƒè¡¡ä»¤ç‰Œä¸åŸå‹ä¹‹é—´çš„äº¤äº’ã€‚åœ¨11ä¸ªå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSpotlighterçš„è°ƒå’Œå¹³å‡å‡†ç¡®ç‡æ¯”CLIPé«˜å‡ºé«˜è¾¾11.19%ï¼Œå¹¶ä¸”å®ç°äº†é«˜è¾¾0.8Kçš„é¢å¤–FPSï¼Œä»…æœ‰é¢å¤–çš„21ä¸ªå‚æ•°ã€‚è¿™äº›ç»“æœè¯æ˜äº†Spotlighteråœ¨æç¤ºè°ƒæ•´æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ç›¸å…³ä»£ç å°†å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/greatest-gourmet/Spotlighter%E4%B8%8A%E3%80%82">https://github.com/greatest-gourmet/Spotlighterä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00905v2">PDF</a> Accepted as EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CLIPåœ¨è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨ï¼Œä½†ä¹ŸæŒ‡å‡ºäº†å†—ä½™å’Œå¼±ç›¸å…³ç‰¹å¾æ‰€å¸¦æ¥çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†Spotlighteræ¡†æ¶ï¼Œå®ƒé€šè¿‡é€‰æ‹©å…³é”®çš„è§†è§‰ä»¤ç‰Œè¿›è¡Œå¾®è°ƒæ¥æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚Spotlighterç»“åˆäº†æ ·æœ¬å’Œè¯­ä¹‰ä¸¤ä¸ªè§’åº¦å¯¹è§†è§‰ä»¤ç‰Œè¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¿ç•™å¾—åˆ†æœ€é«˜çš„ä»¤ç‰Œç”¨äºä¸‹æ¸¸é¢„æµ‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ç±»ç‰¹å®šçš„è¯­ä¹‰è®°å¿†åº“å’Œå­¦ä¹ åŸå‹æ¥å®Œå–„ä»¤ç‰Œé€‰æ‹©ï¼Œä»¥ç¡®ä¿è¯­ä¹‰ä»£è¡¨æ€§å’Œå¼¥è¡¥ä¸¢å¤±çš„ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpotlighteråœ¨å¤šä¸ªå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºCLIPï¼Œä¸”å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPçš„æˆåŠŸè¯æ˜äº†æç¤ºè°ƒæ•´åœ¨è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å†—ä½™å’Œå¼±ç›¸å…³ç‰¹å¾å¯èƒ½å¯¼è‡´å™ªå£°å’Œä¸å¿…è¦çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>Spotlighteræ¡†æ¶æ—¨åœ¨æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œé€šè¿‡é€‰æ‹©å…³é”®çš„è§†è§‰ä»¤ç‰Œè¿›è¡Œå¾®è°ƒã€‚</li>
<li>Spotlighterç»“åˆæ ·æœ¬å’Œè¯­ä¹‰è§’åº¦è¯„ä¼°è§†è§‰ä»¤ç‰Œï¼Œå¹¶ä¿ç•™å¾—åˆ†æœ€é«˜çš„ä»¤ç‰Œã€‚</li>
<li>ç±»ç‰¹å®šçš„è¯­ä¹‰è®°å¿†åº“å’Œå­¦ä¹ åŸå‹ç”¨äºå®Œå–„ä»¤ç‰Œé€‰æ‹©ï¼Œç¡®ä¿è¯­ä¹‰ä»£è¡¨æ€§ã€‚</li>
<li>Spotlighteråœ¨å¤šä¸ªå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºCLIPã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0e94e6ebef072db80b7a56d084e6538f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd12396e0d27d85a31eb54b6fa197fce.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Language-Aware-Information-Maximization-for-Transductive-Few-Shot-CLIP"><a href="#Language-Aware-Information-Maximization-for-Transductive-Few-Shot-CLIP" class="headerlink" title="Language-Aware Information Maximization for Transductive Few-Shot CLIP"></a>Language-Aware Information Maximization for Transductive Few-Shot CLIP</h2><p><strong>Authors:Ghassen Baklouti, Maxime Zanella, Ismail Ben Ayed</strong></p>
<p>Transductive few-shot learning has triggered an abundant literature focusing on vision-only models, but is still at a nascent stage within the recent context of foundational vision-language models (VLMs). Only a few recent methods addressed the problem, pointing to the potential of tranduction in VLMs and to the need for VLM-tailored methods. Building on this momentum, we leverage information-theoretic concepts and recent progress in parameter-efficient fine-tuning (PEFT), developing a highly competitive transductive few-shot CLIP method. Specifically, we introduce a novel Language-aware Information MaximizatiOn (LIMO) loss integrating three complementary terms: (i) the mutual information between the vision inputs and the textual class descriptions; (ii) a Kullback-Leibler (KL) divergence penalizing deviation of the networkâ€™s probabilistic outputs from the text-driven zero-shot predictions; and (iii) a standard cross-entropy loss based on the labeled shots. Furthermore, we challenge the commonly followed fine-tuning practices in the context of transductive few-shot learning, and explore PEFT strategies, completely overlooked in this context. Surprisingly, we observe substantial boosts in performances, which points to the potential of adapting a subset of the modelâ€™s parameters in the transductive few-shot setting. We report comprehensive evaluations, which show that LIMO outperforms the very recent transductive few-shot CLIP methods by a large margin and yields significant gains over the best-performing inductive methods. Our code is publicly available at:[ \href{<a target="_blank" rel="noopener" href="https://github.com/ghassenbaklouti/LIMO%7D%7B/text%7Bhere%7D%7D">https://github.com/ghassenbaklouti/LIMO}{\text{here}}</a> ] </p>
<blockquote>
<p>è½¬å¯¼å¼å°æ ·æœ¬å­¦ä¹ å·²ç»å¼•å‘äº†å¤§é‡å…³äºä»…è§†è§‰æ¨¡å‹çš„æ–‡çŒ®ï¼Œä½†åœ¨åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æœ€è¿‘èƒŒæ™¯ä¸‹ï¼Œå®ƒä»å¤„äºèµ·æ­¥é˜¶æ®µã€‚åªæœ‰å°‘æ•°è¿‘æœŸæ–¹æ³•è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼ŒæŒ‡å‡ºäº†VLMsä¸­çš„ç¿»è¯‘æ½œåŠ›ä»¥åŠé’ˆå¯¹VLMé‡èº«å®šåˆ¶çš„æ–¹æ³•çš„éœ€æ±‚ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬åˆ©ç”¨ä¿¡æ¯è®ºçš„æ¦‚å¿µå’Œå‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰çš„æœ€æ–°è¿›å±•ï¼Œå¼€å‘äº†ä¸€ç§æå…·ç«äº‰åŠ›çš„è½¬å¯¼å¼å°æ ·æœ¬CLIPæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è¯­è¨€æ„ŸçŸ¥ä¿¡æ¯æœ€å¤§åŒ–ï¼ˆLIMOï¼‰æŸå¤±ï¼Œèåˆäº†ä¸‰é¡¹äº’è¡¥çš„æ¡æ¬¾ï¼šï¼ˆiï¼‰è§†è§‰è¾“å…¥å’Œæ–‡æœ¬ç±»åˆ«æè¿°ä¹‹é—´çš„äº’ä¿¡æ¯ï¼›ï¼ˆiiï¼‰Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦æƒ©ç½šç½‘ç»œæ¦‚ç‡è¾“å‡ºä¸æ–‡æœ¬é©±åŠ¨é›¶æ ·æœ¬é¢„æµ‹ä¹‹é—´çš„åå·®ï¼›ï¼ˆiiiï¼‰åŸºäºæœ‰æ ‡ç­¾æ ·æœ¬çš„æ ‡å‡†äº¤å‰ç†µæŸå¤±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è´¨ç–‘è½¬å¯¼å¼å°æ ·æœ¬å­¦ä¹ èƒŒæ™¯ä¸‹é€šå¸¸é‡‡ç”¨çš„å¾®è°ƒå®è·µï¼Œå¹¶æ¢ç´¢åœ¨æ­¤èƒŒæ™¯ä¸‹å®Œå…¨è¢«å¿½è§†çš„PEFTç­–ç•¥ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ€§èƒ½çš„å¤§å¹…æå‡ï¼Œè¿™è¡¨æ˜åœ¨è½¬å¯¼å¼å°æ ·æœ¬è®¾ç½®ä¸­é€‚åº”æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°å…·æœ‰æ½œåŠ›ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†å…¨é¢çš„è¯„ä¼°ï¼Œè¡¨æ˜LIMOåœ¨æœ€è¿‘çš„è½¬å¯¼å¼å°æ ·æœ¬CLIPæ–¹æ³•ä¸Šå…·æœ‰å¾ˆå¤§ä¼˜åŠ¿ï¼Œå¹¶ä¸”åœ¨æœ€ä½³æ€§èƒ½çš„å½’çº³æ–¹æ³•ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æ”¶ç›Šã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[\url{<a target="_blank" rel="noopener" href="https://github.com/ghassenbaklouti/LIMO%7D%7B%E6%AD%A4%E5%A4%84%7D]%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ghassenbaklouti/LIMO}{æ­¤å¤„}]å…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00305v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åŸºäºä¿¡æ¯ç†è®ºæ¦‚å¿µçš„è½¬æ¢å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•çš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è½¬æ¢å°‘æ ·æœ¬å­¦ä¹ é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€æ„ŸçŸ¥ä¿¡æ¯æœ€å¤§åŒ–ï¼ˆLIMOï¼‰æŸå¤±å‡½æ•°ï¼Œå¹¶æŒ‘æˆ˜äº†ç°æœ‰çš„å¾®è°ƒå®è·µã€‚è¯¥ç ”ç©¶è§‚å¯Ÿåˆ°ï¼Œé€šè¿‡è°ƒæ•´éƒ¨åˆ†æ¨¡å‹å‚æ•°è¿›è¡Œè½¬æ¢å°‘æ ·æœ¬è®¾ç½®æ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—æå‡ã€‚è¯¥ç ”ç©¶å…¬å¼€çš„ä»£ç å¯ä¾›è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è½¬æ¢å°‘æ ·æœ¬å­¦ä¹ è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€æ„ŸçŸ¥ä¿¡æ¯æœ€å¤§åŒ–ï¼ˆLIMOï¼‰æŸå¤±å‡½æ•°ï¼Œç»“åˆäº†ä¸‰ç§äº’è¡¥çš„æœ¯è¯­ã€‚</li>
<li>æŒ‘æˆ˜äº†ç°æœ‰çš„å¾®è°ƒå®è·µï¼Œæ¢ç´¢äº†å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰ç­–ç•¥åœ¨è½¬æ¢å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚</li>
<li>é€šè¿‡è°ƒæ•´æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°ï¼Œè§‚å¯Ÿåˆ°æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
<li>LIMOæŸå¤±å‡½æ•°åœ¨æœ€è¿‘çš„è½¬æ¢å°‘æ ·æœ¬CLIPæ–¹æ³•ä¸Šå–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</li>
<li>ä¸è¡¨ç°æœ€ä½³çš„éè½¬æ¢æ–¹æ³•ç›¸æ¯”ï¼ŒLIMOä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00305">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-254ba9baeb346c9b24cc632b38c7f137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a2d81b616e43dc6759f858a6753f9b9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Summarize-Exemplify-Reflect-Data-driven-Insight-Distillation-Empowers-LLMs-for-Few-shot-Tabular-Classification"><a href="#Summarize-Exemplify-Reflect-Data-driven-Insight-Distillation-Empowers-LLMs-for-Few-shot-Tabular-Classification" class="headerlink" title="Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers   LLMs for Few-shot Tabular Classification"></a>Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers   LLMs for Few-shot Tabular Classification</h2><p><strong>Authors:Yifei Yuan, Jiatong Li, Weijia Zhang, Mohammad Aliannejadi, Evangelos Kanoulas, Renjun Hu</strong></p>
<p>Recent studies show the promise of large language models (LLMs) for few-shot tabular classification but highlight challenges due to the variability in structured data. To address this, we propose distilling data into actionable insights to enable robust and effective classification by LLMs. Drawing inspiration from human learning processes, we introduce InsightTab, an insight distillation framework guided by principles of divide-and-conquer, easy-first, and reflective learning. Our approach integrates rule summarization, strategic exemplification, and insight reflection through deep collaboration between LLMs and data modeling techniques. The obtained insights enable LLMs to better align their general knowledge and capabilities with the particular requirements of specific tabular tasks. We extensively evaluate InsightTab on nine datasets. The results demonstrate consistent improvement over state-of-the-art methods. Ablation studies further validate the principle-guided distillation process, while analyses emphasize InsightTabâ€™s effectiveness in leveraging labeled data and managing bias. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶æ˜¾ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°‘é‡è¡¨æ ¼åˆ†ç±»æ–¹é¢çš„æ½œåŠ›ï¼Œä½†åŒæ—¶ä¹Ÿå¼ºè°ƒäº†ç”±äºç»“æ„åŒ–æ•°æ®å˜åŒ–å¤šæ ·æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå°†æ•°æ®æç‚¼æˆå¯æ“ä½œçš„è§è§£ï¼Œä»¥å®ç°LLMçš„ç¨³å¥å’Œæœ‰æ•ˆçš„åˆ†ç±»ã€‚æˆ‘ä»¬å€Ÿé‰´äººç±»å­¦ä¹ è¿‡ç¨‹ï¼Œå¼•å…¥äº†InsightTabï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥åˆ†è€Œæ²»ä¹‹ã€å…ˆæ˜“åéš¾å’Œåæ€å­¦ä¹ åŸåˆ™ä¸ºæŒ‡å¯¼çš„è§è§£æç‚¼æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡LLMå’Œæ•°æ®å»ºæ¨¡æŠ€æœ¯ä¹‹é—´çš„æ·±åº¦åˆä½œï¼Œæ•´åˆè§„åˆ™æ€»ç»“ã€æˆ˜ç•¥ä¾‹è¯å’Œè§è§£åæ€ã€‚æ‰€è·å¾—çš„è§è§£ä½¿LLMèƒ½å¤Ÿæ›´å¥½åœ°å°†å…¶ä¸€èˆ¬çŸ¥è¯†å’Œèƒ½åŠ›ä¸ç‰¹å®šè¡¨æ ¼ä»»åŠ¡çš„å…·ä½“è¦æ±‚ç›¸åŒ¹é…ã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šå¹¿æ³›è¯„ä¼°äº†InsightTabã€‚ç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒè¡¨ç°å‡ºäº†ä¸€è‡´çš„ä¼˜åŠ¿ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†åŸåˆ™æŒ‡å¯¼çš„æç‚¼è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼Œè€Œåˆ†æåˆ™å¼ºè°ƒäº†InsightTabåœ¨åˆ©ç”¨æ ‡è®°æ•°æ®å’Œåè¯¯ç®¡ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21561v1">PDF</a> EMNLP 25 Findings</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°‘æ ·æœ¬è¡¨æ ¼åˆ†ç±»ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ç»“æ„åŒ–æ•°æ®çš„å˜åŒ–æ€§é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡æç‚¼æ•°æ®ä¸ºå¯æ“ä½œæ€§çš„è§è§£æ¥åº”å¯¹ã€‚é€šè¿‡å€Ÿé‰´äººç±»å­¦ä¹ è¿‡ç¨‹çš„åŸåˆ™ï¼Œæˆ‘ä»¬æ¨å‡ºInsightTabï¼Œä¸€ä¸ªç”±åˆ†è€Œæ²»ä¹‹ã€æ˜“å…ˆè¡Œå’Œåæ€å­¦ä¹ åŸåˆ™å¼•å¯¼çš„è§è§£æç‚¼æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è§„åˆ™æ€»ç»“ã€æˆ˜ç•¥å®ä¾‹åŒ–å’Œæ·±åº¦æ´å¯Ÿåæ€ï¼Œå®ç°LLMä¸æ•°æ®å»ºæ¨¡æŠ€æœ¯çš„æ·±åº¦åˆä½œã€‚åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒInsightTabè¾ƒæœ€æ–°æ–¹æ³•å…·æœ‰æŒç»­çš„æ”¹è¿›æ•ˆæœã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†åŸåˆ™æŒ‡å¯¼çš„æç‚¼è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶åˆ†æå¼ºè°ƒäº†InsightTabåœ¨åˆ©ç”¨æ ‡è®°æ•°æ®å’Œåè§ç®¡ç†æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨å°‘æ ·æœ¬è¡¨æ ¼åˆ†ç±»ä»»åŠ¡é¢ä¸´ç»“æ„åŒ–æ•°æ®å˜åŒ–æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºInsightTabæ¡†æ¶ï¼Œæ•´åˆè§„åˆ™æ€»ç»“ã€æˆ˜ç•¥å®ä¾‹åŒ–å’Œæ·±åº¦æ´å¯Ÿåæ€ã€‚</li>
<li>InsightTabæ¡†æ¶å€Ÿé‰´äº†äººç±»å­¦ä¹ çš„åŸåˆ™ï¼Œå¦‚åˆ†è€Œæ²»ä¹‹ã€æ˜“å…ˆè¡Œå’Œåæ€å­¦ä¹ ã€‚</li>
<li>InsightTabé€šè¿‡ä¸æ•°æ®å»ºæ¨¡æŠ€æœ¯çš„æ·±åº¦åˆä½œï¼Œä½¿LLMæ›´å¥½åœ°é€‚åº”ç‰¹å®šè¡¨æ ¼ä»»åŠ¡çš„è¦æ±‚ã€‚</li>
<li>åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒInsightTabè¾ƒç°æœ‰æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>æ¶ˆèç ”ç©¶éªŒè¯äº†åŸåˆ™æŒ‡å¯¼çš„è§è§£æç‚¼è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7799e8552eeaa48303aacadc8c7f3e31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7bfabc70df76a79b636dfaf1a79d399.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d7ece37c47033e8c80749d601398859.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7e8900c7c6be6d5e4b1536e4f751465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38ae3e94db64ad7fedc4fcc318fd4463.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d7c3a750e64314bab25a118258d476f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Neuro-Symbolic-Imitation-Learning-for-Long-Horizon-Planning-and-Acting"><a href="#Few-Shot-Neuro-Symbolic-Imitation-Learning-for-Long-Horizon-Planning-and-Acting" class="headerlink" title="Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and   Acting"></a>Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and   Acting</h2><p><strong>Authors:Pierrick Lorang, Hong Lu, Johannes Huemer, Patrik Zips, Matthias Scheutz</strong></p>
<p>Imitation learning enables intelligent systems to acquire complex behaviors with minimal supervision. However, existing methods often focus on short-horizon skills, require large datasets, and struggle to solve long-horizon tasks or generalize across task variations and distribution shifts. We propose a novel neuro-symbolic framework that jointly learns continuous control policies and symbolic domain abstractions from a few skill demonstrations. Our method abstracts high-level task structures into a graph, discovers symbolic rules via an Answer Set Programming solver, and trains low-level controllers using diffusion policy imitation learning. A high-level oracle filters task-relevant information to focus each controller on a minimal observation and action space. Our graph-based neuro-symbolic framework enables capturing complex state transitions, including non-spatial and temporal relations, that data-driven learning or clustering techniques often fail to discover in limited demonstration datasets. We validate our approach in six domains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers of Hanoi environments, and a distinct Automated Forklift domain with two environments. The results demonstrate high data efficiency with as few as five skill demonstrations, strong zero- and few-shot generalizations, and interpretable decision making. </p>
<blockquote>
<p>æ¨¡ä»¿å­¦ä¹ ä½¿å¾—æ™ºèƒ½ç³»ç»Ÿèƒ½å¤Ÿåœ¨æœ€å°ç›‘ç£ä¸‹è·å¾—å¤æ‚çš„è¡Œä¸ºã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å…³æ³¨çŸ­æœŸæŠ€èƒ½ï¼Œéœ€è¦å¤§é‡æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨è§£å†³é•¿æœŸä»»åŠ¡æˆ–åœ¨ä»»åŠ¡å˜åŒ–å’Œåˆ†å¸ƒè½¬ç§»ä¸­æ³›åŒ–æ—¶é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»å°‘æ•°æŠ€èƒ½æ¼”ç¤ºä¸­è”åˆå­¦ä¹ è¿ç»­æ§åˆ¶æ”¿ç­–å’Œç¬¦å·åŸŸæŠ½è±¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†é«˜çº§ä»»åŠ¡ç»“æ„æŠ½è±¡ä¸ºå›¾å½¢ï¼Œé€šè¿‡å›ç­”é›†ç¼–ç¨‹æ±‚è§£å™¨å‘ç°ç¬¦å·è§„åˆ™ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ”¿ç­–æ¨¡ä»¿å­¦ä¹ è®­ç»ƒä½çº§æ§åˆ¶å™¨ã€‚é«˜çº§æŸ¥è¯¢å™¨è¿‡æ»¤å‡ºä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼Œä½¿æ¯ä¸ªæ§åˆ¶å™¨ä¸“æ³¨äºæœ€å°çš„è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ã€‚æˆ‘ä»¬çš„åŸºäºå›¾å½¢çš„ç¥ç»ç¬¦å·æ¡†æ¶èƒ½å¤Ÿæ•è·å¤æ‚çš„çŠ¶æ€è½¬æ¢ï¼ŒåŒ…æ‹¬éç©ºé—´å’Œæ—¶é—´å…³ç³»ï¼Œè¿™äº›çŠ¶æ€è½¬æ¢åœ¨æœ‰é™æ¼”ç¤ºæ•°æ®é›†ä¸­å¾€å¾€æ— æ³•è¢«æ•°æ®é©±åŠ¨å­¦ä¹ æˆ–èšç±»æŠ€æœ¯å‘ç°ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠå››ä¸ªæœºæ¢°è‡‚ã€å †å ã€å¨æˆ¿ã€è£…é…å’Œæ±‰è¯ºå¡”ç¯å¢ƒçš„å…­ä¸ªé¢†åŸŸä»¥åŠå…·æœ‰ä¸¤ä¸ªç¯å¢ƒçš„ç‹¬ç‰¹è‡ªåŠ¨åŒ–å‰è½¦é¢†åŸŸéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ä»…äº”ä¸ªæŠ€èƒ½æ¼”ç¤ºçš„æƒ…å†µä¸‹å…·æœ‰è¾ƒé«˜çš„æ•°æ®æ•ˆç‡ï¼Œå…·æœ‰å¼ºå¤§çš„é›¶æ¬¡å’Œå°‘æ¬¡æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠå¯è§£é‡Šçš„å†³ç­–åˆ¶å®šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21501v1">PDF</a> Accepted at CoRL 2025; to appear in PMLR</p>
<p><strong>Summary</strong><br>     ç¥ç»ç¬¦å·æ¡†æ¶è”åˆå­¦ä¹ è¿ç»­æ§åˆ¶ç­–ç•¥å’Œç¬¦å·åŸŸæŠ½è±¡ï¼Œé€šè¿‡å°‘é‡æŠ€èƒ½æ¼”ç¤ºå®ç°é«˜æ•ˆæ¨¡ä»¿å­¦ä¹ ã€‚è¯¥æ–¹æ³•æŠ½è±¡å‡ºé«˜çº§ä»»åŠ¡ç»“æ„ä¸ºå›¾ï¼Œé€šè¿‡ç­”æ¡ˆé›†ç¼–ç¨‹æ±‚è§£å™¨å‘ç°ç¬¦å·è§„åˆ™ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£ç­–ç•¥æ¨¡ä»¿å­¦ä¹ è®­ç»ƒä½çº§æ§åˆ¶å™¨ã€‚é«˜çº§åˆ«è¿‡æ»¤å™¨ä¼šè¿‡æ»¤å‡ºä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼Œä½¿æ¯ä¸ªæ§åˆ¶å™¨ä¸“æ³¨äºæœ€å°çš„è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´ã€‚è¯¥æ–¹æ³•èƒ½æ•æ‰å¤æ‚çš„çŠ¶æ€è½¬æ¢ï¼ŒåŒ…æ‹¬éç©ºé—´å’Œæ—¶é—´å…³ç³»ï¼Œåœ¨æœ‰é™æ¼”ç¤ºæ•°æ®é›†ä¸Šä¼˜äºæ•°æ®é©±åŠ¨å­¦ä¹ æˆ–èšç±»æŠ€æœ¯ã€‚éªŒè¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰é«˜æ•ˆçš„æ•°æ®åˆ©ç”¨ç‡ã€å¼ºå¤§çš„é›¶æ¬¡å’Œå°‘æ¬¡æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠå¯è§£é‡Šæ€§çš„å†³ç­–åˆ¶å®šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç¬¦å·æ¡†æ¶ç»“åˆäº†è¿ç»­æ§åˆ¶ç­–ç•¥å’Œç¬¦å·åŸŸæŠ½è±¡çš„è”åˆå­¦ä¹ ã€‚</li>
<li>é€šè¿‡å°‘é‡æŠ€èƒ½æ¼”ç¤ºå®ç°æ¨¡ä»¿å­¦ä¹ ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ä»»åŠ¡ç»“æ„çš„å›¾æŠ½è±¡ã€ç¬¦å·è§„åˆ™çš„å‘ç°ä»¥åŠä½çº§æ§åˆ¶å™¨çš„è®­ç»ƒã€‚</li>
<li>é«˜çº§åˆ«è¿‡æ»¤å™¨æœ‰åŠ©äºä¸“æ³¨äºæœ€å°çš„è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥æ•æ‰å¤æ‚çš„çŠ¶æ€è½¬æ¢ï¼ŒåŒ…æ‹¬éç©ºé—´å’Œæ—¶é—´å…³ç³»ã€‚</li>
<li>éªŒè¯ç»“æœå±•ç¤ºäº†å…¶é«˜æ•ˆæ•°æ®åˆ©ç”¨ç‡ã€å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›å¯è§£é‡Šçš„å†³ç­–åˆ¶å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4926c6d1a0b270cdee5f44bd841cc5d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-278fd7cb6047ddae570148b014a56a23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dcdabff1671ed5d24022953056b25e93.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="WEBEYETRACK-Scalable-Eye-Tracking-for-the-Browser-via-On-Device-Few-Shot-Personalization"><a href="#WEBEYETRACK-Scalable-Eye-Tracking-for-the-Browser-via-On-Device-Few-Shot-Personalization" class="headerlink" title="WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device   Few-Shot Personalization"></a>WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device   Few-Shot Personalization</h2><p><strong>Authors:Eduardo Davalos, Yike Zhang, Namrata Srivastava, Yashvitha Thatigotla, Jorge A. Salas, Sara McFadden, Sun-Joo Cho, Amanda Goodwin, Ashwin TS, Gautam Biswas</strong></p>
<p>With advancements in AI, new gaze estimation methods are exceeding state-of-the-art (SOTA) benchmarks, but their real-world application reveals a gap with commercial eye-tracking solutions. Factors like model size, inference time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking methods lack sufficient accuracy, in particular due to head movement. To tackle these issues, we introduce We bEyeTrack, a framework that integrates lightweight SOTA gaze estimation models directly in the browser. It incorporates model-based head pose estimation and on-device few-shot learning with as few as nine calibration samples (k &lt; 9). WebEyeTrack adapts to new users, achieving SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14. Our open-source code is available at <a target="_blank" rel="noopener" href="https://github.com/RedForestAi/WebEyeTrack">https://github.com/RedForestAi/WebEyeTrack</a>. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½çš„è¿›æ­¥ï¼Œæ–°çš„çœ¼åŠ¨è¿½è¸ªæ–¹æ³•å·²ç»è¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºå‡†æµ‹è¯•ï¼Œä½†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­å´æ˜¾ç¤ºå‡ºä¸å•†ä¸šçœ¼åŠ¨è¿½è¸ªè§£å†³æ–¹æ¡ˆä¹‹é—´çš„å·®è·ã€‚æ¨¡å‹å¤§å°ã€æ¨ç†æ—¶é—´å’Œéšç§ç­‰å› ç´ å¾€å¾€è¢«å¿½è§†ã€‚åŒæ—¶ï¼ŒåŸºäºç½‘ç»œæ‘„åƒå¤´çš„çœ¼åŠ¨è¿½è¸ªæ–¹æ³•ç”±äºç¼ºä¹è¶³å¤Ÿçš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯ç”±äºå¤´éƒ¨ç§»åŠ¨å¯¼è‡´çš„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WebEyeTrackæ¡†æ¶ï¼Œå®ƒç›´æ¥åœ¨æµè§ˆå™¨ä¸­é›†æˆäº†è½»é‡çº§çš„æœ€æ–°çœ¼åŠ¨è¿½è¸ªæ¨¡å‹ã€‚å®ƒç»“åˆäº†åŸºäºæ¨¡å‹çš„å¤´éƒ¨å§¿æ€ä¼°è®¡å’Œåœ¨è®¾å¤‡ä¸Šçš„å°æ ·æœ¬å­¦ä¹ ï¼Œåªéœ€æå°‘æ•°çš„æ ¡å‡†æ ·æœ¬ï¼ˆk &lt; 9ï¼‰ã€‚WebEyeTrackèƒ½å¤Ÿé€‚åº”æ–°ç”¨æˆ·ï¼Œåœ¨GazeCaptureä¸Šçš„è¯¯å·®èŒƒå›´ä¸º2.32å˜ç±³ï¼Œå…·æœ‰å®æ—¶æ¨ç†é€Ÿåº¦ï¼Œåœ¨iPhone 14ä¸Šä¸º2.4æ¯«ç§’ã€‚æˆ‘ä»¬çš„å¼€æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RedForestAi/WebEyeTrack%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RedForestAi/WebEyeTrackæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19544v1">PDF</a> 9 pages, 7 figures, 1 table</p>
<p><strong>Summary</strong><br>éšç€AIæŠ€æœ¯çš„å‘å±•ï¼Œæ–°çš„çœ¼åŠ¨è¿½è¸ªæ–¹æ³•è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯åŸºå‡†ï¼Œä½†å…¶å®é™…åº”ç”¨ä¸å•†ä¸šçœ¼åŠ¨è¿½è¸ªè§£å†³æ–¹æ¡ˆä¹‹é—´å­˜åœ¨å·®è·ã€‚ä¸ºè§£å†³æ¨¡å‹å¤§å°ã€æ¨ç†æ—¶é—´ã€éšç§ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WebEyeTrackæ¡†æ¶ï¼Œç›´æ¥åœ¨æµè§ˆå™¨ä¸­é›†æˆè½»é‡çº§çœ¼åŠ¨è¿½è¸ªæ¨¡å‹ã€‚å®ƒé‡‡ç”¨åŸºäºæ¨¡å‹çš„å¤´éƒ¨å§¿æ€ä¼°è®¡å’Œå°‘é‡æ ·æœ¬ï¼ˆå°‘äº9ä¸ªï¼‰çš„åœ¨çº¿few-shotå­¦ä¹ æŠ€æœ¯ã€‚WebEyeTrackèƒ½å¤Ÿé€‚åº”æ–°ç”¨æˆ·ï¼Œåœ¨GazeCaptureä¸Šå®ç°è¯¯å·®èŒƒå›´ä»…ä¸º2.32å˜ç±³çš„é«˜æ€§èƒ½è¡¨ç°ï¼Œå¹¶åœ¨iPhone 14ä¸Šå®ç°å®æ—¶æ¨ç†é€Ÿåº¦2.4æ¯«ç§’ã€‚æˆ‘ä»¬çš„å¼€æºä»£ç å¯é€šè¿‡ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/RedForestAi/WebEyeTrack%E3%80%82">https://github.com/RedForestAi/WebEyeTrackã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°çœ¼åŠ¨è¿½è¸ªæ–¹æ³•è¶…è¶Šç°æœ‰æŠ€æœ¯åŸºå‡†ï¼Œä½†å®é™…åº”ç”¨ä¸å•†ä¸šè§£å†³æ–¹æ¡ˆå­˜åœ¨å·®è·ã€‚</li>
<li>WebEyeTrackæ¡†æ¶æ—¨åœ¨è§£å†³æ¨¡å‹å¤§å°ã€æ¨ç†æ—¶é—´ã€éšç§ç­‰é—®é¢˜ã€‚</li>
<li>WebEyeTrackç›´æ¥åœ¨æµè§ˆå™¨ä¸­é›†æˆè½»é‡çº§çœ¼åŠ¨è¿½è¸ªæ¨¡å‹ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨åŸºäºæ¨¡å‹çš„å¤´éƒ¨å§¿æ€ä¼°è®¡å’Œå°‘é‡æ ·æœ¬çš„åœ¨çº¿few-shotå­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>WebEyeTrackèƒ½å¤Ÿé€‚åº”æ–°ç”¨æˆ·ï¼Œå®ç°è¯¯å·®èŒƒå›´å°å’Œå®æ—¶æ¨ç†é€Ÿåº¦å¿«çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>WebEyeTrackåœ¨GazeCaptureä¸Šçš„è¯¯å·®èŒƒå›´ä¸º2.32å˜ç±³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6805b80512e002868c162ca6eebbba80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-465757143a39d5411a267ed38976f194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-511dac66364c1f465f4be98d5e31fbfd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54c397a30f8e185d83f521919cc18154.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ba79100b36b42cc37377a15b7884dad.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Bridging-Language-Gaps-Enhancing-Few-Shot-Language-Adaptation"><a href="#Bridging-Language-Gaps-Enhancing-Few-Shot-Language-Adaptation" class="headerlink" title="Bridging Language Gaps: Enhancing Few-Shot Language Adaptation"></a>Bridging Language Gaps: Enhancing Few-Shot Language Adaptation</h2><p><strong>Authors:Philipp Borchert, Jochen De Weerdt, Marie-Francine Moens</strong></p>
<p>The disparity in language resources poses a challenge in multilingual NLP, with high-resource languages benefiting from extensive data, while low-resource languages lack sufficient data for effective training. Our Contrastive Language Alignment with Prompting (CoLAP) method addresses this gap by integrating contrastive learning with cross-lingual representations, facilitating task-specific knowledge transfer from high-resource to lower-resource languages. The primary advantage of our approach is its data efficiency, enabling rapid adaptation to new languages and reducing the need for large labeled datasets. We conduct experiments with multilingual encoder-only and decoder-only language models on natural language understanding tasks, including natural language inference and relation extraction, evaluating performance across both high- and low-resource languages. Our results demonstrate that CoLAP outperforms few-shot cross-lingual transfer baselines and in-context learning, even with limited available data. This effectively narrows the cross-lingual performance gap, contributing to the development of more efficient multilingual NLP techniques. </p>
<blockquote>
<p>è¯­è¨€èµ„æºçš„å·®å¼‚ç»™å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œé«˜èµ„æºè¯­è¨€å—ç›Šäºå¤§é‡æ•°æ®ï¼Œè€Œä½èµ„æºè¯­è¨€åˆ™ç¼ºä¹æœ‰æ•ˆè®­ç»ƒæ‰€éœ€çš„æ•°æ®ã€‚æˆ‘ä»¬çš„å¯¹æ¯”è¯­è¨€å¯¹é½æç¤ºï¼ˆCoLAPï¼‰æ–¹æ³•é€šè¿‡ç»“åˆå¯¹æ¯”å­¦ä¹ ä¸è·¨è¯­è¨€è¡¨ç¤ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä¿ƒè¿›äº†ä»é«˜èµ„æºè¯­è¨€åˆ°ä½èµ„æºè¯­è¨€çš„ç‰¹å®šä»»åŠ¡çŸ¥è¯†è¿ç§»ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä¸»è¦ä¼˜åŠ¿æ˜¯æ•°æ®æ•ˆç‡é«˜ï¼Œèƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°è¯­è¨€ï¼Œå‡å°‘äº†å¯¹å¤§é‡æ ‡è®°æ•°æ®é›†çš„éœ€æ±‚ã€‚æˆ‘ä»¬åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€æ¨ç†å’Œå…³ç³»æŠ½å–ï¼Œè¯„ä¼°äº†é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æœ‰é™çš„æ•°æ®å¯ç”¨çš„æƒ…å†µä¸‹ï¼ŒCoLAPä¹Ÿä¼˜äºå°‘é•œå¤´è·¨è¯­è¨€è¿ç§»åŸºçº¿å’Œå†…ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚è¿™æœ‰æ•ˆåœ°ç¼©å°äº†è·¨è¯­è¨€æ€§èƒ½å·®è·ï¼Œä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„å¤šè¯­è¨€NLPæŠ€æœ¯åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19464v1">PDF</a> 17 pages</p>
<p><strong>Summary</strong></p>
<p>å¯¹æ¯”è¯­è¨€å¯¹é½æç¤ºæ–¹æ³•ï¼ˆCoLAPï¼‰é€šè¿‡ç»“åˆå¯¹æ¯”å­¦ä¹ ä¸è·¨è¯­è¨€è¡¨ç¤ºï¼Œè§£å†³äº†å¤šè¯­ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­èµ„æºåˆ†å¸ƒä¸å‡çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¼˜åŠ¿åœ¨äºæ•°æ®æ•ˆç‡é«˜ï¼Œèƒ½å¿«é€Ÿé€‚åº”æ–°è¯­è¨€å¹¶å‡å°‘å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„éœ€æ±‚ã€‚å®éªŒè¯æ˜ï¼Œåœ¨åŒ…æ‹¬è‡ªç„¶è¯­è¨€æ¨æ–­å’Œå…³ç³»æŠ½å–ç­‰è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šï¼ŒCoLAPåœ¨é«˜ä½èµ„æºè¯­è¨€ä¸Šçš„è¡¨ç°å‡ä¼˜äºå°‘é•œå¤´è·¨è¯­è¨€è½¬ç§»åŸºå‡†çº¿å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå³ä½¿æ•°æ®æœ‰é™ã€‚æ­¤æ–¹æ³•æœ‰æ•ˆç¼©å°äº†è·¨è¯­è¨€æ€§èƒ½å·®è·ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆçš„å¤šè¯­ç§NLPæŠ€æœ¯åšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­ç§NLPé¢ä¸´è¯­è¨€èµ„æºåˆ†å¸ƒä¸å‡çš„æŒ‘æˆ˜ï¼Œé«˜èµ„æºè¯­è¨€å—ç›Šäºå¤§é‡æ•°æ®ï¼Œè€Œä½èµ„æºè¯­è¨€ç¼ºä¹æœ‰æ•ˆè®­ç»ƒæ•°æ®ã€‚</li>
<li>CoLAPæ–¹æ³•é€šè¿‡ç»“åˆå¯¹æ¯”å­¦ä¹ ä¸è·¨è¯­è¨€è¡¨ç¤ºï¼Œè§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>CoLAPæ–¹æ³•çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå…¶æ•°æ®é«˜æ•ˆæ€§ï¼Œèƒ½å¿«é€Ÿé€‚åº”æ–°è¯­è¨€å¹¶å‡å°‘å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„éœ€æ±‚ã€‚</li>
<li>CoLAPåœ¨å¤šç§è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€æ¨æ–­å’Œå…³ç³»æŠ½å–ã€‚</li>
<li>CoLAPåœ¨é«˜ä½èµ„æºè¯­è¨€ä¸Šçš„è¡¨ç°å‡å¾ˆå‡ºè‰²ï¼Œç¼©å°äº†è·¨è¯­è¨€æ€§èƒ½å·®è·ã€‚</li>
<li>å®éªŒè¯æ˜äº†CoLAPæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e0b66f9dc7453c160b2ac2d15bee21ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d49c7f66bb702a08eb8352378d3e0dbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26a52f9ade448b811670a46e4ce10b98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e95bd37d1e93ec69cc1d3585ebcb8ac3.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Heterogeneous-LLM-Methods-for-Ontology-Learning-Few-Shot-Prompting-Ensemble-Typing-and-Attention-Based-Taxonomies"><a href="#Heterogeneous-LLM-Methods-for-Ontology-Learning-Few-Shot-Prompting-Ensemble-Typing-and-Attention-Based-Taxonomies" class="headerlink" title="Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting,   Ensemble Typing, and Attention-Based Taxonomies)"></a>Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting,   Ensemble Typing, and Attention-Based Taxonomies)</h2><p><strong>Authors:Aleksandra Beliaeva, Temurbek Rahmatullaev</strong></p>
<p>We present a comprehensive system for addressing Tasks A, B, and C of the LLMs4OL 2025 challenge, which together span the full ontology construction pipeline: term extraction, typing, and taxonomy discovery. Our approach combines retrieval-augmented prompting, zero-shot classification, and attention-based graph modeling â€“ each tailored to the demands of the respective task. For Task A, we jointly extract domain-specific terms and their ontological types using a retrieval-augmented generation (RAG) pipeline. Training data was reformulated into a document to terms and types correspondence, while test-time inference leverages semantically similar training examples. This single-pass method requires no model finetuning and improves overall performance through lexical augmentation Task B, which involves assigning types to given terms, is handled via a dual strategy. In the few-shot setting (for domains with labeled training data), we reuse the RAG scheme with few-shot prompting. In the zero-shot setting (for previously unseen domains), we use a zero-shot classifier that combines cosine similarity scores from multiple embedding models using confidence-based weighting. In Task C, we model taxonomy discovery as graph inference. Using embeddings of type labels, we train a lightweight cross-attention layer to predict is-a relations by approximating a soft adjacency matrix. These modular, task-specific solutions enabled us to achieve top-ranking results in the official leaderboard across all three tasks. Taken together these strategies showcase the scalability, adaptability, and robustness of LLM-based architectures for ontology learning across heterogeneous domains.   Code is available at: <a target="_blank" rel="noopener" href="https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek">https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek</a> </p>
<blockquote>
<p>æˆ‘ä»¬é’ˆå¯¹LLMs4OL 2025æŒ‘æˆ˜çš„Aã€Bã€Cä¸‰é¡¹ä»»åŠ¡ï¼Œæ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„ç³»ç»Ÿï¼Œæ¶µç›–äº†æ•´ä¸ªæœ¬ä½“æ„å»ºæµç¨‹ï¼šæœ¯è¯­æå–ã€ç±»å‹åˆ’åˆ†å’Œåˆ†ç±»ä½“ç³»å‘ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æ£€ç´¢å¢å¼ºæç¤ºã€é›¶æ ·æœ¬åˆ†ç±»å’ŒåŸºäºæ³¨æ„åŠ›çš„å›¾å½¢å»ºæ¨¡â€”â€”æ¯ä¸€é¡¹éƒ½æ˜¯é’ˆå¯¹å„è‡ªä»»åŠ¡çš„éœ€æ±‚é‡èº«å®šåˆ¶çš„ã€‚é’ˆå¯¹Aä»»åŠ¡ï¼Œæˆ‘ä»¬ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹è”åˆæå–ç‰¹å®šé¢†åŸŸçš„æœ¯è¯­åŠå…¶æœ¬ä½“ç±»å‹ã€‚æˆ‘ä»¬å°†è®­ç»ƒæ•°æ®é‡æ–°æ ¼å¼åŒ–ä¸ºæ–‡æ¡£ä¸æœ¯è¯­å’Œç±»å‹ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œè€Œæµ‹è¯•æ—¶çš„æ¨ç†åˆ™åˆ©ç”¨è¯­ä¹‰ç›¸ä¼¼çš„è®­ç»ƒç¤ºä¾‹ã€‚è¿™ç§å•é€šé“æ–¹æ³•æ— éœ€å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶é€šè¿‡è¯æ±‡å¢å¼ºæé«˜æ•´ä½“æ€§èƒ½ã€‚Bä»»åŠ¡æ˜¯ä¸ºç»™å®šçš„æœ¯è¯­åˆ†é…ç±»å‹ï¼Œé€šè¿‡åŒé‡ç­–ç•¥è¿›è¡Œå¤„ç†ã€‚åœ¨æœ‰é™æ ·æœ¬è®¾ç½®ï¼ˆå¯¹äºæœ‰æ ‡ç­¾è®­ç»ƒæ•°æ®çš„é¢†åŸŸï¼‰ä¸­ï¼Œæˆ‘ä»¬é‡å¤ä½¿ç”¨å¸¦æœ‰æœ‰é™æç¤ºçš„RAGæ–¹æ¡ˆã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ï¼ˆå¯¹äºä»¥å‰æœªè§è¿‡çš„é¢†åŸŸï¼‰ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé›¶æ ·æœ¬åˆ†ç±»å™¨ï¼Œå®ƒç»“åˆäº†å¤šä¸ªåµŒå…¥æ¨¡å‹çš„ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œå¹¶ä½¿ç”¨åŸºäºç½®ä¿¡åº¦çš„åŠ æƒæ–¹å¼ã€‚åœ¨Cä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å°†åˆ†ç±»ä½“ç³»å‘ç°å»ºæ¨¡ä¸ºå›¾å½¢æ¨ç†ã€‚æˆ‘ä»¬ä½¿ç”¨ç±»å‹æ ‡ç­¾çš„åµŒå…¥ï¼Œè®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„è·¨æ³¨æ„åŠ›å±‚æ¥é¢„æµ‹â€œæ˜¯-å…³ç³»â€ï¼ˆis-a relationsï¼‰ï¼Œé€šè¿‡è¿‘ä¼¼è½¯é‚»æ¥çŸ©é˜µæ¥å®ç°ã€‚è¿™äº›æ¨¡å—åŒ–ã€ä»»åŠ¡ç‰¹å®šçš„è§£å†³æ–¹æ¡ˆä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ä¸­å–å¾—å®˜æ–¹æ’è¡Œæ¦œçš„é¡¶å°–æˆç»©ã€‚è¿™äº›ç­–ç•¥ç›¸ç»“åˆå±•ç¤ºäº†LLMæ¶æ„åœ¨ä¸åŒé¢†åŸŸè¿›è¡Œæœ¬ä½“å­¦ä¹ çš„å¯æ‰©å±•æ€§ã€é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚ç›¸å…³ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbekè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19428v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…¨é¢çš„ç³»ç»Ÿï¼Œé’ˆå¯¹LLMs4OL 2025æŒ‘æˆ˜ä¸­çš„ä»»åŠ¡Aã€Bå’ŒCï¼Œæ¶µç›–äº†æ•´ä¸ªæœ¬ä½“æ„å»ºæµç¨‹ï¼šæœ¯è¯­æå–ã€ç±»å‹æ ‡æ³¨å’Œåˆ†ç±»ä½“ç³»å‘ç°ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ£€ç´¢å¢å¼ºæç¤ºã€é›¶æ ·æœ¬åˆ†ç±»å’ŒåŸºäºæ³¨æ„åŠ›çš„å›¾å½¢å»ºæ¨¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é’ˆå¯¹ç‰¹å®šéœ€æ±‚è¿›è¡Œäº†å®šåˆ¶ã€‚é€šè¿‡æ”¹é©è®­ç»ƒæ•°æ®ï¼Œåˆ©ç”¨è¯­ä¹‰ç›¸ä¼¼è®­ç»ƒç¤ºä¾‹ï¼Œå®ç°äº†æ— éœ€æ¨¡å‹å¾®è°ƒçš„ä¸€æ¬¡æ€§é€šè¿‡æ–¹æ³•ï¼Œæé«˜äº†æ•´ä½“æ€§èƒ½ã€‚å¯¹äºä»»åŠ¡Bçš„ç±»å‹åˆ†é…ï¼Œé‡‡ç”¨åŒé‡ç­–ç•¥ï¼Œåœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸‹ä½¿ç”¨RAGæ–¹æ¡ˆï¼Œåœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹ä½¿ç”¨ç»“åˆå¤šç§åµŒå…¥æ¨¡å‹çš„é›¶æ ·æœ¬åˆ†ç±»å™¨ã€‚ä»»åŠ¡Cä¸­çš„åˆ†ç±»ä½“ç³»å‘ç°è¢«å»ºæ¨¡ä¸ºå›¾å½¢æ¨æ–­ï¼Œä½¿ç”¨ç±»å‹æ ‡ç­¾çš„åµŒå…¥è®­ç»ƒè½»é‡çº§äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œé€šè¿‡é¢„æµ‹is-aå…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚è¿™äº›æ¨¡å—åŒ–çš„ä»»åŠ¡ç‰¹å®šè§£å†³æ–¹æ¡ˆï¼Œä½¿å¾—æˆ‘ä»¬åœ¨å®˜æ–¹æ’è¡Œæ¦œä¸Šå®ç°äº†æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡çš„é¦–ä½æ’åã€‚å±•ç¤ºäº†LLMæ¶æ„åœ¨å¤§è§„æ¨¡ã€é€‚åº”æ€§å’Œç¨³å¥æ€§æ–¹é¢çš„ä¼˜åŠ¿ï¼Œé€‚ç”¨äºè·¨å¼‚æ„é¢†åŸŸçš„æœ¬ä½“å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç³»ç»Ÿå…¨é¢åº”å¯¹LLMs4OL 2025æŒ‘æˆ˜ä¸­çš„ä»»åŠ¡Aã€Bå’ŒCï¼Œæ¶µç›–æœ¬ä½“æ„å»ºå…¨æµç¨‹ã€‚</li>
<li>ç»“åˆæ£€ç´¢å¢å¼ºæç¤ºã€é›¶æ ·æœ¬åˆ†ç±»å’ŒåŸºäºæ³¨æ„åŠ›çš„å›¾å½¢å»ºæ¨¡æ–¹æ³•ï¼Œé’ˆå¯¹å„ä»»åŠ¡éœ€æ±‚å®šåˆ¶ã€‚</li>
<li>ä»»åŠ¡Aé‡‡ç”¨åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ç®¡é“è”åˆæå–é¢†åŸŸç‰¹å®šæœ¯è¯­åŠå…¶æœ¬ä½“ç±»å‹ã€‚</li>
<li>ä»»åŠ¡Bå¯¹ç±»å‹åˆ†é…é‡‡ç”¨åŒé‡ç­–ç•¥ï¼Œåˆ†åˆ«å¤„ç†æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾é¢†åŸŸçš„å°‘é‡æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ã€‚</li>
<li>ä»»åŠ¡Cå°†åˆ†ç±»ä½“ç³»å‘ç°å»ºæ¨¡ä¸ºå›¾å½¢æ¨æ–­ï¼Œä½¿ç”¨åµŒå…¥çš„äº¤å‰æ³¨æ„åŠ›å±‚é¢„æµ‹is-aå…³ç³»ã€‚</li>
<li>å®ç°æ‰€æœ‰ä»»åŠ¡åœ¨å®˜æ–¹æ’è¡Œæ¦œä¸Šçš„é¦–ä½æ’åï¼Œè¯æ˜äº†LLMæ¶æ„åœ¨å¤§è§„æ¨¡ã€é€‚åº”æ€§å’Œç¨³å¥æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-552f50649d0b37017d4ee52740499b9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b9dc704dc9bc4963b6fe1a3b95af4d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e753b18dbafb15f4e907870d45648af1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Connectivity-Aware-Text-Line-Segmentation-in-Historical-Documents"><a href="#Few-Shot-Connectivity-Aware-Text-Line-Segmentation-in-Historical-Documents" class="headerlink" title="Few-Shot Connectivity-Aware Text Line Segmentation in Historical   Documents"></a>Few-Shot Connectivity-Aware Text Line Segmentation in Historical   Documents</h2><p><strong>Authors:Rafael Sterzinger, Tingyu Lin, Robert Sablatnig</strong></p>
<p>A foundational task for the digital analysis of documents is text line segmentation. However, automating this process with deep learning models is challenging because it requires large, annotated datasets that are often unavailable for historical documents. Additionally, the annotation process is a labor- and cost-intensive task that requires expert knowledge, which makes few-shot learning a promising direction for reducing data requirements. In this work, we demonstrate that small and simple architectures, coupled with a topology-aware loss function, are more accurate and data-efficient than more complex alternatives. We pair a lightweight UNet++ with a connectivity-aware loss, initially developed for neuron morphology, which explicitly penalizes structural errors like line fragmentation and unintended line merges. To increase our limited data, we train on small patches extracted from a mere three annotated pages per manuscript. Our methodology significantly improves upon the current state-of-the-art on the U-DIADS-TL dataset, with a 200% increase in Recognition Accuracy and a 75% increase in Line Intersection over Union. Our method also achieves an F-Measure score on par with or even exceeding that of the competition winner of the DIVA-HisDB baseline detection task, all while requiring only three annotated pages, exemplifying the efficacy of our approach. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/RafaelSterzinger/acpr_few_shot_hist">https://github.com/RafaelSterzinger/acpr_few_shot_hist</a>. </p>
<blockquote>
<p>æ–‡æ¡£æ•°å­—åˆ†æçš„åŸºç¡€ä»»åŠ¡æ˜¯æ–‡æœ¬è¡Œåˆ†å‰²ã€‚ç„¶è€Œï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨åŒ–æ­¤è¿‡ç¨‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ï¼Œè€Œå¯¹äºå†å²æ–‡æ¡£ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸ä¸å¯ç”¨ã€‚æ­¤å¤–ï¼Œæ ‡æ³¨è¿‡ç¨‹æ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†ä¸”æˆæœ¬é«˜æ˜‚çš„ä»»åŠ¡ï¼Œéœ€è¦ä¸“ä¸šçŸ¥è¯†ï¼Œè¿™ä½¿å¾—å°æ ·æœ¬å­¦ä¹ æˆä¸ºå‡å°‘æ•°æ®éœ€æ±‚çš„é¢‡å…·å‰æ™¯çš„æ–¹å‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜ç®€å•çš„å°è§„æ¨¡æ¶æ„ä¸æ‹“æ‰‘æ„ŸçŸ¥æŸå¤±å‡½æ•°ç›¸ç»“åˆï¼Œæ¯”æ›´å¤æ‚çš„æ›¿ä»£æ–¹æ¡ˆæ›´å‡†ç¡®ä¸”æ›´èŠ‚çœæ•°æ®ã€‚æˆ‘ä»¬å°†è½»é‡çº§çš„UNet++ä¸ä¸“ä¸ºç¥ç»å…ƒå½¢æ€å¼€å‘çš„è¿é€šæ€§æ„ŸçŸ¥æŸå¤±ç›¸ç»“åˆï¼Œè¯¥æŸå¤±æ˜¾å¼æƒ©ç½šçº¿çŠ¶åˆ†è£‚å’Œæ„å¤–åˆå¹¶ä¹‹ç±»çš„ç»“æ„é”™è¯¯ã€‚ä¸ºäº†å¢åŠ æˆ‘ä»¬æœ‰é™çš„æ•°æ®é‡ï¼Œæˆ‘ä»¬åœ¨ä»…æ¥è‡ªæ¯ä»½æ‰‹ç¨¿çš„ä¸‰é¡µæ ‡æ³¨æ•°æ®ä¸­æå–çš„å°å—åŒºåŸŸä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨U-DIADS-TLæ•°æ®é›†ä¸Šå¤§å¹…æ”¹è¿›äº†å½“å‰æœ€æ–°æŠ€æœ¯ï¼Œè¯†åˆ«å‡†ç¡®åº¦æé«˜äº†200%ï¼Œçº¿äº¤é›†æé«˜äº†75%ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å®ç°äº†ä¸DIVA-HisDBåŸºçº¿æ£€æµ‹ä»»åŠ¡ç«èµ›è·å¥–è€…ç›¸å½“çš„F-Measureåˆ†æ•°ï¼Œç”šè‡³æœ‰æ‰€è¶…è¶Šï¼Œè€Œè¿™ä¸€åˆ‡ä»…éœ€è¦ä¸‰é¡µæ ‡æ³¨æ•°æ®å³å¯å®ç°ï¼Œè¿™å……åˆ†è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/RafaelSterzinger/acpr_few_shot_hist%E3%80%82">https://github.com/RafaelSterzinger/acpr_few_shot_histã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19162v1">PDF</a> 15 pages, accepted at ACPR2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†é’ˆå¯¹å†å²æ–‡æ¡£çš„æ•°å­—åˆ†æä¸­çš„æ–‡æœ¬è¡Œåˆ†å‰²ä»»åŠ¡ã€‚æ–‡ç« æŒ‡å‡ºæ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨åŒ–å¤„ç†è¯¥ä»»åŠ¡çš„æŒ‘æˆ˜åœ¨äºç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œä¸”æ ‡æ³¨è¿‡ç¨‹éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œå¤§é‡äººåŠ›æˆæœ¬ã€‚è¯¥ç ”ç©¶é€šè¿‡ç®€å•æ¶æ„ä¸æ‹“æ‰‘æ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ç°äº†é«˜å‡†ç¡®æ€§å’Œæ•°æ®æ•ˆç‡ã€‚è¯¥ç ”ç©¶ä½¿ç”¨è½»é‡çº§UNet++ä¸ä¸“ä¸ºç¥ç»å…ƒå½¢æ€å¼€å‘çš„è¿æ¥æ„ŸçŸ¥æŸå¤±æ¥æƒ©ç½šç»“æ„é”™è¯¯ï¼Œå¦‚çº¿æ¡æ–­è£‚å’Œæ„å¤–åˆå¹¶ã€‚é€šè¿‡è®­ç»ƒæ¥è‡ªä»…ä¸‰é¡µæ‰‹ç¨¿çš„å°å—æ•°æ®ï¼Œè¯¥ç ”ç©¶çš„æ–¹æ³•åœ¨U-DIADS-TLæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†è¯†åˆ«å‡†ç¡®ç‡ï¼Œå®ç°äº†çº¿äº¤å¹¶çš„75%å¢é•¿ã€‚è¯¥ç ”ç©¶çš„æ–¹æ³•åœ¨DIVA-HisDBåŸºçº¿æ£€æµ‹ä»»åŠ¡çš„æ¯”èµ›ä¸­ä¸å† å†›è¡¨ç°ç›¸å½“ï¼Œä»…éœ€ä¸‰é¡µæ ‡æ³¨æ•°æ®ä¾¿å±•ç°å‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³å®ç°å·²å…¬å¼€äº[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è¡Œåˆ†å‰²æ˜¯æ•°å­—åˆ†ææ–‡æ¡£çš„åŸºç¡€ä»»åŠ¡ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨åŒ–å¤„ç†æ–‡æœ¬è¡Œåˆ†å‰²é¢ä¸´ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†å’Œæ ‡æ³¨è¿‡ç¨‹å¤æ‚çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨ç®€å•æ¶æ„ä¸æ‹“æ‰‘æ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¡¨ç°å‡ºé«˜å‡†ç¡®æ€§å’Œæ•°æ®æ•ˆç‡ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨è½»é‡çº§UNet++ä¸è¿æ¥æ„ŸçŸ¥æŸå¤±æ¥å¤„ç†ç¥ç»å…ƒå½¢æ€çš„ç»“æ„é”™è¯¯ã€‚</li>
<li>é€šè¿‡è®­ç»ƒä»…æ¥è‡ªä¸‰é¡µæ‰‹ç¨¿çš„å°å—æ•°æ®ï¼Œè¯¥ç ”ç©¶çš„æ–¹æ³•åœ¨U-DIADS-TLæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜è¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
<li>è¯¥ç ”ç©¶çš„æ–¹æ³•åœ¨DIVA-HisDBåŸºçº¿æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä¸å† å†›è¡¨ç°ç›¸å½“ï¼Œä¸”ä»…éœ€å°‘é‡æ ‡æ³¨æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0aeba0fbfff59b416609bbf96e92364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-585ab68f8b79809c9b58c75ffb4f525b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777219a2f33a9aa0d33bf17dcf3ff28b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Few-shot-Unknown-Class-Discovery-of-Hyperspectral-Images-with-Prototype-Learning-and-Clustering"><a href="#Few-shot-Unknown-Class-Discovery-of-Hyperspectral-Images-with-Prototype-Learning-and-Clustering" class="headerlink" title="Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype   Learning and Clustering"></a>Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype   Learning and Clustering</h2><p><strong>Authors:Chun Liu, Chen Zhang, Zhuo Li, Zheng Li, Wei Yang</strong></p>
<p>Open-set few-shot hyperspectral image (HSI) classification aims to classify image pixels by using few labeled pixels per class, where the pixels to be classified may be not all from the classes that have been seen. To address the open-set HSI classification challenge, current methods focus mainly on distinguishing the unknown class samples from the known class samples and rejecting them to increase the accuracy of identifying known class samples. They fails to further identify or discovery the unknow classes among the samples. This paper proposes a prototype learning and clustering method for discoverying unknown classes in HSIs under the few-shot environment. Using few labeled samples, it strives to develop the ability of infering the prototypes of unknown classes while distinguishing unknown classes from known classes. Once the unknown class samples are rejected by the learned known class classifier, the proposed method can further cluster the unknown class samples into different classes according to their distance to the inferred unknown class prototypes. Compared to existing state-of-the-art methods, extensive experiments on four benchmark HSI datasets demonstrate that our proposed method exhibits competitive performance in open-set few-shot HSI classification tasks. All the codes are available at \href{<a target="_blank" rel="noopener" href="https://github.com/KOBEN-ff/OpenFUCD-main%7D">https://github.com/KOBEN-ff/OpenFUCD-main}</a> {<a target="_blank" rel="noopener" href="https://github.com/KOBEN-ff/OpenFUCD-main%7D">https://github.com/KOBEN-ff/OpenFUCD-main}</a> </p>
<blockquote>
<p>å¼€æ”¾é›†å°æ ·æœ¬é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰åˆ†ç±»çš„ç›®æ ‡æ˜¯ä½¿ç”¨æ¯ç±»å°‘é‡çš„æ ‡è®°åƒç´ å¯¹å›¾åƒåƒç´ è¿›è¡Œåˆ†ç±»ï¼Œå…¶ä¸­å¾…åˆ†ç±»çš„åƒç´ å¯èƒ½å¹¶ä¸éƒ½æ˜¯æ¥è‡ªå·²è§è¿‡çš„ç±»åˆ«ã€‚ä¸ºäº†è§£å†³å¼€æ”¾é›†HSIåˆ†ç±»çš„æŒ‘æˆ˜ï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦å…³æ³¨ä»å·²çŸ¥ç±»åˆ«æ ·æœ¬ä¸­åŒºåˆ†æœªçŸ¥ç±»åˆ«æ ·æœ¬å¹¶æ‹’ç»å®ƒä»¬ï¼Œä»¥æé«˜è¯†åˆ«å·²çŸ¥ç±»åˆ«æ ·æœ¬çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå®ƒä»¬æ— æ³•åœ¨æ ·æœ¬ä¸­è¿›ä¸€æ­¥è¯†åˆ«æˆ–å‘ç°æœªçŸ¥ç±»åˆ«ã€‚æœ¬æ–‡é’ˆå¯¹å°æ ·æœ¬ç¯å¢ƒä¸‹é«˜å…‰è°±å›¾åƒä¸­æœªçŸ¥ç±»åˆ«çš„å‘ç°ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŸå‹å­¦ä¹ å’Œèšç±»çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å°‘é‡æ ‡è®°æ ·æœ¬ï¼ŒåŠªåŠ›å¼€å‘æ¨æ–­æœªçŸ¥ç±»åˆ«åŸå‹çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä»å·²çŸ¥ç±»åˆ«ä¸­åŒºåˆ†æœªçŸ¥ç±»åˆ«ã€‚å½“æœªçŸ¥ç±»åˆ«æ ·æœ¬è¢«å­¦ä¹ åˆ°çš„å·²çŸ¥ç±»åˆ«åˆ†ç±»å™¨æ‹’ç»åï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æ ¹æ®å…¶ä¸æ¨æ–­å‡ºçš„æœªçŸ¥ç±»åˆ«åŸå‹çš„è·ç¦»ï¼Œè¿›ä¸€æ­¥å°†æœªçŸ¥ç±»åˆ«æ ·æœ¬èšç±»åˆ°ä¸åŒçš„ç±»åˆ«ä¸­ã€‚ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å››ä¸ªåŸºå‡†HSIæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¼€æ”¾é›†å°æ ·æœ¬HSIåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ‰€æœ‰ä»£ç éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KOBEN-ff/OpenFUCD-main%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KOBEN-ff/OpenFUCD-mainä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18075v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å¼€æ”¾é›†è¶…å…‰è°±å›¾åƒï¼ˆHSIï¼‰åˆ†ç±»ä»»åŠ¡ä¸­æ ·æœ¬ç±»åˆ«æœªçŸ¥çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŸå‹å­¦ä¹ å’Œèšç±»çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å°‘æ•°æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œæ¨æ–­å‡ºæœªçŸ¥ç±»åˆ«çš„åŸå‹ï¼ŒåŒæ—¶å°†æœªçŸ¥ç±»åˆ«ä¸å·²çŸ¥ç±»åˆ«åŒºåˆ†å¼€ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼€æ”¾é›†å°‘æ ·æœ¬HSIåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾é›†HSIåˆ†ç±»é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åŒºåˆ†å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«æ ·æœ¬ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾§é‡äºæ‹’ç»æœªçŸ¥ç±»åˆ«æ ·æœ¬ä»¥æé«˜è¯†åˆ«å‡†ç¡®ç‡ï¼Œä½†ç¼ºä¹å¯¹æœªçŸ¥ç±»åˆ«çš„è¿›ä¸€æ­¥è¯†åˆ«æˆ–å‘ç°ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŸå‹å­¦ä¹ å’Œèšç±»çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹å‘ç°HSIä¸­çš„æœªçŸ¥ç±»åˆ«ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿé€šè¿‡å°‘é‡æ ‡æ³¨æ ·æœ¬æ¨æ–­æœªçŸ¥ç±»åˆ«çš„åŸå‹ï¼ŒåŒæ—¶åŒºåˆ†æœªçŸ¥ç±»åˆ«ä¸å·²çŸ¥ç±»åˆ«ã€‚</li>
<li>é€šè¿‡å¯¹å››ä¸ªåŸºå‡†HSIæ•°æ®é›†è¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨å¼€æ”¾é›†å°‘æ ·æœ¬HSIåˆ†ç±»ä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æ‰€æœ‰ä»£ç å‡å¯åœ¨æŒ‡å®šçš„GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b8b9df54492fb5ac968c05f57f606dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14a644a9b0ace6fd05f7d9809d1f35b6.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-650026a5c03df615d42f14e98ed1734a.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-92177ef149b183713613682d41bfdc56.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  EvoEmo Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
