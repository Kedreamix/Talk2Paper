<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Delta Activations A Representation for Finetuned Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-4db78834eecb646e4bf81ec3d82f77ec.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models"><a href="#Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models" class="headerlink" title="Delta Activations: A Representation for Finetuned Large Language Models"></a>Delta Activations: A Representation for Finetuned Large Language Models</h2><p><strong>Authors:Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim</strong></p>
<p>The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/OscarXZQ/delta_activations">https://github.com/OscarXZQ/delta_activations</a>. </p>
<blockquote>
<p>å¼ºå¤§çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸä½¿å¾—ç¤¾åŒºèƒ½å¤Ÿåˆ›å»ºå¤§é‡é€‚åº”ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸçš„åè®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºå…ƒæ•°æ®ä¸ä¸€è‡´å’Œéç»“æ„åŒ–ä»“åº“ï¼Œå¯¼èˆªå’Œç†è§£è¿™äº›æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†Delta Activationsï¼Œä¸€ç§é€šè¿‡æµ‹é‡ç›¸å¯¹äºåŸºç¡€æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»çš„å˜åŒ–æ¥è¡¨ç¤ºå¾®è°ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œå¹¶å°†å…¶è¡¨ç¤ºä¸ºå‘é‡åµŒå…¥ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•å…è®¸æŒ‰é¢†åŸŸå’Œä»»åŠ¡è¿›è¡Œæœ‰æ•ˆèšç±»ï¼Œæ­ç¤ºæ¨¡å‹æ™¯è§‚ä¸­çš„ç»“æ„ã€‚Delta Activationsè¿˜æ˜¾ç¤ºå‡ºä»¤äººæ»¡æ„çš„å±æ€§ï¼šå®ƒåœ¨å¾®è°ƒè®¾ç½®ä¸­æ˜¯ç¨³å¥çš„ï¼Œå¹¶ä¸”åœ¨å¾®è°ƒæ•°æ®é›†æ··åˆæ—¶è¡¨ç°å‡ºç´¯åŠ å±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†Delta Activationså¯ä»¥é€šè¿‡å°‘é‡çš„å¾®è°ƒæ¥åµŒå…¥ä»»åŠ¡ï¼Œå¹¶è¿›ä¸€æ­¥æ¢ç´¢å…¶åœ¨æ¨¡å‹é€‰æ‹©å’Œåˆå¹¶ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å¸Œæœ›Delta Activationsèƒ½å¤Ÿä¿ƒè¿›é‡å¤ä½¿ç”¨å…¬å¼€æ¨¡å‹çš„åšæ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/OscarXZQ/delta_activations%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/OscarXZQ/delta_activationsæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04442v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Delta Activationsæ–¹æ³•ï¼Œé€šè¿‡æµ‹é‡å¾®è°ƒæ¨¡å‹ç›¸å¯¹äºåŸºç¡€æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»å˜åŒ–ï¼Œå°†å…¶è¡¨ç¤ºä¸ºå‘é‡åµŒå…¥ã€‚æ­¤æ–¹æ³•å¯ç”¨äºæœ‰æ•ˆåœ°æŒ‰é¢†åŸŸå’Œä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡Œèšç±»ï¼Œæ­ç¤ºæ¨¡å‹æ™¯è§‚ä¸­çš„ç»“æ„ã€‚Delta Activationså…·æœ‰ç¨³å¥æ€§å’Œå¯å åŠ æ€§ï¼Œå¹¶èƒ½åµŒå…¥é€šè¿‡å°‘é‡å¾®è°ƒå®ç°çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ¢è®¨äº†Delta Activationsåœ¨æ¨¡å‹é€‰æ‹©å’Œåˆå¹¶ä¸­çš„åº”ç”¨ï¼Œå¹¶å¸Œæœ›é€šè¿‡å®ƒä¿ƒè¿›å…¬å¼€æ¨¡å‹çš„å¤ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Delta Activationsæ˜¯ä¸€ç§æµ‹é‡å¾®è°ƒæ¨¡å‹ä¸åŸºç¡€æ¨¡å‹ä¹‹é—´å†…éƒ¨æ¿€æ´»å˜åŒ–çš„æ–¹æ³•ï¼Œç”¨äºè¡¨ç¤ºæ¨¡å‹å‘é‡åµŒå…¥ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æŒ‰é¢†åŸŸå’Œä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡Œèšç±»ï¼Œæ­ç¤ºæ¨¡å‹æ™¯è§‚çš„ç»“æ„ã€‚</li>
<li>Delta Activationså…·æœ‰ç¨³å¥æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„å¾®è°ƒè®¾ç½®ä¸‹è¡¨ç°è‰¯å¥½ã€‚</li>
<li>å½“å¾®è°ƒæ•°æ®é›†æ··åˆæ—¶ï¼ŒDelta Activationså±•ç°å‡ºå¯å åŠ çš„ç‰¹æ€§ã€‚</li>
<li>Delta Activationså¯ä»¥é€šè¿‡å¾®è°ƒåµŒå…¥ä»»åŠ¡ï¼Œä¸ºæ¨¡å‹é€‰æ‹©å’Œåˆå¹¶æä¾›æœ‰åŠ›çš„æ”¯æŒã€‚</li>
<li>é€šè¿‡GitHubä¸Šçš„ä»£ç èµ„æºï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/OscarXZQ/delta_activations%EF%BC%89%EF%BC%8C%E7%A0%94%E7%A9%B6%E8%80%85%E5%8F%AF%E4%BB%A5%E6%9B%B4%E5%A5%BD%E5%9C%B0%E7%90%86%E8%A7%A3%E5%92%8C%E5%BA%94%E7%94%A8Delta">https://github.com/OscarXZQ/delta_activationsï¼‰ï¼Œç ”ç©¶è€…å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨Delta</a> Activationsæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8adf90d00afc453c7439301c7c3164d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da384e9c2f4887723b38eee8b93ecfcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ab378fb1d9c7de550bb06fdeeae4410.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19072fa45112b139700171bf76fb15fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e37f715a0d9b0171777c218e8cff9095.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78d74f1c2f2b010766714140f6023cb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0236bc1379c89c847bf7557d645ae2d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ArcMemo-Abstract-Reasoning-Composition-with-Lifelong-LLM-Memory"><a href="#ArcMemo-Abstract-Reasoning-Composition-with-Lifelong-LLM-Memory" class="headerlink" title="ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory"></a>ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory</h2><p><strong>Authors:Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Zhijian Liu, Zhiting Hu, Lianhui Qin</strong></p>
<p>While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query&#x2F;response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. On the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, we confirm that dynamically updating memory during test-time outperforms an otherwise identical fixed memory setting with additional attempts, supporting the hypothesis that solving more problems and abstracting more patterns to memory enables further solutions in a form of self-improvement. Code available at <a target="_blank" rel="noopener" href="https://github.com/matt-seb-ho/arc_memo">https://github.com/matt-seb-ho/arc_memo</a>. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œæ¨ç†æ—¶é—´ç¼©æ”¾ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œè¶Šæ¥è¶Šé•¿å’Œå¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨ç†è¿‡ç¨‹ä¸­å‘ç°çš„æ¨¡å¼å’Œè§è§£ä¼šåœ¨ä¸Šä¸‹æ–‡çª—å£é‡ç½®ä¸ºæ–°çš„æŸ¥è¯¢æ—¶ç«‹å³è¢«ä¸¢å¼ƒã€‚å¤–éƒ¨è®°å¿†æ˜¯ä¿æŒè¿™äº›å‘ç°çš„ä¸€ç§è‡ªç„¶æ–¹å¼ï¼Œæœ€è¿‘çš„å·¥ä½œå·²ç»è¯æ˜äº†è¿™ä¸€ç‚¹å¯¹äºå¯†é›†å‹ä»»åŠ¡æœ‰æ˜æ˜¾çš„ç›Šå¤„ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé€šè¿‡è¶…è¶ŠåŸºäºå®ä¾‹çš„è®°å¿†æ¡ç›®ï¼ˆä¾‹å¦‚ï¼Œç²¾ç¡®çš„æŸ¥è¯¢&#x2F;å“åº”å¯¹æˆ–ä¸åŸå§‹é—®é¢˜ä¸Šä¸‹æ–‡ç´§å¯†è€¦åˆçš„æ‘˜è¦ï¼‰ï¼Œæœç€æ¦‚å¿µå±‚é¢çš„è®°å¿†å‘å±•ï¼Œå­˜åœ¨æ›´å¹¿æ³›åœ°ä½¿ç”¨å’Œæ‰©å±•è¿™äº›è®°å¿†çš„æœºä¼šã€‚æ¦‚å¿µå±‚é¢çš„è®°å¿†æ˜¯å¯é‡å¤ä½¿ç”¨çš„æ¨¡å—åŒ–æŠ½è±¡ï¼Œä»è§£å†³æ–¹æ¡ˆè½¨è¿¹ä¸­æç‚¼å¹¶å­˜å‚¨åœ¨è‡ªç„¶è¯­è¨€ä¸­ã€‚å¯¹äºæœªæ¥çš„æŸ¥è¯¢ï¼Œæœ‰é€‰æ‹©åœ°æ£€ç´¢ç›¸å…³çš„æ¦‚å¿µå¹¶å°†å…¶é›†æˆåˆ°æç¤ºä¸­ï¼Œå®ç°æµ‹è¯•æ—¶çš„æŒç»­å­¦ä¹ è€Œæ— éœ€æƒé‡æ›´æ–°ã€‚æˆ‘ä»¬çš„è®¾è®¡å¼•å…¥äº†ä»æ»šåŠ¨ä¸­æå–æŠ½è±¡æˆæœå’Œä¸ºæ–°æŸ¥è¯¢æ£€ç´¢æ¡ç›®çš„æ–°ç­–ç•¥ï¼Œä¿ƒè¿›äº†é‡å¤ä½¿ç”¨å¹¶å…è®¸è®°å¿†éšç€æ›´å¤šç»éªŒè€Œæ‰©å±•ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ARC-AGIåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— è®°å¿†åŸºå‡†ä¸Šçš„ç›¸å¯¹å¢ç›Šä¸º7.5%ï¼Œå¹¶ä¸”éšç€æ¨ç†è®¡ç®—çš„å¢é•¿è€Œç»§ç»­æ‰©å±•æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°æŠ½è±¡æ¦‚å¿µæ˜¯æœ€ä¸€è‡´çš„è®°å¿†è®¾è®¡ï¼Œåœ¨æ‰€æœ‰æµ‹è¯•çš„æ¨ç†è®¡ç®—è§„æ¨¡ä¸Šéƒ½è¶…è¿‡äº†åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯å®äº†æµ‹è¯•æœŸé—´åŠ¨æ€æ›´æ–°å†…å­˜ä¼˜äºå›ºå®šå†…å­˜è®¾ç½®ï¼Œæ”¯æŒè¿™æ ·çš„å‡è®¾ï¼šè§£å†³æ›´å¤šé—®é¢˜å¹¶å°†æ›´å¤šæ¨¡å¼æŠ½è±¡åˆ°å†…å­˜ä¸­ï¼Œèƒ½å¤Ÿåœ¨è‡ªæˆ‘æå‡çš„å½¢å¼ä¸­å®ç°è¿›ä¸€æ­¥çš„è§£å†³æ–¹æ¡ˆã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/matt-seb-ho/arc_memo%E4%B8%8A%E3%80%82">https://github.com/matt-seb-ho/arc_memoä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04439v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹é€šè¿‡æ¨ç†æ—¶é—´å°ºåº¦å¢å¼ºå…¶å¤„ç†æ—¥ç›Šå¤æ‚æ¨ç†è¿½è¸ªçš„èƒ½åŠ›ï¼Œç„¶è€Œè¿½è¸ªæœŸé—´æ‰€æ­éœ²çš„æ¨¡å¼ä¸æ´è§ä¼šéšç€ä¸Šä¸‹æ–‡çª—å£çš„é‡ç½®è€Œä¸¢å¤±ã€‚å¤–éƒ¨è®°å¿†æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„ä¸€ç§è‡ªç„¶é€”å¾„ï¼Œä¸”å¯¹äºå¯†é›†æ¨ç†ä»»åŠ¡æœ‰ç€æ˜æ˜¾çš„ç›Šå¤„ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé€šè¿‡å®ä¾‹ä¸ºåŸºç¡€çš„è®°å¿†æ¡ç›®è½¬å‘æ¦‚å¿µå±‚æ¬¡çš„è®°å¿†æ˜¯ä¸€ä¸ªæœºä¼šï¼Œæ¦‚å¿µå±‚æ¬¡çš„è®°å¿†èƒ½å¤Ÿæç‚¼è§£å†³æ–¹æ¡ˆè¿½è¸ªä¸­çš„å¯å¤ç”¨æ¨¡å—åŒ–æŠ½è±¡å¹¶ä»¥è‡ªç„¶è¯­è¨€è¿›è¡Œå­˜å‚¨ã€‚å¯¹äºæœªæ¥çš„æŸ¥è¯¢ï¼Œç›¸å…³æ¦‚å¿µå°†è¢«é€‰æ‹©æ€§æ£€ç´¢å¹¶æ•´åˆåˆ°æç¤ºä¸­ï¼Œå®ç°æµ‹è¯•æ—¶é—´çš„æŒç»­å­¦ä¹ è€Œæ— éœ€æƒé‡æ›´æ–°ã€‚æˆ‘ä»¬çš„è®¾è®¡å¼•å…¥äº†åœ¨ç­–ç•¥æ‰§è¡Œæ—¶æŠ½è±¡è®°å½•å…³é”®ç‚¹åŠæ£€ç´¢æ–°æŸ¥è¯¢æ—¶è¯†åˆ«ç›¸åº”æ¡ç›®ç‚¹çš„ç­–ç•¥ï¼Œæé«˜äº†é‡ç”¨çš„å¯èƒ½å¹¶ä½¿è®°å¿†å®¹é‡å¾—ä»¥æ‰©å¤§åº”å¯¹ç´¯ç§¯ç»éªŒã€‚åœ¨å……æ»¡æŒ‘æˆ˜çš„ARC-AGIåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿æ€§èƒ½ä¸”æ— ä½¿ç”¨å†…å­˜åŠŸèƒ½çš„å¤§æ¨¡å‹æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯å¯ä»¥å®ç°7.5%ç›¸å¯¹å¢é•¿ï¼›æ€§èƒ½è¿˜èƒ½åœ¨æ¨æ–­è®¡ç®—è§„æ¨¡æ‰©å±•çš„åŒæ—¶å¾—åˆ°ä¸æ–­æå‡ã€‚æˆ‘ä»¬è¿˜å‘ç°åŠ¨æ€æ›´æ–°å†…å­˜æ›´èƒ½ä¿ƒè¿›é—®é¢˜æ±‚è§£èƒ½åŠ›çš„æå‡ä¸è§£å†³æ–¹æ¡ˆçš„åˆ›æ–°å½¢å¼çš„å‡ºç°ï¼Œæ­¤çŒœæƒ³äº¦è·å¾—å¯¹ç›¸åº”æ§åˆ¶ç»„çš„é¢å¤–éªŒè¯æ”¯æŒã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šï¼š<a target="_blank" rel="noopener" href="https://github.com/matt-seb-ho/arc_memo">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>å…³é”®å‘ç°ç‚¹</strong>ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5eaefe3af829e5f48a0c12b830a59fc6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-191c291c50901f8d16b9f54e07cf6e1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9687ddb2b5c253c0e1a9540f214b0f09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f5d6676e09012452ed72ed8ca38f572.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39553be60f6b95db045cabe89bf9455d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="No-Thoughts-Just-AI-Biased-LLM-Recommendations-Limit-Human-Agency-in-Resume-Screening"><a href="#No-Thoughts-Just-AI-Biased-LLM-Recommendations-Limit-Human-Agency-in-Resume-Screening" class="headerlink" title="No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in   Resume Screening"></a>No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in   Resume Screening</h2><p><strong>Authors:Kyra Wilson, Mattea Sim, Anna-Maria Gueorguieva, Aylin Caliskan</strong></p>
<p>In this study, we conduct a resume-screening experiment (N&#x3D;528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. Simulated AI bias approximates factual and counterfactual estimates of racial bias in real-world AI systems. We investigate peopleâ€™s preferences for White, Black, Hispanic, and Asian candidates (represented through names and affinity groups on quality-controlled resumes) across 1,526 scenarios and measure their unconscious associations between race and status using implicit association tests (IATs), which predict discriminatory hiring decisions but have not been investigated in human-AI collaboration. When making decisions without AI or with AI that exhibits no race-based preferences, people select all candidates at equal rates. However, when interacting with AI favoring a particular group, people also favor those candidates up to 90% of the time, indicating a significant behavioral shift. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. Finally, even if people think AI recommendations are low quality or not important, their decisions are still vulnerable to AI bias under certain circumstances. This work has implications for peopleâ€™s autonomy in AI-HITL scenarios, AI and work, design and evaluation of AI hiring systems, and strategies for mitigating bias in collaborative decision-making tasks. In particular, organizational and regulatory policy should acknowledge the complex nature of AI-HITL decision making when implementing these systems, educating people who use them, and determining which are subject to oversight. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹ç®€å†ç­›é€‰å®éªŒï¼ˆN&#x3D;528ï¼‰ï¼Œäººä»¬ä¸æ¨¡æ‹Ÿäººå·¥æ™ºèƒ½æ¨¡å‹åä½œï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºåŸºäºç§æ—çš„é€‰æ‹©åå¥½ï¼ˆåè§ï¼‰ï¼Œä»¥å¯¹é«˜ä½åœ°ä½çš„16ç§èŒä¸šå€™é€‰äººè¿›è¡Œè¯„ä¼°ã€‚æ¨¡æ‹Ÿäººå·¥æ™ºèƒ½åè§è¿‘ä¼¼äºç°å®ä¸–ç•Œä¸­äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å®é™…å’Œå‡è®¾ç§æ—åè§çš„ä¼°è®¡å€¼ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†äººä»¬å¯¹ç™½äººã€é»‘äººã€è¥¿ç­ç‰™è£”å’Œäºšæ´²è£”å€™é€‰äººï¼ˆé€šè¿‡è´¨é‡æ§åˆ¶çš„ç®€å†ä¸­çš„å§“åå’Œäº²å’Œå›¢ä½“è¡¨ç¤ºï¼‰çš„åå¥½ï¼Œè·¨è¶Š1526ä¸ªåœºæ™¯ï¼Œå¹¶ä½¿ç”¨éšæ€§å…³è”æµ‹è¯•ï¼ˆIATsï¼‰æµ‹é‡ä»–ä»¬å…³äºç§æ—å’Œåœ°ä½çš„æ½œåœ¨å…³è”ã€‚éšæ€§å…³è”æµ‹è¯•èƒ½å¤Ÿé¢„æµ‹æ­§è§†æ€§çš„æ‹›è˜å†³ç­–ï¼Œä½†åœ¨äººç±»ä¸äººå·¥æ™ºèƒ½åä½œä¸­å°šæœªå¾—åˆ°ç ”ç©¶ã€‚åœ¨æ²¡æœ‰äººå·¥æ™ºèƒ½è¾…åŠ©æˆ–ä¸æ²¡æœ‰ç§æ—åå¥½çš„äººå·¥æ™ºèƒ½åä½œæ—¶ï¼Œäººä»¬ä»¥å¹³ç­‰çš„æ–¹å¼é€‰æ‹©æ‰€æœ‰å€™é€‰äººã€‚ç„¶è€Œï¼Œåœ¨ä¸åçˆ±æŸä¸€ç¾¤ä½“çš„äººå·¥æ™ºèƒ½äº¤äº’æ—¶ï¼Œäººä»¬ä¹Ÿä¼šåçˆ±è¿™äº›å€™é€‰äººé«˜è¾¾90%çš„æ—¶é—´ï¼Œè¿™è¡¨æ˜è¡Œä¸ºå‘ç”Ÿäº†é‡å¤§å˜åŒ–ã€‚å¦‚æœäººä»¬åœ¨ç­›é€‰ç®€å†ä¹‹å‰å®Œæˆä¸€é¡¹éšæ€§å…³è”æµ‹è¯•ï¼Œé‚£ä¹ˆé€‰æ‹©èº«ä»½ä¸ç¬¦åˆå¸¸è§ç§æ—åœ°ä½åˆ»æ¿å°è±¡çš„å€™é€‰äººçš„å¯èƒ½æ€§ä¼šå¢åŠ 13%ã€‚å³ä½¿äººä»¬è®¤ä¸ºäººå·¥æ™ºèƒ½çš„å»ºè®®è´¨é‡ä½ä¸‹æˆ–ä¸é‡è¦ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä»–ä»¬çš„å†³ç­–ä»ç„¶ä¼šå—åˆ°äººå·¥æ™ºèƒ½åè§çš„å½±å“ã€‚è¿™é¡¹å·¥ä½œå¯¹äººå·¥æ™ºèƒ½è¾…åŠ©å†³ç­–åœºæ™¯ã€äººå·¥æ™ºèƒ½ä¸å·¥ä½œã€äººå·¥æ™ºèƒ½æ‹›è˜ç³»ç»Ÿçš„è®¾è®¡ä¸è¯„ä¼°ä»¥åŠåä½œå†³ç­–ä»»åŠ¡ä¸­åè§ç¼“è§£ç­–ç•¥å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚ç‰¹åˆ«æ˜¯ç»„ç»‡å’Œç›‘ç®¡æ”¿ç­–åœ¨å®æ–½è¿™äº›ç³»ç»Ÿã€æ•™è‚²ä½¿ç”¨è€…å¹¶ç¡®å®šå“ªäº›ç³»ç»Ÿéœ€è¦ç›‘ç®¡æ—¶ï¼Œåº”æ‰¿è®¤äººå·¥æ™ºèƒ½è¾…åŠ©å†³ç­–å¤æ‚æ€§çš„æœ¬è´¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04404v1">PDF</a> Published in Proceedings of the 2025 AAAI&#x2F;ACM Conference on AI,   Ethics, and Society; code available at   <a target="_blank" rel="noopener" href="https://github.com/kyrawilson/No-Thoughts-Just-AI">https://github.com/kyrawilson/No-Thoughts-Just-AI</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶é€šè¿‡ä¸€é¡¹åŒ…å«528åå‚ä¸è€…çš„ç®€å†ç­›é€‰å®éªŒï¼Œæ¢è®¨äº†äººç±»åœ¨ä¸æ¨¡æ‹Ÿç§æ—åå¥½å‹AIæ¨¡å‹åˆä½œæ—¶ï¼Œå¯¹æ¨¡æ‹ŸAIæ¨¡å‹æ¨èçš„å€™é€‰äººçš„åå¥½æƒ…å†µã€‚è¿™äº›å€™é€‰äººåˆ†åˆ«ä»£è¡¨ç™½äººã€é»‘äººã€è¥¿ç­ç‰™è£”å’Œäºšæ´²è£”çš„é«˜è´¨é‡ç®€å†ã€‚å®éªŒå‘ç°ï¼Œå½“AIæ¨¡å‹è¡¨ç°å‡ºç§æ—åå¥½æ—¶ï¼Œäººç±»åœ¨ä¸AIåˆä½œæ—¶çš„å†³ç­–è¡Œä¸ºä¼šå‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œå€¾å‘é€‰æ‹©ä¸AIæ¨¡å‹æ¨èä¸€è‡´çš„å€™é€‰äººã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®éªŒè¿˜å‘ç°å®Œæˆæ— æ„è¯†å…³è”æµ‹è¯•åçš„äººæ›´èƒ½æŠµåˆ¶åˆ»æ¿å°è±¡ï¼Œåšå‡ºæ›´ç¬¦åˆå…¬æ­£é€‰æ‹©çš„è¡Œä¸ºã€‚å³ä½¿äººä»¬è®¤ä¸ºAIæ¨èä¸é‡è¦æˆ–è´¨é‡ä½åŠ£ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ä»ä¼šå—åˆ°AIåè§çš„å½±å“ã€‚è¿™é¡¹ç ”ç©¶å¯¹äºäººå·¥æ™ºèƒ½ä¸å·¥ä½œçš„å…³ç³»ã€AIæ‹›è˜ç³»ç»Ÿçš„è®¾è®¡è¯„ä¼°ä»¥åŠåä½œå†³ç­–ä»»åŠ¡ä¸­åè§ç¼“è§£ç­–ç•¥å…·æœ‰é‡è¦æ„ä¹‰ã€‚å°¤å…¶æ˜¯ç»„ç»‡æ”¿ç­–å’Œç›‘ç®¡ç­–ç•¥åº”è€ƒè™‘åˆ°äººå·¥æ™ºèƒ½è¾…åŠ©å†³ç­–ç¯å¢ƒçš„å¤æ‚æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¨¡æ‹ŸAIç§æ—åè§å¯é¢„æµ‹äººç±»åœ¨ä¸AIåˆä½œæ—¶çš„ç§æ—åå¥½è¡Œä¸ºã€‚</li>
<li>å½“AIè¡¨ç°å‡ºç§æ—åå¥½æ—¶ï¼Œäººç±»å€¾å‘äºé€‰æ‹©ä¸AIæ¨èä¸€è‡´çš„å€™é€‰äººã€‚</li>
<li>é€šè¿‡æ— æ„è¯†å…³è”æµ‹è¯•è®­ç»ƒçš„äººç±»èƒ½æ›´å¥½åœ°æŠµåˆ¶ç§æ—åˆ»æ¿å°è±¡å¹¶åšå‡ºæ›´å…¬æ­£çš„å†³ç­–ã€‚</li>
<li>AIçš„åè§å¯èƒ½ä¼šå½±å“äººä»¬å¯¹è‡ªèº«å†³ç­–ç‹¬ç«‹æ€§çš„è¯„ä¼°ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹ä¿¡æ¯å¯¹æ¯”æ—¶ã€‚</li>
<li>AIæ¨¡å‹çš„ç§æ—åè§ä¼šå¯¹äººåŠ›èµ„æºå†³ç­–çš„å…¬å¹³æ€§å’Œæœ‰æ•ˆæ€§äº§ç”Ÿå½±å“ã€‚</li>
<li>åœ¨AIæ‹›è˜ç³»ç»Ÿçš„è®¾è®¡å’Œè¯„ä¼°ä¸­ï¼Œåº”è€ƒè™‘åˆ°AIæ¨¡å‹çš„ç§æ—åè§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0552047d5721cf44aa040359d030892a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f9bb70e617c65ed2114037f19c170bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e12c0956d274c8a95f01a0e4362242d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd0facf6de80b4cf25b91b7af6f99da5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e89c15c81f624750c083e60a84b5c182.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Aesthetic-Image-Captioning-with-Saliency-Enhanced-MLLMs"><a href="#Aesthetic-Image-Captioning-with-Saliency-Enhanced-MLLMs" class="headerlink" title="Aesthetic Image Captioning with Saliency Enhanced MLLMs"></a>Aesthetic Image Captioning with Saliency Enhanced MLLMs</h2><p><strong>Authors:Yilin Tao, Jiashui Huang, Huaze Xu, Ling Shao</strong></p>
<p>Aesthetic Image Captioning (AIC) aims to generate textual descriptions of image aesthetics, becoming a key research direction in the field of computational aesthetics. In recent years, pretrained Multimodal Large Language Models (MLLMs) have advanced rapidly, leading to a significant increase in image aesthetics research that integrates both visual and textual modalities. However, most existing studies on image aesthetics primarily focus on predicting aesthetic ratings and have shown limited application in AIC. Existing AIC works leveraging MLLMs predominantly rely on fine-tuning methods without specifically adapting MLLMs to focus on target aesthetic content. To address this limitation, we propose the Aesthetic Saliency Enhanced Multimodal Large Language Model (ASE-MLLM), an end-to-end framework that explicitly incorporates aesthetic saliency into MLLMs. Within this framework, we introduce the Image Aesthetic Saliency Module (IASM), which efficiently and effectively extracts aesthetic saliency features from images. Additionally, we design IAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency features with original image features via a cross-attention mechanism. To the best of our knowledge, ASE-MLLM is the first framework to integrate image aesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments demonstrated that our approach significantly outperformed traditional methods and generic MLLMs on current mainstream AIC benchmarks, achieving state-of-the-art (SOTA) performance. </p>
<blockquote>
<p>ç¾å­¦å›¾åƒæ ‡é¢˜ç”Ÿæˆï¼ˆAICï¼‰æ—¨åœ¨ç”Ÿæˆå›¾åƒç¾å­¦æè¿°çš„æ–‡æœ¬ï¼Œå·²æˆä¸ºè®¡ç®—ç¾å­¦é¢†åŸŸçš„å…³é”®ç ”ç©¶æ–¹å‘ã€‚è¿‘å¹´æ¥ï¼Œé¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å‘å±•è¿…é€Ÿï¼Œæå¤§åœ°æ¨åŠ¨äº†å›¾åƒç¾å­¦ç ”ç©¶çš„å‘å±•ï¼Œè¯¥é¢†åŸŸçš„ç ”ç©¶å®ç°äº†è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„é›†æˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„å›¾åƒç¾å­¦ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é¢„æµ‹ç¾å­¦è¯„åˆ†ä¸Šï¼Œåœ¨AICä¸­çš„åº”ç”¨æœ‰é™ã€‚ç°æœ‰çš„åˆ©ç”¨MLLMçš„AICå·¥ä½œä¸»è¦ä¾èµ–äºå¾®è°ƒæ–¹æ³•ï¼Œè€Œæ²¡æœ‰ä¸“é—¨è°ƒæ•´MLLMä»¥ä¸“æ³¨äºç›®æ ‡ç¾å­¦å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç¾å­¦æ˜¾è‘—æ€§å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆASE-MLLMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œæ˜¾å¼åœ°å°†ç¾å­¦æ˜¾è‘—æ€§çº³å…¥MLLMä¸­ã€‚åœ¨æ­¤æ¡†æ¶å†…ï¼Œæˆ‘ä»¬å¼•å…¥äº†å›¾åƒç¾å­¦æ˜¾è‘—æ€§æ¨¡å—ï¼ˆIASMï¼‰ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿé«˜æ•ˆåœ°ä»å›¾åƒä¸­æå–ç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†IAS-ViTä½œä¸ºMLLMçš„å›¾åƒç¼–ç å™¨ï¼Œè¯¥æ¨¡å—é€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶å°†ç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾ä¸åŸå§‹å›¾åƒç‰¹å¾ç›¸èåˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒASE-MLLMæ˜¯ç¬¬ä¸€ä¸ªå°†å›¾åƒç¾å­¦æ˜¾è‘—æ€§èå…¥MLLMçš„æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯ç”¨äºAICä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å½“å‰ä¸»æµçš„AICåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œé€šç”¨MLLMï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04378v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ç¾å­¦å›¾åƒæ ‡æ³¨ï¼ˆAICï¼‰çš„ç ”ç©¶æ–¹å‘ï¼Œé‡ç‚¹é˜è¿°äº†é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒç¾å­¦ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆASE-MLLMï¼‰ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†å›¾åƒç¾å­¦æ˜¾è‘—æ€§æ¨¡å—ï¼ˆIASMï¼‰å’ŒIAS-ViTå›¾åƒç¼–ç å™¨ï¼Œä»¥é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆåœ°æå–å’Œèåˆç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾å’ŒåŸå§‹å›¾åƒç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»æµç¾å­¦å›¾åƒæ ‡æ³¨åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œé€šç”¨MLLMsï¼Œå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç¾å­¦å›¾åƒæ ‡æ³¨ï¼ˆAICï¼‰æ˜¯è®¡ç®—ç¾å­¦é¢†åŸŸçš„ä¸€ä¸ªå…³é”®ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ç”Ÿæˆå¯¹å›¾åƒç¾å­¦çš„æ–‡æœ¬æè¿°ã€‚</li>
<li>é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒç¾å­¦ç ”ç©¶ä¸­è¿…é€Ÿå‘å±•ï¼Œç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ã€‚</li>
<li>ç°æœ‰å›¾åƒç¾å­¦ç ”ç©¶ä¸»è¦å…³æ³¨é¢„æµ‹ç¾å­¦è¯„åˆ†ï¼Œåœ¨AICä¸­çš„åº”ç”¨æœ‰é™ã€‚</li>
<li>ç°æœ‰åˆ©ç”¨MLLMsçš„AICå·¥ä½œä¸»è¦ä¾èµ–å¾®è°ƒæ–¹æ³•ï¼Œä½†æœªä¸“é—¨é’ˆå¯¹ç›®æ ‡ç¾å­¦å†…å®¹è¿›è¡Œé€‚åº”ã€‚</li>
<li>æå‡ºäº†ç»“åˆç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆASE-MLLMï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶çš„å±€é™æ€§ã€‚</li>
<li>ASE-MLLMå¼•å…¥äº†å›¾åƒç¾å­¦æ˜¾è‘—æ€§æ¨¡å—ï¼ˆIASMï¼‰å’ŒIAS-ViTå›¾åƒç¼–ç å™¨ï¼Œä»¥æå–å’Œèåˆç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾å’ŒåŸå§‹å›¾åƒç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a67da6ce682ab685d0dddb84d7d9cbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-035bc289a0d85b0f35a4d5cce242da4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28adab153a4bfea00172e9611afe2960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebd4e555d0e3e86a2f228c42901fcd51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08ec77594dbb633c31f79e76cde9f5a9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Efficient-Odd-One-Out-Anomaly-Detection"><a href="#Efficient-Odd-One-Out-Anomaly-Detection" class="headerlink" title="Efficient Odd-One-Out Anomaly Detection"></a>Efficient Odd-One-Out Anomaly Detection</h2><p><strong>Authors:Silvio Chito, Paolo Rabino, Tatiana Tommasi</strong></p>
<p>The recently introduced odd-one-out anomaly detection task involves identifying the odd-looking instances within a multi-object scene. This problem presents several challenges for modern deep learning models, demanding spatial reasoning across multiple views and relational reasoning to understand context and generalize across varying object categories and layouts. We argue that these challenges must be addressed with efficiency in mind. To this end, we propose a DINO-based model that reduces the number of parameters by one third and shortens training time by a factor of three compared to the current state-of-the-art, while maintaining competitive performance. Our experimental evaluation also introduces a Multimodal Large Language Model baseline, providing insights into its current limitations in structured visual reasoning tasks. The project page can be found at <a target="_blank" rel="noopener" href="https://silviochito.github.io/EfficientOddOneOut/">https://silviochito.github.io/EfficientOddOneOut/</a> </p>
<blockquote>
<p>æœ€è¿‘å¼•å…¥çš„å¥‡å¶å¼‚å¸¸æ£€æµ‹ä»»åŠ¡æ¶‰åŠåœ¨å¤šç›®æ ‡åœºæ™¯å†…è¯†åˆ«å‡ºå¤–è§‚å¥‡ç‰¹çš„å®ä¾‹ã€‚æ­¤é—®é¢˜ä¸ºç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹å¸¦æ¥äº†å‡ ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦è¿›è¡Œè·¨å¤šä¸ªè§†è§’çš„ç©ºé—´æ¨ç†ä»¥åŠç†è§£ä¸Šä¸‹æ–‡å¹¶æ¨å¹¿ä¸åŒå¯¹è±¡ç±»åˆ«å’Œå¸ƒå±€çš„å…³ç³»æ¨ç†ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨è€ƒè™‘åˆ°æ•ˆç‡çš„æƒ…å†µä¸‹å¿…é¡»è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºDINOçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼Œå‚æ•°æ•°é‡å‡å°‘äº†ä¸‰åˆ†ä¹‹ä¸€ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­äº†ä¸‰å€ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¿˜å¼•å…¥äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†çº¿ï¼Œæä¾›äº†å…¶åœ¨ç»“æ„åŒ–è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„å½“å‰å±€é™æ€§çš„è§è§£ã€‚é¡¹ç›®é¡µé¢ä½äºï¼š[<a target="_blank" rel="noopener" href="https://silviochito.github.io/EfficientOddOneOut/]">https://silviochito.github.io/EfficientOddOneOut/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04326v1">PDF</a> Accepted at ICIAP 2025</p>
<p><strong>Summary</strong></p>
<p>æ–°æå‡ºçš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡è¦æ±‚åœ¨å¤šç›®æ ‡åœºæ™¯ä¸­å‘ç°å¤–è§‚å¥‡ç‰¹çš„å®ä¾‹ã€‚è¿™é¡¹ä»»åŠ¡ç»™ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹å¸¦æ¥æŒ‘æˆ˜ï¼Œè¦æ±‚åœ¨ä¸åŒè§†è§’ä¸‹è¿›è¡Œç©ºé—´æ¨ç†ï¼Œå¹¶ç»“åˆä¸Šä¸‹æ–‡ç†è§£è¿›è¡Œå…³ç³»æ¨ç†ï¼Œä»¥åº”å¯¹ä¸åŒå¯¹è±¡ç±»åˆ«å’Œå¸ƒå±€çš„å˜åŒ–ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºDINOçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å‚æ•°å‡å°‘ä¸‰åˆ†ä¹‹ä¸€ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­ä¸‰å€ï¼ŒåŒæ—¶ä¿æŒä¸å½“å‰æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å®éªŒè¯„ä¼°å¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åŸºçº¿ï¼Œå¹¶å¯¹å…¶ç›®å‰åœ¨ç»“æ„åŒ–è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥‡å¶æ ¡éªŒå¼‚å¸¸æ£€æµ‹ä»»åŠ¡è¦æ±‚è¯†åˆ«å¤šç›®æ ‡åœºæ™¯ä¸­çš„å¼‚å¸¸å®ä¾‹ã€‚</li>
<li>è¯¥ä»»åŠ¡å¯¹ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹æå‡ºäº†ç©ºé—´æ¨ç†å’Œå…³ç³»æ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºDINOçš„æ¨¡å‹ï¼Œå…·æœ‰é«˜æ•ˆçš„å‚æ•°å’Œè®­ç»ƒæ—¶é—´ã€‚</li>
<li>è¯¥æ¨¡å‹å‚æ•°å‡å°‘ä¸‰åˆ†ä¹‹ä¸€ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­ä¸‰å€ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯„ä¼°ä¸­å¼•å…¥äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åŸºçº¿ã€‚</li>
<li>è¯¥åŸºçº¿åœ¨ç»“æ„åŒ–è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c0b81cb78eabd3607de789acaf81f6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84766c39eb1e86fa85d90a3693dc4a67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89621f17d57fe39e6229dfc2606f853e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EvoEmo-Towards-Evolved-Emotional-Policies-for-LLM-Agents-in-Multi-Turn-Negotiation"><a href="#EvoEmo-Towards-Evolved-Emotional-Policies-for-LLM-Agents-in-Multi-Turn-Negotiation" class="headerlink" title="EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation"></a>EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation</h2><p><strong>Authors:Yunbo Long, Liming Xu, Lukas Beckenbauer, Yuhan Liu, Alexandra Brintrup</strong></p>
<p>Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \textit{complex}, \textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines â€“ vanilla strategies and fixed-emotion strategies â€“ for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation. </p>
<blockquote>
<p>å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†çš„æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œæ™ºèƒ½ä½“å¯ä»¥å‚ä¸å¤æ‚çš„å¤šè½®è°ˆåˆ¤ï¼Œä¸ºæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ™ºèƒ½ä½“åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†æƒ…ç»ªåœ¨è¿™ç§è°ˆåˆ¤ä¸­çš„åŠŸèƒ½ä½œç”¨ï¼Œè€Œæ˜¯äº§ç”Ÿè¢«åŠ¨ã€åå¥½é©±åŠ¨çš„æƒ…ç»ªååº”ï¼Œä½¿å®ƒä»¬å®¹æ˜“å—åˆ°å¯¹æ‰‹çš„æ“ä½œå’Œç­–ç•¥æ€§å‰¥å‰Šã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†EvoEmoï¼Œè¿™æ˜¯ä¸€ä¸ªè¿›åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥ä¼˜åŒ–è°ˆåˆ¤ä¸­çš„åŠ¨æ€æƒ…ç»ªè¡¨è¾¾ã€‚EvoEmoå°†æƒ…ç»ªçŠ¶æ€è½¬æ¢å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨åŸºäºç§ç¾¤çš„é—ä¼ ä¼˜åŒ–ç®—æ³•ï¼Œåœ¨å¤šç§è°ˆåˆ¤åœºæ™¯ä¸­æ¼”åŒ–é«˜å›æŠ¥çš„æƒ…ç»ªç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªåŸºå‡†çº¿â€”â€”æ™®é€šç­–ç•¥å’Œå›ºå®šæƒ…ç»ªç­–ç•¥â€”â€”ç”¨äºè¯„ä¼°æƒ…ç»ªæ„ŸçŸ¥è°ˆåˆ¤ã€‚å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒEvoEmoå§‹ç»ˆä¼˜äºè¿™ä¸¤ä¸ªåŸºå‡†çº¿ï¼Œå…·æœ‰æ›´é«˜çš„æˆåŠŸç‡ã€æ›´é«˜çš„æ•ˆç‡å’Œæ›´é«˜çš„ä¹°å®¶èŠ‚çœç‡ã€‚è¿™äº›å‘ç°çªæ˜¾äº†åœ¨å¤šè½®è°ˆåˆ¤ä¸­å¯ç”¨è‡ªé€‚åº”æƒ…ç»ªè¡¨è¾¾å¯¹äºä½¿LLMæ™ºèƒ½ä½“æ›´åŠ æœ‰æ•ˆçš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04310v1">PDF</a> </p>
<p><strong>Summary</strong><br>é“¾æ€ç»´æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨å·²ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè¿›è¡Œå¤æ‚çš„å¤šè½®è°ˆåˆ¤ï¼Œå¼€å¯äº†æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½çš„æ–°é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å¿½è§†äº†æƒ…ç»ªåœ¨è°ˆåˆ¤ä¸­çš„åŠŸèƒ½ä½œç”¨ï¼Œä»…äº§ç”Ÿè¢«åŠ¨ã€åå¥½é©±åŠ¨çš„æƒ…ç»ªååº”ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°å¯¹æ‰‹çš„æˆ˜ç•¥æ“çºµå’Œå‰¥å‰Šã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†EvoEmoï¼Œä¸€ä¸ªè¿›åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–è°ˆåˆ¤ä¸­çš„åŠ¨æ€æƒ…ç»ªè¡¨è¾¾ã€‚EvoEmoå°†æƒ…ç»ªçŠ¶æ€è½¬æ¢å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨åŸºäºç§ç¾¤çš„é—ä¼ ä¼˜åŒ–æ¥è¿›åŒ–ä¸åŒè°ˆåˆ¤åœºæ™¯ä¸‹çš„é«˜å›æŠ¥æƒ…ç»ªç­–ç•¥ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶åŒ…æ‹¬ä¸¤ç§åŸºçº¿ç­–ç•¥ï¼šæ™®é€šç­–ç•¥å’Œå›ºå®šæƒ…ç»ªç­–ç•¥ï¼Œç”¨äºè¯„ä¼°æƒ…æ„Ÿæ„ŸçŸ¥è°ˆåˆ¤ã€‚å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶è¯æ˜ï¼ŒEvoEmoåœ¨å„æ–¹é¢è¡¨ç°å‡ä¼˜äºåŸºçº¿ç­–ç•¥ï¼Œå®ç°æ›´é«˜çš„æˆåŠŸç‡ã€æ•ˆç‡å’Œä¹°å®¶èŠ‚çœã€‚è¿™å¼ºè°ƒäº†è‡ªé€‚åº”æƒ…ç»ªè¡¨è¾¾åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹æ›´é€‚ç”¨äºå¤šè½®è°ˆåˆ¤ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs can engage in complex multi-turn negotiations through Chain-of-Thought (CoT) reasoning, opening new avenues for agentic AI.</li>
<li>Existing LLM agents often generate passive emotional responses, making them vulnerable to manipulation by adversarial counterparts.</li>
<li>EvoEmo is an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations.</li>
<li>EvoEmo models emotional state transitions as a Markov Decision Process and uses genetic optimization to evolve high-reward emotion policies.</li>
<li>An evaluation framework with baselines is proposed to assess emotion-aware negotiation strategies.</li>
<li>Extensive experiments show that EvoEmo outperforms baseline strategies in terms of success rate, efficiency, and buyer savings.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ec82d2610910cdb0ec016c7fd3831b2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4503d4f9510b771b54f02f6c87543ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6be05fccd8c48603ac9de19dca75ec2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Test-Time-Adaptation-for-Speech-Enhancement-via-Domain-Invariant-Embedding-Transformation"><a href="#Test-Time-Adaptation-for-Speech-Enhancement-via-Domain-Invariant-Embedding-Transformation" class="headerlink" title="Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation"></a>Test-Time Adaptation for Speech Enhancement via Domain Invariant   Embedding Transformation</h2><p><strong>Authors:Tobias Raichle, Niels Edinger, Bin Yang</strong></p>
<p>Deep learning-based speech enhancement models achieve remarkable performance when test distributions match training conditions, but often degrade when deployed in unpredictable real-world environments with domain shifts. To address this challenge, we present LaDen (latent denoising), the first test-time adaptation method specifically designed for speech enhancement. Our approach leverages powerful pre-trained speech representations to perform latent denoising, approximating clean speech representations through a linear transformation of noisy embeddings. We show that this transformation generalizes well across domains, enabling effective pseudo-labeling for target domains without labeled target data. The resulting pseudo-labels enable effective test-time adaptation of speech enhancement models across diverse acoustic environments. We propose a comprehensive benchmark spanning multiple datasets with various domain shifts, including changes in noise types, speaker characteristics, and languages. Our extensive experiments demonstrate that LaDen consistently outperforms baseline methods across perceptual metrics, particularly for speaker and language domain shifts. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨æµ‹è¯•åˆ†å¸ƒä¸è®­ç»ƒæ¡ä»¶ç›¸åŒ¹é…æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨éƒ¨ç½²åˆ°å…·æœ‰é¢†åŸŸå·®å¼‚æ€§çš„ä¸å¯é¢„æµ‹çš„ç°å®ä¸–ç•Œä¸­æ—¶ï¼Œæ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LaDenï¼ˆæ½œåœ¨å»å™ªï¼‰ï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºè¯­éŸ³å¢å¼ºè®¾è®¡çš„ç¬¬ä¸€ä¸ªæµ‹è¯•æ—¶é—´é€‚åº”æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒè¯­éŸ³è¡¨ç¤ºæ¥æ‰§è¡Œæ½œåœ¨å»å™ªï¼Œé€šè¿‡å™ªå£°åµŒå…¥çš„çº¿æ€§å˜æ¢æ¥è¿‘ä¼¼æ¸…æ´è¯­éŸ³è¡¨ç¤ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§å˜æ¢åœ¨ä¸åŒçš„é¢†åŸŸä¸­éƒ½èƒ½å¾ˆå¥½åœ°æ¨å¹¿ï¼Œä½¿å¾—èƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡è®°ç›®æ ‡æ•°æ®çš„æƒ…å†µä¸‹å¯¹ç›®æ ‡åŸŸè¿›è¡Œæœ‰æ•ˆçš„ä¼ªæ ‡ç­¾æ ‡æ³¨ã€‚è¿™äº›ç”Ÿæˆçš„ä¼ªæ ‡ç­¾ä½¿å¾—è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨ä¸åŒçš„å£°å­¦ç¯å¢ƒä¸­å®ç°äº†æœ‰æ•ˆçš„æµ‹è¯•æ—¶é—´é€‚åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å¤šä¸ªæ•°æ®é›†ï¼ŒåŒ…æ‹¬å™ªå£°ç±»å‹ã€è¯´è¯äººç‰¹å¾å’Œè¯­è¨€ç­‰æ–¹é¢çš„é¢†åŸŸå·®å¼‚ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLaDenåœ¨æ„ŸçŸ¥æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯´è¯äººå’Œè¯­è¨€é¢†åŸŸå·®å¼‚æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04280v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨æµ‹è¯•åˆ†å¸ƒä¸è®­ç»ƒæ¡ä»¶ç›¸åŒ¹é…æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éƒ¨ç½²åˆ°å…·æœ‰é¢†åŸŸå·®å¼‚æ€§çš„ä¸å¯é¢„æµ‹çœŸå®ä¸–ç•Œç¯å¢ƒä¸­æ—¶ï¼Œæ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LaDenï¼ˆæ½œåœ¨å»å™ªï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºè¯­éŸ³å¢å¼ºè®¾è®¡çš„æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒè¯­éŸ³è¡¨ç¤ºè¿›è¡Œæ½œåœ¨å»å™ªï¼Œé€šè¿‡å™ªå£°åµŒå…¥çš„çº¿æ€§å˜æ¢è¿‘ä¼¼æ¸…æ´è¯­éŸ³è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§å˜æ¢åœ¨ä¸åŒé¢†åŸŸä¹‹é—´å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ç›®æ ‡æ ‡ç­¾æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯¹ç›®æ ‡é¢†åŸŸè¿›è¡Œæœ‰æ•ˆçš„ä¼ªæ ‡ç­¾ç”Ÿæˆã€‚ä¼ªæ ‡ç­¾ä½¿å¾—è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨ä¸åŒçš„å£°å­¦ç¯å¢ƒä¸­å®ç°äº†æœ‰æ•ˆçš„æµ‹è¯•æ—¶è‡ªé€‚åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šä¸ªå…·æœ‰ä¸åŒé¢†åŸŸå·®å¼‚æ€§çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬å™ªå£°ç±»å‹ã€è¯´è¯äººç‰¹å¾å’Œè¯­è¨€çš„å˜åŒ–ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒLaDenåœ¨æ„ŸçŸ¥æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå°¤å…¶åœ¨è¯´è¯äººå’Œè¯­è¨€é¢†åŸŸè½¬ç§»æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LaDenæ˜¯ä¸€ç§é’ˆå¯¹è¯­éŸ³å¢å¼ºæ¨¡å‹çš„æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ã€‚</li>
<li>LaDenåˆ©ç”¨é¢„è®­ç»ƒè¯­éŸ³è¡¨ç¤ºçš„æ½œåœ¨å»å™ªæŠ€æœ¯ã€‚</li>
<li>é€šè¿‡çº¿æ€§å˜æ¢å™ªå£°åµŒå…¥æ¥è¿‘ä¼¼æ¸…æ´è¯­éŸ³è¡¨ç¤ºã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸä¹‹é—´å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¹¶èƒ½ç”Ÿæˆæœ‰æ•ˆçš„ä¼ªæ ‡ç­¾ã€‚</li>
<li>LaDenåœ¨å¤šç§å£°å­¦ç¯å¢ƒä¸‹å®ç°è¯­éŸ³å¢å¼ºæ¨¡å‹çš„è‡ªé€‚åº”ã€‚</li>
<li>æå‡ºä¸€ä¸ªæ¶µç›–å¤šä¸ªæ•°æ®é›†çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸå·®å¼‚ä¸‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04280">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9caed9dc433b0a900fa90f005c3763ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99ef586c2ae488498167235f58182b80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d679a2e4b13e239a074ed22e7250e0a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0fff68cf086b086368aae42689ff911.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37e872d34af63f1aceef049bf8b611d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc29eb7e9affe8939906473e8b132b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09d56deab92e004e34d59f15783d710f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Real-Time-FPGA-Based-Transformers-VLMs-for-Vision-Tasks-SOTA-Designs-and-Optimizations"><a href="#Real-Time-FPGA-Based-Transformers-VLMs-for-Vision-Tasks-SOTA-Designs-and-Optimizations" class="headerlink" title="Real Time FPGA Based Transformers &amp; VLMs for Vision Tasks: SOTA Designs   and Optimizations"></a>Real Time FPGA Based Transformers &amp; VLMs for Vision Tasks: SOTA Designs   and Optimizations</h2><p><strong>Authors:Safa Mohammed Sali, Mahmoud Meribout, Ashiyana Abdul Majeed</strong></p>
<p>Transformers and vision-language models (VLMs) have emerged as dominant architectures in computer vision and multimodal AI, offering state-of-the-art performance in tasks such as image classification, object detection, visual question answering, and caption generation. However, their high computational complexity, large memory footprints, and irregular data access patterns present significant challenges for deployment in latency- and power-constrained environments. Field-programmable gate arrays (FPGAs) provide an attractive hardware platform for such workloads due to their reconfigurability, fine-grained parallelism, and potential for energy-efficient acceleration. This paper presents a comprehensive review of design trade-offs, optimization strategies, and implementation challenges for FPGA-based inference of transformers and VLMs. We examine critical factors such as device-class selection, memory subsystem constraints, dataflow orchestration, quantization strategies, sparsity exploitation, and toolchain choices, alongside modality-specific issues unique to VLMs, including heterogeneous compute balancing and cross-attention memory management. Additionally, we discuss emerging trends in hardware-algorithm co-design, highlighting innovations in attention mechanisms, compression, and modular overlays to improve efficiency and adaptability. Practical issues such as runtime flexibility, verification overhead, and the absence of standardized FPGA multimodal benchmarks are also considered. Finally, we outline future directions toward scalable, portable, and reconfigurable FPGA solutions that adapt to evolving model architectures while sustaining high utilization and predictable performance. This synthesis offers both a technical foundation and a forward-looking perspective to help bridge the gap between advanced multimodal AI models and efficient FPGA deployment. </p>
<blockquote>
<p>Transformerå’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å·²ç»æˆä¸ºè®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„ä¸»å¯¼æ¶æ„ï¼Œå®ƒä»¬åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è§†è§‰é—®ç­”å’Œæ ‡é¢˜ç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„é«˜è®¡ç®—å¤æ‚åº¦ã€å¤§å†…å­˜å ç”¨å’Œä¸è§„åˆ™çš„æ•°æ®è®¿é—®æ¨¡å¼ï¼Œä¸ºåœ¨å»¶è¿Ÿå’ŒåŠŸç‡å—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²å®ƒä»¬å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ç”±äºFPGAï¼ˆç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼‰å…·æœ‰å¯é‡æ„æ€§ã€ç»†ç²’åº¦å¹¶è¡Œæ€§å’ŒèŠ‚èƒ½åŠ é€Ÿæ½œåŠ›ï¼Œå› æ­¤å®ƒæˆä¸ºæ­¤ç±»å·¥ä½œè´Ÿè½½çš„ç†æƒ³ç¡¬ä»¶å¹³å°ã€‚æœ¬æ–‡å¯¹åŸºäºFPGAçš„Transformerå’ŒVLMæ¨ç†è®¾è®¡çš„ä¼˜ç¼ºç‚¹ã€ä¼˜åŒ–ç­–ç•¥å’Œå®ç°æŒ‘æˆ˜è¿›è¡Œäº†å…¨é¢å›é¡¾ã€‚æˆ‘ä»¬ç ”ç©¶äº†å…³é”®å› ç´ ï¼Œå¦‚è®¾å¤‡ç±»åˆ«é€‰æ‹©ã€å†…å­˜å­ç³»ç»Ÿçº¦æŸã€æ•°æ®æµç¼–æ’ã€é‡åŒ–ç­–ç•¥ã€ç¨€ç–åˆ©ç”¨å’Œå·¥å…·é“¾é€‰æ‹©ï¼Œä»¥åŠVLMç‰¹æœ‰çš„æ¨¡æ€ç‰¹å®šé—®é¢˜ï¼ŒåŒ…æ‹¬å¼‚æ„è®¡ç®—å¹³è¡¡å’Œè·¨æ³¨æ„åŠ›å†…å­˜ç®¡ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†ç¡¬ä»¶ç®—æ³•ååŒè®¾è®¡çš„æœ€æ–°è¶‹åŠ¿ï¼Œé‡ç‚¹ä»‹ç»äº†æ³¨æ„åŠ›æœºåˆ¶ã€å‹ç¼©å’Œæ¨¡å—åŒ–å åŠ çš„åˆ›æ–°ï¼Œä»¥æé«˜æ•ˆç‡å’Œé€‚åº”æ€§ã€‚å®é™…çš„é—®é¢˜ï¼Œå¦‚è¿è¡Œæ—¶çš„çµæ´»æ€§ã€éªŒè¯çš„å¼€é”€ä»¥åŠç¼ºä¹æ ‡å‡†åŒ–çš„FPGAå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¹Ÿè¢«è€ƒè™‘åœ¨å†…ã€‚æœ€åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†æœªæ¥æ–¹å‘ï¼Œæœç€å¯æ‰©å±•çš„ã€ä¾¿æºçš„å’Œå¯é‡æ„çš„FPGAè§£å†³æ–¹æ¡ˆå‘å±•ï¼Œä»¥é€‚åº”ä¸æ–­æ¼”å˜çš„æ¨¡å‹æ¶æ„ï¼ŒåŒæ—¶ä¿æŒé«˜åˆ©ç”¨ç‡å’Œå¯é¢„æµ‹çš„æ€§èƒ½ã€‚è¿™ä¸€ç»¼è¿°ä¸ºå…ˆè¿›çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½æ¨¡å‹å’Œæœ‰æ•ˆçš„FPGAéƒ¨ç½²ä¹‹é—´çš„é¸¿æ²Ÿæä¾›äº†ä¸€æ¶æŠ€æœ¯æ¡¥æ¢å’Œå‰ç»æ€§è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04162v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†å°†Transformerå’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰éƒ¨ç½²åˆ°FPGAï¼ˆç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼‰ä¸Šçš„æŒ‘æˆ˜å’Œç­–ç•¥ã€‚æ–‡ç« è¯¦ç»†ä»‹ç»äº†é’ˆå¯¹æ­¤ç±»å·¥ä½œè´Ÿè½½çš„FPGAè®¾è®¡æƒè¡¡ã€ä¼˜åŒ–ç­–ç•¥å’Œå®æ–½æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®¾å¤‡ç±»åˆ«é€‰æ‹©ã€å†…å­˜å­ç³»ç»Ÿçº¦æŸã€æ•°æ®æµç¼–æ’ç­‰å…³é”®å› ç´ ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è®¨è®ºäº†ç¡¬ä»¶ç®—æ³•ååŒè®¾è®¡ä¸­çš„æ–°å…´è¶‹åŠ¿ï¼Œå¹¶å¼ºè°ƒäº†æ³¨æ„åŠ›æœºåˆ¶ã€å‹ç¼©å’Œæ¨¡å—åŒ–å åŠ ç­‰æ–¹é¢çš„åˆ›æ–°ã€‚æœ€åï¼Œæœ¬æ–‡å±•æœ›äº†æœªæ¥FPGAè§£å†³æ–¹æ¡ˆçš„å‘å±•æ–¹å‘ï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆèƒ½å¤Ÿé€‚åº”ä¸æ–­æ¼”å˜çš„æ¨¡å‹æ¶æ„ï¼ŒåŒæ—¶ä¿æŒé«˜åˆ©ç”¨ç‡å’Œå¯é¢„æµ‹çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Transformerå’ŒVLMå·²æˆä¸ºè®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€AIçš„ä¸»å¯¼æ¶æ„ï¼Œå¹¶åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¿™äº›æ¨¡å‹çš„é«˜è®¡ç®—å¤æ‚åº¦ã€å¤§å†…å­˜å ç”¨å’Œä¸è§„åˆ™çš„æ•°æ®è®¿é—®æ¨¡å¼ï¼Œåœ¨å»¶è¿Ÿå’ŒåŠŸç‡å—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>FPGAå› å…¶å¯é‡æ„æ€§ã€ç»†ç²’åº¦å¹¶è¡Œæ€§å’ŒèŠ‚èƒ½åŠ é€Ÿæ½œåŠ›è€Œæˆä¸ºæ­¤ç±»å·¥ä½œè´Ÿè½½çš„å¸å¼•ç¡¬ä»¶å¹³å°ã€‚</li>
<li>æ–‡ç« è¯¦ç»†è®¨è®ºäº†FPGAä¸ŠTransformerå’ŒVLMæ¨ç†çš„è®¾è®¡æƒè¡¡å’Œä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬è®¾å¤‡é€‰æ‹©ã€å†…å­˜çº¦æŸã€æ•°æ®æµç®¡ç†ç­‰å› ç´ ã€‚</li>
<li>æ–‡ç« è¿˜ä»‹ç»äº†ç¡¬ä»¶ç®—æ³•ååŒè®¾è®¡ä¸­çš„æ–°å…´è¶‹åŠ¿ï¼ŒåŒ…æ‹¬æ³¨æ„åŠ›æœºåˆ¶ã€æ¨¡å‹å‹ç¼©ç­‰æ–¹é¢çš„åˆ›æ–°ã€‚</li>
<li>å®é™…åº”ç”¨ä¸­éœ€è€ƒè™‘è¿è¡Œæ—¶çµæ´»æ€§ã€éªŒè¯å¼€é”€å’Œç¼ºä¹æ ‡å‡†åŒ–çš„FPGAå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ç­‰é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78acd434914013ff194d32ed2c33a492.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6eab48125eec3ef4fb63abca2b4541b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bb3330ef40edc4e68e5617385883777.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43ca3dadbf75e31fbf3a8e4213724695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50c08e252c09db2c5ff2b2bd869c6b0c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SelfAug-Mitigating-Catastrophic-Forgetting-in-Retrieval-Augmented-Generation-via-Distribution-Self-Alignment"><a href="#SelfAug-Mitigating-Catastrophic-Forgetting-in-Retrieval-Augmented-Generation-via-Distribution-Self-Alignment" class="headerlink" title="SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented   Generation via Distribution Self-Alignment"></a>SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented   Generation via Distribution Self-Alignment</h2><p><strong>Authors:Yuqing Huang, Rongyang Zhang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Xuyang Zhi, Guiquan Liu, Xin Li, Hao Wang, Enhong Chen</strong></p>
<p>Recent advancements in large language models (LLMs) have revolutionized natural language processing through their remarkable capabilities in understanding and executing diverse tasks. While supervised fine-tuning, particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively enhances task-specific performance, it often leads to catastrophic forgetting, where models lose their previously acquired knowledge and general capabilities. Existing solutions either require access to general instruction data or face limitations in preserving the modelâ€™s original distribution. To overcome these limitations, we propose SelfAug, a self-distribution alignment method that aligns input sequence logits to preserve the modelâ€™s semantic distribution, thereby mitigating catastrophic forgetting and improving downstream performance. Extensive experiments demonstrate that SelfAug achieves a superior balance between downstream learning and general capability retention. Our comprehensive empirical analysis reveals a direct correlation between distribution shifts and the severity of catastrophic forgetting in RAG scenarios, highlighting how the absence of RAG capabilities in general instruction tuning leads to significant distribution shifts during fine-tuning. Our findings not only advance the understanding of catastrophic forgetting in RAG contexts but also provide a practical solution applicable across diverse fine-tuning scenarios. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/USTC-StarTeam/SelfAug">https://github.com/USTC-StarTeam/SelfAug</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å·²é€šè¿‡å…¶åœ¨ç†è§£å’Œæ‰§è¡Œå„ç§ä»»åŠ¡æ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚è™½ç„¶ç›‘ç£å¾®è°ƒï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœºæ™¯ä¸­ï¼Œæœ‰æ•ˆåœ°æé«˜äº†ä»»åŠ¡ç‰¹å®šæ€§èƒ½ï¼Œä½†å®ƒå¸¸å¸¸å¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œå³æ¨¡å‹å¤±å»å…¶å…ˆå‰è·å¾—çš„çŸ¥è¯†å’Œä¸€èˆ¬èƒ½åŠ›ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆè¦ä¹ˆéœ€è¦è®¿é—®ä¸€èˆ¬æŒ‡ä»¤æ•°æ®ï¼Œè¦ä¹ˆåœ¨ä¿æŒæ¨¡å‹åŸå§‹åˆ†å¸ƒæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SelfAugï¼Œä¸€ç§è‡ªæˆ‘åˆ†å¸ƒå¯¹é½æ–¹æ³•ï¼Œé€šè¿‡å¯¹è¾“å…¥åºåˆ—é€»è¾‘è¿›è¡Œå¯¹é½ï¼Œä»¥ä¿ç•™æ¨¡å‹çš„è¯­ä¹‰åˆ†å¸ƒï¼Œä»è€Œç¼“è§£ç¾éš¾æ€§é—å¿˜å¹¶æ”¹å–„ä¸‹æ¸¸æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSelfAugåœ¨ä¸‹æ¸¸å­¦ä¹ å’Œä¸€èˆ¬èƒ½åŠ›ä¿ç•™ä¹‹é—´å®ç°äº†å“è¶Šçš„å¹³è¡¡ã€‚æˆ‘ä»¬çš„ç»¼åˆå®è¯åˆ†ææ­ç¤ºäº†åˆ†å¸ƒåç§»å’ŒRAGåœºæ™¯ä¸­ç¾éš¾æ€§é—å¿˜ä¸¥é‡ç¨‹åº¦çš„ç›´æ¥å…³è”ï¼Œå¼ºè°ƒäº†åœ¨è¿›è¡Œå¾®è°ƒæ—¶ç¼ºä¹RAGèƒ½åŠ›çš„ä¸€èˆ¬æŒ‡ä»¤è°ƒæ•´ä¼šå¯¼è‡´æ˜¾è‘—çš„åˆ†å¸ƒåç§»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†RAGèƒŒæ™¯ä¸‹ç¾éš¾æ€§é—å¿˜çš„ç†è§£ï¼Œè€Œä¸”ä¸ºå„ç§å¾®è°ƒåœºæ™¯æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/USTC-StarTeam/SelfAug%E3%80%82">https://github.com/USTC-StarTeam/SelfAugã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03934v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‘æœŸè¿›å±•å·²å½»åº•æ”¹å˜è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¼å±€ï¼Œå…¶åœ¨ç†è§£å’Œæ‰§è¡Œå„ç§ä»»åŠ¡æ–¹é¢çš„å“è¶Šèƒ½åŠ›å°¤ä¸ºçªå‡ºã€‚è™½ç„¶ç›‘ç£å¾®è°ƒï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœºæ™¯ä¸­ï¼Œèƒ½æœ‰æ•ˆæå‡ä»»åŠ¡ç‰¹å®šæ€§èƒ½ï¼Œä½†å®ƒå¾€å¾€å¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œä½¿æ¨¡å‹å¤±å»å…ˆå‰è·å–çš„çŸ¥è¯†å’Œä¸€èˆ¬èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SelfAugæ–¹æ³•ï¼Œä¸€ç§è‡ªæˆ‘åˆ†å¸ƒå¯¹é½æŠ€æœ¯ï¼Œé€šè¿‡å¯¹è¾“å…¥åºåˆ—é€»è¾‘è¿›è¡Œå¯¹é½ï¼Œä¿ç•™æ¨¡å‹çš„è¯­ä¹‰åˆ†å¸ƒï¼Œä»è€Œå‡è½»ç¾éš¾æ€§é—å¿˜å¹¶æå‡ä¸‹æ¸¸æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒSelfAugåœ¨ä¸‹æ¸¸å­¦ä¹ ä¸ä¸€èˆ¬èƒ½åŠ›ä¿ç•™ä¹‹é—´è¾¾åˆ°äº†å“è¶Šå¹³è¡¡ã€‚æˆ‘ä»¬çš„ç»¼åˆå®è¯åˆ†ææ­ç¤ºäº†åˆ†å¸ƒåç§»ä¸RAGåœºæ™¯ä¸­ç¾éš¾æ€§é—å¿˜ä¸¥é‡æ€§çš„ç›´æ¥å…³è”ï¼Œå¹¶å¼ºè°ƒäº†é€šç”¨æŒ‡ä»¤è°ƒæ•´ä¸­ç¼ºä¹RAGèƒ½åŠ›åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯¼è‡´çš„æ˜¾è‘—åˆ†å¸ƒåç§»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…åŠ æ·±äº†RAGèƒŒæ™¯ä¸‹çš„ç¾éš¾æ€§é—å¿˜çš„ç†è§£ï¼Œè€Œä¸”ä¸ºå„ç§å¾®è°ƒåœºæ™¯æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œå°¤å…¶åœ¨ç†è§£å’Œæ‰§è¡Œå¤šæ ·ä»»åŠ¡ä¸Šã€‚</li>
<li>ç›‘ç£å¾®è°ƒè™½èƒ½æå‡ä»»åŠ¡ç‰¹å®šæ€§èƒ½ï¼Œä½†å¯èƒ½å¯¼è‡´æ¨¡å‹ç¾éš¾æ€§é—å¿˜å…ˆå‰çŸ¥è¯†ã€‚</li>
<li>SelfAugæ–¹æ³•é€šè¿‡è‡ªæˆ‘åˆ†å¸ƒå¯¹é½æŠ€æœ¯æå‡ºè§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¿ç•™æ¨¡å‹è¯­ä¹‰åˆ†å¸ƒã€‚</li>
<li>å®éªŒè¯æ˜SelfAugåœ¨ä¸‹æ¸¸å­¦ä¹ ä¸ä¸€èˆ¬èƒ½åŠ›ä¿ç•™é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>ç»¼åˆå®è¯åˆ†ææ˜¾ç¤ºåˆ†å¸ƒåç§»ä¸RAGåœºæ™¯ä¸­ç¾éš¾æ€§é—å¿˜çš„å…³è”æ€§ã€‚</li>
<li>ç¼ºä¹RAGèƒ½åŠ›åœ¨é€šç”¨æŒ‡ä»¤å¾®è°ƒä¸­ä¼šå¯¼è‡´æ˜¾è‘—åˆ†å¸ƒåç§»ã€‚</li>
<li>ç ”ç©¶ç»“æœä¸ä»…åŠ æ·±äº†å¯¹RAGèƒŒæ™¯ä¸‹ç¾éš¾æ€§é—å¿˜çš„ç†è§£ï¼Œè¿˜ä¸ºå„ç§å¾®è°ƒåœºæ™¯æä¾›å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca74b1233a3e010c5156a9fb1ab8f5d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16512b3c20bc420e711f84a39dce296a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa06bb625f1e697b4df6e8c96e36a15b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="KGBERT4Eth-A-Feature-Complete-Transformer-Powered-by-Knowledge-Graph-for-Multi-Task-Ethereum-Fraud-Detection"><a href="#KGBERT4Eth-A-Feature-Complete-Transformer-Powered-by-Knowledge-Graph-for-Multi-Task-Ethereum-Fraud-Detection" class="headerlink" title="KGBERT4Eth: A Feature-Complete Transformer Powered by Knowledge Graph   for Multi-Task Ethereum Fraud Detection"></a>KGBERT4Eth: A Feature-Complete Transformer Powered by Knowledge Graph   for Multi-Task Ethereum Fraud Detection</h2><p><strong>Authors:Yifan Jia, Ye Tian, Liguo Zhang, Yanbin Wang, Jianguo Sun, Liangliang Song</strong></p>
<p>Ethereumâ€™s rapid ecosystem expansion and transaction anonymity have triggered a surge in malicious activity. Detection mechanisms currently bifurcate into three technical strands: expert-defined features, graph embeddings, and sequential transaction patterns, collectively spanning the complete feature sets of Ethereumâ€™s native data layer. Yet the absence of cross-paradigm integration mechanisms forces practitioners to choose between sacrificing sequential context awareness, structured fund-flow patterns, or human-curated feature insights in their solutions. To bridge this gap, we propose KGBERT4Eth, a feature-complete pre-training encoder that synergistically combines two key components: (1) a Transaction Semantic Extractor, where we train an enhanced Transaction Language Model (TLM) to learn contextual semantic representations from conceptualized transaction records, and (2) a Transaction Knowledge Graph (TKG) that incorporates expert-curated domain knowledge into graph node embeddings to capture fund flow patterns and human-curated feature insights. We jointly optimize pre-training objectives for both components to fuse these complementary features, generating feature-complete embeddings. To emphasize rare anomalous transactions, we design a biased masking prediction task for TLM to focus on statistical outliers, while the Transaction TKG employs link prediction to learn latent transaction relationships and aggregate knowledge. Furthermore, we propose a mask-invariant attention coordination module to ensure stable dynamic information exchange between TLM and TKG during pre-training. KGBERT4Eth significantly outperforms state-of-the-art baselines in both phishing account detection and de-anonymization tasks, achieving absolute F1-score improvements of 8-16% on three phishing detection benchmarks and 6-26% on four de-anonymization datasets. </p>
<blockquote>
<p>ä»¥å¤ªåŠç”Ÿæ€ç³»ç»Ÿçš„å¿«é€Ÿæ‰©å¼ å’Œäº¤æ˜“çš„åŒ¿åæ€§å·²ç»å¼•å‘äº†å¤§é‡æ¶æ„æ´»åŠ¨ã€‚å½“å‰çš„æ£€æµ‹æœºåˆ¶åˆ†ä¸ºä¸‰ç§æŠ€æœ¯ï¼šä¸“å®¶å®šä¹‰çš„ç‰¹å¾ã€å›¾åµŒå…¥å’Œé¡ºåºäº¤æ˜“æ¨¡å¼ï¼Œå®ƒä»¬å…±åŒæ¶µç›–äº†ä»¥å¤ªåŠæœ¬åœ°æ•°æ®å±‚çš„å®Œæ•´ç‰¹å¾é›†ã€‚ç„¶è€Œï¼Œç¼ºä¹è·¨èŒƒå¼æ•´åˆæœºåˆ¶è¿«ä½¿ä»ä¸šè€…åœ¨è§£å†³æ–¹æ¡ˆä¸­ç‰ºç‰²é¡ºåºä¸Šä¸‹æ–‡æ„è¯†ã€ç»“æ„åŒ–èµ„é‡‘æµæ¨¡å¼æˆ–äººå·¥æ•´ç†çš„ç‰¹å¾æ´å¯ŸåŠ›ä¹‹é—´åšå‡ºé€‰æ‹©ã€‚ä¸ºäº†å¼¥å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†KGBERT4Ethï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„é¢„è®­ç»ƒç¼–ç å™¨ï¼Œå®ƒååŒåœ°ç»“åˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰äº¤æ˜“è¯­ä¹‰æå–å™¨ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œè®­ç»ƒäº†ä¸€ä¸ªå¢å¼ºçš„äº¤æ˜“è¯­è¨€æ¨¡å‹ï¼ˆTLMï¼‰ï¼Œä»æ¦‚å¿µåŒ–çš„äº¤æ˜“è®°å½•ä¸­å­¦ä¹ ä¸Šä¸‹æ–‡è¯­ä¹‰è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰äº¤æ˜“çŸ¥è¯†å›¾è°±ï¼ˆTKGï¼‰ï¼Œå®ƒå°†ä¸“å®¶ç­–åˆ’çš„é¢†åŸŸçŸ¥è¯†èå…¥å›¾èŠ‚ç‚¹åµŒå…¥ï¼Œä»¥æ•è·èµ„é‡‘æµæ¨¡å¼å’Œäººå·¥æ•´ç†çš„ç‰¹å¾æ´å¯ŸåŠ›ã€‚æˆ‘ä»¬è”åˆä¼˜åŒ–ä¸¤ä¸ªç»„ä»¶çš„é¢„è®­ç»ƒç›®æ ‡ï¼Œä»¥èåˆè¿™äº›äº’è¡¥ç‰¹å¾ï¼Œç”ŸæˆåŠŸèƒ½å®Œæ•´çš„åµŒå…¥ã€‚ä¸ºäº†å¼ºè°ƒç½•è§çš„å¼‚å¸¸äº¤æ˜“ï¼Œæˆ‘ä»¬ä¸ºTLMè®¾è®¡äº†ä¸€ä¸ªåå‘æ€§çš„æ©ç é¢„æµ‹ä»»åŠ¡ï¼Œä»¥å…³æ³¨ç»Ÿè®¡å¼‚å¸¸å€¼ï¼Œè€Œäº¤æ˜“TKGåˆ™é‡‡ç”¨é“¾æ¥é¢„æµ‹æ¥å­¦ä¹ æ½œåœ¨çš„äº¤æ˜“å…³ç³»å¹¶èšåˆçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ©ç ä¸å˜æ³¨æ„åŠ›åè°ƒæ¨¡å—ï¼Œä»¥ç¡®ä¿åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­TLMå’ŒTKGä¹‹é—´çš„ç¨³å®šåŠ¨æ€ä¿¡æ¯äº¤æ¢ã€‚KGBERT4Ethåœ¨é’“é±¼è´¦æˆ·æ£€æµ‹å’Œå»åŒ¿ååŒ–ä»»åŠ¡ä¸­éƒ½æ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œåœ¨ä¸‰ä¸ªé’“é±¼æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šç»å¯¹F1åˆ†æ•°æé«˜äº†8-16%ï¼Œåœ¨å››ä¸ªå»åŒ¿ååŒ–æ•°æ®é›†ä¸Šæé«˜äº†6-26%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03860v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä»¥å¤ªåŠç”Ÿæ€ç³»ç»Ÿè¿…é€Ÿæ‰©å¼ åŠäº¤æ˜“åŒ¿åæ€§å¼•å‘æ¶æ„æ´»åŠ¨æ¿€å¢ã€‚å½“å‰æ£€æµ‹æœºåˆ¶åŒ…æ‹¬ä¸“å®¶å®šä¹‰ç‰¹å¾ã€å›¾åµŒå…¥å’Œé¡ºåºäº¤æ˜“æ¨¡å¼ä¸‰ç§æŠ€æœ¯ï¼Œä½†ç¼ºä¹è·¨èŒƒå¼æ•´åˆæœºåˆ¶ï¼Œéœ€è¦åœ¨é¡ºåºä¸Šä¸‹æ–‡æ„è¯†ã€ç»“æ„åŒ–èµ„é‡‘æµæ¨¡å¼æˆ–äººç±»ç­–åˆ’ç‰¹å¾æ´å¯ŸåŠ›ä¹‹é—´åšå‡ºé€‰æ‹©ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºKGBERT4Ethï¼Œä¸€ç§åŠŸèƒ½å®Œå¤‡çš„é¢„è®­ç»ƒç¼–ç å™¨ï¼Œç»“åˆä¸¤å¤§å…³é”®ç»„ä»¶ï¼š1ï¼‰äº¤æ˜“è¯­ä¹‰æå–å™¨ï¼Œæˆ‘ä»¬è®­ç»ƒå¢å¼ºå‹äº¤æ˜“è¯­è¨€æ¨¡å‹ï¼ˆTLMï¼‰ä»æ¦‚å¿µåŒ–äº¤æ˜“è®°å½•ä¸­å­¦ä¹ ä¸Šä¸‹æ–‡è¯­ä¹‰è¡¨ç¤ºï¼›2ï¼‰äº¤æ˜“çŸ¥è¯†å›¾è°±ï¼ˆTKGï¼‰ï¼Œå°†ä¸“å®¶ç­–åˆ’é¢†åŸŸçŸ¥è¯†èå…¥å›¾èŠ‚ç‚¹åµŒå…¥ï¼Œä»¥æ•æ‰èµ„é‡‘æµæ¨¡å¼å’Œäººç±»ç­–åˆ’ç‰¹å¾æ´å¯ŸåŠ›ã€‚æˆ‘ä»¬è”åˆä¼˜åŒ–ä¸¤è€…çš„é¢„è®­ç»ƒç›®æ ‡ï¼Œèåˆè¿™äº›äº’è¡¥ç‰¹å¾ï¼Œç”ŸæˆåŠŸèƒ½å®Œå¤‡åµŒå…¥ã€‚æˆ‘ä»¬è®¾è®¡åå‘æ€§æ©ç é¢„æµ‹ä»»åŠ¡ï¼Œä½¿TLMä¾§é‡äºç»Ÿè®¡å¼‚å¸¸å€¼ï¼Œè€ŒTransaction TKGåˆ™é‡‡ç”¨é“¾æ¥é¢„æµ‹å­¦ä¹ æ½œåœ¨äº¤æ˜“å…³ç³»å’ŒèšåˆçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºæ©ç ä¸å˜æ³¨æ„åŠ›åè°ƒæ¨¡å—ï¼Œä»¥ç¡®ä¿TLMå’ŒTKGåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šåŠ¨æ€ä¿¡æ¯äº¤æ¢ã€‚KGBERT4Ethåœ¨é’“é±¼è´¦æˆ·æ£€æµ‹å’Œå»åŒ¿ååŒ–ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œåœ¨ä¸‰ä¸ªé’“é±¼æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šå®ç°8-16%çš„F1åˆ†æ•°ç»å¯¹æå‡ï¼Œåœ¨å››ä¸ªå»åŒ¿ååŒ–æ•°æ®é›†ä¸Šå®ç°6-26%çš„æå‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»¥å¤ªåŠç”Ÿæ€ç³»ç»Ÿæ‰©å¼ å’Œäº¤æ˜“åŒ¿åæ€§å¼•å‘æ¶æ„æ´»åŠ¨å¢åŠ ã€‚</li>
<li>ç°æœ‰æ£€æµ‹æœºåˆ¶åŒ…æ‹¬ä¸“å®¶å®šä¹‰ç‰¹å¾ã€å›¾åµŒå…¥å’Œé¡ºåºäº¤æ˜“æ¨¡å¼ã€‚</li>
<li>KGBERT4Ethé€šè¿‡ç»“åˆäº¤æ˜“è¯­ä¹‰æå–å™¨å’Œäº¤æ˜“çŸ¥è¯†å›¾è°±æ¥å¡«è¡¥ç°æœ‰æŠ€æœ¯ç¼ºå£ã€‚</li>
<li>KGBERT4Ethé€šè¿‡é¢„è®­ç»ƒä¼˜åŒ–ç»“åˆä¸¤è€…çš„ç‰¹å¾ï¼Œç”ŸæˆåŠŸèƒ½å®Œå¤‡çš„åµŒå…¥ã€‚</li>
<li>è®¾è®¡åå‘æ€§æ©ç é¢„æµ‹ä»»åŠ¡æ¥å¼ºè°ƒå¼‚å¸¸äº¤æ˜“ï¼Œå¹¶é‡‡ç”¨é“¾æ¥é¢„æµ‹å­¦ä¹ æ½œåœ¨äº¤æ˜“å…³ç³»ã€‚</li>
<li>KGBERT4Ethæ˜¾è‘—æé«˜é’“é±¼è´¦æˆ·æ£€æµ‹å’Œå»åŒ¿ååŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5e173c83166183021196b52bcd5f5195.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-616c76f3f83a7eb9c25c50273aaafe37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-510bd817f790d32491533f104a6eb78e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d21e67d1ae8ff8603a08b1ca9efd830.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="The-Personality-Illusion-Revealing-Dissociation-Between-Self-Reports-Behavior-in-LLMs"><a href="#The-Personality-Illusion-Revealing-Dissociation-Between-Self-Reports-Behavior-in-LLMs" class="headerlink" title="The Personality Illusion: Revealing Dissociation Between Self-Reports &amp;   Behavior in LLMs"></a>The Personality Illusion: Revealing Dissociation Between Self-Reports &amp;   Behavior in LLMs</h2><p><strong>Authors:Pengrui Han, Rafal Kocielnik, Peiyang Song, Ramit Debnath, Dean Mobbs, Anima Anandkumar, R. Michael Alvarez</strong></p>
<p>Personality traits have long been studied as predictors of human behavior.Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability. </p>
<blockquote>
<p>äººæ ¼ç‰¹è´¨é•¿æœŸä»¥æ¥ä¸€ç›´è¢«ç ”ç©¶ä½œä¸ºäººç±»è¡Œä¸ºçš„é¢„æµ‹æŒ‡æ ‡ã€‚æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›å±•è¡¨æ˜ï¼Œäººå·¥ç³»ç»Ÿä¸­å¯èƒ½å‡ºç°ç±»ä¼¼çš„æ¨¡å¼ï¼Œå…ˆè¿›çš„LLMæ˜¾ç¤ºå‡ºä¸äººç±»çš„å®œäººæ€§å’Œè‡ªæˆ‘è°ƒèŠ‚ç­‰ç‰¹è´¨ç›¸ä¼¼çš„è¡Œä¸ºå€¾å‘ã€‚äº†è§£è¿™äº›æ¨¡å¼è‡³å…³é‡è¦ï¼Œä½†ä¹‹å‰çš„å·¥ä½œä¸»è¦ä¾èµ–äºç®€åŒ–çš„è‡ªæˆ‘æŠ¥å‘Šå’Œå¯å‘å¼æç¤ºï¼Œè¡Œä¸ºéªŒè¯å¾ˆå°‘ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æè¿°äº†LLMäººæ ¼çš„ä¸‰ä¸ªç»´åº¦ï¼šï¼ˆ1ï¼‰ç‰¹è´¨è½®å»“åœ¨è®­ç»ƒé˜¶æ®µçš„åŠ¨æ€å‡ºç°å’Œæ¼”å˜ï¼›ï¼ˆ2ï¼‰è‡ªæˆ‘æŠ¥å‘Šç‰¹è´¨åœ¨è¡Œä¸ºä»»åŠ¡ä¸­çš„é¢„æµ‹æ•ˆåº¦ï¼›ï¼ˆ3ï¼‰æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„ï¼ˆå¦‚äººæ ¼æ³¨å…¥ï¼‰å¯¹è‡ªæˆ‘æŠ¥å‘Šå’Œè¡Œä¸ºçš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒæŒ‡ä»¤å¯¹é½ï¼ˆä¾‹å¦‚RLHFã€æŒ‡ä»¤å¾®è°ƒï¼‰æ˜¾è‘—ç¨³å®šäº†ç‰¹è´¨è¡¨è¾¾ï¼ŒåŠ å¼ºäº†ç‰¹è´¨å…³è”ï¼Œè¿™ç§æ–¹å¼ä¸äººç±»æ•°æ®ç›¸ä¼¼ã€‚ç„¶è€Œï¼Œè¿™äº›è‡ªæˆ‘æŠ¥å‘Šçš„ç‰¹è´¨å¹¶ä¸èƒ½å¯é åœ°é¢„æµ‹è¡Œä¸ºï¼Œè§‚å¯Ÿåˆ°çš„å…³è”é€šå¸¸ä¸äººç±»æ¨¡å¼ç›¸æ‚–ã€‚è™½ç„¶äººæ ¼æ³¨å…¥æˆåŠŸå¼•å¯¼äº†è‡ªæˆ‘æŠ¥å‘Šæœé¢„å®šæ–¹å‘è¿›è¡Œï¼Œä½†å¯¹å®é™…è¡Œä¸ºçš„å½±å“ç”šå¾®æˆ–ä¸ä¸€è‡´ã€‚é€šè¿‡åŒºåˆ†è¡¨é¢å±‚æ¬¡çš„ç‰¹è´¨è¡¨è¾¾å’Œè¡Œä¸ºä¸€è‡´æ€§ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å¯¹LLMäººæ ¼å‡è®¾æå‡ºäº†æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†å¯¹é½å’Œå¯è§£é‡Šæ€§æ–¹é¢éœ€è¦è¿›è¡Œæ›´æ·±å…¥çš„è¯„ä»·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03730v1">PDF</a> We make public all code and source data at   <a target="_blank" rel="noopener" href="https://github.com/psychology-of-AI/Personality-Illusion">https://github.com/psychology-of-AI/Personality-Illusion</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„æ€§æ ¼ç‰¹è´¨ï¼Œå¦‚å‹å–„æ€§å’Œè‡ªæˆ‘è°ƒèŠ‚èƒ½åŠ›ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†LLMæ€§æ ¼çš„ä¸‰ä¸ªç»´åº¦ï¼šè®­ç»ƒé˜¶æ®µç‰¹è´¨è½®å»“çš„åŠ¨æ€å‡ºç°å’Œæ¼”å˜ã€è‡ªæˆ‘æŠ¥å‘Šç‰¹è´¨åœ¨è¡Œä¸ºä»»åŠ¡ä¸­çš„é¢„æµ‹æ•ˆåº¦ä»¥åŠé’ˆå¯¹æ€§å¹²é¢„ï¼ˆå¦‚äººæ ¼æ³¨å…¥ï¼‰å¯¹è‡ªæˆ‘æŠ¥å‘Šå’Œè¡Œä¸ºçš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼ŒæŒ‡ä»¤å¯¹é½ï¼ˆå¦‚RLHFã€æŒ‡ä»¤å¾®è°ƒï¼‰èƒ½æ˜¾è‘—ç¨³å®šç‰¹è´¨è¡¨è¾¾å’Œå¢å¼ºç‰¹è´¨å…³è”æ€§ï¼Œä¸äººç±»æ•°æ®ç›¸ä¼¼ã€‚ä½†è‡ªæˆ‘æŠ¥å‘Šçš„ç‰¹è´¨å¹¶ä¸èƒ½å¯é åœ°é¢„æµ‹è¡Œä¸ºï¼Œè§‚å¯Ÿåˆ°çš„å…³è”ä¸äººç±»æ¨¡å¼å¾€å¾€å­˜åœ¨åˆ†æ­§ã€‚è™½ç„¶äººæ ¼æ³¨å…¥èƒ½å¤ŸæˆåŠŸå¼•å¯¼è‡ªæˆ‘æŠ¥å‘Šæœç€é¢„å®šæ–¹å‘å‘å±•ï¼Œä½†å¯¹å®é™…è¡Œä¸ºçš„å½±å“è¾ƒå°æˆ–ä¸ä¸€è‡´ã€‚ç ”ç©¶åŒºåˆ†äº†è¡¨é¢ç‰¹è´¨è¡¨è¾¾å’Œè¡Œä¸ºçš„è¿è´¯æ€§ï¼ŒæŒ‘æˆ˜äº†å…³äºLLMæ€§æ ¼çš„å‡è®¾ï¼Œå¹¶å¼ºè°ƒéœ€è¦å¯¹é½å’Œè§£é‡Šæ€§è¿›è¡Œæ·±å…¥è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå±•ç°ä¸äººç±»ç›¸ä¼¼çš„æ€§æ ¼ç‰¹è´¨ï¼Œå¦‚å‹å–„æ€§å’Œè‡ªæˆ‘è°ƒèŠ‚èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒé˜¶æ®µç‰¹è´¨è½®å»“çš„åŠ¨æ€æ¼”å˜æ˜¯LLMæ€§æ ¼ç ”ç©¶çš„é‡è¦æ–¹é¢ã€‚</li>
<li>è‡ªæˆ‘æŠ¥å‘Šç‰¹è´¨åœ¨è¡Œä¸ºä»»åŠ¡ä¸­çš„é¢„æµ‹æ•ˆåº¦å—é™ï¼Œä¸äººç±»æ¨¡å¼å­˜åœ¨åˆ†æ­§ã€‚</li>
<li>æŒ‡ä»¤å¯¹é½æŠ€æœ¯èƒ½æ˜¾è‘—ç¨³å®šLLMçš„ç‰¹è´¨è¡¨è¾¾å’Œå¢å¼ºç‰¹è´¨å…³è”æ€§ã€‚</li>
<li>äººæ ¼æ³¨å…¥å¯¹LLMçš„è‡ªæˆ‘æŠ¥å‘Šå½±å“è¾ƒå¤§ï¼Œä½†å¯¹å®é™…è¡Œä¸ºçš„å½±å“è¾ƒå°æˆ–ä¸ä¸€è‡´ã€‚</li>
<li>ç ”ç©¶åŒºåˆ†äº†LLMè¡¨é¢ç‰¹è´¨è¡¨è¾¾ä¸è¡Œä¸ºè¿è´¯æ€§çš„å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d99a49bceb744538570388bf1a18617.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-817eefad2761c98722c7bc4af2766a4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0708ccf022ba691bc498c902328ac5db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b9e8a1458f57e8e0ec7db0710188716.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae67388dc4468198ec974a184c1759cb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Strefer-Empowering-Video-LLMs-with-Space-Time-Referring-and-Reasoning-via-Synthetic-Instruction-Data"><a href="#Strefer-Empowering-Video-LLMs-with-Space-Time-Referring-and-Reasoning-via-Synthetic-Instruction-Data" class="headerlink" title="Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning   via Synthetic Instruction Data"></a>Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning   via Synthetic Instruction Data</h2><p><strong>Authors:Honglu Zhou, Xiangyu Peng, Shrikant Kendre, Michael S. Ryoo, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles</strong></p>
<p>Next-generation AI companions must go beyond general video understanding to resolve spatial and temporal references in dynamic, real-world environments. Existing Video Large Language Models (Video LLMs), while capable of coarse-level comprehension, struggle with fine-grained, spatiotemporal reasoning, especially when user queries rely on time-based event references for temporal anchoring, or gestural cues for spatial anchoring to clarify object references and positions. To bridge this critical gap, we introduce Strefer, a synthetic instruction data generation framework designed to equip Video LLMs with spatiotemporal referring and reasoning capabilities. Strefer produces diverse instruction-tuning data using a data engine that pseudo-annotates temporally dense, fine-grained video metadata, capturing rich spatial and temporal information in a structured manner, including subjects, objects, their locations as masklets, and their action descriptions and timelines. Our approach enhances the ability of Video LLMs to interpret spatial and temporal references, fostering more versatile, space-time-aware reasoning essential for real-world AI companions. Without using proprietary models, costly human annotation, or the need to annotate large volumes of new videos, experimental evaluations show that models trained with data produced by Strefer outperform baselines on tasks requiring spatial and temporal disambiguation. Additionally, these models exhibit enhanced space-time-aware reasoning, establishing a new foundation for perceptually grounded, instruction-tuned Video LLMs. </p>
<blockquote>
<p>ä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½ä¼´ä¾£å¿…é¡»è¶…è¶Šä¸€èˆ¬çš„è§†é¢‘ç†è§£ï¼Œä»¥è§£å†³åŠ¨æ€ç°å®ç¯å¢ƒä¸­çš„ç©ºé—´å’Œæ—¶é—´å‚è€ƒé—®é¢˜ã€‚ç°æœ‰çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰è™½ç„¶å…·å¤‡ç²—çº§åˆ«çš„ç†è§£èƒ½åŠ›ï¼Œä½†åœ¨ç²¾ç»†çš„æ—¶ç©ºæ¨ç†æ–¹é¢å´å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯å½“ç”¨æˆ·çš„æŸ¥è¯¢ä¾èµ–äºåŸºäºæ—¶é—´çš„äº‹ä»¶å¼•ç”¨è¿›è¡Œæ—¶é—´é”šå®šï¼Œæˆ–ä¾èµ–äºæ‰‹åŠ¿çº¿ç´¢è¿›è¡Œç©ºé—´é”šå®šä»¥æ¾„æ¸…å¯¹è±¡å¼•ç”¨å’Œä½ç½®æ—¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Streferï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆæŒ‡ä»¤æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è£…å¤‡è§†é¢‘LLMså…·å¤‡æ—¶ç©ºå¼•ç”¨å’Œæ¨ç†èƒ½åŠ›ã€‚Streferä½¿ç”¨ä¸€ä¸ªæ•°æ®å¼•æ“äº§ç”Ÿå„ç§æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œè¯¥å¼•æ“å¯¹æ—¶é—´å¯†é›†ã€ç²¾ç»†çš„è§†é¢‘å…ƒæ•°æ®è¿›è¡Œä¼ªæ³¨é‡Šï¼Œä»¥ç»“æ„åŒ–çš„æ–¹å¼æ•æ‰ä¸°å¯Œçš„ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸»é¢˜ã€å¯¹è±¡ã€ä½œä¸ºè’™ç‰ˆçš„å®ƒä»¬çš„ä½ç½®ä»¥åŠåŠ¨ä½œæè¿°å’Œæ—¶é—´çº¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†è§†é¢‘LLMså¯¹ç©ºé—´å’Œæ—¶é—´å¼•ç”¨çš„è§£é‡Šèƒ½åŠ›ï¼Œä¿ƒè¿›äº†æ›´é€šç”¨ã€æ—¶ç©ºæ„ŸçŸ¥æ¨ç†èƒ½åŠ›çš„å‘å±•ï¼Œè¿™å¯¹äºç°å®ä¸–ç•Œçš„äººå·¥æ™ºèƒ½ä¼´ä¾£æ¥è¯´æ˜¯è‡³å…³é‡è¦çš„ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨Streferäº§ç”Ÿçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨éœ€è¦ç©ºé—´å’Œæ—¶æ€è¾¨æçš„ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†åŸºå‡†çº¿ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹å±•ç°å‡ºå¢å¼ºçš„æ—¶ç©ºæ„ŸçŸ¥æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ„ŸçŸ¥åŸºç¡€ã€æŒ‡ä»¤è°ƒæ•´çš„Video LLMså¥ å®šäº†æ–°åŸºç¡€ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬å¹¶æœªä½¿ç”¨ä¸“æœ‰æ¨¡å‹ã€æ˜‚è´µçš„çš„äººåŠ›æ ‡æ³¨æˆ–éœ€è¦æ ‡æ³¨å¤§é‡æ–°è§†é¢‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03501v1">PDF</a> This technical report serves as the archival version of our paper   accepted at the ICCV 2025 Workshop. For more information, please visit our   project website: <a target="_blank" rel="noopener" href="https://strefer.github.io/">https://strefer.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£AIä¼´ä¾£éœ€è¦è¶…è¶Šä¸€èˆ¬çš„è§†é¢‘ç†è§£ï¼Œä»¥è§£å†³åŠ¨æ€ç°å®ç¯å¢ƒä¸­çš„ç©ºé—´å’Œæ—¶é—´å‚è€ƒé—®é¢˜ã€‚ç°æœ‰è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰è™½ç„¶å…·å¤‡ç²—ç•¥çº§åˆ«çš„ç†è§£èƒ½åŠ›ï¼Œä½†åœ¨ç²¾ç»†ç²’åº¦çš„æ—¶ç©ºæ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ç”¨æˆ·æŸ¥è¯¢ä¾èµ–äºåŸºäºæ—¶é—´çš„äº‹ä»¶å‚è€ƒè¿›è¡Œæ—¶é—´é”šå®šï¼Œæˆ–åˆ©ç”¨å§¿æ€çº¿ç´¢è¿›è¡Œç©ºé—´é”šå®šä»¥æ¾„æ¸…å¯¹è±¡å‚è€ƒå’Œä½ç½®æ—¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Streferï¼Œä¸€ä¸ªåˆæˆæŒ‡ä»¤æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨é…å¤‡Video LLMså…·å¤‡æ—¶ç©ºå‚è€ƒå’Œæ¨ç†èƒ½åŠ›ã€‚Streferä½¿ç”¨æ•°æ®å¼•æ“äº§ç”Ÿå¤šæ ·åŒ–çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œä»¥ä¼ªæ³¨é‡Šæ–¹å¼å¯†é›†æ ‡æ³¨è§†é¢‘çš„æ—¶é—´ç²¾ç»†ç²’åº¦å…ƒæ•°æ®ï¼Œä»¥ç»“æ„åŒ–æ–¹å¼æ•æ‰ä¸°å¯Œçš„ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸»é¢˜ã€å¯¹è±¡ã€ä½œä¸ºé®ç½©çš„åœ°ç‚¹ä»¥åŠåŠ¨ä½œæè¿°å’Œæ—¶é—´çº¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†Video LLMså¯¹ç©ºé—´å’Œæ—¶é—´çš„ç†è§£èƒ½åŠ›ï¼Œä¿ƒè¿›äº†æ›´é€šç”¨çš„æ—¶ç©ºæ„ŸçŸ¥æ¨ç†èƒ½åŠ›çš„å‘å±•ï¼Œè¿™å¯¹ç°å®ä¸–ç•Œçš„AIä¼´ä¾£è‡³å…³é‡è¦ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨Streferäº§ç”Ÿçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨éœ€è¦ç©ºé—´å’Œæ—¶æ€è§£æçš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å±•ç°äº†å¢å¼ºçš„æ—¶ç©ºæ„ŸçŸ¥æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ„ŸçŸ¥åŸºç¡€çš„æŒ‡ä»¤è°ƒæ•´å‹Video LLMså¥ å®šäº†æ–°åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‹ä¸€ä»£AIéœ€è¦è§£å†³åŠ¨æ€ç°å®ç¯å¢ƒä¸­çš„ç©ºé—´å’Œæ—¶é—´å‚è€ƒé—®é¢˜ã€‚</li>
<li>ç°æœ‰Video LLMsåœ¨ç²¾ç»†ç²’åº¦çš„æ—¶ç©ºæ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>Streferæ˜¯ä¸€ä¸ªåˆæˆæŒ‡ä»¤æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºVideo LLMsçš„æ—¶ç©ºå‚è€ƒå’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>Streferä½¿ç”¨æ•°æ®å¼•æ“ä¼ªæ³¨é‡Šè§†é¢‘çš„æ—¶é—´ç²¾ç»†ç²’åº¦å…ƒæ•°æ®ã€‚</li>
<li>Streferæé«˜äº†Video LLMså¯¹ç©ºé—´å’Œæ—¶é—´çš„ç†è§£èƒ½åŠ›ï¼Œä¿ƒè¿›äº†æ›´é€šç”¨çš„æ—¶ç©ºæ„ŸçŸ¥æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚</li>
<li>ä½¿ç”¨Streferäº§ç”Ÿçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨éœ€è¦ç©ºé—´å’Œæ—¶æ€è§£æçš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0545a66520741b90c5ab46d3cb19e03b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81c4125e1ea06d0fdecbe22d46c8c4bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfc953a6a2959282045ebce4e5d5ced9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10ec375a8381dee71dadaa987615ce93.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Continuous-Saudi-Sign-Language-Recognition-A-Vision-Transformer-Approach"><a href="#Continuous-Saudi-Sign-Language-Recognition-A-Vision-Transformer-Approach" class="headerlink" title="Continuous Saudi Sign Language Recognition: A Vision Transformer   Approach"></a>Continuous Saudi Sign Language Recognition: A Vision Transformer   Approach</h2><p><strong>Authors:Soukeina Elhassen, Lama Al Khuzayem, Areej Alhothali, Ohoud Alzamzami, Nahed Alowaidi</strong></p>
<p>Sign language (SL) is an essential communication form for hearing-impaired and deaf people, enabling engagement within the broader society. Despite its significance, limited public awareness of SL often leads to inequitable access to educational and professional opportunities, thereby contributing to social exclusion, particularly in Saudi Arabia, where over 84,000 individuals depend on Saudi Sign Language (SSL) as their primary form of communication. Although certain technological approaches have helped to improve communication for individuals with hearing impairments, there continues to be an urgent requirement for more precise and dependable translation techniques, especially for Arabic sign language variants like SSL. Most state-of-the-art solutions have primarily focused on non-Arabic sign languages, resulting in a considerable absence of resources dedicated to Arabic sign language, specifically SSL. The complexity of the Arabic language and the prevalence of isolated sign language datasets that concentrate on individual words instead of continuous speech contribute to this issue. To address this gap, our research represents an important step in developing SSL resources. To address this, we introduce the first continuous Saudi Sign Language dataset called KAU-CSSL, focusing on complete sentences to facilitate further research and enable sophisticated recognition systems for SSL recognition and translation. Additionally, we propose a transformer-based model, utilizing a pretrained ResNet-18 for spatial feature extraction and a Transformer Encoder with Bidirectional LSTM for temporal dependencies, achieving 99.02% accuracy at signer dependent mode and 77.71% accuracy at signer independent mode. This development leads the way to not only improving communication tools for the SSL community but also making a substantial contribution to the wider field of sign language. </p>
<blockquote>
<p>æ‰‹è¯­ï¼ˆSLï¼‰å¯¹äºå¬éšœå’Œè‹å“‘äººç¾¤æ¥è¯´æ˜¯ä¸€ç§é‡è¦çš„æ²Ÿé€šå½¢å¼ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿèå…¥æ›´å¹¿æ³›çš„ç¤¾ä¼šã€‚å°½ç®¡æ‰‹è¯­éå¸¸é‡è¦ï¼Œä½†å…¬ä¼—å¯¹æ‰‹è¯­çš„è®¤çŸ¥æœ‰é™ï¼Œå¾€å¾€å¯¼è‡´ä»–ä»¬åœ¨æ•™è‚²å’ŒèŒä¸šæœºä¼šæ–¹é¢å­˜åœ¨ä¸å…¬å¹³çš„å¾…é‡ï¼Œä»è€ŒåŠ å‰§äº†ç¤¾ä¼šæ’æ–¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²™ç‰¹é˜¿æ‹‰ä¼¯ã€‚è¶…è¿‡8ä¸‡4åƒåä¸ªäººä¾èµ–æ²™ç‰¹æ‰‹è¯­ï¼ˆSSLï¼‰ä½œä¸ºä»–ä»¬çš„ä¸»è¦æ²Ÿé€šæ–¹å¼ã€‚å°½ç®¡æŸäº›æŠ€æœ¯æ–¹æ³•å·²ç»å¸®åŠ©å¬éšœäººå£«æ”¹å–„æ²Ÿé€šï¼Œä½†ä»ç„¶å­˜åœ¨å¯¹æ›´ç²¾ç¡®å’Œå¯é çš„ç¿»è¯‘æŠ€æœ¯çš„è¿«åˆ‡éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåƒSSLè¿™æ ·çš„é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­å˜ä½“ã€‚å¤§å¤šæ•°æœ€æ–°è§£å†³æ–¹æ¡ˆä¸»è¦å…³æ³¨éé˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­ï¼Œå¯¼è‡´ç¼ºä¹ä¸“é—¨é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­çš„èµ„æºï¼Œç‰¹åˆ«æ˜¯SSLã€‚é˜¿æ‹‰ä¼¯è¯­è¯­è¨€çš„å¤æ‚æ€§ä»¥åŠä¾§é‡äºå•ä¸ªå•è¯çš„å­¤ç«‹æ‰‹è¯­æ•°æ®é›†çš„å­˜åœ¨è€Œä¸æ˜¯è¿ç»­è¯­éŸ³ï¼ŒåŠ å‰§äº†è¿™ä¸€é—®é¢˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬çš„ç ”ç©¶åœ¨å¼€å‘SSLèµ„æºæ–¹é¢è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ç¬¬ä¸€ä¸ªè¿ç»­çš„æ²™ç‰¹æ‰‹è¯­æ•°æ®é›†KAU-CSSLï¼Œä¸“æ³¨äºå®Œæ•´çš„å¥å­ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å¹¶å¯ç”¨å…ˆè¿›çš„SSLè¯†åˆ«å’Œç¿»è¯‘ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºTransformerçš„æ¨¡å‹ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ResNet-18è¿›è¡Œç©ºé—´ç‰¹å¾æå–å’Œå¸¦æœ‰åŒå‘LSTMçš„Transformerç¼–ç å™¨è¿›è¡Œæ—¶é—´ä¾èµ–æ€§å¤„ç†ï¼Œåœ¨ç­¾çº¦äººä¾èµ–æ¨¡å¼ä¸‹è¾¾åˆ°99.02%çš„å‡†ç¡®ç‡ï¼Œåœ¨ç­¾çº¦äººç‹¬ç«‹æ¨¡å¼ä¸‹è¾¾åˆ°77.71%çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€å‘å±•ä¸ä»…ä¸ºSSLç¤¾åŒºæ”¹å–„äº†æ²Ÿé€šå·¥å…·ï¼Œè€Œä¸”ä¸ºæ›´å¹¿æ³›çš„æ‰‹è¯­é¢†åŸŸåšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03467v1">PDF</a> 23 pages, 13 figures, 5 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰‹è¯­ï¼ˆSLï¼‰å¯¹äºå¬éšœäººå£«è‡³å…³é‡è¦ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´å¹¿æ³›çš„ç¤¾ä¼šä¸­å‚ä¸äº¤æµã€‚ç„¶è€Œï¼Œå…¬ä¼—å¯¹æ‰‹è¯­çš„è®¤çŸ¥æœ‰é™ï¼Œå¯¼è‡´å¬éšœäººå£«åœ¨æ•™è‚²åŠèŒä¸šæœºä¼šæ–¹é¢é­å—ä¸å…¬å¹³å¾…é‡ï¼Œè¿›è€ŒåŠ å‰§ç¤¾ä¼šæ’æ–¥ã€‚ç‰¹åˆ«æ˜¯åœ¨æ²™ç‰¹é˜¿æ‹‰ä¼¯ï¼Œæœ‰è¶…è¿‡8.4ä¸‡äººä¾èµ–æ²™ç‰¹æ‰‹è¯­ï¼ˆSSLï¼‰ä½œä¸ºä¸»è¦çš„äº¤æµæ–¹å¼ã€‚å°½ç®¡å·²æœ‰æŠ€æœ¯æ–¹æ³•æ”¹å–„äº†å¬éšœäººå£«çš„æ²Ÿé€šçŠ¶å†µï¼Œä½†ä»æ€¥éœ€æ›´ç²¾ç¡®å’Œå¯é çš„æ‰‹è¯­ç¿»è¯‘æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­å˜ä½“å¦‚SSLã€‚å½“å‰æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆä¸»è¦å…³æ³¨éé˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­ï¼Œå¯¼è‡´é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­çš„èµ„æºåŒ®ä¹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ˜¯å¼€å‘SSLèµ„æºçš„é‡è¦ä¸€æ­¥ã€‚æˆ‘ä»¬å‘å¸ƒäº†é¦–ä¸ªè¿ç»­çš„æ²™ç‰¹æ‰‹è¯­æ•°æ®é›†KAU-CSSLï¼Œä¸“æ³¨äºå®Œæ•´çš„å¥å­ï¼Œä»¥ä¿ƒè¿›SSLè¯†åˆ«å’Œç ”ç©¶ï¼Œå¹¶æ¨åŠ¨å…ˆè¿›çš„è¯†åˆ«ç³»ç»Ÿçš„å‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºtransformerçš„æ¨¡å‹ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ResNet-18è¿›è¡Œç©ºé—´ç‰¹å¾æå–å’ŒTransformerç¼–ç å™¨ä¸åŒå‘LSTMå¤„ç†æ—¶åºä¾èµ–å…³ç³»ï¼Œåœ¨ç­¾åä¾èµ–æ¨¡å¼ä¸‹å‡†ç¡®ç‡è¾¾åˆ°äº†99.02%ï¼Œç­¾åç‹¬ç«‹æ¨¡å¼ä¸‹å‡†ç¡®ç‡è¾¾åˆ°äº†77.71%ã€‚è¿™ä¸€å‘å±•ä¸ä»…æœ‰åŠ©äºæ”¹å–„SSLç¾¤ä½“çš„æ²Ÿé€šå·¥å…·ï¼Œè€Œä¸”ä¸ºæ›´å¹¿æ³›çš„æ‰‹è¯­é¢†åŸŸåšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰‹è¯­æ˜¯å¬éšœäººå£«å’Œè‹äººå‚ä¸ç¤¾ä¼šäº¤æµçš„å…³é”®æ–¹å¼ã€‚</li>
<li>å…¬ä¼—å¯¹æ‰‹è¯­çš„è®¤çŸ¥æœ‰é™å¯¼è‡´ç¤¾äº¤æ’æ–¥é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²™ç‰¹é˜¿æ‹‰ä¼¯ç­‰å›½å®¶ã€‚</li>
<li>å½“å‰çš„æŠ€æœ¯å’Œèµ„æºä¸»è¦å…³æ³¨éé˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­ï¼Œç¼ºä¹é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­æ‰‹è¯­ï¼ˆå¦‚SSLï¼‰çš„èµ„æºã€‚</li>
<li>æå‡ºé¦–ä¸ªè¿ç»­çš„æ²™ç‰¹æ‰‹è¯­æ•°æ®é›†KAU-CSSLï¼Œä¸“æ³¨äºå®Œæ•´å¥å­çš„æ‰‹è¯­è¯†åˆ«å’Œç¿»è¯‘ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºä¸€ç§åŸºäºtransformerçš„æ¨¡å‹ï¼Œç”¨äºæé«˜SSLè¯†åˆ«å’Œç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆç©ºé—´ç‰¹å¾æå–å’Œæ—¶åºä¾èµ–å¤„ç†ï¼Œå®ç°è¾ƒé«˜çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24feeaecabc76ac9906185bf0e998888.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad87b00d0cc82c0e4cb3bf3255c10a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cee333334c9458fb131b826e58afd3cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efe927fc7dcd9e36b9cf5cfa5ce9f8d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c0828ce638aade094a1bf98868a197b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fb7a2044ccd753d6019f5c53c362fe4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SESGO-Spanish-Evaluation-of-Stereotypical-Generative-Outputs"><a href="#SESGO-Spanish-Evaluation-of-Stereotypical-Generative-Outputs" class="headerlink" title="SESGO: Spanish Evaluation of Stereotypical Generative Outputs"></a>SESGO: Spanish Evaluation of Stereotypical Generative Outputs</h2><p><strong>Authors:Melissa Robles, Catalina Bernal, Denniss Raigoso, Mateo Dulce Rubio</strong></p>
<p>This paper addresses the critical gap in evaluating bias in multilingual Large Language Models (LLMs), with a specific focus on Spanish language within culturally-aware Latin American contexts. Despite widespread global deployment, current evaluations remain predominantly US-English-centric, leaving potential harms in other linguistic and cultural contexts largely underexamined. We introduce a novel, culturally-grounded framework for detecting social biases in instruction-tuned LLMs. Our approach adapts the underspecified question methodology from the BBQ dataset by incorporating culturally-specific expressions and sayings that encode regional stereotypes across four social categories: gender, race, socioeconomic class, and national origin. Using more than 4,000 prompts, we propose a new metric that combines accuracy with the direction of error to effectively balance model performance and bias alignment in both ambiguous and disambiguated contexts. To our knowledge, our work presents the first systematic evaluation examining how leading commercial LLMs respond to culturally specific bias in the Spanish language, revealing varying patterns of bias manifestation across state-of-the-art models. We also contribute evidence that bias mitigation techniques optimized for English do not effectively transfer to Spanish tasks, and that bias patterns remain largely consistent across different sampling temperatures. Our modular framework offers a natural extension to new stereotypes, bias categories, or languages and cultural contexts, representing a significant step toward more equitable and culturally-aware evaluation of AI systems in the diverse linguistic environments where they operate. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°ä¸­çš„å…³é”®ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡åŒ–æ„è¯†å¼ºçƒˆçš„æ‹‰ä¸ç¾æ´²èƒŒæ™¯ä¸‹çš„è¥¿ç­ç‰™è¯­è¯­è¨€è¯„ä¼°ã€‚å°½ç®¡åœ¨å…¨çƒèŒƒå›´å†…å¹¿æ³›åº”ç”¨ï¼Œä½†å½“å‰çš„è¯„ä¼°ä»ç„¶ä¸»è¦ä»¥ç¾å›½è‹±è¯­ä¸ºä¸­å¿ƒï¼Œå¯¼è‡´åœ¨å…¶ä»–è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹çš„æ½œåœ¨å±å®³åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«å¿½è§†ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ã€ä»¥æ–‡åŒ–ä¸ºåŸºç¡€çš„å¤šè¯­è¨€æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹æŒ‡ä»¤å¾®è°ƒLLMä¸­çš„ç¤¾ä¼šåè§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨BBQæ•°æ®é›†ä¸­çš„æœªæŒ‡å®šé—®é¢˜æ–¹æ³•è®ºï¼Œé€šè¿‡èå…¥åŒ…å«å››ä¸ªç¤¾ä¼šç±»åˆ«ï¼ˆæ€§åˆ«ã€ç§æ—ã€ç¤¾ä¼šç»æµé˜¶å±‚å’Œæ°‘æ—èµ·æºï¼‰çš„åŒºåŸŸåˆ»æ¿å°è±¡çš„æ–‡åŒ–ç‰¹å®šè¡¨è¾¾æ–¹å¼å’Œè¯´æ³•ã€‚ä½¿ç”¨è¶…è¿‡4,000ä¸ªæç¤ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œè¯¥æ ‡å‡†ç»“åˆäº†å‡†ç¡®æ€§å’Œè¯¯å·®æ–¹å‘ï¼Œä»¥åœ¨æ¨¡ç³Šå’Œæ˜ç¡®çš„è¯­å¢ƒä¸­æœ‰æ•ˆå¹³è¡¡æ¨¡å‹æ€§èƒ½å’Œåè§å¯¹é½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„å·¥ä½œé¦–æ¬¡ç³»ç»Ÿåœ°è¯„ä¼°äº†é¢†å…ˆçš„å•†ä¸šLLMå¦‚ä½•åº”å¯¹è¥¿ç­ç‰™è¯­ä¸­çš„ç‰¹å®šåè§ï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹ä¹‹é—´åè§è¡¨ç°çš„å·®å¼‚æ¨¡å¼ã€‚æˆ‘ä»¬è¿˜æä¾›è¯æ®æ˜¾ç¤ºï¼Œé’ˆå¯¹è‹±è¯­ä¼˜åŒ–çš„åè§ç¼“è§£æŠ€æœ¯å¹¶ä¸èƒ½æœ‰æ•ˆåœ°è½¬ç§»åˆ°è¥¿ç­ç‰™è¯­ä»»åŠ¡ï¼Œå¹¶ä¸”åè§æ¨¡å¼åœ¨ä¸åŒçš„é‡‡æ ·æ¸©åº¦ä¹‹é—´ä»ç„¶ä¿æŒé«˜åº¦ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å—åŒ–æ¡†æ¶ä¸ºæ–°çš„åˆ»æ¿å°è±¡ã€åè§ç±»åˆ«æˆ–è¯­è¨€å’Œæ–‡åŒ–ç¯å¢ƒæä¾›äº†è‡ªç„¶çš„æ‰©å±•ï¼Œè¿™æ˜¯åœ¨å¤šå…ƒçš„è¯­è¨€ç¯å¢ƒä¸­å®ç°æ›´åŠ å…¬å¹³å’Œæ–‡åŒ–æ„è¯†çš„AIç³»ç»Ÿè¯„ä¼°çš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03329v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡å…³æ³¨å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯„ä¼°åè§æ–¹é¢çš„ç©ºç™½ï¼Œç‰¹åˆ«å…³æ³¨æ‹‰ä¸ç¾æ´²æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¥¿ç­ç‰™è¯­ã€‚å½“å‰è¯„ä¼°ä¸»è¦åå‘ç¾å›½è‹±è¯­è¯­å¢ƒï¼Œå¿½è§†äº†å…¶ä»–è¯­è¨€å’Œæ–‡åŒ–çš„æ½œåœ¨å±å®³ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§æ–°çš„æ–‡åŒ–æ ¹åŸºæ¡†æ¶æ¥æ£€æµ‹æŒ‡ä»¤è®­ç»ƒLLMä¸­çš„ç¤¾ä¼šåè§ã€‚è¯¥ç ”ç©¶ç»“åˆäº†æ–‡åŒ–ç‰¹å®šè¡¨è¾¾å’Œè°šè¯­ï¼Œæå‡ºä¸€ç§æ–°æŒ‡æ ‡æ¥å¹³è¡¡æ¨¡å‹æ€§èƒ½å’Œåè§å¯¹é½ã€‚ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿåœ°è¯„ä¼°äº†å•†ä¸šé¢†å…ˆLLMå¦‚ä½•åº”å¯¹è¥¿ç­ç‰™è¯­ä¸­çš„æ–‡åŒ–ç‰¹å®šåè§ï¼Œå‘ç°ä¸åŒæ¨¡å‹åœ¨åè§è¡¨ç°ä¸Šçš„ä¸åŒæ¨¡å¼ã€‚ç ”ç©¶è¿˜å‘ç°é’ˆå¯¹è‹±è¯­çš„åè§ç¼“è§£æŠ€æœ¯å¹¶ä¸é€‚ç”¨äºè¥¿ç­ç‰™è¯­ä»»åŠ¡ï¼Œä¸”åè§æ¨¡å¼åœ¨ä¸åŒé‡‡æ ·æ¸©åº¦ä¸‹å¤§ä½“ä¸€è‡´ã€‚è¯¥ç ”ç©¶æ¨¡å—åŒ–æ¡†æ¶æ˜“äºæ‰©å±•åˆ°æ–°çš„åˆ»æ¿å°è±¡ã€åè§ç±»åˆ«æˆ–è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ï¼Œæ˜¯æœç€æ›´å…¬å¹³å’Œæ–‡åŒ–æ„è¯†çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿè¯„ä¼°è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å…³æ³¨å¤šè¯­è¨€LLMæ¨¡å‹åœ¨è¯„ä¼°åè§æ–¹é¢çš„ä¸è¶³ï¼Œå¼ºè°ƒéœ€è¦æ›´å…¨é¢çš„æ–¹æ³•ä»¥é€‚åº”ä¸åŒè¯­è¨€å’Œæ–‡åŒ–çš„è¯­å¢ƒã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†å¯¹è¥¿ç­ç‰™è¯­ç­‰è¯­è¨€çš„é‡è§†ï¼Œå› ä¸ºå®ƒä»£è¡¨äº†å…¨çƒè®¸å¤šåœ°åŒºçš„è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†æ–°çš„æ–‡åŒ–æ ¹åŸºæ¡†æ¶æ¥æ£€æµ‹LLMä¸­çš„ç¤¾ä¼šåè§ï¼Œç»“åˆäº†æ–‡åŒ–ç‰¹å®šè¡¨è¾¾å’Œè°šè¯­æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿåœ°è¯„ä¼°äº†å•†ä¸šé¢†å…ˆLLMå¦‚ä½•åº”å¯¹è¥¿ç­ç‰™è¯­ä¸­çš„æ–‡åŒ–ç‰¹å®šåè§ï¼Œå‘ç°ä¸åŒæ¨¡å‹åœ¨å¤„ç†åè§æ–¹é¢çš„å·®å¼‚ã€‚</li>
<li>ç ”ç©¶å‘ç°é’ˆå¯¹è‹±è¯­çš„åè§ç¼“è§£æŠ€æœ¯å¹¶ä¸é€‚ç”¨äºè¥¿ç­ç‰™è¯­ä»»åŠ¡ï¼Œè¿™è¡¨æ˜å¯¹ä¸åŒè¯­è¨€çš„æ–‡åŒ–é€‚åº”æ€§åœ¨äººå·¥æ™ºèƒ½æ¨¡å‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶æ˜¾ç¤ºåè§æ¨¡å¼åœ¨ä¸åŒé‡‡æ ·æ¸©åº¦ä¸‹å¤§ä½“ä¸€è‡´ï¼Œè¿™è¡¨æ˜æ¸©åº¦é‡‡æ ·å¹¶ä¸èƒ½è§£å†³è¯­è¨€æ¨¡å‹ä¸­çš„åè§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b729440b6b07b86d8e4f6170ca9c76b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f094133d568013050e80de04c90e17c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f05c632027bb2a4c375769bf91850abe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b644c0f35711c9d59bb1c5545d7f024d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AgenTracer-Who-Is-Inducing-Failure-in-the-LLM-Agentic-Systems"><a href="#AgenTracer-Who-Is-Inducing-Failure-in-the-LLM-Agentic-Systems" class="headerlink" title="AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?"></a>AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?</h2><p><strong>Authors:Guibin Zhang, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, Shuicheng Yan</strong></p>
<p>Large Language Model (LLM)-based agentic systems, often comprising multiple models, complex tool invocations, and orchestration protocols, substantially outperform monolithic agents. Yet this very sophistication amplifies their fragility, making them more prone to system failure. Pinpointing the specific agent or step responsible for an error within long execution traces defines the task of agentic system failure attribution. Current state-of-the-art reasoning LLMs, however, remain strikingly inadequate for this challenge, with accuracy generally below 10%. To address this gap, we propose AgenTracer, the first automated framework for annotating failed multi-agent trajectories via counterfactual replay and programmed fault injection, producing the curated dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a lightweight failure tracer trained with multi-granular reinforcement learning, capable of efficiently diagnosing errors in verbose multi-agent interactions. On the Who&amp;When benchmark, AgenTracer-8B outperforms giant proprietary LLMs like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS with 4.8-14.2% performance gains, empowering self-correcting and self-evolving agentic AI. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†ç³»ç»Ÿï¼Œé€šå¸¸åŒ…å«å¤šä¸ªæ¨¡å‹ã€å¤æ‚çš„å·¥å…·è°ƒç”¨å’ŒååŒåè®®ï¼Œåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå•ä¸€ä»£ç†ã€‚ç„¶è€Œï¼Œè¿™ç§å¤æ‚æ€§ä¹Ÿå¢åŠ äº†å…¶è„†å¼±æ€§ï¼Œä½¿å…¶æ›´å®¹æ˜“å‡ºç°ç³»ç»Ÿæ•…éšœã€‚ç¡®å®šé•¿æ‰§è¡Œè½¨è¿¹ä¸­å¯¼è‡´é”™è¯¯çš„å…·ä½“ä»£ç†æˆ–æ­¥éª¤ï¼Œå°±æ˜¯ä»£ç†ç³»ç»Ÿå¤±è´¥å½’å› çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨ç†å‹LLMå¯¹æ­¤æŒ‘æˆ˜ä»ç„¶æ˜¾å¾—æ˜æ˜¾ä¸è¶³ï¼Œå‡†ç¡®ç‡é€šå¸¸ä½äº10%ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†AgenTracerï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡åäº‹å®å›æ”¾å’Œç¼–ç¨‹æ•…éšœæ³¨å…¥æ¥æ³¨é‡Šå¤±è´¥çš„å¤šä»£ç†è½¨è¿¹çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œäº§ç”Ÿäº†ç²¾é€‰æ•°æ®é›†TracerTrajã€‚åˆ©ç”¨è¿™ä¸€èµ„æºï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè½»é‡çº§çš„å¤±è´¥è¿½è¸ªå™¨AgenTracer-8Bï¼Œå®ƒé‡‡ç”¨å¤šç²’åº¦å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¯Šæ–­å†—é•¿çš„å¤šä»£ç†äº¤äº’ä¸­çš„é”™è¯¯ã€‚åœ¨Who&amp;WhenåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgenTracer-8Bçš„æ€§èƒ½ä¼˜äºè¯¸å¦‚Gemini-2.5-Proå’ŒClaude-4-Sonnetç­‰å¤§å‹ä¸“æœ‰LLMï¼Œé«˜å‡ºé«˜è¾¾18.18%ï¼Œåœ¨LLMä»£ç†å¤±è´¥å½’å› æ–¹é¢æ ‘ç«‹äº†æ–°çš„æ ‡å‡†ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒAgenTracer-8Bä¸ºç°æˆçš„å¤šä»£ç†ç³»ç»Ÿï¼ˆå¦‚MetaGPTå’ŒMaASï¼‰æä¾›äº†å¯æ“ä½œçš„åé¦ˆï¼Œå®ç°äº†4.8%~14.2%çš„æ€§èƒ½æå‡ï¼Œèµ‹äºˆäº†ä»£ç†å‹äººå·¥æ™ºèƒ½è‡ªæˆ‘çº æ­£å’Œè‡ªæˆ‘è¿›åŒ–çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03312v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM-based agenticç³»ç»Ÿå› å…¶å¤æ‚æ€§è€Œå±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†ç³»ç»Ÿå¤±è´¥çš„è„†å¼±æ€§ã€‚å¤šä»£ç†è½¨è¿¹æ ‡æ³¨çš„è‡ªåŠ¨åŒ–æ¡†æ¶AgenTraceré€šè¿‡å‡è®¾æ€§å›æ”¾å’Œç¨‹åºæ•…éšœæ³¨å…¥äº§ç”Ÿæ•°æ®é›†TracerTrajï¼Œèƒ½æœ‰æ•ˆè¯Šæ–­å†—é•¿å¤šä»£ç†äº¤äº’ä¸­çš„é”™è¯¯ã€‚æ–°å¼€å‘çš„å¤±è´¥è¿½è¸ªå™¨AgenTracer-8Båœ¨å¤šä»£ç†å¤±è´¥å½’å› æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå¤§å‹ä¸“æœ‰LLMï¼Œå¹¶ä¸ºç°æœ‰å¤šä»£ç†ç³»ç»Ÿæä¾›å¯æ“ä½œçš„åé¦ˆï¼Œå®ç°è‡ªæˆ‘ä¿®æ­£å’Œè¿›åŒ–çš„ä»£ç†AIã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based agenticç³»ç»Ÿå…·æœ‰å“è¶Šæ€§èƒ½ä½†è„†å¼±æ€§è¾ƒé«˜ï¼Œæ˜“å—åˆ°ç³»ç»Ÿæ•…éšœçš„å½±å“ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„LLMåœ¨å¤„ç†ä»£ç†ç³»ç»Ÿæ•…éšœå½’å› æ—¶å­˜åœ¨ä¸è¶³ï¼Œå‡†ç¡®åº¦ä¸€èˆ¬ä½äº10%ã€‚</li>
<li>AgenTraceræ˜¯é¦–ä¸ªé€šè¿‡å‡è®¾æ€§å›æ”¾å’Œç¨‹åºæ•…éšœæ³¨å…¥æ ‡æ³¨å¤±è´¥çš„å¤šä»£ç†è½¨è¿¹çš„è‡ªåŠ¨åŒ–æ¡†æ¶ã€‚</li>
<li>TracerTrajæ•°æ®é›†ç”¨äºè®­ç»ƒå¤±è´¥è¿½è¸ªå™¨ï¼Œå¯æœ‰æ•ˆè¯Šæ–­å†—é•¿å¤šä»£ç†äº¤äº’ä¸­çš„é”™è¯¯ã€‚</li>
<li>AgenTracer-8Båœ¨å¤šä»£ç†å¤±è´¥å½’å› æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå¤§å‹ä¸“æœ‰LLMã€‚</li>
<li>AgenTracer-8Bå¯ä¸ºç°æœ‰çš„å¤šä»£ç†ç³»ç»Ÿæä¾›å¯æ“ä½œçš„åé¦ˆï¼Œä¿ƒè¿›è‡ªæˆ‘ä¿®æ­£å’Œè¿›åŒ–çš„ä»£ç†AIã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67e904c0e4730b1e7304e33fafd8468a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71825900da7db7efc0aba05d495c64e5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Plan-Verification-for-LLM-Based-Embodied-Task-Completion-Agents"><a href="#Plan-Verification-for-LLM-Based-Embodied-Task-Completion-Agents" class="headerlink" title="Plan Verification for LLM-Based Embodied Task Completion Agents"></a>Plan Verification for LLM-Based Embodied Task Completion Agents</h2><p><strong>Authors:Ananth Hariharan, Vardhan Dongre, Dilek Hakkani-TÃ¼r, Gokhan Tur</strong></p>
<p>Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœºå™¨äººä»»åŠ¡è®¡åˆ’ä»¥åŠç›¸åº”çš„äººç±»æ¼”ç¤ºå¯èƒ½ä¼šå­˜åœ¨å™ªå£°ï¼ŒåŒ…å«ä¸å¿…è¦çš„åŠ¨ä½œã€å†—ä½™çš„å¯¼èˆªä»¥åŠé€»è¾‘é”™è¯¯ï¼Œä»è€Œé™ä½ç­–ç•¥è´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿­ä»£éªŒè¯æ¡†æ¶ï¼Œå…¶ä¸­Judge LLMå¯¹åŠ¨ä½œåºåˆ—è¿›è¡Œæ‰¹åˆ¤ï¼Œè€ŒPlanner LLMåº”ç”¨ä¿®è®¢æ„è§ï¼Œä»è€Œäº§ç”Ÿè¶Šæ¥è¶Šå¹²å‡€ã€ç©ºé—´è¿è´¯æ€§è¶Šæ¥è¶Šå¼ºçš„è½¨è¿¹ã€‚ä¸åŒäºåŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¾èµ–äºè‡ªç„¶è¯­è¨€æç¤ºï¼Œå®ç°å¯¹å¤šç§é”™è¯¯ç±»å‹çš„å¹¿æ³›æ¦‚æ‹¬ï¼ŒåŒ…æ‹¬æ— å…³åŠ¨ä½œã€çŸ›ç›¾ä»¥åŠç¼ºå¤±æ­¥éª¤ã€‚åœ¨æ¥è‡ªTEAChæœºå™¨äººæ•°æ®é›†çš„æ‰‹åŠ¨æ³¨é‡ŠåŠ¨ä½œé›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å››ç§æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT o4-miniã€DeepSeek-R1ã€Gemini 2.5ã€LLaMA 4 Scoutï¼‰ä¸Šå®ç°äº†é«˜è¾¾90%çš„æŸ¥å…¨ç‡å’Œ100%çš„ç²¾ç¡®åº¦ã€‚ä¼˜åŒ–å¾ªç¯å¿«é€Ÿæ”¶æ•›ï¼Œ96.5%çš„åºåˆ—æœ€å¤šéœ€è¦ä¸‰æ¬¡è¿­ä»£ï¼ŒåŒæ—¶æé«˜æ—¶é—´æ•ˆç‡å’Œç©ºé—´åŠ¨ä½œç»„ç»‡ã€‚å…³é”®çš„æ˜¯ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†äººç±»é”™è¯¯æ¢å¤æ¨¡å¼è€Œä¸æ˜¯å¿½ç•¥å®ƒä»¬ï¼Œä¸ºä»Šåçš„ç¨³å¥çº æ­£è¡Œä¸ºç ”ç©¶æä¾›æ”¯æŒã€‚é€šè¿‡ç¡®ç«‹è®¡åˆ’éªŒè¯ä½œä¸ºå¯é çš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›ï¼Œç”¨äºç©ºé—´è§„åˆ’å’ŒåŠ¨ä½œä¼˜åŒ–ï¼Œæˆ‘ä»¬ä¸ºæœºå™¨äººå­¦ä¹ ä¸­æ¨¡ä»¿å­¦ä¹ çš„é«˜å“è´¨è®­ç»ƒæ•°æ®æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02761v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„AIä»»åŠ¡è®¡åˆ’å’Œç›¸åº”çš„äººç±»æ¼”ç¤ºå¯èƒ½å­˜åœ¨å™ªå£°ï¼ŒåŒ…æ‹¬ä¸å¿…è¦çš„åŠ¨ä½œã€å†—ä½™å¯¼èˆªå’Œé€»è¾‘é”™è¯¯ç­‰ï¼Œå¯¼è‡´ç­–ç•¥è´¨é‡ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿­ä»£éªŒè¯æ¡†æ¶ï¼Œç”±åˆ¤æ–­LLMå¯¹åŠ¨ä½œåºåˆ—è¿›è¡Œæ‰¹åˆ¤ï¼Œè§„åˆ’LLMè¿›è¡Œä¿®è®¢ï¼Œç”Ÿæˆè¶Šæ¥è¶Šæ¸…æ™°ã€ç©ºé—´è¿è´¯çš„è½¨è¿¹ã€‚ä¸åŒäºåŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¾èµ–äºè‡ªç„¶è¯­è¨€æç¤ºï¼Œèƒ½å¤Ÿå¹¿æ³›æ¦‚æ‹¬å„ç§é”™è¯¯ç±»å‹ï¼ŒåŒ…æ‹¬æ— å…³åŠ¨ä½œã€çŸ›ç›¾ã€é—æ¼æ­¥éª¤ç­‰ã€‚åœ¨TEACh AIæ•°æ®é›†ä¸Šçš„ä¸€ç»„æ‰‹åŠ¨æ ‡æ³¨åŠ¨ä½œä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å››ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å®ç°äº†é«˜è¾¾90%çš„å¬å›ç‡å’Œ100%çš„ç²¾ç¡®åº¦ã€‚æ”¹è¿›å¾ªç¯è¿…é€Ÿæ”¶æ•›ï¼Œå¤§å¤šæ•°åºåˆ—æœ€å¤šéœ€è¦ä¸‰æ¬¡è¿­ä»£ï¼Œæé«˜äº†æ—¶é—´æ•ˆç‡å¹¶ä¼˜åŒ–äº†ç©ºé—´è¡ŒåŠ¨ç»„ç»‡ã€‚è¯¥æ–¹æ³•ä¿ç•™äº†äººç±»é”™è¯¯æ¢å¤æ¨¡å¼ï¼Œä¸ºä»Šåçš„çº æ­£è¡Œä¸ºç ”ç©¶æä¾›äº†æ”¯æŒã€‚é€šè¿‡å»ºç«‹å¯é çš„è®¡åˆ’éªŒè¯ä½œä¸ºLLMåœ¨ç©ºé—´è§„åˆ’å’Œè¡ŒåŠ¨æ”¹è¿›æ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºæ¨¡ä»¿å­¦ä¹ åœ¨å®ä½“AIä¸­æä¾›äº†ä¸€æ¡é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„å¯æ‰©å±•è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs can detect and critique noisy action sequences in embodied AI task plans.</li>
<li>An iterative verification framework with Judge LLM and Planner LLM is proposed to improve trajectory quality.</li>
<li>The framework achieves high recall and precision on manually annotated actions from the TEACh dataset.</li>
<li>The refinement loop converges quickly, with most sequences requiring only a few iterations.</li>
<li>The method preserves human error-recovery patterns, supporting future work on robust corrective behavior.</li>
<li>The framework establishes plan verification as a reliable LLM capability for spatial planning and action refinement.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-460a8b0f0889b4930e3bb6d1cf576174.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f2a488f052b24539e9a2bf70a418bf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5af66958361bc5aa8165ef4d7ee2128.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5010aa281127e9dd3f95287a5d3ef7c6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Enhancing-Large-Language-Model-for-Knowledge-Graph-Completion-via-Structure-Aware-Alignment-Tuning"><a href="#Enhancing-Large-Language-Model-for-Knowledge-Graph-Completion-via-Structure-Aware-Alignment-Tuning" class="headerlink" title="Enhancing Large Language Model for Knowledge Graph Completion via   Structure-Aware Alignment-Tuning"></a>Enhancing Large Language Model for Knowledge Graph Completion via   Structure-Aware Alignment-Tuning</h2><p><strong>Authors:Yu Liu, Yanan Cao, Xixun Lin, Yanmin Shang, Shi Wang, Shirui Pan</strong></p>
<p>Knowledge graph completion (KGC) aims to infer new knowledge and make predictions from knowledge graphs. Recently, large language models (LLMs) have exhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily focus on designing task-specific instructions, achieving promising advancements. However, there are still two critical challenges. First, existing methods often ignore the inconsistent representation spaces between natural language and graph structures. Second, most approaches design separate instructions for different KGC tasks, leading to duplicate works and time-consuming processes. To address these challenges, we propose SAT, a novel framework that enhances LLMs for KGC via structure-aware alignment-tuning. Specifically, we first introduce hierarchical knowledge alignment to align graph embeddings with the natural language space through multi-task contrastive learning. Then, we propose structural instruction tuning to guide LLMs in performing structure-aware reasoning over KGs, using a unified graph instruction combined with a lightweight knowledge adapter. Experimental results on two KGC tasks across four benchmark datasets demonstrate that SAT significantly outperforms state-of-the-art methods, especially in the link prediction task with improvements ranging from 8.7% to 29.8%. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¨åœ¨ä»çŸ¥è¯†å›¾è°±ä¸­æ¨æ–­æ–°çŸ¥è¯†å¹¶è¿›è¡Œé¢„æµ‹ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºäº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚LLMå¢å¼ºçš„KGCæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è®¾è®¡é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ–¹æ³•æŒ‡ä»¤ï¼Œå¹¶å–å¾—äº†å¯å–œçš„è¿›å±•ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†è‡ªç„¶è¯­è¨€ä¸å›¾å½¢ç»“æ„ä¹‹é—´ä¸ä¸€è‡´çš„è¡¨ç¤ºç©ºé—´ã€‚å…¶æ¬¡ï¼Œå¤§å¤šæ•°æ–¹æ³•ä¸ºä¸åŒçš„KGCä»»åŠ¡è®¾è®¡äº†å•ç‹¬çš„æ–¹æ³•æŒ‡ä»¤ï¼Œå¯¼è‡´å·¥ä½œé‡å¤ä¸”è€—æ—¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SATï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç»“æ„æ„ŸçŸ¥å¯¹é½è°ƒæ•´å¢å¼ºLLMçš„æ–°æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥åˆ†å±‚çŸ¥è¯†å¯¹é½ï¼Œé€šè¿‡å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ å°†å›¾å½¢åµŒå…¥ä¸è‡ªç„¶è¯­è¨€ç©ºé—´å¯¹é½ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºç»“æ„æŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼LLMåœ¨çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œç»“æ„æ„ŸçŸ¥æ¨ç†ï¼Œä½¿ç”¨ç»Ÿä¸€çš„å›¾å½¢æŒ‡ä»¤å’Œè½»é‡çº§çŸ¥è¯†é€‚é…å™¨ã€‚åœ¨ä¸¤ä¸ªKGCä»»åŠ¡çš„å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSATæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå°¤å…¶åœ¨é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­çš„æ”¹è¿›èŒƒå›´ä»8.7%åˆ°29.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01166v1">PDF</a> EMNLP 2025, Main, Long Paper</p>
<p><strong>æ‘˜è¦</strong><br>    LLMå¢å¼ºçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ–¹æ³•ä¸»è¦å…³æ³¨è®¾è®¡ç‰¹å®šä»»åŠ¡æŒ‡ä»¤ï¼Œå¹¶å–å¾—æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½ç•¥äº†è‡ªç„¶è¯­è¨€ä¸å›¾å½¢ç»“æ„ä¹‹é—´çš„ä¸ä¸€è‡´è¡¨ç¤ºç©ºé—´ï¼Œå¹¶ä¸ºä¸åŒçš„KGCä»»åŠ¡è®¾è®¡å•ç‹¬çš„æŒ‡ä»¤ï¼Œå¯¼è‡´å·¥ä½œé‡å¤å’Œè€—æ—¶ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºSATæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„æ„ŸçŸ¥å¯¹é½è°ƒæ•´å¢å¼ºLLMè¿›è¡ŒKGCã€‚æˆ‘ä»¬å¼•å…¥åˆ†å±‚çŸ¥è¯†å¯¹é½å’Œç»“æ„åŒ–æŒ‡ä»¤è°ƒæ•´ï¼Œå®ç°çŸ¥è¯†å›¾è°±çš„ç»“æ„æ„ŸçŸ¥æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSATåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ä¸¤ä¸ªKGCä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­çš„æ”¹è¿›èŒƒå›´ä»8.7%åˆ°29.8%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¨åœ¨ä»çŸ¥è¯†å›¾è°±ä¸­æ¨æ–­æ–°çŸ¥è¯†å¹¶è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰LLMå¢å¼ºçš„KGCæ–¹æ³•ä¸»è¦å…³æ³¨è®¾è®¡ç‰¹å®šä»»åŠ¡æŒ‡ä»¤ï¼Œä½†ä»é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸ä¸€è‡´çš„è¡¨ç¤ºç©ºé—´å’Œé‡å¤è€—æ—¶çš„å·¥ä½œæµç¨‹ã€‚</li>
<li>SATæ¡†æ¶é€šè¿‡ç»“æ„æ„ŸçŸ¥å¯¹é½è°ƒæ•´å¢å¼ºLLMè¿›è¡ŒKGCï¼Œè§£å†³ä»¥ä¸ŠæŒ‘æˆ˜ã€‚</li>
<li>SATå¼•å…¥åˆ†å±‚çŸ¥è¯†å¯¹é½ï¼Œé€šè¿‡å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ å°†å›¾å½¢åµŒå…¥ä¸è‡ªç„¶è¯­è¨€ç©ºé—´å¯¹é½ã€‚</li>
<li>SATæå‡ºç»“æ„åŒ–æŒ‡ä»¤è°ƒæ•´ï¼Œä½¿ç”¨ç»Ÿä¸€çš„å›¾å½¢æŒ‡ä»¤å’Œè½»é‡çº§çŸ¥è¯†é€‚é…å™¨ï¼Œå¼•å¯¼LLMè¿›è¡Œç»“æ„æ„ŸçŸ¥æ¨ç†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSATåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„KGCä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5022dad64f42670a38417353f1e51a75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b08cfcaf26e8527bb118c0bf1d38bc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aec31d3ef9c446813f9d6e003460a142.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c24d273564b47bbe404ee9a4e1352f01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8afb58719b671e20bb7990f219fbc833.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4afb11ac9b9c6bd35e361fb94c055f65.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SCOUT-Toward-Sub-Quadratic-Attention-via-Segment-Compression-for-Optimized-Utility-in-Transformers"><a href="#SCOUT-Toward-Sub-Quadratic-Attention-via-Segment-Compression-for-Optimized-Utility-in-Transformers" class="headerlink" title="SCOUT: Toward Sub-Quadratic Attention via Segment Compression for   Optimized Utility in Transformers"></a>SCOUT: Toward Sub-Quadratic Attention via Segment Compression for   Optimized Utility in Transformers</h2><p><strong>Authors:Aref Jafari, Yuhe Fan, Benyamin Jamialahmadi, Parsa Farinneya, Boxing Chen, Marzieh S. Tahaei</strong></p>
<p>Transformers have demonstrated strong performance across a wide range of sequence modeling tasks, but their quadratic attention complexity limits scalability to long sequences. Linear models such as Mamba and sliding-window attention (SWA) address this by mixing tokens through recurrent or localized operations with fixed-size memory, achieving efficient inference. However, these methods risk degrading performance on long sequences due to their inability to retain detailed information from distant tokens. We propose SCOUT (Segment Compression for Optimized Utility in Transformers), a hybrid architecture that compresses tokens locally within fixed-size segments and applies attention only over these compressed representations. Each token embedding is first enriched via a linear local mixer, Mamba or SWA, that integrates recent context. Then, instead of attending to all previous tokens, each token sparsely attends to a small number of compressed checkpoint tokens that summarize the input history. This design retains much of the expressivity of full attention while substantially reducing the computational and memory cost. By attending to compressed history rather than all previous tokens, SCOUT incurs slightly higher memory than purely linear models, but its growth rate remains sub-quadratic and far more scalable than that of full Transformers. We analyze SCOUTâ€™s computational and memory efficiency and evaluate it empirically on long-context language modeling and reasoning tasks. SCOUT with both Mamba and SWA mixers outperforms strong long-sequence baselines under the same computational budget, matches full-attention Transformers on language modeling and common-sense reasoning tasks at 400M and 1.3B scales. Moreover, our SCOUT achieves higher end-to-end throughput than SOTA models, while delivering comparable results on long sequence benchmarks. </p>
<blockquote>
<p>Transformeråœ¨å¹¿æ³›çš„åºåˆ—å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å…¶äºŒæ¬¡æ–¹çš„æ³¨æ„åŠ›å¤æ‚åº¦é™åˆ¶äº†å…¶å¯¹é•¿åºåˆ—çš„å¯æ‰©å±•æ€§ã€‚åƒMambaå’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆSWAï¼‰è¿™æ ·çš„çº¿æ€§æ¨¡å‹é€šè¿‡é€’å½’æˆ–å±€éƒ¨æ“ä½œä¸å›ºå®šå¤§å°çš„å†…å­˜ç»“åˆä»¤ç‰Œï¼Œå®ç°äº†é«˜æ•ˆçš„æ¨ç†ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç”±äºæ— æ³•ä¿ç•™æ¥è‡ªè¿œå¤„ä»¤ç‰Œçš„è¯¦ç»†ä¿¡æ¯ï¼Œæœ‰å¯èƒ½åœ¨é•¿åºåˆ—ä¸Šé™ä½æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†SCOUTï¼ˆç”¨äºTransformerä¼˜åŒ–çš„åˆ†æ®µå‹ç¼©ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¶æ„ï¼Œå®ƒåœ¨å›ºå®šå¤§å°çš„åˆ†æ®µå†…å±€éƒ¨å‹ç¼©ä»¤ç‰Œï¼Œå¹¶ä¸”åªåœ¨è¿™äº›å‹ç¼©çš„è¡¨ç¤ºä¸Šåº”ç”¨æ³¨æ„åŠ›ã€‚æ¯ä¸ªä»¤ç‰ŒåµŒå…¥é¦–å…ˆé€šè¿‡çº¿æ€§å±€éƒ¨æ··åˆå™¨ï¼ˆMambaæˆ–SWAï¼‰è¿›è¡Œä¸°å¯Œï¼Œè¯¥æ··åˆå™¨ç»“åˆäº†æœ€è¿‘çš„ä¸Šä¸‹æ–‡ã€‚ç„¶åï¼Œæ¯ä¸ªä»¤ç‰Œä¸æ˜¯å…³æ³¨æ‰€æœ‰ä¹‹å‰çš„ä»¤ç‰Œï¼Œè€Œæ˜¯å…³æ³¨å°‘é‡å‹ç¼©çš„æ£€æŸ¥ç‚¹ä»¤ç‰Œï¼Œè¿™äº›ä»¤ç‰Œæ€»ç»“äº†è¾“å…¥å†å²ã€‚è¿™ç§è®¾è®¡ä¿ç•™äº†å…¨æ³¨æ„åŠ›çš„è¡¨ç°åŠ›çš„å¤§éƒ¨åˆ†ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚é€šè¿‡å…³æ³¨å‹ç¼©å†å²è€Œä¸æ˜¯æ‰€æœ‰ä¹‹å‰çš„ä»¤ç‰Œï¼ŒSCOUTçš„å†…å­˜ä½¿ç”¨é‡ç•¥é«˜äºçº¯çº¿æ€§æ¨¡å‹ï¼Œä½†å…¶å¢é•¿ç‡ä»æ˜¯æ¬¡äºŒæ¬¡çš„ï¼Œå¹¶ä¸”æ¯”å…¨Transformeræ›´å¯æ‰©å±•ã€‚æˆ‘ä»¬åˆ†æäº†SCOUTçš„è®¡ç®—å’Œå†…å­˜æ•ˆç‡ï¼Œå¹¶åœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡å’Œæ¨ç†ä»»åŠ¡ä¸Šå¯¹å…¶è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚ä½¿ç”¨Mambaå’ŒSWAæ··åˆå™¨çš„SCOUTåœ¨ç›¸åŒçš„è®¡ç®—é¢„ç®—ä¸‹è¶…è¶Šäº†å¼ºå¤§çš„é•¿åºåˆ—åŸºçº¿æµ‹è¯•ï¼Œåœ¨è§„æ¨¡ä¸º4äº¿å’Œ13äº¿çš„å»ºæ¨¡å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šä¸å…¨æ³¨æ„åŠ›Transformerç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„SCOUTåœ¨é•¿åºåˆ—åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ¯”SOTAæ¨¡å‹æ›´é«˜çš„ç«¯åˆ°ç«¯ååé‡ï¼ŒåŒæ—¶äº§ç”Ÿäº†ç›¸å½“çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00935v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSCOUTçš„æ··åˆæ¶æ„ï¼Œæ—¨åœ¨è§£å†³Transformeråœ¨å¤„ç†é•¿åºåˆ—æ—¶é¢ä¸´çš„æ€§èƒ½ç“¶é¢ˆã€‚SCOUTé€šè¿‡å±€éƒ¨å‹ç¼©tokenå¹¶åœ¨å‹ç¼©è¡¨ç¤ºä¸Šåº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ä¿ç•™å…¨æ³¨æ„åŠ›è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶ï¼Œå¤§å¹…åº¦é™ä½äº†è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚åˆ†æè¡¨æ˜ï¼ŒSCOUTåœ¨å†…å­˜ä½¿ç”¨ä¸Šç•¥é«˜äºçº¯çº¿æ€§æ¨¡å‹ï¼Œä½†å…¶å¢é•¿ç‡ä»ä½äºäºŒæ¬¡æ–¹çš„å…¨Transformerï¼Œè¡¨ç°å‡ºæ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚åœ¨é•¿æ—¶é—´åºåˆ—å»ºæ¨¡å’Œæ¨ç†ä»»åŠ¡ä¸Šï¼ŒSCOUTè¡¨ç°ä¼˜å¼‚ï¼Œä¸å…¨æ³¨æ„åŠ›Transformerç›¸åŒ¹é…ï¼Œå¹¶åœ¨é•¿åºåˆ—åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ›´é«˜çš„ç«¯åˆ°ç«¯ååé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformersé¢ä¸´å¤„ç†é•¿åºåˆ—æ—¶çš„æ€§èƒ½ç“¶é¢ˆï¼Œä¸»è¦å› ä¸ºå…¶äºŒæ¬¡æ–¹çš„æ³¨æ„åŠ›å¤æ‚åº¦ã€‚</li>
<li>çº¿æ€§æ¨¡å‹å¦‚Mambaå’ŒSWAé€šè¿‡å›ºå®šå¤§å°çš„å†…å­˜è¿›è¡Œé€’å½’æˆ–å±€éƒ¨æ“ä½œå®ç°é«˜æ•ˆæ¨ç†ï¼Œä½†å¯èƒ½ç‰ºç‰²é•¿åºåˆ—çš„æ€§èƒ½ã€‚</li>
<li>SCOUTæ¶æ„é€šè¿‡å±€éƒ¨å‹ç¼©tokenå¹¶åœ¨å‹ç¼©è¡¨ç¤ºä¸Šåº”ç”¨æ³¨æ„åŠ›æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>SCOUTä½¿ç”¨çº¿æ€§å±€éƒ¨æ··åˆå™¨æ¥ä¸°å¯Œæ¯ä¸ªtokenåµŒå…¥ï¼Œåªå…³æ³¨ä¸€å°éƒ¨åˆ†å‹ç¼©çš„æ£€æŸ¥ç‚¹tokenï¼Œä»è€Œæ€»ç»“è¾“å…¥å†å²ã€‚</li>
<li>SCOUTåœ¨å†…å­˜ä½¿ç”¨ä¸Šç•¥é«˜äºçº¿æ€§æ¨¡å‹ï¼Œä½†å¢é•¿é€Ÿç‡ä»ç„¶è¿œä½äºå…¨Transformerï¼Œè¡¨ç°å‡ºæ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>åœ¨é•¿æ—¶é—´åºåˆ—å»ºæ¨¡å’Œæ¨ç†ä»»åŠ¡ä¸Šï¼ŒSCOUTè¡¨ç°ä¼˜å¼‚ï¼Œä¸å…¨æ³¨æ„åŠ›Transformerç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-059865832dc7bda291dab023d6eaf325.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-410e981382225e0d2504aa9c7f8807b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebe75dbb6cc77156c7533517ba99917e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2b49662de18a389b491fa5e0f2075bb.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Challenges-and-Applications-of-Large-Language-Models-A-Comparison-of-GPT-and-DeepSeek-family-of-models"><a href="#Challenges-and-Applications-of-Large-Language-Models-A-Comparison-of-GPT-and-DeepSeek-family-of-models" class="headerlink" title="Challenges and Applications of Large Language Models: A Comparison of   GPT and DeepSeek family of models"></a>Challenges and Applications of Large Language Models: A Comparison of   GPT and DeepSeek family of models</h2><p><strong>Authors:Shubham Sharma, Sneha Tuli, Narendra Badam</strong></p>
<p>Large Language Models (LLMs) are transforming AI across industries, but their development and deployment remain complex. This survey reviews 16 key challenges in building and using LLMs and examines how these challenges are addressed by two state-of-the-art models with unique approaches: OpenAIâ€™s closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a large open source Mixture-of-Experts model. Through this comparison, we showcase the trade-offs between closed source models (robust safety, fine-tuned reliability) and open source models (efficiency, adaptability). We also explore LLM applications across different domains (from chatbots and coding tools to healthcare and education), highlighting which model attributes are best suited for each use case. This article aims to guide AI researchers, developers, and decision-makers in understanding current LLM capabilities, limitations, and best practices. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨å„è¡Œä¸šå˜é©äººå·¥æ™ºèƒ½ï¼Œä½†å…¶å¼€å‘å’Œéƒ¨ç½²ä»ç„¶å¤æ‚ã€‚è¿™ç¯‡ç»¼è¿°æ¢³ç†äº†æ„å»ºå’Œä½¿ç”¨LLMçš„16é¡¹å…³é”®æŒ‘æˆ˜ï¼Œå¹¶ç ”ç©¶è¿™äº›æŒ‘æˆ˜æ˜¯å¦‚ä½•é€šè¿‡ä¸¤ä¸ªé‡‡ç”¨ç‹¬ç‰¹æ–¹æ³•çš„æœ€å…ˆè¿›æ¨¡å‹æ¥åº”å¯¹çš„ï¼Œåˆ†åˆ«æ˜¯OpenAIçš„é—­æºGPT-4oï¼ˆ2024å¹´5æœˆæ›´æ–°ï¼‰å’ŒDeepSeek-V3-0324ï¼ˆä¸€ä¸ªå¤§å‹å¼€æºçš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œäº2025å¹´3æœˆå‘å¸ƒï¼‰ã€‚é€šè¿‡æ¯”è¾ƒï¼Œæˆ‘ä»¬å±•ç¤ºäº†é—­æºæ¨¡å‹ï¼ˆç¨³å¥çš„å®‰å…¨æ€§å’Œç²¾ç»†è°ƒæ•´çš„å¯é æ€§ï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆæ•ˆç‡ã€é€‚åº”æ€§ï¼‰ä¹‹é—´çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†LLMåœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨ï¼ˆä»èŠå¤©æœºå™¨äººå’Œç¼–ç å·¥å…·åˆ°åŒ»ç–—ä¿å¥å’Œæ•™è‚²ï¼‰ï¼Œå¹¶å¼ºè°ƒå“ªäº›æ¨¡å‹å±æ€§æœ€é€‚åˆæ¯ä¸ªç”¨ä¾‹ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºAIç ”ç©¶äººå‘˜ã€å¼€å‘äººå‘˜å’Œå†³ç­–è€…æä¾›æŒ‡å¯¼ï¼Œäº†è§£å½“å‰LLMçš„èƒ½åŠ›ã€å±€é™æ€§å’Œæœ€ä½³å®è·µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21377v1">PDF</a> 18 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨å„è¡Œä¸šæ¨åŠ¨äººå·¥æ™ºèƒ½çš„å˜é©ï¼Œä½†å…¶å¼€å‘å’Œéƒ¨ç½²ä»ç„¶é¢ä¸´å¤æ‚æ€§ã€‚æœ¬æ–‡å›é¡¾äº†æ„å»ºå’Œä½¿ç”¨LLMçš„16é¡¹å…³é”®æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å¼€æºå¤§å‹æ··åˆä¸“å®¶æ¨¡å‹DeepSeek-V3-0324å’ŒOpenAIçš„é—­æºGPT-4oï¼ˆMay 2024æ›´æ–°ç‰ˆï¼‰è¿™ä¸¤ç§å…ˆè¿›æŠ€æœ¯ï¼Œå±•ç¤ºäº†å¦‚ä½•åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨å¸®åŠ©AIç ”ç©¶äººå‘˜ã€å¼€å‘äººå‘˜å’Œå†³ç­–è€…äº†è§£å½“å‰LLMçš„èƒ½åŠ›ã€å±€é™æ€§å’Œæœ€ä½³å®è·µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½çš„å˜é©ï¼Œä½†å…¶å¼€å‘å’Œéƒ¨ç½²å­˜åœ¨å¤æ‚æ€§ã€‚</li>
<li>æ„å»ºäº†LLMçš„16é¡¹å…³é”®æŒ‘æˆ˜å›é¡¾ã€‚</li>
<li>å¯¹æ¯”äº†é—­æºæ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆå¦‚DeepSeek-V3-0324ï¼‰çš„ä¼˜ç¼ºç‚¹ã€‚</li>
<li>é—­æºæ¨¡å‹å…·æœ‰ç¨³å¥çš„å®‰å…¨æ€§å’Œç²¾ç»†è°ƒæ•´åçš„å¯é æ€§ï¼Œè€Œå¼€æºæ¨¡å‹å…·æœ‰æ•ˆç‡å’Œé€‚åº”æ€§ã€‚</li>
<li>LLMåœ¨ä¸åŒé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œå¦‚èŠå¤©æœºå™¨äººã€ç¼–ç å·¥å…·ã€åŒ»ç–—ä¿å¥å’Œæ•™è‚²ç­‰ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†é’ˆå¯¹æ¯ä¸ªç”¨ä¾‹é€‰æ‹©é€‚åˆçš„æ¨¡å‹å±æ€§çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4c2b1316dc4de71f218bc202e397aaf2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-211457cc2e376633afd261983c3bd9d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522372ee571ee7687f22c2f0a7757ebd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e6547cabba37c375d2170a9697d049b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db1608d36bcb674113445af6a6b7429a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Estimating-2D-Keypoints-of-Surgical-Tools-Using-Vision-Language-Models-with-Low-Rank-Adaptation"><a href="#Estimating-2D-Keypoints-of-Surgical-Tools-Using-Vision-Language-Models-with-Low-Rank-Adaptation" class="headerlink" title="Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models   with Low-Rank Adaptation"></a>Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models   with Low-Rank Adaptation</h2><p><strong>Authors:Krit Duangprom, Tryphon Lambrou, Binod Bhattarai</strong></p>
<p>This paper presents a novel pipeline for 2D keypoint estima- tion of surgical tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network (CNN) or Transformer-based approaches, which often suffer from overfitting in small-scale medical datasets, our method harnesses the generalization capabilities of pre-trained VLMs. We carefully design prompts to create an instruction-tuning dataset and use them to align visual features with semantic keypoint descriptions. Experimental results show that with only two epochs of fine tuning, the adapted VLM outperforms the baseline models, demonstrating the ef- fectiveness of LoRA in low-resource scenarios. This approach not only improves keypoint detection performance, but also paves the way for future work in 3D surgical hands and tools pose estimation. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨ç»è¿‡ä½ç§©è°ƒæ•´ï¼ˆLoRAï¼‰æŠ€æœ¯å¾®è°ƒè¿‡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œæ‰‹æœ¯å·¥å…·äºŒç»´å…³é”®ç‚¹ä¼°è®¡çš„æ–°å‹ç®¡é“ã€‚ä¸åŒäºä¼ ç»Ÿå—å°è§„æ¨¡åŒ»ç–—æ•°æ®é›†è¿‡æ‹Ÿåˆå›°æ‰°çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–åŸºäºTransformerçš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒVLMsçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†æç¤ºæ¥åˆ›å»ºæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬å°†è§†è§‰ç‰¹å¾ä¸è¯­ä¹‰å…³é”®ç‚¹æè¿°å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»…è¿›è¡Œä¸¤è½®å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œç»è¿‡é€‚åº”çš„VLMä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†LoRAåœ¨ä½èµ„æºåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†å…³é”®ç‚¹æ£€æµ‹æ€§èƒ½ï¼Œè€Œä¸”ä¸ºæœªæ¥çš„ä¸‰ç»´æ‰‹æœ¯æ‰‹å’Œå·¥å…·å§¿æ€ä¼°è®¡å·¥ä½œé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20830v1">PDF</a> Accepted to MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„äºŒç»´å…³é”®ç‚¹æ£€æµ‹æµç¨‹ï¼Œé’ˆå¯¹æ‰‹æœ¯å™¨æ¢°è¿›è¡Œæ£€æµ‹ï¼Œé‡‡ç”¨äº†ç»è¿‡LoRAæŠ€æœ¯è°ƒä¼˜çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ä¸åŒäºä¼ ç»Ÿæ˜“å—å°è§„æ¨¡åŒ»å­¦æ•°æ®é›†è¿‡æ‹Ÿåˆå½±å“çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–åŸºäºTransformerçš„æ–¹æ³•ï¼Œæœ¬ç ”ç©¶å……åˆ†åˆ©ç”¨äº†é¢„è®­ç»ƒVLMsçš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡ç²¾å¿ƒè®¾è®¡æç¤ºè¯­åˆ›å»ºäº†æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå¹¶å°†è§†è§‰ç‰¹å¾ä¸ç›®æ ‡å…³é”®ç‚¹æè¿°è¿›è¡Œå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡ä¸¤ä¸ªè®­ç»ƒå‘¨æœŸçš„ç²¾ç»†è°ƒæ•´åï¼ŒVLMæ€§èƒ½è¶…è¿‡äº†åŸºå‡†æ¨¡å‹ï¼Œè¯æ˜äº†LoRAåœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚æ­¤æ–¹æ³•ä¸ä»…æé«˜äº†å…³é”®ç‚¹æ£€æµ‹æ€§èƒ½ï¼Œä¹Ÿä¸ºæœªæ¥ä¸‰ç»´æ‰‹æœ¯å™¨æ¢°å§¿æ€ä¼°è®¡ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œæ‰‹æœ¯å™¨æ¢°çš„äºŒç»´å…³é”®ç‚¹æ£€æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„ç®¡é“æµç¨‹ç”¨äºå…³é”®ç‚¹æ£€æµ‹ã€‚</li>
<li>é€šè¿‡LoRAæŠ€æœ¯è°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„æ•°æ®é›†å’Œæç¤ºè¯­è¿›è¡Œè§†è§‰ç‰¹å¾ä¸ç›®æ ‡è¯­ä¹‰å…³é”®ç‚¹æè¿°çš„å¯¹é½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å°‘é‡è®­ç»ƒå‘¨æœŸè°ƒæ•´åï¼ŒVLMæ€§èƒ½ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</li>
<li>æ–¹æ³•æé«˜äº†å…³é”®ç‚¹æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-54e8583cb2af6392910483e98c6f7a87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b2e803add99b041fc6dfbb8d2fa1c3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4db78834eecb646e4bf81ec3d82f77ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9636c0694868fbf74a2c744686ce012.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-889728f20263401a6f3520e529224f93.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-92177ef149b183713613682d41bfdc56.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  EvoEmo Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-61193cbc365b65378c62dd7ebbee2363.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  ArcMemo Abstract Reasoning Composition with Lifelong LLM Memory
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
