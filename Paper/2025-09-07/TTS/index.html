<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  AUDETER A Large-scale Dataset for Deepfake Audio Detection in Open   Worlds">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-41abcaaf44cd5b6288db938b78329f6c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-08-æ›´æ–°"><a href="#2025-09-08-æ›´æ–°" class="headerlink" title="2025-09-08 æ›´æ–°"></a>2025-09-08 æ›´æ–°</h1><h2 id="AUDETER-A-Large-scale-Dataset-for-Deepfake-Audio-Detection-in-Open-Worlds"><a href="#AUDETER-A-Large-scale-Dataset-for-Deepfake-Audio-Detection-in-Open-Worlds" class="headerlink" title="AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open   Worlds"></a>AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open   Worlds</h2><p><strong>Authors:Qizhou Wang, Hanxun Huang, Guansong Pang, Sarah Erfani, Christopher Leckie</strong></p>
<p>Speech generation systems can produce remarkably realistic vocalisations that are often indistinguishable from human speech, posing significant authenticity challenges. Although numerous deepfake detection methods have been developed, their effectiveness in real-world environments remains unrealiable due to the domain shift between training and test samples arising from diverse human speech and fast evolving speech synthesis systems. This is not adequately addressed by current datasets, which lack real-world application challenges with diverse and up-to-date audios in both real and deep-fake categories. To fill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale, highly diverse deepfake audio dataset for comprehensive evaluation and robust development of generalised models for deepfake audio detection. It consists of over 4,500 hours of synthetic audio generated by 11 recent TTS models and 10 vocoders with a broad range of TTS&#x2F;vocoder patterns, totalling 3 million audio clips, making it the largest deepfake audio dataset by scale. Through extensive experiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods trained on existing datasets struggle to generalise to novel deepfake audio samples and suffer from high false positive rates on unseen human voice, underscoring the need for a comprehensive dataset; and ii) these methods trained on AUDETER achieve highly generalised detection performance and significantly reduce detection error rate by 44.1% to 51.6%, achieving an error rate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild dataset, paving the way for training generalist deepfake audio detectors. AUDETER is available on GitHub. </p>
<blockquote>
<p>è¯­éŸ³ç”Ÿæˆç³»ç»Ÿèƒ½å¤Ÿäº§ç”Ÿéå¸¸é€¼çœŸçš„è¯­éŸ³ï¼Œè¿™äº›è¯­éŸ³é€šå¸¸ä¸äººç±»è¯­éŸ³æ— æ³•åŒºåˆ†ï¼Œä»è€Œæ„æˆäº†é‡å¤§çš„çœŸå®æ€§æŒ‘æˆ˜ã€‚å°½ç®¡å·²ç»å¼€å‘äº†è®¸å¤šæ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•ï¼Œä½†ç”±äºè®­ç»ƒæ ·æœ¬å’Œæµ‹è¯•æ ·æœ¬ä¹‹é—´å­˜åœ¨çš„é¢†åŸŸåç§»ä»¥åŠäººç±»è¯­éŸ³çš„å¤šæ ·æ€§å’Œå¿«é€Ÿæ¼”å˜çš„è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œå®ƒä»¬åœ¨ç°å®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶ä¸å¯é ã€‚å½“å‰çš„æ•°æ®é›†æ²¡æœ‰å……åˆ†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒä»¬ç¼ºä¹çœŸå®ä¸–ç•Œçš„åº”ç”¨æŒ‘æˆ˜ï¼Œåœ¨çœŸå®å’Œæ·±åº¦ä¼ªé€ ç±»åˆ«ä¸­éƒ½ç¼ºä¹å¤šæ ·æ€§å’Œæœ€æ–°çš„éŸ³é¢‘æ•°æ®ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†AUDETERï¼ˆéŸ³é¢‘æ·±åº¦ä¼ªé€ æµ‹è¯•èŒƒå›´ï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜åº¦å¤šæ ·åŒ–çš„æ·±åº¦ä¼ªé€ éŸ³é¢‘æ•°æ®é›†ï¼Œç”¨äºå…¨é¢è¯„ä¼°å’Œç¨³å¥å¼€å‘é€‚ç”¨äºæ·±åº¦ä¼ªé€ éŸ³é¢‘æ£€æµ‹çš„é€šç”¨æ¨¡å‹ã€‚å®ƒç”±è¶…è¿‡4500å°æ—¶çš„äººé€ éŸ³é¢‘ç»„æˆï¼Œè¿™äº›éŸ³é¢‘ç”±11ä¸ªæœ€æ–°çš„æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢æ¨¡å‹å’Œ10ä¸ªç¼–ç å™¨ç”Ÿæˆï¼Œæ¶µç›–äº†å¹¿æ³›çš„TTS&#x2F;ç¼–ç å™¨æ¨¡å¼ï¼Œæ€»è®¡300ä¸‡ä¸ªéŸ³é¢‘å‰ªè¾‘ï¼Œä½¿å…¶æˆä¸ºè§„æ¨¡æœ€å¤§çš„æ·±åº¦ä¼ªé€ éŸ³é¢‘æ•°æ®é›†ã€‚é€šè¿‡å¯¹AUDETERè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°ï¼šiï¼‰åœ¨ç°æœ‰æ•°æ®é›†ä¸Šè®­ç»ƒçš„æœ€æ–°æ–¹æ³•å¾ˆéš¾æ¨å¹¿åˆ°æ–°çš„æ·±åº¦ä¼ªé€ éŸ³é¢‘æ ·æœ¬ä¸Šï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„çœŸäººå£°éŸ³ä¸Šå‡ºç°äº†è¾ƒé«˜çš„è¯¯æŠ¥ç‡ï¼Œè¿™çªæ˜¾äº†å…¨é¢æ•°æ®é›†çš„éœ€æ±‚ï¼›iiï¼‰åœ¨AUDETERä¸Šè®­ç»ƒçš„æ–¹æ³•å®ç°äº†é«˜åº¦é€šç”¨çš„æ£€æµ‹æ€§èƒ½ï¼Œå¹¶å°†æ£€æµ‹é”™è¯¯ç‡é™ä½äº†44.1%è‡³51.6%ï¼Œåœ¨æµè¡Œçš„In-the-Wildæ•°æ®é›†ä¸Šçš„è·¨åŸŸæ ·æœ¬çš„é”™è¯¯ç‡ä»…ä¸º4.17%ï¼Œè¿™ä¸ºè®­ç»ƒé€šç”¨çš„æ·±åº¦ä¼ªé€ éŸ³é¢‘æ£€æµ‹å™¨é“ºå¹³äº†é“è·¯ã€‚AUDETERæ•°æ®é›†å·²åœ¨GitHubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04345v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯­éŸ³ç”Ÿæˆç³»ç»Ÿäº§ç”Ÿçš„è¯­éŸ³éå¸¸é€¼çœŸï¼Œç»™äººç±»è¯­éŸ³å¸¦æ¥çœŸå®æ€§çš„æŒ‘æˆ˜ã€‚å°½ç®¡å·²å¼€å‘äº†è®¸å¤šæ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•ï¼Œä½†åœ¨ç°å®ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå®ƒä»¬çš„æœ‰æ•ˆæ€§ä»ä¸å¯é ã€‚ä¸ºäº†è§£å†³å½“å‰æ•°æ®é›†åœ¨çœŸå®å’Œæ·±åº¦ä¼ªé€ ç±»åˆ«ä¸­éƒ½ç¼ºä¹å¤šæ ·åŒ–å’Œæœ€æ–°éŸ³é¢‘çš„é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†AUDETERæ•°æ®é›†ã€‚å®ƒåŒ…å«è¶…è¿‡4,500å°æ—¶ç”±11ç§æœ€æ–°æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹å’Œ10ç§ç¼–ç å™¨ç”Ÿæˆçš„åˆæˆéŸ³é¢‘ï¼Œå…±300ä¸‡éŸ³é¢‘ç‰‡æ®µï¼Œæ˜¯ç›®å‰æœ€å¤§çš„æ·±åº¦ä¼ªé€ éŸ³é¢‘æ•°æ®é›†ã€‚é€šè¿‡AUDETERçš„å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ•°æ®é›†è®­ç»ƒçš„å…ˆè¿›æ–¹æ³•éš¾ä»¥æ¨å¹¿åˆ°æ–°çš„æ·±åº¦ä¼ªé€ éŸ³é¢‘æ ·æœ¬ä¸Šï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„çœŸäººå£°éŸ³ä¸Šå‡é˜³æ€§ç‡è¾ƒé«˜ï¼Œå‡¸æ˜¾äº†éœ€è¦å…¨é¢çš„æ•°æ®é›†ï¼›åœ¨AUDETERä¸Šè®­ç»ƒçš„æ–¹æ³•å®ç°äº†é«˜åº¦é€šç”¨çš„æ£€æµ‹æ€§èƒ½ï¼Œæ£€æµ‹é”™è¯¯ç‡é™ä½äº†44.1%è‡³51.6%ï¼Œåœ¨æµè¡Œçš„In-the-Wildæ•°æ®é›†ä¸Šçš„è·¨åŸŸæ ·æœ¬é”™è¯¯ç‡ä»…ä¸º4.17%ï¼Œä¸ºè®­ç»ƒé€šç”¨æ·±åº¦ä¼ªé€ éŸ³é¢‘æ£€æµ‹å™¨é“ºå¹³äº†é“è·¯ã€‚AUDETERæ•°æ®é›†å·²åœ¨GitHubä¸Šæä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³ç”Ÿæˆç³»ç»Ÿäº§ç”Ÿçš„è¯­éŸ³ä¸ç°å®ä¸­çš„å‡ ä¹æ— æ³•åŒºåˆ†ï¼Œç»™çœŸå®æ€§çš„é‰´åˆ«å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•çš„å®é™…è¡¨ç°ä¸å¤Ÿç¨³å®šï¼Œå­˜åœ¨è®­ç»ƒæ ·æœ¬å’Œæµ‹è¯•æ ·æœ¬é¢†åŸŸåç§»çš„é—®é¢˜ã€‚</li>
<li>å½“å‰æ•°æ®é›†ç¼ºä¹å¤šæ ·åŒ–å’Œæœ€æ–°çš„éŸ³é¢‘æ ·æœ¬ï¼Œå¯¼è‡´ç°æœ‰æ¨¡å‹éš¾ä»¥åº”å¯¹çœŸå®ä¸–ç•ŒæŒ‘æˆ˜ã€‚</li>
<li>ä»‹ç»äº†AUDETERæ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡åˆæˆéŸ³é¢‘å’ŒçœŸå®éŸ³é¢‘æ ·æœ¬ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å…ˆè¿›çš„æ–¹æ³•åœ¨AUDETERæ•°æ®é›†ä¸Šçš„è®­ç»ƒè¡¨ç°å‡ºé«˜åº¦é€šç”¨çš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒAUDETERæ˜¾è‘—é™ä½äº†æ£€æµ‹é”™è¯¯ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aeeae8164fa712d234bb17371286ac8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1d5ed7e92e83ef4355ddd4445b64faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1dc10b26f8ead8eb84957d9e58e6653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-052eaa05e7f4a3edc4e5a055d53247fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf47d3a597644c4bca22019d6d48ba16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf958217a4dbed33dbf1d6e532c3eecc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01ade5bbd91954e5a0e0138ad0f5b03a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da67fc51e28a7c90ef25ddbc170fa337.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LibriQuote-A-Speech-Dataset-of-Fictional-Character-Utterances-for-Expressive-Zero-Shot-Speech-Synthesis"><a href="#LibriQuote-A-Speech-Dataset-of-Fictional-Character-Utterances-for-Expressive-Zero-Shot-Speech-Synthesis" class="headerlink" title="LibriQuote: A Speech Dataset of Fictional Character Utterances for   Expressive Zero-Shot Speech Synthesis"></a>LibriQuote: A Speech Dataset of Fictional Character Utterances for   Expressive Zero-Shot Speech Synthesis</h2><p><strong>Authors:Gaspard Michel, Elena V. Epure, Christophe Cerisara</strong></p>
<p>Text-to-speech (TTS) systems have recently achieved more expressive and natural speech synthesis by scaling to large speech datasets. However, the proportion of expressive speech in such large-scale corpora is often unclear. Besides, existing expressive speech corpora are typically smaller in scale and primarily used for benchmarking TTS systems. In this paper, we introduce the LibriQuote dataset, an English corpus derived from read audiobooks, designed for both fine-tuning and benchmarking expressive zero-shot TTS system. The training dataset includes 12.7K hours of read, non-expressive speech and 5.3K hours of mostly expressive speech drawn from character quotations. Each utterance in the expressive subset is supplemented with the context in which it was written, along with pseudo-labels of speech verbs and adverbs used to describe the quotation (\textit{e.g. &#96;&#96;he whispered softlyâ€™â€™}). Additionally, we provide a challenging 7.5 hour test set intended for benchmarking TTS systems: given a neutral reference speech as input, we evaluate systemâ€™s ability to synthesize an expressive utterance while preserving reference timbre. We validate qualitatively the test set by showing that it covers a wide range of emotions compared to non-expressive speech, along with various accents. Extensive subjective and objective evaluations show that fine-tuning a baseline TTS system on LibriQuote significantly improves its synthesized speech intelligibility, and that recent systems fail to synthesize speech as expressive and natural as the ground-truth utterances. The dataset and evaluation code are freely available. Audio samples can be found at <a target="_blank" rel="noopener" href="https://libriquote.github.io/">https://libriquote.github.io/</a>. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿæœ€è¿‘é€šè¿‡æ‰©å±•åˆ°å¤§è§„æ¨¡è¯­éŸ³æ•°æ®é›†ï¼Œå®ç°äº†æ›´å…·è¡¨ç°åŠ›å’Œæ›´è‡ªç„¶çš„è¯­éŸ³åˆæˆã€‚ç„¶è€Œï¼Œæ­¤ç±»å¤§è§„æ¨¡è¯­æ–™åº“ä¸­è¡¨è¾¾æ€§è¯­éŸ³çš„æ¯”ä¾‹é€šå¸¸æ˜¯ä¸æ˜ç¡®çš„ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è¡¨è¾¾æ€§è¯­éŸ³è¯­æ–™åº“è§„æ¨¡é€šå¸¸è¾ƒå°ï¼Œä¸»è¦ç”¨äºè¯„ä¼°TTSç³»ç»Ÿçš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LibriQuoteæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä»æœ—è¯»æœ‰å£°ä¹¦ç±ä¸­è¡ç”Ÿå‡ºæ¥çš„è‹±è¯­è¯­æ–™åº“ï¼Œæ—¨åœ¨ç”¨äºå¾®è°ƒä»¥åŠè¯„ä¼°è¡¨ç°æ€§é›¶æ ·æœ¬TTSç³»ç»Ÿã€‚è®­ç»ƒæ•°æ®é›†åŒ…å«12.7Kå°æ—¶çš„æœ—è¯»éè¡¨è¾¾æ€§è¯­éŸ³å’Œ5.3Kå°æ—¶çš„å¤§éƒ¨åˆ†è¡¨è¾¾æ€§è¯­éŸ³ï¼Œè¿™äº›è¯­éŸ³æ¥è‡ªè§’è‰²å¼•ç”¨ã€‚è¡¨è¾¾æ€§å­é›†ä¸­çš„æ¯ä¸ªè¯­å¥éƒ½è¾…ä»¥ä¹¦é¢ä¸Šä¸‹æ–‡ï¼Œä»¥åŠä¸æè¿°å¼•ç”¨çš„åŠ¨è¯å’Œå‰¯è¯ä¼ªæ ‡ç­¾ï¼ˆä¾‹å¦‚â€œä»–è½»å£°ç»†è¯­â€ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„7.5å°æ—¶æµ‹è¯•é›†ï¼Œæ—¨åœ¨è¯„ä¼°TTSç³»ç»Ÿçš„æ€§èƒ½ï¼šç»™å®šä¸­æ€§å‚è€ƒè¯­éŸ³ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬è¯„ä¼°ç³»ç»Ÿåˆæˆè¡¨è¾¾æ€§è¯­å¥çš„åŒæ—¶ä¿ç•™å‚è€ƒéŸ³è‰²çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å®šæ€§éªŒè¯æµ‹è¯•é›†ï¼Œè¡¨æ˜å…¶è¦†ç›–çš„æƒ…æ„ŸèŒƒå›´ä¸æœ—è¯»éè¡¨è¾¾æ€§è¯­éŸ³ç›¸æ¯”å¹¿æ³›ï¼Œä¸”å¸¦æœ‰å„ç§å£éŸ³ã€‚å¤§é‡ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°è¡¨æ˜ï¼Œåœ¨LibriQuoteä¸Šå¾®è°ƒåŸºçº¿TTSç³»ç»Ÿå¯æ˜¾è‘—æé«˜åˆæˆè¯­éŸ³çš„å¯æ‡‚åº¦ï¼Œè€Œç°æœ‰ç³»ç»Ÿæ— æ³•åˆæˆä¸çœŸå®è¯è¯­ä¸€æ ·æœ‰è¡¨ç°åŠ›å’Œè‡ªç„¶çš„è¯­éŸ³ã€‚è¯¥æ•°æ®é›†å’Œè¯„ä¼°ä»£ç å¯å…è´¹æä¾›ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://libriquote.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://libriquote.github.io/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04072v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†LibriQuoteæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä»æœ‰å£°è¯»ç‰©ä¸­è¡ç”Ÿçš„è‹±è¯­è¯­æ–™åº“ï¼Œæ—¨åœ¨ç”¨äºå¾®è°ƒä¸åŸºå‡†æµ‹è¯•è¡¨è¾¾æ€§é›¶æ ·æœ¬TTSç³»ç»Ÿã€‚è¯¥è®­ç»ƒæ•°æ®é›†åŒ…å«12.7Kå°æ—¶çš„æœ—è¯»éè¡¨è¾¾æ€§è¯­éŸ³å’Œ5.3Kå°æ—¶çš„ä¸»è¦æ˜¯è¡¨è¾¾æ€§è¯­éŸ³ã€‚è¡¨è¾¾æ€§å­é›†ä¸­çš„æ¯ä¸ªè¯è¯­éƒ½é™„æœ‰ä¹¦é¢ä¸Šä¸‹æ–‡ä»¥åŠæè¿°å¼•ç”¨çš„ä¼ªæ ‡ç­¾ï¼ˆå¦‚â€œä»–è½»å£°ç»†è¯­â€ï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„7.5å°æ—¶æµ‹è¯•é›†ï¼Œç”¨äºåŸºå‡†æµ‹è¯•TTSç³»ç»Ÿï¼šç»™å®šä¸­æ€§å‚è€ƒè¯­éŸ³ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬è¯„ä¼°ç³»ç»Ÿåˆæˆè¡¨è¾¾æ€§è¯è¯­çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å‚è€ƒçš„éŸ³è‰²ã€‚é€šè¿‡éªŒè¯è¡¨æ˜ï¼Œæµ‹è¯•é›†æ¶µç›–äº†ä¸éè¡¨è¾¾æ€§è¯­éŸ³ç›¸æ¯”çš„å¹¿æ³›æƒ…ç»ªä»¥åŠå„ç§å£éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LibriQuoteæ•°æ®é›†æ˜¯ä¸€ä¸ªç”¨äºTTSç ”ç©¶çš„è‹±è¯­è¯­æ–™åº“ï¼ŒåŒ…å«æœ—è¯»çš„éè¡¨è¾¾æ€§å’Œè¡¨è¾¾æ€§è¯­éŸ³ã€‚</li>
<li>è¡¨è¾¾æ€§å­é›†ä¸­çš„æ¯ä¸ªè¯è¯­éƒ½é™„æœ‰ä¹¦é¢ä¸Šä¸‹æ–‡å’Œæè¿°å¼•ç”¨çš„ä¼ªæ ‡ç­¾ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ª7.5å°æ—¶çš„æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°TTSç³»ç»Ÿåœ¨åˆæˆè¡¨è¾¾æ€§è¯­éŸ³æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•é›†æ¶µç›–äº†å¹¿æ³›çš„æƒ…ç»ªå’Œå¤šç§å£éŸ³ã€‚</li>
<li>ç›¸æ¯”éè¡¨è¾¾æ€§è¯­éŸ³ï¼Œè¡¨è¾¾æ€§è¯­éŸ³åœ¨æ•°æ®é›†ä¸­çš„æ¯”ä¾‹ä¸æ˜ç¡®ã€‚</li>
<li>é€šè¿‡åœ¨LibriQuoteä¸Šå¾®è°ƒåŸºå‡†TTSç³»ç»Ÿï¼Œå¯ä»¥æ˜¾è‘—æé«˜åˆæˆè¯­éŸ³çš„æ¸…æ™°åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5de02ed09c2907da9803230da55ab723.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2adde8b5c1c1a3343e2b559bb146336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e93a144eb07484f1689f4c165b36189d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57581fd90cca8d3f7f6832c9fee2e621.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2325ebecdc2fc98b9c8ad0d6f4ea1c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ce64769d4f73b46a5a8d85c7681c12d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64aead185e670f8d8956e1ed38e80061.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="WenetSpeech-Yue-A-Large-scale-Cantonese-Speech-Corpus-with-Multi-dimensional-Annotation"><a href="#WenetSpeech-Yue-A-Large-scale-Cantonese-Speech-Corpus-with-Multi-dimensional-Annotation" class="headerlink" title="WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with   Multi-dimensional Annotation"></a>WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with   Multi-dimensional Annotation</h2><p><strong>Authors:Longhao Li, Zhao Guo, Hongjie Chen, Yuhang Dai, Ziyu Zhang, Hongfei Xue, Tianlun Zuo, Chengyou Wang, Shuiyuan Wang, Jie Li, Xin Xu, Hui Bu, Binbin Zhang, Ruibin Yuan, Ziya Zhou, Wei Xue, Lei Xie</strong></p>
<p>The development of speech understanding and generation has been significantly accelerated by the availability of large-scale, high-quality speech datasets. Among these, ASR and TTS are regarded as the most established and fundamental tasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9 million native speakers worldwide, limited annotated resources have hindered progress and resulted in suboptimal ASR and TTS performance. To address this challenge, we propose WenetSpeech-Pipe, an integrated pipeline for building large-scale speech corpus with multi-dimensional annotation tailored for speech understanding and generation. It comprises six modules: Audio Collection, Speaker Attributes Annotation, Speech Quality Annotation, Automatic Speech Recognition, Text Postprocessing and Recognizer Output Voting, enabling rich and high-quality annotations. Based on this pipeline, we release WenetSpeech-Yue, the first large-scale Cantonese speech corpus with multi-dimensional annotation for ASR and TTS, covering 21,800 hours across 10 domains with annotations including ASR transcription, text confidence, speaker identity, age, gender, speech quality scores, among other annotations. We also release WSYue-eval, a comprehensive Cantonese benchmark with two components: WSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long utterances, code-switching, and diverse acoustic conditions, and WSYue-TTS-eval, with base and coverage subsets for standard and generalization testing. Experimental results show that models trained on WenetSpeech-Yue achieve competitive results against state-of-the-art (SOTA) Cantonese ASR and TTS systems, including commercial and LLM-based models, highlighting the value of our dataset and pipeline. </p>
<blockquote>
<p>å¤§è§„æ¨¡é«˜è´¨é‡è¯­éŸ³æ•°æ®é›†çš„å¯ç”¨æ€§æå¤§åœ°æ¨åŠ¨äº†è¯­éŸ³è¯†åˆ«å’Œç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚å…¶ä¸­ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰è¢«è®¤ä¸ºæ˜¯æœ€æˆç†Ÿã€æœ€åŸºæœ¬çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¯¹äºå…¨çƒçº¦æœ‰8490ä¸‡ä½¿ç”¨è€…çš„ç²¤è¯­ï¼ˆåˆç§°å¹¿ä¸œè¯ï¼‰ï¼Œæœ‰é™çš„æ ‡æ³¨èµ„æºé˜»ç¢äº†å…¶è¿›å±•ï¼Œå¯¼è‡´ASRå’ŒTTSæ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†WenetSpeech-Pipeï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºè¯­éŸ³ç†è§£å’Œç”Ÿæˆé‡èº«å®šåˆ¶çš„å¤§è§„æ¨¡è¯­éŸ³è¯­æ–™åº“æ„å»ºçš„ç»¼åˆç®¡é“ï¼Œå…·æœ‰å¤šç»´åº¦æ ‡æ³¨ã€‚å®ƒåŒ…å«å…­ä¸ªæ¨¡å—ï¼šéŸ³é¢‘æ”¶é›†ã€è¯´è¯äººå±æ€§æ ‡æ³¨ã€è¯­éŸ³è´¨é‡æ ‡æ³¨ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åå¤„ç†å’Œè¯†åˆ«å™¨è¾“å‡ºæŠ•ç¥¨ï¼Œä»¥å®ç°ä¸°å¯Œä¸”é«˜è´¨é‡çš„æ ‡æ³¨ã€‚åŸºäºæ­¤ç®¡é“ï¼Œæˆ‘ä»¬å‘å¸ƒäº†WenetSpeech-Yueï¼Œè¿™æ˜¯é¦–ä¸ªå…·æœ‰å¤šç»´åº¦æ ‡æ³¨çš„ç²¤è¯­å¤§è§„æ¨¡è¯­éŸ³è¯­æ–™åº“ï¼Œç”¨äºASRå’ŒTTSï¼Œè¦†ç›–10ä¸ªé¢†åŸŸçš„21800å°æ—¶ï¼Œæ ‡æ³¨åŒ…æ‹¬ASRè½¬å½•ã€æ–‡æœ¬ç½®ä¿¡åº¦ã€è¯´è¯äººèº«ä»½ã€å¹´é¾„ã€æ€§åˆ«ã€è¯­éŸ³è´¨é‡è¯„åˆ†ç­‰å…¶ä»–æ ‡æ³¨ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†WSYue-evalï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ç²¤è¯­åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šWSYue-ASR-evalï¼Œç”¨äºè¯„ä¼°çŸ­å¥å’Œé•¿å¥ã€è¯­è¨€åˆ‡æ¢å’Œå„ç§å£°å­¦æ¡ä»¶ä¸‹çš„ASRï¼›ä»¥åŠWSYue-TTS-evalï¼ŒåŒ…å«ç”¨äºæ ‡å‡†å’Œé€šç”¨æµ‹è¯•çš„åŸºå‡†å’Œè¦†ç›–èŒƒå›´å­é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨WenetSpeech-Yueä¸Šè®­ç»ƒçš„æ¨¡å‹ä¸æœ€æ–°çš„ç²¤è¯­ASRå’ŒTTSç³»ç»Ÿç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬å•†ä¸šå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºç¡€æ¨¡å‹ï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬æ•°æ®é›†å’Œç®¡é“çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03959v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è¯­éŸ³æ•°æ®é›†æå¤§åœ°æ¨åŠ¨äº†è¯­éŸ³è¯†åˆ«å’Œç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚é’ˆå¯¹ç²¤è¯­ï¼ˆè¶…è¿‡8åƒä¸‡ç²¤è¯­æ¯è¯­è€…ï¼‰ï¼Œèµ„æºå—é™é™åˆ¶äº†è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºWenetSpeech-Pipeé›†æˆç®¡é“ï¼Œç”¨äºæ„å»ºå¤§è§„æ¨¡è¯­éŸ³è¯­æ–™åº“ï¼Œå…·æœ‰é’ˆå¯¹è¯­éŸ³ç†è§£å’Œç”Ÿæˆçš„å¤šç»´åº¦æ³¨é‡Šã€‚åŸºäºè¯¥ç®¡é“ï¼Œæˆ‘ä»¬å‘å¸ƒäº†WenetSpeech-Yueç²¤è¯­å¤§å‹è¯­éŸ³è¯­æ–™åº“å’Œå¤šç»´åº¦æ³¨é‡Šçš„ASRå’ŒTTSæ•°æ®é›†ï¼Œè¦†ç›–è¶…è¿‡äºŒåä¸€ä¸‡å…«åƒå°æ—¶çš„éŸ³é¢‘æ•°æ®ã€‚åŒæ—¶å‘å¸ƒè¯„ä¼°åŸºå‡†WSYue-evalï¼ŒåŒ…å«ä¸¤ä¸ªç»„ä»¶ç”¨äºè¯„ä¼°ASRå’ŒTTSæ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºWenetSpeech-Yueè®­ç»ƒçš„æ¨¡å‹åœ¨ç²¤è¯­ASRå’ŒTTSç³»ç»Ÿæ–¹é¢è¾¾åˆ°ä¸šç•Œå‰æ²¿æ°´å¹³ã€‚è¿™çªæ˜¾äº†æ•°æ®é›†çš„ä»·å€¼å’Œæ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é«˜è´¨é‡è¯­éŸ³æ•°æ®é›†æ¨åŠ¨è¯­éŸ³è¯†åˆ«å’Œç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚</li>
<li>é’ˆå¯¹ç²¤è¯­ï¼Œèµ„æºå—é™å¯¼è‡´è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ€§èƒ½ä¸ä½³ã€‚</li>
<li>WenetSpeech-Pipeæ˜¯ä¸€ä¸ªé›†æˆç®¡é“ï¼Œç”¨äºæ„å»ºå¤§è§„æ¨¡è¯­éŸ³è¯­æ–™åº“å¹¶å…·æœ‰å¤šç»´åº¦æ³¨é‡ŠåŠŸèƒ½ã€‚</li>
<li>WenetSpeech-Yueæ˜¯é¦–ä¸ªç²¤è¯­å¤§å‹è¯­éŸ³è¯­æ–™åº“ï¼Œç”¨äºASRå’ŒTTSçš„è¯„ä¼°å’Œè®­ç»ƒã€‚å®ƒè¦†ç›–äº†è¶…è¿‡äºŒåä¸€ä¸‡å…«åƒå°æ—¶çš„éŸ³é¢‘æ•°æ®å¹¶åŒ…æ‹¬å¤šç§æ³¨é‡Šç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-62b44d42a53b42a895f853468d4d8f5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d68bb830dc32bd1cafaf09ed5da72c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5287fc63fb61101fb566f76721a794e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08191cf1e6dd2e03e06c98efa514d89a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd6b2f995a79a167332bc1604a4ff929.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SwinSRGAN-Swin-Transformer-based-Generative-Adversarial-Network-for-High-Fidelity-Speech-Super-Resolution"><a href="#SwinSRGAN-Swin-Transformer-based-Generative-Adversarial-Network-for-High-Fidelity-Speech-Super-Resolution" class="headerlink" title="SwinSRGAN: Swin Transformer-based Generative Adversarial Network for   High-Fidelity Speech Super-Resolution"></a>SwinSRGAN: Swin Transformer-based Generative Adversarial Network for   High-Fidelity Speech Super-Resolution</h2><p><strong>Authors:Jiajun Yuan, Xiaochen Wang, Yuhang Xiao, Yulin Wu, Chenhao Hu, Xueyang Lv</strong></p>
<p>Speech super-resolution (SR) reconstructs high-frequency content from low-resolution speech signals. Existing systems often suffer from representation mismatch in two-stage mel-vocoder pipelines and from over-smoothing of hallucinated high-band content by CNN-only generators. Diffusion and flow models are computationally expensive, and their robustness across domains and sampling rates remains limited. We propose SwinSRGAN, an end-to-end framework operating on Modified Discrete Cosine Transform (MDCT) magnitudes. It is a Swin Transformer-based U-Net that captures long-range spectro-temporal dependencies with a hybrid adversarial scheme combines time-domain MPD&#x2F;MSD discriminators with a multi-band MDCT discriminator specialized for the high-frequency band. We employs a sparse-aware regularizer on arcsinh-compressed MDCT to better preserve transient components. The system upsamples inputs at various sampling rates to 48 kHz in a single pass and operates in real time. On standard benchmarks, SwinSRGAN reduces objective error and improves ABX preference scores. In zero-shot tests on HiFi-TTS without fine-tuning, it outperforms NVSR and mdctGAN, demonstrating strong generalization across datasets </p>
<blockquote>
<p>è¯­éŸ³è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯ä»ä½åˆ†è¾¨ç‡è¯­éŸ³ä¿¡å·ä¸­é‡å»ºé«˜é¢‘å†…å®¹ã€‚ç°æœ‰ç³»ç»Ÿå¸¸å¸¸åœ¨ä¸¤çº§æ¢…å°”é¢‘è°±è§£ç å™¨ç®¡é“ä¸­å­˜åœ¨è¡¨ç¤ºä¸åŒ¹é…çš„é—®é¢˜ï¼Œå¹¶ä¸”ä»…ç”±CNNç”Ÿæˆå™¨ç”Ÿæˆçš„å‡é«˜é¢‘å†…å®¹ä¼šå‡ºç°è¿‡åº¦å¹³æ»‘çš„æƒ…å†µã€‚æ‰©æ•£å’Œæµæ¨¡å‹è®¡ç®—å¼€é”€å¤§ï¼Œå…¶åœ¨ä¸åŒé¢†åŸŸå’Œé‡‡æ ·ç‡ä¹‹é—´çš„é²æ£’æ€§ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†SwinSRGANï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¿®æ”¹åçš„ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆMDCTï¼‰å¹…åº¦çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚å®ƒæ˜¯ä¸€ç§åŸºäºSwin Transformerçš„U-Netï¼Œèƒ½å¤Ÿæ•æ‰é•¿æœŸå…‰è°±æ—¶é—´ä¾èµ–å…³ç³»ï¼Œå¹¶é‡‡ç”¨äº†æ··åˆå¯¹æŠ—æ–¹æ¡ˆï¼Œå°†æ—¶åŸŸMPD&#x2F;MSDé‰´åˆ«å™¨ä¸ä¸“é—¨é’ˆå¯¹é«˜é¢‘é¢‘å¸¦çš„å¤šé¢‘å¸¦MDCTé‰´åˆ«å™¨ç›¸ç»“åˆã€‚æˆ‘ä»¬åœ¨äºšè¾›å‹ç¼©çš„MDCTä¸Šé‡‡ç”¨äº†ç¨€ç–æ„ŸçŸ¥æ­£åˆ™åŒ–å™¨ï¼Œä»¥æ›´å¥½åœ°ä¿ç•™ç¬æ€åˆ†é‡ã€‚è¯¥ç³»ç»Ÿåœ¨ä¸€æ¬¡ä¼ é€’ä¸­å¯å°†å„ç§é‡‡æ ·ç‡çš„ä¸Šé‡‡æ ·è‡³48 kHzï¼Œå¹¶å®æ—¶è¿è¡Œã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSwinSRGANé™ä½äº†å®¢è§‚è¯¯å·®å¹¶æé«˜äº†ABXåå¥½åˆ†æ•°ã€‚åœ¨æœªç»å¾®è°ƒçš„é«˜ä¿çœŸåº¦æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆHiFi-TTSï¼‰çš„é›¶æ ·æœ¬æµ‹è¯•ä¸­ï¼Œå®ƒçš„æ€§èƒ½ä¼˜äºNVSRå’ŒmdctGANï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03913v1">PDF</a> 5 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºSwin Transformerçš„ç«¯åˆ°ç«¯è¯­éŸ³è¶…åˆ†è¾¨ç‡æ¡†æ¶SwinSRGANã€‚å®ƒé€šè¿‡ç»“åˆMDCTå¹…åº¦ä¸Šçš„é•¿æ—¶é—´è°±æ—¶ç©ºä¾èµ–æ€§å’Œæ··åˆå¯¹æŠ—æ–¹æ¡ˆï¼Œè§£å†³äº†ç°æœ‰ç³»ç»Ÿå­˜åœ¨çš„è¡¨ç¤ºä¸åŒ¹é…å’Œè¿‡å¹³æ»‘é—®é¢˜ã€‚è¯¥æ¡†æ¶èƒ½åœ¨å•ä¸ªé€šé“ä¸­ä»¥å®æ—¶æ–¹å¼å°†è¾“å…¥é‡‡æ ·ç‡æé«˜åˆ°48kHzï¼ŒåŒæ—¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å‡å°‘äº†å®¢è§‚è¯¯å·®å¹¶æé«˜äº†ABXåå¥½åˆ†æ•°ã€‚åœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒSwinSRGANåœ¨HiFi-TTSä¸Šçš„é›¶æ ·æœ¬æµ‹è¯•è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºNVSRå’ŒmdctGANï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SwinSRGANæ˜¯ä¸€ä¸ªåŸºäºSwin Transformerçš„ç«¯åˆ°ç«¯è¯­éŸ³è¶…åˆ†è¾¨ç‡æ¡†æ¶ï¼Œç”¨äºä»ä½åˆ†è¾¨ç‡è¯­éŸ³ä¿¡å·é‡å»ºé«˜é¢‘å†…å®¹ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰ç³»ç»Ÿå­˜åœ¨çš„è¡¨ç¤ºä¸åŒ¹é…å’Œè¿‡å¹³æ»‘é—®é¢˜ï¼Œé€šè¿‡ç»“åˆé•¿æ—¶é—´è°±æ—¶ç©ºä¾èµ–æ€§å’Œæ··åˆå¯¹æŠ—æ–¹æ¡ˆï¼Œæé«˜äº†è¯­éŸ³è¶…åˆ†è¾¨ç‡çš„æ•ˆæœã€‚</li>
<li>SwinSRGANèƒ½å¤Ÿå®æ—¶åœ°å°†è¾“å…¥é‡‡æ ·ç‡æé«˜åˆ°48kHzï¼Œæ»¡è¶³äº†å®æ—¶å¤„ç†çš„éœ€æ±‚ã€‚</li>
<li>åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSwinSRGANå‡å°‘äº†å®¢è§‚è¯¯å·®ï¼Œæé«˜äº†ABXåå¥½åˆ†æ•°ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>SwinSRGANé€šè¿‡ç¨€ç–æ„ŸçŸ¥æ­£åˆ™åŒ–å™¨å¯¹arcsinhå‹ç¼©çš„MDCTè¿›è¡Œå¤„ç†ï¼Œæ›´å¥½åœ°ä¿ç•™äº†ç¬æ€æˆåˆ†ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬æµ‹è¯•ä¸­è·å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-45dbe35fec2f83d2fdbf676819266e66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd69170d12a5de30f9e509ad2991b163.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b4ce33f0e70dd0de51cafc5562644db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d33a71b7d790f7bf80e311a017ea1175.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-level-SSL-Feature-Gating-for-Audio-Deepfake-Detection"><a href="#Multi-level-SSL-Feature-Gating-for-Audio-Deepfake-Detection" class="headerlink" title="Multi-level SSL Feature Gating for Audio Deepfake Detection"></a>Multi-level SSL Feature Gating for Audio Deepfake Detection</h2><p><strong>Authors:Hoan My Tran, Damien Lolive, Aghilas Sini, Arnaud Delhay, Pierre-FranÃ§ois Marteau, David Guennec</strong></p>
<p>Recent advancements in generative AI, particularly in speech synthesis, have enabled the generation of highly natural-sounding synthetic speech that closely mimics human voices. While these innovations hold promise for applications like assistive technologies, they also pose significant risks, including misuse for fraudulent activities, identity theft, and security threats. Current research on spoofing detection countermeasures remains limited by generalization to unseen deepfake attacks and languages. To address this, we propose a gating mechanism extracting relevant feature from the speech foundation XLS-R model as a front-end feature extractor. For downstream back-end classifier, we employ Multi-kernel gated Convolution (MultiConv) to capture both local and global speech artifacts. Additionally, we introduce Centered Kernel Alignment (CKA) as a similarity metric to enforce diversity in learned features across different MultiConv layers. By integrating CKA with our gating mechanism, we hypothesize that each component helps improving the learning of distinct synthetic speech patterns. Experimental results demonstrate that our approach achieves state-of-the-art performance on in-domain benchmarks while generalizing robustly to out-of-domain datasets, including multilingual speech samples. This underscores its potential as a versatile solution for detecting evolving speech deepfake threats. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆå¼äººå·¥æ™ºèƒ½é¢†åŸŸçš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³åˆæˆæ–¹é¢ï¼Œå·²ç»èƒ½å¤Ÿå®ç°ç”Ÿæˆé«˜åº¦è‡ªç„¶é€¼çœŸã€ç´§å¯†æ¨¡ä»¿äººå£°çš„åˆæˆè¯­éŸ³ã€‚è™½ç„¶è¿™äº›åˆ›æ–°æŠ€æœ¯è¾…åŠ©æŠ€æœ¯ç­‰é¢†åŸŸçš„åº”ç”¨å‰æ™¯å¹¿é˜”ï¼Œä½†å®ƒä»¬ä¹Ÿå¸¦æ¥äº†é‡å¤§é£é™©ï¼ŒåŒ…æ‹¬è¢«è¯¯ç”¨äºæ¬ºè¯ˆæ´»åŠ¨ã€èº«ä»½ç›—çªƒå’Œå®‰å…¨å¨èƒç­‰ã€‚ç›®å‰å…³äºæ¬ºéª—æ£€æµ‹å¯¹ç­–çš„ç ”ç©¶ä»å—é™äºå¯¹æœªè§è¿‡çš„æ·±åº¦ä¼ªé€ æ”»å‡»å’Œè¯­è¨€çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é—¨æ§æœºåˆ¶ï¼Œä»è¯­éŸ³åŸºç¡€XLS-Ræ¨¡å‹ä¸­æå–ç›¸å…³ç‰¹å¾ä½œä¸ºå‰ç«¯ç‰¹å¾æå–å™¨ã€‚å¯¹äºä¸‹æ¸¸åç«¯åˆ†ç±»å™¨ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ ¸é—¨æ§å·ç§¯ï¼ˆMultiConvï¼‰æ¥æ•æ‰å±€éƒ¨å’Œå…¨å±€è¯­éŸ³ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸­å¿ƒæ ¸å¯¹é½ï¼ˆCKAï¼‰ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡ï¼Œä»¥å¼ºåˆ¶ä¸åŒMultiConvå±‚ä¹‹é—´å­¦ä¹ ç‰¹å¾çš„å¤šæ ·æ€§ã€‚é€šè¿‡å°†CKAä¸æˆ‘ä»¬çš„é—¨æ§æœºåˆ¶ç›¸ç»“åˆï¼Œæˆ‘ä»¬å‡è®¾æ¯ä¸ªç»„ä»¶æœ‰åŠ©äºæ”¹è¿›å¯¹ä¸åŒåˆæˆè¯­éŸ³æ¨¡å¼çš„è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°åŸŸå†…åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿèƒ½ç¨³å¥åœ°æ¨å¹¿åˆ°åŸŸå¤–æ•°æ®é›†ï¼ŒåŒ…æ‹¬å¤šè¯­ç§è¯­éŸ³æ ·æœ¬ã€‚è¿™å‡¸æ˜¾äº†å…¶åœ¨æ£€æµ‹ä¸æ–­å‘å±•çš„è¯­éŸ³æ·±åº¦ä¼ªé€ å¨èƒæ–¹é¢çš„é€šç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03409v1">PDF</a> This paper has been accepted by ACM MM 2025</p>
<p><strong>Summary</strong><br>     ç”Ÿæˆå¼AIåœ¨è¯­éŸ³åˆæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•èƒ½å¤Ÿç”Ÿæˆé«˜åº¦è‡ªç„¶ã€é€¼çœŸæ¨¡ä»¿äººç±»å£°éŸ³çš„åˆæˆè¯­éŸ³ã€‚å°½ç®¡è¿™äº›åˆ›æ–°åœ¨è¾…åŠ©æŠ€æœ¯ç­‰é¢†åŸŸå…·æœ‰åº”ç”¨å‰æ™¯ï¼Œä½†å®ƒä»¬ä¹Ÿå¸¦æ¥æ¬ºè¯ˆæ´»åŠ¨ã€èº«ä»½ç›—ç”¨å’Œå®‰å…¨å¨èƒç­‰é£é™©ã€‚é’ˆå¯¹å½“å‰æ¬ºéª—æ£€æµ‹å¯¹ç­–åœ¨åº”å¯¹æœªè§è¿‡çš„æ·±åº¦ä¼ªé€ æ”»å‡»å’Œè¯­è¨€æ–¹é¢çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æå–è¯­éŸ³åŸºç¡€XLS-Ræ¨¡å‹ç›¸å…³ç‰¹å¾çš„é—¨æ§æœºåˆ¶ä½œä¸ºå‰ç«¯ç‰¹å¾æå–å™¨ã€‚å¯¹äºä¸‹æ¸¸åç«¯åˆ†ç±»å™¨ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ ¸é—¨æ§å·ç§¯ï¼ˆMultiConvï¼‰æ•æ‰å±€éƒ¨å’Œå…¨å±€è¯­éŸ³ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥ä¸­å¿ƒæ ¸å¯¹é½ï¼ˆCKAï¼‰ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡ï¼Œä»¥å¼ºåŒ–ä¸åŒMultiConvå±‚å­¦ä¹ ç‰¹å¾çš„å¤šæ ·æ€§ã€‚é€šè¿‡æ•´åˆCKAä¸æˆ‘ä»¬çš„é—¨æ§æœºåˆ¶ï¼Œæˆ‘ä»¬å‡è®¾æ¯ä¸ªç»„ä»¶æœ‰åŠ©äºæ”¹å–„å¯¹ä¸åŒåˆæˆè¯­éŸ³æ¨¡å¼çš„è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°é¢†åŸŸå†…çš„åŸºå‡†æµ‹è¯•æ—¶è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å¤šå…ƒåŒ–æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šè¯­ç§è¯­éŸ³æ ·æœ¬ã€‚è¿™çªæ˜¾äº†å…¶ä½œä¸ºæ£€æµ‹ä¸æ–­å‘å±•çš„è¯­éŸ³æ·±åº¦ä¼ªé€ å¨èƒçš„é€šç”¨è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIå¯ä»¥ç”Ÿæˆé«˜åº¦è‡ªç„¶çš„åˆæˆè¯­éŸ³ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä½†ä¹Ÿå­˜åœ¨è¢«ç”¨äºæ¬ºè¯ˆå’Œèº«ä»½ç›—ç”¨çš„é£é™©ã€‚</li>
<li>å½“å‰æ¬ºéª—æ£€æµ‹å¯¹ç­–åœ¨åº”å¯¹æ·±åº¦ä¼ªé€ æ”»å‡»å’Œè¯­è¨€æ–¹é¢çš„å±€é™æ€§éœ€è¦æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æå‡ºçš„é—¨æ§æœºåˆ¶åˆ©ç”¨XLS-Ræ¨¡å‹æå–ç›¸å…³ç‰¹å¾ï¼Œä½œä¸ºå‰ç«¯ç‰¹å¾æå–å™¨ã€‚</li>
<li>é‡‡ç”¨å¤šæ ¸é—¨æ§å·ç§¯ï¼ˆMultiConvï¼‰æ•æ‰è¯­éŸ³çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚</li>
<li>å¼•å…¥ä¸­å¿ƒæ ¸å¯¹é½ï¼ˆCKAï¼‰ä»¥å¢å¼ºä¸åŒå±‚å­¦ä¹ ç‰¹å¾çš„å¤šæ ·æ€§ã€‚</li>
<li>CKAä¸é—¨æ§æœºåˆ¶çš„ç»“åˆæœ‰åŠ©äºæé«˜å¯¹ä¸åŒåˆæˆè¯­éŸ³æ¨¡å¼çš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢†åŸŸå†…è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬åœ¨å¤šè¯­ç§è¯­éŸ³æ ·æœ¬ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03409">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e549bb331a05a55341f34b63a6712c2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4862ac93323c59d490c7405528f1a613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa47877443b0a579996042fc724480fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0be598e835c8a95864fd706397351535.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c384b5ca1fc2e3b1b5dc7578b038683.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Scale-Donâ€™t-Fine-tune-Guiding-Multimodal-LLMs-for-Efficient-Visual-Place-Recognition-at-Test-Time"><a href="#Scale-Donâ€™t-Fine-tune-Guiding-Multimodal-LLMs-for-Efficient-Visual-Place-Recognition-at-Test-Time" class="headerlink" title="Scale, Donâ€™t Fine-tune: Guiding Multimodal LLMs for Efficient Visual   Place Recognition at Test-Time"></a>Scale, Donâ€™t Fine-tune: Guiding Multimodal LLMs for Efficient Visual   Place Recognition at Test-Time</h2><p><strong>Authors:Jintao Cheng, Weibin Li, Jiehao Luo, Xiaoyu Tang, Zhijian He, Jin Wu, Yao Zou, Wei Zhang</strong></p>
<p>Visual Place Recognition (VPR) has evolved from handcrafted descriptors to deep learning approaches, yet significant challenges remain. Current approaches, including Vision Foundation Models (VFMs) and Multimodal Large Language Models (MLLMs), enhance semantic understanding but suffer from high computational overhead and limited cross-domain transferability when fine-tuned. To address these limitations, we propose a novel zero-shot framework employing Test-Time Scaling (TTS) that leverages MLLMsâ€™ vision-language alignment capabilities through Guidance-based methods for direct similarity scoring. Our approach eliminates two-stage processing by employing structured prompts that generate length-controllable JSON outputs. The TTS framework with Uncertainty-Aware Self-Consistency (UASC) enables real-time adaptation without additional training costs, achieving superior generalization across diverse environments. Experimental results demonstrate significant improvements in cross-domain VPR performance with up to 210$\times$ computational efficiency gains. </p>
<blockquote>
<p>è§†è§‰åœºæ‰€è¯†åˆ«ï¼ˆVPRï¼‰å·²ç»ä»æ‰‹å·¥æè¿°ç¬¦è¿›åŒ–åˆ°æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä½†ä»å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œæé«˜äº†è¯­ä¹‰ç†è§£ï¼Œä½†åœ¨å¾®è°ƒæ—¶å­˜åœ¨è®¡ç®—å¼€é”€å¤§ã€è·¨åŸŸè¿ç§»èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é›¶æ ·æœ¬æ¡†æ¶ï¼Œé‡‡ç”¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨MLLMsçš„è§†è§‰è¯­è¨€å¯¹é½èƒ½åŠ›ï¼Œé€šè¿‡åŸºäºæŒ‡å¯¼çš„æ–¹æ³•ç›´æ¥è¿›è¡Œç›¸ä¼¼æ€§è¯„åˆ†ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é‡‡ç”¨ç»“æ„åŒ–çš„æç¤ºæ¥ç”Ÿæˆé•¿åº¦å¯æ§çš„JSONè¾“å‡ºï¼Œæ¶ˆé™¤äº†ä¸¤é˜¶æ®µå¤„ç†ã€‚å…·æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆUASCï¼‰çš„TTSæ¡†æ¶å¯å®ç°å®æ—¶é€‚åº”ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆæœ¬ï¼Œåœ¨å¤šç§ç¯å¢ƒä¸­å®ç°äº†ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è·¨åŸŸVPRæ€§èƒ½æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œè®¡ç®—æ•ˆç‡æé«˜äº†é«˜è¾¾210å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02129v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„é›¶æ ·æœ¬æ¡†æ¶ï¼Œåˆ©ç”¨æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰æŠ€æœ¯å’ŒåŸºäºå¼•å¯¼çš„æ–¹æ³•å®ç°è§†è§‰å’Œè¯­è¨€çš„ç›´æ¥ç›¸ä¼¼æ€§è¯„åˆ†ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰è¯­è¨€å¯¹é½èƒ½åŠ›ï¼Œä»¥è§£å†³å½“å‰è§†è§‰å®šä½ï¼ˆVPRï¼‰æ‰€é¢ä¸´çš„è®¡ç®—é‡å¤§ã€è·¨åŸŸè½¬ç§»èƒ½åŠ›ä¸è¶³ç­‰é—®é¢˜ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ç»“æ„åŒ–æç¤ºç”Ÿæˆå¯æ§é•¿åº¦çš„JSONè¾“å‡ºï¼Œå¹¶å¼•å…¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªä¸€è‡´æ€§ï¼ˆUASCï¼‰ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆæœ¬å³å¯å®ç°å®æ—¶é€‚åº”ï¼Œæ˜¾è‘—æé«˜äº†è·¨åŸŸç¯å¢ƒä¸‹çš„VPRæ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰è§†è§‰å®šä½ï¼ˆVPRï¼‰æ–¹æ³•è™½ç„¶å·²ä»æ‰‹å·¥æè¿°ç¬¦å‘å±•åˆ°æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä½†ä»é¢ä¸´è®¡ç®—é‡å¤§å’Œè·¨åŸŸè½¬ç§»èƒ½åŠ›æœ‰é™çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æé«˜äº†è¯­ä¹‰ç†è§£ï¼Œä½†åœ¨ç²¾ç»†è°ƒæ•´æ—¶ä»å­˜åœ¨é«˜è®¡ç®—å¼€é”€å’Œè·¨åŸŸé€‚åº”æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„é›¶æ ·æœ¬æ¡†æ¶ï¼Œé‡‡ç”¨æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨MLLMsçš„è§†è§‰è¯­è¨€å¯¹é½èƒ½åŠ›è¿›è¡Œç›´æ¥ç›¸ä¼¼æ€§è¯„åˆ†ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»“æ„åŒ–æç¤ºç”Ÿæˆé•¿åº¦å¯æ§çš„JSONè¾“å‡ºï¼Œç®€åŒ–äº†å¤„ç†æµç¨‹ã€‚</li>
<li>å¼•å…¥çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªä¸€è‡´æ€§ï¼ˆUASCï¼‰ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹å®ç°å®æ—¶é€‚åº”ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è·¨åŸŸè§†è§‰å®šä½æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œè®¡ç®—æ•ˆç‡æé«˜äº†é«˜è¾¾210å€ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºæœªæ¥çš„è§†è§‰å®šä½ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ï¼Œæœ‰æœ›æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8b1b853f486e11bd3fe12bfc03fee6a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ab5469179f31d76e2da46986c57afa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61719fb174e8a6d40b88f505d636b8ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-913dfddbd293a4fd75f70d5333033c67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20894abb500a2c992eae6e4801a7cbad.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FireRedTTS-2-Towards-Long-Conversational-Speech-Generation-for-Podcast-and-Chatbot"><a href="#FireRedTTS-2-Towards-Long-Conversational-Speech-Generation-for-Podcast-and-Chatbot" class="headerlink" title="FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast   and Chatbot"></a>FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast   and Chatbot</h2><p><strong>Authors:Kun Xie, Feiyu Shen, Junjie Li, Fenglong Xie, Xu Tang, Yao Hu</strong></p>
<p>Current dialogue generation approaches typically require the complete dialogue text before synthesis and produce a single, inseparable speech containing all voices, making them unsuitable for interactive chat; moreover, they suffer from unstable synthesis, inaccurate speaker transitions, and incoherent prosody. In this work, we present FireRedTTS-2, a long-form streaming TTS system for multi-speaker dialogue generation, delivering stable, natural speech with reliable speaker switching and context-aware prosody. A new 12.5Hz streaming speech tokenizer accelerates training and inference, extends maximum dialogue length, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications. We adopt a text-speech interleaved format, concatenating speaker-labeled text with aligned speech tokens in chronological order, and model it with a dual-transformer: a large decoder-only transformer predicts tokens at the first layer, and a smaller one completes subsequent layers. Experimental results show that FireRedTTS-2 integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit contextual cues. In podcast generation, it surpasses existing systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in objective intelligibility, speaker-turn reliability, and perceived naturalness with context-consistent prosody. Our demos are available at <a target="_blank" rel="noopener" href="https://fireredteam.github.io/demos/firered_tts_2">https://fireredteam.github.io/demos/firered_tts_2</a>. </p>
<blockquote>
<p>å½“å‰å¯¹è¯ç”Ÿæˆæ–¹æ³•é€šå¸¸éœ€è¦åœ¨åˆæˆä¹‹å‰æ‹¥æœ‰å®Œæ•´çš„å¯¹è¯æ–‡æœ¬ï¼Œå¹¶ç”ŸæˆåŒ…å«æ‰€æœ‰å£°éŸ³çš„ä¸€ä¸ªä¸å¯åˆ†å‰²çš„è¯­éŸ³ï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆè¿›è¡Œäº¤äº’å¼èŠå¤©ã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¿˜å­˜åœ¨åˆæˆä¸ç¨³å®šã€å‘è¨€äººè½¬æ¢ä¸å‡†ç¡®ã€è¯­è°ƒä¸è¿è´¯ç­‰é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FireRedTTS-2ï¼Œè¿™æ˜¯ä¸€æ¬¾ç”¨äºå¤šå‘è¨€äººå¯¹è¯ç”Ÿæˆçš„é•¿å½¢å¼æµå¼TTSç³»ç»Ÿï¼Œèƒ½å¤Ÿç”Ÿæˆç¨³å®šã€è‡ªç„¶çš„è¯­éŸ³ï¼Œå…·å¤‡å¯é çš„å‘å£°äººåˆ‡æ¢å’Œè¯­å¢ƒæ„ŸçŸ¥çš„è¯­è°ƒã€‚ä¸€æ¬¾æ–°çš„12.5Hzæµå¼è¯­éŸ³æ ‡è®°å™¨åŠ é€Ÿäº†è®­ç»ƒå’Œæ¨ç†ï¼Œå»¶é•¿äº†æœ€å¤§å¯¹è¯é•¿åº¦ï¼Œä¸°å¯Œäº†è¯­ä¹‰ç¼–ç ï¼Œç¨³å®šäº†æ–‡æœ¬åˆ°æ ‡è®°çš„å»ºæ¨¡ï¼Œå¹¶æ”¯æŒå®æ—¶åº”ç”¨çš„é«˜ä¿çœŸæµå¼ç”Ÿæˆã€‚æˆ‘ä»¬é‡‡ç”¨æ–‡æœ¬è¯­éŸ³äº¤é”™æ ¼å¼ï¼ŒæŒ‰æ—¶é—´é¡ºåºå°†å¸¦æ ‡ç­¾çš„å‘è¨€è€…æ–‡æœ¬ä¸å¯¹é½çš„è¯­éŸ³æ ‡è®°è¿æ¥èµ·æ¥ï¼Œå¹¶ç”¨åŒå˜å‹å™¨å¯¹å…¶è¿›è¡Œå»ºæ¨¡ï¼šä¸€ä¸ªå¤§å‹ä»…è§£ç å™¨å˜å‹å™¨åœ¨ç¬¬ä¸€å±‚é¢„æµ‹æ ‡è®°ï¼Œå¦ä¸€ä¸ªè¾ƒå°çš„å˜å‹å™¨å®Œæˆåç»­å±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFireRedTTS-2æ— ç¼é›†æˆèŠå¤©æ¡†æ¶ï¼Œåœ¨å¾®è°ƒæœ€å°‘çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥æ ¹æ®éšå¼ä¸Šä¸‹æ–‡çº¿ç´¢äº§ç”Ÿå¯Œæœ‰æƒ…æ„Ÿçš„è¯­éŸ³ã€‚åœ¨æ’­å®¢ç”Ÿæˆæ–¹é¢ï¼Œå®ƒåœ¨å®¢è§‚æ¸…æ™°åº¦ã€å‘è¨€äººè½®æ›¿å¯é æ€§å’Œå…·æœ‰è¯­å¢ƒè¿è´¯æ€§çš„è¯­è°ƒæ„ŸçŸ¥è‡ªç„¶åº¦ç­‰æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„ç³»ç»Ÿï¼ŒåŒ…æ‹¬MoonCastã€Zipvoice-Dialogueå’ŒMOSS-TTSDã€‚æˆ‘ä»¬çš„æ¼”ç¤ºè§†é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://fireredteam.github.io/demos/firered_tts_2">ç½‘ç«™é“¾æ¥</a>è§‚çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02020v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†FireRedTTS-2ï¼Œä¸€ä¸ªç”¨äºå¤šè¯´è¯è€…å¯¹è¯ç”Ÿæˆçš„é•¿å½¢å¼æµå¼TTSç³»ç»Ÿã€‚ç›¸è¾ƒäºä¼ ç»Ÿå¯¹è¯ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç¨³å®šã€è‡ªç„¶åœ°åˆæˆè¯­éŸ³ï¼Œæ”¯æŒå¯é çš„è¯´è¯è€…åˆ‡æ¢å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­è°ƒã€‚é‡‡ç”¨12.5Hzæµå¼è¯­éŸ³æ ‡è®°å™¨åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ï¼Œæ‰©å±•æœ€å¤§å¯¹è¯é•¿åº¦ï¼Œç¼–ç ä¸°å¯Œçš„è¯­ä¹‰ä»¥ç¨³å®šæ–‡æœ¬åˆ°æ ‡è®°çš„å»ºæ¨¡ï¼Œå¹¶æ”¯æŒé«˜ä¿çœŸæµå¼ç”Ÿæˆç”¨äºå®æ—¶åº”ç”¨ã€‚é€šè¿‡æ–‡æœ¬è¯­éŸ³äº¤ç»‡æ ¼å¼ï¼ŒæŒ‰æ—¶é—´é¡ºåºè¿æ¥è¯´è¯è€…æ ‡è®°çš„æ–‡æœ¬ä¸å¯¹é½çš„è¯­éŸ³æ ‡è®°ï¼Œå¹¶ç”¨åŒå˜å‹å™¨æ¨¡å‹è¿›è¡Œå»ºæ¨¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFireRedTTS-2æ— ç¼é›†æˆèŠå¤©æ¡†æ¶ï¼Œç»è¿‡æœ€å°å¾®è°ƒå³å¯äº§ç”Ÿå—éšæ€§ä¸Šä¸‹æ–‡çº¿ç´¢å¼•å¯¼çš„æƒ…æ„Ÿè¡¨è¾¾è¯­éŸ³ã€‚åœ¨æ’­å®¢ç”Ÿæˆæ–¹é¢ï¼Œå®ƒåœ¨å®¢è§‚æ¸…æ™°åº¦ã€è¯´è¯è€…åˆ‡æ¢å¯é æ€§å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§çš„è¯­è°ƒæ„ŸçŸ¥è‡ªç„¶æ€§ç­‰æ–¹é¢è¶…è¿‡äº†MoonCastã€Zipvoice-Dialogueå’ŒMOSS-TTSDç­‰ç°æœ‰ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ¼”ç¤ºè§†é¢‘å¯åœ¨ <a target="_blank" rel="noopener" href="https://fireredteam.github.io/demos/firered_tts_2">https://fireredteam.github.io/demos/firered_tts_2</a> æŸ¥çœ‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>FireRedTTS-2æ˜¯ä¸€ä¸ªé•¿å½¢å¼æµå¼TTSç³»ç»Ÿï¼Œç”¨äºå¤šè¯´è¯è€…å¯¹è¯ç”Ÿæˆã€‚</li>
<li>ç³»ç»Ÿèƒ½ç¨³å®šã€è‡ªç„¶åœ°åˆæˆè¯­éŸ³ï¼Œæ”¯æŒå¯é è¯´è¯è€…åˆ‡æ¢å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­è°ƒã€‚</li>
<li>é‡‡ç”¨12.5Hzæµå¼è¯­éŸ³æ ‡è®°å™¨åŠ é€Ÿè®­ç»ƒã€æ‰©å±•å¯¹è¯é•¿åº¦å¹¶ç¼–ç ä¸°å¯Œçš„è¯­ä¹‰ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬è¯­éŸ³äº¤ç»‡æ ¼å¼ä¸åŒå˜å‹å™¨æ¨¡å‹è¿›è¡Œå»ºæ¨¡ï¼Œæé«˜æ€§èƒ½ã€‚</li>
<li>FireRedTTS-2å¯æ— ç¼é›†æˆèŠå¤©æ¡†æ¶ï¼Œå¹¶äº§ç”Ÿå—ä¸Šä¸‹æ–‡å¼•å¯¼çš„æƒ…æ„Ÿè¡¨è¾¾è¯­éŸ³ã€‚</li>
<li>åœ¨æ’­å®¢ç”Ÿæˆæ–¹é¢ï¼Œè¯¥ç³»ç»Ÿåœ¨æ¸…æ™°åº¦ã€è¯´è¯è€…åˆ‡æ¢å’Œè¯­è°ƒè‡ªç„¶æ€§ç­‰æ–¹é¢è¶…è¶Šäº†ç°æœ‰ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-732b08367eaec057df4e08a01ff10fad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ef6ecdf44b0d29be8d337fcdeb3f6af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-721a86349a66b33ea24c48f73bd459e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76fbaaba9fdfcfed7bde79caf19e0e83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e355a49e15bdbb99888ef772565dded.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a6374bb02581b225e191a1dda54063b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dae9f3f73260c93c4c3ded5ff25248e2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SimulMEGA-MoE-Routers-are-Advanced-Policy-Makers-for-Simultaneous-Speech-Translation"><a href="#SimulMEGA-MoE-Routers-are-Advanced-Policy-Makers-for-Simultaneous-Speech-Translation" class="headerlink" title="SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation"></a>SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation</h2><p><strong>Authors:Chenyang Le, Bing Han, Jinshun Li, Songyong Chen, Yanmin Qian</strong></p>
<p>Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs. </p>
<blockquote>
<p>åŒæ—¶è¯­éŸ³ç¿»è¯‘ï¼ˆSimulSTï¼‰é€šè¿‡è”åˆä¼˜åŒ–è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘ï¼Œåœ¨ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶ä¸‹å®ç°å®æ—¶è·¨è¯­è¨€äº¤æµã€‚ç°æœ‰ç³»ç»Ÿåœ¨å¹³è¡¡ç¿»è¯‘è´¨é‡ã€å»¶è¿Ÿå’Œè¯­ä¹‰è¿è´¯æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯­è¨€å¤šå¯¹å¤šçš„åœºæ™¯ä¸­ï¼Œä¸åŒçš„è¯»å–å’Œå†™å…¥ç­–ç•¥é˜»ç¢äº†ç»Ÿä¸€ç­–ç•¥å­¦ä¹ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SimulMEGAï¼ˆåŸºäºä¸“å®¶æ··åˆé—¨æ§çš„å®æ—¶ç”Ÿæˆï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— ç›‘ç£çš„ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†åŸºäºå‰ç¼€çš„è®­ç»ƒä¸ä¸“å®¶æ··åˆç²¾ç‚¼å™¨ç›¸ç»“åˆï¼Œä»¥éšå¼çš„æ–¹å¼å­¦ä¹ æœ‰æ•ˆçš„è¯»å–å’Œå†™å…¥å†³ç­–ï¼Œè€Œä¸ä¼šå¢åŠ æ¨ç†æ—¶é—´å¼€é”€ã€‚æˆ‘ä»¬çš„è®¾è®¡åªéœ€è¦å¯¹æ ‡å‡†å˜å‹å™¨æ¶æ„è¿›è¡Œæœ€å°é™åº¦çš„ä¿®æ”¹ï¼Œå°±å¯ä»¥æ¨å¹¿åˆ°è¯­éŸ³åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„æµå¼å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡å¯¹å…­ç§è¯­è¨€å¯¹çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬çš„5äº¿å‚æ•°è¯­éŸ³åˆ°æ–‡æœ¬æ¨¡å‹çš„æ€§èƒ½è¶…è¿‡äº†æ— ç¼åŸºçº¿ï¼Œåœ¨å¹³å‡å»¶è¿Ÿ1.5ç§’çš„æƒ…å†µä¸‹ï¼ŒBLEUå¾—åˆ†é™ä½ä¸åˆ°7%ï¼Œåœ¨3ç§’å»¶è¿Ÿçš„æƒ…å†µä¸‹ï¼Œé™ä½ä¸åˆ°3%ã€‚æˆ‘ä»¬é€šè¿‡åœ¨å…·æœ‰å•å‘éª¨å¹²çš„æµå¼TTSä¸­æ‰©å±•SimulMEGAï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶é€šç”¨æ€§ï¼Œè·å¾—äº†æ›´å¥½çš„å»¶è¿Ÿè´¨é‡æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01200v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†SimulMEGAï¼ˆåŸºäºä¸“å®¶é—¨æ§çš„å®æ—¶ç”Ÿæˆæ–¹æ³•ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§£å†³å¤šè¯­è¨€åŒæ—¶ç¿»è¯‘é—®é¢˜çš„æ–°æ–¹æ³•ã€‚SimulMEGAç»“åˆäº†å‰ç¼€è®­ç»ƒä¸ä¸“å®¶æ··åˆç²¾ç‚¼å™¨ï¼Œåœ¨æ— éœ€é¢å¤–æ¨ç†æ—¶é—´å¼€é”€çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥éšå¼å­¦ä¹ è¯»å†™å†³ç­–ç­–ç•¥ã€‚é€šè¿‡å¯¹æ ‡å‡†è½¬æ¢å™¨æ¶æ„è¿›è¡Œå°‘é‡ä¿®æ”¹ï¼Œè¯¥æ¨¡å‹å¯åœ¨è¯­éŸ³åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„æµåª’ä½“ä»»åŠ¡ä¸­é€šç”¨åŒ–ã€‚åœ¨å…­ç§è¯­è¨€å¯¹çš„ç»¼åˆè¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬çš„è¯­éŸ³åˆ°æ–‡æœ¬æ¨¡å‹åœ¨å¹³å‡å»¶è¿Ÿä»…ä¸ºå‡ ç§’çš„æƒ…å†µä¸‹å®ç°äº†å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†SimulMEGAåœ¨å…·æœ‰å•å‘éª¨å¹²çš„æµå¼æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SimulMEGAè§£å†³äº†åŒæ—¶ç¿»è¯‘ä¸­å­˜åœ¨çš„é—®é¢˜ï¼ŒåŒ…æ‹¬å¹³è¡¡ç¿»è¯‘è´¨é‡ã€å»¶è¿Ÿå’Œè¯­ä¹‰è¿è´¯æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å‰ç¼€è®­ç»ƒä¸ä¸“å®¶æ··åˆç²¾ç‚¼å™¨ï¼Œèƒ½éšå¼å­¦ä¹ è¯»å†™å†³ç­–ç­–ç•¥ã€‚</li>
<li>SimulMEGAä»…éœ€å¯¹æ ‡å‡†è½¬æ¢å™¨æ¶æ„è¿›è¡Œæœ€å°ä¿®æ”¹ï¼Œå³å¯åº”ç”¨äºè¯­éŸ³åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„æµåª’ä½“ä»»åŠ¡ã€‚</li>
<li>åœ¨å…­ç§è¯­è¨€å¯¹çš„æµ‹è¯•ä¸­ï¼ŒSimulMEGAçš„è¯­éŸ³åˆ°æ–‡æœ¬æ¨¡å‹åœ¨è¾ƒçŸ­å»¶è¿Ÿä¸‹å®ç°äº†é«˜ç¿»è¯‘è´¨é‡ã€‚</li>
<li>SimulMEGAé€šè¿‡åˆ©ç”¨å•å‘éª¨å¹²ï¼Œå±•ç¤ºäº†åœ¨æµå¼æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­çš„ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æœªå¢åŠ æ¨ç†æ—¶é—´å¼€é”€çš„å‰æä¸‹æé«˜äº†ç¿»è¯‘æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26489addf658b993d6290d1f1ae21778.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce5b5111ae557683a6ee2e36b7ca4e35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38e5b98d73926dee6d1adfcf8a4fd647.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MPO-Multidimensional-Preference-Optimization-for-Language-Model-based-Text-to-Speech"><a href="#MPO-Multidimensional-Preference-Optimization-for-Language-Model-based-Text-to-Speech" class="headerlink" title="MPO: Multidimensional Preference Optimization for Language Model-based   Text-to-Speech"></a>MPO: Multidimensional Preference Optimization for Language Model-based   Text-to-Speech</h2><p><strong>Authors:Kangxiang Xia, Xinfa Zhu, Jixun Yao, Lei Xie</strong></p>
<p>In recent years, text-to-speech (TTS) has seen impressive advancements through large-scale language models, achieving human-level speech quality. Integrating human feedback has proven effective for enhancing robustness in these systems. However, current approaches face challenges in optimizing TTS with preference data across multiple dimensions and often suffer from performance degradation due to overconfidence in rewards. We propose Multidimensional Preference Optimization (MPO) to better align TTS systems with human preferences. MPO introduces a preference set that streamlines the construction of data for multidimensional preference optimization, enabling alignment with multiple dimensions. Additionally, we incorporate regularization during training to address the typical degradation issues in DPO-based approaches. Our experiments demonstrate MPOâ€™s effectiveness, showing significant improvements in intelligibility, speaker similarity, and prosody compared to baseline systems. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯é€šè¿‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›æ­¥ï¼Œå®ç°äº†äººç±»çº§åˆ«çš„è¯­éŸ³è´¨é‡ã€‚é›†æˆäººç±»åé¦ˆå·²ç»è¯æ˜å¯ä»¥æé«˜è¿™äº›ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨ä¼˜åŒ–å…·æœ‰å¤šä¸ªç»´åº¦çš„åå¥½æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”ç”±äºè¿‡äºä¾èµ–å¥–åŠ±è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºå¤šç»´åº¦åå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰æ¥æ›´å¥½åœ°å°†TTSç³»ç»Ÿä¸äººç±»åå¥½å¯¹é½ã€‚MPOå¼•å…¥äº†ä¸€ä¸ªåå¥½é›†ï¼Œç®€åŒ–äº†ç”¨äºå¤šç»´åº¦åå¥½ä¼˜åŒ–çš„æ•°æ®æ„å»ºï¼Œå®ç°äº†ä¸å¤šä¸ªç»´åº¦çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†æ­£åˆ™åŒ–ï¼Œä»¥è§£å†³DPOæ–¹æ³•å¸¸è§çš„é€€åŒ–é—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†MPOçš„æœ‰æ•ˆæ€§ï¼Œä¸åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼Œåœ¨æ¸…æ™°åº¦ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œè¯­è°ƒæ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00685v1">PDF</a> Accepted by NCMMSC2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯é€šè¿‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå®ç°äº†äººç±»æ°´å¹³çš„è¯­éŸ³è´¨é‡ã€‚é€šè¿‡æ•´åˆäººç±»åé¦ˆæé«˜äº†ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨ä¼˜åŒ–å…·æœ‰å¤šç»´åº¦åå¥½æ•°æ®çš„TTSæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶å¸¸å› è¿‡åº¦è‡ªä¿¡äºå¥–åŠ±è€Œå‡ºç°æ€§èƒ½ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºå¤šç»´åº¦åå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰ä»¥æ›´å¥½åœ°ä½¿TTSç³»ç»Ÿä¸äººç±»åå¥½å¯¹é½ã€‚MPOå¼•å…¥åå¥½é›†ï¼Œç®€åŒ–äº†ç”¨äºå¤šç»´åº¦åå¥½ä¼˜åŒ–çš„æ•°æ®æ„å»ºï¼Œå®ç°äº†ä¸å¤šä¸ªç»´åº¦çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥äº†æ­£åˆ™åŒ–ï¼Œä»¥è§£å†³DPOæ–¹æ³•å¸¸è§çš„é€€åŒ–é—®é¢˜ã€‚å®éªŒè¯æ˜MPOçš„æœ‰æ•ˆæ€§ï¼Œåœ¨å¯æ‡‚åº¦ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè¯­è°ƒæ–¹é¢ç›¸æ¯”åŸºçº¿ç³»ç»Ÿæœ‰æ˜æ˜¾æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯è¿‘å¹´é€šè¿‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>æ•´åˆäººç±»åé¦ˆèƒ½æé«˜TTSç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚</li>
<li>å½“å‰TTSä¼˜åŒ–é¢ä¸´å¤šç»´åº¦åå¥½æ•°æ®ä¼˜åŒ–æŒ‘æˆ˜åŠæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>æå‡ºå¤šç»´åº¦åå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰æ–¹æ³•ä»¥æ›´å¥½åœ°ä½¿TTSç³»ç»Ÿä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>MPOå¼•å…¥åå¥½é›†ï¼Œç®€åŒ–æ•°æ®æ„å»ºï¼Œå®ç°ä¸å¤šä¸ªç»´åº¦çš„å¯¹é½ã€‚</li>
<li>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥æ­£åˆ™åŒ–ï¼Œè§£å†³DPOæ–¹æ³•å¸¸è§çš„é€€åŒ–é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3226177ccf1b07cde45a65581c5a1810.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9add0f64f43031e99e9f2c65f001d5e7.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef58a5db1a63afde397c64847af0e07e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801508&auth_key=1759801508-0-0-07d00a4865e88d24d9664eb3d3544841&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ee6b852101aeb9ee9ab9f1477c85cd3c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801515&auth_key=1759801515-0-0-f863bd34e9168c5e0e63c89cbd587dca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Towards-Improved-Speech-Recognition-through-Optimized-Synthetic-Data-Generation"><a href="#Towards-Improved-Speech-Recognition-through-Optimized-Synthetic-Data-Generation" class="headerlink" title="Towards Improved Speech Recognition through Optimized Synthetic Data   Generation"></a>Towards Improved Speech Recognition through Optimized Synthetic Data   Generation</h2><p><strong>Authors:Yanis Perrin, Gilles Boulianne</strong></p>
<p>Supervised training of speech recognition models requires access to transcribed audio data, which often is not possible due to confidentiality issues. Our approach to this problem is to generate synthetic audio from a text-only corpus using a state-of-the-art text-to-speech model with voice cloning capabilities. Our goal is to achieve automatic speech recognition (ASR) performance comparable to models trained on real data. We explore ways to optimize synthetic data generation through finetuning, filtering and evaluation, and its use for training an end-to-end encoder-decoder ASR model. Experiments were conducted using two datasets of spontaneous, conversational speech in Qu&#39;ebec French. We show that improving data generation leads to large improvements in the final ASR system trained on synthetic data. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«çš„ç›‘ç£è®­ç»ƒæ¨¡å‹éœ€è¦è®¿é—®è½¬å½•çš„éŸ³é¢‘æ•°æ®ï¼Œä½†ç”±äºä¿å¯†é—®é¢˜ï¼Œè¿™é€šå¸¸æ˜¯ä¸å¯èƒ½çš„ã€‚æˆ‘ä»¬è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•æ˜¯åˆ©ç”¨å…·æœ‰è¯­éŸ³å…‹éš†åŠŸèƒ½çš„æœ€æ–°æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ï¼Œä»ä»…åŒ…å«æ–‡æœ¬çš„è¯­æ–™åº“ä¸­ç”ŸæˆåˆæˆéŸ³é¢‘ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å®ç°ä¸åœ¨çœŸå®æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸å½“çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½ã€‚æˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡å¾®è°ƒã€è¿‡æ»¤å’Œè¯„ä¼°ä¼˜åŒ–åˆæˆæ•°æ®ç”Ÿæˆçš„æ–¹æ³•ï¼Œä»¥åŠå…¶ç”¨äºè®­ç»ƒç«¯åˆ°ç«¯ç¼–ç å™¨-è§£ç å™¨ASRæ¨¡å‹çš„åº”ç”¨ã€‚å®éªŒä½¿ç”¨äº†é­åŒ—å…‹æ³•è¯­çš„ä¸¤å¥—è‡ªç„¶å¯¹è¯è¯­éŸ³æ•°æ®é›†ã€‚æˆ‘ä»¬è¯æ˜äº†æ”¹è¿›æ•°æ®ç”Ÿæˆå¯ä»¥å¤§å¤§æé«˜åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æœ€ç»ˆASRç³»ç»Ÿçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21631v1">PDF</a> 12 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”±äºä¿å¯†é—®é¢˜ï¼Œè¯­éŸ³è¯†åˆ«æ¨¡å‹æ— æ³•è·å–è½¬å½•éŸ³é¢‘æ•°æ®çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡é‡‡ç”¨æœ€æ–°è¯­éŸ³å…‹éš†æŠ€æœ¯ï¼Œç”ŸæˆåŸºäºçº¯æ–‡æœ¬è¯­æ–™åº“çš„åˆæˆéŸ³é¢‘æ•°æ®ã€‚ç›®æ ‡æ˜¯å®ç°ä¸çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹ç›¸å½“çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¾®è°ƒã€è¿‡æ»¤å’Œè¯„ä¼°ä¼˜åŒ–åˆæˆæ•°æ®çš„ç”Ÿæˆï¼Œå¹¶åˆ©ç”¨è¿™äº›åˆæˆæ•°æ®è®­ç»ƒç«¯åˆ°ç«¯çš„ç¼–ç å™¨è§£ç å™¨ASRæ¨¡å‹ã€‚å®éªŒé‡‡ç”¨é­åŒ—å…‹æ³•è¯­çš„ä¸¤ä»½è‡ªç„¶å¯¹è¯è¯­éŸ³æ•°æ®é›†ï¼Œè¯æ˜äº†æ”¹è¿›æ•°æ®ç”Ÿæˆå¯¹åŸºäºåˆæˆæ•°æ®çš„æœ€ç»ˆASRç³»ç»Ÿæ€§èƒ½æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å›¢é˜Ÿè§£å†³äº†å› ä¿å¯†é—®é¢˜æ— æ³•è·å–è½¬å½•éŸ³é¢‘æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†æœ€æ–°è¯­éŸ³å…‹éš†æŠ€æœ¯ï¼Œç”ŸæˆåŸºäºçº¯æ–‡æœ¬è¯­æ–™åº“çš„åˆæˆéŸ³é¢‘æ•°æ®ã€‚</li>
<li>ç ”ç©¶ç›®æ ‡æ˜¯å®ç°ä¸çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹ç›¸å½“çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¾®è°ƒã€è¿‡æ»¤å’Œè¯„ä¼°ä¼˜åŒ–äº†åˆæˆæ•°æ®çš„ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>åˆ©ç”¨åˆæˆæ•°æ®è®­ç»ƒäº†ç«¯åˆ°ç«¯çš„ç¼–ç å™¨è§£ç å™¨ASRæ¨¡å‹ã€‚</li>
<li>å®éªŒé‡‡ç”¨äº†é­åŒ—å…‹æ³•è¯­çš„ä¸¤ä»½è‡ªç„¶å¯¹è¯è¯­éŸ³æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f5d06f400989d676a07da2676476e7f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801522&auth_key=1759801522-0-0-3aff4876abffc466a1ed7771eda5eb5a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-a3b8f3db35983d23403a350d480fbbe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e5d12c5e96eb90339714d148e5aa4d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d47fb43817366f2ba13d31ddb1facee.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b651a86c95ee479d16f9bf5beb59573a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801549&auth_key=1759801549-0-0-c30dc2697d09eceda9c18cd27745a869&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-81a7d97e1826d14169508c581fa5c467.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0f17589ce80f9af6e692f775030359e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801562&auth_key=1759801562-0-0-69f5285cfd4da5a4afb97e727485912f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94cd614459a32e0b39e1e1fd0fbb1084~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801569&auth_key=1759801569-0-0-4450926907e4133015244082c0200cef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CLEAR-Continuous-Latent-Autoregressive-Modeling-for-High-quality-and-Low-latency-Speech-Synthesis"><a href="#CLEAR-Continuous-Latent-Autoregressive-Modeling-for-High-quality-and-Low-latency-Speech-Synthesis" class="headerlink" title="CLEAR: Continuous Latent Autoregressive Modeling for High-quality and   Low-latency Speech Synthesis"></a>CLEAR: Continuous Latent Autoregressive Modeling for High-quality and   Low-latency Speech Synthesis</h2><p><strong>Authors:Chun Yat Wu, Jiajun Deng, Guinan Li, Qiuqiang Kong, Simon Lui</strong></p>
<p>Autoregressive (AR) language models have emerged as powerful solutions for zero-shot text-to-speech (TTS) synthesis, capable of generating natural speech from a few seconds of audio prompts. However, conventional AR-based TTS systems relying on discrete audio tokens face the challenge of lossy compression during tokenization, requiring longer discrete token sequences to capture the same information as continuous ones, which adds inference latency and complicates AR modeling. To address this challenge, this paper proposes the Continuous Latent Autoregressive model (CLEAR), a unified zero-shot TTS framework that directly models continuous audio representations. More specifically, CLEAR introduces an enhanced variational autoencoder with shortcut connections, which achieves a high compression ratio to map waveforms into compact continuous latents. A lightweight MLP-based rectified flow head that operates independently for each hidden state is presented to model the continuous latent probability distribution, and trained jointly with the AR model within a single-stage framework. Experiments show that the proposed zero-shot CLEAR TTS can synthesize high-quality speech with low latency. Compared to state-of-the-art (SOTA) TTS models, CLEAR delivers competitive performance in robustness, speaker similarity and naturalness, while offering a lower real-time factor (RTF). In particular, CLEAR achieves SOTA results on the LibriSpeech test-clean dataset, with a word error rate of 1.88% and an RTF of 0.29. Moreover, CLEAR facilitates streaming speech synthesis with a first-frame delay of 96ms, while maintaining high-quality speech synthesis. </p>
<blockquote>
<p>è‡ªå›å½’ï¼ˆARï¼‰è¯­è¨€æ¨¡å‹å·²ä½œä¸ºé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆçš„å¼ºå¤§è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®å‡ ç§’é’Ÿçš„éŸ³é¢‘æç¤ºç”Ÿæˆè‡ªç„¶è¯­éŸ³ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºARçš„TTSç³»ç»Ÿä¾èµ–äºç¦»æ•£éŸ³é¢‘ä»¤ç‰Œé¢ä¸´ç€ä»¤ç‰ŒåŒ–è¿‡ç¨‹ä¸­çš„æœ‰æŸå‹ç¼©æŒ‘æˆ˜ï¼Œéœ€è¦æ›´é•¿çš„ç¦»æ•£ä»¤ç‰Œåºåˆ—æ¥æ•è·ä¸è¿ç»­ä»¤ç‰Œç›¸åŒçš„ä¿¡æ¯ï¼Œè¿™å¢åŠ äº†æ¨ç†å»¶è¿Ÿå¹¶å¤æ‚åŒ–äº†ARå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è¿ç»­æ½œåœ¨è‡ªå›å½’æ¨¡å‹ï¼ˆCLEARï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„é›¶æ ·æœ¬TTSæ¡†æ¶ï¼Œç›´æ¥å¯¹è¿ç»­éŸ³é¢‘è¡¨ç¤ºè¿›è¡Œå»ºæ¨¡ã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒCLEARå¼•å…¥äº†ä¸€ä¸ªå¸¦æœ‰å¿«æ·æ–¹å¼è¿æ¥çš„é«˜çº§å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼Œå®ç°äº†é«˜å‹ç¼©æ¯”ï¼Œå°†æ³¢å½¢æ˜ å°„åˆ°ç´§å‡‘çš„è¿ç»­æ½œåœ¨ç©ºé—´ã€‚è¿˜æå‡ºäº†ä¸€ç§åŸºäºMLPçš„æ ¡æ­£æµå¤´ï¼Œå®ƒé’ˆå¯¹æ¯ä¸ªéšè—çŠ¶æ€ç‹¬ç«‹è¿è¡Œï¼Œç”¨äºå¯¹è¿ç»­æ½œåœ¨æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä¸ARæ¨¡å‹åœ¨åŒä¸€ä¸ªå•ä¸€é˜¶æ®µæ¡†æ¶å†…è”åˆè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„é›¶æ ·æœ¬CLEAR TTSå¯ä»¥åˆæˆé«˜è´¨é‡è¯­éŸ³ï¼Œå¹¶å…·æœ‰ä½å»¶è¿Ÿã€‚ä¸æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰TTSæ¨¡å‹ç›¸æ¯”ï¼ŒCLEARåœ¨ç¨³å¥æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè‡ªç„¶æ€§æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒæ—¶æä¾›äº†æ›´ä½çš„å®æ—¶å› å­ï¼ˆRTFï¼‰ã€‚ç‰¹åˆ«æ˜¯åœ¨LibriSpeechæµ‹è¯•æ¸…æ´æ•°æ®é›†ä¸Šï¼ŒCLEARå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œè¯é”™è¯¯ç‡ä¸º1.88%ï¼ŒRTFä¸º0.29%ã€‚æ­¤å¤–ï¼ŒCLEARå¯å®ç°æµå¼è¯­éŸ³åˆæˆï¼Œé¦–å¸§å»¶è¿Ÿä¸º96æ¯«ç§’ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡è¯­éŸ³åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19098v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>è¿ç»­æ½œåœ¨è‡ªå›å½’æ¨¡å‹ï¼ˆCLEARï¼‰æ˜¯ä¸€ç§é›¶æ ·æœ¬æ–‡æœ¬-è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆçš„æ–°è§£å†³æ–¹æ¡ˆï¼Œå®ƒç›´æ¥å¯¹è¿ç»­éŸ³é¢‘è¡¨ç¤ºè¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡ä½¿ç”¨å¢å¼ºå‹å˜åˆ†è‡ªç¼–ç å™¨ä¸å¿«æ·è¿æ¥ï¼Œä»¥åŠåŸºäºMLPçš„æ ¡æ­£æµå¤´ï¼ŒCLEARå®ç°äº†æ³¢å½¢åˆ°ç´§å‡‘è¿ç»­æ½œåœ¨ç©ºé—´çš„æ˜ å°„ï¼Œå¹¶åœ¨å•é˜¶æ®µæ¡†æ¶ä¸­ä¸ARæ¨¡å‹è”åˆè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒCLEARèƒ½åˆæˆé«˜è´¨é‡è¯­éŸ³ï¼Œå…·æœ‰ä½å»¶è¿Ÿã€‚åœ¨LibriSpeechæµ‹è¯•é›†ä¸Šï¼ŒCLEARè¡¨ç°å“è¶Šï¼Œè¯é”™è¯¯ç‡ä¸º1.88%ï¼Œå®æ—¶å› å­ä¸º0.29ã€‚æ­¤å¤–ï¼ŒCLEARæ”¯æŒæµå¼è¯­éŸ³åˆæˆï¼Œé¦–å¸§å»¶è¿Ÿä»…ä¸º96æ¯«ç§’ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡è¯­éŸ³åˆæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’ï¼ˆARï¼‰è¯­è¨€æ¨¡å‹å·²æˆä¸ºé›¶æ ·æœ¬TTSåˆæˆçš„å¼ºå¤§è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä¼ ç»ŸAR-based TTSç³»ç»Ÿé¢ä¸´ç¦»æ•£éŸ³é¢‘ä»¤ç‰ŒæŸå¤±å‹ç¼©çš„æŒ‘æˆ˜ã€‚</li>
<li>CLEARæ¨¡å‹ç›´æ¥å¯¹è¿ç»­éŸ³é¢‘è¡¨ç¤ºè¿›è¡Œå»ºæ¨¡ï¼Œè§£å†³äº†ä»¤ç‰ŒåŒ–è¿‡ç¨‹ä¸­çš„æŸå¤±å‹ç¼©é—®é¢˜ã€‚</li>
<li>CLEARä½¿ç”¨å¢å¼ºå‹å˜åˆ†è‡ªç¼–ç å™¨å’ŒåŸºäºMLPçš„æ ¡æ­£æµå¤´è¿›è¡ŒéŸ³é¢‘è¡¨ç¤ºã€‚</li>
<li>CLEARå®ç°äº†æ³¢å½¢åˆ°ç´§å‡‘è¿ç»­æ½œåœ¨ç©ºé—´çš„æ˜ å°„ï¼Œæé«˜äº†å‹ç¼©æ¯”ã€‚</li>
<li>CLEARåœ¨LibriSpeechæµ‹è¯•é›†ä¸Šè¡¨ç°å“è¶Šï¼Œå…·æœ‰è¾ƒä½çš„è¯é”™è¯¯ç‡å’Œå®æ—¶å› å­ã€‚</li>
<li>CLEARæ”¯æŒæµå¼è¯­éŸ³åˆæˆï¼Œå…·æœ‰è¾ƒä½çš„é¦–å¸§å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡è¯­éŸ³åˆæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-483a5dff0f2a59a42e1142e136db8881~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801576&auth_key=1759801576-0-0-6e77ab733e5371a1b0cd4b750a9da8a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0996660c3ed225eec4df988df9208fb1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801583&auth_key=1759801583-0-0-44c59d42119d473f8819db5cdbecd3f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Unseen-Speaker-and-Language-Adaptation-for-Lightweight-Text-To-Speech-with-Adapters"><a href="#Unseen-Speaker-and-Language-Adaptation-for-Lightweight-Text-To-Speech-with-Adapters" class="headerlink" title="Unseen Speaker and Language Adaptation for Lightweight Text-To-Speech   with Adapters"></a>Unseen Speaker and Language Adaptation for Lightweight Text-To-Speech   with Adapters</h2><p><strong>Authors:Alessio Falai, Ziyao Zhang, Akos Gangoly</strong></p>
<p>In this paper we investigate cross-lingual Text-To-Speech (TTS) synthesis through the lens of adapters, in the context of lightweight TTS systems. In particular, we compare the tasks of unseen speaker and language adaptation with the goal of synthesising a target voice in a target language, in which the target voice has no recordings therein. Results from objective evaluations demonstrate the effectiveness of adapters in learning language-specific and speaker-specific information, allowing pre-trained models to learn unseen speaker identities or languages, while avoiding catastrophic forgetting of the original modelâ€™s speaker or language information. Additionally, to measure how native the generated voices are in terms of accent, we propose and validate an objective metric inspired by mispronunciation detection techniques in second-language (L2) learners. The paper also provides insights into the impact of adapter placement, configuration and the number of speakers used. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»é€‚é…å™¨çš„è§’åº¦æ¢è®¨äº†è½»é‡çº§TTSç³»ç»Ÿä¸­çš„è·¨è¯­è¨€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†æœªè§è¯´è¯äººå’Œè¯­è¨€é€‚é…çš„ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯åˆæˆç›®æ ‡è¯­è¨€ä¸­çš„ç›®æ ‡è¯­éŸ³ï¼Œå…¶ä¸­ç›®æ ‡è¯­éŸ³æ²¡æœ‰ç›¸åº”çš„å½•éŸ³ã€‚å®¢è§‚è¯„ä¼°çš„ç»“æœè¯æ˜äº†é€‚é…å™¨åœ¨å­¦ä¹ ç‰¹å®šè¯­è¨€å’Œç‰¹å®šè¯´è¯äººä¿¡æ¯æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä½¿å¾—é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æœªçŸ¥çš„è¯´è¯äººæˆ–è¯­è¨€ï¼ŒåŒæ—¶é¿å…å¯¹åŸå§‹æ¨¡å‹çš„è¯´è¯äººæˆ–è¯­è¨€ä¿¡æ¯çš„ç¾éš¾æ€§é—å¿˜ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¡¡é‡ç”Ÿæˆè¯­éŸ³çš„å£éŸ³æœ‰å¤šåœ°é“ï¼Œæˆ‘ä»¬æå‡ºå¹¶éªŒè¯äº†ä¸€ç§å—ç¬¬äºŒè¯­è¨€å­¦ä¹ è€…å‘éŸ³é”™è¯¯æ£€æµ‹æŠ€æœ¯å¯å‘çš„å®¢è§‚æŒ‡æ ‡ã€‚æœ¬æ–‡è¿˜æä¾›äº†å…³äºé€‚é…å™¨ä½ç½®ã€é…ç½®ä»¥åŠä½¿ç”¨è¯´è¯äººæ•°é‡ç­‰æ–¹é¢çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18006v1">PDF</a> Accepted at IEEE MLSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºé€‚é…å™¨çš„è·¨è¯­è¨€æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆï¼Œåœ¨è½»é‡çº§TTSç³»ç»Ÿçš„èƒŒæ™¯ä¸‹ï¼Œå¯¹æ¯”äº†æœªè§è¯´è¯äººå’Œè¯­è¨€é€‚é…çš„ä»»åŠ¡ï¼Œæ—¨åœ¨åˆæˆç›®æ ‡è¯­è¨€çš„ç›®æ ‡å£°éŸ³ï¼Œå…¶ä¸­ç›®æ ‡å£°éŸ³æ²¡æœ‰ç›¸åº”çš„å½•éŸ³ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†é€‚é…å™¨åœ¨å­¦ä¹ è¯­è¨€ç‰¹æ€§å’Œè¯´è¯äººç‰¹æ€§ä¿¡æ¯æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿè®©é¢„è®­ç»ƒæ¨¡å‹å­¦ä¹ æœªçŸ¥çš„è¯´è¯äººæˆ–è¯­è¨€ï¼ŒåŒæ—¶é¿å…å¯¹åŸå§‹æ¨¡å‹çš„è¯´è¯äººæˆ–è¯­è¨€ä¿¡æ¯é—å¿˜ã€‚æ­¤å¤–ï¼Œä¸ºè¯„ä¼°ç”Ÿæˆå£°éŸ³çš„å£éŸ³æœ¬åœ°åŒ–ç¨‹åº¦ï¼Œæœ¬æ–‡å—äºŒè¯­å­¦ä¹ è€…å‘éŸ³é”™è¯¯æ£€æµ‹æŠ€æœ¯çš„å¯å‘ï¼Œæå‡ºå¹¶éªŒè¯äº†ä¸€é¡¹å®¢è§‚æŒ‡æ ‡ã€‚æœ¬æ–‡è¿˜æ·±å…¥æ¢è®¨äº†é€‚é…å™¨çš„ä½ç½®ã€é…ç½®ä»¥åŠä½¿ç”¨è¯´è¯äººçš„æ•°é‡æ‰€äº§ç”Ÿçš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€‚é…å™¨åœ¨è·¨è¯­è¨€TTSåˆæˆä¸­å±•ç°æœ‰æ•ˆæ€§ï¼Œèƒ½å­¦ä¹ è¯­è¨€ç‰¹æ€§å’Œè¯´è¯äººç‰¹æ€§ã€‚</li>
<li>é€‚é…å™¨èƒ½è®©é¢„è®­ç»ƒæ¨¡å‹é€‚åº”æœªè§è¿‡çš„è¯´è¯äººæˆ–è¯­è¨€ï¼ŒåŒæ—¶ä¿ç•™åŸæœ‰çŸ¥è¯†ã€‚</li>
<li>æå‡ºäº†ä¸€é¡¹åŸºäºäºŒè¯­å­¦ä¹ è€…å‘éŸ³é”™è¯¯æ£€æµ‹æŠ€æœ¯çš„å£éŸ³è¯„ä¼°å®¢è§‚æŒ‡æ ‡ã€‚</li>
<li>é€‚é…å™¨çš„ä½ç½®ã€é…ç½®ä»¥åŠä½¿ç”¨è¯´è¯äººçš„æ•°é‡å¯¹TTSæ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>é€‚é…å™¨æœ‰åŠ©äºé¿å…ç¾éš¾æ€§é—å¿˜ï¼Œå³åŸæœ‰æ¨¡å‹çš„è¯´è¯äººæˆ–è¯­è¨€ä¿¡æ¯ä¸ä¼šä¸¢å¤±ã€‚</li>
<li>åœ¨è·¨è¯­è¨€TTSåˆæˆä¸­ï¼Œé€‚é…å™¨çš„åº”ç”¨æœ‰åŠ©äºæé«˜è¯­éŸ³åˆæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹TTSç³»ç»Ÿçš„è¿›ä¸€æ­¥ä¼˜åŒ–å’Œæ”¹è¿›å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e1f8070ba57a9705736974e42d5807a1.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-41abcaaf44cd5b6288db938b78329f6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801598&auth_key=1759801598-0-0-85f01834c39018b8998e478afd4d5ef8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-1783e22de462ffdf33644c6afa6abe34.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b3f932c27f76e990028fb0fdba68b0a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801611&auth_key=1759801611-0-0-4e03f9ab0a1aa700491359d2a6c05dd2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Improving-French-Synthetic-Speech-Quality-via-SSML-Prosody-Control"><a href="#Improving-French-Synthetic-Speech-Quality-via-SSML-Prosody-Control" class="headerlink" title="Improving French Synthetic Speech Quality via SSML Prosody Control"></a>Improving French Synthetic Speech Quality via SSML Prosody Control</h2><p><strong>Authors:Nassima Ould Ouali, Awais Hussain Sani, Ruben Bueno, Jonah Dauvet, Tim Luka Horstmann, Eric Moulines</strong></p>
<p>Despite recent advances, synthetic voices often lack expressiveness due to limited prosody control in commercial text-to-speech (TTS) systems. We introduce the first end-to-end pipeline that inserts Speech Synthesis Markup Language (SSML) tags into French text to control pitch, speaking rate, volume, and pause duration. We employ a cascaded architecture with two QLoRA-fine-tuned Qwen 2.5-7B models: one predicts phrase-break positions and the other performs regression on prosodic targets, generating commercial TTS-compatible SSML markup. Evaluated on a 14-hour French podcast corpus, our method achieves 99.2% F1 for break placement and reduces mean absolute error on pitch, rate, and volume by 25-40% compared with prompting-only large language models (LLMs) and a BiLSTM baseline. In perceptual evaluation involving 18 participants across over 9 hours of synthesized audio, SSML-enhanced speech generated by our pipeline significantly improves naturalness, with the mean opinion score increasing from 3.20 to 3.87 (p &lt; 0.005). Additionally, 15 of 18 listeners preferred our enhanced synthesis. These results demonstrate substantial progress in bridging the expressiveness gap between synthetic and natural French speech. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/hi-paris/Prosody-Control-French-TTS">https://github.com/hi-paris/Prosody-Control-French-TTS</a>. </p>
<blockquote>
<p>å°½ç®¡è¿‘æœŸæœ‰æ‰€è¿›å±•ï¼Œä½†ç”±äºå•†ä¸šæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­éŸµå¾‹æ§åˆ¶æœ‰é™ï¼Œåˆæˆè¯­éŸ³å¾€å¾€ç¼ºä¹è¡¨ç°åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†é¦–ä¸ªç«¯åˆ°ç«¯çš„ç®¡é“ï¼Œé€šè¿‡å‘æ³•è¯­æ–‡æœ¬æ’å…¥è¯­éŸ³åˆæˆæ ‡è®°è¯­è¨€ï¼ˆSSMLï¼‰æ ‡ç­¾æ¥æ§åˆ¶éŸ³è°ƒã€è¯­é€Ÿã€éŸ³é‡å’Œæš‚åœæŒç»­æ—¶é—´ã€‚æˆ‘ä»¬é‡‡ç”¨çº§è”æ¶æ„ï¼Œä½¿ç”¨ä¸¤ä¸ªå¾®è°ƒè¿‡çš„QLoRA Qwen 2.5-7Bæ¨¡å‹ï¼šä¸€ä¸ªç”¨äºé¢„æµ‹çŸ­è¯­æ–­ç‚¹ä½ç½®ï¼Œå¦ä¸€ä¸ªåˆ™å¯¹éŸµå¾‹ç›®æ ‡è¿›è¡Œå›å½’ï¼Œç”Ÿæˆä¸å•†ä¸šTTSå…¼å®¹çš„SSMLæ ‡è®°ã€‚åœ¨14å°æ—¶çš„æ³•è¯­æ’­å®¢è¯­æ–™åº“ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–­ç‚¹æ”¾ç½®æ–¹é¢è¾¾åˆ°äº†99.2%çš„F1åˆ†æ•°ï¼Œä¸ä»…ä½¿ç”¨æç¤ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒBiLSTMåŸºçº¿ç›¸æ¯”ï¼Œåœ¨éŸ³è°ƒã€é€Ÿåº¦å’ŒéŸ³é‡æ–¹é¢çš„å¹³å‡ç»å¯¹è¯¯å·®é™ä½äº†25-40%ã€‚åœ¨æ¶‰åŠ18åå‚ä¸è€…ã€è¶…è¿‡9å°æ—¶åˆæˆéŸ³é¢‘çš„æ„ŸçŸ¥è¯„ä¼°ä¸­ï¼Œé€šè¿‡æˆ‘ä»¬çš„ç®¡é“ç”Ÿæˆçš„SSMLå¢å¼ºè¯­éŸ³çš„è‡ªç„¶åº¦æ˜¾è‘—æé«˜ï¼Œå¹³å‡æ„è§åˆ†æ•°ä»3.20æé«˜åˆ°3.87ï¼ˆp &lt; 0.005ï¼‰ã€‚æ­¤å¤–ï¼Œ18åå¬ä¼—ä¸­æœ‰15äººæ›´å–œæ¬¢æˆ‘ä»¬å¢å¼ºçš„åˆæˆè¯­éŸ³ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåœ¨ç¼©å°åˆæˆæ³•è¯­å’Œè‡ªç„¶æ³•è¯­ä¹‹é—´è¡¨ç°åŠ›å·®è·æ–¹é¢å–å¾—äº†å®è´¨æ€§è¿›å±•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hi-paris/Prosody-Control-French-TTS%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/hi-paris/Prosody-Control-French-TTSä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17494v1">PDF</a> 13 pages, 9 figures, 6 tables. Accepted for presentation at ICNLSP   2025 (Odense, Denmark). Code and demo:   <a target="_blank" rel="noopener" href="https://github.com/hi-paris/Prosody-Control-French-TTS">https://github.com/hi-paris/Prosody-Control-French-TTS</a>. ACM Class: I.2.7;   H.5.5</p>
<p><strong>æ‘˜è¦</strong><br>æ–‡æœ¬å¼•å…¥äº†ä¸€ç§ç«¯åˆ°ç«¯çš„ç®¡é“ï¼Œé€šè¿‡æ’å…¥è¯­éŸ³åˆæˆæ ‡è®°è¯­è¨€ï¼ˆSSMLï¼‰æ ‡ç­¾æ¥æ§åˆ¶æ³•è¯­æ–‡æœ¬çš„éŸ³è°ƒã€è¯­é€Ÿã€éŸ³é‡å’Œåœé¡¿æ—¶é—´ï¼Œä»è€Œå¼¥è¡¥äº†å•†ä¸šæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­è¡¨è¾¾èƒ½åŠ›çš„æœ‰é™æ€§ã€‚é‡‡ç”¨çº§è”æ¶æ„ï¼Œé€šè¿‡ä¸¤ä¸ªå¾®è°ƒè¿‡çš„Qwen 2.5-7Bæ¨¡å‹ï¼Œä¸€ä¸ªé¢„æµ‹è¯­å¥æ–­ç‚¹ä½ç½®ï¼Œå¦ä¸€ä¸ªå¯¹è¯­è°ƒç›®æ ‡è¿›è¡Œå›å½’ï¼Œç”Ÿæˆä¸å•†ä¸šTTSå…¼å®¹çš„SSMLæ ‡è®°ã€‚åœ¨14å°æ—¶æ³•è¯­æ’­å®¢è¯­æ–™åº“ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯¥æ–¹æ³•åœ¨æ–­ç‚¹æ”¾ç½®æ–¹é¢è¾¾åˆ°99.2%çš„F1åˆ†æ•°ï¼Œä¸ä»…ä½¿ç”¨æç¤ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒBiLSTMåŸºçº¿ç›¸æ¯”ï¼Œåœ¨éŸ³è°ƒã€è¯­é€Ÿå’ŒéŸ³é‡æ–¹é¢çš„å¹³å‡ç»å¯¹è¯¯å·®é™ä½äº†25-40%ã€‚åœ¨æ¶‰åŠ18åå‚ä¸è€…ã€è¶…è¿‡9å°æ—¶åˆæˆéŸ³é¢‘çš„æ„ŸçŸ¥è¯„ä¼°ä¸­ï¼Œé€šè¿‡æˆ‘ä»¬çš„ç®¡é“ç”Ÿæˆçš„SSMLå¢å¼ºè¯­éŸ³çš„è‡ªç„¶åº¦æ˜¾è‘—æé«˜ï¼Œå¹³å‡æ„è§åˆ†æ•°ä»3.20æé«˜åˆ°3.87ï¼ˆp&lt;0.005ï¼‰ã€‚æ­¤å¤–ï¼Œæœ‰15åå¬ä¼—æ›´å–œæ¬¢æˆ‘ä»¬å¢å¼ºçš„åˆæˆæ•ˆæœã€‚è¯¥ç ”ç©¶æ˜¾è‘—ç¼©å°äº†åˆæˆæ³•è¯­å’Œè‡ªç„¶æ³•è¯­åœ¨è¡¨è¾¾åŠ›æ–¹é¢çš„å·®è·ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€æä¾›ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>å•†ä¸šTTSç³»ç»Ÿå­˜åœ¨è¡¨è¾¾èƒ½åŠ›çš„å±€é™æ€§ï¼Œç¼ºä¹è¯­è°ƒæ§åˆ¶ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯ç®¡é“ï¼Œä½¿ç”¨SSMLæ ‡ç­¾å¢å¼ºæ³•è¯­çš„è¯­è°ƒæ§åˆ¶ã€‚</li>
<li>é‡‡ç”¨çº§è”æ¶æ„å’ŒQLoRAå¾®è°ƒæ¨¡å‹ï¼Œå®ç°ç²¾å‡†åœ°é¢„æµ‹è¯­å¥æ–­ç‚¹ä½ç½®å’Œè¯­è°ƒç›®æ ‡å›å½’ã€‚</li>
<li>åœ¨æ³•è¯­æ’­å®¢è¯­æ–™åº“ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ–­ç‚¹æ”¾ç½®ã€è¯­è°ƒæ§åˆ¶æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨éŸ³è°ƒã€è¯­é€Ÿå’ŒéŸ³é‡æ–¹é¢æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
<li>æ„ŸçŸ¥è¯„ä¼°æ˜¾ç¤ºï¼Œé€šè¿‡è¯¥ç®¡é“ç”Ÿæˆçš„è¯­éŸ³è‡ªç„¶åº¦æ˜¾è‘—æé«˜ï¼Œè·å¾—å¤§éƒ¨åˆ†å¬ä¼—çš„åå¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8fb886d7d88389275ec54e6b52b59d8.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-8bfc85cfc5333b2d2141a44bc66b2690~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801626&auth_key=1759801626-0-0-c193dba9db126b7181083f029480ec7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-2760eed66c4a360f3960abe8a2e52d28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eac20b658b86e9ca5b9a76536333b5c.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-e3a802d60c584405d39f9e4e841ba09b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801645&auth_key=1759801645-0-0-92761ea2fe7558bd88062f7b961f9da1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RephraseTTS-Dynamic-Length-Text-based-Speech-Insertion-with-Speaker-Style-Transfer"><a href="#RephraseTTS-Dynamic-Length-Text-based-Speech-Insertion-with-Speaker-Style-Transfer" class="headerlink" title="RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker   Style Transfer"></a>RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker   Style Transfer</h2><p><strong>Authors:Neeraj Matiyali, Siddharth Srivastava, Gaurav Sharma</strong></p>
<p>We propose a method for the task of text-conditioned speech insertion, i.e. inserting a speech sample in an input speech sample, conditioned on the corresponding complete text transcript. An example use case of the task would be to update the speech audio when corrections are done on the corresponding text transcript. The proposed method follows a transformer-based non-autoregressive approach that allows speech insertions of variable lengths, which are dynamically determined during inference, based on the text transcript and tempo of the available partial input. It is capable of maintaining the speakerâ€™s voice characteristics, prosody and other spectral properties of the available speech input. Results from our experiments and user study on LibriTTS show that our method outperforms baselines based on an existing adaptive text to speech method. We also provide numerous qualitative results to appreciate the quality of the output from the proposed method. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬æ¡ä»¶è¯­éŸ³æ’å…¥ä»»åŠ¡çš„æ–¹æ³•ï¼Œå³åœ¨è¾“å…¥è¯­éŸ³æ ·æœ¬ä¸­æ’å…¥ä¸€ä¸ªè¯­éŸ³æ ·æœ¬ï¼Œè¯¥æ’å…¥å—ç›¸åº”å®Œæ•´æ–‡æœ¬è½¬å½•çš„åˆ¶çº¦ã€‚è¯¥ä»»åŠ¡çš„ä¸€ä¸ªç¤ºä¾‹ç”¨ä¾‹æ˜¯åœ¨ç›¸åº”æ–‡æœ¬è½¬å½•è¿›è¡Œæ›´æ­£æ—¶æ›´æ–°è¯­éŸ³éŸ³é¢‘ã€‚æ‰€æå‡ºçš„æ–¹æ³•é‡‡ç”¨åŸºäºè½¬æ¢å™¨çš„éè‡ªå›å½’æ–¹æ³•ï¼Œå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ ¹æ®æ–‡æœ¬è½¬å½•å’Œç°æœ‰éƒ¨åˆ†è¾“å…¥çš„è¯­é€ŸåŠ¨æ€ç¡®å®šå¯å˜é•¿åº¦çš„è¯­éŸ³æ’å…¥ã€‚å®ƒèƒ½å¤Ÿä¿æŒè¯´è¯äººçš„å£°éŸ³ç‰¹å¾ã€è¯­è°ƒå’Œå…¶ä»–ç°æœ‰è¯­éŸ³è¾“å…¥çš„é¢‘è°±ç‰¹æ€§ã€‚åœ¨LibriTTSä¸Šè¿›è¡Œçš„å®éªŒå’Œç”¨æˆ·ç ”ç©¶çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºäºç°æœ‰è‡ªé€‚åº”æ–‡æœ¬åˆ°è¯­éŸ³æ–¹æ³•çš„åŸºçº¿ã€‚æˆ‘ä»¬è¿˜æä¾›äº†è®¸å¤šå®šæ€§ç»“æœï¼Œä»¥è¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„è¾“å‡ºè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17031v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>æå‡ºä¸€ç§ç”¨äºæ–‡æœ¬æ¡ä»¶è¯­éŸ³æ’å…¥ä»»åŠ¡çš„æ–¹æ³•ï¼Œå³æ ¹æ®ç›¸åº”çš„å®Œæ•´æ–‡æœ¬è½¬å½•åœ¨è¾“å…¥è¯­éŸ³æ ·æœ¬ä¸­æ’å…¥è¯­éŸ³æ ·æœ¬ã€‚è¯¥æ–¹æ³•çš„ç”¨ä¾‹æ˜¯å½“å¯¹ç›¸åº”çš„æ–‡æœ¬è½¬å½•è¿›è¡Œæ›´æ­£æ—¶ï¼Œæ›´æ–°è¯­éŸ³éŸ³é¢‘ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºè½¬æ¢å™¨çš„éè‡ªå›å½’æ–¹æ³•ï¼Œå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ ¹æ®æ–‡æœ¬è½¬å½•å’Œç°æœ‰éƒ¨åˆ†è¾“å…¥çš„è¯­é€ŸåŠ¨æ€ç¡®å®šå¯å˜é•¿åº¦çš„è¯­éŸ³æ’å…¥ã€‚å®ƒèƒ½å¤Ÿä¿æŒè¯´è¯äººçš„è¯­éŸ³ç‰¹å¾ã€è¯­è°ƒå’Œå…¶ä»–å…‰è°±å±æ€§ã€‚åœ¨LibriTTSä¸Šçš„å®éªŒå’Œç”¨æˆ·ç ”ç©¶ç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºåŸºäºç°æœ‰è‡ªé€‚åº”æ–‡æœ¬åˆ°è¯­éŸ³æ–¹æ³•çš„åŸºçº¿ã€‚è¿˜æä¾›è®¸å¤šå®šæ€§ç»“æœæ¥å±•ç¤ºæ‰€æå‡ºæ–¹æ³•çš„è¾“å‡ºè´¨é‡ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æå‡ºäº†æ–‡æœ¬æ¡ä»¶è¯­éŸ³æ’å…¥çš„æ–°æ–¹æ³•ï¼Œé€‚ç”¨äºæ’å…¥ä¸æ–‡æœ¬è½¬å½•ç›¸å¯¹åº”çš„è¯­éŸ³æ ·æœ¬ã€‚</li>
<li>æ–¹æ³•é‡‡ç”¨åŸºäºè½¬æ¢å™¨çš„éè‡ªå›å½’æ–¹å¼ï¼Œå…è®¸åŠ¨æ€ç¡®å®šè¯­éŸ³æ’å…¥çš„é•¿åº¦ã€‚</li>
<li>æ–¹æ³•èƒ½åŸºäºç°æœ‰éƒ¨åˆ†è¾“å…¥å’Œæ–‡æœ¬è½¬å½•ä¿æŒè¯­éŸ³çš„è¯´è¯äººç‰¹å¾ã€è¯­è°ƒå’Œå…¶ä»–å…‰è°±å±æ€§ã€‚</li>
<li>åœ¨LibriTTSæ•°æ®é›†ä¸Šçš„å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¯æ˜è¯¥æ–¹æ³•æ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æä¾›çš„å®šæ€§ç»“æœå±•ç¤ºäº†è¯¥æ–¹æ³•çš„è‰¯å¥½è¾“å‡ºè´¨é‡ã€‚</li>
<li>è¿™ç§æŠ€æœ¯å¯ä»¥åº”ç”¨äºè¯­éŸ³ç¼–è¾‘å’Œè¯­éŸ³æ›¿æ¢ç­‰åœºæ™¯ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-13c574f3d2b342b2c093c54f0e65e261~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801654&auth_key=1759801654-0-0-4b0639944abb82cec38d0fdc94c3bef7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2f396ca32597db98a6187c5850856f99~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801661&auth_key=1759801661-0-0-91cb4f4e4ecba5efaf5625b743b38496&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03c9bf4723a66bf83c0d700e26221be7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801668&auth_key=1759801668-0-0-b30a175bf2688c4e3fab450eb3e5afca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Trust-but-Verify-A-Survey-on-Verification-Design-for-Test-time-Scaling"><a href="#Trust-but-Verify-A-Survey-on-Verification-Design-for-Test-time-Scaling" class="headerlink" title="Trust but Verify! A Survey on Verification Design for Test-time Scaling"></a>Trust but Verify! A Survey on Verification Design for Test-time Scaling</h2><p><strong>Authors:V Venktesh, Mandeep Rathee, Avishek Anand</strong></p>
<p>Test-time scaling (TTS) has emerged as a new frontier for scaling the performance of Large Language Models. In test-time scaling, by using more computational resources during inference, LLMs can improve their reasoning process and task performance. Several approaches have emerged for TTS such as distilling reasoning traces from another model or exploring the vast decoding search space by employing a verifier. The verifiers serve as reward models that help score the candidate outputs from the decoding process to diligently explore the vast solution space and select the best outcome. This paradigm commonly termed has emerged as a superior approach owing to parameter free scaling at inference time and high performance gains. The verifiers could be prompt-based, fine-tuned as a discriminative or generative model to verify process paths, outcomes or both. Despite their widespread adoption, there is no detailed collection, clear categorization and discussion of diverse verification approaches and their training mechanisms. In this survey, we cover the diverse approaches in the literature and present a unified view of verifier training, types and their utility in test-time scaling. Our repository can be found at <a target="_blank" rel="noopener" href="https://github.com/elixir-research-group/Verifierstesttimescaling.github.io">https://github.com/elixir-research-group/Verifierstesttimescaling.github.io</a>. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°å‰æ²¿ã€‚åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ä¸­ï¼Œé€šè¿‡æ¨ç†è¿‡ç¨‹ä¸­çš„æ›´å¤šè®¡ç®—èµ„æºçš„ä½¿ç”¨ï¼ŒLLMå¯ä»¥æ”¹å–„å…¶æ¨ç†è¿‡ç¨‹å’Œä»»åŠ¡æ€§èƒ½ã€‚å¤šç§TTSæ–¹æ³•å·²ç»å‡ºç°ï¼Œä¾‹å¦‚ä»å¦ä¸€ä¸ªæ¨¡å‹ä¸­æå–æ¨ç†ç—•è¿¹ï¼Œæˆ–è€…é€šè¿‡é‡‡ç”¨éªŒè¯å™¨æ¥æ¢ç´¢åºå¤§çš„è§£ç æœç´¢ç©ºé—´ã€‚éªŒè¯å™¨ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œå¸®åŠ©å¯¹è§£ç è¿‡ç¨‹ä¸­çš„å€™é€‰è¾“å‡ºè¿›è¡Œè¯„åˆ†ï¼Œä»è€Œä»”ç»†æ¢ç´¢å·¨å¤§çš„è§£å†³æ–¹æ¡ˆç©ºé—´å¹¶é€‰æ‹©æœ€ä½³ç»“æœã€‚è¿™ä¸€èŒƒå¼é€šå¸¸è¢«è®¤ä¸ºæ˜¯ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå› ä¸ºå®ƒå…·æœ‰æ¨ç†æ—¶é—´å‚æ•°è‡ªç”±ç¼©æ”¾å’Œæ€§èƒ½å¢ç›Šé«˜çš„ä¼˜ç‚¹ã€‚éªŒè¯å™¨å¯ä»¥æ˜¯åŸºäºæç¤ºçš„ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºåˆ¤åˆ«æ¨¡å‹æˆ–ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥éªŒè¯è¿‡ç¨‹è·¯å¾„ã€ç»“æœæˆ–ä¸¤è€…ã€‚å°½ç®¡å®ƒä»¬å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œä½†å…³äºå„ç§éªŒè¯æ–¹æ³•å’Œå…¶è®­ç»ƒæœºåˆ¶çš„è¯¦ç»†æ”¶é›†ã€æ˜ç¡®åˆ†ç±»å’Œè®¨è®ºä»ç„¶ç¼ºä¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¶µç›–äº†æ–‡çŒ®ä¸­çš„ä¸åŒæ–¹æ³•ï¼Œå¹¶å¯¹éªŒè¯å™¨çš„è®­ç»ƒã€ç±»å‹åŠå…¶åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ä¸­çš„å®ç”¨æ€§è¿›è¡Œäº†ç»Ÿä¸€çš„é˜è¿°ã€‚æˆ‘ä»¬çš„ä»“åº“å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/elixir-research-group/Verifierstesttimescaling.github.io">https://github.com/elixir-research-group/Verifierstesttimescaling.github.io</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16665v2">PDF</a> 18 pages</p>
<p><strong>Summary</strong></p>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰å·²æˆä¸ºæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°å‰æ²¿ã€‚åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ä¸­ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨æ›´å¤šçš„è®¡ç®—èµ„æºï¼ŒLLMå¯ä»¥æ”¹å–„å…¶æ¨ç†è¿‡ç¨‹å¹¶æå‡ä»»åŠ¡æ€§èƒ½ã€‚æœ¬æ–‡ä»‹ç»äº†å¤šç§TTSæ–¹æ³•ï¼Œå¦‚é€šè¿‡å¦ä¸€ç§æ¨¡å‹æç‚¼æ¨ç†ç—•è¿¹å’Œåˆ©ç”¨éªŒè¯å™¨æ‰©å±•åºå¤§çš„è§£ç æœç´¢ç©ºé—´ç­‰ã€‚éªŒè¯å™¨ä½œä¸ºä¸€ç§å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è§£ç è¿‡ç¨‹ä¸­è¯„ä¼°å€™é€‰è¾“å‡ºï¼Œè®¤çœŸæ¢ç´¢åºå¤§çš„è§£ç©ºé—´å¹¶é€‰æ‹©æœ€ä½³ç»“æœã€‚å°½ç®¡éªŒè¯å™¨å·²å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†å…³äºå„ç§éªŒè¯æ–¹æ³•å’Œå…¶è®­ç»ƒæœºåˆ¶çš„è¯¦ç»†æ”¶é›†å’Œåˆ†ç±»è®¨è®ºä»ä¸è¶³ã€‚æœ¬æ–‡ç»¼è¿°äº†æ–‡çŒ®ä¸­çš„ä¸åŒæ–¹æ³•ï¼Œå¹¶å¯¹éªŒè¯å™¨çš„è®­ç»ƒã€ç±»å‹åŠå…¶åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ä¸­çš„åº”ç”¨æä¾›äº†ç»Ÿä¸€è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ˜¯æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°é¢†åŸŸã€‚</li>
<li>é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨æ›´å¤šè®¡ç®—èµ„æºï¼ŒLLMå¯ä»¥æé«˜å…¶æ¨ç†èƒ½åŠ›å’Œä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>TTSçš„æ–¹æ³•åŒ…æ‹¬é€šè¿‡å¦ä¸€ç§æ¨¡å‹æç‚¼æ¨ç†ç—•è¿¹å’Œåˆ©ç”¨éªŒè¯å™¨æ‰©å±•è§£ç æœç´¢ç©ºé—´ã€‚</li>
<li>éªŒè¯å™¨ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿè¯„ä¼°å€™é€‰è¾“å‡ºå¹¶é€‰æ‹©æœ€ä½³ç»“æœã€‚</li>
<li>éªŒè¯å™¨å¯ä»¥åŸºäºæç¤ºè¿›è¡Œè®­ç»ƒï¼Œä¹Ÿå¯ä»¥ä½œä¸ºåˆ¤åˆ«æˆ–ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥éªŒè¯è¿‡ç¨‹è·¯å¾„ã€ç»“æœæˆ–ä¸¤è€…ã€‚</li>
<li>å½“å‰å¯¹äºéªŒè¯å™¨å’Œå…¶è®­ç»ƒæœºåˆ¶çš„è¯¦ç»†æ”¶é›†å’Œåˆ†ç±»è®¨è®ºä»ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e8b2d50378a90b75a091015f59539db3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801675&auth_key=1759801675-0-0-5721530e0f7ea64e6eced81e185521ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5187a4ed3d737ae82c446b892b0afc6d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801683&auth_key=1759801683-0-0-97f17761c82a553d98ccd2716400a49b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-d538edfadd23fbda27cfe7dab2e262dc.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-643b3583482a1b46ee75ce4c843fbdc4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801696&auth_key=1759801696-0-0-f6c92cdee86ac96f0fb93c1666787033&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech"><a href="#Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech" class="headerlink" title="Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech"></a>Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech</h2><p><strong>Authors:Jingyuan Xing, Zhipeng Li, Jialong Mai, Xiaofen Xing, Xiangmin Xu</strong></p>
<p>Advances in speech representation and large language models have enhanced zero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS models face challenges in capturing the complex correlations between acoustic and semantic features, resulting in a lack of expressiveness and similarity. The primary reason lies in the complex relationship between semantic and acoustic features, which manifests independent and interdependent aspects.This paper introduces a TTS framework that combines both autoregressive (AR) and non-autoregressive (NAR) modules to harmonize the independence and interdependence of acoustic and semantic information. The AR model leverages the proposed Parallel Tokenizer to synthesize the top semantic and acoustic tokens simultaneously. In contrast, considering the interdependence, the Coupled NAR model predicts detailed tokens based on the general AR modelâ€™s output. Parallel GPT, built on this architecture, is designed to improve zero-shot text-to-speech synthesis through its parallel structure. Experiments on English and Chinese datasets demonstrate that the proposed model significantly outperforms the quality and efficiency of the synthesis of existing zero-shot TTS models. Speech demos are available at <a target="_blank" rel="noopener" href="https://t1235-ch.github.io/pgpt/">https://t1235-ch.github.io/pgpt/</a>. </p>
<blockquote>
<p>éšç€è¯­éŸ³è¡¨ç¤ºå’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›¶æ ·æœ¬TTSæ¨¡å‹åœ¨æ•æ‰å£°éŸ³å’Œè¯­ä¹‰ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³è”æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´è¡¨ç°åŠ›ä¸è¶³å’Œç›¸ä¼¼æ€§ä¸è¶³ã€‚ä¸»è¦åŸå› åœ¨äºè¯­ä¹‰å’Œå£°éŸ³ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œè¡¨ç°ä¸ºç‹¬ç«‹å’Œç›¸äº’ä¾å­˜çš„æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04141v2">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing (TASLP)</p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºå…ˆè¿›çš„è¯­éŸ³è¡¨ç¤ºå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰é›¶æ ·æœ¬TTSæ¨¡å‹åœ¨æ•æ‰å£°å­¦ç‰¹å¾ä¸è¯­ä¹‰ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³è”æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´è¡¨ç°åŠ›ä¸è¶³å’Œç›¸ä¼¼æ€§ä¸è¶³ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç»“åˆè‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å—çš„TTSæ¡†æ¶ï¼Œä»¥åè°ƒå£°å­¦ç‰¹å¾å’Œè¯­ä¹‰ä¿¡æ¯çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä¾èµ–æ€§ã€‚ARæ¨¡å‹åˆ©ç”¨æå‡ºçš„å¹¶è¡Œåˆ†è¯å™¨åŒæ—¶åˆæˆé¡¶çº§è¯­ä¹‰å’Œå£°éŸ³æ ‡è®°ã€‚è€ƒè™‘åˆ°ç›¸äº’ä¾èµ–æ€§ï¼Œè€¦åˆçš„NARæ¨¡å‹åŸºäºARæ¨¡å‹çš„è¾“å‡ºé¢„æµ‹è¯¦ç»†çš„æ ‡è®°ã€‚åŸºäºè¿™ç§æ¶æ„æ„å»ºçš„å¹¶è¡ŒGPTæ—¨åœ¨é€šè¿‡å…¶å¹¶è¡Œç»“æ„æé«˜é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çš„åˆæˆè´¨é‡ã€‚åœ¨è‹±è¯­å’Œä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åˆæˆè´¨é‡å’Œæ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚è¯­éŸ³æ¼”ç¤ºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://t1235-ch.github.io/pgpt/">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å…ˆè¿›çš„è¯­éŸ³è¡¨ç¤ºå’Œå¤§å‹è¯­è¨€æ¨¡å‹æå‡äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰TTSæ¨¡å‹åœ¨æ•æ‰å£°å­¦ç‰¹å¾ä¸è¯­ä¹‰ç‰¹å¾çš„å¤æ‚å…³è”æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„TTSæ¡†æ¶ç»“åˆARå’ŒNARæ¨¡å—ï¼Œä»¥åè°ƒå£°å­¦è¯­ä¹‰çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä¾èµ–æ€§ã€‚</li>
<li>ARæ¨¡å‹åˆ©ç”¨å¹¶è¡Œåˆ†è¯å™¨åŒæ—¶åˆæˆè¯­ä¹‰å’Œå£°éŸ³æ ‡è®°ã€‚</li>
<li>NARæ¨¡å‹åŸºäºARæ¨¡å‹çš„è¾“å‡ºè¿›è¡Œè¯¦ç»†çš„é¢„æµ‹ã€‚</li>
<li>æ„å»ºçš„å¹¶è¡ŒGPTæ¶æ„æé«˜äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çš„åˆæˆè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-92a2dee01b41509d20bdbac5197239e9.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa37142f69f7e05c7ee1f1ccc92eb754~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801711&auth_key=1759801711-0-0-ba5e081a2a512942bdf5a7f6326ffd34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a92c8ad1224a54c6c9f7d2d2a40d2c8b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801717&auth_key=1759801717-0-0-2bdfec2388bfb22088872dbf92f0ea3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-0d0677a13b72211796ef8e9ac8a1254d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="IndexTTS2-A-Breakthrough-in-Emotionally-Expressive-and-Duration-Controlled-Auto-Regressive-Zero-Shot-Text-to-Speech"><a href="#IndexTTS2-A-Breakthrough-in-Emotionally-Expressive-and-Duration-Controlled-Auto-Regressive-Zero-Shot-Text-to-Speech" class="headerlink" title="IndexTTS2: A Breakthrough in Emotionally Expressive and   Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech"></a>IndexTTS2: A Breakthrough in Emotionally Expressive and   Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</h2><p><strong>Authors:Siyi Zhou, Yiquan Zhou, Yi He, Xun Zhou, Jinchao Wang, Wei Deng, Jingchen Shu</strong></p>
<p>Existing autoregressive large-scale text-to-speech (TTS) models have advantages in speech naturalness, but their token-by-token generation mechanism makes it difficult to precisely control the duration of synthesized speech. This becomes a significant limitation in applications requiring strict audio-visual synchronization, such as video dubbing. This paper introduces IndexTTS2, which proposes a novel, general, and autoregressive model-friendly method for speech duration control. The method supports two generation modes: one explicitly specifies the number of generated tokens to precisely control speech duration; the other freely generates speech in an autoregressive manner without specifying the number of tokens, while faithfully reproducing the prosodic features of the input prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional expression and speaker identity, enabling independent control over timbre and emotion. In the zero-shot setting, the model can accurately reconstruct the target timbre (from the timbre prompt) while perfectly reproducing the specified emotional tone (from the style prompt). To enhance speech clarity in highly emotional expressions, we incorporate GPT latent representations and design a novel three-stage training paradigm to improve the stability of the generated speech. Additionally, to lower the barrier for emotional control, we designed a soft instruction mechanism based on text descriptions by fine-tuning Qwen3, effectively guiding the generation of speech with the desired emotional orientation. Finally, experimental results on multiple datasets show that IndexTTS2 outperforms state-of-the-art zero-shot TTS models in terms of word error rate, speaker similarity, and emotional fidelity. Audio samples are available at: <a target="_blank" rel="noopener" href="https://index-tts.github.io/index-tts2.github.io/">https://index-tts.github.io/index-tts2.github.io/</a> </p>
<blockquote>
<p>ç°æœ‰çš„å¤§å‹è‡ªå›å½’æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹åœ¨è¯­éŸ³è‡ªç„¶æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å…¶é€ä¸ªæ ‡è®°çš„ç”Ÿæˆæœºåˆ¶ä½¿å¾—éš¾ä»¥ç²¾ç¡®æ§åˆ¶åˆæˆè¯­éŸ³çš„æŒç»­æ—¶é—´ã€‚è¿™åœ¨éœ€è¦ä¸¥æ ¼éŸ³è§†é¢‘åŒæ­¥çš„åº”ç”¨ä¸­æˆä¸ºä¸€ä¸ªé‡è¦é™åˆ¶ï¼Œä¾‹å¦‚åœ¨è§†é¢‘é…éŸ³ä¸­ã€‚æœ¬æ–‡ä»‹ç»äº†IndexTTS2ï¼Œå®ƒæå‡ºäº†ä¸€ç§æ–°é¢–ã€é€šç”¨ã€é€‚ç”¨äºè‡ªå›å½’æ¨¡å‹çš„è¯­éŸ³æŒç»­æ—¶é—´æ§åˆ¶æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼šä¸€ç§æ˜ç¡®æŒ‡å®šç”Ÿæˆçš„æ ‡è®°æ•°é‡ä»¥ç²¾ç¡®æ§åˆ¶è¯­éŸ³æŒç»­æ—¶é—´ï¼›å¦ä¸€ç§ä»¥è‡ªå›å½’çš„æ–¹å¼è‡ªç”±ç”Ÿæˆè¯­éŸ³ï¼Œæ— éœ€æŒ‡å®šæ ‡è®°æ•°é‡ï¼ŒåŒæ—¶å¿ å®å†ç°è¾“å…¥æç¤ºçš„éŸµå¾‹ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒIndexTTS2å®ç°äº†æƒ…æ„Ÿè¡¨è¾¾å’Œè¯´è¯äººèº«ä»½çš„è§£è€¦ï¼Œå®ç°äº†å¯¹éŸ³è‰²å’Œæƒ…æ„Ÿçš„ç‹¬ç«‹æ§åˆ¶ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼Œè¯¥æ¨¡å‹å¯ä»¥å‡†ç¡®é‡å»ºç›®æ ‡éŸ³è‰²ï¼ˆæ¥è‡ªéŸ³è‰²æç¤ºï¼‰ï¼ŒåŒæ—¶å®Œç¾å†ç°æŒ‡å®šçš„æƒ…æ„ŸåŸºè°ƒï¼ˆæ¥è‡ªé£æ ¼æç¤ºï¼‰ã€‚ä¸ºäº†æé«˜é«˜åº¦æƒ…æ„Ÿè¡¨è¾¾ä¸­çš„è¯­éŸ³æ¸…æ™°åº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†GPTæ½œåœ¨è¡¨ç¤ºï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°å‹çš„ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œä»¥æé«˜ç”Ÿæˆè¯­éŸ³çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†é™ä½æƒ…æ„Ÿæ§åˆ¶çš„éšœç¢ï¼Œæˆ‘ä»¬åŸºäºæ–‡æœ¬æè¿°è®¾è®¡äº†ä¸€ç§æŸ”å’Œçš„æŒ‡ä»¤æœºåˆ¶ï¼Œé€šè¿‡å¾®è°ƒQwen3ï¼Œæœ‰æ•ˆåœ°å¼•å¯¼ç”Ÿæˆå…·æœ‰æ‰€éœ€æƒ…æ„Ÿå€¾å‘çš„è¯­éŸ³ã€‚æœ€åï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒIndexTTS2åœ¨è¯é”™è¯¯ç‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿä¿çœŸåº¦æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://index-tts.github.io/index-tts2.github.io/%E3%80%82">https://index-tts.github.io/index-tts2.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21619v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºIndexTTS2çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰è‡ªå›å½’å¤§è§„æ¨¡TTSæ¨¡å‹åœ¨è¯­éŸ³åˆæˆæ—¶é•¿æ§åˆ¶ä¸Šçš„å›°éš¾ã€‚IndexTTS2æå‡ºäº†ä¸€ç§æ–°é¢–ã€é€šç”¨ä¸”ä¸è‡ªå›å½’æ¨¡å‹å…¼å®¹çš„è¯­éŸ³æ—¶é•¿æ§åˆ¶æ–¹æ³•ï¼Œæ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼šä¸€ç§æ˜¯é€šè¿‡æ˜ç¡®æŒ‡å®šç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡æ¥æ§åˆ¶è¯­éŸ³æ—¶é•¿ï¼›å¦ä¸€ç§æ˜¯ä»¥è‡ªå›å½’æ–¹å¼è‡ªç”±ç”Ÿæˆè¯­éŸ³ï¼ŒåŒæ—¶å¿ å®ä¿ç•™è¾“å…¥æç¤ºçš„éŸµå¾‹ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒIndexTTS2å®ç°äº†æƒ…æ„Ÿè¡¨è¾¾å’Œè¯´è¯äººèº«ä»½çš„è§£è€¦ï¼Œèƒ½å¤Ÿç‹¬ç«‹æ§åˆ¶éŸ³è‰²å’Œæƒ…æ„Ÿã€‚é€šè¿‡ç»“åˆGPTæ½œåœ¨è¡¨å¾å¹¶è®¾è®¡æ–°é¢–çš„ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œæé«˜äº†ç”Ÿæˆè¯­éŸ³çš„æ¸…æ™°åº¦å’Œç¨³å®šæ€§ã€‚åŒæ—¶ï¼Œä¸ºé™ä½æƒ…æ„Ÿæ§åˆ¶çš„éšœç¢ï¼Œé€šè¿‡å¾®è°ƒQwen3å¹¶è®¾è®¡åŸºäºæ–‡æœ¬æè¿°çš„è½¯æŒ‡ä»¤æœºåˆ¶ï¼Œæœ‰æ•ˆå¼•å¯¼ç”Ÿæˆå…·æœ‰æ‰€éœ€æƒ…æ„Ÿå€¾å‘çš„è¯­éŸ³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIndexTTS2åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼Œç›¸è¾ƒäºå…ˆè¿›TTSæ¨¡å‹ï¼Œå…¶åœ¨å•è¯é”™è¯¯ç‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿä¿çœŸåº¦æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>IndexTTS2æ¨¡å‹è§£å†³äº†è‡ªå›å½’å¤§è§„æ¨¡æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹åœ¨è¯­éŸ³æ—¶é•¿æ§åˆ¶ä¸Šçš„éš¾é¢˜ã€‚</li>
<li>æ¨¡å‹æ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼Œä¸€ç§å¯ç²¾ç¡®æ§åˆ¶è¯­éŸ³æ—¶é•¿ï¼Œå¦ä¸€ç§å¯è‡ªå›å½’æ–¹å¼ç”Ÿæˆè¯­éŸ³å¹¶ä¿ç•™è¾“å…¥éŸµå¾‹ã€‚</li>
<li>IndexTTS2å®ç°äº†æƒ…æ„Ÿè¡¨è¾¾å’Œè¯´è¯äººèº«ä»½çš„è§£è€¦ï¼Œå…è®¸ç‹¬ç«‹æ§åˆ¶ã€‚</li>
<li>èå…¥GPTæ½œåœ¨è¡¨å¾å’Œä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œæé«˜è¯­éŸ³æ¸…æ™°åº¦å’Œç¨³å®šæ€§ã€‚</li>
<li>é€šè¿‡è½¯æŒ‡ä»¤æœºåˆ¶å¼•å¯¼ç”Ÿæˆå…·æœ‰æ‰€éœ€æƒ…æ„Ÿå€¾å‘çš„è¯­éŸ³ã€‚</li>
<li>IndexTTS2åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–å…ˆè¿›TTSæ¨¡å‹ã€‚</li>
<li>æä¾›éŸ³é¢‘æ ·æœ¬ä»¥ä¾›å¬ä¼—ä½“éªŒï¼š<a target="_blank" rel="noopener" href="https://index-tts.github.io/index-tts2.github.io/%E3%80%82">https://index-tts.github.io/index-tts2.github.io/ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-143a226c48b39cf9c70506b021559f71~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801731&auth_key=1759801731-0-0-125b0282c5ebe0aa8bacc9da47b5cd9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b400bb0c5bd3e546ea1942b196a1c31b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801738&auth_key=1759801738-0-0-769ba9cd1c4f179deb8acbe90358e0ad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4daaad6462edf136227e6ff210a71a6d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801745&auth_key=1759801745-0-0-63fa88e0a1ff2fba9681933fba7134a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb462602d929561088e608ef54d54295~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801751&auth_key=1759801751-0-0-247deb6f6cce2a82d1216ca72cba8915&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-e2e66d6a1c2fdbaf53b74ea127d1d555.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FELLE-Autoregressive-Speech-Synthesis-with-Token-Wise-Coarse-to-Fine-Flow-Matching"><a href="#FELLE-Autoregressive-Speech-Synthesis-with-Token-Wise-Coarse-to-Fine-Flow-Matching" class="headerlink" title="FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine   Flow Matching"></a>FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine   Flow Matching</h2><p><strong>Authors:Hui Wang, Shujie Liu, Lingwei Meng, Jinyu Li, Yifan Yang, Shiwan Zhao, Haiyang Sun, Yanqing Liu, Haoqin Sun, Jiaming Zhou, Yan Lu, Yong Qin</strong></p>
<p>To advance continuous-valued token modeling and temporal-coherence enforcement, we propose FELLE, an autoregressive model that integrates language modeling with token-wise flow matching. By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, FELLE effectively predicts continuous-valued tokens (mel-spectrograms). For each continuous-valued token, FELLE modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability. Furthermore, to enhance synthesis quality, FELLE introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language modelâ€™s output. Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in <a target="_blank" rel="noopener" href="https://aka.ms/felle">https://aka.ms/felle</a>. </p>
<blockquote>
<p>ä¸ºæ¨åŠ¨è¿ç»­å€¼ä»¤ç‰Œå»ºæ¨¡å’Œæ—¶é—´è¿è´¯æ€§æ‰§è¡Œçš„å‘å±•ï¼Œæˆ‘ä»¬æå‡ºäº†FELLEï¼Œè¿™æ˜¯ä¸€ç§å°†è¯­è¨€å»ºæ¨¡ä¸ä»¤ç‰Œçº§æµåŒ¹é…ç›¸ç»“åˆçš„è‡ªåŠ¨ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨é€’å½’ç‰¹æ€§å’ŒæµåŒ¹é…çš„ç”Ÿæˆæ•ˆèƒ½ï¼ŒFELLEå¯ä»¥æœ‰æ•ˆåœ°é¢„æµ‹è¿ç»­å€¼ä»¤ç‰Œï¼ˆæ¢…å°”é¢‘è°±å›¾ï¼‰ã€‚å¯¹äºæ¯ä¸ªè¿ç»­å€¼ä»¤ç‰Œï¼ŒFELLEé€šè¿‡ç»“åˆä¸Šä¸€æ­¥çš„ä¿¡æ¯æ¥ä¿®æ”¹æµåŒ¹é…ä¸­çš„ä¸€èˆ¬å…ˆéªŒåˆ†å¸ƒï¼Œæé«˜äº†è¿è´¯æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜åˆæˆè´¨é‡ï¼ŒFELLEå¼•å…¥äº†ä¸€ç§ä»ç²—åˆ°ç»†çš„æµåŒ¹é…æœºåˆ¶ï¼Œä»¥å±‚æ¬¡ç»“æ„ç”Ÿæˆè¿ç»­å€¼ä»¤ç‰Œï¼Œä¾èµ–äºè¯­è¨€æ¨¡å‹çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è‡ªåŠ¨å›å½’æ¢…å°”é¢‘è°±å›¾å»ºæ¨¡ä¸­èå…¥æµåŒ¹é…æŠ€æœ¯å…·æœ‰æ½œåŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜TTSç”Ÿæˆè´¨é‡ï¼Œè¯¦æƒ…å‚è§<a target="_blank" rel="noopener" href="https://aka.ms/felle%E3%80%82">https://aka.ms/felleã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11128v2">PDF</a> Accepted by ACM Multimedia 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºFELLEçš„è‡ªå›å½’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è¯­è¨€å»ºæ¨¡å’ŒåŸºäºtokençš„æµåŒ¹é…æŠ€æœ¯ï¼Œç”¨äºæ¨è¿›è¿ç»­å€¼tokenå»ºæ¨¡å’Œæ—¶åºä¸€è‡´æ€§å¼ºåŒ–ã€‚é€šè¿‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’ç‰¹æ€§å’ŒæµåŒ¹é…çš„ç”Ÿæˆæ•ˆèƒ½ï¼ŒFELLEèƒ½å¤Ÿæœ‰æ•ˆåœ°é¢„æµ‹è¿ç»­å€¼tokenï¼ˆmelé¢‘è°±å›¾ï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆå‰ä¸€æ­¥çš„ä¿¡æ¯ï¼Œæ”¹è¿›äº†æµåŒ¹é…ä¸­çš„ä¸€èˆ¬å…ˆéªŒåˆ†å¸ƒï¼Œæé«˜äº†è¿è´¯æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†æå‡åˆæˆè´¨é‡ï¼ŒFELLEå¼•å…¥äº†ä»ç²—åˆ°ç»†çš„æµåŒ¹é…æœºåˆ¶ï¼Œæ ¹æ®è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œå±‚æ¬¡æ€§åœ°ç”Ÿæˆè¿ç»­å€¼tokenã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è‡ªå›å½’melé¢‘è°±å›¾å»ºæ¨¡ä¸­èå…¥æµåŒ¹é…æŠ€æœ¯å…·æœ‰æ½œåŠ›ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ–‡æœ¬åˆ°è¯­éŸ³ç”Ÿæˆçš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FELLEæ˜¯ä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œç»“åˆäº†è¯­è¨€å»ºæ¨¡å’ŒåŸºäºtokençš„æµåŒ¹é…æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’ç‰¹æ€§å’ŒæµåŒ¹é…çš„ç”Ÿæˆæ•ˆèƒ½ï¼Œæœ‰æ•ˆåœ°é¢„æµ‹è¿ç»­å€¼tokenï¼ˆmelé¢‘è°±å›¾ï¼‰ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç»“åˆå‰ä¸€æ­¥çš„ä¿¡æ¯æ”¹è¿›äº†ä¸€èˆ¬å…ˆéªŒåˆ†å¸ƒï¼Œå¢å¼ºäº†æ—¶åºè¿è´¯æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>å¼•å…¥äº†ä»ç²—åˆ°ç»†çš„æµåŒ¹é…æœºåˆ¶æ¥æå‡è¯­éŸ³åˆæˆè´¨é‡ã€‚</li>
<li>è¯­è¨€æ¨¡å‹çš„è¾“å‡ºä½œä¸ºæ¡ä»¶ç”¨äºå±‚æ¬¡æ€§åœ°ç”Ÿæˆè¿ç»­å€¼tokenã€‚</li>
<li>å®éªŒè¯æ˜äº†èå…¥æµåŒ¹é…æŠ€æœ¯åœ¨è‡ªå›å½’melé¢‘è°±å›¾å»ºæ¨¡ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25849596a31e33533e567f5e9ca71d03.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c1bff9d8057a4c643bb2ecdbe94e973d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801773&auth_key=1759801773-0-0-133a39224aca334d56a4acef141b8f70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7a6d4236c10cfb8e5f9be36aee31cf77~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801780&auth_key=1759801780-0-0-4196987f48f22d8a85aea95eb147f6bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7aa10b85c495c84f03ef998951d543df~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801787&auth_key=1759801787-0-0-eed7c7c901a08b65c211aca71feb8b07&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2956b0fd22339b53187a82b4091d6462~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801793&auth_key=1759801793-0-0-577edd78fe08ddb9b45ef8cb49906050&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Systematic-Survey"><a href="#Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Systematic-Survey" class="headerlink" title="Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Systematic Survey"></a>Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Systematic Survey</h2><p><strong>Authors:Tianxin Xie, Yan Rong, Pengfei Zhang, Wenwu Wang, Li Liu</strong></p>
<p>Text-to-speech (TTS) has advanced from generating natural-sounding speech to enabling fine-grained control over attributes like emotion, timbre, and style. Driven by rising industrial demand and breakthroughs in deep learning, e.g., diffusion and large language models (LLMs), controllable TTS has become a rapidly growing research area. This survey provides the first comprehensive review of controllable TTS methods, from traditional control techniques to emerging approaches using natural language prompts. We categorize model architectures, control strategies, and feature representations, while also summarizing challenges, datasets, and evaluations in controllable TTS. This survey aims to guide researchers and practitioners by offering a clear taxonomy and highlighting future directions in this fast-evolving field. One can visit <a target="_blank" rel="noopener" href="https://github.com/imxtx/awesome-controllabe-speech-synthesis">https://github.com/imxtx/awesome-controllabe-speech-synthesis</a> for a comprehensive paper list and updates. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰å·²ä»ç”Ÿæˆè‡ªç„¶å£°éŸ³çš„è¯­éŸ³å‘å±•åˆ°å®ç°å¯¹æƒ…æ„Ÿã€éŸ³è´¨å’Œé£æ ¼ç­‰å±æ€§çš„ç²¾ç»†æ§åˆ¶ã€‚éšç€å·¥ä¸šéœ€æ±‚çš„å¢é•¿å’Œæ·±åº¦å­¦ä¹ ç­‰é¢†åŸŸçš„çªç ´ï¼Œå¦‚æ‰©æ•£æ¨¡å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨åŠ¨ï¼Œå¯æ§TTSå·²æˆä¸ºä¸€ä¸ªå¿«é€Ÿå‘å±•çš„ç ”ç©¶é¢†åŸŸã€‚è¿™ç¯‡ç»¼è¿°æä¾›äº†ä»ä¼ ç»Ÿçš„æ§åˆ¶æŠ€å·§åˆ°ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„æ–°å…´æ–¹æ³•ä¹‹é—´å¯æ§TTSæ–¹æ³•çš„é¦–æ¬¡å…¨é¢å›é¡¾ã€‚æˆ‘ä»¬å¯¹æ¨¡å‹æ¶æ„ã€æ§åˆ¶ç­–ç•¥å’Œç‰¹å¾è¡¨ç¤ºè¿›è¡Œåˆ†ç±»ï¼ŒåŒæ—¶æ€»ç»“äº†å¯æ§TTSä¸­çš„æŒ‘æˆ˜ã€æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•ã€‚æœ¬ç»¼è¿°æ—¨åœ¨é€šè¿‡æä¾›æ¸…æ™°çš„åˆ†ç±»å­¦å’Œé«˜äº®å¿«é€Ÿè¿›åŒ–é¢†åŸŸçš„æœªæ¥æ–¹å‘ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æŒ‡å¯¼ã€‚å¯ä»¥é€šè¿‡è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/imxtx/awesome-controllabe-speech-synthesis">https://github.com/imxtx/awesome-controllabe-speech-synthesis</a> æ¥è·å–å…¨é¢çš„è®ºæ–‡åˆ—è¡¨å’Œæœ€æ–°æ›´æ–°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06602v3">PDF</a> The first comprehensive survey on controllable TTS. Accepted to the   EMNLP 2025 main conference</p>
<p><strong>Summary</strong><br>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰å·²ä»ç”Ÿæˆè‡ªç„¶è¯­éŸ³å‘å±•åˆ°å®ç°å¯¹æƒ…æ„Ÿã€éŸ³è‰²å’Œé£æ ¼ç­‰å±æ€§çš„ç²¾ç»†æ§åˆ¶ã€‚å—æ—¥ç›Šå¢é•¿çš„éœ€æ±‚å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çªç ´çš„æ¨åŠ¨ï¼Œå¦‚æ‰©æ•£æŠ€æœ¯å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¯æ§TTSå·²æˆä¸ºä¸€ä¸ªå¿«é€Ÿå¢é•¿çš„ç ”ç©¶é¢†åŸŸã€‚è¿™ç¯‡ç»¼è¿°é¦–æ¬¡å…¨é¢å›é¡¾äº†å¯æ§TTSæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿæ§åˆ¶æŠ€æœ¯å’Œæ–°å…´çš„è‡ªç„¶è¯­è¨€æç¤ºæ–¹æ³•ã€‚æœ¬æ–‡åˆ†ç±»äº†æ¨¡å‹æ¶æ„ã€æ§åˆ¶ç­–ç•¥å’Œç‰¹å¾è¡¨ç¤ºï¼Œå¹¶æ€»ç»“äº†å¯æ§TTSä¸­çš„æŒ‘æˆ˜ã€æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•ã€‚æœ¬ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æ¸…æ™°çš„åˆ†ç±»å’Œåœ¨è¿™ä¸ªå¿«é€Ÿæ¼”å˜çš„é¢†åŸŸçš„æœªæ¥æ–¹å‘çš„æŒ‡å—ã€‚æœ‰å…³å®Œæ•´çš„è®ºæ–‡åˆ—è¡¨å’Œæ›´æ–°ï¼Œè¯·è®¿é—®ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TTSæŠ€æœ¯å·²ä»åŸºç¡€è¯­éŸ³åˆæˆå‘å±•è‡³ç²¾ç»†æ§åˆ¶æƒ…æ„Ÿã€éŸ³è‰²å’Œé£æ ¼ç­‰å±æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯æ§TTSé¢†åŸŸèµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†å¯æ§TTSæ–¹æ³•çš„é¦–æ¬¡å…¨é¢ç»¼è¿°ï¼Œæ¶µç›–ä¼ ç»Ÿæ§åˆ¶æŠ€æœ¯å’Œæ–°å…´çš„è‡ªç„¶è¯­è¨€æç¤ºæ–¹æ³•ã€‚</li>
<li>æ¨¡å‹æ¶æ„ã€æ§åˆ¶ç­–ç•¥å’Œç‰¹å¾è¡¨ç¤ºåœ¨TTSä¸­èµ·åˆ°é‡è¦ä½œç”¨ï¼ŒåŒæ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>ç»¼è¿°æ€»ç»“äº†ç›¸å…³çš„æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†æŒ‡å—ã€‚</li>
<li>å¯ä»¥é€šè¿‡ç‰¹å®šé“¾æ¥è·å–æ›´å…¨é¢çš„è®ºæ–‡åˆ—è¡¨å’Œæœ€æ–°æ›´æ–°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a100e6fa0ac6afe0048a6ce6245ffdf2.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b4db72ece9e3b6c62f6ef43bf8b30f60~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801808&auth_key=1759801808-0-0-33bb44c0f3dccb1323d8db7cfdecc2dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1f8a21569b1b11e9027a42845442489a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801815&auth_key=1759801815-0-0-33c1c1ac2edeac719d0dafbb2cd5171a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bc87e16a38ff07f758e6a917f19a0d62~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801822&auth_key=1759801822-0-0-d5b7f34362e4e1f8ac16334ec36a73ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e46d77ca2891c83cb7e160e3c48104a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801832&auth_key=1759801832-0-0-9e87087eadccc08a6adb542c73059070&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-27af2cdace379ea866444d44fb3fbafe.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception"><a href="#I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception" class="headerlink" title="I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception"></a>I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception</h2><p><strong>Authors:Jiawei Zhang, Tian-Hao Zhang, Jun Wang, Jiaran Gao, Xinyuan Qian, Xu-Cheng Yin</strong></p>
<p>Controlling the style and characteristics of speech synthesis is crucial for adapting the output to specific contexts and user requirements. Previous Text-to-speech (TTS) works have focused primarily on the technical aspects of producing natural-sounding speech, such as intonation, rhythm, and clarity. However, they overlook the fact that there is a growing emphasis on spatial perception of synthesized speech, which may provide immersive experience in gaming and virtual reality. To solve this issue, in this paper, we present a novel multi-modal TTS approach, namely Image-indicated Immersive Text-to-speech Synthesis (I2TTS). Specifically, we introduce a scene prompt encoder that integrates visual scene prompts directly into the synthesis pipeline to control the speech generation process. Additionally, we propose a reverberation classification and refinement technique that adjusts the synthesized mel-spectrogram to enhance the immersive experience, ensuring that the involved reverberation condition matches the scene accurately. Experimental results demonstrate that our model achieves high-quality scene and spatial matching without compromising speech naturalness, marking a significant advancement in the field of context-aware speech synthesis. Project demo page: <a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> Index Terms-Speech synthesis, scene prompt, spatial perception </p>
<blockquote>
<p>æ§åˆ¶è¯­éŸ³åˆæˆçš„é£æ ¼å’Œç‰¹æ€§å¯¹äºé€‚åº”ç‰¹å®šçš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¦æ±‚è‡³å…³é‡è¦ã€‚ä¹‹å‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨äº§ç”Ÿè‡ªç„¶è¯­éŸ³çš„æŠ€æœ¯æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½ç•¥äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œå³å¯¹åˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥çš„é‡è§†ç¨‹åº¦æ­£åœ¨ä¸æ–­å¢é•¿ï¼Œè¿™å¯èƒ½ä¼šåœ¨æ¸¸æˆå’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸæä¾›æ²‰æµ¸å¼ä½“éªŒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼TTSæ–¹æ³•ï¼Œå³å›¾åƒæŒ‡ç¤ºæ²‰æµ¸å¼æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆI2TTSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå®ƒå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥é›†æˆåˆ°åˆæˆç®¡é“ä¸­ï¼Œä»¥æ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥å¯¹åˆæˆçš„æ¢…å°”é¢‘è°±è¿›è¡Œè°ƒæ•´ï¼Œä»¥å¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œå¹¶ç¡®ä¿æ‰€æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¿æŒè¯­éŸ³è‡ªç„¶æ€§çš„åŒæ—¶å®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œæ ‡å¿—ç€ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯­éŸ³åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚é¡¹ç›®æ¼”ç¤ºé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> ç´¢å¼•æœ¯è¯­-è¯­éŸ³åˆæˆã€åœºæ™¯æç¤ºã€ç©ºé—´æ„ŸçŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13314v4">PDF</a> Accepted by APSIPA ASC2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬é‡ç‚¹ä»‹ç»äº†ä¸€ç§åä¸ºI2TTSçš„å¤šæ¨¡æ€TTSæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¼•å…¥åœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥èå…¥åˆæˆç®¡é“ï¼Œæ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚åŒæ—¶æå‡ºä¸€ç§æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œè°ƒæ•´åˆæˆmelé¢‘è°±å›¾ï¼Œå¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œç¡®ä¿æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚æ­¤æ–¹æ³•åœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯­éŸ³åˆæˆé¢†åŸŸå®ç°äº†æ˜¾è‘—è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆæˆéœ€è¦é€‚åº”ç‰¹å®šä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¦æ±‚ï¼Œæ§åˆ¶é£æ ¼å’Œç‰¹æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ä»¥å¾€TTSç ”ç©¶ä¸»è¦å…³æ³¨è‡ªç„¶è¯­éŸ³çš„æŠ€æœ¯æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ã€‚</li>
<li>æ–‡æœ¬å¼ºè°ƒäº†åˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥çš„æ—¥ç›Šå¢é•¿é‡è¦æ€§ï¼Œå°¤å…¶åœ¨æ¸¸æˆå’Œè™šæ‹Ÿç°å®ä¸­çš„æ²‰æµ¸å¼ä½“éªŒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€TTSæ–¹æ³•I2TTSï¼Œå¼•å…¥åœºæ™¯æç¤ºç¼–ç å™¨ç›´æ¥èå…¥è§†è§‰åœºæ™¯æç¤ºåˆ°åˆæˆç®¡é“ä¸­ã€‚</li>
<li>I2TTSæ–¹æ³•é€šè¿‡è°ƒæ•´åˆæˆmelé¢‘è°±å›¾ï¼Œä½¿ç”¨æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œå¢å¼ºäº†æ²‰æµ¸å¼ä½“éªŒã€‚</li>
<li>å®éªŒç»“æœè¯æ˜ï¼ŒI2TTSæ¨¡å‹å®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œä¸”ä¸ç‰ºç‰²è¯­éŸ³çš„è‡ªç„¶æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-172a80751488cbda6eb94673dff1d38f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c12fa7c93134b1fe314aa6ec280ebd28~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801854&auth_key=1759801854-0-0-1ef4a8a66ee9583b3fc82e74c4e809ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-b580801026f76554a6484f6287279b9f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-13ceced552a9794056e5109a0a06c845~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801867&auth_key=1759801867-0-0-680d7a00cf9050e65bc948a14b41c460&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d1cb3130dc8e0118f1f51b8900d31682~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801874&auth_key=1759801874-0-0-a27e26daa083cb4853519920a5292d9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  TaleDiffusion Multi-Character Story Generation with Dialogue Rendering
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7314b58fae444ebe507db6dc94c2adc1.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  A Primer on Causal and Statistical Dataset Biases for Fair and Robust   Image Analysis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
