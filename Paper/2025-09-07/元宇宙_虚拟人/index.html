<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
    <meta name="description" content="å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Hyper Diffusion Avatars Dynamic Human Avatar Generation using Network   Weight Space Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>å…ƒå®‡å®™/è™šæ‹Ÿäºº | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-cf62327d584f9334a71420e5574fb1e5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                                <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                å…ƒå®‡å®™/è™šæ‹Ÿäºº
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    36 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Hyper-Diffusion-Avatars-Dynamic-Human-Avatar-Generation-using-Network-Weight-Space-Diffusion"><a href="#Hyper-Diffusion-Avatars-Dynamic-Human-Avatar-Generation-using-Network-Weight-Space-Diffusion" class="headerlink" title="Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network   Weight Space Diffusion"></a>Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network   Weight Space Diffusion</h2><p><strong>Authors:Dongliang Cao, Guoxing Sun, Marc Habermann, Florian Bernard</strong></p>
<p>Creating human avatars is a highly desirable yet challenging task. Recent advancements in radiance field rendering have achieved unprecedented photorealism and real-time performance for personalized dynamic human avatars. However, these approaches are typically limited to person-specific rendering models trained on multi-view video data for a single individual, limiting their ability to generalize across different identities. On the other hand, generative approaches leveraging prior knowledge from pre-trained 2D diffusion models can produce cartoonish, static human avatars, which are animated through simple skeleton-based articulation. Therefore, the avatars generated by these methods suffer from lower rendering quality compared to person-specific rendering methods and fail to capture pose-dependent deformations such as cloth wrinkles. In this paper, we propose a novel approach that unites the strengths of person-specific rendering and diffusion-based generative modeling to enable dynamic human avatar generation with both high photorealism and realistic pose-dependent deformations. Our method follows a two-stage pipeline: first, we optimize a set of person-specific UNets, with each network representing a dynamic human avatar that captures intricate pose-dependent deformations. In the second stage, we train a hyper diffusion model over the optimized network weights. During inference, our method generates network weights for real-time, controllable rendering of dynamic human avatars. Using a large-scale, cross-identity, multi-view video dataset, we demonstrate that our approach outperforms state-of-the-art human avatar generation methods. </p>
<blockquote>
<p>åˆ›å»ºäººç±»è™šæ‹Ÿå½¢è±¡æ˜¯ä¸€é¡¹éå¸¸ç†æƒ³ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œè¾å°„åœºæ¸²æŸ“æŠ€æœ¯çš„è¿›å±•ä¸ºå®ç°ä¸ªæ€§åŒ–åŠ¨æ€äººç±»è™šæ‹Ÿå½¢è±¡çš„è¶…çœŸå®æ„Ÿå’Œå®æ—¶æ€§èƒ½æä¾›äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä»…é™äºé’ˆå¯¹å•ä¸ªä¸ªä½“çš„å¤šè§†è§’è§†é¢‘æ•°æ®è®­ç»ƒçš„ç‰¹å®šäººç‰©æ¸²æŸ“æ¨¡å‹ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨è·¨ä¸åŒèº«ä»½æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†çš„ç”Ÿæˆæ–¹æ³•ï¼Œå¯ä»¥äº§ç”Ÿå¡é€šå¼çš„é™æ€äººç±»è™šæ‹Ÿå½¢è±¡ï¼Œé€šè¿‡ç®€å•çš„åŸºäºéª¨éª¼çš„å…³èŠ‚è¿åŠ¨è¿›è¡ŒåŠ¨ç”»ã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•ç”Ÿæˆçš„è™šæ‹Ÿå½¢è±¡ä¸ç‰¹å®šäººç‰©æ¸²æŸ“æ–¹æ³•ç›¸æ¯”ï¼Œæ¸²æŸ“è´¨é‡è¾ƒä½ï¼Œå¹¶ä¸”æ— æ³•æ•æ‰å§¿åŠ¿ç›¸å…³çš„å˜å½¢ï¼Œå¦‚è¡£ç‰©è¤¶çš±ç­‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆç‰¹å®šäººç‰©æ¸²æŸ“å’ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆå»ºæ¨¡ä¼˜ç‚¹çš„æ–°æ–¹æ³•ï¼Œä»¥å®ç°å…·æœ‰é«˜åº¦çœŸå®æ„Ÿå’Œç°å®å§¿åŠ¿ç›¸å…³å˜å½¢èƒ½åŠ›çš„åŠ¨æ€äººç±»è™šæ‹Ÿå½¢è±¡ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•éµå¾ªä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä¼˜åŒ–ä¸€ç»„ç‰¹å®šäººç‰©çš„U-Netç½‘ç»œï¼Œæ¯ä¸ªç½‘ç»œä»£è¡¨ä¸€ä¸ªåŠ¨æ€äººç±»è™šæ‹Ÿå½¢è±¡ï¼Œæ•æ‰å¤æ‚çš„å§¿åŠ¿ç›¸å…³å˜å½¢ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬åœ¨ä¼˜åŒ–åçš„ç½‘ç»œæƒé‡ä¸Šè®­ç»ƒä¸€ä¸ªè¶…æ‰©æ•£æ¨¡å‹ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆç½‘ç»œæƒé‡ä»¥è¿›è¡Œå®æ—¶ã€å¯æ§çš„åŠ¨æ€äººç±»è™šæ‹Ÿå½¢è±¡æ¸²æŸ“ã€‚ä½¿ç”¨å¤§è§„æ¨¡è·¨èº«ä»½å¤šè§†è§’è§†é¢‘æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„äººç±»è™šæ‹Ÿå½¢è±¡ç”Ÿæˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04145v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆäººç‰©ç‰¹å®šæ¸²æŸ“å’Œæ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ï¼Œå®ç°äº†åŠ¨æ€äººç±»è§’è‰²ç”Ÿæˆï¼Œå…·æœ‰é«˜åº¦çš„é€¼çœŸæ€§å’ŒçœŸå®çš„å§¿æ€ä¾èµ–æ€§å˜å½¢ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆä¼˜åŒ–äººç‰©ç‰¹å®šçš„U-Netç½‘ç»œï¼Œæ•æ‰å¤æ‚çš„å§¿æ€ä¾èµ–æ€§å˜å½¢ï¼›ç„¶åè®­ç»ƒè¶…æ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆç½‘ç»œæƒé‡ï¼Œå®ç°å®æ—¶å¯æ§çš„åŠ¨æ€äººç±»è§’è‰²æ¸²æŸ“ã€‚ä½¿ç”¨å¤§è§„æ¨¡è·¨èº«ä»½å¤šè§†è§’è§†é¢‘æ•°æ®é›†ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æŠ€æœ¯çš„äººç±»è§’è‰²ç”Ÿæˆæ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ç»“åˆäº†äººç‰©ç‰¹å®šæ¸²æŸ“å’Œæ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°äº†åŠ¨æ€äººç±»è§’è‰²ç”Ÿæˆã€‚</li>
<li>æ–¹æ³•é‡‡ç”¨ä¸¤ä¸ªé˜¶æ®µï¼šä¼˜åŒ–äººç‰©ç‰¹å®šçš„U-Netç½‘ç»œå’Œè®­ç»ƒè¶…æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>U-Netç½‘ç»œæ•æ‰å¤æ‚çš„å§¿æ€ä¾èµ–æ€§å˜å½¢ã€‚</li>
<li>æ–¹æ³•ä½¿ç”¨å¤§è§„æ¨¡è·¨èº«ä»½å¤šè§†è§’è§†é¢‘æ•°æ®é›†è¿›è¡ŒéªŒè¯ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†åŠ¨æ€äººç±»è§’è‰²çš„å®æ—¶å¯æ§æ¸²æŸ“ã€‚</li>
<li>è®ºæ–‡æå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åŠ¨æ€äººç±»è§’è‰²ç”ŸæˆæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04145">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af0850f4472804ec863e372c2c691158.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0047514c2359d57a860b030a91d836d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0987c336d460e33cd429a432a2db7ab9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53968388a743fd21cc350c1ecc609efd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TeRA-Rethinking-Text-guided-Realistic-3D-Avatar-Generation"><a href="#TeRA-Rethinking-Text-guided-Realistic-3D-Avatar-Generation" class="headerlink" title="TeRA: Rethinking Text-guided Realistic 3D Avatar Generation"></a>TeRA: Rethinking Text-guided Realistic 3D Avatar Generation</h2><p><strong>Authors:Yanwen Wang, Yiyu Zhuang, Jiawei Zhang, Li Wang, Yifei Zeng, Xun Cao, Xinxin Zuo, Hao Zhu</strong></p>
<p>In this paper, we rethink text-to-avatar generative models by proposing TeRA, a more efficient and effective framework than the previous SDS-based models and general large 3D generative models. Our approach employs a two-stage training strategy for learning a native 3D avatar generative model. Initially, we distill a decoder to derive a structured latent space from a large human reconstruction model. Subsequently, a text-controlled latent diffusion model is trained to generate photorealistic 3D human avatars within this latent space. TeRA enhances the model performance by eliminating slow iterative optimization and enables text-based partial customization through a structured 3D human representation. Experiments have proven our approachâ€™s superiority over previous text-to-avatar generative models in subjective and objective evaluation. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹TeRAçš„æå‡ºï¼Œé‡æ–°æ€è€ƒäº†æ–‡æœ¬åˆ°è™šæ‹Ÿè§’è‰²çš„ç”Ÿæˆæ¨¡å‹ã€‚TeRaæ˜¯ä¸€ä¸ªæ¯”ä¹‹å‰çš„åŸºäºSDSçš„æ¨¡å‹å’Œä¸€èˆ¬çš„3Dç”Ÿæˆæ¨¡å‹æ›´åŠ é«˜æ•ˆå’Œæœ‰æ•ˆçš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥å­¦ä¹ ä¸€ä¸ªåŸç”Ÿçš„3Dè™šæ‹Ÿè§’è‰²ç”Ÿæˆæ¨¡å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»å¤§å‹äººç±»é‡å»ºæ¨¡å‹ä¸­æç‚¼å‡ºä¸€ä¸ªè§£ç å™¨ï¼Œä»¥è·å–ç»“æ„åŒ–æ½œåœ¨ç©ºé—´ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸€æ½œåœ¨ç©ºé—´å†…è®­ç»ƒä¸€ä¸ªå—æ–‡æœ¬æ§åˆ¶çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆé€¼çœŸçš„3Däººç±»è™šæ‹Ÿè§’è‰²ã€‚TeRaé€šè¿‡æ¶ˆé™¤ç¼“æ…¢çš„è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–3Däººç±»è¡¨ç¤ºå®ç°äº†åŸºäºæ–‡æœ¬çš„å±€éƒ¨å®šåˆ¶ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä»·ä¸Šéƒ½ä¼˜äºä¹‹å‰çš„æ–‡æœ¬åˆ°è™šæ‹Ÿè§’è‰²ç”Ÿæˆæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02466v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºä¸€ä¸ªæ›´é«˜æ•ˆä¸”æœ‰æ•ˆçš„æ–‡æœ¬è½¬åŒ–èº«åƒç”Ÿæˆæ¨¡å‹æ¡†æ¶TeRAï¼Œæ”¹è¿›äº†ä¼ ç»Ÿçš„SDSæ¨¡å‹å’Œä¸€èˆ¬çš„3Dç”Ÿæˆæ¨¡å‹ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥å­¦ä¹ åŸç”Ÿ3DåŒ–èº«ç”Ÿæˆæ¨¡å‹ï¼Œé¦–å…ˆé€šè¿‡å¤§å‹äººç±»é‡å»ºæ¨¡å‹è’¸é¦è§£ç å™¨å¾—åˆ°ç»“æ„åŒ–æ½œåœ¨ç©ºé—´ï¼Œç„¶åè®­ç»ƒæ–‡æœ¬æ§åˆ¶çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨è¯¥æ½œåœ¨ç©ºé—´å†…ç”Ÿæˆé€¼çœŸçš„3Däººç±»åŒ–èº«ã€‚TeRAæé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œé¿å…äº†ç¼“æ…¢çš„è¿­ä»£ä¼˜åŒ–ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–3Däººç±»è¡¨ç°å®ç°åŸºäºæ–‡æœ¬çš„ä¸ªæ€§åŒ–å®šåˆ¶ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°ä¸Šï¼ŒTeRAä¼˜äºå…¶ä»–æ–‡æœ¬åŒ–èº«ç”Ÿæˆæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TeRAæ˜¯ä¸€ä¸ªæ›´é«˜æ•ˆå’Œæœ‰æ•ˆçš„æ–‡æœ¬è½¬åŒ–èº«ç”Ÿæˆæ¨¡å‹æ¡†æ¶ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥å­¦ä¹ åŸç”Ÿ3DåŒ–èº«ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¤§å‹äººç±»é‡å»ºæ¨¡å‹è’¸é¦è§£ç å™¨å¾—åˆ°ç»“æ„åŒ–æ½œåœ¨ç©ºé—´ã€‚</li>
<li>æ–‡æœ¬æ§åˆ¶çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”¨äºç”Ÿæˆé€¼çœŸçš„3Däººç±»åŒ–èº«ã€‚</li>
<li>TeRAé¿å…äº†ç¼“æ…¢çš„è¿­ä»£ä¼˜åŒ–ï¼Œå¹¶å®ç°åŸºäºæ–‡æœ¬çš„ä¸ªæ€§åŒ–å®šåˆ¶ã€‚</li>
<li>å®éªŒè¯æ˜TeRAåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°ä¸Šä¼˜äºå…¶ä»–æ–‡æœ¬åŒ–èº«ç”Ÿæˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02466">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cade8db3468c7dd4adec2a3df9579a30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b97cb3efb58a2fa986f37d6817ae3e57.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GaussianGAN-Real-Time-Photorealistic-controllable-Human-Avatars"><a href="#GaussianGAN-Real-Time-Photorealistic-controllable-Human-Avatars" class="headerlink" title="GaussianGAN: Real-Time Photorealistic controllable Human Avatars"></a>GaussianGAN: Real-Time Photorealistic controllable Human Avatars</h2><p><strong>Authors:Mohamed Ilyes Lakhal, Richard Bowden</strong></p>
<p>Photorealistic and controllable human avatars have gained popularity in the research community thanks to rapid advances in neural rendering, providing fast and realistic synthesis tools. However, a limitation of current solutions is the presence of noticeable blurring. To solve this problem, we propose GaussianGAN, an animatable avatar approach developed for photorealistic rendering of people in real-time. We introduce a novel Gaussian splatting densification strategy to build Gaussian points from the surface of cylindrical structures around estimated skeletal limbs. Given the camera calibration, we render an accurate semantic segmentation with our novel view segmentation module. Finally, a UNet generator uses the rendered Gaussian splatting features and the segmentation maps to create photorealistic digital avatars. Our method runs in real-time with a rendering speed of 79 FPS. It outperforms previous methods regarding visual perception and quality, achieving a state-of-the-art results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and 33.39db on the Thuman4 dataset. </p>
<blockquote>
<p>å…·æœ‰çœŸå®æ„Ÿå’Œå¯æ§æ€§çš„äººç±»è™šæ‹Ÿå½¢è±¡ç”±äºå…¶å¿«é€Ÿå‘å±•çš„ç¥ç»ç½‘ç»œæ¸²æŸ“æŠ€æœ¯æ‰€æä¾›çš„å¿«é€Ÿä¸”çœŸå®çš„åˆæˆå·¥å…·ï¼Œåœ¨å­¦æœ¯ç•Œä¸­å—åˆ°äº†æ¬¢è¿ã€‚ç„¶è€Œï¼Œå½“å‰è§£å†³æ–¹æ¡ˆçš„ä¸€ä¸ªå±€é™æ€§æ˜¯å­˜åœ¨æ˜æ˜¾çš„æ¨¡ç³Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GaussianGANï¼Œè¿™æ˜¯ä¸€ç§ä¸ºå®æ—¶æ¸²æŸ“äººçš„çœŸå®æ„Ÿè€Œå¼€å‘çš„å¯åŠ¨æ€è°ƒæ•´çš„è™šæ‹Ÿå½¢è±¡æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„Gaussian splatting densificationç­–ç•¥ï¼Œä»ä¼°è®¡çš„éª¨éª¼è‚¢ä½“çš„åœ†æŸ±å½¢ç»“æ„è¡¨é¢æ„å»ºGaussianç‚¹ã€‚æ ¹æ®ç›¸æœºæ ¡å‡†ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¨æ–°çš„è§†å›¾åˆ†å‰²æ¨¡å—è¿›è¡Œå‡†ç¡®çš„è¯­ä¹‰åˆ†å‰²ã€‚æœ€åï¼ŒUNetç”Ÿæˆå™¨ä½¿ç”¨æ¸²æŸ“çš„Gaussian splattingç‰¹å¾å’Œåˆ†å‰²å›¾æ¥åˆ›å»ºå…·æœ‰çœŸå®æ„Ÿçš„æ•°å­—è™šæ‹Ÿå½¢è±¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®æ—¶è¿è¡Œï¼Œæ¸²æŸ“é€Ÿåº¦ä¸º79 FPSã€‚åœ¨è§†è§‰æ„ŸçŸ¥å’Œè´¨é‡æ–¹é¢ï¼Œå®ƒä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œåœ¨ZJU Mocapæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æ¯åƒç´ ä¿çœŸåº¦32.94dbçš„æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨Thuman4æ•°æ®é›†ä¸Šè¾¾åˆ°äº†33.39dbã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01681v1">PDF</a> IEEE conference series on Automatic Face and Gesture Recognition 2025</p>
<p><strong>Summary</strong></p>
<p>å½“å‰ç¥ç»ç½‘ç»œæ¸²æŸ“æŠ€æœ¯çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†äººç±»è§’è‰²æ¨¡å‹çš„æµè¡Œã€‚ç„¶è€Œï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆå­˜åœ¨æ¨¡ç³Šé—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†GaussianGANæ–¹æ³•ï¼Œé‡‡ç”¨é«˜æ–¯æŠ•å°„ç¨ åŒ–ç­–ç•¥æ„å»ºé«˜æ–¯ç‚¹ï¼Œé€šè¿‡ç›¸æœºæ ¡å‡†è¿›è¡Œç²¾ç¡®è¯­ä¹‰åˆ†å‰²ï¼Œå¹¶ä½¿ç”¨UNetç”Ÿæˆå™¨åˆ›å»ºé€¼çœŸçš„æ•°å­—è§’è‰²æ¨¡å‹ã€‚è¯¥æ–¹æ³•å®æ—¶è¿è¡Œï¼Œæ¸²æŸ“é€Ÿåº¦è¾¾åˆ°79 FPSï¼Œåœ¨ZJU Mocapå’ŒThuman4æ•°æ®é›†ä¸Šçš„åƒç´ ä¿çœŸåº¦è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€ç¥ç»ç½‘ç»œæ¸²æŸ“æŠ€æœ¯çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†äººç±»è§’è‰²æ¨¡å‹çš„æµè¡Œã€‚</p>
<p>äºŒã€ç°æœ‰è§£å†³æ–¹æ¡ˆå­˜åœ¨æ¨¡ç³Šé—®é¢˜ï¼Œéœ€è¦æ–°æ–¹æ³•è§£å†³ã€‚</p>
<p>ä¸‰ã€GaussianGANæ–¹æ³•é‡‡ç”¨é«˜æ–¯æŠ•å°„ç¨ åŒ–ç­–ç•¥æ„å»ºé«˜æ–¯ç‚¹ã€‚</p>
<p>å››ã€é€šè¿‡ç›¸æœºæ ¡å‡†è¿›è¡Œç²¾ç¡®è¯­ä¹‰åˆ†å‰²ã€‚</p>
<p>äº”ã€ä½¿ç”¨UNetç”Ÿæˆå™¨åˆ›å»ºé€¼çœŸçš„æ•°å­—è§’è‰²æ¨¡å‹ã€‚</p>
<p>å…­ã€è¯¥æ–¹æ³•å®æ—¶è¿è¡Œï¼Œæ¸²æŸ“é€Ÿåº¦è¾¾åˆ°79 FPSã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f7b676141f63ff1d15275c7f91fa9c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-204ca48c59e1cdfb974db2a1305f5680.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df57ded81908c67c95640c9f5ef8431e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e1ff40271780e7d4a36c17ccb0a9bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38222ed290d3951369e340488f2ef745.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbb1aa7e7edd6eb0dbed22c153990927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4be3b80635361b110517e16e7005ec4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Im2Haircut-Single-view-Strand-based-Hair-Reconstruction-for-Human-Avatars"><a href="#Im2Haircut-Single-view-Strand-based-Hair-Reconstruction-for-Human-Avatars" class="headerlink" title="Im2Haircut: Single-view Strand-based Hair Reconstruction for Human   Avatars"></a>Im2Haircut: Single-view Strand-based Hair Reconstruction for Human   Avatars</h2><p><strong>Authors:Vanessa Sklyarova, Egor Zakharov, Malte Prinzler, Giorgio Becherini, Michael J. Black, Justus Thies</strong></p>
<p>We present a novel approach for 3D hair reconstruction from single photographs based on a global hair prior combined with local optimization. Capturing strand-based hair geometry from single photographs is challenging due to the variety and geometric complexity of hairstyles and the lack of ground truth training data. Classical reconstruction methods like multi-view stereo only reconstruct the visible hair strands, missing the inner structure of hairstyles and hampering realistic hair simulation. To address this, existing methods leverage hairstyle priors trained on synthetic data. Such data, however, is limited in both quantity and quality since it requires manual work from skilled artists to model the 3D hairstyles and create near-photorealistic renderings. To address this, we propose a novel approach that uses both, real and synthetic data to learn an effective hairstyle prior. Specifically, we train a transformer-based prior model on synthetic data to obtain knowledge of the internal hairstyle geometry and introduce real data in the learning process to model the outer structure. This training scheme is able to model the visible hair strands depicted in an input image, while preserving the general 3D structure of hairstyles. We exploit this prior to create a Gaussian-splatting-based reconstruction method that creates hairstyles from one or more images. Qualitative and quantitative comparisons with existing reconstruction pipelines demonstrate the effectiveness and superior performance of our method for capturing detailed hair orientation, overall silhouette, and backside consistency. For additional results and code, please refer to <a target="_blank" rel="noopener" href="https://im2haircut.is.tue.mpg.de/">https://im2haircut.is.tue.mpg.de</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»å•å¼ ç…§ç‰‡è¿›è¡Œ3Då¤´å‘é‡å»ºçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºå…¨å±€å¤´å‘å…ˆéªŒå€¼ç»“åˆå±€éƒ¨ä¼˜åŒ–ã€‚ç”±äºå‘å‹çš„å¤šæ ·æ€§å’Œå‡ ä½•å¤æ‚æ€§ï¼Œä»¥åŠç¼ºä¹çœŸå®è®­ç»ƒæ•°æ®ï¼Œä»å•å¼ ç…§ç‰‡æ•æ‰åŸºäºç»†çº¿çš„å¤´å‘å‡ ä½•ç»“æ„æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„é‡å»ºæ–¹æ³•ï¼ˆå¦‚å¤šè§†è§’ç«‹ä½“æŠ€æœ¯ï¼‰åªèƒ½é‡å»ºå¯è§çš„å¤´å‘ç»†çº¿ï¼Œå¿½ç•¥äº†å‘å‹çš„å†…éƒ¨ç»“æ„ï¼Œé˜»ç¢äº†çœŸå®å¤´å‘æ¨¡æ‹Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åˆ©ç”¨åŸºäºåˆæˆæ•°æ®çš„å‘å‹å…ˆéªŒå€¼ã€‚ç„¶è€Œï¼Œæ­¤ç±»æ•°æ®åœ¨æ•°é‡å’Œè´¨é‡ä¸Šå‡å—åˆ°é™åˆ¶ï¼Œå› ä¸ºéœ€è¦ç†Ÿç»ƒè‰ºæœ¯å®¶çš„æ‰‹å·¥å·¥ä½œæ¥å»ºç«‹3Då‘å‹å¹¶åˆ›å»ºæ¥è¿‘çœŸå®ç…§ç‰‡çš„æ¸²æŸ“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼ŒåŒæ—¶ä½¿ç”¨çœŸå®å’Œåˆæˆæ•°æ®æ¥å­¦ä¹ æœ‰æ•ˆçš„å‘å‹å…ˆéªŒå€¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒäº†ä¸€ä¸ªåŸºäºè½¬æ¢å™¨çš„å…ˆéªŒæ¨¡å‹ï¼Œä»¥è·å¾—å‘å‹å†…éƒ¨å‡ ä½•çš„çŸ¥è¯†ï¼Œå¹¶åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¼•å…¥çœŸå®æ•°æ®æ¥æ¨¡æ‹Ÿå¤–éƒ¨ç»“æ„ã€‚è¿™ç§è®­ç»ƒæ–¹æ¡ˆèƒ½å¤Ÿæ¨¡æ‹Ÿè¾“å…¥å›¾åƒä¸­æç»˜çš„å¯è§å¤´å‘ç»†çº¿ï¼ŒåŒæ—¶ä¿æŒå‘å‹çš„æ•´ä½“3Dç»“æ„ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€å…ˆéªŒå€¼åˆ›å»ºäº†ä¸€ç§åŸºäºé«˜æ–¯å¹³é“ºçš„é‡å»ºæ–¹æ³•ï¼Œå¯ä»¥ä»ä¸€å¼ æˆ–å¤šå¼ ç…§ç‰‡åˆ›å»ºå‘å‹ã€‚ä¸ç°æœ‰é‡å»ºç®¡é“çš„è´¨é‡å’Œæ•°é‡æ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•æ‰è¯¦ç»†çš„å¤´å‘æ–¹å‘ã€æ•´ä½“è½®å»“å’ŒèƒŒé¢ä¸€è‡´æ€§æ–¹é¢éƒ½éå¸¸æœ‰æ•ˆä¸”æ€§èƒ½å“è¶Šã€‚æ›´å¤šç»“æœå’Œä»£ç è¯·å‚è§ï¼š<a target="_blank" rel="noopener" href="https://im2haircut.is.tue.mpg.de./">https://im2haircut.is.tue.mpg.deã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01469v1">PDF</a> For more results please refer to the project page   <a target="_blank" rel="noopener" href="https://im2haircut.is.tue.mpg.de/">https://im2haircut.is.tue.mpg.de</a></p>
<p><strong>Summary</strong><br>åŸºäºåˆæˆå’ŒçœŸå®æ•°æ®ç»“åˆçš„æ–¹æ³•ï¼Œé€šè¿‡å…¨çƒå¤´å‘å…ˆéªŒä¸å±€éƒ¨ä¼˜åŒ–ç›¸ç»“åˆï¼Œå®ç°äº†ä»å•å¼ ç…§ç‰‡ä¸­è¿›è¡Œä¸‰ç»´å¤´å‘é‡å»ºã€‚è¯¥æ–¹æ³•è§£å†³äº†å› å‘å‹å¤šæ ·æ€§å’Œå‡ ä½•å¤æ‚æ€§ä»¥åŠç¼ºä¹çœŸå®è®­ç»ƒæ•°æ®å¯¼è‡´çš„å¤´å‘é‡å»ºéš¾é¢˜ã€‚ä¸ä¼ ç»Ÿé‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½é‡å»ºå¯è§çš„å¤´å‘ï¼Œè¿˜èƒ½ä¿ç•™å‘å‹å†…éƒ¨ç»“æ„ï¼Œå®ç°äº†æ›´çœŸå®çš„å¤´å‘æ¨¡æ‹Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç»“åˆçœŸå®å’Œåˆæˆæ•°æ®çš„æ–°å‹å¤´å‘é‡å»ºæ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨åŸºäºåˆæˆæ•°æ®çš„è½¬æ¢å™¨æ¨¡å‹å­¦ä¹ å‘å‹å…ˆéªŒï¼Œäº†è§£å‘å‹å†…éƒ¨ç»“æ„ã€‚</li>
<li>åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¼•å…¥çœŸå®æ•°æ®ï¼Œä»¥æ¨¡æ‹Ÿå¤–éƒ¨ç»“æ„ã€‚</li>
<li>é€šè¿‡é«˜æ–¯å¹³é“ºæŠ€æœ¯é‡å»ºå‘å‹ï¼Œå¯ä»ä¸€å¼ æˆ–å¤šå¼ å›¾åƒåˆ›å»ºå‘å‹ã€‚</li>
<li>æ–¹æ³•èƒ½æœ‰æ•ˆæ•æ‰å¤´å‘ç»†èŠ‚æ–¹å‘ã€æ•´ä½“è½®å»“å’ŒèƒŒé¢ä¸€è‡´æ€§ã€‚</li>
<li>ä¸ç°æœ‰é‡å»ºæµç¨‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-63d7d4ef87df38c81cb089a39aa6a32c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdfb67f320a14d1f03c293af4cc52b01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9705821fabd9cf760524fc5b5b233ae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eefc3d01957892ce2b3a00c0bd60490f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DevilSight-Augmenting-Monocular-Human-Avatar-Reconstruction-through-a-Virtual-Perspective"><a href="#DevilSight-Augmenting-Monocular-Human-Avatar-Reconstruction-through-a-Virtual-Perspective" class="headerlink" title="DevilSight: Augmenting Monocular Human Avatar Reconstruction through a   Virtual Perspective"></a>DevilSight: Augmenting Monocular Human Avatar Reconstruction through a   Virtual Perspective</h2><p><strong>Authors:Yushuo Chen, Ruizhi Shao, Youxin Pang, Hongwen Zhang, Xinyi Wu, Rihui Wu, Yebin Liu</strong></p>
<p>We present a novel framework to reconstruct human avatars from monocular videos. Recent approaches have struggled either to capture the fine-grained dynamic details from the input or to generate plausible details at novel viewpoints, which mainly stem from the limited representational capacity of the avatar model and insufficient observational data. To overcome these challenges, we propose to leverage the advanced video generative model, Human4DiT, to generate the human motions from alternative perspective as an additional supervision signal. This approach not only enriches the details in previously unseen regions but also effectively regularizes the avatar representation to mitigate artifacts. Furthermore, we introduce two complementary strategies to enhance video generation: To ensure consistent reproduction of human motion, we inject the physical identity into the model through video fine-tuning. For higher-resolution outputs with finer details, a patch-based denoising algorithm is employed. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches and validate the effectiveness of our proposed strategies. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•ç›®è§†é¢‘ä¸­é‡å»ºäººç±»åŒ–èº«ã€‚æœ€è¿‘çš„æ–¹æ³•è¦ä¹ˆéš¾ä»¥ä»è¾“å…¥ä¸­æ•è·ç²¾ç»†çš„åŠ¨æ€ç»†èŠ‚ï¼Œè¦ä¹ˆéš¾ä»¥åœ¨æ–°è§†è§’ç”Ÿæˆåˆç†çš„ç»†èŠ‚ï¼Œè¿™ä¸»è¦æºäºåŒ–èº«æ¨¡å‹çš„æœ‰é™è¡¨ç¤ºèƒ½åŠ›å’Œè§‚å¯Ÿæ•°æ®çš„ä¸è¶³ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æè®®åˆ©ç”¨å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹Human4DiTï¼Œä»æ›¿ä»£è§†è§’ç”Ÿæˆäººç±»è¿åŠ¨ä½œä¸ºé¢å¤–çš„ç›‘ç£ä¿¡å·ã€‚è¿™ç§æ–¹æ³•ä¸ä»…ä¸°å¯Œäº†ä¹‹å‰æœªè§åŒºåŸŸçš„ç»†èŠ‚ï¼Œè¿˜æœ‰æ•ˆåœ°è°ƒæ•´äº†åŒ–èº«è¡¨ç¤ºï¼Œå‡è½»äº†ä¼ªå½±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§äº’è¡¥çš„ç­–ç•¥æ¥æå‡è§†é¢‘ç”Ÿæˆï¼šä¸ºç¡®ä¿äººç±»è¿åŠ¨çš„ä¸€è‡´å†ç°ï¼Œæˆ‘ä»¬é€šè¿‡è§†é¢‘å¾®è°ƒå°†ç‰©ç†èº«ä»½æ³¨å…¥æ¨¡å‹ã€‚å¯¹äºæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºå’Œæ›´ç²¾ç»†çš„ç»†èŠ‚ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºè¡¥ä¸çš„å»å™ªç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€è¿‘çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶éªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨Human4DiTè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»å•ç›®è§†é¢‘ä¸­é‡å»ºäººç±»è§’è‰²ï¼ˆavatarsï¼‰ã€‚è¯¥æ–¹æ³•å…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨æ•æ‰ç²¾ç»†åŠ¨æ€ç»†èŠ‚æˆ–ç”Ÿæˆæ–°é¢–è§†è§’æ—¶çš„å±€é™æ€§ï¼Œé€šè¿‡å¼•å…¥é¢å¤–çš„ç›‘ç£ä¿¡å·ç”Ÿæˆäººç±»è¿åŠ¨ï¼Œä¸ä»…ä¸°å¯Œäº†ä¹‹å‰æœªè§åŒºåŸŸçš„ç»†èŠ‚ï¼Œè¿˜æœ‰æ•ˆè§„èŒƒäº†è§’è‰²è¡¨ç¤ºï¼Œå‡è½»äº†ä¼ªå½±ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥ä¸¤ç§å¢å¼ºè§†é¢‘ç”Ÿæˆç­–ç•¥ï¼šé€šè¿‡è§†é¢‘å¾®è°ƒå°†ç‰©ç†èº«ä»½æ³¨å…¥æ¨¡å‹ä»¥ç¡®ä¿äººç±»è¿åŠ¨çš„è¿è´¯å†ç°ï¼›é‡‡ç”¨åŸºäºè¡¥ä¸çš„å»å™ªç®—æ³•ï¼Œä»¥è·å–æ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºå’Œæ›´ç²¾ç»†çš„ç»†èŠ‚ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€æ–°å…ˆè¿›æŠ€æœ¯ï¼ŒéªŒè¯äº†æ‰€æç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨Human4DiTè§†é¢‘ç”Ÿæˆæ¨¡å‹ä»å•ç›®è§†é¢‘ä¸­é‡å»ºäººç±»è§’è‰²ã€‚</li>
<li>å¼•å…¥é¢å¤–çš„ç›‘ç£ä¿¡å·ç”Ÿæˆäººç±»è¿åŠ¨ï¼Œå…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>é€šè¿‡ä¸°å¯Œç»†èŠ‚å’Œæœ‰æ•ˆè§„èŒƒè§’è‰²è¡¨ç¤ºï¼Œæé«˜è§†é¢‘è´¨é‡ã€‚</li>
<li>å¼•å…¥ä¸¤ç§å¢å¼ºè§†é¢‘ç”Ÿæˆç­–ç•¥ï¼šç¡®ä¿è¿è´¯çš„çš„è¿åŠ¨å†ç°å’Œæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºã€‚</li>
<li>é€šè¿‡è§†é¢‘å¾®è°ƒå°†ç‰©ç†èº«ä»½æ³¨å…¥æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨åŸºäºè¡¥ä¸çš„å»å™ªç®—æ³•ä»¥è·å–æ›´ç²¾ç»†çš„ç»†èŠ‚ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•ä¼˜äºæœ€æ–°å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4cea6bc223411174e354b3c26c7945ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04b706ea8dad8c67513ead472b4786b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc57bd9804a47f5551d3e68fbc1dcbf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bad3751f2b48134dbf638a1d04cc49c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-973d6a78d1e870124b7bc65793fe86b8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OmniHuman-1-5-Instilling-an-Active-Mind-in-Avatars-via-Cognitive-Simulation"><a href="#OmniHuman-1-5-Instilling-an-Active-Mind-in-Avatars-via-Cognitive-Simulation" class="headerlink" title="OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive   Simulation"></a>OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive   Simulation</h2><p><strong>Authors:Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, Mingyuan Gao</strong></p>
<p>Existing video avatar models can produce fluid human animations, yet they struggle to move beyond mere physical likeness to capture a characterâ€™s authentic essence. Their motions typically synchronize with low-level cues like audio rhythm, lacking a deeper semantic understanding of emotion, intent, or context. To bridge this gap, \textbf{we propose a framework designed to generate character animations that are not only physically plausible but also semantically coherent and expressive.} Our model, \textbf{OmniHuman-1.5}, is built upon two key technical contributions. First, we leverage Multimodal Large Language Models to synthesize a structured textual representation of conditions that provides high-level semantic guidance. This guidance steers our motion generator beyond simplistic rhythmic synchronization, enabling the production of actions that are contextually and emotionally resonant. Second, to ensure the effective fusion of these multimodal inputs and mitigate inter-modality conflicts, we introduce a specialized Multimodal DiT architecture with a novel Pseudo Last Frame design. The synergy of these components allows our model to accurately interpret the joint semantics of audio, images, and text, thereby generating motions that are deeply coherent with the character, scene, and linguistic content. Extensive experiments demonstrate that our model achieves leading performance across a comprehensive set of metrics, including lip-sync accuracy, video quality, motion naturalness and semantic consistency with textual prompts. Furthermore, our approach shows remarkable extensibility to complex scenarios, such as those involving multi-person and non-human subjects. Homepage: \href{<a target="_blank" rel="noopener" href="https://omnihuman-lab.github.io/v1_5/%7D">https://omnihuman-lab.github.io/v1_5/}</a> </p>
<blockquote>
<p>ç°æœ‰è§†é¢‘è§’è‰²æ¨¡å‹å¯ä»¥äº§ç”Ÿæµç•…çš„äººç‰©åŠ¨ç”»ï¼Œä½†å®ƒä»¬éš¾ä»¥è¶…è¶Šå•çº¯çš„ç‰©ç†ç›¸ä¼¼æ€§ï¼Œæ•æ‰è§’è‰²çš„çœŸå®æœ¬è´¨ã€‚å®ƒä»¬çš„åŠ¨ä½œé€šå¸¸ä¸éŸ³é¢‘èŠ‚å¥ç­‰ä½çº§çº¿ç´¢åŒæ­¥ï¼Œç¼ºä¹æƒ…æ„Ÿã€æ„å›¾æˆ–ä¸Šä¸‹æ–‡çš„æ·±å±‚è¯­ä¹‰ç†è§£ã€‚ä¸ºäº†å¼¥è¿™ä¸€å·®è·ï¼Œ<strong>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆä¸ä»…åœ¨ç‰©ç†ä¸Šåˆç†ï¼Œè€Œä¸”åœ¨è¯­ä¹‰ä¸Šè¿è´¯ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è§’è‰²åŠ¨ç”»ã€‚</strong>æˆ‘ä»¬çš„æ¨¡å‹â€œOmniHuman-1.5â€å»ºç«‹åœ¨ä¸¤é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ä¹‹ä¸Šã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆç»“æ„åŒ–æ–‡æœ¬è¡¨ç¤ºæ¡ä»¶ï¼Œä¸ºåŠ¨ä½œç”Ÿæˆå™¨æä¾›é«˜çº§è¯­ä¹‰æŒ‡å¯¼ã€‚è¿™ç§æŒ‡å¯¼ä½¿åŠ¨ä½œç”Ÿæˆå™¨è¶…è¶Šäº†ç®€å•çš„èŠ‚å¥åŒæ­¥ï¼Œèƒ½å¤Ÿäº§ç”Ÿåœ¨ä¸Šä¸‹æ–‡å’Œæƒ…æ„Ÿä¸Šäº§ç”Ÿå…±é¸£çš„åŠ¨ä½œã€‚å…¶æ¬¡ï¼Œä¸ºäº†ç¡®ä¿è¿™äº›å¤šæ¨¡æ€è¾“å…¥çš„æœ‰æ•ˆèåˆå¹¶ç¼“è§£è·¨æ¨¡æ€å†²çªï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¸¦æœ‰æ–°é¢–Pseudo Last Frameè®¾è®¡çš„ä¸“ç”¨å¤šæ¨¡æ€DiTæ¶æ„ã€‚è¿™äº›ç»„ä»¶çš„ååŒä½œç”¨ä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®è§£é‡ŠéŸ³é¢‘ã€å›¾åƒå’Œæ–‡æœ¬çš„å…±åŒè¯­ä¹‰ï¼Œä»è€Œç”Ÿæˆä¸è§’è‰²ã€åœºæ™¯å’Œè¯­è¨€å†…å®¹æ·±åº¦ä¸€è‡´çš„åŠ¨ä½œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åŒ…æ‹¬å”‡åŒæ­¥å‡†ç¡®æ€§ã€è§†é¢‘è´¨é‡ã€åŠ¨ä½œè‡ªç„¶æ€§ä»¥åŠæ–‡æœ¬æç¤ºçš„è¯­ä¹‰ä¸€è‡´æ€§ç­‰å…¨é¢æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹äºæ¶‰åŠå¤šäººä»¥åŠéäººç±»ä¸»é¢˜çš„å¤æ‚åœºæ™¯å±•ç°å‡ºæƒŠäººçš„å¯æ‰©å±•æ€§ã€‚å®˜ç½‘é“¾æ¥ï¼š[<a target="_blank" rel="noopener" href="https://omnihuman-lab.github.io/v1_5/]">https://omnihuman-lab.github.io/v1_5/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19209v1">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://omnihuman-lab.github.io/v1_5/">https://omnihuman-lab.github.io/v1_5/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°æœ‰è§†é¢‘è§’è‰²æ¨¡å‹å­˜åœ¨éš¾ä»¥æ•æ‰è§’è‰²çœŸå®æœ¬è´¨çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆè§’è‰²åŠ¨ç”»çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸ä»…ç‰©ç†ä¸Šå¯è¡Œï¼Œè€Œä¸”è¯­ä¹‰ä¸Šè¿è´¯ä¸”å¯Œæœ‰è¡¨ç°åŠ›ã€‚é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåˆæˆç»“æ„åŒ–æ–‡æœ¬è¡¨ç¤ºæ¡ä»¶ï¼Œä¸ºè¿åŠ¨ç”Ÿæˆå™¨æä¾›é«˜çº§è¯­ä¹‰æŒ‡å¯¼ï¼Œå®ç°äº†åŸºäºä¸Šä¸‹æ–‡å’Œæƒ…æ„Ÿçš„åŠ¨ä½œç”Ÿæˆã€‚åŒæ—¶ï¼Œå¼•å…¥å¤šæ¨¡æ€DiTæ¶æ„å’Œä¼ªæœ€åä¸€å¸§è®¾è®¡ï¼Œç¡®ä¿å¤šæ¨¡æ€è¾“å…¥çš„èåˆå¹¶ç¼“è§£è·¨æ¨¡æ€å†²çªã€‚è¯¥æ¨¡å‹å‡†ç¡®è§£è¯»éŸ³é¢‘ã€å›¾åƒå’Œæ–‡æœ¬çš„è”åˆè¯­ä¹‰ï¼Œç”Ÿæˆä¸è§’è‰²ã€åœºæ™¯å’Œè¯­è¨€å†…å®¹æ·±åº¦ä¸€è‡´çš„åŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—äº†é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†é¢‘è§’è‰²æ¨¡å‹å­˜åœ¨éš¾ä»¥æ•æ‰è§’è‰²çœŸå®æœ¬è´¨çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸ä»…ç‰©ç†å¯è¡Œè€Œä¸”è¯­ä¹‰è¿è´¯ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è§’è‰²åŠ¨ç”»ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æä¾›é«˜çº§è¯­ä¹‰æŒ‡å¯¼ï¼Œå®ç°åŸºäºä¸Šä¸‹æ–‡å’Œæƒ…æ„Ÿçš„åŠ¨ä½œç”Ÿæˆã€‚</li>
<li>å¼•å…¥å¤šæ¨¡æ€DiTæ¶æ„å’Œä¼ªæœ€åä¸€å¸§è®¾è®¡ï¼Œç¡®ä¿å¤šæ¨¡æ€è¾“å…¥çš„èåˆå¹¶ç¼“è§£è·¨æ¨¡æ€å†²çªã€‚</li>
<li>æ¨¡å‹èƒ½å‡†ç¡®è§£è¯»éŸ³é¢‘ã€å›¾åƒå’Œæ–‡æœ¬çš„è”åˆè¯­ä¹‰ï¼Œç”Ÿæˆä¸è§’è‰²ã€åœºæ™¯å’Œè¯­è¨€å†…å®¹æ·±åº¦ä¸€è‡´çš„åŠ¨ä½œã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—äº†é¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬å”‡åŒæ­¥å‡†ç¡®æ€§ã€è§†é¢‘è´¨é‡ã€è¿åŠ¨è‡ªç„¶æ€§å’Œä¸æ–‡æœ¬æç¤ºçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e379dcc10fcbe178e63e85b6c65c4800.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7906699ca7451de74ac415b4a6e0017b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-418be3fbf0ba72e07fdf885f911aa1b2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Diverse-Signer-Avatars-with-Manual-and-Non-Manual-Feature-Modelling-for-Sign-Language-Production"><a href="#Diverse-Signer-Avatars-with-Manual-and-Non-Manual-Feature-Modelling-for-Sign-Language-Production" class="headerlink" title="Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for   Sign Language Production"></a>Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for   Sign Language Production</h2><p><strong>Authors:Mohamed Ilyes Lakhal, Richard Bowden</strong></p>
<p>The diversity of sign representation is essential for Sign Language Production (SLP) as it captures variations in appearance, facial expressions, and hand movements. However, existing SLP models are often unable to capture diversity while preserving visual quality and modelling non-manual attributes such as emotions. To address this problem, we propose a novel approach that leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital avatars from a generated reference image. We propose a novel sign feature aggregation module that explicitly models the non-manual features (\textit{e.g.}, the face) and the manual features (\textit{e.g.}, the hands). We show that our proposed module ensures the preservation of linguistic content while seamlessly using reference images with different ethnic backgrounds to ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show that our pipeline achieves superior visual quality compared to state-of-the-art methods, with significant improvements on perceptual metrics. </p>
<blockquote>
<p>æ‰‹åŠ¿è¡¨è¾¾çš„å¤šæ ·æ€§å¯¹äºæ‰‹è¯­ç”Ÿæˆï¼ˆSLPï¼‰è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒæ•æ‰äº†å¤–è§‚ã€é¢éƒ¨è¡¨æƒ…å’Œæ‰‹éƒ¨åŠ¨ä½œçš„å˜åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SLPæ¨¡å‹å¾€å¾€æ— æ³•åœ¨æ‰‹åŠ¿å¤šæ ·æ€§çš„åŒæ—¶ä¿æŒè§†è§‰è´¨é‡å¹¶å»ºæ¨¡éæ‰‹åŠ¨å±æ€§ï¼ˆå¦‚æƒ…ç»ªï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelï¼ŒLDMï¼‰ä»ç”Ÿæˆçš„å‚è€ƒå›¾åƒä¸­åˆæˆé€¼çœŸçš„æ•°å­—è™šæ‹Ÿå½¢è±¡çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ‰‹åŠ¿ç‰¹å¾èšåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—æ˜ç¡®åœ°å»ºæ¨¡éæ‰‹åŠ¨ç‰¹å¾ï¼ˆä¾‹å¦‚è„¸éƒ¨ï¼‰å’Œæ‰‹åŠ¨ç‰¹å¾ï¼ˆä¾‹å¦‚æ‰‹éƒ¨ï¼‰ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å—ç¡®ä¿äº†è¯­è¨€å†…å®¹çš„ä¿ç•™ï¼ŒåŒæ—¶æ— ç¼åœ°ä½¿ç”¨å…·æœ‰ä¸åŒç§æ—èƒŒæ™¯çš„å‚è€ƒå›¾åƒæ¥ç¡®ä¿å¤šæ ·æ€§ã€‚åœ¨YouTube-SL-25æ‰‹è¯­æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨è§†è§‰è´¨é‡æ–¹é¢è¾¾åˆ°äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ„ŸçŸ¥æŒ‡æ ‡ä¸Šå–å¾—äº†é‡å¤§æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15988v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒæ‰‹åŠ¿è¯­è¨€ç”Ÿæˆï¼ˆSLPï¼‰ä¸­ç¬¦å·è¡¨ç¤ºå¤šæ ·æ€§çš„é‡è¦æ€§ï¼Œæ¶µç›–å¤–è§‚ã€é¢éƒ¨è¡¨æƒ…å’Œæ‰‹éƒ¨åŠ¨ä½œçš„å˜åŒ–ã€‚é’ˆå¯¹ç°æœ‰SLPæ¨¡å‹åœ¨æ•æ‰å¤šæ ·æ€§æ—¶éš¾ä»¥ä¿æŒè§†è§‰è´¨é‡åŠæ¨¡æ‹Ÿéæ‰‹åŠ¨å±æ€§ï¼ˆå¦‚æƒ…ç»ªï¼‰çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åˆæˆåŸºäºå‚è€ƒå›¾åƒçš„å…‰ç…§ç°å®æ•°å­—è™šæ‹Ÿäººçš„æ–°æ–¹æ³•ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„ç¬¦å·ç‰¹å¾èšåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—æ˜¾å¼åœ°æ¨¡æ‹Ÿéæ‰‹åŠ¨ç‰¹å¾ï¼ˆå¦‚é¢éƒ¨ï¼‰å’Œæ‰‹åŠ¨ç‰¹å¾ï¼ˆå¦‚æ‰‹éƒ¨ï¼‰ï¼Œç¡®ä¿è¯­è¨€å†…å®¹çš„ä¿ç•™ï¼ŒåŒæ—¶ä½¿ç”¨å…·æœ‰ä¸åŒç§æ—èƒŒæ™¯çš„å‚è€ƒå›¾åƒæ¥ç¡®ä¿å¤šæ ·æ€§ã€‚åœ¨YouTube-SL-25æ‰‹è¯­æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥ç®¡é“åœ¨è§†è§‰è´¨é‡æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œæ„ŸçŸ¥æŒ‡æ ‡ä¸Šæœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ ·æ€§åœ¨æ‰‹åŠ¿è¯­è¨€ç”Ÿæˆä¸­çš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬å¤–è§‚ã€é¢éƒ¨è¡¨æƒ…å’Œæ‰‹éƒ¨åŠ¨ä½œçš„å·®å¼‚ã€‚</li>
<li>ç°æœ‰æ‰‹åŠ¿è¯­è¨€ç”Ÿæˆæ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ï¼šéš¾ä»¥æ•æ‰å¤šæ ·æ€§å¹¶ä¿æŒè§†è§‰è´¨é‡åŠæ¨¡æ‹Ÿéæ‰‹åŠ¨å±æ€§ã€‚</li>
<li>æå‡ºä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åˆæˆåŸºäºå‚è€ƒå›¾åƒçš„å…‰ç…§ç°å®æ•°å­—è™šæ‹Ÿäººçš„æ–°æ–¹æ³•ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„ç¬¦å·ç‰¹å¾èšåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—æ˜¾å¼æ¨¡æ‹Ÿéæ‰‹åŠ¨å’Œæ‰‹åŠ¨ç‰¹å¾ã€‚</li>
<li>è¯¥æ¨¡å—èƒ½ç¡®ä¿è¯­è¨€å†…å®¹çš„ä¿ç•™åŠåˆ©ç”¨ä¸åŒç§æ—èƒŒæ™¯çš„å‚è€ƒå›¾åƒæ¥ç¡®ä¿å¤šæ ·æ€§ã€‚</li>
<li>åœ¨YouTube-SL-25æ‰‹è¯­æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥ç®¡é“åœ¨è§†è§‰è´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6b55ae439f275df96ebb7acc831fc047.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3553f32ef1f451484778ef8e12be6758.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a16b557f8e32daa7621699f8033498ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a95541f9f0faf3fd61f5c301917a90de.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Supervising-3D-Talking-Head-Avatars-with-Analysis-by-Audio-Synthesis"><a href="#Supervising-3D-Talking-Head-Avatars-with-Analysis-by-Audio-Synthesis" class="headerlink" title="Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis"></a>Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis</h2><p><strong>Authors:Radek DanÄ›Äek, Carolin Schmitt, Senya Polikovsky, Michael J. Black</strong></p>
<p>In order to be widely applicable, speech-driven 3D head avatars must articulate their lips in accordance with speech, while also conveying the appropriate emotions with dynamically changing facial expressions. The key problem is that deterministic models produce high-quality lip-sync but without rich expressions, whereas stochastic models generate diverse expressions but with lower lip-sync quality. To get the best of both, we seek a stochastic model with accurate lip-sync. To that end, we develop a new approach based on the following observation: if a method generates realistic 3D lip motions, it should be possible to infer the spoken audio from the lip motion. The inferred speech should match the original input audio, and erroneous predictions create a novel supervision signal for training 3D talking head avatars with accurate lip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under Neural Differentiable Elocution Reconstruction), a 3D talking head avatar framework that introduces a novel supervision mechanism via differentiable sound production. First, we train a novel mesh-to-speech model that regresses audio from facial animation. Then, we incorporate this model into a diffusion-based talking avatar framework. During training, the mesh-to-speech model takes the generated animation and produces a sound that is compared to the input speech, creating a differentiable analysis-by-audio-synthesis supervision loop. Our extensive qualitative and quantitative experiments demonstrate that THUNDER significantly improves the quality of the lip-sync of talking head avatars while still allowing for generation of diverse, high-quality, expressive facial animations. The code and models will be available at <a target="_blank" rel="noopener" href="https://thunder.is.tue.mpg.de/">https://thunder.is.tue.mpg.de/</a> </p>
<blockquote>
<p>ä¸ºäº†å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼Œè¯­éŸ³é©±åŠ¨çš„3Då¤´éƒ¨åŒ–èº«å¿…é¡»æ ¹æ®è¯­éŸ³è¿›è¡Œå”‡éƒ¨åŠ¨ä½œï¼ŒåŒæ—¶å€ŸåŠ©åŠ¨æ€å˜åŒ–çš„é¢éƒ¨è¡¨æƒ…ä¼ è¾¾é€‚å½“çš„æƒ…ç»ªã€‚å…³é”®é—®é¢˜åœ¨äºï¼Œç¡®å®šæ€§æ¨¡å‹è™½ç„¶èƒ½äº§ç”Ÿé«˜è´¨é‡çš„å”‡éƒ¨åŒæ­¥æ•ˆæœï¼Œä½†ç¼ºä¹ä¸°å¯Œçš„è¡¨æƒ…ï¼›è€Œéšæœºæ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆå„ç§è¡¨æƒ…ï¼Œä½†å”‡éƒ¨åŒæ­¥è´¨é‡è¾ƒä½ã€‚ä¸ºäº†ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬å¯»æ±‚å…·æœ‰ç²¾ç¡®å”‡éƒ¨åŒæ­¥çš„éšæœºæ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åŸºäºä»¥ä¸‹è§‚å¯Ÿç»“æœå¼€å‘äº†ä¸€ç§æ–°æ–¹æ³•ï¼šå¦‚æœä¸€ç§æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„3Då”‡éƒ¨è¿åŠ¨ï¼Œé‚£ä¹ˆå°±åº”è¯¥å¯ä»¥ä»å”‡éƒ¨è¿åŠ¨ä¸­æ¨æ–­å‡ºè¯­éŸ³ã€‚æ¨æ–­å‡ºçš„è¯­éŸ³åº”ä¸åŸå§‹è¾“å…¥éŸ³é¢‘ç›¸åŒ¹é…ï¼Œé”™è¯¯çš„é¢„æµ‹ä¼šåˆ›å»ºä¸€ç§æ–°å‹ç›‘ç£ä¿¡å·ï¼Œç”¨äºè®­ç»ƒå…·æœ‰ç²¾ç¡®å”‡éƒ¨åŒæ­¥çš„3Då¯¹è¯å¤´éƒ¨åŒ–èº«ã€‚ä¸ºäº†å±•ç¤ºè¿™ä¸€æ•ˆæœï¼Œæˆ‘ä»¬æå‡ºäº†THUNDERï¼ˆç¥ç»å¯å¾®è¯­éŸ³é‡å»ºä¸‹çš„å¯¹è¯å¤´éƒ¨ï¼ŒTalking Heads Under Neural Differentiable Elocution Reconstructionï¼‰é¡¹ç›®ï¼Œè¿™æ˜¯ä¸€ä¸ª3Då¯¹è¯å¤´éƒ¨åŒ–èº«æ¡†æ¶ï¼Œé€šè¿‡å¯å¾®å£°éŸ³äº§ç”Ÿå¼•å…¥äº†ä¸€ç§æ–°å‹ç›‘ç£æœºåˆ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§æ–°å‹ç½‘æ ¼è¯­éŸ³æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»é¢éƒ¨åŠ¨ç”»å›å½’éŸ³é¢‘ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¯¥æ¨¡å‹çº³å…¥åŸºäºæ‰©æ•£çš„å¯¹è¯åŒ–èº«æ¡†æ¶ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç½‘æ ¼è¯­éŸ³æ¨¡å‹ä¼šåˆ©ç”¨ç”Ÿæˆçš„åŠ¨ç”»äº§ç”Ÿå£°éŸ³ï¼Œå¹¶ä¸è¾“å…¥è¯­éŸ³è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªå¯å¾®éŸ³é¢‘åˆæˆåˆ†æç›‘ç£å¾ªç¯ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒTHUNDERé¡¹ç›®åœ¨æ”¹å–„å¯¹è¯å¤´éƒ¨åŒ–èº«çš„å”‡éƒ¨åŒæ­¥è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼ŒåŒæ—¶ä»èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡ã€å¯Œæœ‰è¡¨ç°åŠ›çš„é¢éƒ¨åŠ¨ç”»ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://thunder.is.tue.mpg.de/]">https://thunder.is.tue.mpg.de/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13386v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨æ„å»ºå¹¿æ³›é€‚ç”¨çš„è¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´å¤´éƒ¨è§’è‰²æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œå¯å¾®å£°éŸ³ç”ŸæˆæŠ€æœ¯çš„å…¨æ–°ç›‘ç£æœºåˆ¶THUNDERæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é¢éƒ¨åŠ¨ç”»å›å½’éŸ³é¢‘çš„æ¨¡å‹ï¼Œå¹¶åœ¨æ‰©æ•£å¼è¯´è¯å¤´åƒæ¡†æ¶ä¸­å¼•å…¥æ­¤æ¨¡å‹ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æ¯”è¾ƒç”Ÿæˆçš„åŠ¨ç”»äº§ç”Ÿçš„å£°éŸ³ä¸è¾“å…¥è¯­éŸ³ï¼Œåˆ›å»ºä¸€ä¸ªå¯å¾®çš„éŸ³é¢‘åˆæˆç›‘ç£å¾ªç¯ï¼Œä»è€Œåœ¨æé«˜å”‡å½¢åŒæ­¥è´¨é‡çš„åŒæ—¶ï¼Œå…è®¸ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è¡¨æƒ…åŠ¨ç”»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´å¤´éƒ¨è§’è‰²éœ€è¦å®ç°å”‡éŸ³åŒæ­¥å’Œè¡¨æƒ…ä¼ è¾¾ã€‚</li>
<li>ç°æœ‰ç¡®å®šæ€§æ¨¡å‹è™½èƒ½å®ç°é«˜è´¨é‡å”‡éŸ³åŒæ­¥ï¼Œä½†è¡¨æƒ…è¡¨ç°ä¸è¶³ï¼›éšæœºæ¨¡å‹è™½æœ‰å¤šæ ·åŒ–è¡¨æƒ…ï¼Œä½†å”‡éŸ³åŒæ­¥è´¨é‡è¾ƒä½ã€‚</li>
<li>THUNDERæ¡†æ¶ç»“åˆé¢éƒ¨åŠ¨ç”»å›å½’éŸ³é¢‘çš„æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°é«˜è´¨é‡å”‡éŸ³åŒæ­¥ä¸å¤šæ ·åŒ–è¡¨æƒ…çš„å®Œç¾ç»“åˆã€‚</li>
<li>å¼•å…¥å¯å¾®å£°éŸ³äº§ç”ŸæŠ€æœ¯ï¼Œåˆ›å»ºäº†ä¸€ä¸ªé€šè¿‡æ¯”è¾ƒç”ŸæˆåŠ¨ç”»ä¸è¾“å…¥è¯­éŸ³çš„å£°éŸ³æ¥è®­ç»ƒçš„ç›‘ç£æœºåˆ¶ã€‚</li>
<li>THUNDERæ¡†æ¶åœ¨å®šæ€§å’Œå®šé‡å®éªŒä¸­éƒ½è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œæ˜¾è‘—æé«˜äº†è¯´è¯å¤´åƒçš„å”‡å½¢åŒæ­¥è´¨é‡ã€‚</li>
<li>è¯¥æ¡†æ¶å°†æä¾›ä¸°å¯Œçš„è¡¨æƒ…åŠ¨ç”»ç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-263e794e8bf7f017a90a0f66d1a7b854.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd053ebd184e3da36aefc04005e2af98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf62327d584f9334a71420e5574fb1e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f132b8a95854acbc57588b51873dddb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Legacy-Learning-Strategy-Based-on-Few-Shot-Font-Generation-Models-for-Automatic-Text-Design-in-Metaverse-Content"><a href="#Legacy-Learning-Strategy-Based-on-Few-Shot-Font-Generation-Models-for-Automatic-Text-Design-in-Metaverse-Content" class="headerlink" title="Legacy Learning Strategy Based on Few-Shot Font Generation Models for   Automatic Text Design in Metaverse Content"></a>Legacy Learning Strategy Based on Few-Shot Font Generation Models for   Automatic Text Design in Metaverse Content</h2><p><strong>Authors:Younghwi Kim, Dohee Kim, Seok Chan Jeong, Sunghyun Sim</strong></p>
<p>The metaverse consists of hardware, software, and content, among which text design plays a critical role in enhancing user immersion and usability as a content element. However, in languages such as Korean and Chinese that require thousands of unique glyphs, creating new text designs involves high costs and complexity. To address this, this study proposes a training strategy called Legacy Learning, which recombines and transforms structures based on existing text design models. This approach enables the generation of new text designs and improves quality without manual design processes. To evaluate Legacy Learning, it was applied to Korean and Chinese text designs. Additionally, we compared results before and after on seven state of the art text generation models. As a result, text designs generated using Legacy Learning showed over a 30% difference in Frechet Inception Distance (FID) and Learned Perceptual Image Patch Similarity (LPIPS) metrics compared to the originals, and also exhibited meaningful style variations in visual comparisons. Furthermore, the repeated learning process improved the structural consistency of the generated characters, and an OCR based evaluation showed increasing recognition accuracy across iterations, indicating improved legibility of the generated glyphs. In addition, a System Usability Scale (SUS) survey was conducted to evaluate usability among metaverse content designers and general users. The expert group recorded a score of 95.78 (â€œBest Imaginableâ€), while the non expert group scored 76.42 (â€œExcellentâ€), indicating an overall high level of usability. These results suggest that Legacy Learning can significantly improve both the production efficiency and quality of text design in the metaverse environment. </p>
<blockquote>
<p>å…ƒå®‡å®™ç”±ç¡¬ä»¶ã€è½¯ä»¶å’Œå†…å®¹æ„æˆï¼Œå…¶ä¸­æ–‡æœ¬è®¾è®¡ä½œä¸ºå†…å®¹è¦ç´ åœ¨æå‡ç”¨æˆ·æ²‰æµ¸æ„Ÿå’Œå¯ç”¨æ€§æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œåœ¨éŸ©è¯­å’Œä¸­æ–‡ç­‰éœ€è¦æˆåƒä¸Šä¸‡ç‹¬ç‰¹å­—ç¬¦çš„è¯­è¨€ä¸­ï¼Œåˆ›å»ºæ–°çš„æ–‡æœ¬è®¾è®¡æ¶‰åŠé«˜æ˜‚çš„æˆæœ¬å’Œå¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºLegacy Learningçš„è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºç°æœ‰çš„æ–‡æœ¬è®¾è®¡æ¨¡å‹é‡æ–°ç»„åˆå’Œè½¬æ¢ç»“æ„ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ–°çš„æ–‡æœ¬è®¾è®¡ï¼Œå¹¶åœ¨æ— éœ€æ‰‹åŠ¨è®¾è®¡æµç¨‹çš„æƒ…å†µä¸‹æé«˜è´¨é‡ã€‚ä¸ºäº†è¯„ä¼°Legacy Learningçš„æ•ˆæœï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºéŸ©è¯­å’Œä¸­æ–‡çš„æ–‡æœ¬è®¾è®¡ï¼Œå¹¶å°†ç»“æœä¸ä¸ƒæ¬¾æœ€å…ˆè¿›çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœæ˜¾å‘Šï¼Œä½¿ç”¨Legacy Learningç”Ÿæˆçš„æ–‡æœ¬è®¾è®¡åœ¨Frechet Inception Distanceï¼ˆFIDï¼‰å’ŒLearned Perceptual Image Patch Similarityï¼ˆLPIPSï¼‰æŒ‡æ ‡ä¸Šä¸åŸå§‹è®¾è®¡ç›¸æ¯”æœ‰è¶…è¿‡30%çš„å·®å¼‚ï¼Œè§†è§‰æ¯”è¾ƒä¸­ä¹Ÿå‘ˆç°å‡ºæœ‰æ„ä¹‰çš„è®¾è®¡é£æ ¼å˜åŒ–ã€‚æ­¤å¤–ï¼Œé‡å¤å­¦ä¹ è¿‡ç¨‹æé«˜äº†ç”Ÿæˆå­—ç¬¦çš„ç»“æ„ä¸€è‡´æ€§ï¼ŒåŸºäºOCRçš„è¯„ä¼°æ˜¾ç¤ºè¿­ä»£è¿‡ç¨‹ä¸­çš„è¯†åˆ«å‡†ç¡®ç‡ä¸æ–­æé«˜ï¼Œè¡¨æ˜ç”Ÿæˆçš„å­—ç¬¦å¯è¯»æ€§æœ‰æ‰€æé«˜ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†ç³»ç»Ÿå¯ç”¨æ€§é‡è¡¨ï¼ˆSUSï¼‰è°ƒæŸ¥ï¼Œä»¥è¯„ä¼°å…ƒå®‡å®™å†…å®¹è®¾è®¡å¸ˆå’Œæ™®é€šç”¨æˆ·çš„å¯ç”¨æ€§ã€‚ä¸“å®¶ç»„å¾—åˆ†ä¸º95.78åˆ†ï¼ˆâ€œæœ€ä½³æƒ³è±¡â€ï¼‰ï¼Œè€Œéä¸“å®¶ç»„å¾—åˆ†ä¸º76.42åˆ†ï¼ˆâ€œä¼˜ç§€â€ï¼‰ï¼Œè¡¨æ˜æ•´ä½“å¯ç”¨æ€§è¾ƒé«˜ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLegacy Learningå¯ä»¥æ˜¾è‘—æé«˜å…ƒå®‡å®™ç¯å¢ƒä¸­æ–‡æœ¬è®¾è®¡çš„ç”Ÿäº§æ•ˆç‡å’Œè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16900v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æœ¬ç ”ç©¶çš„è®­ç»ƒç­–ç•¥ä¸ºLegacy Learningï¼Œé€šè¿‡é‡ç»„å’Œè½¬æ¢ç°æœ‰æ–‡æœ¬è®¾è®¡æ¨¡å‹çš„ç»“æ„ï¼Œç”Ÿæˆæ–°çš„æ–‡æœ¬è®¾è®¡ï¼Œæé«˜äº†è´¨é‡å¹¶é™ä½äº†æ‰‹åŠ¨è®¾è®¡è¿‡ç¨‹çš„æˆæœ¬ã€‚åœ¨éŸ©è¯­å’Œä¸­æ–‡æ–‡æœ¬è®¾è®¡ä¸­çš„åº”ç”¨ä»¥åŠä¸ƒä¸ªæœ€æ–°æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„å‰åå¯¹æ¯”ç»“æœæ˜¾ç¤ºï¼ŒLegacy Learningç”Ÿæˆçš„æ–‡æœ¬è®¾è®¡åœ¨Frechet Inception Distanceï¼ˆFIDï¼‰å’ŒLearned Perceptual Image Patch Similarityï¼ˆLPIPSï¼‰æŒ‡æ ‡ä¸Šä¸åŸè®¾è®¡ç›¸æ¯”æœ‰è¶…è¿‡30%çš„å·®å¼‚ï¼Œå¹¶ä¸”åœ¨è§†è§‰æ¯”è¾ƒä¸­å±•ç°å‡ºæœ‰æ„ä¹‰çš„é£æ ¼å˜åŒ–ã€‚æ­¤å¤–ï¼Œé‡å¤å­¦ä¹ è¿‡ç¨‹æé«˜äº†ç”Ÿæˆå­—ç¬¦çš„ç»“æ„ä¸€è‡´æ€§ï¼ŒOCRè¯„ä¼°æ˜¾ç¤ºè¯†åˆ«å‡†ç¡®ç‡éšç€è¿­ä»£è€Œæé«˜ï¼Œè¡¨æ˜ç”Ÿæˆçš„å­—ç¬¦å¯è¯†åˆ«æ€§æé«˜ã€‚ç³»ç»Ÿå¯ç”¨æ€§é‡è¡¨è°ƒæŸ¥æ˜¾ç¤ºä¸“å®¶å’Œéä¸“å®¶ç”¨æˆ·çš„ä½¿ç”¨ä½“éªŒè‰¯å¥½ï¼Œè¯æ˜Legacy Learningåœ¨æ”¹å–„è™šæ‹Ÿå…ƒå®‡å®™ç¯å¢ƒä¸­æ–‡æœ¬è®¾è®¡çš„ç”Ÿäº§æ•ˆç‡å’Œè´¨é‡æ–¹é¢æœ‰æ˜æ˜¾æˆæ•ˆã€‚è¯¥ç­–ç•¥ä¸ä»…æé«˜äº†ç”¨æˆ·ä½“éªŒï¼Œè¿˜æ¨åŠ¨äº†å…ƒå®‡å®™ä¸­æ–‡æœ¬è®¾è®¡çš„åˆ›æ–°å’Œå‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Legacy Learningç­–ç•¥é€šè¿‡é‡ç»„å’Œè½¬æ¢ç°æœ‰æ–‡æœ¬è®¾è®¡æ¨¡å‹çš„ç»“æ„ï¼Œå®ç°äº†æ–°æ–‡æœ¬è®¾è®¡çš„è‡ªåŠ¨ç”Ÿæˆï¼Œæé«˜äº†ç”Ÿäº§æ•ˆç‡å’Œè´¨é‡ã€‚</li>
<li>åœ¨éŸ©è¯­å’Œä¸­æ–‡æ–‡æœ¬è®¾è®¡ä¸­çš„åº”ç”¨éªŒè¯äº†Legacy Learningçš„æœ‰æ•ˆæ€§ï¼Œä¸ä¼ ç»Ÿè®¾è®¡ç›¸æ¯”ï¼Œæ–°ç”Ÿæˆçš„æ–‡æœ¬è®¾è®¡åœ¨è§†è§‰é£æ ¼å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>é€šè¿‡Frechet Inception Distanceï¼ˆFIDï¼‰å’ŒLearned Perceptual Image Patch Similarityï¼ˆLPIPSï¼‰ç­‰åº¦é‡æ ‡å‡†ï¼Œè¯æ˜äº†Legacy Learningç”Ÿæˆçš„æ–‡æœ¬è®¾è®¡åœ¨è´¨é‡ä¸Šå®ç°äº†æ˜¾è‘—æå‡ã€‚</li>
<li>OCRè¯„ä¼°ç»“æœæ˜¾ç¤ºå­—ç¬¦çš„å¯è¯†åˆ«æ€§éšç€è¿­ä»£çš„è¿›è¡Œè€Œæé«˜ï¼ŒéªŒè¯äº†Legacy Learningåœ¨æé«˜å­—ç¬¦è´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç³»ç»Ÿå¯ç”¨æ€§é‡è¡¨è°ƒæŸ¥ç»“æœæ˜¾ç¤ºä¸“å®¶å’Œéä¸“å®¶ç”¨æˆ·å‡å¯¹Legacy Learningç”Ÿæˆçš„æ–‡æœ¬è®¾è®¡ç»™å‡ºäº†é«˜åº¦è¯„ä»·ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜ç”¨æˆ·ä½“éªŒæ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>Legacy Learningä¸ä»…æé«˜äº†å…ƒå®‡å®™ä¸­æ–‡æœ¬è®¾è®¡çš„ç”Ÿäº§æ•ˆç‡å’Œè´¨é‡ï¼Œè€Œä¸”æ¨åŠ¨äº†æ–‡æœ¬è®¾è®¡çš„åˆ›æ–°å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-58d76c206bec2494740e03d1e4b5533a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4a5b70f7e016e60a6cfa0cc26a189e7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                                    <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-69f2bfe6f45af1301fb21137b1b7cd75.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  SSGaussian Semantic-Aware and Structure-Preserving 3D Style Transfer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-2bc346488cc2f961ad5a020e9350b097.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Enhancing Robustness in Post-Processing Watermarking An Ensemble Attack   Network Using CNNs and Transformers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
