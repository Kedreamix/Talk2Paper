<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
    <meta name="description" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Adaptive Contrast Adjustment Module A Clinically-Inspired Plug-and-Play   Approach for Enhanced Fetal Plane Classification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-22313f99f725fd1b85daf282b7437b22.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    44 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Adaptive-Contrast-Adjustment-Module-A-Clinically-Inspired-Plug-and-Play-Approach-for-Enhanced-Fetal-Plane-Classification"><a href="#Adaptive-Contrast-Adjustment-Module-A-Clinically-Inspired-Plug-and-Play-Approach-for-Enhanced-Fetal-Plane-Classification" class="headerlink" title="Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play   Approach for Enhanced Fetal Plane Classification"></a>Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play   Approach for Enhanced Fetal Plane Classification</h2><p><strong>Authors:Yang Chen, Sanglin Zhao, Baoyu Chen, Mans Gustaf</strong></p>
<p>Fetal ultrasound standard plane classification is essential for reliable prenatal diagnosis but faces inherent challenges, including low tissue contrast, boundary ambiguity, and operator-dependent image quality variations. To overcome these limitations, we propose a plug-and-play adaptive contrast adjustment module (ACAM), whose core design is inspired by the clinical practice of doctors adjusting image contrast to obtain clearer and more discriminative structural information. The module employs a shallow texture-sensitive network to predict clinically plausible contrast parameters, transforms input images into multiple contrast-enhanced views through differentiable mapping, and fuses them within downstream classifiers. Validated on a multi-center dataset of 12,400 images across six anatomical categories, the module consistently improves performance across diverse models, with accuracy of lightweight models increasing by 2.02 percent, accuracy of traditional models increasing by 1.29 percent, and accuracy of state-of-the-art models increasing by 1.15 percent. The innovation of the module lies in its content-aware adaptation capability, replacing random preprocessing with physics-informed transformations that align with sonographer workflows while improving robustness to imaging heterogeneity through multi-view fusion. This approach effectively bridges low-level image features with high-level semantics, establishing a new paradigm for medical image analysis under real-world image quality variations. </p>
<blockquote>
<p>èƒå„¿è¶…å£°æ ‡å‡†å¹³é¢åˆ†ç±»å¯¹äºå¯é çš„äº§å‰è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´ç€å›ºæœ‰çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç»„ç»‡å¯¹æ¯”åº¦ä½ã€è¾¹ç•Œæ¨¡ç³Šå’Œä¾èµ–äºæ“ä½œå‘˜çš„å›¾åƒè´¨é‡å˜åŒ–ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„è‡ªé€‚åº”å¯¹æ¯”åº¦è°ƒæ•´æ¨¡å—ï¼ˆACAMï¼‰ï¼Œå…¶æ ¸å¿ƒè®¾è®¡çµæ„Ÿæ¥æºäºåŒ»ç”Ÿè°ƒæ•´å›¾åƒå¯¹æ¯”åº¦ä»¥è·å–æ›´æ¸…æ™°ã€æ›´å…·åŒºåˆ†åº¦çš„ç»“æ„ä¿¡æ¯çš„ä¸´åºŠå®è·µã€‚è¯¥æ¨¡å—é‡‡ç”¨æµ…çº¹ç†æ•æ„Ÿç½‘ç»œæ¥é¢„æµ‹ä¸´åºŠä¸Šåˆç†çš„å¯¹æ¯”åº¦å‚æ•°ï¼Œé€šè¿‡å¯å¾®åˆ†æ˜ å°„å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºå¤šä¸ªå¯¹æ¯”åº¦å¢å¼ºçš„è§†å›¾ï¼Œå¹¶åœ¨ä¸‹æ¸¸åˆ†ç±»å™¨ä¸­èåˆå®ƒä»¬ã€‚åœ¨å¤šä¸­å¿ƒæ•°æ®é›†ï¼ˆæ¶µç›–12400å¼ å›¾åƒå’Œå…­ä¸ªè§£å‰–ç±»åˆ«ï¼‰ä¸Šè¿›è¡ŒéªŒè¯ï¼Œè¯¥æ¨¡å—åœ¨å„ç§æ¨¡å‹ä¸Šçš„è¡¨ç°å§‹ç»ˆæœ‰æ‰€æå‡ï¼Œè½»é‡çº§æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜2.02%ï¼Œä¼ ç»Ÿæ¨¡å‹çš„å‡†ç¡®ç‡æé«˜1.29%ï¼Œå…ˆè¿›æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜1.15%ã€‚è¯¥æ¨¡å—çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶åŸºäºå†…å®¹æ„ŸçŸ¥çš„è‡ªé€‚åº”èƒ½åŠ›ï¼Œç”¨åŸºäºç‰©ç†çš„è½¬æ¢æ›¿ä»£äº†éšæœºé¢„å¤„ç†ï¼Œè¿™ä¸è¶…å£°åŒ»å¸ˆçš„å·¥ä½œæµç¨‹ç›¸ç¬¦ï¼ŒåŒæ—¶é€šè¿‡å¤šè§†å›¾èåˆæé«˜äº†å¯¹æˆåƒå¼‚è´¨æ€§çš„ç¨³å¥æ€§ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å°†ä½çº§å›¾åƒç‰¹å¾ä¸é«˜çº§è¯­ä¹‰ç›¸ç»“åˆï¼Œä¸ºç°å®ä¸–ç•Œä¸­å›¾åƒè´¨é‡å˜åŒ–çš„åŒ»å­¦å›¾åƒåˆ†æå»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00808v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å³æ’å³ç”¨çš„è‡ªé€‚åº”å¯¹æ¯”åº¦è°ƒæ•´æ¨¡å—ï¼ˆACAMï¼‰ï¼Œç”¨äºè§£å†³èƒå„¿è¶…å£°æ ‡å‡†å¹³é¢åˆ†ç±»åœ¨äº§å‰è¯Šæ–­ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å—é€šè¿‡è°ƒæ•´å›¾åƒå¯¹æ¯”åº¦ï¼Œè·å¾—æ›´æ¸…æ™°ã€æ›´å…·è¾¨è¯†åº¦çš„ç»“æ„ä¿¡æ¯ï¼Œé‡‡ç”¨æµ…çº¹ç†æ•æ„Ÿç½‘ç»œé¢„æµ‹åˆç†çš„å¯¹æ¯”åº¦å‚æ•°ï¼Œå°†è¾“å…¥å›¾åƒè½¬åŒ–ä¸ºå¤šç§å¯¹æ¯”åº¦å¢å¼ºè§†å›¾ï¼Œå¹¶åœ¨ä¸‹æ¸¸åˆ†ç±»å™¨ä¸­èåˆã€‚åœ¨å¤šä¸­å¿ƒæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œè¯¥æ¨¡å—åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„æ€§èƒ½å‡æœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒå„¿è¶…å£°æ ‡å‡†å¹³é¢åˆ†ç±»å¯¹äº§å‰è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨ç»„ç»‡å¯¹æ¯”åº¦ä½ã€è¾¹ç•Œæ¨¡ç³Šå’Œå›¾åƒè´¨é‡æ“ä½œè€…å·®å¼‚ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºè‡ªé€‚åº”å¯¹æ¯”åº¦è°ƒæ•´æ¨¡å—ï¼ˆACAMï¼‰ï¼Œçµæ„Ÿæ¥æºäºåŒ»ç”Ÿè°ƒæ•´å›¾åƒå¯¹æ¯”åº¦ä»¥è·å–æ›´æ¸…æ™°ç»“æ„ä¿¡æ¯çš„ä¸´åºŠå®è·µã€‚</li>
<li>ACAMä½¿ç”¨æµ…çº¹ç†æ•æ„Ÿç½‘ç»œé¢„æµ‹åˆç†çš„å¯¹æ¯”åº¦å‚æ•°ï¼Œå°†è¾“å…¥å›¾åƒè½¬åŒ–ä¸ºå¤šç§å¯¹æ¯”åº¦å¢å¼ºè§†å›¾ã€‚</li>
<li>ACAMé€šè¿‡å¯å¾®æ˜ å°„å®ç°å¤šç§è§†å›¾èåˆï¼Œå¹¶åœ¨ä¸‹æ¸¸åˆ†ç±»å™¨ä¸­ä½¿ç”¨ã€‚</li>
<li>åœ¨åŒ…å«12,400å¼ å›¾åƒçš„å¤šä¸­å¿ƒæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼ŒACAMæå‡äº†ä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ACAMçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶å†…å®¹æ„ŸçŸ¥é€‚åº”èƒ½åŠ›ï¼Œèƒ½æ›¿æ¢éšæœºé¢„å¤„ç†ï¼Œé‡‡ç”¨ç¬¦åˆè¶…å£°å·¥ä½œè€…å·¥ä½œæµç¨‹çš„ç‰©ç†ä¿¡æ¯è½¬æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79c8b0b458f59846dc7ae23c188bd708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-784e3689841e0376d91c4e4f7f25af93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dfa6df11ae0ed6fc29a197fef7f9a12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-477b0421db9ea54c42cd503cb7d0b65f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HCCM-Hierarchical-Cross-Granularity-Contrastive-and-Matching-Learning-for-Natural-Language-Guided-Drones"><a href="#HCCM-Hierarchical-Cross-Granularity-Contrastive-and-Matching-Learning-for-Natural-Language-Guided-Drones" class="headerlink" title="HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning   for Natural Language-Guided Drones"></a>HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning   for Natural Language-Guided Drones</h2><p><strong>Authors:Hao Ruan, Jinliang Lin, Yingxin Lai, Zhiming Luo, Shaozi Li</strong></p>
<p>Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such as target matching and navigation. However, the wide field of view and complex compositional semantics in drone scenarios pose challenges for vision-language understanding. Mainstream Vision-Language Models (VLMs) emphasize global alignment while lacking fine-grained semantics, and existing hierarchical methods depend on precise entity partitioning and strict containment, limiting effectiveness in dynamic environments. To address this, we propose the Hierarchical Cross-Granularity Contrastive and Matching learning (HCCM) framework with two components: (1) Region-Global Image-Text Contrastive Learning (RG-ITC), which avoids precise scene partitioning and captures hierarchical local-to-global semantics by contrasting local visual regions with global text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM), which dispenses with rigid constraints and instead evaluates local semantic consistency within global cross-modal representations, enhancing compositional reasoning. Moreover, drone text descriptions are often incomplete or ambiguous, destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation (MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM achieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text retrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot generalization with 39.93% mean recall (mR), outperforming fine-tuned baselines. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€æŒ‡å¯¼çš„æ— äººæœºï¼ˆNLGDï¼‰ä¸ºç›®æ ‡åŒ¹é…å’Œå¯¼èˆªç­‰ä»»åŠ¡æä¾›äº†æ–°çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œæ— äººæœºåœºæ™¯ä¸­çš„å¹¿é˜”è§†é‡å’Œå¤æ‚çš„ç»„åˆè¯­ä¹‰å¯¹è§†è§‰è¯­è¨€ç†è§£æå‡ºäº†æŒ‘æˆ˜ã€‚ä¸»æµçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼ºè°ƒå…¨å±€å¯¹é½ï¼Œä½†ç¼ºä¹ç²¾ç»†çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œç°æœ‰çš„å±‚æ¬¡æ–¹æ³•ä¾èµ–äºç²¾ç¡®å®ä½“åˆ†å‰²å’Œä¸¥æ ¼åŒ…å«å…³ç³»ï¼Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å±‚æ¬¡åŒ–è·¨ç²’åº¦å¯¹æ¯”åŒ¹é…å­¦ä¹ ï¼ˆHCCMï¼‰æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼šï¼ˆ1ï¼‰åŒºåŸŸå…¨å±€å›¾åƒæ–‡æœ¬å¯¹æ¯”å­¦ä¹ ï¼ˆRG-ITCï¼‰ï¼Œé€šè¿‡å¯¹æ¯”å±€éƒ¨è§†è§‰åŒºåŸŸä¸å…¨å±€æ–‡æœ¬ï¼Œé¿å…ç²¾ç¡®åœºæ™¯åˆ†å‰²ï¼Œæ•æ‰å±‚æ¬¡åŒ–çš„å±€éƒ¨åˆ°å…¨å±€è¯­ä¹‰ï¼› ï¼ˆ2ï¼‰åŒºåŸŸå…¨å±€å›¾åƒæ–‡æœ¬åŒ¹é…ï¼ˆRG-ITMï¼‰ï¼Œæ‘’å¼ƒäº†åˆšæ€§çº¦æŸï¼Œè½¬è€Œè¯„ä¼°å…¨å±€è·¨æ¨¡æ€è¡¨ç¤ºä¸­çš„å±€éƒ¨è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¢å¼ºäº†ç»„åˆæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ— äººæœºæ–‡æœ¬æè¿°å¾€å¾€ä¸å®Œæ•´æˆ–æ¨¡ç³Šï¼Œå¯¼è‡´å¯¹é½ä¸ç¨³å®šã€‚HCCMå¼•å…¥äº†ä¸€ç§åŠ¨é‡å¯¹æ¯”å’Œè’¸é¦ï¼ˆMCDï¼‰æœºåˆ¶ï¼Œæé«˜äº†ç¨³å¥æ€§ã€‚åœ¨GeoText-1 652ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHCCMå®ç°äº†æœ€å…ˆè¿›çš„Recall@1æŒ‡æ ‡ï¼Œå›¾åƒæ£€ç´¢ä¸º28.8%ï¼Œæ–‡æœ¬æ£€ç´¢ä¸º14.7%ã€‚åœ¨æœªè§è¿‡çš„ERAæ•°æ®é›†ä¸Šï¼ŒHCCMè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹³å‡å¬å›ç‡ä¸º39.93%ï¼ˆmRï¼‰ï¼Œè¶…è¿‡äº†å¾®è°ƒåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21539v1">PDF</a> Accepted by ACM MMâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºHierarchical Cross-Granularity Contrastive and Matching learningï¼ˆHCCMï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªç„¶è¯­è¨€å¼•å¯¼æ— äººæœºåœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªç»„ä»¶ï¼šRegion-Global Image-Text Contrastive Learningï¼ˆRG-ITCï¼‰å’ŒRegion-Global Image-Text Matchingï¼ˆRG-ITMï¼‰ã€‚RG-ITCé€šè¿‡å¯¹æ¯”å±€éƒ¨è§†è§‰åŒºåŸŸä¸å…¨å±€æ–‡æœ¬ï¼Œæ•æ‰å±‚æ¬¡åŒ–çš„å±€éƒ¨åˆ°å…¨å±€è¯­ä¹‰ï¼›RG-ITMåˆ™è¯„ä¼°å±€éƒ¨è¯­ä¹‰ä¸€è‡´æ€§åœ¨å…¨å±€è·¨æ¨¡æ€è¡¨ç¤ºä¸­çš„è¡¨ç°ï¼Œå¢å¼ºç»„åˆæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†Momentum Contrast and Distillationï¼ˆMCDï¼‰æœºåˆ¶æé«˜æ— äººæœºæ–‡æœ¬æè¿°çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHCCMæ¡†æ¶åœ¨GeoText-1652æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„å¬å›ç‡ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„ERAæ•°æ®é›†ä¸Šå±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NLGDï¼ˆè‡ªç„¶è¯­è¨€å¼•å¯¼æ— äººæœºï¼‰åœ¨ç›®æ ‡åŒ¹é…å’Œå¯¼èˆªç­‰ä»»åŠ¡ä¸­ä»£è¡¨äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œä½†é¢ä¸´ç€è§†é‡å¹¿é˜”å’Œåœºæ™¯è¯­ä¹‰å¤æ‚åº¦çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸»æµVision-Language Modelsï¼ˆVLMsï¼‰å¼ºè°ƒå…¨å±€å¯¹é½ï¼Œä½†ç¼ºä¹ç²¾ç»†çš„è¯­ä¹‰ç²’åº¦ï¼Œè€Œç°æœ‰çš„åˆ†å±‚æ–¹æ³•ä¾èµ–äºç²¾ç¡®å®ä½“åˆ†åŒºå’Œä¸¥æ ¼åŒ…å«å…³ç³»ï¼Œè¿™åœ¨åŠ¨æ€ç¯å¢ƒä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>HCCMæ¡†æ¶é€šè¿‡Region-Global Image-Text Contrastive Learningï¼ˆRG-ITCï¼‰æ•æ‰å±‚æ¬¡åŒ–çš„å±€éƒ¨åˆ°å…¨å±€è¯­ä¹‰ï¼Œé¿å…ç²¾ç¡®åœºæ™¯åˆ†å‰²ã€‚</li>
<li>HCCMæ¡†æ¶ä¸­çš„Region-Global Image-Text Matchingï¼ˆRG-ITMï¼‰è¯„ä¼°å±€éƒ¨è¯­ä¹‰ä¸€è‡´æ€§åœ¨å…¨å±€è·¨æ¨¡æ€è¡¨ç¤ºä¸­çš„è¡¨ç°ï¼Œå¢å¼ºäº†ç»„åˆæ¨ç†èƒ½åŠ›ï¼Œå¹¶æ‘’å¼ƒäº†åˆšæ€§çº¦æŸã€‚</li>
<li>æ— äººæœºæ–‡æœ¬æè¿°å¸¸å¸¸ä¸å®Œæ•´æˆ–æ¨¡ç³Šï¼ŒHCCMæ¡†æ¶å¼•å…¥äº†Momentum Contrast and Distillationï¼ˆMCDï¼‰æœºåˆ¶æ¥æé«˜é²æ£’æ€§ã€‚</li>
<li>åœ¨GeoText-1652æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHCCMæ¡†æ¶å®ç°äº†å…ˆè¿›çš„å¬å›ç‡è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21539">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3eefbd83389967ea3b8fddb5fc6399ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0383ddb92aab8e115db5519756258f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9f5334d7cdd5efc93ba8109c6912e58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1363a7a84862fa32f99f8c9691960df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-725cab3204370be551b43e76b90c544a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Contrastive-Learning-through-Auxiliary-Branch-for-Video-Object-Detection"><a href="#Contrastive-Learning-through-Auxiliary-Branch-for-Video-Object-Detection" class="headerlink" title="Contrastive Learning through Auxiliary Branch for Video Object Detection"></a>Contrastive Learning through Auxiliary Branch for Video Object Detection</h2><p><strong>Authors:Lucas Rakotoarivony</strong></p>
<p>Video object detection is a challenging task because videos often suffer from image deterioration such as motion blur, occlusion, and deformable shapes, making it significantly more difficult than detecting objects in still images. Prior approaches have improved video object detection performance by employing feature aggregation and complex post-processing techniques, though at the cost of increased computational demands. To improve robustness to image degradation without additional computational load during inference, we introduce a straightforward yet effective Contrastive Learning through Auxiliary Branch (CLAB) method. First, we implement a constrastive auxiliary branch using a contrastive loss to enhance the feature representation capability of the video object detectorâ€™s backbone. Next, we propose a dynamic loss weighting strategy that emphasizes auxiliary feature learning early in training while gradually prioritizing the detection task as training converges. We validate our approach through comprehensive experiments and ablation studies, demonstrating consistent performance gains. Without bells and whistles, CLAB reaches a performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101, respectively, on the ImageNet VID dataset, thus achieving state-of-the-art performance for CNN-based models without requiring additional post-processing methods. </p>
<blockquote>
<p>è§†é¢‘ç›®æ ‡æ£€æµ‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºè§†é¢‘å¸¸å¸¸ä¼šå—åˆ°å›¾åƒé€€åŒ–é—®é¢˜çš„å½±å“ï¼Œå¦‚è¿åŠ¨æ¨¡ç³Šã€é®æŒ¡å’Œå¯å˜å½¢çŠ¶ï¼Œè¿™ä½¿å¾—è§†é¢‘ä¸­çš„ç›®æ ‡æ£€æµ‹æ¯”é™æ€å›¾åƒä¸­çš„æ£€æµ‹æ›´ä¸ºå›°éš¾ã€‚å…ˆå‰çš„æ–¹æ³•é€šè¿‡é‡‡ç”¨ç‰¹å¾èšåˆå’Œå¤æ‚çš„åå¤„ç†æŠ€æœ¯æé«˜äº†è§†é¢‘ç›®æ ‡æ£€æµ‹çš„æ€§èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿå¢åŠ äº†è®¡ç®—éœ€æ±‚ã€‚ä¸ºäº†æé«˜å¯¹å›¾åƒé€€åŒ–çš„é²æ£’æ€§ï¼ŒåŒæ—¶åœ¨ä¸å¢åŠ æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—è´Ÿè½½çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„é€šè¿‡è¾…åŠ©åˆ†æ”¯è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼ˆCLABï¼‰çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹æ¯”æŸå¤±å®ç°äº†ä¸€ä¸ªå¯¹æ¯”è¾…åŠ©åˆ†æ”¯ï¼Œä»¥å¢å¼ºè§†é¢‘ç›®æ ‡æ£€æµ‹å™¨ä¸»å¹²ç‰¹å¾è¡¨ç¤ºçš„èƒ½åŠ›ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€æŸå¤±æƒé‡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨è®­ç»ƒæ—©æœŸå¼ºè°ƒè¾…åŠ©ç‰¹å¾å­¦ä¹ ï¼Œéšç€è®­ç»ƒçš„æ”¶æ•›é€æ¸ä¼˜å…ˆè€ƒè™‘æ£€æµ‹ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„å®éªŒå’Œæ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æŒç»­çš„æ€§èƒ½æå‡ã€‚åœ¨ä¸ä½¿ç”¨ä»»ä½•é¢å¤–æŠ€å·§çš„æƒ…å†µä¸‹ï¼ŒCLABåœ¨ImageNet VIDæ•°æ®é›†ä¸Šè¾¾åˆ°äº†ä½¿ç”¨ResNet-101å’ŒResNeXt-101çš„84.0%å’Œ85.2%çš„mAPæ€§èƒ½ï¼Œä»è€Œåœ¨åŸºäºCNNçš„æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”æ— éœ€é¢å¤–çš„åå¤„ç†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20551v1">PDF</a> Accepted paper for ACIVS 2025</p>
<p><strong>Summary</strong><br>è§†é¢‘ç›®æ ‡æ£€æµ‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºè§†é¢‘ç»å¸¸å‡ºç°å›¾åƒé€€åŒ–é—®é¢˜ï¼Œå¦‚è¿åŠ¨æ¨¡ç³Šã€é®æŒ¡å’Œå¯å˜å½¢å½¢çŠ¶ã€‚ä¸ºæé«˜å¯¹å›¾åƒé€€åŒ–çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¸å¢åŠ æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—è´Ÿæ‹…ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¯¹æ¯”å­¦ä¹ è¾…åŠ©åˆ†æ”¯ï¼ˆCLABï¼‰æ–¹æ³•ã€‚é€šè¿‡å®ç°å¯¹æ¯”è¾…åŠ©åˆ†æ”¯å¹¶ä½¿ç”¨å¯¹æ¯”æŸå¤±å¢å¼ºè§†é¢‘ç›®æ ‡æ£€æµ‹å™¨ä¸»å¹²çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œå†æå‡ºåŠ¨æ€æŸå¤±æƒé‡ç­–ç•¥ï¼Œåœ¨è®­ç»ƒåˆæœŸå¼ºè°ƒè¾…åŠ©ç‰¹å¾å­¦ä¹ ï¼Œéšç€è®­ç»ƒæ”¶æ•›é€æ¸ä»¥æ£€æµ‹ä»»åŠ¡ä¸ºä¸»ã€‚å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°ç¨³å¥ï¼Œæ€§èƒ½æå‡æ˜æ˜¾ã€‚åœ¨ImageNet VIDæ•°æ®é›†ä¸Šï¼ŒCLABä½¿ç”¨ResNet-101å’ŒResNeXt-101åˆ†åˆ«è¾¾åˆ°äº†84.0%å’Œ85.2%çš„mAPï¼Œå®ç°äº†åŸºäºCNNæ¨¡å‹çš„æœ€æ–°æ€§èƒ½ï¼Œæ— éœ€é¢å¤–çš„åå¤„ç†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç›®æ ‡æ£€æµ‹é¢ä¸´å›¾åƒé€€åŒ–æŒ‘æˆ˜ï¼Œå¦‚è¿åŠ¨æ¨¡ç³Šã€é®æŒ¡å’Œå½¢çŠ¶å˜å½¢ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ è¾…åŠ©åˆ†æ”¯ï¼ˆCLABï¼‰æ–¹æ³•æ—¨åœ¨æé«˜è§†é¢‘ç›®æ ‡æ£€æµ‹æ€§èƒ½å¹¶åº”å¯¹å›¾åƒé€€åŒ–é—®é¢˜ã€‚</li>
<li>CLABæ–¹æ³•é€šè¿‡å®ç°å¯¹æ¯”è¾…åŠ©åˆ†æ”¯å¹¶ä½¿ç”¨å¯¹æ¯”æŸå¤±æ¥å¢å¼ºè§†é¢‘ç›®æ ‡æ£€æµ‹å™¨çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>åŠ¨æ€æŸå¤±æƒé‡ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³è¡¡äº†è¾…åŠ©ç‰¹å¾å­¦ä¹ ä¸æ£€æµ‹ä»»åŠ¡çš„ä¼˜å…ˆçº§ã€‚</li>
<li>CLABæ–¹æ³•åœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹æé«˜äº†è§†é¢‘ç›®æ ‡æ£€æµ‹çš„é²æ£’æ€§ã€‚</li>
<li>åœ¨ImageNet VIDæ•°æ®é›†ä¸Šï¼ŒCLABæ–¹æ³•å®ç°äº†æœ€æ–°æ€§èƒ½ï¼Œä½¿ç”¨ResNet-101å’ŒResNeXt-101è¾¾åˆ°è¾ƒé«˜çš„mAPå€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eef89b90e4f8393e7301aa21aebaaf13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8698a6c9de68c9ecaed852947fb9b616.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35a57636f10d4876dfa9e7ecc121de37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aadcf6e4c46198d9d74243b5509d408e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DNP-Guided-Contrastive-Reconstruction-with-a-Reverse-Distillation-Transformer-for-Medical-Anomaly-Detection"><a href="#DNP-Guided-Contrastive-Reconstruction-with-a-Reverse-Distillation-Transformer-for-Medical-Anomaly-Detection" class="headerlink" title="DNP-Guided Contrastive Reconstruction with a Reverse Distillation   Transformer for Medical Anomaly Detection"></a>DNP-Guided Contrastive Reconstruction with a Reverse Distillation   Transformer for Medical Anomaly Detection</h2><p><strong>Authors:Luhu Li, Bowen Lin, Mukhtiar Khan, Shujun Fu</strong></p>
<p>Anomaly detection in medical images is challenging due to limited annotations and a domain gap compared to natural images. Existing reconstruction methods often rely on frozen pre-trained encoders, which limits adaptation to domain-specific features and reduces localization accuracy. Prototype-based learning offers interpretability and clustering benefits but suffers from prototype collapse, where few prototypes dominate training, harming diversity and generalization. To address this, we propose a unified framework combining a trainable encoder with prototype-guided reconstruction and a novel Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum branch, enables stable domain-adaptive feature learning. A lightweight Prototype Extractor mines informative normal prototypes to guide the decoder via attention for precise reconstruction. Our loss enforces balanced prototype use through diversity constraints and per-prototype normalization, effectively preventing collapse. Experiments on multiple medical imaging benchmarks show significant improvements in representation quality and anomaly localization, outperforming prior methods. Visualizations and prototype assignment analyses further validate the effectiveness of our anti-collapse mechanism and enhanced interpretability. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä¸­çš„å¼‚å¸¸æ£€æµ‹é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŸå› å°±åœ¨äºä¸å¤©ç„¶å›¾åƒç›¸æ¯”ï¼ŒåŒ»å­¦å›¾åƒçš„æ³¨è§£æœ‰é™ä¸”é¢†åŸŸå·®è·è¾ƒå¤§ã€‚ç°æœ‰çš„é‡å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºå›ºå®šçš„é¢„è®­ç»ƒç¼–ç å™¨ï¼Œè¿™å°±é™åˆ¶äº†å…¶å¯¹ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾é€‚åº”ï¼Œå¹¶é™ä½äº†å®šä½ç²¾åº¦ã€‚åŸºäºåŸå‹çš„å­¦ä¹ æä¾›äº†å¯è§£é‡Šæ€§å’Œèšç±»ä¼˜åŠ¿ï¼Œä½†å´å­˜åœ¨åŸå‹åå¡Œçš„é—®é¢˜ï¼Œå°‘æ•°åŸå‹ä¼šä¸»å¯¼è®­ç»ƒè¿‡ç¨‹ï¼Œä»è€ŒæŸå®³å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå¯è®­ç»ƒç¼–ç å™¨ã€åŸå‹å¼•å¯¼é‡å»ºå’Œæ–°å‹å¤šæ ·æ€§æ„ŸçŸ¥å¯¹é½æŸå¤±çš„ç»Ÿä¸€æ¡†æ¶ã€‚å¯è®­ç»ƒçš„ç¼–ç å™¨é€šè¿‡åŠ¨é‡åˆ†æ”¯å¢å¼ºåŠŸèƒ½ï¼Œå¯å®ç°ç¨³å®šçš„åŸŸè‡ªé€‚åº”ç‰¹å¾å­¦ä¹ ã€‚è½»é‡çº§åŸå‹æå–å™¨èƒ½å¤ŸæŒ–æ˜å‡ºä¿¡æ¯ä¸°å¯Œçš„æ­£å¸¸åŸå‹ï¼Œé€šè¿‡æ³¨æ„åŠ›å¼•å¯¼è§£ç å™¨è¿›è¡Œç²¾ç¡®é‡å»ºã€‚æˆ‘ä»¬çš„æŸå¤±é€šè¿‡å¤šæ ·æ€§å’Œæ¯ä¸ªåŸå‹çš„å½’ä¸€åŒ–å¼ºåˆ¶å¹³è¡¡åŸå‹çš„ä½¿ç”¨ï¼Œæœ‰æ•ˆåœ°é˜²æ­¢äº†åŸå‹çš„åå¡Œã€‚åœ¨å¤šä¸ªåŒ»å­¦å½±åƒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¡¨ç¤ºè´¨é‡å’Œå¼‚å¸¸å®šä½æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚å¯è§†åŒ–å’ŒåŸå‹åˆ†é…åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬é˜²åå¡Œæœºåˆ¶å’Œå¢å¼ºå¯è§£é‡Šæ€§çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19573v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†åŒ»å­¦å›¾åƒå¼‚å¸¸æ£€æµ‹ä¸­çš„æ ‡æ³¨æœ‰é™å’Œé¢†åŸŸå·®å¼‚é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰é‡å»ºæ–¹æ³•ä¾èµ–å†»ç»“é¢„è®­ç»ƒç¼–ç å™¨çš„é—®é¢˜ï¼Œæå‡ºç»“åˆå¯è®­ç»ƒç¼–ç å™¨ã€åŸå‹å¼•å¯¼é‡å»ºå’Œæ–°å‹å¤šæ ·æ€§æ„ŸçŸ¥å¯¹é½æŸå¤±çš„ç»Ÿä¸€æ¡†æ¶ã€‚å¯è®­ç»ƒç¼–ç å™¨é€šè¿‡åŠ¨é‡åˆ†æ”¯å¢å¼ºï¼Œå®ç°ç¨³å®šçš„é¢†åŸŸè‡ªé€‚åº”ç‰¹å¾å­¦ä¹ ã€‚è½»é‡çº§åŸå‹æå–å™¨æŒ–æ˜æ­£å¸¸åŸå‹ä¿¡æ¯ï¼Œé€šè¿‡æ³¨æ„åŠ›å¼•å¯¼è§£ç å™¨è¿›è¡Œç²¾ç¡®é‡å»ºã€‚æŸå¤±å‡½æ•°é€šè¿‡å¤šæ ·æ€§å’Œæ¯ä¸ªåŸå‹çš„å½’ä¸€åŒ–å¼ºåˆ¶å¹³è¡¡åŸå‹ä½¿ç”¨ï¼Œæœ‰æ•ˆé˜²æ­¢åŸå‹å´©æºƒã€‚åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¡¨ç¤ºè´¨é‡å’Œå¼‚å¸¸å®šä½æ–¹é¢æ˜¾è‘—æé«˜ï¼Œä¼˜äºå…ˆå‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå¼‚å¸¸æ£€æµ‹é¢ä¸´æ ‡æ³¨æœ‰é™å’Œé¢†åŸŸå·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰é‡å»ºæ–¹æ³•ä¾èµ–å†»ç»“çš„é¢„è®­ç»ƒç¼–ç å™¨ï¼Œé™åˆ¶äº†é¢†åŸŸç‰¹å®šç‰¹å¾çš„é€‚åº”å’Œå®šä½ç²¾åº¦ã€‚</li>
<li>æå‡ºç»“åˆå¯è®­ç»ƒç¼–ç å™¨ã€åŸå‹å¼•å¯¼é‡å»ºå’Œå¤šæ ·æ€§æ„ŸçŸ¥å¯¹é½æŸå¤±çš„ç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>å¯è®­ç»ƒç¼–ç å™¨é€šè¿‡åŠ¨é‡åˆ†æ”¯å¢å¼ºï¼Œå®ç°ç¨³å®šçš„é¢†åŸŸè‡ªé€‚åº”ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>åŸå‹æå–å™¨æŒ–æ˜æ­£å¸¸åŸå‹ä¿¡æ¯ï¼Œé€šè¿‡æ³¨æ„åŠ›å¼•å¯¼ç²¾ç¡®é‡å»ºã€‚</li>
<li>æŸå¤±å‡½æ•°è®¾è®¡å¼ºåˆ¶å¹³è¡¡åŸå‹ä½¿ç”¨ï¼Œæœ‰æ•ˆé˜²æ­¢åŸå‹å´©æºƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb689f8d0b096a2efd86191c1cfb10a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-656f1173d85aa59c1e9d414af449abf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5751844a163d82df964fc55ed42060ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62b33d42460cba7052723c7a18fcfdfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba9ac90d53a23cc659bd10968c26851e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ModAn-MulSupCon-Modality-and-Anatomy-Aware-Multi-Label-Supervised-Contrastive-Pretraining-for-Medical-Imaging"><a href="#ModAn-MulSupCon-Modality-and-Anatomy-Aware-Multi-Label-Supervised-Contrastive-Pretraining-for-Medical-Imaging" class="headerlink" title="ModAn-MulSupCon: Modality-and Anatomy-Aware Multi-Label Supervised   Contrastive Pretraining for Medical Imaging"></a>ModAn-MulSupCon: Modality-and Anatomy-Aware Multi-Label Supervised   Contrastive Pretraining for Medical Imaging</h2><p><strong>Authors:Eichi Takaya, Ryusei Inamori</strong></p>
<p>Background and objective: Expert annotations limit large-scale supervised pretraining in medical imaging, while ubiquitous metadata (modality, anatomical region) remain underused. We introduce ModAn-MulSupCon, a modality- and anatomy-aware multi-label supervised contrastive pretraining method that leverages such metadata to learn transferable representations.   Method: Each imageâ€™s modality and anatomy are encoded as a multi-hot vector. A ResNet-18 encoder is pretrained on a mini subset of RadImageNet (miniRIN, 16,222 images) with a Jaccard-weighted multi-label supervised contrastive loss, and then evaluated by fine-tuning and linear probing on three binary classification tasksâ€“ACL tear (knee MRI), lesion malignancy (breast ultrasound), and nodule malignancy (thyroid ultrasound).   Result: With fine-tuning, ModAn-MulSupCon achieved the best AUC on MRNet-ACL (0.964) and Thyroid (0.763), surpassing all baselines ($p&lt;0.05$), and ranked second on Breast (0.926) behind SimCLR (0.940; not significant). With the encoder frozen, SimCLR&#x2F;ImageNet were superior, indicating that ModAn-MulSupCon representations benefit most from task adaptation rather than linear separability.   Conclusion: Encoding readily available modality&#x2F;anatomy metadata as multi-label targets provides a practical, scalable pretraining signal that improves downstream accuracy when fine-tuning is feasible. ModAn-MulSupCon is a strong initialization for label-scarce clinical settings, whereas SimCLR&#x2F;ImageNet remain preferable for frozen-encoder deployments. </p>
<blockquote>
<p>èƒŒæ™¯ä¸ç›®æ ‡ï¼šä¸“å®¶æ³¨é‡Šé™åˆ¶äº†åŒ»å­¦å½±åƒçš„å¤§è§„æ¨¡æœ‰ç›‘ç£é¢„è®­ç»ƒï¼Œè€Œæ™®éå­˜åœ¨çš„å…ƒæ•°æ®ï¼ˆæ¨¡æ€ã€è§£å‰–éƒ¨ä½ï¼‰å´è¢«ä½¿ç”¨ä¸è¶³ã€‚æˆ‘ä»¬å¼•å…¥äº†ModAn-MulSupConï¼Œè¿™æ˜¯ä¸€ç§æ¨¡æ€å’Œè§£å‰–ç»“æ„æ„ŸçŸ¥çš„å¤šæ ‡ç­¾ç›‘ç£å¯¹æ¯”é¢„è®­ç»ƒæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ­¤ç±»å…ƒæ•°æ®æ¥å­¦ä¹ å¯è¿ç§»çš„è¡¨ç¤ºã€‚æ–¹æ³•ï¼šå°†æ¯å¼ å›¾åƒçš„æ¨¡æ€å’Œè§£å‰–ç»“æ„ç¼–ç ä¸ºå¤šçƒ­å‘é‡ã€‚ä½¿ç”¨JaccardåŠ æƒå¤šæ ‡ç­¾ç›‘ç£å¯¹æ¯”æŸå¤±åœ¨RadImageNetçš„ä¸€ä¸ªå°å­é›†ï¼ˆminiRINï¼ŒåŒ…å«16,222å¼ å›¾åƒï¼‰ä¸Šå¯¹ResNet-18ç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡å¾®è°ƒçº¿æ€§æ¢æµ‹åœ¨ä¸‰é¡¹äºŒåˆ†ç±»ä»»åŠ¡ï¼ˆè†å…³èŠ‚MRIçš„ACLæ’•è£‚ã€ä¹³è…ºè¶…å£°çš„ç—…å˜æ¶æ€§ç¨‹åº¦ä»¥åŠç”²çŠ¶è…ºè¶…å£°çš„ç»“èŠ‚æ¶æ€§ç¨‹åº¦ï¼‰ä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚ç»“æœï¼šé€šè¿‡å¾®è°ƒï¼ŒModAn-MulSupConåœ¨MRNet-ACLï¼ˆAUCä¸º0.964ï¼‰å’Œç”²çŠ¶è…ºï¼ˆAUCä¸º0.763ï¼‰ä¸Šçš„è¡¨ç°æœ€ä½³ï¼Œè¶…è¿‡äº†æ‰€æœ‰åŸºçº¿ï¼ˆp&lt;0.05ï¼‰ï¼Œå¹¶åœ¨ä¹³è…ºï¼ˆAUCä¸º0.926ï¼‰ä¸Šæ’åç¬¬äºŒï¼Œä»…æ¬¡äºSimCLRï¼ˆAUCä¸º0.940ï¼Œæ— æ˜¾è‘—å·®å¼‚ï¼‰ã€‚å½“ç¼–ç å™¨å†»ç»“æ—¶ï¼ŒSimCLR&#x2F;ImageNetè¡¨ç°æ›´ä½³ï¼Œè¿™è¡¨æ˜ModAn-MulSupConçš„è¡¨ç¤ºå½¢å¼æ›´å—ç›Šäºä»»åŠ¡é€‚åº”æ€§è€Œéçº¿æ€§å¯åˆ†æ€§ã€‚ç»“è®ºï¼šå°†æ˜“äºè·å¾—çš„æ¨¡æ€&#x2F;è§£å‰–ç»“æ„å…ƒæ•°æ®ç¼–ç ä¸ºå¤šæ ‡ç­¾ç›®æ ‡æä¾›äº†ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„é¢„è®­ç»ƒä¿¡å·ï¼Œåœ¨å¾®è°ƒå¯è¡Œçš„æƒ…å†µä¸‹å¯æé«˜ä¸‹æ¸¸å‡†ç¡®æ€§ã€‚ModAn-MulSupConæ˜¯æ ‡ç­¾ç¨€ç¼ºçš„ä¸´åºŠç¯å¢ƒä¸­çš„å¼ºå¤§åˆå§‹åŒ–æ–¹æ³•ï¼Œè€ŒSimCLR&#x2F;ImageNetå¯¹äºå†»ç»“ç¼–ç å™¨éƒ¨ç½²æ›´ä¸ºå¯å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18613v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ModAn-MulSupConæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ¨¡æ€å’Œè§£å‰–ç»“æ„ä¿¡æ¯çš„æœ‰ç›‘ç£å¯¹æ¯”é¢„è®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†å›¾åƒçš„æ¨¡æ€å’Œè§£å‰–ç»“æ„ç¼–ç ä¸ºå¤šçƒ­å‘é‡ï¼Œå¹¶ä½¿ç”¨JaccardåŠ æƒå¤šæ ‡ç­¾ç›‘ç£å¯¹æ¯”æŸå¤±åœ¨RadImageNetçš„ä¸€ä¸ªå°å­é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚é€šè¿‡å¾®è°ƒï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³AUCå€¼ã€‚ç»“è®ºè¡¨æ˜ï¼Œå°†æ˜“äºè·å¾—çš„æ¨¡æ€&#x2F;è§£å‰–ç»“æ„ä¿¡æ¯ç¼–ç ä¸ºå¤šæ ‡ç­¾ç›®æ ‡ï¼Œæä¾›äº†ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„é¢„è®­ç»ƒä¿¡å·ï¼Œåœ¨å¾®è°ƒå¯è¡Œçš„æƒ…å†µä¸‹ï¼Œèƒ½æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ModAn-MulSupConæ˜¯ä¸€ç§åˆ©ç”¨æ¨¡æ€å’Œè§£å‰–ç»“æ„ä¿¡æ¯çš„æœ‰ç›‘ç£å¯¹æ¯”é¢„è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¤šçƒ­å‘é‡ç¼–ç å›¾åƒçš„æ¨¡æ€å’Œè§£å‰–ç»“æ„ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨JaccardåŠ æƒå¤šæ ‡ç­¾ç›‘ç£å¯¹æ¯”æŸå¤±åœ¨RadImageNetçš„ä¸€ä¸ªå°å­é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>é€šè¿‡å¾®è°ƒï¼ŒModAn-MulSupConåœ¨ä¸‰ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³AUCå€¼ã€‚</li>
<li>ModAn-MulSupConåœ¨æ ‡ç­¾ç¨€ç¼ºçš„ä¸´åºŠç¯å¢ƒä¸­æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åˆå§‹åŒ–æ–¹æ³•ã€‚</li>
<li>åœ¨å†»ç»“ç¼–ç å™¨éƒ¨ç½²æ—¶ï¼ŒSimCLR&#x2F;ImageNetä»ç„¶æ˜¯æ›´ç†æƒ³çš„é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c7a291feca0e9d731b1e77bd7ac5b3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c83ecb71c4a66261d74523f1a67c0fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4af56237f06b48dfd9fb28bcb0daa09f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c14de5b2569ae23c6bea9162a25d5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bee7aab2b18738c98f3897a63a063a62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c194da6f5571b99e0579d0c684ec6c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolution"><a href="#CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolution" class="headerlink" title="CATformer: Contrastive Adversarial Transformer for Image   Super-Resolution"></a>CATformer: Contrastive Adversarial Transformer for Image   Super-Resolution</h2><p><strong>Authors:Qinyi Tian, Spence Cox, Laura E. Dalton</strong></p>
<p>Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired transformer, which progressively refines latent representations, with an auxiliary transformer branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and decoded using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among transformer-, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired transformers in super-resolution. </p>
<blockquote>
<p>è¶…åˆ†è¾¨ç‡æŠ€æœ¯ä¾ç„¶æ˜¯ä¸€ç§å¯ä»¥æå‡ä½åˆ†è¾¨ç‡å›¾åƒè´¨é‡çš„é¢‡å…·å‰æ™¯çš„æŠ€æœ¯ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†CATformerï¼ˆå¯¹æ¯”å¯¹æŠ—æ€§è½¬æ¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œï¼Œèåˆäº†æ‰©æ•£å¯å‘ç‰¹å¾ç»†åŒ–ã€å¯¹æŠ—æ€§å­¦ä¹ ä¸å¯¹æ¯”å­¦ä¹ ã€‚CATformeré‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œç»“åˆä¸»è¦æ‰©æ•£å¯å‘è½¬æ¢å™¨ï¼Œé€æ­¥ä¼˜åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œä»¥åŠä¸€ä¸ªè¾…åŠ©è½¬æ¢å™¨åˆ†æ”¯ï¼Œé€šè¿‡å­¦ä¹ æ½œåœ¨å¯¹æ¯”å¢å¼ºå¯¹å™ªå£°çš„é²æ£’æ€§ã€‚è¿™äº›äº’è¡¥è¡¨ç¤ºé€šè¿‡æ·±åº¦æ®‹å·®æ®‹å·®å¯†é›†å—è¿›è¡Œèåˆå’Œè§£ç ï¼Œä»¥æé«˜é‡å»ºè´¨é‡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCATformeråœ¨æ•ˆç‡å’Œè§†è§‰å›¾åƒè´¨é‡æ–¹é¢è¶…è¶Šäº†æœ€è¿‘çš„åŸºäºè½¬æ¢å™¨å’Œæ‰©æ•£çš„æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œç¼©å°äº†åŸºäºè½¬æ¢å™¨ã€æ‰©æ•£å’ŒGANçš„æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºæ‰©æ•£å¯å‘è½¬æ¢å™¨çš„å®é™…åº”ç”¨åœ¨è¶…åˆ†è¾¨ç‡é¢†åŸŸå¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17708v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CATformeræ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œï¼Œç»“åˆäº†æ‰©æ•£å¯å‘ç‰¹å¾ä¼˜åŒ–ã€å¯¹æŠ—å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ ï¼Œç”¨äºæé«˜ä½åˆ†è¾¨ç‡å›¾åƒçš„è¶…åˆ†è¾¨ç‡ã€‚å®ƒé‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œä¸»åˆ†æ”¯ä¸ºæ‰©æ•£å¯å‘å˜å‹å™¨ï¼Œç”¨äºé€æ­¥ä¼˜åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œè¾…åŠ©åˆ†æ”¯ä¸ºå¢å¼ºå™ªå£°é²æ£’æ€§çš„å¯¹æ¯”è¾…åŠ©å˜å‹å™¨ã€‚ä¸¤è€…èåˆå¹¶ä½¿ç”¨æ·±åº¦Residual-in-Residual Dense Blocksè¿›è¡Œè§£ç ï¼Œä»¥æé«˜é‡å»ºè´¨é‡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCATformeråœ¨æ•ˆç‡å’Œè§†è§‰å›¾åƒè´¨é‡æ–¹é¢ä¼˜äºæœ€æ–°çš„åŸºäºå˜å‹å™¨å’Œæ‰©æ•£çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CATformeræ˜¯ä¸€ç§ç»“åˆæ‰©æ•£å¯å‘ç‰¹å¾ä¼˜åŒ–ã€å¯¹æŠ—å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ çš„æ–°å‹ç¥ç»ç½‘ç»œã€‚</li>
<li>é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œä¸»åˆ†æ”¯ç”¨äºä¼˜åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œè¾…åŠ©åˆ†æ”¯å¢å¼ºå™ªå£°é²æ£’æ€§ã€‚</li>
<li>é€šè¿‡ç»“åˆè¿™ä¸¤ç§è¡¨ç¤ºå¹¶ä½¿ç”¨æ·±åº¦Residual-in-Residual Dense Blocksè§£ç ï¼Œæé«˜å›¾åƒé‡å»ºè´¨é‡ã€‚</li>
<li>CATformeråœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–åŸºäºå˜å‹å™¨å’Œæ‰©æ•£çš„æ–¹æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶ç¼©å°äº†åŸºäºå˜å‹å™¨ã€æ‰©æ•£å’ŒGANçš„æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</li>
<li>CATformerä¸ºæ‰©æ•£å¯å‘å˜å‹å™¨åœ¨è¶…åˆ†è¾¨ç‡å®é™…åº”ç”¨ä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17708">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a98965b00ed9e65b5db306e9c4661723.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31ae8de490f71b23e3af54dcbc851cd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffcb7da9a2d9366c4dc6b83d57058c2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83842ff9321d1bf83aac6cf72964c1cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c6861ee19e892a19ee335d5bedf357d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Contrastive Prompt Clustering for Weakly Supervised Semantic   Segmentation"></a>Contrastive Prompt Clustering for Weakly Supervised Semantic   Segmentation</h2><p><strong>Authors:Wangyu Wu, Zhenhong Chen, Xiaowen Ma, Wenqiao Zhang, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained attention for its cost-effectiveness. Most existing methods emphasize inter-class separation, often neglecting the shared semantics among related categories and lacking fine-grained discrimination. To address this, we propose Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large Language Models (LLMs) to derive category clusters that encode intrinsic inter-class relationships, and further introduces a class-aware patch-level contrastive loss to enforce intra-class consistency and inter-class separation. This hierarchical design leverages clusters as coarse-grained semantic priors while preserving fine-grained boundaries, thereby reducing confusion among visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014 demonstrate that CPC surpasses existing state-of-the-art methods in WSSS. </p>
<blockquote>
<p>åˆ©ç”¨å›¾åƒçº§æ ‡ç­¾è¿›è¡Œå¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰å› å…¶æˆæœ¬æ•ˆç›Šè€Œå—åˆ°å…³æ³¨ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•å¼ºè°ƒç±»é—´åˆ†ç¦»ï¼Œå¾€å¾€å¿½ç•¥äº†ç›¸å…³ç±»åˆ«ä¹‹é—´çš„å…±äº«è¯­ä¹‰ï¼Œä¸”ç¼ºä¹ç²¾ç»†çš„ç²’åº¦é‰´åˆ«ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹æ¯”æç¤ºèšç±»ï¼ˆCPCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„WSSSæ¡†æ¶ã€‚CPCåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ¨å¯¼ç¼–ç å†…åœ¨ç±»é—´å…³ç³»çš„ç±»åˆ«èšç±»ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§ç±»æ„ŸçŸ¥çš„è¡¥ä¸çº§å¯¹æ¯”æŸå¤±ï¼Œä»¥åŠ å¼ºç±»å†…ä¸€è‡´æ€§å’Œç±»é—´åˆ†ç¦»ã€‚è¿™ç§å±‚æ¬¡åŒ–çš„è®¾è®¡åˆ©ç”¨èšç±»ä½œä¸ºç²—ç²’åº¦çš„è¯­ä¹‰å…ˆéªŒï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†çš„è¾¹ç•Œï¼Œä»è€Œå‡å°‘è§†è§‰ä¸Šç›¸ä¼¼ç±»åˆ«ä¹‹é—´çš„æ··æ·†ã€‚åœ¨PASCAL VOC 2012å’ŒMS COCO 2014ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCPCåœ¨WSSSä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17009v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰çš„æ–°æ¡†æ¶â€”â€”å¯¹æ¯”æç¤ºèšç±»ï¼ˆCPCï¼‰ã€‚CPCåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨å¯¼ç±»åˆ«èšç±»ï¼Œç¼–ç ç±»åˆ«é—´çš„å†…åœ¨å…³ç³»ï¼Œå¹¶å¼•å…¥ç±»æ„ŸçŸ¥è¡¥ä¸çº§å¯¹æ¯”æŸå¤±ï¼Œä»¥åŠ å¼ºç±»å†…ä¸€è‡´æ€§å’Œç±»é—´åˆ†ç¦»ã€‚è¿™ç§å±‚æ¬¡è®¾è®¡åˆ©ç”¨èšç±»ä½œä¸ºç²—ç²’åº¦è¯­ä¹‰å…ˆéªŒï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†è¾¹ç•Œï¼Œå‡å°‘è§†è§‰ç›¸ä¼¼ç±»åˆ«ä¹‹é—´çš„æ··æ·†ã€‚å®éªŒè¡¨æ˜ï¼ŒCPCåœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„WSSSå…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰å› æˆæœ¬æ•ˆç›Šè€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰WSSSæ–¹æ³•å¾€å¾€å¿½è§†ç›¸å…³ç±»åˆ«ä¹‹é—´çš„å…±äº«è¯­ä¹‰ï¼Œç¼ºä¹ç²¾ç»†ç²’åº¦é‰´åˆ«ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„WSSSæ¡†æ¶â€”â€”å¯¹æ¯”æç¤ºèšç±»ï¼ˆCPCï¼‰ã€‚</li>
<li>CPCåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨å¯¼ç±»åˆ«èšç±»ï¼Œç¼–ç ç±»åˆ«é—´çš„å†…åœ¨å…³ç³»ã€‚</li>
<li>CPCå¼•å…¥ç±»æ„ŸçŸ¥è¡¥ä¸çº§å¯¹æ¯”æŸå¤±ï¼Œä»¥åŠ å¼ºç±»å†…ä¸€è‡´æ€§å’Œç±»é—´åˆ†ç¦»ã€‚</li>
<li>å±‚æ¬¡è®¾è®¡åˆ©ç”¨èšç±»ä½œä¸ºç²—ç²’åº¦è¯­ä¹‰å…ˆéªŒï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†è¾¹ç•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf40a7b0c676728135b697cc20a1e1bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc03bf19f4621d2044bc2d96e3a20412.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learning"><a href="#Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learning" class="headerlink" title="Multimodal Medical Endoscopic Image Analysis via Progressive   Disentangle-aware Contrastive Learning"></a>Multimodal Medical Endoscopic Image Analysis via Progressive   Disentangle-aware Contrastive Learning</h2><p><strong>Authors:Junhao Wu, Yun Li, Junhao Li, Jingliang Bian, Xiaomao Fan, Wenbin Lei, Ruxin Wang</strong></p>
<p>Accurate segmentation of laryngo-pharyngeal tumors is crucial for precise diagnosis and effective treatment planning. However, traditional single-modality imaging methods often fall short of capturing the complex anatomical and pathological features of these tumors. In this study, we present an innovative multi-modality representation learning framework based on the &#96;Align-Disentangle-Fusionâ€™ mechanism that seamlessly integrates 2D White Light Imaging (WLI) and Narrow Band Imaging (NBI) pairs to enhance segmentation performance. A cornerstone of our approach is multi-scale distribution alignment, which mitigates modality discrepancies by aligning features across multiple transformer layers. Furthermore, a progressive feature disentanglement strategy is developed with the designed preliminary disentanglement and disentangle-aware contrastive learning to effectively separate modality-specific and shared features, enabling robust multimodal contrastive learning and efficient semantic fusion. Comprehensive experiments on multiple datasets demonstrate that our method consistently outperforms state-of-the-art approaches, achieving superior accuracy across diverse real clinical scenarios. </p>
<blockquote>
<p>å–‰å’½è‚¿ç˜¤å‡†ç¡®åˆ†å‰²å¯¹äºç²¾ç¡®è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å•ä¸€æˆåƒæ–¹æ³•å¾€å¾€éš¾ä»¥æ•æ‰è¿™äº›è‚¿ç˜¤çš„å¤æ‚è§£å‰–å’Œç—…ç†ç‰¹å¾ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºâ€œå¯¹é½-åˆ†ç¦»-èåˆâ€æœºåˆ¶çš„åˆ›æ–°å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæ— ç¼é›†æˆäº†2Dç™½å…‰æˆåƒï¼ˆWLIï¼‰å’Œçª„å¸¦æˆåƒï¼ˆNBIï¼‰é…å¯¹ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å¤šå°ºåº¦åˆ†å¸ƒå¯¹é½ï¼Œå®ƒé€šè¿‡å¯¹é½å¤šä¸ªtransformerå±‚ä¸­çš„ç‰¹å¾æ¥å‡è½»æ¨¡æ€å·®å¼‚ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ç§æ¸è¿›çš„ç‰¹å¾åˆ†ç¦»ç­–ç•¥ï¼Œé€šè¿‡è®¾è®¡åˆæ­¥åˆ†ç¦»å’Œè§£çº ç¼ æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°åˆ†ç¦»äº†æ¨¡æ€ç‰¹å®šç‰¹å¾å’Œå…±äº«ç‰¹å¾ï¼Œå®ç°äº†ç¨³å¥çš„å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ å’Œé«˜æ•ˆçš„è¯­ä¹‰èåˆã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸€ç›´ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶åœ¨å¤šç§å®é™…ä¸´åºŠåœºæ™¯ä¸­å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16882v1">PDF</a> 12 pages,6 figures, 6 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºâ€œå¯¹é½-åˆ†ç¦»-èåˆâ€æœºåˆ¶çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— ç¼é›†æˆäº†2Dç™½å…‰æˆåƒï¼ˆWLIï¼‰å’Œçª„å¸¦æˆåƒï¼ˆNBIï¼‰é…å¯¹ï¼Œä»¥æé«˜å–‰å’½è‚¿ç˜¤åˆ†å‰²çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å¤šå°ºåº¦åˆ†å¸ƒå¯¹é½ï¼Œé€šè¿‡å¯¹é½å¤šå±‚transformerçš„ç‰¹å¾æ¥å‡å°‘æ¨¡æ€å·®å¼‚ã€‚åŒæ—¶ï¼Œå¼€å‘äº†ä¸€ç§æ¸è¿›å¼ç‰¹å¾åˆ†ç¦»ç­–ç•¥ï¼Œé€šè¿‡åˆæ­¥åˆ†ç¦»å’Œåˆ†ç¦»æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°åˆ†ç¦»äº†æ¨¡æ€ç‰¹å®šå’Œå…±äº«ç‰¹å¾ï¼Œå®ç°äº†ç¨³å¥çš„å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ å’Œé«˜æ•ˆçš„è¯­ä¹‰èåˆã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸€è‡´ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œåœ¨å¤šç§çœŸå®ä¸´åºŠåœºæ™¯ä¸‹å…·æœ‰å“è¶Šå‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„æ¡†æ¶ï¼Œé›†æˆäº†2Dç™½å…‰æˆåƒå’Œçª„å¸¦æˆåƒï¼Œä»¥æé«˜å–‰å’½è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å¤šå°ºåº¦åˆ†å¸ƒå¯¹é½æ˜¯è¯¥æ–¹æ³•çš„æ ¸å¿ƒï¼Œé€šè¿‡å¯¹é½å¤šå±‚transformerçš„ç‰¹å¾æ¥å‡å°‘æ¨¡æ€å·®å¼‚ã€‚</li>
<li>é‡‡ç”¨äº†æ¸è¿›å¼ç‰¹å¾åˆ†ç¦»ç­–ç•¥ï¼Œé€šè¿‡åˆæ­¥åˆ†ç¦»å’Œåˆ†ç¦»æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ æœ‰æ•ˆåˆ†ç¦»æ¨¡æ€ç‰¹å®šå’Œå…±äº«ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ç¨³å¥çš„å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ å’Œé«˜æ•ˆçš„è¯­ä¹‰èåˆã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§çœŸå®ä¸´åºŠåœºæ™¯ä¸‹å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¥½çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16882">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-63e3c2352f5743d0064ebe66876104f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0688f3c97906c94c21448d25ac776ebb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319276a72c9eb46a34d0671d7f1109f0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Predicting-brain-tumour-enhancement-from-non-contrast-MR-imaging-with-artificial-intelligence"><a href="#Predicting-brain-tumour-enhancement-from-non-contrast-MR-imaging-with-artificial-intelligence" class="headerlink" title="Predicting brain tumour enhancement from non-contrast MR imaging with   artificial intelligence"></a>Predicting brain tumour enhancement from non-contrast MR imaging with   artificial intelligence</h2><p><strong>Authors:James K Ruffle, Samia Mohinta, Guilherme Pombo, Asthik Biswas, Alan Campbell, Indran Davagnanam, David Doig, Ahmed Hamman, Harpreet Hyare, Farrah Jabeen, Emma Lim, Dermot Mallon, Stephanie Owen, Sophie Wilkinson, Sebastian Brandner, Parashkev Nachev</strong></p>
<p>Brain tumour imaging assessment typically requires both pre- and post-contrast MRI, but gadolinium administration is not always desirable, such as in frequent follow-up, renal impairment, allergy, or paediatric patients. We aimed to develop and validate a deep learning model capable of predicting brain tumour contrast enhancement from non-contrast MRI sequences alone. We assembled 11089 brain MRI studies from 10 international datasets spanning adult and paediatric populations with various neuro-oncological states, including glioma, meningioma, metastases, and post-resection appearances. Deep learning models (nnU-Net, SegResNet, SwinUNETR) were trained to predict and segment enhancing tumour using only non-contrast T1-, T2-, and T2&#x2F;FLAIR-weighted images. Performance was evaluated on 1109 held-out test patients using patient-level detection metrics and voxel-level segmentation accuracy. Model predictions were compared against 11 expert radiologists who each reviewed 100 randomly selected patients. The best-performing nnU-Net achieved 83% balanced accuracy, 91.5% sensitivity, and 74.4% specificity in detecting enhancing tumour. Enhancement volume predictions strongly correlated with ground truth (R2 0.859). The model outperformed expert radiologists, who achieved 69.8% accuracy, 75.9% sensitivity, and 64.7% specificity. 76.8% of test patients had Dice over 0.3 (acceptable detection), 67.5% had Dice over 0.5 (good detection), and 50.2% had Dice over 0.7 (excellent detection). Deep learning can identify contrast-enhancing brain tumours from non-contrast MRI with clinically relevant performance. These models show promise as screening tools and may reduce gadolinium dependence in neuro-oncology imaging. Future work should evaluate clinical utility alongside radiology experts. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤æˆåƒè¯„ä¼°é€šå¸¸éœ€è¦å¯¹æ¯”å‰‚å‰åçš„MRIï¼Œä½†åœ¨é¢‘ç¹éšè®¿ã€è‚¾åŠŸèƒ½å—æŸã€è¿‡æ•æˆ–å„¿ç§‘æ‚£è€…ä¸­ï¼Œé’†çš„ç»™è¯å¹¶ä¸æ€»æ˜¯ç†æƒ³çš„ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘å’ŒéªŒè¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»…ä»éå¯¹æ¯”MRIåºåˆ—ä¸­é¢„æµ‹è„‘è‚¿ç˜¤çš„å¯¹æ¯”å¢å¼ºæ•ˆæœã€‚æˆ‘ä»¬æ±‡é›†äº†æ¥è‡ªæˆäººå’Œå„¿ç§‘ç­‰ä¸åŒç¥ç»è‚¿ç˜¤çŠ¶æ€çš„å›½é™…æ•°æ®é›†ï¼ŒåŒ…æ‹¬èƒ¶è´¨ç˜¤ã€è„‘è†œç˜¤ã€è½¬ç§»ç˜¤å’Œæœ¯åè¡¨ç°ï¼Œå…±æ¶‰åŠ10ä¸ªæ•°æ®é›†çš„11089ä¸ªè„‘MRIç ”ç©¶ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆnnU-Netã€SegResNetã€SwinUNETRï¼‰ç»è¿‡è®­ç»ƒï¼Œä»…ä½¿ç”¨éå¯¹æ¯”T1ã€T2å’ŒT2 &#x2F; FLAIRåŠ æƒå›¾åƒæ¥é¢„æµ‹å’Œåˆ†å‰²å¢å¼ºè‚¿ç˜¤ã€‚åœ¨1109åç‹¬ç«‹æµ‹è¯•æ‚£è€…ä¸Šçš„æ€§èƒ½è¯„ä¼°é‡‡ç”¨äº†æ‚£è€…çº§åˆ«çš„æ£€æµ‹æŒ‡æ ‡å’Œä½“ç´ çº§åˆ«çš„åˆ†å‰²å‡†ç¡®æ€§ã€‚æ¨¡å‹é¢„æµ‹ä¸11ä½ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œäº†æ¯”è¾ƒï¼Œæ¯ä½æ”¾å°„ç§‘åŒ»ç”Ÿå‡å®¡æŸ¥äº†éšæœºé€‰æ‹©çš„100åæ‚£è€…ã€‚è¡¨ç°æœ€ä½³çš„nnU-Netåœ¨æ£€æµ‹å¢å¼ºè‚¿ç˜¤æ–¹é¢çš„å¹³è¡¡å‡†ç¡®åº¦ä¸º83%ï¼Œæ•æ„Ÿåº¦ä¸º91.5%ï¼Œç‰¹å¼‚åº¦ä¸º74.4%ã€‚å¢å¼ºä½“ç§¯é¢„æµ‹ä¸çœŸå®å€¼é«˜åº¦ç›¸å…³ï¼ˆRÂ² 0.859ï¼‰ã€‚è¯¥æ¨¡å‹çš„æ€§èƒ½ä¼˜äºä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œåè€…çš„å‡†ç¡®åº¦ä¸º69.8%ï¼Œæ•æ„Ÿåº¦ä¸º75.9%ï¼Œç‰¹å¼‚åº¦ä¸º64.7%ã€‚76.8%çš„æµ‹è¯•æ‚£è€…çš„Diceç³»æ•°å¤§äº0.3ï¼ˆå¯æ¥å—æ£€æµ‹ï¼‰ï¼Œ67.5%çš„Diceç³»æ•°å¤§äº0.5ï¼ˆè‰¯å¥½æ£€æµ‹ï¼‰ï¼Œ50.2%çš„Diceç³»æ•°å¤§äº0.7ï¼ˆä¼˜è‰¯æ£€æµ‹ï¼‰ã€‚æ·±åº¦å­¦ä¹ å¯ä»¥ä»éå¯¹æ¯”MRIä¸­è¯†åˆ«å‡ºå¯¹æ¯”å¢å¼ºçš„è„‘è‚¿ç˜¤ï¼Œå¹¶å…·æœ‰ä¸´åºŠæ„ä¹‰ã€‚è¿™äº›æ¨¡å‹ä½œä¸ºç­›æŸ¥å·¥å…·æ˜¾ç¤ºå‡ºä¸€å®šçš„å‰æ™¯ï¼Œå¹¶å¯èƒ½å‡å°‘ç¥ç»è‚¿ç˜¤å­¦æˆåƒä¸­å¯¹é’†çš„ä¾èµ–ã€‚æœªæ¥çš„å·¥ä½œåº”ä¸æ”¾å°„å­¦ä¸“å®¶ä¸€èµ·è¯„ä¼°å…¶ä¸´åºŠå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16650v1">PDF</a> 38 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºéå¯¹æ¯”MRIåºåˆ—é¢„æµ‹è„‘è‚¿ç˜¤å¯¹æ¯”å¢å¼ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ç ”ç©¶ã€‚è¯¥ç ”ç©¶åˆ©ç”¨å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚nnU-Netã€SegResNetå’ŒSwinUNETRï¼‰è®­ç»ƒæ¨¡å‹ï¼Œä»…ä½¿ç”¨éå¯¹æ¯”T1ã€T2å’ŒT2&#x2F;FLAIRåŠ æƒå›¾åƒæ¥é¢„æµ‹å’Œåˆ†å‰²å¢å¼ºè‚¿ç˜¤ã€‚æ¨¡å‹æ€§èƒ½é€šè¿‡æ‚£è€…çº§åˆ«çš„æ£€æµ‹æŒ‡æ ‡å’Œä½“ç´ çº§åˆ«çš„åˆ†å‰²ç²¾åº¦è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¯„ä¼°ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ£€æµ‹å¢å¼ºè‚¿ç˜¤æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶æœ‰æœ›ä½œä¸ºç­›æŸ¥å·¥å…·å‡å°‘ç¥ç»è‚¿ç˜¤å­¦ä¸­é’†çš„ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨å¼€å‘å¹¶éªŒè¯ä¸€ç§èƒ½å¤Ÿä»éå¯¹æ¯”MRIåºåˆ—é¢„æµ‹è„‘è‚¿ç˜¤å¯¹æ¯”å¢å¼ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨äº†å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ï¼Œæ•°æ®æ¥æºæ¶µç›–æˆäººå’Œå„¿ç«¥çš„ä¸åŒç¥ç»è‚¿ç˜¤çŠ¶æ€æ‚£è€…ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½é€šè¿‡ä¸¥æ ¼çš„æ‚£è€…çº§åˆ«å’Œä½“ç´ çº§åˆ«è¯„ä¼°æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>æœ€ä½³æ¨¡å‹nnU-Netåœ¨æ£€æµ‹å¢å¼ºè‚¿ç˜¤æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œä¸ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ¨¡å‹é¢„æµ‹ç»“æœä¸çœŸå®ç»“æœé«˜åº¦ç›¸å…³ï¼Œå¢å¼ºä½“ç§¯é¢„æµ‹ä¸çœŸå®å€¼çš„ç›¸å…³æ€§è¾¾åˆ°RÂ² 0.859ã€‚</li>
<li>è¯¥æ¨¡å‹æœ‰æœ›ä½œä¸ºç­›æŸ¥å·¥å…·ï¼Œå‡å°‘ç¥ç»è‚¿ç˜¤å­¦ä¸­å¯¹é’†çš„ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-53bdd080883fbdb356c7b57249e826b2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Visual-Perturbation-and-Adaptive-Hard-Negative-Contrastive-Learning-for-Compositional-Reasoning-in-Vision-Language-Models"><a href="#Visual-Perturbation-and-Adaptive-Hard-Negative-Contrastive-Learning-for-Compositional-Reasoning-in-Vision-Language-Models" class="headerlink" title="Visual Perturbation and Adaptive Hard Negative Contrastive Learning for   Compositional Reasoning in Vision-Language Models"></a>Visual Perturbation and Adaptive Hard Negative Contrastive Learning for   Compositional Reasoning in Vision-Language Models</h2><p><strong>Authors:Xin Huang, Ruibin Li, Tong Jia, Wei Zheng, Ya Wang</strong></p>
<p>Vision-Language Models (VLMs) are essential for multimodal tasks, especially compositional reasoning (CR) tasks, which require distinguishing fine-grained semantic differences between visual and textual embeddings. However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model. Moreover, negative samples are typically treated uniformly, without considering their difficulty levels, and the alignment of positive samples is insufficient, which leads to challenges in aligning difficult sample pairs. To address these issues, we propose Adaptive Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard negatives into the visual domain to generate semantically disturbed image-based negatives for training the model, thereby enhancing its overall performance. AHNPL also introduces a contrastive learning approach using a multimodal hard negative loss to improve the modelâ€™s discrimination of hard negatives within each modality and a dynamic margin loss that adjusts the contrastive margin according to sample difficulty to enhance the distinction of challenging sample pairs. Experiments on three public datasets demonstrate that our method effectively boosts VLMsâ€™ performance on complex CR tasks. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/nynu-BDAI/AHNPL">https://github.com/nynu-BDAI/AHNPL</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹äºå¤šæ¨¡æ€ä»»åŠ¡è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯éœ€è¦åŒºåˆ†è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´ç»†å¾®è¯­ä¹‰å·®å¼‚çš„ç»„åˆæ¨ç†ï¼ˆCRï¼‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ç”ŸæˆåŸºäºæ–‡æœ¬çš„ç¡¬è´Ÿæ ·æœ¬æ¥å¾®è°ƒæ¨¡å‹ï¼Œå¿½ç•¥äº†åŸºäºå›¾åƒçš„è´Ÿæ ·æœ¬çš„é‡è¦æ€§ï¼Œè¿™å¯¼è‡´è§†è§‰ç¼–ç å™¨çš„è®­ç»ƒä¸è¶³ï¼Œå¹¶æœ€ç»ˆå½±å“æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè´Ÿæ ·æœ¬é€šå¸¸è¢«ä¸€è§†åŒä»ï¼Œæ²¡æœ‰è€ƒè™‘å…¶éš¾åº¦æ°´å¹³ï¼Œæ­£æ ·æœ¬çš„å¯¹é½ä¹Ÿä¸è¶³ï¼Œè¿™å¯¼è‡´éš¾ä»¥å¯¹é½æ ·æœ¬å¯¹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ç¡¬è´Ÿæ‰°åŠ¨å­¦ä¹ ï¼ˆAHNPLï¼‰ã€‚AHNPLå°†åŸºäºæ–‡æœ¬çš„ç¡¬è´Ÿæ ·æœ¬è½¬æ¢ä¸ºè§†è§‰é¢†åŸŸï¼Œä»¥ç”Ÿæˆç”¨äºè®­ç»ƒæ¨¡å‹çš„åœ¨è¯­ä¹‰ä¸Šå—å¹²æ‰°çš„åŸºäºå›¾åƒçš„è´Ÿæ ·æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚AHNPLè¿˜ä»‹ç»äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨å¤šæ¨¡æ€ç¡¬è´ŸæŸå¤±æ¥æé«˜æ¨¡å‹åœ¨æ¯ä¸ªæ¨¡æ€å†…å¯¹ç¡¬è´Ÿæ ·æœ¬çš„è¾¨åˆ«èƒ½åŠ›ï¼Œä»¥åŠä¸€ç§åŠ¨æ€è¾¹è·æŸå¤±ï¼Œæ ¹æ®æ ·æœ¬éš¾åº¦è°ƒæ•´å¯¹æ¯”è¾¹è·ï¼Œä»¥æé«˜å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬å¯¹çš„åŒºåˆ†åº¦ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æé«˜äº†VLMåœ¨å¤æ‚çš„CRä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nynu-BDAI/AHNPL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nynu-BDAI/AHNPLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15576v2">PDF</a> Accepted at the International Joint Conference on Artificial   Intelligence (IJCAI 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éœ€è¦åŒºåˆ†è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ç»†å¾®è¯­ä¹‰å·®å¼‚çš„ç»„æˆæ¨ç†ï¼ˆCRï¼‰ä»»åŠ¡æ—¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬ç”Ÿæˆç¡¬è´Ÿæ ·æœ¬è¿›è¡Œå¾®è°ƒçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç¡¬è´Ÿæ‰°åŠ¨å­¦ä¹ ï¼ˆAHNPLï¼‰çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†æ–‡æœ¬ç”Ÿæˆçš„ç¡¬è´Ÿæ ·æœ¬è½¬åŒ–ä¸ºå›¾åƒé¢†åŸŸçš„è´Ÿæ ·æœ¬ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚åŒæ—¶ï¼ŒAHNPLè¿˜å¼•å…¥äº†å¯¹æ¯”å­¦ä¹ æ–¹æ³•å’ŒåŠ¨æ€è¾¹ç•ŒæŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹å¯¹å›°éš¾æ ·æœ¬çš„åŒºåˆ†èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡VLMsåœ¨å¤æ‚CRä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤„ç†å¤šæ¨¡æ€ä»»åŠ¡æ—¶é¢ä¸´åŒºåˆ†è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ç»†å¾®è¯­ä¹‰å·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬ç”Ÿæˆçš„ç¡¬è´Ÿæ ·æœ¬è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œå¿½ç•¥äº†å›¾åƒè´Ÿæ ·æœ¬çš„é‡è¦æ€§ã€‚</li>
<li>AHNPLæ–¹æ³•å°†æ–‡æœ¬ç”Ÿæˆçš„ç¡¬è´Ÿæ ·æœ¬è½¬åŒ–ä¸ºå›¾åƒé¢†åŸŸçš„è´Ÿæ ·æœ¬ï¼Œä»¥æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚</li>
<li>AHNPLå¼•å…¥äº†å¯¹æ¯”å­¦ä¹ æ–¹æ³•å’Œå¤šæ¨¡æ€ç¡¬è´ŸæŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åŠ¨æ€è¾¹ç•ŒæŸå¤±èƒ½æ ¹æ®æ ·æœ¬éš¾åº¦è°ƒæ•´å¯¹æ¯”è¾¹ç•Œï¼Œæé«˜å›°éš¾æ ·æœ¬çš„åŒºåˆ†èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒAHNPLæ–¹æ³•åœ¨å¤æ‚ç»„æˆæ¨ç†ä»»åŠ¡ä¸Šèƒ½æœ‰æ•ˆæå‡VLMsçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e23ae3721211e250347b5b4ed2cec71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1004ca344ee68838ce925e68fa95898f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bad1755fdea4b3fceaefd7ff9aa8ef7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92e9a61c76361a95438b820942e15985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-736ebbecd410d8af99ec91aa7ee509fc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Accurate-and-lightweight-dehazing-via-multi-receptive-field-non-local-network-and-novel-contrastive-regularization"><a href="#Accurate-and-lightweight-dehazing-via-multi-receptive-field-non-local-network-and-novel-contrastive-regularization" class="headerlink" title="Accurate and lightweight dehazing via multi-receptive-field non-local   network and novel contrastive regularization"></a>Accurate and lightweight dehazing via multi-receptive-field non-local   network and novel contrastive regularization</h2><p><strong>Authors:Zewei He, Zixuan Chen, Jinlei Li, Ziqian Lu, Xuecheng Sun, Hao Luo, Zhe-Ming Lu, Evangelos K. Markakis</strong></p>
<p>Recently, deep learning-based methods have dominated image dehazing domain. A multi-receptive-field non-local network (MRFNLN) consisting of the multi-stream feature attention block (MSFAB) and the cross non-local block (CNLB) is presented in this paper to further enhance the performance. We start with extracting richer features for dehazing. Specifically, a multi-stream feature extraction (MSFE) sub-block, which contains three parallel convolutions with different receptive fields (i.e., $1\times 1$, $3\times 3$, $5\times 5$), is designed for extracting multi-scale features. Following MSFE, an attention sub-block is employed to make the model adaptively focus on important channels&#x2F;regions. These two sub-blocks constitute our MSFAB. Then, we design a cross non-local block (CNLB), which can capture long-range dependencies beyond the query. Instead of the same input source of query branch, the key and value branches are enhanced by fusing more preceding features. CNLB is computation-friendly by leveraging a spatial pyramid down-sampling (SPDS) strategy to reduce the computation and memory consumption without sacrificing the performance. Last but not least, a novel detail-focused contrastive regularization (DFCR) is presented by emphasizing the low-level details and ignoring the high-level semantic information in a representation space specially designed for dehazing. Comprehensive experimental results demonstrate that the proposed MRFNLN model outperforms recent state-of-the-art dehazing methods with less than 1.5 Million parameters. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨å›¾åƒå»é›¾é¢†åŸŸå æ®äº†ä¸»å¯¼åœ°ä½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”±å¤šæµç‰¹å¾æ³¨æ„åŠ›å—ï¼ˆMSFABï¼‰å’Œäº¤å‰éå±€éƒ¨å—ï¼ˆCNLBï¼‰ç»„æˆçš„å¤šæ„Ÿå—é‡éå±€éƒ¨ç½‘ç»œï¼ˆMRFNLNï¼‰ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬ä»æå–æ›´ä¸°å¯Œç”¨äºå»é›¾çš„ç‰¹å¾å¼€å§‹ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¤šæµç‰¹å¾æå–ï¼ˆMSFEï¼‰å­å—ï¼Œå…¶ä¸­åŒ…å«ä¸‰ä¸ªå…·æœ‰ä¸åŒæ„Ÿå—é‡ï¼ˆå³1Ã—1ã€3Ã—3ã€5Ã—5ï¼‰çš„å¹¶è¡Œå·ç§¯ï¼Œç”¨äºæå–å¤šå°ºåº¦ç‰¹å¾ã€‚åœ¨MSFEä¹‹åï¼Œé‡‡ç”¨æ³¨æ„åŠ›å­å—ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨é‡è¦çš„é€šé“&#x2F;åŒºåŸŸã€‚è¿™ä¸¤ä¸ªå­å—æ„æˆäº†æˆ‘ä»¬çš„MSFABã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªäº¤å‰éå±€éƒ¨å—ï¼ˆCNLBï¼‰ï¼Œå®ƒèƒ½å¤Ÿæ•è·æŸ¥è¯¢ä¹‹å¤–çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ä¸æŸ¥è¯¢åˆ†æ”¯çš„ç›¸åŒè¾“å…¥æºä¸åŒï¼Œå…³é”®å€¼å’Œåˆ†æ”¯é€šè¿‡èåˆæ›´å¤šçš„å…ˆå‰ç‰¹å¾è¿›è¡Œå¢å¼ºã€‚CNLBé€šè¿‡åˆ©ç”¨ç©ºé—´é‡‘å­—å¡”ä¸‹é‡‡æ ·ï¼ˆSPDSï¼‰ç­–ç•¥åœ¨è®¡ç®—å’Œå†…å­˜æ¶ˆè€—æ–¹é¢æ›´åŠ å‹å¥½ï¼ŒåŒæ—¶ä¸ä¼šç‰ºç‰²æ€§èƒ½ã€‚æœ€åä½†å¹¶éæœ€ä¸é‡è¦çš„æ˜¯ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç»†èŠ‚èšç„¦å¯¹æ¯”æ­£åˆ™åŒ–ï¼ˆDFCRï¼‰ï¼Œå®ƒé€šè¿‡å¼ºè°ƒå»é›¾è¡¨ç¤ºç©ºé—´ä¸­çš„ä½çº§ç»†èŠ‚å¹¶å¿½ç•¥é«˜çº§è¯­ä¹‰ä¿¡æ¯æ¥å®ç°ã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºMRFNLNæ¨¡å‹åœ¨å‚æ•°å°‘äº150ä¸‡çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºæœ€æ–°çš„å…ˆè¿›å»é›¾æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16494v3">PDF</a> submitted to the IEEE Journal for possible publication</p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒå»é›¾æ–¹æ³•ï¼Œé‡‡ç”¨å¤šæ„Ÿå—é‡éå±€éƒ¨ç½‘ç»œï¼ˆMRFNLNï¼‰ï¼ŒåŒ…å«å¤šæµç‰¹å¾æ³¨æ„åŠ›å—ï¼ˆMSFABï¼‰å’Œäº¤å‰éå±€éƒ¨å—ï¼ˆCNLBï¼‰ã€‚é€šè¿‡è®¾è®¡å¤šæµç‰¹å¾æå–å­å—ä»¥æå–æ›´ä¸°å¯Œçš„å»é›¾ç‰¹å¾ï¼Œå¹¶å¼•å…¥æ³¨æ„åŠ›å­å—ä½¿æ¨¡å‹è‡ªé€‚åº”å…³æ³¨é‡è¦é€šé“&#x2F;åŒºåŸŸã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„ç»†èŠ‚èšç„¦å¯¹æ¯”æ­£åˆ™åŒ–ï¼ˆDFCRï¼‰ï¼Œåœ¨ä¸“é—¨è®¾è®¡çš„å»é›¾è¡¨ç¤ºç©ºé—´ä¸­å¼ºè°ƒä½å±‚æ¬¡ç»†èŠ‚è€Œå¿½ç•¥é«˜å±‚æ¬¡è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MRFNLNæ¨¡å‹åœ¨å‚æ•°å°‘äº150ä¸‡çš„æƒ…å†µä¸‹ï¼Œä¼˜äºæœ€æ–°çš„å»é›¾æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ„Ÿå—é‡éå±€éƒ¨ç½‘ç»œï¼ˆMRFNLNï¼‰ç”¨äºå›¾åƒå»é›¾ï¼Œç»“åˆäº†å¤šæµç‰¹å¾æ³¨æ„åŠ›å—ï¼ˆMSFABï¼‰å’Œäº¤å‰éå±€éƒ¨å—ï¼ˆCNLBï¼‰ã€‚</li>
<li>å¤šæµç‰¹å¾æå–å­å—ï¼ˆMSFEï¼‰åŒ…å«ä¸åŒæ„Ÿå—é‡çš„å¹¶è¡Œå·ç§¯ï¼Œä»¥æå–å¤šå°ºåº¦ç‰¹å¾ã€‚</li>
<li>æ³¨æ„åŠ›å­å—ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”å…³æ³¨é‡è¦çš„é€šé“&#x2F;åŒºåŸŸã€‚</li>
<li>äº¤å‰éå±€éƒ¨å—ï¼ˆCNLBï¼‰èƒ½å¤Ÿæ•æ‰è¶…å‡ºæŸ¥è¯¢çš„é•¿ç¨‹ä¾èµ–æ€§ï¼Œå¹¶é€šè¿‡èåˆæ›´å¤šå…ˆå‰ç‰¹å¾æ¥å¢å¼ºå…³é”®å’Œå€¼åˆ†æ”¯ã€‚</li>
<li>é‡‡ç”¨ç©ºé—´é‡‘å­—å¡”ä¸‹é‡‡æ ·ï¼ˆSPDSï¼‰ç­–ç•¥ï¼Œé™ä½è®¡ç®—é‡å’Œå†…å­˜æ¶ˆè€—ï¼Œä¸å½±å“æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹ç»†èŠ‚èšç„¦å¯¹æ¯”æ­£åˆ™åŒ–ï¼ˆDFCRï¼‰ï¼Œåœ¨å»é›¾çš„ä¸“é—¨è¡¨ç¤ºç©ºé—´ä¸­å¼ºè°ƒä½å±‚æ¬¡ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.16494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-967e8616bc345612f0a5f26e3821adf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d78e4375caccc87238f44e754fd9ca76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fedc4b5ea852be08acc40866f0364715.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22313f99f725fd1b85daf282b7437b22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-549317a8198332fd625b990760dd3e92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-672a844451e6dfea56b337b49b5294cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bc8364b077029609b43427d08b44d6f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1bbbb934850718bf94cc89a9bb6bcd1f.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Ultrasound-based detection and malignancy prediction of breast lesions   eligible for biopsy A multi-center clinical-scenario study using nomograms,   large language models, and radiologist evaluation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d72f2a50eb0ef8ddafc53cbace7db756.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  CryptoFace End-to-End Encrypted Face Recognition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
