<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="无监督/半监督/对比学习">
    <meta name="description" content="无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-09-07  Adaptive Contrast Adjustment Module A Clinically-Inspired Plug-and-Play   Approach for Enhanced Fetal Plane Classification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>无监督/半监督/对比学习 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-22313f99f725fd1b85daf282b7437b22.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">无监督/半监督/对比学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">无监督/半监督/对比学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                无监督/半监督/对比学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    44 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-07-更新"><a href="#2025-09-07-更新" class="headerlink" title="2025-09-07 更新"></a>2025-09-07 更新</h1><h2 id="Adaptive-Contrast-Adjustment-Module-A-Clinically-Inspired-Plug-and-Play-Approach-for-Enhanced-Fetal-Plane-Classification"><a href="#Adaptive-Contrast-Adjustment-Module-A-Clinically-Inspired-Plug-and-Play-Approach-for-Enhanced-Fetal-Plane-Classification" class="headerlink" title="Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play   Approach for Enhanced Fetal Plane Classification"></a>Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play   Approach for Enhanced Fetal Plane Classification</h2><p><strong>Authors:Yang Chen, Sanglin Zhao, Baoyu Chen, Mans Gustaf</strong></p>
<p>Fetal ultrasound standard plane classification is essential for reliable prenatal diagnosis but faces inherent challenges, including low tissue contrast, boundary ambiguity, and operator-dependent image quality variations. To overcome these limitations, we propose a plug-and-play adaptive contrast adjustment module (ACAM), whose core design is inspired by the clinical practice of doctors adjusting image contrast to obtain clearer and more discriminative structural information. The module employs a shallow texture-sensitive network to predict clinically plausible contrast parameters, transforms input images into multiple contrast-enhanced views through differentiable mapping, and fuses them within downstream classifiers. Validated on a multi-center dataset of 12,400 images across six anatomical categories, the module consistently improves performance across diverse models, with accuracy of lightweight models increasing by 2.02 percent, accuracy of traditional models increasing by 1.29 percent, and accuracy of state-of-the-art models increasing by 1.15 percent. The innovation of the module lies in its content-aware adaptation capability, replacing random preprocessing with physics-informed transformations that align with sonographer workflows while improving robustness to imaging heterogeneity through multi-view fusion. This approach effectively bridges low-level image features with high-level semantics, establishing a new paradigm for medical image analysis under real-world image quality variations. </p>
<blockquote>
<p>胎儿超声标准平面分类对于可靠的产前诊断至关重要，但面临着固有的挑战，包括组织对比度低、边界模糊和依赖于操作员的图像质量变化。为了克服这些局限性，我们提出了一种即插即用的自适应对比度调整模块（ACAM），其核心设计灵感来源于医生调整图像对比度以获取更清晰、更具区分度的结构信息的临床实践。该模块采用浅纹理敏感网络来预测临床上合理的对比度参数，通过可微分映射将输入图像转换为多个对比度增强的视图，并在下游分类器中融合它们。在多中心数据集（涵盖12400张图像和六个解剖类别）上进行验证，该模块在各种模型上的表现始终有所提升，轻量级模型的准确率提高2.02%，传统模型的准确率提高1.29%，先进模型的准确率提高1.15%。该模块的创新之处在于其基于内容感知的自适应能力，用基于物理的转换替代了随机预处理，这与超声医师的工作流程相符，同时通过多视图融合提高了对成像异质性的稳健性。这种方法有效地将低级图像特征与高级语义相结合，为现实世界中图像质量变化的医学图像分析建立了新的范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00808v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种即插即用的自适应对比度调整模块（ACAM），用于解决胎儿超声标准平面分类在产前诊断中的挑战。该模块通过调整图像对比度，获得更清晰、更具辨识度的结构信息，采用浅纹理敏感网络预测合理的对比度参数，将输入图像转化为多种对比度增强视图，并在下游分类器中融合。在多中心数据集上验证，该模块在不同模型上的性能均有所提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>胎儿超声标准平面分类对产前诊断至关重要，但存在组织对比度低、边界模糊和图像质量操作者差异等挑战。</li>
<li>提出自适应对比度调整模块（ACAM），灵感来源于医生调整图像对比度以获取更清晰结构信息的临床实践。</li>
<li>ACAM使用浅纹理敏感网络预测合理的对比度参数，将输入图像转化为多种对比度增强视图。</li>
<li>ACAM通过可微映射实现多种视图融合，并在下游分类器中使用。</li>
<li>在包含12,400张图像的多中心数据集上进行验证，ACAM提升了不同模型的性能。</li>
<li>ACAM的创新之处在于其内容感知适应能力，能替换随机预处理，采用符合超声工作者工作流程的物理信息转换。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00808">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-79c8b0b458f59846dc7ae23c188bd708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-784e3689841e0376d91c4e4f7f25af93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dfa6df11ae0ed6fc29a197fef7f9a12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-477b0421db9ea54c42cd503cb7d0b65f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HCCM-Hierarchical-Cross-Granularity-Contrastive-and-Matching-Learning-for-Natural-Language-Guided-Drones"><a href="#HCCM-Hierarchical-Cross-Granularity-Contrastive-and-Matching-Learning-for-Natural-Language-Guided-Drones" class="headerlink" title="HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning   for Natural Language-Guided Drones"></a>HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning   for Natural Language-Guided Drones</h2><p><strong>Authors:Hao Ruan, Jinliang Lin, Yingxin Lai, Zhiming Luo, Shaozi Li</strong></p>
<p>Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such as target matching and navigation. However, the wide field of view and complex compositional semantics in drone scenarios pose challenges for vision-language understanding. Mainstream Vision-Language Models (VLMs) emphasize global alignment while lacking fine-grained semantics, and existing hierarchical methods depend on precise entity partitioning and strict containment, limiting effectiveness in dynamic environments. To address this, we propose the Hierarchical Cross-Granularity Contrastive and Matching learning (HCCM) framework with two components: (1) Region-Global Image-Text Contrastive Learning (RG-ITC), which avoids precise scene partitioning and captures hierarchical local-to-global semantics by contrasting local visual regions with global text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM), which dispenses with rigid constraints and instead evaluates local semantic consistency within global cross-modal representations, enhancing compositional reasoning. Moreover, drone text descriptions are often incomplete or ambiguous, destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation (MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM achieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text retrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot generalization with 39.93% mean recall (mR), outperforming fine-tuned baselines. </p>
<blockquote>
<p>自然语言指导的无人机（NLGD）为目标匹配和导航等任务提供了新的范式。然而，无人机场景中的广阔视野和复杂的组合语义对视觉语言理解提出了挑战。主流的视觉语言模型（VLM）强调全局对齐，但缺乏精细的语义信息，而现有的层次方法依赖于精确实体分割和严格包含关系，在动态环境中的有效性受到限制。为了解决这一问题，我们提出了层次化跨粒度对比匹配学习（HCCM）框架，包含两个组件：（1）区域全局图像文本对比学习（RG-ITC），通过对比局部视觉区域与全局文本，避免精确场景分割，捕捉层次化的局部到全局语义； （2）区域全局图像文本匹配（RG-ITM），摒弃了刚性约束，转而评估全局跨模态表示中的局部语义一致性，增强了组合推理能力。此外，无人机文本描述往往不完整或模糊，导致对齐不稳定。HCCM引入了一种动量对比和蒸馏（MCD）机制，提高了稳健性。在GeoText-1 652上的实验表明，HCCM实现了最先进的Recall@1指标，图像检索为28.8%，文本检索为14.7%。在未见过的ERA数据集上，HCCM表现出强大的零样本泛化能力，平均召回率为39.93%（mR），超过了微调基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21539v1">PDF</a> Accepted by ACM MM’25</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为Hierarchical Cross-Granularity Contrastive and Matching learning（HCCM）的框架，旨在解决自然语言引导无人机在视觉语言理解方面所面临的挑战。该框架包括两个组件：Region-Global Image-Text Contrastive Learning（RG-ITC）和Region-Global Image-Text Matching（RG-ITM）。RG-ITC通过对比局部视觉区域与全局文本，捕捉层次化的局部到全局语义；RG-ITM则评估局部语义一致性在全局跨模态表示中的表现，增强组合推理能力。此外，还引入了Momentum Contrast and Distillation（MCD）机制提高无人机文本描述的鲁棒性。实验结果表明，HCCM框架在GeoText-1652数据集上取得了最先进的召回率，并且在未见过的ERA数据集上展现了强大的零样本泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NLGD（自然语言引导无人机）在目标匹配和导航等任务中代表了一种新的范式，但面临着视野广阔和场景语义复杂度的挑战。</li>
<li>主流Vision-Language Models（VLMs）强调全局对齐，但缺乏精细的语义粒度，而现有的分层方法依赖于精确实体分区和严格包含关系，这在动态环境中限制了其有效性。</li>
<li>HCCM框架通过Region-Global Image-Text Contrastive Learning（RG-ITC）捕捉层次化的局部到全局语义，避免精确场景分割。</li>
<li>HCCM框架中的Region-Global Image-Text Matching（RG-ITM）评估局部语义一致性在全局跨模态表示中的表现，增强了组合推理能力，并摒弃了刚性约束。</li>
<li>无人机文本描述常常不完整或模糊，HCCM框架引入了Momentum Contrast and Distillation（MCD）机制来提高鲁棒性。</li>
<li>在GeoText-1652数据集上的实验表明，HCCM框架实现了先进的召回率表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21539">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3eefbd83389967ea3b8fddb5fc6399ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0383ddb92aab8e115db5519756258f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9f5334d7cdd5efc93ba8109c6912e58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1363a7a84862fa32f99f8c9691960df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-725cab3204370be551b43e76b90c544a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Contrastive-Learning-through-Auxiliary-Branch-for-Video-Object-Detection"><a href="#Contrastive-Learning-through-Auxiliary-Branch-for-Video-Object-Detection" class="headerlink" title="Contrastive Learning through Auxiliary Branch for Video Object Detection"></a>Contrastive Learning through Auxiliary Branch for Video Object Detection</h2><p><strong>Authors:Lucas Rakotoarivony</strong></p>
<p>Video object detection is a challenging task because videos often suffer from image deterioration such as motion blur, occlusion, and deformable shapes, making it significantly more difficult than detecting objects in still images. Prior approaches have improved video object detection performance by employing feature aggregation and complex post-processing techniques, though at the cost of increased computational demands. To improve robustness to image degradation without additional computational load during inference, we introduce a straightforward yet effective Contrastive Learning through Auxiliary Branch (CLAB) method. First, we implement a constrastive auxiliary branch using a contrastive loss to enhance the feature representation capability of the video object detector’s backbone. Next, we propose a dynamic loss weighting strategy that emphasizes auxiliary feature learning early in training while gradually prioritizing the detection task as training converges. We validate our approach through comprehensive experiments and ablation studies, demonstrating consistent performance gains. Without bells and whistles, CLAB reaches a performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101, respectively, on the ImageNet VID dataset, thus achieving state-of-the-art performance for CNN-based models without requiring additional post-processing methods. </p>
<blockquote>
<p>视频目标检测是一项具有挑战性的任务，因为视频常常会受到图像退化问题的影响，如运动模糊、遮挡和可变形状，这使得视频中的目标检测比静态图像中的检测更为困难。先前的方法通过采用特征聚合和复杂的后处理技术提高了视频目标检测的性能，但同时也增加了计算需求。为了提高对图像退化的鲁棒性，同时在不增加推理过程中的计算负载的情况下，我们提出了一种简单有效的通过辅助分支进行对比学习（CLAB）的方法。首先，我们使用对比损失实现了一个对比辅助分支，以增强视频目标检测器主干特征表示的能力。接下来，我们提出了一种动态损失权重策略，该策略在训练早期强调辅助特征学习，随着训练的收敛逐渐优先考虑检测任务。我们通过全面的实验和消融研究验证了我们的方法，证明了其持续的性能提升。在不使用任何额外技巧的情况下，CLAB在ImageNet VID数据集上达到了使用ResNet-101和ResNeXt-101的84.0%和85.2%的mAP性能，从而在基于CNN的模型中实现了最先进的性能，且无需额外的后处理方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20551v1">PDF</a> Accepted paper for ACIVS 2025</p>
<p><strong>Summary</strong><br>视频目标检测是一项具有挑战性的任务，因为视频经常出现图像退化问题，如运动模糊、遮挡和可变形形状。为提高对图像退化的鲁棒性，同时不增加推理过程中的计算负担，我们提出了一种简单有效的对比学习辅助分支（CLAB）方法。通过实现对比辅助分支并使用对比损失增强视频目标检测器主干的特征表示能力，再提出动态损失权重策略，在训练初期强调辅助特征学习，随着训练收敛逐渐以检测任务为主。实验和消融研究表明，我们的方法表现稳健，性能提升明显。在ImageNet VID数据集上，CLAB使用ResNet-101和ResNeXt-101分别达到了84.0%和85.2%的mAP，实现了基于CNN模型的最新性能，无需额外的后处理方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频目标检测面临图像退化挑战，如运动模糊、遮挡和形状变形。</li>
<li>对比学习辅助分支（CLAB）方法旨在提高视频目标检测性能并应对图像退化问题。</li>
<li>CLAB方法通过实现对比辅助分支并使用对比损失来增强视频目标检测器的特征表示能力。</li>
<li>动态损失权重策略在训练过程中平衡了辅助特征学习与检测任务的优先级。</li>
<li>CLAB方法在不增加计算负担的情况下提高了视频目标检测的鲁棒性。</li>
<li>在ImageNet VID数据集上，CLAB方法实现了最新性能，使用ResNet-101和ResNeXt-101达到较高的mAP值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20551">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eef89b90e4f8393e7301aa21aebaaf13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8698a6c9de68c9ecaed852947fb9b616.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35a57636f10d4876dfa9e7ecc121de37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aadcf6e4c46198d9d74243b5509d408e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DNP-Guided-Contrastive-Reconstruction-with-a-Reverse-Distillation-Transformer-for-Medical-Anomaly-Detection"><a href="#DNP-Guided-Contrastive-Reconstruction-with-a-Reverse-Distillation-Transformer-for-Medical-Anomaly-Detection" class="headerlink" title="DNP-Guided Contrastive Reconstruction with a Reverse Distillation   Transformer for Medical Anomaly Detection"></a>DNP-Guided Contrastive Reconstruction with a Reverse Distillation   Transformer for Medical Anomaly Detection</h2><p><strong>Authors:Luhu Li, Bowen Lin, Mukhtiar Khan, Shujun Fu</strong></p>
<p>Anomaly detection in medical images is challenging due to limited annotations and a domain gap compared to natural images. Existing reconstruction methods often rely on frozen pre-trained encoders, which limits adaptation to domain-specific features and reduces localization accuracy. Prototype-based learning offers interpretability and clustering benefits but suffers from prototype collapse, where few prototypes dominate training, harming diversity and generalization. To address this, we propose a unified framework combining a trainable encoder with prototype-guided reconstruction and a novel Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum branch, enables stable domain-adaptive feature learning. A lightweight Prototype Extractor mines informative normal prototypes to guide the decoder via attention for precise reconstruction. Our loss enforces balanced prototype use through diversity constraints and per-prototype normalization, effectively preventing collapse. Experiments on multiple medical imaging benchmarks show significant improvements in representation quality and anomaly localization, outperforming prior methods. Visualizations and prototype assignment analyses further validate the effectiveness of our anti-collapse mechanism and enhanced interpretability. </p>
<blockquote>
<p>医学图像中的异常检测面临诸多挑战，原因就在于与天然图像相比，医学图像的注解有限且领域差距较大。现有的重建方法通常依赖于固定的预训练编码器，这就限制了其对特定领域的特征适应，并降低了定位精度。基于原型的学习提供了可解释性和聚类优势，但却存在原型坍塌的问题，少数原型会主导训练过程，从而损害多样性和泛化能力。为解决这一问题，我们提出了一种结合可训练编码器、原型引导重建和新型多样性感知对齐损失的统一框架。可训练的编码器通过动量分支增强功能，可实现稳定的域自适应特征学习。轻量级原型提取器能够挖掘出信息丰富的正常原型，通过注意力引导解码器进行精确重建。我们的损失通过多样性和每个原型的归一化强制平衡原型的使用，有效地防止了原型的坍塌。在多个医学影像基准测试上的实验表明，我们的方法在表示质量和异常定位方面都有显著提高，优于以前的方法。可视化和原型分配分析进一步验证了我们防坍塌机制和增强可解释性的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19573v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文解决了医学图像异常检测中的标注有限和领域差异问题。针对现有重建方法依赖冻结预训练编码器的问题，提出结合可训练编码器、原型引导重建和新型多样性感知对齐损失的统一框架。可训练编码器通过动量分支增强，实现稳定的领域自适应特征学习。轻量级原型提取器挖掘正常原型信息，通过注意力引导解码器进行精确重建。损失函数通过多样性和每个原型的归一化强制平衡原型使用，有效防止原型崩溃。在多个医学图像基准测试上的实验表明，该方法在表示质量和异常定位方面显著提高，优于先前方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像异常检测面临标注有限和领域差异的挑战。</li>
<li>现有重建方法依赖冻结的预训练编码器，限制了领域特定特征的适应和定位精度。</li>
<li>提出结合可训练编码器、原型引导重建和多样性感知对齐损失的统一框架。</li>
<li>可训练编码器通过动量分支增强，实现稳定的领域自适应特征学习。</li>
<li>原型提取器挖掘正常原型信息，通过注意力引导精确重建。</li>
<li>损失函数设计强制平衡原型使用，有效防止原型崩溃。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19573">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eb689f8d0b096a2efd86191c1cfb10a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-656f1173d85aa59c1e9d414af449abf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5751844a163d82df964fc55ed42060ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62b33d42460cba7052723c7a18fcfdfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba9ac90d53a23cc659bd10968c26851e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ModAn-MulSupCon-Modality-and-Anatomy-Aware-Multi-Label-Supervised-Contrastive-Pretraining-for-Medical-Imaging"><a href="#ModAn-MulSupCon-Modality-and-Anatomy-Aware-Multi-Label-Supervised-Contrastive-Pretraining-for-Medical-Imaging" class="headerlink" title="ModAn-MulSupCon: Modality-and Anatomy-Aware Multi-Label Supervised   Contrastive Pretraining for Medical Imaging"></a>ModAn-MulSupCon: Modality-and Anatomy-Aware Multi-Label Supervised   Contrastive Pretraining for Medical Imaging</h2><p><strong>Authors:Eichi Takaya, Ryusei Inamori</strong></p>
<p>Background and objective: Expert annotations limit large-scale supervised pretraining in medical imaging, while ubiquitous metadata (modality, anatomical region) remain underused. We introduce ModAn-MulSupCon, a modality- and anatomy-aware multi-label supervised contrastive pretraining method that leverages such metadata to learn transferable representations.   Method: Each image’s modality and anatomy are encoded as a multi-hot vector. A ResNet-18 encoder is pretrained on a mini subset of RadImageNet (miniRIN, 16,222 images) with a Jaccard-weighted multi-label supervised contrastive loss, and then evaluated by fine-tuning and linear probing on three binary classification tasks–ACL tear (knee MRI), lesion malignancy (breast ultrasound), and nodule malignancy (thyroid ultrasound).   Result: With fine-tuning, ModAn-MulSupCon achieved the best AUC on MRNet-ACL (0.964) and Thyroid (0.763), surpassing all baselines ($p&lt;0.05$), and ranked second on Breast (0.926) behind SimCLR (0.940; not significant). With the encoder frozen, SimCLR&#x2F;ImageNet were superior, indicating that ModAn-MulSupCon representations benefit most from task adaptation rather than linear separability.   Conclusion: Encoding readily available modality&#x2F;anatomy metadata as multi-label targets provides a practical, scalable pretraining signal that improves downstream accuracy when fine-tuning is feasible. ModAn-MulSupCon is a strong initialization for label-scarce clinical settings, whereas SimCLR&#x2F;ImageNet remain preferable for frozen-encoder deployments. </p>
<blockquote>
<p>背景与目标：专家注释限制了医学影像的大规模有监督预训练，而普遍存在的元数据（模态、解剖部位）却被使用不足。我们引入了ModAn-MulSupCon，这是一种模态和解剖结构感知的多标签监督对比预训练方法，它利用此类元数据来学习可迁移的表示。方法：将每张图像的模态和解剖结构编码为多热向量。使用Jaccard加权多标签监督对比损失在RadImageNet的一个小子集（miniRIN，包含16,222张图像）上对ResNet-18编码器进行预训练，然后通过微调线性探测在三项二分类任务（膝关节MRI的ACL撕裂、乳腺超声的病变恶性程度以及甲状腺超声的结节恶性程度）上对其进行评估。结果：通过微调，ModAn-MulSupCon在MRNet-ACL（AUC为0.964）和甲状腺（AUC为0.763）上的表现最佳，超过了所有基线（p&lt;0.05），并在乳腺（AUC为0.926）上排名第二，仅次于SimCLR（AUC为0.940，无显著差异）。当编码器冻结时，SimCLR&#x2F;ImageNet表现更佳，这表明ModAn-MulSupCon的表示形式更受益于任务适应性而非线性可分性。结论：将易于获得的模态&#x2F;解剖结构元数据编码为多标签目标提供了一种实用且可扩展的预训练信号，在微调可行的情况下可提高下游准确性。ModAn-MulSupCon是标签稀缺的临床环境中的强大初始化方法，而SimCLR&#x2F;ImageNet对于冻结编码器部署更为可取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18613v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了ModAn-MulSupCon方法，这是一种利用模态和解剖结构信息的有监督对比预训练方法。该方法将图像的模态和解剖结构编码为多热向量，并使用Jaccard加权多标签监督对比损失在RadImageNet的一个小子集上进行预训练。通过微调，该方法在三个二分类任务上取得了最佳AUC值。结论表明，将易于获得的模态&#x2F;解剖结构信息编码为多标签目标，提供了一种实用且可扩展的预训练信号，在微调可行的情况下，能提高下游任务的准确性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>ModAn-MulSupCon是一种利用模态和解剖结构信息的有监督对比预训练方法。</li>
<li>该方法利用多热向量编码图像的模态和解剖结构信息。</li>
<li>使用Jaccard加权多标签监督对比损失在RadImageNet的一个小子集上进行预训练。</li>
<li>通过微调，ModAn-MulSupCon在三个二分类任务上取得了最佳AUC值。</li>
<li>ModAn-MulSupCon在标签稀缺的临床环境中是一个强大的初始化方法。</li>
<li>在冻结编码器部署时，SimCLR&#x2F;ImageNet仍然是更理想的选择。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18613">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9c7a291feca0e9d731b1e77bd7ac5b3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c83ecb71c4a66261d74523f1a67c0fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4af56237f06b48dfd9fb28bcb0daa09f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c14de5b2569ae23c6bea9162a25d5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bee7aab2b18738c98f3897a63a063a62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c194da6f5571b99e0579d0c684ec6c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolution"><a href="#CATformer-Contrastive-Adversarial-Transformer-for-Image-Super-Resolution" class="headerlink" title="CATformer: Contrastive Adversarial Transformer for Image   Super-Resolution"></a>CATformer: Contrastive Adversarial Transformer for Image   Super-Resolution</h2><p><strong>Authors:Qinyi Tian, Spence Cox, Laura E. Dalton</strong></p>
<p>Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired transformer, which progressively refines latent representations, with an auxiliary transformer branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and decoded using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among transformer-, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired transformers in super-resolution. </p>
<blockquote>
<p>超分辨率技术依然是一种可以提升低分辨率图像质量的颇具前景的技术。本研究介绍了CATformer（对比对抗性转换器），这是一种新型神经网络，融合了扩散启发特征细化、对抗性学习与对比学习。CATformer采用双分支架构，结合主要扩散启发转换器，逐步优化潜在表示，以及一个辅助转换器分支，通过学习潜在对比增强对噪声的鲁棒性。这些互补表示通过深度残差残差密集块进行融合和解码，以提高重建质量。在基准数据集上的广泛实验表明，CATformer在效率和视觉图像质量方面超越了最近的基于转换器和扩散的方法。这项工作缩小了基于转换器、扩散和GAN的方法之间的性能差距，为扩散启发转换器的实际应用在超分辨率领域奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17708v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CATformer是一种新型神经网络，结合了扩散启发特征优化、对抗学习和对比学习，用于提高低分辨率图像的超分辨率。它采用双分支架构，主分支为扩散启发变压器，用于逐步优化潜在表示，辅助分支为增强噪声鲁棒性的对比辅助变压器。两者融合并使用深度Residual-in-Residual Dense Blocks进行解码，以提高重建质量。在基准数据集上的实验表明，CATformer在效率和视觉图像质量方面优于最新的基于变压器和扩散的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CATformer是一种结合扩散启发特征优化、对抗学习和对比学习的新型神经网络。</li>
<li>采用双分支架构，主分支用于优化潜在表示，辅助分支增强噪声鲁棒性。</li>
<li>通过结合这两种表示并使用深度Residual-in-Residual Dense Blocks解码，提高图像重建质量。</li>
<li>CATformer在基准数据集上的实验表现优于其他基于变压器和扩散的方法。</li>
<li>该研究缩小了基于变压器、扩散和GAN的方法之间的性能差距。</li>
<li>CATformer为扩散启发变压器在超分辨率实际应用中的应用奠定了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17708">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a98965b00ed9e65b5db306e9c4661723.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31ae8de490f71b23e3af54dcbc851cd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffcb7da9a2d9366c4dc6b83d57058c2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83842ff9321d1bf83aac6cf72964c1cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c6861ee19e892a19ee335d5bedf357d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Contrastive Prompt Clustering for Weakly Supervised Semantic   Segmentation"></a>Contrastive Prompt Clustering for Weakly Supervised Semantic   Segmentation</h2><p><strong>Authors:Wangyu Wu, Zhenhong Chen, Xiaowen Ma, Wenqiao Zhang, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained attention for its cost-effectiveness. Most existing methods emphasize inter-class separation, often neglecting the shared semantics among related categories and lacking fine-grained discrimination. To address this, we propose Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large Language Models (LLMs) to derive category clusters that encode intrinsic inter-class relationships, and further introduces a class-aware patch-level contrastive loss to enforce intra-class consistency and inter-class separation. This hierarchical design leverages clusters as coarse-grained semantic priors while preserving fine-grained boundaries, thereby reducing confusion among visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014 demonstrate that CPC surpasses existing state-of-the-art methods in WSSS. </p>
<blockquote>
<p>利用图像级标签进行弱监督语义分割（WSSS）因其成本效益而受到关注。大多数现有方法强调类间分离，往往忽略了相关类别之间的共享语义，且缺乏精细的粒度鉴别。为了解决这一问题，我们提出了对比提示聚类（CPC），这是一种新型的WSSS框架。CPC利用大型语言模型（LLM）来推导编码内在类间关系的类别聚类，并进一步引入了一种类感知的补丁级对比损失，以加强类内一致性和类间分离。这种层次化的设计利用聚类作为粗粒度的语义先验，同时保留精细的边界，从而减少视觉上相似类别之间的混淆。在PASCAL VOC 2012和MS COCO 2014上的实验表明，CPC在WSSS中超越了现有的最先进方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17009v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于弱监督语义分割（WSSS）的新框架——对比提示聚类（CPC）。CPC利用大型语言模型（LLM）推导类别聚类，编码类别间的内在关系，并引入类感知补丁级对比损失，以加强类内一致性和类间分离。这种层次设计利用聚类作为粗粒度语义先验，同时保留精细边界，减少视觉相似类别之间的混淆。实验表明，CPC在PASCAL VOC 2012和MS COCO 2014数据集上的表现超越了现有的WSSS先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>弱监督语义分割（WSSS）因成本效益而受到关注。</li>
<li>现有WSSS方法往往忽视相关类别之间的共享语义，缺乏精细粒度鉴别。</li>
<li>提出了一种新的WSSS框架——对比提示聚类（CPC）。</li>
<li>CPC利用大型语言模型（LLM）推导类别聚类，编码类别间的内在关系。</li>
<li>CPC引入类感知补丁级对比损失，以加强类内一致性和类间分离。</li>
<li>层次设计利用聚类作为粗粒度语义先验，同时保留精细边界。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17009">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf40a7b0c676728135b697cc20a1e1bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc03bf19f4621d2044bc2d96e3a20412.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learning"><a href="#Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learning" class="headerlink" title="Multimodal Medical Endoscopic Image Analysis via Progressive   Disentangle-aware Contrastive Learning"></a>Multimodal Medical Endoscopic Image Analysis via Progressive   Disentangle-aware Contrastive Learning</h2><p><strong>Authors:Junhao Wu, Yun Li, Junhao Li, Jingliang Bian, Xiaomao Fan, Wenbin Lei, Ruxin Wang</strong></p>
<p>Accurate segmentation of laryngo-pharyngeal tumors is crucial for precise diagnosis and effective treatment planning. However, traditional single-modality imaging methods often fall short of capturing the complex anatomical and pathological features of these tumors. In this study, we present an innovative multi-modality representation learning framework based on the &#96;Align-Disentangle-Fusion’ mechanism that seamlessly integrates 2D White Light Imaging (WLI) and Narrow Band Imaging (NBI) pairs to enhance segmentation performance. A cornerstone of our approach is multi-scale distribution alignment, which mitigates modality discrepancies by aligning features across multiple transformer layers. Furthermore, a progressive feature disentanglement strategy is developed with the designed preliminary disentanglement and disentangle-aware contrastive learning to effectively separate modality-specific and shared features, enabling robust multimodal contrastive learning and efficient semantic fusion. Comprehensive experiments on multiple datasets demonstrate that our method consistently outperforms state-of-the-art approaches, achieving superior accuracy across diverse real clinical scenarios. </p>
<blockquote>
<p>喉咽肿瘤准确分割对于精确诊断和治疗计划至关重要。然而，传统的单一成像方法往往难以捕捉这些肿瘤的复杂解剖和病理特征。在本研究中，我们提出了一种基于“对齐-分离-融合”机制的创新多模态表示学习框架，无缝集成了2D白光成像（WLI）和窄带成像（NBI）配对，以提高分割性能。我们的方法的核心是多尺度分布对齐，它通过对齐多个transformer层中的特征来减轻模态差异。此外，还开发了一种渐进的特征分离策略，通过设计初步分离和解纠缠感知对比学习，有效地分离了模态特定特征和共享特征，实现了稳健的多模态对比学习和高效的语义融合。在多个数据集上的综合实验表明，我们的方法一直优于最先进的方法，并在多种实际临床场景中实现了更高的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16882v1">PDF</a> 12 pages,6 figures, 6 tables</p>
<p><strong>摘要</strong></p>
<p>本文提出了一种基于“对齐-分离-融合”机制的多模态表示学习框架，该框架无缝集成了2D白光成像（WLI）和窄带成像（NBI）配对，以提高喉咽肿瘤分割的性能。该方法的核心是多尺度分布对齐，通过对齐多层transformer的特征来减少模态差异。同时，开发了一种渐进式特征分离策略，通过初步分离和分离感知对比学习，有效地分离了模态特定和共享特征，实现了稳健的多模态对比学习和高效的语义融合。在多个数据集上的综合实验表明，该方法一致优于最新技术，在多种真实临床场景下具有卓越准确性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了一种基于多模态表示学习的框架，集成了2D白光成像和窄带成像，以提高喉咽肿瘤分割性能。</li>
<li>多尺度分布对齐是该方法的核心，通过对齐多层transformer的特征来减少模态差异。</li>
<li>采用了渐进式特征分离策略，通过初步分离和分离感知对比学习有效分离模态特定和共享特征。</li>
<li>该方法实现了稳健的多模态对比学习和高效的语义融合。</li>
<li>在多个数据集上的实验表明，该方法在多种真实临床场景下具有卓越性能。</li>
<li>与现有技术相比，该方法具有更好的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16882">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-63e3c2352f5743d0064ebe66876104f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0688f3c97906c94c21448d25ac776ebb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319276a72c9eb46a34d0671d7f1109f0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Predicting-brain-tumour-enhancement-from-non-contrast-MR-imaging-with-artificial-intelligence"><a href="#Predicting-brain-tumour-enhancement-from-non-contrast-MR-imaging-with-artificial-intelligence" class="headerlink" title="Predicting brain tumour enhancement from non-contrast MR imaging with   artificial intelligence"></a>Predicting brain tumour enhancement from non-contrast MR imaging with   artificial intelligence</h2><p><strong>Authors:James K Ruffle, Samia Mohinta, Guilherme Pombo, Asthik Biswas, Alan Campbell, Indran Davagnanam, David Doig, Ahmed Hamman, Harpreet Hyare, Farrah Jabeen, Emma Lim, Dermot Mallon, Stephanie Owen, Sophie Wilkinson, Sebastian Brandner, Parashkev Nachev</strong></p>
<p>Brain tumour imaging assessment typically requires both pre- and post-contrast MRI, but gadolinium administration is not always desirable, such as in frequent follow-up, renal impairment, allergy, or paediatric patients. We aimed to develop and validate a deep learning model capable of predicting brain tumour contrast enhancement from non-contrast MRI sequences alone. We assembled 11089 brain MRI studies from 10 international datasets spanning adult and paediatric populations with various neuro-oncological states, including glioma, meningioma, metastases, and post-resection appearances. Deep learning models (nnU-Net, SegResNet, SwinUNETR) were trained to predict and segment enhancing tumour using only non-contrast T1-, T2-, and T2&#x2F;FLAIR-weighted images. Performance was evaluated on 1109 held-out test patients using patient-level detection metrics and voxel-level segmentation accuracy. Model predictions were compared against 11 expert radiologists who each reviewed 100 randomly selected patients. The best-performing nnU-Net achieved 83% balanced accuracy, 91.5% sensitivity, and 74.4% specificity in detecting enhancing tumour. Enhancement volume predictions strongly correlated with ground truth (R2 0.859). The model outperformed expert radiologists, who achieved 69.8% accuracy, 75.9% sensitivity, and 64.7% specificity. 76.8% of test patients had Dice over 0.3 (acceptable detection), 67.5% had Dice over 0.5 (good detection), and 50.2% had Dice over 0.7 (excellent detection). Deep learning can identify contrast-enhancing brain tumours from non-contrast MRI with clinically relevant performance. These models show promise as screening tools and may reduce gadolinium dependence in neuro-oncology imaging. Future work should evaluate clinical utility alongside radiology experts. </p>
<blockquote>
<p>脑肿瘤成像评估通常需要对比剂前后的MRI，但在频繁随访、肾功能受损、过敏或儿科患者中，钆的给药并不总是理想的。我们的目标是开发和验证一个深度学习模型，该模型能够仅从非对比MRI序列中预测脑肿瘤的对比增强效果。我们汇集了来自成人和儿科等不同神经肿瘤状态的国际数据集，包括胶质瘤、脑膜瘤、转移瘤和术后表现，共涉及10个数据集的11089个脑MRI研究。深度学习模型（nnU-Net、SegResNet、SwinUNETR）经过训练，仅使用非对比T1、T2和T2 &#x2F; FLAIR加权图像来预测和分割增强肿瘤。在1109名独立测试患者上的性能评估采用了患者级别的检测指标和体素级别的分割准确性。模型预测与11位专家放射科医生进行了比较，每位放射科医生均审查了随机选择的100名患者。表现最佳的nnU-Net在检测增强肿瘤方面的平衡准确度为83%，敏感度为91.5%，特异度为74.4%。增强体积预测与真实值高度相关（R² 0.859）。该模型的性能优于专家放射科医生，后者的准确度为69.8%，敏感度为75.9%，特异度为64.7%。76.8%的测试患者的Dice系数大于0.3（可接受检测），67.5%的Dice系数大于0.5（良好检测），50.2%的Dice系数大于0.7（优良检测）。深度学习可以从非对比MRI中识别出对比增强的脑肿瘤，并具有临床意义。这些模型作为筛查工具显示出一定的前景，并可能减少神经肿瘤学成像中对钆的依赖。未来的工作应与放射学专家一起评估其临床实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16650v1">PDF</a> 38 pages</p>
<p><strong>Summary</strong></p>
<p>基于非对比MRI序列预测脑肿瘤对比增强的深度学习模型研究。该研究利用多种深度学习模型（如nnU-Net、SegResNet和SwinUNETR）训练模型，仅使用非对比T1、T2和T2&#x2F;FLAIR加权图像来预测和分割增强肿瘤。模型性能通过患者级别的检测指标和体素级别的分割精度进行评估，并与专家放射科医生的评估结果进行比较。结果显示，深度学习模型在检测增强肿瘤方面表现出良好的性能，并有望作为筛查工具减少神经肿瘤学中钆的依赖。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究旨在开发并验证一种能够从非对比MRI序列预测脑肿瘤对比增强的深度学习模型。</li>
<li>使用了多种深度学习模型进行训练和预测，数据来源涵盖成人和儿童的不同神经肿瘤状态患者。</li>
<li>模型性能通过严格的患者级别和体素级别评估指标进行评估。</li>
<li>最佳模型nnU-Net在检测增强肿瘤方面表现出良好性能，与专家放射科医生相比具有优势。</li>
<li>模型预测结果与真实结果高度相关，增强体积预测与真实值的相关性达到R² 0.859。</li>
<li>该模型有望作为筛查工具，减少神经肿瘤学中对钆的依赖。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-53bdd080883fbdb356c7b57249e826b2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Visual-Perturbation-and-Adaptive-Hard-Negative-Contrastive-Learning-for-Compositional-Reasoning-in-Vision-Language-Models"><a href="#Visual-Perturbation-and-Adaptive-Hard-Negative-Contrastive-Learning-for-Compositional-Reasoning-in-Vision-Language-Models" class="headerlink" title="Visual Perturbation and Adaptive Hard Negative Contrastive Learning for   Compositional Reasoning in Vision-Language Models"></a>Visual Perturbation and Adaptive Hard Negative Contrastive Learning for   Compositional Reasoning in Vision-Language Models</h2><p><strong>Authors:Xin Huang, Ruibin Li, Tong Jia, Wei Zheng, Ya Wang</strong></p>
<p>Vision-Language Models (VLMs) are essential for multimodal tasks, especially compositional reasoning (CR) tasks, which require distinguishing fine-grained semantic differences between visual and textual embeddings. However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model. Moreover, negative samples are typically treated uniformly, without considering their difficulty levels, and the alignment of positive samples is insufficient, which leads to challenges in aligning difficult sample pairs. To address these issues, we propose Adaptive Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard negatives into the visual domain to generate semantically disturbed image-based negatives for training the model, thereby enhancing its overall performance. AHNPL also introduces a contrastive learning approach using a multimodal hard negative loss to improve the model’s discrimination of hard negatives within each modality and a dynamic margin loss that adjusts the contrastive margin according to sample difficulty to enhance the distinction of challenging sample pairs. Experiments on three public datasets demonstrate that our method effectively boosts VLMs’ performance on complex CR tasks. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/nynu-BDAI/AHNPL">https://github.com/nynu-BDAI/AHNPL</a>. </p>
<blockquote>
<p>视觉语言模型（VLMs）对于多模态任务至关重要，特别是需要区分视觉和文本嵌入之间细微语义差异的组合推理（CR）任务。然而，现有方法主要通过生成基于文本的硬负样本来微调模型，忽略了基于图像的负样本的重要性，这导致视觉编码器的训练不足，并最终影响模型的总体性能。此外，负样本通常被一视同仁，没有考虑其难度水平，正样本的对齐也不足，这导致难以对齐样本对。为了解决这些问题，我们提出了自适应硬负扰动学习（AHNPL）。AHNPL将基于文本的硬负样本转换为视觉领域，以生成用于训练模型的在语义上受干扰的基于图像的负样本，从而提高模型的总体性能。AHNPL还介绍了一种对比学习方法，使用多模态硬负损失来提高模型在每个模态内对硬负样本的辨别能力，以及一种动态边距损失，根据样本难度调整对比边距，以提高具有挑战性的样本对的区分度。在三个公共数据集上的实验表明，我们的方法有效地提高了VLM在复杂的CR任务上的性能。源代码可在<a target="_blank" rel="noopener" href="https://github.com/nynu-BDAI/AHNPL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nynu-BDAI/AHNPL找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15576v2">PDF</a> Accepted at the International Joint Conference on Artificial   Intelligence (IJCAI 2025)</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨视觉语言模型（VLMs）在多模态任务中的关键挑战，特别是在处理需要区分视觉和文本嵌入细微语义差异的组成推理（CR）任务时。针对现有方法主要依赖文本生成硬负样本进行微调的问题，本文提出了一种自适应硬负扰动学习（AHNPL）的方法。该方法将文本生成的硬负样本转化为图像领域的负样本，以提高模型的性能。同时，AHNPL还引入了对比学习方法和动态边界损失来优化模型对困难样本的区分能力。实验证明，该方法能有效提升VLMs在复杂CR任务上的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs在处理多模态任务时面临区分视觉和文本嵌入细微语义差异的挑战。</li>
<li>现有方法主要依赖文本生成的硬负样本进行模型微调，忽略了图像负样本的重要性。</li>
<li>AHNPL方法将文本生成的硬负样本转化为图像领域的负样本，以提高模型的训练效果。</li>
<li>AHNPL引入了对比学习方法和多模态硬负损失来优化模型性能。</li>
<li>动态边界损失能根据样本难度调整对比边界，提高困难样本的区分能力。</li>
<li>实验证明，AHNPL方法在复杂组成推理任务上能有效提升VLMs的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2e23ae3721211e250347b5b4ed2cec71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1004ca344ee68838ce925e68fa95898f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bad1755fdea4b3fceaefd7ff9aa8ef7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92e9a61c76361a95438b820942e15985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-736ebbecd410d8af99ec91aa7ee509fc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Accurate-and-lightweight-dehazing-via-multi-receptive-field-non-local-network-and-novel-contrastive-regularization"><a href="#Accurate-and-lightweight-dehazing-via-multi-receptive-field-non-local-network-and-novel-contrastive-regularization" class="headerlink" title="Accurate and lightweight dehazing via multi-receptive-field non-local   network and novel contrastive regularization"></a>Accurate and lightweight dehazing via multi-receptive-field non-local   network and novel contrastive regularization</h2><p><strong>Authors:Zewei He, Zixuan Chen, Jinlei Li, Ziqian Lu, Xuecheng Sun, Hao Luo, Zhe-Ming Lu, Evangelos K. Markakis</strong></p>
<p>Recently, deep learning-based methods have dominated image dehazing domain. A multi-receptive-field non-local network (MRFNLN) consisting of the multi-stream feature attention block (MSFAB) and the cross non-local block (CNLB) is presented in this paper to further enhance the performance. We start with extracting richer features for dehazing. Specifically, a multi-stream feature extraction (MSFE) sub-block, which contains three parallel convolutions with different receptive fields (i.e., $1\times 1$, $3\times 3$, $5\times 5$), is designed for extracting multi-scale features. Following MSFE, an attention sub-block is employed to make the model adaptively focus on important channels&#x2F;regions. These two sub-blocks constitute our MSFAB. Then, we design a cross non-local block (CNLB), which can capture long-range dependencies beyond the query. Instead of the same input source of query branch, the key and value branches are enhanced by fusing more preceding features. CNLB is computation-friendly by leveraging a spatial pyramid down-sampling (SPDS) strategy to reduce the computation and memory consumption without sacrificing the performance. Last but not least, a novel detail-focused contrastive regularization (DFCR) is presented by emphasizing the low-level details and ignoring the high-level semantic information in a representation space specially designed for dehazing. Comprehensive experimental results demonstrate that the proposed MRFNLN model outperforms recent state-of-the-art dehazing methods with less than 1.5 Million parameters. </p>
<blockquote>
<p>最近，基于深度学习的方法在图像去雾领域占据了主导地位。本文提出了一种由多流特征注意力块（MSFAB）和交叉非局部块（CNLB）组成的多感受野非局部网络（MRFNLN），以进一步提高性能。我们从提取更丰富用于去雾的特征开始。具体来说，设计了一个多流特征提取（MSFE）子块，其中包含三个具有不同感受野（即1×1、3×3、5×5）的并行卷积，用于提取多尺度特征。在MSFE之后，采用注意力子块使模型能够自适应地关注重要的通道&#x2F;区域。这两个子块构成了我们的MSFAB。然后，我们设计了一个交叉非局部块（CNLB），它能够捕获查询之外的长距离依赖关系。与查询分支的相同输入源不同，关键值和分支通过融合更多的先前特征进行增强。CNLB通过利用空间金字塔下采样（SPDS）策略在计算和内存消耗方面更加友好，同时不会牺牲性能。最后但并非最不重要的是，提出了一种新的细节聚焦对比正则化（DFCR），它通过强调去雾表示空间中的低级细节并忽略高级语义信息来实现。综合实验结果表明，所提出MRFNLN模型在参数少于150万的情况下，性能优于最新的先进去雾方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16494v3">PDF</a> submitted to the IEEE Journal for possible publication</p>
<p><strong>Summary</strong><br>     本论文提出了一种基于深度学习的图像去雾方法，采用多感受野非局部网络（MRFNLN），包含多流特征注意力块（MSFAB）和交叉非局部块（CNLB）。通过设计多流特征提取子块以提取更丰富的去雾特征，并引入注意力子块使模型自适应关注重要通道&#x2F;区域。同时，设计了一种新型的细节聚焦对比正则化（DFCR），在专门设计的去雾表示空间中强调低层次细节而忽略高层次语义信息。实验结果表明，所提出的MRFNLN模型在参数少于150万的情况下，优于最新的去雾方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种多感受野非局部网络（MRFNLN）用于图像去雾，结合了多流特征注意力块（MSFAB）和交叉非局部块（CNLB）。</li>
<li>多流特征提取子块（MSFE）包含不同感受野的并行卷积，以提取多尺度特征。</li>
<li>注意力子块使模型能够自适应关注重要的通道&#x2F;区域。</li>
<li>交叉非局部块（CNLB）能够捕捉超出查询的长程依赖性，并通过融合更多先前特征来增强关键和值分支。</li>
<li>采用空间金字塔下采样（SPDS）策略，降低计算量和内存消耗，不影响性能。</li>
<li>引入了一种新型细节聚焦对比正则化（DFCR），在去雾的专门表示空间中强调低层次细节。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.16494">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-967e8616bc345612f0a5f26e3821adf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d78e4375caccc87238f44e754fd9ca76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fedc4b5ea852be08acc40866f0364715.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22313f99f725fd1b85daf282b7437b22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-549317a8198332fd625b990760dd3e92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-672a844451e6dfea56b337b49b5294cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bc8364b077029609b43427d08b44d6f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">无监督/半监督/对比学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1bbbb934850718bf94cc89a9bb6bcd1f.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-09-07  Ultrasound-based detection and malignancy prediction of breast lesions   eligible for biopsy A multi-center clinical-scenario study using nomograms,   large language models, and radiologist evaluation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d72f2a50eb0ef8ddafc53cbace7db756.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-09-07  CryptoFace End-to-End Encrypted Face Recognition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
