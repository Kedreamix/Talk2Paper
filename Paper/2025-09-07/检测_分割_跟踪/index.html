<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Differential Morphological Profile Neural Networks for Semantic   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-28758a3572f844aeef49743a5127ccca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-22
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Differential-Morphological-Profile-Neural-Networks-for-Semantic-Segmentation"><a href="#Differential-Morphological-Profile-Neural-Networks-for-Semantic-Segmentation" class="headerlink" title="Differential Morphological Profile Neural Networks for Semantic   Segmentation"></a>Differential Morphological Profile Neural Networks for Semantic   Segmentation</h2><p><strong>Authors:David Huangal, J. Alex Hurt</strong></p>
<p>Semantic segmentation of overhead remote sensing imagery enables applications in mapping, urban planning, and disaster response. State-of-the-art segmentation networks are typically developed and tuned on ground-perspective photographs and do not directly address remote sensing challenges such as extreme scale variation, foreground-background imbalance, and large image sizes. We explore the incorporation of the differential morphological profile (DMP), a multi-scale shape extraction method based on grayscale morphology, into modern segmentation networks. Prior studies have shown that the DMP can provide critical shape information to Deep Neural Networks to enable superior detection and classification performance in overhead imagery. In this work, we extend prior DMPNet work beyond classification and object detection by integrating DMP features into three state-of-the-art convolutional and transformer semantic segmentation architectures. We utilize both direct input, which adapts the input stem of feature extraction architectures to accept DMP channels, and hybrid architectures, a dual-stream design that fuses RGB and DMP encoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP differentials and structuring element shapes to more effectively provide shape information to the model. Our results show that while non-DMP models generally outperform the direct-input variants, hybrid DMP consistently outperforms direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and Recall. </p>
<blockquote>
<p>é«˜ç©ºé¥æ„Ÿå½±åƒçš„è¯­ä¹‰åˆ†å‰²åœ¨åœ°å›¾ç»˜åˆ¶ã€åŸå¸‚è§„åˆ’å’Œç¾å®³åº”å¯¹ç­‰æ–¹é¢éƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚æœ€å…ˆè¿›çš„åˆ†å‰²ç½‘ç»œé€šå¸¸æ˜¯åœ¨åœ°é¢ç…§ç‰‡ä¸Šå¼€å‘å’Œè°ƒæ•´çš„ï¼Œå¹¶æ²¡æœ‰ç›´æ¥è§£å†³é¥æ„Ÿé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æç«¯å°ºåº¦å˜åŒ–ã€å‰æ™¯èƒŒæ™¯ä¸å¹³è¡¡å’Œå›¾åƒå°ºå¯¸å¤§ç­‰ã€‚æˆ‘ä»¬æ¢ç´¢å°†å·®åˆ†å½¢æ€å‰–é¢ï¼ˆDMPï¼‰è¿™ä¸€åŸºäºç°åº¦å½¢æ€å­¦çš„å¤šå°ºåº¦å½¢çŠ¶æå–æ–¹æ³•èå…¥ç°ä»£åˆ†å‰²ç½‘ç»œã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒDMPå¯ä»¥ä¸ºæ·±åº¦ç¥ç»ç½‘ç»œæä¾›å…³é”®å½¢çŠ¶ä¿¡æ¯ï¼Œä»è€Œåœ¨é«˜ç©ºå›¾åƒä¸­å®ç°å“è¶Šçš„æ£€æµ‹å’Œåˆ†ç±»æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†DMPç‰¹å¾é›†æˆåˆ°ä¸‰ç§æœ€å…ˆè¿›çš„å·ç§¯å’Œtransformerè¯­ä¹‰åˆ†å‰²æ¶æ„ä¸­ï¼Œå°†å…ˆå‰çš„DMPNetå·¥ä½œä»åˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹æ‰©å±•ã€‚æˆ‘ä»¬é‡‡ç”¨ç›´æ¥è¾“å…¥çš„æ–¹å¼ï¼Œå³è°ƒæ•´ç‰¹å¾æå–æ¶æ„çš„è¾“å…¥å¹²ä»¥æ¥å—DMPé€šé“ï¼Œä»¥åŠæ··åˆæ¶æ„ï¼ˆä¸€ç§åŒæµè®¾è®¡ï¼Œèåˆäº†RGBå’ŒDMPç¼–ç å™¨ï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨iSAIDåŸºå‡†æ•°æ®é›†è¯„ä¼°äº†å„ç§DMPå·®å¼‚å’Œç»“æ„å…ƒç´ å½¢çŠ¶ï¼Œä»¥æ›´æœ‰æ•ˆåœ°ä¸ºæ¨¡å‹æä¾›å½¢çŠ¶ä¿¡æ¯ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶éDMPæ¨¡å‹é€šå¸¸è¡¨ç°ä¼˜äºç›´æ¥è¾“å…¥å˜ä½“ï¼Œä½†æ··åˆDMPå§‹ç»ˆä¼˜äºç›´æ¥è¾“å…¥ï¼Œå¹¶åœ¨mIoUã€F1å’Œå¬å›ç‡æ–¹é¢èƒ½å¤Ÿè¶…è¶ŠéDMPæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04268v1">PDF</a> 14 pages, 7 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†å°†å·®åˆ†å½¢æ€å­¦è½®å»“ï¼ˆDMPï¼‰èå…¥ç°ä»£åˆ†å‰²ç½‘ç»œï¼Œä»¥åº”å¯¹é¥æ„Ÿå½±åƒè¯­ä¹‰åˆ†å‰²çš„æŒ‘æˆ˜ã€‚é€šè¿‡åœ¨iSAIDåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œå‘ç°æ··åˆDMPæ¶æ„åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç›´æ¥è¾“å…¥å’ŒéDMPæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²åœ¨é¥æ„Ÿå½±åƒä¸­çš„åº”ç”¨å¹¿æ³›ï¼ŒåŒ…æ‹¬åœ°å›¾ç»˜åˆ¶ã€åŸå¸‚è§„åˆ’å’Œç¾å®³å“åº”ç­‰é¢†åŸŸã€‚</li>
<li>å½“å‰å…ˆè¿›çš„åˆ†å‰²ç½‘ç»œä¸»è¦åœ¨åœ°é¢ç…§ç‰‡ä¸Šå¼€å‘å’Œè°ƒæ•´ï¼Œæœªèƒ½ç›´æ¥åº”å¯¹é¥æ„Ÿå½±åƒä¸­çš„æç«¯å°ºåº¦å˜åŒ–ã€å‰æ™¯ä¸èƒŒæ™¯ä¸å¹³è¡¡åŠå¤§å›¾åƒå°ºå¯¸ç­‰é—®é¢˜ã€‚</li>
<li>DMPä½œä¸ºä¸€ç§å¤šå°ºåº¦å½¢çŠ¶æå–æ–¹æ³•ï¼Œèƒ½åŸºäºç°åº¦å½¢æ€å­¦æä¾›å…³é”®å½¢çŠ¶ä¿¡æ¯ã€‚</li>
<li>æœ¬æ–‡å°†DMPç‰¹å¾èå…¥ä¸‰ç§å…ˆè¿›çš„å·ç§¯å’Œtransformerè¯­ä¹‰åˆ†å‰²æ¶æ„ä¸­ã€‚</li>
<li>é‡‡ç”¨ç›´æ¥è¾“å…¥å’Œæ··åˆæ¶æ„ä¸¤ç§æ–¹æ³•æ•´åˆDMPç‰¹å¾ï¼Œå…¶ä¸­æ··åˆæ¶æ„é‡‡ç”¨åŒæµè®¾è®¡ï¼ŒèåˆRGBå’ŒDMPç¼–ç å™¨ã€‚</li>
<li>åœ¨iSAIDåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒéDMPæ¨¡å‹é€šå¸¸ä¼˜äºç›´æ¥è¾“å…¥å˜ä½“ï¼Œè€Œæ··åˆDMPåœ¨mIoUã€F1å’ŒRecallç­‰æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-64fb5cfeef76f7c68febfcd086d49257.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b2ac6e860fa721edfbc2d27d4881896.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bd0b6bb82914f2b549c24974fd678c3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Real-time-Object-Detection-and-Associated-Hardware-Accelerators-Targeting-Autonomous-Vehicles-A-Review"><a href="#Real-time-Object-Detection-and-Associated-Hardware-Accelerators-Targeting-Autonomous-Vehicles-A-Review" class="headerlink" title="Real-time Object Detection and Associated Hardware Accelerators   Targeting Autonomous Vehicles: A Review"></a>Real-time Object Detection and Associated Hardware Accelerators   Targeting Autonomous Vehicles: A Review</h2><p><strong>Authors:Safa Sali, Anis Meribout, Ashiyana Majeed, Mahmoud Meribout, Juan Pablo, Varun Tiwari, Asma Baobaid</strong></p>
<p>The efficiency of object detectors depends on factors like detection accuracy, processing time, and computational resources. Processing time is crucial for real-time applications, particularly for autonomous vehicles (AVs), where instantaneous responses are vital for safety. This review paper provides a concise yet comprehensive survey of real-time object detection (OD) algorithms for autonomous cars delving into their hardware accelerators (HAs). Non-neural network-based algorithms, which use statistical image processing, have been entirely substituted by AI algorithms, such as different models of convolutional neural networks (CNNs). Their intrinsically parallel features led them to be deployable into edge-based HAs of various types, where GPUs and, to a lesser extent, ASIC (application-specific integrated circuit) remain the most widely used. Throughputs of hundreds of frames&#x2F;s (fps) could be reached; however, handling object detection for all the cameras available in a typical AV requires further hardware and algorithmic improvements. The intensive competition between AV providers has limited the disclosure of algorithms, firmware, and even hardware platform details. This remains a hurdle for researchers, as commercial systems provide valuable insights while academics undergo lengthy training and testing on restricted datasets and road scenarios. Consequently, many AV research papers may not be reflected in end products, being developed under limited conditions. This paper surveys state-of-the-art OD algorithms and aims to bridge the gap with technologies in commercial AVs. To our knowledge, this aspect has not been addressed in earlier surveys. Hence, the paper serves as a tangible reference for researchers designing future generations of vehicles, expected to be fully autonomous for comfort and safety. </p>
<blockquote>
<p>ç›®æ ‡æ£€æµ‹å™¨çš„æ•ˆç‡å–å†³äºæ£€æµ‹ç²¾åº¦ã€å¤„ç†æ—¶é—´å’Œè®¡ç®—èµ„æºç­‰å› ç´ ã€‚å¤„ç†æ—¶é—´å¯¹äºå®æ—¶åº”ç”¨è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­ï¼Œç¬æ—¶å“åº”å¯¹å®‰å…¨è‡³å…³é‡è¦ã€‚è¿™ç¯‡ç»¼è¿°è®ºæ–‡å¯¹è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„å®æ—¶ç›®æ ‡æ£€æµ‹ç®—æ³•è¿›è¡Œäº†ç®€æ˜è€Œå…¨é¢çš„è°ƒæŸ¥ï¼Œæ·±å…¥æ¢è®¨äº†å…¶ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚åŸºäºéç¥ç»ç½‘ç»œç®—æ³•çš„ç»Ÿè®¡å›¾åƒå¤„ç†å·²ç»è¢«AIç®—æ³•æ‰€å–ä»£ï¼Œå¦‚å„ç§å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚å®ƒä»¬å›ºæœ‰çš„å¹¶è¡Œç‰¹æ€§ä½¿å…¶å¯ä»¥éƒ¨ç½²åˆ°å„ç§ç±»å‹çš„è¾¹ç¼˜åŸºäºHAsä¸­ï¼Œå…¶ä¸­GPUå’Œè¾ƒå°ç¨‹åº¦çš„ASICä»ç„¶æ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„ã€‚å¯ä»¥è¾¾åˆ°æ•°ç™¾å¸§æ¯ç§’çš„ååé‡ï¼›ç„¶è€Œï¼Œå¤„ç†å…¸å‹è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸Šæ‰€æœ‰æ‘„åƒå¤´ä¸Šçš„ç›®æ ‡æ£€æµ‹éœ€è¦è¿›ä¸€æ­¥çš„ç¡¬ä»¶å’Œç®—æ³•æ”¹è¿›ã€‚è‡ªåŠ¨é©¾é©¶æä¾›å•†ä¹‹é—´çš„æ¿€çƒˆç«äº‰é™åˆ¶äº†ç®—æ³•ã€å›ºä»¶ç”šè‡³æ˜¯ç¡¬ä»¶å¹³å°ç»†èŠ‚çš„æŠ«éœ²ã€‚è¿™å¯¹äºç ”ç©¶äººå‘˜æ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå•†ä¸šç³»ç»Ÿæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œè€Œå­¦æœ¯ç•Œåˆ™åœ¨æœ‰é™çš„æ•°æ®é›†å’Œé“è·¯åœºæ™¯ä¸Šè¿›è¡Œå†—é•¿çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚å› æ­¤ï¼Œè®¸å¤šè‡ªåŠ¨é©¾é©¶æ±½è½¦ç ”ç©¶è®ºæ–‡å¯èƒ½æ— æ³•åæ˜ åœ¨æœ€ç»ˆäº§å“ä¸­ï¼Œè¿™äº›äº§å“æ˜¯åœ¨æœ‰é™æ¡ä»¶ä¸‹å¼€å‘çš„ã€‚è¿™ç¯‡è®ºæ–‡è°ƒæŸ¥äº†æœ€æ–°çš„ç›®æ ‡æ£€æµ‹ç®—æ³•ï¼Œæ—¨åœ¨å¼¥è¡¥ä¸å•†ä¸šè‡ªåŠ¨é©¾é©¶æ±½è½¦æŠ€æœ¯çš„å·®è·ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ–¹é¢çš„å†…å®¹åœ¨ä¹‹å‰çš„è°ƒæŸ¥ä¸­å°šæœªå¾—åˆ°é‡è§†ã€‚å› æ­¤ï¼Œè¯¥è®ºæ–‡å¯¹äºè®¾è®¡æœªæ¥å‡ ä»£æ±½è½¦çš„ç ”ç©¶äººå‘˜å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ï¼Œæœªæ¥çš„æ±½è½¦é¢„è®¡ä¼šå®ç°å®Œå…¨çš„è‡ªåŠ¨åŒ–ï¼Œä»¥æé«˜èˆ’é€‚æ€§å’Œå®‰å…¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04173v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ç»¼è¿°äº†é¢å‘è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å®æ—¶ç›®æ ‡æ£€æµ‹ç®—æ³•åŠå…¶ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚æ–‡ç« æŒ‡å‡ºç›®æ ‡æ£€æµ‹æ•ˆç‡ä¾èµ–äºæ£€æµ‹å‡†ç¡®æ€§ã€å¤„ç†æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚å¯¹äºè‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼Œå¤„ç†æ—¶é—´å°¤ä¸ºå…³é”®ã€‚å½“å‰ï¼Œç¥ç»ç½‘ç»œç®—æ³•å·²æ›¿ä»£éç¥ç»ç½‘ç»œç®—æ³•ï¼Œå…¶ä¸­å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹å› å…¶å¹¶è¡Œç‰¹æ€§å¹¿æ³›åº”ç”¨äºè¾¹ç¼˜å‹ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚ç„¶è€Œï¼Œå¤„ç†æ‰€æœ‰æ‘„åƒå¤´çš„æ•°æ®ä»éœ€è¦ç¡¬ä»¶å’Œç®—æ³•çš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚æ–‡ç« å¼ºè°ƒå•†ä¸šç³»ç»Ÿæä¾›æœ‰ä»·å€¼ä¿¡æ¯çš„åŒæ—¶ï¼Œå­¦æœ¯ç•Œé¢ä¸´çš„é™åˆ¶é˜»ç¢äº†ç ”ç©¶çš„å‘å±•ã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥ç°æœ‰è°ƒæŸ¥ä¸­çš„ç©ºç™½ï¼Œä¸ºæœªæ¥è½¦è¾†è®¾è®¡æä¾›å®é™…å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç›®æ ‡æ£€æµ‹æ•ˆç‡ä¾èµ–äºæ£€æµ‹å‡†ç¡®æ€§ã€å¤„ç†æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚</li>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†éœ€è¦ç¬æ—¶å“åº”ï¼Œå¤„ç†æ—¶é—´è‡³å…³é‡è¦ã€‚</li>
<li>éç¥ç»ç½‘ç»œç®—æ³•å·²è¢«AIç®—æ³•å¦‚å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹æ›¿ä»£ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹çš„å¹¶è¡Œç‰¹æ€§ä½¿å…¶é€‚ç”¨äºå„ç§è¾¹ç¼˜å‹ç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œå°¤å…¶æ˜¯GPUå’ŒASICã€‚</li>
<li>ç›®æ ‡æ£€æµ‹ä»éœ€è¿›ä¸€æ­¥æ”¹è¿›ï¼Œä»¥å¤„ç†è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸Šæ‰€æœ‰æ‘„åƒå¤´çš„æ•°æ®ã€‚</li>
<li>å•†ä¸šç³»ç»Ÿå¯¹è‡ªåŠ¨é©¾é©¶ç›®æ ‡æ£€æµ‹ç®—æ³•çš„ä¿¡æ¯æŠ«éœ²æœ‰é™ï¼Œé˜»ç¢äº†å­¦æœ¯ç ”ç©¶çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c74f0a995f6bb2b8c959510de76fe1a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fdd0fa6df5bd8fa296e418e62749ff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26230f38bc0f13781bdcd3776761c17d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-679200a057bad882fcb59d5749d844c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-015ef4efd80844bf045a3484904ebeb6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Focus-Through-Motion-RGB-Event-Collaborative-Token-Sparsification-for-Efficient-Object-Detection"><a href="#Focus-Through-Motion-RGB-Event-Collaborative-Token-Sparsification-for-Efficient-Object-Detection" class="headerlink" title="Focus Through Motion: RGB-Event Collaborative Token Sparsification for   Efficient Object Detection"></a>Focus Through Motion: RGB-Event Collaborative Token Sparsification for   Efficient Object Detection</h2><p><strong>Authors:Nan Yang, Yang Wang, Zhanwen Liu, Yuchao Dai, Yang Liu, Xiangmo Zhao</strong></p>
<p>Existing RGB-Event detection methods process the low-information regions of both modalities (background in images and non-event regions in event data) uniformly during feature extraction and fusion, resulting in high computational costs and suboptimal performance. To mitigate the computational redundancy during feature extraction, researchers have respectively proposed token sparsification methods for the image and event modalities. However, these methods employ a fixed number or threshold for token selection, hindering the retention of informative tokens for samples with varying complexity. To achieve a better balance between accuracy and efficiency, we propose FocusMamba, which performs adaptive collaborative sparsification of multimodal features and efficiently integrates complementary information. Specifically, an Event-Guided Multimodal Sparsification (EGMS) strategy is designed to identify and adaptively discard low-information regions within each modality by leveraging scene content changes perceived by the event camera. Based on the sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed to effectively capture and integrate complementary features from both modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate that the proposed method achieves superior performance in both accuracy and efficiency compared to existing methods. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/Zizzzzzzz/FocusMamba">https://github.com/Zizzzzzzz/FocusMamba</a>. </p>
<blockquote>
<p>ç°æœ‰RGB-äº‹ä»¶æ£€æµ‹æ–¹æ³•åœ¨ç‰¹å¾æå–å’Œèåˆè¿‡ç¨‹ä¸­ä¼šç»Ÿä¸€å¤„ç†ä¸¤ç§æ¨¡æ€çš„ä½ä¿¡æ¯åŒºåŸŸï¼ˆå›¾åƒä¸­çš„èƒŒæ™¯äº‹ä»¶æ•°æ®ä¸­éäº‹ä»¶åŒºåŸŸï¼‰ï¼Œè¿™å¯¼è‡´äº†è¾ƒé«˜çš„è®¡ç®—æˆæœ¬å’Œæ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†å‡å°‘ç‰¹å¾æå–è¿‡ç¨‹ä¸­çš„è®¡ç®—å†—ä½™ï¼Œç ”ç©¶äººå‘˜åˆ†åˆ«é’ˆå¯¹å›¾åƒå’Œäº‹ä»¶æ¨¡æ€æå‡ºäº†ä»¤ç‰Œç¨€ç–åŒ–æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä½¿ç”¨å›ºå®šçš„æ•°é‡æˆ–é˜ˆå€¼æ¥è¿›è¡Œä»¤ç‰Œé€‰æ‹©ï¼Œé˜»ç¢äº†ä¸åŒå¤æ‚åº¦æ ·æœ¬çš„ä¿¡æ¯ä»¤ç‰Œçš„ä¿ç•™ã€‚ä¸ºäº†å¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†FocusMambaï¼Œå®ƒèƒ½å¤Ÿå¯¹å¤šæ¨¡æ€ç‰¹å¾è¿›è¡Œè‡ªé€‚åº”ååŒç¨€ç–åŒ–ï¼Œå¹¶æœ‰æ•ˆåœ°èåˆäº’è¡¥ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†ä¸€ç§äº‹ä»¶å¼•å¯¼çš„å¤šæ¨¡æ€ç¨€ç–åŒ–ï¼ˆEGMSï¼‰ç­–ç•¥ï¼Œé€šè¿‡åˆ©ç”¨äº‹ä»¶ç›¸æœºæ„ŸçŸ¥çš„åœºæ™¯å†…å®¹å˜åŒ–æ¥è¯†åˆ«å’Œè‡ªé€‚åº”ä¸¢å¼ƒæ¯ä¸ªæ¨¡æ€ä¸­çš„ä½ä¿¡æ¯åŒºåŸŸã€‚åŸºäºç¨€ç–åŒ–ç»“æœï¼Œæå‡ºäº†è·¨æ¨¡æ€ç„¦ç‚¹èåˆï¼ˆCMFFï¼‰æ¨¡å—ï¼Œä»¥æœ‰æ•ˆæ•è·å¹¶èåˆä¸¤ç§æ¨¡æ€çš„äº’è¡¥ç‰¹å¾ã€‚åœ¨DSEC-Detå’ŒPKU-DAVIS-SODæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Zizzzzzzz/FocusMamba%E4%B8%8A%E6%8F%9B%E4%BA%8E%E3%80%82">https://github.com/Zizzzzzzz/FocusMambaä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03872v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºä¼ ç»ŸRGB-Eventæ£€æµ‹æ–¹æ³•åœ¨å¤„ç†å›¾åƒå’Œäº‹ä»¶æ•°æ®çš„ä½ä¿¡æ¯åŒºåŸŸæ—¶å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºè¿›è¡Œç‰¹å¾æå–æ—¶çš„ä»¤ç‰Œç¨€ç–åŒ–æ–¹æ³•ã€‚ä½†å½“å‰çš„æ–¹æ³•åœ¨ä»¤ç‰Œé€‰æ‹©æ—¶é‡‡ç”¨å›ºå®šçš„æ•°å€¼æˆ–é˜ˆå€¼ï¼Œæ— æ³•ä¿ç•™æ ·æœ¬ä¸åŒå¤æ‚æ€§ä¸‹çš„ä¿¡æ¯ä»¤ç‰Œã€‚æœ¬æ–‡æå‡ºçš„FocusMambaæ–¹æ³•èƒ½å¤Ÿå®ç°å¤šæ¨¡æ€ç‰¹å¾çš„è‡ªé€‚åº”ååŒç¨€ç–åŒ–ï¼Œå¹¶æœ‰æ•ˆåœ°èåˆäº’è¡¥ä¿¡æ¯ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡äº‹ä»¶å¼•å¯¼çš„å¤šæ¨¡æ€ç¨€ç–åŒ–ç­–ç•¥ï¼Œè‡ªé€‚åº”åœ°ä¸¢å¼ƒå„æ¨¡æ€çš„ä½ä¿¡æ¯åŒºåŸŸï¼Œåˆ©ç”¨äº‹ä»¶ç›¸æœºæ„ŸçŸ¥çš„åœºæ™¯å†…å®¹å˜åŒ–æ¥å®ç°è¿™ä¸€ç›®çš„ã€‚æ ¹æ®ç¨€ç–åŒ–ç»“æœï¼Œæå‡ºè·¨æ¨¡æ€èšç„¦èåˆæ¨¡å—ï¼Œä»¥æ•æ‰å¹¶èåˆä¸¤ç§æ¨¡æ€çš„äº’è¡¥ç‰¹å¾ã€‚åœ¨DSEC-Detå’ŒPKU-DAVIS-SODæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RGB-Eventæ£€æµ‹æ–¹æ³•é¢ä¸´è®¡ç®—æˆæœ¬é«˜å’Œæ€§èƒ½ä¸ä½³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä½ä¿¡æ¯åŒºåŸŸæ—¶ã€‚</li>
<li>å½“å‰çš„ç‰¹å¾æå–æ–¹æ³•é‡‡ç”¨å›ºå®šçš„ä»¤ç‰Œé€‰æ‹©æœºåˆ¶ï¼Œä¸åˆ©äºå¤„ç†å¤æ‚åº¦ä¸åŒçš„æ ·æœ¬ã€‚</li>
<li>FocusMambaæ–¹æ³•é€šè¿‡è‡ªé€‚åº”ååŒç¨€ç–åŒ–å¤šæ¨¡æ€ç‰¹å¾æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>äº‹ä»¶å¼•å¯¼çš„å¤šæ¨¡æ€ç¨€ç–åŒ–ç­–ç•¥èƒ½å¤Ÿè‡ªé€‚åº”åœ°è¯†åˆ«å¹¶ä¸¢å¼ƒä½ä¿¡æ¯åŒºåŸŸã€‚</li>
<li>è·¨æ¨¡æ€èšç„¦èåˆæ¨¡å—æœ‰æ•ˆæ•æ‰å¹¶èåˆäº†ä¸¤ç§æ¨¡æ€çš„äº’è¡¥ç‰¹å¾ã€‚</li>
<li>åœ¨DSEC-Detå’ŒPKU-DAVIS-SODæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒFocusMambaæ–¹æ³•è¾ƒç°æœ‰æ–¹æ³•æ›´å…·ä¼˜åŠ¿å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9c43c045ad32896452fc8e87f33320a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a35142718199ec6f4bd55c06c8a715b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb74a9edd9a5638aa47d0754517f3b40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31a1d64a98506ff97444056394d6cb96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2174d2249d7bb028d5e7b546ae3ee086.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bdc4c09ec79f8e3da6876b6425a008e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AutoDetect-Designing-an-Autoencoder-based-Detection-Method-for-Poisoning-Attacks-on-Object-Detection-Applications-in-the-Military-Domain"><a href="#AutoDetect-Designing-an-Autoencoder-based-Detection-Method-for-Poisoning-Attacks-on-Object-Detection-Applications-in-the-Military-Domain" class="headerlink" title="AutoDetect: Designing an Autoencoder-based Detection Method for   Poisoning Attacks on Object Detection Applications in the Military Domain"></a>AutoDetect: Designing an Autoencoder-based Detection Method for   Poisoning Attacks on Object Detection Applications in the Military Domain</h2><p><strong>Authors:Alma M. Liezenga, Stefan Wijnja, Puck de Haan, Niels W. T. Brink, Jip J. van Stijn, Yori Kamphuis, Klamer Schutte</strong></p>
<p>Poisoning attacks pose an increasing threat to the security and robustness of Artificial Intelligence systems in the military domain. The widespread use of open-source datasets and pretrained models exacerbates this risk. Despite the severity of this threat, there is limited research on the application and detection of poisoning attacks on object detection systems. This is especially problematic in the military domain, where attacks can have grave consequences. In this work, we both investigate the effect of poisoning attacks on military object detectors in practice, and the best approach to detect these attacks. To support this research, we create a small, custom dataset featuring military vehicles: MilCivVeh. We explore the vulnerability of military object detectors for poisoning attacks by implementing a modified version of the BadDet attack: a patch-based poisoning attack. We then assess its impact, finding that while a positive attack success rate is achievable, it requires a substantial portion of the data to be poisoned â€“ raising questions about its practical applicability. To address the detection challenge, we test both specialized poisoning detection methods and anomaly detection methods from the visual industrial inspection domain. Since our research shows that both classes of methods are lacking, we introduce our own patch detection method: AutoDetect, a simple, fast, and lightweight autoencoder-based method. Our method shows promising results in separating clean from poisoned samples using the reconstruction error of image slices, outperforming existing methods, while being less time- and memory-intensive. We urge that the availability of large, representative datasets in the military domain is a prerequisite to further evaluate risks of poisoning attacks and opportunities patch detection. </p>
<blockquote>
<p>ä¸­æ¯’æ”»å‡»å¯¹å†›äº‹é¢†åŸŸäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨å’Œç¨³å¥æ€§æ„æˆæ—¥ç›Šä¸¥é‡çš„å¨èƒã€‚å¼€æºæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹çš„å¹¿æ³›ä½¿ç”¨åŠ å‰§äº†è¿™ä¸€é£é™©ã€‚å°½ç®¡è¿™ä¸€å¨èƒååˆ†ä¸¥é‡ï¼Œä½†å…³äºç‰©ä½“æ£€æµ‹ç³»ç»Ÿä¸­æ¯’æ”»å‡»çš„åº”ç”¨å’Œæ£€æµ‹çš„ç ”ç©¶å´ååˆ†æœ‰é™ã€‚åœ¨å†›äº‹é¢†åŸŸä¸­ï¼Œæ”»å‡»å¯èƒ½å¸¦æ¥ä¸¥é‡åæœï¼Œè¿™ä¸€é—®é¢˜å°¤ä¸ºä¸¥é‡ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ä»…å¯¹å®è·µä¸­ä¸­æ¯’æ”»å‡»å¯¹å†›äº‹ç‰©ä½“æ£€æµ‹å™¨çš„å½±å“è¿›è¡Œäº†è°ƒæŸ¥ï¼Œè¿˜ç ”ç©¶äº†æ£€æµ‹è¿™äº›æ”»å‡»çš„æœ€ä½³æ–¹æ³•ã€‚ä¸ºäº†æ”¯æŒè¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«å†›ç”¨è½¦è¾†çš„å°å‹è‡ªå®šä¹‰æ•°æ®é›†ï¼šMilCivVehã€‚æˆ‘ä»¬é€šè¿‡å®ç°ä¸€ç§åŸºäºè¡¥ä¸çš„ä¸­æ¯’æ”»å‡»â€”â€”BadDetæ”»å‡»çš„æ”¹è¿›ç‰ˆï¼Œæ¥æ¢ç´¢å†›äº‹ç‰©ä½“æ£€æµ‹å™¨å¯¹ä¸­æ¯’æ”»å‡»çš„è„†å¼±æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯„ä¼°äº†å®ƒçš„å½±å“ï¼Œå‘ç°è™½ç„¶å¯ä»¥å®ç°ç§¯æçš„æ”»å‡»æˆåŠŸç‡ï¼Œä½†éœ€è¦å¤§é‡æ•°æ®è¢«æ¯’å®³â€”â€”è¿™å¼•å‘äº†å¯¹å…¶å®é™…å¯è¡Œæ€§çš„è´¨ç–‘ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03179v1">PDF</a> To be presented at SPIE: Sensors + Imaging, Artificial Intelligence   for Security and Defence Applications II</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å†›äº‹äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ä¸­æ¯’æ”»å‡»å¨èƒæ—¥ç›Šä¸¥é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è±¡æ£€æµ‹ç³»ç»Ÿä¸­ã€‚ç ”ç©¶å›¢é˜Ÿè°ƒæŸ¥äº†ä¸­æ¯’æ”»å‡»å¯¹å†›äº‹å¯¹è±¡æ£€æµ‹å™¨çš„å½±å“ï¼Œå¹¶æ¢ç´¢äº†æ£€æµ‹è¿™äº›æ”»å‡»çš„æœ€ä½³æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªå†›äº‹è½¦è¾†ä¸“ç”¨çš„å°å‹æ•°æ®é›†MilCivVehï¼Œå¹¶å®ç°äº†åŸºäºè¡¥ä¸çš„ä¸­æ¯’æ”»å‡»BadDetçš„ä¿®æ”¹ç‰ˆã€‚è¯„ä¼°å‘ç°ï¼Œè™½ç„¶æ”»å‡»æˆåŠŸç‡é«˜ï¼Œä½†éœ€è¦å¤§é‡æ•°æ®è¢«ç¯¡æ”¹ï¼Œå®ç”¨æ€§å­˜ç–‘ã€‚é’ˆå¯¹æ£€æµ‹éš¾é¢˜ï¼Œä»–ä»¬æµ‹è¯•äº†ä¸“é—¨çš„æ¯’è¯æ£€æµ‹æ–¹æ³•å’Œæ¥è‡ªè§†è§‰å·¥ä¸šæ£€æµ‹é¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œä½†æ•ˆæœä¸ä½³ã€‚å› æ­¤ï¼Œä»–ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„è¡¥ä¸æ£€æµ‹æ–¹æ³•AutoDetectï¼Œèƒ½å¤Ÿåˆ©ç”¨å›¾åƒåˆ‡ç‰‡çš„é‡å»ºè¯¯å·®æ¥åŒºåˆ†æ¸…æ´æ ·æœ¬å’Œä¸­æ¯’æ ·æœ¬ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸­æ¯’æ”»å‡»å¯¹å†›äº‹äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¨èƒæ—¥ç›Šä¸¥é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è±¡æ£€æµ‹ç³»ç»Ÿä¸­ã€‚</li>
<li>ç›®å‰é’ˆå¯¹å†›äº‹å¯¹è±¡æ£€æµ‹ç³»ç»Ÿçš„ä¸­æ¯’æ”»å‡»ç ”ç©¶æœ‰é™ã€‚</li>
<li>åˆ›å»ºä¸€ä¸ªå†›äº‹è½¦è¾†ä¸“ç”¨çš„å°å‹æ•°æ®é›†MilCivVehä»¥æ”¯æŒç ”ç©¶ã€‚</li>
<li>å®ç°äº†åŸºäºè¡¥ä¸çš„ä¸­æ¯’æ”»å‡»BadDetçš„ä¿®æ”¹ç‰ˆï¼Œå¹¶è¯„ä¼°äº†å…¶å½±å“ã€‚</li>
<li>æ£€æµ‹ä¸­æ¯’æ”»å‡»çš„æŒ‘æˆ˜åœ¨äºç°æœ‰æ–¹æ³•çš„æ•ˆæœä¸ä½³ã€‚</li>
<li>æå‡ºçš„AutoDetectæ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåˆ©ç”¨å›¾åƒåˆ‡ç‰‡çš„é‡å»ºè¯¯å·®æ¥åŒºåˆ†æ¸…æ´æ ·æœ¬å’Œä¸­æ¯’æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb346271c7ed9b2b48ba9481d2349467.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="InstaDA-Augmenting-Instance-Segmentation-Data-with-Dual-Agent-System"><a href="#InstaDA-Augmenting-Instance-Segmentation-Data-with-Dual-Agent-System" class="headerlink" title="InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System"></a>InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System</h2><p><strong>Authors:Xianbao Hou, Yonghao He, Zeyd Boukhers, John See, Hu Su, Wei Sui, Cong Yang</strong></p>
<p>Acquiring high-quality instance segmentation data is challenging due to the labor-intensive nature of the annotation process and significant class imbalances within datasets. Recent studies have utilized the integration of Copy-Paste and diffusion models to create more diverse datasets. However, these studies often lack deep collaboration between large language models (LLMs) and diffusion models, and underutilize the rich information within the existing training data. To address these limitations, we propose InstaDA, a novel, training-free Dual-Agent system designed to augment instance segmentation datasets. First, we introduce a Text-Agent (T-Agent) that enhances data diversity through collaboration between LLMs and diffusion models. This agent features a novel Prompt Rethink mechanism, which iteratively refines prompts based on the generated images. This process not only fosters collaboration but also increases image utilization and optimizes the prompts themselves. Additionally, we present an Image-Agent (I-Agent) aimed at enriching the overall data distribution. This agent augments the training set by generating new instances conditioned on the training images. To ensure practicality and efficiency, both agents operate as independent and automated workflows, enhancing usability. Experiments conducted on the LVIS 1.0 validation set indicate that InstaDA achieves significant improvements, with an increase of +4.0 in box average precision (AP) and +3.3 in mask AP compared to the baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common categories and mask AP gains of +0.2 on common categories and +0.5 on frequent categories. </p>
<blockquote>
<p>è·å–é«˜è´¨é‡çš„å®ä¾‹åˆ†å‰²æ•°æ®æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºæ ‡æ³¨è¿‡ç¨‹åŠ³åŠ¨å¯†é›†ä¸”æ•°æ®é›†ä¸­å­˜åœ¨ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡ç»“åˆCopy-Pasteå’Œæ‰©æ•£æ¨¡å‹æ¥åˆ›å»ºæ›´å¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶å¾€å¾€ç¼ºä¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æ·±åº¦åä½œï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†åˆ©ç”¨ç°æœ‰è®­ç»ƒæ•°æ®ä¸­çš„ä¸°å¯Œä¿¡æ¯ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†InstaDAï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹åŒä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºå®ä¾‹åˆ†å‰²æ•°æ®é›†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–‡æœ¬ä»£ç†ï¼ˆT-Agentï¼‰ï¼Œå®ƒé€šè¿‡LLMå’Œæ‰©æ•£æ¨¡å‹çš„åä½œæ¥æé«˜æ•°æ®å¤šæ ·æ€§ã€‚è¯¥ä»£ç†å…·æœ‰æ–°é¢–çš„æç¤ºåæ€æœºåˆ¶ï¼Œå¯ä»¥æ ¹æ®ç”Ÿæˆçš„å›¾åƒè¿­ä»£ä¼˜åŒ–æç¤ºã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ä»…ä¿ƒè¿›äº†åä½œï¼Œè¿˜æé«˜äº†å›¾åƒåˆ©ç”¨ç‡å¹¶ä¼˜åŒ–äº†æç¤ºæœ¬èº«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†å›¾åƒä»£ç†ï¼ˆI-Agentï¼‰ï¼Œæ—¨åœ¨ä¸°å¯Œæ•´ä½“æ•°æ®åˆ†å¸ƒã€‚è¯¥ä»£ç†é€šè¿‡åŸºäºè®­ç»ƒå›¾åƒç”Ÿæˆæ–°å®ä¾‹æ¥ä¸°å¯Œè®­ç»ƒé›†ã€‚ä¸ºç¡®ä¿å®ç”¨æ€§å’Œæ•ˆç‡ï¼Œä¸¤ä¸ªä»£ç†ä½œä¸ºç‹¬ç«‹ã€è‡ªåŠ¨åŒ–çš„å·¥ä½œæµç¨‹è¿›è¡Œæ“ä½œï¼Œå¢å¼ºäº†æ˜“ç”¨æ€§ã€‚åœ¨LVIS 1.0éªŒè¯é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒInstaDAå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œæ¡†å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰æé«˜äº†+4.0ï¼Œæ©ç APæé«˜äº†+3.3ï¼Œè¶…è¿‡äº†åŸºçº¿ã€‚æ­¤å¤–ï¼Œå®ƒè¶…è¶Šäº†é¢†å…ˆæ¨¡å‹DiverGenï¼Œåœ¨æ¡†APä¸Šæé«˜äº†+0.3ï¼Œåœ¨æ©ç APä¸Šæé«˜äº†+0.1ã€‚åœ¨å¸¸è§ç±»åˆ«ä¸Šæ¡†APå¢åŠ äº†+0.7ï¼Œæ©ç APåœ¨å¸¸è§ç±»åˆ«ä¸Šå¢åŠ äº†+0.2ï¼Œåœ¨é¢‘ç¹ç±»åˆ«ä¸Šå¢åŠ äº†+0.5ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02973v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å®ä¾‹åˆ†å‰²æ•°æ®è·å–å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†InstaDAè®­ç»ƒå…è´¹åŒä»£ç†ç³»ç»Ÿï¼Œé€šè¿‡æ–‡æœ¬ä»£ç†å’Œå›¾åƒä»£ç†å¢å¼ºæ•°æ®å¤šæ ·æ€§å’Œä¸°å¯Œæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒInstaDAåœ¨LVIS 1.0éªŒè¯é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œæé«˜äº†ç›’å­å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰å’Œæ©è†œAPã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>InstaDAæ˜¯ä¸€ä¸ªè®­ç»ƒå…è´¹çš„åŒä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºå®ä¾‹åˆ†å‰²æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬ä»£ç†ï¼ˆT-Agentï¼‰å’Œå›¾åƒä»£ç†ï¼ˆI-Agentï¼‰å¢å¼ºæ•°æ®å¤šæ ·æ€§å’Œä¸°å¯Œæ€§ã€‚</li>
<li>T-Agentå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„Prompt Rethinkæœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥è¿­ä»£ä¼˜åŒ–æç¤ºå¹¶å¢åŠ å›¾åƒåˆ©ç”¨ç‡ã€‚</li>
<li>I-Agentæ—¨åœ¨ä¸°å¯Œæ•´ä½“æ•°æ®åˆ†å¸ƒï¼Œé€šè¿‡åŸºäºè®­ç»ƒå›¾åƒç”Ÿæˆæ–°å®ä¾‹æ¥å®ç°ã€‚</li>
<li>ä¸¤ä¸ªä»£ç†ä½œä¸ºç‹¬ç«‹è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹è¿è¡Œï¼Œæé«˜äº†å®ç”¨æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒInstaDAåœ¨LVIS 1.0éªŒè¯é›†ä¸Šå–å¾—äº†æ”¹è¿›ï¼ŒåŒ…æ‹¬ç›’å­å¹³å‡ç²¾åº¦å’Œæ©è†œAPçš„æ˜¾è‘—æé«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3139b3040451b68ddd382b752666887e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-159c7e4bb1c29f43491dbec6c1215a96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0b9f956812c4ea28eea01172d744d0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8f2a265aa6e90b9051c100a0c87a565.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f68b720446c9e7e1b8d771b09543bb7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e4a63937daeda1eeff116d0e094a017.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Explaining-What-Machines-See-XAI-Strategies-in-Deep-Object-Detection-Models"><a href="#Explaining-What-Machines-See-XAI-Strategies-in-Deep-Object-Detection-Models" class="headerlink" title="Explaining What Machines See: XAI Strategies in Deep Object Detection   Models"></a>Explaining What Machines See: XAI Strategies in Deep Object Detection   Models</h2><p><strong>Authors:FatemehSadat Seyedmomeni, Mohammad Ali Keyvanrad</strong></p>
<p>In recent years, deep learning has achieved unprecedented success in various computer vision tasks, particularly in object detection. However, the black-box nature and high complexity of deep neural networks pose significant challenges for interpretability, especially in critical domains such as autonomous driving, medical imaging, and security systems. Explainable Artificial Intelligence (XAI) aims to address this challenge by providing tools and methods to make model decisions more transparent, interpretable, and trust-worthy for humans. This review provides a comprehensive analysis of state-of-the-art explain-ability methods specifically applied to object detection models. The paper be-gins by categorizing existing XAI techniques based on their underlying mechanisms-perturbation-based, gradient-based, backpropagation-based, and graph-based methods. Notable methods such as D-RISE, BODEM, D-CLOSE, and FSOD are discussed in detail. Furthermore, the paper investigates their applicability to various object detection architectures, including YOLO, SSD, Faster R-CNN, and EfficientDet. Statistical analysis of publication trends from 2022 to mid-2025 shows an accelerating interest in explainable object detection, indicating its increasing importance. The study also explores common datasets and evaluation metrics, and highlights the major challenges associated with model interpretability. By providing a structured taxonomy and a critical assessment of existing methods, this review aims to guide researchers and practitioners in selecting suitable explainability techniques for object detection applications and to foster the development of more interpretable AI systems. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ åœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸã€‚ç„¶è€Œï¼Œæ·±åº¦ç¥ç»ç½‘ç»œçš„é»‘ç›’æ€§å’Œé«˜å¤æ‚æ€§ç»™å¯è§£é‡Šæ€§å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—æˆåƒå’Œå®‰å…¨ç³»ç»Ÿç­‰å…³é”®é¢†åŸŸã€‚å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ—¨åœ¨é€šè¿‡æä¾›å·¥å…·å’Œæ–¹æ³•æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½¿æ¨¡å‹å†³ç­–æ›´åŠ é€æ˜ã€å¯è§£é‡Šå’Œå€¼å¾—ä¿¡èµ–ã€‚è¿™ç¯‡ç»¼è¿°å¯¹ä¸“é—¨åº”ç”¨äºç›®æ ‡æ£€æµ‹æ¨¡å‹çš„æœ€å…ˆè¿›è§£é‡Šæ–¹æ³•è¿›è¡Œäº†ç»¼åˆåˆ†æã€‚æ–‡ç« é¦–å…ˆæ ¹æ®ç°æœ‰XAIæŠ€æœ¯çš„åº•å±‚æœºåˆ¶è¿›è¡Œåˆ†ç±»ï¼ŒåŒ…æ‹¬åŸºäºæ‰°åŠ¨çš„æ–¹æ³•ã€åŸºäºæ¢¯åº¦çš„æ–¹æ³•ã€åŸºäºåå‘ä¼ æ’­çš„æ–¹æ³•å’ŒåŸºäºå›¾çš„æ–¹æ³•ã€‚å¯¹D-RISEã€BODEMã€D-CLOSEå’ŒFSODç­‰å€¼å¾—æ³¨æ„çš„æ–¹æ³•è¿›è¡Œäº†è¯¦ç»†ä»‹ç»ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†å®ƒä»¬åœ¨å„ç§ç›®æ ‡æ£€æµ‹æ¶æ„ï¼ˆåŒ…æ‹¬YOLOã€SSDã€Faster R-CNNå’ŒEfficientDetï¼‰ä¸­çš„é€‚ç”¨æ€§ã€‚ä»2022å¹´åˆ°2025å¹´ä¸­æœŸå…³äºè§£é‡Šæ€§ç›®æ ‡æ£€æµ‹çš„å‡ºç‰ˆç‰©è¶‹åŠ¿ç»Ÿè®¡åˆ†ææ˜¾ç¤ºï¼Œäººä»¬å¯¹è§£é‡Šæ€§ç›®æ ‡æ£€æµ‹çš„å…´è¶£æ­£åœ¨åŠ é€Ÿå¢é•¿ï¼Œè¿™è¡¨æ˜å…¶é‡è¦æ€§æ—¥ç›Šå¢åŠ ã€‚è¯¥ç ”ç©¶è¿˜æ¢è®¨äº†å¸¸è§æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æŒ‡å‡ºäº†ä¸æ¨¡å‹è§£é‡Šæ€§ç›¸å…³çš„é‡å¤§æŒ‘æˆ˜ã€‚é€šè¿‡æä¾›ç°æœ‰æ–¹æ³•çš„ç»“æ„åŒ–åˆ†ç±»å’Œæ‰¹åˆ¤æ€§è¯„ä¼°ï¼Œæœ¬ç»¼è¿°æ—¨åœ¨æŒ‡å¯¼ç ”ç©¶è€…å’Œå®è·µè€…ä¸ºç›®æ ‡æ£€æµ‹åº”ç”¨é€‰æ‹©åˆé€‚çš„è§£é‡ŠæŠ€æœ¯ï¼Œå¹¶æ¨åŠ¨å¼€å‘æ›´å…·è§£é‡Šæ€§çš„AIç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01991v1">PDF</a> 71 pages, 47 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç»¼è¿°äº†åº”ç”¨äºå¯¹è±¡æ£€æµ‹æ¨¡å‹çš„æœ€æ–°è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹å†³ç­–çš„ä¸é€æ˜å’Œä¸å¯è§£é‡Šæ€§é—®é¢˜ã€‚æ–‡ç« åˆ†ç±»ä»‹ç»äº†åŸºäºæ‰°åŠ¨ã€æ¢¯åº¦ã€åå‘ä¼ æ’­å’Œå›¾å½¢æœºåˆ¶çš„XAIæŠ€æœ¯ï¼Œå¹¶è¯¦ç»†è®¨è®ºäº†å‡ ç§è‘—åæ–¹æ³•ï¼Œå¦‚D-RISEã€BODEMã€D-CLOSEå’ŒFSODã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ç ”ç©¶äº†è¿™äº›æ–¹æ³•åœ¨YOLOã€SSDã€Faster R-CNNå’ŒEfficientDetç­‰å¯¹è±¡æ£€æµ‹æ¶æ„ä¸­çš„åº”ç”¨ã€‚ç»Ÿè®¡æ•°æ®æ˜¾ç¤ºï¼Œä»2022å¹´åˆ°2025å¹´ä¸­ï¼Œå¯¹å¯è§£é‡Šæ€§å¯¹è±¡æ£€æµ‹çš„å…´è¶£æ­£åœ¨åŠ é€Ÿå¢é•¿ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æä¾›ç»“æ„åŒ–çš„åˆ†ç±»å’Œé‡è¦è¯„ä¼°ï¼ŒæŒ‡å¯¼ç ”ç©¶è€…å’Œå®è·µè€…é€‰æ‹©é€‚åˆå¯¹è±¡æ£€æµ‹åº”ç”¨çš„è§£é‡ŠæŠ€æœ¯ï¼Œæ¨åŠ¨æ›´å¯è§£é‡Šçš„AIç³»ç»Ÿçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸­çš„æˆåŠŸå¸¦æ¥äº†æ¨¡å‹å†³ç­–é€æ˜æ€§å’Œè§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•æ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½¿æ¨¡å‹å†³ç­–æ›´åŠ é€æ˜ã€å¯è§£é‡Šå’Œå€¼å¾—ä¿¡èµ–ã€‚</li>
<li>ç»¼è¿°å¯¹å½“å‰åº”ç”¨äºå¯¹è±¡æ£€æµ‹æ¨¡å‹çš„XAIæŠ€æœ¯è¿›è¡Œäº†å…¨é¢åˆ†æï¼ŒåŒ…æ‹¬åŸºäºæ‰°åŠ¨ã€æ¢¯åº¦ã€åå‘ä¼ æ’­å’Œå›¾å½¢çš„æ–¹æ³•ã€‚</li>
<li>æ–‡ç« è¯¦ç»†è®¨è®ºäº†å‡ ç§è‘—åçš„XAIæ–¹æ³•ï¼Œå¦‚D-RISEã€BODEMã€D-CLOSEå’ŒFSODã€‚</li>
<li>XAIæŠ€æœ¯åœ¨å¤šç§å¯¹è±¡æ£€æµ‹æ¶æ„ï¼ˆå¦‚YOLOã€SSDã€Faster R-CNNå’ŒEfficientDetï¼‰ä¸­æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç»Ÿè®¡æ•°æ®æ˜¾ç¤ºï¼Œå¯¹å¯è§£é‡Šæ€§å¯¹è±¡æ£€æµ‹çš„ç ”ç©¶å…´è¶£æ­£åœ¨åŠ é€Ÿå¢é•¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8f9f2399bd04bd99e43ac2030e666ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f41f7f312af9f6c025a0d6cc2619e53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b368a911e725f28158eaef4c2732bef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc50f87cf0c3fba65debf1cb08f23d59.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Image-Quality-Enhancement-and-Detection-of-Small-and-Dense-Objects-in-Industrial-Recycling-Processes"><a href="#Image-Quality-Enhancement-and-Detection-of-Small-and-Dense-Objects-in-Industrial-Recycling-Processes" class="headerlink" title="Image Quality Enhancement and Detection of Small and Dense Objects in   Industrial Recycling Processes"></a>Image Quality Enhancement and Detection of Small and Dense Objects in   Industrial Recycling Processes</h2><p><strong>Authors:Oussama Messai, Abbass Zein-Eddine, Abdelouahid Bentamou, MickaÃ«l Picq, Nicolas Duquesne, StÃ©phane Puydarrieux, Yann Gavet</strong></p>
<p>This paper tackles two key challenges: detecting small, dense, and overlapping objects (a major hurdle in computer vision) and improving the quality of noisy images, especially those encountered in industrial environments. [1, 2]. Our focus is on evaluating methods built on supervised deep learning. We perform an analysis of these methods, using a newly developed dataset comprising over 10k images and 120k instances. By evaluating their performance, accuracy, and computational efficiency, we identify the most reliable detection systems and highlight the specific challenges they address in industrial applications. This paper also examines the use of deep learning models to improve image quality in noisy industrial environments. We introduce a lightweight model based on a fully connected convolutional network. Additionally, we suggest potential future directions for further enhancing the effectiveness of the model. The repository of the dataset and proposed model can be found at: <a target="_blank" rel="noopener" href="https://github.com/o-messai/SDOOD">https://github.com/o-messai/SDOOD</a>, <a target="_blank" rel="noopener" href="https://github.com/o-messai/DDSRNet">https://github.com/o-messai/DDSRNet</a> </p>
<blockquote>
<p>æœ¬æ–‡è§£å†³äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ£€æµ‹å°ã€å¯†é›†ã€é‡å çš„ç‰©ä½“ï¼ˆè®¡ç®—æœºè§†è§‰ä¸­çš„ä¸»è¦éšœç¢ï¼‰ä»¥åŠæé«˜å«å™ªå›¾åƒçš„è´¨é‡ï¼Œå°¤å…¶æ˜¯å·¥ä¸šç¯å¢ƒä¸­é‡åˆ°çš„å›¾åƒã€‚ [1, 2]ã€‚æˆ‘ä»¬çš„é‡ç‚¹æ˜¯å¯¹åŸºäºæœ‰ç›‘ç£æ·±åº¦å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨æ–°å¼€å‘çš„åŒ…å«è¶…è¿‡10kå¼ å›¾åƒå’Œ12ä¸‡ä¸ªå®ä¾‹çš„æ•°æ®é›†å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œåˆ†æã€‚é€šè¿‡è¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ã€å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œæˆ‘ä»¬ç¡®å®šäº†æœ€å¯é çš„æ£€æµ‹ç³»ç»Ÿï¼Œå¹¶å¼ºè°ƒäº†å®ƒä»¬åœ¨å·¥ä¸šåº”ç”¨ä¸­è§£å†³çš„å…·ä½“æŒ‘æˆ˜ã€‚æœ¬æ–‡è¿˜æ¢è®¨äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æé«˜å·¥ä¸šç¯å¢ƒä¸­å«å™ªå›¾åƒè´¨é‡çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå…¨è¿æ¥å·ç§¯ç½‘ç»œçš„è½»é‡çº§æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹æœ‰æ•ˆæ€§çš„æ½œåœ¨æœªæ¥æ–¹å‘ã€‚æ•°æ®é›†å’Œæ‰€æå‡ºæ¨¡å‹çš„å­˜å‚¨åº“å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/o-messai/SDOOD">https://github.com/o-messai/SDOOD</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/o-messai/DDSRNet%E3%80%82">https://github.com/o-messai/DDSRNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01332v1">PDF</a> Event: Seventeenth International Conference on Quality Control by   Artificial Vision (QCAV2025), 2025, Yamanashi Prefecture, Japan</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯æ£€æµ‹å°ã€å¯†é›†å’Œé‡å çš„ç‰©ä½“ï¼ˆè®¡ç®—æœºè§†è§‰ä¸­çš„ä¸»è¦éšœç¢ï¼‰ï¼ŒäºŒæ˜¯æé«˜å·¥ä¸šç¯å¢ƒä¸­å™ªå£°å›¾åƒçš„è´¨é‡ã€‚æ–‡ç« é‡ç‚¹è¯„ä¼°åŸºäºç›‘ç£æ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨æ–°å¼€å‘çš„åŒ…å«è¶…è¿‡10kå›¾åƒå’Œ120kå®ä¾‹çš„æ•°æ®é›†è¿›è¡Œåˆ†æã€‚é€šè¿‡è¯„ä¼°å…¶æ€§èƒ½ã€å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œç¡®å®šäº†æœ€å¯é çš„æ£€æµ‹ç³»ç»Ÿï¼Œå¹¶å¼ºè°ƒäº†å®ƒä»¬åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„ç‰¹å®šæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†åŸºäºå…¨å·ç§¯ç½‘ç»œçš„è½»é‡çº§æ¨¡å‹æ¥æé«˜å™ªå£°å·¥ä¸šç¯å¢ƒä¸­çš„å›¾åƒè´¨é‡ï¼Œå¹¶æå‡ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹æœ‰æ•ˆæ€§çš„æ½œåœ¨æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡è§£å†³äº†æ£€æµ‹å°ã€å¯†é›†å’Œé‡å ç‰©ä½“ä»¥åŠæé«˜å·¥ä¸šç¯å¢ƒä¸­å™ªå£°å›¾åƒè´¨é‡ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æ–‡ç« ä½¿ç”¨äº†æ–°å¼€å‘çš„åŒ…å«å¤§é‡å›¾åƒå’Œå®ä¾‹çš„æ•°æ®é›†æ¥è¯„ä¼°åŸºäºç›‘ç£æ·±åº¦å­¦ä¹ çš„æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¯¹æ€§èƒ½ã€å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡çš„è¯„ä¼°ï¼Œç¡®å®šäº†æœ€å¯é çš„æ£€æµ‹ç³»ç»Ÿã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†è¿™äº›ç³»ç»Ÿåœ¨å·¥ä¸šåº”ç”¨ä¸­çš„ç‰¹å®šæŒ‘æˆ˜ã€‚</li>
<li>ä»‹ç»äº†åŸºäºå…¨å·ç§¯ç½‘ç»œçš„è½»é‡çº§æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å™ªå£°å·¥ä¸šç¯å¢ƒä¸­çš„å›¾åƒè´¨é‡ã€‚</li>
<li>æ–‡ç« æå‡ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹æœ‰æ•ˆæ€§çš„æ½œåœ¨æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01332">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-892f35cb448d2bba6feb48f7191e310d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ae1b0a2cf662d5629a58c255bb51549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f02ae284f5e63680b1c6ff5d6dbca4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0939caf7be369ab3f5005bc0e3fd9721.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8521693b52d741edc91ea33645c23a59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c90140cbd0b654b54ca9dc1807ee2fdc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multi-Representation-Adapter-with-Neural-Architecture-Search-for-Efficient-Range-Doppler-Radar-Object-Detection"><a href="#Multi-Representation-Adapter-with-Neural-Architecture-Search-for-Efficient-Range-Doppler-Radar-Object-Detection" class="headerlink" title="Multi-Representation Adapter with Neural Architecture Search for   Efficient Range-Doppler Radar Object Detection"></a>Multi-Representation Adapter with Neural Architecture Search for   Efficient Range-Doppler Radar Object Detection</h2><p><strong>Authors:Zhiwei Lin, Weicheng Zheng, Yongtao Wang</strong></p>
<p>Detecting objects efficiently from radar sensors has recently become a popular trend due to their robustness against adverse lighting and weather conditions compared with cameras. This paper presents an efficient object detection model for Range-Doppler (RD) radar maps. Specifically, we first represent RD radar maps with multi-representation, i.e., heatmaps and grayscale images, to gather high-level object and fine-grained texture features. Then, we design an additional Adapter branch, an Exchanger Module with two modes, and a Primary-Auxiliary Fusion Module to effectively extract, exchange, and fuse features from the multi-representation inputs, respectively. Furthermore, we construct a supernet with various width and fusion operations in the Adapter branch for the proposed model and employ a One-Shot Neural Architecture Search method to further improve the modelâ€™s efficiency while maintaining high performance. Experimental results demonstrate that our model obtains favorable accuracy and efficiency trade-off. Moreover, we achieve new state-of-the-art performance on RADDet and CARRADA datasets with mAP@50 of 71.9 and 57.1, respectively. </p>
<blockquote>
<p>æ£€æµ‹é›·è¾¾ä¼ æ„Ÿå™¨ä¸­çš„ç‰©ä½“ç”±äºå…¶å¯¹æŠ—æ¶åŠ£å…‰ç…§å’Œå¤©æ°”æ¡ä»¶çš„ç¨³å¥æ€§ï¼Œä¸ç›¸æœºç›¸æ¯”ï¼Œæœ€è¿‘æˆä¸ºäº†ä¸€ä¸ªæµè¡Œè¶‹åŠ¿ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹Range-Dopplerï¼ˆRDï¼‰é›·è¾¾å›¾çš„é«˜æ•ˆç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¤šç§è¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚çƒ­å›¾å’Œç°åº¦å›¾åƒï¼‰æ¥è¡¨ç¤ºRDé›·è¾¾å›¾ï¼Œä»¥è·å–é«˜çº§ç›®æ ‡å’Œç²¾ç»†çº¹ç†ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé¢å¤–çš„é€‚é…å™¨åˆ†æ”¯ã€ä¸€ä¸ªå…·æœ‰ä¸¤ç§æ¨¡å¼çš„äº¤æ¢å™¨æ¨¡å—ä»¥åŠä¸€ä¸ªä¸»è¾…åŠ©èåˆæ¨¡å—ï¼Œä»¥æœ‰æ•ˆåœ°ä»å¤šè¡¨ç¤ºè¾“å…¥ä¸­æå–ã€äº¤æ¢å’Œèåˆç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨é€‚é…å™¨åˆ†æ”¯ä¸­æ„å»ºäº†å…·æœ‰ä¸åŒå®½åº¦å’Œèåˆæ“ä½œçš„è¶…ç½‘ï¼Œä¸ºæ‰€æå‡ºçš„æ¨¡å‹é‡‡ç”¨äº†ä¸€æ¬¡æ€§ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ•ˆç‡å¹¶ä¿æŒé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†æœ‰åˆ©çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨RADDetå’ŒCARRADAæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°çš„æ€§èƒ½ï¼ŒmAP@50åˆ†åˆ«ä¸º71.9å’Œ57.1ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01280v1">PDF</a> Accepted by ICANN 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹Range-Doppleré›·è¾¾åœ°å›¾çš„é«˜æ•ˆå¯¹è±¡æ£€æµ‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šè¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚çƒ­å›¾å’Œç°åº¦å›¾åƒï¼‰è¡¨ç¤ºé›·è¾¾åœ°å›¾ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªAdapteråˆ†æ”¯ã€ä¸€ä¸ªå¸¦æœ‰ä¸¤ç§æ¨¡å¼çš„Exchangeræ¨¡å—ä»¥åŠä¸€ä¸ªPrimary-Auxiliaryèåˆæ¨¡å—ï¼Œä»¥æœ‰æ•ˆåœ°æå–ã€äº¤æ¢å’Œèåˆæ¥è‡ªå¤šè¡¨ç¤ºè¾“å…¥çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ„å»ºå…·æœ‰ä¸åŒå®½åº¦å’Œèåˆæ“ä½œçš„supernetï¼Œå¹¶é‡‡ç”¨ä¸€æ¬¡æ€§ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ•ˆç‡å¹¶ä¿æŒé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨RADDetå’ŒCARRADAæ•°æ®é›†ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹Range-Doppleré›·è¾¾åœ°å›¾çš„é«˜æ•ˆå¯¹è±¡æ£€æµ‹æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨å¤šè¡¨ç¤ºæ–¹æ³•ï¼ˆçƒ­å›¾å’Œç°åº¦å›¾åƒï¼‰æ¥è¡¨ç¤ºé›·è¾¾åœ°å›¾ï¼Œä»¥è·å–é«˜çº§å¯¹è±¡ç‰¹å¾å’Œç²¾ç»†çº¹ç†ç‰¹å¾ã€‚</li>
<li>è®ºæ–‡è®¾è®¡äº†Adapteråˆ†æ”¯ã€Exchangeræ¨¡å—å’ŒPrimary-Auxiliaryèåˆæ¨¡å—ï¼Œä»¥æœ‰æ•ˆåœ°æå–ã€äº¤æ¢å’Œèåˆç‰¹å¾ã€‚</li>
<li>é€šè¿‡æ„å»ºsupernetå¹¶é‡‡ç”¨ä¸€æ¬¡æ€§ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ–¹æ³•ï¼Œæ¨¡å‹åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æé«˜äº†æ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨RADDetå’ŒCARRADAæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜è¶Šçš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨RADDetæ•°æ®é›†ä¸Šçš„mAP@50è¾¾åˆ°äº†71.9ï¼Œåœ¨CARRADAæ•°æ®é›†ä¸Šçš„mAP@50è¾¾åˆ°äº†57.1ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01280">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a76a60a13a05a802e9c39c47f28d0b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4983c13c37a9dc412a02fda6a5fa9d95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65e5c896703bb4c58fef6e19f7d834eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b44c5b65c804f539fcfd556c6e32cf3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="No-More-Sibling-Rivalry-Debiasing-Human-Object-Interaction-Detection"><a href="#No-More-Sibling-Rivalry-Debiasing-Human-Object-Interaction-Detection" class="headerlink" title="No More Sibling Rivalry: Debiasing Human-Object Interaction Detection"></a>No More Sibling Rivalry: Debiasing Human-Object Interaction Detection</h2><p><strong>Authors:Bin Yang, Yulin Zhang, Hong-Yu Zhou, Sibei Yang</strong></p>
<p>Detection transformers have been applied to human-object interaction (HOI) detection, enhancing the localization and recognition of human-action-object triplets in images. Despite remarkable progress, this study identifies a critical issue-â€œToxic Siblingsâ€ bias-which hinders the interaction decoderâ€™s learning, as numerous similar yet distinct HOI triplets interfere with and even compete against each other both input side and output side to the interaction decoder. This bias arises from high confusion among sibling triplets&#x2F;categories, where increased similarity paradoxically reduces precision, as oneâ€™s gain comes at the expense of its toxic siblingâ€™s decline. To address this, we propose two novel debiasing learning objectives-â€œcontrastive-then-calibrationâ€ and â€œmerge-then-splitâ€-targeting the input and output perspectives, respectively. The former samples sibling-like incorrect HOI triplets and reconstructs them into correct ones, guided by strong positional priors. The latter first learns shared features among sibling categories to distinguish them from other groups, then explicitly refines intra-group differentiation to preserve uniqueness. Experiments show that we significantly outperform both the baseline (+9.18% mAP on HICO-Det) and the state-of-the-art (+3.59% mAP) across various settings. </p>
<blockquote>
<p>æ£€æµ‹è½¬æ¢å™¨å·²åº”ç”¨äºäººæœºäº¤äº’ï¼ˆHOIï¼‰æ£€æµ‹ï¼Œæé«˜äº†å›¾åƒä¸­äººç±»åŠ¨ä½œå¯¹è±¡ä¸‰å…ƒç»„çš„å®šä½å’Œè¯†åˆ«èƒ½åŠ›ã€‚å°½ç®¡æœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†æœ¬ç ”ç©¶å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜â€”â€”â€œæœ‰æ¯’åŒèƒâ€åè§ï¼Œå®ƒé˜»ç¢äº†äº¤äº’è§£ç å™¨çš„å­¦ä¹ ã€‚å› ä¸ºè®¸å¤šç›¸ä¼¼ä½†åˆä¸åŒçš„HOIä¸‰å…ƒç»„åœ¨è¾“å…¥å’Œè¾“å‡ºæ–¹é¢éƒ½ç›¸äº’å¹²æ‰°å’Œç«äº‰ï¼Œç»™äº¤äº’è§£ç å™¨å¸¦æ¥äº†å¹²æ‰°ã€‚è¿™ç§åè§æ¥æºäºåŒçº§ä¸‰å…ƒç»„&#x2F;ç±»åˆ«ä¹‹é—´çš„æ··æ·†ç¨‹åº¦å¾ˆé«˜ï¼Œç›¸ä¼¼æ€§å¢åŠ åè€Œé™ä½äº†ç²¾åº¦ï¼Œå› ä¸ºä¸€æ–¹çš„æ”¶ç›Šæ˜¯ä»¥å…¶æœ‰æ¯’åŒèƒçš„è¡°é€€ä¸ºä»£ä»·çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°çš„å»åå­¦ä¹ ç›®æ ‡ï¼šâ€œå¯¹æ¯”æ ¡å‡†â€å’Œâ€œåˆå¹¶åˆ†å‰²â€ï¼Œåˆ†åˆ«é’ˆå¯¹è¾“å…¥å’Œè¾“å‡ºè§’åº¦ã€‚å‰è€…é‡‡æ ·ç±»ä¼¼åŒèƒçš„ä¸æ­£ç¡®HOIä¸‰å…ƒç»„ï¼Œå¹¶åœ¨å¼ºä½ç½®å…ˆéªŒçš„æŒ‡å¯¼ä¸‹é‡å»ºä¸ºæ­£ç¡®çš„ä¸‰å…ƒç»„ã€‚åè€…é¦–å…ˆå­¦ä¹ åŒçº§ç±»åˆ«ä¹‹é—´çš„å…±äº«ç‰¹å¾æ¥åŒºåˆ†å®ƒä»¬ä¸å…¶ä»–ç»„åˆ«ï¼Œç„¶åæ˜¾å¼æ”¹è¿›ç»„å†…å·®å¼‚ä»¥ä¿ç•™ç‹¬ç‰¹æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨å„ç§è®¾ç½®ä¸‹æ˜¾è‘—ä¼˜äºåŸºçº¿ï¼ˆHICO-Detä¸Šçš„mAPæé«˜9.18%ï¼‰å’Œæœ€æ–°æŠ€æœ¯ï¼ˆmAPæé«˜3.59%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00760v1">PDF</a> Accept to ICCV2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ£€æµ‹è½¬æ¢å™¨å·²åº”ç”¨äºäººæœºäº¤äº’ï¼ˆHOIï¼‰æ£€æµ‹ï¼Œæé«˜äº†å›¾åƒä¸­äººæœºäº¤äº’ä¸‰å…ƒç»„çš„å®šä½å’Œè¯†åˆ«èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å‘ç°äº†ä¸€ç§åä¸ºâ€œæœ‰æ¯’å…„å¼Ÿâ€åå·®çš„å…³é”®é—®é¢˜ï¼Œé˜»ç¢äº†äº¤äº’è§£ç å™¨çš„å­¦ä¹ ã€‚ä¼—å¤šç›¸ä¼¼ä½†ä¸åŒçš„HOIä¸‰å…ƒç»„åœ¨è¾“å…¥å’Œè¾“å‡ºæ–¹é¢éƒ½å¹²æ‰°ç”šè‡³ç«äº‰äº¤äº’è§£ç å™¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°é¢–çš„åå·®æ ¡æ­£å­¦ä¹ ç›®æ ‡â€”â€”â€œå¯¹æ¯”ç„¶åæ ¡å‡†â€å’Œâ€œåˆå¹¶ç„¶åæ‹†åˆ†â€ï¼Œåˆ†åˆ«é’ˆå¯¹è¾“å…¥å’Œè¾“å‡ºè§†è§’ã€‚å‰è€…é‡‡æ ·ç±»ä¼¼çš„ä¸æ­£ç¡®HOIä¸‰å…ƒç»„ï¼Œå¹¶åœ¨å¼ºä½ç½®å…ˆéªŒçš„æŒ‡å¯¼ä¸‹é‡å»ºä¸ºæ­£ç¡®çš„ä¸‰å…ƒç»„ã€‚åè€…é¦–å…ˆå­¦ä¹ å…„å¼Ÿç±»åˆ«ä¹‹é—´çš„å…±äº«ç‰¹å¾ï¼Œå°†å®ƒä»¬ä¸å…¶ä»–ç»„åŒºåˆ†å¼€ï¼Œç„¶åæ˜¾å¼åœ°æ”¹è¿›ç»„å†…å·®å¼‚ä»¥ä¿ç•™å”¯ä¸€æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨ä¸åŒè®¾ç½®ä¸‹æ˜¾è‘—ä¼˜äºåŸºçº¿ï¼ˆ+9.18ï¼…çš„HICO-Det mAPï¼‰å’Œæœ€æ–°æŠ€æœ¯ï¼ˆ+3.59ï¼…çš„mAPï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ£€æµ‹è½¬æ¢å™¨åœ¨äººæœºäº¤äº’æ£€æµ‹ä¸­çš„åº”ç”¨æé«˜äº†å›¾åƒä¸­ä¸‰å…ƒç»„çš„å®šä½å’Œè¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å‘ç°äº†ä¸€ä¸ªåä¸ºâ€œæœ‰æ¯’å…„å¼Ÿâ€åå·®çš„å…³é”®é—®é¢˜ï¼Œå½±å“äº¤äº’è§£ç å™¨çš„å­¦ä¹ ã€‚</li>
<li>ç›¸ä¼¼ä½†ä¸åŒçš„HOIä¸‰å…ƒç»„åœ¨è¾“å…¥å’Œè¾“å‡ºæ–¹é¢å¯¹è§£ç å™¨é€ æˆå¹²æ‰°ã€‚</li>
<li>ä¸ºè§£å†³â€œæœ‰æ¯’å…„å¼Ÿâ€åå·®é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§æ–°çš„åå·®æ ¡æ­£å­¦ä¹ ç›®æ ‡ã€‚</li>
<li>â€œå¯¹æ¯”ç„¶åæ ¡å‡†â€æ–¹æ³•é€šè¿‡é‡‡æ ·å¹¶é‡å»ºç±»ä¼¼çš„ä¸æ­£ç¡®HOIä¸‰å…ƒç»„æ¥æ ¡æ­£åå·®ã€‚</li>
<li>â€œåˆå¹¶ç„¶åæ‹†åˆ†â€æ–¹æ³•é€šè¿‡å­¦ä¹ å’ŒåŒºåˆ†å…„å¼Ÿç±»åˆ«åŠç»„å†…å·®å¼‚æ¥æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-418b9b019b2d335dc60fd962ebb86702.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee805550b4dca2b68306001fe7fc391e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-924544929eb95a93c70fb246d6f2f7dc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Domain-Adaptation-Based-Crossmodal-Knowledge-Distillation-for-3D-Semantic-Segmentation"><a href="#Domain-Adaptation-Based-Crossmodal-Knowledge-Distillation-for-3D-Semantic-Segmentation" class="headerlink" title="Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D   Semantic Segmentation"></a>Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D   Semantic Segmentation</h2><p><strong>Authors:Jialiang Kang, Jiawen Wang, Dingsheng Luo</strong></p>
<p>Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous driving. Traditional approaches rely on extensive annotated data for point cloud analysis, incurring high costs and time investments. In contrast, realworld image datasets offer abundant availability and substantial scale. To mitigate the burden of annotating 3D LiDAR point clouds, we propose two crossmodal knowledge distillation methods: Unsupervised Domain Adaptation Knowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge Distillation (FSKD). Leveraging readily available spatio-temporally synchronized data from cameras and LiDARs in autonomous driving scenarios, we directly apply a pretrained 2D image model to unlabeled 2D data. Through crossmodal knowledge distillation with known 2D-3D correspondence, we actively align the output of the 3D network with the corresponding points of the 2D network, thereby obviating the necessity for 3D annotations. Our focus is on preserving modality-general information while filtering out modality-specific details during crossmodal distillation. To achieve this, we deploy self-calibrated convolution on 3D point clouds as the foundation of our domain adaptation module. Rigorous experimentation validates the effectiveness of our proposed methods, consistently surpassing the performance of state-of-the-art approaches in the field. </p>
<blockquote>
<p>åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œ3Dæ¿€å…‰é›·è¾¾æ•°æ®çš„è¯­ä¹‰åˆ†å‰²æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤§é‡çš„ç‚¹äº‘åˆ†ææ ‡æ³¨æ•°æ®ï¼Œéœ€è¦å¤§é‡çš„æ—¶é—´å’Œèµ„é‡‘æŠ•å…¥ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç°å®ä¸–ç•Œå›¾åƒæ•°æ®é›†åˆ™æ›´ä¸ºä¸°å¯Œä¸”è§„æ¨¡åºå¤§ã€‚ä¸ºäº†å‡è½»å¯¹3Dæ¿€å…‰é›·è¾¾ç‚¹äº‘çš„æ ‡æ³¨è´Ÿæ‹…ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼šæ— ç›‘ç£åŸŸè‡ªé€‚åº”çŸ¥è¯†è’¸é¦ï¼ˆUDAKDï¼‰å’ŒåŸºäºç‰¹å¾å’Œè¯­ä¹‰çš„çŸ¥è¯†è’¸é¦ï¼ˆFSKDï¼‰ã€‚æˆ‘ä»¬åˆ©ç”¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­æ‘„åƒå¤´å’Œæ¿€å…‰é›·è¾¾çš„æ—¶ç©ºåŒæ­¥æ•°æ®ï¼Œç›´æ¥å°†é¢„è®­ç»ƒçš„2Då›¾åƒæ¨¡å‹åº”ç”¨äºæ— æ ‡ç­¾çš„2Dæ•°æ®ã€‚é€šè¿‡å…·æœ‰å·²çŸ¥2D-3Då¯¹åº”å…³ç³»çš„è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼Œæˆ‘ä»¬ä¸»åŠ¨å°†3Dç½‘ç»œçš„è¾“å‡ºä¸å¯¹åº”çš„2Dç½‘ç»œçš„ç‚¹å¯¹é½ï¼Œä»è€Œé¿å…äº†ä½¿ç”¨æ˜‚è´µçš„3Dæ³¨é‡Šçš„éœ€è¦ã€‚æˆ‘ä»¬çš„é‡ç‚¹æ˜¯åœ¨è·¨æ¨¡æ€è’¸é¦è¿‡ç¨‹ä¸­ä¿ç•™æ¨¡æ€é€šç”¨ä¿¡æ¯ï¼ŒåŒæ—¶è¿‡æ»¤æ‰æ¨¡æ€ç‰¹å®šçš„ç»†èŠ‚ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç»´ç‚¹äº‘ä¸Šéƒ¨ç½²äº†è‡ªæ ¡å‡†å·ç§¯ä½œä¸ºæˆ‘ä»¬çš„åŸŸè‡ªé€‚åº”æ¨¡å—çš„åŸºç¡€ã€‚ä¸¥æ ¼çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ç›¸å…³é¢†åŸŸå†…è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00379v1">PDF</a> ICRA 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­ï¼Œåˆ©ç”¨è¯­ä¹‰åˆ†å‰²å¯¹3Dæ¿€å…‰é›·è¾¾æ•°æ®è¿›è¡Œå¤„ç†çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®æ¥è¿›è¡Œç‚¹äº‘åˆ†æï¼Œæˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚ä¸ºå‡è½»æ ‡æ³¨è´Ÿæ‹…ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼šæ— ç›‘ç£åŸŸé€‚åº”çŸ¥è¯†è’¸é¦ï¼ˆUDAKDï¼‰å’Œç‰¹å¾è¯­ä¹‰çŸ¥è¯†è’¸é¦ï¼ˆFSKDï¼‰ã€‚åˆ©ç”¨è‡ªä¸»é©¾é©¶åœºæ™¯ä¸­ç›¸æœºå’Œæ¿€å…‰é›·è¾¾çš„æ—¶ç©ºåŒæ­¥æ•°æ®ï¼Œå°†é¢„è®­ç»ƒçš„2Då›¾åƒæ¨¡å‹ç›´æ¥åº”ç”¨äºæ— æ ‡ç­¾çš„2Dæ•°æ®ã€‚é€šè¿‡è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ä¸å·²çŸ¥çš„2D-3Då¯¹åº”å…³ç³»ï¼Œä½¿3Dç½‘ç»œçš„è¾“å‡ºä¸å¯¹åº”çš„2Dç½‘ç»œç‚¹å¯¹é½ï¼Œæ— éœ€3Dæ ‡æ³¨ã€‚ç ”ç©¶é‡ç‚¹æ˜¯ä¿ç•™æ¨¡æ€é€šç”¨ä¿¡æ¯ï¼Œè¿‡æ»¤æ‰æ¨¡æ€ç‰¹å®šç»†èŠ‚ï¼Œåœ¨è·¨æ¨¡æ€è’¸é¦ä¸­éƒ¨ç½²è‡ªæ ¡å‡†å·ç§¯åœ¨3Dç‚¹äº‘ä¸Šä½œä¸ºåŸŸé€‚åº”æ¨¡å—çš„åŸºç¡€ã€‚å®éªŒè¯æ˜ï¼Œæ‰€ææ–¹æ³•æ•ˆæœæ˜¾è‘—ï¼Œè¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„3Dæ¿€å…‰é›·è¾¾æ•°æ®å¤„ç†æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜ä¸”è€—æ—¶ã€‚</li>
<li>æå‡ºä¸¤ç§è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼šUDAKDå’ŒFSKDä»¥å‡è½»æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>åˆ©ç”¨æ—¶ç©ºåŒæ­¥çš„ç›¸æœºå’Œæ¿€å…‰é›·è¾¾æ•°æ®ï¼Œåº”ç”¨é¢„è®­ç»ƒçš„2Då›¾åƒæ¨¡å‹äºæ— æ ‡ç­¾çš„2Dæ•°æ®ã€‚</li>
<li>é€šè¿‡è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦å®ç°ä¸å·²çŸ¥çš„2D-3Då¯¹åº”å…³ç³»å¯¹é½è¾“å‡ºï¼Œæ— éœ€é¢å¤–çš„3Dæ ‡æ³¨ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹åœ¨äºä¿ç•™æ¨¡æ€é€šç”¨ä¿¡æ¯å¹¶è¿‡æ»¤æ‰ç‰¹å®šç»†èŠ‚ï¼Œéƒ¨ç½²è‡ªæ ¡å‡†å·ç§¯äº3Dç‚¹äº‘ä¸ºåŸºç¡€æ„å»ºåŸŸé€‚åº”æ¨¡å—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2fc53483632ea231dbd24a5168c61951.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9686d3af79dd10e7bc2cfb8281b187fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af62ad2d3ccbabe4647bf6fc655f8827.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d8b7ded26eaa10d03a61ae6bc98926d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64f0df45a5dff808485d56215b4eb33c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FLORA-Efficient-Synthetic-Data-Generation-for-Object-Detection-in-Low-Data-Regimes-via-finetuning-Flux-LoRA"><a href="#FLORA-Efficient-Synthetic-Data-Generation-for-Object-Detection-in-Low-Data-Regimes-via-finetuning-Flux-LoRA" class="headerlink" title="FLORA: Efficient Synthetic Data Generation for Object Detection in   Low-Data Regimes via finetuning Flux LoRA"></a>FLORA: Efficient Synthetic Data Generation for Object Detection in   Low-Data Regimes via finetuning Flux LoRA</h2><p><strong>Authors:Alvaro Patricio, Atabak Dehban, Rodrigo Ventura</strong></p>
<p>Recent advances in diffusion-based generative models have demonstrated significant potential in augmenting scarce datasets for object detection tasks. Nevertheless, most recent models rely on resource-intensive full fine-tuning of large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA V100) and thousands of synthetic images. To address these limitations, we propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces computational requirements, enabling synthetic dataset generation with a consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our approach on seven diverse object detection datasets. Our results demonstrate that training object detectors with just 500 synthetic images generated by our approach yields superior detection performance compared to models trained on 5000 synthetic images from the ODGEN baseline, achieving improvements of up to 21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass state-of-the-art performance with far greater efficiency, as FLORA achieves superior results using only 10% of the data and a fraction of the computational cost. This work demonstrates that a quality and efficiency-focused approach is more effective than brute-force generation, making advanced synthetic data creation more practical and accessible for real-world scenarios. </p>
<blockquote>
<p>è¿‘æœŸåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„æ–°è¿›å±•å·²ç»æ˜¾ç¤ºå‡ºåœ¨å¢å¼ºç¨€ç¼ºæ•°æ®é›†ä»¥è¿›è¡Œç›®æ ‡æ£€æµ‹ä»»åŠ¡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æœ€æ–°æ¨¡å‹ä¾èµ–äºå¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„å…¨ç²¾ç»†è°ƒæ•´ï¼Œè¿™éœ€è¦ä¼ä¸šçº§GPUï¼ˆä¾‹å¦‚NVIDIA V100ï¼‰å’Œæ•°åƒå¼ åˆæˆå›¾åƒï¼Œè®¡ç®—èµ„æºæ¶ˆè€—å¤§ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Flux LoRA Augmentationï¼ˆFLORAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨Flux 1.1 Devæ‰©æ•£æ¨¡å‹ï¼Œä»…é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒã€‚è¿™å¤§å¤§é™ä½äº†è®¡ç®—è¦æ±‚ï¼Œä½¿ç”¨æ¶ˆè´¹çº§GPUï¼ˆä¾‹å¦‚NVIDIA RTX 4090ï¼‰å³å¯ç”Ÿæˆåˆæˆæ•°æ®é›†ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªä¸åŒçš„ç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šå®è¯è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆ500å¼ åˆæˆå›¾åƒè®­ç»ƒç›®æ ‡æ£€æµ‹å™¨ï¼Œå…¶æ£€æµ‹æ€§èƒ½ä¼˜äºä½¿ç”¨ODGENåŸºçº¿ç”Ÿæˆçš„5000å¼ åˆæˆå›¾åƒè®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨mAP@.50:.95æŒ‡æ ‡ä¸Šæé«˜äº†é«˜è¾¾21.3%ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œä»¥æ›´é«˜çš„æ•ˆç‡è¶…è¶Šæœ€æ–°æŠ€æœ¯æ€§èƒ½æ˜¯å¯èƒ½çš„ï¼Œå› ä¸ºFLORAä»…ä½¿ç”¨10%çš„æ•°æ®å’Œä¸€å°éƒ¨åˆ†è®¡ç®—æˆæœ¬å°±å–å¾—äº†ä¼˜è¶Šçš„ç»“æœã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œä»¥è´¨é‡å’Œæ•ˆç‡ä¸ºä¸­å¿ƒçš„æ–¹æ³•æ¯”æš´åŠ›ç”Ÿæˆæ›´æœ‰æ•ˆï¼Œä½¿å¾—é«˜çº§åˆæˆæ•°æ®çš„åˆ›å»ºåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­æ›´åŠ å®ç”¨å’Œå¯è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21712v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨å¯¹è±¡æ£€æµ‹ä»»åŠ¡çš„ç¨€ç¼ºæ•°æ®é›†å¢å¼ºæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚é’ˆå¯¹å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„å…¨é¢å¾®è°ƒæ‰€å¸¦æ¥çš„èµ„æºå¯†é›†é—®é¢˜ï¼Œæå‡ºFlux LoRA Augmentationï¼ˆFLORAï¼‰è½»é‡çº§åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ã€‚è¯¥æ–¹æ³•ä½¿ç”¨Flux 1.1 Devæ‰©æ•£æ¨¡å‹ï¼Œä»…é€šè¿‡Low-Rank Adaptationï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—é™ä½è®¡ç®—è¦æ±‚ï¼Œå¯åœ¨æ¶ˆè´¹çº§GPUä¸Šç”Ÿæˆåˆæˆæ•°æ®é›†ã€‚åœ¨ä¸ƒä¸ªä¸åŒçš„å¯¹è±¡æ£€æµ‹æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œä»…ä½¿ç”¨500å¼ ç”±è¯¥æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒè®­ç»ƒçš„ç‰©ä½“æ£€æµ‹å™¨ï¼Œå…¶æ£€æµ‹æ€§èƒ½ä¼˜äºä½¿ç”¨ODGENåŸºçº¿ç”Ÿæˆçš„5000å¼ åˆæˆå›¾åƒçš„æ¨¡å‹ï¼Œæé«˜äº†é«˜è¾¾21.3%çš„mAP@.50:.95ã€‚æ­¤å·¥ä½œè¯æ˜äº†ä»¥è´¨é‡å’Œæ•ˆç‡ä¸ºå¯¼å‘çš„æ–¹æ³•æ¯”æš´åŠ›ç”Ÿæˆæ›´æœ‰æ•ˆï¼Œä½¿é«˜çº§åˆæˆæ•°æ®åˆ›å»ºåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­æ›´åŠ å®ç”¨å’Œå¯è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨å¯¹è±¡æ£€æµ‹æ•°æ®é›†å¢å¼ºæ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>Flux LoRA Augmentation (FLORA)æ˜¯ä¸€ç§è½»é‡çº§çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>FLORAä½¿ç”¨Flux 1.1 Devæ‰©æ•£æ¨¡å‹å¹¶ç»“åˆLow-Rank Adaptation (LoRA)è¿›è¡Œå¾®è°ƒï¼Œé™ä½è®¡ç®—éœ€æ±‚ã€‚</li>
<li>FLORAå¯åœ¨æ¶ˆè´¹çº§GPUä¸Šå®æ–½ã€‚</li>
<li>åœ¨ä¸ƒä¸ªå¯¹è±¡æ£€æµ‹æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒFLORAç”Ÿæˆçš„å°‘é‡åˆæˆå›¾åƒè®­ç»ƒçš„æ£€æµ‹å™¨æ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>FLORAæ–¹æ³•å®ç°äº†é«˜æ•ˆçš„æ•°æ®ä½¿ç”¨ï¼Œä»…ä½¿ç”¨10%çš„æ•°æ®å’Œè¾ƒå°‘çš„è®¡ç®—æˆæœ¬å°±è¾¾åˆ°äº†ä¼˜è¶Šçš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-56c1c4cf0530ed03f35ac38acdbe0127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-611985dabfdd390c7ecf749923f3d752.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3daece63425796ec6591e3f90de2fba7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c7c7ff29721c3e59a438454afb2bfab.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Contrastive-Learning-through-Auxiliary-Branch-for-Video-Object-Detection"><a href="#Contrastive-Learning-through-Auxiliary-Branch-for-Video-Object-Detection" class="headerlink" title="Contrastive Learning through Auxiliary Branch for Video Object Detection"></a>Contrastive Learning through Auxiliary Branch for Video Object Detection</h2><p><strong>Authors:Lucas Rakotoarivony</strong></p>
<p>Video object detection is a challenging task because videos often suffer from image deterioration such as motion blur, occlusion, and deformable shapes, making it significantly more difficult than detecting objects in still images. Prior approaches have improved video object detection performance by employing feature aggregation and complex post-processing techniques, though at the cost of increased computational demands. To improve robustness to image degradation without additional computational load during inference, we introduce a straightforward yet effective Contrastive Learning through Auxiliary Branch (CLAB) method. First, we implement a constrastive auxiliary branch using a contrastive loss to enhance the feature representation capability of the video object detectorâ€™s backbone. Next, we propose a dynamic loss weighting strategy that emphasizes auxiliary feature learning early in training while gradually prioritizing the detection task as training converges. We validate our approach through comprehensive experiments and ablation studies, demonstrating consistent performance gains. Without bells and whistles, CLAB reaches a performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101, respectively, on the ImageNet VID dataset, thus achieving state-of-the-art performance for CNN-based models without requiring additional post-processing methods. </p>
<blockquote>
<p>è§†é¢‘ç›®æ ‡æ£€æµ‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºè§†é¢‘ç»å¸¸å—åˆ°å›¾åƒé€€åŒ–é—®é¢˜çš„å½±å“ï¼Œå¦‚è¿åŠ¨æ¨¡ç³Šã€é®æŒ¡å’Œå¯å˜å½¢å½¢çŠ¶ï¼Œè¿™ä½¿å¾—å®ƒæ¯”åœ¨é™æ­¢å›¾åƒä¸­æ£€æµ‹ç›®æ ‡æ›´åŠ å›°éš¾ã€‚å…ˆå‰çš„æ–¹æ³•é€šè¿‡é‡‡ç”¨ç‰¹å¾èšåˆå’Œå¤æ‚çš„åå¤„ç†æŠ€æœ¯æé«˜äº†è§†é¢‘ç›®æ ‡æ£€æµ‹çš„æ€§èƒ½ï¼Œä½†å¢åŠ äº†è®¡ç®—éœ€æ±‚ã€‚ä¸ºäº†æé«˜å¯¹å›¾åƒé€€åŒ–çš„é²æ£’æ€§ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸éœ€è¦å¢åŠ è®¡ç®—è´Ÿè½½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„é€šè¿‡è¾…åŠ©åˆ†æ”¯è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼ˆCLABï¼‰çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹æ¯”æŸå¤±å®ç°äº†ä¸€ä¸ªå¯¹æ¯”è¾…åŠ©åˆ†æ”¯ï¼Œä»¥å¢å¼ºè§†é¢‘ç›®æ ‡æ£€æµ‹å™¨ä¸»å¹²ç‰¹å¾è¡¨ç¤ºçš„èƒ½åŠ›ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€æŸå¤±åŠ æƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¼ºè°ƒåœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µå­¦ä¹ è¾…åŠ©ç‰¹å¾ï¼Œéšç€è®­ç»ƒçš„æ”¶æ•›ï¼Œé€æ¸ä¼˜å…ˆé‡è§†æ£€æµ‹ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„å®éªŒå’Œæ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æŒç»­çš„æ€§èƒ½æå‡ã€‚æ²¡æœ‰é¢å¤–çš„ä¿®é¥°å’Œé™„åŠ ç»„ä»¶ï¼ŒCLABåœ¨ImageNet VIDæ•°æ®é›†ä¸Šä½¿ç”¨ResNet-101å’ŒResNeXt-101åˆ†åˆ«è¾¾åˆ°äº†84.0%å’Œ85.2%çš„mAPæ€§èƒ½ï¼Œä»è€Œåœ¨ä¸éœ€è¦é¢å¤–åå¤„ç†æ–¹æ³•çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†åŸºäºCNNçš„æ¨¡å‹çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20551v1">PDF</a> Accepted paper for ACIVS 2025</p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘ç›®æ ‡æ£€æµ‹ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè§†é¢‘å¸¸å¸¸å­˜åœ¨å›¾åƒé€€åŒ–é—®é¢˜ï¼Œå¦‚è¿åŠ¨æ¨¡ç³Šã€é®æŒ¡å’Œå˜å½¢ç­‰ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¯¹æ¯”å­¦ä¹ è¾…åŠ©åˆ†æ”¯ï¼ˆCLABï¼‰æ–¹æ³•ï¼Œä»¥æé«˜è§†é¢‘å¯¹è±¡æ£€æµ‹å™¨å¯¹å›¾åƒé€€åŒ–çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¸å¢åŠ æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—è´Ÿæ‹…ã€‚é€šè¿‡å®æ–½å¯¹æ¯”è¾…åŠ©åˆ†æ”¯å’Œä½¿ç”¨åŠ¨æ€æŸå¤±åŠ æƒç­–ç•¥ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒåˆæœŸå¼ºè°ƒè¾…åŠ©ç‰¹å¾å­¦ä¹ ï¼Œéšç€è®­ç»ƒçš„æ”¶æ•›é€æ¸ä¾§é‡äºæ£€æµ‹ä»»åŠ¡ã€‚åœ¨ImageNet VIDæ•°æ®é›†ä¸Šï¼ŒCLABæ–¹æ³•è¾¾åˆ°äº†ä½¿ç”¨ResNet-101å’ŒResNeXt-101æ—¶çš„84.0%å’Œ85.2%çš„mAPï¼Œå®ç°äº†åŸºäºCNNçš„æ¨¡å‹çš„æœ€ä½³æ€§èƒ½ï¼Œæ— éœ€é¢å¤–çš„åå¤„ç†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†é¢‘ç›®æ ‡æ£€æµ‹é¢ä¸´å›¾åƒé€€åŒ–æŒ‘æˆ˜ï¼Œå¦‚è¿åŠ¨æ¨¡ç³Šã€é®æŒ¡å’Œå˜å½¢ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ è¾…åŠ©åˆ†æ”¯ï¼ˆCLABï¼‰æ–¹æ³•é€šè¿‡å¢å¼ºç‰¹å¾è¡¨ç¤ºèƒ½åŠ›æé«˜è§†é¢‘å¯¹è±¡æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>CLABæ–¹æ³•å®æ–½äº†ä¸€ä¸ªå¯¹æ¯”è¾…åŠ©åˆ†æ”¯å’ŒåŠ¨æ€æŸå¤±åŠ æƒç­–ç•¥ï¼Œä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>CLABæ–¹æ³•åœ¨ImageNet VIDæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œä½¿ç”¨ResNet-101å’ŒResNeXt-101æ—¶åˆ†åˆ«è¾¾åˆ°84.0%å’Œ85.2%çš„mAPã€‚</li>
<li>CLABæ–¹æ³•ä¸å¢åŠ é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ï¼Œä¸”ä¸éœ€è¦é¢å¤–çš„åå¤„ç†æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eef89b90e4f8393e7301aa21aebaaf13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8698a6c9de68c9ecaed852947fb9b616.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35a57636f10d4876dfa9e7ecc121de37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aadcf6e4c46198d9d74243b5509d408e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Pseudo-Boxes-via-Data-Level-LiDAR-Camera-Fusion-for-Unsupervised-3D-Object-Detection"><a href="#Enhancing-Pseudo-Boxes-via-Data-Level-LiDAR-Camera-Fusion-for-Unsupervised-3D-Object-Detection" class="headerlink" title="Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for   Unsupervised 3D Object Detection"></a>Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for   Unsupervised 3D Object Detection</h2><p><strong>Authors:Mingqian Ji, Jian Yang, Shanshan Zhang</strong></p>
<p>Existing LiDAR-based 3D object detectors typically rely on manually annotated labels for training to achieve good performance. However, obtaining high-quality 3D labels is time-consuming and labor-intensive. To address this issue, recent works explore unsupervised 3D object detection by introducing RGB images as an auxiliary modal to assist pseudo-box generation. However, these methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB images. Yet, such a label-level fusion strategy brings limited improvements to the quality of pseudo-boxes, as it overlooks the complementary nature in terms of LiDAR and RGB image data. To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. Specifically, we utilize vision foundation models for instance segmentation and depth estimation on images and introduce a bi-directional fusion method, where real points acquire category labels from the 2D space, while 2D pixels are projected onto 3D to enhance real point density. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4$%$ mAP on the nuScenes validation benchmark. </p>
<blockquote>
<p>ç°æœ‰çš„åŸºäºæ¿€å…‰é›·è¾¾çš„3Dç›®æ ‡æ£€æµ‹å™¨é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨æ ‡æ³¨çš„æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°è‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè·å–é«˜è´¨é‡çš„3Dæ ‡ç­¾æ˜¯è€—æ—¶ä¸”åŠ³åŠ¨å¯†é›†å‹çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ€è¿‘çš„ç ”ç©¶é€šè¿‡å¼•å…¥RGBå›¾åƒä½œä¸ºè¾…åŠ©æ¨¡å¼æ¥è¾…åŠ©ä¼ªæ¡†ç”Ÿæˆï¼Œä»è€Œæ¢ç´¢äº†æ— ç›‘ç£çš„3Dç›®æ ‡æ£€æµ‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åªæ˜¯å°†æ¿€å…‰é›·è¾¾ç‚¹äº‘å’ŒRGBå›¾åƒç”Ÿæˆçš„ä¼ªæ¡†è¿›è¡Œç®€å•æ•´åˆã€‚ç„¶è€Œï¼Œè¿™ç§æ ‡ç­¾å±‚é¢çš„èåˆç­–ç•¥å¯¹ä¼ªæ¡†è´¨é‡çš„æé«˜æœ‰é™ï¼Œå› ä¸ºå®ƒå¿½è§†äº†æ¿€å…‰é›·è¾¾å’ŒRGBå›¾åƒæ•°æ®ä¹‹é—´çš„äº’è¡¥æ€§ã€‚ä¸ºäº†å…‹æœä¸Šè¿°å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å±‚é¢èåˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ—©æœŸé˜¶æ®µå°±å°†RGBå›¾åƒå’Œæ¿€å…‰é›·è¾¾æ•°æ®è¿›è¡Œæ•´åˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹å¯¹å›¾åƒè¿›è¡Œå®ä¾‹åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŒå‘èåˆæ–¹æ³•ï¼Œå…¶ä¸­çœŸå®ç‚¹ä»äºŒç»´ç©ºé—´è·å–ç±»åˆ«æ ‡ç­¾ï¼Œè€ŒäºŒç»´åƒç´ åˆ™æŠ•å½±åˆ°ä¸‰ç»´ä»¥å¢å¼ºçœŸå®ç‚¹çš„å¯†åº¦ã€‚ä¸ºäº†å‡è½»æ·±åº¦å’Œåˆ†å‰²ä¼°è®¡ä¸­çš„å™ªå£°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å±€éƒ¨å’Œå…¨å±€æ»¤æ³¢æ–¹æ³•ï¼Œé‡‡ç”¨å±€éƒ¨åŠå¾„æ»¤æ³¢æ¥æŠ‘åˆ¶æ·±åº¦ä¼°è®¡è¯¯å·®ï¼Œå…¨å±€ç»Ÿè®¡æ»¤æ³¢æ¥å»é™¤ç”±åˆ†å‰²å¼•èµ·çš„å¼‚å¸¸å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ•°æ®å±‚é¢èåˆçš„åŠ¨æ€è‡ªæ¼”åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨å¯†é›†è¡¨ç¤ºä¸‹è¿­ä»£ä¼˜åŒ–ä¼ªæ¡†ï¼Œæ˜¾è‘—æé«˜äº†å®šä½ç²¾åº¦ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨æˆ‘ä»¬æ–¹æ³•è®­ç»ƒçš„æ£€æµ‹å™¨åœ¨nuSceneséªŒè¯åŸºå‡†æµ‹è¯•ä¸Šçš„mAPä¸º28.4%ï¼Œæ˜¾è‘—ä¼˜äºé‡‡ç”¨ä¹‹å‰æœ€å…ˆè¿›æ–¹æ³•è®­ç»ƒçš„æ£€æµ‹å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20530v1">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong><br>åŸºäºæ¿€å…‰é›·è¾¾çš„3Då¯¹è±¡æ£€æµ‹é€šå¸¸ä¾èµ–æ‰‹åŠ¨æ ‡æ³¨çš„æ ‡ç­¾è¿›è¡Œè®­ç»ƒä»¥è¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ï¼Œä½†è·å–é«˜è´¨é‡çš„3Dæ ‡ç­¾è€—æ—¶è€—åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿‘æœŸç ”ç©¶å°è¯•å¼•å…¥RGBå›¾åƒä½œä¸ºè¾…åŠ©æ¨¡æ€ä»¥ä¿ƒè¿›ä¼ªç®±ç”Ÿæˆï¼Œä½†ç®€å•èåˆä¼ªç®±æ•°æ®ä¸RGBå›¾åƒæ•ˆæœæœ‰é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ•°æ®çº§åˆ«èåˆæ¡†æ¶ï¼Œåœ¨åˆæœŸé˜¶æ®µèåˆRGBå›¾åƒå’Œæ¿€å…‰é›·è¾¾æ•°æ®ã€‚åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œå›¾åƒå®ä¾‹åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ï¼Œå¼•å…¥åŒå‘èåˆæ–¹æ³•ï¼Œå®ç°2Dç‚¹ä¸3Dç‚¹çš„ç›¸äº’æ ‡æ³¨å¢å¼ºã€‚é€šè¿‡å±€éƒ¨å’Œå…¨å±€æ»¤æ³¢æ–¹æ³•å‡å°‘æ·±åº¦ä¼°è®¡å’Œåˆ†å‰²å¼•èµ·çš„å™ªå£°ï¼Œå¹¶æå‡ºåŸºäºæ•°æ®çº§åˆ«èåˆçš„åŠ¨æ€è‡ªæ¼”åŒ–ç­–ç•¥ï¼Œè¿­ä»£ä¼˜åŒ–ä¼ªç®±åœ¨å¯†é›†è¡¨ç¤ºä¸‹çš„å®šä½ç²¾åº¦ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•è®­ç»ƒçš„æ£€æµ‹å™¨è¾ƒå…ˆå‰é¡¶å°–æ–¹æ³•æœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ï¼ŒmAPæé«˜è‡³28.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°æœ‰åŸºäºæ¿€å…‰é›·è¾¾çš„3Då¯¹è±¡æ£€æµ‹ä¾èµ–æ‰‹åŠ¨æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>å¼•å…¥RGBå›¾åƒä½œä¸ºè¾…åŠ©æ¨¡æ€çš„æ–¹æ³•è™½ç„¶å‡ºç°ï¼Œä½†èåˆç­–ç•¥æœ‰é™ï¼Œæ”¹å–„ä¸æ˜æ˜¾ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ•°æ®çº§åˆ«èåˆæ¡†æ¶ï¼Œåœ¨åˆæœŸé˜¶æ®µç»“åˆRGBå›¾åƒå’Œæ¿€å…‰é›·è¾¾æ•°æ®ã€‚</li>
<li>åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œå®ä¾‹åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ï¼Œå¹¶é‡‡ç”¨åŒå‘èåˆæ–¹æ³•å¢å¼º2Dä¸3Dç‚¹çš„æ ‡æ³¨ã€‚</li>
<li>é€šè¿‡å±€éƒ¨å’Œå…¨å±€æ»¤æ³¢å‡å°‘ä¼°è®¡è¯¯å·®å’Œå™ªå£°ï¼Œå¹¶å¼•å…¥åŠ¨æ€è‡ªæ¼”åŒ–ç­–ç•¥è¿­ä»£ä¼˜åŒ–ä¼ªç®±å®šä½ç²¾åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22f3bb636749dc27333a371ba48d8bcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc13e2004dd7e6ec6a56bca795618d5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13c642f4a9d6723ff46dfc405e0c51a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f62289d5335a0f21c6e261ede6a7a49e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9c21542da34ecfc59d1b447cfee0067.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b2e0f4f4431fa97b57acacb54305945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2de68f49f7f88ac51a7646e317fe3972.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="OpenM3D-Open-Vocabulary-Multi-view-Indoor-3D-Object-Detection-without-Human-Annotations"><a href="#OpenM3D-Open-Vocabulary-Multi-view-Indoor-3D-Object-Detection-without-Human-Annotations" class="headerlink" title="OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without   Human Annotations"></a>OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without   Human Annotations</h2><p><strong>Authors:Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo</strong></p>
<p>Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡ï¼ˆOVï¼‰3Då¯¹è±¡æ£€æµ‹æ˜¯ä¸€ä¸ªæ–°å…´é¢†åŸŸï¼Œä¸åŸºäº3Dç‚¹äº‘çš„æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºå›¾åƒçš„æ–¹æ³•çš„æ¢ç´¢ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬ä»‹ç»äº†OpenM3Dï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼€æ”¾è¯æ±‡çš„å¤šè§†è§’å®¤å†…3Då¯¹è±¡æ£€æµ‹å™¨ï¼Œæ— éœ€äººå·¥æ³¨é‡Šå³å¯è¿›è¡Œè®­ç»ƒã€‚ç‰¹åˆ«æ˜¯ï¼ŒOpenM3Dæ˜¯ä¸€ä¸ªå•é˜¶æ®µæ£€æµ‹å™¨ï¼Œé€‚åº”äºä»ImGeoNetæ¨¡å‹è¯±å¯¼çš„2Dvoxelç‰¹å¾ã€‚ä¸ºäº†æ”¯æŒOVï¼Œå®ƒä¸ç±»æ— å…³çš„3Då®šä½æŸå¤±å’Œé«˜è´¨é‡çš„3Dä¼ªç›’ä¸€èµ·è”åˆè®­ç»ƒï¼Œä»¥åŠä¸éœ€è¦å„ç§é¢„è®­ç»ƒCLIPç‰¹å¾çš„voxelè¯­ä¹‰å¯¹é½æŸå¤±ã€‚æˆ‘ä»¬éµå¾ªOV-3DETçš„è®­ç»ƒè®¾ç½®ï¼Œæä¾›æœ‰å§¿æ€çš„RGB-Då›¾åƒï¼Œä½†ä¸å¯ç”¨3Dæ¡†æˆ–ç±»çš„æ‰‹å·¥æ³¨é‡Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å›¾åµŒå…¥æŠ€æœ¯çš„3Dä¼ªç›’ç”Ÿæˆæ–¹æ³•ï¼Œå°†2Dæ®µç»„åˆæˆè¿è´¯çš„3Dç»“æ„ã€‚æˆ‘ä»¬çš„ä¼ªç›’å­åœ¨ç²¾åº¦å’Œå¬å›ç‡æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬OV-3DETä¸­æå‡ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä»ä¸æ¯ä¸ªè¿è´¯çš„3Dç»“æ„ç›¸å…³çš„2Dæ®µä¸­é‡‡æ ·å„ç§CLIPç‰¹å¾ï¼Œä»¥ä¸ç›¸åº”çš„voxelç‰¹å¾å¯¹é½ã€‚è®­ç»ƒé«˜åº¦å‡†ç¡®çš„å•é˜¶æ®µæ£€æµ‹å™¨çš„å…³é”®æ˜¯éœ€è¦åŒæ—¶å­¦ä¹ è¿™ä¸¤ç§æŸå¤±ä»¥è¾¾åˆ°é«˜è´¨é‡ç›®æ ‡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒOpenM3Dæ˜¯ä¸€ç§é«˜æ•ˆçš„æ£€æµ‹å™¨ï¼Œä»…éœ€è¦å¤šè§†è§’å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨ScanNet200å’ŒARKitSceneså®¤å†…åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œé€Ÿåº¦ï¼ˆæ¯ç§’0.3ç§’ï¼‰ã€‚æˆ‘ä»¬åœ¨ä½¿ç”¨åŸºäºViT CLIPçš„OVåˆ†ç±»å™¨èå…¥æˆ‘ä»¬çš„ç±»æ— å…³æ£€æµ‹å™¨å’Œå¤šè§†è§’æ·±åº¦ä¼°è®¡å™¨çš„å¼ºå¤§ä¸¤é˜¶æ®µæ–¹æ³•ä¸Šï¼Œåœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦æ–¹é¢éƒ½è¡¨ç°å‡ºè¶…è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20063v1">PDF</a> ICCV2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†OpenM3Dï¼Œä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨çš„å¼€æ”¾è¯æ±‡å®¤å†…3Dç›®æ ‡æ£€æµ‹å™¨ã€‚å®ƒé‡‡ç”¨å•é˜¶æ®µæ£€æµ‹å™¨ï¼Œé€šè¿‡ImGeoNetæ¨¡å‹è¯±å¯¼çš„äºŒç»´ä½“ç´ ç‰¹å¾è¿›è¡Œè®­ç»ƒã€‚ä¸ºæ”¯æŒå¼€æ”¾è¯æ±‡ï¼Œå®ƒä¸ç±»æ— å…³çš„3Då®šä½æŸå¤±å’Œé«˜è´¨é‡çš„ä¸‰ç»´ä¼ªè£…ç®±ä»¥åŠä½“ç´ è¯­ä¹‰å¯¹é½æŸå¤±è¿›è¡Œè”åˆè®­ç»ƒã€‚æå‡ºä¸€ç§ä½¿ç”¨å›¾åµŒå…¥æŠ€æœ¯çš„ä¸‰ç»´ä¼ªè£…ç®±ç”Ÿæˆæ–¹æ³•ï¼Œå°†äºŒç»´ç‰‡æ®µç»„åˆæˆè¿è´¯çš„ä¸‰ç»´ç»“æ„ã€‚åœ¨ScanNet200å’ŒARKitSceneså®¤å†…åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOpenM3Dè¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚æœ¬æ–‡è¿˜æ¢è®¨äº†ä¸€ç§ç»“åˆå•é˜¶æ®µæ£€æµ‹å™¨ã€åŸºäºViT CLIPçš„å¼€æ”¾è¯æ±‡åˆ†ç±»å™¨å’Œå¤šè§†å›¾æ·±åº¦ä¼°è®¡å™¨çš„åŸºå‡†æ–¹æ¡ˆã€‚æ­¤æ–¹æ³•èƒ½è¿›ä¸€æ­¥æå‡å‡†ç¡®æ€§ä¸é€Ÿåº¦ã€‚æ€»ç»“ä¸ºä¸€å¥è¯å³ï¼šâ€œç ”ç©¶å›¢é˜Ÿæå‡ºæ— éœ€äººå·¥æ ‡æ³¨çš„å¼€æ”¾å¼å®¤å†…ä¸‰ç»´ç›®æ ‡æ£€æµ‹å™¨OpenM3Dï¼ŒåŸºäºå›¾åµŒå…¥æŠ€æœ¯å’Œå¤šç§ç‰¹å¾å¯¹é½æŠ€æœ¯ï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦ã€‚â€</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OpenM3Dæ˜¯ä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨çš„å¼€æ”¾è¯æ±‡å®¤å†…ä¸‰ç»´ç›®æ ‡æ£€æµ‹å™¨ã€‚</li>
<li>OpenM3Dä½¿ç”¨å•é˜¶æ®µæ£€æµ‹å™¨ç»“åˆäºŒç»´ä½“ç´ ç‰¹å¾è¿›è¡Œè®­ç»ƒã€‚å®ƒè”åˆè®­ç»ƒæ”¯æŒå¼€æ”¾è¯æ±‡æŠ€æœ¯å¹¶ä¸ç±»æ— å…³çš„3Då®šä½æŸå¤±ä»¥åŠä½“ç´ è¯­ä¹‰å¯¹é½æŸå¤±è¿›è¡Œç»“åˆã€‚</li>
<li>æå‡ºä¸€ç§åˆ©ç”¨å›¾åµŒå…¥æŠ€æœ¯ç”Ÿæˆä¸‰ç»´ä¼ªè£…ç®±çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†äºŒç»´ç‰‡æ®µå½¢æˆè¿è´¯çš„ä¸‰ç»´ç»“æ„ï¼Œå¹¶å®ç°äº†è¾ƒé«˜çš„ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚</li>
<li>OpenM3Dåˆ©ç”¨å¤šç§CLIPç‰¹å¾æ¥å¯¹é½æ¯ä¸ªè¿è´¯çš„ä¸‰ç»´ç»“æ„å¯¹åº”çš„ä½“ç´ ç‰¹å¾ã€‚è¿™ä¸€å…³é”®æ­¥éª¤æœ‰åŠ©äºæé«˜å•é˜¶æ®µæ£€æµ‹å™¨çš„å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2f34544b513221d73b41c0822ad2ab95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3321507207d7beac680f3c8953b9836.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-332864918e26e244a60f12897ad0276c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ProMSC-MIS-Prompt-based-Multimodal-Semantic-Communication-for-Multi-Spectral-Image-Segmentation"><a href="#ProMSC-MIS-Prompt-based-Multimodal-Semantic-Communication-for-Multi-Spectral-Image-Segmentation" class="headerlink" title="ProMSC-MIS: Prompt-based Multimodal Semantic Communication for   Multi-Spectral Image Segmentation"></a>ProMSC-MIS: Prompt-based Multimodal Semantic Communication for   Multi-Spectral Image Segmentation</h2><p><strong>Authors:Haoshuo Zhang, Yufei Bo, Meixia Tao</strong></p>
<p>Multimodal semantic communication has great potential to enhance downstream task performance by integrating complementary information across modalities. This paper introduces ProMSC-MIS, a novel Prompt-based Multimodal Semantic Communication framework for Multi-Spectral Image Segmentation. It enables efficient task-oriented transmission of spatially aligned RGB and thermal images over band-limited channels. Our framework has two main design novelties. First, by leveraging prompt learning and contrastive learning, unimodal semantic encoders are pre-trained to learn diverse and complementary semantic representations by using features from one modality as prompts for another. Second, a semantic fusion module that combines cross-attention mechanism and squeeze-and-excitation (SE) networks is designed to effectively fuse cross-modal features. Experimental results demonstrate that ProMSC-MIS substantially outperforms conventional image transmission combined with state-of-the-art segmentation methods. Notably, it reduces the required channel bandwidth by 50%â€“70% at the same segmentation performance, while also decreasing the storage overhead and computational complexity by 26% and 37%, respectively. Ablation studies also validate the effectiveness of the proposed pre-training and semantic fusion strategies. Our scheme is highly suitable for applications such as autonomous driving and nighttime surveillance. </p>
<blockquote>
<p>å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡é€šè¿‡æ•´åˆå„æ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ï¼Œå…·æœ‰å·¨å¤§çš„æ½œåŠ›æ¥æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æœ¬æ–‡ä»‹ç»äº†ProMSC-MISï¼Œä¸€ä¸ªåŸºäºæç¤ºçš„æ–°å‹å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œç”¨äºå¤šå…‰è°±å›¾åƒåˆ†å‰²ã€‚å®ƒèƒ½å¤Ÿå®ç°ç©ºé—´å¯¹é½çš„RGBå’Œçƒ­å›¾åƒåœ¨å¸¦å®½å—é™é€šé“ä¸Šçš„é«˜æ•ˆä»»åŠ¡å¯¼å‘ä¼ è¾“ã€‚æˆ‘ä»¬çš„æ¡†æ¶æœ‰ä¸¤ä¸ªä¸»è¦çš„è®¾è®¡æ–°é¢–ä¹‹å¤„ã€‚é¦–å…ˆï¼Œé€šè¿‡åˆ©ç”¨æç¤ºå­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ ï¼Œå•æ¨¡æ€è¯­ä¹‰ç¼–ç å™¨è¢«é¢„è®­ç»ƒä»¥ä½¿ç”¨ä¸€ç§æ¨¡æ€çš„ç‰¹å¾ä½œä¸ºå¦ä¸€ç§æ¨¡æ€çš„æç¤ºæ¥å­¦ä¹ å¤šæ ·ä¸”äº’è¡¥çš„è¯­ä¹‰è¡¨ç¤ºã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰èåˆæ¨¡å—ï¼Œå®ƒç»“åˆäº†äº¤å‰æ³¨æ„æœºåˆ¶å’ŒæŒ¤å‹æ¿€åŠ±ï¼ˆSEï¼‰ç½‘ç»œï¼Œä»¥æœ‰æ•ˆåœ°èåˆè·¨æ¨¡æ€ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProMSC-MISåœ¨ç›¸åŒçš„åˆ†å‰²æ€§èƒ½ä¸‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå›¾åƒä¼ è¾“ç»“åˆæœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ï¼Œå¤§å¹…é™ä½äº†æ‰€éœ€çš„ä¿¡é“å¸¦å®½è¾¾50%-70%ï¼ŒåŒæ—¶å‡å°‘äº†å­˜å‚¨å¼€é”€å’Œè®¡ç®—å¤æ‚åº¦åˆ†åˆ«ä¸º26%å’Œ37%ã€‚æ¶ˆèç ”ç©¶ä¹ŸéªŒè¯äº†æ‰€æå‡ºé¢„è®­ç»ƒå’Œè¯­ä¹‰èåˆç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆéå¸¸é€‚åˆè‡ªåŠ¨é©¾é©¶å’Œå¤œé—´ç›‘æ§ç­‰åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20057v1">PDF</a> arXiv admin note: text overlap with arXiv:2508.17920</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡çš„å¤šå…‰è°±å›¾åƒåˆ†å‰²æ–¹æ³•å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºæç¤ºçš„ProMSC-MISæ¡†æ¶ï¼Œèƒ½æœ‰æ•ˆæ•´åˆä¸åŒæ¨¡æ€é—´çš„äº’è¡¥ä¿¡æ¯ä»¥æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚è¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨æç¤ºå­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ ï¼Œè®­ç»ƒå•æ¨¡æ€è¯­ä¹‰ç¼–ç å™¨ä»¥å­¦ä¹ å¤šæ ·ä¸”äº’è¡¥çš„è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨èåˆè·¨æ³¨æ„åŠ›æœºåˆ¶å’ŒæŒ¤å‹æ¿€åŠ±ç½‘ç»œçš„è¯­ä¹‰èåˆæ¨¡å—ï¼Œä»¥å®ç°è·¨æ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆèåˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProMSC-MISåœ¨é™ä½ä¿¡é“å¸¦å®½éœ€æ±‚çš„åŒæ—¶ï¼Œå®ç°äº†ä¸ä¼ ç»Ÿå›¾åƒä¼ è¾“ç»“åˆçš„æœ€å…ˆè¿›åˆ†å‰²æ–¹æ³•çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆè¿˜é™ä½äº†å­˜å‚¨å¼€é”€å’Œè®¡ç®—å¤æ‚åº¦ï¼Œå…·æœ‰å¾ˆé«˜çš„å®é™…åº”ç”¨ä»·å€¼ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶å’Œå¤œé—´ç›‘æ§ç­‰é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡èƒ½å¢å¼ºä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œé€šè¿‡æ•´åˆä¸åŒæ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>ProMSC-MISæ¡†æ¶æ˜¯ä¸€ç§åŸºäºæç¤ºçš„å¤šåª’ä½“è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œç”¨äºå¤šå…‰è°±å›¾åƒåˆ†å‰²ã€‚</li>
<li>åˆ©ç”¨æç¤ºå­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒå•æ¨¡æ€è¯­ä¹‰ç¼–ç å™¨ï¼Œå­¦ä¹ å¤šæ ·ä¸”äº’è¡¥çš„è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>è¯­ä¹‰èåˆæ¨¡å—ç»“åˆäº†è·¨æ³¨æ„åŠ›æœºåˆ¶å’ŒæŒ¤å‹æ¿€åŠ±ç½‘ç»œï¼Œæœ‰æ•ˆèåˆè·¨æ¨¡æ€ç‰¹å¾ã€‚</li>
<li>ProMSC-MISåœ¨é™ä½ä¿¡é“å¸¦å®½éœ€æ±‚çš„åŒæ—¶å®ç°äº†å‡ºè‰²çš„åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒProMSC-MISé™ä½äº†å­˜å‚¨å¼€é”€å’Œè®¡ç®—å¤æ‚åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25541e0def0e2a3fbf36712ba49ea8c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b701aca46bab5b8b3faac6b64f031bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a7384c806baf5d0a97fd01f57c52115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8206ba97a7d76383ee5f59e2bc7ef30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0cde4f15dfb6a88370f51d3b4d78d86.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Quantization-Robustness-to-Input-Degradations-for-Object-Detection"><a href="#Quantization-Robustness-to-Input-Degradations-for-Object-Detection" class="headerlink" title="Quantization Robustness to Input Degradations for Object Detection"></a>Quantization Robustness to Input Degradations for Object Detection</h2><p><strong>Authors:Toghrul Karimov, Hassan Imani, Allan Kazakov</strong></p>
<p>Post-training quantization (PTQ) is crucial for deploying efficient object detection models, like YOLO, on resource-constrained devices. However, the impact of reduced precision on model robustness to real-world input degradations such as noise, blur, and compression artifacts is a significant concern. This paper presents a comprehensive empirical study evaluating the robustness of YOLO models (nano to extra-large scales) across multiple precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8 (TensorRT). We introduce and evaluate a degradation-aware calibration strategy for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix of clean and synthetically degraded images. Models were benchmarked on the COCO dataset under seven distinct degradation conditions (including various types and levels of noise, blur, low contrast, and JPEG compression) and a mixed-degradation scenario. Results indicate that while Static INT8 TensorRT engines offer substantial speedups (<del>1.5-3.3x) with a moderate accuracy drop (</del>3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did not yield consistent, broad improvements in robustness over standard clean-data calibration across most models and degradations. A notable exception was observed for larger model scales under specific noise conditions, suggesting model capacity may influence the efficacy of this calibration approach. These findings highlight the challenges in enhancing PTQ robustness and provide insights for deploying quantized detectors in uncontrolled environments. All code and evaluation tables are available at <a target="_blank" rel="noopener" href="https://github.com/AllanK24/QRID">https://github.com/AllanK24/QRID</a>. </p>
<blockquote>
<p>é’ˆå¯¹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²é«˜æ•ˆçš„ç‰©ä½“æ£€æµ‹æ¨¡å‹ï¼ˆå¦‚YOLOï¼‰è€Œè¨€ï¼Œè®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰æ˜¯éå¸¸å…³é”®çš„ã€‚ç„¶è€Œï¼Œå‡å°‘ç²¾åº¦å¯¹æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­é¢å¯¹è¾“å…¥é€€åŒ–ï¼ˆå¦‚å™ªå£°ã€æ¨¡ç³Šå’Œå‹ç¼©ä¼ªå½±ï¼‰çš„ç¨³å¥æ€§çš„å½±å“æ˜¯ä¸€ä¸ªé‡å¤§å…³æ³¨ç‚¹ã€‚æœ¬æ–‡è¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œè¯„ä¼°äº†YOLOæ¨¡å‹ï¼ˆä»çº³ç±³åˆ°è¶…å¤§å°ºåº¦ï¼‰åœ¨ä¸åŒç²¾åº¦æ ¼å¼ä¸‹çš„ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬FP32ã€FP16ï¼ˆTensorRTï¼‰ã€åŠ¨æ€UINT8ï¼ˆONNXï¼‰å’Œé™æ€INT8ï¼ˆTensorRTï¼‰ã€‚æˆ‘ä»¬é’ˆå¯¹é™æ€INT8 PTQå¼•å…¥å¹¶è¯„ä¼°äº†ä¸€ç§é€€åŒ–æ„ŸçŸ¥æ ¡å‡†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†TensorRTæ ¡å‡†è¿‡ç¨‹æš´éœ²äºå¹²å‡€å›¾åƒå’Œåˆæˆé€€åŒ–å›¾åƒçš„æ··åˆä½“ä¸­ã€‚åœ¨COCOæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹åœ¨ä¸ƒç§ä¸åŒçš„é€€åŒ–æ¡ä»¶ï¼ˆåŒ…æ‹¬å„ç§ç±»å‹å’Œçº§åˆ«çš„å™ªå£°ã€æ¨¡ç³Šã€ä½å¯¹æ¯”åº¦å’ŒJPEGå‹ç¼©ï¼‰ä»¥åŠæ··åˆé€€åŒ–åœºæ™¯ä¸‹é¢è¿›è¡Œäº†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œé™æ€INT8 TensorRTå¼•æ“åœ¨å¹²å‡€æ•°æ®ä¸Šå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼ˆçº¦1.5-3.3å€ï¼‰ï¼ŒåŒæ—¶ç²¾åº¦ä¸‹é™å¹…åº¦é€‚ä¸­ï¼ˆçº¦3-7% mAP50-95ï¼‰ã€‚ç„¶è€Œï¼Œæå‡ºçš„é€€åŒ–æ„ŸçŸ¥æ ¡å‡†å¹¶æœªåœ¨æ‰€æœ‰æ¨¡å‹å’Œé€€åŒ–æƒ…å†µä¸‹å®ç°ä¸€è‡´ä¸”å¹¿æ³›çš„ç¨³å¥æ€§æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ç‰¹å®šå™ªå£°æ¡ä»¶ä¸‹ï¼Œè¾ƒå¤§çš„æ¨¡å‹å°ºåº¦è¡¨ç°å‡ºäº†æ”¹è¿›ï¼Œè¿™è¡¨æ˜æ¨¡å‹å®¹é‡å¯èƒ½å½±å“æ­¤æ ¡å‡†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å‘ç°çªå‡ºäº†æé«˜PTQç¨³å¥æ€§çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºåœ¨ä¸å—æ§ç¯å¢ƒä¸­éƒ¨ç½²é‡åŒ–æ£€æµ‹å™¨æä¾›äº†è§è§£ã€‚æ‰€æœ‰ä»£ç å’Œè¯„ä¼°è¡¨æ ¼å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AllanK24/QRID">https://github.com/AllanK24/QRID</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19600v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²YOLOç­‰é«˜æ•ˆç›®æ ‡æ£€æµ‹æ¨¡å‹çš„é—®é¢˜ï¼Œæœ¬æ–‡ç ”ç©¶äº†é‡åŒ–è®­ç»ƒï¼ˆPTQï¼‰å¯¹æ¨¡å‹é²æ£’æ€§çš„å½±å“ã€‚æ–‡ç« å¯¹YOLOæ¨¡å‹åœ¨ä¸åŒç²¾åº¦æ ¼å¼ä¸‹çš„é²æ£’æ€§è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼ŒåŒ…æ‹¬FP32ã€FP16ï¼ˆTensorRTï¼‰ã€åŠ¨æ€UINT8ï¼ˆONNXï¼‰å’Œé™æ€INT8ï¼ˆTensorRTï¼‰ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§é’ˆå¯¹é™æ€INT8 PTQçš„é€€åŒ–æ„ŸçŸ¥æ ¡å‡†ç­–ç•¥ï¼Œåœ¨TensorRTæ ¡å‡†è¿‡ç¨‹ä¸­ä½¿ç”¨å¹²å‡€å’Œåˆæˆé€€åŒ–å›¾åƒçš„ç»„åˆã€‚æ¨¡å‹åœ¨COCOæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸ƒç§ä¸åŒçš„é€€åŒ–æ¡ä»¶ä¸‹çš„åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬å„ç§ç±»å‹çš„å™ªå£°ã€æ¨¡ç³Šã€ä½å¯¹æ¯”åº¦ä»¥åŠJPEGå‹ç¼©ç­‰ï¼‰ï¼Œç»“æœè¡¨æ˜é™æ€INT8 TensorRTå¼•æ“åœ¨æ¸…æ´æ•°æ®ä¸Šå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼ˆçº¦1.5-3.3å€ï¼‰ï¼Œç²¾åº¦ä¸‹é™å¹…åº¦çº¦ä¸ºä¸­ç­‰ï¼ˆçº¦3-7% mAP50-95ï¼‰ã€‚ç„¶è€Œï¼Œé€€åŒ–æ„ŸçŸ¥æ ¡å‡†å¹¶æœªåœ¨æ‰€æœ‰æ¨¡å‹å’Œé€€åŒ–æ¡ä»¶ä¸‹äº§ç”Ÿä¸€è‡´æ€§çš„ç¨³å¥æ€§æ”¹è¿›ã€‚æ–‡ç« æŒ‡å‡ºï¼Œåœ¨æŸäº›ç‰¹å®šå™ªå£°æ¡ä»¶ä¸‹çš„å¤§å‹æ¨¡å‹å¯èƒ½ä¼šæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œè¿™æš—ç¤ºæ¨¡å‹çš„å®¹é‡å¯èƒ½ä¼šå½±å“æ ¡å‡†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶çªå‡ºäº†å¢å¼ºPTQé²æ£’æ€§çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºåœ¨ä¸å—æ§åˆ¶çš„ç¯å¢ƒä¸­éƒ¨ç½²é‡åŒ–æ£€æµ‹å™¨æä¾›äº†è§è§£ã€‚ä»£ç å’Œè¯„ä¼°è¡¨å‡å¯åœ¨é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/AllanK24/QRID">https://github.com/AllanK24/QRID</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶å¯¹YOLOæ¨¡å‹åœ¨ä¸åŒç²¾åº¦æ ¼å¼ä¸‹çš„é²æ£’æ€§è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ã€‚</li>
<li>ä»‹ç»äº†é’ˆå¯¹é™æ€INT8 PTQçš„é€€åŒ–æ„ŸçŸ¥æ ¡å‡†ç­–ç•¥ï¼Œæ¶‰åŠTensorRTæ ¡å‡†è¿‡ç¨‹ä¸­ä½¿ç”¨å¹²å‡€å’Œåˆæˆé€€åŒ–å›¾åƒçš„ç»„åˆã€‚</li>
<li>æ¨¡å‹åœ¨å¤šç§é€€åŒ–æ¡ä»¶ä¸‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å„ç§ç±»å‹çš„å™ªå£°ã€æ¨¡ç³Šã€ä½å¯¹æ¯”åº¦ä»¥åŠJPEGå‹ç¼©ç­‰ã€‚</li>
<li>é™æ€INT8 TensorRTå¼•æ“åœ¨æ¸…æ´æ•°æ®ä¸Šå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œä¸­ç­‰ç²¾åº¦æŸå¤±ã€‚</li>
<li>é€€åŒ–æ„ŸçŸ¥æ ¡å‡†åœ¨æŸäº›ç‰¹å®šæ¡ä»¶ä¸‹ï¼ˆå¦‚å¤§å‹æ¨¡å‹é¢å¯¹ç‰¹å®šå™ªå£°æ¡ä»¶ï¼‰å¯èƒ½ä¼šæœ‰æ˜æ˜¾æ”¹è¿›æ•ˆæœï¼Œä½†åœ¨æ‰€æœ‰æƒ…å†µä¸‹æœªäº§ç”Ÿä¸€è‡´æ€§æ”¹å–„ã€‚</li>
<li>æ¨¡å‹å®¹é‡å¯èƒ½å½±å“æ ¡å‡†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c57eedfbcb19cbb0148c9b961b9cef44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17b7a3b852bab2815514e6a57ad0e4d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ba5094037dd1d1e71ae4879b2a83138.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a04dd86c65a97fd41d9fcc539379f02c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="JWST-detection-of-a-carbon-dioxide-dominated-gas-coma-surrounding-interstellar-object-3I-ATLAS"><a href="#JWST-detection-of-a-carbon-dioxide-dominated-gas-coma-surrounding-interstellar-object-3I-ATLAS" class="headerlink" title="JWST detection of a carbon dioxide dominated gas coma surrounding   interstellar object 3I&#x2F;ATLAS"></a>JWST detection of a carbon dioxide dominated gas coma surrounding   interstellar object 3I&#x2F;ATLAS</h2><p><strong>Authors:Martin A. Cordiner, Nathaniel X. Roth, Michael S. P. Kelley, Dennis Bodewits, Steven B. Charnley, Maria N. Drozdovskaya, Davide Farnocchia, Marco Micheli, Stefanie N. Milam, Cyrielle Opitom, Megan E. Schwamb, Cristina A. Thomas</strong></p>
<p>3I&#x2F;ATLAS is the third confirmed interstellar object to visit our Solar System, and only the second to display a clear coma. Infrared spectroscopy with the James Webb Space Telescope (JWST) provides the opportunity to measure its coma composition and determine the primary activity drivers. We report the first results from our JWST NIRSpec campaign for 3I&#x2F;ATLAS, at an inbound heliocentric distance of $r_H&#x3D;3.32$ au. The spectral images (spanning 0.6-5.3 $\mu$m) reveal a CO2 dominated coma, with enhanced outgassing in the sunward direction, and the presence of H2O, CO, OCS, water ice and dust. The coma CO2&#x2F;H2O mixing ratio of $8.0\pm1.0$ is among the highest ever observed in a comet, and is 4.4-sigma above the trend as a function of heliocentric distance for long-period and Jupiter-family comets (excluding the outlier C&#x2F;2016 R2). Our observations are compatible with an intrinsically CO2-rich nucleus, which may indicate that 3I&#x2F;ATLAS contains ices exposed to higher levels of radiation than Solar System comets, or that it formed close to the CO2 ice line in its parent protoplanetary disk. A low coma H2O gas abundance may also be implied, for example, due to inhibited heat penetration into the nucleus, which could suppress the H2O sublimation rate relative to CO2 and CO. </p>
<blockquote>
<p>3I&#x2F;ATLASæ˜¯ç¬¬ä¸‰ä¸ªç¡®è®¤è®¿é—®æˆ‘ä»¬å¤ªé˜³ç³»çš„æ˜Ÿé™…ç‰©ä½“ï¼Œä¹Ÿæ˜¯ç¬¬äºŒä¸ªæ˜¾ç¤ºå‡ºæ¸…æ™°å½—å‘çš„ç‰©ä½“ã€‚é€šè¿‡è©¹å§†æ–¯Â·éŸ¦ä¼¯å¤ªç©ºæœ›è¿œé•œï¼ˆJWSTï¼‰çš„çº¢å¤–å…‰è°±æä¾›äº†æµ‹é‡å…¶å½—å‘æˆåˆ†å¹¶ç¡®å®šä¸»è¦æ´»åŠ¨é©±åŠ¨å› ç´ çš„æœºä¼šã€‚æˆ‘ä»¬æŠ¥å‘Šäº†ä½¿ç”¨JWST NIRSpecå¯¹3I&#x2F;ATLASçš„é¦–æ¬¡ç»“æœï¼Œå…¶å‘åœ°å¿ƒè·ç¦»ä¸ºr_H&#x3D;3.32å¤©æ–‡å•ä½ã€‚å…‰è°±å›¾åƒï¼ˆè¦†ç›–0.6-5.3å¾®ç±³ï¼‰æ˜¾ç¤ºä»¥CO2ä¸ºä¸»çš„å½—å‘ï¼Œåœ¨å‘é˜³æ–¹å‘å‡ºç°å¢å¼ºå‡ºæ°”ç°è±¡ï¼Œå¹¶å­˜åœ¨H2Oã€COã€OCSã€æ°´å†°å’Œå°˜åŸƒã€‚å½—å‘ä¸­çš„CO2&#x2F;H2Oæ··åˆæ¯”ä¸º8.0Â±1.0ï¼Œè¿™æ˜¯åœ¨å½—æ˜Ÿä¸­è§‚å¯Ÿåˆ°çš„æœ€é«˜å€¼ä¹‹ä¸€ï¼Œå¹¶ä¸”é«˜äºé•¿å‘¨æœŸå’Œæœ¨æ˜Ÿå®¶æ—å½—æ˜Ÿéšå‘å¿ƒè·ç¦»å˜åŒ–çš„è¶‹åŠ¿ï¼ˆæ’é™¤å¼‚å¸¸å€¼C&#x2F;2016 R2ï¼‰ã€‚æˆ‘ä»¬çš„è§‚æµ‹ç»“æœä¸å›ºæœ‰çš„CO2ä¸°å¯Œçš„æ ¸å¿ƒç›¸ä¸€è‡´ï¼Œè¿™å¯èƒ½è¡¨æ˜3I&#x2F;ATLASåŒ…å«çš„å†°æš´éœ²äºæ¯”å¤ªé˜³ç³»å½—æ˜Ÿæ›´é«˜çš„è¾å°„æ°´å¹³ä¸‹ï¼Œæˆ–è€…å®ƒåœ¨å…¶æ¯è¡Œæ˜Ÿç›˜é™„è¿‘å½¢æˆäºCO2å†°çº¿é™„è¿‘ã€‚å½—å‘çš„H2Oæ°”ä½“å«é‡è¾ƒä½å¯èƒ½æ˜¯ç”±äºçƒ­é‡éš¾ä»¥ç©¿é€æ ¸å¿ƒï¼Œè¿™å¯èƒ½ä¼šæŠ‘åˆ¶ç›¸å¯¹äºCO2å’ŒCOçš„H2Oå‡åé€Ÿç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18209v2">PDF</a> Submitted to ApJ Letters 2025-08-25</p>
<p><strong>Summary</strong>ï¼š</p>
<p>3I&#x2F;ATLASæ˜¯ç¬¬ä¸‰ä¸ªç¡®è®¤è®¿é—®å¤ªé˜³ç³»çš„å¤–å¤ªé˜³ç³»å¤©ä½“ï¼Œä½¿ç”¨è©¹å§†æ–¯éŸ¦ä¼¯å¤ªç©ºæœ›è¿œé•œï¼ˆJWSTï¼‰çš„çº¢å¤–å…‰è°±åˆ†æå…¶ç»„æˆï¼Œæ­ç¤ºå…¶ä»¥äºŒæ°§åŒ–ç¢³ä¸ºä¸»çš„å½—å‘ï¼Œå¹¶å‘ç°å…¶ä»–æˆåˆ†å¦‚H2Oã€COã€OCSç­‰ã€‚å…¶CO2ä¸H2Oçš„æ··åˆæ¯”ä¾‹æé«˜ï¼Œå¯èƒ½æš—ç¤ºå…¶æš´éœ²åœ¨æ›´é«˜æ°´å¹³çš„è¾å°„ä¸‹æˆ–å½¢æˆäºæ¥è¿‘äºŒæ°§åŒ–ç¢³å†°çº¿çš„ä½ç½®ã€‚è§‚æµ‹ç»“æœä¹Ÿå¯èƒ½æš—ç¤ºå…¶æ°´è’¸æ°”çš„å«é‡è¾ƒä½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>3I&#x2F;ATLASæ˜¯ç¬¬ä¸‰ä¸ªç¡®è®¤è®¿é—®å¤ªé˜³ç³»çš„å¤–å¤ªé˜³ç³»å¯¹è±¡ï¼Œç¬¬äºŒä¸ªæ˜¾ç¤ºæ¸…æ™°å½—å‘çš„å¯¹è±¡ã€‚</li>
<li>ä½¿ç”¨JWSTçš„NIRSpecå¯¹å…¶è¿›è¡Œäº†è§‚æµ‹ï¼Œæ­ç¤ºäº†ä»¥CO2ä¸ºä¸»çš„å½—å‘ç»„æˆã€‚</li>
<li>å‘ç°å½—å‘ä¸­åŒ…æ‹¬H2Oã€COã€OCSç­‰ç»„åˆ†ã€‚</li>
<li>CO2ä¸H2Oçš„æ··åˆæ¯”ä¾‹æé«˜ï¼Œè¿™å¯èƒ½æ˜¯å…¶ä¸ä¼—ä¸åŒçš„ç‰¹å¾ä¹‹ä¸€ã€‚</li>
<li>è¿™ç§é«˜æ··åˆæ¯”ä¾‹å¯èƒ½æš—ç¤ºå…¶æš´éœ²åœ¨æ›´é«˜æ°´å¹³çš„è¾å°„ä¸‹ï¼Œæˆ–è€…æ˜¯åœ¨å…¶å½¢æˆçš„è¡Œæ˜Ÿç›˜å†…æ›´æ¥è¿‘äºŒæ°§åŒ–ç¢³å†°çº¿çš„åœ°æ–¹å½¢æˆã€‚</li>
<li>è§‚æµ‹ç»“æœä¹Ÿå¯èƒ½æš—ç¤ºå…¶æ°´è’¸æ°”çš„å«é‡è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-032e5df9780f55433da8e74e16391b5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5235005bb6b8b5a44d2e994b9ad172c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3369e3a3437a3b552013f9c01ebfcce7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e2b546684cca7b4a64388219bda59ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de85ff577308c53c5a00c80b3b4917cf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Emerging-Semantic-Segmentation-from-Positive-and-Negative-Coarse-Label-Learning"><a href="#Emerging-Semantic-Segmentation-from-Positive-and-Negative-Coarse-Label-Learning" class="headerlink" title="Emerging Semantic Segmentation from Positive and Negative Coarse Label   Learning"></a>Emerging Semantic Segmentation from Positive and Negative Coarse Label   Learning</h2><p><strong>Authors:Le Zhang, Fuping Wu, Arun Thirunavukarasu, Kevin Bronik, Thomas Nichols, Bartlomiej W. Papiez</strong></p>
<p>Large annotated datasets are vital for training segmentation models, but pixel-level labeling is time-consuming, error-prone, and often requires scarce expert annotators, especially in medical imaging. In contrast, coarse annotations are quicker, cheaper, and easier to produce, even by non-experts. In this paper, we propose to use coarse drawings from both positive (target) and negative (background) classes in the image, even with noisy pixels, to train a convolutional neural network (CNN) for semantic segmentation. We present a method for learning the true segmentation label distributions from purely noisy coarse annotations using two coupled CNNs. The separation of the two CNNs is achieved by high fidelity with the characters of the noisy training annotations. We propose to add a complementary label learning that encourages estimating negative label distribution. To illustrate the properties of our method, we first use a toy segmentation dataset based on MNIST. We then present the quantitative results of experiments using publicly available datasets: Cityscapes dataset for multi-class segmentation, and retinal images for medical applications. In all experiments, our method outperforms state-of-the-art methods, particularly in the cases where the ratio of coarse annotations is small compared to the given dense annotations. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†å¯¹äºè®­ç»ƒåˆ†å‰²æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†æ˜¯åƒç´ çº§æ ‡æ³¨è€—æ—¶ã€æ˜“å‡ºé”™ï¼Œå¹¶ä¸”ç»å¸¸éœ€è¦ç¨€ç¼ºçš„ä¸“å®¶æ ‡æ³¨è€…ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç²—ç•¥æ ‡æ³¨æ›´å¿«ã€æ›´ä¾¿å®œã€æ›´å®¹æ˜“ç”Ÿæˆï¼Œç”šè‡³éä¸“å®¶ä¹Ÿå¯ä»¥è¿›è¡Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å›¾åƒä¸­æ­£ç±»ï¼ˆç›®æ ‡ï¼‰å’Œè´Ÿç±»ï¼ˆèƒŒæ™¯ï¼‰çš„ç²—ç•¥ç»˜å›¾ï¼Œå³ä½¿æœ‰å™ªå£°åƒç´ ï¼Œæ¥è®­ç»ƒç”¨äºè¯­ä¹‰åˆ†å‰²çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä»çº¯ç²¹çš„å™ªå£°ç²—ç•¥æ ‡æ³¨ä¸­å­¦ä¹ çœŸæ­£çš„åˆ†å‰²æ ‡ç­¾åˆ†å¸ƒï¼Œä½¿ç”¨ä¸¤ä¸ªè€¦åˆçš„CNNã€‚ä¸¤ä¸ªCNNçš„é«˜ä¿çœŸåˆ†ç¦»æ˜¯é€šè¿‡å™ªå£°è®­ç»ƒæ ‡æ³¨çš„ç‰¹å¾æ¥å®ç°çš„ã€‚æˆ‘ä»¬æå‡ºæ·»åŠ ä¸€ç§äº’è¡¥æ ‡ç­¾å­¦ä¹ ï¼Œé¼“åŠ±ä¼°è®¡è´Ÿæ ‡ç­¾åˆ†å¸ƒã€‚ä¸ºäº†è¯´æ˜æˆ‘ä»¬æ–¹æ³•çš„ç‰¹æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨åŸºäºMNISTçš„ç©å…·åˆ†å‰²æ•°æ®é›†ä¸Šä½¿ç”¨å®ƒã€‚ç„¶åï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨å…¬å¼€å¯ç”¨æ•°æ®é›†è¿›è¡Œçš„å®éªŒçš„å®šé‡ç»“æœï¼šç”¨äºå¤šç±»åˆ†å‰²çš„Cityscapesæ•°æ®é›†å’Œç”¨äºåŒ»å­¦åº”ç”¨çš„è§†ç½‘è†œå›¾åƒã€‚åœ¨æ‰€æœ‰å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸ç»™å®šå¯†é›†æ ‡æ³¨ç›¸æ¯”ï¼Œç²—ç•¥æ ‡æ³¨çš„æ¯”ä¾‹è¾ƒå°çš„æƒ…å†µä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18186v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†å¯¹äºè®­ç»ƒåˆ†å‰²æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†åƒç´ çº§æ ‡æ³¨è€—æ—¶ã€æ˜“å‡ºé”™ï¼Œä¸”é€šå¸¸éœ€è¦ç¨€ç¼ºçš„ä¸“å®¶æ ‡æ³¨è€…ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒä¸­ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç²—ç•¥æ ‡æ³¨æ›´å¿«ã€æ›´ä¾¿å®œï¼Œç”šè‡³éä¸“å®¶ä¹Ÿèƒ½è½»æ¾åˆ¶ä½œã€‚æœ¬æ–‡æå‡ºä½¿ç”¨å›¾åƒä¸­æ­£è´Ÿç±»ï¼ˆç›®æ ‡åŠèƒŒæ™¯ï¼‰çš„ç²—ç•¥ç»˜å›¾ï¼Œç”šè‡³å¸¦æœ‰å™ªå£°åƒç´ ï¼Œæ¥è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»çº¯ç²¹çš„å™ªå£°ç²—ç•¥æ ‡æ³¨ä¸­å­¦ä¹ çœŸæ­£çš„åˆ†å‰²æ ‡ç­¾åˆ†å¸ƒçš„æ–¹æ³•ï¼Œä½¿ç”¨ä¸¤ä¸ªè€¦åˆçš„CNNã€‚ä¸¤ä¸ªCNNçš„é«˜ä¿çœŸåˆ†ç¦»æ˜¯ç”±å™ªå£°è®­ç»ƒæ ‡æ³¨çš„ç‰¹å¾å®ç°çš„ã€‚æˆ‘ä»¬æå‡ºæ·»åŠ ä¸€ç§è¡¥å……æ ‡ç­¾å­¦ä¹ ï¼Œä»¥é¼“åŠ±ä¼°è®¡è´Ÿæ ‡ç­¾åˆ†å¸ƒã€‚ä¸ºäº†è¯´æ˜æˆ‘ä»¬æ–¹æ³•çš„ç‰¹æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨åŸºäºMNISTçš„ç©å…·åˆ†å‰²æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å…¬å¼€æ•°æ®é›†ä¸Šå®éªŒçš„æ•°é‡ç»“æœï¼šç”¨äºå¤šç±»åˆ†å‰²çš„Cityscapesæ•°æ®é›†å’Œç”¨äºåŒ»å­¦åº”ç”¨çš„è§†ç½‘è†œå›¾åƒã€‚æ‰€æœ‰å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²—ç•¥æ ‡æ³¨ä¸ç»™å®šå¯†é›†æ ‡æ³¨ç›¸æ¯”æ‰€å æ¯”ä¾‹è¾ƒå°çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åƒç´ çº§æ ‡æ³¨å¯¹äºè®­ç»ƒåˆ†å‰²æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨æ—¶é—´æˆæœ¬é«˜ã€æ˜“å‡ºé”™å’Œç¼ºä¹ä¸“å®¶æ ‡æ³¨è€…çš„é—®é¢˜ã€‚</li>
<li>ç›¸æ¯”åƒç´ çº§æ ‡æ³¨ï¼Œä½¿ç”¨ç²—ç•¥æ ‡æ³¨æ›´åŠ å¿«é€Ÿã€ç»æµä¸”æ˜“äºåˆ¶ä½œã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨å›¾åƒä¸­çš„æ­£è´Ÿç±»ç²—ç•¥ç»˜å›¾æ¥è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªè€¦åˆçš„CNNä»å™ªå£°ç²—ç•¥æ ‡æ³¨ä¸­å­¦ä¹ çœŸæ­£çš„åˆ†å‰²æ ‡ç­¾åˆ†å¸ƒã€‚</li>
<li>å™ªå£°è®­ç»ƒæ ‡æ³¨çš„ç‰¹å¾å†³å®šäº†ä¸¤ä¸ªCNNçš„é«˜ä¿çœŸåˆ†ç¦»ã€‚</li>
<li>æ·»åŠ äº†ä¸€ç§è¡¥å……æ ‡ç­¾å­¦ä¹ ä»¥é¼“åŠ±ä¼°è®¡è´Ÿæ ‡ç­¾åˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9dfb1c044947fcabfa906c8499013951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1ca52d6a317adf25328a7301c9f338a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="BirdRecorderâ€™s-AI-on-Sky-Safeguarding-birds-of-prey-by-detection-and-classification-of-tiny-objects-around-wind-turbines"><a href="#BirdRecorderâ€™s-AI-on-Sky-Safeguarding-birds-of-prey-by-detection-and-classification-of-tiny-objects-around-wind-turbines" class="headerlink" title="BirdRecorderâ€™s AI on Sky: Safeguarding birds of prey by detection and   classification of tiny objects around wind turbines"></a>BirdRecorderâ€™s AI on Sky: Safeguarding birds of prey by detection and   classification of tiny objects around wind turbines</h2><p><strong>Authors:Nico Klar, Nizam Gifary, Felix P. G. Ziegler, Frank Sehnke, Anton Kaifel, Eric Price, Aamir Ahmad</strong></p>
<p>The urgent need for renewable energy expansion, particularly wind power, is hindered by conflicts with wildlife conservation. To address this, we developed BirdRecorder, an advanced AI-based anti-collision system to protect endangered birds, especially the red kite (Milvus milvus). Integrating robotics, telemetry, and high-performance AI algorithms, BirdRecorder aims to detect, track, and classify avian species within a range of 800 m to minimize bird-turbine collisions.   BirdRecorder integrates advanced AI methods with optimized hardware and software architectures to enable real-time image processing. Leveraging Single Shot Detector (SSD) for detection, combined with specialized hardware acceleration and tracking algorithms, our system achieves high detection precision while maintaining the speed necessary for real-time decision-making. By combining these components, BirdRecorder outperforms existing approaches in both accuracy and efficiency.   In this paper, we summarize results on field tests and performance of the BirdRecorder system. By bridging the gap between renewable energy expansion and wildlife conservation, BirdRecorder contributes to a more sustainable coexistence of technology and nature. </p>
<blockquote>
<p>å¯¹äºå¯å†ç”Ÿèƒ½æºçš„æ‰©å¼ ï¼Œå°¤å…¶æ˜¯é£åŠ›å‘ç”µçš„è¿«åˆ‡éœ€æ±‚ï¼Œä¸é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ä¹‹é—´å­˜åœ¨å†²çªã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†BirdRecorderï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„åŸºäºAIçš„é˜²æ’ç³»ç»Ÿï¼Œæ—¨åœ¨ä¿æŠ¤æ¿’å±é¸Ÿç±»ï¼Œå°¤å…¶æ˜¯çº¢é¸¢ï¼ˆMilvus milvusï¼‰ã€‚BirdRecorderèåˆäº†æœºå™¨äººæŠ€æœ¯ã€é¥æµ‹æŠ€æœ¯å’Œé«˜æ€§èƒ½AIç®—æ³•ï¼Œæ—¨åœ¨åœ¨800ç±³çš„èŒƒå›´å†…æ£€æµ‹ã€è·Ÿè¸ªå’Œåˆ†ç±»é¸Ÿç±»ç‰©ç§ï¼Œä»¥å°½é‡å‡å°‘é¸Ÿç±»ä¸æ¶¡è½®æœºçš„ç¢°æ’ã€‚BirdRecorderç»“åˆäº†å…ˆè¿›çš„AIæ–¹æ³•ï¼Œå¹¶ä¼˜åŒ–äº†ç¡¬ä»¶å’Œè½¯ä»¶æ¶æ„ï¼Œä»¥å®ç°å®æ—¶å›¾åƒå¤„ç†ã€‚å®ƒåˆ©ç”¨å•é•œå¤´æ£€æµ‹å™¨ï¼ˆSSDï¼‰è¿›è¡Œæ£€æµ‹ï¼Œç»“åˆä¸“é—¨çš„ç¡¬ä»¶åŠ é€Ÿå’Œè·Ÿè¸ªç®—æ³•ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä¿æŒå®æ—¶å†³ç­–æ‰€éœ€é€Ÿåº¦çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ£€æµ‹ç²¾åº¦ã€‚é€šè¿‡ç»“åˆè¿™äº›ç»„ä»¶ï¼ŒBirdRecorderåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æœ¬æ–‡æ€»ç»“äº†BirdRecorderç³»ç»Ÿåœ¨å®é™…æµ‹è¯•å’Œæ€§èƒ½æ–¹é¢çš„æˆæœã€‚é€šè¿‡å¼¥è¡¥å¯å†ç”Ÿèƒ½æºæ‰©å¼ å’Œé‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ä¹‹é—´çš„é¸¿æ²Ÿï¼ŒBirdRecorderä¸ºæŠ€æœ¯ä¸è‡ªç„¶çš„æ›´å¯æŒç»­å…±å­˜åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18136v1">PDF</a> 18 pages, 1 figures, to appear in Proceedings of the 19th   International Conference on Intelligent Autonomous Systems (IAS-19), Genoa,   Italy, 2025</p>
<p><strong>Summary</strong><br>é£åŠ›å‘ç”µç­‰å¯å†ç”Ÿèƒ½æºæ‰©å¼ ä¸é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤å­˜åœ¨å†²çªã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†BirdRecorderç³»ç»Ÿï¼Œåˆ©ç”¨AIæŠ€æœ¯ä¿æŠ¤æ¿’å±é¸Ÿç±»å¦‚çº¢é¸¢ã€‚è¯¥ç³»ç»Ÿç»“åˆæœºå™¨äººæŠ€æœ¯ã€é¥æµ‹å’Œå…ˆè¿›çš„äººå·¥æ™ºèƒ½ç®—æ³•ï¼Œåœ¨800ç±³èŒƒå›´å†…æ£€æµ‹ã€è·Ÿè¸ªå’Œåˆ†ç±»é¸Ÿç±»ï¼Œæœ€å°åŒ–é¸Ÿä¸é£åŠ›æ¶¡è½®çš„ç¢°æ’ã€‚BirdRecorderç³»ç»Ÿé‡‡ç”¨å…ˆè¿›çš„AIæ–¹æ³•ï¼Œç»“åˆä¼˜åŒ–çš„è½¯ç¡¬ä»¶æ¶æ„ï¼Œå®ç°å®æ—¶å›¾åƒå¤„ç†ã€‚åˆ©ç”¨SSDæ£€æµ‹å™¨ç»“åˆç¡¬ä»¶åŠ é€Ÿå’Œè·Ÿè¸ªç®—æ³•ï¼Œç³»ç»Ÿå…·æœ‰é«˜æ£€æµ‹ç²¾åº¦å’Œå®æ—¶å†³ç­–çš„é€Ÿåº¦ã€‚BirdRecorderåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚æœ¬æ–‡æ€»ç»“äº†BirdRecorderç³»ç»Ÿçš„ç°åœºæµ‹è¯•ç»“æœå’Œæ€§èƒ½è¡¨ç°ï¼Œä¸ºå¯å†ç”Ÿèƒ½æºæ‰©å¼ ä¸é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ä¹‹é—´çš„å¹³è¡¡åšå‡ºäº†è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>BirdRecorderæ˜¯ä¸€ç§å…ˆè¿›çš„äººå·¥æ™ºèƒ½é˜²ç¢°æ’ç³»ç»Ÿï¼Œæ—¨åœ¨ä¿æŠ¤æ¿’å±é¸Ÿç±»å…å—é£åŠ›æ¶¡è½®æœºçš„ä¼¤å®³ã€‚</li>
<li>ç³»ç»Ÿç»“åˆäº†æœºå™¨äººæŠ€æœ¯ã€é¥æµ‹å’ŒAIç®—æ³•æ¥æ£€æµ‹ã€è·Ÿè¸ªå’Œåˆ†ç±»é¸Ÿç±»ã€‚</li>
<li>BirdRecorderå…·æœ‰å®æ—¶å›¾åƒå¤„ç†åŠŸèƒ½ï¼Œé‡‡ç”¨SSDæ£€æµ‹å™¨å®ç°é«˜æ£€æµ‹ç²¾åº¦å’Œå¿«é€Ÿå†³ç­–ã€‚</li>
<li>ç³»ç»Ÿåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
<li>BirdRecorderçš„ç°åœºæµ‹è¯•ç»“æœå’Œæ€§èƒ½è¡¨ç°å¾—åˆ°äº†æ€»ç»“ã€‚</li>
<li>è¯¥ç³»ç»Ÿæœ‰åŠ©äºå¹³è¡¡å¯å†ç”Ÿèƒ½æºæ‰©å¼ å’Œé‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ä¹‹é—´çš„å…³ç³»ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd18ec93e45798b22f4b854ce0cddddc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28758a3572f844aeef49743a5127ccca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7cdbf392cbc2c5fb589fffc7748bdd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b0c7bd1ae30279f513609141d3dd95c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e976e32660b8ba93fe4dc2284e985948.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Prompt-based-Multimodal-Semantic-Communication-for-Multi-spectral-Image-Segmentation"><a href="#Prompt-based-Multimodal-Semantic-Communication-for-Multi-spectral-Image-Segmentation" class="headerlink" title="Prompt-based Multimodal Semantic Communication for Multi-spectral Image   Segmentation"></a>Prompt-based Multimodal Semantic Communication for Multi-spectral Image   Segmentation</h2><p><strong>Authors:Haoshuo Zhang, Yufei Bo, Hongwei Zhang, Meixia Tao</strong></p>
<p>Multimodal semantic communication has gained widespread attention due to its ability to enhance downstream task performance. A key challenge in such systems is the effective fusion of features from different modalities, which requires the extraction of rich and diverse semantic representations from each modality. To this end, we propose ProMSC-MIS, a Prompt-based Multimodal Semantic Communication system for Multi-spectral Image Segmentation. Specifically, we propose a pre-training algorithm where features from one modality serve as prompts for another, guiding unimodal semantic encoders to learn diverse and complementary semantic representations. We further introduce a semantic fusion module that combines cross-attention mechanisms and squeeze-and-excitation (SE) networks to effectively fuse cross-modal features. Simulation results show that ProMSC-MIS significantly outperforms benchmark methods across various channel-source compression levels, while maintaining low computational complexity and storage overhead. Our scheme has great potential for applications such as autonomous driving and nighttime surveillance. </p>
<blockquote>
<p>å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡å› å…¶èƒ½å¢å¼ºä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½è€Œå¤‡å—å…³æ³¨ã€‚æ­¤ç±»ç³»ç»Ÿçš„å…³é”®æŒ‘æˆ˜åœ¨äºæœ‰æ•ˆåœ°èåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼Œè¿™éœ€è¦ä»æ¯ä¸ªæ¨¡æ€ä¸­æå–ä¸°å¯Œå’Œå¤šæ ·åŒ–çš„è¯­ä¹‰è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ProMSC-MISï¼Œä¸€ä¸ªåŸºäºæç¤ºçš„å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼Œç”¨äºå¤šå…‰è°±å›¾åƒåˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢„è®­ç»ƒç®—æ³•ï¼Œå…¶ä¸­æŸä¸€æ¨¡æ€çš„ç‰¹å¾ä¸ºå¦ä¸€æ¨¡æ€æä¾›æç¤ºï¼Œå¼•å¯¼å•æ¨¡æ€è¯­ä¹‰ç¼–ç å™¨å­¦ä¹ å¤šæ ·åŒ–å’Œäº’è¡¥çš„è¯­ä¹‰è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè¯­ä¹‰èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—ç»“åˆäº†äº¤å‰æ³¨æ„æœºåˆ¶å’ŒæŒ¤å‹æ¿€å‘ï¼ˆSEï¼‰ç½‘ç»œï¼Œä»¥æœ‰æ•ˆåœ°èåˆè·¨æ¨¡æ€ç‰¹å¾ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§é€šé“æºå‹ç¼©çº§åˆ«ä¸Šï¼ŒProMSC-MISçš„æ€§èƒ½å‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—å¤æ‚æ€§å’Œå­˜å‚¨å¼€é”€ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨è‡ªåŠ¨é©¾é©¶å’Œå¤œé—´ç›‘æ§ç­‰åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17920v2">PDF</a> The full-length version, arXiv:2508.20057, has been updated</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡å› å…¶èƒ½æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚å…³é”®æŒ‘æˆ˜åœ¨äºå¦‚ä½•æœ‰æ•ˆèåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼Œè¿™éœ€è¦ä»æ¯ç§æ¨¡æ€ä¸­æå–ä¸°å¯Œå¤šæ ·çš„è¯­ä¹‰è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæç¤ºçš„å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ç³»ç»ŸProMSC-MISï¼Œç”¨äºå¤šå…‰è°±å›¾åƒåˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢„è®­ç»ƒç®—æ³•ï¼Œå…¶ä¸­ä¸€ç§æ¨¡æ€çš„ç‰¹å¾ä¸ºå¦ä¸€ç§æ¨¡æ€æä¾›æç¤ºï¼Œå¼•å¯¼å•æ¨¡æ€è¯­ä¹‰ç¼–ç å™¨å­¦ä¹ å¤šæ ·ä¸”äº’è¡¥çš„è¯­ä¹‰è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè¯­ä¹‰èåˆæ¨¡å—ï¼Œç»“åˆäº¤å‰æ³¨æ„æœºåˆ¶å’ŒæŒ¤å‹æ¿€åŠ±ç½‘ç»œï¼Œæœ‰æ•ˆåœ°èåˆäº†è·¨æ¨¡æ€ç‰¹å¾ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒProMSC-MISåœ¨å¤šç§é€šé“æºå‹ç¼©æ°´å¹³ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—å¤æ‚åº¦å’Œå­˜å‚¨å¼€é”€ã€‚è¯¥æ–¹æ¡ˆåœ¨è‡ªåŠ¨é©¾é©¶å’Œå¤œé—´ç›‘æ§ç­‰é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡èƒ½å¢å¼ºä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œå…³é”®æŒ‘æˆ˜åœ¨äºä¸åŒæ¨¡æ€ç‰¹å¾çš„èåˆã€‚</li>
<li>ProMSC-MISæ˜¯ä¸€ä¸ªåŸºäºæç¤ºçš„å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼Œç”¨äºå¤šå…‰è°±å›¾åƒåˆ†å‰²ã€‚</li>
<li>æå‡ºäº†é¢„è®­ç»ƒç®—æ³•ï¼Œé€šè¿‡ä¸€ç§æ¨¡æ€çš„ç‰¹å¾æç¤ºå¦ä¸€ç§æ¨¡æ€ï¼Œå­¦ä¹ å¤šæ ·ä¸”äº’è¡¥çš„è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥äº†è¯­ä¹‰èåˆæ¨¡å—ï¼Œç»“åˆäº¤å‰æ³¨æ„æœºåˆ¶å’ŒæŒ¤å‹æ¿€åŠ±ç½‘ç»œï¼Œæœ‰æ•ˆèåˆè·¨æ¨¡æ€ç‰¹å¾ã€‚</li>
<li>ä»¿çœŸç»“æœæ˜¾ç¤ºProMSC-MISåœ¨å¤šç§å‹ç¼©æ°´å¹³ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ProMSC-MISå…·æœ‰è¾ƒä½çš„è®¡ç®—å¤æ‚åº¦å’Œå­˜å‚¨å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7f8128674e050dda63db32a566a2d88e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-906df5e6d0177bd26a0e640d7a7d646a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa1fdbffb2b5201107cec69ec7190f54.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b6f854056bd2c18f713e13643a671e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fdc3787ba76f3ce1a274cadeba405bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b652c2b2e1b6f929579c4680a03edfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-024e1be695b44c9ebc5041965db187fe.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d72f2a50eb0ef8ddafc53cbace7db756.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  CryptoFace End-to-End Encrypted Face Recognition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1a18a3a6abfb6c9ac649e6950b4fd7fe.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Aesthetic Image Captioning with Saliency Enhanced MLLMs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
