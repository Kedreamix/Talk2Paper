<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  TaleDiffusion Multi-Character Story Generation with Dialogue Rendering">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-d1cb3130dc8e0118f1f51b8900d31682~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801874&auth_key=1759801874-0-0-a27e26daa083cb4853519920a5292d9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-08-æ›´æ–°"><a href="#2025-09-08-æ›´æ–°" class="headerlink" title="2025-09-08 æ›´æ–°"></a>2025-09-08 æ›´æ–°</h1><h2 id="TaleDiffusion-Multi-Character-Story-Generation-with-Dialogue-Rendering"><a href="#TaleDiffusion-Multi-Character-Story-Generation-with-Dialogue-Rendering" class="headerlink" title="TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering"></a>TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering</h2><p><strong>Authors:Ayan Banerjee, Josep LladÃ³s, Umapada Pal, Anjan Dutta</strong></p>
<p>Text-to-story visualization is challenging due to the need for consistent interaction among multiple characters across frames. Existing methods struggle with character consistency, leading to artifact generation and inaccurate dialogue rendering, which results in disjointed storytelling. In response, we introduce TaleDiffusion, a novel framework for generating multi-character stories with an iterative process, maintaining character consistency, and accurate dialogue assignment via postprocessing. Given a story, we use a pre-trained LLM to generate per-frame descriptions, character details, and dialogues via in-context learning, followed by a bounded attention-based per-box mask technique to control character interactions and minimize artifacts. We then apply an identity-consistent self-attention mechanism to ensure character consistency across frames and region-aware cross-attention for precise object placement. Dialogues are also rendered as bubbles and assigned to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion outperforms existing methods in consistency, noise reduction, and dialogue rendering. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°æ•…äº‹å¯è§†åŒ–æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦åœ¨å¤šä¸ªå¸§ä¹‹é—´è¿›è¡ŒæŒç»­çš„è§’è‰²äº’åŠ¨ã€‚ç°æœ‰æ–¹æ³•åœ¨è§’è‰²ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´å‡ºç°ä¼ªå½±å’Œå¯¹è¯æ¸²æŸ“ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œä»è€Œå¯¼è‡´æ•…äº‹å™è¿°ä¸è¿è´¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TaleDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è¿­ä»£è¿‡ç¨‹ç”Ÿæˆå¤šè§’è‰²æ•…äº‹çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡åæœŸå¤„ç†æ¥ä¿æŒè§’è‰²ä¸€è‡´æ€§å’Œå‡†ç¡®çš„å¯¹è¯åˆ†é…ã€‚ç»™å®šä¸€ä¸ªæ•…äº‹ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„LLMé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ç”Ÿæˆæ¯å¸§æè¿°ã€è§’è‰²ç»†èŠ‚å’Œå¯¹è¯ï¼Œç„¶åé€šè¿‡åŸºäºè¾¹ç•Œçš„æ³¨æ„åŠ›æ„ŸçŸ¥æ¡†æ©ç æŠ€æœ¯æ¥æ§åˆ¶è§’è‰²äº’åŠ¨å¹¶å°½é‡å‡å°‘ä¼ªå½±ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åº”ç”¨èº«ä»½ä¸€è‡´çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»¥ç¡®ä¿è·¨å¸§çš„è§’è‰²ä¸€è‡´æ€§ï¼Œä»¥åŠåŒºåŸŸæ„ŸçŸ¥çš„è·¨æ³¨æ„åŠ›ä»¥å®ç°ç²¾ç¡®çš„å¯¹è±¡æ”¾ç½®ã€‚å¯¹è¯ä¹Ÿè¢«æ¸²æŸ“ä¸ºæ°”æ³¡ï¼Œå¹¶é€šè¿‡CLIPSegåˆ†é…ç»™è§’è‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸€è‡´æ€§ã€é™å™ªå’Œå¯¹è¯æ¸²æŸ“æ–¹é¢ï¼ŒTaleDiffusionä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04123v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTaleDiffusionçš„æ–°æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤šè§’è‰²æ•…äº‹ã€‚è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£è¿‡ç¨‹ç»´æŠ¤è§’è‰²ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡åå¤„ç†å®ç°å‡†ç¡®çš„å¯¹è¯åˆ†é…ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ¯å¸§æè¿°ã€è§’è‰²ç»†èŠ‚å’Œå¯¹è¯ï¼Œé‡‡ç”¨åŸºäºæœ‰ç•Œæ³¨æ„åŠ›çš„æ¯æ¡†æ©ç æŠ€æœ¯æ§åˆ¶è§’è‰²äº¤äº’å¹¶å‡å°‘ä¼ªå½±ã€‚é€šè¿‡èº«ä»½ä¸€è‡´çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿è·¨å¸§çš„è§’è‰²ä¸€è‡´æ€§ï¼Œä»¥åŠåŒºåŸŸæ„ŸçŸ¥çš„è·¨æ³¨æ„åŠ›å®ç°ç²¾ç¡®çš„å¯¹è±¡æ”¾ç½®ã€‚å¯¹è¯ä»¥æ°”æ³¡å½¢å¼å‘ˆç°ï¼Œå¹¶é€šè¿‡CLIPSegåˆ†é…ç»™è§’è‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTaleDiffusionåœ¨ä¸€è‡´æ€§ã€é™å™ªå’Œå¯¹è¯æ¸²æŸ“æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TaleDiffusionæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå¤šè§’è‰²æ•…äº‹çš„å…¨æ–°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£è¿‡ç¨‹ç»´æŠ¤è§’è‰²ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ¯å¸§çš„æè¿°ã€è§’è‰²ç»†èŠ‚å’Œå¯¹è¯ã€‚</li>
<li>é‡‡ç”¨åŸºäºæœ‰ç•Œæ³¨æ„åŠ›çš„æ¯æ¡†æ©ç æŠ€æœ¯æ¥æ§åˆ¶è§’è‰²äº¤äº’å¹¶å‡å°‘ä¼ªå½±ã€‚</li>
<li>é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿è·¨å¸§çš„è§’è‰²ä¸€è‡´æ€§ã€‚</li>
<li>é‡‡ç”¨åŒºåŸŸæ„ŸçŸ¥çš„è·¨æ³¨æ„åŠ›å®ç°ç²¾ç¡®çš„å¯¹è±¡æ”¾ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dd73ae4b5667c4f5a6a50230afbb9aa1.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b67282e01ab5edafdcc4f062d2bd1e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801890&auth_key=1759801890-0-0-2233abcca85818952a46b2018b7c2acf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-7758e741f3a34ba4a006bb89df8f0f8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fda63a021f4110ed7ca3a9dd6ddc1d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5132a763b2d96b95fa865be519dfc0c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Stable-and-Personalised-Profiles-for-Lexical-Alignment-in-Spoken-Human-Agent-Dialogue"><a href="#Towards-Stable-and-Personalised-Profiles-for-Lexical-Alignment-in-Spoken-Human-Agent-Dialogue" class="headerlink" title="Towards Stable and Personalised Profiles for Lexical Alignment in Spoken   Human-Agent Dialogue"></a>Towards Stable and Personalised Profiles for Lexical Alignment in Spoken   Human-Agent Dialogue</h2><p><strong>Authors:Keara Schaaij, Roel Boumans, Tibor Bosse, Iris Hendrickx</strong></p>
<p>Lexical alignment, where speakers start to use similar words across conversation, is known to contribute to successful communication. However, its implementation in conversational agents remains underexplored, particularly considering the recent advancements in large language models (LLMs). As a first step towards enabling lexical alignment in human-agent dialogue, this study draws on strategies for personalising conversational agents and investigates the construction of stable, personalised lexical profiles as a basis for lexical alignment. Specifically, we varied the amounts of transcribed spoken data used for construction as well as the number of items included in the profiles per part-of-speech (POS) category and evaluated profile performance across time using recall, coverage, and cosine similarity metrics. It was shown that smaller and more compact profiles, created after 10 min of transcribed speech containing 5 items for adjectives, 5 items for conjunctions, and 10 items for adverbs, nouns, pronouns, and verbs each, offered the best balance in both performance and data efficiency. In conclusion, this study offers practical insights into constructing stable, personalised lexical profiles, taking into account minimal data requirements, serving as a foundational step toward lexical alignment strategies in conversational agents. </p>
<blockquote>
<p>è¯æ±‡å¯¹é½å¯¹è¯è¿‡ç¨‹ä¸­ï¼Œè¯´è¯è€…å¼€å§‹ä½¿ç”¨ç›¸ä¼¼çš„è¯æ±‡ï¼Œè¿™å¯¹äºæˆåŠŸäº¤æµèµ·ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå®ƒåœ¨å¯¹è¯ä»£ç†ä¸­çš„å®ç°ä»ç„¶é²œæœ‰ç ”ç©¶ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ã€‚ä½œä¸ºå®ç°äººæœºå¯¹è¯è¯æ±‡å¯¹é½çš„ç¬¬ä¸€æ­¥ï¼Œæœ¬ç ”ç©¶å€Ÿé‰´ä¸ªæ€§åŒ–å¯¹è¯ä»£ç†çš„ç­–ç•¥ï¼Œå¹¶ç ”ç©¶æ„å»ºç¨³å®šã€ä¸ªæ€§åŒ–çš„è¯æ±‡è¡¨ä½œä¸ºè¯æ±‡å¯¹é½çš„åŸºç¡€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸åŒé•¿åº¦çš„è¯æ±‡è¡¨ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒè¯æ±‡æ•°é‡å¯¹è¯æ±‡å¯¹é½çš„å½±å“ï¼Œä½¿ç”¨å¬å›ç‡ã€è¦†ç›–ç‡å’Œä½™å¼¦ç›¸ä¼¼åº¦ç­‰æŒ‡æ ‡æ¥è¯„ä¼°è¯æ±‡è¡¨éšæ—¶é—´å˜åŒ–çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡åŒ…å«å½¢å®¹è¯5é¡¹ã€è¿è¯5é¡¹ä»¥åŠæ¯ä¸ªåŒ…å«å‰¯è¯ã€åè¯ã€ä»£è¯å’ŒåŠ¨è¯å„å«å¤§çº¦äº”é¡¹å¤§çº¦æŒç»­çº¦ååˆ†é’Ÿå†…å®¹çš„è¯­éŸ³å†…å®¹æ„å»ºçš„è¯æ±‡è¡¨èƒ½æœ€ä½³åœ°å®ç°æ€§èƒ½å’Œæ•°æ®æ•ˆç‡çš„å¹³è¡¡ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬ç ”ç©¶ä»æ„å»ºç¨³å®šçš„ä¸ªæ€§åŒ–è¯æ±‡è¡¨è§’åº¦å…¥æ‰‹ï¼Œæ·±å…¥æ¢è®¨äº†å¦‚ä½•å°½å¯èƒ½é™ä½æ•°æ®é‡éœ€æ±‚ï¼Œä½œä¸ºå¯¹è¯ä»£ç†ä¸­å®ç°è¯æ±‡å¯¹é½ç­–ç•¥çš„åŸºç¡€æ­¥éª¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04104v1">PDF</a> Accepted for TSD 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è¯æ±‡å¯¹é½åœ¨å¯¹è¯äº¤æµä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨äººä¸æ™ºèƒ½å¯¹è¯ç³»ç»Ÿä¸­çš„è¿ç”¨ã€‚ç ”ç©¶æå‡ºä¸ªæ€§åŒ–å¯¹è¯ç³»ç»Ÿçš„ç­–ç•¥ï¼Œå¹¶æ„å»ºç¨³å®šã€ä¸ªæ€§åŒ–çš„è¯æ±‡è¡¨ä½œä¸ºè¯æ±‡å¯¹é½çš„åŸºç¡€ã€‚é€šè¿‡è°ƒæ•´è½¬å½•è¯­éŸ³æ•°æ®çš„æ•°é‡å’Œè¯æ±‡è¡¨ä¸­æ¯ä¸ªè¯ç±»çš„é¡¹ç›®æ•°é‡ï¼Œç ”ç©¶è¯„ä¼°äº†è¯æ±‡è¡¨çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæ„å»ºåŒ…å«å½¢å®¹è¯5é¡¹ã€è¿è¯5é¡¹ä»¥åŠå‰¯è¯ã€åè¯ã€ä»£è¯å’ŒåŠ¨è¯å„10é¡¹çš„ç®€çŸ­è¯æ±‡è¡¨å¯è¾¾åˆ°æœ€ä½³çš„æ€§èƒ½ä¸æ•°æ®æ•ˆç‡å¹³è¡¡ã€‚è¿™ä¸ºå¯¹è¯ç³»ç»Ÿä¸­çš„è¯æ±‡å¯¹é½ç­–ç•¥æä¾›äº†å®è·µæ€§çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯æ±‡å¯¹é½åœ¨å¯¹è¯äº¤æµä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨äººä¸æ™ºèƒ½å¯¹è¯ç³»ç»Ÿçš„äº¤äº’ä¸­ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸ªæ€§åŒ–å¯¹è¯ç³»ç»Ÿçš„ç­–ç•¥ï¼Œè¿™æ˜¯å®ç°è¯æ±‡å¯¹é½çš„é‡è¦ä¸€æ­¥ã€‚</li>
<li>æ„å»ºç¨³å®šã€ä¸ªæ€§åŒ–çš„è¯æ±‡è¡¨æ˜¯å®ç°è¯æ±‡å¯¹é½çš„åŸºç¡€ã€‚</li>
<li>è°ƒæ•´è½¬å½•è¯­éŸ³æ•°æ®çš„æ•°é‡å’Œè¯æ±‡è¡¨ä¸­æ¯ä¸ªè¯ç±»çš„é¡¹ç›®æ•°é‡ä¼šå½±å“è¯æ±‡è¡¨çš„æ€§èƒ½ã€‚</li>
<li>ç®€çŸ­ä¸”åŒ…å«ç‰¹å®šé¡¹ç›®æ•°é‡çš„è¯æ±‡è¡¨åœ¨æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¯¹è¯ç³»ç»Ÿä¸­è¯æ±‡å¯¹é½ç­–ç•¥çš„å®è·µæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d38b412d2625044de8e815a535310fd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20c6af2761883fb2c4ff9caf4ee8af85.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Chatbot-Deployment-Considerations-for-Application-Agnostic-Human-Machine-Dialogues"><a href="#Chatbot-Deployment-Considerations-for-Application-Agnostic-Human-Machine-Dialogues" class="headerlink" title="Chatbot Deployment Considerations for Application-Agnostic Human-Machine   Dialogues"></a>Chatbot Deployment Considerations for Application-Agnostic Human-Machine   Dialogues</h2><p><strong>Authors:Pablo Rivas, Chelsi Chelsi, Nishit Nishit, Laharika Ravula</strong></p>
<p>Automatic conversation systems based on natural language responses are becoming ubiquitous, in part, due to major advances in computational linguistics and machine learning. The easy access to robust and affordable platforms are causing companies to have an unprecedented rush to adopt chatbot technologies for customer service and support. However, this rush has caused judgment lapses when releasing chatbot technologies into production systems. This paper aims to shed light on basic, elemental, considerations that technologists must consider before deploying a chatbot. Our approach takes one particular case to draw lessons for those considering the implementation of chatbots. By looking at this case-study, we aim to call for consideration of societal values as a paramount factor before deploying a chatbot and consider the societal implications of releasing these types of systems. </p>
<blockquote>
<p>åŸºäºè‡ªç„¶è¯­è¨€å›åº”çš„è‡ªåŠ¨å¯¹è¯ç³»ç»Ÿæ­£å˜å¾—æ— å¤„ä¸åœ¨ï¼Œéƒ¨åˆ†åŸå› æ˜¯è®¡ç®—è¯­è¨€å­¦å’Œæœºå™¨å­¦ä¹ æ–¹é¢çš„é‡å¤§è¿›å±•ã€‚å®¹æ˜“è·å¾—ç¨³å¥ä¸”å®æƒ çš„å¹³å°æ­£åœ¨ä½¿å…¬å¸ç«ç›¸é‡‡ç”¨èŠå¤©æœºå™¨äººæŠ€æœ¯è¿›è¡Œå®¢æˆ·æœåŠ¡å’Œæ”¯æŒã€‚ç„¶è€Œï¼Œè¿™ç§ä»“ä¿ƒæ¨åŠ¨å¯¼è‡´åœ¨å°†èŠå¤©æœºå™¨äººæŠ€æœ¯æŠ•æ”¾ç”Ÿäº§ç³»ç»Ÿæ—¶å‡ºç°åˆ¤æ–­å¤±è¯¯ã€‚æœ¬æ–‡æ—¨åœ¨é˜æ˜åœ¨éƒ¨ç½²èŠå¤©æœºå™¨äººä¹‹å‰ï¼ŒæŠ€æœ¯äººå‘˜å¿…é¡»è€ƒè™‘çš„åŸºæœ¬è¦ç´ ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥ä¸€ä¸ªç‰¹å®šæ¡ˆä¾‹ä¸ºä¾‹ï¼Œä¸ºé‚£äº›æ­£åœ¨è€ƒè™‘å®æ–½èŠå¤©æœºå™¨äººçš„äººæä¾›ç»éªŒæ•™è®­ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å‘¼ååœ¨éƒ¨ç½²èŠå¤©æœºå™¨äººä¹‹å‰ï¼Œé¦–å…ˆè€ƒè™‘ç¤¾ä¼šä»·å€¼è§‚æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„å› ç´ ï¼Œå¹¶è€ƒè™‘å‘å¸ƒè¿™äº›ç±»å‹çš„ç³»ç»Ÿæ‰€å¸¦æ¥çš„ç¤¾ä¼šå½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02611v1">PDF</a> The Third Workshop on Reasoning and Learning for Human-Machine   Dialogues at the Thirty-Fourth AAAI Conference on Artificial Intelligence   (AAAI-20)</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨å¯¹è¯ç³»ç»ŸåŸºäºè‡ªç„¶è¯­è¨€å“åº”çš„æŠ€æœ¯æ—¥ç›Šæ™®åŠï¼Œå¾—ç›Šäºè®¡ç®—è¯­è¨€å­¦å’Œæœºå™¨å­¦ä¹ æ–¹é¢çš„é‡å¤§è¿›å±•ã€‚ç¨³å¥ä¸”ç»æµå®æƒ çš„å¹³å°çš„æ™®åŠï¼Œä½¿å¾—ä¼ä¸šçº·çº·é‡‡ç”¨èŠå¤©æœºå™¨äººæŠ€æœ¯è¿›è¡Œå®¢æˆ·æœåŠ¡å’Œæ”¯æŒã€‚ç„¶è€Œï¼Œè¿™ç§åŒ†å¿™å°†èŠå¤©æœºå™¨äººæŠ€æœ¯æŠ•å…¥ç”Ÿäº§ç³»ç»Ÿé€ æˆäº†åˆ¤æ–­å¤±è¯¯ã€‚æœ¬æ–‡æ—¨åœ¨é˜è¿°åœ¨éƒ¨ç½²èŠå¤©æœºå™¨äººä¹‹å‰ï¼ŒæŠ€æœ¯äººå‘˜éœ€è¦è€ƒè™‘çš„åŸºæœ¬è¦ç´ ã€‚é€šè¿‡ä¸ªæ¡ˆç ”ç©¶ï¼Œæˆ‘ä»¬å‘¼ååœ¨éƒ¨ç½²èŠå¤©æœºå™¨äººä¹‹å‰è€ƒè™‘ç¤¾ä¼šä»·å€¼è§‚è¿™ä¸€é‡è¦å› ç´ ï¼Œå¹¶è€ƒè™‘å‘å¸ƒè¿™äº›ç³»ç»Ÿå¯¹ç¤¾ä¼šçš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨å¯¹è¯ç³»ç»ŸåŸºäºè‡ªç„¶è¯­è¨€å“åº”çš„æŠ€æœ¯æ—¥ç›Šæ™®åŠï¼Œå¾—ç›Šäºè®¡ç®—è¯­è¨€å­¦å’Œæœºå™¨å­¦ä¹ çš„è¿›å±•ã€‚</li>
<li>èŠå¤©æœºå™¨äººæŠ€æœ¯è¢«ä¼ä¸šå¹¿æ³›é‡‡ç”¨ï¼Œç”¨äºå®¢æˆ·æœåŠ¡å’Œæ”¯æŒã€‚</li>
<li>åœ¨éƒ¨ç½²èŠå¤©æœºå™¨äººæŠ€æœ¯åˆ°ç”Ÿäº§ç³»ç»Ÿæ—¶ï¼Œå­˜åœ¨åˆ¤æ–­å¤±è¯¯çš„æƒ…å†µã€‚</li>
<li>æŠ€æœ¯äººå‘˜åœ¨éƒ¨ç½²èŠå¤©æœºå™¨äººä¹‹å‰ï¼Œéœ€è¦è€ƒè™‘åŸºæœ¬è¦ç´ ã€‚</li>
<li>ç¤¾ä¼šä»·å€¼æ˜¯ä¸€ä¸ªé‡è¦çš„å› ç´ ï¼Œéœ€è¦åœ¨éƒ¨ç½²èŠå¤©æœºå™¨äººä¹‹å‰è€ƒè™‘ã€‚</li>
<li>å‘å¸ƒè¿™äº›ç³»ç»Ÿå¯¹ç¤¾ä¼šçš„å½±å“éœ€è¦è¢«é‡è§†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-965c38c121586ed63d39e885807f91e8.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-d03d37ac87a4e6552f07c39a63968379~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801940&auth_key=1759801940-0-0-607b8489d26163c2110c6f8ac73bab63&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ddd39025bcddcfccbbbbf656ecf69ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801946&auth_key=1759801946-0-0-5d112c00355a2491fef9416982c7de82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Quantitative-imaging-of-478-keV-prompt-gamma-rays-from-boron-neutron-capture-reactions"><a href="#Quantitative-imaging-of-478-keV-prompt-gamma-rays-from-boron-neutron-capture-reactions" class="headerlink" title="Quantitative imaging of 478-keV prompt gamma rays from boron neutron   capture reactions"></a>Quantitative imaging of 478-keV prompt gamma rays from boron neutron   capture reactions</h2><p><strong>Authors:Tetsuya Mizumoto, Shotaro Komura, Atsushi Takada, Yoshinori Sakurai, Toru Tanimori</strong></p>
<p>The accurate imaging and quantitative measurement of 478-keV prompt gamma rays are critical for advancing boron neutron capture therapy (BNCT), a promising cancer treatment. Although numerical simulations have indicated that such measurements are feasible, their practical application has proven challenging. This study introduces a gamma-ray imaging detector designed specifically for precise BNCT measurements. Using boron-rich phantom samples, we successfully imaged 478-keV gamma rays and established a linear correlation between gamma-ray production and boron concentration. Furthermore, applying this technique in a recognized BNCT treatment facility demonstrated the detectorâ€™s effectiveness in monitoring boron dose distribution during neutron irradiation, both in pretreatment diagnostics and throughout the treatment process. </p>
<blockquote>
<p>å¯¹478-keVå³æ—¶ä¼½é©¬å°„çº¿çš„ç²¾ç¡®æˆåƒå’Œå®šé‡æµ‹é‡å¯¹äºå‘å±•ç¡¼ä¸­å­æ•è·ç–—æ³•ï¼ˆBNCTï¼‰è¿™ä¸€å‰æ™¯å…‰æ˜çš„ç™Œç—‡æ²»ç–—æ–¹æ³•è‡³å…³é‡è¦ã€‚å°½ç®¡æ•°å€¼æ¨¡æ‹Ÿè¡¨æ˜è¿™æ ·çš„æµ‹é‡æ˜¯å¯è¡Œçš„ï¼Œä½†å…¶å®é™…åº”ç”¨å´è¯æ˜æ˜¯æœ‰æŒ‘æˆ˜æ€§çš„ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§ä¸“é—¨è®¾è®¡ç”¨äºç²¾ç¡®BNCTæµ‹é‡çš„ä¼½é©¬å°„çº¿æˆåƒæ¢æµ‹å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨å¯Œå«ç¡¼çš„å¹»å½±æ ·æœ¬æˆåŠŸåœ°å¯¹478-keVä¼½é©¬å°„çº¿è¿›è¡Œäº†æˆåƒï¼Œå¹¶å»ºç«‹äº†ä¼½é©¬å°„çº¿äº§é‡ä¸ç¡¼æµ“åº¦ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚æ­¤å¤–ï¼Œåœ¨å…¬è®¤çš„BNCTæ²»ç–—è®¾æ–½ä¸­åº”ç”¨æ­¤æŠ€æœ¯ï¼Œè¯æ˜äº†è¯¥æ¢æµ‹å™¨åœ¨ç›‘æµ‹ä¸­å­ç…§å°„è¿‡ç¨‹ä¸­çš„ç¡¼å‰‚é‡åˆ†å¸ƒæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæ— è®ºæ˜¯åœ¨æ²»ç–—å‰è¯Šæ–­è¿˜æ˜¯åœ¨æ•´ä¸ªæ²»ç–—è¿‡ç¨‹ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00435v1">PDF</a> 35 pages, 8 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç¡¼ä¸­å­ä¿˜è·ç–—æ³•ï¼ˆBNCTï¼‰ä¸­478-keVå³æ—¶ä¼½é©¬å°„çº¿çš„ç²¾ç¡®æˆåƒå’Œå®šé‡æµ‹é‡çš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶è®¾è®¡äº†ä¸€ç§ä¸“é—¨ç”¨äºç²¾ç¡®BNCTæµ‹é‡çš„ä¼½é©¬å°„çº¿æˆåƒæ¢æµ‹å™¨ï¼Œé€šè¿‡å¯¹å¯Œç¡¼ Phantomæ ·æœ¬çš„æˆåŠŸæˆåƒï¼Œå»ºç«‹äº†ä¼½é©¬å°„çº¿äº§é‡ä¸ç¡¼æµ“åº¦ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚æ­¤å¤–ï¼Œåœ¨å®é™…åº”ç”¨çš„BNCTæ²»ç–—è®¾æ–½ä¸­ä½¿ç”¨è¯¥æ¢æµ‹å™¨ï¼Œæœ‰æ•ˆç›‘æµ‹äº†ä¸­å­ç…§å°„è¿‡ç¨‹ä¸­çš„ç¡¼å‰‚é‡åˆ†å¸ƒï¼Œå¯ç”¨äºæ²»ç–—å‰è¯Šæ–­ä»¥åŠæ•´ä¸ªæ²»ç–—è¿‡ç¨‹çš„ç›‘æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¡¼ä¸­å­ä¿˜è·ç–—æ³•ï¼ˆBNCTï¼‰æ˜¯ä¸€ç§æœ‰å‰é€”çš„ç™Œç—‡æ²»ç–—æ–¹æ³•ï¼Œéœ€è¦å‡†ç¡®æˆåƒå’Œå®šé‡æµ‹é‡478-keVçš„å³æ—¶ä¼½é©¬å°„çº¿æ¥ä¿ƒè¿›å…¶å‘å±•ã€‚</li>
<li>é€šè¿‡æ•°å€¼æ¨¡æ‹Ÿè¯å®äº†æµ‹é‡478-keVä¼½é©¬å°„çº¿çš„å¯è¡Œæ€§ï¼Œä½†å®é™…åº”ç”¨å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç ”ç©¶å¼€å‘äº†ä¸€ç§ä¸“é—¨ç”¨äºBNCTæµ‹é‡çš„ä¼½é©¬å°„çº¿æˆåƒæ¢æµ‹å™¨ã€‚</li>
<li>ä½¿ç”¨å¯Œç¡¼ Phantomæ ·æœ¬æˆåŠŸæˆåƒï¼Œè¯æ˜äº†æ¢æµ‹å™¨çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å»ºç«‹äº†ä¼½é©¬å°„çº¿äº§é‡ä¸ç¡¼æµ“åº¦ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚</li>
<li>æ¢æµ‹å™¨å¯ç”¨äºå®é™…åº”ç”¨çš„BNCTæ²»ç–—è®¾æ–½ä¸­ï¼Œç›‘æµ‹ä¸­å­ç…§å°„è¿‡ç¨‹ä¸­çš„ç¡¼å‰‚é‡åˆ†å¸ƒã€‚</li>
<li>è¯¥æŠ€æœ¯æ—¢å¯ç”¨äºæ²»ç–—å‰è¯Šæ–­ï¼Œä¹Ÿå¯åœ¨æ•´ä¸ªæ²»ç–—è¿‡ç¨‹ä¸­è¿›è¡Œç›‘æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6b6358596a319d66c4e28e4659f28a3a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Schema-Guided-Response-Generation-using-Multi-Frame-Dialogue-State-for-Motivational-Interviewing-Systems"><a href="#Schema-Guided-Response-Generation-using-Multi-Frame-Dialogue-State-for-Motivational-Interviewing-Systems" class="headerlink" title="Schema-Guided Response Generation using Multi-Frame Dialogue State for   Motivational Interviewing Systems"></a>Schema-Guided Response Generation using Multi-Frame Dialogue State for   Motivational Interviewing Systems</h2><p><strong>Authors:Jie Zeng, Yukiko I. Nakano</strong></p>
<p>The primary goal of Motivational Interviewing (MI) is to help clients build their own motivation for behavioral change. To support this in dialogue systems, it is essential to guide large language models (LLMs) to generate counselor responses aligned with MI principles. By employing a schema-guided approach, this study proposes a method for updating multi-frame dialogue states and a strategy decision mechanism that dynamically determines the response focus in a manner grounded in MI principles. The proposed method was implemented in a dialogue system and evaluated through a user study. Results showed that the proposed system successfully generated MI-favorable responses and effectively encouraged the userâ€™s (clientâ€™s) deliberation by asking eliciting questions. </p>
<blockquote>
<p>åŠ¨æœºæ€§è®¿è°ˆï¼ˆMIï¼‰çš„ä¸»è¦ç›®æ ‡æ˜¯å¸®åŠ©å®¢æˆ·å»ºç«‹è‡ªå·±çš„è¡Œä¸ºæ”¹å˜åŠ¨æœºã€‚ä¸ºäº†åœ¨å¯¹è¯ç³»ç»Ÿä¸­æ”¯æŒè¿™ä¸€ç‚¹ï¼Œå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä¸MIåŸåˆ™ä¸€è‡´çš„å’¨è¯¢å¸ˆå›åº”è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é‡‡ç”¨æ¨¡å¼å¼•å¯¼çš„æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§æ›´æ–°å¤šå¸§å¯¹è¯çŠ¶æ€çš„æ–¹æ³•å’Œä¸€ç§ç­–ç•¥å†³ç­–æœºåˆ¶ï¼Œä»¥åŠ¨æ€ç¡®å®šä»¥MIåŸåˆ™ä¸ºåŸºç¡€çš„ååº”é‡ç‚¹ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸€ä¸ªå¯¹è¯ç³»ç»Ÿä¸­å¾—åˆ°äº†å®ç°ï¼Œå¹¶é€šè¿‡ç”¨æˆ·ç ”ç©¶è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»ŸæˆåŠŸç”Ÿæˆäº†æœ‰åˆ©äºMIçš„å“åº”ï¼Œå¹¶é€šè¿‡æå‡ºå¼•å¯¼æ€§é—®é¢˜æœ‰æ•ˆåœ°é¼“åŠ±äº†ç”¨æˆ·ï¼ˆå®¢æˆ·ï¼‰çš„æ€è€ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20635v1">PDF</a> 28pages, 15 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•é€šè¿‡é‡‡ç”¨æ¨¡å¼å¼•å¯¼çš„æ–¹æ³•ï¼Œåœ¨å¯¹è¯ç³»ç»Ÿä¸­åº”ç”¨åŠ¨æœºæ€§è®¿è°ˆï¼ˆMIï¼‰çš„åŸåˆ™ï¼Œä»¥å¸®åŠ©ç”¨æˆ·ï¼ˆå³å®¢æˆ·ï¼‰å»ºç«‹æ”¹å˜è¡Œä¸ºçš„åŠ¨åŠ›ã€‚é€šè¿‡æ›´æ–°å¤šå¸§å¯¹è¯çŠ¶æ€å’Œç­–ç•¥å†³ç­–æœºåˆ¶ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆç¬¦åˆMIåŸåˆ™çš„å›ç­”ï¼Œå¹¶é€šè¿‡æå‡ºé—®é¢˜é¼“åŠ±ç”¨æˆ·æ·±æ€ã€‚ç»è¿‡ç”¨æˆ·ç ”ç©¶è¯„ä¼°ï¼Œè¯¥ç³»ç»ŸæˆåŠŸç”Ÿæˆäº†æœ‰åˆ©äºMIçš„å›åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æœºæ€§è®¿è°ˆï¼ˆMIï¼‰çš„æ ¸å¿ƒç›®æ ‡æ˜¯å¸®åŠ©å®¢æˆ·å»ºç«‹æ”¹å˜è¡Œä¸ºçš„åŠ¨åŠ›ã€‚</li>
<li>åœ¨å¯¹è¯ç³»ç»Ÿä¸­åº”ç”¨MIåŸåˆ™æ—¶ï¼Œéœ€è¦å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆç¬¦åˆMIåŸåˆ™çš„å›ç­”ã€‚</li>
<li>é‡‡ç”¨æ¨¡å¼å¼•å¯¼çš„æ–¹æ³•å¯ä»¥æ›´æ–°å¤šå¸§å¯¹è¯çŠ¶æ€ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿæ›´çµæ´»åœ°ä¸ç”¨æˆ·äº¤æµã€‚</li>
<li>ç­–ç•¥å†³ç­–æœºåˆ¶èƒ½å¤ŸåŠ¨æ€ç¡®å®šå›åº”çš„é‡ç‚¹ï¼Œç¡®ä¿å¯¹è¯ç¬¦åˆMIåŸåˆ™ã€‚</li>
<li>é€šè¿‡ç”¨æˆ·ç ”ç©¶è¯„ä¼°ï¼Œè¯¥ç³»ç»ŸæˆåŠŸç”Ÿæˆäº†æœ‰åˆ©äºMIçš„å›åº”ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡æå‡ºé—®é¢˜é¼“åŠ±ç”¨æˆ·æ·±æ€ï¼Œæœ‰åŠ©äºå¢å¼ºç”¨æˆ·çš„åŠ¨æœºå’Œè‡ªæˆ‘åæ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7306460e5ddcec2ab3930e4046f83572~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801961&auth_key=1759801961-0-0-43f37add955cd1433be6af51fbc414dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bcee6ea1128290848822b5492368532f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801969&auth_key=1759801969-0-0-1b39a6e904c1ef298a2004c90ecca97e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a541555b1b8fbffb484f9dbf6029fee9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801977&auth_key=1759801977-0-0-83f79d9ee1622e4d053a6fb9a7314f31&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OSUM-EChat-Enhancing-End-to-End-Empathetic-Spoken-Chatbot-via-Understanding-Driven-Spoken-Dialogue"><a href="#OSUM-EChat-Enhancing-End-to-End-Empathetic-Spoken-Chatbot-via-Understanding-Driven-Spoken-Dialogue" class="headerlink" title="OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue"></a>OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue</h2><p><strong>Authors:Xuelong Geng, Qijie Shao, Hongfei Xue, Shuiyuan Wang, Hanke Xie, Zhao Guo, Yi Zhao, Guojian Li, Wenjie Tian, Chengyou Wang, Zhixian Zhao, Kangxiang Xia, Ziyu Zhang, Zhennan Lin, Tianlun Zuo, Mingchen Shao, Yuang Cao, Guobin Ma, Longhao Li, Yuhang Dai, Dehui Gao, Dake Guo, Lei Xie</strong></p>
<p>Empathy is crucial in enabling natural interactions within spoken dialogue systems, allowing machines to recognize and respond appropriately to paralinguistic cues such as age, gender, and emotion. Recent advancements in end-to-end speech language models, which unify speech understanding and generation, provide promising solutions. However, several challenges persist, including an over-reliance on large-scale dialogue datasets, insufficient extraction of paralinguistic cues vital for conveying empathy, and the lack of empathy-specific datasets and evaluation frameworks. To address these issues, we introduce OSUM-EChat, an open-source, end-to-end spoken dialogue system designed to enhance empathetic interactions, particularly in resource-limited settings. OSUM-EChat introduces two key innovations: (1) a three-stage understanding-driven spoken dialogue training strategy that extends the capabilities of a large speech understanding model to spoken dialogue tasks, and (2) a linguistic-paralinguistic dual thinking mechanism that integrates paralinguistic understanding through a chain of thought with dialogue generation, enabling the system to produce more empathetic responses. This approach reduces reliance on large-scale dialogue datasets while maintaining high-quality empathetic interactions. Additionally, we introduce the EChat-200K dataset, a rich corpus of empathetic speech-to-speech dialogues, and the EChat-eval benchmark, a comprehensive framework for evaluating the empathetic capabilities of dialogue systems. Experimental results demonstrate that OSUM-EChat outperforms end-to-end spoken dialogue models regarding empathetic responsiveness, validating its effectiveness. </p>
<blockquote>
<p>å…±æƒ…åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿå†…çš„è‡ªç„¶äº¤äº’ä¸­è‡³å…³é‡è¦ï¼Œå®ƒè®©æœºå™¨èƒ½å¤Ÿè¯†åˆ«å’Œé€‚å½“å›åº”è¯¸å¦‚å¹´é¾„ã€æ€§åˆ«å’Œæƒ…ç»ªç­‰å‰¯è¯­è¨€çº¿ç´¢ã€‚ç«¯åˆ°ç«¯çš„è¯­è¨€æ¨¡å‹æœ€è¿‘çš„è¿›æ­¥ï¼Œè¿™äº›æ¨¡å‹ç»Ÿä¸€äº†è¯­éŸ³ç†è§£å’Œç”Ÿæˆï¼Œæä¾›äº†å¾ˆæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿‡åº¦ä¾èµ–å¤§è§„æ¨¡çš„å¯¹è¯æ•°æ®é›†ï¼Œå¯¹ä¼ é€’å…±æƒ…è‡³å…³é‡è¦çš„å‰¯è¯­è¨€çº¿ç´¢æå–ä¸è¶³ï¼Œä»¥åŠç¼ºä¹ä¸“é—¨é’ˆå¯¹å…±æƒ…çš„æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OSUM-EChatï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„ç«¯åˆ°ç«¯å£è¯­å¯¹è¯ç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºå…±æƒ…äº¤äº’ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚OSUM-EChatå¼•å…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯ä»¥ç†è§£ä¸ºä¸»å¯¼çš„ä¸‰é˜¶æ®µå£è¯­å¯¹è¯è®­ç»ƒç­–ç•¥ï¼Œå®ƒæ‰©å±•äº†å¤§å‹è¯­éŸ³ç†è§£æ¨¡å‹åœ¨å£è¯­å¯¹è¯ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼›äºŒæ˜¯è¯­è¨€-å‰¯è¯­è¨€åŒé‡æ€è€ƒæœºåˆ¶ï¼Œå®ƒå°†å‰¯è¯­è¨€ç†è§£èå…¥æ€ç»´é“¾è¿›è¡Œå¯¹è¯ç”Ÿæˆï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿäº§ç”Ÿæ›´å…·å…±æƒ…çš„å›åº”ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„å…±æƒ…äº¤äº’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†EChat-200Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä»½ä¸°å¯Œçš„å…±æƒ…è¯­éŸ³å¯¹è¯è¯­æ–™åº“ï¼Œä»¥åŠEChat-evalåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°å¯¹è¯ç³»ç»Ÿå…±æƒ…èƒ½åŠ›çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSUM-EChatåœ¨å…±æƒ…å“åº”æ–¹é¢ä¼˜äºç«¯åˆ°ç«¯çš„å£è¯­å¯¹è¯æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09600v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…±æƒ…åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­å®ç°è‡ªç„¶äº¤äº’è‡³å…³é‡è¦ï¼Œå®ƒè®©æœºå™¨èƒ½å¤Ÿè¯†åˆ«å’Œå›åº”å¹´é¾„ã€æ€§åˆ«å’Œæƒ…æ„Ÿç­‰å‰¯è¯­è¨€çº¿ç´¢ã€‚å°½ç®¡ç«¯åˆ°ç«¯çš„è‡ªç„¶è¯­è¨€æ¨¡å‹åœ¨ç»Ÿä¸€è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¾èµ–å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†ã€æœªèƒ½å……åˆ†æå–å¯¹è¡¨è¾¾å…±æƒ…è‡³å…³é‡è¦çš„å‰¯è¯­è¨€çº¿ç´¢ä»¥åŠç¼ºä¹é’ˆå¯¹å…±æƒ…çš„ç‰¹å®šæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OSUM-EChatè¿™ä¸€å¼€æºçš„ç«¯åˆ°ç«¯å£è¯­å¯¹è¯ç³»ç»Ÿï¼Œæ—¨åœ¨åŠ å¼ºå…±æƒ…äº¤äº’ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚OSUM-EChatå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯ä»¥ç†è§£ä¸ºæ ¸å¿ƒçš„ä¸‰é˜¶æ®µå£è¯­å¯¹è¯è®­ç»ƒç­–ç•¥ï¼Œæ‰©å±•äº†å¤§è§„æ¨¡è¯­éŸ³ç†è§£æ¨¡å‹åœ¨å£è¯­å¯¹è¯ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼›äºŒæ˜¯è¯­è¨€-å‰¯è¯­è¨€åŒé‡æ€è€ƒæœºåˆ¶ï¼Œé€šè¿‡ä¸€ç³»åˆ—æ€è€ƒå°†å‰¯è¯­è¨€ç†è§£ä¸å¯¹è¯ç”Ÿæˆç›¸ç»“åˆï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿäº§ç”Ÿæ›´å…±æƒ…çš„å›åº”ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„å…±æƒ…äº¤äº’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†EChat-200Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä»½ä¸°å¯Œçš„å…±æƒ…æ€§è¯­éŸ³å¯¹è¯è¯­æ–™åº“ï¼Œä»¥åŠEChat-evalåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°å¯¹è¯ç³»ç»Ÿå…±æƒ…èƒ½åŠ›çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSUM-EChatåœ¨å…±æƒ…å“åº”æ–¹é¢ä¼˜äºç«¯åˆ°ç«¯çš„å£è¯­å¯¹è¯æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…±æƒ…åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­å¯¹äºå®ç°è‡ªç„¶äº¤äº’è‡³å…³é‡è¦ï¼Œå…è®¸æœºå™¨è¯†åˆ«å¹¶é€‚å½“å›åº”å‰¯è¯­è¨€çº¿ç´¢ã€‚</li>
<li>ç«¯åˆ°ç«¯çš„è‡ªç„¶è¯­è¨€æ¨¡å‹åœ¨ç»Ÿä¸€è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—è¿›å±•ï¼Œä½†ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>OSUM-EChatæ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºå…±æƒ…äº¤äº’çš„å¼€æºç«¯åˆ°ç«¯å£è¯­å¯¹è¯ç³»ç»Ÿã€‚</li>
<li>OSUM-EChatå¼•å…¥äº†ä¸‰é˜¶æ®µå£è¯­å¯¹è¯è®­ç»ƒç­–ç•¥å’Œè¯­è¨€-å‰¯è¯­è¨€åŒé‡æ€è€ƒæœºåˆ¶ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡å‡å°‘å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†çš„ä¾èµ–ï¼Œç»´æŒäº†é«˜è´¨é‡çš„å…±æƒ…äº¤äº’ã€‚</li>
<li>æ¨å‡ºäº†EChat-200Kæ•°æ®é›†ï¼ŒåŒ…å«ä¸°å¯Œçš„å…±æƒ…æ€§è¯­éŸ³å¯¹è¯å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5d386e2fc84fcc3e17c4d8e633ad8089~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801985&auth_key=1759801985-0-0-418ef720c205ee0a9623a9219b19d94e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-74141b369f2361d8250ad455babb0565.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8736de6347a296ecdc6e30496e4b354~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801998&auth_key=1759801998-0-0-c5f64744038082a8f0e9cad6868f7315&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4bd9a04fbafc7b2d741a1d510deeaf94~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802005&auth_key=1759802005-0-0-0f5a38a6910bf8729f2ddbd3d4be114c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1532fe434229d1e26038d8d42cfc6836~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802012&auth_key=1759802012-0-0-735ff7e7afb1ef87179570bea8dced54&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-28c3b03132ebd4dd671c6c43f74be212.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges"><a href="#Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges" class="headerlink" title="Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges"></a>Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges</h2><p><strong>Authors:Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding</strong></p>
<p>Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the â€œLLM-as-a-judgeâ€ paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness. </p>
<blockquote>
<p>è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹è¯èƒ½åŠ›ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å½“å‰çš„ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–äºâ€œLLMä½œä¸ºè¯„åˆ¤å‘˜â€çš„æ¨¡å¼ï¼Œå³æç¤ºLLMå……å½“è¯„ä¼°å‘˜ï¼Œä»¥è¯„ä¼°å¯¹è¯è´¨é‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸ä¼šå—åˆ°å„ç§åè§çš„å½±å“ï¼Œä»è€Œç ´åäº†è¯„ä¼°ç»“æœçš„å¯é æ€§å’Œä¸€è‡´æ€§ã€‚ä¸ºäº†å‡å°‘è¿™äº›åè§ï¼Œæœ€è¿‘çš„æ–¹æ³•é‡‡ç”¨å¤šä¸ªLLMä½œä¸ºè¯„åˆ¤å‘˜ï¼Œå¹¶å°†ä»–ä»¬çš„åˆ¤æ–­æ±‡æ€»èµ·æ¥ä»¥é€‰æ‹©æœ€ä½³è¯„ä¼°ç»“æœã€‚å°½ç®¡è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­å´äº§ç”Ÿäº†å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤šè½®å¯¹è¯è¯„ä¼°å™¨ï¼Œå®ƒé€šè¿‡å°†ä¸€ä¸ªæ¨¡å‹æ±‡æ€»å¤šä¸ªLLMè¯„åˆ¤å‘˜çš„åå¥½çŸ¥è¯†æ¥æ•æ‰é›†ä½“æ™ºæ…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™äº†å¤šç§å¤šè¯„åˆ¤å‘˜åé¦ˆçš„ä¼˜åŠ¿ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è¯„ä¼°æˆæœ¬ï¼Œå®ç°äº†å¿«é€Ÿçµæ´»çš„å¯¹è¯è´¨é‡è¯„ä¼°ã€‚åœ¨ä¸ƒä¸ªå•ä¸€è¯„åˆ†å’Œé…å¯¹æ¯”è¾ƒå¯¹è¯è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸‹éƒ½ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç°äº†å…¶é«˜æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00454v2">PDF</a> 15 pages, 2 pages, under review</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›è¯„ä¼°ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å½“å‰ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–äºâ€œLLMä½œä¸ºè¯„åˆ¤è€…â€çš„æ¨¡å¼ï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨åè§é—®é¢˜ï¼Œå½±å“è¯„ä¼°ç»“æœçš„å¯é æ€§å’Œä¸€è‡´æ€§ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œè¿‘æœŸæ–¹æ³•é‡‡ç”¨å¤šä¸ªLLMä½œä¸ºè¯„åˆ¤è€…å¹¶æ±‡èšåˆ¤æ–­ä»¥é€‰æ‹©æœ€ä½³è¯„ä¼°ã€‚å°½ç®¡æœ‰æ•ˆï¼Œä½†å¤šè¯„åˆ¤è€…æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šäº§ç”Ÿå·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚æœ¬æ–‡æå‡ºä¸€ç§é«˜æ•ˆçš„å¤šè½®å¯¹è¯è¯„ä¼°å™¨ï¼Œé€šè¿‡æ±‡èšå¤šä¸ªLLMè¯„åˆ¤è€…çš„åå¥½çŸ¥è¯†åˆ°ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ï¼Œæ•æ‰é›†ä½“æ™ºæ…§ã€‚è¯¥æ–¹æ³•åœ¨ä¿ç•™å¤šæ ·åŒ–å¤šè¯„åˆ¤è€…åé¦ˆä¼˜åŠ¿çš„åŒæ—¶ï¼Œå¤§å¹…é™ä½äº†è¯„ä¼°æˆæœ¬ï¼Œå®ç°äº†å¿«é€Ÿçµæ´»çš„å¯¹è¯è´¨é‡è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è¯èƒ½åŠ›è¯„ä¼°å¯¹äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>å½“å‰ä¸»æµè¯„ä¼°æ–¹æ³•ä¾èµ–äºLLMä½œä¸ºè¯„åˆ¤è€…ï¼Œä½†å­˜åœ¨åè§é—®é¢˜ã€‚</li>
<li>å¤šè¯„åˆ¤è€…æ–¹æ³•èƒ½æœ‰æ•ˆç¼“è§£åè§ï¼Œä½†è®¡ç®—å¼€é”€å¤§ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹å¯¹è¯è¯„ä¼°å™¨ï¼Œèåˆå¤šä¸ªLLMçš„åˆ¤æ–­ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†è¯„ä¼°æ•ˆç‡å¹¶é™ä½äº†æˆæœ¬ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šç§å¯¹è¯è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00454">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-03ebd54930b9b519f32e0d7fe7cada0a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802027&auth_key=1759802027-0-0-1cd4f07e058f5083775defcecf84eaea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21e3d2da2b2991cb17167fc9a02a728e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802035&auth_key=1759802035-0-0-c6265a194e1720d007d6af0288c38a0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03ad3cd1b591002d79d265a5df7b056d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802042&auth_key=1759802042-0-0-2a34b8e824346edcc804dd41c18dabff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-763c5040c1dc72f7a3bc76762bd98530.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5a6292e608e6f1c68743c9802b9c5d5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Binaural-Target-Speaker-Extraction-using-HRTFs"><a href="#Binaural-Target-Speaker-Extraction-using-HRTFs" class="headerlink" title="Binaural Target Speaker Extraction using HRTFs"></a>Binaural Target Speaker Extraction using HRTFs</h2><p><strong>Authors:Yoav Ellinson, Sharon Gannot</strong></p>
<p>In this work, we aim to imitate the human ability to selectively attend to a single speaker, even in the presence of multiple simultaneous talkers. To achieve this, we propose a novel approach for binaural target speaker extraction that leverages the listenerâ€™s Head-Related Transfer Function (HRTF) to isolate the desired speaker. Notably, our method does not rely on speaker embeddings, making it speaker-independent and enabling strong generalization across multiple speech datasets and languages. We employ a fully complex-valued neural network that operates directly on the complex-valued Short-Time Fourier transform (STFT) of the mixed audio signals, and compare it to a Real-Imaginary (RI)-based neural network, demonstrating the advantages of the former. We first evaluate the method in an anechoic, noise-free scenario, achieving excellent extraction performance while preserving the binaural cues of the target signal. We then extend the evaluation to reverberant conditions. Our method proves robust, maintaining speech clarity and source directionality while simultaneously reducing reverberation. A comparative analysis with existing binaural Target Speaker Extraction (TSE) methods demonstrates that our approach attains performance on par with competing techniques in terms of noise reduction and perceptual quality, while offering a clear advantage in preserving binaural cues.Demo-page: <a target="_blank" rel="noopener" href="https://bi-ctse-hrtf.github.io/">https://bi-ctse-hrtf.github.io</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨¡ä»¿äººç±»æœ‰é€‰æ‹©åœ°å…³æ³¨å•ä¸€è¯´è¯è€…çš„èƒ½åŠ›ï¼Œå³ä½¿åœ¨æœ‰å¤šä¸ªåŒæ—¶è¯´è¯çš„æƒ…å¢ƒä¸­ä¹Ÿèƒ½å¦‚æ­¤ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åˆ©ç”¨åŒè€³ç›®æ ‡è¯´è¯è€…æå–çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¬ä¼—çš„å¤´éƒ¨ç›¸å…³ä¼ è¾“å‡½æ•°ï¼ˆHRTFï¼‰æ¥åˆ†ç¦»ç›®æ ‡è¯´è¯è€…çš„å£°éŸ³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–äºè¯´è¯è€…åµŒå…¥ï¼Œä½¿å…¶æˆä¸ºç‹¬ç«‹äºè¯´è¯è€…çš„æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªè¯­éŸ³æ•°æ®é›†å’Œè¯­è¨€ä¹‹é—´å®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªå…¨å¤æ•°ç¥ç»ç½‘ç»œï¼Œç›´æ¥åœ¨æ··åˆéŸ³é¢‘ä¿¡å·çš„å¤æ•°çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰ä¸Šæ“ä½œï¼Œå¹¶å°†å…¶ä¸åŸºäºå®è™šï¼ˆRIï¼‰çš„ç¥ç»ç½‘ç»œè¿›è¡Œæ¯”è¾ƒï¼Œå±•ç¤ºäº†å‰è€…çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨æ— æ··å“ã€æ— å™ªå£°çš„åœºæ™¯ä¸‹è¯„ä¼°è¯¥æ–¹æ³•ï¼Œåœ¨ä¿æŒç›®æ ‡ä¿¡å·çš„åŒè€³çº¿ç´¢çš„åŒæ—¶å®ç°äº†å‡ºè‰²çš„æå–æ€§èƒ½ã€‚ç„¶åæˆ‘ä»¬å°†è¯„ä¼°æ‰©å±•åˆ°æ··å“æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¯æ˜æ˜¯ç¨³å¥çš„ï¼Œåœ¨ä¿æŒè¯­éŸ³æ¸…æ™°åº¦å’Œæºæ–¹å‘æ€§çš„åŒæ—¶å‡å°‘äº†æ··å“ã€‚ä¸ç°æœ‰çš„åŒè€³ç›®æ ‡è¯´è¯è€…æå–ï¼ˆTSEï¼‰æ–¹æ³•çš„æ¯”è¾ƒåˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é™å™ªå’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢çš„æ€§èƒ½ä¸ç«äº‰æŠ€æœ¯ç›¸å½“ï¼ŒåŒæ—¶åœ¨ä¿ç•™åŒè€³çº¿ç´¢æ–¹é¢å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚æ¼”ç¤ºé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://bi-ctse-hrtf.github.io/">https://bi-ctse-hrtf.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19369v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨äººç±»å¬è§‰ç³»ç»Ÿé€‰æ‹©æ€§æ³¨æ„å•ä¸€è¯´è¯è€…çš„èƒ½åŠ›ï¼Œå®ç°åŒè€³ç›®æ ‡è¯´è¯è€…æå–çš„æ–°æ–¹æ³•ã€‚åˆ©ç”¨å¬è€…å¤´éƒ¨ç›¸å…³ä¼ è¾“å‡½æ•°ï¼ˆHRTFï¼‰éš”ç¦»ç›®æ ‡è¯´è¯è€…çš„å£°éŸ³ã€‚æ­¤æ–¹æ³•æ— éœ€ä¾èµ–è¯´è¯è€…åµŒå…¥ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯è·¨è¶Šå¤šä¸ªè¯­éŸ³æ•°æ®é›†å’Œè¯­è¨€ä½¿ç”¨ã€‚ç ”ç©¶é‡‡ç”¨äº†å®Œå…¨å¤æ•°ç¥ç»ç½‘ç»œç›´æ¥å¤„ç†æ··åˆéŸ³é¢‘ä¿¡å·çš„å¤æ•°çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰ï¼Œå¹¶ä¸åŸºäºå®è™šæ•°ï¼ˆRIï¼‰çš„ç¥ç»ç½‘ç»œè¿›è¡Œäº†æ¯”è¾ƒï¼Œæ˜¾ç¤ºäº†å‰è€…çš„ä¼˜åŠ¿ã€‚åœ¨æ— å£°å­¦å›å£°å’Œå™ªå£°çš„æƒ…å¢ƒä¸­è¯„ä¼°æ­¤æ–¹æ³•ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æå–æ€§èƒ½ä¸”ä¿ç•™äº†ç›®æ ‡ä¿¡å·çš„åŒè€³çº¿ç´¢ã€‚æ­¤å¤–ï¼Œåœ¨å…·æœ‰å›å£°çš„æ¡ä»¶ä¸‹è¿›è¡Œè¯„ä¼°æ—¶ï¼Œè¯¥æ–¹æ³•ä¿æŒäº†è‰¯å¥½çš„è¡¨ç°ï¼Œè¯æ˜å…¶åœ¨ç»´æŒè¯­éŸ³æ¸…æ™°åº¦å’Œæ¥æºæ–¹å‘æ€§çš„åŒæ—¶å‡å°‘å›å£°çš„èƒ½åŠ›ã€‚ä¸ç°æœ‰çš„åŒè€³ç›®æ ‡è¯´è¯è€…æå–æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨é™å™ªå’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢è¡¨ç°ç›¸å½“ï¼Œä½†åœ¨ä¿ç•™åŒè€³çº¿ç´¢æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ¨¡ä»¿äººç±»é€‰æ‹©æ€§æ³¨æ„å•ä¸€è¯´è¯è€…çš„èƒ½åŠ›è¿›è¡ŒåŒè€³ç›®æ ‡è¯´è¯è€…æå–çš„æ–°æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨å¬è€…å¤´éƒ¨ç›¸å…³ä¼ è¾“å‡½æ•°ï¼ˆHRTFï¼‰æ¥éš”ç¦»ç›®æ ‡è¯´è¯è€…çš„å£°éŸ³ï¼Œæ— éœ€ä¾èµ–è¯´è¯è€…åµŒå…¥ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨å®Œå…¨å¤æ•°ç¥ç»ç½‘ç»œå¤„ç†æ··åˆéŸ³é¢‘ä¿¡å·çš„å¤æ•°çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰ï¼Œæ˜¾ç¤ºå‡ºè¾ƒé«˜çš„æ€§èƒ½ã€‚</li>
<li>åœ¨æ— å£°å­¦å›å£°å’Œå™ªå£°çš„ç¯å¢ƒä¸­è¯„ä¼°æ­¤æ–¹æ³•æ—¶ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„æå–æ€§èƒ½å¹¶ä¿ç•™äº†ç›®æ ‡ä¿¡å·çš„åŒè€³çº¿ç´¢ã€‚</li>
<li>åœ¨å…·æœ‰å›å£°çš„æ¡ä»¶ä¸‹è¯„ä¼°æ—¶ï¼Œè¯¥æ–¹æ³•èƒ½ç»´æŒè¯­éŸ³æ¸…æ™°åº¦å’Œæ¥æºæ–¹å‘æ€§ï¼ŒåŒæ—¶å‡å°‘å›å£°ã€‚</li>
<li>ä¸ç°æœ‰åŒè€³ç›®æ ‡è¯´è¯è€…æå–æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨é™å™ªå’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢è¡¨ç°ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3a046874d9935e8f5a4986c76c946292~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802064&auth_key=1759802064-0-0-090dcb4d0b4dc511418f2fe88864d889&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b7eba0f601648533514d9e25af4d973~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802071&auth_key=1759802071-0-0-08497e412dea626b59ac4920520a4b76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-0322f65685bb6548e8005bda809d1840.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-d1cb3130dc8e0118f1f51b8900d31682~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802084&auth_key=1759802084-0-0-959f9b54753a93f008803ef2a8453c9f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SPIN-ODE-Stiff-Physics-Informed-Neural-ODE-for-Chemical-Reaction-Rate-Estimation"><a href="#SPIN-ODE-Stiff-Physics-Informed-Neural-ODE-for-Chemical-Reaction-Rate-Estimation" class="headerlink" title="SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate   Estimation"></a>SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate   Estimation</h2><p><strong>Authors:Wenqing Peng, Zhi-Song Liu, Michael Boy</strong></p>
<p>Estimating rate coefficients from complex chemical reactions is essential for advancing detailed chemistry. However, the stiffness inherent in real-world atmospheric chemistry systems poses severe challenges, leading to training instability and poor convergence, which hinder effective rate coefficient estimation using learning-based approaches. To address this, we propose a Stiff Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction modelling. Our method introduces a three-stage optimisation process: first, a black-box neural ODE is trained to fit concentration trajectories; second, a Chemical Reaction Neural Network (CRNN) is pre-trained to learn the mapping between concentrations and their time derivatives; and third, the rate coefficients are fine-tuned by integrating with the pre-trained CRNN. Extensive experiments on both synthetic and newly proposed real-world datasets validate the effectiveness and robustness of our approach. As the first work addressing stiff neural ODE for chemical rate coefficient discovery, our study opens promising directions for integrating neural networks with detailed chemistry. </p>
<blockquote>
<p>ä»å¤æ‚çš„åŒ–å­¦ååº”ä¸­ä¼°è®¡é€Ÿç‡ç³»æ•°å¯¹äºæ¨è¿›è¯¦ç»†çš„åŒ–å­¦ç ”ç©¶è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒçœŸå®ä¸–ç•Œå¤§æ°”åŒ–å­¦ç³»ç»Ÿå›ºæœ‰çš„åˆšæ€§é—®é¢˜å¸¦æ¥äº†ä¸¥é‡çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ”¶æ•›æ€§å·®ï¼Œé˜»ç¢äº†åŸºäºå­¦ä¹ çš„é€Ÿç‡ç³»æ•°ä¼°è®¡çš„æœ‰æ•ˆè¿›è¡Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºåŒ–å­¦ååº”å»ºæ¨¡çš„åˆšç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆSPIN-ODEï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„ä¼˜åŒ–è¿‡ç¨‹ï¼šé¦–å…ˆï¼Œè®­ç»ƒä¸€ä¸ªé»‘ç®±ç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹æ¥æ‹Ÿåˆæµ“åº¦è½¨è¿¹ï¼›å…¶æ¬¡ï¼Œé¢„è®­ç»ƒä¸€ä¸ªåŒ–å­¦ååº”ç¥ç»ç½‘ç»œï¼ˆCRNNï¼‰æ¥å­¦ä¹ æµ“åº¦åŠå…¶æ—¶é—´å¯¼æ•°ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼›æœ€åï¼Œé€šè¿‡ä¸é¢„è®­ç»ƒçš„CRNNé›†æˆæ¥å¾®è°ƒé€Ÿç‡ç³»æ•°ã€‚åœ¨åˆæˆæ•°æ®å’Œæ–°æå‡ºçš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚ä½œä¸ºè§£å†³é’ˆå¯¹åŒ–å­¦é€Ÿç‡ç³»æ•°å‘ç°çš„åˆšæ€§ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹çš„ç¬¬ä¸€é¡¹å·¥ä½œï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸ºç¥ç»ç½‘ç»œä¸è¯¦ç»†åŒ–å­¦çš„èåˆå¼€è¾Ÿäº†å……æ»¡å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05625v3">PDF</a> Accepted at the European Conference on Artificial Intelligence (ECAI)   2025</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆNeural ODEï¼‰ç»“åˆåŒ–å­¦åŠ¨åŠ›å­¦å»ºæ¨¡çš„æ–¹æ³•è¢«æå‡ºï¼Œç”¨äºè§£å†³çœŸå®å¤§æ°”åŒ–å­¦ç³»ç»Ÿåˆšåº¦å¸¦æ¥çš„è®­ç»ƒä¸ç¨³å®šå’Œæ”¶æ•›æ€§å·®çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šè®­ç»ƒç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹ä»¥æ‹Ÿåˆæµ“åº¦è½¨è¿¹ï¼Œé¢„è®­ç»ƒåŒ–å­¦ååº”ç¥ç»ç½‘ç»œï¼ˆCRNNï¼‰å­¦ä¹ æµ“åº¦åŠå…¶æ—¶é—´å¯¼æ•°çš„æ˜ å°„å…³ç³»ï¼Œä»¥åŠé€šè¿‡æ•´åˆé¢„è®­ç»ƒçš„CRNNå¯¹é€Ÿç‡ç³»æ•°è¿›è¡Œå¾®è°ƒã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨åˆæˆå’Œæ–°æå‡ºçœŸå®æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œä¸ºç¥ç»ç½‘ç»œä¸è¯¦ç»†åŒ–å­¦çš„ç»“åˆæ‰“å¼€äº†æ–°çš„ç ”ç©¶è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœŸå®å¤§æ°”åŒ–å­¦ç³»ç»Ÿçš„åˆšåº¦ç»™åŸºäºå­¦ä¹ çš„é€Ÿç‡ç³»æ•°ä¼°è®¡å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šå’Œæ”¶æ•›æ€§å·®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºSPIN-ODEçš„åˆšç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹æ¡†æ¶ï¼Œç”¨äºè§£å†³åŒ–å­¦ååº”å»ºæ¨¡ä¸­çš„é—®é¢˜ã€‚</li>
<li>SPIN-ODEåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µçš„ä¼˜åŒ–è¿‡ç¨‹ï¼šè®­ç»ƒç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹ã€é¢„è®­ç»ƒåŒ–å­¦ååº”ç¥ç»ç½‘ç»œï¼Œä»¥åŠå¾®è°ƒé€Ÿç‡ç³»æ•°ã€‚</li>
<li>æ–¹æ³•é€šè¿‡æ•´åˆé¢„è®­ç»ƒçš„CRNNå¯¹é€Ÿç‡ç³»æ•°è¿›è¡Œå¾®è°ƒï¼Œå®ç°æ›´æœ‰æ•ˆçš„å­¦ä¹ ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>ä½œä¸ºé¦–æ¬¡è§£å†³åŒ–å­¦é€Ÿç‡ç³»æ•°å‘ç°çš„åˆšç¥ç»ODEé—®é¢˜çš„ç ”ç©¶ï¼Œè¯¥ç ”ç©¶å…·æœ‰å¼€åˆ›æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1cfaf82f0ecd39838e1c9c2857e501b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802092&auth_key=1759802092-0-0-45686fdc9223616c1c627ab4f88ce4e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bd604bb454af9b738ce38aaa3f021afd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802099&auth_key=1759802099-0-0-05749ca17cbe03165891145d6f6340b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9590e894aa48458e54eeb5106b2d0c9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802106&auth_key=1759802106-0-0-bb088ce2401ad67373ac7743b3fdb109&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40c591f8f8f837c29ed278cabef5e8f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802113&auth_key=1759802113-0-0-99c30887d865feeb8d4c3b6bce20dbcd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1cf799577c3a0511aef80651f1849896~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802119&auth_key=1759802119-0-0-7d42721d97b2032761c5dcc63c892888&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Dial-In-LLM-Human-Aligned-LLM-in-the-loop-Intent-Clustering-for-Customer-Service-Dialogues"><a href="#Dial-In-LLM-Human-Aligned-LLM-in-the-loop-Intent-Clustering-for-Customer-Service-Dialogues" class="headerlink" title="Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for   Customer Service Dialogues"></a>Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for   Customer Service Dialogues</h2><p><strong>Authors:Mengze Hong, Wailing Ng, Chen Jason Zhang, Yuanfeng Song, Di Jiang</strong></p>
<p>Discovering customer intentions is crucial for automated service agents, yet existing intent clustering methods often fall short due to their reliance on embedding distance metrics and neglect of underlying semantic structures. To address these limitations, we propose an LLM-in-the-loop (LLM-ITL) intent clustering framework, integrating the language understanding capabilities of LLMs into conventional clustering algorithms. Specifically, this paper (1) examines the effectiveness of fine-tuned LLMs in semantic coherence evaluation and intent cluster naming, achieving over 95% accuracy aligned with human judgments; (2) designs an LLM-ITL framework that facilitates the iterative discovery of coherent intent clusters and the optimal number of clusters; and (3) introduces context-aware techniques tailored for customer service dialogue. Since existing English benchmarks lack sufficient semantic diversity and intent coverage, we further present a comprehensive Chinese dialogue intent dataset comprising over 100k real customer service calls with 1,507 human-annotated clusters. The proposed approaches significantly outperform LLM-guided baselines, achieving notable improvements in clustering quality, cost efficiency, and downstream applications. Combined with several best practices, our findings highlight the prominence of LLM-in-the-loop techniques for scalable dialogue data mining. </p>
<blockquote>
<p>å‘ç°å®¢æˆ·æ„å›¾å¯¹äºè‡ªåŠ¨æœåŠ¡ä»£ç†è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„æ„å›¾èšç±»æ–¹æ³•å¾€å¾€å› ä¾èµ–åµŒå…¥è·ç¦»åº¦é‡è€Œå¿½è§†æ½œåœ¨è¯­ä¹‰ç»“æ„è€Œè¡¨ç°ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§LLM-in-the-loopï¼ˆLLM-ITLï¼‰æ„å›¾èšç±»æ¡†æ¶ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­è¨€ç†è§£èƒ½åŠ›é›†æˆåˆ°ä¼ ç»Ÿçš„èšç±»ç®—æ³•ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡ï¼ˆ1ï¼‰ç ”ç©¶äº†å¾®è°ƒåçš„LLMsåœ¨è¯­ä¹‰è¿è´¯æ€§è¯„ä¼°å’Œæ„å›¾èšç±»å‘½åä¸­çš„æœ‰æ•ˆæ€§ï¼Œå…¶å‡†ç¡®ç‡ä¸äººç±»åˆ¤æ–­å¯¹é½è¶…è¿‡95%ï¼›ï¼ˆ2ï¼‰è®¾è®¡äº†ä¸€ä¸ªLLM-ITLæ¡†æ¶ï¼Œä¾¿äºå‘ç°è¿è´¯çš„æ„å›¾ç°‡å’Œæœ€ä¼˜ç°‡æ•°ï¼›ï¼ˆ3ï¼‰å¼•å…¥äº†é¢å‘å®¢æˆ·æœåŠ¡å¯¹è¯çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŠ€æœ¯ã€‚ç”±äºç°æœ‰çš„è‹±è¯­åŸºå‡†æµ‹è¯•ç¼ºä¹è¶³å¤Ÿçš„è¯­ä¹‰å¤šæ ·æ€§å’Œæ„å›¾è¦†ç›–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æä¾›äº†ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡å¯¹è¯æ„å›¾æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10ä¸‡æ¡çœŸå®çš„å®¢æˆ·æœåŠ¡å‘¼å«è®°å½•ï¼ŒåŒ…å«1507ä¸ªäººå·¥æ ‡æ³¨çš„ç°‡ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºLLMå¼•å¯¼çš„åŸºç¡€çº¿æ–¹æ³•ï¼Œåœ¨èšç±»è´¨é‡ã€æˆæœ¬æ•ˆç‡å’Œä¸‹æ¸¸åº”ç”¨æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç»“åˆè‹¥å¹²æœ€ä½³å®è·µï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‡¸æ˜¾äº†LLM-in-the-loopæŠ€æœ¯åœ¨å¯æ‰©å±•å¯¹è¯æ•°æ®æŒ–æ˜ä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09049v4">PDF</a> Accepted by EMNLP 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°æœ‰çš„æ„å›¾èšç±»æ–¹æ³•åœ¨è¯­ä¹‰ç†è§£ä¸Šçš„ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ„å›¾èšç±»æ¡†æ¶â€”â€”LLM-in-the-loopï¼ˆLLM-ITLï¼‰ã€‚è¯¥æ¡†æ¶ä¸ä»…åˆ©ç”¨LLMçš„è¯­è¨€ç†è§£èƒ½åŠ›æ¥æå‡ä¼ ç»Ÿèšç±»ç®—æ³•çš„æ•ˆæœï¼Œè¿˜é’ˆå¯¹å®¢æˆ·æœåŠ¡å¯¹è¯å¼•å…¥äº†è¯­å¢ƒæ„ŸçŸ¥æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¼¥è¡¥ç°æœ‰è‹±è¯­åŸºå‡†æµ‹è¯•åœ¨è¯­ä¹‰å¤šæ ·æ€§å’Œæ„å›¾è¦†ç›–ä¸Šçš„ä¸è¶³ï¼Œæœ¬ç ”ç©¶è¿˜æ¨å‡ºäº†ä¸€ä»½åŒ…å«10ä¸‡æ¡ä»¥ä¸ŠçœŸå®å®¢æˆ·æœåŠ¡é€šè¯å’Œ1507ä¸ªäººå·¥æ ‡æ³¨èšç±»çš„ä¸­æ–‡å¯¹è¯æ„å›¾æ•°æ®é›†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLM-ITLæ–¹æ³•åœ¨èšç±»è´¨é‡ã€æˆæœ¬æ•ˆç›Šå’Œä¸‹æ¸¸åº”ç”¨æ–¹é¢å‡æ˜¾è‘—ä¼˜äºLLMå¼•å¯¼çš„åŸºç¡€æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-ITLæ¡†æ¶ç»“åˆäº†LLMçš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œæå‡äº†ä¼ ç»Ÿæ„å›¾èšç±»ç®—æ³•çš„æ•ˆæœã€‚</li>
<li>LLM-ITLæ¡†æ¶èƒ½å¤Ÿè¿­ä»£å‘ç°è¿è´¯çš„æ„å›¾ç°‡å’Œæœ€ä½³ç°‡æ•°é‡ã€‚</li>
<li>é’ˆå¯¹å®¢æˆ·æœåŠ¡å¯¹è¯ï¼Œå¼•å…¥äº†è¯­å¢ƒæ„ŸçŸ¥æŠ€æœ¯ã€‚</li>
<li>ç°æœ‰çš„è‹±è¯­åŸºå‡†æµ‹è¯•åœ¨è¯­ä¹‰å¤šæ ·æ€§å’Œæ„å›¾è¦†ç›–æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æ¨å‡ºäº†ä¸€ä»½åŒ…å«è¶…è¿‡10ä¸‡æ¡çœŸå®å®¢æˆ·æœåŠ¡é€šè¯çš„ä¸­æ–‡å¯¹è¯æ„å›¾æ•°æ®é›†ã€‚</li>
<li>LLM-ITLæ–¹æ³•åœ¨èšç±»è´¨é‡ã€æˆæœ¬æ•ˆç›Šå’Œä¸‹æ¸¸åº”ç”¨æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9473ceb830ce303ec933b09f315152c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802127&auth_key=1759802127-0-0-9efdc1eb5a1bd1a67738dca9cc67db21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6ead2cb5ad080503198b58dc554a006~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802134&auth_key=1759802134-0-0-87988df112885b77a1f29bbb2ca64cdc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3ae1054a115fd0cdf3597b0d8caa4484~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802141&auth_key=1759802141-0-0-24443e54cbc6910d242f576cf2b020f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-c942d7d1283ed11034322e6f6351e0a4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="From-Intents-to-Conversations-Generating-Intent-Driven-Dialogues-with-Contrastive-Learning-for-Multi-Turn-Classification"><a href="#From-Intents-to-Conversations-Generating-Intent-Driven-Dialogues-with-Contrastive-Learning-for-Multi-Turn-Classification" class="headerlink" title="From Intents to Conversations: Generating Intent-Driven Dialogues with   Contrastive Learning for Multi-Turn Classification"></a>From Intents to Conversations: Generating Intent-Driven Dialogues with   Contrastive Learning for Multi-Turn Classification</h2><p><strong>Authors:Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim</strong></p>
<p>In conversational AI systems, a critical challenge in training effective multi-turn intent classification models lies in the generation of large-scale, domain-specific, multilingual dialogue datasets. In this paper, we introduce Chain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs) with Large Language Models (LLMs) to generate intent-driven, context-aware dialogues through self-play. Our method first extracts domain-specific intent transition patterns from real-world e-commerce chat logs, which guide the modeling of turn-level dynamics and intent sequences. LLMs are then employed to parameterize the emission probabilities of HMMs, enabling the generation of natural, coherent utterances aligned with predicted intents and dialogue context. We also propose MINT-CL, a multi-task contrastive learning framework for multi-turn intent classification, which improves performance while reducing dependence on large-scale annotated datasets. Empirical results demonstrate that our approach outperforms competitive baselines in dialogue generation quality and classification accuracy, particularly in multilingual settings. To facilitate future research, we release MINT-E, a comprehensive, multilingual, intent-aware multi-turn dialogue corpus derived from the e-commerce domain\footnote{The reproduced source code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/junhua/chain-of-intent">https://github.com/junhua/chain-of-intent</a>. </p>
<blockquote>
<p>åœ¨å¯¹è¯å¼AIç³»ç»Ÿä¸­ï¼Œè®­ç»ƒæœ‰æ•ˆçš„å¤šè½®æ„å›¾åˆ†ç±»æ¨¡å‹çš„å…³é”®æŒ‘æˆ˜åœ¨äºç”Ÿæˆå¤§è§„æ¨¡ã€ç‰¹å®šé¢†åŸŸçš„å¤šè¯­è¨€å¯¹è¯æ•°æ®é›†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Intenté“¾ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒå°†éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMsï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡è‡ªæˆ‘å¯¹æŠ—ç”Ÿæˆæ„å›¾é©±åŠ¨ã€æ„è¯†å½¢æ€çš„å¯¹è¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä»ç°å®ä¸–ç•Œä¸­çš„ç”µå­å•†åŠ¡èŠå¤©æ—¥å¿—ä¸­æå–ç‰¹å®šé¢†åŸŸçš„æ„å›¾è½¬æ¢æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¼•å¯¼è½®çº§åŠ¨æ€å’Œæ„å›¾åºåˆ—çš„å»ºæ¨¡ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹éšé©¬å°”å¯å¤«æ¨¡å‹çš„å‘å°„æ¦‚ç‡è¿›è¡Œå‚æ•°åŒ–ï¼Œä½¿ç”Ÿæˆçš„å¯¹è¯ç¬¦åˆé¢„æœŸçš„æ„å›¾å’Œå¯¹è¯èƒŒæ™¯çš„è‡ªç„¶è¿è´¯æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†MINT-CLï¼Œä¸€ä¸ªç”¨äºå¤šè½®æ„å›¾åˆ†ç±»çš„å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåœ¨æé«˜æ€§èƒ½çš„åŒæ—¶å‡å°‘äº†å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯¹è¯ç”Ÿæˆè´¨é‡å’Œåˆ†ç±»å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç«äº‰å¯¹æ‰‹çš„åŸºçº¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†MINT-Eï¼Œä¸€ä¸ªå…¨é¢çš„ã€å¤šè¯­è¨€çš„ã€æ„å›¾æ„ŸçŸ¥çš„å¤šè½®å¯¹è¯è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“æ¥æºäºç”µå­å•†åŠ¡é¢†åŸŸã€‚ï¼ˆæ³¨ï¼šå¤ç°æºä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/junhua/chain-of-intent%E6%89%BE%E5%88%B0%E3%80%82%EF%BC%89">https://github.com/junhua/chain-of-intentæ‰¾åˆ°ã€‚ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14252v3">PDF</a> Accepted to Proceedings of CIKMâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºChain-of-Intentçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMsï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé€šè¿‡è‡ªæˆ‘åšå¼ˆç”Ÿæˆæ„å›¾é©±åŠ¨ã€è¯­å¢ƒæ„ŸçŸ¥çš„å¯¹è¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†MINT-CLï¼Œä¸€ä¸ªå¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¤šè½®æ„å›¾åˆ†ç±»ï¼Œå¯æé«˜æ€§èƒ½å¹¶å‡å°‘å¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯¹è¯ç”Ÿæˆè´¨é‡å’Œåˆ†ç±»å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯­ç§ç¯å¢ƒä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chain-of-Intentæ¡†æ¶ç»“åˆäº†éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMsï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä»¥ç”Ÿæˆä¸è¯­å¢ƒç›¸å…³çš„å¯¹è¯ã€‚</li>
<li>æå‡ºMINT-CLæ¡†æ¶ï¼Œç”¨äºå¤šè½®æ„å›¾åˆ†ç±»ï¼Œæé«˜æ€§èƒ½å¹¶å‡å°‘å¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚</li>
<li>ä½¿ç”¨çœŸå®ç”µå•†èŠå¤©æ—¥å¿—ä¸­çš„æ„å›¾è½¬æ¢æ¨¡å¼æ¥æŒ‡å¯¼è½®æ¬¡çº§åˆ«åŠ¨æ€å’Œæ„å›¾åºåˆ—çš„å»ºæ¨¡ã€‚</li>
<li>LLMsè¢«ç”¨æ¥å‚æ•°åŒ–HMMsçš„å‘å°„æ¦‚ç‡ï¼Œä»¥ç”Ÿæˆä¸é¢„æµ‹æ„å›¾å’Œå¯¹è¯è¯­å¢ƒç›¸ç¬¦çš„è‡ªç„¶ã€è¿è´¯çš„è¯­å¥ã€‚</li>
<li>æ¡†æ¶åœ¨å¯¹è¯ç”Ÿæˆè´¨é‡å’Œåˆ†ç±»å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯­ç§ç¯å¢ƒä¸­ã€‚</li>
<li>é‡Šæ”¾äº†MINT-Eæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä»ç”µå•†é¢†åŸŸè¡ç”Ÿçš„ç»¼åˆã€å¤šè¯­ç§ã€æ„å›¾æ„ŸçŸ¥çš„å¤šè½®å¯¹è¯è¯­æ–™åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-70cd5b797d5129fa49311a906589f475.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05de018233719bc29bc298f5f389a098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-678e9d769f455aa330e855876213d2b5.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a9776c6c76a2a52ad0fb6916d6dc80c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802175&auth_key=1759802175-0-0-9a00f09d4ea21df4f015ee414547835b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a6d15254d981e0cd40ba832ae405ed6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802181&auth_key=1759802181-0-0-bc65d7b7ae9bd92dec172729aed76c55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Talk-Listen-Connect-How-Humans-and-AI-Evaluate-Empathy-in-Responses-to-Emotionally-Charged-Narratives"><a href="#Talk-Listen-Connect-How-Humans-and-AI-Evaluate-Empathy-in-Responses-to-Emotionally-Charged-Narratives" class="headerlink" title="Talk, Listen, Connect: How Humans and AI Evaluate Empathy in Responses   to Emotionally Charged Narratives"></a>Talk, Listen, Connect: How Humans and AI Evaluate Empathy in Responses   to Emotionally Charged Narratives</h2><p><strong>Authors:Mahnaz Roshanaei, Rezvaneh Rezapour, Magy Seif El-Nasr</strong></p>
<p>Social interactions promote well-being, yet barriers like geographic distance, time limitations, and mental health conditions can limit face-to-face interactions. Emotionally responsive AI systems, such as chatbots, offer new opportunities for social and emotional support, but raise critical questions about how empathy is perceived and experienced in human-AI interactions. This study examines how empathy is evaluated in AI-generated versus human responses. Using personal narratives, we explored how persona attributes (e.g., gender, empathic traits, shared experiences) and story qualities affect empathy ratings. We compared responses from standard and fine-tuned AI models with human judgments. Results show that while humans are highly sensitive to emotional vividness and shared experience, AI-responses are less influenced by these cues, often lack nuance in empathic expression. These findings highlight challenges in designing emotionally intelligent systems that respond meaningfully across diverse users and contexts, and informs the design of ethically aware tools to support social connection and well-being. </p>
<blockquote>
<p>ç¤¾ä¼šäº’åŠ¨æœ‰åŠ©äºæå‡ç¦ç¥‰ï¼Œç„¶è€Œåœ°ç†è·ç¦»ã€æ—¶é—´é™åˆ¶å’Œå¿ƒç†å¥åº·çŠ¶å†µç­‰éšœç¢å¯èƒ½ä¼šé™åˆ¶é¢å¯¹é¢çš„äº’åŠ¨ã€‚æƒ…ç»ªååº”å‹çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œå¦‚èŠå¤©æœºå™¨äººï¼Œæä¾›äº†æ–°çš„ç¤¾äº¤å’Œæƒ…æ„Ÿæ”¯æŒçš„æœºä¼šï¼Œä½†ç”±æ­¤å¼•å‘äº†å…³äºäººæœºäº¤äº’ä¸­åŒæƒ…å¦‚ä½•è¢«æ„ŸçŸ¥å’Œä½“éªŒçš„é‡è¦é—®é¢˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è°ƒæŸ¥äººå·¥æ™ºèƒ½ç”Ÿæˆçš„ååº”ä¸äººç±»ååº”ä¸­çš„åŒæƒ…è¯„ä»·å¦‚ä½•ã€‚é€šè¿‡ä¸ªäººå™äº‹çš„æ–¹å¼ï¼Œæˆ‘ä»¬æ¢è®¨äº†äººæ ¼å±æ€§ï¼ˆå¦‚æ€§åˆ«ã€ç§»æƒ…ç‰¹è´¨ã€å…±äº«ç»éªŒï¼‰å’Œæ•…äº‹è´¨é‡å¦‚ä½•å½±å“ç§»æƒ…è¯„åˆ†ã€‚æˆ‘ä»¬å°†æ ‡å‡†çš„äººå·¥æ™ºèƒ½æ¨¡å‹å’Œç»è¿‡å¾®è°ƒçš„äººå·¥æ™ºèƒ½æ¨¡å‹çš„ååº”ä¸äººç±»çš„åˆ¤æ–­è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶äººç±»å¯¹æƒ…ç»ªç”ŸåŠ¨æ€§å’Œå…±äº«ç»éªŒçš„æ•æ„Ÿæ€§å¾ˆé«˜ï¼Œä½†äººå·¥æ™ºèƒ½çš„ååº”å—åˆ°è¿™äº›çº¿ç´¢çš„å½±å“è¾ƒå°ï¼Œå¾€å¾€ç¼ºä¹å¾®å¦™çš„ç§»æƒ…è¡¨è¾¾ã€‚è¿™äº›å‘ç°çªæ˜¾äº†åœ¨è®¾è®¡å’Œå¼€å‘æœ‰æ„ä¹‰çš„æƒ…ç»ªæ™ºèƒ½ç³»ç»Ÿæ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œè¿™äº›ç³»ç»Ÿèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„ç”¨æˆ·å’Œæƒ…å¢ƒèƒŒæ™¯ä¸‹ä½œå‡ºååº”ï¼Œå¹¶æç¤ºæˆ‘ä»¬è®¾è®¡å‡ºæœ‰é“å¾·æ„è¯†çš„å·¥å…·æ¥æ”¯æŒç¤¾äº¤è”ç³»å’Œç¦ç¥‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15550v2">PDF</a> 21 pages, 4 figures, 6 tables. Title updated from â€œTalk, Listen,   Connect: Navigating Empathy in Human-AI Interactionsâ€ to â€œTalk, Listen,   Connect: How Humans and AI Evaluate Empathy in Responses to Emotionally   Charged Narrativesâ€ in this version. This is version 2 (v2) of the paper. All   previous citations of arXiv:2409.15550 with the old title still refer to the   same paper</p>
<p><strong>Summary</strong><br>     ç¤¾äº¤äº’åŠ¨æœ‰åŠ©äºæå‡å¹¸ç¦æ„Ÿï¼Œä½†åœ°ç†è·ç¦»ã€æ—¶é—´é™åˆ¶å’Œå¿ƒç†å¥åº·çŠ¶å†µç­‰éšœç¢é™åˆ¶äº†é¢å¯¹é¢çš„äº¤æµã€‚æƒ…æ„Ÿå“åº”çš„AIç³»ç»Ÿå¦‚èŠå¤©æœºå™¨äººæä¾›äº†æ–°çš„ç¤¾äº¤å’Œæƒ…æ„Ÿæ”¯æŒçš„æœºä¼šï¼Œä½†ä¹Ÿå¼•å‘äº†å…³äºäººç±»ä¸AIäº’åŠ¨ä¸­å¦‚ä½•æ„ŸçŸ¥å’Œä½“éªŒåŒæƒ…å¿ƒçš„å…³é”®é—®é¢˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†AIç”Ÿæˆå“åº”ä¸äººç±»å“åº”ä¸­çš„åŒæƒ…å¿ƒå¦‚ä½•è¯„ä¼°ã€‚é€šè¿‡ä¸ªäººå™äº‹ï¼Œæˆ‘ä»¬æ¢è®¨äº†äººæ ¼å±æ€§ï¼ˆå¦‚æ€§åˆ«ã€åŒæƒ…ç‰¹è´¨ã€å…±åŒç»å†ï¼‰å’Œæ•…äº‹å“è´¨å¯¹åŒæƒ…å¿ƒè¯„ä»·çš„å½±å“ã€‚æˆ‘ä»¬å°†æ ‡å‡†AIæ¨¡å‹å’Œç»è¿‡å¾®è°ƒåçš„AIæ¨¡å‹çš„ååº”ä¸äººç±»çš„åˆ¤æ–­è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶äººç±»å¯¹æƒ…æ„Ÿç”ŸåŠ¨æ€§å’Œå…±åŒç»å†çš„æ•æ„Ÿæ€§å¾ˆé«˜ï¼Œä½†AIçš„ååº”å—è¿™äº›çº¿ç´¢çš„å½±å“è¾ƒå°ï¼Œå¾€å¾€ç¼ºä¹å¾®å¦™çš„åŒç†å¿ƒè¡¨è¾¾ã€‚è¿™äº›å‘ç°çªå‡ºäº†åœ¨è®¾è®¡èƒ½å¤Ÿé€‚åº”ä¸åŒç”¨æˆ·å’Œä¸Šä¸‹æ–‡ç¯å¢ƒçš„æ™ºèƒ½ç³»ç»Ÿæ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¹Ÿä¸ºæ”¯æŒç¤¾ä¼šè”ç³»å’Œå¹¸ç¦æ„Ÿçš„é“å¾·æ„è¯†å·¥å…·çš„è®¾è®¡æä¾›äº†ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤äº’åŠ¨å¯¹æå‡å¹¸ç¦æ„Ÿè‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨åœ°ç†ã€æ—¶é—´ç­‰éšœç¢é™åˆ¶é¢å¯¹é¢çš„äº¤æµã€‚</li>
<li>æƒ…æ„Ÿå“åº”çš„AIç³»ç»Ÿå¦‚èŠå¤©æœºå™¨äººå¯ä¸ºç¤¾äº¤å’Œæƒ…æ„Ÿæ”¯æŒæä¾›æ–°æœºä¼šã€‚</li>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†AIç”Ÿæˆå“åº”ä¸äººç±»å“åº”ä¸­åŒæƒ…å¿ƒè¯„ä¼°çš„å·®å¼‚ã€‚</li>
<li>äººæ ¼å±æ€§å’Œæ•…äº‹å“è´¨å½±å“åŒæƒ…å¿ƒè¯„ä»·ã€‚</li>
<li>AIå¯¹äºæƒ…æ„Ÿç”ŸåŠ¨æ€§å’Œå…±åŒç»å†çš„æ•æ„Ÿæ€§è¾ƒä½ï¼Œç¼ºä¹å¾®å¦™çš„åŒç†å¿ƒè¡¨è¾¾ã€‚</li>
<li>åœ¨è®¾è®¡æ™ºèƒ½ç³»ç»Ÿæ—¶ï¼Œéœ€è¦è€ƒè™‘åˆ°é€‚åº”ä¸åŒç”¨æˆ·å’Œä¸Šä¸‹æ–‡ç¯å¢ƒçš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15550">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bccac1779a86eb1b5eaebc2e31f1cec0.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-956b2fd9c1e9fcd3c5f38c31e14ea825~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802196&auth_key=1759802196-0-0-92402548d47e07309dfa8e0d1ed0c191&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8be3d509100261e1c7c293545aba3251~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802203&auth_key=1759802203-0-0-8d069ab0c53339c3fe7c47eba5c41c89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ed80373b4c0561cfe6db59597653c368.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Embracing Aleatoric Uncertainty Generating Diverse 3D Human Motion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-41abcaaf44cd5b6288db938b78329f6c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  AUDETER A Large-scale Dataset for Deepfake Audio Detection in Open   Worlds
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
