<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive 方向最新论文已更新，请持续关注 Update in 2025-09-08  TaleDiffusion Multi-Character Story Generation with Dialogue Rendering">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-d1cb3130dc8e0118f1f51b8900d31682~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801874&auth_key=1759801874-0-0-a27e26daa083cb4853519920a5292d9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-08-更新"><a href="#2025-09-08-更新" class="headerlink" title="2025-09-08 更新"></a>2025-09-08 更新</h1><h2 id="TaleDiffusion-Multi-Character-Story-Generation-with-Dialogue-Rendering"><a href="#TaleDiffusion-Multi-Character-Story-Generation-with-Dialogue-Rendering" class="headerlink" title="TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering"></a>TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering</h2><p><strong>Authors:Ayan Banerjee, Josep Lladós, Umapada Pal, Anjan Dutta</strong></p>
<p>Text-to-story visualization is challenging due to the need for consistent interaction among multiple characters across frames. Existing methods struggle with character consistency, leading to artifact generation and inaccurate dialogue rendering, which results in disjointed storytelling. In response, we introduce TaleDiffusion, a novel framework for generating multi-character stories with an iterative process, maintaining character consistency, and accurate dialogue assignment via postprocessing. Given a story, we use a pre-trained LLM to generate per-frame descriptions, character details, and dialogues via in-context learning, followed by a bounded attention-based per-box mask technique to control character interactions and minimize artifacts. We then apply an identity-consistent self-attention mechanism to ensure character consistency across frames and region-aware cross-attention for precise object placement. Dialogues are also rendered as bubbles and assigned to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion outperforms existing methods in consistency, noise reduction, and dialogue rendering. </p>
<blockquote>
<p>文本到故事可视化是一个挑战，因为需要在多个帧之间进行持续的角色互动。现有方法在角色一致性方面存在困难，导致出现伪影和对话渲染不准确的问题，从而导致故事叙述不连贯。为了解决这个问题，我们引入了TaleDiffusion，这是一个通过迭代过程生成多角色故事的新框架，通过后期处理来保持角色一致性和准确的对话分配。给定一个故事，我们使用预训练的LLM通过上下文学习生成每帧描述、角色细节和对话，然后通过基于边界的注意力感知框掩码技术来控制角色互动并尽量减少伪影。接着，我们应用身份一致的自注意力机制以确保跨帧的角色一致性，以及区域感知的跨注意力以实现精确的对象放置。对话也被渲染为气泡，并通过CLIPSeg分配给角色。实验结果表明，在一致性、降噪和对话渲染方面，TaleDiffusion优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04123v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为TaleDiffusion的新框架，用于生成多角色故事。该框架通过迭代过程维护角色一致性，并通过后处理实现准确的对话分配。利用预训练的大型语言模型生成每帧描述、角色细节和对话，采用基于有界注意力的每框掩码技术控制角色交互并减少伪影。通过身份一致的自注意力机制确保跨帧的角色一致性，以及区域感知的跨注意力实现精确的对象放置。对话以气泡形式呈现，并通过CLIPSeg分配给角色。实验结果表明，TaleDiffusion在一致性、降噪和对话渲染方面优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TaleDiffusion是一个用于生成多角色故事的全新框架。</li>
<li>该框架通过迭代过程维护角色一致性。</li>
<li>通过预训练的大型语言模型生成每帧的描述、角色细节和对话。</li>
<li>采用基于有界注意力的每框掩码技术来控制角色交互并减少伪影。</li>
<li>通过自注意力机制确保跨帧的角色一致性。</li>
<li>采用区域感知的跨注意力实现精确的对象放置。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04123">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dd73ae4b5667c4f5a6a50230afbb9aa1.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b67282e01ab5edafdcc4f062d2bd1e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801890&auth_key=1759801890-0-0-2233abcca85818952a46b2018b7c2acf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-7758e741f3a34ba4a006bb89df8f0f8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fda63a021f4110ed7ca3a9dd6ddc1d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5132a763b2d96b95fa865be519dfc0c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Stable-and-Personalised-Profiles-for-Lexical-Alignment-in-Spoken-Human-Agent-Dialogue"><a href="#Towards-Stable-and-Personalised-Profiles-for-Lexical-Alignment-in-Spoken-Human-Agent-Dialogue" class="headerlink" title="Towards Stable and Personalised Profiles for Lexical Alignment in Spoken   Human-Agent Dialogue"></a>Towards Stable and Personalised Profiles for Lexical Alignment in Spoken   Human-Agent Dialogue</h2><p><strong>Authors:Keara Schaaij, Roel Boumans, Tibor Bosse, Iris Hendrickx</strong></p>
<p>Lexical alignment, where speakers start to use similar words across conversation, is known to contribute to successful communication. However, its implementation in conversational agents remains underexplored, particularly considering the recent advancements in large language models (LLMs). As a first step towards enabling lexical alignment in human-agent dialogue, this study draws on strategies for personalising conversational agents and investigates the construction of stable, personalised lexical profiles as a basis for lexical alignment. Specifically, we varied the amounts of transcribed spoken data used for construction as well as the number of items included in the profiles per part-of-speech (POS) category and evaluated profile performance across time using recall, coverage, and cosine similarity metrics. It was shown that smaller and more compact profiles, created after 10 min of transcribed speech containing 5 items for adjectives, 5 items for conjunctions, and 10 items for adverbs, nouns, pronouns, and verbs each, offered the best balance in both performance and data efficiency. In conclusion, this study offers practical insights into constructing stable, personalised lexical profiles, taking into account minimal data requirements, serving as a foundational step toward lexical alignment strategies in conversational agents. </p>
<blockquote>
<p>词汇对齐对话过程中，说话者开始使用相似的词汇，这对于成功交流起着重要作用。然而，它在对话代理中的实现仍然鲜有研究，尤其是考虑到最近的大型语言模型（LLM）的进展。作为实现人机对话词汇对齐的第一步，本研究借鉴个性化对话代理的策略，并研究构建稳定、个性化的词汇表作为词汇对齐的基础。具体来说，我们构建了不同长度的词汇表，并评估了不同词汇数量对词汇对齐的影响，使用召回率、覆盖率和余弦相似度等指标来评估词汇表随时间变化的性能。研究表明，经过包含形容词5项、连词5项以及每个包含副词、名词、代词和动词各含大约五项大约持续约十分钟内容的语音内容构建的词汇表能最佳地实现性能和数据效率的平衡。总的来说，本研究从构建稳定的个性化词汇表角度入手，深入探讨了如何尽可能降低数据量需求，作为对话代理中实现词汇对齐策略的基础步骤。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04104v1">PDF</a> Accepted for TSD 2025</p>
<p><strong>Summary</strong></p>
<p>这篇论文探讨了词汇对齐在对话交流中的重要性，特别是在人与智能对话系统中的运用。研究提出个性化对话系统的策略，并构建稳定、个性化的词汇表作为词汇对齐的基础。通过调整转录语音数据的数量和词汇表中每个词类的项目数量，研究评估了词汇表的性能。结果表明，构建包含形容词5项、连词5项以及副词、名词、代词和动词各10项的简短词汇表可达到最佳的性能与数据效率平衡。这为对话系统中的词汇对齐策略提供了实践性的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>词汇对齐在对话交流中至关重要，尤其是在人与智能对话系统的交互中。</li>
<li>研究提出了个性化对话系统的策略，这是实现词汇对齐的重要一步。</li>
<li>构建稳定、个性化的词汇表是实现词汇对齐的基础。</li>
<li>调整转录语音数据的数量和词汇表中每个词类的项目数量会影响词汇表的性能。</li>
<li>简短且包含特定项目数量的词汇表在性能和数据效率之间达到了最佳平衡。</li>
<li>该研究为对话系统中词汇对齐策略的实践提供了有价值的见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04104">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d38b412d2625044de8e815a535310fd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20c6af2761883fb2c4ff9caf4ee8af85.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Chatbot-Deployment-Considerations-for-Application-Agnostic-Human-Machine-Dialogues"><a href="#Chatbot-Deployment-Considerations-for-Application-Agnostic-Human-Machine-Dialogues" class="headerlink" title="Chatbot Deployment Considerations for Application-Agnostic Human-Machine   Dialogues"></a>Chatbot Deployment Considerations for Application-Agnostic Human-Machine   Dialogues</h2><p><strong>Authors:Pablo Rivas, Chelsi Chelsi, Nishit Nishit, Laharika Ravula</strong></p>
<p>Automatic conversation systems based on natural language responses are becoming ubiquitous, in part, due to major advances in computational linguistics and machine learning. The easy access to robust and affordable platforms are causing companies to have an unprecedented rush to adopt chatbot technologies for customer service and support. However, this rush has caused judgment lapses when releasing chatbot technologies into production systems. This paper aims to shed light on basic, elemental, considerations that technologists must consider before deploying a chatbot. Our approach takes one particular case to draw lessons for those considering the implementation of chatbots. By looking at this case-study, we aim to call for consideration of societal values as a paramount factor before deploying a chatbot and consider the societal implications of releasing these types of systems. </p>
<blockquote>
<p>基于自然语言回应的自动对话系统正变得无处不在，部分原因是计算语言学和机器学习方面的重大进展。容易获得稳健且实惠的平台正在使公司竞相采用聊天机器人技术进行客户服务和支持。然而，这种仓促推动导致在将聊天机器人技术投放生产系统时出现判断失误。本文旨在阐明在部署聊天机器人之前，技术人员必须考虑的基本要素。我们的方法以一个特定案例为例，为那些正在考虑实施聊天机器人的人提供经验教训。通过案例研究，我们呼吁在部署聊天机器人之前，首先考虑社会价值观是一个至关重要的因素，并考虑发布这些类型的系统所带来的社会影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02611v1">PDF</a> The Third Workshop on Reasoning and Learning for Human-Machine   Dialogues at the Thirty-Fourth AAAI Conference on Artificial Intelligence   (AAAI-20)</p>
<p><strong>Summary</strong><br>自动对话系统基于自然语言响应的技术日益普及，得益于计算语言学和机器学习方面的重大进展。稳健且经济实惠的平台的普及，使得企业纷纷采用聊天机器人技术进行客户服务和支持。然而，这种匆忙将聊天机器人技术投入生产系统造成了判断失误。本文旨在阐述在部署聊天机器人之前，技术人员需要考虑的基本要素。通过个案研究，我们呼吁在部署聊天机器人之前考虑社会价值观这一重要因素，并考虑发布这些系统对社会的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动对话系统基于自然语言响应的技术日益普及，得益于计算语言学和机器学习的进展。</li>
<li>聊天机器人技术被企业广泛采用，用于客户服务和支持。</li>
<li>在部署聊天机器人技术到生产系统时，存在判断失误的情况。</li>
<li>技术人员在部署聊天机器人之前，需要考虑基本要素。</li>
<li>社会价值是一个重要的因素，需要在部署聊天机器人之前考虑。</li>
<li>发布这些系统对社会的影响需要被重视。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02611">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-965c38c121586ed63d39e885807f91e8.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-d03d37ac87a4e6552f07c39a63968379~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801940&auth_key=1759801940-0-0-607b8489d26163c2110c6f8ac73bab63&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ddd39025bcddcfccbbbbf656ecf69ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801946&auth_key=1759801946-0-0-5d112c00355a2491fef9416982c7de82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Quantitative-imaging-of-478-keV-prompt-gamma-rays-from-boron-neutron-capture-reactions"><a href="#Quantitative-imaging-of-478-keV-prompt-gamma-rays-from-boron-neutron-capture-reactions" class="headerlink" title="Quantitative imaging of 478-keV prompt gamma rays from boron neutron   capture reactions"></a>Quantitative imaging of 478-keV prompt gamma rays from boron neutron   capture reactions</h2><p><strong>Authors:Tetsuya Mizumoto, Shotaro Komura, Atsushi Takada, Yoshinori Sakurai, Toru Tanimori</strong></p>
<p>The accurate imaging and quantitative measurement of 478-keV prompt gamma rays are critical for advancing boron neutron capture therapy (BNCT), a promising cancer treatment. Although numerical simulations have indicated that such measurements are feasible, their practical application has proven challenging. This study introduces a gamma-ray imaging detector designed specifically for precise BNCT measurements. Using boron-rich phantom samples, we successfully imaged 478-keV gamma rays and established a linear correlation between gamma-ray production and boron concentration. Furthermore, applying this technique in a recognized BNCT treatment facility demonstrated the detector’s effectiveness in monitoring boron dose distribution during neutron irradiation, both in pretreatment diagnostics and throughout the treatment process. </p>
<blockquote>
<p>对478-keV即时伽马射线的精确成像和定量测量对于发展硼中子捕获疗法（BNCT）这一前景光明的癌症治疗方法至关重要。尽管数值模拟表明这样的测量是可行的，但其实际应用却证明是有挑战性的。本研究介绍了一种专门设计用于精确BNCT测量的伽马射线成像探测器。我们使用富含硼的幻影样本成功地对478-keV伽马射线进行了成像，并建立了伽马射线产量与硼浓度之间的线性关系。此外，在公认的BNCT治疗设施中应用此技术，证明了该探测器在监测中子照射过程中的硼剂量分布方面的有效性，无论是在治疗前诊断还是在整个治疗过程中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00435v1">PDF</a> 35 pages, 8 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对硼中子俘获疗法（BNCT）中478-keV即时伽马射线的精确成像和定量测量的研究。该研究设计了一种专门用于精确BNCT测量的伽马射线成像探测器，通过对富硼 Phantom样本的成功成像，建立了伽马射线产量与硼浓度之间的线性关系。此外，在实际应用的BNCT治疗设施中使用该探测器，有效监测了中子照射过程中的硼剂量分布，可用于治疗前诊断以及整个治疗过程的监测。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>硼中子俘获疗法（BNCT）是一种有前途的癌症治疗方法，需要准确成像和定量测量478-keV的即时伽马射线来促进其发展。</li>
<li>通过数值模拟证实了测量478-keV伽马射线的可行性，但实际应用具有挑战性。</li>
<li>研究开发了一种专门用于BNCT测量的伽马射线成像探测器。</li>
<li>使用富硼 Phantom样本成功成像，证明了探测器的有效性。</li>
<li>建立了伽马射线产量与硼浓度之间的线性关系。</li>
<li>探测器可用于实际应用的BNCT治疗设施中，监测中子照射过程中的硼剂量分布。</li>
<li>该技术既可用于治疗前诊断，也可在整个治疗过程中进行监测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00435">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6b6358596a319d66c4e28e4659f28a3a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Schema-Guided-Response-Generation-using-Multi-Frame-Dialogue-State-for-Motivational-Interviewing-Systems"><a href="#Schema-Guided-Response-Generation-using-Multi-Frame-Dialogue-State-for-Motivational-Interviewing-Systems" class="headerlink" title="Schema-Guided Response Generation using Multi-Frame Dialogue State for   Motivational Interviewing Systems"></a>Schema-Guided Response Generation using Multi-Frame Dialogue State for   Motivational Interviewing Systems</h2><p><strong>Authors:Jie Zeng, Yukiko I. Nakano</strong></p>
<p>The primary goal of Motivational Interviewing (MI) is to help clients build their own motivation for behavioral change. To support this in dialogue systems, it is essential to guide large language models (LLMs) to generate counselor responses aligned with MI principles. By employing a schema-guided approach, this study proposes a method for updating multi-frame dialogue states and a strategy decision mechanism that dynamically determines the response focus in a manner grounded in MI principles. The proposed method was implemented in a dialogue system and evaluated through a user study. Results showed that the proposed system successfully generated MI-favorable responses and effectively encouraged the user’s (client’s) deliberation by asking eliciting questions. </p>
<blockquote>
<p>动机性访谈（MI）的主要目标是帮助客户建立自己的行为改变动机。为了在对话系统中支持这一点，引导大型语言模型（LLM）生成与MI原则一致的咨询师回应至关重要。本研究采用模式引导的方法，提出了一种更新多帧对话状态的方法和一种策略决策机制，以动态确定以MI原则为基础的反应重点。所提出的方法在一个对话系统中得到了实现，并通过用户研究进行了评估。结果表明，该系统成功生成了有利于MI的响应，并通过提出引导性问题有效地鼓励了用户（客户）的思考。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20635v1">PDF</a> 28pages, 15 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了如何通过采用模式引导的方法，在对话系统中应用动机性访谈（MI）的原则，以帮助用户（即客户）建立改变行为的动力。通过更新多帧对话状态和策略决策机制，该系统能够生成符合MI原则的回答，并通过提出问题鼓励用户深思。经过用户研究评估，该系统成功生成了有利于MI的回应。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动机性访谈（MI）的核心目标是帮助客户建立改变行为的动力。</li>
<li>在对话系统中应用MI原则时，需要引导大型语言模型（LLMs）生成符合MI原则的回答。</li>
<li>采用模式引导的方法可以更新多帧对话状态，使系统能够更灵活地与用户交流。</li>
<li>策略决策机制能够动态确定回应的重点，确保对话符合MI原则。</li>
<li>通过用户研究评估，该系统成功生成了有利于MI的回应。</li>
<li>该系统通过提出问题鼓励用户深思，有助于增强用户的动机和自我反思。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-7306460e5ddcec2ab3930e4046f83572~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801961&auth_key=1759801961-0-0-43f37add955cd1433be6af51fbc414dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bcee6ea1128290848822b5492368532f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801969&auth_key=1759801969-0-0-1b39a6e904c1ef298a2004c90ecca97e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a541555b1b8fbffb484f9dbf6029fee9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801977&auth_key=1759801977-0-0-83f79d9ee1622e4d053a6fb9a7314f31&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OSUM-EChat-Enhancing-End-to-End-Empathetic-Spoken-Chatbot-via-Understanding-Driven-Spoken-Dialogue"><a href="#OSUM-EChat-Enhancing-End-to-End-Empathetic-Spoken-Chatbot-via-Understanding-Driven-Spoken-Dialogue" class="headerlink" title="OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue"></a>OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue</h2><p><strong>Authors:Xuelong Geng, Qijie Shao, Hongfei Xue, Shuiyuan Wang, Hanke Xie, Zhao Guo, Yi Zhao, Guojian Li, Wenjie Tian, Chengyou Wang, Zhixian Zhao, Kangxiang Xia, Ziyu Zhang, Zhennan Lin, Tianlun Zuo, Mingchen Shao, Yuang Cao, Guobin Ma, Longhao Li, Yuhang Dai, Dehui Gao, Dake Guo, Lei Xie</strong></p>
<p>Empathy is crucial in enabling natural interactions within spoken dialogue systems, allowing machines to recognize and respond appropriately to paralinguistic cues such as age, gender, and emotion. Recent advancements in end-to-end speech language models, which unify speech understanding and generation, provide promising solutions. However, several challenges persist, including an over-reliance on large-scale dialogue datasets, insufficient extraction of paralinguistic cues vital for conveying empathy, and the lack of empathy-specific datasets and evaluation frameworks. To address these issues, we introduce OSUM-EChat, an open-source, end-to-end spoken dialogue system designed to enhance empathetic interactions, particularly in resource-limited settings. OSUM-EChat introduces two key innovations: (1) a three-stage understanding-driven spoken dialogue training strategy that extends the capabilities of a large speech understanding model to spoken dialogue tasks, and (2) a linguistic-paralinguistic dual thinking mechanism that integrates paralinguistic understanding through a chain of thought with dialogue generation, enabling the system to produce more empathetic responses. This approach reduces reliance on large-scale dialogue datasets while maintaining high-quality empathetic interactions. Additionally, we introduce the EChat-200K dataset, a rich corpus of empathetic speech-to-speech dialogues, and the EChat-eval benchmark, a comprehensive framework for evaluating the empathetic capabilities of dialogue systems. Experimental results demonstrate that OSUM-EChat outperforms end-to-end spoken dialogue models regarding empathetic responsiveness, validating its effectiveness. </p>
<blockquote>
<p>共情在口语对话系统内的自然交互中至关重要，它让机器能够识别和适当回应诸如年龄、性别和情绪等副语言线索。端到端的语言模型最近的进步，这些模型统一了语音理解和生成，提供了很有前景的解决方案。然而，仍存在一些挑战，包括过度依赖大规模的对话数据集，对传递共情至关重要的副语言线索提取不足，以及缺乏专门针对共情的数据集和评估框架。为了解决这些问题，我们推出了OSUM-EChat，这是一个开源的端到端口语对话系统，旨在增强共情交互，特别是在资源有限的环境中。OSUM-EChat引入了两项关键创新：一是以理解为主导的三阶段口语对话训练策略，它扩展了大型语音理解模型在口语对话任务上的能力；二是语言-副语言双重思考机制，它将副语言理解融入思维链进行对话生成，使系统能够产生更具共情的回应。这种方法减少了大规模对话数据集的依赖，同时保持了高质量的共情交互。此外，我们还推出了EChat-200K数据集，这是一份丰富的共情语音对话语料库，以及EChat-eval基准测试，这是一个全面评估对话系统共情能力的框架。实验结果表明，OSUM-EChat在共情响应方面优于端到端的口语对话模型，验证了其有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09600v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>共情在口语对话系统中实现自然交互至关重要，它让机器能够识别和回应年龄、性别和情感等副语言线索。尽管端到端的自然语言模型在统一语音理解和生成方面取得了进展，但仍存在依赖大规模对话数据集、未能充分提取对表达共情至关重要的副语言线索以及缺乏针对共情的特定数据集和评估框架等挑战。为解决这些问题，我们推出了OSUM-EChat这一开源的端到端口语对话系统，旨在加强共情交互，特别是在资源有限的环境中。OSUM-EChat引入了两个关键创新点：一是以理解为核心的三阶段口语对话训练策略，扩展了大规模语音理解模型在口语对话任务上的能力；二是语言-副语言双重思考机制，通过一系列思考将副语言理解与对话生成相结合，使系统能够产生更共情的回应。这种方法减少了大规模对话数据集的依赖，同时保持了高质量的共情交互。此外，我们还推出了EChat-200K数据集，这是一份丰富的共情性语音对话语料库，以及EChat-eval基准测试，这是一个全面评估对话系统共情能力的框架。实验结果表明，OSUM-EChat在共情响应方面优于端到端的口语对话模型，验证了其有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>共情在口语对话系统中对于实现自然交互至关重要，允许机器识别并适当回应副语言线索。</li>
<li>端到端的自然语言模型在统一语音理解和生成方面取得进展，但仍存在挑战。</li>
<li>OSUM-EChat是一个旨在增强共情交互的开源端到端口语对话系统。</li>
<li>OSUM-EChat引入了三阶段口语对话训练策略和语言-副语言双重思考机制。</li>
<li>该系统通过减少大规模对话数据集的依赖，维持了高质量的共情交互。</li>
<li>推出了EChat-200K数据集，包含丰富的共情性语音对话内容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09600">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5d386e2fc84fcc3e17c4d8e633ad8089~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801985&auth_key=1759801985-0-0-418ef720c205ee0a9623a9219b19d94e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-74141b369f2361d8250ad455babb0565.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8736de6347a296ecdc6e30496e4b354~resize:0:q75.jpg?source=1f5c5e47&expiration=1759801998&auth_key=1759801998-0-0-c5f64744038082a8f0e9cad6868f7315&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4bd9a04fbafc7b2d741a1d510deeaf94~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802005&auth_key=1759802005-0-0-0f5a38a6910bf8729f2ddbd3d4be114c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1532fe434229d1e26038d8d42cfc6836~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802012&auth_key=1759802012-0-0-735ff7e7afb1ef87179570bea8dced54&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-28c3b03132ebd4dd671c6c43f74be212.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges"><a href="#Learning-an-Efficient-Multi-Turn-Dialogue-Evaluator-from-Multiple-Judges" class="headerlink" title="Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges"></a>Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges</h2><p><strong>Authors:Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding</strong></p>
<p>Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the “LLM-as-a-judge” paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness. </p>
<blockquote>
<p>评估大型语言模型（LLM）的对话能力仍然是一项具有挑战性的任务。当前的主流方法主要依赖于“LLM作为评判员”的模式，即提示LLM充当评估员，以评估对话质量。然而，这些方法常常会受到各种偏见的影响，从而破坏了评估结果的可靠性和一致性。为了减少这些偏见，最近的方法采用多个LLM作为评判员，并将他们的判断汇总起来以选择最佳评估结果。尽管这种方法有效，但在推理过程中却产生了巨大的计算开销。在本文中，我们提出了一种高效的多轮对话评估器，它通过将一个模型汇总多个LLM评判员的偏好知识来捕捉集体智慧。我们的方法保留了多种多评判员反馈的优势，同时大大降低了评估成本，实现了快速灵活的对话质量评估。在七个单一评分和配对比较对话评估基准测试上的广泛实验表明，我们的方法在多种场景下都优于现有基线，展现了其高效性和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00454v2">PDF</a> 15 pages, 2 pages, under review</p>
<p><strong>Summary</strong></p>
<p>大语言模型的对话能力评估仍然是一项具有挑战性的任务。当前主流方法主要依赖于“LLM作为评判者”的模式，但这种方法存在偏见问题，影响评估结果的可靠性和一致性。为缓解这一问题，近期方法采用多个LLM作为评判者并汇聚判断以选择最佳评估。尽管有效，但多评判者方法在推理过程中会产生巨大的计算开销。本文提出一种高效的多轮对话评估器，通过汇聚多个LLM评判者的偏好知识到一个单一模型中，捕捉集体智慧。该方法在保留多样化多评判者反馈优势的同时，大幅降低了评估成本，实现了快速灵活的对话质量评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对话能力评估对于大语言模型（LLM）仍具挑战性。</li>
<li>当前主流评估方法依赖于LLM作为评判者，但存在偏见问题。</li>
<li>多评判者方法能有效缓解偏见，但计算开销大。</li>
<li>本文提出一种新型对话评估器，融合多个LLM的判断。</li>
<li>该方法提高了评估效率并降低了成本。</li>
<li>实验证明该方法在多种对话评估基准测试中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00454">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-03ebd54930b9b519f32e0d7fe7cada0a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802027&auth_key=1759802027-0-0-1cd4f07e058f5083775defcecf84eaea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21e3d2da2b2991cb17167fc9a02a728e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802035&auth_key=1759802035-0-0-c6265a194e1720d007d6af0288c38a0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03ad3cd1b591002d79d265a5df7b056d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802042&auth_key=1759802042-0-0-2a34b8e824346edcc804dd41c18dabff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-763c5040c1dc72f7a3bc76762bd98530.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5a6292e608e6f1c68743c9802b9c5d5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Binaural-Target-Speaker-Extraction-using-HRTFs"><a href="#Binaural-Target-Speaker-Extraction-using-HRTFs" class="headerlink" title="Binaural Target Speaker Extraction using HRTFs"></a>Binaural Target Speaker Extraction using HRTFs</h2><p><strong>Authors:Yoav Ellinson, Sharon Gannot</strong></p>
<p>In this work, we aim to imitate the human ability to selectively attend to a single speaker, even in the presence of multiple simultaneous talkers. To achieve this, we propose a novel approach for binaural target speaker extraction that leverages the listener’s Head-Related Transfer Function (HRTF) to isolate the desired speaker. Notably, our method does not rely on speaker embeddings, making it speaker-independent and enabling strong generalization across multiple speech datasets and languages. We employ a fully complex-valued neural network that operates directly on the complex-valued Short-Time Fourier transform (STFT) of the mixed audio signals, and compare it to a Real-Imaginary (RI)-based neural network, demonstrating the advantages of the former. We first evaluate the method in an anechoic, noise-free scenario, achieving excellent extraction performance while preserving the binaural cues of the target signal. We then extend the evaluation to reverberant conditions. Our method proves robust, maintaining speech clarity and source directionality while simultaneously reducing reverberation. A comparative analysis with existing binaural Target Speaker Extraction (TSE) methods demonstrates that our approach attains performance on par with competing techniques in terms of noise reduction and perceptual quality, while offering a clear advantage in preserving binaural cues.Demo-page: <a target="_blank" rel="noopener" href="https://bi-ctse-hrtf.github.io/">https://bi-ctse-hrtf.github.io</a> </p>
<blockquote>
<p>在这项工作中，我们的目标是模仿人类有选择地关注单一说话者的能力，即使在有多个同时说话的情境中也能如此。为了实现这一目标，我们提出了一种新型的利用双耳目标说话者提取的方法，该方法利用听众的头部相关传输函数（HRTF）来分离目标说话者的声音。值得注意的是，我们的方法不依赖于说话者嵌入，使其成为独立于说话者的方法，并在多个语音数据集和语言之间实现了强大的泛化能力。我们采用一个全复数神经网络，直接在混合音频信号的复数短时傅里叶变换（STFT）上操作，并将其与基于实虚（RI）的神经网络进行比较，展示了前者的优势。我们首先在无混响、无噪声的场景下评估该方法，在保持目标信号的双耳线索的同时实现了出色的提取性能。然后我们将评估扩展到混响条件。我们的方法证明是稳健的，在保持语音清晰度和源方向性的同时减少了混响。与现有的双耳目标说话者提取（TSE）方法的比较分析表明，我们的方法在降噪和感知质量方面的性能与竞争技术相当，同时在保留双耳线索方面具有明显的优势。演示页面：<a target="_blank" rel="noopener" href="https://bi-ctse-hrtf.github.io/">https://bi-ctse-hrtf.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19369v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种利用人类听觉系统选择性注意单一说话者的能力，实现双耳目标说话者提取的新方法。利用听者头部相关传输函数（HRTF）隔离目标说话者的声音。此方法无需依赖说话者嵌入，具有良好的泛化能力，可跨越多个语音数据集和语言使用。研究采用了完全复数神经网络直接处理混合音频信号的复数短时傅里叶变换（STFT），并与基于实虚数（RI）的神经网络进行了比较，显示了前者的优势。在无声学回声和噪声的情境中评估此方法，表现出优异的提取性能且保留了目标信号的双耳线索。此外，在具有回声的条件下进行评估时，该方法保持了良好的表现，证明其在维持语音清晰度和来源方向性的同时减少回声的能力。与现有的双耳目标说话者提取方法相比，本文方法在降噪和感知质量方面表现相当，但在保留双耳线索方面具有明显优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种模仿人类选择性注意单一说话者的能力进行双耳目标说话者提取的新方法。</li>
<li>利用听者头部相关传输函数（HRTF）来隔离目标说话者的声音，无需依赖说话者嵌入，具有良好的泛化能力。</li>
<li>采用完全复数神经网络处理混合音频信号的复数短时傅里叶变换（STFT），显示出较高的性能。</li>
<li>在无声学回声和噪声的环境中评估此方法时，表现出良好的提取性能并保留了目标信号的双耳线索。</li>
<li>在具有回声的条件下评估时，该方法能维持语音清晰度和来源方向性，同时减少回声。</li>
<li>与现有双耳目标说话者提取方法相比，在降噪和感知质量方面表现相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19369">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3a046874d9935e8f5a4986c76c946292~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802064&auth_key=1759802064-0-0-090dcb4d0b4dc511418f2fe88864d889&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b7eba0f601648533514d9e25af4d973~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802071&auth_key=1759802071-0-0-08497e412dea626b59ac4920520a4b76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-0322f65685bb6548e8005bda809d1840.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-d1cb3130dc8e0118f1f51b8900d31682~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802084&auth_key=1759802084-0-0-959f9b54753a93f008803ef2a8453c9f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SPIN-ODE-Stiff-Physics-Informed-Neural-ODE-for-Chemical-Reaction-Rate-Estimation"><a href="#SPIN-ODE-Stiff-Physics-Informed-Neural-ODE-for-Chemical-Reaction-Rate-Estimation" class="headerlink" title="SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate   Estimation"></a>SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate   Estimation</h2><p><strong>Authors:Wenqing Peng, Zhi-Song Liu, Michael Boy</strong></p>
<p>Estimating rate coefficients from complex chemical reactions is essential for advancing detailed chemistry. However, the stiffness inherent in real-world atmospheric chemistry systems poses severe challenges, leading to training instability and poor convergence, which hinder effective rate coefficient estimation using learning-based approaches. To address this, we propose a Stiff Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction modelling. Our method introduces a three-stage optimisation process: first, a black-box neural ODE is trained to fit concentration trajectories; second, a Chemical Reaction Neural Network (CRNN) is pre-trained to learn the mapping between concentrations and their time derivatives; and third, the rate coefficients are fine-tuned by integrating with the pre-trained CRNN. Extensive experiments on both synthetic and newly proposed real-world datasets validate the effectiveness and robustness of our approach. As the first work addressing stiff neural ODE for chemical rate coefficient discovery, our study opens promising directions for integrating neural networks with detailed chemistry. </p>
<blockquote>
<p>从复杂的化学反应中估计速率系数对于推进详细的化学研究至关重要。然而，真实世界大气化学系统固有的刚性问题带来了严重的挑战，导致训练不稳定和收敛性差，阻碍了基于学习的速率系数估计的有效进行。为了解决这一问题，我们提出了一个用于化学反应建模的刚物理信息神经网络常微分方程（SPIN-ODE）框架。我们的方法引入了一个三阶段的优化过程：首先，训练一个黑箱神经网络常微分方程来拟合浓度轨迹；其次，预训练一个化学反应神经网络（CRNN）来学习浓度及其时间导数之间的映射关系；最后，通过与预训练的CRNN集成来微调速率系数。在合成数据和新提出的真实世界数据集上的大量实验验证了我们的方法的有效性和稳健性。作为解决针对化学速率系数发现的刚性神经常微分方程的第一项工作，我们的研究为神经网络与详细化学的融合开辟了充满希望的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05625v3">PDF</a> Accepted at the European Conference on Artificial Intelligence (ECAI)   2025</p>
<p><strong>Summary</strong></p>
<p>神经网络常微分方程（Neural ODE）结合化学动力学建模的方法被提出，用于解决真实大气化学系统刚度带来的训练不稳定和收敛性差的问题。该方法包括三个阶段：训练神经网络常微分方程以拟合浓度轨迹，预训练化学反应神经网络（CRNN）学习浓度及其时间导数的映射关系，以及通过整合预训练的CRNN对速率系数进行微调。实验证明该方法在合成和新提出真实数据集上的有效性和稳健性，为神经网络与详细化学的结合打开了新的研究路径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>真实大气化学系统的刚度给基于学习的速率系数估计带来了挑战，导致训练不稳定和收敛性差。</li>
<li>提出了一种名为SPIN-ODE的刚物理信息神经网络常微分方程框架，用于解决化学反应建模中的问题。</li>
<li>SPIN-ODE包括三个阶段的优化过程：训练神经网络常微分方程、预训练化学反应神经网络，以及微调速率系数。</li>
<li>方法通过整合预训练的CRNN对速率系数进行微调，实现更有效的学习。</li>
<li>广泛实验证明该方法在合成和真实数据集上的有效性和稳健性。</li>
<li>作为首次解决化学速率系数发现的刚神经ODE问题的研究，该研究具有开创性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05625">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1cfaf82f0ecd39838e1c9c2857e501b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802092&auth_key=1759802092-0-0-45686fdc9223616c1c627ab4f88ce4e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bd604bb454af9b738ce38aaa3f021afd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802099&auth_key=1759802099-0-0-05749ca17cbe03165891145d6f6340b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9590e894aa48458e54eeb5106b2d0c9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802106&auth_key=1759802106-0-0-bb088ce2401ad67373ac7743b3fdb109&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40c591f8f8f837c29ed278cabef5e8f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802113&auth_key=1759802113-0-0-99c30887d865feeb8d4c3b6bce20dbcd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1cf799577c3a0511aef80651f1849896~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802119&auth_key=1759802119-0-0-7d42721d97b2032761c5dcc63c892888&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Dial-In-LLM-Human-Aligned-LLM-in-the-loop-Intent-Clustering-for-Customer-Service-Dialogues"><a href="#Dial-In-LLM-Human-Aligned-LLM-in-the-loop-Intent-Clustering-for-Customer-Service-Dialogues" class="headerlink" title="Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for   Customer Service Dialogues"></a>Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for   Customer Service Dialogues</h2><p><strong>Authors:Mengze Hong, Wailing Ng, Chen Jason Zhang, Yuanfeng Song, Di Jiang</strong></p>
<p>Discovering customer intentions is crucial for automated service agents, yet existing intent clustering methods often fall short due to their reliance on embedding distance metrics and neglect of underlying semantic structures. To address these limitations, we propose an LLM-in-the-loop (LLM-ITL) intent clustering framework, integrating the language understanding capabilities of LLMs into conventional clustering algorithms. Specifically, this paper (1) examines the effectiveness of fine-tuned LLMs in semantic coherence evaluation and intent cluster naming, achieving over 95% accuracy aligned with human judgments; (2) designs an LLM-ITL framework that facilitates the iterative discovery of coherent intent clusters and the optimal number of clusters; and (3) introduces context-aware techniques tailored for customer service dialogue. Since existing English benchmarks lack sufficient semantic diversity and intent coverage, we further present a comprehensive Chinese dialogue intent dataset comprising over 100k real customer service calls with 1,507 human-annotated clusters. The proposed approaches significantly outperform LLM-guided baselines, achieving notable improvements in clustering quality, cost efficiency, and downstream applications. Combined with several best practices, our findings highlight the prominence of LLM-in-the-loop techniques for scalable dialogue data mining. </p>
<blockquote>
<p>发现客户意图对于自动服务代理至关重要，但现有的意图聚类方法往往因依赖嵌入距离度量而忽视潜在语义结构而表现不足。为了解决这些局限性，我们提出了一种LLM-in-the-loop（LLM-ITL）意图聚类框架，将大型语言模型的语言理解能力集成到传统的聚类算法中。具体来说，本文（1）研究了微调后的LLMs在语义连贯性评估和意图聚类命名中的有效性，其准确率与人类判断对齐超过95%；（2）设计了一个LLM-ITL框架，便于发现连贯的意图簇和最优簇数；（3）引入了面向客户服务对话的上下文感知技术。由于现有的英语基准测试缺乏足够的语义多样性和意图覆盖，我们进一步提供了一个全面的中文对话意图数据集，包含超过10万条真实的客户服务呼叫记录，包含1507个人工标注的簇。所提出的方法显著优于LLM引导的基础线方法，在聚类质量、成本效率和下游应用方面取得了显著的改进。结合若干最佳实践，我们的研究凸显了LLM-in-the-loop技术在可扩展对话数据挖掘中的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09049v4">PDF</a> Accepted by EMNLP 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>基于现有的意图聚类方法在语义理解上的不足，本文提出了一种结合了大型语言模型（LLM）的意图聚类框架——LLM-in-the-loop（LLM-ITL）。该框架不仅利用LLM的语言理解能力来提升传统聚类算法的效果，还针对客户服务对话引入了语境感知技术。此外，为了弥补现有英语基准测试在语义多样性和意图覆盖上的不足，本研究还推出了一份包含10万条以上真实客户服务通话和1507个人工标注聚类的中文对话意图数据集。研究表明，LLM-ITL方法在聚类质量、成本效益和下游应用方面均显著优于LLM引导的基础方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-ITL框架结合了LLM的语言理解能力，提升了传统意图聚类算法的效果。</li>
<li>LLM-ITL框架能够迭代发现连贯的意图簇和最佳簇数量。</li>
<li>针对客户服务对话，引入了语境感知技术。</li>
<li>现有的英语基准测试在语义多样性和意图覆盖方面存在不足。</li>
<li>推出了一份包含超过10万条真实客户服务通话的中文对话意图数据集。</li>
<li>LLM-ITL方法在聚类质量、成本效益和下游应用方面表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09049">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9473ceb830ce303ec933b09f315152c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802127&auth_key=1759802127-0-0-9efdc1eb5a1bd1a67738dca9cc67db21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6ead2cb5ad080503198b58dc554a006~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802134&auth_key=1759802134-0-0-87988df112885b77a1f29bbb2ca64cdc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3ae1054a115fd0cdf3597b0d8caa4484~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802141&auth_key=1759802141-0-0-24443e54cbc6910d242f576cf2b020f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-c942d7d1283ed11034322e6f6351e0a4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="From-Intents-to-Conversations-Generating-Intent-Driven-Dialogues-with-Contrastive-Learning-for-Multi-Turn-Classification"><a href="#From-Intents-to-Conversations-Generating-Intent-Driven-Dialogues-with-Contrastive-Learning-for-Multi-Turn-Classification" class="headerlink" title="From Intents to Conversations: Generating Intent-Driven Dialogues with   Contrastive Learning for Multi-Turn Classification"></a>From Intents to Conversations: Generating Intent-Driven Dialogues with   Contrastive Learning for Multi-Turn Classification</h2><p><strong>Authors:Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim</strong></p>
<p>In conversational AI systems, a critical challenge in training effective multi-turn intent classification models lies in the generation of large-scale, domain-specific, multilingual dialogue datasets. In this paper, we introduce Chain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs) with Large Language Models (LLMs) to generate intent-driven, context-aware dialogues through self-play. Our method first extracts domain-specific intent transition patterns from real-world e-commerce chat logs, which guide the modeling of turn-level dynamics and intent sequences. LLMs are then employed to parameterize the emission probabilities of HMMs, enabling the generation of natural, coherent utterances aligned with predicted intents and dialogue context. We also propose MINT-CL, a multi-task contrastive learning framework for multi-turn intent classification, which improves performance while reducing dependence on large-scale annotated datasets. Empirical results demonstrate that our approach outperforms competitive baselines in dialogue generation quality and classification accuracy, particularly in multilingual settings. To facilitate future research, we release MINT-E, a comprehensive, multilingual, intent-aware multi-turn dialogue corpus derived from the e-commerce domain\footnote{The reproduced source code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/junhua/chain-of-intent">https://github.com/junhua/chain-of-intent</a>. </p>
<blockquote>
<p>在对话式AI系统中，训练有效的多轮意图分类模型的关键挑战在于生成大规模、特定领域的多语言对话数据集。在本文中，我们介绍了Intent链，这是一个新型框架，它将隐马尔可夫模型（HMMs）与大型语言模型（LLMs）相结合，通过自我对抗生成意图驱动、意识形态的对话。我们的方法首先从现实世界中的电子商务聊天日志中提取特定领域的意图转换模式，这些模式引导轮级动态和意图序列的建模。然后，我们采用大型语言模型对隐马尔可夫模型的发射概率进行参数化，使生成的对话符合预期的意图和对话背景的自然连贯性。我们还提出了MINT-CL，一个用于多轮意图分类的多任务对比学习框架，它在提高性能的同时减少了大规模标注数据集的依赖。经验结果表明，我们的方法在对话生成质量和分类准确性方面优于竞争对手的基线方法，特别是在多语言环境中。为了促进未来的研究，我们发布了MINT-E，一个全面的、多语言的、意图感知的多轮对话语料库，该语料库来源于电子商务领域。（注：复现源代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/junhua/chain-of-intent%E6%89%BE%E5%88%B0%E3%80%82%EF%BC%89">https://github.com/junhua/chain-of-intent找到。）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14252v3">PDF</a> Accepted to Proceedings of CIKM’25</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为Chain-of-Intent的新框架，该框架结合了隐马尔可夫模型（HMMs）与大型语言模型（LLMs），通过自我博弈生成意图驱动、语境感知的对话。此外，还提出了MINT-CL，一个多任务对比学习框架，用于多轮意图分类，可提高性能并减少对大规模标注数据集的依赖。实验结果表明，该方法在对话生成质量和分类准确性方面优于竞争对手，特别是在多语种环境中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chain-of-Intent框架结合了隐马尔可夫模型（HMMs）与大型语言模型（LLMs），以生成与语境相关的对话。</li>
<li>提出MINT-CL框架，用于多轮意图分类，提高性能并减少对大规模标注数据集的依赖。</li>
<li>使用真实电商聊天日志中的意图转换模式来指导轮次级别动态和意图序列的建模。</li>
<li>LLMs被用来参数化HMMs的发射概率，以生成与预测意图和对话语境相符的自然、连贯的语句。</li>
<li>框架在对话生成质量和分类准确性方面表现出优异的性能，特别是在多语种环境中。</li>
<li>释放了MINT-E数据集，这是一个从电商领域衍生的综合、多语种、意图感知的多轮对话语料库。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14252">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-70cd5b797d5129fa49311a906589f475.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05de018233719bc29bc298f5f389a098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-678e9d769f455aa330e855876213d2b5.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a9776c6c76a2a52ad0fb6916d6dc80c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802175&auth_key=1759802175-0-0-9a00f09d4ea21df4f015ee414547835b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a6d15254d981e0cd40ba832ae405ed6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802181&auth_key=1759802181-0-0-bc65d7b7ae9bd92dec172729aed76c55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Talk-Listen-Connect-How-Humans-and-AI-Evaluate-Empathy-in-Responses-to-Emotionally-Charged-Narratives"><a href="#Talk-Listen-Connect-How-Humans-and-AI-Evaluate-Empathy-in-Responses-to-Emotionally-Charged-Narratives" class="headerlink" title="Talk, Listen, Connect: How Humans and AI Evaluate Empathy in Responses   to Emotionally Charged Narratives"></a>Talk, Listen, Connect: How Humans and AI Evaluate Empathy in Responses   to Emotionally Charged Narratives</h2><p><strong>Authors:Mahnaz Roshanaei, Rezvaneh Rezapour, Magy Seif El-Nasr</strong></p>
<p>Social interactions promote well-being, yet barriers like geographic distance, time limitations, and mental health conditions can limit face-to-face interactions. Emotionally responsive AI systems, such as chatbots, offer new opportunities for social and emotional support, but raise critical questions about how empathy is perceived and experienced in human-AI interactions. This study examines how empathy is evaluated in AI-generated versus human responses. Using personal narratives, we explored how persona attributes (e.g., gender, empathic traits, shared experiences) and story qualities affect empathy ratings. We compared responses from standard and fine-tuned AI models with human judgments. Results show that while humans are highly sensitive to emotional vividness and shared experience, AI-responses are less influenced by these cues, often lack nuance in empathic expression. These findings highlight challenges in designing emotionally intelligent systems that respond meaningfully across diverse users and contexts, and informs the design of ethically aware tools to support social connection and well-being. </p>
<blockquote>
<p>社会互动有助于提升福祉，然而地理距离、时间限制和心理健康状况等障碍可能会限制面对面的互动。情绪反应型的人工智能系统，如聊天机器人，提供了新的社交和情感支持的机会，但由此引发了关于人机交互中同情如何被感知和体验的重要问题。本研究旨在调查人工智能生成的反应与人类反应中的同情评价如何。通过个人叙事的方式，我们探讨了人格属性（如性别、移情特质、共享经验）和故事质量如何影响移情评分。我们将标准的人工智能模型和经过微调的人工智能模型的反应与人类的判断进行了比较。结果表明，虽然人类对情绪生动性和共享经验的敏感性很高，但人工智能的反应受到这些线索的影响较小，往往缺乏微妙的移情表达。这些发现突显了在设计和开发有意义的情绪智能系统方面所面临的挑战，这些系统能够在多样化的用户和情境背景下作出反应，并提示我们设计出有道德意识的工具来支持社交联系和福祉。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15550v2">PDF</a> 21 pages, 4 figures, 6 tables. Title updated from “Talk, Listen,   Connect: Navigating Empathy in Human-AI Interactions” to “Talk, Listen,   Connect: How Humans and AI Evaluate Empathy in Responses to Emotionally   Charged Narratives” in this version. This is version 2 (v2) of the paper. All   previous citations of arXiv:2409.15550 with the old title still refer to the   same paper</p>
<p><strong>Summary</strong><br>     社交互动有助于提升幸福感，但地理距离、时间限制和心理健康状况等障碍限制了面对面的交流。情感响应的AI系统如聊天机器人提供了新的社交和情感支持的机会，但也引发了关于人类与AI互动中如何感知和体验同情心的关键问题。本研究探讨了AI生成响应与人类响应中的同情心如何评估。通过个人叙事，我们探讨了人格属性（如性别、同情特质、共同经历）和故事品质对同情心评价的影响。我们将标准AI模型和经过微调后的AI模型的反应与人类的判断进行了比较。结果表明，虽然人类对情感生动性和共同经历的敏感性很高，但AI的反应受这些线索的影响较小，往往缺乏微妙的同理心表达。这些发现突出了在设计能够适应不同用户和上下文环境的智能系统方面面临的挑战，也为支持社会联系和幸福感的道德意识工具的设计提供了信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社交互动对提升幸福感至关重要，但存在地理、时间等障碍限制面对面的交流。</li>
<li>情感响应的AI系统如聊天机器人可为社交和情感支持提供新机会。</li>
<li>本研究探讨了AI生成响应与人类响应中同情心评估的差异。</li>
<li>人格属性和故事品质影响同情心评价。</li>
<li>AI对于情感生动性和共同经历的敏感性较低，缺乏微妙的同理心表达。</li>
<li>在设计智能系统时，需要考虑到适应不同用户和上下文环境的挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15550">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bccac1779a86eb1b5eaebc2e31f1cec0.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-956b2fd9c1e9fcd3c5f38c31e14ea825~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802196&auth_key=1759802196-0-0-92402548d47e07309dfa8e0d1ed0c191&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8be3d509100261e1c7c293545aba3251~resize:0:q75.jpg?source=1f5c5e47&expiration=1759802203&auth_key=1759802203-0-0-8d069ab0c53339c3fe7c47eba5c41c89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ed80373b4c0561cfe6db59597653c368.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-09-08  Embracing Aleatoric Uncertainty Generating Diverse 3D Human Motion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-41abcaaf44cd5b6288db938b78329f6c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-09-08  AUDETER A Large-scale Dataset for Deepfake Audio Detection in Open   Worlds
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
