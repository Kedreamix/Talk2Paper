<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  SSGaussian Semantic-Aware and Structure-Preserving 3D Style Transfer">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-cf5693abbe3c3c025881e809c9791603.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-29
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    63 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="SSGaussian-Semantic-Aware-and-Structure-Preserving-3D-Style-Transfer"><a href="#SSGaussian-Semantic-Aware-and-Structure-Preserving-3D-Style-Transfer" class="headerlink" title="SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer"></a>SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</h2><p><strong>Authors:Jimin Xu, Bosheng Qin, Tao Jin, Zhou Zhao, Zhenhui Ye, Jun Yu, Fei Wu</strong></p>
<p>Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page <a target="_blank" rel="noopener" href="https://jm-xu.github.io/SSGaussian">https://jm-xu.github.io/SSGaussian</a> for immersive visualization. </p>
<blockquote>
<p>è¿‘æœŸç¥ç»è¡¨å¾é¢†åŸŸçš„è¿›å±•ï¼Œå¦‚ç¥ç»è¾å°„åœºå’Œ3Dé«˜æ–¯æ‹¼è´´ï¼Œå¢åŠ äº†å°†é£æ ¼è¿ç§»åº”ç”¨äº3Dåœºæ™¯çš„å…´è¶£ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•èƒ½å¤Ÿå°†é£æ ¼æ¨¡å¼è½¬ç§»åˆ°ä¸€è‡´çš„3Dç¥ç»è¡¨å¾ä¸Šï¼Œä½†å®ƒä»¬éš¾ä»¥æœ‰æ•ˆåœ°ä»å‚è€ƒé£æ ¼å›¾åƒä¸­æå–å¹¶è½¬ç§»é«˜çº§é£æ ¼è¯­ä¹‰ã€‚æ­¤å¤–ï¼Œé£æ ¼åŒ–çš„ç»“æœé€šå¸¸ç¼ºä¹ç»“æ„æ¸…æ™°åº¦å’Œåˆ†ç¦»åº¦ï¼Œä½¿å¾—éš¾ä»¥åœ¨3Dåœºæ™¯ä¸­åŒºåˆ†ä¸åŒçš„å®ä¾‹æˆ–å¯¹è±¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„3Dé£æ ¼è¿ç§»ç®¡é“ï¼Œè¯¥ç®¡é“æœ‰æ•ˆåœ°ç»“åˆäº†æ¥è‡ªé¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬çš„ç®¡é“ç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£å…ˆéªŒçŸ¥è¯†ç”Ÿæˆå…³é”®è§†è§’çš„é£æ ¼åŒ–æ¸²æŸ“ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†é£æ ¼åŒ–çš„å…³é”®è§†å›¾è½¬ç§»åˆ°3Dè¡¨å¾ä¸Šã€‚è¿™ä¸ªè¿‡ç¨‹èå…¥äº†ä¸¤ç§åˆ›æ–°è®¾è®¡ã€‚ç¬¬ä¸€ç§æ˜¯è·¨è§†å›¾é£æ ¼å¯¹é½ï¼Œå®ƒå°†è·¨è§†å›¾æ³¨æ„åŠ›æ’å…¥åˆ°UNetçš„æœ€åä¸€ä¸ªä¸Šé‡‡æ ·å—ä¸­ï¼Œå…è®¸è·¨å¤šä¸ªå…³é”®è§†å›¾è¿›è¡Œç‰¹å¾äº¤äº’ã€‚è¿™ç¡®ä¿æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é£æ ¼åŒ–å…³é”®è§†å›¾æ—¢ä¿æŒé£æ ¼å¿ å®åº¦åˆä¿æŒå®ä¾‹çº§ä¸€è‡´æ€§ã€‚ç¬¬äºŒç§æ˜¯å®ä¾‹çº§é£æ ¼è¿ç§»ï¼Œå®ƒæœ‰æ•ˆåœ°åˆ©ç”¨é£æ ¼åŒ–å…³é”®è§†å›¾ä¹‹é—´çš„å®ä¾‹çº§ä¸€è‡´æ€§å¹¶å°†å…¶è½¬ç§»åˆ°3Dè¡¨å¾ä¸Šã€‚è¿™å¯¼è‡´äº†ä¸€ç§ç»“æ„æ›´ç´§å‡‘ã€è§†è§‰è¿è´¯ä¸”è‰ºæœ¯æ„Ÿå¢å¼ºçš„é£æ ¼åŒ–ã€‚å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„3Dé£æ ¼è¿ç§»ç®¡é“åœ¨å¹¿æ³›çš„åœºæ™¯ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ­£é¢é¢å¯¹åˆ°å…·æœ‰æŒ‘æˆ˜æ€§çš„360åº¦ç¯å¢ƒã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ <a target="_blank" rel="noopener" href="https://jm-xu.github.io/SSGaussian">https://jm-xu.github.io/SSGaussian</a> ä½“éªŒæ²‰æµ¸å¼å¯è§†åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04379v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¥ç»ç½‘ç»œè¡¨ç°çš„æ–°è¿›å±•ï¼Œå¦‚ç¥ç»è¾å°„åœºå’Œ3Dé«˜æ–¯å–·ç»˜ï¼Œé£æ ¼è½¬ç§»åœ¨3Dåœºæ™¯ä¸­çš„åº”ç”¨å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•åœ¨å°†é£æ ¼æ¨¡å¼è½¬ç§»åˆ°3Dä¸€è‡´çš„ç¥ç»ç½‘ç»œè¡¨ç°æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥æœ‰æ•ˆæå–å’Œè½¬ç§»æ¥è‡ªå‚è€ƒé£æ ¼å›¾åƒçš„é«˜çº§é£æ ¼è¯­ä¹‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dé£æ ¼è½¬ç§»ç®¡é“ï¼Œè¯¥ç®¡é“æœ‰æ•ˆåœ°ç»“åˆäº†æ¥è‡ªé¢„è®­ç»ƒ2Dæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚æ­¤ç®¡é“åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£å…ˆéªŒç”Ÿæˆå…³é”®è§†è§’çš„ä¸ªæ€§åŒ–æ¸²æŸ“ï¼›ç„¶åï¼Œå°†è¿™äº›ä¸ªæ€§åŒ–å…³é”®è§†è§’è½¬ç§»åˆ°3Dè¡¨ç°ã€‚æ­¤è¿‡ç¨‹ä¸­èå…¥äº†ä¸¤ä¸ªåˆ›æ–°è®¾è®¡ï¼šä¸€æ˜¯è·¨è§†å›¾é£æ ¼å¯¹é½ï¼Œå°†è·¨è§†å›¾æ³¨æ„åŠ›æ’å…¥UNetçš„æœ€åä¸€ä¸ªä¸Šé‡‡æ ·å—ä¸­ï¼Œå…è®¸è·¨å¤šä¸ªå…³é”®è§†å›¾çš„ç‰¹å¾äº¤äº’ã€‚è¿™ç¡®ä¿æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ä¸ªæ€§åŒ–å…³é”®è§†å›¾æ—¢ä¿æŒé£æ ¼å¿ å®åº¦åˆä¿æŒå®ä¾‹çº§åˆ«çš„ä¸€è‡´æ€§ã€‚äºŒæ˜¯å®ä¾‹çº§åˆ«é£æ ¼è½¬ç§»ï¼Œæœ‰æ•ˆåˆ©ç”¨ä¸ªæ€§åŒ–å…³é”®è§†å›¾ä¹‹é—´çš„å®ä¾‹çº§åˆ«ä¸€è‡´æ€§å¹¶å°†å…¶è½¬ç§»åˆ°3Dè¡¨ç°ä¸Šã€‚è¿™äº§ç”Ÿäº†æ›´å…·ç»“æ„åŒ–ã€è§†è§‰è¿è´¯å’Œè‰ºæœ¯æ€§çš„ä¸ªæ€§åŒ–ç»“æœã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„3Dé£æ ¼è½¬ç§»ç®¡é“åœ¨å¤šç§åœºæ™¯ä¸­éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬æ­£é¢å’Œ360åº¦ç¯å¢ƒç­‰æŒ‘æˆ˜åœºæ™¯ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢è¿›è¡Œæ²‰æµ¸å¼å¯è§†åŒ–ä½“éªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œè¡¨ç°çš„æ–°æŠ€æœ¯å¦‚ç¥ç»è¾å°„åœºå’Œ3Dé«˜æ–¯å–·ç»˜æ¨åŠ¨äº†é£æ ¼è½¬ç§»åœ¨3Dåœºæ™¯ä¸­çš„åº”ç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨é£æ ¼è½¬ç§»æ—¶éš¾ä»¥æœ‰æ•ˆæå–å’Œè½¬ç§»é«˜çº§é£æ ¼è¯­ä¹‰ã€‚</li>
<li>æå‡ºçš„3Dé£æ ¼è½¬ç§»ç®¡é“ç»“åˆé¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œç”Ÿæˆä¸ªæ€§åŒ–çš„å…³é”®è§†è§’æ¸²æŸ“ã€‚</li>
<li>ç®¡é“åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç”Ÿæˆä¸ªæ€§åŒ–å…³é”®è§†è§’å’Œå°†å…¶è½¬ç§»åˆ°3Dè¡¨ç°ã€‚</li>
<li>åˆ›æ–°è®¾è®¡åŒ…æ‹¬è·¨è§†å›¾é£æ ¼å¯¹é½å’Œå®ä¾‹çº§åˆ«é£æ ¼è½¬ç§»ï¼Œç¡®ä¿ä¸ªæ€§åŒ–å’Œå®ä¾‹ä¸€è‡´æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬æ­£é¢å’Œ360åº¦ç¯å¢ƒç­‰æŒ‘æˆ˜åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ab201b9002d3043f3c2fa757a2486aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51fbc68971f9c30676eea8e9113a3621.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa55afde79abd9b66db45961165ae60b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5209a07d609d95b6437c0e0fb21acab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f2fc2a761bda7a794279a51053eaaf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0820592e385239ef4b2825c3a2a9c03.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Unifi3D-A-Study-on-3D-Representations-for-Generation-and-Reconstruction-in-a-Common-Framework"><a href="#Unifi3D-A-Study-on-3D-Representations-for-Generation-and-Reconstruction-in-a-Common-Framework" class="headerlink" title="Unifi3D: A Study on 3D Representations for Generation and Reconstruction   in a Common Framework"></a>Unifi3D: A Study on 3D Representations for Generation and Reconstruction   in a Common Framework</h2><p><strong>Authors:Nina Wiedemann, Sainan Liu, Quentin Leboutet, Katelyn Gao, Benjamin Ummenhofer, Michael Paulitsch, Kai Yuan</strong></p>
<p>Following rapid advancements in text and image generation, research has increasingly shifted towards 3D generation. Unlike the well-established pixel-based representation in images, 3D representations remain diverse and fragmented, encompassing a wide variety of approaches such as voxel grids, neural radiance fields, signed distance functions, point clouds, or octrees, each offering distinct advantages and limitations. In this work, we present a unified evaluation framework designed to assess the performance of 3D representations in reconstruction and generation. We compare these representations based on multiple criteria: quality, computational efficiency, and generalization performance. Beyond standard model benchmarking, our experiments aim to derive best practices over all steps involved in the 3D generation pipeline, including preprocessing, mesh reconstruction, compression with autoencoders, and generation. Our findings highlight that reconstruction errors significantly impact overall performance, underscoring the need to evaluate generation and reconstruction jointly. We provide insights that can inform the selection of suitable 3D models for various applications, facilitating the development of more robust and application-specific solutions in 3D generation. The code for our framework is available at <a target="_blank" rel="noopener" href="https://github.com/isl-org/unifi3d">https://github.com/isl-org/unifi3d</a>. </p>
<blockquote>
<p>éšç€æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆçš„å¿«é€Ÿå‘å±•ï¼Œç ”ç©¶è¶Šæ¥è¶Šå¤šåœ°è½¬å‘3Dç”Ÿæˆã€‚ä¸å›¾åƒä¸­å·²å»ºç«‹åŸºäºåƒç´ çš„è¡¨ç¤ºä¸åŒï¼Œ3Dè¡¨ç¤ºä»ç„¶å¤šæ ·ä¸”åˆ†æ•£ï¼Œæ¶µç›–äº†è¯¸å¦‚ä½“ç´ ç½‘æ ¼ã€ç¥ç»è¾å°„åœºã€æœ‰å‘è·ç¦»å‡½æ•°ã€ç‚¹äº‘æˆ–å…«å‰æ ‘ç­‰å¤šç§æ–¹æ³•ï¼Œæ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°3Dè¡¨ç¤ºåœ¨é‡å»ºå’Œç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬åŸºäºå¤šä¸ªæ ‡å‡†å¯¹è¿™äº›è¡¨ç¤ºè¿›è¡Œæ¯”è¾ƒï¼šè´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–æ€§èƒ½ã€‚é™¤äº†æ ‡å‡†æ¨¡å‹åŸºå‡†æµ‹è¯•å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¿˜æ—¨åœ¨å¾—å‡ºæ¶‰åŠæ•´ä¸ª3Dç”Ÿæˆç®¡é“æ‰€æœ‰æ­¥éª¤çš„æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬é¢„å¤„ç†ã€ç½‘æ ¼é‡å»ºã€ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨çš„å‹ç¼©å’Œç”Ÿæˆã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡å»ºè¯¯å·®å¯¹æ•´ä½“æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ï¼Œå¼ºè°ƒéœ€è¦è”åˆè¯„ä¼°å’Œç”Ÿæˆå’Œé‡å»ºã€‚æˆ‘ä»¬æä¾›çš„è§è§£å¯ä»¥ä¸ºå„ç§åº”ç”¨é€‰æ‹©åˆé€‚çš„ä¸‰ç»´æ¨¡å‹æä¾›ä¿¡æ¯ï¼Œä¿ƒè¿›å¼€å‘æ›´ç¨³å¥å’Œé’ˆå¯¹ç‰¹å®šåº”ç”¨çš„è§£å†³æ–¹æ¡ˆåœ¨ä¸‰ç»´ç”Ÿæˆä¸­ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/isl-org/unifi3d">https://github.com/isl-org/unifi3d</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02474v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆçš„å¿«é€Ÿå‘å±•ï¼Œç ”ç©¶é€æ¸è½¬å‘3Dç”Ÿæˆã€‚å½“å‰å­˜åœ¨å¤šç§ä¸åŒçš„3Dè¡¨ç¤ºæ–¹æ³•ï¼Œå¦‚ä½“ç´ ç½‘æ ¼ã€ç¥ç»è¾å°„åœºã€å¸¦ç¬¦å·è·ç¦»å‡½æ•°ç­‰ï¼Œå„æœ‰å…¶ç‹¬ç‰¹ä¼˜åŠ¿ä¸å±€é™ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒ3Dè¡¨ç¤ºåœ¨é‡å»ºå’Œç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚é€šè¿‡å®éªŒå¯¹æ¯”ï¼Œæœ¬æ–‡å¼ºè°ƒé‡å»ºè¯¯å·®å¯¹æ•´ä½“æ€§èƒ½çš„å½±å“ï¼Œå¹¶å¼ºè°ƒè”åˆè¯„ä¼°ç”Ÿæˆå’Œé‡å»ºçš„é‡è¦æ€§ã€‚æœ¬æ–‡æä¾›äº†å…³äºé€‰æ‹©é€‚åˆä¸åŒåº”ç”¨çš„æœ€ä½³3Dæ¨¡å‹çš„è§è§£ï¼Œä¿ƒè¿›äº†æ›´ç¨³å¥ã€ç‰¹å®šåº”ç”¨çš„3Dç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç ”ç©¶è¶‹åŠ¿è½¬å‘ä¸‰ç»´ç”Ÿæˆé¢†åŸŸï¼Œå±•ç¤ºäº†å¤šæ ·åŒ–çš„ä¸‰ç»´è¡¨ç¤ºæ–¹æ³•åŠå…¶ä¼˜ç¼ºç‚¹ã€‚</li>
<li>æå‡ºä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°ä¸åŒä¸‰ç»´è¡¨ç¤ºæ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒå¯¹æ¯”äº†å„ç§ä¸‰ç»´è¡¨ç¤ºåœ¨é‡å»ºå’Œç”Ÿæˆæ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>é‡å»ºè¯¯å·®å¯¹æ•´ä½“æ€§èƒ½å…·æœ‰æ˜¾è‘—å½±å“ï¼Œéœ€è¦è”åˆè¯„ä¼°ç”Ÿæˆå’Œé‡å»ºã€‚</li>
<li>æä¾›äº†å…³äºé€‰æ‹©é€‚åˆä¸åŒåº”ç”¨çš„æœ€ä½³ä¸‰ç»´æ¨¡å‹çš„è§è§£ã€‚</li>
<li>æœ‰åŠ©äºå¼€å‘æ›´ç¨³å¥ã€ç‰¹å®šåº”ç”¨çš„è§£å†³æ–¹æ¡ˆåœ¨ä¸‰ç»´ç”Ÿæˆé¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2d34c6cc1dbfe09f558c88fdb4137569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0af980df3f2e466a93a8d037428da5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4665632bf3f72d69193d84a46e68012f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Spectrogram-Patch-Codec-A-2D-Block-Quantized-VQ-VAE-and-HiFi-GAN-for-Neural-Speech-Coding"><a href="#Spectrogram-Patch-Codec-A-2D-Block-Quantized-VQ-VAE-and-HiFi-GAN-for-Neural-Speech-Coding" class="headerlink" title="Spectrogram Patch Codec: A 2D Block-Quantized VQ-VAE and HiFi-GAN for   Neural Speech Coding"></a>Spectrogram Patch Codec: A 2D Block-Quantized VQ-VAE and HiFi-GAN for   Neural Speech Coding</h2><p><strong>Authors:Luis Felipe Chary, Miguel Arjona Ramirez</strong></p>
<p>We present a neural speech codec that challenges the need for complex residual vector quantization (RVQ) stacks by introducing a simpler, single-stage quantization approach. Our method operates directly on the mel-spectrogram, treating it as a 2D data and quantizing non-overlapping 4x4 patches into a single, shared codebook. This patchwise design simplifies the architecture, enables low-latency streaming, and yields a discrete latent grid. To ensure high-fidelity synthesis, we employ a late-stage adversarial fine-tuning for the VQ-VAE and train a HiFi-GAN vocoder from scratch on the codecâ€™s reconstructed spectrograms. Operating at approximately 7.5 kbits&#x2F;s for 16 kHz speech, our system was evaluated against several state-of-the-art neural codecs using objective metrics such as STOI, PESQ, MCD, and ViSQOL. The results demonstrate that our simplified, non-residual architecture achieves competitive perceptual quality and intelligibility, validating it as an effective and open foundation for future low-latency codec designs. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¥ç»è¯­éŸ³ç¼–è§£ç å™¨ï¼Œé€šè¿‡å¼•å…¥ä¸€ç§æ›´ç®€å•çš„ä¸€é˜¶æ®µé‡åŒ–æ–¹æ³•ï¼ŒæŒ‘æˆ˜äº†å¤æ‚çš„å‰©ä½™çŸ¢é‡é‡åŒ–ï¼ˆRVQï¼‰å †æ ˆçš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥åœ¨æ¢…å°”é¢‘è°±å›¾ä¸Šæ“ä½œï¼Œå°†å…¶è§†ä¸ºäºŒç»´æ•°æ®ï¼Œå¹¶å°†éé‡å çš„4x4å—é‡åŒ–åˆ°ä¸€ä¸ªå•ä¸€çš„å…±äº«ä»£ç ç°¿ä¸­ã€‚è¿™ç§å—çº§è®¾è®¡ç®€åŒ–äº†æ¶æ„ï¼Œå®ç°äº†ä½å»¶è¿Ÿæµï¼Œå¹¶äº§ç”Ÿäº†ç¦»æ•£æ½œåœ¨ç½‘æ ¼ã€‚ä¸ºäº†ç¡®ä¿é«˜ä¿çœŸåˆæˆï¼Œæˆ‘ä»¬å¯¹VQ-VAEé‡‡ç”¨åæœŸå¯¹æŠ—å¾®è°ƒï¼Œä»å¤´å¼€å§‹è®­ç»ƒHiFi-GANç¼–ç å™¨è§£ç å™¨çš„é‡å»ºé¢‘è°±å›¾ã€‚å¯¹äº16kHzçš„è¯­éŸ³ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å¤§çº¦7.5kbits&#x2F;sçš„é€Ÿç‡ä¸‹è¿è¡Œï¼Œé‡‡ç”¨STOIã€PESQã€MCDå’ŒViSQOLç­‰å®¢è§‚æŒ‡æ ‡å¯¹ç³»ç»Ÿè¿›è¡Œè¯„ä¼°ï¼Œä¸å‡ ç§å…ˆè¿›çš„ç¥ç»ç¼–è§£ç å™¨è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬è¿™ç§ç®€åŒ–è€Œéå‰©ä½™ç»“æ„çš„ç¼–è§£ç å™¨è¾¾åˆ°äº†å…·æœ‰ç«äº‰åŠ›çš„æ„ŸçŸ¥è´¨é‡å’Œæ¸…æ™°åº¦ï¼ŒéªŒè¯äº†å®ƒä½œä¸ºæœªæ¥ä½å»¶è¿Ÿç¼–è§£ç å™¨è®¾è®¡çš„æœ‰æ•ˆå’Œå¼€æ”¾åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02244v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç¥ç»ç½‘ç»œè¯­éŸ³ç¼–è§£ç å™¨ï¼Œå®ƒé‡‡ç”¨ç®€å•çš„å•é˜¶æ®µé‡åŒ–æ–¹æ³•ï¼ŒæŒ‘æˆ˜äº†å¤æ‚çš„æ®‹å·®çŸ¢é‡é‡åŒ–ï¼ˆRVQï¼‰å †æ ˆçš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•ç›´æ¥åœ¨æ¢…å°”é¢‘è°±å›¾ä¸Šæ“ä½œï¼Œå°†å…¶è§†ä¸ºäºŒç»´æ•°æ®ï¼Œå¹¶å°†éé‡å çš„4x4å—é‡åŒ–åˆ°å•ä¸ªå…±äº«ä»£ç ç°¿ä¸­ã€‚è¿™ç§å—çº§è®¾è®¡ç®€åŒ–äº†æ¶æ„ï¼Œå®ç°äº†ä½å»¶è¿Ÿæµï¼Œå¹¶äº§ç”Ÿäº†ç¦»æ•£æ½œåœ¨ç½‘æ ¼ã€‚ä¸ºç¡®ä¿é«˜ä¿çœŸåˆæˆï¼Œæœ¬æ–‡é‡‡ç”¨æ™šæœŸå¯¹æŠ—å¾®è°ƒæŠ€æœ¯ä¼˜åŒ–VQ-VAEï¼Œå¹¶ä½¿ç”¨HiFi-GANä»å¤´è®­ç»ƒç¼–è§£ç å™¨çš„é‡å»ºé¢‘è°±å›¾ã€‚åœ¨å¤§çº¦é’ˆå¯¹çš„æ˜¯è¾“å…¥é‡‡æ ·ç‡ä¸ºå¯¹é‡‡ç”¨æœ‰ç€é€‚ç”¨äºé«˜é€Ÿçš„ç½‘ç»œæ•°æ®åŒ…ç­‰çš„åŸºäºç™½ç‰¹å¾çš„é«˜åº¦æ— åºé›†æˆæ´»åŠ¨åº“çš„å¹¿å‘ŠåŠ¨æ€é…ç½®ç¼“å­˜çš„ä¼˜åŒ–æƒ…å†µä¸ç¡®å®šé€Ÿç‡å˜åŒ–çš„è‡ªåŠ¨å»ºæ¨¡è¯­è¨€çš„ç†è§£å­¦ä¹ æ–¹é¢çš„å­¦ä¹ è¿‡ç¨‹çš„æµç•…æ€§å’Œä¾¿åˆ©æ€§ä½¿ç”¨ç‰¹å®šçš„è¯­æ–™æ ·æœ¬æ„å»ºä»»åŠ¡é›†åˆçš„æ•°æ®æ ‡æ³¨æ“ä½œè¡¨ç°ä¸‹è¿›è¡Œç¥ç»ç½‘ç»œå»ºæ¨¡ç¼–ç æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œå¯¹äºé«˜è¾¾ä¸€å®šå­—èŠ‚é€Ÿç‡çš„éŸ³é¢‘ç¼–ç å™¨è®¾è®¡ï¼Œå…¶å¯å®ç°ä¼˜å¼‚çš„æ„ŸçŸ¥è´¨é‡å’Œå¯ç†è§£æ€§ï¼Œè¯æ˜äº†å…¶ä½œä¸ºæœªæ¥ä½å»¶è¿Ÿç¼–ç å™¨è®¾è®¡çš„æœ‰æ•ˆå’Œå¼€æ”¾åŸºç¡€çš„ä»·å€¼ã€‚ç³»ç»Ÿå¯¹ä¸åŒçš„ç¥ç»ç½‘ç»œç¼–è§£ç å™¨è¿›è¡Œäº†è¯„ä¼°æ¯”è¾ƒå®¢è§‚æŒ‡æ ‡åŒ…æ‹¬STOIã€PESQã€MCDå’ŒViSQOLç­‰è¯„ä¼°ç»“æœè¯æ˜ç®€åŒ–éæ®‹å·®æ¶æ„åœ¨æ„ŸçŸ¥è´¨é‡å’Œå¯ç†è§£æ€§æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œè¯­éŸ³ç¼–è§£ç å™¨æ–¹æ³•ï¼Œé‡‡ç”¨ç®€å•çš„å•é˜¶æ®µé‡åŒ–æ–¹æ³•ï¼ŒæŒ‘æˆ˜äº†å¤æ‚RVQæ¶æ„çš„éœ€æ±‚ã€‚<br>äºŒã€ç›´æ¥åœ¨æ¢…å°”é¢‘è°±å›¾ä¸Šæ“ä½œï¼Œé‡‡ç”¨å—çº§è®¾è®¡ç®€åŒ–äº†æ¶æ„ï¼Œå®ç°äº†ä½å»¶è¿Ÿæµã€‚<br>ä¸‰ã€ä½¿ç”¨å…±äº«ä»£ç ç°¿è¿›è¡Œé‡åŒ–å¤„ç†ï¼Œç”Ÿæˆç¦»æ•£æ½œåœ¨ç½‘æ ¼ã€‚<br>å››ã€é‡‡ç”¨æ™šæœŸå¯¹æŠ—å¾®è°ƒæŠ€æœ¯ä¼˜åŒ–VQ-VAEæ¨¡å‹æ€§èƒ½ã€‚è®­ç»ƒHiFi-GANä»å¤´å¼€å§‹é‡å»ºé¢‘è°±å›¾ã€‚è®­ç»ƒç»“æœè‰¯å¥½ã€‚æµ‹è¯•è¡¨æ˜è¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡å’Œå¯ç†è§£æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚æ­¤è®ºæ–‡æä¾›äº†ä¸€ä¸ªå…³äºæœªæ¥çš„ä½å»¶è¿Ÿç¼–è§£ç å™¨è®¾è®¡çš„å¼€æ”¾æ¡†æ¶æ€è·¯å’ŒæŠ€æœ¯æŒ‡å¯¼è·¯çº¿é’ˆå¯¹ç›®æ ‡é«˜åº¦è¿ç»­ç¨³å®šçš„éƒ¨ç½²å¹³å°çš„ååŒæ¨è¿›äº§ç”Ÿæ•ˆç‡èµ„æºè€—è´¹éœ€æ±‚æå…¶è‰¯å¥½çš„å®ç”¨ä»·å€¼å’Œå·¥ä½œåˆ›æ–°äº®ç‚¹æŠ€æœ¯ï¼›ç»è¿‡å……åˆ†è¯æ˜èƒ½æœ‰æ•ˆé€‚é…é«˜è´¨é‡åª’ä½“æ–‡ä»¶çš„è·¨è®¾å¤‡å¤„ç†å’Œè¿ç§»åº”ç”¨çš„ä¼˜ç§€æ€§èƒ½å’Œæœªæ¥å‘å±•è¶‹åŠ¿è¯„ä¼°ç ”ç©¶çš„é‡è¦æ„ä¹‰å’Œæ–¹æ³•è®ºçš„è¿›æ­¥æ€§å’Œé¢†å…ˆæ€§å¾—åˆ°äº†è®¤å¯å’Œæ”¹è¿›å‘å±•æ–¹æ¡ˆæå‡æ‰©å±•æ–¹å‘æå…¶æ˜æ˜¾å› æ­¤æœ¬ç³»ç»Ÿéå¸¸æœ‰ä»·å€¼æ¨å¹¿åº”ç”¨å‰æ™¯ååˆ†å¹¿é˜”å®é™…åº”ç”¨æ•ˆæœæ˜¾è‘—ä¼˜è¶Šæ€§æä¸ºçªå‡ºå®é™…åº”ç”¨ç¯å¢ƒåŠå…¶éœ€æ±‚æ¡ä»¶ä¸°å¯Œå¤šæ ·æœ¬æ–¹æ³•æœ‰æ•ˆæé«˜äº†åº”ç”¨æ€§èƒ½æå‡æ˜æ˜¾æ”¹å–„ç”¨æˆ·ä½“éªŒåŒæ—¶å…·æœ‰å¾ˆå¥½çš„ç»æµæ€§æˆæœ¬è¾ƒä½å¯å®ç°äº§ä¸šåŒ–æ¨å¹¿åº”ç”¨çš„å‰æ™¯å¹¿é˜”å¯ä¸ºè¡Œä¸šå¸¦æ¥è‰¯å¥½çš„ç»æµæ•ˆç›Šå’Œç¤¾ä¼šæ•ˆç›Šæä¾›åˆ›æ–°å‹çš„äº§ä¸šåŒ–æ–¹æ¡ˆæœåŠ¡ç§‘æŠ€åˆ›æ–°äº§ä¸šå‡çº§æå‡è¡Œä¸šçš„ç«äº‰åŠ›åŠ©åŠ›ç¤¾ä¼šè¿›æ­¥å’Œé«˜è´¨é‡å‘å±•ä½“ç³»åŒ–è½åœ°èƒ½åŠ›æå…¶é‡è¦åœ¨æ­¤ç³»ç»Ÿæ–¹é¢æœ¬æ–‡çš„è´¡çŒ®ä¸å®¹å°è§‘æä¾›éå¸¸ä¼˜ç§€çš„ç ”ç©¶æˆæœå±•ç¤ºäº†ç‹¬ç‰¹è§†è§’çš„æ–°ç†è®ºå’ŒæŠ€æœ¯å‰æ²¿æ‹¥æœ‰æé«˜åŸåˆ›æ€§å’Œæ˜¾è‘—æ½œåœ¨åº”ç”¨ä»·å€¼å¯ä»¥å¾ˆå¥½åœ°æœåŠ¡äºè¡Œä¸šå‘å±•æä¾›æœ‰æ•ˆçš„ç†è®ºæ”¯æ’‘å’ŒæŠ€æœ¯æ”¯æŒå€¼å¾—æ·±å…¥ç ”ç©¶å’Œæ¨å¹¿åº”ç”¨çš„ä»·å€¼è¾ƒé«˜é‡è¦æ€§çªå‡ºå½±å“å¹¿æ³›åœ¨å¤šä¸ªæ–¹é¢ç»™å‡ºäº†å“è¶Šçš„è´¡çŒ®æå¤§åœ°ä¿ƒè¿›äº†é¢†åŸŸå†…çš„ç ”ç©¶è¿›ç¨‹ä½“ç°äº†å‡ºè‰²çš„ç ”ç©¶æˆæœä¸ç ”ç©¶æ°´å¹³å¹¶ä¸ºåç»­çš„ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–è§£ç æŠ€æœ¯çš„å‘å±•æä¾›äº†æ–°çš„ç ”ç©¶æ€è·¯å’Œæ–¹å‘ä»¥å®ç°åœ¨å¤æ‚ç¯å¢ƒä¸‹éŸ³é¢‘ä¿¡æ¯çš„å¿«é€Ÿé«˜æ•ˆä¼ è¾“ä¸å‡†ç¡®è§£ç æé«˜éŸ³é¢‘ä¼ è¾“æŠ€æœ¯çš„å®ç”¨æ€§å’Œå¯é æ€§å…·æœ‰æå…¶é‡è¦çš„æ„ä¹‰ã€‚ä»¥ä¸‹æ˜¯ä»¥ç²¾ç®€æ–¹å¼å‘ˆç°çš„å…³é”®è§è§£ï¼š<br>äº”ã€ç³»ç»Ÿä½¿ç”¨æ¢…å°”é¢‘è°±å›¾ä½œä¸ºè¾“å…¥æ•°æ®ï¼Œç®€åŒ–äº†éŸ³é¢‘ç¼–ç å™¨çš„å¤æ‚æ€§å¹¶æé«˜äº†æ€§èƒ½ã€‚<br>å…­ã€é€šè¿‡å…±äº«ä»£ç ç°¿è¿›è¡Œé‡åŒ–å¤„ç†ï¼Œé™ä½äº†ç³»ç»Ÿå¤æ‚åº¦å¹¶æé«˜äº†ç¼–ç æ•ˆç‡ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d89286b075de67383a0acb4076d5827.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cae6c80ebd27fb62480ce15fb08d11f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf5693abbe3c3c025881e809c9791603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d7935025315951c9a42ccd50ee83ef1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BSNeRF-Broadband-Spectral-Neural-Radiance-Fields-for-Snapshot-Multispectral-Light-field-Imaging"><a href="#BSNeRF-Broadband-Spectral-Neural-Radiance-Fields-for-Snapshot-Multispectral-Light-field-Imaging" class="headerlink" title="BSNeRF: Broadband Spectral Neural Radiance Fields for Snapshot   Multispectral Light-field Imaging"></a>BSNeRF: Broadband Spectral Neural Radiance Fields for Snapshot   Multispectral Light-field Imaging</h2><p><strong>Authors:Erqi Huang, John Restrepo, Xun Cao, Ivo Ihrke</strong></p>
<p>Snapshot Multispectral Light-field Imaging (SMLI) is an emerging computational imaging technique that captures high-dimensional data (x, y, z, $\theta$, $\phi$, $\lambda$) in a single shot using a low-dimensional sensor. The accuracy of high-dimensional data reconstruction depends on representing the spectrum using neural radiance field models, which requires consideration of broadband spectral decoupling during optimization. Currently, some SMLI approaches avoid the challenge of model decoupling by either reducing light-throughput or prolonging imaging time. In this work, we propose a broadband spectral neural radiance field (BSNeRF) for SMLI systems. Experiments show that our model successfully decouples a broadband multiplexed spectrum. Consequently, this approach enhances multispectral light-field image reconstruction and further advances plenoptic imaging. </p>
<blockquote>
<p>å…‰è°±å¤šè§†è§’ç¬æ€æˆåƒæŠ€æœ¯ï¼ˆSMLIï¼‰æ˜¯ä¸€ç§æ–°å…´çš„è®¡ç®—æˆåƒæŠ€æœ¯ï¼Œå®ƒèƒ½åœ¨ä¸€æ¬¡ä½ç»´ä¼ æ„Ÿå™¨æ‹æ‘„ä¸­è·å¾—é«˜ç»´æ•°æ®ï¼ˆxï¼Œyï¼Œzï¼ŒÎ¸ï¼ŒÏ†ï¼ŒÎ»ï¼‰ã€‚é«˜ç»´æ•°æ®çš„é‡å»ºç²¾åº¦å–å†³äºåˆ©ç”¨ç¥ç»è¾å°„åœºæ¨¡å‹æ¥è¡¨ç¤ºå…‰è°±ã€‚åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­éœ€è¦è€ƒè™‘å®½å¸¦å…‰è°±è§£è€¦ã€‚ç›®å‰ï¼Œä¸€äº›SMLIæ–¹æ³•é€šè¿‡é™ä½å…‰é€šé‡æˆ–å»¶é•¿æˆåƒæ—¶é—´æ¥é¿å…æ¨¡å‹è§£è€¦çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºSMLIç³»ç»Ÿæå‡ºäº†ä¸€ä¸ªå®½å¸¦å…‰è°±ç¥ç»è¾å°„åœºï¼ˆBSNeRFï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æˆåŠŸåœ°å®ç°äº†å®½å¸¦å¤šè·¯å¤ç”¨å…‰è°±çš„è§£è€¦ã€‚å› æ­¤ï¼Œè¿™ç§æ–¹æ³•å¢å¼ºäº†å¤šå…‰è°±å…‰åœºå›¾åƒçš„é‡å»ºï¼Œå¹¶è¿›ä¸€æ­¥æ¨åŠ¨äº†å…¨æ™¯æˆåƒæŠ€æœ¯çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01070v1">PDF</a> Presented in ISCS25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Snapshot Multispectral Light-field Imagingï¼ˆSMLIï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨å•æ¬¡æ‹æ‘„ä¸­æ•è·é«˜ç»´åº¦æ•°æ®ã€‚æ–‡ç« æŒ‡å‡ºï¼Œåˆ©ç”¨ç¥ç»è¾å°„åœºæ¨¡å‹è¡¨ç¤ºå…‰è°±å¯¹äºé«˜ç»´åº¦æ•°æ®é‡å»ºçš„å‡†ç¡®æ€§è‡³å…³é‡è¦ï¼Œä¸”ä¼˜åŒ–è¿‡ç¨‹ä¸­éœ€è¦è€ƒè™‘å®½å¸¦å…‰è°±è§£è€¦é—®é¢˜ã€‚ç›®å‰éƒ¨åˆ†SMLIæ–¹æ³•é€šè¿‡é™ä½å…‰é€šé‡æˆ–å»¶é•¿æˆåƒæ—¶é—´æ¥é¿å…æ¨¡å‹è§£è€¦çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºSMLIç³»ç»Ÿçš„å®½å¸¦å…‰è°±ç¥ç»è¾å°„åœºï¼ˆBSNeRFï¼‰ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹æˆåŠŸå®ç°äº†å®½å¸¦å¤šè·¯å¤ç”¨å…‰è°±çš„è§£è€¦ï¼Œæé«˜äº†å¤šå…‰è°±å…‰åœºå›¾åƒçš„é‡å»ºæ•ˆæœï¼Œè¿›ä¸€æ­¥æ¨åŠ¨äº†plenopticæˆåƒçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMLIæŠ€æœ¯èƒ½å¤Ÿåœ¨å•æ¬¡æ‹æ‘„ä¸­æ•è·é«˜ç»´åº¦æ•°æ®ã€‚</li>
<li>ç¥ç»è¾å°„åœºæ¨¡å‹å¯¹äºå…‰è°±çš„è¡¨ç¤ºå¯¹äºé«˜ç»´åº¦æ•°æ®é‡å»ºçš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚</li>
<li>å®½å¸¦å…‰è°±è§£è€¦æ˜¯ä¼˜åŒ–ç¥ç»è¾å°„åœºæ¨¡å‹çš„å…³é”®å› ç´ ã€‚</li>
<li>ç›®å‰éƒ¨åˆ†SMLIæ–¹æ³•é‡‡å–é™ä½å…‰é€šé‡æˆ–å»¶é•¿æˆåƒæ—¶é—´çš„æ–¹å¼é¿å…æ¨¡å‹è§£è€¦æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºBSNeRFçš„å®½å¸¦å…‰è°±ç¥ç»è¾å°„åœºæ¨¡å‹ã€‚</li>
<li>BSNeRFæ¨¡å‹æˆåŠŸå®ç°äº†å®½å¸¦å¤šè·¯å¤ç”¨å…‰è°±çš„è§£è€¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7b6794c2b1dea6a2e5c14e58c3c46b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79caab462aa77fc92aac12f22c12507b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GS-TG-3D-Gaussian-Splatting-Accelerator-with-Tile-Grouping-for-Reducing-Redundant-Sorting-while-Preserving-Rasterization-Efficiency"><a href="#GS-TG-3D-Gaussian-Splatting-Accelerator-with-Tile-Grouping-for-Reducing-Redundant-Sorting-while-Preserving-Rasterization-Efficiency" class="headerlink" title="GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing   Redundant Sorting while Preserving Rasterization Efficiency"></a>GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing   Redundant Sorting while Preserving Rasterization Efficiency</h2><p><strong>Authors:Joongho Jo, Jongsun Park</strong></p>
<p>3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to neural radiance fields (NeRF) as it offers high speed as well as high image quality in novel view synthesis. Despite these advancements, 3D-GS still struggles to meet the frames per second (FPS) demands of real-time applications. In this paper, we introduce GS-TG, a tile-grouping-based accelerator that enhances 3D-GS rendering speed by reducing redundant sorting operations and preserving rasterization efficiency. GS-TG addresses a critical trade-off issue in 3D-GS rendering: increasing the tile size effectively reduces redundant sorting operations, but it concurrently increases unnecessary rasterization computations. So, during sorting of the proposed approach, GS-TG groups small tiles (for making large tiles) to share sorting operations across tiles within each group, significantly reducing redundant computations. During rasterization, a bitmask assigned to each Gaussian identifies relevant small tiles, to enable efficient sharing of sorting results. Consequently, GS-TG enables sorting to be performed as if a large tile size is used by grouping tiles during the sorting stage, while allowing rasterization to proceed with the original small tiles by using bitmasks in the rasterization stage. GS-TG is a lossless method requiring no retraining or fine-tuning and it can be seamlessly integrated with previous 3D-GS optimization techniques. Experimental results show that GS-TG achieves an average speed-up of 1.54 times over state-of-the-art 3D-GS accelerators. </p>
<blockquote>
<p>3Dé«˜æ–¯æ‘Šé“ºï¼ˆ3D-GSï¼‰ä½œä¸ºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ï¼Œå› ä¸ºå®ƒåœ¨æ–°å‹è§†å›¾åˆæˆä¸­æä¾›äº†é«˜é€Ÿå’Œé«˜å›¾åƒè´¨é‡ã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œä½†3D-GSä»ç„¶éš¾ä»¥æ»¡è¶³æ¯ç§’å¸§æ•°ï¼ˆFPSï¼‰çš„å®æ—¶åº”ç”¨éœ€æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GS-TGï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç“¦ç‰‡åˆ†ç»„æŠ€æœ¯çš„åŠ é€Ÿå™¨ï¼Œå®ƒé€šè¿‡å‡å°‘å†—ä½™æ’åºæ“ä½œå’Œä¿æŒå…‰æ …åŒ–æ•ˆç‡æ¥æé«˜3D-GSçš„æ¸²æŸ“é€Ÿåº¦ã€‚GS-TGè§£å†³äº†3D-GSæ¸²æŸ“ä¸­çš„ä¸€ä¸ªå…³é”®æƒè¡¡é—®é¢˜ï¼šå¢åŠ ç“¦ç‰‡å¤§å°å¯ä»¥æœ‰æ•ˆå‡å°‘å†—ä½™æ’åºæ“ä½œï¼Œä½†åŒæ—¶ä¼šå¢åŠ ä¸å¿…è¦çš„å…‰æ …åŒ–è®¡ç®—é‡ã€‚å› æ­¤ï¼Œåœ¨æ’åºè¿‡ç¨‹ä¸­ï¼ŒGS-TGå°†å°ç“¦ç‰‡åˆ†ç»„ï¼ˆä»¥åˆ¶ä½œå¤§ç“¦ç‰‡ï¼‰ï¼Œä»è€Œåœ¨æ¯ç»„å†…å…±äº«ç“¦ç‰‡çš„æ’åºæ“ä½œï¼Œä»è€Œå¤§å¤§å‡å°‘å†—ä½™è®¡ç®—ã€‚åœ¨å…‰æ …åŒ–è¿‡ç¨‹ä¸­ï¼Œåˆ†é…ç»™æ¯ä¸ªé«˜æ–¯å€¼çš„ä½æ©ç ç”¨äºæ ‡è¯†ç›¸å…³çš„å°ç“¦ç‰‡ï¼Œä»è€Œå®ç°æ’åºç»“æœçš„å…±äº«ã€‚å› æ­¤ï¼ŒGS-TGèƒ½å¤Ÿåœ¨æ’åºé˜¶æ®µé€šè¿‡åˆ†ç»„ç“¦ç‰‡æ¥æ¨¡æ‹Ÿä½¿ç”¨è¾ƒå¤§çš„ç“¦ç‰‡å¤§å°è¿›è¡Œæ’åºï¼ŒåŒæ—¶å…è®¸åœ¨å…‰æ …åŒ–é˜¶æ®µä½¿ç”¨åŸå§‹çš„å°ç“¦ç‰‡å¹¶ä½¿ç”¨ä½æ©ç è¿›è¡Œå…‰æ …åŒ–ã€‚GS-TGæ˜¯ä¸€ç§æ— æŸæ–¹æ³•ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒï¼Œå®ƒå¯ä»¥æ— ç¼é›†æˆåˆ°å…ˆå‰çš„3D-GSä¼˜åŒ–æŠ€æœ¯ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGS-TGä¸æœ€å…ˆè¿›çš„3D-GSåŠ é€Ÿå™¨ç›¸æ¯”ï¼Œå¹³å‡é€Ÿåº¦æé«˜äº†1.54å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00911v2">PDF</a> DAC 2025</p>
<p><strong>Summary</strong></p>
<p>3Dé«˜æ–¯æ ·æ¡ï¼ˆ3D-GSï¼‰åœ¨ä¸‰ç»´æ¸²æŸ“æŠ€æœ¯ä¸­å…·æœ‰æ½œåŠ›ï¼Œå°½ç®¡åœ¨å¸§é€Ÿç‡ä¸Šæœ‰æ‰€æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†GS-TGåŠ é€ŸæŠ€æœ¯ï¼Œå®ƒé€šè¿‡ä¼˜åŒ–æ ·æ¡æ“ä½œçš„æ•ˆç‡å¹¶é¿å…ä¸å¿…è¦çš„å†—ä½™ï¼Œä¼˜åŒ–äº†è¿™ä¸€ç¼ºé™·ã€‚å…·ä½“è€Œè¨€ï¼ŒGS-TGé€šè¿‡åˆ†ç»„æŠ€æœ¯å‡å°‘äº†å†—ä½™æ’åºæ“ä½œï¼ŒåŒæ—¶ä¿æŒå…‰æ …åŒ–æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒGS-TGç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„3D-GSåŠ é€Ÿå™¨å¹³å‡æé€Ÿäº†1.54å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3Dé«˜æ–¯æ ·æ¡ï¼ˆ3D-GSï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆä¸­æä¾›é«˜è´¨é‡å›¾åƒå’Œé«˜é€Ÿæ¸²æŸ“çš„æ½œåŠ›ã€‚</li>
<li>GS-TGæ˜¯ä¸€ç§åŸºäºåˆ†ç»„æŠ€æœ¯çš„åŠ é€Ÿå™¨ï¼Œæ—¨åœ¨ä¼˜åŒ–3D-GSçš„æ¸²æŸ“é€Ÿåº¦ã€‚</li>
<li>GS-TGé€šè¿‡å‡å°‘å†—ä½™æ’åºæ“ä½œå’Œæé«˜å…‰æ …åŒ–æ•ˆç‡æ¥è§£å†³å…³é”®æƒè¡¡é—®é¢˜ã€‚</li>
<li>GS-TGé‡‡ç”¨åˆ†ç»„æŠ€æœ¯å®ç°ç±»ä¼¼å¤§ç“¦ç‰‡å¤§å°çš„æ’åºæ•ˆæœï¼ŒåŒæ—¶ä¿æŒåŸå§‹å°ç“¦ç‰‡çš„å…‰æ …åŒ–å¤„ç†ã€‚</li>
<li>GS-TGé›†æˆåˆ°ç°æœ‰æŠ€æœ¯ä¸­æ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒï¼Œä¸”ä¸ºæ— æŸæ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1fa4ffd586de6cf482702a4fd8fd00a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-213e736ada0c8b20cd25b6d15cfcb506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6baf94b8955eef41c666d74c18a27e46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0110abdbe4a146dab31b7fa8ed065cd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a178121b5f3d6a3ec22cfa4b3bbe16bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76188bd1a8110ee792767fc4e1cb1779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1883d5d2ff2a8d39d8c9818a8825fc4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SWAGSplatting-Semantic-guided-Water-scene-Augmented-Gaussian-Splatting"><a href="#SWAGSplatting-Semantic-guided-Water-scene-Augmented-Gaussian-Splatting" class="headerlink" title="SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting"></a>SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting</h2><p><strong>Authors:Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, Nantheera Anantrasirichai</strong></p>
<p>Accurate 3D reconstruction in underwater environments remains a complex challenge due to issues such as light distortion, turbidity, and limited visibility. AI-based techniques have been applied to address these issues, however, existing methods have yet to fully exploit the potential of AI, particularly in integrating language models with visual processing. In this paper, we propose a novel framework that leverages multimodal cross-knowledge to create semantic-guided 3D Gaussian Splatting for robust and high-fidelity deep-sea scene reconstruction. By embedding an extra semantic feature into each Gaussian primitive and supervised by the CLIP extracted semantic feature, our method enforces semantic and structural awareness throughout the training. The dedicated semantic consistency loss ensures alignment with high-level scene understanding. Besides, we propose a novel stage-wise training strategy, combining coarse-to-fine learning with late-stage parameter refinement, to further enhance both stability and reconstruction quality. Extensive results show that our approach consistently outperforms state-of-the-art methods on SeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement of up to 3.09 dB on average in terms of PSNR, making it a strong candidate for applications in underwater exploration and marine perception. </p>
<blockquote>
<p>åœ¨æ°´ä¸‹ç¯å¢ƒä¸­å®ç°ç²¾ç¡®çš„3Dé‡å»ºæ˜¯ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨å…‰ç•¸å˜ã€æµ‘æµŠå’Œå¯è§åº¦æœ‰é™ç­‰é—®é¢˜ã€‚è™½ç„¶äººå·¥æ™ºèƒ½æŠ€æœ¯å·²åº”ç”¨äºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•å°šæœªå……åˆ†åˆ©ç”¨AIçš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†è¯­è¨€æ¨¡å‹ä¸è§†è§‰å¤„ç†ç›¸ç»“åˆæ–¹é¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€äº¤å‰çŸ¥è¯†åˆ›å»ºè¯­ä¹‰å¼•å¯¼3Dé«˜æ–¯æ‹¼è´´å›¾çš„æ–°æ¡†æ¶ï¼Œä»¥å®ç°ç¨³å¥å’Œé«˜ä¿çœŸæ·±æµ·åœºæ™¯é‡å»ºã€‚æˆ‘ä»¬é€šè¿‡å°†é¢å¤–çš„è¯­ä¹‰ç‰¹å¾åµŒå…¥æ¯ä¸ªé«˜æ–¯åŸå§‹ç‰¹å¾ä¸­ï¼Œå¹¶ä½¿ç”¨CLIPæå–çš„è¯­ä¹‰ç‰¹å¾è¿›è¡Œç›‘ç£ï¼Œä»è€Œåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å¼ºåˆ¶æ‰§è¡Œè¯­ä¹‰å’Œç»“æ„åŒ–æ„è¯†ã€‚ä¸“ç”¨çš„è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ç¡®ä¿ä¸é«˜çº§åœºæ™¯ç†è§£çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç»“åˆä»ç²—åˆ°ç»†çš„å­¦ä¹ ä¸åæœŸå‚æ•°ç»†åŒ–ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç¨³å®šæ€§å’Œé‡å»ºè´¨é‡ã€‚å¤§é‡ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨SeaThru-NeRFå’ŒSubmerged3Dæ•°æ®é›†ä¸Šçš„ä¸‰é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨PSNRæ–¹é¢å¹³å‡æé«˜äº†é«˜è¾¾3.09 dBï¼Œä½¿å…¶æˆä¸ºæ°´ä¸‹æ¢ç´¢å’Œæµ·æ´‹æ„ŸçŸ¥åº”ç”¨çš„æœ‰åŠ›å€™é€‰è€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00800v1">PDF</a> Submitted to SIGGRAPH Asia 2025 Technical Communications</p>
<p><strong>Summary</strong><br>æ°´ä¸‹ç¯å¢ƒçš„ç²¾å‡†ä¸‰ç»´é‡å»ºæ˜¯ä¸€ä¸ªå……æ»¡æŒ‘æˆ˜çš„è¯¾é¢˜ï¼Œå› ä¸ºé¢ä¸´ç€å…‰æ³¢å¤±çœŸã€æµ‘æµŠåº¦å’Œè§†çº¿è·ç¦»æœ‰é™ç­‰é—®é¢˜ã€‚è™½ç„¶äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨è§£å†³è¿™äº›é—®é¢˜æ–¹é¢æœ‰æ‰€åº”ç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•å°šæœªå……åˆ†åˆ©ç”¨äººå·¥æ™ºèƒ½çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†è¯­è¨€æ¨¡å‹ä¸è§†è§‰å¤„ç†ç›¸ç»“åˆæ–¹é¢ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€äº¤å‰çŸ¥è¯†åˆ›å»ºè¯­ä¹‰å¼•å¯¼çš„ä¸‰ç»´é«˜æ–¯é£æº…ï¼Œå®ç°ç¨³å¥å’Œé«˜ä¿çœŸæ·±æµ·åœºæ™¯é‡å»ºã€‚è¯¥æ–¹æ³•é€šè¿‡å°†é¢å¤–çš„è¯­ä¹‰ç‰¹å¾åµŒå…¥æ¯ä¸ªé«˜æ–¯åŸºæœ¬å•å…ƒï¼Œå¹¶ä½¿ç”¨CLIPæå–çš„è¯­ä¹‰ç‰¹å¾è¿›è¡Œç›‘ç£ï¼Œä»è€Œåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å¼ºåˆ¶æ‰§è¡Œè¯­ä¹‰å’Œç»“æ„æ€§æ„è¯†ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„é˜¶æ®µæ€§è®­ç»ƒç­–ç•¥ï¼Œç»“åˆä»ç²—åˆ°ç»†çš„å­¦ä¹ ä¸åæœŸå‚æ•°ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜ç¨³å®šæ€§å’Œé‡å»ºè´¨é‡ã€‚åœ¨SeaThru-NeRFå’ŒSubmerged3Dæ•°æ®é›†ä¸Šçš„å¤§é‡ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå¹³å‡å³°å€¼ä¿¡å™ªæ¯”æé«˜äº†é«˜è¾¾3.09åˆ†è´ï¼Œæˆä¸ºæ°´ä¸‹æ¢ç´¢å’Œæµ·æ´‹æ„ŸçŸ¥åº”ç”¨çš„æœ‰åŠ›å€™é€‰è€…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹ç¯å¢ƒä¸‰ç»´é‡å»ºé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚å…‰æ³¢å¤±çœŸã€æµ‘æµŠåº¦å’Œè§†çº¿æœ‰é™ç­‰ã€‚</li>
<li>ç°æœ‰AIæŠ€æœ¯åœ¨è§£å†³è¿™äº›é—®é¢˜æ—¶æœªèƒ½å……åˆ†åˆ©ç”¨å…¶æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“åˆè¯­è¨€æ¨¡å‹å’Œè§†è§‰å¤„ç†æ–¹é¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€äº¤å‰çŸ¥è¯†å’Œè¯­ä¹‰å¼•å¯¼çš„ä¸‰ç»´é«˜æ–¯é£æº…å®ç°æ·±æµ·åœºæ™¯çš„ç¨³å¥å’Œé«˜ä¿çœŸé‡å»ºã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬å°†è¯­ä¹‰ç‰¹å¾åµŒå…¥é«˜æ–¯åŸºæœ¬å•å…ƒï¼Œå¹¶åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å¼ºåˆ¶æ‰§è¡Œè¯­ä¹‰å’Œç»“æ„æ€§æ„è¯†ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„é˜¶æ®µæ€§è®­ç»ƒç­–ç•¥ï¼Œç»“åˆäº†ä»ç²—åˆ°ç»†çš„å­¦ä¹ ä¸åæœŸå‚æ•°ä¼˜åŒ–ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-527338f237c068f7f42502182a7e2bf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faeb12ef2aabe853912ccf23564fc3a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e0781ceb513940107b1c78e652d4b21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-707e4becebcb951d0926bf1e27248df8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07f0ade90f6d6a8751dbc9073d6681ba.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Molecular-Beam-Epitaxy-of-2H-TaS-2-few-layers-on-GaN-0001"><a href="#Molecular-Beam-Epitaxy-of-2H-TaS-2-few-layers-on-GaN-0001" class="headerlink" title="Molecular Beam Epitaxy of 2H-TaS$_2$ few-layers on GaN(0001)"></a>Molecular Beam Epitaxy of 2H-TaS$_2$ few-layers on GaN(0001)</h2><p><strong>Authors:Constantin Hilbrunner, Tobias Meyer, Joerg Malindretos, Angela Rizzi</strong></p>
<p>2H-TaS$_2$ few layers have been grown epitaxially onto GaN(0001). A high substrate growth temperature of 825$^{\circ}$C induces best structural properties of the overlayer, as revealed by in-situ electron diffraction (RHEED and LEED). The 2D-overlayer grows unstrained right after deposition of a monolayer. However, evidence of pits at the interface is provided by scanning transmission electron microscopy, most probably due to GaN thermal decomposition at the high growth temperature. In-situ x-ray photoemission spectroscopy shows core level shifts that are consistently related to electron transfer from the n-GaN(0001) to the 2H-TaS$_2$ epitaxial layer as well as the formation of a high concentration of nitrogen vacancies close to the interface. Further, no chemical reaction at the interface between the substrate and the grown TaS$_2$ overlayer is deduced from XPS, which corroborates the possibility of integration of 2D 2H-TaS$_2$ with an important 3D semiconducting material like GaN. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œ2H-TaS$_2$è¶…è–„å±‚å·²åœ¨GaNï¼ˆ0001ï¼‰ä¸Šè¿›è¡Œå¤–å»¶ç”Ÿé•¿ã€‚é«˜è¡¬åº•ç”Ÿé•¿æ¸©åº¦825$^{\circ}$Cä½¿è¦†ç›–å±‚çš„ç»“æ„æ€§èƒ½æœ€ä½³ï¼ŒåŸä½ç”µå­è¡å°„ï¼ˆRHEEDå’ŒLEEDï¼‰æ­ç¤ºäº†è¿™ç‰¹ç‚¹ã€‚äºŒç»´è¦†ç›–å±‚åœ¨å•å±‚æ²‰ç§¯åç«‹åˆ»æ— åº”å˜ç”Ÿé•¿ã€‚ç„¶è€Œï¼Œæ‰«æé€å°„ç”µå­æ˜¾å¾®é•œæä¾›äº†ç•Œé¢å‡¹é™·çš„è¯æ®ï¼Œè¿™å¾ˆå¯èƒ½å½’å› äºé«˜æ¸©ç”Ÿé•¿è¿‡ç¨‹ä¸­GaNçš„çƒ­åˆ†è§£ã€‚åŸä½Xå°„çº¿å…‰ç”µå­èƒ½è°±æ˜¾ç¤ºæ ¸å¿ƒèƒ½çº§ç§»åŠ¨ï¼Œè¿™ä¸ä»n-GaNï¼ˆ0001ï¼‰åˆ°å¤–å»¶ç”Ÿé•¿çš„2H-TaS$_2$å±‚çš„ç”µå­è½¬ç§»æœ‰å…³ï¼ŒåŒæ—¶è¡¨æ˜åœ¨ç•Œé¢é™„è¿‘å½¢æˆäº†é«˜æµ“åº¦çš„æ°®ç©ºä½ã€‚æ­¤å¤–ï¼Œä»XPSæ¨æ–­ï¼Œåœ¨è¡¬åº•å’Œç”Ÿé•¿çš„TaS$_2$è¦†ç›–å±‚ä¹‹é—´ç•Œé¢å¤„æ²¡æœ‰å‘ç”ŸåŒ–å­¦ååº”ï¼Œè¿™è¯å®äº†å°†äºŒç»´çš„2H-TaS$_2$ä¸é‡è¦çš„ä¸‰ç»´åŠå¯¼ä½“ææ–™å¦‚GaNé›†æˆçš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21537v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨GaN(0001)ä¸Šå¤–å»¶ç”Ÿé•¿2H-TaS$_2$è–„è†œçš„è¿‡ç¨‹ã€‚é€šè¿‡åŸä½ç”µå­è¡å°„ï¼ˆRHEEDå’ŒLEEDï¼‰å‘ç°ï¼Œé«˜è¡¬åº•ç”Ÿé•¿æ¸©åº¦ï¼ˆ825$^{\circ}$Cï¼‰æœ‰åŠ©äºè·å¾—æœ€ä½³è–„è†œç»“æ„ç‰¹æ€§ã€‚è¯¥äºŒç»´è–„è†œåœ¨å•å±‚æ²‰ç§¯åæ— åº”å˜ç”Ÿé•¿ï¼Œä½†æ‰«æé€å°„ç”µå­æ˜¾å¾®é•œæ˜¾ç¤ºç•Œé¢å¤„æœ‰å‘æ´ï¼Œå¯èƒ½æ˜¯ç”±äºGaNåœ¨é«˜æ¸©ä¸‹çš„çƒ­åˆ†è§£æ‰€è‡´ã€‚åŸä½Xå°„çº¿å…‰ç”µå­èƒ½è°±æ˜¾ç¤ºæ ¸å¿ƒèƒ½çº§åç§»ï¼Œè¿™ä¸n-GaN(0001)å‘äºŒç»´å¤–å»¶å±‚2H-TaS$_2$çš„ç”µå­è½¬ç§»ä¸€è‡´ï¼ŒåŒæ—¶åœ¨ç•Œé¢é™„è¿‘å½¢æˆé«˜æµ“åº¦æ°®ç©ºä½ã€‚XPSç»“æœè¡¨æ˜ï¼Œè¡¬åº•ä¸ç”Ÿé•¿çš„TaS$_2$è–„è†œé—´æ²¡æœ‰åŒ–å­¦ååº”ï¼Œè¿™ä¸ºå°†äºŒç»´ææ–™TaS$_2$ä¸é‡è¦çš„ä¸‰ç»´åŠå¯¼ä½“ææ–™GaNé›†æˆæä¾›äº†å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è¾¾825$^{\circ}$Cçš„è¡¬åº•ç”Ÿé•¿æ¸©åº¦æœ‰åŠ©äºå®ç°å¤–å»¶ç”Ÿé•¿çš„TaS$_2$è–„è†œçš„æœ€ä½³ç»“æ„ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡åŸä½ç”µå­è¡å°„æŠ€æœ¯æ­ç¤ºäº†è–„è†œçš„ç»“æ„ç‰¹æ€§ã€‚</li>
<li>æ‰«æé€å°„ç”µå­æ˜¾å¾®é•œè§‚å¯Ÿåˆ°ç•Œé¢å¤„çš„å‘æ´ï¼Œè¿™å¯èƒ½ä¸GaNåœ¨é«˜æ¸©ä¸‹çš„çƒ­åˆ†è§£æœ‰å…³ã€‚</li>
<li>åŸä½Xå°„çº¿å…‰ç”µå­èƒ½è°±è¡¨æ˜ç”µå­ä»GaNå‘TaS$_2$è½¬ç§»ï¼Œç•Œé¢å¤„æ°®ç©ºä½æµ“åº¦è¾ƒé«˜ã€‚</li>
<li>XPSç»“æœè¡¨æ˜è¡¬åº•ä¸ç”Ÿé•¿çš„TaS$_2$è–„è†œé—´æ²¡æœ‰åŒ–å­¦ååº”ã€‚</li>
<li>ç ”ç©¶ç»“æœä¸ºäºŒç»´ææ–™TaS$_2$ä¸ä¸‰ç»´åŠå¯¼ä½“ææ–™GaNçš„é›†æˆæä¾›äº†å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4392f5138cb0099b0463bc725a5fdc94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83be58be9c035947f0b737905a6c26a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-076e99a3b2f2dc479c1d455283047858.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7546c6129c1c81ddd3eeb456b3d4a9d5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leveraging-Discriminative-Latent-Representations-for-Conditioning-GAN-Based-Speech-Enhancement"><a href="#Leveraging-Discriminative-Latent-Representations-for-Conditioning-GAN-Based-Speech-Enhancement" class="headerlink" title="Leveraging Discriminative Latent Representations for Conditioning   GAN-Based Speech Enhancement"></a>Leveraging Discriminative Latent Representations for Conditioning   GAN-Based Speech Enhancement</h2><p><strong>Authors:Shrishti Saha Shetu, EmanuÃ«l A. P. Habets, Andreas Brendel</strong></p>
<p>Generative speech enhancement methods based on generative adversarial networks (GANs) and diffusion models have shown promising results in various speech enhancement tasks. However, their performance in very low signal-to-noise ratio (SNR) scenarios remains under-explored and limited, as these conditions pose significant challenges to both discriminative and generative state-of-the-art methods. To address this, we propose a method that leverages latent features extracted from discriminative speech enhancement models as generic conditioning features to improve GAN-based speech enhancement. The proposed method, referred to as DisCoGAN, demonstrates performance improvements over baseline models, particularly in low-SNR scenarios, while also maintaining competitive or superior performance in high-SNR conditions and on real-world recordings. We also conduct a comprehensive evaluation of conventional GAN-based architectures, including GANs trained end-to-end, GANs as a first processing stage, and post-filtering GANs, as well as discriminative models under low-SNR conditions. We show that DisCoGAN consistently outperforms existing methods. Finally, we present an ablation study that investigates the contributions of individual components within DisCoGAN and analyzes the impact of the discriminative conditioning method on overall performance. </p>
<blockquote>
<p>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºæ–¹æ³•åœ¨å„ç§è¯­éŸ³å¢å¼ºä»»åŠ¡ä¸­å·²æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰åœºæ™¯ä¸‹çš„æ€§èƒ½ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢å’Œé™åˆ¶ï¼Œå› ä¸ºè¿™äº›æ¡ä»¶å¯¹æœ€å…ˆè¿›çš„åˆ¤åˆ«å’Œç”Ÿæˆæ–¹æ³•éƒ½æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä»åˆ¤åˆ«å¼è¯­éŸ³å¢å¼ºæ¨¡å‹ä¸­æå–çš„æ½œåœ¨ç‰¹å¾ä½œä¸ºé€šç”¨æ¡ä»¶ç‰¹å¾ï¼Œä»¥æ”¹è¿›åŸºäºGANçš„è¯­éŸ³å¢å¼ºã€‚æ‰€æå‡ºçš„æ–¹æ³•è¢«ç§°ä¸ºDisCoGANï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨ä½SNRåœºæ™¯ä¸‹çš„æ€§èƒ½æœ‰æ‰€æå‡ï¼ŒåŒæ—¶åœ¨é«˜SNRæ¡ä»¶å’ŒçœŸå®ä¸–ç•Œè®°å½•ä¸­ä¿æŒç«äº‰åŠ›æˆ–æ›´å‡ºè‰²çš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜å¯¹ä¼ ç»Ÿçš„åŸºäºGANçš„æ¶æ„è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬ç«¯åˆ°ç«¯è®­ç»ƒçš„GANsã€ä½œä¸ºç¬¬ä¸€ä¸ªå¤„ç†é˜¶æ®µçš„GANsã€åæ»¤æ³¢GANsï¼Œä»¥åŠä½SNRæ¡ä»¶ä¸‹çš„åˆ¤åˆ«æ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜DisCoGANå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ¶ˆèç ”ç©¶ï¼Œç ”ç©¶äº†DisCoGANä¸­å•ä¸ªç»„ä»¶çš„è´¡çŒ®ï¼Œå¹¶åˆ†æäº†åˆ¤åˆ«æ¡ä»¶æ–¹æ³•å¯¹æ•´ä½“æ€§èƒ½çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20859v1">PDF</a> This manuscript has been submitted to IEEE Transactions on Audio,   Speech and Language Processing</p>
<p><strong>Summary</strong><br>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ–¹æ³•åœ¨å¤šç§è¯­éŸ³å¢å¼ºä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†åœ¨ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰åœºæ™¯ä¸‹æ€§èƒ½å—é™ä¸”å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨åˆ¤åˆ«å¼è¯­éŸ³å¢å¼ºæ¨¡å‹æå–çš„ç‰¹å¾ä½œä¸ºé€šç”¨æ¡ä»¶ç‰¹å¾æ¥æ”¹å–„åŸºäºGANçš„è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œç§°ä¹‹ä¸ºDisCoGANã€‚è¯¥æ–¹æ³•åœ¨ä½SNRåœºæ™¯ä¸‹ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹æœ‰æ˜æ˜¾æ€§èƒ½æå‡ï¼ŒåŒæ—¶åœ¨é«˜SNRæ¡ä»¶å’ŒçœŸå®ä¸–ç•Œå½•éŸ³ä¸­ä¿æŒç«äº‰åŠ›æˆ–æ›´å‡ºè‰²çš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜å¯¹å¸¸è§„GANæ¶æ„å’Œåˆ¤åˆ«æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¯æ˜äº†DisCoGANçš„ä¼˜è¶Šæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œæ¢è®¨äº†DisCoGANå†…éƒ¨ç»„ä»¶çš„è´¡çŒ®å’Œåˆ¤åˆ«æ¡ä»¶æ–¹æ³•å¯¹æ•´ä½“æ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>åœ¨ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰åœºæ™¯ä¸‹ï¼Œç°æœ‰æ–¹æ³•æ€§èƒ½å—é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•DisCoGANï¼Œåˆ©ç”¨åˆ¤åˆ«æ¨¡å‹çš„ç‰¹å¾æ¥æå‡GANåœ¨è¯­éŸ³å¢å¼ºä¸­çš„æ€§èƒ½ã€‚</li>
<li>DisCoGANåœ¨ä½SNRåœºæ™¯ä¸‹ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹æœ‰æ˜æ˜¾æ€§èƒ½æå‡ã€‚</li>
<li>DisCoGANåœ¨é«˜SNRæ¡ä»¶å’ŒçœŸå®ä¸–ç•Œå½•éŸ³ä¸­è¡¨ç°ç«äº‰åŠ›æˆ–æ›´å‡ºè‰²ã€‚</li>
<li>å¯¹å¸¸è§„GANæ¶æ„å’Œåˆ¤åˆ«æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒéªŒè¯äº†DisCoGANçš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5915df52a3654c47eaff55c20bd7ea5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-812d9251c6a213591c05b3dd4a711746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34cd7c761fcace5f94d4b1f57b23b09e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56a588fd6b656bfca97c034d71f06dfc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Revisiting-the-Privacy-Risks-of-Split-Inference-A-GAN-Based-Data-Reconstruction-Attack-via-Progressive-Feature-Optimization"><a href="#Revisiting-the-Privacy-Risks-of-Split-Inference-A-GAN-Based-Data-Reconstruction-Attack-via-Progressive-Feature-Optimization" class="headerlink" title="Revisiting the Privacy Risks of Split Inference: A GAN-Based Data   Reconstruction Attack via Progressive Feature Optimization"></a>Revisiting the Privacy Risks of Split Inference: A GAN-Based Data   Reconstruction Attack via Progressive Feature Optimization</h2><p><strong>Authors:Yixiang Qiu, Yanhan Liu, Hongyao Yu, Hao Fang, Bin Chen, Shu-Tao Xia, Ke Xu</strong></p>
<p>The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰çš„å¤æ‚æ€§å¢é•¿å¯¼è‡´é‡‡ç”¨äº†åˆ†è£‚æ¨ç†ï¼ˆSIï¼‰è¿™ç§åä½œèŒƒå¼ï¼Œè¯¥èŒƒå¼åœ¨è¾¹ç¼˜è®¾å¤‡å’Œäº‘ä¹‹é—´åˆ†é…è®¡ç®—ä»»åŠ¡ï¼Œä»¥å‡å°‘å»¶è¿Ÿå¹¶ä¿æŠ¤ç”¨æˆ·éšç§ã€‚ç„¶è€Œï¼Œæ•°æ®é‡å»ºæ”»å‡»ï¼ˆDRAsï¼‰çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼ŒSIä¸­äº¤æ¢çš„ä¸­é—´ç‰¹å¾å¯èƒ½è¢«åˆ©ç”¨æ¥æ¢å¤æ•æ„Ÿè¾“å…¥æ•°æ®ï¼Œä»è€Œå¸¦æ¥é‡å¤§çš„éšç§é£é™©ã€‚ç°æœ‰çš„DRAsé€šå¸¸åªåœ¨æµ…å±‚æ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è¯­ä¹‰å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œé™åˆ¶äº†å…¶é‡å»ºè´¨é‡å’Œåœ¨æ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šçš„é€šç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–°å‹DRAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰æ¸è¿›ç‰¹å¾ä¼˜åŒ–ï¼ˆPFOï¼‰åŠŸèƒ½ï¼Œå°†ç”Ÿæˆå™¨åˆ†è§£ä¸ºåˆ†å±‚å—å¹¶å¢é‡ä¼˜åŒ–ä¸­é—´è¡¨ç¤ºï¼Œä»¥æé«˜é‡å»ºå›¾åƒè¯­ä¹‰ä¿çœŸåº¦ã€‚ä¸ºäº†ç¨³å®šä¼˜åŒ–å¹¶æé«˜å›¾åƒçš„çœŸå®æ€§ï¼Œæˆ‘ä»¬åœ¨é‡å»ºè¿‡ç¨‹ä¸­å¼•å…¥äº†L1çƒçº¦æŸã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§å¤šæ•°æ”»å‡»é¢å‰è¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡åœºæ™¯ã€è¶…å‡ºåˆ†å¸ƒè®¾ç½®ä»¥åŠå¯¹æŠ—æ›´æ·±ã€æ›´å¤æ‚çš„DNNsæ—¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20613v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ ç½‘ç»œçš„å¤æ‚æ€§å¢é•¿ä¿ƒè¿›äº†åˆ†å‰²æ¨ç†ï¼ˆSplit Inferenceï¼ŒSIï¼‰è¿™ä¸€åä½œèŒƒå¼çš„åº”ç”¨ï¼Œè¯¥èŒƒå¼å°†è®¡ç®—ä»»åŠ¡åœ¨è¾¹ç¼˜è®¾å¤‡å’Œäº‘ç«¯ä¹‹é—´è¿›è¡Œåˆ†åŒºï¼Œä»¥é™ä½å»¶è¿Ÿå¹¶ä¿æŠ¤ç”¨æˆ·éšç§ã€‚ç„¶è€Œï¼Œæ•°æ®é‡å»ºæ”»å‡»ï¼ˆData Reconstruction Attacksï¼ŒDRAsï¼‰æ˜¾ç¤ºï¼ŒSIä¸­äº¤æ¢çš„ä¸­é—´ç‰¹å¾å¯èƒ½è¢«ç”¨æ¥æ¢å¤æ•æ„Ÿè¾“å…¥æ•°æ®ï¼Œå¸¦æ¥é‡å¤§éšç§é£é™©ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„DRAæ¡†æ¶ï¼Œå¹¶å¼•å…¥æ¸è¿›ç‰¹å¾ä¼˜åŒ–ï¼ˆProgressive Feature Optimizationï¼ŒPFOï¼‰ï¼Œä»¥æ”¹å–„é‡å»ºå›¾åƒçš„è´¨é‡å¹¶æé«˜å…¶è¯­ä¹‰ä¿çœŸåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é«˜æ¸…åœºæ™¯ã€éåˆ†å¸ƒè®¾ç½®ä»¥åŠå¯¹æŠ—æ›´æ·±æ›´å¤æ‚æ·±åº¦ç¥ç»ç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ”»å‡»æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰çš„å¤æ‚æ€§å¢é•¿æ¨åŠ¨äº†Split Inferenceï¼ˆSIï¼‰çš„åº”ç”¨ï¼Œä»¥ä¼˜åŒ–è®¡ç®—å¹¶ä¿æŠ¤éšç§ã€‚</li>
<li>æ•°æ®é‡å»ºæ”»å‡»ï¼ˆDRAsï¼‰æ­ç¤ºäº†ä¸­é—´ç‰¹å¾å¯èƒ½è¢«ç”¨äºæ¢å¤æ•æ„Ÿè¾“å…¥æ•°æ®çš„é£é™©ã€‚</li>
<li>ç°æœ‰DRAsé€šå¸¸ä»…åœ¨æµ…å±‚æ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œä¸”æœªèƒ½å……åˆ†åˆ©ç”¨è¯­ä¹‰å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºGANçš„DRAæ¡†æ¶ï¼Œå¼•å…¥æ¸è¿›ç‰¹å¾ä¼˜åŒ–ï¼ˆPFOï¼‰æé«˜é‡å»ºå›¾åƒçš„è´¨é‡å’Œè¯­ä¹‰ä¿çœŸåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åˆ†è§£ç”Ÿæˆå™¨ä¸ºå±‚æ¬¡å—å¹¶å¢é‡ä¼˜åŒ–ä¸­é—´è¡¨ç¤ºæ¥å®ç°ã€‚</li>
<li>L1-ballçº¦æŸè¢«å¼•å…¥ä»¥ä¼˜åŒ–ç¨³å®šæ€§å’Œæé«˜å›¾åƒé€¼çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54fff541b55d8fd7c223e0e571d9efec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7873759da6809fd4e60c464b2b06ccc1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f1750f769c8d6b44376df275ce5b43e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27e96681eaff16f54dca0604fc89986f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c78917b622581b741a7288fdacb454c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13679b502687e67a732c944c8a66abd0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-we-make-NeRF-based-visual-localization-privacy-preserving"><a href="#Can-we-make-NeRF-based-visual-localization-privacy-preserving" class="headerlink" title="Can we make NeRF-based visual localization privacy-preserving?"></a>Can we make NeRF-based visual localization privacy-preserving?</h2><p><strong>Authors:Maxime Pietrantoni, Martin Humenberger, Torsten Sattler, Gabriela Csurka</strong></p>
<p>Visual localization (VL) is the task of estimating the camera pose in a known scene. VL methods, a.o., can be distinguished based on how they represent the scene, e.g., explicitly through a (sparse) point cloud or a collection of images or implicitly through the weights of a neural network. Recently, NeRF-based methods have become popular for VL. While NeRFs offer high-quality novel view synthesis, they inadvertently encode fine scene details, raising privacy concerns when deployed in cloud-based localization services as sensitive information could be recovered. In this paper, we tackle this challenge on two ends. We first propose a new protocol to assess privacy-preservation of NeRF-based representations. We show that NeRFs trained with photometric losses store fine-grained details in their geometry representations, making them vulnerable to privacy attacks, even if the head that predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving Neural Segmentation Field), a NeRF variant trained with segmentation supervision instead of RGB images. These segmentation labels are learned in a self-supervised manner, ensuring they are coarse enough to obscure identifiable scene details while remaining discriminativeness in 3D. The segmentation space of ppNeSF can be used for accurate visual localization, yielding state-of-the-art results. </p>
<blockquote>
<p>è§†è§‰å®šä½ï¼ˆVLï¼‰æ˜¯ä¼°è®¡å·²çŸ¥åœºæ™¯ä¸­çš„ç›¸æœºå§¿æ€çš„ä»»åŠ¡ã€‚VLæ–¹æ³•ï¼Œå¯ä»¥åŸºäºå®ƒä»¬å¦‚ä½•è¡¨ç¤ºåœºæ™¯æ¥åŒºåˆ†ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡ï¼ˆç¨€ç–ï¼‰ç‚¹äº‘æˆ–å›¾åƒé›†åˆè¿›è¡Œæ˜¾å¼è¡¨ç¤ºï¼Œæˆ–é€šè¿‡ç¥ç»ç½‘ç»œçš„æƒé‡è¿›è¡Œéšå¼è¡¨ç¤ºã€‚æœ€è¿‘ï¼ŒåŸºäºNeRFçš„æ–¹æ³•åœ¨VLä¸­å˜å¾—æµè¡Œã€‚è™½ç„¶NeRFæä¾›äº†é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆï¼Œä½†å®ƒä»¬æ— æ„ä¸­ç¼–ç äº†åœºæ™¯çš„ç»†èŠ‚ä¿¡æ¯ï¼Œå½“éƒ¨ç½²åœ¨äº‘ç«¯çš„å®šä½æœåŠ¡ä¸­æ—¶å¼•å‘äº†éšç§æ‹…å¿§ï¼Œå› ä¸ºå¯èƒ½æ¢å¤å‡ºæ•æ„Ÿä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»ä¸¤ä¸ªæ–¹é¢è§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åè®®æ¥è¯„ä¼°åŸºäºNeRFè¡¨ç¤ºçš„éšç§ä¿æŠ¤ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œç”¨å…‰åº¦æŸå¤±è®­ç»ƒçš„NeRFåœ¨å…¶å‡ ä½•è¡¨ç¤ºä¸­å­˜å‚¨äº†è¯¦ç»†çš„ç»†èŠ‚ä¿¡æ¯ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°éšç§æ”»å‡»ï¼Œå³ä½¿ç§»é™¤äº†é¢„æµ‹é¢œè‰²çš„å¤´éƒ¨ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ppNeSFï¼ˆéšç§ä¿æŠ¤ç¥ç»ç½‘ç»œåˆ†å‰²åœºï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨åˆ†å‰²ç›‘ç£è€Œä¸æ˜¯RGBå›¾åƒè®­ç»ƒçš„NeRFå˜ä½“ã€‚è¿™äº›åˆ†å‰²æ ‡ç­¾ä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼å­¦ä¹ ï¼Œç¡®ä¿å®ƒä»¬è¶³å¤Ÿç²—ç•¥ä»¥æ©ç›–å¯è¯†åˆ«çš„åœºæ™¯ç»†èŠ‚ï¼ŒåŒæ—¶åœ¨3Dä¸­ä¿æŒè¾¨åˆ«åŠ›ã€‚ppNeSFçš„åˆ†å‰²ç©ºé—´å¯ç”¨äºç²¾ç¡®è§†è§‰å®šä½ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18971v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰å®šä½ï¼ˆVLï¼‰æ˜¯ä¼°è®¡å·²çŸ¥åœºæ™¯ä¸­ç›¸æœºå§¿æ€çš„ä»»åŠ¡ã€‚VLæ–¹æ³•å¯æ ¹æ®å¦‚ä½•è¡¨ç¤ºåœºæ™¯æ¥åŒºåˆ†ï¼Œä¾‹å¦‚é€šè¿‡ï¼ˆç¨€ç–ï¼‰ç‚¹äº‘æˆ–å›¾åƒé›†åˆæ˜¾å¼è¡¨ç¤ºï¼Œæˆ–é€šè¿‡ç¥ç»ç½‘ç»œæƒé‡éšå¼è¡¨ç¤ºã€‚æœ€è¿‘ï¼ŒåŸºäºNeRFçš„æ–¹æ³•åœ¨VLä¸­å˜å¾—æµè¡Œã€‚è™½ç„¶NeRFæä¾›äº†é«˜è´¨é‡çš„æ–°è§†å›¾åˆæˆï¼Œä½†å®ƒä»¬æ— æ„ä¸­ç¼–ç äº†åœºæ™¯çš„ç²¾ç»†ç»†èŠ‚ï¼Œå½“éƒ¨ç½²åœ¨äº‘ç«¯çš„å®šä½æœåŠ¡ä¸­æ—¶ä¼šå¼•å‘éšç§æ‹…å¿§ï¼Œå› ä¸ºå¯èƒ½æ¢å¤æ•æ„Ÿä¿¡æ¯ã€‚æœ¬æ–‡è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„ä¸¤ç«¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°åè®®æ¥è¯„ä¼°NeRFè¡¨ç¤ºçš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨å…‰åº¦æŸå¤±è®­ç»ƒçš„NeRFåœ¨å…¶å‡ ä½•è¡¨ç¤ºä¸­å­˜å‚¨äº†ç²¾ç»†çš„ç»†èŠ‚ï¼Œå³ä½¿ç§»é™¤é¢„æµ‹é¢œè‰²çš„å¤´éƒ¨ï¼Œä¹Ÿå®¹æ˜“å—åˆ°éšç§æ”»å‡»ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ppNeSFï¼ˆéšç§ä¿æŠ¤ç¥ç»åˆ†å‰²åœºï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨åˆ†å‰²ç›‘ç£è€Œä¸æ˜¯RGBå›¾åƒè®­ç»ƒçš„NeRFå˜ä½“ã€‚è¿™äº›åˆ†å‰²æ ‡ç­¾ä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼å­¦ä¹ ï¼Œç¡®ä¿å®ƒä»¬è¶³å¤Ÿç²—ç³™ä»¥æ©ç›–åœºæ™¯çš„å¯è¯†åˆ«ç»†èŠ‚ï¼ŒåŒæ—¶åœ¨3Dä¸­ä¿æŒè¾¨åˆ«åŠ›ã€‚ppNeSFçš„åˆ†å‰²ç©ºé—´å¯ç”¨äºå‡†ç¡®è§†è§‰å®šä½ï¼Œå–å¾—æœ€æ–°ç»“æœã€‚</p>
<p><strong>è¦ç‚¹æ€»ç»“</strong></p>
<ol>
<li>åŸºäºNeRFçš„æ–¹æ³•å·²æˆä¸ºè§†è§‰å®šä½çš„ä¸»æµæ–¹æ³•ï¼Œå¹¶å…·æœ‰è¾ƒé«˜çš„å®šä½å‡†ç¡®æ€§ã€‚</li>
<li>NeRFå› å…¶å¯¹åœºæ™¯çš„ç²¾ç»†ç»†èŠ‚ç¼–ç è€Œåœ¨äº‘ç«¯å®šä½æœåŠ¡ä¸­å¼•å‘éšç§æ‹…å¿§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åè®®æ¥è¯„ä¼°NeRFåœ¨éšç§ä¿æŠ¤æ–¹é¢çš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºåŸºäºå…‰åº¦æŸå¤±è®­ç»ƒçš„NeRFæ˜“å—éšç§æ”»å‡»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„NeRFå˜ä½“ppNeSFï¼Œé€šè¿‡åˆ†å‰²ç›‘ç£è®­ç»ƒï¼Œèƒ½åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ä¿æŒå¯¹åœºæ™¯çš„ç²¾ç»†è¡¨ç¤ºå’Œå®šä½å‡†ç¡®æ€§ã€‚</li>
<li>ppNeSFçš„åˆ†å‰²ç©ºé—´å¯ä»¥ç”¨äºå‡†ç¡®è§†è§‰å®šä½ï¼Œå¹¶å–å¾—äº†ä¸šç•Œæœ€ä½³æ•ˆæœã€‚</li>
<li>æå‡ºäº†ä»¥è‡ªæˆ‘ç›‘ç£æ–¹å¼å­¦ä¹ åˆ†å‰²æ ‡ç­¾çš„æ–¹æ³•ï¼Œç¡®ä¿äº†éšç§ä¿æŠ¤å’Œå®šä½å‡†ç¡®æ€§çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e51164fcbef2b6640e429bf54446bd2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-671451410c531db70e44df82262c46f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-594aeeee4e895085048e87b6d14ec3fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20d68586ca67b45502c92e4ccebed34a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c620a1b71b02d75211718f364fe6c90d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Align-3D-Representation-and-Text-Embedding-for-3D-Content-Personalization"><a href="#Align-3D-Representation-and-Text-Embedding-for-3D-Content-Personalization" class="headerlink" title="Align 3D Representation and Text Embedding for 3D Content   Personalization"></a>Align 3D Representation and Text Embedding for 3D Content   Personalization</h2><p><strong>Authors:Qi Song, Ziyuan Luo, Ka Chun Cheung, Simon See, Renjie Wan</strong></p>
<p>Recent advances in NeRF and 3DGS have significantly enhanced the efficiency and quality of 3D content synthesis. However, efficient personalization of generated 3D content remains a critical challenge. Current 3D personalization approaches predominantly rely on knowledge distillation-based methods, which require computationally expensive retraining procedures. To address this challenge, we propose \textbf{Invert3D}, a novel framework for convenient 3D content personalization. Nowadays, vision-language models such as CLIP enable direct image personalization through aligned vision-text embedding spaces. However, the inherent structural differences between 3D content and 2D images preclude direct application of these techniques to 3D personalization. Our approach bridges this gap by establishing alignment between 3D representations and text embedding spaces. Specifically, we develop a camera-conditioned 3D-to-text inverse mechanism that projects 3D contents into a 3D embedding aligned with text embeddings. This alignment enables efficient manipulation and personalization of 3D content through natural language prompts, eliminating the need for computationally retraining procedures. Extensive experiments demonstrate that Invert3D achieves effective personalization of 3D content. Our work is available at: <a target="_blank" rel="noopener" href="https://github.com/qsong2001/Invert3D">https://github.com/qsong2001/Invert3D</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒNeRFå’Œ3DGSçš„è¿›æ­¥å¤§å¤§æé«˜äº†3Då†…å®¹åˆæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚ç„¶è€Œï¼Œç”Ÿæˆ3Då†…å®¹çš„ä¸ªæ€§åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚å½“å‰çš„3Dä¸ªæ€§åŒ–æ–¹æ³•ä¸»è¦ä¾èµ–äºåŸºäºçŸ¥è¯†è’¸é¦çš„æ–¹æ³•ï¼Œè¿™éœ€è¦è®¡ç®—æ˜‚è´µçš„å†è®­ç»ƒè¿‡ç¨‹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œInvert3Dâ€ï¼Œä¸€ä¸ªä¾¿äº3Då†…å®¹ä¸ªæ€§åŒ–çš„æ–°å‹æ¡†æ¶ã€‚å¦‚ä»Šï¼Œè¯¸å¦‚CLIPä¹‹ç±»çš„è§†è§‰è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡å¯¹é½çš„è§†è§‰æ–‡æœ¬åµŒå…¥ç©ºé—´å®ç°ç›´æ¥å›¾åƒä¸ªæ€§åŒ–ã€‚ç„¶è€Œï¼Œ3Då†…å®¹ä¸2Då›¾åƒä¹‹é—´çš„å†…åœ¨ç»“æ„å·®å¼‚ä½¿å¾—è¿™äº›æŠ€æœ¯æ— æ³•ç›´æ¥åº”ç”¨äº3Dä¸ªæ€§åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å»ºç«‹3Dè¡¨ç¤ºä¸æ–‡æœ¬åµŒå…¥ç©ºé—´ä¹‹é—´çš„å¯¹é½æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å—ç›¸æœºæ¡ä»¶é™åˆ¶çš„3Dåˆ°æ–‡æœ¬çš„é€†å‘æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å°†3Då†…å®¹æŠ•å½±åˆ°ä¸æ–‡æœ¬åµŒå…¥å¯¹é½çš„3DåµŒå…¥ä¸­ã€‚è¿™ç§å¯¹é½å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°3Då†…å®¹çš„é«˜æ•ˆæ“ä½œå’Œä¸ªæ€§åŒ–ï¼Œè€Œæ— éœ€è¿›è¡Œè®¡ç®—æœºå†è®­ç»ƒè¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒInvert3Dèƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„3Då†…å®¹ä¸ªæ€§åŒ–ã€‚æˆ‘ä»¬çš„å·¥ä½œåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/qsong2001/Invert3D">https://github.com/qsong2001/Invert3D</a> å¯ä¾›æŸ¥é˜…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16932v1">PDF</a> </p>
<p><strong>Summary</strong><br>NeRFå’Œ3DGSçš„æœ€æ–°è¿›å±•å¤§å¤§æé«˜äº†3Då†…å®¹åˆæˆçš„æ•ˆç‡å’Œè´¨é‡ï¼Œä½†ç”Ÿæˆå†…å®¹çš„ä¸ªæ€§åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–åŸºäºçŸ¥è¯†è’¸é¦çš„æ–¹æ³•ï¼Œè¿™éœ€è¦æ˜‚è´µçš„é‡æ–°è®­ç»ƒè¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Invert3Dæ¡†æ¶ï¼Œé€šè¿‡å»ºç«‹3Dè¡¨ç¤ºä¸æ–‡æœ¬åµŒå…¥ç©ºé—´çš„å¯¹é½ï¼Œå®ç°ä¾¿æ·çš„3Då†…å®¹ä¸ªæ€§åŒ–ã€‚è¯¥æ¡†æ¶é€šè¿‡ç›¸æœºæ¡ä»¶çš„3Dåˆ°æ–‡æœ¬çš„é€†å‘æœºåˆ¶ï¼Œå°†3Då†…å®¹æŠ•å½±åˆ°ä¸æ–‡æœ¬åµŒå…¥å¯¹é½çš„3DåµŒå…¥ç©ºé—´ï¼Œä»è€Œå®ç°é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºé«˜æ•ˆæ“ä½œå’Œä¸ªæ€§åŒ–3Då†…å®¹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFå’Œ3DGSæŠ€æœ¯æé«˜äº†3Då†…å®¹åˆæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚</li>
<li>3Då†…å®¹ä¸ªæ€§åŒ–ä»æ˜¯å…³é”®æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–çŸ¥è¯†è’¸é¦ï¼Œéœ€æ˜‚è´µé‡æ–°è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>Invert3Dæ¡†æ¶æ—¨åœ¨è§£å†³æ­¤æŒ‘æˆ˜ï¼Œå®ç°ä¾¿æ·çš„3Då†…å®¹ä¸ªæ€§åŒ–ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å»ºç«‹3Dè¡¨ç¤ºä¸æ–‡æœ¬åµŒå…¥ç©ºé—´çš„å¯¹é½æ¥å·¥ä½œã€‚</li>
<li>é€šè¿‡ç›¸æœºæ¡ä»¶çš„3Dåˆ°æ–‡æœ¬çš„é€†å‘æœºåˆ¶ï¼Œå°†3Då†…å®¹æŠ•å½±åˆ°ä¸æ–‡æœ¬åµŒå…¥å¯¹é½çš„3DåµŒå…¥ç©ºé—´ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿å¾—é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºé«˜æ•ˆæ“ä½œå’Œä¸ªæ€§åŒ–3Då†…å®¹æˆä¸ºå¯èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16932">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b06f9937b2408489f9917b3006ffc73e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73648665b730e3f4c413eace652cdf48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4a6c005961984c910e943512edc8230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2e4a9bdf3d21a222835da3dc630b03b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-3D-Gaussian-Splatting-Applications-Segmentation-Editing-and-Generation"><a href="#A-Survey-on-3D-Gaussian-Splatting-Applications-Segmentation-Editing-and-Generation" class="headerlink" title="A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing,   and Generation"></a>A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing,   and Generation</h2><p><strong>Authors:Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, Henghui Ding</strong></p>
<p>3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at <a target="_blank" rel="noopener" href="https://github.com/heshuting555/Awesome-3DGS-Applications">https://github.com/heshuting555/Awesome-3DGS-Applications</a>. </p>
<blockquote>
<p>3Dé«˜æ–¯å»¶å±•ï¼ˆ3DGSï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œè¿‘æœŸå·²ç»æˆä¸ºç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰åœ¨ä¸‰ç»´åœºæ™¯è¡¨ç¤ºæ–¹é¢çš„æœ‰åŠ›æ›¿ä»£å“ï¼Œå®ƒä»¥çœŸå®æ€§èƒ½è¡¨ç°æä¾›é«˜ä¿çœŸç…§ç‰‡çº§æ¸²æŸ“ã€‚é™¤äº†æ–°è§†è§’åˆæˆä¹‹å¤–ï¼Œ3DGSçš„æ˜ç¡®å’Œç´§å‡‘ç‰¹æ€§è¿˜ä½¿å¾—å®ƒåœ¨éœ€è¦å‡ ä½•å’Œè¯­ä¹‰ç†è§£çš„ä¸‹æ¸¸åº”ç”¨æ–¹é¢å…·æœ‰å¹¿æ³›åº”ç”¨ã€‚è¿™ç¯‡ç»¼è¿°å…¨é¢æ¦‚è¿°äº†3DGSåº”ç”¨çš„æœ€æ–°è¿›å±•ã€‚å®ƒé¦–å…ˆä»‹ç»äº†æ”¯æŒ3DGSåº”ç”¨ä¸­çš„è¯­ä¹‰ç†è§£å’Œæ§åˆ¶çš„äºŒç»´åŸºç¡€æ¨¡å‹ï¼Œç„¶åå›é¡¾äº†åŸºäºNeRFçš„æ–¹æ³•ï¼Œä»¥ä¸ºå…¶3DGSå¯¹åº”ç‰©æä¾›ä¿¡æ¯ã€‚æ¥ç€æˆ‘ä»¬å°†3DGSåº”ç”¨åˆ†ç±»ä¸ºåˆ†å‰²ã€ç¼–è¾‘ã€ç”Ÿæˆå’Œå…¶ä»–åŠŸèƒ½ä»»åŠ¡ã€‚å¯¹äºæ¯ä¸€é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬æ€»ç»“äº†ä»£è¡¨æ€§æ–¹æ³•ã€ç›‘ç£ç­–ç•¥å’Œå­¦ä¹ èŒƒå¼ï¼Œå¹¶å¼ºè°ƒäº†å…±äº«è®¾è®¡åŸåˆ™å’Œæ–°å…´è¶‹åŠ¿ã€‚æ­¤å¤–ï¼Œè¿˜æ€»ç»“äº†å¸¸ç”¨æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œä»¥åŠåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå¯¹è¿‘æœŸæ–¹æ³•è¿›è¡Œå¯¹æ¯”åˆ†æã€‚ä¸ºäº†æ”¯æŒæŒç»­çš„ç ”ç©¶å’Œå¼€å‘ï¼Œè®ºæ–‡ã€ä»£ç å’Œèµ„æºçš„æŒç»­æ›´æ–°ä»“åº“å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/heshuting555/Awesome-3DGS-Applications%E3%80%82">https://github.com/heshuting555/Awesome-3DGS-Applicationsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09977v2">PDF</a> GitHub Repo:   <a target="_blank" rel="noopener" href="https://github.com/heshuting555/Awesome-3DGS-Applications">https://github.com/heshuting555/Awesome-3DGS-Applications</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œæœ¬æ–‡å…¨é¢æ¦‚è¿°äº†å…¶åœ¨ä¸‰ç»´åœºæ™¯è¡¨ç¤ºæ–¹é¢çš„åº”ç”¨ã€‚æ–‡ç« ä»‹ç»äº†æ”¯æŒä¸‰ç»´é«˜æ–¯æ‹¼è´´åº”ç”¨çš„äºŒç»´åŸºç¡€æ¨¡å‹ï¼Œå›é¡¾äº†åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ–¹æ³•ä»¥å¯å‘å…¶å¯¹åº”çš„3DGSæŠ€æœ¯ã€‚æ–‡ç« å°†3DGSåº”ç”¨åˆ†ä¸ºåˆ†å‰²ã€ç¼–è¾‘ã€ç”Ÿæˆå’Œå…¶ä»–åŠŸèƒ½ä»»åŠ¡ï¼Œå¹¶å¯¹æ¯ä¸ªä»»åŠ¡çš„ä»£è¡¨æ€§æ–¹æ³•ã€ç›‘ç£ç­–ç•¥å’Œå­¦èŒƒå¼è¿›è¡Œäº†æ€»ç»“ï¼ŒåŒæ—¶å¼ºè°ƒäº†å…±äº«çš„è®¾è®¡åŸåˆ™å’Œæ–°å…´è¶‹åŠ¿ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¦‚è¿°äº†å¸¸ç”¨æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œå¹¶åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå¯¹è¿‘æœŸæ–¹æ³•è¿›è¡Œå¯¹æ¯”åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D Gaussian Splatting (3DGS) ä½œä¸ºNeural Radiance Fields (NeRF) çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆï¼Œå¯å®ç°é«˜è´¨é‡çš„ä¸‰ç»´åœºæ™¯è¡¨ç¤ºã€‚</li>
<li>3DGS å…·æœ‰é«˜ä¿çœŸåº¦ç…§ç‰‡çº§æ¸²æŸ“å’Œå®æ—¶æ€§èƒ½ã€‚</li>
<li>3DGS æ”¯æŒå¹¿æ³›çš„ä¸‹æ¸¸åº”ç”¨ï¼ŒåŒ…æ‹¬åˆ†å‰²ã€ç¼–è¾‘ã€ç”Ÿæˆå’Œå…¶ä»–åŠŸèƒ½ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å‡ ä½•å’Œè¯­ä¹‰ç†è§£ã€‚</li>
<li>æ–‡ç« å›é¡¾äº†NeRFæ–¹æ³•ï¼Œä¸ºç†è§£å…¶å¯¹åº”çš„3DGSæŠ€æœ¯æä¾›äº†åŸºç¡€ã€‚</li>
<li>æ–‡ç« æä¾›äº†æ•°æ®é›†å’Œè¯„ä¼°åè®®çš„æ¦‚è¿°ï¼Œå¹¶è¿›è¡Œè¿‘æœŸæ–¹æ³•çš„å¯¹æ¯”åˆ†æã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90658ca9dc3cb65d83dc753592e01736.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-394820f81e7c358bb3bfd5f8c441fa94.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Learning-Heterogeneous-Mixture-of-Scene-Experts-for-Large-scale-Neural-Radiance-Fields"><a href="#Learning-Heterogeneous-Mixture-of-Scene-Experts-for-Large-scale-Neural-Radiance-Fields" class="headerlink" title="Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural   Radiance Fields"></a>Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural   Radiance Fields</h2><p><strong>Authors:Zhenxing Mi, Ping Yin, Xue Xiao, Dan Xu</strong></p>
<p>Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decompose scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes ($&gt;6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released at <a target="_blank" rel="noopener" href="https://github.com/MiZhenxing/Switch-NeRF">https://github.com/MiZhenxing/Switch-NeRF</a>. </p>
<blockquote>
<p>è¿‘æœŸçš„NeRFåœ¨å¤§è§„æ¨¡åœºæ™¯åº”ç”¨çš„æ–¹æ³•å¼ºè°ƒäº†åœºæ™¯åˆ†è§£å¯¹äºå¯æ‰©å±•NeRFçš„é‡è¦æ€§ã€‚å°½ç®¡å·²ç»å®ç°äº†åˆç†çš„å¯æ‰©å±•æ€§ï¼Œä½†ä»å­˜åœ¨å‡ ä¸ªå…³é”®é—®é¢˜å°šæœªæ¢ç´¢ï¼Œå³å¯å­¦ä¹ çš„åˆ†è§£ã€åœºæ™¯å¼‚è´¨æ€§çš„å»ºæ¨¡å’Œå»ºæ¨¡æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Switch-NeRF++ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼‚è´¨æ··åˆå“ˆå¸Œä¸“å®¶ï¼ˆHMoHEï¼‰ç½‘ç»œï¼Œå®ƒåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…è§£å†³äº†è¿™äº›æŒ‘æˆ˜ã€‚è¿™æ˜¯ä¸€ç§é«˜åº¦å¯æ‰©å±•çš„NeRFï¼Œèƒ½å¤Ÿä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼æœ‰æ•ˆåœ°å¯¹å¤§è§„æ¨¡åœºæ™¯è¿›è¡Œå¼‚è´¨åˆ†è§£å’Œå¼‚è´¨NeRFå»ºæ¨¡ã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œé—¨æ§ç½‘ç»œå­¦ä¹ åˆ†è§£åœºæ™¯å¹¶å°†3Dç‚¹åˆ†é…ç»™ä¸“é—¨çš„NeRFä¸“å®¶ã€‚é—¨æ§ç½‘ç»œé€šè¿‡æˆ‘ä»¬æå‡ºçš„ç¨€ç–é—¨æ§æ··åˆä¸“å®¶ï¼ˆMoEï¼‰NeRFæ¡†æ¶ä¸ä¸“å®¶å…±åŒä¼˜åŒ–ã€‚æˆ‘ä»¬é‡‡ç”¨äº†åŸºäºå“ˆå¸Œçš„é—¨æ§ç½‘ç»œå’Œä¸åŒçš„å¼‚è´¨å“ˆå¸Œä¸“å®¶ã€‚åŸºäºå“ˆå¸Œçš„é—¨æ§ç½‘ç»œæœ‰æ•ˆåœ°å­¦ä¹ äº†å¤§è§„æ¨¡åœºæ™¯çš„åˆ†è§£ã€‚ä¸åŒçš„å¼‚è´¨å“ˆå¸Œä¸“å®¶ç”±ä¸åŒåˆ†è¾¨ç‡èŒƒå›´çš„å“ˆå¸Œç½‘æ ¼ç»„æˆï¼Œèƒ½å¤Ÿå®ç°ä¸åŒåœºæ™¯éƒ¨åˆ†å¼‚è´¨è¡¨ç¤ºçš„æœ‰æ•ˆå­¦ä¹ ã€‚è¿™äº›è®¾è®¡é€‰æ‹©ä½¿æˆ‘ä»¬çš„æ¡†æ¶æˆä¸ºç”¨äºç°å®ä¸–ç•Œå¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡çš„ç«¯åˆ°ç«¯å’Œé«˜åº¦å¯æ‰©å±•çš„NeRFè§£å†³æ–¹æ¡ˆï¼Œä»¥å®ç°è´¨é‡å’Œæ•ˆç‡ã€‚æˆ‘ä»¬åœ¨ç°æœ‰çš„å¤§è§„æ¨¡NeRFæ•°æ®é›†å’Œæ¥è‡ªUrbanBISçš„æ–°çš„å¤§è§„æ¨¡åœºæ™¯ï¼ˆ&gt; 6.5km^2ï¼‰æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è½»æ¾åœ°æ‰©å±•åˆ°å„ç§å¤§è§„æ¨¡åœºæ™¯ï¼Œå¹¶å®ç°æœ€å…ˆè¿›çš„åœºæ™¯æ¸²æŸ“ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¸²æŸ“æ–¹é¢ä¸Switch-NeRFç›¸æ¯”åˆ†åˆ«å®ç°äº†8å€å’Œ1 ç»“ç‚¹çš„åŠ é€Ÿã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiZhenxing/Switch-NeRF%E5%B8%AE%E5%A4%9F%E3%80%82">https://github.com/MiZhenxing/Switch-NeRFå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02005v2">PDF</a> Accepted by TPAMI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Switch-NeRF++ï¼Œä¸€ç§åŸºäºHeterogeneous Mixture of Hash Experts (HMoHE)ç½‘ç»œçš„ç«¯åˆ°ç«¯é«˜æ•ˆNeRFæ¨¡å‹ã€‚è¯¥æ¨¡å‹è§£å†³äº†å¤§è§„æ¨¡åœºæ™¯NeRFä¸­çš„å‡ ä¸ªå…³é”®é—®é¢˜ï¼ŒåŒ…æ‹¬åœºæ™¯åˆ†è§£ã€åœºæ™¯å¼‚è´¨æ€§çš„å»ºæ¨¡å’Œå»ºæ¨¡æ•ˆç‡ã€‚Switch-NeRF++é€šè¿‡å¼•å…¥åŸºäºå“ˆå¸Œçš„é—¨æ§ç½‘ç»œå’Œä¸åŒçš„å¼‚è´¨å“ˆå¸Œä¸“å®¶æ¥å®ç°å¤§è§„æ¨¡åœºæ™¯çš„å¼‚è´¨åˆ†è§£å’Œé«˜æ•ˆå»ºæ¨¡ã€‚è¯¥æ–¹æ³•å®ç°äº†é«˜è´¨é‡çš„æ¸²æŸ“æ•ˆæœå’Œæ˜¾è‘—çš„è®¡ç®—æ•ˆç‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Switch-NeRF++æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„NeRFæ¨¡å‹ï¼Œç”¨äºå¤„ç†å¤§è§„æ¨¡åœºæ™¯ã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº†Heterogeneous Mixture of Hash Experts (HMoHE)ç½‘ç»œæ¥è§£å†³åœºæ™¯åˆ†è§£å’Œåœºæ™¯å¼‚è´¨æ€§çš„å»ºæ¨¡é—®é¢˜ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ä¸€ä¸ªé—¨æ§ç½‘ç»œå­¦ä¹ åœºæ™¯åˆ†è§£ï¼Œå¹¶å°†3Dç‚¹åˆ†é…ç»™ç‰¹å®šçš„NeRFä¸“å®¶ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨åŸºäºå“ˆå¸Œçš„é—¨æ§ç½‘ç»œå’Œä¸åŒçš„å¼‚è´¨å“ˆå¸Œä¸“å®¶æ¥å®ç°é«˜æ•ˆçš„åœºæ™¯åˆ†è§£ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡èŒƒå›´çš„å“ˆå¸Œç½‘æ ¼ï¼Œæ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ åœºæ™¯çš„å¼‚è´¨è¡¨ç¤ºã€‚</li>
<li>Switch-NeRF++åœ¨å¤§å‹åœºæ™¯æ¸²æŸ“ä¸­å®ç°äº†é«˜è´¨é‡å’Œé«˜æ•ˆæ€§ï¼Œç›¸æ¯”Switch-NeRFæœ‰8å€çš„è®­ç»ƒåŠ é€Ÿå’Œ16å€çš„æ¸²æŸ“åŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2507c25cfa219ac6bfa089017c87d4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be857d65831ebe7dd6889595af3a1659.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e367880c071cdfe7a77deda3d4c7f03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e852baa47d2e01d993b822fcd0a72350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bca64570592ea1d02d92e88e7c1edb88.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Sat-DN-Implicit-Surface-Reconstruction-from-Multi-View-Satellite-Images-with-Depth-and-Normal-Supervision"><a href="#Sat-DN-Implicit-Surface-Reconstruction-from-Multi-View-Satellite-Images-with-Depth-and-Normal-Supervision" class="headerlink" title="Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images   with Depth and Normal Supervision"></a>Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images   with Depth and Normal Supervision</h2><p><strong>Authors:Tianle Liu, Shuangming Zhao, Wanshou Jiang, Bingxuan Guo</strong></p>
<p>With advancements in satellite imaging technology, acquiring high-resolution multi-view satellite imagery has become increasingly accessible, enabling rapid and location-independent ground model reconstruction. However, traditional stereo matching methods struggle to capture fine details, and while neural radiance fields (NeRFs) achieve high-quality reconstructions, their training time is prohibitively long. Moreover, challenges such as low visibility of building facades, illumination and style differences between pixels, and weakly textured regions in satellite imagery further make it hard to reconstruct reasonable terrain geometry and detailed building facades. To address these issues, we propose Sat-DN, a novel framework leveraging a progressively trained multi-resolution hash grid reconstruction architecture with explicit depth guidance and surface normal consistency constraints to enhance reconstruction quality. The multi-resolution hash grid accelerates training, while the progressive strategy incrementally increases the learning frequency, using coarse low-frequency geometry to guide the reconstruction of fine high-frequency details. The depth and normal constraints ensure a clear building outline and correct planar distribution. Extensive experiments on the DFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving state-of-the-art results in both qualitative and quantitative evaluations. The code is available at <a target="_blank" rel="noopener" href="https://github.com/costune/SatDN">https://github.com/costune/SatDN</a>. </p>
<blockquote>
<p>éšç€å«æ˜ŸæˆåƒæŠ€æœ¯çš„è¿›æ­¥ï¼Œè·å–é«˜åˆ†è¾¨ç‡çš„å¤šè§†è§’å«æ˜Ÿå›¾åƒå˜å¾—è¶Šæ¥è¶Šå®¹æ˜“ï¼Œèƒ½å¤Ÿå®ç°å¿«é€Ÿä¸”ç‹¬ç«‹äºä½ç½®çš„åœ°é¢æ¨¡å‹é‡å»ºã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç«‹ä½“åŒ¹é…æ–¹æ³•éš¾ä»¥æ•æ‰ç²¾ç»†ç»†èŠ‚ï¼Œè™½ç„¶ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å¯ä»¥å®ç°é«˜è´¨é‡çš„é‡å»ºï¼Œä½†å…¶è®­ç»ƒæ—¶é—´éå¸¸é•¿ã€‚æ­¤å¤–ï¼Œå«æ˜Ÿå›¾åƒä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚å»ºç­‘ç‰©å¢™é¢çš„ä½å¯è§æ€§ã€åƒç´ ä¹‹é—´çš„å…‰ç…§å’Œé£æ ¼å·®å¼‚ä»¥åŠå«æ˜Ÿå›¾åƒä¸­çº¹ç†è¾ƒå¼±çš„åŒºåŸŸï¼Œè¿›ä¸€æ­¥åŠ å¤§äº†åˆç†åœ°å½¢å‡ ä½•å’Œè¯¦ç»†å»ºç­‘ç‰©å¢™é¢é‡å»ºçš„éš¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Sat-DNï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨é€æ­¥è®­ç»ƒçš„å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼é‡å»ºæ¶æ„çš„æ–°å‹æ¡†æ¶ï¼Œå…·æœ‰æ˜ç¡®çš„æ·±åº¦æŒ‡å¯¼å’Œè¡¨é¢æ³•çº¿ä¸€è‡´æ€§çº¦æŸï¼Œä»¥æé«˜é‡å»ºè´¨é‡ã€‚å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼å¯ä»¥åŠ é€Ÿè®­ç»ƒï¼Œè€Œé€æ­¥ç­–ç•¥å¯ä»¥é€æ­¥æé«˜å­¦ä¹ é¢‘ç‡ï¼Œä½¿ç”¨ç²—ç³™çš„ä½é¢‘å‡ ä½•æ¥æŒ‡å¯¼ç²¾ç»†é«˜é¢‘ç»†èŠ‚çš„é‡å»ºã€‚æ·±åº¦å’Œæ³•çº¿çº¦æŸç¡®ä¿äº†æ¸…æ™°çš„å»ºç­‘è½®å»“å’Œæ­£ç¡®çš„å¹³é¢åˆ†å¸ƒã€‚åœ¨DFC2019æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSat-DNä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/costune/SatDN">https://github.com/costune/SatDN</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08352v2">PDF</a> </p>
<p><strong>Summary</strong><br>éšç€å«æ˜ŸæˆåƒæŠ€æœ¯çš„è¿›æ­¥ï¼Œè·å–é«˜åˆ†è¾¨ç‡çš„å¤šè§†è§’å«æ˜Ÿå›¾åƒè¶Šæ¥è¶Šå®¹æ˜“ï¼Œæ¨åŠ¨äº†åœ°é¢æ¨¡å‹çš„å¿«é€Ÿé‡å»ºã€‚ç„¶è€Œï¼Œä¼ ç»Ÿç«‹ä½“åŒ¹é…æ–¹æ³•éš¾ä»¥æ•æ‰ç²¾ç»†ç»†èŠ‚ï¼Œè€Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è™½ç„¶èƒ½è¿›è¡Œé«˜è´¨é‡é‡å»ºï¼Œä½†è®­ç»ƒæ—¶é—´è¿‡é•¿ã€‚é’ˆå¯¹å«æ˜Ÿå›¾åƒé‡å»ºä¸­çš„åœ°å½¢å‡ ä½•å’Œå»ºç­‘ç«‹é¢é‡å»ºéš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Sat-DNæ¡†æ¶ï¼Œé‡‡ç”¨æ¸è¿›å¼è®­ç»ƒçš„å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼é‡å»ºæ¶æ„ï¼Œå…·æœ‰æ˜ç¡®çš„æ·±åº¦æŒ‡å¯¼å’Œè¡¨é¢æ³•çº¿ä¸€è‡´æ€§çº¦æŸï¼Œä»¥æé«˜é‡å»ºè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å«æ˜ŸæˆåƒæŠ€æœ¯è¿›æ­¥ä½¿å¾—è·å–é«˜åˆ†è¾¨ç‡å¤šè§†è§’å›¾åƒå˜å¾—å®¹æ˜“ï¼Œæ¨åŠ¨äº†åœ°é¢æ¨¡å‹é‡å»ºã€‚</li>
<li>ä¼ ç»Ÿç«‹ä½“åŒ¹é…æ–¹æ³•éš¾ä»¥æ•æ‰ç²¾ç»†ç»†èŠ‚ï¼Œè€ŒNeRFè®­ç»ƒæ—¶é—´è¿‡é•¿ã€‚</li>
<li>Sat-DNæ¡†æ¶åˆ©ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼é‡å»ºæ¶æ„ï¼ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>æ¸è¿›å¼è®­ç»ƒç­–ç•¥èƒ½é€æ­¥æé«˜å­¦ä¹ é¢‘ç‡ï¼Œåˆ©ç”¨ç²—ä½é¢‘å‡ ä½•å¼•å¯¼ç²¾ç»†é«˜é¢‘ç»†èŠ‚çš„é‡å»ºã€‚</li>
<li>æ·±åº¦çº¦æŸå’Œæ³•çº¿çº¦æŸç¡®ä¿æ¸…æ™°çš„å»ºç­‘è½®å»“å’Œæ­£ç¡®çš„å¹³é¢åˆ†å¸ƒã€‚</li>
<li>åœ¨DFC2019æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSat-DNæ¡†æ¶åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½å®ç°äº†æœ€ä½³ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08352">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aeb95f0fdb868863502b8dc16e6578f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f006cbd5ea732b7cfb8645777d3dbe3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ZIM-Zero-Shot-Image-Matting-for-Anything"><a href="#ZIM-Zero-Shot-Image-Matting-for-Anything" class="headerlink" title="ZIM: Zero-Shot Image Matting for Anything"></a>ZIM: Zero-Shot Image Matting for Anything</h2><p><strong>Authors:Beomyoung Kim, Chanyong Shin, Joonhyun Jeong, Hyungsik Jung, Se-Yun Lee, Sewhan Chun, Dong-Hyun Hwang, Joonsang Yu</strong></p>
<p>The recent segmentation foundation model, Segment Anything Model (SAM), exhibits strong zero-shot segmentation capabilities, but it falls short in generating fine-grained precise masks. To address this limitation, we propose a novel zero-shot image matting model, called ZIM, with two key contributions: First, we develop a label converter that transforms segmentation labels into detailed matte labels, constructing the new SA1B-Matte dataset without costly manual annotations. Training SAM with this dataset enables it to generate precise matte masks while maintaining its zero-shot capability. Second, we design the zero-shot matting model equipped with a hierarchical pixel decoder to enhance mask representation, along with a prompt-aware masked attention mechanism to improve performance by enabling the model to focus on regions specified by visual prompts. We evaluate ZIM using the newly introduced MicroMat-3K test set, which contains high-quality micro-level matte labels. Experimental results show that ZIM outperforms existing methods in fine-grained mask generation and zero-shot generalization. Furthermore, we demonstrate the versatility of ZIM in various downstream tasks requiring precise masks, such as image inpainting and 3D NeRF. Our contributions provide a robust foundation for advancing zero-shot matting and its downstream applications across a wide range of computer vision tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/naver-ai/ZIM">https://github.com/naver-ai/ZIM</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„åˆ†å‰²åŸºç¡€æ¨¡å‹Segment Anything Modelï¼ˆSAMï¼‰å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬åˆ†å‰²èƒ½åŠ›ï¼Œä½†åœ¨ç”Ÿæˆç²¾ç»†ç²’åº¦ç²¾ç¡®è’™ç‰ˆæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹é›¶æ ·æœ¬å›¾åƒæŠ å›¾æ¨¡å‹ZIMï¼Œå…¶ä¸»è¦è´¡çŒ®æœ‰ä¸¤ç‚¹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ ‡ç­¾è½¬æ¢å™¨ï¼Œèƒ½å¤Ÿå°†åˆ†å‰²æ ‡ç­¾è½¬æ¢ä¸ºè¯¦ç»†çš„è’™ç‰ˆæ ‡ç­¾ï¼Œä»è€Œæ„å»ºæ— éœ€æ˜‚è´µæ‰‹åŠ¨æ³¨é‡Šçš„æ–°SA1B-Matteæ•°æ®é›†ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†å¯¹SAMè¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥åœ¨ä¿æŒå…¶é›¶æ ·æœ¬èƒ½åŠ›çš„åŒæ—¶ï¼Œç”Ÿæˆç²¾ç¡®çš„è’™ç‰ˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé…å¤‡åˆ†å±‚åƒç´ è§£ç å™¨çš„é›¶æ ·æœ¬æŠ å›¾æ¨¡å‹ï¼Œä»¥å¢å¼ºè’™ç‰ˆè¡¨ç¤ºï¼ŒåŒæ—¶è®¾è®¡äº†ä¸€ç§æç¤ºæ„ŸçŸ¥çš„æ©ç æ³¨æ„æœºåˆ¶ï¼Œé€šè¿‡ä½¿æ¨¡å‹å…³æ³¨è§†è§‰æç¤ºæŒ‡å®šçš„åŒºåŸŸæ¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨æ–°å¼•å…¥çš„MicroMat-3Kæµ‹è¯•é›†å¯¹ZIMè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥æµ‹è¯•é›†åŒ…å«é«˜è´¨é‡çš„å¾®è§‚çº§åˆ«è’™ç‰ˆæ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZIMåœ¨ç²¾ç»†ç²’åº¦è’™ç‰ˆç”Ÿæˆå’Œé›¶æ ·æœ¬æ³›åŒ–æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ZIMåœ¨å„ç§éœ€è¦ç²¾ç¡®è’™ç‰ˆçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ï¼Œå¦‚å›¾åƒè¡¥å…¨å’Œ3D NeRFã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸ºæ¨è¿›é›¶æ ·æœ¬æŠ å›¾åŠå…¶åœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„ä¸‹æ¸¸åº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/naver-ai/ZIM%E3%80%82">https://github.com/naver-ai/ZIMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00626v2">PDF</a> ICCV 2025 (Highlight)</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹çš„é›¶æ ·æœ¬å›¾åƒæŠ å›¾æ¨¡å‹ZIMè¢«æå‡ºï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨ç²¾ç»†ç²’åº¦é®ç½©ç”Ÿæˆä¸Šçš„ä¸è¶³ã€‚å®ƒé€šè¿‡æ ‡ç­¾è½¬æ¢å™¨å’Œå±‚æ¬¡åŒ–åƒç´ è§£ç å™¨ç­‰æŠ€æœ¯ï¼Œæå‡äº†é®ç½©çš„ç”Ÿæˆç²¾åº¦å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ZIMæ¨¡å‹åœ¨MicroMat-3Kæµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†åœ¨å›¾åƒä¿®å¤å’Œ3D NeRFç­‰éœ€è¦ç²¾ç»†é®ç½©çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZIMæ¨¡å‹è§£å†³äº†SAMåœ¨ç²¾ç»†ç²’åº¦é®ç½©ç”Ÿæˆä¸Šçš„ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ ‡ç­¾è½¬æ¢å™¨ï¼Œå°†åˆ†å‰²æ ‡ç­¾è½¬æ¢ä¸ºè¯¦ç»†çš„é®ç½©æ ‡ç­¾ï¼Œæ„å»ºäº†SA1B-Matteæ•°æ®é›†ï¼Œæ— éœ€æ˜‚è´µçš„äººå·¥æ³¨é‡Šã€‚</li>
<li>é€šè¿‡å±‚æ¬¡åŒ–åƒç´ è§£ç å™¨å¢å¼ºé®ç½©è¡¨ç¤ºï¼Œæé«˜ç”Ÿæˆç²¾ç»†é®ç½©çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥æç¤ºæ„ŸçŸ¥çš„æ©ç æ³¨æ„æœºåˆ¶ï¼Œæ ¹æ®è§†è§‰æç¤ºæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨MicroMat-3Kæµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå±•ç¤ºåœ¨å¤šç§éœ€è¦ç²¾ç»†é®ç½©çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚</li>
<li>ZIMæ¨¡å‹æå‡äº†é›¶æ ·æœ¬æŠ å›¾æŠ€æœ¯çš„æ°´å¹³ï¼Œä¸ºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å¹¿æ³›åº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-87fa17e6193e99aebbe32490668b97ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20909b718c247eb4d1740c8e4d2733e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81561536c231de59ffa9acb585efabae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc66070eb378eb68f5d8cd0233fd15f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5d8001a315695d8187f3f5ac931920e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f0820592e385239ef4b2825c3a2a9c03.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Durian Dual Reference-guided Portrait Animation with Attribute Transfer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-69f2bfe6f45af1301fb21137b1b7cd75.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  SSGaussian Semantic-Aware and Structure-Preserving 3D Style Transfer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
