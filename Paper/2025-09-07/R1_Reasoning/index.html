<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  ArcMemo Abstract Reasoning Composition with Lifelong LLM Memory">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-61193cbc365b65378c62dd7ebbee2363.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="ArcMemo-Abstract-Reasoning-Composition-with-Lifelong-LLM-Memory"><a href="#ArcMemo-Abstract-Reasoning-Composition-with-Lifelong-LLM-Memory" class="headerlink" title="ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory"></a>ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory</h2><p><strong>Authors:Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Zhijian Liu, Zhiting Hu, Lianhui Qin</strong></p>
<p>While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query&#x2F;response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. On the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, we confirm that dynamically updating memory during test-time outperforms an otherwise identical fixed memory setting with additional attempts, supporting the hypothesis that solving more problems and abstracting more patterns to memory enables further solutions in a form of self-improvement. Code available at <a target="_blank" rel="noopener" href="https://github.com/matt-seb-ho/arc_memo">https://github.com/matt-seb-ho/arc_memo</a>. </p>
<blockquote>
<p>åœ¨æ¨ç†æ—¶é—´ç¼©æ”¾ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œè¶Šæ¥è¶Šé•¿å’Œå¼ºå¤§çš„æ¨ç†è½¨è¿¹çš„åŒæ—¶ï¼Œè¿™äº›è½¨è¿¹ä¸­å‘ç°çš„æ¨¡å¼å’Œè§è§£åœ¨é’ˆå¯¹æ–°æŸ¥è¯¢é‡ç½®ä¸Šä¸‹æ–‡çª—å£åå°±ä¼šç«‹å³è¢«ä¸¢å¼ƒã€‚å¤–éƒ¨è®°å¿†æ˜¯ä¿ç•™è¿™äº›å‘ç°çš„ä¸€ç§è‡ªç„¶æ–¹å¼ï¼Œæœ€è¿‘çš„å·¥ä½œå·²ç»æ˜¾ç¤ºå‡ºå®ƒåœ¨éœ€è¦å¤§é‡æ¨ç†çš„ä»»åŠ¡ä¸­çš„æ˜ç¡®ä¼˜åŠ¿ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé€šè¿‡è¶…è¶ŠåŸºäºå®ä¾‹çš„è®°å¿†æ¡ç›®ï¼ˆä¾‹å¦‚ä¸åŸå§‹é—®é¢˜ä¸Šä¸‹æ–‡ç´§å¯†è€¦åˆçš„ç¡®åˆ‡æŸ¥è¯¢&#x2F;å“åº”å¯¹æˆ–æ‘˜è¦ï¼‰ï¼Œä½¿æ­¤ç±»è®°å¿†æ›´å…·å¹¿æ³›çš„å¯é‡ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œè¿™æ˜¯ä¸€ä¸ªæœºä¼šã€‚æˆ‘ä»¬æœç€æ¦‚å¿µå±‚é¢çš„è®°å¿†å‘å±•ï¼šä»è§£å†³æ–¹æ¡ˆè½¨è¿¹ä¸­æç‚¼å‡ºçš„å¯é‡ç”¨ã€æ¨¡å—åŒ–æŠ½è±¡ï¼Œä»¥è‡ªç„¶è¯­è¨€å­˜å‚¨ã€‚å¯¹äºæœªæ¥çš„æŸ¥è¯¢ï¼Œç›¸å…³æ¦‚å¿µä¼šè¢«é€‰æ‹©æ€§åœ°æ£€ç´¢å¹¶é›†æˆåˆ°æç¤ºä¸­ï¼Œå®ç°æµ‹è¯•æ—¶çš„æŒç»­å­¦ä¹ è€Œæ— éœ€æƒé‡æ›´æ–°ã€‚æˆ‘ä»¬çš„è®¾è®¡å¼•å…¥äº†ä»rolloutsä¸­æå–æŠ½è±¡æˆæœå’Œä¸ºæ–°æŸ¥è¯¢æ£€ç´¢æ¡ç›®çš„æ–°ç­–ç•¥ï¼Œä¿ƒè¿›äº†é‡ç”¨å¹¶å…è®¸è®°å¿†éšç€é¢å¤–çš„ç»éªŒè€Œæ‰©å±•ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ARC-AGIåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹äºå¼ºå¤§çš„æ— è®°å¿†åŸºçº¿å–å¾—äº†7.5%çš„ç›¸å¯¹å¢ç›Šï¼Œéšç€æ¨ç†è®¡ç®—çš„æ€§èƒ½æŒç»­æé«˜ã€‚æˆ‘ä»¬å‘ç°æŠ½è±¡æ¦‚å¿µæ˜¯æœ€ä¸€è‡´çš„è®°å¿†è®¾è®¡ï¼Œåœ¨æ‰€æœ‰æµ‹è¯•çš„æ¨ç†è®¡ç®—è§„æ¨¡ä¸Šéƒ½è¶…è¿‡äº†åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯å®ï¼Œåœ¨æµ‹è¯•è¿‡ç¨‹ä¸­åŠ¨æ€æ›´æ–°å†…å­˜ä¼˜äºå›ºå®šå†…å­˜è®¾ç½®ï¼Œæ”¯æŒå‡è®¾ï¼šè§£å†³æ›´å¤šé—®é¢˜å¹¶å°†æ›´å¤šæ¨¡å¼æŠ½è±¡åˆ°å†…å­˜ä¸­ï¼Œèƒ½å¤Ÿä»¥è‡ªæˆ‘æ”¹è¿›çš„å½¢å¼å®ç°è¿›ä¸€æ­¥è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/matt-seb-ho/arc_memo%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/matt-seb-ho/arc_memoè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04439v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§æ¨¡å‹æ¨ç†æ—¶å¼•å…¥å¤–éƒ¨è®°å¿†å­˜å‚¨æ˜¯ååˆ†å¿…è¦çš„ã€‚å¤–éƒ¨è®°å¿†èƒ½å¤Ÿå¸®åŠ©å­˜å‚¨æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‘ç°çš„æ¨¡å¼å’Œè§è§£ï¼Œé¿å…åœ¨ä¸Šä¸‹æ–‡çª—å£é‡ç½®åä¸¢å¤±è¿™äº›ä¿¡æ¯ã€‚æœ€æ–°ç ”ç©¶å€¾å‘äºä»è§£å†³æ–¹æ¡ˆè½¨è¿¹ä¸­æç‚¼å‡ºå¯é‡å¤ä½¿ç”¨çš„æ¦‚å¿µçº§æŠ½è±¡ï¼Œè€Œéå®ä¾‹çº§è®°å¿†æ¡ç›®ã€‚è¿™ç§æ–¹å¼ä¸ä»…æé«˜äº†è®°å¿†çš„å¤ç”¨æ€§ï¼Œè¿˜å…è®¸è®°å¿†éšç€ç»éªŒçš„å¢åŠ è€Œæ‰©å±•ã€‚åœ¨ARC-AGIåŸºå‡†æµ‹è¯•ä¸­ï¼Œé‡‡ç”¨æ­¤æ–¹æ³•çš„æ¨¡å‹æ€§èƒ½ç›¸è¾ƒäºæ— è®°å¿†åŸºå‡†æµ‹è¯•æœ‰ç›¸å¯¹7.5%çš„æå‡ï¼Œå¹¶ä¸”æ€§èƒ½ä¼šéšç€æ¨ç†è®¡ç®—è€Œä¸æ–­æå‡ã€‚åŠ¨æ€æ›´æ–°å†…å­˜çš„æ–¹å¼åœ¨æµ‹è¯•æ—¶ä¼˜äºå›ºå®šå†…å­˜è®¾ç½®ï¼Œè¯æ˜äº†é€šè¿‡è§£å†³æ›´å¤šé—®é¢˜å’ŒæŠ½è±¡æ›´å¤šæ¨¡å¼åˆ°å†…å­˜å¯ä»¥è¿›ä¸€æ­¥å®ç°è‡ªæˆ‘æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¤–éƒ¨è®°å¿†å­˜å‚¨å¯¹å­˜å‚¨æ¨¡å‹å‘ç°çš„æ¨¡å¼å’Œè§è§£è‡³å…³é‡è¦ã€‚</li>
<li>ä¸å®ä¾‹çº§è®°å¿†ç›¸æ¯”ï¼Œæ¦‚å¿µçº§æŠ½è±¡è®°å¿†æ›´å…·å¤ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>åœ¨ARC-AGIåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¼•å…¥å¤–éƒ¨è®°å¿†çš„æ¦‚å¿µçº§æŠ½è±¡æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨åŠ¨æ€æ›´æ–°å†…å­˜çš„æ–¹å¼æœ‰åŠ©äºæå‡æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ã€‚</li>
<li>æ¦‚å¿µçº§æŠ½è±¡è®¾è®¡èƒ½ç¨³å®šæé«˜æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶åœ¨ä¸åŒçš„æ¨ç†è®¡ç®—è§„æ¨¡ä¸‹ã€‚</li>
<li>ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šå¯ä¾›å‚è€ƒå’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5eaefe3af829e5f48a0c12b830a59fc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-191c291c50901f8d16b9f54e07cf6e1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9687ddb2b5c253c0e1a9540f214b0f09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f5d6676e09012452ed72ed8ca38f572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39553be60f6b95db045cabe89bf9455d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EvoEmo-Towards-Evolved-Emotional-Policies-for-LLM-Agents-in-Multi-Turn-Negotiation"><a href="#EvoEmo-Towards-Evolved-Emotional-Policies-for-LLM-Agents-in-Multi-Turn-Negotiation" class="headerlink" title="EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation"></a>EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation</h2><p><strong>Authors:Yunbo Long, Liming Xu, Lukas Beckenbauer, Yuhan Liu, Alexandra Brintrup</strong></p>
<p>Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \textit{complex}, \textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines â€“ vanilla strategies and fixed-emotion strategies â€“ for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation. </p>
<blockquote>
<p>æœ€è¿‘å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†çš„ç ”ç©¶è¡¨æ˜ï¼Œæ™ºèƒ½ä½“å¯ä»¥å‚ä¸å¤æ‚çš„å¤šè½®è°ˆåˆ¤ï¼Œä¸ºæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ™ºèƒ½ä½“åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†æƒ…ç»ªåœ¨è¿™ç§è°ˆåˆ¤ä¸­çš„åŠŸèƒ½ä½œç”¨ï¼Œè€Œæ˜¯äº§ç”Ÿè¢«åŠ¨ã€åå¥½é©±åŠ¨çš„æƒ…ç»ªååº”ï¼Œä½¿å®ƒä»¬å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§å¯¹æ‰‹çš„æ“çºµå’Œæˆ˜ç•¥åˆ©ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†EvoEmoï¼Œè¿™æ˜¯ä¸€ä¸ªè¿›åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¼˜åŒ–äº†è°ˆåˆ¤ä¸­çš„åŠ¨æ€æƒ…ç»ªè¡¨è¾¾ã€‚EvoEmoå°†æƒ…ç»ªçŠ¶æ€è½¬æ¢å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶åŸºäºç¾¤ä½“é—ä¼ ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨ä¸åŒçš„è°ˆåˆ¤åœºæ™¯ä¸­æ¼”åŒ–å‡ºé«˜å›æŠ¥çš„æƒ…ç»ªç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªåŸºå‡†çº¿â€”â€”æ™®é€šç­–ç•¥å’Œå›ºå®šæƒ…ç»ªç­–ç•¥â€”â€”ç”¨äºè¯„ä¼°æƒ…æ„Ÿæ„ŸçŸ¥è°ˆåˆ¤ã€‚å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒEvoEmoå§‹ç»ˆä¼˜äºè¿™ä¸¤ä¸ªåŸºå‡†çº¿ï¼Œå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ã€æ›´é«˜çš„æ•ˆç‡å’Œæ›´é«˜çš„ä¹°å®¶èŠ‚çœã€‚è¿™äº›å‘ç°çªæ˜¾äº†åœ¨å¤šè½®è°ˆåˆ¤ä¸­ï¼Œè‡ªé€‚åº”çš„æƒ…ç»ªè¡¨è¾¾å¯¹äºä½¿LLMæ™ºèƒ½ä½“æ›´åŠ æœ‰æ•ˆçš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04310v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†çš„ç ”ç©¶æ˜¾ç¤ºï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿå‚ä¸å¤æ‚å¤šè½®è°ˆåˆ¤ï¼Œä¸ºæ™ºèƒ½ä½“AIå¼€å¯äº†æ–°é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰LLMæ™ºèƒ½ä½“åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†æƒ…ç»ªåœ¨è°ˆåˆ¤ä¸­çš„åŠŸèƒ½ä½œç”¨ï¼Œåªäº§ç”Ÿè¢«åŠ¨ã€åå¥½é©±åŠ¨çš„æƒ…ç»ªååº”ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°å¯¹æ‰‹çš„ç­–ç•¥æ€§æ“çºµå’Œåˆ©ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬æå‡ºäº†EvoEmoï¼Œä¸€ä¸ªè¿›åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–è°ˆåˆ¤ä¸­çš„åŠ¨æ€æƒ…ç»ªè¡¨è¾¾ã€‚EvoEmoå°†æƒ…ç»ªçŠ¶æ€è½¬å˜æ¨¡æ‹Ÿä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶è¿ç”¨åŸºäºç¾¤ä½“çš„é—ä¼ ä¼˜åŒ–ç®—æ³•æ¥æ¼”åŒ–ä¸åŒè°ˆåˆ¤åœºæ™¯ä¸‹çš„é«˜å›æŠ¥æƒ…ç»ªç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè®¾å®šäº†ä¸¤ç§åŸºçº¿ç­–ç•¥ï¼šå¸¸è§„ç­–ç•¥å’Œå›ºå®šæƒ…ç»ªç­–ç•¥ï¼Œç”¨äºè¯„ä¼°æƒ…ç»ªæ„ŸçŸ¥è°ˆåˆ¤çš„è¡¨ç°ã€‚å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒEvoEmoå§‹ç»ˆä¼˜äºåŸºçº¿ç­–ç•¥ï¼Œå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ã€æ•ˆç‡å’Œä¹°å®¶èŠ‚çœã€‚è¿™ä¸€å‘ç°å‡¸æ˜¾äº†è‡ªé€‚åº”æƒ…ç»ªè¡¨è¾¾åœ¨ä½¿LLMæ™ºèƒ½ä½“å®ç°æ›´æœ‰æ•ˆçš„å¤šè½®è°ˆåˆ¤ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs now can engage in complex, multi-turn negotiations through Chain-of-Thought (CoT) reasoning, opening new avenues for agentic AI.</li>
<li>Existing LLM agents tend to generate passive and preference-driven emotional responses, making them vulnerable to manipulation and strategic exploitation.</li>
<li>EvoEmo framework is proposed to optimize dynamic emotional expression in negotiations by modeling emotional state transitions as a Markov Decision Process.</li>
<li>EvoEmo employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios.</li>
<li>An evaluation framework with baselines is introduced to assess the performance of emotion-aware negotiation strategies.</li>
<li>EvoEmo consistently outperforms baseline strategies, achieving higher success rates, efficiency, and buyer savings in extensive experiments and ablation studies.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ec82d2610910cdb0ec016c7fd3831b2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4503d4f9510b771b54f02f6c87543ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6be05fccd8c48603ac9de19dca75ec2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MAGneT-Coordinated-Multi-Agent-Generation-of-Synthetic-Multi-Turn-Mental-Health-Counseling-Sessions"><a href="#MAGneT-Coordinated-Multi-Agent-Generation-of-Synthetic-Multi-Turn-Mental-Health-Counseling-Sessions" class="headerlink" title="MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn   Mental Health Counseling Sessions"></a>MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn   Mental Health Counseling Sessions</h2><p><strong>Authors:Aishik Mandal, Tanmoy Chakraborty, Iryna Gurevych</strong></p>
<p>The growing demand for scalable psychological counseling highlights the need for fine-tuning open-source Large Language Models (LLMs) with high-quality, privacy-compliant data, yet such data remains scarce. Here we introduce MAGneT, a novel multi-agent framework for synthetic psychological counseling session generation that decomposes counselor response generation into coordinated sub-tasks handled by specialized LLM agents, each modeling a key psychological technique. Unlike prior single-agent approaches, MAGneT better captures the structure and nuance of real counseling. In addition, we address inconsistencies in prior evaluation protocols by proposing a unified evaluation framework integrating diverse automatic and expert metrics. Furthermore, we expand the expert evaluations from four aspects of counseling in previous works to nine aspects, enabling a more thorough and robust assessment of data quality. Empirical results show that MAGneT significantly outperforms existing methods in quality, diversity, and therapeutic alignment of the generated counseling sessions, improving general counseling skills by 3.2% and CBT-specific skills by 4.3% on average on cognitive therapy rating scale (CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases on average across all aspects. Moreover, fine-tuning an open-source model on MAGneT-generated sessions shows better performance, with improvements of 6.3% on general counseling skills and 7.3% on CBT-specific skills on average on CTRS over those fine-tuned with sessions generated by baseline methods. We also make our code and data public. </p>
<blockquote>
<p>ä¸æ–­å¢é•¿çš„å¿ƒç†å’¨è¯¢éœ€æ±‚å¼ºè°ƒäº†ä½¿ç”¨é«˜è´¨é‡ã€ç¬¦åˆéšç§è¦æ±‚çš„æ•°æ®å¯¹å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒçš„å¿…è¦æ€§ï¼Œä½†æ­¤ç±»æ•°æ®ä»ç„¶ç¨€ç¼ºã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†MAGneTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåˆæˆå¿ƒç†å’¨è¯¢ä¼šè¯ç”Ÿæˆçš„æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ƒå°†å’¨è¯¢è€…å“åº”ç”Ÿæˆåˆ†è§£ä¸ºç”±ä¸“ä¸šLLMæ™ºèƒ½ä½“å¤„ç†çš„å„ç§åè°ƒå­ä»»åŠ¡ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“éƒ½æ¨¡æ‹Ÿäº†ä¸€ç§å…³é”®çš„å¿ƒç†æŠ€æœ¯ã€‚ä¸ä¹‹å‰çš„å•æ™ºèƒ½ä½“æ–¹æ³•ä¸åŒï¼ŒMAGneTèƒ½æ›´å¥½åœ°æ•æ‰ç°å®å’¨è¯¢çš„ç»“æ„å’Œç»†å¾®å·®åˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é’ˆå¯¹å…ˆå‰è¯„ä¼°åè®®çš„ä¸ä¸€è‡´æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†å¤šç§è‡ªåŠ¨å’Œä¸“å®¶åº¦é‡æ ‡å‡†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å°†ä¹‹å‰ä½œå“ä¸­å…³äºå’¨è¯¢çš„ä¸“å®¶è¯„ä¼°ä»å››ä¸ªæ–¹é¢æ‰©å±•åˆ°ä¹ä¸ªæ–¹é¢ï¼Œä»è€Œèƒ½å¤Ÿå¯¹æ•°æ®è´¨é‡è¿›è¡Œæ›´å…¨é¢ã€æ›´ç¨³å¥çš„è¯„ä¼°ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç”Ÿæˆçš„å’¨è¯¢ä¼šè¯çš„è´¨é‡ã€å¤šæ ·æ€§å’Œæ²»ç–—ä¸€è‡´æ€§æ–¹é¢ï¼ŒMAGneTæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨è®¤çŸ¥ç–—æ³•è¯„å®šé‡è¡¨ï¼ˆCTRSï¼‰ä¸Šï¼Œå¹³å‡æé«˜äº†ä¸€èˆ¬å’¨è¯¢æŠ€èƒ½3.2%ï¼Œç‰¹å®šäºCBTçš„æŠ€èƒ½æé«˜4.3%ã€‚å…³é”®çš„æ˜¯ï¼Œä¸“å®¶å¹³å‡åœ¨æ‰€æœ‰æ–¹é¢éƒ½å–œæ¬¢MAGneTç”Ÿæˆçš„ä¼šè¯å 77.2%ã€‚æ­¤å¤–ï¼Œä½¿ç”¨MAGneTç”Ÿæˆçš„ä¼šè¯å¯¹å¼€æºæ¨¡å‹è¿›è¡Œå¾®è°ƒæ˜¾ç¤ºå‡ºäº†æ›´å¥½çš„æ€§èƒ½ï¼Œåœ¨CTRSä¸Šç›¸è¾ƒäºä½¿ç”¨åŸºçº¿æ–¹æ³•ç”Ÿæˆçš„ä¼šè¯è¿›è¡Œå¾®è°ƒçš„ä¸€èˆ¬å’¨è¯¢æŠ€èƒ½å’ŒCBTç‰¹å®šæŠ€èƒ½å¹³å‡åˆ†åˆ«æé«˜äº†6.3%å’Œ7.3%ã€‚æˆ‘ä»¬è¿˜å…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04183v1">PDF</a> 25 pages, 29 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MAGneTï¼Œä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆåˆæˆå¿ƒç†å’¨è¯¢ä¼šè¯ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸“é—¨åŒ–LLMæ™ºèƒ½ä½“å¤„ç†åè°ƒçš„å­ä»»åŠ¡æ¥å¾®è°ƒå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥åº”å¯¹å¿ƒç†å’¨è¯¢æœåŠ¡éœ€æ±‚çš„å¢é•¿ã€‚MAGneTé€šè¿‡åˆ†è§£å’¨è¯¢å¸ˆååº”ç”Ÿæˆè¿‡ç¨‹ï¼Œæ›´å¥½åœ°æ•æ‰çœŸå®å’¨è¯¢çš„ç»“æ„å’Œç»†å¾®å·®åˆ«ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ•°æ®è´¨é‡ï¼Œå¹¶æ‰©å±•äº†ä»å››ä¸ªæ–¹é¢çš„ä¸“å®¶è¯„ä»·åˆ°ä¹ä¸ªæ–¹é¢ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒMAGneTåœ¨ç”Ÿæˆçš„å’¨è¯¢ä¼šè¯çš„è´¨é‡ã€å¤šæ ·æ€§å’Œæ²»ç–—å¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¸“å®¶è¯„ä¼°æ˜¾ç¤ºï¼ŒMAGneTç”Ÿæˆçš„ä¼šè¯åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½å¾—åˆ°äº†ä¸“å®¶çš„é’çã€‚æ­¤å¤–ï¼Œä½¿ç”¨MAGneTç”Ÿæˆçš„ä¼šè¯å¯¹å¼€æºæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ç”Ÿæˆçš„ä¼šè¯ï¼Œåœ¨ä¸€èˆ¬å’¨è¯¢æŠ€èƒ½å’ŒCBTç‰¹å®šæŠ€èƒ½ä¸Šéƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†MAGneTæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç”Ÿæˆåˆæˆå¿ƒç†å’¨è¯¢ä¼šè¯çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚</li>
<li>MAGneTé€šè¿‡ä¸“é—¨åŒ–LLMæ™ºèƒ½ä½“å¤„ç†åè°ƒçš„å­ä»»åŠ¡æ¥å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>MAGneTæ›´å¥½åœ°æ•æ‰äº†çœŸå®å¿ƒç†å’¨è¯¢çš„ç»“æ„å’Œç»†å¾®å·®åˆ«ã€‚</li>
<li>æå‡ºä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ•´åˆè‡ªåŠ¨å’Œä¸“å®¶è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æ‰©å±•äº†ä¸“å®¶è¯„ä»·çš„æ–¹é¢ã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºMAGneTåœ¨ä¼šè¯è´¨é‡ã€å¤šæ ·æ€§å’Œæ²»ç–—å¯¹é½æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ä¸“å®¶æ›´å€¾å‘äºé€‰æ‹©MAGneTç”Ÿæˆçš„ä¼šè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3b5a49c86e5a6ecc5b9b2f6901c6210f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c105ab50ee50439a15cdbae0a1c48a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a856ea83da8d9d573cdb0fe9787d20d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dd910476efb23e0d9705fc1a24f5052.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86de154d03ca4323749f62825050507a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f113ea0a939db2b96d7df5d5cec9f4fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-414638e514e34dc1c99ed3298530254b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00fc98b6b9d644564e8ec4ad07f5695f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Intermediate-Languages-Matter-Formal-Languages-and-LLMs-affect-Neurosymbolic-Reasoning"><a href="#Intermediate-Languages-Matter-Formal-Languages-and-LLMs-affect-Neurosymbolic-Reasoning" class="headerlink" title="Intermediate Languages Matter: Formal Languages and LLMs affect   Neurosymbolic Reasoning"></a>Intermediate Languages Matter: Formal Languages and LLMs affect   Neurosymbolic Reasoning</h2><p><strong>Authors:Alexander Beiser, David Penz, Nysret Musliu</strong></p>
<p>Large language models (LLMs) achieve astonishing results on a wide range of tasks. However, their formal reasoning ability still lags behind. A promising approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators from natural to formal languages and symbolic solvers for deriving correct results. Still, the contributing factors to the success of Neurosymbolic LLM reasoning remain unclear. This paper demonstrates that one previously overlooked factor is the choice of the formal language. We introduce the intermediate language challenge: selecting a suitable formal language for neurosymbolic reasoning. By comparing four formal languages across three datasets and seven LLMs, we show that the choice of formal language affects both syntactic and semantic reasoning capabilities. We also discuss the varying effects across different LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†æƒŠäººçš„æˆç»©ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ­£å¼æ¨ç†èƒ½åŠ›ä»ç„¶æ»åã€‚ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•æ˜¯ç¥ç»ç¬¦å·LLMæ¨ç†ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯åˆ©ç”¨LLMä½œä¸ºä»è‡ªç„¶è¯­è¨€åˆ°æ­£å¼è¯­è¨€å’Œç¬¦å·æ±‚è§£å™¨çš„ç¿»è¯‘å™¨ï¼Œä»¥å¾—å‡ºæ­£ç¡®ç»“æœã€‚ç„¶è€Œï¼Œç¥ç»ç¬¦å·LLMæ¨ç†æˆåŠŸçš„å…³é”®å› ç´ ä»ä¸æ¸…æ¥šã€‚æœ¬æ–‡è¯æ˜äº†ä»¥å‰è¢«å¿½è§†çš„ä¸€ä¸ªå› ç´ å°±æ˜¯æ­£å¼è¯­è¨€çš„é€‰æ‹©é—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸­é—´è¯­è¨€æŒ‘æˆ˜ï¼šä¸ºç¥ç»ç¬¦å·æ¨ç†é€‰æ‹©åˆé€‚çš„æ­£å¼è¯­è¨€ã€‚é€šè¿‡æ¯”è¾ƒä¸‰ç§æ•°æ®é›†å’Œä¸ƒä¸ªLLMçš„å››ç§æ­£å¼è¯­è¨€ï¼Œæˆ‘ä»¬è¯æ˜äº†æ­£å¼è¯­è¨€çš„é€‰æ‹©å½±å“è¯­æ³•å’Œè¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†åœ¨ä¸åŒLLMä¹‹é—´çš„ä¸åŒå½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04083v1">PDF</a> To appear in the proceedings of The Second Workshop on Knowledge   Graphs and Neurosymbolic AI (KG-NeSy) Co-located with SEMANTiCS 2025   Conference, Vienna, Austria - September 3rd, 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨å½¢å¼æ¨ç†æ–¹é¢ä»æœ‰ä¸è¶³ã€‚ç¥ç»ç¬¦å·å‹LLMæ¨ç†æ–¹æ³•å±•ç°å‡ºæ½œåŠ›ï¼Œé€šè¿‡å°†LLMsä½œä¸ºä»è‡ªç„¶è¯­è¨€åˆ°å½¢å¼è¯­è¨€çš„ç¿»è¯‘å™¨ï¼Œå¹¶ç»“åˆç¬¦å·æ±‚è§£å™¨æ¥æ¨å¯¼æ­£ç¡®ç»“æœã€‚ç„¶è€Œï¼Œç¥ç»ç¬¦å·å‹LLMæ¨ç†æˆåŠŸçš„å…³é”®å› ç´ å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡æ­ç¤ºäº†ä¸€ä¸ªä¹‹å‰è¢«å¿½è§†çš„å…³é”®å› ç´ â€”â€”å½¢å¼è¯­è¨€çš„é€‰æ‹©ã€‚æˆ‘ä»¬æå‡ºäº†ä¸­é—´è¯­è¨€æŒ‘æˆ˜ï¼Œå³é€‰æ‹©é€‚åˆç¥ç»ç¬¦å·æ¨ç†çš„å½¢å¼è¯­è¨€ã€‚é€šè¿‡å¯¹å››ç§å½¢å¼è¯­è¨€åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œä¸ƒä¸ªLLMsä¸Šçš„æ¯”è¾ƒï¼Œæˆ‘ä»¬å‘ç°å½¢å¼è¯­è¨€çš„é€‰æ‹©å½±å“è¯­æ³•å’Œè¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å½¢å¼æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>ç¥ç»ç¬¦å·å‹LLMæ¨ç†æ–¹æ³•æ˜¯ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ³•ï¼Œç»“åˆLLMså’Œç¬¦å·æ±‚è§£å™¨ã€‚</li>
<li>å½¢å¼è¯­è¨€çš„é€‰æ‹©æ˜¯ç¥ç»ç¬¦å·å‹LLMæ¨ç†æˆåŠŸçš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚</li>
<li>ä¸åŒå½¢å¼è¯­è¨€å¯¹è¯­æ³•å’Œè¯­ä¹‰æ¨ç†èƒ½åŠ›çš„å½±å“ä¸åŒã€‚</li>
<li>ä¸­é—´è¯­è¨€æŒ‘æˆ˜åœ¨äºä¸ºç¥ç»ç¬¦å·æ¨ç†é€‰æ‹©é€‚å½“çš„å½¢å¼è¯­è¨€ã€‚</li>
<li>è®ºæ–‡é€šè¿‡å®è¯æ¯”è¾ƒäº†å››ç§å½¢å¼è¯­è¨€åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œä¸ƒä¸ªLLMsä¸Šçš„è¡¨ç°ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†åœ¨ä¸åŒLLMsä¸­ï¼Œå½¢å¼è¯­è¨€é€‰æ‹©çš„å½±å“å­˜åœ¨å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-545fd9b5857cccb60dda951812033bef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bc0f97a13b93ea30b6e7d3a82229177.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa6c4f271f7d59251c021005d68ac57d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RTQA-Recursive-Thinking-for-Complex-Temporal-Knowledge-Graph-Question-Answering-with-Large-Language-Models"><a href="#RTQA-Recursive-Thinking-for-Complex-Temporal-Knowledge-Graph-Question-Answering-with-Large-Language-Models" class="headerlink" title="RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question   Answering with Large Language Models"></a>RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question   Answering with Large Language Models</h2><p><strong>Authors:Zhaoyan Gong, Juan Li, Zhiqiang Liu, Lei Liang, Huajun Chen, Wen Zhang</strong></p>
<p>Current temporal knowledge graph question answering (TKGQA) methods primarily focus on implicit temporal constraints, lacking the capability of handling more complex temporal queries, and struggle with limited reasoning abilities and error propagation in decomposition frameworks. We propose RTQA, a novel framework to address these challenges by enhancing reasoning over TKGs without requiring training. Following recursive thinking, RTQA recursively decomposes questions into sub-problems, solves them bottom-up using LLMs and TKG knowledge, and employs multi-path answer aggregation to improve fault tolerance. RTQA consists of three core components: the Temporal Question Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements in â€œMultipleâ€ and â€œComplexâ€ categories, outperforming state-of-the-art methods. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/zjukg/RTQA">https://github.com/zjukg/RTQA</a>. </p>
<blockquote>
<p>å½“å‰çš„æ—¶é—´çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆTKGQAï¼‰æ–¹æ³•ä¸»è¦å…³æ³¨éšå¼æ—¶é—´çº¦æŸï¼Œç¼ºä¹å¤„ç†æ›´å¤æ‚æ—¶é—´æŸ¥è¯¢çš„èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨åˆ†è§£æ¡†æ¶ä¸­çš„æ¨ç†èƒ½åŠ›å’Œé”™è¯¯ä¼ æ’­æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†RTQAï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡å¢å¼ºå¯¹TKGçš„æ¨ç†èƒ½åŠ›æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè€Œæ— éœ€è¿›è¡Œè®­ç»ƒã€‚RTQAéµå¾ªé€’å½’æ€ç»´ï¼Œå°†é—®é¢˜é€’å½’åœ°åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’ŒTKGçŸ¥è¯†è‡ªä¸‹è€Œä¸Šè§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶é‡‡ç”¨å¤šè·¯å¾„ç­”æ¡ˆèšåˆæ¥æé«˜å®¹é”™æ€§ã€‚RTQAç”±ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼šæ—¶é—´é—®é¢˜åˆ†è§£å™¨ã€é€’å½’æ±‚è§£å™¨å’Œç­”æ¡ˆèšåˆå™¨ã€‚åœ¨MultiTQå’ŒTimelineKGQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œâ€œå¤šä¸ªâ€å’Œâ€œå¤æ‚â€ç±»åˆ«çš„Hits@1æœ‰æ˜¾è‘—æ”¹å–„ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjukg/RTQA%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/zjukg/RTQAä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03995v1">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRTQAçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰æ—¶åºçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆTKGQAï¼‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é€’å½’æ€è€ƒçš„æ–¹å¼ï¼Œå°†é—®é¢˜é€’å½’åˆ†è§£ä¸ºå­é—®é¢˜å¹¶è§£å†³ï¼Œä»¥åŠ å¼ºåœ¨TKGä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚RTQAé‡‡ç”¨å¤šè·¯å¾„ç­”æ¡ˆèšåˆæé«˜å®¹é”™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRTQAåœ¨MultiTQå’ŒTimelineKGQAåŸºå‡†æµ‹è¯•ä¸­â€œå¤šä¸ªâ€å’Œâ€œå¤æ‚â€ç±»åˆ«çš„Hits@1æ”¹è¿›æ˜¾è‘—ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RTQAæ˜¯ä¸€ä¸ªé’ˆå¯¹æ—¶åºçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆTKGQAï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>RTQAé€šè¿‡é€’å½’æ€è€ƒçš„æ–¹å¼ï¼Œå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜å¹¶è§£å†³ã€‚</li>
<li>RTQAåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒTKGçŸ¥è¯†æ¥åº•éƒ¨è§£å†³å­é—®é¢˜ã€‚</li>
<li>RTQAé‡‡ç”¨å¤šè·¯å¾„ç­”æ¡ˆèšåˆï¼Œä»¥æé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œå®¹é”™æ€§ã€‚</li>
<li>RTQAæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæ—¶åºé—®é¢˜åˆ†è§£å™¨ã€é€’å½’æ±‚è§£å™¨å’Œç­”æ¡ˆèšåˆå™¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRTQAåœ¨MultiTQå’ŒTimelineKGQAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc2c5b9fb85a80746465178200913ae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b4062011ce46bec9a47b4cad317eafd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84cb27d912775c708306c07d44b0d62c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1da2192286274d7ab6d9aeae22768936.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MTQA-Matrix-of-Thought-for-Enhanced-Reasoning-in-Complex-Question-Answering"><a href="#MTQA-Matrix-of-Thought-for-Enhanced-Reasoning-in-Complex-Question-Answering" class="headerlink" title="MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question   Answering"></a>MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question   Answering</h2><p><strong>Authors:Fengxiao Tang, Yufeng Li, Zongzong Wu, Ming Zhao</strong></p>
<p>Complex Question Answering (QA) is a fundamental and challenging task in NLP. While large language models (LLMs) exhibit impressive performance in QA, they suffer from significant performance degradation when facing complex and abstract QA tasks due to insufficient reasoning capabilities. Works such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMsâ€™ reasoning abilities, but they face issues such as in-layer redundancy in tree structures and single paths in chain structures. Although some studies utilize Retrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the challenge of effectively utilizing large amounts of information involving multiple entities and hops remains critical. To address this, we propose the Matrix of Thought (MoT), a novel and efficient LLM thought structure. MoT explores the problem in both horizontal and vertical dimensions through the â€œcolumn-cell communicationâ€ mechanism, enabling LLMs to actively engage in multi-strategy and deep-level thinking, reducing redundancy within the column cells and enhancing reasoning capabilities. Furthermore, we develop a fact-correction mechanism by constructing knowledge units from retrieved knowledge graph triples and raw text to enhance the initial knowledge for LLM reasoning and correct erroneous answers. This leads to the development of an efficient and accurate QA framework (MTQA). Experimental results show that our framework outperforms state-of-the-art methods on four widely-used datasets in terms of F1 and EM scores, with reasoning time only 14.4% of the baseline methods, demonstrating both its efficiency and accuracy. The code for this framework is available at <a target="_blank" rel="noopener" href="https://github.com/lyfiter/mtqa">https://github.com/lyfiter/mtqa</a>. </p>
<blockquote>
<p>å¤æ‚é—®ç­”ï¼ˆQAï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€é¡¹åŸºæœ¬ä¸”å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨QAä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœï¼Œä½†ç”±äºç¼ºä¹è¶³å¤Ÿçš„æ¨ç†èƒ½åŠ›ï¼Œå®ƒä»¬åœ¨é¢å¯¹å¤æ‚å’ŒæŠ½è±¡çš„QAä»»åŠ¡æ—¶ä¼šå‡ºç°æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚è¯¸å¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œæ€ç»´æ ‘ï¼ˆToTï¼‰ç­‰ä½œå“æ—¨åœ¨å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬é¢ä¸´ç€å¦‚æ ‘ç»“æ„ä¸­çš„å±‚å†…å†—ä½™å’Œé“¾ç»“æ„ä¸­çš„å•ä¸€è·¯å¾„ç­‰é—®é¢˜ã€‚å°½ç®¡ä¸€äº›ç ”ç©¶åˆ©ç”¨å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•æ¥è¾…åŠ©LLMè¿›è¡Œæ¨ç†ï¼Œä½†æœ‰æ•ˆåˆ©ç”¨æ¶‰åŠå¤šä¸ªå®ä½“å’Œè·³è·ƒçš„å¤§é‡ä¿¡æ¯çš„æŒ‘æˆ˜ä»ç„¶å¾ˆå…³é”®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ€ç»´çŸ©é˜µï¼ˆMoTï¼‰è¿™ä¸€æ–°é¢–é«˜æ•ˆçš„LLMæ€ç»´ç»“æ„ã€‚MoTé€šè¿‡â€œåˆ—å•å…ƒæ ¼é€šä¿¡â€æœºåˆ¶åœ¨æ°´å¹³å’Œå‚ç›´ç»´åº¦ä¸Šæ¢ç´¢é—®é¢˜ï¼Œä½¿LLMèƒ½å¤Ÿç§¯æå‚ä¸å¤šç­–ç•¥å’Œæ·±åº¦æ€è€ƒï¼Œå‡å°‘åˆ—å•å…ƒæ ¼å†…çš„å†—ä½™ï¼Œæé«˜æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§äº‹å®æ ¡æ­£æœºåˆ¶ï¼Œé€šè¿‡ä»æ£€ç´¢åˆ°çš„çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„å’ŒåŸå§‹æ–‡æœ¬ä¸­æ„å»ºçŸ¥è¯†å•å…ƒï¼Œå¢å¼ºLLMæ¨ç†çš„åˆå§‹çŸ¥è¯†å¹¶çº æ­£é”™è¯¯çš„ç­”æ¡ˆã€‚è¿™å¯¼è‡´äº†ä¸€ä¸ªé«˜æ•ˆå‡†ç¡®çš„é—®ç­”æ¡†æ¶ï¼ˆMTQAï¼‰çš„å‘å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šçš„F1å’ŒEMå¾—åˆ†æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ¨ç†æ—¶é—´ä»…ä¸ºåŸºçº¿æ–¹æ³•çš„14.4%ï¼Œè¿™è¯æ˜äº†å®ƒçš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lyfiter/mtqa%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lyfiter/mtqaä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03918v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è¿™ä¸ªæ–‡æœ¬ä¸­ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºMatrix of Thoughtï¼ˆMoTï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æŠ½è±¡é—®ç­”ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡â€œåˆ—å•å…ƒæ ¼é€šä¿¡â€æœºåˆ¶åœ¨æ°´å¹³å’Œå‚ç›´ç»´åº¦ä¸Šæ¢ç´¢é—®é¢˜ï¼Œä½¿LLMsèƒ½å¤Ÿç§¯æå‚ä¸å¤šç­–ç•¥å’Œæ·±åº¦æ€è€ƒï¼Œå‡å°‘åˆ—å•å…ƒæ ¼å†…çš„å†—ä½™å¹¶å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ç§åŸºäºæ£€ç´¢çš„çŸ¥è¯†å•å…ƒçš„äº‹å®æ ¡æ­£æœºåˆ¶ï¼Œä»¥æé«˜LLMæ¨ç†çš„åˆå§‹çŸ¥è¯†å¹¶çº æ­£é”™è¯¯ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šçš„F1å’ŒEMå¾—åˆ†ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ¨ç†æ—¶é—´ä»…ä¸ºåŸºçº¿æ–¹æ³•çš„14.4%ï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚å’ŒæŠ½è±¡é—®ç­”ä»»åŠ¡ä¸­å› æ¨ç†èƒ½åŠ›ä¸è¶³è€Œæ€§èƒ½ä¸‹é™ã€‚</li>
<li>Chain-of-Thought (CoT) å’Œ Tree-of-Thought (ToT) ç­‰æ–¹æ³•æ—¨åœ¨å¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨å¦‚æ ‘ç»“æ„ä¸­çš„å†—ä½™å’Œé“¾ç»“æ„ä¸­çš„å•ä¸€è·¯å¾„ç­‰é—®é¢˜ã€‚</li>
<li>Matrix of Thought (MoT) æ¡†æ¶è¢«æå‡ºï¼Œé€šè¿‡â€œåˆ—å•å…ƒæ ¼é€šä¿¡â€æœºåˆ¶åœ¨æ°´å¹³å’Œå‚ç›´ç»´åº¦ä¸Šæ¢ç´¢é—®é¢˜ï¼Œä½¿LLMsèƒ½å¤Ÿç§¯æå‚ä¸å¤šç­–ç•¥å’Œæ·±åº¦æ€è€ƒã€‚</li>
<li>MoTæ¡†æ¶é€šè¿‡æ„å»ºçŸ¥è¯†å•å…ƒæ¥å¢å¼ºLLMçš„åˆå§‹çŸ¥è¯†ï¼Œå¹¶å¼€å‘äº†ä¸€ç§äº‹å®æ ¡æ­£æœºåˆ¶æ¥çº æ­£é”™è¯¯ç­”æ¡ˆã€‚</li>
<li>MoTæ¡†æ¶åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>MoTæ¡†æ¶çš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-61193cbc365b65378c62dd7ebbee2363.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f6bc810f58932d700ceb740797a1ecd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df48bfa226f80b45f6583283c53c9911.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Foundation-Model-for-Chest-X-ray-Interpretation-with-Grounded-Reasoning-via-Online-Reinforcement-Learning"><a href="#A-Foundation-Model-for-Chest-X-ray-Interpretation-with-Grounded-Reasoning-via-Online-Reinforcement-Learning" class="headerlink" title="A Foundation Model for Chest X-ray Interpretation with Grounded   Reasoning via Online Reinforcement Learning"></a>A Foundation Model for Chest X-ray Interpretation with Grounded   Reasoning via Online Reinforcement Learning</h2><p><strong>Authors:Qika Lin, Yifan Zhu, Bin Pu, Ling Huang, Haoran Luo, Jingying Ma, Zhen Peng, Tianzhe Zhao, Fangzhi Xu, Jian Zhang, Kai He, Zhonghong Ou, Swapnil Mishra, Mengling Feng</strong></p>
<p>Medical foundation models (FMs) have shown tremendous promise amid the rapid advancements in artificial intelligence (AI) technologies. However, current medical FMs typically generate answers in a black-box manner, lacking transparent reasoning processes and locally grounded interpretability, which hinders their practical clinical deployments. To this end, we introduce DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It leverages a sequential training pipeline: initially fine-tuned on curated CXR instruction data to equip with fundamental CXR interpretation capabilities, then exposed to high-quality synthetic reasoning samples to enable cold-start reasoning, and finally refined via online reinforcement learning to enhance both grounded reasoning quality and generation performance. Thus, the model produces both an answer and reasoning steps tied to the imageâ€™s local regions for each query. Quantitative evaluation demonstrates substantial improvements in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent) tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking framework using advanced language models to evaluate answer quality, further highlighting the superiority of DeepMedix-R1. Expert review of generated reasoning steps reveals greater interpretability and clinical plausibility compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall preference). Collectively, our work advances medical FM development toward holistic, transparent, and clinically actionable modeling for CXR interpretation. </p>
<blockquote>
<p>åŒ»ç–—åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŒ»ç–—åŸºç¡€æ¨¡å‹é€šå¸¸é‡‡ç”¨é»‘ç›’æ–¹å¼ç”Ÿæˆç­”æ¡ˆï¼Œç¼ºä¹é€æ˜çš„æ¨ç†è¿‡ç¨‹å’ŒåŸºäºæœ¬åœ°çš„å¯è§£é‡Šæ€§ï¼Œè¿™é˜»ç¢äº†å…¶åœ¨å®é™…ä¸´åºŠéƒ¨ç½²ä¸­çš„åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeepMedix-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ç”¨äºèƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰è§£è¯»çš„åŒ»ç–—åŸºç¡€æ¨¡å‹ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§åºè´¯è®­ç»ƒç®¡é“ï¼šé¦–å…ˆï¼Œåœ¨ç²¾é€‰çš„CXRæŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥å…·å¤‡åŸºæœ¬çš„CXRè§£è¯»èƒ½åŠ›ï¼›ç„¶åæš´éœ²äºé«˜è´¨é‡åˆæˆæ¨ç†æ ·æœ¬ä¸­ä»¥å®ç°å†·å¯åŠ¨æ¨ç†ï¼›æœ€åé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ”¹è¿›ï¼Œä»¥æé«˜åŸºäºæœ¬åœ°åŒºåŸŸçš„æ¨ç†è´¨é‡å’Œç”Ÿæˆæ€§èƒ½ã€‚å› æ­¤ï¼Œè¯¥æ¨¡å‹ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆä¸å›¾åƒå±€éƒ¨åŒºåŸŸç›¸å…³çš„ç­”æ¡ˆå’Œæ¨ç†æ­¥éª¤ã€‚å®šé‡è¯„ä¼°è¡¨æ˜ï¼Œåœ¨æŠ¥å‘Šç”Ÿæˆï¼ˆå¦‚å¯¹LLaVA-Radå’ŒMedGemmaåˆ†åˆ«æé«˜14.54%å’Œ31.32%ï¼‰å’Œè§†è§‰é—®ç­”ï¼ˆå¦‚å¯¹MedGemmaå’ŒCheXagentåˆ†åˆ«æé«˜57.75%å’Œ23.06%ï¼‰ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¸ºäº†è¿›è¡Œç¨³å¥çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†Report Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨å…ˆè¿›è¯­è¨€æ¨¡å‹è¯„ä¼°ç­”æ¡ˆè´¨é‡çš„åŸºå‡†æ¡†æ¶ï¼Œè¿›ä¸€æ­¥çªæ˜¾äº†DeepMedix-R1çš„ä¼˜è¶Šæ€§ã€‚å¯¹ç”Ÿæˆçš„æ¨ç†æ­¥éª¤çš„ä¸“å®¶è¯„å®¡æ˜¾ç¤ºï¼Œä¸å·²å»ºç«‹çš„Qwen2.5-VL-7Bæ¨¡å‹ç›¸æ¯”ï¼Œå…¶è§£é‡Šæ€§å’Œä¸´åºŠå¯ä¿¡åº¦æ›´é«˜ï¼ˆæ€»ä½“åå¥½ä¸º0.7416å¯¹0.2584ï¼‰ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ¨åŠ¨äº†åŒ»ç–—åŸºç¡€æ¨¡å‹å‘å…¨é¢ã€é€æ˜ã€é€‚ç”¨äºä¸´åºŠæ“ä½œçš„CXRè§£è¯»å»ºæ¨¡æ–¹å‘å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03906v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong>ï¼šDeepMedix-R1æ˜¯ä¸€ç§ç”¨äºèƒ¸Xå…‰ï¼ˆCXRï¼‰è§£è¯»çš„å…¨å±€åŒ»å­¦åŸºç¡€æ¨¡å‹ã€‚å®ƒé€šè¿‡åºè´¯è®­ç»ƒç®¡é“ï¼Œåœ¨CXRè§£è¯»æ•°æ®åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼Œåˆ©ç”¨é«˜è´¨é‡åˆæˆæ¨ç†æ ·æœ¬è¿›è¡Œå†·å¯åŠ¨æ¨ç†ï¼Œå¹¶é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ è¿›è¡Œç²¾ç‚¼ï¼Œæé«˜äº†æ¨ç†è´¨é‡å’Œç”Ÿæˆæ€§èƒ½ã€‚æ¨¡å‹ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆä¸å›¾åƒå±€éƒ¨åŒºåŸŸç›¸å…³è”çš„ç­”æ¡ˆå’Œæ¨ç†æ­¥éª¤ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒDeepMedix-R1åœ¨æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚è¿˜æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°ç­”æ¡ˆè´¨é‡çš„Report ArenaåŸºå‡†æ¡†æ¶ï¼Œä»¥çªæ˜¾DeepMedix-R1çš„ä¼˜è¶Šæ€§ã€‚ä¸Qwen2.5-VL-7Bæ¨¡å‹ç›¸æ¯”ï¼Œç”Ÿæˆçš„æ¨ç†æ­¥éª¤æ›´å…·å¯è§£é‡Šæ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DeepMedix-R1æ˜¯ä¸€ä¸ªç”¨äºèƒ¸Xå…‰è§£è¯»çš„å…¨å±€åŒ»å­¦åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å½“å‰åŒ»å­¦åŸºç¡€æ¨¡å‹ç¼ºä¹é€æ˜æ¨ç†è¿‡ç¨‹å’Œå±€éƒ¨åŒ–è§£é‡Šæ€§çš„ç¼ºç‚¹ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨åºè´¯è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å¾®è°ƒé˜¶æ®µã€å†·å¯åŠ¨æ¨ç†é˜¶æ®µå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ç²¾ç‚¼é˜¶æ®µã€‚</li>
<li>DeepMedix-R1å¯ä»¥ç”Ÿæˆä¸å›¾åƒå±€éƒ¨åŒºåŸŸç›¸å…³è”çš„ç­”æ¡ˆå’Œæ¨ç†æ­¥éª¤ï¼Œå¢å¼ºäº†å…¶å¯è§£é‡Šæ€§å’Œä¸´åºŠå®ç”¨æ€§ã€‚</li>
<li>ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹ï¼ˆLLaVA-Radã€MedGemmaã€MedGemmaå’ŒCheXagentï¼‰ï¼ŒDeepMedix-R1åœ¨æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æå‡ºçš„Report ArenaåŸºå‡†æ¡†æ¶ç”¨äºè¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼Œè¿›ä¸€æ­¥çªæ˜¾DeepMedix-R1çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ä¸Qwen2.5-VL-7Bæ¨¡å‹ç›¸æ¯”ï¼ŒDeepMedix-R1ç”Ÿæˆçš„æ¨ç†æ­¥éª¤æ›´å…·å¯è§£é‡Šæ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cbbcc1af58efb135a2de8ece3d2debbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ffed779ba67c21b42c1ebdf11dd9290.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df3755f147d5d908de4cb6244d59c196.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a49c6a256c7b4859869d3cd6355b82b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VulRTex-A-Reasoning-Guided-Approach-to-Identify-Vulnerabilities-from-Rich-Text-Issue-Report"><a href="#VulRTex-A-Reasoning-Guided-Approach-to-Identify-Vulnerabilities-from-Rich-Text-Issue-Report" class="headerlink" title="VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from   Rich-Text Issue Report"></a>VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from   Rich-Text Issue Report</h2><p><strong>Authors:Ziyou Jiang, Mingyang Li, Guowei Yang, Lin Shi, Qing Wang</strong></p>
<p>Software vulnerabilities exist in open-source software (OSS), and the developers who discover these vulnerabilities may submit issue reports (IRs) to describe their details. Security practitioners need to spend a lot of time manually identifying vulnerability-related IRs from the community, and the time gap may be exploited by attackers to harm the system. Previously, researchers have proposed automatic approaches to facilitate identifying these vulnerability-related IRs, but these works focus on textual descriptions but lack the comprehensive analysis of IRâ€™s rich-text information. In this paper, we propose VulRTex, a reasoning-guided approach to identify vulnerability-related IRs with their rich-text information. In particular, VulRTex first utilizes the reasoning ability of the Large Language Model (LLM) to prepare the Vulnerability Reasoning Database with historical IRs. Then, it retrieves the relevant cases from the prepared reasoning database to generate reasoning guidance, which guides LLM to identify vulnerabilities by reasoning analysis on target IRsâ€™ rich-text information. To evaluate the performance of VulRTex, we conduct experiments on 973,572 IRs, and the results show that VulRTex achieves the highest performance in identifying the vulnerability-related IRs and predicting CWE-IDs when the dataset is imbalanced, outperforming the best baseline with +11.0% F1, +20.2% AUPRC, and +10.5% Macro-F1, and 2x lower time cost than baseline reasoning approaches. Furthermore, VulRTex has been applied to identify 30 emerging vulnerabilities across 10 representative OSS projects in 2024â€™s GitHub IRs, and 11 of them are successfully assigned CVE-IDs, which illustrates VulRTexâ€™s practicality. </p>
<blockquote>
<p>åœ¨å¼€æºè½¯ä»¶ï¼ˆOSSï¼‰ä¸­å­˜åœ¨è½¯ä»¶æ¼æ´ï¼Œå‘ç°è¿™äº›æ¼æ´çš„å¼€å‘è€…å¯ä»¥æäº¤é—®é¢˜æŠ¥å‘Šï¼ˆIRsï¼‰æ¥æè¿°å…¶ç»†èŠ‚ã€‚å®‰å…¨ä¸“å®¶éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´æ‰‹åŠ¨ä»ç¤¾åŒºä¸­è¯†åˆ«ä¸æ¼æ´æœ‰å…³çš„é—®é¢˜æŠ¥å‘Šï¼Œè€Œè¿™æ®µæ—¶é—´å·®è·å¯èƒ½ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨æ¥å±å®³ç³»ç»Ÿã€‚ä¹‹å‰ï¼Œç ”ç©¶äººå‘˜å·²ç»æå‡ºäº†è‡ªåŠ¨æ–¹æ³•æ¥å¸®åŠ©è¯†åˆ«è¿™äº›ä¸æ¼æ´æœ‰å…³çš„é—®é¢˜æŠ¥å‘Šï¼Œä½†è¿™äº›å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æè¿°ä¸Šï¼Œç¼ºä¹é—®é¢˜æŠ¥å‘Šçš„ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯çš„ç»¼åˆåˆ†æã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VulRTexï¼Œè¿™æ˜¯ä¸€ç§ä»¥æ¨ç†ä¸ºæŒ‡å¯¼çš„æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«ä¸æ¼æ´ç›¸å…³çš„é—®é¢˜æŠ¥å‘ŠåŠå…¶ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯ã€‚å…·ä½“è€Œè¨€ï¼ŒVulRTexé¦–å…ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ¥æ„å»ºåŒ…å«å†å²é—®é¢˜æŠ¥å‘Šçš„æ¼æ´æ¨ç†æ•°æ®åº“ã€‚ç„¶åï¼Œå®ƒä»æ„å»ºçš„æ¨ç†æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ¡ˆä¾‹ä»¥ç”Ÿæˆæ¨ç†æŒ‡å¯¼ï¼Œè¯¥æŒ‡å¯¼é€šè¿‡ç›®æ ‡é—®é¢˜æŠ¥å‘Šçš„ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ¨ç†åˆ†ææ¥æŒ‡å¯¼LLMè¯†åˆ«æ¼æ´ã€‚ä¸ºäº†è¯„ä¼°VulRTexçš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨973,572ä¸ªé—®é¢˜æŠ¥å‘Šä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼Œå½“æ•°æ®é›†ä¸å¹³è¡¡æ—¶ï¼ŒVulRTexåœ¨è¯†åˆ«ä¸æ¼æ´ç›¸å…³çš„é—®é¢˜æŠ¥å‘Šå’Œé¢„æµ‹CWE-IDsæ–¹é¢è¾¾åˆ°äº†æœ€é«˜æ€§èƒ½ï¼Œä¸æœ€ä½³åŸºçº¿ç›¸æ¯”æé«˜äº†+11.0%çš„F1åˆ†æ•°ï¼Œ+20.2%çš„AUPRCå’Œ+10.5%çš„å®è§‚F1åˆ†æ•°ï¼Œå¹¶ä¸”æ—¶é—´æˆæœ¬æ¯”åŸºçº¿æ¨ç†æ–¹æ³•é™ä½äº†ä¸¤å€ã€‚æ­¤å¤–ï¼ŒVulRTexå·²åº”ç”¨äºè¯†åˆ«2024å¹´GitHubé—®é¢˜æŠ¥å‘Šä¸­è·¨10ä¸ªä»£è¡¨æ€§OSSé¡¹ç›®çš„30ä¸ªæ–°å…´æ¼æ´ï¼Œå…¶ä¸­æˆåŠŸåˆ†é…äº†CVE-IDsçš„æœ‰11ä¸ªæ¡ˆä¾‹ï¼Œè¿™è¯æ˜äº†VulRTexçš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03875v1">PDF</a> 25 pages, 7 figures, submitting to TOSEM journal</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVulRTexçš„æ–°æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«ä¸è½¯ä»¶æ¼æ´ç›¸å…³çš„å¼€æºè½¯ä»¶é—®é¢˜æŠ¥å‘Šï¼ˆIRsï¼‰ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å’Œä¸°å¯Œçš„æ–‡æœ¬ä¿¡æ¯ï¼Œé€šè¿‡å»ºç«‹æ¼æ´æ¨ç†æ•°æ®åº“å’Œæ£€ç´¢ç›¸å…³æ¡ˆä¾‹è¿›è¡Œæ¨ç†åˆ†æï¼Œä»¥è¯†åˆ«ç›®æ ‡IRsä¸­çš„æ¼æ´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVulRTexåœ¨è¯†åˆ«ä¸é¢„æµ‹ä¸å¹³è¡¡æ•°æ®é›†ä¸­çš„æ¼æ´ç›¸å…³IRså’ŒCWE-IDsæ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä¼˜äºæœ€ä½³åŸºçº¿æ–¹æ³•ï¼Œå¹¶å…·æœ‰è¾ƒä½çš„æ—¶é—´æˆæœ¬ã€‚æ­¤å¤–ï¼ŒVulRTexå·²æˆåŠŸåº”ç”¨äºè¯†åˆ«GitHub IRsä¸­ä»£è¡¨OSSé¡¹ç›®çš„æ–°å…´æ¼æ´ï¼Œå¹¶æˆåŠŸåˆ†é…äº†CVE-IDsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VulRTexæ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¼æ´è¯†åˆ«çš„æ–¹æ³•ï¼Œç»“åˆäº†ä¸°å¯Œçš„æ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡å»ºç«‹æ¼æ´æ¨ç†æ•°æ®åº“å’Œæ£€ç´¢ç›¸å…³æ¡ˆä¾‹ï¼ŒVulRTexå®ç°äº†é«˜æ•ˆçš„æ¼æ´è¯†åˆ«ã€‚</li>
<li>VulRTexåœ¨è¯†åˆ«ä¸é¢„æµ‹ä¸å¹³è¡¡æ•°æ®é›†ä¸­çš„æ¼æ´ç›¸å…³IRså’ŒCWE-IDsæ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>VulRTexä¼˜äºæœ€ä½³åŸºçº¿æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„F1åˆ†æ•°ã€AUPRCå’ŒMacro-F1ï¼Œå¹¶ä¸”æ—¶é—´æˆæœ¬æ›´ä½ã€‚</li>
<li>VulRTexå¯åº”ç”¨äºè¯†åˆ«æ–°å…´æ¼æ´ï¼Œå¹¶åœ¨å®é™…é¡¹ç›®ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>åœ¨å®éªŒè¯„ä¼°ä¸­ï¼ŒVulRTexæˆåŠŸè¯†åˆ«äº†GitHub IRsä¸­çš„ä»£è¡¨æ€§OSSé¡¹ç›®çš„æ–°å…´æ¼æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03875">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d83f98afa23daab4cdc5507935eb5a42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b6153c637622d5a999fd7027274c24a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6db4db5a33fe502d1d2799ab1b83c8e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Comprehensive-Survey-on-Trustworthiness-in-Reasoning-with-Large-Language-Models"><a href="#A-Comprehensive-Survey-on-Trustworthiness-in-Reasoning-with-Large-Language-Models" class="headerlink" title="A Comprehensive Survey on Trustworthiness in Reasoning with Large   Language Models"></a>A Comprehensive Survey on Trustworthiness in Reasoning with Large   Language Models</h2><p><strong>Authors:Yanbo Wang, Yongcan Yu, Jian Liang, Ran He</strong></p>
<p>The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{<a target="_blank" rel="noopener" href="https://github.com/ybwang119/Awesome-reasoning-safety%7D%7Bhttps://github.com/ybwang119/Awesome-reasoning-safety%7D">https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}</a>. </p>
<blockquote>
<p>é•¿æ–‡æœ¬è¿ç»­æ¨ç†ï¼ˆLong-CoTï¼‰çš„å‘å±•å·²ç»æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­è¨€ç†è§£ã€å¤æ‚é—®é¢˜è§£å†³å’Œä»£ç ç”Ÿæˆã€‚è¿™ç§èŒƒå¼ä½¿æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œå…³äºåŸºäºCoTçš„æ¨ç†å¦‚ä½•å½±å“è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦çš„å…¨é¢ç†è§£ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡å›é¡¾äº†å…³äºæ¨ç†æ¨¡å‹å’ŒCoTæŠ€æœ¯çš„æœ€æ–°ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨å¯ä¿¡æ¨ç†çš„äº”ä¸ªæ ¸å¿ƒç»´åº¦ï¼šçœŸå®æ€§ã€å®‰å…¨æ€§ã€ç¨³å¥æ€§ã€å…¬å¹³æ€§å’Œéšç§æ€§ã€‚å¯¹äºæ¯ä¸ªæ–¹é¢ï¼Œæˆ‘ä»¬æŒ‰æ—¶é—´é¡ºåºæ¸…æ™°ä¸”ç»“æ„åŒ–åœ°æ¦‚è¿°äº†æœ€è¿‘çš„ç ”ç©¶ï¼Œä»¥åŠå¯¹å…¶æ–¹æ³•ã€å‘ç°å’Œå±€é™æ€§çš„è¯¦ç»†åˆ†æã€‚æœ¬æ–‡æœ«å°¾è¿˜é™„ä¸Šäº†æœªæ¥ç ”ç©¶æ–¹å‘ä»¥ä¾›å‚è€ƒå’Œè®¨è®ºã€‚æ€»çš„æ¥è¯´ï¼Œè™½ç„¶æ¨ç†æŠ€æœ¯é€šè¿‡å‡å°‘å¹»è§‰ã€æ£€æµ‹æœ‰å®³å†…å®¹å’Œæé«˜ç¨³å¥æ€§è€Œæœ‰æœ›å¢å¼ºæ¨¡å‹çš„å¯ä¿¡åº¦ï¼Œä½†æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹æœ¬èº«åœ¨å®‰å…¨ã€ç¨³å¥æ€§å’Œéšç§æ–¹é¢å¾€å¾€å­˜åœ¨ç›¸å½“çš„æˆ–æ›´å¤§çš„æ¼æ´ã€‚é€šè¿‡ç»¼åˆè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¸ºäººå·¥æ™ºèƒ½å®‰å…¨ç•Œæä¾›å…³äºæ¨ç†å¯ä¿¡åº¦æœ€æ–°è¿›å±•çš„å®è´µå’ŒåŠæ—¶èµ„æºã€‚ç›¸å…³è®ºæ–‡çš„å®Œæ•´åˆ—è¡¨å¯è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://github.com/ybwang119/Awesome-reasoning-safety]%E3%80%82">https://github.com/ybwang119/Awesome-reasoning-safety]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03871v1">PDF</a> 38 pages. This survey considers papers published up to June 30, 2025.   Work in progress</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºCoTçš„æ¨ç†æŠ€æœ¯å¦‚ä½•å½±å“è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦ã€‚æ–‡ç« ä»çœŸå®æ€§ã€å®‰å…¨æ€§ã€ç¨³å¥æ€§ã€å…¬å¹³æ€§å’Œéšç§æ€§äº”ä¸ªæ ¸å¿ƒç»´åº¦è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œåˆ†æäº†æœ€æ–°çš„ç ”ç©¶æˆæœå’Œæ–¹æ³•ï¼ŒæŒ‡å‡ºäº†å…¶å±€é™æ€§ã€‚å°½ç®¡è¿™äº›æŠ€æœ¯æœ‰æ½œåŠ›æé«˜æ¨¡å‹çš„ä¿¡ä»»åº¦ï¼Œä½†ç°æœ‰çš„å…ˆè¿›æ¨ç†æ¨¡å‹åœ¨å®‰å…¨æ€§ã€ç¨³å¥æ€§å’Œéšç§æ–¹é¢ä»ç„¶å­˜åœ¨è®¸å¤šè„†å¼±æ€§ã€‚æ–‡ç« å‘¼åAIå®‰å…¨ç¤¾åŒºæŒç»­å…³æ³¨è¿™ä¸€é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Long-CoTæ¨ç†çš„å‘å±•æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬è¯­è¨€ç†è§£ã€å¤æ‚é—®é¢˜è§£å†³å’Œä»£ç ç”Ÿæˆã€‚</li>
<li>CoTæ¨ç†æŠ€æœ¯èƒ½å¤Ÿç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>è®ºæ–‡å…¨é¢ç»¼è¿°äº†å…³äºæ¨ç†æ¨¡å‹å’ŒCoTæŠ€æœ¯çš„æœ€æ–°ç ”ç©¶ï¼Œé‡ç‚¹åˆ†æäº†äº”ä¸ªç»´åº¦ï¼šçœŸå®æ€§ã€å®‰å…¨æ€§ã€ç¨³å¥æ€§ã€å…¬å¹³æ€§å’Œéšç§æ€§ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºå°½ç®¡æ¨ç†æŠ€æœ¯æœ‰æ½œåŠ›æé«˜æ¨¡å‹çš„å¯ä¿¡åº¦ï¼Œä½†ç°æœ‰çš„æ¨ç†æ¨¡å‹åœ¨å®‰å…¨æ€§ã€ç¨³å¥æ€§å’Œéšç§æ–¹é¢ä»å­˜åœ¨è®¸å¤šè„†å¼±æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b636c61785b141909faa312b3db76d0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc96a09aa03e9f3b5ccc225a6fdc89f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14546cf0f3a6b32de054025d55ba4ac6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62adfec07d744e927cbac36f0ed68dab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-372becc63fbc788c1ec226af9b7e0e84.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Learning-to-Deliberate-Meta-policy-Collaboration-for-Agentic-LLMs-with-Multi-agent-Reinforcement-Learning"><a href="#Learning-to-Deliberate-Meta-policy-Collaboration-for-Agentic-LLMs-with-Multi-agent-Reinforcement-Learning" class="headerlink" title="Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with   Multi-agent Reinforcement Learning"></a>Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with   Multi-agent Reinforcement Learning</h2><p><strong>Authors:Wei Yang, Jesse Thomason</strong></p>
<p>Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agentsâ€™ internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å…¶æœ‰æ•ˆæ€§é€šå¸¸å—åˆ°å›ºå®šåä½œåè®®çš„é™åˆ¶ã€‚è¿™äº›æ¡†æ¶é€šå¸¸ä¾§é‡äºå®è§‚å±‚é¢çš„åè°ƒï¼Œè€Œå¿½è§†äº†æ™ºèƒ½ä½“çš„å†…éƒ¨å†³ç­–èƒ½åŠ›ã€‚è¿™ä¸€å…³é”®çš„å…ƒè®¤çŸ¥ç›²ç‚¹å°†æ™ºèƒ½ä½“è§†ä¸ºè¢«åŠ¨çš„æ‰§è¡Œè€…ï¼Œæ— æ³•æ ¹æ®ä¸ç¡®å®šæ€§æˆ–ä¿¡å¿ƒç­‰å†…éƒ¨è®¤çŸ¥çŠ¶æ€æ¥é€‚åº”å…¶ç­–ç•¥ã€‚æˆ‘ä»¬å¼•å…¥äº†å…ƒç­–ç•¥å†³ç­–æ¡†æ¶ï¼ˆMPDFï¼‰ï¼Œåœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæ™ºèƒ½ä½“å­¦ä¹ åœ¨ä¸€ç»„é«˜çº§å…ƒè®¤çŸ¥è¡ŒåŠ¨ä¸Šçš„åˆ†æ•£ç­–ç•¥ï¼šåšæŒã€ä¼˜åŒ–å’Œè®©æ­¥ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿç­–ç•¥æ¢¯åº¦åœ¨æ­¤ç¯å¢ƒä¸­çš„ä¸ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†SoftRankPOï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚SoftRankPOé€šè¿‡æ ¹æ®é€šè¿‡å¹³æ»‘æ­£æ€åˆ†å¸ƒæ‰€æ˜ å°„çš„å¥–åŠ±æ’åå¡‘é€ ä¼˜åŠ¿æ¥ç¨³å®šè®­ç»ƒï¼Œè¿™ä½¿å¾—å­¦ä¹ è¿‡ç¨‹å¯¹å¥–åŠ±æ–¹å·®å…·æœ‰é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å…­ç§åŸºäºå¯å‘å¼å’Œå­¦ä¹ å‹çš„å¤šæ™ºèƒ½ä½“æ¨ç†ç®—æ³•ç›¸æ¯”ï¼Œä½¿ç”¨SoftRankPOçš„MPDFåœ¨äº”ä¸ªæ•°å­¦å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡å‡†ç¡®åº¦æé«˜äº†4-5%çš„ç»å¯¹å¢ç›Šã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿå­¦ä¹ è‡ªé€‚åº”å…ƒè®¤çŸ¥ç­–ç•¥æä¾›äº†èŒƒä¾‹ï¼Œå°†é‡ç‚¹ä»è®¾è®¡å›ºå®šåè®®è½¬å‘å­¦ä¹ åŠ¨æ€å†³ç­–ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03817v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚æ¨ç†æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å—é™äºå›ºå®šçš„åä½œåè®®ã€‚ç°æœ‰æ¡†æ¶å¿½ç•¥æ™ºèƒ½ä½“çš„å†…éƒ¨è®¤çŸ¥çŠ¶æ€ï¼Œå¯¼è‡´æ™ºèƒ½ä½“éš¾ä»¥é€‚åº”ç­–ç•¥è°ƒæ•´ã€‚æœ¬æ–‡æå‡ºå…ƒç­–ç•¥å®¡è®®æ¡†æ¶ï¼ˆMPDFï¼‰ï¼Œæ™ºèƒ½ä½“åœ¨æ­¤æ¡†æ¶ä¸‹å­¦ä¹ å¯¹ä¸€ç»„é«˜çº§å…ƒè®¤çŸ¥è¡ŒåŠ¨è¿›è¡Œåˆ†æ•£å†³ç­–ï¼šæŒç»­ã€ç²¾è¿›å’Œè®©æ­¥ã€‚ä¸ºè§£å†³æ­¤ç¯å¢ƒä¸‹ä¼ ç»Ÿç­–ç•¥æ¢¯åº¦çš„ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åä¸ºSoftRankPOçš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å…­ç§æœ€å…ˆè¿›çš„å¯å‘å¼å’Œå­¦ä¹ å‹å¤šæ™ºèƒ½ä½“æ¨ç†ç®—æ³•ç›¸æ¯”ï¼ŒMPDFä¸SoftRankPOç›¸ç»“åˆåœ¨äº”ä¸ªæ•°å­¦å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¹³å‡å‡†ç¡®ç‡æé«˜4-5%ã€‚æœ¬æ–‡ä¸ºå­¦ä¹ è‡ªé€‚åº”ã€å…ƒè®¤çŸ¥ç­–ç•¥çš„å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿæä¾›äº†èŒƒä¾‹ï¼Œå°†é‡ç‚¹ä»è®¾è®¡å›ºå®šåè®®è½¬å‘å­¦ä¹ åŠ¨æ€å†³ç­–ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚æ¨ç†ä¸Šå…·æœ‰æ½œåŠ›ï¼Œä½†å—é™äºå›ºå®šåä½œåè®®ã€‚</li>
<li>ç°æœ‰æ¡†æ¶å¿½ç•¥æ™ºèƒ½ä½“çš„å†…éƒ¨è®¤çŸ¥çŠ¶æ€ï¼Œå¯¼è‡´ç­–ç•¥è°ƒæ•´å›°éš¾ã€‚</li>
<li>æå‡ºå…ƒç­–ç•¥å®¡è®®æ¡†æ¶ï¼ˆMPDFï¼‰ï¼Œå…è®¸æ™ºèƒ½ä½“å­¦ä¹ åˆ†æ•£å†³ç­–çš„é«˜çº§å…ƒè®¤çŸ¥è¡ŒåŠ¨ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åä¸ºSoftRankPOçš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè§£å†³ç­–ç•¥æ¢¯åº¦çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚</li>
<li>MPDFä¸SoftRankPOç»“åˆåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†è¾ƒé«˜çš„å¹³å‡å‡†ç¡®ç‡ã€‚</li>
<li>æœ¬æ–‡ä¸ºè‡ªé€‚åº”ã€å…ƒè®¤çŸ¥ç­–ç•¥çš„å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿæä¾›äº†èŒƒä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2ef2f24985868f458a46a250de8df2d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df0578129bf9f60fda65df6d313c64f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abd2e3a77b11ba36c95e8f2dfff4a318.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a81d6164779b08a20b42147a05e7893d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1f4b61384dbee875b8d30537c96a4e9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Emergent-Hierarchical-Reasoning-in-LLMs-through-Reinforcement-Learning"><a href="#Emergent-Hierarchical-Reasoning-in-LLMs-through-Reinforcement-Learning" class="headerlink" title="Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning"></a>Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning</h2><p><strong>Authors:Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like <code>aha moments&quot;, </code>length-scalingâ€™â€™ and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²è¯æ˜åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†é©±åŠ¨è¿™ä¸€æˆåŠŸçš„æ½œåœ¨æœºåˆ¶ä»å¤§å¤šæœªçŸ¥ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºï¼Œâ€œå•Šå“ˆæ—¶åˆ»â€ã€â€œé•¿åº¦ç¼©æ”¾â€å’Œç†µåŠ¨åŠ›å­¦ç­‰ä»¤äººå›°æƒ‘çš„ç°è±¡å¹¶ä¸æ˜¯å­¤ç«‹å‘ç”Ÿçš„ï¼Œè€Œæ˜¯æ–°å…´æ¨ç†å±‚æ¬¡çš„æ ‡å¿—ï¼Œä¸äººç±»è®¤çŸ¥ä¸­çš„é«˜çº§æˆ˜ç•¥è§„åˆ’ä¸ä½çº§ç¨‹åºæ‰§è¡Œåˆ†ç¦»çš„æƒ…å†µç›¸ç±»ä¼¼ã€‚æˆ‘ä»¬å‘ç°äº†å¼•äººæ³¨ç›®çš„ä¸¤é˜¶æ®µåŠ¨æ€è¿‡ç¨‹ï¼šåˆæœŸï¼Œæ¨¡å‹å—åˆ°ç¨‹åºæ­£ç¡®æ€§çš„çº¦æŸï¼Œå¿…é¡»æé«˜å…¶ä½çº§æŠ€èƒ½ã€‚å­¦ä¹ ç“¶é¢ˆç„¶åæœæ–­è½¬ç§»ï¼Œæ€§èƒ½æå‡ç”±é«˜çº§æˆ˜ç•¥è§„åˆ’çš„æ¢ç´¢å’ŒæŒæ¡æ‰€é©±åŠ¨ã€‚è¿™ç§è§è§£æ­ç¤ºäº†ç°è¡ŒRLç®—æ³•ï¼ˆå¦‚GRPOï¼‰çš„æ ¸å¿ƒä½æ•ˆä¹‹å¤„ï¼Œè¿™äº›ç®—æ³•ç›²ç›®åœ°æ–½åŠ ä¼˜åŒ–å‹åŠ›ï¼Œå¹¶åœ¨æ‰€æœ‰ä»¤ç‰Œä¸­ç¨€é‡Šå­¦ä¹ ä¿¡å·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HIerarchy-Aware Credit Assignmentï¼ˆHICRAï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†ä¼˜åŒ–å·¥ä½œé›†ä¸­åœ¨å¯¹è§„åˆ’ä»¤ç‰Œå½±å“è¾ƒå¤§çš„åœ°æ–¹ã€‚HICRAæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œè¯æ˜å…³æ³¨è¿™ä¸€æˆ˜ç•¥ç“¶é¢ˆæ˜¯è§£é”é«˜çº§æ¨ç†çš„å…³é”®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†è¯­ä¹‰ç†µä½œä¸ºè¡¡é‡æˆ˜ç•¥æ¢ç´¢çš„ä¼˜è¶ŠæŒ‡æ ‡ï¼Œç›¸è¾ƒäºè¯¯å¯¼æ€§æŒ‡æ ‡ï¼ˆå¦‚ä»¤ç‰Œçº§ç†µï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03646v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶ä»å¤§å¤šæœªçŸ¥ã€‚åˆ†æè¡¨æ˜ï¼Œâ€œå•Šå“ˆæ—¶åˆ»â€ã€â€œé•¿åº¦ç¼©æ”¾â€å’Œç†µåŠ¨åŠ›å­¦ç­‰ä»¤äººå›°æƒ‘çš„ç°è±¡å¹¶ä¸æ˜¯å­¤ç«‹äº‹ä»¶ï¼Œè€Œæ˜¯æ–°å…´æ¨ç†å±‚æ¬¡ç»“æ„çš„æ ‡å¿—ï¼Œç±»ä¼¼äºäººç±»è®¤çŸ¥ä¸­é«˜çº§æˆ˜ç•¥è§„åˆ’ä¸ä½çº§ç¨‹åºæ‰§è¡Œçš„åˆ†ç¦»ã€‚æˆ‘ä»¬å‘ç°äº†å¼•äººæ³¨ç›®çš„ä¸¤é˜¶æ®µåŠ¨æ€è¿‡ç¨‹ï¼šåˆæœŸï¼Œæ¨¡å‹å—ç¨‹åºæ­£ç¡®æ€§çº¦æŸï¼Œå¿…é¡»æé«˜ä½çº§æŠ€èƒ½ã€‚å­¦ä¹ ç“¶é¢ˆéšåå‘ç”Ÿå†³å®šæ€§è½¬ç§»ï¼Œæ€§èƒ½æå‡æºäºé«˜çº§æˆ˜ç•¥è§„åˆ’çš„æ¢ç´¢å’ŒæŒæ¡ã€‚è¿™æ­ç¤ºäº†ç°æœ‰RLç®—æ³•ï¼ˆå¦‚GRPOï¼‰çš„æ ¸å¿ƒä½æ•ˆä¹‹å¤„ï¼Œå®ƒä»¬ä»¥æ— çŸ¥çš„æ–¹å¼æ–½åŠ ä¼˜åŒ–å‹åŠ›ï¼Œå°†å­¦ä¹ ä¿¡å·åˆ†æ•£åˆ°æ‰€æœ‰æ ‡è®°ä¸­ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚ä¿¡ç”¨åˆ†é…ï¼ˆHICRAï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†ä¼˜åŒ–å·¥ä½œé›†ä¸­åœ¨å½±å“è§„åˆ’çš„å…³é”®æ ‡è®°ä¸Šã€‚HICRAæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œè¯æ˜å…³æ³¨è¿™ä¸€æˆ˜ç•¥ç“¶é¢ˆæ˜¯è§£é”é«˜çº§æ¨ç†çš„å…³é”®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†è¯­ä¹‰ç†µä½œä¸ºè¡¡é‡æˆ˜ç•¥æ¢ç´¢çš„æŒ‡å—é’ˆä¼˜äºè¯¯å¯¼æ€§æŒ‡æ ‡å¦‚æ ‡è®°çº§ç†µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶ä»éœ€è¦æ·±å…¥ç ”ç©¶ã€‚</li>
<li>æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåˆçº§é˜¶æ®µæ³¨é‡æé«˜ä½çº§æŠ€èƒ½ï¼Œéšåè½¬å‘é«˜çº§æˆ˜ç•¥è§„åˆ’å’Œæ¢ç´¢ã€‚</li>
<li>ç°æœ‰RLç®—æ³•åœ¨ä¼˜åŒ–æ–¹é¢å­˜åœ¨æ ¸å¿ƒä½æ•ˆé—®é¢˜ï¼Œè¡¨ç°ä¸ºä¼˜åŒ–å‹åŠ›çš„æ— çŸ¥æ–½åŠ å’Œå­¦ä¹ ä¿¡å·çš„åˆ†æ•£ã€‚</li>
<li>æå‡ºäº†åˆ†å±‚ä¿¡ç”¨åˆ†é…ï¼ˆHICRAï¼‰ç®—æ³•ï¼Œä¸“æ³¨äºå½±å“è§„åˆ’çš„å…³é”®æ ‡è®°è¿›è¡Œä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
<li>è¯­ä¹‰ç†µè¢«éªŒè¯ä¸ºæ›´æœ‰æ•ˆçš„æŒ‡æ ‡ï¼Œèƒ½æ›´å‡†ç¡®åœ°è¡¡é‡æˆ˜ç•¥æ¢ç´¢è¿‡ç¨‹ã€‚</li>
<li>â€œå•Šå“ˆæ—¶åˆ»â€ã€â€œé•¿åº¦ç¼©æ”¾â€å’Œç†µåŠ¨åŠ›å­¦ç­‰ç°è±¡è¢«è§†ä¸ºæ–°å…´æ¨ç†å±‚æ¬¡ç»“æ„çš„æ ‡å¿—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd7fc897bb2d829f2fcb5db772f297c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b0814acdfa2684105e497792ee8ddd9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5763e1e90c6b91995856241f2e6b11eb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Strefer-Empowering-Video-LLMs-with-Space-Time-Referring-and-Reasoning-via-Synthetic-Instruction-Data"><a href="#Strefer-Empowering-Video-LLMs-with-Space-Time-Referring-and-Reasoning-via-Synthetic-Instruction-Data" class="headerlink" title="Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning   via Synthetic Instruction Data"></a>Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning   via Synthetic Instruction Data</h2><p><strong>Authors:Honglu Zhou, Xiangyu Peng, Shrikant Kendre, Michael S. Ryoo, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles</strong></p>
<p>Next-generation AI companions must go beyond general video understanding to resolve spatial and temporal references in dynamic, real-world environments. Existing Video Large Language Models (Video LLMs), while capable of coarse-level comprehension, struggle with fine-grained, spatiotemporal reasoning, especially when user queries rely on time-based event references for temporal anchoring, or gestural cues for spatial anchoring to clarify object references and positions. To bridge this critical gap, we introduce Strefer, a synthetic instruction data generation framework designed to equip Video LLMs with spatiotemporal referring and reasoning capabilities. Strefer produces diverse instruction-tuning data using a data engine that pseudo-annotates temporally dense, fine-grained video metadata, capturing rich spatial and temporal information in a structured manner, including subjects, objects, their locations as masklets, and their action descriptions and timelines. Our approach enhances the ability of Video LLMs to interpret spatial and temporal references, fostering more versatile, space-time-aware reasoning essential for real-world AI companions. Without using proprietary models, costly human annotation, or the need to annotate large volumes of new videos, experimental evaluations show that models trained with data produced by Strefer outperform baselines on tasks requiring spatial and temporal disambiguation. Additionally, these models exhibit enhanced space-time-aware reasoning, establishing a new foundation for perceptually grounded, instruction-tuned Video LLMs. </p>
<blockquote>
<p>ä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½ä¼´ä¾£å¿…é¡»è¶…è¶Šä¸€èˆ¬çš„è§†é¢‘ç†è§£ï¼Œä»¥è§£å†³åŠ¨æ€ç°å®ç¯å¢ƒä¸­çš„æ—¶ç©ºå‚è€ƒé—®é¢˜ã€‚ç°æœ‰çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰è™½ç„¶èƒ½å¤Ÿè¿›è¡Œç²—ç•¥çº§åˆ«çš„ç†è§£ï¼Œä½†åœ¨ç²¾ç»†çš„æ—¶ç©ºæ¨ç†æ–¹é¢å´é‡åˆ°å›°éš¾ï¼Œå°¤å…¶æ˜¯å½“ç”¨æˆ·çš„æŸ¥è¯¢ä¾èµ–äºåŸºäºæ—¶é—´çš„äº‹ä»¶å¼•ç”¨è¿›è¡Œæ—¶é—´é”šå®šï¼Œæˆ–ä¾èµ–äºæ‰‹åŠ¿çº¿ç´¢è¿›è¡Œç©ºé—´é”šå®šä»¥æ¾„æ¸…å¯¹è±¡å¼•ç”¨å’Œä½ç½®æ—¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Streferï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆæŒ‡ä»¤æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨é…å¤‡è§†é¢‘LLMsçš„æ—¶ç©ºå¼•ç”¨å’Œæ¨ç†èƒ½åŠ›ã€‚Streferä½¿ç”¨ä¸€ä¸ªæ•°æ®å¼•æ“äº§ç”Ÿå„ç§å„æ ·çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œè¯¥å¼•æ“å¯¹æ—¶é—´å¯†é›†ã€ç²¾ç»†çš„è§†é¢‘å…ƒæ•°æ®è¿›è¡Œä¼ªæ³¨é‡Šï¼Œä»¥ç»“æ„åŒ–çš„æ–¹å¼æ•æ‰ä¸°å¯Œçš„ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸»é¢˜ã€å¯¹è±¡ã€ä½œä¸ºè’™ç‰ˆçš„å®ƒä»¬çš„ä½ç½®ä»¥åŠåŠ¨ä½œæè¿°å’Œæ—¶é—´çº¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†è§†é¢‘LLMså¯¹æ—¶ç©ºå¼•ç”¨çš„è§£é‡Šèƒ½åŠ›ï¼Œä¿ƒè¿›äº†æ›´é€šç”¨ã€æ—¶ç©ºæ„ŸçŸ¥æ¨ç†çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºç°å®ä¸–ç•Œçš„äººå·¥æ™ºèƒ½ä¼´ä¾£è‡³å…³é‡è¦ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨Streferäº§ç”Ÿçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨éœ€è¦ç©ºé—´å’Œæ—¶é—´è¾¨åˆ«çš„ä»»åŠ¡ä¸Šè¶…è¶Šäº†åŸºçº¿ï¼Œä¸”è¿™äº›æ¨¡å‹å±•ç°å‡ºå¢å¼ºçš„æ—¶ç©ºæ„ŸçŸ¥æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ„ŸçŸ¥æ¥åœ°ã€æŒ‡ä»¤è°ƒæ•´çš„è§†é¢‘LLMså»ºç«‹äº†æ–°çš„åŸºç¡€ï¼Œæ— éœ€ä½¿ç”¨ä¸“æœ‰æ¨¡å‹ã€æ˜‚è´µçš„äººå·¥æ³¨é‡Šæˆ–éœ€è¦æ³¨é‡Šå¤§é‡æ–°è§†é¢‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03501v1">PDF</a> This technical report serves as the archival version of our paper   accepted at the ICCV 2025 Workshop. For more information, please visit our   project website: <a target="_blank" rel="noopener" href="https://strefer.github.io/">https://strefer.github.io/</a></p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£AIä¼´ä¾£éœ€è¦è¶…è¶Šä¸€èˆ¬çš„è§†é¢‘ç†è§£ï¼Œä»¥è§£å†³åŠ¨æ€ç°å®ç¯å¢ƒä¸­çš„ç©ºé—´å’Œæ—¶é—´å‚è€ƒé—®é¢˜ã€‚ç°æœ‰çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰è™½ç„¶å…·æœ‰ç²—ç•¥çº§åˆ«çš„ç†è§£èƒ½åŠ›ï¼Œä½†åœ¨ç²¾ç»†ç²’åº¦çš„æ—¶ç©ºæ¨ç†æ–¹é¢å´å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯å½“ç”¨æˆ·æŸ¥è¯¢ä¾èµ–äºåŸºäºæ—¶é—´çš„äº‹ä»¶å‚è€ƒè¿›è¡Œæ—¶é—´é”šå®šï¼Œæˆ–åˆ©ç”¨å§¿åŠ¿çº¿ç´¢è¿›è¡Œç©ºé—´é”šå®šä»¥æ˜ç¡®å¯¹è±¡å‚è€ƒå’Œä½ç½®æ—¶ã€‚ä¸ºè§£å†³è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Streferï¼Œä¸€ä¸ªåˆæˆæŒ‡ä»¤æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨é…å¤‡è§†é¢‘LLMså…·å¤‡æ—¶ç©ºå¼•ç”¨å’Œæ¨ç†èƒ½åŠ›ã€‚Streferä½¿ç”¨ä¸€ä¸ªæ•°æ®å¼•æ“äº§ç”Ÿå¤šæ ·åŒ–æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œä»¥æ¨¡æ‹Ÿæ³¨é‡Šæ–¹å¼å¯†é›†åœ°æ•æ‰è§†é¢‘ä¸­çš„æ—¶é—´å’Œç©ºé—´ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸»é¢˜ã€å¯¹è±¡ã€å®ƒä»¬çš„å®šä½ä»¥åŠè¡ŒåŠ¨æè¿°å’Œæ—¶é—´è½´ã€‚è¯¥æ–¹æ³•æé«˜äº†è§†é¢‘LLMsè§£é‡Šç©ºé—´å’Œæ—¶é—´å‚è€ƒçš„èƒ½åŠ›ï¼Œä¿ƒè¿›äº†æ›´çµæ´»ã€æ—¶ç©ºæ„ŸçŸ¥çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å¯¹äºç°å®ä¸–ç•Œçš„AIä¼´ä¾£è‡³å…³é‡è¦ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨Streferäº§ç”Ÿçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨éœ€è¦ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨çš„ä»»åŠ¡ä¸Šè¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸”è¿™äº›æ¨¡å‹å±•ç°äº†å¢å¼ºçš„æ—¶ç©ºæ„ŸçŸ¥æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ„ŸçŸ¥åŸºç¡€çš„ã€æŒ‡ä»¤è°ƒæ•´çš„è§†é¢‘LLMså»ºç«‹äº†æ–°çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‹ä¸€ä»£AIéœ€è¦è§£å†³åŠ¨æ€ç°å®ç¯å¢ƒä¸­çš„ç©ºé—´å’Œæ—¶é—´å‚è€ƒé—®é¢˜ã€‚</li>
<li>ç°æœ‰è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰åœ¨ç²¾ç»†ç²’åº¦çš„æ—¶ç©ºæ¨ç†ä¸Šå­˜åœ¨å›°éš¾ã€‚</li>
<li>Streferæ˜¯ä¸€ä¸ªåˆæˆæŒ‡ä»¤æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†é¢‘LLMsçš„æ—¶ç©ºå¼•ç”¨å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>Streferé€šè¿‡æ•°æ®å¼•æ“æ¨¡æ‹Ÿæ³¨é‡Šæ–¹å¼æ•æ‰è§†é¢‘ä¸­çš„æ—¶é—´å’Œç©ºé—´ä¿¡æ¯ã€‚</li>
<li>Streferæé«˜äº†è§†é¢‘LLMsè§£é‡Šç©ºé—´å’Œæ—¶é—´å‚è€ƒçš„èƒ½åŠ›ï¼Œä¿ƒè¿›æ›´çµæ´»çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨Streferäº§ç”Ÿçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨éœ€è¦ç©ºé—´å’Œæ—¶é—´çš„ä»»åŠ¡ä¸Šè¡¨ç°è¶…è¶ŠåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0545a66520741b90c5ab46d3cb19e03b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81c4125e1ea06d0fdecbe22d46c8c4bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfc953a6a2959282045ebce4e5d5ced9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10ec375a8381dee71dadaa987615ce93.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="On-Entropy-Control-in-LLM-RL-Algorithms"><a href="#On-Entropy-Control-in-LLM-RL-Algorithms" class="headerlink" title="On Entropy Control in LLM-RL Algorithms"></a>On Entropy Control in LLM-RL Algorithms</h2><p><strong>Authors:Han Shen</strong></p>
<p>For RL algorithms, appropriate entropy control is crucial to their effectiveness. To control the policy entropy, a commonly used method is entropy regularization, which is adopted in various popular RL algorithms including PPO, SAC and A3C. Although entropy regularization proves effective in robotic and games RL conventionally, studies found that it gives weak to no gains in LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL setting. Specifically, we first argue that the conventional entropy regularization suffers from the LLMâ€™s extremely large response space and the sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy control method that utilizes a new clamped entropy bonus with an automatically adjusted coefficient. The clamped entropy is evaluated with the re-normalized policy defined on certain smaller token space, which encourages exploration within a more compact response set. In addition, the algorithm automatically adjusts entropy coefficient according to the clamped entropy value, effectively controlling the entropy-induced bias while leveraging the entropyâ€™s benefits. AEnt is tested in math-reasoning tasks under different base models and datasets, and it is observed that AEnt outperforms the baselines consistently across multiple benchmarks. </p>
<blockquote>
<p>å¯¹äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•æ¥è¯´ï¼Œé€‚å½“çš„ç†µæ§åˆ¶å¯¹å…¶æ•ˆæœè‡³å…³é‡è¦ã€‚ä¸ºäº†æ§åˆ¶ç­–ç•¥ç†µï¼Œä¸€ç§å¸¸ç”¨çš„æ–¹æ³•æ˜¯ç†µæ­£åˆ™åŒ–ï¼Œå®ƒå·²è¢«å¹¿æ³›åº”ç”¨äºå„ç§æµè¡Œçš„RLç®—æ³•ä¸­ï¼ŒåŒ…æ‹¬PPOã€SACå’ŒA3Cã€‚å°½ç®¡ç†µæ­£åˆ™åŒ–åœ¨æœºå™¨äººå’Œæ¸¸æˆé¢†åŸŸçš„RLä¸­è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ç ”ç©¶å‘ç°å®ƒåœ¨LLM-RLè®­ç»ƒä¸­å‡ ä¹æ²¡æœ‰æ•ˆæœã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLM-RLè®¾ç½®ä¸­ç†µå¥–åŠ±çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¤ä¸ºä¼ ç»Ÿçš„ç†µæ­£åˆ™åŒ–é¢ä¸´ç€LLMæå…¶å¤§çš„å“åº”ç©ºé—´å’Œæœ€ä¼˜è¾“å‡ºçš„ç¨€ç–æ€§é—®é¢˜ã€‚ä½œä¸ºä¸€ç§è¡¥æ•‘æªæ–½ï¼Œæˆ‘ä»¬æå‡ºäº†AEntï¼Œè¿™æ˜¯ä¸€ç§ç†µæ§åˆ¶æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨äº†ä¸€ç§æ–°çš„å¸¦è‡ªåŠ¨è°ƒæ•´ç³»æ•°çš„å¤¹æŒç†µå¥–åŠ±ã€‚å¤¹æŒç†µæ˜¯åœ¨æŸäº›è¾ƒå°çš„ç¬¦å·ç©ºé—´ä¸Šå®šä¹‰çš„é‡æ–°å½’ä¸€åŒ–ç­–ç•¥ä¸Šè¯„ä¼°çš„ï¼Œè¿™é¼“åŠ±åœ¨ä¸€ä¸ªæ›´ç´§å‡‘çš„å“åº”é›†å†…è¿›è¡Œæ¢ç´¢ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•ä¼šæ ¹æ®å¤¹æŒç†µå€¼è‡ªåŠ¨è°ƒæ•´ç†µç³»æ•°ï¼Œæœ‰æ•ˆåœ°æ§åˆ¶ç”±ç†µå¼•èµ·çš„åè§ï¼ŒåŒæ—¶åˆ©ç”¨ç†µçš„å¥½å¤„ã€‚AEntåœ¨ä¸åŒçš„åŸºç¡€æ¨¡å‹å’Œæ•°æ®é›†ä¸‹è¿›è¡Œäº†æ•°å­¦æ¨ç†ä»»åŠ¡çš„æµ‹è¯•ï¼Œè§‚å¯Ÿå‘ç°ï¼ŒAEntåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03493v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œé€‚å½“çš„ç†µæ§åˆ¶å¯¹å…¶æ•ˆæœè‡³å…³é‡è¦ã€‚ä¸ºäº†æ§åˆ¶ç­–ç•¥ç†µï¼Œå¸¸ç”¨çš„æ–¹æ³•æ˜¯ç†µæ­£åˆ™åŒ–ï¼Œå®ƒè¢«å¹¿æ³›åº”ç”¨äºPPOã€SACå’ŒA3Cç­‰æµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ã€‚å°½ç®¡ç†µæ­£åˆ™åŒ–åœ¨æœºå™¨äººå’Œæ¸¸æˆç­‰é¢†åŸŸçš„å¼ºåŒ–å­¦ä¹ ä¸­è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†åœ¨LLM-RLè®­ç»ƒä¸­ï¼Œå®ƒçš„ä½œç”¨å¾®ä¹å…¶å¾®ç”šè‡³æ¯«æ— å¢ç›Šã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLM-RLç¯å¢ƒä¸­ç†µå¥–åŠ±çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¤ä¸ºä¼ ç»Ÿçš„ç†µæ­£åˆ™åŒ–å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹å“åº”ç©ºé—´æå¤§å’Œæœ€ä¼˜è¾“å‡ºç¨€ç–æ€§çš„å›°æ‰°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ–°çš„å¤¹æŒç†µå¥–åŠ±çš„ç†µæ§åˆ¶æ–¹æ³•AEntï¼Œå¹¶è‡ªåŠ¨è°ƒæ•´ç³»æ•°ã€‚å¤¹æŒç†µæ˜¯åœ¨è¾ƒå°çš„æ ‡è®°ç©ºé—´ä¸Šå®šä¹‰çš„å½’ä¸€åŒ–ç­–ç•¥è¯„ä¼°çš„ï¼Œè¿™é¼“åŠ±åœ¨ä¸€ä¸ªæ›´ç´§å‡‘çš„å“åº”é›†åˆå†…è¿›è¡Œæ¢ç´¢ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•ä¼šæ ¹æ®å¤¹æŒçš„ç†µå€¼è‡ªåŠ¨è°ƒæ•´ç†µç³»æ•°ï¼Œåœ¨åˆ©ç”¨ç†µçš„å¥½å¤„çš„åŒæ—¶æœ‰æ•ˆåœ°æ§åˆ¶ç†µå¼•èµ·çš„åè§ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­æµ‹è¯•AEntï¼Œå‘ç°å…¶åœ¨ä¸åŒåŸºç¡€æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€‚å½“çš„ç†µæ§åˆ¶åœ¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿçš„ç†µæ­£åˆ™åŒ–åœ¨LLM-RLè®­ç»ƒä¸­æ•ˆæœä¸ä½³ã€‚</li>
<li>LLMçš„å“åº”ç©ºé—´æå¤§å’Œæœ€ä¼˜è¾“å‡ºç¨€ç–æ€§æ˜¯ä¼ ç»Ÿç†µæ­£åˆ™åŒ–é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç†µæ§åˆ¶æ–¹æ³•AEntï¼Œé‡‡ç”¨å¤¹æŒç†µå¥–åŠ±å’Œè‡ªåŠ¨è°ƒæ•´ç³»æ•°ã€‚</li>
<li>å¤¹æŒç†µé¼“åŠ±åœ¨æ›´ç´§å‡‘çš„å“åº”é›†åˆå†…è¿›è¡Œæ¢ç´¢ã€‚</li>
<li>AEntèƒ½å¤Ÿè‡ªåŠ¨è°ƒæ•´ç†µç³»æ•°ï¼Œæ§åˆ¶ç†µå¼•èµ·çš„åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-edd3abaa1fe3347d0e12a13333f4a72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-092e22d926b5e609417b11a43f6af9a0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Beyond-Correctness-Harmonizing-Process-and-Outcome-Rewards-through-RL-Training"><a href="#Beyond-Correctness-Harmonizing-Process-and-Outcome-Rewards-through-RL-Training" class="headerlink" title="Beyond Correctness: Harmonizing Process and Outcome Rewards through RL   Training"></a>Beyond Correctness: Harmonizing Process and Outcome Rewards through RL   Training</h2><p><strong>Authors:Chenlu Ye, Zhou Yu, Ziji Zhang, Hao Chen, Narayanan Sadagopan, Jing Huang, Tong Zhang, Anurag Beniwal</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) has emerged to be a predominant paradigm for mathematical reasoning tasks, offering stable improvements in reasoning ability. However, Outcome Reward Models (ORMs) in RLVR are too coarse-grained to distinguish flawed reasoning within correct answers or valid reasoning within incorrect answers. This lack of granularity introduces noisy and misleading gradients significantly and hinders further progress in reasoning process quality. While Process Reward Models (PRMs) offer fine-grained guidance for intermediate steps, they frequently suffer from inaccuracies and are susceptible to reward hacking.   To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an effective data process curation method that harmonizes noisy, fine-grained process rewards with accurate, coarse-grained outcome rewards. Rather than naively blending PRM and ORM in the objective function (arXiv:archive&#x2F;2506.18896), PROF leverages their complementary strengths through consistency-driven sample selection. Our approach retains correct responses with higher averaged process values and incorrect responses with lower averaged process values, while maintaining positive&#x2F;negative training sample balance. Extensive experiments demonstrate that our method not only consistently improves the final accuracy over $4%$ compared to the blending approaches, but also strengthens the quality of intermediate reasoning steps. Codes and training recipes are available at <a target="_blank" rel="noopener" href="https://github.com/Chenluye99/PROF">https://github.com/Chenluye99/PROF</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„ä¸»è¦èŒƒå¼ï¼Œä¸ºæé«˜æ¨ç†èƒ½åŠ›æä¾›äº†ç¨³å®šçš„æ”¹è¿›ã€‚ç„¶è€Œï¼ŒRLVRä¸­çš„ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰è¿‡äºç²—ç³™ï¼Œæ— æ³•åŒºåˆ†æ­£ç¡®ç­”æ¡ˆä¸­çš„é”™è¯¯æ¨ç†æˆ–é”™è¯¯ç­”æ¡ˆä¸­çš„åˆç†æ¨ç†ã€‚è¿™ç§ç¼ºä¹ç²¾ç»†åº¦å¼•å…¥äº†å¤§é‡å™ªå£°å’Œè¯¯å¯¼æ€§çš„æ¢¯åº¦ï¼Œé˜»ç¢äº†æ¨ç†è¿‡ç¨‹è´¨é‡çš„è¿›ä¸€æ­¥æé«˜ã€‚è™½ç„¶è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä¸ºä¸­é—´æ­¥éª¤æä¾›äº†ç²¾ç»†çš„æŒ‡å¯¼ï¼Œä½†å®ƒä»¬ç»å¸¸å­˜åœ¨ä¸å‡†ç¡®çš„æƒ…å†µï¼Œå¹¶å®¹æ˜“å—åˆ°å¥–åŠ±æ”»å‡»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å›°å¢ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†è¿‡ç¨‹ä¸€è‡´æ€§è¿‡æ»¤å™¨ï¼ˆPROFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ•°æ®è¿‡ç¨‹ç­›é€‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåè°ƒå™ªå£°ã€ç²¾ç»†åŒ–çš„è¿‡ç¨‹å¥–åŠ±ä¸å‡†ç¡®ã€ç²—ç•¥çš„ç»“æœå¥–åŠ±ã€‚PROFé€šè¿‡ä¸€è‡´æ€§é©±åŠ¨çš„æ ·æœ¬é€‰æ‹©ï¼Œè€Œä¸æ˜¯ç®€å•åœ°èåˆPRMå’ŒORMåœ¨ç›®æ ‡å‡½æ•°ä¸­ï¼ˆarXiv:archive&#x2F;2506.18896ï¼‰ï¼Œåˆ©ç”¨å®ƒä»¬çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™äº†å…·æœ‰è¾ƒé«˜å¹³å‡è¿‡ç¨‹å€¼çš„æ­£ç¡®å“åº”å’Œå…·æœ‰è¾ƒä½å¹³å‡è¿‡ç¨‹å€¼çš„é”™è¯¯å“åº”ï¼ŒåŒæ—¶ä¿æŒæ­£è´Ÿè®­ç»ƒæ ·æœ¬çš„å¹³è¡¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¸èåˆæ–¹æ³•ç›¸æ¯”ï¼Œæœ€ç»ˆç²¾åº¦æŒç»­æé«˜äº†è¶…è¿‡4%ï¼Œè€Œä¸”è¿˜åŠ å¼ºäº†ä¸­é—´æ¨ç†æ­¥éª¤çš„è´¨é‡ã€‚ä»£ç å’ŒåŸ¹è®­é…æ–¹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Chenluye99/PROF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Chenluye99/PROFæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03403v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºæ•°å­¦æ¨ç†ä»»åŠ¡çš„ä¸»è¦èŒƒå¼ï¼Œå¯¹æé«˜æ¨ç†èƒ½åŠ›å…·æœ‰ç¨³å®šä½œç”¨ã€‚ç„¶è€Œï¼ŒRLVRä¸­çš„ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰è¿‡äºç²—ç³™ï¼Œæ— æ³•åŒºåˆ†æ­£ç¡®ç­”æ¡ˆä¸­çš„é”™è¯¯æ¨ç†æˆ–é”™è¯¯ç­”æ¡ˆä¸­çš„åˆç†æ¨ç†ã€‚è¿™ç§ç¼ºä¹ç²’åº¦å¼•å…¥äº†å™ªå£°å’Œè¯¯å¯¼æ€§çš„æ¢¯åº¦ï¼Œæ˜¾è‘—é˜»ç¢äº†æ¨ç†è¿‡ç¨‹è´¨é‡çš„è¿›ä¸€æ­¥æé«˜ã€‚è™½ç„¶è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä¸ºä¸­é—´æ­¥éª¤æä¾›äº†ç²¾ç»†æŒ‡å¯¼ï¼Œä½†å®ƒä»¬ç»å¸¸å­˜åœ¨ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œå¹¶å®¹æ˜“å—åˆ°å¥–åŠ±ç ´è§£çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¿‡ç¨‹ä¸€è‡´æ€§è¿‡æ»¤å™¨ï¼ˆPROFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ•°æ®è¿‡ç¨‹ç­›é€‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåè°ƒå™ªå£°ã€ç²¾ç»†çš„è¿‡ç¨‹å¥–åŠ±ä¸å‡†ç¡®ã€ç²—ç³™çš„ç»“æœå¥–åŠ±ã€‚PROFé€šè¿‡ä¸€è‡´æ€§é©±åŠ¨çš„æ ·æœ¬é€‰æ‹©ï¼Œæœ‰æ•ˆç»“åˆPRMå’ŒORMçš„ä¼˜ç‚¹ï¼Œè€Œä¸æ˜¯ç®€å•åœ°å°†å…¶èå…¥ç›®æ ‡å‡½æ•°ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™å…·æœ‰è¾ƒé«˜å¹³å‡è¿‡ç¨‹å€¼çš„æ­£ç¡®å“åº”å¹¶æ’é™¤è¾ƒä½å¹³å‡è¿‡ç¨‹å€¼çš„é”™è¯¯å“åº”ï¼ŒåŒæ—¶ä¿æŒæ­£è´Ÿè®­ç»ƒæ ·æœ¬çš„å¹³è¡¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å°†æœ€ç»ˆç²¾åº¦æé«˜äº†è¶…è¿‡4%ï¼Œè€Œä¸”æé«˜äº†ä¸­é—´æ¨ç†æ­¥éª¤çš„è´¨é‡ã€‚ç›¸å…³ä»£ç å’ŒåŸ¹è®­é£Ÿè°±å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Chenluye99/PROF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Chenluye99/PROFæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²ç”¨äºæ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œå¹¶åœ¨æé«˜æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºç¨³å®šçš„æ•ˆæœã€‚</li>
<li>ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMsï¼‰è¿‡äºç²—ç³™ï¼Œæ— æ³•å‡†ç¡®åŒºåˆ†ä¸åŒæ¨ç†è´¨é‡çš„æƒ…å†µï¼Œå¯¼è‡´å™ªå£°å’Œè¯¯å¯¼æ€§çš„æ¢¯åº¦ã€‚</li>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰è™½ç„¶èƒ½æä¾›ç²¾ç»†æŒ‡å¯¼ï¼Œä½†å­˜åœ¨ä¸å‡†ç¡®å’Œæ˜“å—å¥–åŠ±ç ´è§£å½±å“çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥è¿‡ç¨‹ä¸€è‡´æ€§è¿‡æ»¤å™¨ï¼ˆPROFï¼‰ï¼Œç»“åˆPRMå’ŒORMçš„ä¼˜ç‚¹ï¼Œé€šè¿‡ä¸€è‡´æ€§é©±åŠ¨çš„æ ·æœ¬é€‰æ‹©æ¥æé«˜æ•°æ®è¿‡ç¨‹è´¨é‡ã€‚</li>
<li>PROFèƒ½æé«˜æ­£ç¡®å“åº”çš„ä¿ç•™ç‡å¹¶æ’é™¤é”™è¯¯å“åº”ï¼ŒåŒæ—¶ä¿æŒæ­£è´Ÿè®­ç»ƒæ ·æœ¬çš„å¹³è¡¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒPROFä¸ä»…æé«˜äº†æœ€ç»ˆç²¾åº¦è¶…è¿‡4%ï¼Œè€Œä¸”æé«˜äº†ä¸­é—´æ¨ç†æ­¥éª¤çš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71dbc747cd472216fbc4247a62be7a45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-830e45c73f34ec24b509a1330b20d32c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f8932f3dfea0f0aa75c98842660c0d8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Language-Models-Do-Not-Follow-Occamâ€™s-Razor-A-Benchmark-for-Inductive-and-Abductive-Reasoning"><a href="#Language-Models-Do-Not-Follow-Occamâ€™s-Razor-A-Benchmark-for-Inductive-and-Abductive-Reasoning" class="headerlink" title="Language Models Do Not Follow Occamâ€™s Razor: A Benchmark for Inductive   and Abductive Reasoning"></a>Language Models Do Not Follow Occamâ€™s Razor: A Benchmark for Inductive   and Abductive Reasoning</h2><p><strong>Authors:Yunxin Sun, Abulhair Saparov</strong></p>
<p>Reasoning is a core capability in artificial intelligence systems, for which large language models (LLMs) have recently shown remarkable progress. However, most work focuses exclusively on deductive reasoning, which is problematic since other types of reasoning are also essential in solving real-world problems, and they are less explored. This work focuses on evaluating LLMsâ€™ inductive and abductive reasoning capabilities. We introduce a programmable and synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example consists of an incomplete world model and a set of observations. The task for the intelligent agent is to produce hypotheses to explain observations under the incomplete world model to solve each reasoning example. We propose a new metric to evaluate the quality of hypotheses based on Occamâ€™s Razor. We evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs can perform inductive and abductive reasoning in simple scenarios, but struggle with complex world models and producing high-quality hypotheses, even with popular reasoning-enhancing techniques such as in-context learning and RLVR. </p>
<blockquote>
<p>æ¨ç†æ˜¯äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿™æ–¹é¢æœ€è¿‘å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å·¥ä½œåªä¸“æ³¨äºæ¼”ç»æ¨ç†ï¼Œè¿™æ˜¯æœ‰é—®é¢˜çš„ï¼Œå› ä¸ºå…¶ä»–ç±»å‹çš„æ¨ç†åœ¨è§£å†³ç°å®ä¸–ç•Œçš„é—®é¢˜æ—¶ä¹Ÿæ˜¯å¿…ä¸å¯å°‘çš„ï¼Œè€Œä¸”å®ƒä»¬çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚è¿™é¡¹å·¥ä½œé‡ç‚¹è¯„ä¼°LLMçš„å½’çº³æ¨ç†å’Œæº¯å› æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯ç¼–ç¨‹çš„åˆæˆæ•°æ®é›†InAbHyDï¼ˆå‘éŸ³ä¸ºin-a-bidï¼‰ï¼Œæ¯ä¸ªæ¨ç†ç¤ºä¾‹éƒ½åŒ…å«ä¸€ä¸ªä¸å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹å’Œä¸€ç»„è§‚å¯Ÿç»“æœã€‚æ™ºèƒ½ä»£ç†çš„ä»»åŠ¡æ˜¯åœ¨ä¸å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹ä¸‹äº§ç”Ÿå‡è®¾æ¥è§£é‡Šè§‚å¯Ÿç»“æœï¼Œä»¥è§£å†³æ¯ä¸ªæ¨ç†ç¤ºä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æŒ‡æ ‡æ¥è¯„ä¼°å‡è®¾çš„è´¨é‡ï¼Œè¯¥æŒ‡æ ‡åŸºäºå¥¥å¡å§†å‰ƒåˆ€åŸåˆ™ã€‚æˆ‘ä»¬è¯„ä¼°å’Œåˆ†æäº†å½“å‰çš„ä¸€äº›å…ˆè¿›LLMã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLLMå¯ä»¥åœ¨ç®€å•åœºæ™¯ä¸­æ‰§è¡Œå½’çº³æ¨ç†å’Œæº¯å› æ¨ç†ï¼Œä½†åœ¨å¤„ç†å¤æ‚ä¸–ç•Œæ¨¡å‹å’Œäº§ç”Ÿé«˜è´¨é‡å‡è®¾æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå³ä½¿åœ¨æµè¡Œçš„å¢å¼ºæ¨ç†æŠ€æœ¯å¦‚ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒRLVRçš„å¸®åŠ©ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03345v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›æ–¹é¢å·²å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ä»…ä¸“æ³¨äºæ¼”ç»æ¨ç†ï¼Œå¿½ç•¥äº†å½’çº³å’Œæº¯å› æ¨ç†ç­‰åŒæ ·é‡è¦çš„æ–¹é¢ã€‚æœ¬æ–‡ä¸“æ³¨äºè¯„ä¼°LLMsçš„å½’çº³å’Œæº¯å› æ¨ç†èƒ½åŠ›ï¼Œä»‹ç»äº†ä¸€ä¸ªæ–°çš„å¯ç¼–ç¨‹åˆæˆæ•°æ®é›†InAbHyDï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä»£ç†åœ¨ä¸å®Œå…¨ä¸–ç•Œæ¨¡å‹ä¸‹äº§ç”Ÿè§£é‡Šè§‚å¯Ÿç»“æœçš„å‡è®¾çš„èƒ½åŠ›ã€‚æœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åŸºäºå¥¥å¡å§†å‰ƒåˆ€åŸç†çš„æ–°è¯„ä¼°æŒ‡æ ‡ã€‚åˆ†æè¡¨æ˜ï¼ŒLLMsåœ¨ç®€å•åœºæ™¯ä¸­å¯ä»¥è¿›è¡Œå½’çº³å’Œæº¯å› æ¨ç†ï¼Œä½†åœ¨å¤æ‚ä¸–ç•Œæ¨¡å‹å’Œäº§ç”Ÿé«˜è´¨é‡å‡è®¾æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå³ä½¿æ˜¯æµè¡Œçš„å¢å¼ºæ¨ç†æŠ€æœ¯å¦‚ä¸Šä¸‹æ–‡å†…å­¦ä¹ å’ŒRLVRä¹Ÿæ— æ³•å®Œå…¨è§£å†³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰ç ”ç©¶å¤šä¸“æ³¨äºæ¼”ç»æ¨ç†ï¼Œå¿½ç•¥äº†å…¶ä»–ç±»å‹çš„æ¨ç†å¦‚å½’çº³å’Œæº¯å› æ¨ç†ã€‚</li>
<li>ä»‹ç»äº†æ–°çš„å¯ç¼–ç¨‹åˆæˆæ•°æ®é›†InAbHyDï¼ŒåŒ…å«ä¸å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹å’Œè§‚å¯Ÿé›†ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä»£ç†åœ¨ç»™å®šä¸–ç•Œæ¨¡å‹ä¸‹äº§ç”Ÿè§£é‡Šè§‚å¯Ÿç»“æœçš„å‡è®¾çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†åŸºäºå¥¥å¡å§†å‰ƒåˆ€åŸç†çš„æ–°å‡è®¾è´¨é‡è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>LLMsåœ¨ç®€å•åœºæ™¯ä¸­å¯ä»¥æ‰§è¡Œå½’çº³å’Œæº¯å› æ¨ç†ä»»åŠ¡ã€‚</li>
<li>LLMsåœ¨å¤„ç†å¤æ‚ä¸–ç•Œæ¨¡å‹å’Œç”Ÿæˆé«˜è´¨é‡å‡è®¾æ–¹é¢é‡åˆ°å›°éš¾ã€‚</li>
<li>æµè¡Œçš„å¢å¼ºæ¨ç†æŠ€æœ¯å¦‚ä¸Šä¸‹æ–‡å†…å­¦ä¹ å’ŒRLVRè™½æœ‰ä¸€å®šå¸®åŠ©ï¼Œä½†æ— æ³•å®Œå…¨è§£å†³LLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96e4166491566ee444b3269729f59313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbfe44f70aae4656b919988dc5e9ace3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4db96d2d1cfa20b6243ec7189be07f47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9c213577fa28a1c6607a73b09ce76da.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="VulnRepairEval-An-Exploit-Based-Evaluation-Framework-for-Assessing-Large-Language-Model-Vulnerability-Repair-Capabilities"><a href="#VulnRepairEval-An-Exploit-Based-Evaluation-Framework-for-Assessing-Large-Language-Model-Vulnerability-Repair-Capabilities" class="headerlink" title="VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing   Large Language Model Vulnerability Repair Capabilities"></a>VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing   Large Language Model Vulnerability Repair Capabilities</h2><p><strong>Authors:Weizhe Wang, Wei Ma, Qiang Hu, Yao Zhang, Jianfei Sun, Bin Wu, Yang Liu, Guangquan Xu, Lingxiao Jiang</strong></p>
<p>The adoption of Large Language Models (LLMs) for automated software vulnerability patching has shown promising outcomes on carefully curated evaluation sets. Nevertheless, existing datasets predominantly rely on superficial validation methods rather than exploit-based verification, leading to overestimated performance in security-sensitive applications. This paper introduces VulnRepairEval, an evaluation framework anchored in functional Proof-of-Concept (PoC) exploits. Our framework delivers a comprehensive, containerized evaluation pipeline that enables reproducible differential assessment, where repair success requires the original exploit to fail execution against the modified code. The benchmark construction involved extensive data curation: we processed over 400 CVEs and approximately 2,500 potential sources to extract a collection of authentic vulnerability instances (23 Python CVEs) amenable to automated testing with working PoCs. Through VulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and observe a significant performance deficit: even the top-performing model successfully addresses merely 5&#x2F;23 instances (about 21.7%), exposing critical weaknesses in security-focused applications. Our failure analysis reveals that most unsuccessful attempts stem from imprecise vulnerability identification and patches containing syntactic or semantic errors. Enhanced prompting strategies and multi-agent approaches yield minimal improvements, with overall effectiveness remaining largely unaffected. This work contributes a stringent, practical evaluation framework for LLM-driven vulnerability remediation and underscores the necessity for assessment protocols that authentically reflect real-world exploitation scenarios. </p>
<blockquote>
<p>é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–è½¯ä»¶æ¼æ´ä¿®è¡¥ï¼Œåœ¨ç²¾å¿ƒç­–åˆ’çš„è¯„ä¼°é›†ä¸Šå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†ä¸»è¦ä¾èµ–æµ…å±‚æ¬¡çš„éªŒè¯æ–¹æ³•ï¼Œè€ŒéåŸºäºæ”»å‡»çš„éªŒè¯æ–¹æ³•ï¼Œè¿™å¯¼è‡´åœ¨å®‰å…¨æ€§æ•æ„Ÿçš„åº”ç”¨ä¸­æ€§èƒ½è¢«é«˜ä¼°ã€‚æœ¬æ–‡ä»‹ç»äº†VulnRepairEvalï¼Œä¸€ä¸ªä»¥åŠŸèƒ½æ€§æ¦‚å¿µè¯æ˜ï¼ˆPoCï¼‰æ”»å‡»ä¸ºåŸºç¡€çš„è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶æä¾›äº†ä¸€ä¸ªå…¨é¢ã€å®¹å™¨åŒ–çš„è¯„ä¼°æµç¨‹ï¼Œèƒ½å¤Ÿå®ç°å¯é‡ç°çš„å·®å¼‚åŒ–è¯„ä¼°ï¼Œå…¶ä¸­ä¿®å¤æˆåŠŸéœ€è¦åŸå§‹æ”»å‡»å¯¹ä¿®æ”¹åçš„ä»£ç æ— æ³•æ‰§è¡Œã€‚åŸºå‡†æµ‹è¯•æ„å»ºæ¶‰åŠå¤§é‡æ•°æ®æ•´ç†ï¼šæˆ‘ä»¬å¤„ç†äº†è¶…è¿‡400ä¸ªCVEå’Œå¤§çº¦2500ä¸ªæ½œåœ¨æ¥æºï¼Œæå–å‡ºå¯ç”¨äºè‡ªåŠ¨åŒ–æµ‹è¯•çš„çœŸå®æ¼æ´å®ä¾‹é›†åˆï¼ˆ23ä¸ªPython CVEï¼‰ã€‚é€šè¿‡VulnRepairEvalï¼Œæˆ‘ä»¬å¯¹12ä¸ªæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶è§‚å¯Ÿåˆ°æ˜¾è‘—çš„æ€§èƒ½ä¸è¶³ï¼šå³ä½¿è¡¨ç°æœ€ä½³çš„æ¨¡å‹ä¹Ÿä»…èƒ½æˆåŠŸè§£å†³5&#x2F;23ä¸ªå®ä¾‹ï¼ˆçº¦21.7%ï¼‰ï¼Œæš´éœ²å‡ºåœ¨å®‰å…¨å¯¼å‘åº”ç”¨ä¸­çš„å…³é”®å¼±ç‚¹ã€‚æˆ‘ä»¬çš„å¤±è´¥åˆ†æè¡¨æ˜ï¼Œå¤§å¤šæ•°ä¸æˆåŠŸå°è¯•æºäºä¸ç²¾ç¡®çš„æ¼æ´è¯†åˆ«å’Œè¡¥ä¸ä¸­çš„è¯­æ³•æˆ–è¯­ä¹‰é”™è¯¯ã€‚å¢å¼ºæç¤ºç­–ç•¥å’Œå¤šå…ƒä»£ç†æ–¹æ³•äº§ç”Ÿçš„æ”¹è¿›å¾®ä¹å…¶å¾®ï¼Œæ€»ä½“æ•ˆæœåŸºæœ¬ä¸å—å½±å“ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ¼æ´ä¿®å¤æä¾›äº†ä¸€ä¸ªä¸¥æ ¼ã€å®ç”¨çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦è¯„ä¼°åè®®çœŸå®åæ˜ ç°å®ä¸–ç•Œæ”»å‡»åœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03331v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–è½¯ä»¶æ¼æ´ä¿®è¡¥çš„è¯„ä¼°æ¡†æ¶VulnRepairEvalã€‚è¯¥æ¡†æ¶åŸºäºåŠŸèƒ½æ€§Proof-of-Conceptï¼ˆPOCï¼‰æ¼æ´è¿›è¡Œç»¼åˆè¯„ä»·ï¼Œæä¾›äº†ä¸€ä¸ªå®¹å™¨åŒ–çš„è¯„ä¼°ç®¡é“ï¼Œå®ç°äº†å¯é‡å¤çš„å·®å¼‚åŒ–è¯„ä¼°ã€‚é€šè¿‡å¯¹çœŸå®æ¼æ´å®ä¾‹çš„è¯„ä¼°ï¼Œå‘ç°ç°æœ‰LLMsåœ¨è‡ªåŠ¨åŒ–æµ‹è¯•ä¸­å­˜åœ¨æ˜¾è‘—æ€§èƒ½ç¼ºé™·ï¼Œå¤§éƒ¨åˆ†å¤±è´¥æºäºä¸ç²¾ç¡®çš„æ¼æ´è¯†åˆ«å’ŒåŒ…å«è¯­æ³•æˆ–è¯­ä¹‰é”™è¯¯çš„è¡¥ä¸ã€‚æ–‡ç« å¼ºè°ƒäº†éœ€è¦åæ˜ çœŸå®ä¸–ç•Œåˆ©ç”¨åœºæ™¯çš„è¯„ä»·åè®®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–è½¯ä»¶æ¼æ´ä¿®è¡¥é¢†åŸŸå…·æœ‰åº”ç”¨å‰æ™¯ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ä¸»è¦ä¾èµ–æµ…å±‚æ¬¡çš„éªŒè¯æ–¹æ³•ï¼Œè€ŒéåŸºäºæ”»å‡»çš„éªŒè¯ï¼Œå¯¼è‡´åœ¨å®‰å…¨æ€§æ•æ„Ÿåº”ç”¨ä¸­çš„æ€§èƒ½è¢«é«˜ä¼°ã€‚</li>
<li>å¼•å…¥VulnRepairEvalè¯„ä¼°æ¡†æ¶ï¼ŒåŸºäºåŠŸèƒ½æ€§Proof-of-Conceptï¼ˆPOCï¼‰æ¼æ´è¿›è¡Œç»¼åˆè¯„ä»·ã€‚</li>
<li>é€šè¿‡å¯¹çœŸå®æ¼æ´å®ä¾‹çš„è¯„ä¼°ï¼Œå‘ç°LLMså­˜åœ¨æ˜¾è‘—æ€§èƒ½ç¼ºé™·ï¼Œå¤§éƒ¨åˆ†å¤±è´¥æºäºä¸ç²¾ç¡®çš„æ¼æ´è¯†åˆ«å’ŒåŒ…å«é”™è¯¯çš„è¡¥ä¸ã€‚</li>
<li>å¢å¼ºæç¤ºç­–ç•¥å’Œé‡‡ç”¨å¤šæ™ºèƒ½ä½“æ–¹æ³•åªèƒ½å¸¦æ¥å¾®å°æ”¹è¿›ï¼Œæ€»ä½“æ•ˆæœå½±å“ä¸å¤§ã€‚</li>
<li>è¯¥æ–‡æä¾›äº†ä¸€ä¸ªä¸¥æ ¼çš„å®ç”¨è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºLLMé©±åŠ¨çš„è½¯ä»¶æ¼æ´ä¿®å¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3aaa053609bf7e9a06d7adcb26492041.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eee0a9ccc2c7e2af3e3cdc853dd4711.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-917500af50b05135feed9ae3d12951ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6b518c441e99d59095589df0d8f43a0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AgenTracer-Who-Is-Inducing-Failure-in-the-LLM-Agentic-Systems"><a href="#AgenTracer-Who-Is-Inducing-Failure-in-the-LLM-Agentic-Systems" class="headerlink" title="AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?"></a>AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?</h2><p><strong>Authors:Guibin Zhang, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, Shuicheng Yan</strong></p>
<p>Large Language Model (LLM)-based agentic systems, often comprising multiple models, complex tool invocations, and orchestration protocols, substantially outperform monolithic agents. Yet this very sophistication amplifies their fragility, making them more prone to system failure. Pinpointing the specific agent or step responsible for an error within long execution traces defines the task of agentic system failure attribution. Current state-of-the-art reasoning LLMs, however, remain strikingly inadequate for this challenge, with accuracy generally below 10%. To address this gap, we propose AgenTracer, the first automated framework for annotating failed multi-agent trajectories via counterfactual replay and programmed fault injection, producing the curated dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a lightweight failure tracer trained with multi-granular reinforcement learning, capable of efficiently diagnosing errors in verbose multi-agent interactions. On the Who&amp;When benchmark, AgenTracer-8B outperforms giant proprietary LLMs like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS with 4.8-14.2% performance gains, empowering self-correcting and self-evolving agentic AI. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†ç³»ç»Ÿï¼Œé€šå¸¸åŒ…å«å¤šä¸ªæ¨¡å‹ã€å¤æ‚çš„å·¥å…·è°ƒç”¨å’ŒååŒåè®®ï¼Œåœ¨æ€§èƒ½ä¸Šå¤§å¤§ä¼˜äºå•ä¸€ä»£ç†ã€‚ç„¶è€Œï¼Œè¿™ç§å¤æ‚æ€§ä¹Ÿå¢åŠ äº†å…¶è„†å¼±æ€§ï¼Œä½¿å…¶æ›´å®¹æ˜“å‡ºç°ç³»ç»Ÿæ•…éšœã€‚ç¡®å®šé•¿æ‰§è¡Œè½¨è¿¹ä¸­ç‰¹å®šä»£ç†æˆ–æ­¥éª¤çš„é”™è¯¯è´£ä»»ï¼Œè¿™å®šä¹‰äº†ä»£ç†ç³»ç»Ÿå¤±è´¥å½’å› çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç›®å‰æœ€å…ˆè¿›çš„æ¨ç†LLMå¯¹æ­¤æŒ‘æˆ˜ä»ç„¶æ˜æ˜¾ä¸è¶³ï¼Œå‡†ç¡®ç‡é€šå¸¸ä½äº10%ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†AgenTracerï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡åäº‹å®å›æ”¾å’Œç¼–ç¨‹æ•…éšœæ³¨å…¥æ¥æ³¨é‡Šå¤±è´¥çš„å¤šä»£ç†è½¨è¿¹çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œäº§ç”Ÿäº†ç²¾é€‰æ•°æ®é›†TracerTrajã€‚åˆ©ç”¨è¿™ä¸€èµ„æºï¼Œæˆ‘ä»¬å¼€å‘äº†è½»é‡çº§çš„å¤±è´¥è¿½è¸ªå™¨AgenTracer-8Bï¼Œå®ƒé‡‡ç”¨å¤šç²’åº¦å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆè¯Šæ–­å†—é•¿çš„å¤šä»£ç†äº¤äº’ä¸­çš„é”™è¯¯ã€‚åœ¨Who&amp;WhenåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgenTracer-8Bçš„æ€§èƒ½è¶…è¶Šäº†åƒGemini-2.5-Proå’ŒClaude-4-Sonnetç­‰å¤§å‹ä¸“æœ‰LLMï¼Œé«˜å‡ºæœ€å¤šè¾¾18.18%ï¼Œåœ¨LLMä»£ç†å¤±è´¥å½’å› æ–¹é¢æ ‘ç«‹äº†æ–°çš„æ ‡å‡†ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒAgenTracer-8Bä¸ºç°æˆçš„å¤šä»£ç†ç³»ç»Ÿï¼ˆå¦‚MetaGPTå’ŒMaASï¼‰æä¾›äº†å¯æ“ä½œåé¦ˆï¼Œå®ç°äº†4.8%~14.2%çš„æ€§èƒ½æå‡ï¼Œèµ‹èƒ½è‡ªæˆ‘ä¿®æ­£å’Œè‡ªæˆ‘è¿›åŒ–çš„ä»£ç†äººå·¥æ™ºèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03312v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè™½ç„¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶å¤æ‚æ€§ä¹Ÿå¢åŠ äº†ç³»ç»Ÿå¤±è´¥çš„è„†å¼±æ€§ã€‚ç³»ç»Ÿå¤±è´¥å½’å› äºç‰¹å®šæ™ºèƒ½ä½“çš„ä»»åŠ¡æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨ç†LLMå¯¹æ­¤ä»»åŠ¡çš„å‡†ç¡®æ€§æ™®éä½äº10%ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºAgenTraceræ¡†æ¶ï¼Œé€šè¿‡åäº‹å®å›æ”¾å’Œç¼–ç¨‹æ•…éšœæ³¨å…¥æ¥æ ‡æ³¨å¤±è´¥çš„å¤šå…ƒæ™ºèƒ½ä½“è½¨è¿¹ï¼Œå¹¶å¼€å‘äº†åŸºäºæ­¤èµ„æºçš„AgenTracer-8Bè½»é‡çº§å¤±è´¥è¿½è¸ªå™¨ï¼Œä½¿ç”¨å¤šç²’åº¦å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œèƒ½æœ‰æ•ˆè¯Šæ–­å†—é•¿å¤šå…ƒæ™ºèƒ½ä½“äº¤äº’ä¸­çš„é”™è¯¯ã€‚åœ¨Who&amp;WhenåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgenTracer-8Bè¡¨ç°ä¼˜äºå¤§å‹ä¸“æœ‰LLMï¼Œå¦‚Gemini-2.5-Proå’ŒClaude-4-Sonnetï¼Œå‡†ç¡®ç‡æå‡é«˜è¾¾18.18%ï¼Œä¸ºLLMæ™ºèƒ½ä½“å¤±è´¥å½’å› è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½ä¸ºç°æˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¦‚MetaGPTå’ŒMaASæä¾›å¯æ“ä½œåé¦ˆï¼Œæ€§èƒ½æå‡4.8%~14.2%ï¼ŒåŠ©åŠ›æ™ºèƒ½ä½“AIçš„è‡ªæˆ‘ä¿®æ­£å’Œè¿›åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å¤æ‚æ€§å¢åŠ äº†ç³»ç»Ÿå¤±è´¥çš„è„†å¼±æ€§ã€‚</li>
<li>å½“å‰LLMåœ¨æ™ºèƒ½ä½“ç³»ç»Ÿæ•…éšœå½’å› æ–¹é¢çš„å‡†ç¡®æ€§è¾ƒä½ã€‚</li>
<li>AgenTraceræ¡†æ¶é€šè¿‡åäº‹å®å›æ”¾å’Œç¼–ç¨‹æ•…éšœæ³¨å…¥æ¥æ ‡æ³¨å¤±è´¥çš„å¤šå…ƒæ™ºèƒ½ä½“è½¨è¿¹ã€‚</li>
<li>AgenTracer-8Bæ˜¯è½»é‡çº§çš„å¤±è´¥è¿½è¸ªå™¨ï¼Œä½¿ç”¨å¤šç²’åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œèƒ½æœ‰æ•ˆè¯Šæ–­å†—é•¿å¤šå…ƒæ™ºèƒ½ä½“äº¤äº’ä¸­çš„é”™è¯¯ã€‚</li>
<li>AgenTracer-8Båœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–å¤§å‹ä¸“æœ‰LLMï¼Œå‡†ç¡®ç‡æ˜¾è‘—æå‡ã€‚</li>
<li>AgenTracer-8Bèƒ½ä¸ºç°æœ‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›åé¦ˆï¼ŒåŠ©åŠ›å…¶æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67e904c0e4730b1e7304e33fafd8468a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71825900da7db7efc0aba05d495c64e5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Loong-Synthesize-Long-Chain-of-Thoughts-at-Scale-through-Verifiers"><a href="#Loong-Synthesize-Long-Chain-of-Thoughts-at-Scale-through-Verifiers" class="headerlink" title="Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers"></a>Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers</h2><p><strong>Authors:Xingyue Huang,  Rishabh, Gregor Franke, Ziyi Yang, Jiamu Bai, Weijie Bai, Jinhe Bi, Zifeng Ding, Yiqun Duan, Chengyu Fan, Wendong Fan, Xin Gao, Ruohao Guo, Yuan He, Zhuangzhuang He, Xianglong Hu, Neil Johnson, Bowen Li, Fangru Lin, Siyu Lin, Tong Liu, Yunpu Ma, Hao Shen, Hao Sun, Beibei Wang, Fangyijie Wang, Hao Wang, Haoran Wang, Yang Wang, Yifeng Wang, Zhaowei Wang, Ziyang Wang, Yifan Wu, Zikai Xiao, Chengxing Xie, Fan Yang, Junxiao Yang, Qianshuo Ye, Ziyu Ye, Guangtao Zeng, Yuwen Ebony Zhang, Zeyu Zhang, Zihao Zhu, Bernard Ghanem, Philip Torr, Guohao Li</strong></p>
<p>Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across a diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LoongBench, a curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LoongEnv, a modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LoongBench on a broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct a comprehensive analysis of synthetic data generated by LoongEnv, examining correctness, difficulty, and diversity. Code and documentation are available at <a target="_blank" rel="noopener" href="https://github.com/camel-ai/loong">https://github.com/camel-ai/loong</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥è¡¨æ˜ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å¯ä»¥æ˜¾è‘—æé«˜å…¶æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸï¼Œè¿™äº›é¢†åŸŸçš„æ­£ç¡®ç­”æ¡ˆå¯ä»¥è‡ªåŠ¨è¯„ä¼°ã€‚ç„¶è€Œï¼Œç”±äºé«˜è´¨é‡ã€å¯éªŒè¯çš„æ•°æ®é›†ç¨€ç¼ºä»¥åŠäººå·¥ç›‘ç£æˆæœ¬é«˜ï¼Œå°†è¿™ç§æˆåŠŸæ‰©å±•åˆ°å…¶ä»–æ¨ç†å¯†é›†å‹é¢†åŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é¾™é¡¹ç›®ï¼šä¸€ä¸ªè·¨å¤šç§æ¨ç†å¯†é›†å‹é¢†åŸŸçš„å¯æ‰©å±•åˆæˆæ•°æ®ç”Ÿæˆå’ŒéªŒè¯çš„å¼€æºæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰LoongBenchï¼Œä¸€ä¸ªç²¾é€‰çš„ç§å­æ•°æ®é›†ï¼ŒåŒ…å«12ä¸ªé¢†åŸŸï¼ˆå¦‚é«˜çº§æ•°å­¦ã€åŒ–å­¦ã€é€»è¾‘ï¼‰çš„8729ä¸ªäººå·¥å®¡æ ¸è¿‡çš„ä¾‹å­ï¼Œæ¯ä¸ªä¾‹å­éƒ½é…æœ‰å¯æ‰§è¡Œä»£ç å’Œä¸°å¯Œçš„å…ƒæ•°æ®ï¼›ï¼ˆ2ï¼‰LoongEnvï¼Œä¸€ä¸ªæ¨¡å—åŒ–åˆæˆæ•°æ®ç”Ÿæˆç¯å¢ƒï¼Œæ”¯æŒå¤šç§æç¤ºç­–ç•¥ï¼Œä»¥äº§ç”Ÿæ–°çš„é—®ç­”ä»£ç ä¸‰å…ƒç»„ã€‚è¿™ä¸¤ä¸ªç»„ä»¶å…±åŒå½¢æˆäº†ä¸€ä¸ªä»£ç†ç¯å¢ƒå¾ªç¯ï¼Œä½¿å¼ºåŒ–å­¦ä¹ æˆä¸ºå¯èƒ½ï¼Œå…¶ä¸­åŸºäºLLMçš„ä»£ç†ä¼šå› äº§ç”Ÿä¸ä»£ç æ‰§è¡Œç­”æ¡ˆç›¸ç¬¦çš„Chain-of-Thoughtï¼ˆCoTï¼‰è§£å†³æ–¹æ¡ˆè€Œè·å¾—å¥–åŠ±ã€‚æˆ‘ä»¬å®è¯åœ°å¯¹LoongBenchè¿›è¡Œå¹¿æ³›åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¼€æºå’Œä¸“æœ‰LLMï¼Œä»¥è¯„ä¼°é¢†åŸŸè¦†ç›–å¹¶æ­ç¤ºæ€§èƒ½ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹LoongEnvç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œæ£€æŸ¥äº†å…¶æ­£ç¡®æ€§ã€éš¾åº¦å’Œå¤šæ ·æ€§ã€‚ç›¸å…³ä»£ç å’Œæ–‡æ¡£å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/camel-ai/loong%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/camel-ai/loongæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03059v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Loongé¡¹ç›®ï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆå’ŒéªŒè¯è·¨å¤šä¸ªæ¨ç†å¯†é›†å‹é¢†åŸŸçš„å¤§è§„æ¨¡åˆæˆæ•°æ®çš„å¼€æºæ¡†æ¶ã€‚è¯¥é¡¹ç›®åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šLoongBenchï¼Œä¸€ä¸ªåŒ…å«8729ä¸ªç»è¿‡äººå·¥å®¡æ ¸çš„ç¤ºä¾‹çš„ç²¾é€‰ç§å­æ•°æ®é›†ï¼Œè·¨è¶Š12ä¸ªé¢†åŸŸï¼ˆå¦‚é«˜çº§æ•°å­¦ã€åŒ–å­¦ã€é€»è¾‘ç­‰ï¼‰ï¼Œæ¯ä¸ªç¤ºä¾‹éƒ½é…æœ‰å¯æ‰§è¡Œä»£ç å’Œä¸°å¯Œçš„å…ƒæ•°æ®ï¼›LoongEnvï¼Œä¸€ä¸ªæ¨¡å—åŒ–åˆæˆæ•°æ®ç”Ÿæˆç¯å¢ƒï¼Œæ”¯æŒå¤šç§æç¤ºç­–ç•¥ï¼Œä»¥ç”Ÿæˆæ–°çš„é—®é¢˜-ç­”æ¡ˆ-ä»£ç ä¸‰å…ƒç»„ã€‚è¯¥é¡¹ç›®é€šè¿‡å¼ºåŒ–å­¦ä¹ å¥–åŠ±LLMä»£ç†ç”Ÿæˆä¸ä»£ç æ‰§è¡Œç­”æ¡ˆå¯¹é½çš„Chain-of-Thoughtï¼ˆCoTï¼‰è§£å†³æ–¹æ¡ˆã€‚å®è¯è¡¨æ˜ï¼ŒLoongBenchåœ¨å¹¿æ³›çš„å¼€æºå’Œä¸“æœ‰LLMä¸Šå…·æœ‰ä¼˜ç§€çš„é¢†åŸŸè¦†ç›–æ€§èƒ½ï¼Œå¹¶æ­ç¤ºäº†æ€§èƒ½ç“¶é¢ˆã€‚åŒæ—¶ï¼Œå¯¹LoongEnvç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡Œäº†æ­£ç¡®æ€§ã€éš¾åº¦å’Œå¤šæ ·æ€§çš„ç»¼åˆåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Loongé¡¹ç›®æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå’ŒéªŒè¯è·¨å¤šä¸ªæ¨ç†å¯†é›†å‹é¢†åŸŸçš„åˆæˆæ•°æ®çš„å¼€æºæ¡†æ¶ã€‚</li>
<li>åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šLoongBenchï¼ˆç§å­æ•°æ®é›†ï¼‰å’ŒLoongEnvï¼ˆåˆæˆæ•°æ®ç”Ÿæˆç¯å¢ƒï¼‰ã€‚</li>
<li>LoongBenchæä¾›äº†åŒ…å«ä¸°å¯Œå…ƒæ•°æ®å’Œé…å¯¹ä»£ç çš„é—®é¢˜æ•°æ®é›†ï¼Œé€‚ç”¨äºæ¨ç†å¯†é›†å‹é¢†åŸŸã€‚</li>
<li>LoongEnvèƒ½å¤Ÿç”Ÿæˆæ–°çš„é—®é¢˜-ç­”æ¡ˆ-ä»£ç ä¸‰å…ƒç»„ï¼Œæ”¯æŒå¤šç§æç¤ºç­–ç•¥ã€‚</li>
<li>è¯¥é¡¹ç›®åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¥–åŠ±LLMä»£ç†ç”Ÿæˆä¸ä»£ç æ‰§è¡Œç­”æ¡ˆå¯¹é½çš„Chain-of-Thoughtï¼ˆCoTï¼‰è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒLoongBenchåœ¨å¤šç§LLMä¸Šå…·æœ‰è‰¯å¥½çš„é¢†åŸŸè¦†ç›–æ€§èƒ½ï¼Œå¹¶æ­ç¤ºäº†æ€§èƒ½ç“¶é¢ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-94370632949f95c796ab13fd024f620e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d40c29fa950c90fb00f98ecc784940.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60e654277903802b009df89bcc446303.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations"><a href="#Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations" class="headerlink" title="Jointly Reinforcing Diversity and Quality in Language Model Generations"></a>Jointly Reinforcing Diversity and Quality in Language Model Generations</h2><p><strong>Authors:Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, Tianlu Wang</strong></p>
<p>Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses. </p>
<blockquote>
<p>å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è¿›è¡Œè®­ç»ƒåï¼Œé€šå¸¸ä¼šåœ¨ç‰ºç‰²å¤šæ ·æ€§çš„æƒ…å†µä¸‹ä¼˜å…ˆè€ƒè™‘å‡†ç¡®æ€§å’Œæœ‰ç”¨æ€§ã€‚è¿™äº§ç”Ÿäº†ä¸€ç§ç´§å¼ å…³ç³»ï¼šè™½ç„¶è®­ç»ƒåå¯ä»¥æé«˜å“åº”è´¨é‡ï¼Œä½†å®ƒä¹ŸåŠ å‰§äº†è¾“å‡ºåˆ†å¸ƒï¼Œå‡å°‘äº†æƒ³æ³•çš„èŒƒå›´ï¼Œé™åˆ¶äº†è¯­è¨€æ¨¡å‹åœ¨åˆ›æ„å’Œæ¢ç´¢æ€§ä»»åŠ¡ï¼ˆå¦‚å¤´è„‘é£æš´ã€è®²æ•…äº‹æˆ–è§£å†³é—®é¢˜ï¼‰ä¸­çš„ç”¨å¤„ã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨å¤šæ ·æ€§æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆDARLINGï¼‰æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒæ—¶ä¼˜åŒ–å“åº”è´¨é‡å’Œè¯­ä¹‰å¤šæ ·æ€§çš„æ¡†æ¶ã€‚DARLINGçš„æ ¸å¿ƒæ˜¯å¼•å…¥ä¸€ä¸ªå­¦ä¹ åˆ°çš„åˆ†åŒºå‡½æ•°æ¥åº¦é‡è¶…è¶Šè¡¨é¢å±‚æ¬¡è¯æ±‡å˜åŒ–çš„å¤šæ ·æ€§ã€‚ç„¶åå°†è¿™ç§å¤šæ ·æ€§ä¿¡å·ä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„è´¨é‡å¥–åŠ±ç›¸ç»“åˆï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ—¢é«˜è´¨é‡åˆç‹¬ç‰¹çš„è¾“å‡ºã€‚åœ¨å¤šæ¨¡å‹å®¶æ—å’Œè§„æ¨¡çš„å®éªŒä¸­å‘ç°ï¼ŒDARLINGå¯ä»¥åº”ç”¨äºä¸¤ç§æ¨¡å¼ï¼šä¸å¯éªŒè¯çš„ä»»åŠ¡ï¼ˆæŒ‡ä»¤éµå¾ªå’Œåˆ›é€ æ€§å†™ä½œï¼‰å’Œå¯éªŒè¯çš„ä»»åŠ¡ï¼ˆç«èµ›æ•°å­¦ï¼‰ã€‚åœ¨ç¬¬ä¸€ç§æ¨¡å¼çš„äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDARLINGå§‹ç»ˆä¼˜äºä»…æ³¨é‡è´¨é‡çš„å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œäº§ç”Ÿå‡ºåŒæ—¶é«˜è´¨é‡å’Œæ–°é¢–çš„è¾“å‡ºç‰ˆæœ¬ã€‚åœ¨ç¬¬äºŒç§æ¨¡å¼ä¸‹ï¼ŒDARLINGåœ¨pass@1ï¼ˆè§£å†³æ–¹æ¡ˆè´¨é‡ï¼‰å’Œpass@kï¼ˆè§£å†³æ–¹æ¡ˆå¤šæ ·æ€§ï¼‰æ–¹é¢å–å¾—äº†æ›´é«˜çš„æˆç»©ã€‚æœ€æ˜æ˜¾çš„æ˜¯ï¼Œå¯¹å¤šæ ·æ€§çš„æ˜ç¡®ä¼˜åŒ–ä¿ƒè¿›äº†åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢ï¼Œè¡¨ç°ä¸ºæ›´é«˜è´¨é‡çš„å“åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02534v1">PDF</a> 29 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„å‡†ç¡®æ€§ã€æœ‰ç”¨æ€§å’Œå¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åä¸ºDARLINGçš„å¤šæ ·æ€§æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯è”åˆä¼˜åŒ–å“åº”è´¨é‡å’Œè¯­ä¹‰å¤šæ ·æ€§ã€‚DARLINGé€šè¿‡å¼•å…¥ä¸€ä¸ªç”¨äºè¡¡é‡å¤šæ ·æ€§çš„å­¦ä¹ åˆ†å‰²å‡½æ•°æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥å¤šæ ·æ€§ä¿¡å·ä¸å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­çš„è´¨é‡å¥–åŠ±ç›¸ç»“åˆï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ—¢é«˜è´¨é‡åˆç‹¬ç‰¹æ€§çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDARLINGåœ¨å¤šä¸ªæ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œæ—¢é€‚ç”¨äºééªŒè¯ä»»åŠ¡ï¼ˆå¦‚æŒ‡ä»¤éµå¾ªå’Œåˆ›é€ æ€§å†™ä½œï¼‰ï¼Œä¹Ÿé€‚ç”¨äºéªŒè¯ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦ç«èµ›ï¼‰ï¼Œä¸”èƒ½åŒæ—¶æé«˜è¾“å‡ºè´¨é‡å’Œæ–°é¢–æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ—¶é¢ä¸´å‡†ç¡®æ€§ã€æœ‰ç”¨æ€§å’Œå¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>DARLINGæ¡†æ¶æ—¨åœ¨è”åˆä¼˜åŒ–å“åº”è´¨é‡å’Œè¯­ä¹‰å¤šæ ·æ€§ã€‚</li>
<li>DARLINGå¼•å…¥å­¦ä¹ åˆ†å‰²å‡½æ•°æ¥è¡¡é‡å¤šæ ·æ€§ï¼Œå¹¶ä¸ä»…ä»…å…³æ³¨è¡¨é¢ä¸Šçš„è¯æ±‡å˜åŒ–ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆå¤šæ ·æ€§ä¿¡å·ä¸å¼ºåŒ–å­¦ä¹ ä¸­çš„è´¨é‡å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜è´¨é‡è¾“å‡ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜DARLINGåœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼ŒåŒ…æ‹¬ééªŒè¯ä»»åŠ¡å’ŒéªŒè¯ä»»åŠ¡ã€‚</li>
<li>åœ¨ééªŒè¯ä»»åŠ¡ä¸Šï¼ŒDARLINGèƒ½æ˜¾è‘—æé«˜è¾“å‡ºè´¨é‡å’Œæ–°é¢–æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-61f6a1f1a1e30a92b9ebf249ea646833.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86af92bdd32144328a5c06fcaae20ff5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af01c0aa28bd7160aca29bf3cac99c6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bb631a8b16f4ccfde5ad876fbff7e70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-416d0b948a1163dab1674daa4a76f53e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c1274cb18f186882820c6dfda2271dc.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR"><a href="#Implicit-Actor-Critic-Coupling-via-a-Supervised-Learning-Framework-for-RLVR" class="headerlink" title="Implicit Actor Critic Coupling via a Supervised Learning Framework for   RLVR"></a>Implicit Actor Critic Coupling via a Supervised Learning Framework for   RLVR</h2><p><strong>Authors:Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang</strong></p>
<p>Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor $\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at <a target="_blank" rel="noopener" href="https://github.com/ritzz-ai/PACS">https://github.com/ritzz-ai/PACS</a>. </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„è¿›æ­¥ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåº”å¯¹æ•°å­¦å’Œç¼–ç¨‹ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡ã€‚RLVRåˆ©ç”¨å¯éªŒè¯çš„ç»“æœå¥–åŠ±æ¥æŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿LLMsèƒ½å¤Ÿä»¥è¸å®å¯é çš„æ–¹å¼é€æ­¥æ”¹è¿›è¾“å‡ºè´¨é‡ã€‚å°½ç®¡å…·æœ‰æ½œåŠ›ï¼Œä½†RLVRèŒƒå¼ä¹Ÿå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•å¸¸å¸¸å—åˆ°å¥–åŠ±ä¿¡å·ç¨€ç–å’Œç­–ç•¥æ¢¯åº¦æ›´æ–°ä¸ç¨³å®šçš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºRLçš„æ–¹æ³•ä¸­ã€‚</p>
</blockquote>
<p>ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PACSè¿™ä¸€æ–°å‹RLVRæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç›‘ç£å­¦ä¹ æ¡†æ¶å®ç°äº†éšå¼æ¼”å‘˜è¯„è®ºå®¶è€¦åˆã€‚æˆ‘ä»¬å°†ç»“æœå¥–åŠ±è§†ä¸ºå¯é¢„æµ‹çš„æ ‡ç­¾ï¼Œå°†RLVRé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºåœ¨ç­–ç•¥æ¨¡å‹å‚æ•°åŒ–çš„è¯„åˆ†å‡½æ•°ä¸Šçš„ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚è¯¦ç»†çš„æ¢¯åº¦åˆ†æè¡¨æ˜ï¼Œè¿™ç§ç›‘ç£å…¬å¼åŒ–æœ¬è´¨ä¸Šæ¢å¤äº†ç»å…¸çš„æ”¿ç­–æ¢¯åº¦æ›´æ–°ï¼ŒåŒæ—¶éšå¼åœ°è€¦åˆäº†æ¼”å‘˜å’Œè¯„è®ºå®¶çš„è§’è‰²ï¼Œä»è€Œå®ç°äº†æ›´ç¨³å®šã€æ›´æœ‰æ•ˆçš„è®­ç»ƒã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02522v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é¢†åŸŸçš„è¿›å±•ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåº”å¯¹æ•°å­¦å’Œç¼–ç¨‹ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡ã€‚RLVRåˆ©ç”¨å¯éªŒè¯çš„ç»“æœå¥–åŠ±æ¥æŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿LLMèƒ½å¤Ÿåœ¨æœ‰æ ¹æ®å’Œå¯é çš„æ–¹å¼ä¸‹é€æ­¥æ”¹è¿›è¾“å‡ºè´¨é‡ã€‚é’ˆå¯¹RLVRé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ç¨€ç–å¥–åŠ±ä¿¡å·å’Œä¸ç¨³å®šç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„RLVRæ¡†æ¶PACSï¼Œå®ƒé€šè¿‡ç›‘ç£å­¦ä¹ æ¡†æ¶å®ç°äº†éšå¼è¡ŒåŠ¨è€…è¯„è®ºå®¶è€¦åˆã€‚å°†ç»“æœå¥–åŠ±è§†ä¸ºå¯é¢„æµ‹çš„æ ‡ç­¾ï¼Œå°†RLVRé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºç­–ç•¥æ¨¡å‹å‚æ•°åŒ–çš„è¯„åˆ†å‡½æ•°çš„ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚PACSåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†å¼ºå¤§çš„RLVRåŸºçº¿ï¼Œå¦‚PPOå’ŒGRPOï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨AIME 2025ä¸Šï¼ŒPACSåœ¨pass@256è¾¾åˆ°äº†59.78%ï¼Œç›¸è¾ƒäºPPOå’ŒGRPOåˆ†åˆ«æé«˜äº†13.32å’Œ14.36ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™ä¸ªç®€å•è€Œå¼ºå¤§çš„æ¡†æ¶ä¸ºLLMçš„å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„åè®­ç»ƒæä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRæŠ€æœ¯ä½¿LLMsèƒ½å¤Ÿå¤„ç†æ•°å­¦å’Œç¼–ç¨‹ç­‰å¤æ‚æ¨ç†ä»»åŠ¡ã€‚</li>
<li>RLVRåˆ©ç”¨å¯éªŒè¯çš„ç»“æœå¥–åŠ±æ¥æŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œæé«˜è¾“å‡ºè´¨é‡ã€‚</li>
<li>ç°æœ‰RLVRæ–¹æ³•é¢ä¸´ç¨€ç–å¥–åŠ±ä¿¡å·å’Œä¸ç¨³å®šç­–ç•¥æ¢¯åº¦æ›´æ–°ç­‰æŒ‘æˆ˜ã€‚</li>
<li>PACSæ˜¯ä¸€ç§æ–°å‹çš„RLVRæ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ å®ç°äº†è¡ŒåŠ¨è€…ä¸è¯„è®ºå®¶çš„éšå¼è€¦åˆã€‚</li>
<li>PACSå°†RLVRé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>PACSåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†PPOå’ŒGRPOç­‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd7f70365959ac006b18c7fb6476f3f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bcd5868d9b62efcfd391c59f8aec16f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ac4b450b3b07618939266f8a0994f69.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-4db78834eecb646e4bf81ec3d82f77ec.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Delta Activations A Representation for Finetuned Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0daa51ebdd570f809ddf18c803552883.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Think2Sing Orchestrating Structured Motion Subtitles for Singing-Driven   3D Head Animation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
