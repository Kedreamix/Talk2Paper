<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Think2Sing Orchestrating Structured Motion Subtitles for Singing-Driven   3D Head Animation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0daa51ebdd570f809ddf18c803552883.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    56 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Think2Sing-Orchestrating-Structured-Motion-Subtitles-for-Singing-Driven-3D-Head-Animation"><a href="#Think2Sing-Orchestrating-Structured-Motion-Subtitles-for-Singing-Driven-3D-Head-Animation" class="headerlink" title="Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven   3D Head Animation"></a>Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven   3D Head Animation</h2><p><strong>Authors:Zikai Huang, Yihan Zhou, Xuemiao Xu, Cheng Xu, Xiaofen Xing, Jing Qin, Shengfeng He</strong></p>
<p>Singing-driven 3D head animation is a challenging yet promising task with applications in virtual avatars, entertainment, and education. Unlike speech, singing involves richer emotional nuance, dynamic prosody, and lyric-based semantics, requiring the synthesis of fine-grained, temporally coherent facial motion. Existing speech-driven approaches often produce oversimplified, emotionally flat, and semantically inconsistent results, which are insufficient for singing animation. To address this, we propose Think2Sing, a diffusion-based framework that leverages pretrained large language models to generate semantically coherent and temporally consistent 3D head animations, conditioned on both lyrics and acoustics. A key innovation is the introduction of motion subtitles, an auxiliary semantic representation derived through a novel Singing Chain-of-Thought reasoning process combined with acoustic-guided retrieval. These subtitles contain precise timestamps and region-specific motion descriptions, serving as interpretable motion priors. We frame the task as a motion intensity prediction problem, enabling finer control over facial regions and improving the modeling of expressive motion. To support this, we create a multimodal singing dataset with synchronized video, acoustic descriptors, and motion subtitles, enabling diverse and expressive motion learning. Extensive experiments show that Think2Sing outperforms state-of-the-art methods in realism, expressiveness, and emotional fidelity, while also offering flexible, user-controllable animation editing. </p>
<blockquote>
<p>å”±æ­Œé©±åŠ¨çš„3Då¤´éƒ¨åŠ¨ç”»æ˜¯ä¸€é¡¹å……æ»¡æŒ‘æˆ˜ä½†å‰æ™¯å…‰æ˜çš„ä»»åŠ¡ï¼Œåœ¨è™šæ‹Ÿè§’è‰²ã€å¨±ä¹å’Œæ•™è‚²ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ä¸åŒäºè¯­éŸ³ï¼Œå”±æ­ŒåŒ…å«äº†æ›´ä¸°å¯Œçš„æƒ…æ„Ÿç»†å¾®å·®åˆ«ã€åŠ¨æ€éŸµå¾‹å’ŒåŸºäºæ­Œè¯çš„è¯­ä¹‰ï¼Œè¦æ±‚åˆæˆç²¾ç»†ã€æ—¶é—´è¿è´¯çš„é¢éƒ¨è¿åŠ¨ã€‚ç°æœ‰çš„è¯­éŸ³é©±åŠ¨æ–¹æ³•å¾€å¾€äº§ç”Ÿè¿‡äºç®€åŒ–ã€æƒ…æ„Ÿå¹³æ·¡ã€è¯­ä¹‰ä¸ä¸€è‡´çš„ç»“æœï¼Œå¯¹äºå”±æ­ŒåŠ¨ç”»æ¥è¯´è¿œè¿œä¸å¤Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Think2Singï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆè¯­ä¹‰ä¸€è‡´ã€æ—¶é—´è¿è´¯çš„3Då¤´éƒ¨åŠ¨ç”»ï¼Œæ ¹æ®æ­Œè¯å’Œå£°å­¦æ¡ä»¶è¿›è¡Œé©±åŠ¨ã€‚ä¸€ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†è¿åŠ¨å­—å¹•ï¼Œè¿™æ˜¯ä¸€ç§è¾…åŠ©è¯­ä¹‰è¡¨ç¤ºï¼Œé€šè¿‡æ–°é¢–çš„æ­Œå£°æ€ç»´é“¾æ¨ç†è¿‡ç¨‹ä¸å£°å­¦å¼•å¯¼æ£€ç´¢ç›¸ç»“åˆå¾—å‡ºã€‚è¿™äº›å­—å¹•åŒ…å«ç²¾ç¡®çš„æ—¶é—´æˆ³å’ŒåŒºåŸŸç‰¹å®šçš„è¿åŠ¨æè¿°ï¼Œä½œä¸ºå¯è§£é‡Šçš„è¿åŠ¨å…ˆéªŒã€‚æˆ‘ä»¬å°†ä»»åŠ¡æ¡†æ¶è®¾å®šä¸ºè¿åŠ¨å¼ºåº¦é¢„æµ‹é—®é¢˜ï¼Œå®ç°å¯¹é¢éƒ¨åŒºåŸŸçš„æ›´ç²¾ç»†æ§åˆ¶ï¼Œå¹¶æ”¹è¿›äº†è¡¨è¾¾æ€§è¿åŠ¨çš„å»ºæ¨¡ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€æ­Œå”±æ•°æ®é›†ï¼ŒåŒ…å«åŒæ­¥è§†é¢‘ã€å£°å­¦æè¿°ç¬¦å’Œè¿åŠ¨å­—å¹•ï¼Œå®ç°äº†å¤šæ ·åŒ–å’Œè¡¨è¾¾æ€§çš„è¿åŠ¨å­¦ä¹ ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒThink2Singåœ¨çœŸå®æ€§ã€è¡¨è¾¾åŠ›å’Œæƒ…æ„Ÿä¿çœŸåº¦æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶æä¾›äº†çµæ´»ã€ç”¨æˆ·å¯æ§åˆ¶çš„åŠ¨ç”»ç¼–è¾‘åŠŸèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02278v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å”±æ­Œé©±åŠ¨çš„3Då¤´éƒ¨åŠ¨ç”»æŠ€æœ¯ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨è™šæ‹Ÿè§’è‰²ã€å¨±ä¹å’Œæ•™è‚²ç­‰é¢†åŸŸçš„åº”ç”¨å‰æ™¯ã€‚ç°æœ‰è¯­éŸ³é©±åŠ¨çš„æ–¹æ³•åœ¨å¤„ç†å”±æ­ŒåŠ¨ç”»æ—¶å­˜åœ¨ç®€åŒ–ã€æƒ…æ„Ÿå¹³æ·¡å’Œè¯­ä¹‰ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶Think2Singï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ ¹æ®æ­Œè¯å’Œå£°å­¦æ¡ä»¶ç”Ÿæˆè¯­ä¹‰è¿è´¯ã€æ—¶é—´ä¸€è‡´çš„3Då¤´éƒ¨åŠ¨ç”»ã€‚å…³é”®åˆ›æ–°ç‚¹åŒ…æ‹¬è¿åŠ¨å­—å¹•çš„å¼•å…¥ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ–°é¢–çš„å”±æ­Œæ€ç»´é“¾è¿‡ç¨‹ç»“åˆå£°å­¦å¼•å¯¼æ£€ç´¢å¾—åˆ°çš„è¾…åŠ©è¯­ä¹‰è¡¨ç¤ºã€‚è¿åŠ¨å­—å¹•åŒ…å«ç²¾ç¡®çš„æ—¶é—´æˆ³å’ŒåŒºåŸŸç‰¹å®šçš„è¿åŠ¨æè¿°ï¼Œä½œä¸ºå¯è§£é‡Šçš„è¿åŠ¨å…ˆéªŒã€‚æ–‡ç« å°†ä»»åŠ¡æ¡†æ¶å®šä¹‰ä¸ºè¿åŠ¨å¼ºåº¦é¢„æµ‹é—®é¢˜ï¼Œå®ç°å¯¹é¢éƒ¨åŒºåŸŸçš„ç²¾ç»†æ§åˆ¶ï¼Œå¹¶æ”¹è¿›äº†è¡¨è¾¾æ€§è¿åŠ¨çš„å»ºæ¨¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å”±æ­Œé©±åŠ¨çš„3Då¤´éƒ¨åŠ¨ç”»åœ¨è™šæ‹Ÿè§’è‰²ã€å¨±ä¹å’Œæ•™è‚²é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç°æœ‰è¯­éŸ³é©±åŠ¨æ–¹æ³•åœ¨å”±æ­ŒåŠ¨ç”»å¤„ç†ä¸Šè¡¨ç°ä¸è¶³ï¼Œå­˜åœ¨ç®€åŒ–ã€æƒ…æ„Ÿå¹³æ·¡å’Œè¯­ä¹‰ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>Think2Singæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ ¹æ®æ­Œè¯å’Œå£°å­¦æ¡ä»¶ç”Ÿæˆæ›´é€¼çœŸçš„3Då¤´éƒ¨åŠ¨ç”»ã€‚</li>
<li>å¼•å…¥è¿åŠ¨å­—å¹•ä½œä¸ºè¾…åŠ©è¯­ä¹‰è¡¨ç¤ºï¼Œç»“åˆæ–°é¢–çš„å”±æ­Œæ€ç»´é“¾è¿‡ç¨‹å’Œå£°å­¦å¼•å¯¼æ£€ç´¢å¾—åˆ°ã€‚</li>
<li>è¿åŠ¨å­—å¹•åŒ…å«ç²¾ç¡®æ—¶é—´æˆ³å’ŒåŒºåŸŸç‰¹å®šè¿åŠ¨æè¿°ï¼Œä½œä¸ºå¯è§£é‡Šçš„è¿åŠ¨å…ˆéªŒã€‚</li>
<li>å°†ä»»åŠ¡æ¡†æ¶å®šä¹‰ä¸ºè¿åŠ¨å¼ºåº¦é¢„æµ‹é—®é¢˜ï¼Œå®ç°å¯¹é¢éƒ¨åŒºåŸŸçš„ç²¾ç»†æ§åˆ¶ï¼Œæ”¹è¿›è¡¨è¾¾æ€§è¿åŠ¨çš„å»ºæ¨¡ã€‚</li>
<li>åˆ›å»ºäº†å¤šæ¨¡æ€å”±æ­Œæ•°æ®é›†ï¼Œæ”¯æŒå¤šæ ·åŒ–å’Œè¡¨è¾¾æ€§çš„è¿åŠ¨å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1cbc09108ff1faf1bf33005cb7a9f678.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0daa51ebdd570f809ddf18c803552883.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40e74a43898a384e56a659069188761a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c623353bf5ceb6516d9e3100f066ddd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Talk-Less-Call-Right-Enhancing-Role-Play-LLM-Agents-with-Automatic-Prompt-Optimization-and-Role-Prompting"><a href="#Talk-Less-Call-Right-Enhancing-Role-Play-LLM-Agents-with-Automatic-Prompt-Optimization-and-Role-Prompting" class="headerlink" title="Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic   Prompt Optimization and Role Prompting"></a>Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic   Prompt Optimization and Role Prompting</h2><p><strong>Authors:Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul</strong></p>
<p>This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) human-crafted role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniquesâ€“character-card&#x2F;scene-contract design and strict enforcement of function callingâ€“which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/scb-10x/apo">https://github.com/scb-10x/apo</a>. </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šè°ƒæŸ¥äº†åœ¨Commonsense Persona-grounded Dialogue Challengeï¼ˆCPDCï¼‰2025çš„APIèµ›é“ä¸­ï¼Œå¦‚ä½•æç¤ºå·¥å…·å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰®æ¼”è§’è‰²å¯¹è¯ä»£ç†çš„æ–¹æ³•ã€‚åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œå¯¹è¯ä»£ç†å¾€å¾€äº§ç”Ÿè¿‡é•¿çš„è§’è‰²å†…å“åº”ï¼ˆè¯´å¾—å¤ªå¤šï¼‰ï¼ŒåŒæ—¶æœªèƒ½æ ¹æ®è§’è‰²æœ‰æ•ˆåœ°ä½¿ç”¨å·¥å…·ï¼ˆè¡¨ç°ä¸è¶³ï¼‰ï¼Œä¾‹å¦‚ç”Ÿæˆä¸å­˜åœ¨çš„å‡½æ•°è°ƒç”¨æˆ–åœ¨å›ç­”é—®é¢˜ä¹‹å‰è¿›è¡Œä¸å¿…è¦çš„å·¥å…·è°ƒç”¨ã€‚æˆ‘ä»¬æ¢ç´¢äº†å››ç§æç¤ºæ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼š1ï¼‰åŸºæœ¬è§’è‰²æç¤ºï¼Œ2ï¼‰äººå·¥åˆ¶ä½œçš„è§’è‰²æç¤ºï¼Œ3ï¼‰è‡ªåŠ¨æç¤ºä¼˜åŒ–ï¼ˆAPOï¼‰ï¼Œä»¥åŠ4ï¼‰åŸºäºè§„åˆ™çš„è§’è‰²æç¤ºã€‚åŸºäºè§„åˆ™çš„è§’è‰²æç¤ºï¼ˆRRPï¼‰æ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œå®ƒé€šè¿‡ä¸¤ç§æ–°æŠ€æœ¯â€”â€”è§’è‰²å¡&#x2F;åœºæ™¯åˆçº¦è®¾è®¡å’Œä¸¥æ ¼çš„å‡½æ•°è°ƒç”¨æ‰§è¡Œâ€”â€”å®ç°äº†æ•´ä½“å¾—åˆ†0.571ï¼Œæé«˜äº†é›¶æ ·æœ¬åŸºçº¿å¾—åˆ†0.519ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œä¸æ›´ç²¾ç»†çš„æ–¹æ³•ï¼ˆå¦‚APOï¼‰ç›¸æ¯”ï¼ŒRRPè®¾è®¡å¯ä»¥æ˜¾è‘—æé«˜è§’è‰²æ‰®æ¼”å¯¹è¯ä»£ç†çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚ä¸ºäº†æ”¯æŒæœªæ¥åœ¨å¼€å‘ä¸ªæ€§åŒ–æç¤ºæ–¹é¢çš„åŠªåŠ›ï¼Œæˆ‘ä»¬å…¬å¼€äº†æ‰€æœ‰è¡¨ç°æœ€ä½³çš„æç¤ºå’ŒAPOå·¥å…·ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/scb-10x/apo">https://github.com/scb-10x/apo</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00482v1">PDF</a> 17 pages, 2 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æŠ¥å‘Šæ¢è®¨äº†å¦‚ä½•å¼•å¯¼å·¥å…·å¢å¼ºå‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨Commonsense Persona-grounded Dialogue Challengeï¼ˆCPDCï¼‰çš„APIèµ›é“ä¸­æ‰®æ¼”è§’è‰²å¯¹è¯ä»£ç†çš„æ–¹æ³•ã€‚åœ¨æ­¤åœºæ™¯ä¸­ï¼Œå¯¹è¯ä»£ç†å¸¸å¸¸äº§ç”Ÿè¿‡é•¿çš„è§’è‰²å†…å“åº”ï¼ˆè¯´è¯è¿‡å¤šï¼‰ï¼ŒåŒæ—¶æœªèƒ½æ ¹æ®è§’è‰²æœ‰æ•ˆåœ°ä½¿ç”¨å·¥å…·ï¼ˆè¡¨ç°ä¸è¶³ï¼‰ï¼Œä¾‹å¦‚ç”Ÿæˆä¸å­˜åœ¨çš„å‡½æ•°è°ƒç”¨æˆ–åœ¨å›ç­”ä¹‹å‰è¿›è¡Œä¸å¿…è¦çš„å·¥å…·è°ƒç”¨ã€‚æœ¬æ–‡æ¢ç´¢äº†å››ç§æç¤ºæ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼šåŸºæœ¬è§’è‰²æç¤ºã€äººå·¥æ„å»ºè§’è‰²æç¤ºã€è‡ªåŠ¨æç¤ºä¼˜åŒ–ï¼ˆAPOï¼‰å’ŒåŸºäºè§„åˆ™çš„è§’è‰²æç¤ºã€‚é€šè¿‡ä¸¤ç§æ–°æŠ€æœ¯â€”â€”è§’è‰²å¡&#x2F;åœºæ™¯åˆçº¦è®¾è®¡å’Œä¸¥æ ¼çš„åŠŸèƒ½è°ƒç”¨å¼ºåˆ¶æ‰§è¡Œï¼Œè§„åˆ™åŸºç¡€çš„è§’è‰²æç¤ºï¼ˆRRPï¼‰æ–¹æ³•å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæ€»ä½“å¾—åˆ†ä¸º0.571ï¼Œç›¸è¾ƒäºé›¶åŸºå‡†çº¿çš„0.519æœ‰æ‰€æå‡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç›¸è¾ƒäºæ›´ä¸ºå¤æ‚çš„æ–¹æ³•ï¼ˆå¦‚APOï¼‰ï¼ŒRRPè®¾è®¡èƒ½å¤§å¹…å¢å¼ºè§’è‰²æ‰®æ¼”å¯¹è¯ä»£ç†çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚ä¸ºäº†æ”¯æŒæœªæ¥åœ¨å¼€å‘ä¸ªæ€§æç¤ºæ–¹é¢çš„åŠªåŠ›ï¼Œæˆ‘ä»¬å…¬å¼€äº†æ‰€æœ‰è¡¨ç°æœ€ä½³çš„æç¤ºå’ŒAPOå·¥å…·ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/scb-10x/apo">https://github.com/scb-10x/apo</a>è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æŠ¥å‘Šæ¢è®¨äº†å¦‚ä½•å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šå¯¹è¯æŒ‘æˆ˜ä¸­æ‰®æ¼”è§’è‰²å¯¹è¯ä»£ç†çš„é—®é¢˜ã€‚</li>
<li>å¯¹è¯ä»£ç†åœ¨é¢å¯¹ä»»åŠ¡æ—¶å¸¸å¸¸å‡ºç°è¿‡åº¦å“åº”å’Œè¡¨ç°ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>å››ç§æç¤ºæ–¹æ³•è¢«ç”¨äºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå…¶ä¸­è§„åˆ™åŸºç¡€çš„è§’è‰²æç¤ºæ–¹æ³•è¡¨ç°æœ€ä½³ã€‚</li>
<li>é€šè¿‡è§’è‰²å¡å’Œåœºæ™¯åˆçº¦è®¾è®¡ä»¥åŠä¸¥æ ¼çš„åŠŸèƒ½è°ƒç”¨å¼ºåˆ¶æ‰§è¡Œï¼Œè§„åˆ™åŸºç¡€çš„è§’è‰²æç¤ºæ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•ç›¸è¾ƒäºè‡ªåŠ¨æç¤ºä¼˜åŒ–ç­‰æ›´å¤æ‚çš„æ–¹æ³•æ›´ä¸ºæœ‰æ•ˆå’Œå¯é ã€‚</li>
<li>æŠ¥å‘Šå…¬å¼€äº†æ‰€æœ‰è¡¨ç°æœ€ä½³çš„æç¤ºå’Œè‡ªåŠ¨æç¤ºä¼˜åŒ–å·¥å…·ä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-48b3c006cbd0496743433445ad631236.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2650c7ffcb1580d7faaca6f9aadc05f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f89eda6d2e0f5994b39f1a80d332c919.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7e667c7fcca4e021a4d89f87813c648.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EmoCAST-Emotional-Talking-Portrait-via-Emotive-Text-Description"><a href="#EmoCAST-Emotional-Talking-Portrait-via-Emotive-Text-Description" class="headerlink" title="EmoCAST: Emotional Talking Portrait via Emotive Text Description"></a>EmoCAST: Emotional Talking Portrait via Emotive Text Description</h2><p><strong>Authors:Yiguo Jiang, Xiaodong Cun, Yong Zhang, Yudian Zheng, Fan Tang, Chi-Man Pun</strong></p>
<p>Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are primarily collected in lab settings, further exacerbating these shortcomings. Consequently, these limitations substantially hinder practical applications in real-world scenarios. To address these challenges, we propose EmoCAST, a diffusion-based framework with two key modules for precise text-driven emotional synthesis. In appearance modeling, emotional prompts are integrated through a text-guided decoupled emotive module, enhancing the spatial knowledge to improve emotion comprehension. To improve the relationship between audio and emotion, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide more precise facial motion synthesis. Additionally, we construct an emotional talking head dataset with comprehensive emotive text descriptions to optimize the frameworkâ€™s performance. Based on the proposed dataset, we propose an emotion-aware sampling training strategy and a progressive functional training strategy that further improve the modelâ€™s ability to capture nuanced expressive features and achieve accurate lip-synchronization. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: <a target="_blank" rel="noopener" href="https://github.com/GVCLab/EmoCAST">https://github.com/GVCLab/EmoCAST</a> </p>
<blockquote>
<p>æƒ…ç»ªåŒ–è¯´è¯äººå¤´éƒ¨åˆæˆæ—¨åœ¨ç”Ÿæˆå…·æœ‰ç”ŸåŠ¨è¡¨æƒ…çš„è‚–åƒè§†é¢‘ã€‚ç°æœ‰æ–¹æ³•ä»å­˜åœ¨äºæ§åˆ¶çµæ´»æ€§ã€åŠ¨ä½œè‡ªç„¶æ€§å’Œè¡¨æƒ…è´¨é‡æ–¹é¢çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œç›®å‰å¯ç”¨çš„æ•°æ®é›†ä¸»è¦åœ¨å®éªŒå®¤ç¯å¢ƒä¸­æ”¶é›†ï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†è¿™äº›ä¸è¶³ã€‚å› æ­¤ï¼Œè¿™äº›å±€é™åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„å®ç”¨å—åˆ°äº†å¾ˆå¤§çš„é˜»ç¢ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†EmoCASTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œå…·æœ‰ä¸¤ä¸ªç²¾ç¡®æ–‡æœ¬é©±åŠ¨çš„æƒ…æ„Ÿåˆæˆçš„å…³é”®æ¨¡å—ã€‚åœ¨å¤–è§‚å»ºæ¨¡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ–‡æœ¬æŒ‡å¯¼çš„è§£è€¦æƒ…æ„Ÿæ¨¡å—æ•´åˆæƒ…æ„Ÿæç¤ºï¼Œå¢å¼ºç©ºé—´çŸ¥è¯†ä»¥æé«˜æƒ…æ„Ÿç†è§£ã€‚ä¸ºäº†æ”¹å–„éŸ³é¢‘å’Œæƒ…æ„Ÿä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæƒ…æ„ŸéŸ³é¢‘æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥æ•æ‰å—æ§æƒ…æ„Ÿä¸é©±åŠ¨éŸ³é¢‘ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œç”Ÿæˆæƒ…æ„Ÿæ„ŸçŸ¥ç‰¹å¾ï¼Œä»¥å¼•å¯¼æ›´ç²¾ç¡®çš„é¢éƒ¨åŠ¨ä½œåˆæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæƒ…æ„Ÿè¯´è¯äººå¤´éƒ¨æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å…¨é¢çš„æƒ…æ„Ÿæ–‡æœ¬æè¿°ï¼Œä»¥ä¼˜åŒ–æ¡†æ¶çš„æ€§èƒ½ã€‚åŸºäºæ‰€æå‡ºçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æƒ…æ„Ÿæ„ŸçŸ¥é‡‡æ ·è®­ç»ƒç­–ç•¥å’Œä¸€ç§æ¸è¿›åŠŸèƒ½è®­ç»ƒç­–ç•¥ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹æ•æ‰ç»†å¾®è¡¨æƒ…ç‰¹å¾çš„èƒ½åŠ›ï¼Œå®ç°å‡†ç¡®çš„å”‡åŒæ­¥ã€‚æ€»ä½“è€Œè¨€ï¼ŒEmoCASTåœ¨ç”Ÿæˆé€¼çœŸã€æƒ…æ„Ÿä¸°å¯Œã€éŸ³é¢‘åŒæ­¥çš„è¯´è¯äººå¤´è§†é¢‘æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/GVCLab/EmoCAST">https://github.com/GVCLab/EmoCAST</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20615v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æƒ…æ„Ÿè¯´è¯äººå¤´éƒ¨åˆæˆæ—¨åœ¨ç”Ÿæˆå…·æœ‰ç”ŸåŠ¨è¡¨æƒ…çš„è¯´è¯äººåƒè§†é¢‘ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æ§åˆ¶çµæ´»æ€§ã€åŠ¨ä½œè‡ªç„¶æ€§å’Œè¡¨æƒ…è´¨é‡æ–¹é¢çš„å±€é™æ€§ï¼Œä»¥åŠæ•°æ®é›†ä¸»è¦åœ¨å®éªŒå®¤ç¯å¢ƒä¸‹æ”¶é›†çš„ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†EmoCASTï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œå…·æœ‰ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼Œç”¨äºç²¾ç¡®çš„æ–‡å­—é©±åŠ¨æƒ…æ„Ÿåˆæˆã€‚é€šè¿‡å¤–è§‚å»ºæ¨¡ä¸­çš„æƒ…æ„Ÿæç¤ºé›†æˆï¼Œä»¥åŠéŸ³é¢‘ä¸æƒ…æ„Ÿå…³ç³»çš„å¼ºåŒ–ï¼Œç”Ÿæˆäº†æ›´å…·æƒ…æ„Ÿæ„ŸçŸ¥ç‰¹å¾çš„è„¸éƒ¨è¿åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæƒ…æ„Ÿè¯´è¯äººå¤´æ•°æ®é›†ï¼ŒåŒ…å«å…¨é¢çš„æƒ…æ„Ÿæ–‡æœ¬æè¿°ï¼Œä»¥ä¼˜åŒ–æ¡†æ¶æ€§èƒ½ã€‚æ€»ä½“æ¥è¯´ï¼ŒEmoCASTåœ¨ç”ŸæˆçœŸå®ã€æƒ…æ„Ÿä¸°å¯Œã€éŸ³é¢‘åŒæ­¥çš„è°ˆè¯è§†é¢‘æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æƒ…æ„Ÿè¯´è¯äººå¤´éƒ¨åˆæˆæ—¨åœ¨ç”Ÿæˆå…·æœ‰ç”ŸåŠ¨è¡¨æƒ…çš„è°ˆè¯äººåƒè§†é¢‘ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æ§åˆ¶çµæ´»æ€§ã€åŠ¨ä½œè‡ªç„¶æ€§å’Œè¡¨æƒ…è´¨é‡æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ä¸»è¦åœ¨å®éªŒå®¤ç¯å¢ƒä¸‹æ”¶é›†ï¼ŒåŠ å‰§äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>EmoCASTæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œå…·æœ‰ä¸¤ä¸ªå…³é”®æ¨¡å—ç”¨äºç²¾ç¡®çš„æ–‡å­—é©±åŠ¨æƒ…æ„Ÿåˆæˆã€‚</li>
<li>é€šè¿‡å¤–è§‚å»ºæ¨¡å’Œæƒ…æ„Ÿæç¤ºé›†æˆï¼Œæé«˜äº†ç©ºé—´çŸ¥è¯†ï¼Œæ”¹å–„äº†æƒ…æ„Ÿç†è§£ã€‚</li>
<li>å¼•å…¥æƒ…æ„ŸéŸ³é¢‘æ³¨æ„åŠ›æ¨¡å—ï¼Œæ•æ‰æ§åˆ¶æƒ…æ„Ÿå’Œé©±åŠ¨éŸ³é¢‘ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œç”Ÿæˆæƒ…æ„Ÿæ„ŸçŸ¥ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7d4930128d0d8fae17ab4933ca6b135c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7104d358c5c57beeef356c2ca83d43ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6eb3183d68637daf56c067bfd3173d82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c649d2e353951f67edb18af0ad8adb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58a3382c8492190f67696c063db0f96d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="InfinityHuman-Towards-Long-Term-Audio-Driven-Human"><a href="#InfinityHuman-Towards-Long-Term-Audio-Driven-Human" class="headerlink" title="InfinityHuman: Towards Long-Term Audio-Driven Human"></a>InfinityHuman: Towards Long-Term Audio-Driven Human</h2><p><strong>Authors:Xiaodi Li, Pan Xie, Yi Ren, Qijun Gan, Chen Zhang, Fangyuan Kong, Xiang Yin, Bingyue Peng, Zehuan Yuan</strong></p>
<p>Audio-driven human animation has attracted wide attention thanks to its practical applications. However, critical challenges remain in generating high-resolution, long-duration videos with consistent appearance and natural hand motions. Existing methods extend videos using overlapping motion frames but suffer from error accumulation, leading to identity drift, color shifts, and scene instability. Additionally, hand movements are poorly modeled, resulting in noticeable distortions and misalignment with the audio. In this work, we propose InfinityHuman, a coarse-to-fine framework that first generates audio-synchronized representations, then progressively refines them into high-resolution, long-duration videos using a pose-guided refiner. Since pose sequences are decoupled from appearance and resist temporal degradation, our pose-guided refiner employs stable poses and the initial frame as a visual anchor to reduce drift and improve lip synchronization. Moreover, to enhance semantic accuracy and gesture realism, we introduce a hand-specific reward mechanism trained with high-quality hand motion data. Experiments on the EMTD and HDTF datasets show that InfinityHuman achieves state-of-the-art performance in video quality, identity preservation, hand accuracy, and lip-sync. Ablation studies further confirm the effectiveness of each module. Code will be made public. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„äººç‰©åŠ¨ç”»å› å…¶å®é™…åº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é•¿æ—¶é•¿çš„è§†é¢‘æ—¶ï¼Œä»ç„¶å­˜åœ¨ä¸€è‡´æ€§å¤–è§‚å’Œè‡ªç„¶æ‰‹éƒ¨åŠ¨ä½œæ–¹é¢çš„é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡ä½¿ç”¨é‡å çš„è¿åŠ¨å¸§æ¥æ‰©å±•è§†é¢‘ï¼Œä½†å­˜åœ¨è¯¯å·®ç´¯ç§¯çš„é—®é¢˜ï¼Œä»è€Œå¯¼è‡´èº«ä»½æ¼‚ç§»ã€è‰²å½©åç§»å’Œåœºæ™¯ä¸ç¨³å®šã€‚æ­¤å¤–ï¼Œæ‰‹éƒ¨åŠ¨ä½œå»ºæ¨¡è¾ƒå·®ï¼Œå¯¼è‡´æ˜æ˜¾çš„å¤±çœŸå’Œä¸éŸ³é¢‘çš„ä¸å¯¹é½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†InfinityHumanï¼Œè¿™æ˜¯ä¸€ä¸ªä»ç²—åˆ°ç»†çš„æ¡†æ¶ï¼Œé¦–å…ˆç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨å§¿æ€å¼•å¯¼ç²¾ç‚¼å™¨é€æ­¥å°†å…¶ç²¾ç»†åŒ–ä¸ºé«˜åˆ†è¾¨ç‡ã€é•¿æ—¶é•¿çš„è§†é¢‘ã€‚ç”±äºå§¿æ€åºåˆ—ä¸å¤–è§‚è§£è€¦å¹¶æŠµæŠ—æ—¶é—´é€€åŒ–ï¼Œæˆ‘ä»¬çš„å§¿æ€å¼•å¯¼ç²¾ç‚¼å™¨é‡‡ç”¨ç¨³å®šçš„å§¿æ€å’Œåˆå§‹å¸§ä½œä¸ºè§†è§‰é”šç‚¹ï¼Œä»¥å‡å°‘æ¼‚ç§»å¹¶æ”¹å–„å”‡éƒ¨åŒæ­¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜è¯­ä¹‰å‡†ç¡®æ€§å’Œæ‰‹åŠ¿ç°å®æ„Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ‰‹ç‰¹å®šçš„å¥–åŠ±æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä½¿ç”¨é«˜è´¨é‡çš„æ‰‹éƒ¨è¿åŠ¨æ•°æ®è¿›è¡Œè®­ç»ƒã€‚åœ¨EMTDå’ŒHDTFæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒInfinityHumanåœ¨è§†é¢‘è´¨é‡ã€èº«ä»½ä¿ç•™ã€æ‰‹éƒ¨å‡†ç¡®æ€§å’Œå”‡åŒæ­¥æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å°†å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20210v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://infinityhuman.github.io/">https://infinityhuman.github.io/</a></p>
<p><strong>Summary</strong><br>     éŸ³é¢‘é©±åŠ¨çš„äººè„¸åŠ¨ç”»å› å…¶å®é™…åº”ç”¨è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é•¿æ—¶é•¿è§†é¢‘çš„æŒ‘æˆ˜ï¼Œå¦‚èº«ä»½æ¼‚ç§»ã€è‰²å½©ç§»ä½ã€åœºæ™¯ä¸ç¨³å®šä»¥åŠæ‰‹éƒ¨åŠ¨ä½œå»ºæ¨¡ä¸è‰¯ç­‰é—®é¢˜ã€‚æ­¤å·¥ä½œæå‡ºInfinityHumanï¼Œä¸€ä¸ªç”±ç²—åˆ°ç»†çš„æ¡†æ¶ï¼Œå…ˆç”ŸæˆéŸ³é¢‘åŒæ­¥çš„è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨å§¿æ€å¼•å¯¼çš„ç»†åŒ–å™¨é€æ­¥ç»†åŒ–æˆé«˜åˆ†è¾¨ç‡ã€é•¿æ—¶é•¿çš„è§†é¢‘ã€‚è¯¥ç»†åŒ–å™¨åˆ©ç”¨ç¨³å®šçš„å§¿æ€å’Œåˆå§‹å¸§ä½œä¸ºè§†è§‰é”šç‚¹ï¼Œå‡å°‘æ¼‚ç§»å¹¶æé«˜å”‡éƒ¨åŒæ­¥ã€‚æ­¤å¤–ï¼Œä¸ºæé«˜è¯­ä¹‰å‡†ç¡®æ€§å’Œæ‰‹åŠ¿çœŸå®æ„Ÿï¼Œå¼•å…¥åŸºäºé«˜è´¨é‡æ‰‹éƒ¨è¿åŠ¨æ•°æ®çš„æ‰‹éƒ¨ç‰¹å®šå¥–åŠ±æœºåˆ¶ã€‚åœ¨EMTDå’ŒHDTFæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒInfinityHumanåœ¨è§†é¢‘è´¨é‡ã€èº«ä»½ä¿ç•™ã€æ‰‹éƒ¨å‡†ç¡®æ€§å’Œå”‡éƒ¨åŒæ­¥æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘é©±åŠ¨çš„äººè„¸åŠ¨ç”»å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä½†ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°å­˜æ–¹æ³•é€šè¿‡é‡å è¿åŠ¨å¸§å»¶é•¿è§†é¢‘ï¼Œä½†ä¼šå¯¼è‡´è¯¯å·®ç´¯ç§¯ï¼Œå‡ºç°èº«ä»½æ¼‚ç§»ã€è‰²å½©ç§»ä½å’Œåœºæ™¯ä¸ç¨³å®šç­‰é—®é¢˜ã€‚</li>
<li>InfinityHumanæ¡†æ¶é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„ç”Ÿæˆæ–¹å¼ï¼Œé¦–å…ˆç”ŸæˆéŸ³é¢‘åŒæ­¥çš„è¡¨ç¤ºï¼Œç„¶åé€æ­¥ç»†åŒ–ã€‚</li>
<li>å§¿æ€å¼•å¯¼ç»†åŒ–å™¨åˆ©ç”¨ç¨³å®šå§¿æ€å’Œåˆå§‹å¸§ä½œä¸ºè§†è§‰é”šç‚¹ï¼Œæé«˜è§†é¢‘è´¨é‡ï¼Œå‡å°‘èº«ä»½æ¼‚ç§»ã€‚</li>
<li>å¼•å…¥æ‰‹éƒ¨ç‰¹å®šå¥–åŠ±æœºåˆ¶ï¼ŒåŸºäºé«˜è´¨é‡æ‰‹éƒ¨è¿åŠ¨æ•°æ®ï¼Œæé«˜è¯­ä¹‰å‡†ç¡®æ€§å’Œæ‰‹åŠ¿çœŸå®æ„Ÿã€‚</li>
<li>åœ¨EMTDå’ŒHDTFæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒInfinityHumanåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å°†å…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-43de056a3003ea3b6ae0b7d032b1e51e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-651063cdc4ae788f2912112a59387883.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2f77751a07592a1df88815c785faba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9139af2ecbc0f6c877fc739e2358fa62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5eac5d84d18d577e536bff14cade872d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Wan-S2V-Audio-Driven-Cinematic-Video-Generation"><a href="#Wan-S2V-Audio-Driven-Cinematic-Video-Generation" class="headerlink" title="Wan-S2V: Audio-Driven Cinematic Video Generation"></a>Wan-S2V: Audio-Driven Cinematic Video Generation</h2><p><strong>Authors:Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo</strong></p>
<p>Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„éŸ³é¢‘é©±åŠ¨è§’è‰²åŠ¨ç”»æ–¹æ³•ä¸»è¦åº”ç”¨åœ¨æ¼”è®²å’Œæ­Œå”±åœºæ™¯ä¸­ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ›´å¤æ‚çš„ç”µå½±å’Œç”µè§†åˆ¶ä½œä¸­å¸¸å¸¸è¡¨ç°ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³ç»†å¾®çš„è§’è‰²äº’åŠ¨ã€é€¼çœŸçš„èº«ä½“åŠ¨ä½œå’ŒåŠ¨æ€çš„æ‘„å½±ç­‰é«˜çº§å…ƒç´ çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³é•¿æœŸå­˜åœ¨çš„ç”µå½±çº§è§’è‰²åŠ¨ç”»æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éŸ³é¢‘é©±åŠ¨æ¨¡å‹ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºWan-S2Vï¼ŒåŸºäºWanæ„å»ºã€‚æˆ‘ä»¬çš„æ¨¡å‹ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œåœ¨ç”µå½±è¯­å¢ƒä¸‹å®ç°äº†æ˜¾è‘—å¢å¼ºçš„è¡¨ç°åŠ›å’Œä¿çœŸåº¦ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸é¡¶å°–æ¨¡å‹ï¼ˆå¦‚Hunyuan-Avatarå’ŒOmnihumanï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœä¸€è‡´è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºè¿™äº›ç°æœ‰è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å…¶åœ¨é•¿è§†é¢‘ç”Ÿæˆå’Œç²¾ç¡®è§†é¢‘å”‡åŒæ­¥ç¼–è¾‘ä¸­çš„åº”ç”¨ï¼Œæ¢ç´¢äº†æˆ‘ä»¬çš„æ–¹æ³•çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18621v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å½“å‰ä¸»æµéŸ³é¢‘é©±åŠ¨è§’è‰²åŠ¨ç”»æ–¹æ³•åœ¨è¯­éŸ³å’Œæ­Œå”±åœºæ™¯è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ›´å¤æ‚å½±è§†åˆ¶ä½œä¸­è¡¨ç°æ¬ ä½³ï¼Œéš¾ä»¥æ»¡è¶³å½±è§†çº§è§’è‰²åŠ¨ç”»çš„éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºWan-S2Vçš„éŸ³é¢‘é©±åŠ¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨WanåŸºç¡€ä¸Šæ„å»ºï¼Œå®ç°äº†ç”µå½±çº§è¯­å¢ƒä¸‹æ›´é«˜çš„è¡¨è¾¾åŠ›å’Œé€¼çœŸåº¦ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå°†Wan-S2Vä¸é¡¶å°–æ¨¡å‹å¦‚Hunyuan-Avatarå’ŒOmnihumanè¿›è¡Œå¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†è¯¥æ–¹æ³•åœ¨é•¿è§†é¢‘ç”Ÿæˆå’Œç²¾ç¡®è§†é¢‘å”‡åŒæ­¥ç¼–è¾‘ä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰éŸ³é¢‘é©±åŠ¨è§’è‰²åŠ¨ç”»æ–¹æ³•åœ¨å¤æ‚å½±è§†åˆ¶ä½œä¸­å­˜åœ¨å±€é™ï¼Œéš¾ä»¥æ»¡è¶³ç”µå½±çº§è§’è‰²åŠ¨ç”»çš„éœ€æ±‚ã€‚</li>
<li>Wan-S2Væ¨¡å‹åœ¨WanåŸºç¡€ä¸Šæ„å»ºï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€éš¾é¢˜ã€‚</li>
<li>Wan-S2Væ¨¡å‹åœ¨è¡¨è¾¾åŠ›å’Œé€¼çœŸåº¦ä¸Šè¾ƒç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒWan-S2Væ˜¾è‘—ä¼˜äºå…¶ä»–é¡¶å°–æ¨¡å‹ã€‚</li>
<li>Wan-S2Væ¨¡å‹å¯åº”ç”¨äºé•¿è§†é¢‘ç”Ÿæˆå’Œç²¾ç¡®è§†é¢‘å”‡åŒæ­¥ç¼–è¾‘ç­‰å¤šä¸ªé¢†åŸŸã€‚</li>
<li>è¯¥æ¨¡å‹çš„æå‡ºæ¨åŠ¨äº†éŸ³é¢‘é©±åŠ¨è§’è‰²åŠ¨ç”»é¢†åŸŸçš„å‘å±•ã€‚</li>
<li>Wan-S2Væ¨¡å‹ä¸ºå½±è§†åˆ¶ä½œå¸¦æ¥äº†æ›´é«˜æ•ˆã€æ›´çœŸå®çš„è§’è‰²åŠ¨ç”»è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7173a5c88039d79e874cbf72c4bb3fd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01535dd25f1aa09931c0919d6a3689b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fe972aa946a94674be400b9aa3799cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18917f0d50dedd6bb4253a01b20a4a9e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Talking-to-Robots-A-Practical-Examination-of-Speech-Foundation-Models-for-HRI-Applications"><a href="#Talking-to-Robots-A-Practical-Examination-of-Speech-Foundation-Models-for-HRI-Applications" class="headerlink" title="Talking to Robots: A Practical Examination of Speech Foundation Models   for HRI Applications"></a>Talking to Robots: A Practical Examination of Speech Foundation Models   for HRI Applications</h2><p><strong>Authors:Theresa Pekarek Rosin, Julia Gachot, Henri-Leon Kordt, Matthias Kerzel, Stefan Wermter</strong></p>
<p>Automatic Speech Recognition (ASR) systems in real-world settings need to handle imperfect audio, often degraded by hardware limitations or environmental noise, while accommodating diverse user groups. In human-robot interaction (HRI), these challenges intersect to create a uniquely challenging recognition environment. We evaluate four state-of-the-art ASR systems on eight publicly available datasets that capture six dimensions of difficulty: domain-specific, accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis demonstrates significant variations in performance, hallucination tendencies, and inherent biases, despite similar scores on standard benchmarks. These limitations have serious implications for HRI, where recognition errors can interfere with task performance, user trust, and safety. </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œçš„è®¾ç½®ä¸­ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿéœ€è¦å¤„ç†ä¸å®Œç¾çš„éŸ³é¢‘ï¼Œè¿™äº›éŸ³é¢‘ç»å¸¸å› ç¡¬ä»¶é™åˆ¶æˆ–ç¯å¢ƒå™ªå£°è€Œé€€åŒ–ï¼ŒåŒæ—¶è¿˜è¦é€‚åº”ä¸åŒçš„ç”¨æˆ·ç¾¤ä½“ã€‚åœ¨äººæœºäº¤äº’ï¼ˆHRIï¼‰ä¸­ï¼Œè¿™äº›æŒ‘æˆ˜äº¤ç»‡åœ¨ä¸€èµ·ï¼Œåˆ›é€ äº†ä¸€ä¸ªç‹¬ç‰¹ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯†åˆ«ç¯å¢ƒã€‚æˆ‘ä»¬åœ¨å…«ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¯„ä¼°äº†å››ç§æœ€æ–°å‰æ²¿çš„ASRç³»ç»Ÿï¼Œè¿™äº›æ•°æ®é›†æ•æ‰åˆ°äº†å…­ä¸ªç»´åº¦çš„éš¾åº¦ï¼šç‰¹å®šé¢†åŸŸçš„ã€å¸¦å£éŸ³çš„ã€å˜ˆæ‚çš„ã€å¹´é¾„å˜åŒ–çš„ã€å—æŸçš„ä»¥åŠå³å…´çš„è¯­éŸ³ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå°½ç®¡åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¾—åˆ†ç›¸ä¼¼ï¼Œä½†åœ¨æ€§èƒ½ã€å¹»è§‰å€¾å‘å’Œå†…åœ¨åè§æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™äº›é™åˆ¶å¯¹HRIæœ‰ä¸¥é‡çš„å½±å“ï¼Œè¯†åˆ«é”™è¯¯å¯èƒ½ä¼šå¹²æ‰°ä»»åŠ¡æ€§èƒ½ã€ç”¨æˆ·ä¿¡ä»»å’Œå®‰å…¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17753v1">PDF</a> Accepted at the workshop on Foundation Models for Social Robotics   (FoMoSR) at ICSR 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå®ƒä»¬éœ€è¦åº”å¯¹ç¡¬ä»¶é™åˆ¶å’Œç¯å¢ƒå™ªå£°æ‰€å¸¦æ¥çš„éŸ³é¢‘å¤±çœŸé—®é¢˜ï¼ŒåŒæ—¶è¿˜éœ€è¦æ»¡è¶³ä¸åŒç”¨æˆ·ç¾¤ä½“çš„éœ€æ±‚ã€‚åœ¨äººæœºäº’åŠ¨ï¼ˆHRIï¼‰é¢†åŸŸï¼Œè¿™äº›æŒ‘æˆ˜ç›¸äº’äº¤ç»‡ï¼Œå½¢æˆäº†ä¸€ä¸ªç‹¬ç‰¹çš„è¯†åˆ«ç¯å¢ƒã€‚æœ¬æ–‡å¯¹å››ç§å‰æ²¿çš„ASRç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶‰åŠå…«ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œæ¶µç›–äº†é¢†åŸŸç‰¹å®šã€å£éŸ³ã€å™ªå£°ã€å¹´é¾„å·®å¼‚ã€å—æŸä»¥åŠè‡ªå‘æ€§çš„å…­ä¸ªéš¾åº¦ç»´åº¦ã€‚åˆ†ææ˜¾ç¤ºï¼Œè™½ç„¶è¿™äº›ç³»ç»Ÿåœ¨æ ‡å‡†æµ‹è¯•ä¸Šå¾—åˆ†ç›¸è¿‘ï¼Œä½†åœ¨å®é™…ç¯å¢ƒä¸‹è¡¨ç°æ˜¾è‘—ä¸ä¸€ï¼Œå­˜åœ¨è¯¯è§£å€¾å‘å’Œå›ºæœ‰åè§ã€‚è¿™äº›é™åˆ¶å¯¹äººæœºäº’åŠ¨äº§ç”Ÿäº†ä¸¥é‡å½±å“ï¼Œè¯†åˆ«é”™è¯¯å¯èƒ½å¹²æ‰°ä»»åŠ¡æ‰§è¡Œã€ç”¨æˆ·ä¿¡ä»»å’Œå®‰å…¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRç³»ç»Ÿåœ¨ç°å®åœºæ™¯ä¸‹é¢ä¸´å¤šç§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¡¬ä»¶é™åˆ¶å’Œç¯å¢ƒå™ªå£°å¯¼è‡´çš„éŸ³é¢‘å¤±çœŸã€‚</li>
<li>ASRç³»ç»Ÿåœ¨HRIé¢†åŸŸçš„åº”ç”¨é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦å¤„ç†å¤šç§ä¸åŒçš„ç”¨æˆ·ç¾¤ä½“å’Œç¯å¢ƒå› ç´ ã€‚</li>
<li>å¯¹å››ç§å‰æ²¿ASRç³»ç»Ÿçš„è¯„ä¼°è¡¨æ˜ï¼Œåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>ASRç³»ç»Ÿåœ¨å¤„ç†ä¸åŒç»´åº¦ï¼ˆå¦‚é¢†åŸŸç‰¹å®šã€å£éŸ³ã€å™ªå£°ç­‰ï¼‰çš„å›°éš¾æ—¶ï¼Œè¡¨ç°å‡ºä¸åŒçš„æ€§èƒ½ã€‚</li>
<li>ASRç³»ç»Ÿåœ¨å®é™…ç¯å¢ƒä¸‹å­˜åœ¨è¯¯è§£å€¾å‘å’Œå›ºæœ‰åè§ã€‚</li>
<li>ASRç³»ç»Ÿçš„è¯†åˆ«é”™è¯¯å¯èƒ½å¯¹HRIçš„ä»»åŠ¡æ‰§è¡Œã€ç”¨æˆ·ä¿¡ä»»å’Œå®‰å…¨äº§ç”Ÿä¸¥é‡å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a54e9371c69d474f31d696336e706a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e19120c2a186baefadcae86972e3873.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MemoryTalker-Personalized-Speech-Driven-3D-Facial-Animation-via-Audio-Guided-Stylization"><a href="#MemoryTalker-Personalized-Speech-Driven-3D-Facial-Animation-via-Audio-Guided-Stylization" class="headerlink" title="MemoryTalker: Personalized Speech-Driven 3D Facial Animation via   Audio-Guided Stylization"></a>MemoryTalker: Personalized Speech-Driven 3D Facial Animation via   Audio-Guided Stylization</h2><p><strong>Authors:Hyung Kyu Kim, Sangmin Lee, Hak Gu Kim</strong></p>
<p>Speech-driven 3D facial animation aims to synthesize realistic facial motion sequences from given audio, matching the speakerâ€™s speaking style. However, previous works often require priors such as class labels of a speaker or additional 3D facial meshes at inference, which makes them fail to reflect the speaking style and limits their practical use. To address these issues, we propose MemoryTalker which enables realistic and accurate 3D facial motion synthesis by reflecting speaking style only with audio input to maximize usability in applications. Our framework consists of two training stages: 1-stage is storing and retrieving general motion (i.e., Memorizing), and 2-stage is to perform the personalized facial motion synthesis (i.e., Animating) with the motion memory stylized by the audio-driven speaking style feature. In this second stage, our model learns about which facial motion types should be emphasized for a particular piece of audio. As a result, our MemoryTalker can generate a reliable personalized facial animation without additional prior information. With quantitative and qualitative evaluations, as well as user study, we show the effectiveness of our model and its performance enhancement for personalized facial animation over state-of-the-art methods. </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨æ ¹æ®ç»™å®šçš„éŸ³é¢‘åˆæˆé€¼çœŸçš„é¢éƒ¨è¿åŠ¨åºåˆ—ï¼Œä»¥åŒ¹é…è¯´è¯è€…çš„è¯´è¯é£æ ¼ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„å·¥ä½œé€šå¸¸éœ€è¦å…ˆéªŒä¿¡æ¯ï¼Œå¦‚è¯´è¯è€…çš„ç±»åˆ«æ ‡ç­¾æˆ–æ¨ç†æ—¶çš„é¢å¤–3Dé¢éƒ¨ç½‘æ ¼ï¼Œè¿™ä½¿å¾—å®ƒä»¬æ— æ³•åæ˜ è¯´è¯é£æ ¼ï¼Œå¹¶é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MemoryTalkerï¼Œå®ƒé€šè¿‡ä»…ä½¿ç”¨éŸ³é¢‘è¾“å…¥æ¥åæ˜ è¯´è¯é£æ ¼ï¼Œå®ç°äº†é€¼çœŸä¸”å‡†ç¡®çš„3Dé¢éƒ¨è¿åŠ¨åˆæˆï¼Œä»¥æœ€å¤§åŒ–å…¶åœ¨åº”ç”¨ç¨‹åºä¸­çš„å¯ç”¨æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç”±ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µç»„æˆï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å­˜å‚¨å’Œæ£€ç´¢ä¸€èˆ¬è¿åŠ¨ï¼ˆå³è®°å¿†ï¼‰ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯æ‰§è¡Œä¸ªæ€§åŒ–çš„é¢éƒ¨è¿åŠ¨åˆæˆï¼ˆå³åŠ¨ç”»ï¼‰ä¸ç”±éŸ³é¢‘é©±åŠ¨çš„è¯´è¯é£æ ¼ç‰¹å¾è¿›è¡Œè¿åŠ¨è®°å¿†çš„é£æ ¼åŒ–ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ å“ªç±»é¢éƒ¨è¿åŠ¨åº”è¯¥ä¸ºç‰¹å®šçš„éŸ³é¢‘ç‰‡æ®µè€Œå¼ºè°ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„MemoryTalkerå¯ä»¥åœ¨æ²¡æœ‰é¢å¤–å…ˆéªŒä¿¡æ¯çš„æƒ…å†µä¸‹ç”Ÿæˆå¯é çš„ä¸ªäººé¢éƒ¨åŠ¨ç”»ã€‚é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°ä»¥åŠç”¨æˆ·ç ”ç©¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨ä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»æ–¹é¢çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20562v2">PDF</a> Accepted in ICCV 2025; Project Page:   <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/ICCV25-MemoryTalker/">https://cau-irislab.github.io/ICCV25-MemoryTalker/</a></p>
<p><strong>Summary</strong><br>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨æ ¹æ®ç»™å®šçš„éŸ³é¢‘åˆæˆé€¼çœŸçš„é¢éƒ¨è¿åŠ¨åºåˆ—ï¼ŒåŒ¹é…è¯´è¯è€…çš„è¯´è¯é£æ ¼ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æ–¹æ³•é€šå¸¸éœ€è¦å…ˆéªŒä¿¡æ¯ï¼Œå¦‚è¯´è¯è€…çš„ç±»åˆ«æ ‡ç­¾æˆ–é¢å¤–çš„3Dé¢éƒ¨ç½‘æ ¼è¿›è¡Œæ¨æ–­ï¼Œè¿™ä½¿å¾—å®ƒä»¬æ— æ³•åæ˜ è¯´è¯é£æ ¼å¹¶é™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MemoryTalkerï¼Œé€šè¿‡ä»…ä½¿ç”¨éŸ³é¢‘è¾“å…¥åæ˜ è¯´è¯é£æ ¼ï¼Œå®ç°é€¼çœŸä¸”å‡†ç¡®çš„3Dé¢éƒ¨è¿åŠ¨åˆæˆï¼Œä»¥åœ¨åº”ç”¨ç¨‹åºä¸­æœ€å¤§åŒ–å¯ç”¨æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å­˜å‚¨å’Œæ£€ç´¢é€šç”¨è¿åŠ¨ï¼ˆå³è®°å¿†ï¼‰ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯åˆ©ç”¨éŸ³é¢‘é©±åŠ¨çš„è¯´è¯é£æ ¼ç‰¹å¾è¿›è¡Œä¸ªæ€§åŒ–é¢éƒ¨è¿åŠ¨åˆæˆï¼ˆå³åŠ¨ç”»ï¼‰ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ å¦‚ä½•å¼ºè°ƒé’ˆå¯¹ç‰¹å®šéŸ³é¢‘çš„é¢éƒ¨è¿åŠ¨ç±»å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„MemoryTalkeræ— éœ€é¢å¤–çš„å…ˆéªŒä¿¡æ¯å³å¯ç”Ÿæˆå¯é çš„ä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MemoryTalkeræ—¨åœ¨è§£å†³è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ä¸­åæ˜ è¯´è¯é£æ ¼çš„é—®é¢˜ï¼Œå¹¶æé«˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯ç”¨æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä»…ä½¿ç”¨éŸ³é¢‘è¾“å…¥å®ç°é€¼çœŸä¸”å‡†ç¡®çš„3Dé¢éƒ¨è¿åŠ¨åˆæˆã€‚</li>
<li>MemoryTalkeré‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶ï¼šå­˜å‚¨å’Œæ£€ç´¢é€šç”¨è¿åŠ¨ï¼ˆè®°å¿†é˜¶æ®µï¼‰ä»¥åŠä¸ªæ€§åŒ–é¢éƒ¨è¿åŠ¨åˆæˆï¼ˆåŠ¨ç”»é˜¶æ®µï¼‰ã€‚</li>
<li>åœ¨åŠ¨ç”»é˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ å¦‚ä½•æ ¹æ®ç‰¹å®šéŸ³é¢‘å¼ºè°ƒé¢éƒ¨è¿åŠ¨ç±»å‹ã€‚</li>
<li>MemoryTalkeræ— éœ€é¢å¤–çš„å…ˆéªŒä¿¡æ¯å³å¯ç”Ÿæˆä¸ªæ€§åŒ–çš„é¢éƒ¨åŠ¨ç”»ã€‚</li>
<li>é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°ä»¥åŠç”¨æˆ·ç ”ç©¶ï¼ŒéªŒè¯äº†MemoryTalkeræ¨¡å‹çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨ä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»æ–¹é¢çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8374585a8d55f38b4e182cbb9b79665.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-982c1f88db6a173cd8272c364c314244.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1a802e74c5ab27b4c99c030e5711785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5e6ef2308508c4ae3b5aa446931ad9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f532c24820dee0974fe32b8ae3b69646.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FaceEditTalker-Controllable-Talking-Head-Generation-with-Facial-Attribute-Editing"><a href="#FaceEditTalker-Controllable-Talking-Head-Generation-with-Facial-Attribute-Editing" class="headerlink" title="FaceEditTalker: Controllable Talking Head Generation with Facial   Attribute Editing"></a>FaceEditTalker: Controllable Talking Head Generation with Facial   Attribute Editing</h2><p><strong>Authors:Guanwen Feng, Zhiyuan Ma, Yunan Li, Jiahao Yang, Junwei Jing, Qiguang Miao</strong></p>
<p>Recent advances in audio-driven talking head generation have achieved impressive results in lip synchronization and emotional expression. However, they largely overlook the crucial task of facial attribute editing. This capability is indispensable for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service. In these key domains, flexible adjustment of visual attributes, such as hairstyle, accessories, and subtle facial features, is essential for aligning with user preferences, reflecting diverse brand identities and adapting to varying contextual demands. In this paper, we present FaceEditTalker, a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos. Our method consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator. This design ensures temporal coherence, visual fidelity, and identity preservation across frames. Extensive experiments on public datasets demonstrate that our method achieves comparable or superior performance to representative baseline methods in lip-sync accuracy, video quality, and attribute controllability. Project page: <a target="_blank" rel="noopener" href="https://peterfanfan.github.io/FaceEditTalker/">https://peterfanfan.github.io/FaceEditTalker/</a> </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨è°ˆè¯å¤´éƒ¨ç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•åœ¨å”‡éƒ¨åŒæ­¥å’Œæƒ…æ„Ÿè¡¨è¾¾æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œä»–ä»¬å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†é¢éƒ¨å±æ€§ç¼–è¾‘è¿™ä¸€å…³é”®ä»»åŠ¡ã€‚è¿™ç§èƒ½åŠ›å¯¹äºå®ç°æ·±åº¦ä¸ªæ€§åŒ–ä»¥åŠæ‹“å±•å®é™…åº”ç”¨èŒƒå›´ï¼ˆåŒ…æ‹¬ç”¨æˆ·å®šåˆ¶çš„æ•°å­—åŒ–èº«ã€å¼•äººå…¥èƒœçš„åœ¨çº¿æ•™è‚²å†…å®¹å’Œç‰¹å®šå“ç‰Œçš„æ•°å­—å®¢æˆ·æœåŠ¡ï¼‰æ˜¯å¿…ä¸å¯å°‘çš„ã€‚åœ¨è¿™äº›å…³é”®é¢†åŸŸä¸­ï¼Œè§†è§‰å±æ€§çš„çµæ´»è°ƒæ•´ï¼ˆå¦‚å‘å‹ã€é…é¥°å’Œå¾®å¦™çš„é¢éƒ¨ç‰¹å¾ï¼‰å¯¹äºç¬¦åˆç”¨æˆ·åå¥½ã€åæ˜ å¤šæ ·åŒ–çš„å“ç‰Œèº«ä»½ä»¥åŠé€‚åº”å„ç§ä¸Šä¸‹æ–‡éœ€æ±‚è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FaceEditTalkerï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆé«˜è´¨é‡ã€ä¸éŸ³é¢‘åŒæ­¥çš„è°ˆè¯å¤´éƒ¨è§†é¢‘çš„åŒæ—¶ï¼Œå®ç°é¢éƒ¨å±æ€§çš„å¯æ§æ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šå›¾åƒç‰¹å¾ç©ºé—´ç¼–è¾‘æ¨¡å—ï¼Œè¯¥æ¨¡å—æå–è¯­ä¹‰å’Œç»†èŠ‚ç‰¹å¾ï¼Œå¹¶å…è®¸å¯¹è¡¨æƒ…ã€å‘å‹å’Œé…é¥°ç­‰å±æ€§è¿›è¡Œçµæ´»æ§åˆ¶ï¼›éŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—å°†è¿™äº›ç¼–è¾‘åçš„ç‰¹å¾ä¸éŸ³é¢‘å¼•å¯¼çš„é¢éƒ¨åœ°æ ‡ç›¸èåˆï¼Œä»¥é©±åŠ¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆå™¨ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†æ—¶é—´è¿è´¯æ€§ã€è§†è§‰ä¿çœŸåº¦å’Œè·¨å¸§çš„èº«ä»½ä¿ç•™ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å”‡åŒæ­¥å‡†ç¡®æ€§ã€è§†é¢‘è´¨é‡å’Œå±æ€§å¯æ§æ€§æ–¹é¢è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†ä»£è¡¨æ€§åŸºå‡†æ–¹æ³•çš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://peterfanfan.github.io/FaceEditTalker/">https://peterfanfan.github.io/FaceEditTalker/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22141v2">PDF</a> </p>
<p><strong>Summary</strong><br>éšç€éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨ç”ŸæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œå”‡åŒæ­¥å’Œæƒ…ç»ªè¡¨è¾¾æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œé¢éƒ¨å±æ€§ç¼–è¾‘è¿™ä¸€å…³é”®ä»»åŠ¡å´è¢«å¿½è§†äº†ã€‚ä¸ºå®ç°æ·±åº¦ä¸ªæ€§åŒ–å¹¶æ‰©å±•å®é™…åº”ç”¨èŒƒå›´ï¼Œå¦‚ç”¨æˆ·å®šåˆ¶çš„æ•°å­—åŒ–èº«ã€å¼•äººå…¥èƒœçš„åœ¨çº¿æ•™è‚²å†…å®¹ä»¥åŠç‰¹å®šå“ç‰Œçš„æ•°å­—å®¢æˆ·æœåŠ¡ï¼Œçµæ´»çš„é¢éƒ¨å±æ€§è°ƒæ•´è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†FaceEditTalkeræ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆé«˜è´¨é‡ã€ä¸éŸ³é¢‘åŒæ­¥çš„è¯´è¯äººå¤´è§†é¢‘çš„åŒæ—¶ï¼Œå®ç°å¯æ§çš„é¢éƒ¨å±æ€§æ“ä½œã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå›¾åƒç‰¹å¾ç©ºé—´ç¼–è¾‘æ¨¡å—å’ŒéŸ³é¢‘é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆæ¨¡å—ã€‚å‰è€…æå–è¯­ä¹‰å’Œç»†èŠ‚ç‰¹å¾ï¼Œå¹¶å…è®¸çµæ´»æ§åˆ¶è¡¨æƒ…ã€å‘å‹å’Œé…é¥°ç­‰å±æ€§ï¼›åè€…èåˆè¿™äº›ç¼–è¾‘åçš„ç‰¹å¾ä¸éŸ³é¢‘å¼•å¯¼çš„é¢éƒ¨åœ°æ ‡ï¼Œé©±åŠ¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆå™¨ã€‚è¯¥è®¾è®¡ç¡®ä¿äº†è·¨å¸§çš„æ—¶é—´è¿è´¯æ€§ã€è§†è§‰ä¿çœŸåº¦å’Œèº«ä»½ä¿ç•™ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å”‡åŒæ­¥å‡†ç¡®æ€§ã€è§†é¢‘è´¨é‡å’Œå±æ€§å¯æ§æ€§æ–¹é¢è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†ä»£è¡¨æ€§åŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€è¿‘éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨ç”ŸæˆæŠ€æœ¯åœ¨å”‡åŒæ­¥å’Œæƒ…ç»ªè¡¨è¾¾ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¿½è§†äº†é¢éƒ¨å±æ€§ç¼–è¾‘çš„é‡è¦æ€§ã€‚</li>
<li>é¢éƒ¨å±æ€§ç¼–è¾‘å¯¹äºå®ç°æ·±åº¦ä¸ªæ€§åŒ–ä»¥åŠæ‰©å±•å®é™…åº”ç”¨èŒƒå›´è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬ç”¨æˆ·å®šåˆ¶çš„æ•°å­—åŒ–èº«ã€åœ¨çº¿æ•™è‚²å’Œå“ç‰Œå®¢æˆ·æœåŠ¡ç­‰ã€‚</li>
<li>FaceEditTalkeræ¡†æ¶ç»“åˆäº†å›¾åƒç‰¹å¾ç©ºé—´ç¼–è¾‘å’ŒéŸ³é¢‘é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆï¼Œå®ç°äº†é«˜è´¨é‡ä¸”å¯æ§çš„è¯´è¯äººå¤´è§†é¢‘ç”Ÿæˆã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå›¾åƒç‰¹å¾ç©ºé—´ç¼–è¾‘æ¨¡å—å…è®¸çµæ´»æ§åˆ¶è¡¨æƒ…ã€å‘å‹å’Œé…é¥°ç­‰å±æ€§ï¼›éŸ³é¢‘é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆæ¨¡å—ç¡®ä¿äº†æ—¶é—´è¿è´¯æ€§ã€è§†è§‰ä¿çœŸåº¦å’Œèº«ä»½ä¿ç•™ã€‚</li>
<li>FaceEditTalkeråœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†æˆ–è¶…è¶Šäº†ç°æœ‰æ–¹æ³•åœ¨å”‡åŒæ­¥ã€è§†é¢‘è´¨é‡å’Œå±æ€§æ§åˆ¶æ–¹é¢çš„æ ‡å‡†ã€‚</li>
<li>è¯¥é¡¹ç›®æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œè§£å†³äº†é¢éƒ¨å±æ€§ç¼–è¾‘å’ŒéŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´è§†é¢‘ç”Ÿæˆä¹‹é—´çš„èåˆé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dc45a247f2b118f16c4374645b32bf37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac7ea775324bfba4884060cd0b58e58a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c717c3f6e2fc9a61af5350a7207dc323.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab90a8e627de28197e9bf353c859ffec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dea6145cd2ab625efa4db819dcb716ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1ba7750e4f6607aa41de30928ec1cc0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FlowDubber-Movie-Dubbing-with-LLM-based-Semantic-aware-Learning-and-Flow-Matching-based-Voice-Enhancing"><a href="#FlowDubber-Movie-Dubbing-with-LLM-based-Semantic-aware-Learning-and-Flow-Matching-based-Voice-Enhancing" class="headerlink" title="FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and   Flow Matching based Voice Enhancing"></a>FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and   Flow Matching based Voice Enhancing</h2><p><strong>Authors:Gaoxiang Cong, Liang Li, Jiadong Pan, Zhedong Zhang, Amin Beheshti, Anton van den Hengel, Yuankai Qi, Qingming Huang</strong></p>
<p>Movie Dubbing aims to convert scripts into speeches that align with the given movie clip in both temporal and emotional aspects while preserving the vocal timbre of a given brief reference audio. Existing methods focus primarily on reducing the word error rate while ignoring the importance of lip-sync and acoustic quality. To address these issues, we propose a large language model (LLM) based flow matching architecture for dubbing, named FlowDubber, which achieves high-quality audio-visual sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving better acoustic quality via the proposed voice-enhanced flow matching than previous works. First, we introduce Qwen2.5 as the backbone of LLM to learn the in-context sequence from movie scripts and reference audio. Then, the proposed semantic-aware learning focuses on capturing LLM semantic knowledge at the phoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment with lip movement, reducing ambiguities where similar phonemes might be confused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves acoustic quality in two aspects, which introduces an LLM-based acoustics flow matching guidance to strengthen clarity and uses affine style prior to enhance identity when recovering noise into mel-spectrograms via gradient vector field prediction. Extensive experiments demonstrate that our method outperforms several state-of-the-art methods on two primary benchmarks. </p>
<blockquote>
<p>ç”µå½±é…éŸ³æ—¨åœ¨å°†å‰§æœ¬è½¬æ¢ä¸ºä¸ç»™å®šç”µå½±ç‰‡æ®µåœ¨æ—¶é—´å’Œæƒ…æ„Ÿæ–¹é¢éƒ½å¯¹é½çš„è¯­éŸ³ï¼ŒåŒæ—¶ä¿ç•™ç»™å®šç®€çŸ­å‚è€ƒéŸ³é¢‘çš„éŸ³è‰²ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é™ä½è¯é”™è¯¯ç‡ï¼Œè€Œå¿½è§†å”‡åŒæ­¥å’ŒéŸ³è´¨çš„é‡è¦æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é…éŸ³æµåŒ¹é…æ¶æ„ï¼Œåä¸ºFlowDubberã€‚å®ƒé€šè¿‡ç»“åˆå¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹å’ŒåŒå¯¹æ¯”å¯¹é½ï¼Œä»¥åŠé€šè¿‡æå‡ºçš„è¯­éŸ³å¢å¼ºæµåŒ¹é…ï¼Œå®ç°äº†é«˜è´¨é‡çš„å£°éŸ³è§†è§‰åŒæ­¥å’Œå‘éŸ³ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥Qwen2.5ä½œä¸ºLLMçš„åç«¯ï¼Œå­¦ä¹ ç”µå½±å‰§æœ¬å’Œå‚è€ƒéŸ³é¢‘çš„ä¸Šä¸‹æ–‡åºåˆ—ã€‚ç„¶åï¼Œæå‡ºçš„è¯­ä¹‰æ„ŸçŸ¥å­¦ä¹ ä¾§é‡äºåœ¨éŸ³ç´ çº§åˆ«æ•è·LLMè¯­ä¹‰çŸ¥è¯†ã€‚æ¥ä¸‹æ¥ï¼ŒåŒå¯¹æ¯”å¯¹é½ï¼ˆDCAï¼‰é€šè¿‡å”‡åŠ¨å¢å¼ºç›¸äº’å¯¹é½ï¼Œå‡å°‘ç›¸ä¼¼éŸ³ç´ çš„æ··æ·†ã€‚æœ€åï¼Œæå‡ºçš„åŸºäºæµçš„è¯­éŸ³å¢å¼ºï¼ˆFVEï¼‰ä»ä¸¤ä¸ªæ–¹é¢æé«˜äº†éŸ³è´¨ï¼šå®ƒå¼•å…¥äº†ä¸€ç§åŸºäºLLMçš„å£°å­¦æµåŒ¹é…æŒ‡å¯¼ï¼Œä»¥æé«˜æ¸…æ™°åº¦ï¼Œå¹¶åœ¨é€šè¿‡æ¢¯åº¦çŸ¢é‡åœºé¢„æµ‹æ¢å¤å™ªå£°æ—¶ï¼Œé‡‡ç”¨ä»¿å°„é£æ ¼å¢å¼ºèº«ä»½ç‰¹å¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºå‡ ç§æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01263v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”µå½±é…éŸ³çš„ä»»åŠ¡ï¼Œæ—¨åœ¨å°†å‰§æœ¬è½¬æ¢ä¸ºä¸ç»™å®šç”µå½±ç‰‡æ®µåœ¨æ—¶é—´å’Œæƒ…æ„Ÿæ–¹é¢å¯¹é½çš„æ¼”è®²ï¼ŒåŒæ—¶ä¿ç•™ç®€çŸ­å‚è€ƒéŸ³é¢‘çš„éŸ³è‰²ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é™ä½è¯é”™è¯¯ç‡è€Œå¿½è§†å”‡åŒæ­¥å’ŒéŸ³è´¨çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„FlowDubberæ¶æ„ã€‚é€šè¿‡å¼•å…¥LLMä½œä¸ºéª¨å¹²ã€è¯­ä¹‰æ„ŸçŸ¥å­¦ä¹ ã€åŒé‡å¯¹æ¯”å¯¹é½å’ŒåŸºäºæµçš„è¯­éŸ³å¢å¼ºï¼Œå®ç°äº†é«˜è´¨é‡çš„ç”µå½±é…éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå½±é…éŸ³ä»»åŠ¡æ—¨åœ¨å°†å‰§æœ¬è½¬æ¢ä¸ºä¸ç”µå½±ç‰‡æ®µå¯¹é½çš„æ¼”è®²ï¼Œéœ€è€ƒè™‘æ—¶é—´å’Œæƒ…æ„Ÿå› ç´ ï¼ŒåŒæ—¶ä¿ç•™å‚è€ƒéŸ³é¢‘çš„éŸ³è‰²ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¯é”™è¯¯ç‡ï¼Œè€Œå¿½è§†å”‡åŒæ­¥å’ŒéŸ³è´¨ã€‚</li>
<li>æå‡ºçš„FlowDubberæ¶æ„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†é«˜è´¨é‡çš„ç”µå½±é…éŸ³ã€‚</li>
<li>Qwen2.5ä½œä¸ºLLMçš„éª¨å¹²ï¼Œå­¦ä¹ ç”µå½±å‰§æœ¬å’Œå‚è€ƒéŸ³é¢‘çš„ä¸Šä¸‹æ–‡åºåˆ—ã€‚</li>
<li>è¯­ä¹‰æ„ŸçŸ¥å­¦ä¹ ä¸“æ³¨äºæ•æ‰LLMåœ¨éŸ³ç´ çº§åˆ«çš„è¯­ä¹‰çŸ¥è¯†ã€‚</li>
<li>åŒé‡å¯¹æ¯”å¯¹é½ï¼ˆDCAï¼‰é€šè¿‡åŠ å¼ºå”‡åŠ¨ä¸è¯­éŸ³çš„ç›¸äº’å¯¹é½ï¼Œå‡å°‘ç›¸ä¼¼éŸ³ç´ çš„æ··æ·†ã€‚</li>
<li>åŸºäºæµçš„è¯­éŸ³å¢å¼ºï¼ˆFVEï¼‰åœ¨ä¸¤ä¸ªæ–¹é¢æé«˜äº†éŸ³è´¨ï¼Œé€šè¿‡å¼•å…¥LLMçš„å£°å­¦æµåŒ¹é…æŒ‡å¯¼å¢å¼ºæ¸…æ™°åº¦ï¼Œå¹¶ä½¿ç”¨ä»¿å°„é£æ ¼å…ˆéªŒåœ¨é€šè¿‡æ¢¯åº¦çŸ¢é‡åœºé¢„æµ‹æ¢å¤å™ªå£°æ—¶å¢å¼ºèº«ä»½è¯†åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17c9efe729badab71e9853627936dc25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c0cf497fc35292286f4b873785feb2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f558c95985312c5df8d4c222e9aeeaba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba3b60115a52f361967d1809141b8bff.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Supervising-3D-Talking-Head-Avatars-with-Analysis-by-Audio-Synthesis"><a href="#Supervising-3D-Talking-Head-Avatars-with-Analysis-by-Audio-Synthesis" class="headerlink" title="Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis"></a>Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis</h2><p><strong>Authors:Radek DanÄ›Äek, Carolin Schmitt, Senya Polikovsky, Michael J. Black</strong></p>
<p>In order to be widely applicable, speech-driven 3D head avatars must articulate their lips in accordance with speech, while also conveying the appropriate emotions with dynamically changing facial expressions. The key problem is that deterministic models produce high-quality lip-sync but without rich expressions, whereas stochastic models generate diverse expressions but with lower lip-sync quality. To get the best of both, we seek a stochastic model with accurate lip-sync. To that end, we develop a new approach based on the following observation: if a method generates realistic 3D lip motions, it should be possible to infer the spoken audio from the lip motion. The inferred speech should match the original input audio, and erroneous predictions create a novel supervision signal for training 3D talking head avatars with accurate lip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under Neural Differentiable Elocution Reconstruction), a 3D talking head avatar framework that introduces a novel supervision mechanism via differentiable sound production. First, we train a novel mesh-to-speech model that regresses audio from facial animation. Then, we incorporate this model into a diffusion-based talking avatar framework. During training, the mesh-to-speech model takes the generated animation and produces a sound that is compared to the input speech, creating a differentiable analysis-by-audio-synthesis supervision loop. Our extensive qualitative and quantitative experiments demonstrate that THUNDER significantly improves the quality of the lip-sync of talking head avatars while still allowing for generation of diverse, high-quality, expressive facial animations. The code and models will be available at <a target="_blank" rel="noopener" href="https://thunder.is.tue.mpg.de/">https://thunder.is.tue.mpg.de/</a> </p>
<blockquote>
<p>ä¸ºäº†å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œè¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´å¤´éƒ¨åŒ–èº«å¿…é¡»æ ¹æ®è¯­éŸ³è¿›è¡Œå”‡éƒ¨åŠ¨ä½œçš„è¡¨è¾¾ï¼ŒåŒæ—¶å€ŸåŠ©åŠ¨æ€å˜åŒ–çš„é¢éƒ¨è¡¨æƒ…æ¥ä¼ è¾¾é€‚å½“çš„æƒ…ç»ªã€‚å…³é”®é—®é¢˜åœ¨äºç¡®å®šæ€§æ¨¡å‹è™½ç„¶èƒ½äº§ç”Ÿé«˜è´¨é‡çš„å”‡éƒ¨åŒæ­¥æ•ˆæœï¼Œä½†ç¼ºä¹ä¸°å¯Œçš„è¡¨æƒ…ï¼›è€Œéšæœºæ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆå¤šæ ·åŒ–çš„è¡¨æƒ…ï¼Œä½†å”‡éƒ¨åŒæ­¥è´¨é‡è¾ƒä½ã€‚ä¸ºäº†å…¼é¡¾ä¸¤è€…ä¹‹ä¼˜ç‚¹ï¼Œæˆ‘ä»¬å¯»æ±‚å…·æœ‰ç²¾ç¡®å”‡éƒ¨åŒæ­¥çš„éšæœºæ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åŸºäºä»¥ä¸‹è§‚å¯Ÿç»“æœå¼€å‘äº†ä¸€ç§æ–°æ–¹æ³•ï¼šå¦‚æœä¸€ä¸ªæ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„ä¸‰ç»´å”‡éƒ¨è¿åŠ¨ï¼Œé‚£ä¹ˆå°±åº”è¯¥èƒ½å¤Ÿä»å”‡éƒ¨è¿åŠ¨ä¸­æ¨æ–­å‡ºè¯­éŸ³ã€‚æ¨æ–­å‡ºçš„è¯­éŸ³åº”ä¸åŸå§‹è¾“å…¥éŸ³é¢‘ç›¸åŒ¹é…ï¼Œé”™è¯¯çš„é¢„æµ‹ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ç›‘ç£ä¿¡å·ï¼Œç”¨äºè®­ç»ƒå…·æœ‰ç²¾ç¡®å”‡éƒ¨åŒæ­¥çš„ä¸‰ç»´è¯´è¯å¤´éƒ¨åŒ–èº«ã€‚ä¸ºäº†å±•ç¤ºè¿™ä¸€æ•ˆæœï¼Œæˆ‘ä»¬æå‡ºäº†THUNDERï¼ˆç¥ç»å¯å¾®è¯­éŸ³é‡å»ºä¸‹çš„è¯´è¯å¤´ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰ç»´è¯´è¯å¤´éƒ¨åŒ–èº«æ¡†æ¶ï¼Œé€šè¿‡å¯å¾®åˆ†çš„å£°éŸ³äº§ç”Ÿå¼•å…¥äº†ä¸€ç§æ–°å‹ç›‘ç£æœºåˆ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§æ–°å‹ç½‘æ ¼åˆ°è¯­éŸ³æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»é¢éƒ¨åŠ¨ç”»å›å½’éŸ³é¢‘ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¯¥æ¨¡å‹çº³å…¥åŸºäºæ‰©æ•£çš„è¯´è¯åŒ–èº«æ¡†æ¶ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç½‘æ ¼åˆ°è¯­éŸ³æ¨¡å‹ä¼šæ¥å—ç”Ÿæˆçš„åŠ¨ç”»å¹¶äº§ç”Ÿå£°éŸ³ï¼Œè¯¥å£°éŸ³ä¼šä¸è¾“å…¥è¯­éŸ³è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªå¯å¾®åˆ†çš„åˆ†æ-ç”±éŸ³é¢‘åˆæˆç›‘ç£å¾ªç¯ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒTHUNDERæ˜¾è‘—æé«˜äº†è¯´è¯å¤´éƒ¨åŒ–èº«çš„å”‡éƒ¨åŒæ­¥è´¨é‡ï¼ŒåŒæ—¶ä»èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡ã€å¯Œæœ‰è¡¨ç°åŠ›çš„é¢éƒ¨åŠ¨ç”»ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://thunder.is.tue.mpg.de/">https://thunder.is.tue.mpg.de/</a> ä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13386v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ›å»ºå…·æœ‰å®æ—¶è¯­éŸ³é©±åŠ¨çš„é¢éƒ¨è¡¨æƒ…å’Œå£å‹åŒæ­¥çš„3Då¤´åƒçš„æŒ‘æˆ˜ã€‚ç¡®å®šæ€§æ¨¡å‹å¯ä»¥äº§ç”Ÿé«˜è´¨é‡çš„å£å‹åŒæ­¥ï¼Œä½†ç¼ºä¹ä¸°å¯Œçš„è¡¨æƒ…ï¼Œè€Œéšæœºæ¨¡å‹åˆ™èƒ½ç”Ÿæˆå¤šæ ·åŒ–çš„è¡¨æƒ…ï¼Œä½†å£å‹åŒæ­¥è´¨é‡è¾ƒä½ã€‚ä¸ºäº†ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†ä¸€ç§åŸºäºç”ŸæˆçœŸå®3Då”‡éƒ¨è¿åŠ¨å¯ä»¥ä»å…¶è¿åŠ¨ä¸­æ¨æ–­è¯­éŸ³çš„æ–°æ–¹æ³•ï¼Œå¹¶æ®æ­¤æ„å»ºäº†THUNDERæ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°å‹ç›‘ç£æœºåˆ¶ï¼Œé€šè¿‡å¯å¾®å£°éŸ³äº§ç”Ÿæ¥å®ç°å¯¹è¯´è¯äººå¤´éƒ¨çš„ç²¾å‡†æ§åˆ¶ï¼Œè¿›è€Œå®ç°äº†å£å‹åŒæ­¥çš„æ”¹è¿›å’Œè¡¨æƒ…åŠ¨ç”»çš„å¤šæ ·åŒ–ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­éŸ³é©±åŠ¨çš„3Då¤´åƒéœ€è¦åŒæ­¥å£å‹ä»¥ä¼ è¾¾è¯­éŸ³å’ŒåŠ¨æ€å˜åŒ–çš„é¢éƒ¨è¡¨æƒ…ã€‚</li>
<li>ç¡®å®šæ€§æ¨¡å‹æä¾›é«˜è´¨é‡çš„å£å‹åŒæ­¥ä½†ç¼ºä¹ä¸°å¯Œè¡¨æƒ…ï¼Œè€Œéšæœºæ¨¡å‹åˆ™äº§ç”Ÿå¤šæ ·åŒ–çš„è¡¨æƒ…ä½†å£å‹åŒæ­¥è´¨é‡è¾ƒä½ã€‚</li>
<li>æ–°æ–¹æ³•é€šè¿‡ç”ŸæˆçœŸå®çš„3Då”‡éƒ¨è¿åŠ¨å¹¶æ®æ­¤æ¨æ–­è¯­éŸ³ï¼Œä¸ºè®­ç»ƒå…·æœ‰ç²¾å‡†å£å‹åŒæ­¥çš„è¯´è¯äººå¤´æ¨¡æä¾›äº†åŸºç¡€ã€‚</li>
<li>THUNDERæ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°å‹ç›‘ç£æœºåˆ¶ï¼Œé€šè¿‡å¯å¾®å£°éŸ³äº§ç”Ÿæ¥å®ç°å¯¹è¯´è¯äººå¤´éƒ¨çš„ç²¾å‡†æ§åˆ¶ã€‚</li>
<li>THUNDERæ˜¾è‘—æé«˜äº†å¤´åƒçš„å£å‹åŒæ­¥è´¨é‡ï¼Œå¹¶å…è®¸ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è¡¨æƒ…åŠ¨ç”»ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-263e794e8bf7f017a90a0f66d1a7b854.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd053ebd184e3da36aefc04005e2af98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf62327d584f9334a71420e5574fb1e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f132b8a95854acbc57588b51873dddb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Removing-Averaging-Personalized-Lip-Sync-Driven-Characters-Based-on-Identity-Adapter"><a href="#Removing-Averaging-Personalized-Lip-Sync-Driven-Characters-Based-on-Identity-Adapter" class="headerlink" title="Removing Averaging: Personalized Lip-Sync Driven Characters Based on   Identity Adapter"></a>Removing Averaging: Personalized Lip-Sync Driven Characters Based on   Identity Adapter</h2><p><strong>Authors:Yanyu Zhu, Lichen Bai, Jintao Xu, Hai-tao Zheng</strong></p>
<p>Recent advances in diffusion-based lip-syncing generative models have demonstrated their ability to produce highly synchronized talking face videos for visual dubbing. Although these models excel at lip synchronization, they often struggle to maintain fine-grained control over facial details in generated images. In this work, we identify â€œlip averagingâ€ phenomenon where the model fails to preserve subtle facial details when dubbing unseen in-the-wild videos. This issue arises because the commonly used UNet backbone primarily integrates audio features into visual representations in the latent space via cross-attention mechanisms and multi-scale fusion, but it struggles to retain fine-grained lip details in the generated faces. To address this issue, we propose UnAvgLip, which extracts identity embeddings from reference videos to generate highly faithful facial sequences while maintaining accurate lip synchronization. Specifically, our method comprises two primary components: (1) an Identity Perceiver module that encodes facial embeddings to align with conditioned audio features; and (2) an ID-CrossAttn module that injects facial embeddings into the generation process, enhancing modelâ€™s capability of identity retention. Extensive experiments demonstrate that, at a modest training and inference cost, UnAvgLip effectively mitigates the â€œaveragingâ€ phenomenon in lip inpainting, significantly preserving unique facial characteristics while maintaining precise lip synchronization. Compared with the original approach, our method demonstrates significant improvements of 5% on the identity consistency metric and 2% on the SSIM metric across two benchmark datasets (HDTF and LRW). </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„å”‡åŒæ­¥ç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•å·²ç»è¯æ˜å®ƒä»¬åœ¨è§†è§‰é…éŸ³ä¸­ç”Ÿæˆé«˜åº¦åŒæ­¥çš„è¯´è¯é¢éƒ¨è§†é¢‘çš„èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨å”‡åŒæ­¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨ä¿æŒç”Ÿæˆå›¾åƒçš„é¢éƒ¨ç»†èŠ‚æ–¹é¢å¾€å¾€é¢ä¸´å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†â€œå”‡éƒ¨å¹³å‡åŒ–â€ç°è±¡ï¼Œå³æ¨¡å‹åœ¨é…éŸ³æœªè§è¿‡çš„é‡å¤–è§†é¢‘æ—¶ï¼Œæ— æ³•ä¿ç•™å¾®å¦™çš„é¢éƒ¨ç»†èŠ‚ã€‚è¿™ä¸ªé—®é¢˜å‡ºç°çš„åŸå› æ˜¯ï¼Œå¸¸ç”¨çš„UNetä¸»å¹²ç½‘ç»œä¸»è¦é€šè¿‡è·¨æ³¨æ„æœºåˆ¶å’Œå¤šå°ºåº¦èåˆå°†éŸ³é¢‘ç‰¹å¾é›†æˆåˆ°è§†è§‰è¡¨ç¤ºä¸­ï¼Œä½†åœ¨ç”Ÿæˆçš„é¢éƒ¨ä¸­éš¾ä»¥ä¿ç•™ç²¾ç»†çš„å”‡éƒ¨ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UnAvgLipï¼Œå®ƒä»å‚è€ƒè§†é¢‘ä¸­æå–èº«ä»½åµŒå…¥ï¼Œä»¥ç”Ÿæˆé«˜åº¦å¿ å®çš„é¢éƒ¨åºåˆ—ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®çš„å”‡éƒ¨åŒæ­¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰èº«ä»½æ„ŸçŸ¥å™¨æ¨¡å—ï¼Œå®ƒç¼–ç é¢éƒ¨åµŒå…¥ä»¥ä¸æ¡ä»¶éŸ³é¢‘ç‰¹å¾å¯¹é½ï¼›ï¼ˆ2ï¼‰ID-CrossAttnæ¨¡å—ï¼Œå®ƒå°†é¢éƒ¨åµŒå…¥æ³¨å…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œå¢å¼ºæ¨¡å‹ä¿ç•™èº«ä»½çš„èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨é€‚åº¦çš„è®­ç»ƒå’Œæ¨ç†æˆæœ¬ä¸‹ï¼ŒUnAvgLipæœ‰æ•ˆåœ°ç¼“è§£äº†å”‡éƒ¨è¡¥å…¨ä¸­çš„â€œå¹³å‡åŒ–â€ç°è±¡ï¼Œåœ¨ä¿æŒç²¾ç¡®å”‡éƒ¨åŒæ­¥çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¿ç•™äº†ç‹¬ç‰¹çš„é¢éƒ¨ç‰¹å¾ã€‚ä¸åŸå§‹æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨HDTFå’ŒLRWä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šèº«ä»½ä¸€è‡´æ€§æŒ‡æ ‡æé«˜äº†5%ï¼ŒSSIMæŒ‡æ ‡æé«˜äº†2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06397v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºæ‰©æ•£çš„å”‡åŒæ­¥ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿåœ¨è§†è§‰é…éŸ³ä¸­ç”Ÿæˆé«˜åº¦åŒæ­¥çš„è¯´è¯é¢éƒ¨è§†é¢‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ä¿æŒé¢éƒ¨ç»†èŠ‚æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†â€œå”‡éƒ¨å¹³å‡åŒ–â€ç°è±¡ï¼Œå³æ¨¡å‹åœ¨ä¸ºæœªè§è¿‡çš„é‡ç”Ÿè§†é¢‘é…éŸ³æ—¶ï¼Œæ— æ³•ä¿æŒå¾®å¦™çš„é¢éƒ¨ç»†èŠ‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UnAvgLipæ–¹æ³•ï¼Œå®ƒé€šè¿‡æå–å‚è€ƒè§†é¢‘çš„èº«ä»½åµŒå…¥æ¥ç”Ÿæˆé«˜åº¦çœŸå®çš„é¢éƒ¨åºåˆ—ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®çš„å”‡éƒ¨åŒæ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£åŸºäºçš„å”‡åŒæ­¥ç”Ÿæˆæ¨¡å‹èƒ½åˆ¶ä½œé«˜åº¦åŒæ­¥çš„è¯´è¯é¢éƒ¨è§†é¢‘ï¼Œä½†ç»´æŒé¢éƒ¨ç»†èŠ‚æœ‰éš¾åº¦ã€‚</li>
<li>å‡ºç°â€œå”‡éƒ¨å¹³å‡åŒ–â€ç°è±¡ï¼Œå³åœ¨ä¸ºæœªè§è§†é¢‘é…éŸ³æ—¶ï¼Œæ¨¡å‹æ— æ³•ä¿ç•™å¾®å¦™çš„é¢éƒ¨ç»†èŠ‚ã€‚</li>
<li>UnAvgLipæ–¹æ³•é€šè¿‡æå–å‚è€ƒè§†é¢‘çš„èº«ä»½åµŒå…¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>UnAvgLipåŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šIdentity Perceiveræ¨¡å—å’ŒID-CrossAttnæ¨¡å—ã€‚</li>
<li>Identity Perceiveræ¨¡å—ç¼–ç é¢éƒ¨åµŒå…¥ï¼Œä¸æ¡ä»¶éŸ³é¢‘ç‰¹å¾å¯¹é½ã€‚</li>
<li>ID-CrossAttnæ¨¡å—å°†é¢éƒ¨åµŒå…¥æ³¨å…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œå¢å¼ºäº†æ¨¡å‹ä¿ç•™èº«ä»½çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06397">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdeb4d8587baeeb30f8abf54a9e1a5c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60bbd9fa079b48d747c922c14424aa5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-022041c06f04a9af96144a48618c1db1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ARTalk-Speech-Driven-3D-Head-Animation-via-Autoregressive-Model"><a href="#ARTalk-Speech-Driven-3D-Head-Animation-via-Autoregressive-Model" class="headerlink" title="ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model"></a>ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model</h2><p><strong>Authors:Xuangeng Chu, Nabarun Goswami, Ziteng Cui, Hanqin Wang, Tatsuya Harada</strong></p>
<p>Speech-driven 3D facial animation aims to generate realistic lip movements and facial expressions for 3D head models from arbitrary audio clips. Although existing diffusion-based methods are capable of producing natural motions, their slow generation speed limits their application potential. In this paper, we introduce a novel autoregressive model that achieves real-time generation of highly synchronized lip movements and realistic head poses and eye blinks by learning a mapping from speech to a multi-scale motion codebook. Furthermore, our model can adapt to unseen speaking styles, enabling the creation of 3D talking avatars with unique personal styles beyond the identities seen during training. Extensive evaluations and user studies demonstrate that our method outperforms existing approaches in lip synchronization accuracy and perceived quality. </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨ä»ä»»æ„éŸ³é¢‘ç‰‡æ®µä¸­ä¸º3Då¤´éƒ¨æ¨¡å‹ç”Ÿæˆé€¼çœŸçš„å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨è¡¨æƒ…ã€‚å°½ç®¡ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿè‡ªç„¶è¿åŠ¨ï¼Œä½†å…¶ç¼“æ…¢çš„ç”Ÿæˆé€Ÿåº¦é™åˆ¶äº†å…¶åº”ç”¨æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹è‡ªå›å½’æ¨¡å‹ï¼Œé€šè¿‡å­¦ä¹ ä¸å¤šå°ºåº¦è¿åŠ¨å­—å…¸ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œå®ç°é«˜åº¦åŒæ­¥çš„å˜´å”‡åŠ¨ä½œå’Œé€¼çœŸçš„å¤´éƒ¨å§¿åŠ¿ä»¥åŠçœ¨çœ¼ç­‰åŠ¨ä½œçš„å®æ—¶ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿé€‚åº”æœªè§è¿‡çš„è¯´è¯é£æ ¼ï¼Œä½¿èƒ½å¤Ÿåˆ›å»ºå…·æœ‰ç‹¬ç‰¹ä¸ªäººé£æ ¼çš„3Då¯¹è¯è§’è‰²ï¼Œè€Œä¸ä»…ä»…æ˜¯è®­ç»ƒæœŸé—´è§è¿‡çš„èº«ä»½ã€‚å…¨é¢çš„è¯„ä¼°å’Œç”¨æˆ·ç ”ç©¶è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å˜´å”‡åŒæ­¥ç²¾åº¦å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20323v5">PDF</a> SIGGRAPH Asia 2025, More video demonstrations, code, models and data   can be found on our project website: <a target="_blank" rel="noopener" href="http://xg-chu.site/project_artalk/">http://xg-chu.site/project_artalk/</a></p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªå›å½’æ¨¡å‹çš„å®æ—¶è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨å¤šå°ºåº¦è¿åŠ¨ä»£ç æœ¬å®ç°é«˜åº¦åŒæ­¥çš„å”‡åŠ¨å’Œé€¼çœŸçš„å¤´éƒ¨å§¿æ€ä»¥åŠçœ¨çœ¼åŠ¨ä½œï¼Œå¹¶èƒ½é€‚åº”æœªè§è¿‡çš„è¯´è¯é£æ ¼ï¼Œåˆ›å»ºå…·æœ‰ç‹¬ç‰¹ä¸ªæ€§çš„3Då¤´åƒã€‚æ­¤æ–¹æ³•åœ¨å”‡åŒæ­¥å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æŠ€æœ¯å®ç°äº†åŸºäºè‡ªå›å½’æ¨¡å‹çš„å®æ—¶è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»ã€‚</li>
<li>é€šè¿‡å­¦ä¹ ä»è¯­éŸ³åˆ°å¤šå°ºåº¦è¿åŠ¨ä»£ç æœ¬çš„æ˜ å°„ï¼Œç”Ÿæˆé«˜åº¦åŒæ­¥çš„å”‡åŠ¨ã€‚</li>
<li>æŠ€æœ¯èƒ½å¤Ÿäº§ç”Ÿé€¼çœŸçš„å¤´éƒ¨å§¿æ€å’Œçœ¨çœ¼åŠ¨ä½œã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è¯´è¯é£æ ¼ï¼Œåˆ›å»ºå…·æœ‰ç‹¬ç‰¹ä¸ªæ€§çš„3Då¤´åƒã€‚</li>
<li>è¯¥æŠ€æœ¯åœ¨å”‡åŒæ­¥å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æŠ€æœ¯æä¾›äº†æ›´é«˜çš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-686ff4b274424be396e16ebdc1cf7c94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c1b345f44a5d85d30309f49e6634bd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c24bf308a7cdcb60c26dff5966abc9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e864805977eae35332b2b10def55dfb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-424838aab6b94b28f984c6e468937e45.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Talk-Listen-Connect-How-Humans-and-AI-Evaluate-Empathy-in-Responses-to-Emotionally-Charged-Narratives"><a href="#Talk-Listen-Connect-How-Humans-and-AI-Evaluate-Empathy-in-Responses-to-Emotionally-Charged-Narratives" class="headerlink" title="Talk, Listen, Connect: How Humans and AI Evaluate Empathy in Responses   to Emotionally Charged Narratives"></a>Talk, Listen, Connect: How Humans and AI Evaluate Empathy in Responses   to Emotionally Charged Narratives</h2><p><strong>Authors:Mahnaz Roshanaei, Rezvaneh Rezapour, Magy Seif El-Nasr</strong></p>
<p>Social interactions promote well-being, yet barriers like geographic distance, time limitations, and mental health conditions can limit face-to-face interactions. Emotionally responsive AI systems, such as chatbots, offer new opportunities for social and emotional support, but raise critical questions about how empathy is perceived and experienced in human-AI interactions. This study examines how empathy is evaluated in AI-generated versus human responses. Using personal narratives, we explored how persona attributes (e.g., gender, empathic traits, shared experiences) and story qualities affect empathy ratings. We compared responses from standard and fine-tuned AI models with human judgments. Results show that while humans are highly sensitive to emotional vividness and shared experience, AI-responses are less influenced by these cues, often lack nuance in empathic expression. These findings highlight challenges in designing emotionally intelligent systems that respond meaningfully across diverse users and contexts, and informs the design of ethically aware tools to support social connection and well-being. </p>
<blockquote>
<p>ç¤¾ä¼šäº’åŠ¨æœ‰åŠ©äºä¿ƒè¿›ç¦ç¥‰ï¼Œç„¶è€Œï¼Œåœ°ç†è·ç¦»ã€æ—¶é—´é™åˆ¶å’Œå¿ƒç†å¥åº·çŠ¶å†µç­‰éšœç¢é™åˆ¶äº†é¢å¯¹é¢çš„äº’åŠ¨ã€‚æƒ…æ„Ÿå“åº”çš„AIç³»ç»Ÿï¼Œå¦‚èŠå¤©æœºå™¨äººï¼Œä¸ºç¤¾ä¼šå’Œæƒ…æ„Ÿæ”¯æŒæä¾›äº†æ–°çš„æœºä¼šï¼Œä½†ä¹Ÿå¼•å‘äº†å…³äºäººç±»ä¸AIäº’åŠ¨ä¸­å¦‚ä½•æ„ŸçŸ¥å’Œä½“éªŒå…±æƒ…çš„å…³é”®é—®é¢˜ã€‚æœ¬ç ”ç©¶è€ƒå¯Ÿäº†AIç”Ÿæˆå“åº”ä¸äººç±»å“åº”ä¸­çš„å…±æƒ…è¯„ä¼°æ–¹å¼ã€‚é€šè¿‡ä¸ªäººå™äº‹ï¼Œæˆ‘ä»¬æ¢è®¨äº†äººæ ¼å±æ€§ï¼ˆå¦‚æ€§åˆ«ã€å…±æƒ…ç‰¹è´¨ã€å…±äº«ç»éªŒï¼‰å’Œæ•…äº‹è´¨é‡å¦‚ä½•å½±å“å…±æƒ…è¯„çº§ã€‚æˆ‘ä»¬å°†æ ‡å‡†AIæ¨¡å‹å’Œç»è¿‡å¾®è°ƒåçš„AIæ¨¡å‹çš„å“åº”ä¸äººç±»åˆ¤æ–­è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶äººç±»å¯¹æƒ…ç»ªç”ŸåŠ¨æ€§å’Œå…±äº«ç»éªŒé«˜åº¦æ•æ„Ÿï¼Œä½†AIçš„å“åº”å—è¿™äº›çº¿ç´¢çš„å½±å“è¾ƒå°ï¼Œå¾€å¾€ç¼ºä¹å¾®å¦™çš„å…±æƒ…è¡¨è¾¾ã€‚è¿™äº›å‘ç°çªæ˜¾äº†åœ¨è®¾è®¡å’Œæƒ…æ„Ÿæ™ºèƒ½ç³»ç»Ÿæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œè¿™äº›ç³»ç»Ÿéœ€è¦åœ¨ä¸åŒçš„ç”¨æˆ·å’ŒèƒŒæ™¯ä¸‹åšå‡ºæœ‰æ„ä¹‰çš„å“åº”ï¼Œå¹¶ä¸ºæ”¯æŒç¤¾ä¼šè”ç³»å’Œç¦ç¥‰çš„ä¼¦ç†æ„è¯†å·¥å…·çš„è®¾è®¡æä¾›äº†ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15550v2">PDF</a> 21 pages, 4 figures, 6 tables. Title updated from â€œTalk, Listen,   Connect: Navigating Empathy in Human-AI Interactionsâ€ to â€œTalk, Listen,   Connect: How Humans and AI Evaluate Empathy in Responses to Emotionally   Charged Narrativesâ€ in this version. This is version 2 (v2) of the paper. All   previous citations of arXiv:2409.15550 with the old title still refer to the   same paper</p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººç­‰æƒ…æ„Ÿå“åº”ç³»ç»Ÿä¸ºç¤¾ä¼šäº’åŠ¨æä¾›äº†æ–°çš„æœºä¼šï¼Œä½†äººä»¬å¯¹å…¶ä¸­çš„å…±æƒ…ä½“éªŒæ„ŸçŸ¥å­˜åœ¨è´¨ç–‘ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸ªäººå™äº‹ï¼Œæ¢è®¨äº†äººæ ¼å±æ€§ï¼ˆå¦‚æ€§åˆ«ã€å…±æƒ…ç‰¹è´¨ã€å…±åŒç»å†ï¼‰å’Œæ•…äº‹è´¨é‡å¦‚ä½•å½±å“å…±æƒ…è¯„ä»·ã€‚å¯¹æ¯”äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ ‡å‡†å“åº”å’Œç²¾ç»†è°ƒæ•´å“åº”ä¸äººçš„åˆ¤æ–­ï¼Œå‘ç°äººç±»å¯¹äºæƒ…æ„Ÿçš„ç”ŸåŠ¨æ€§å’Œå…±åŒç»å†é«˜åº¦æ•æ„Ÿï¼Œè€Œäººå·¥æ™ºèƒ½çš„å“åº”åˆ™è¾ƒå°‘å—åˆ°è¿™äº›çº¿ç´¢çš„å½±å“ï¼Œå¾€å¾€ç¼ºä¹å¾®å¦™çš„å…±æƒ…è¡¨è¾¾ã€‚è¿™ä¸ºè®¾è®¡èƒ½å¤Ÿè·¨ä¸åŒç”¨æˆ·å’Œæƒ…å¢ƒåšå‡ºæœ‰æ„ä¹‰å“åº”çš„æƒ…æ„Ÿæ™ºèƒ½ç³»ç»Ÿå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œä¹Ÿä¸ºæ”¯æŒç¤¾ä¼šè”ç³»å’Œç¦ç¥‰çš„é“å¾·å·¥å…·çš„è®¾è®¡æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾ä¼šäº’åŠ¨å¯¹ç¦ç¥‰æœ‰ç§¯æå½±å“ï¼Œä½†åœ°ç†è·ç¦»ã€æ—¶é—´é™åˆ¶å’Œç²¾ç¥å¥åº·ç­‰éšœç¢é™åˆ¶äº†é¢å¯¹é¢çš„äº¤æµã€‚</li>
<li>æƒ…æ„Ÿå“åº”çš„AIç³»ç»Ÿå¦‚èŠå¤©æœºå™¨äººä¸ºç¤¾ä¼šå’Œæƒ…æ„Ÿæ”¯æŒæä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>äººç±»å¯¹äºæƒ…æ„Ÿçš„ç”ŸåŠ¨æ€§å’Œå…±åŒç»å†é«˜åº¦æ•æ„Ÿï¼Œè€ŒAIå“åº”å—å…¶å½±å“è¾ƒå°ã€‚</li>
<li>AIåœ¨è¡¨è¾¾å…±æƒ…æ—¶å¸¸å¸¸ç¼ºä¹ç»†å¾®å·®åˆ«ã€‚</li>
<li>è®¾è®¡æƒ…æ„Ÿæ™ºèƒ½ç³»ç»Ÿæ—¶éœ€è¦è€ƒè™‘è·¨ä¸åŒç”¨æˆ·å’Œæƒ…å¢ƒçš„æœ‰æ„ä¹‰å“åº”ã€‚</li>
<li>éœ€è¦è®¾è®¡é“å¾·å·¥å…·ä»¥æ”¯æŒç¤¾ä¼šè”ç³»å’Œç¦ç¥‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15550">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bccac1779a86eb1b5eaebc2e31f1cec0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-956b2fd9c1e9fcd3c5f38c31e14ea825.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8be3d509100261e1c7c293545aba3251.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="JambaTalk-Speech-Driven-3D-Talking-Head-Generation-Based-on-Hybrid-Transformer-Mamba-Model"><a href="#JambaTalk-Speech-Driven-3D-Talking-Head-Generation-Based-on-Hybrid-Transformer-Mamba-Model" class="headerlink" title="JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid   Transformer-Mamba Model"></a>JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid   Transformer-Mamba Model</h2><p><strong>Authors:Farzaneh Jafari, Stefano Berretti, Anup Basu</strong></p>
<p>In recent years, the talking head generation has become a focal point for researchers. Considerable effort is being made to refine lip-sync motion, capture expressive facial expressions, generate natural head poses, and achieve high-quality video. However, no single model has yet achieved equivalence across all quantitative and qualitative metrics. We introduce Jamba, a hybrid Transformer-Mamba model, to animate a 3D face. Mamba, a pioneering Structured State Space Model (SSM) architecture, was developed to overcome the limitations of conventional Transformer architectures, particularly in handling long sequences. This challenge has constrained traditional models. Jamba combines the advantages of both the Transformer and Mamba approaches, offering a comprehensive solution. Based on the foundational Jamba block, we present JambaTalk to enhance motion variety and lip sync through multimodal integration. Extensive experiments reveal that our method achieves performance comparable or superior to state-of-the-art models. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè¯´è¯äººå¤´éƒ¨ç”Ÿæˆå·²æˆä¸ºç ”ç©¶äººå‘˜çš„å…³æ³¨ç‚¹ã€‚äººä»¬æ­£åœ¨ä»˜å‡ºç›¸å½“å¤§çš„åŠªåŠ›æ¥æ”¹è¿›å”‡éƒ¨åŒæ­¥åŠ¨ä½œï¼Œæ•æ‰é¢éƒ¨è¡¨æƒ…ï¼Œç”Ÿæˆè‡ªç„¶å¤´éƒ¨å§¿åŠ¿ï¼Œå¹¶å®ç°é«˜è´¨é‡è§†é¢‘ã€‚ç„¶è€Œï¼Œç›®å‰è¿˜æ²¡æœ‰ä»»ä½•å•ä¸€æ¨¡å‹èƒ½å¤Ÿåœ¨æ‰€æœ‰å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šå®ç°ç­‰æ•ˆã€‚æˆ‘ä»¬å¼•å…¥äº†Jambaï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆçš„Transformer-Mambaæ¨¡å‹ï¼Œç”¨äºé©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»ã€‚Mambaæ˜¯ä¸€ç§å¼€åˆ›æ€§çš„ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰æ¶æ„ï¼Œæ—¨åœ¨å…‹æœä¼ ç»ŸTransformeræ¶æ„çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿åºåˆ—æ–¹é¢çš„æŒ‘æˆ˜ã€‚Jambaç»“åˆäº†Transformerå’ŒMambaæ–¹æ³•çš„ä¼˜ç‚¹ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è§£å†³æ–¹æ¡ˆã€‚åŸºäºåŸºæœ¬çš„Jambaæ¨¡å—ï¼Œæˆ‘ä»¬æ¨å‡ºäº†JambaTalkï¼Œé€šè¿‡å¤šæ¨¡å¼èåˆå¢å¼ºè¿åŠ¨å¤šæ ·æ€§å’Œå”‡éƒ¨åŒæ­¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„æ¨¡å‹æ€§èƒ½ç›¸å½“æˆ–æ›´ä¼˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01627v2">PDF</a> 23 pages with 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯´è¯äººå¤´éƒ¨ç”Ÿæˆçš„ç ”ç©¶è¿›å±•ï¼Œå¹¶å¼•å…¥äº†åä¸ºJambaçš„æ··åˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†Transformerå’ŒMambaä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œç”¨äºåŠ¨ç”»åŒ–3Dé¢éƒ¨ã€‚Mambaæ˜¯ä¸€ç§æ–°å‹çš„SSMæ¶æ„ï¼Œæ—¨åœ¨å…‹æœä¼ ç»ŸTransformeråœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„å±€é™æ€§ã€‚JambaTalkçš„æå‡ºå¢å¼ºäº†è¿åŠ¨å¤šæ ·æ€§å’Œå”‡åŒæ­¥æ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸å½“æˆ–æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯´è¯äººå¤´éƒ¨ç”Ÿæˆå·²æˆä¸ºç ”ç©¶ç„¦ç‚¹ï¼Œé›†ä¸­äºæ”¹è¿›å”‡åŒæ­¥è¿åŠ¨ã€é¢éƒ¨è¡¨æƒ…æ•æ‰ã€è‡ªç„¶å¤´éƒ¨å§¿æ€ç”ŸæˆåŠé«˜è´¨é‡è§†é¢‘ã€‚</li>
<li>å½“å‰å°šæœªæœ‰å•ä¸€æ¨¡å‹åœ¨æ‰€æœ‰å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šå®ç°ç­‰æ•ˆæ€§ã€‚</li>
<li>å¼•å…¥Jambaæ¨¡å‹ï¼Œç»“åˆäº†Transformerå’ŒMambaä¸¤ç§æ–¹æ³•ï¼Œä»¥åŠ¨ç”»åŒ–3Dé¢éƒ¨ã€‚</li>
<li>Mambaæ˜¯ä¸€ç§æ–°å‹çš„SSMæ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸTransformerå¤„ç†é•¿åºåˆ—æ—¶çš„å±€é™æ€§ã€‚</li>
<li>JambaTalkå¢å¼ºäº†è¿åŠ¨å¤šæ ·æ€§å’Œå”‡åŒæ­¥æ•ˆæœã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºJambaæ¨¡å‹æ€§èƒ½ä¼˜è¶Šï¼Œä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-207ef8a6efc196121f0d62d75a836ee6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf59206e9f2b39bfe8fefb88b32d14cb.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-61193cbc365b65378c62dd7ebbee2363.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  ArcMemo Abstract Reasoning Composition with Lifelong LLM Memory
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-26/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-464a4a37ca3512d844e052a097c93086.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-26  ANT Adaptive Neural Temporal-Aware Text-to-Motion Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
