<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  Durian Dual Reference-guided Portrait Animation with Attribute Transfer">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f0820592e385239ef4b2825c3a2a9c03.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-07-æ›´æ–°"><a href="#2025-09-07-æ›´æ–°" class="headerlink" title="2025-09-07 æ›´æ–°"></a>2025-09-07 æ›´æ–°</h1><h2 id="Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer"><a href="#Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer" class="headerlink" title="Durian: Dual Reference-guided Portrait Animation with Attribute Transfer"></a>Durian: Dual Reference-guided Portrait Animation with Attribute Transfer</h2><p><strong>Authors:Hyunsoo Cha, Byungjun Kim, Hanbyul Joo</strong></p>
<p>We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Durianæ–¹æ³•ï¼Œè¿™æ˜¯ç¬¬ä¸€ç§é›¶æ ·æœ¬æ–¹å¼ç”Ÿæˆè‚–åƒåŠ¨ç”»è§†é¢‘çš„æ–¹æ³•ï¼Œå¯ä»¥å°†ç»™å®šå‚è€ƒå›¾åƒçš„é¢éƒ¨å±æ€§è½¬ç§»åˆ°ç›®æ ‡è‚–åƒä¸Šã€‚ä¸ºäº†å®ç°è·¨å¸§çš„é«˜ä¿çœŸå’Œç©ºé—´ä¸€è‡´çš„å±æ€§è½¬ç§»ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒå‚è€ƒç½‘ç»œï¼Œå®ƒå°†è‚–åƒå’Œå±æ€§å›¾åƒçš„ç©ºé—´ç‰¹å¾æ³¨å…¥æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨è‡ªæˆ‘é‡å»ºå…¬å¼å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»åŒä¸€è‚–åƒè§†é¢‘ä¸­é‡‡æ ·ä¸¤å¸§ï¼šä¸€å¸§ä½œä¸ºå±æ€§å‚è€ƒï¼Œå¦ä¸€å¸§ä½œä¸ºç›®æ ‡è‚–åƒï¼Œç„¶åæ ¹æ®è¿™äº›è¾“å…¥åŠå…¶ç›¸åº”çš„æ©ç å¯¹å‰©ä½™å¸§è¿›è¡Œé‡å»ºã€‚ä¸ºäº†æ”¯æŒå…·æœ‰ä¸åŒç©ºé—´èŒƒå›´çš„å±æ€§è½¬ç§»ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å…³é”®ç‚¹æ¡ä»¶å›¾åƒç”Ÿæˆçš„æ©è†œæ‰©å±•ç­–ç•¥è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨ç©ºé—´å’Œå¤–è§‚çº§åˆ«çš„å˜æ¢æ¥å¢å¼ºå±æ€§å’Œè‚–åƒå›¾åƒï¼Œä»¥æé«˜å®ƒä»¬ä¹‹é—´ä½ç½®ä¸å¯¹é½çš„é²æ£’æ€§ã€‚è¿™äº›ç­–ç•¥ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„å±æ€§å’Œé‡å¤–å‚è€ƒç»„åˆä¸­æœ‰æ•ˆæ¨å¹¿ï¼Œå³ä½¿åœ¨æ²¡æœ‰æ˜ç¡®çš„ä¸‰å…ƒç»„ç›‘ç£çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚Durianåœ¨å¸¦æœ‰å±æ€§è½¬ç§»çš„è‚–åƒåŠ¨ç”»æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå…¶åŒå‚è€ƒè®¾è®¡èƒ½å¤Ÿåœ¨å•æ¬¡ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°å¤šå±æ€§ç»„åˆï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04434v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://hyunsoocha.github.io/durian">https://hyunsoocha.github.io/durian</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åä¸ºDuriançš„æ–¹æ³•ï¼Œå®ƒèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ä»ç»™å®šçš„å‚è€ƒå›¾åƒå‘ç›®æ ‡è‚–åƒç”ŸæˆåŠ¨ç”»è§†é¢‘ã€‚é€šè¿‡å¼•å…¥åŒé‡å‚è€ƒç½‘ç»œï¼Œå®ç°äº†è·¨å¸§çš„é«˜ä¿çœŸå’Œç©ºé—´ä¸€è‡´æ€§çš„å±æ€§è½¬ç§»ã€‚é‡‡ç”¨è‡ªé‡å»ºè®­ç»ƒæ¨¡å¼ï¼Œé€šè¿‡å…³é”®ç‚¹çš„æ¡ä»¶å›¾åƒç”Ÿæˆè¿›è¡Œæ©è†œæ‰©å±•ç­–ç•¥æ”¯æŒä¸åŒç©ºé—´èŒƒå›´çš„å±æ€§è½¬ç§»ã€‚é€šè¿‡å¢å¼ºå±æ€§å’Œè‚–åƒå›¾åƒçš„ç©ºé—´å’Œå¤–è§‚çº§åˆ«çš„è½¬æ¢ï¼Œæé«˜äº†å¯¹å®ƒä»¬ä¹‹é—´ä½ç½®ä¸åŒ¹é…é—®é¢˜çš„ç¨³å¥æ€§ã€‚å°½ç®¡æ²¡æœ‰æ˜ç¡®çš„ä¸‰ä¸ªæ ·æœ¬ç›‘ç£ï¼Œä½†Durianåœ¨è‚–åƒåŠ¨ç”»å±æ€§è½¬ç§»æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…¶åŒé‡å‚è€ƒè®¾è®¡èƒ½å¤Ÿåœ¨å•æ¬¡ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°å¤šå±æ€§ç»„åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Durianæ˜¯ç¬¬ä¸€ç§èƒ½å¤Ÿåœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹å®ç°ä»ç»™å®šå‚è€ƒå›¾åƒå‘ç›®æ ‡è‚–åƒç”ŸæˆåŠ¨ç”»è§†é¢‘çš„æ–¹æ³•ã€‚</li>
<li>åŒé‡å‚è€ƒç½‘ç»œè¢«å¼•å…¥ä»¥å®ç°é«˜ä¿çœŸå’Œç©ºé—´ä¸€è‡´æ€§çš„è·¨å¸§å±æ€§è½¬ç§»ã€‚</li>
<li>é‡‡ç”¨è‡ªé‡å»ºè®­ç»ƒæ¨¡å¼ï¼Œé€šè¿‡ç›¸åŒè‚–åƒè§†é¢‘é‡‡æ ·çš„ä¸¤ä¸ªå¸§ï¼Œä¸€ä¸ªä½œä¸ºå±æ€§å‚è€ƒï¼Œå¦ä¸€ä¸ªä½œä¸ºç›®æ ‡è‚–åƒï¼Œè¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¼•å…¥æ©è†œæ‰©å±•ç­–ç•¥ï¼Œæ”¯æŒä¸åŒç©ºé—´èŒƒå›´çš„å±æ€§è½¬ç§»ï¼Œå¹¶é€šè¿‡å…³é”®ç‚¹çš„æ¡ä»¶å›¾åƒç”Ÿæˆè¿›è¡Œè®­ç»ƒã€‚</li>
<li>é€šè¿‡å¢å¼ºå±æ€§å’Œè‚–åƒå›¾åƒçš„ç©ºé—´å’Œå¤–è§‚çº§åˆ«çš„è½¬æ¢ï¼Œæé«˜æ¨¡å‹ç¨³å¥æ€§ï¼Œåº”å¯¹ä½ç½®ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>Durianåœ¨è‚–åƒåŠ¨ç”»å±æ€§è½¬ç§»æ–¹é¢è¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-98881fef7d5ef8229a6de43a9d9f557e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-209b81a67b327f51bdad9e4c00483453.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f3e6d8612c12c04da96b87ce1d60057.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SSGaussian-Semantic-Aware-and-Structure-Preserving-3D-Style-Transfer"><a href="#SSGaussian-Semantic-Aware-and-Structure-Preserving-3D-Style-Transfer" class="headerlink" title="SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer"></a>SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</h2><p><strong>Authors:Jimin Xu, Bosheng Qin, Tao Jin, Zhou Zhao, Zhenhui Ye, Jun Yu, Fei Wu</strong></p>
<p>Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page <a target="_blank" rel="noopener" href="https://jm-xu.github.io/SSGaussian">https://jm-xu.github.io/SSGaussian</a> for immersive visualization. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç¥ç»è¡¨å¾é¢†åŸŸçš„è¿›å±•ï¼Œå¦‚ç¥ç»è¾å°„åœºå’Œ3Dé«˜æ–¯å–·æ¶‚ç­‰æŠ€æœ¯ï¼Œå¢åŠ äº†å°†é£æ ¼è¿ç§»åº”ç”¨äº3Dåœºæ™¯çš„å…´è¶£ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•èƒ½å¤Ÿå°†é£æ ¼æ¨¡å¼è½¬ç§»åˆ°ä¸€è‡´çš„3Dç¥ç»è¡¨å¾ä¸Šï¼Œä½†å®ƒä»¬éš¾ä»¥æœ‰æ•ˆåœ°ä»å‚è€ƒé£æ ¼å›¾åƒä¸­æå–å¹¶è½¬ç§»é«˜çº§é£æ ¼è¯­ä¹‰ã€‚æ­¤å¤–ï¼Œé£æ ¼åŒ–çš„ç»“æœé€šå¸¸ç¼ºä¹ç»“æ„æ¸…æ™°åº¦å’Œåˆ†ç¦»åº¦ï¼Œä½¿å¾—éš¾ä»¥åœ¨3Dåœºæ™¯ä¸­åŒºåˆ†ä¸åŒçš„å®ä¾‹æˆ–å¯¹è±¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dé£æ ¼è¿ç§»ç®¡é“ï¼Œè¯¥ç®¡é“æœ‰æ•ˆåœ°æ•´åˆäº†æ¥è‡ªé¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬çš„ç®¡é“ç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£å…ˆéªŒçŸ¥è¯†ç”Ÿæˆå…³é”®è§†ç‚¹çš„é£æ ¼åŒ–æ¸²æŸ“ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†é£æ ¼åŒ–çš„å…³é”®è§†å›¾è½¬ç§»åˆ°3Dè¡¨å¾ä¸Šã€‚è¿™ä¸€è¿‡ç¨‹é‡‡ç”¨äº†ä¸¤é¡¹åˆ›æ–°è®¾è®¡ã€‚é¦–å…ˆæ˜¯è·¨è§†å›¾é£æ ¼å¯¹é½ï¼Œå®ƒå°†è·¨è§†å›¾æ³¨æ„åŠ›æ’å…¥åˆ°UNetçš„æœ€åä¸€ä¸ªä¸Šé‡‡æ ·å—ä¸­ï¼Œå…è®¸è·¨å¤šä¸ªå…³é”®è§†å›¾è¿›è¡Œç‰¹å¾äº¤äº’ã€‚è¿™ç¡®ä¿æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é£æ ¼åŒ–å…³é”®è§†å›¾æ—¢ä¿æŒé£æ ¼å¿ å®åº¦åˆä¿æŒå®ä¾‹çº§ä¸€è‡´æ€§ã€‚å…¶æ¬¡æ˜¯å®ä¾‹çº§é£æ ¼è¿ç§»ï¼Œå®ƒæœ‰æ•ˆåœ°åˆ©ç”¨é£æ ¼åŒ–å…³é”®è§†å›¾ä¹‹é—´çš„å®ä¾‹çº§ä¸€è‡´æ€§ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ°3Dè¡¨å¾ä¸Šã€‚è¿™å¯¼è‡´äº†ä¸€ç§æ›´å…·ç»“æ„ã€è§†è§‰è¿è´¯æ€§å’Œè‰ºæœ¯ä¸°å¯Œæ€§çš„é£æ ¼åŒ–ã€‚å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„3Dé£æ ¼è¿ç§»ç®¡é“åœ¨å„ç§åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„ç®¡é“ï¼Œæ— è®ºæ˜¯æ­£é¢åœºæ™¯è¿˜æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„360åº¦ç¯å¢ƒã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jm-xu.github.io/SSGaussian">é“¾æ¥åœ°å€</a>ä»¥è·å–æ²‰æµ¸å¼å¯è§†åŒ–ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04379v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸç¥ç»ç½‘ç»œè¡¨ç¤ºæŠ€æœ¯å¦‚Neural Radiance Fieldså’Œ3D Gaussian Splattingçš„å‘å±•ï¼Œå¼•å‘äº†å°†é£æ ¼è¿ç§»åº”ç”¨äº3Dåœºæ™¯çš„å…´è¶£ã€‚ç°æœ‰æ–¹æ³•è™½èƒ½å°†é£æ ¼æ¨¡å¼è½¬ç§»åˆ°3Dä¸€è‡´çš„ç¥ç»ç½‘ç»œè¡¨ç¤ºä¸Šï¼Œä½†éš¾ä»¥æœ‰æ•ˆæå–å’Œè½¬ç§»å‚è€ƒé£æ ¼å›¾åƒçš„é«˜çº§è¯­ä¹‰ã€‚æ­¤å¤–ï¼Œé£æ ¼åŒ–çš„ç»“æœå¾€å¾€ç¼ºä¹ç»“æ„æ¸…æ™°åº¦å’Œåˆ†ç¦»åº¦ï¼Œéš¾ä»¥åŒºåˆ†3Dåœºæ™¯ä¸­çš„ä¸åŒå®ä¾‹æˆ–å¯¹è±¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°é¢–çš„3Dé£æ ¼è¿ç§»ç®¡é“ï¼Œæœ‰æ•ˆæ•´åˆäº†é¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚è¯¥ç®¡é“åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£å…ˆéªŒç”Ÿæˆå…³é”®è§†è§’çš„é£æ ¼åŒ–æ¸²æŸ“ï¼›ç„¶åï¼Œå°†è¿™äº›é£æ ¼åŒ–çš„å…³é”®è§†å›¾è½¬ç§»åˆ°3Dè¡¨ç¤ºä¸Šã€‚è¿™ä¸€è¿‡ç¨‹èå…¥äº†ä¸¤é¡¹åˆ›æ–°è®¾è®¡ã€‚é¦–å…ˆæ˜¯è·¨è§†å›¾é£æ ¼å¯¹é½ï¼Œå°†è·¨è§†å›¾æ³¨æ„åŠ›æ’å…¥UNetçš„æœ€åä¸€ä¸ªä¸Šé‡‡æ ·å—ä¸­ï¼Œå…è®¸è·¨å¤šä¸ªå…³é”®è§†å›¾è¿›è¡Œç‰¹å¾äº¤äº’ã€‚è¿™ç¡®ä¿æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é£æ ¼åŒ–å…³é”®è§†å›¾æ—¢ä¿æŒé£æ ¼å¿ å®åº¦åˆä¿æŒå®ä¾‹çº§ä¸€è‡´æ€§ã€‚å…¶æ¬¡æ˜¯å®ä¾‹çº§é£æ ¼è¿ç§»ï¼Œå®ƒæœ‰æ•ˆåœ°åˆ©ç”¨é£æ ¼åŒ–å…³é”®è§†å›¾ä¹‹é—´çš„å®ä¾‹çº§ä¸€è‡´æ€§ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ°3Dè¡¨ç¤ºä¸Šã€‚è¿™äº§ç”Ÿäº†ä¸€ç§æ›´å…·ç»“æ„ã€è§†è§‰è¿è´¯ä¸”è‰ºæœ¯æ„Ÿæ›´å¼ºçš„é£æ ¼åŒ–ã€‚å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„3Dé£æ ¼è¿ç§»ç®¡é“åœ¨å¤šç§åœºæ™¯ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ­£é¢å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„360åº¦ç¯å¢ƒã€‚æ²‰æµ¸å¼å¯è§†åŒ–è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jm-xu.github.io/SSGaussian">https://jm-xu.github.io/SSGaussian</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰3Dé£æ ¼è¿ç§»æ–¹æ³•åœ¨æå–å’Œè½¬ç§»é«˜çº§é£æ ¼è¯­ä¹‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºçš„3Dé£æ ¼è¿ç§»ç®¡é“åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç”Ÿæˆé£æ ¼åŒ–æ¸²æŸ“çš„å…³é”®è§†è§’ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ°3Dè¡¨ç¤ºã€‚</li>
<li>ç®¡é“èå…¥äº†ä¸¤é¡¹åˆ›æ–°è®¾è®¡ï¼šè·¨è§†å›¾é£æ ¼å¯¹é½å’Œå®ä¾‹çº§é£æ ¼è¿ç§»ã€‚</li>
<li>è·¨è§†å›¾é£æ ¼å¯¹é½é€šè¿‡è·¨è§†å›¾æ³¨æ„åŠ›ç¡®ä¿é£æ ¼åŒ–å…³é”®è§†å›¾çš„é£æ ¼å¿ å®åº¦å’Œå®ä¾‹çº§ä¸€è‡´æ€§ã€‚</li>
<li>å®ä¾‹çº§é£æ ¼è¿ç§»åˆ©ç”¨é£æ ¼åŒ–å…³é”®è§†å›¾ä¹‹é—´çš„å®ä¾‹çº§ä¸€è‡´æ€§ï¼Œå®ç°æ›´å…·ç»“æ„ã€è§†è§‰è¿è´¯ä¸”è‰ºæœ¯æ„Ÿå¼ºçš„é£æ ¼åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤„ç†æ­£é¢å’Œ360åº¦ç¯å¢ƒç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ab201b9002d3043f3c2fa757a2486aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51fbc68971f9c30676eea8e9113a3621.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa55afde79abd9b66db45961165ae60b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5209a07d609d95b6437c0e0fb21acab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f2fc2a761bda7a794279a51053eaaf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0820592e385239ef4b2825c3a2a9c03.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MEPG-Multi-Expert-Planning-and-Generation-for-Compositionally-Rich-Image-Generation"><a href="#MEPG-Multi-Expert-Planning-and-Generation-for-Compositionally-Rich-Image-Generation" class="headerlink" title="MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image   Generation"></a>MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image   Generation</h2><p><strong>Authors:Yuan Zhao, Liu Lin</strong></p>
<p>Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality   and style diversity. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„å›¾åƒè´¨é‡ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´å¤æ‚ã€å¤šå…ƒç´ æç¤ºå’Œé£æ ¼å¤šæ ·æ€§æœ‰é™çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šä¸“å®¶è§„åˆ’å’Œç”Ÿæˆæ¡†æ¶ï¼ˆMEPGï¼‰ï¼Œè¯¥æ¡†æ¶ååŒæ•´åˆäº†ä½ç½®æ„ŸçŸ¥å’Œé£æ ¼æ„ŸçŸ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç©ºé—´è¯­ä¹‰ä¸“å®¶æ¨¡å—ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰ä½ç½®é£æ ¼æ„ŸçŸ¥ï¼ˆPSAï¼‰æ¨¡å—ï¼Œå®ƒåˆ©ç”¨ç›‘ç£å¾®è°ƒLLMå°†è¾“å…¥æç¤ºåˆ†è§£ä¸ºç²¾ç¡®çš„ç©ºé—´åæ ‡å’Œé£æ ¼ç¼–ç è¯­ä¹‰æŒ‡ä»¤ï¼›ï¼ˆ2ï¼‰å¤šä¸“å®¶æ‰©æ•£ï¼ˆMEDï¼‰æ¨¡å—ï¼Œé€šè¿‡å±€éƒ¨åŒºåŸŸå’Œå…¨å±€åŒºåŸŸçš„åŠ¨æ€ä¸“å®¶è·¯ç”±å®ç°è·¨åŒºåŸŸç”Ÿæˆã€‚é’ˆå¯¹æ¯ä¸ªå±€éƒ¨åŒºåŸŸçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œé€šè¿‡åŸºäºæ³¨æ„åŠ›çš„é—¨æ§æœºåˆ¶ï¼Œæœ‰é€‰æ‹©åœ°æ¿€æ´»ç‰¹æ®Šæ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œç°å®ä¸»ä¹‰è€…ä¸“å®¶ã€é£æ ¼åŒ–ä¸“å®¶ï¼‰ç­‰ã€‚è¯¥æ¶æ„æ”¯æŒä¸“å®¶æ¨¡å‹çš„è½»æ¾é›†æˆå’Œæ›¿æ¢ï¼Œå…·æœ‰å¾ˆå¼ºçš„å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œäº¤äº’å¼ç•Œé¢å…è®¸å®æ—¶ç©ºé—´å¸ƒå±€ç¼–è¾‘å’Œä»ä¸“å®¶ç»„åˆä¸­é€‰æ‹©æ¯ä¸ªåŒºåŸŸçš„é£æ ¼ã€‚å®éªŒè¡¨æ˜ï¼ŒMEPGåœ¨å›¾åƒè´¨é‡å’Œé£æ ¼å¤šæ ·æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºå…·æœ‰ç›¸åŒèƒŒæ™¯çš„åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04126v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬å†…å®¹å…³äºå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå…¶é¢ä¸´å¤æ‚å¤šå…ƒç´ æç¤ºå’Œé£æ ¼å¤šæ ·æ€§å—é™çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†å¤šä¸“å®¶è§„åˆ’å’Œç”Ÿæˆæ¡†æ¶ï¼ˆMEPGï¼‰ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä½ç½®æ„ŸçŸ¥å’Œé£æ ¼æ„ŸçŸ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç©ºé—´è¯­ä¹‰ä¸“å®¶æ¨¡å—ã€‚MEPGåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸€æ˜¯ä½ç½®é£æ ¼æ„ŸçŸ¥ï¼ˆPSAï¼‰æ¨¡å—ï¼Œç”¨äºå°†è¾“å…¥æç¤ºåˆ†è§£ä¸ºç²¾ç¡®çš„ç©ºé—´åæ ‡å’Œé£æ ¼ç¼–ç è¯­ä¹‰æŒ‡ä»¤ï¼›äºŒæ˜¯å¤šä¸“å®¶æ‰©æ•£ï¼ˆMEDï¼‰æ¨¡å—ï¼Œé€šè¿‡åŠ¨æ€ä¸“å®¶è·¯ç”±åœ¨æœ¬åœ°åŒºåŸŸå’Œå…¨å±€åŒºåŸŸä¹‹é—´å®ç°è·¨åŒºåŸŸç”Ÿæˆã€‚MEPGèƒ½å¤Ÿå®æ—¶ç¼–è¾‘ç©ºé—´å¸ƒå±€å¹¶ä¸ºæ¯ä¸ªåŒºåŸŸé€‰æ‹©ä¸“å®¶é£æ ¼ï¼Œå› æ­¤å…¶å›¾åƒè´¨é‡å’Œé£æ ¼å¤šæ ·æ€§å‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å†…å®¹ä¸»è¦ä»‹ç»äº†å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å¤æ‚å¤šå…ƒç´ æç¤ºå’Œé£æ ¼å¤šæ ·æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºå¤šä¸“å®¶è§„åˆ’å’Œç”Ÿæˆæ¡†æ¶ï¼ˆMEPGï¼‰çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œç©ºé—´è¯­ä¹‰ä¸“å®¶æ¨¡å—ã€‚</li>
<li>MEPGåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä½ç½®é£æ ¼æ„ŸçŸ¥æ¨¡å—ï¼ˆPSAï¼‰å’Œå¤šä¸“å®¶æ‰©æ•£æ¨¡å—ï¼ˆMEDï¼‰ã€‚</li>
<li>PSAæ¨¡å—èƒ½å¤Ÿå°†è¾“å…¥æç¤ºåˆ†è§£ä¸ºç²¾ç¡®çš„ç©ºé—´åæ ‡å’Œé£æ ¼ç¼–ç è¯­ä¹‰æŒ‡ä»¤ã€‚</li>
<li>MEDæ¨¡å—é€šè¿‡åŠ¨æ€ä¸“å®¶è·¯ç”±åœ¨æœ¬åœ°å’Œå…¨å±€åŒºåŸŸä¹‹é—´å®ç°è·¨åŒºåŸŸç”Ÿæˆï¼Œæé«˜å›¾åƒè´¨é‡å’Œé£æ ¼å¤šæ ·æ€§ã€‚</li>
<li>MEPGæ”¯æŒè½»æ¾é›†æˆå’Œæ›¿æ¢ä¸“å®¶æ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04126">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-21e1d487bbd028229bb2c2ba21908aff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58f0223d7df4471819594eaff8ac719f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d3d93bef6052d51b33bd70286c2d529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfedbb1471af6d48bda016079a40e79e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2244c2a3a7f5563fccdfa7612b656c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e5022de2e8f490d310b3fa9e7def9fc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Fitting-Image-Diffusion-Models-on-Video-Datasets"><a href="#Fitting-Image-Diffusion-Models-on-Video-Datasets" class="headerlink" title="Fitting Image Diffusion Models on Video Datasets"></a>Fitting Image Diffusion Models on Video Datasets</h2><p><strong>Authors:Juhun Lee, Simon S. Woo</strong></p>
<p>Image diffusion models are trained on independently sampled static images. While this is the bedrock task protocol in generative modeling, capturing the temporal world through the lens of static snapshots is information-deficient by design. This limitation leads to slower convergence, limited distributional coverage, and reduced generalization. In this work, we propose a simple and effective training strategy that leverages the temporal inductive bias present in continuous video frames to improve diffusion training. Notably, the proposed method requires no architectural modification and can be seamlessly integrated into standard diffusion training pipelines. We evaluate our method on the HandCo dataset, where hand-object interactions exhibit dense temporal coherence and subtle variations in finger articulation often result in semantically distinct motions. Empirically, our method accelerates convergence by over 2$\text{x}$ faster and achieves lower FID on both training and validation distributions. It also improves generative diversity by encouraging the model to capture meaningful temporal variations. We further provide an optimization analysis showing that our regularization reduces the gradient variance, which contributes to faster convergence. </p>
<blockquote>
<p>å›¾åƒæ‰©æ•£æ¨¡å‹æ˜¯åœ¨ç‹¬ç«‹é‡‡æ ·çš„é™æ€å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚è™½ç„¶è¿™æ˜¯ç”Ÿæˆæ¨¡å‹ä¸­çš„åŸºæœ¬ä»»åŠ¡åè®®ï¼Œä½†é€šè¿‡é™æ€å¿«ç…§æ•æ‰ç°å®ä¸–ç•Œçš„æ—¶é—´è½´åœ¨è®¾è®¡ä¸Šæœ¬èº«å°±å­˜åœ¨ä¿¡æ¯ç¼ºå¤±ã€‚è¿™ä¸€å±€é™æ€§å¯¼è‡´äº†æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ã€åˆ†å¸ƒè¦†ç›–æœ‰é™ä»¥åŠæ³›åŒ–èƒ½åŠ›é™ä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨è¿ç»­è§†é¢‘å¸§ä¸­å­˜åœ¨çš„æ—¶é—´å½’çº³åè§æ¥æ”¹å–„æ‰©æ•£è®­ç»ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸éœ€è¦å¯¹æ¶æ„è¿›è¡Œä¿®æ”¹ï¼Œå¹¶ä¸”å¯ä»¥æ— ç¼é›†æˆåˆ°æ ‡å‡†æ‰©æ•£è®­ç»ƒç®¡é“ä¸­ã€‚æˆ‘ä»¬åœ¨HandCoæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯¥æ•°æ®é›†çš„æ‰‹åŠ¨äº¤äº’è¡¨ç°å‡ºå¯†é›†çš„æ—¶é—´è¿è´¯æ€§ï¼Œæ‰‹æŒ‡å…³èŠ‚çš„ç»†å¾®å˜åŒ–é€šå¸¸ä¼šå¯¼è‡´è¯­ä¹‰ä¸Šæˆªç„¶ä¸åŒçš„åŠ¨ä½œã€‚ä»ç»éªŒä¸Šçœ‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†æ”¶æ•›é€Ÿåº¦æé«˜äº†ä¸¤å€ä»¥ä¸Šï¼Œå¹¶åœ¨è®­ç»ƒå’ŒéªŒè¯åˆ†å¸ƒä¸Šå®ç°äº†æ›´ä½çš„FIDã€‚å®ƒè¿˜èƒ½é€šè¿‡é¼“åŠ±æ¨¡å‹æ•æ‰æœ‰æ„ä¹‰çš„æ—¶åºå˜åŒ–æ¥æé«˜ç”Ÿæˆå¤šæ ·æ€§ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æä¾›äº†ä¼˜åŒ–åˆ†æï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ­£åˆ™åŒ–é™ä½äº†æ¢¯åº¦æ–¹å·®ï¼Œä»è€Œæœ‰åŠ©äºæ›´å¿«çš„æ”¶æ•›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03794v1">PDF</a> ICCV25 Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨è¿ç»­è§†é¢‘å¸§ä¸­çš„æ—¶é—´å½’çº³åè§æ”¹è¿›æ‰©æ•£è®­ç»ƒçš„ç­–ç•¥ã€‚è¯¥ç­–ç•¥æ— éœ€å¯¹æ¶æ„è¿›è¡Œä¿®æ”¹ï¼Œå¯æ— ç¼é›†æˆåˆ°æ ‡å‡†æ‰©æ•£è®­ç»ƒç®¡é“ä¸­ã€‚åœ¨HandCoæ•°æ®é›†ä¸Šè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åŠ é€Ÿäº†æ”¶æ•›é€Ÿåº¦ï¼Œé™ä½äº†FIDå€¼ï¼Œæé«˜äº†ç”Ÿæˆå¤šæ ·æ€§ï¼Œå¹¶é¼“åŠ±æ¨¡å‹æ•æ‰æœ‰æ„ä¹‰çš„æ—¶é—´å˜åŒ–ã€‚ä¼˜åŒ–åˆ†ææ˜¾ç¤ºï¼Œæ­£åˆ™åŒ–é™ä½äº†æ¢¯åº¦æ–¹å·®ï¼Œæœ‰åŠ©äºåŠ å¿«æ”¶æ•›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç‹¬ç«‹é‡‡æ ·çš„é™æ€å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨æ•æ‰æ—¶é—´ä¸–ç•Œæ—¶å­˜åœ¨ä¿¡æ¯ç¼ºå¤±çš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åˆ©ç”¨è§†é¢‘å¸§ä¸­çš„æ—¶é—´å½’çº³åè§æ¥æ”¹å–„æ‰©æ•£è®­ç»ƒçš„ç­–ç•¥ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ä¿®æ”¹æ¶æ„ï¼Œå¯é¡ºåˆ©èå…¥æ ‡å‡†æ‰©æ•£è®­ç»ƒæµç¨‹ã€‚</li>
<li>åœ¨HandCoæ•°æ®é›†ä¸Šæµ‹è¯•æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åŠ é€Ÿäº†æ”¶æ•›é€Ÿåº¦è¶…è¿‡2å€ã€‚</li>
<li>æ–¹æ³•é™ä½äº†FIDå€¼ï¼Œåœ¨è®­ç»ƒå’ŒéªŒè¯åˆ†å¸ƒä¸Šéƒ½è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡é¼“åŠ±æ•æ‰æœ‰æ„ä¹‰çš„æ—¶é—´å˜åŒ–ï¼Œæé«˜äº†ç”Ÿæˆå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3b24a3ebd156f26c29b0efad1a299daa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa5949bf1270240871f7a2684bea2bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ca9fb20989ad05e3eb1a23e0d954a1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-994de9fda41914ab40a46447dc9e1866.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b736f627f0f2ef05f418477e021304f1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SynBT-High-quality-Tumor-Synthesis-for-Breast-Tumor-Segmentation-by-3D-Diffusion-Model"><a href="#SynBT-High-quality-Tumor-Synthesis-for-Breast-Tumor-Segmentation-by-3D-Diffusion-Model" class="headerlink" title="SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D   Diffusion Model"></a>SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D   Diffusion Model</h2><p><strong>Authors:Hongxu Yang, Edina Timko, Levente Lippenszky, Vanda Czipczer, Lehel Ferenczi</strong></p>
<p>Synthetic tumors in medical images offer controllable characteristics that facilitate the training of machine learning models, leading to an improved segmentation performance. However, the existing methods of tumor synthesis yield suboptimal performances when tumor occupies a large spatial volume, such as breast tumor segmentation in MRI with a large field-of-view (FOV), while commonly used tumor generation methods are based on small patches. In this paper, we propose a 3D medical diffusion model, called SynBT, to generate high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed model consists of a patch-to-volume autoencoder, which is able to compress the high-resolution MRIs into compact latent space, while preserving the resolution of volumes with large FOV. Using the obtained latent space feature vector, a mask-conditioned diffusion model is used to synthesize breast tumors within selected regions of breast tissue, resulting in realistic tumor appearances. We evaluated the proposed method for a tumor segmentation task, which demonstrated the proposed high-quality tumor synthesis method can facilitate the common segmentation models with performance improvement of 2-3% Dice Score on a large public dataset, and therefore provides benefits for tumor segmentation in MRI images. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä¸­çš„åˆæˆè‚¿ç˜¤å…·æœ‰å¯æ§ç‰¹æ€§ï¼Œæœ‰åŠ©äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“è‚¿ç˜¤å æ®è¾ƒå¤§ç©ºé—´ä½“ç§¯æ—¶ï¼Œç°æœ‰çš„è‚¿ç˜¤åˆæˆæ–¹æ³•è¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚åœ¨å…·æœ‰å¤§è§†é‡ï¼ˆFOVï¼‰çš„MRIä¸­è¿›è¡Œä¹³è…ºç™Œåˆ†å‰²ã€‚è€Œå¸¸ç”¨çš„è‚¿ç˜¤ç”Ÿæˆæ–¹æ³•åŸºäºå°æ–‘å—ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSynBTçš„3DåŒ»å­¦æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåœ¨å¢å¼ºMRIå›¾åƒä¸­ç”Ÿæˆé«˜è´¨é‡ä¹³è…ºç™Œã€‚æ‰€æå‡ºçš„æ¨¡å‹åŒ…æ‹¬ä¸€ä¸ªä»æ–‘å—åˆ°ä½“ç§¯çš„è‡ªç¼–ç å™¨ï¼Œèƒ½å¤Ÿå°†é«˜åˆ†è¾¨ç‡MRIå‹ç¼©æˆç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤§è§†é‡ä½“ç§¯çš„åˆ†è¾¨ç‡ã€‚åˆ©ç”¨è·å¾—çš„æ½œåœ¨ç©ºé—´ç‰¹å¾å‘é‡ï¼Œä½¿ç”¨å¸¦æ©è†œæ¡ä»¶çš„æ‰©æ•£æ¨¡å‹åœ¨é€‰å®šåŒºåŸŸçš„ä¹³è…ºç»„ç»‡ä¸­åˆæˆä¹³è…ºç™Œï¼Œäº§ç”Ÿé€¼çœŸçš„è‚¿ç˜¤å¤–è§‚ã€‚æˆ‘ä»¬å¯¹æ‰€æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œé«˜è´¨é‡è‚¿ç˜¤åˆæˆæ–¹æ³•èƒ½å¤Ÿä¿ƒè¿›å…¬å…±æ•°æ®é›†ä¸ŠDiceå¾—åˆ†æé«˜2-3%ï¼Œå› æ­¤ï¼Œå¯¹MRIå›¾åƒçš„è‚¿ç˜¤åˆ†å‰²å…·æœ‰ç›Šå¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03267v1">PDF</a> Accepted by MICCAI 2025 Deep-Breath Workshop. Supported by IHI   SYNTHIA project</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSynBTçš„3DåŒ»å­¦æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡å¯¹æ¯”å¢å¼ºMRIå›¾åƒä¸­çš„ä¹³è…ºç™Œè‚¿ç˜¤ã€‚è¯¥æ¨¡å‹åŒ…æ‹¬ä¸€ä¸ªç”±è¡¥ä¸åˆ°ä½“ç§¯çš„è‡ªç¼–ç å™¨ï¼Œèƒ½å¤Ÿå‹ç¼©é«˜åˆ†è¾¨ç‡MRIåˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤§è§†é‡çš„ä½“ç§¯åˆ†è¾¨ç‡ã€‚åˆ©ç”¨è·å¾—çš„æ½œåœ¨ç©ºé—´ç‰¹å¾å‘é‡ï¼Œé‡‡ç”¨æ©è†œæ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨ä¹³è…ºç»„ç»‡é€‰å®šåŒºåŸŸåˆæˆä¹³è…ºç™Œï¼Œäº§ç”Ÿé€¼çœŸçš„è‚¿ç˜¤å¤–è§‚ã€‚åœ¨å¤§å‹å…¬å…±æ•°æ®é›†ä¸Šçš„è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡è¯„ä¼°è¡¨æ˜ï¼Œè¯¥é«˜è´¨é‡è‚¿ç˜¤åˆæˆæ–¹æ³•èƒ½æé«˜å¸¸è§çš„åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œæé«˜Diceå¾—åˆ†ç‡ä¸º2-3%ï¼Œä»è€Œä¸ºMRIå›¾åƒä¸­çš„è‚¿ç˜¤åˆ†å‰²æä¾›äº†ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆåŒ»å­¦å›¾åƒä¸­çš„è‚¿ç˜¤å…·æœ‰å¯æ§ç‰¹æ€§ï¼Œæœ‰åŠ©äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>ç°æœ‰è‚¿ç˜¤åˆæˆæ–¹æ³•åœ¨è‚¿ç˜¤å æ®è¾ƒå¤§ç©ºé—´ä½“ç§¯æ—¶è¡¨ç°ä¸ä½³ï¼Œå¦‚åœ¨å¤§è§†é‡MRIä¸­çš„ä¹³è…ºç™Œåˆ†å‰²ã€‚</li>
<li>æå‡ºçš„SynBTæ¨¡å‹æ˜¯ä¸€ä¸ª3DåŒ»å­¦æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡çš„å¯¹æ¯”å¢å¼ºMRIå›¾åƒä¸­çš„ä¹³è…ºç™Œè‚¿ç˜¤ã€‚</li>
<li>SynBTæ¨¡å‹åŒ…æ‹¬ä¸€ä¸ªç”±è¡¥ä¸åˆ°ä½“ç§¯çš„è‡ªç¼–ç å™¨ï¼Œèƒ½å¤Ÿå‹ç¼©é«˜åˆ†è¾¨ç‡MRIåˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤§è§†é‡çš„ä½“ç§¯åˆ†è¾¨ç‡ã€‚</li>
<li>ä½¿ç”¨è·å¾—çš„æ½œåœ¨ç©ºé—´ç‰¹å¾å‘é‡ï¼ŒSynBTæ¨¡å‹å¯ä»¥åœ¨ä¹³è…ºç»„ç»‡é€‰å®šåŒºåŸŸåˆæˆä¹³è…ºç™Œï¼Œäº§ç”Ÿé€¼çœŸçš„è‚¿ç˜¤å¤–è§‚ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼ŒSynBTæ¨¡å‹æœ‰åŠ©äºæé«˜è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹å…¬å…±æ•°æ®é›†ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f50b5c6253aeef5d14d0b484228a94ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44ae15c4d6dcc8cd75fa20b5af59f954.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c63b9af8e8178353b459f3842e27ebe8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DCDB-Dynamic-Conditional-Dual-Diffusion-Bridge-for-Ill-posed-Multi-Tasks"><a href="#DCDB-Dynamic-Conditional-Dual-Diffusion-Bridge-for-Ill-posed-Multi-Tasks" class="headerlink" title="DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed   Multi-Tasks"></a>DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed   Multi-Tasks</h2><p><strong>Authors:Chengjie Huang, Jiafeng Yan, Jing Li, Lu Bai</strong></p>
<p>Conditional diffusion models have made impressive progress in the field of image processing, but the characteristics of constructing data distribution pathways make it difficult to exploit the intrinsic correlation between tasks in multi-task scenarios, which is even worse in ill-posed tasks with a lack of training data. In addition, traditional static condition control makes it difficult for networks to learn in multi-task scenarios with its dynamically evolving characteristics. To address these challenges, we propose a dynamic conditional double diffusion bridge training paradigm to build a general framework for ill-posed multi-tasks. Firstly, this paradigm decouples the diffusion and condition generation processes, avoiding the dependence of the diffusion model on supervised data in ill-posed tasks. Secondly, generated by the same noise schedule, dynamic conditions are used to gradually adjust their statistical characteristics, naturally embed time-related information, and reduce the difficulty of network learning. We analyze the learning objectives of the network under different conditional forms in the single-step denoising process and compare the changes in its attention weights in the network, demonstrating the superiority of our dynamic conditions. Taking dehazing and visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve the best performance in multiple indicators on public datasets. The code has been publicly released at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DCDB-D3C2">https://anonymous.4open.science/r/DCDB-D3C2</a>. </p>
<blockquote>
<p>åœ¨å›¾åƒå¤„ç†é¢†åŸŸï¼Œæ¡ä»¶æ‰©æ•£æ¨¡å‹å·²ç»å–å¾—äº†ä»¤äººç©ç›®çš„è¿›å±•ã€‚ç„¶è€Œï¼Œæ„å»ºæ•°æ®åˆ†å¸ƒé€”å¾„çš„ç‰¹å¾ä½¿å¾—åœ¨å¤šä»»åŠ¡åœºæ™¯ä¸­åˆ©ç”¨ä»»åŠ¡ä¹‹é—´çš„å†…åœ¨å…³è”å˜å¾—å›°éš¾ï¼Œåœ¨ç¼ºä¹è®­ç»ƒæ•°æ®çš„ç—…æ€ä»»åŠ¡ä¸­è¿™ç§æƒ…å†µæ›´ä¸ºä¸¥é‡ã€‚å¦å¤–ï¼Œä¼ ç»Ÿçš„é™æ€æ¡ä»¶æ§åˆ¶ä½¿å¾—ç½‘ç»œåœ¨å…·æœ‰åŠ¨æ€æ¼”å˜ç‰¹å¾çš„å¤šä»»åŠ¡åœºæ™¯ä¸­å­¦ä¹ å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€æ¡ä»¶åŒé‡æ‰©æ•£æ¡¥è®­ç»ƒèŒƒå¼ï¼Œä»¥å»ºç«‹ä¸€ä¸ªé€‚ç”¨äºç—…æ€å¤šä»»åŠ¡çš„é€šç”¨æ¡†æ¶ã€‚é¦–å…ˆï¼Œè¯¥èŒƒå¼è§£è€¦äº†æ‰©æ•£å’Œæ¡ä»¶ç”Ÿæˆè¿‡ç¨‹ï¼Œé¿å…äº†åœ¨ç—…æ€ä»»åŠ¡ä¸­æ‰©æ•£æ¨¡å‹å¯¹ç›‘ç£æ•°æ®çš„ä¾èµ–ã€‚å…¶æ¬¡ï¼Œé€šè¿‡ç›¸åŒçš„å™ªå£°è°ƒåº¦ç”ŸæˆåŠ¨æ€æ¡ä»¶ï¼Œé€æ­¥è°ƒæ•´å…¶ç»Ÿè®¡ç‰¹å¾ï¼Œè‡ªç„¶åµŒå…¥ä¸æ—¶é—´ç›¸å…³çš„ä¿¡æ¯ï¼Œé™ä½ç½‘ç»œå­¦ä¹ çš„éš¾åº¦ã€‚æˆ‘ä»¬åˆ†æäº†å•æ­¥å»å™ªè¿‡ç¨‹ä¸­ä¸åŒæ¡ä»¶ä¸‹ç½‘ç»œçš„å­¦ä¹ ç›®æ ‡ï¼Œå¹¶æ¯”è¾ƒäº†ç½‘ç»œä¸­æ³¨æ„åŠ›æƒé‡çš„å˜åŒ–ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„åŠ¨æ€æ¡ä»¶çš„ä¼˜è¶Šæ€§ã€‚ä»¥å»é›¾å’Œå¯è§å…‰-çº¢å¤–èåˆä½œä¸ºå…¸å‹çš„ç—…æ€å¤šä»»åŠ¡åœºæ™¯ï¼Œæˆ‘ä»¬åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨ï¼š[<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DCDB-D3C2%E3%80%82]">https://anonymous.4open.science/r/DCDB-D3C2ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03044v1">PDF</a> 15 pages,6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŠ¨æ€æ¡ä»¶åŒé‡æ‰©æ•£æ¡¥è®­ç»ƒèŒƒå¼ï¼Œæ„å»ºäº†ä¸€ä¸ªé’ˆå¯¹å¤æ‚å¤šä»»åŠ¡é—®é¢˜çš„é€šç”¨æ¡†æ¶ã€‚é€šè¿‡è§£è€¦æ‰©æ•£å’Œæ¡ä»¶ç”Ÿæˆè¿‡ç¨‹ï¼Œè§£å†³äº†ç¼ºä¹ç›‘ç£æ•°æ®çš„é—®é¢˜ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€æ¡ä»¶é€æ­¥è°ƒæ•´ç»Ÿè®¡ç‰¹å¾ï¼Œå‡å°‘ç½‘ç»œå­¦ä¹ éš¾åº¦ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†å»é›¾å’Œå¯è§å…‰çº¢å¤–èåˆç­‰å…¸å‹å¤æ‚å¤šä»»åŠ¡åœºæ™¯çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€æ¡ä»¶åŒé‡æ‰©æ•£æ¡¥è®­ç»ƒèŒƒå¼è§£å†³äº†åœ¨å›¾åƒå¤„ç†çš„å¤æ‚å¤šä»»åŠ¡åœºæ™¯ä¸­ï¼Œä¼ ç»Ÿé™æ€æ¡ä»¶æ§åˆ¶éš¾ä»¥åº”å¯¹çš„é—®é¢˜ã€‚</li>
<li>è§£è€¦æ‰©æ•£å’Œæ¡ä»¶ç”Ÿæˆè¿‡ç¨‹ä½¿å¾—æ¨¡å‹åœ¨ç¼ºä¹ç›‘ç£æ•°æ®çš„ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ã€‚</li>
<li>åŠ¨æ€æ¡ä»¶èƒ½å¤Ÿæ ¹æ®å™ªå£°æ—¶é—´è¡¨è¿›è¡Œè°ƒæ•´ï¼Œè‡ªç„¶åµŒå…¥æ—¶é—´ç›¸å…³ä¿¡æ¯ï¼Œé™ä½ç½‘ç»œå­¦ä¹ éš¾åº¦ã€‚</li>
<li>è¯¥è®­ç»ƒèŒƒå¼åœ¨å…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†å»é›¾å’Œå¯è§å…‰çº¢å¤–èåˆç­‰ä»»åŠ¡çš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ç½‘ç»œå­¦ä¹ ç›®æ ‡åœ¨ä¸åŒæ¡ä»¶ä¸‹è¿›è¡Œäº†åˆ†æï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>æ³¨æ„åŠ›æƒé‡åœ¨ç½‘ç»œä¸­çš„å˜åŒ–è¡¨æ˜åŠ¨æ€æ¡ä»¶æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c761c337e079de7d5d4874d43fbd6a54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-911a15c327318990fe48cf8050598501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c87d85779fba31f3f6af19d689d8bef.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-Robustness-in-Post-Processing-Watermarking-An-Ensemble-Attack-Network-Using-CNNs-and-Transformers"><a href="#Enhancing-Robustness-in-Post-Processing-Watermarking-An-Ensemble-Attack-Network-Using-CNNs-and-Transformers" class="headerlink" title="Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack   Network Using CNNs and Transformers"></a>Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack   Network Using CNNs and Transformers</h2><p><strong>Authors:Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen</strong></p>
<p>Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the modelâ€™s internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:<a target="_blank" rel="noopener" href="https://github.com/aiiu-lab/DeepRobustWatermark">https://github.com/aiiu-lab/DeepRobustWatermark</a>. </p>
<blockquote>
<p>è¿‘æœŸå…³äºæ·±åº¦æ°´å°çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºå¤„ç†è¿‡ç¨‹ä¸­çš„æ°´å°æŠ€æœ¯ï¼Œè¿™ç§æŠ€æœ¯å°†æ°´å°è¿‡ç¨‹é›†æˆåˆ°å›¾åƒç”Ÿæˆä¸­ã€‚ç„¶è€Œï¼Œåå¤„ç†æ°´å°æŠ€æœ¯èƒ½å¤Ÿåœ¨å›¾åƒç”ŸæˆååµŒå…¥æ°´å°ï¼Œå› æ­¤å…·æœ‰æ›´å¤§çš„çµæ´»æ€§ã€‚å®ƒå¯ä»¥åº”ç”¨äºä»»ä½•ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºï¼ˆä¾‹å¦‚GANsã€æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œè€Œæ— éœ€è®¿é—®æ¨¡å‹çš„å†…éƒ¨ç»“æ„ã€‚å®ƒè¿˜å…è®¸ç”¨æˆ·å°†ç‹¬ç‰¹çš„æ°´å°åµŒå…¥åˆ°å•ç‹¬çš„å›¾åƒä¸­ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶ä¸“æ³¨äºåå¤„ç†æ°´å°æŠ€æœ¯ï¼Œå¹¶é€šè¿‡åœ¨è®­ç»ƒä¸­ç»“åˆé›†æˆæ”»å‡»ç½‘ç»œæ¥æé«˜å…¶ç¨³å¥æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨CNNå’ŒTransformeråœ¨ç©ºé—´å’Œé¢‘ç‡é¢†åŸŸæ„å»ºäº†å„ç§ç‰ˆæœ¬çš„æ”»å‡»ç½‘ç»œï¼Œä»¥ç ”ç©¶æ¯ç§ç»„åˆå¦‚ä½•å½±å“æ°´å°æ¨¡å‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ç©ºåŸŸä½¿ç”¨CNNç»“åˆæ”»å‡»ç½‘ç»œï¼Œåœ¨é¢‘åŸŸä½¿ç”¨Transformerç»“åˆæ”»å‡»ç½‘ç»œï¼Œèƒ½å¤Ÿåœ¨æ°´å°æ¨¡å‹ä¸­å®ç°æœ€é«˜çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨WAVESåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œä»¥å¹³å‡ä½å‡†ç¡®ç‡ä¸ºæŒ‡æ ‡ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„é›†æˆæ”»å‡»ç½‘ç»œåœ¨å„ç§å‹åŠ›æµ‹è¯•ä¸‹æ˜¾è‘—æé«˜äº†åŸºçº¿æ°´å°æ–¹æ³•çš„ç¨³å¥æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨WAVESå®šä¹‰çš„å†ç”Ÿæ”»å‡»ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†StegaStampçš„å‡†ç¡®ç‡é«˜è¾¾18.743%ã€‚ä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/aiiu-lab/DeepRobustWatermark%E3%80%82">https://github.com/aiiu-lab/DeepRobustWatermarkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03006v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ·±åº¦æ°´å°æŠ€æœ¯ä¸­çš„åå¤„ç†æ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨å›¾åƒç”ŸæˆååµŒå…¥æ°´å°ï¼Œå…·æœ‰æ›´å¤§çš„çµæ´»æ€§ã€‚ç ”ç©¶é€šè¿‡é›†æˆç»„åˆæ”»å‡»ç½‘ç»œæé«˜äº†å…¶ç¨³å¥æ€§ï¼Œå¹¶æ¢è®¨äº†ä¸åŒæ”»å‡»ç½‘ç»œç»“æ„ï¼ˆåŒ…æ‹¬CNNå’ŒTransformerï¼‰å¯¹æ°´å°æ¨¡å‹ç¨³å¥æ€§çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆç©ºé—´åŸŸCNNæ”»å‡»ç½‘ç»œä¸é¢‘ç‡åŸŸTransformeræ”»å‡»ç½‘ç»œå¯è·å¾—æœ€ä½³ç¨³å¥æ€§ã€‚åœ¨WAVESåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†åŸºçº¿æ°´å°æ–¹æ³•çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨WAVESå®šä¹‰çš„å†ç”Ÿæ”»å‡»ä¸­ï¼Œæé«˜äº†StegaStampæ–¹æ³•è¾¾18.743%ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨åå¤„ç†æ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨å›¾åƒç”ŸæˆååµŒå…¥æ°´å°ï¼Œå±•ç°æ›´å¤§çš„çµæ´»æ€§ã€‚</li>
<li>æå‡ºé›†æˆç»„åˆæ”»å‡»ç½‘ç»œä»¥æé«˜æ°´å°æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡æ„å»ºä¸åŒæ”»å‡»ç½‘ç»œç»“æ„ï¼ˆCNNå’ŒTransformerï¼‰ï¼Œæ¢ç©¶å…¶å¯¹æ°´å°æ¨¡å‹ç¨³å¥æ€§çš„å½±å“ã€‚</li>
<li>ç»“åˆç©ºé—´åŸŸCNNæ”»å‡»ç½‘ç»œä¸é¢‘ç‡åŸŸTransformeræ”»å‡»ç½‘ç»œè·å¾—æœ€ä½³ç¨³å¥æ€§ç»“æœã€‚</li>
<li>åœ¨WAVESåŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜åŸºçº¿æ°´å°æ–¹æ³•çš„ç¨³å¥æ€§ã€‚</li>
<li>å¯¹äºå†ç”Ÿæ”»å‡»ï¼Œè¯¥æ–¹æ³•æ”¹è¿›äº†StegaStampæ–¹æ³•è¾¾18.743%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31d9a8eb646e4b9694a615a59ab71f4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa5648be02b7878b66a80855f977a8a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7946b3dbb9a0533fbf22ac7279e04e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2bc346488cc2f961ad5a020e9350b097.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3481b10ba88f480acfd4ac0a7e7dcccd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d14c8f238fa9a41bb14fb8640d88cf8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="InstaDA-Augmenting-Instance-Segmentation-Data-with-Dual-Agent-System"><a href="#InstaDA-Augmenting-Instance-Segmentation-Data-with-Dual-Agent-System" class="headerlink" title="InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System"></a>InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System</h2><p><strong>Authors:Xianbao Hou, Yonghao He, Zeyd Boukhers, John See, Hu Su, Wei Sui, Cong Yang</strong></p>
<p>Acquiring high-quality instance segmentation data is challenging due to the labor-intensive nature of the annotation process and significant class imbalances within datasets. Recent studies have utilized the integration of Copy-Paste and diffusion models to create more diverse datasets. However, these studies often lack deep collaboration between large language models (LLMs) and diffusion models, and underutilize the rich information within the existing training data. To address these limitations, we propose InstaDA, a novel, training-free Dual-Agent system designed to augment instance segmentation datasets. First, we introduce a Text-Agent (T-Agent) that enhances data diversity through collaboration between LLMs and diffusion models. This agent features a novel Prompt Rethink mechanism, which iteratively refines prompts based on the generated images. This process not only fosters collaboration but also increases image utilization and optimizes the prompts themselves. Additionally, we present an Image-Agent (I-Agent) aimed at enriching the overall data distribution. This agent augments the training set by generating new instances conditioned on the training images. To ensure practicality and efficiency, both agents operate as independent and automated workflows, enhancing usability. Experiments conducted on the LVIS 1.0 validation set indicate that InstaDA achieves significant improvements, with an increase of +4.0 in box average precision (AP) and +3.3 in mask AP compared to the baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common categories and mask AP gains of +0.2 on common categories and +0.5 on frequent categories. </p>
<blockquote>
<p>è·å–é«˜è´¨é‡çš„å®ä¾‹åˆ†å‰²æ•°æ®æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæ ‡æ³¨è¿‡ç¨‹åŠ³åŠ¨å¯†é›†ï¼Œä¸”æ•°æ®é›†ä¸­å­˜åœ¨æ˜¾è‘—çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡ç»“åˆCopy-Pasteå’Œæ‰©æ•£æ¨¡å‹æ¥åˆ›å»ºæ›´å¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶å¾€å¾€ç¼ºä¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æ·±åº¦åä½œï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†åˆ©ç”¨ç°æœ‰è®­ç»ƒæ•°æ®ä¸­çš„ä¸°å¯Œä¿¡æ¯ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†InstaDAï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„ã€æ— éœ€è®­ç»ƒçš„åŒä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºå®ä¾‹åˆ†å‰²æ•°æ®é›†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–‡æœ¬ä»£ç†ï¼ˆT-Agentï¼‰ï¼Œå®ƒé€šè¿‡LLMå’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„åä½œæ¥æé«˜æ•°æ®å¤šæ ·æ€§ã€‚è¯¥ä»£ç†å…·æœ‰æ–°é¢–çš„Prompt Rethinkæœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä¼šæ ¹æ®ç”Ÿæˆçš„å›¾åƒè¿­ä»£ä¼˜åŒ–æç¤ºã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ä»…ä¿ƒè¿›äº†åä½œï¼Œè¿˜æé«˜äº†å›¾åƒåˆ©ç”¨ç‡å¹¶ä¼˜åŒ–äº†æç¤ºæœ¬èº«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†æ—¨åœ¨ä¸°å¯Œæ•´ä½“æ•°æ®åˆ†å¸ƒçš„å›¾åƒä»£ç†ï¼ˆI-Agentï¼‰ã€‚è¯¥ä»£ç†é€šè¿‡åŸºäºè®­ç»ƒå›¾åƒç”Ÿæˆæ–°å®ä¾‹æ¥ä¸°å¯Œè®­ç»ƒé›†ã€‚ä¸ºç¡®ä¿å®ç”¨æ€§å’Œæ•ˆç‡ï¼Œä¸¤ä¸ªä»£ç†éƒ½ä½œä¸ºç‹¬ç«‹ä¸”è‡ªåŠ¨åŒ–çš„å·¥ä½œæµç¨‹è¿è¡Œï¼Œå¢å¼ºäº†æ˜“ç”¨æ€§ã€‚åœ¨LVIS 1.0éªŒè¯é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒInstaDAå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œç›¸å¯¹äºåŸºçº¿æé«˜äº†+4.0çš„æ¡†å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰å’Œ+3.3çš„æ©æ¨¡APã€‚æ­¤å¤–ï¼Œå®ƒåœ¨é¢†å…ˆæ¨¡å‹DiverGençš„åŸºç¡€ä¸Šæé«˜äº†+0.3çš„æ¡†APå’Œ+0.1çš„æ©æ¨¡APï¼Œåœ¨å¸¸è§ç±»åˆ«ä¸Šæ¡†APæé«˜äº†+0.7ï¼Œæ©æ¨¡APåœ¨å¸¸è§ç±»åˆ«ä¸Šæé«˜äº†+0.2ï¼Œåœ¨é¢‘ç¹ç±»åˆ«ä¸Šæé«˜äº†+0.5ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02973v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹é«˜è´¨é‡å®ä¾‹åˆ†å‰²æ•°æ®è·å–çš„æŒ‘æˆ˜ï¼Œå¦‚æ ‡æ³¨è¿‡ç¨‹åŠ³åŠ¨å¼ºåº¦é«˜å’Œæ•°æ®é›†ç±»åˆ«ä¸å¹³è¡¡ç­‰é—®é¢˜ï¼Œæœ€è¿‘ç ”ç©¶å¼€å§‹ç»“åˆCopy-Pasteå’Œæ‰©æ•£æ¨¡å‹æ¥åˆ›å»ºæ›´å¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æ·±åº¦åˆä½œæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸”æœªèƒ½å……åˆ†åˆ©ç”¨ç°æœ‰è®­ç»ƒæ•°æ®çš„ä¸°å¯Œä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºInstaDAï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹åŒä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºå®ä¾‹åˆ†å‰²æ•°æ®é›†ã€‚å…¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥Text-Agentï¼ˆT-Agentï¼‰é€šè¿‡LLMså’Œæ‰©æ•£æ¨¡å‹çš„åä½œæé«˜æ•°æ®å¤šæ ·æ€§ã€‚è¯¥ä»£ç†å…·æœ‰æ–°é¢–çš„Prompt Rethinkæœºåˆ¶ï¼Œå¯æ ¹æ®ç”Ÿæˆçš„å›¾åƒè¿­ä»£ä¼˜åŒ–æç¤ºã€‚è¿™ä¸ä»…ä¿ƒè¿›äº†åä½œï¼Œè¿˜æé«˜äº†å›¾åƒåˆ©ç”¨ç‡å¹¶ä¼˜åŒ–äº†æç¤ºæœ¬èº«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†Image-Agentï¼ˆI-Agentï¼‰ï¼Œæ—¨åœ¨ä¸°å¯Œæ•´ä½“æ•°æ®åˆ†å¸ƒã€‚è¯¥ä»£ç†é€šè¿‡åŸºäºè®­ç»ƒå›¾åƒç”Ÿæˆæ–°å®ä¾‹æ¥ä¸°å¯Œè®­ç»ƒé›†ã€‚ä¸ºç¡®ä¿å®ç”¨æ€§å’Œæ•ˆç‡ï¼Œä¸¤ä¸ªä»£ç†ä½œä¸ºç‹¬ç«‹è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹è¿è¡Œï¼Œå¢å¼ºäº†æ˜“ç”¨æ€§ã€‚åœ¨LVIS 1.0éªŒè¯é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒInstaDAåœ¨ç›’å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰å’Œæ©è†œAPæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè¾ƒåŸºå‡†æ–¹æ³•åˆ†åˆ«æé«˜äº†+4.0å’Œ+3.3çš„APã€‚å¹¶ä¸”ä¸é¢†å…ˆçš„æ¨¡å‹DiverGenç›¸æ¯”ï¼Œåœ¨ç›’APå’Œæ©è†œAPä¸Šåˆ†åˆ«æé«˜äº†+0.3å’Œ+0.1ï¼Œåœ¨å¸¸è§ç±»åˆ«å’Œé¢‘ç¹ç±»åˆ«çš„ç›’APå’Œæ©è†œAPä¸Šåˆ†åˆ«å–å¾—äº†æ˜¾è‘—çš„å¢ç›Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é«˜è´¨é‡å®ä¾‹åˆ†å‰²æ•°æ®è·å–å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› æ ‡æ³¨è¿‡ç¨‹åŠ³åŠ¨å¼ºåº¦é«˜å’Œæ•°æ®é›†ç±»åˆ«ä¸å¹³è¡¡ç­‰é—®é¢˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¼€å§‹ç»“åˆCopy-Pasteå’Œæ‰©æ•£æ¨¡å‹åˆ›å»ºå¤šæ ·åŒ–æ•°æ®é›†ï¼Œä½†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„æ·±åº¦åˆä½œæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºInstaDAç³»ç»Ÿï¼ŒåŒ…æ‹¬Text-Agentå’ŒImage-Agentï¼Œåˆ†åˆ«é€šè¿‡æ–‡æœ¬å’Œå›¾åƒå¢å¼ºæ•°æ®å¤šæ ·æ€§å’Œæ•´ä½“æ•°æ®åˆ†å¸ƒã€‚</li>
<li>Text-Agentä¸­çš„Prompt Rethinkæœºåˆ¶å¯è¿­ä»£ä¼˜åŒ–æç¤ºï¼Œä¿ƒè¿›åä½œå¹¶æé«˜å›¾åƒåˆ©ç”¨ç‡ã€‚</li>
<li>Image-Agentæ—¨åœ¨ç”ŸæˆåŸºäºè®­ç»ƒå›¾åƒçš„æ–°å®ä¾‹ï¼Œä»¥ä¸°å¯Œæ•°æ®é›†çš„å¤šæ ·æ€§ã€‚</li>
<li>ä¸¤ä¸ªä»£ç†ä½œä¸ºç‹¬ç«‹è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹è¿è¡Œï¼Œæé«˜äº†å®ç”¨æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3139b3040451b68ddd382b752666887e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-159c7e4bb1c29f43491dbec6c1215a96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0b9f956812c4ea28eea01172d744d0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8f2a265aa6e90b9051c100a0c87a565.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f68b720446c9e7e1b8d771b09543bb7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e4a63937daeda1eeff116d0e094a017.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enhancing-Zero-Shot-Pedestrian-Attribute-Recognition-with-Synthetic-Data-Generation-A-Comparative-Study-with-Image-To-Image-Diffusion-Models"><a href="#Enhancing-Zero-Shot-Pedestrian-Attribute-Recognition-with-Synthetic-Data-Generation-A-Comparative-Study-with-Image-To-Image-Diffusion-Models" class="headerlink" title="Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data   Generation: A Comparative Study with Image-To-Image Diffusion Models"></a>Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data   Generation: A Comparative Study with Image-To-Image Diffusion Models</h2><p><strong>Authors:Pablo Ayuso-Albizu, Juan C. SanMiguel, Pablo Carballeira</strong></p>
<p>Pedestrian Attribute Recognition (PAR) involves identifying various human attributes from images with applications in intelligent monitoring systems. The scarcity of large-scale annotated datasets hinders the generalization of PAR models, specially in complex scenarios involving occlusions, varying poses, and diverse environments. Recent advances in diffusion models have shown promise for generating diverse and realistic synthetic images, allowing to expand the size and variability of training data. However, the potential of diffusion-based data expansion for generating PAR-like images remains underexplored. Such expansion may enhance the robustness and adaptability of PAR models in real-world scenarios. This paper investigates the effectiveness of diffusion models in generating synthetic pedestrian images tailored to PAR tasks. We identify key parameters of img2img diffusion-based data expansion; including text prompts, image properties, and the latest enhancements in diffusion-based data augmentation, and examine their impact on the quality of generated images for PAR. Furthermore, we employ the best-performing expansion approach to generate synthetic images for training PAR models, by enriching the zero-shot datasets. Experimental results show that prompt alignment and image properties are critical factors in image generation, with optimal selection leading to a 4.5% improvement in PAR recognition performance. </p>
<blockquote>
<p>è¡Œäººå±æ€§è¯†åˆ«ï¼ˆPARï¼‰æ¶‰åŠä»å›¾åƒä¸­è¯†åˆ«å„ç§äººç±»å±æ€§ï¼Œåœ¨æ™ºèƒ½ç›‘æ§ç³»ç»Ÿä¸­æœ‰å¹¿æ³›åº”ç”¨ã€‚å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„ç¨€ç¼ºé˜»ç¢äº†PARæ¨¡å‹åœ¨æ¶‰åŠé®æŒ¡ã€ä¸åŒå§¿åŠ¿å’Œå¤šæ ·ç¯å¢ƒç­‰å¤æ‚åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•åœ¨ç”Ÿæˆå¤šæ ·ä¸”ç°å®çš„åˆæˆå›¾åƒæ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä»è€Œå¯ä»¥æ‰©å¤§è®­ç»ƒæ•°æ®çš„å¤§å°å’Œå˜åŒ–ã€‚ç„¶è€Œï¼ŒåŸºäºæ‰©æ•£çš„æ•°æ®æ‰©å±•åœ¨ç”Ÿæˆç±»ä¼¼äºPARå›¾åƒæ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™ç§æ‰©å±•å¯èƒ½æé«˜PARæ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé’ˆå¯¹PARä»»åŠ¡çš„åˆæˆè¡Œäººå›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ç¡®å®šäº†åŸºäºimg2imgæ‰©æ•£çš„æ•°æ®æ‰©å±•çš„å…³é”®å‚æ•°ï¼ŒåŒ…æ‹¬æ–‡æœ¬æç¤ºã€å›¾åƒå±æ€§å’ŒåŸºäºæ‰©æ•£çš„æ•°æ®å¢å¼ºçš„æœ€æ–°æ”¹è¿›ï¼Œå¹¶ç ”ç©¶äº†å®ƒä»¬å¯¹ç”Ÿæˆçš„ç”¨äºPARçš„å›¾åƒè´¨é‡çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä¸°å¯Œé›¶æ ·æœ¬æ•°æ®é›†ï¼Œé‡‡ç”¨è¡¨ç°æœ€ä½³çš„æ‰©å±•æ–¹æ³•ç”Ÿæˆåˆæˆå›¾åƒæ¥è®­ç»ƒPARæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæç¤ºå¯¹é½å’Œå›¾åƒå±æ€§æ˜¯å›¾åƒç”Ÿæˆçš„å…³é”®å› ç´ ï¼Œæœ€ä¼˜é€‰æ‹©å¯¼è‡´PARè¯†åˆ«æ€§èƒ½æé«˜äº†4.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02161v1">PDF</a> Paper accepted at AVSS 2025 conference</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé’ˆå¯¹è¡Œäººå±æ€§è¯†åˆ«ï¼ˆPARï¼‰ä»»åŠ¡çš„åˆæˆè¡Œäººå›¾åƒä¸­çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°æ®æ‰©å±•ï¼Œå¢å¼ºPARæ¨¡å‹çš„é²æ£’æ€§å’Œé€‚åº”ç°å®åœºæ™¯çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæç¤ºå¯¹é½å’Œå›¾åƒå±æ€§æ˜¯å›¾åƒç”Ÿæˆçš„å…³é”®å› ç´ ï¼Œæœ€ä¼˜é€‰æ‹©å¯æé«˜PARè¯†åˆ«æ€§èƒ½4.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡Œäººå±æ€§è¯†åˆ«ï¼ˆPARï¼‰åœ¨æ™ºèƒ½ç›‘æ§ç³»ç»Ÿä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†é™åˆ¶äº†å…¶æ¨¡å‹çš„é€šç”¨æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå¤šæ ·ä¸”çœŸå®çš„åˆæˆå›¾åƒä¸Šæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œå¯ç”¨äºæ‰©å±•PARçš„è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆè¡Œäººå›¾åƒæ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡æ¢è®¨äº†img2imgæ‰©æ•£æ¨¡å‹çš„å…³é”®å‚æ•°ï¼ŒåŒ…æ‹¬æ–‡æœ¬æç¤ºã€å›¾åƒå±æ€§ä»¥åŠæœ€æ–°çš„æ‰©æ•£æ¨¡å‹å¢å¼ºæŠ€æœ¯ã€‚</li>
<li>å®éªŒå‘ç°ï¼Œæç¤ºå¯¹é½å’Œå›¾åƒå±æ€§å¯¹ç”Ÿæˆå›¾åƒçš„è´¨é‡æœ‰é‡è¦å½±å“ã€‚</li>
<li>æœ€ä¼˜çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•å¯æé«˜PARè¯†åˆ«æ€§èƒ½è¾¾4.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7a8375ca8120220691ee8a89ed03c0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cd27c3fe0edaebfc5b29bd79301a417.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17351fcad08eb42131c4e3208e18c526.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-063e971d4306a4d6ba018f77a9b1a1a3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Foundations-and-Models-in-Modern-Computer-Vision-Key-Building-Blocks-in-Landmark-Architectures"><a href="#Foundations-and-Models-in-Modern-Computer-Vision-Key-Building-Blocks-in-Landmark-Architectures" class="headerlink" title="Foundations and Models in Modern Computer Vision: Key Building Blocks in   Landmark Architectures"></a>Foundations and Models in Modern Computer Vision: Key Building Blocks in   Landmark Architectures</h2><p><strong>Authors:Radu-Andrei Bourceanu, Neil De La Fuente, Jan Grimm, Andrei Jardan, Andriy Manucharyan, Cornelius Weiss, Daniel Cremers, Roman Pflugfelder</strong></p>
<p>This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analysis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer architecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recognition. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models. </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šé€šè¿‡åˆ†æå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡ï¼Œæ¢è®¨äº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ã€‚åˆ†æä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„å¼€å§‹ã€‚æˆ‘ä»¬å›é¡¾äº†ResNetï¼Œå®ƒå¼•å…¥äº†æ®‹å·®è¿æ¥ï¼Œå…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå®ç°äº†å¯¹æ·±åº¦å·ç§¯ç½‘ç»œçš„æœ‰æ•ˆè®­ç»ƒã€‚ä¹‹åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—çš„Vision Transformerï¼ˆViTï¼‰ï¼Œè¿™å¼€åˆ›äº†æ–°çš„èŒƒå¼ï¼Œè¯æ˜äº†åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚åŸºäºè¿™äº›è§†è§‰è¡¨ç¤ºéª¨å¹²ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”Ÿæˆæ¨¡å‹ã€‚åˆ†æäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ–°å‹å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹é€šè¿‡ç”Ÿæˆå™¨ä¸é‰´åˆ«å™¨çš„å¯¹æŠ—æ¥å­¦ä¹ å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚ç„¶åä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ï¼Œé€šè¿‡åœ¨æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¿ç»­çš„å»å™ªè¿‡ç¨‹ï¼Œæ”¹è¿›äº†å…ˆå‰çš„ç”Ÿæˆæ–¹æ³•ã€‚LDMså®ç°äº†é«˜ä¿çœŸåº¦çš„åˆæˆï¼Œå…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œä»£è¡¨äº†å½“å‰å›¾åƒç”Ÿæˆçš„æœ€æ–°æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡å°‘å¯¹æ ‡å®šæ•°æ®ä¾èµ–æ€§çš„è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚DINOæ˜¯ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ åŒ¹é…åŠ¨é‡æ›´æ–°çš„æ•™å¸ˆçš„è¾“å‡ºï¼Œäº§ç”Ÿå…·æœ‰å¼ºå¤§k-NNåˆ†ç±»æ€§èƒ½çš„ç‰¹å¾ã€‚æœ€åä»¥Masked Autoencodersï¼ˆMAEï¼‰ä¸ºä¾‹ï¼Œå®ƒé‡‡ç”¨å¯¹ç§°çš„ç¼–ç å™¨-è§£ç å™¨è®¾è®¡æ¥é‡å»ºé«˜åº¦é®æŒ¡çš„è¾“å…¥ï¼Œæä¾›äº†ä¸€ç§å¯ä¼¸ç¼©ä¸”æœ‰æ•ˆçš„é¢„è®­ç»ƒå¤§è§„æ¨¡è§†è§‰æ¨¡å‹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23357v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ï¼Œé€šè¿‡è€ƒå¯Ÿå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡è¿›è¡Œäº†æ·±å…¥ç ”è®¨ã€‚æ–‡ç« é¦–å…ˆä»‹ç»äº†å›¾åƒè¯†åˆ«çš„å¥ åŸºæ¶æ„ResNetï¼Œå®ƒå¼•å…¥æ®‹å·®è¿æ¥å…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿è®­ç»ƒæ›´æ·±çš„å·ç§¯ç½‘ç»œå˜å¾—æœ‰æ•ˆã€‚æ¥ç€æ¢è®¨äº†åº”ç”¨Transformeræ¶æ„äºå›¾åƒè¡¥ä¸åºåˆ—çš„Vision Transformerï¼ˆViTï¼‰ï¼Œå±•ç¤ºäº†æ³¨æ„åŠ›æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶äº†ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†æ½œæ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰å¦‚ä½•åœ¨æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œé¡ºåºå»å™ªè¿‡ç¨‹ï¼Œå®ç°äº†é«˜ä¿çœŸåˆæˆå’Œæ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚æœ€åï¼Œæ¢ç´¢äº†å‡å°‘å¯¹æ ‡æ³¨æ•°æ®ä¾èµ–çš„è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œå¦‚DINOå’Œé®ç½©è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ResNeté€šè¿‡å¼•å…¥æ®‹å·®è¿æ¥è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿è®­ç»ƒæ·±å±‚ç½‘ç»œæ›´ä¸ºæœ‰æ•ˆã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—ï¼Œå±•ç¤ºäº†æ³¨æ„åŠ›æ¨¡å‹åœ¨å›¾åƒè¯†åˆ«ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰é€šè¿‡å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹å­¦ä¹ å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>æ½œæ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå»å™ªï¼Œå®ç°äº†é«˜ä¿çœŸåˆæˆå’Œé«˜æ•ˆè®¡ç®—ã€‚</li>
<li>è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯å‡å°‘äº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼ŒDINOå’ŒMAEæ˜¯å…¶ä¸­çš„ä»£è¡¨æ–¹æ³•ã€‚</li>
<li>DINOåˆ©ç”¨è‡ªè’¸é¦æ¡†æ¶ï¼Œä½¿å­¦ç”Ÿç½‘ç»œå­¦ä¹ åŒ¹é…åŠ¨é‡æ›´æ–°çš„æ•™å¸ˆè¾“å‡ºï¼Œè·å¾—å¼ºå¤§çš„k-NNåˆ†ç±»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a18a3a6abfb6c9ac649e6950b4fd7fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb308890c31345f2aab07dd14fc1774c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43c7b7523d1996c598c4c86034ca318b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-498ae1a3c68c14971d462838ba0e1fa5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a42b24e9ff5111ddf2f9f617b925344.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LOTS-of-Fashion-Multi-Conditioning-for-Image-Generation-via-Sketch-Text-Pairing"><a href="#LOTS-of-Fashion-Multi-Conditioning-for-Image-Generation-via-Sketch-Text-Pairing" class="headerlink" title="LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text   Pairing"></a>LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text   Pairing</h2><p><strong>Authors:Federico Girella, Davide Talon, Ziyue Liu, Zanxi Ruan, Yiming Wang, Marco Cristani</strong></p>
<p>Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion modelâ€™s multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization. </p>
<blockquote>
<p>æœè£…è®¾è®¡æ˜¯ä¸€ä¸ªå¤æ‚çš„åˆ›é€ æ€§è¿‡ç¨‹ï¼Œèåˆäº†è§†è§‰å’Œæ–‡æœ¬è¡¨è¾¾ã€‚è®¾è®¡å¸ˆé€šè¿‡è‰å›¾ä¼ è¾¾æƒ³æ³•ï¼Œè¿™äº›è‰å›¾å®šä¹‰äº†ç©ºé—´ç»“æ„å’Œè®¾è®¡å…ƒç´ ï¼Œå¹¶é€šè¿‡æ–‡æœ¬æè¿°æ•æ‰ææ–™ã€çº¹ç†å’Œé£æ ¼ç»†èŠ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç”¨äºæœè£…å›¾åƒç”Ÿæˆçš„æœ¬åœ°åŒ–æ–‡æœ¬å’Œè‰å›¾ï¼ˆLOTSï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç»„åˆè‰å›¾æ–‡æœ¬ç”Ÿæˆå®Œæ•´æ—¶å°šå¤–è§‚çš„æ–¹æ³•ã€‚LOTSåˆ©ç”¨å…¨å±€æè¿°ä¸é…å¯¹å±€éƒ¨è‰å›¾+æ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºæ­¥éª¤çš„åˆå¹¶ç­–ç•¥æ¥è¿›è¡Œæ‰©æ•£é€‚åº”ã€‚é¦–å…ˆï¼Œæ¨¡å—åŒ–é…å¯¹ä¸­å¿ƒè¡¨ç¤ºæ³•å°†è‰å›¾å’Œæ–‡æœ¬ç¼–ç æˆå…±äº«æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™ç‹¬ç«‹å±€éƒ¨ç‰¹å¾ï¼›ç„¶åï¼Œæ‰©æ•£å¯¹æŒ‡å¯¼é˜¶æ®µé€šè¿‡æ‰©æ•£æ¨¡å‹å¤šæ­¥å»å™ªè¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›å¯¼å‘æœºåˆ¶å°†å±€éƒ¨å’Œå…¨å±€æ¡ä»¶ç›¸ç»“åˆã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨Fashionpediaçš„åŸºç¡€ä¸Šæ„å»ºäº†Sketchyæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ¯å¼ å›¾åƒéƒ½æä¾›å¤šä¸ªæ–‡æœ¬è‰å›¾å¯¹çš„æ•°æ®é›†ã€‚å®šé‡ç»“æœè¡¨æ˜ï¼ŒLOTSåœ¨å…¨çƒå’Œæœ¬åœ°åŒ–æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å›¾åƒç”Ÿæˆæ€§èƒ½ï¼Œè€Œå®šæ€§ç¤ºä¾‹å’Œäººç±»è¯„ä¼°ç ”ç©¶åˆ™çªå‡ºäº†å…¶å‰æ‰€æœªæœ‰çš„è®¾è®¡å®šåˆ¶æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22627v2">PDF</a> Accepted at ICCV25 (Oral). Project page:   <a target="_blank" rel="noopener" href="https://intelligolabs.github.io/lots/">https://intelligolabs.github.io/lots/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºå±€éƒ¨æ–‡æœ¬å’Œè‰å›¾ï¼ˆLOTSï¼‰çš„æ—¶å°šå›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å…¨å±€æè¿°ä¸å±€éƒ¨è‰å›¾æ–‡æœ¬ä¿¡æ¯ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªè¿‡ç¨‹å®ç°æ—¶å°šè®¾è®¡çš„ç”Ÿæˆã€‚LOTSæ–¹æ³•é‡‡ç”¨æ¨¡å—åŒ–é…å¯¹è¡¨ç¤ºæ³•ï¼Œå°†è‰å›¾ä¸æ–‡æœ¬ç¼–ç åˆ°å…±äº«æ½œåœ¨ç©ºé—´ä¸­ï¼ŒåŒæ—¶ä¿ç•™ç‹¬ç«‹å±€éƒ¨ç‰¹å¾ã€‚é€šè¿‡æ‰©æ•£é…å¯¹å¼•å¯¼é˜¶æ®µï¼Œå°†å±€éƒ¨å’Œå…¨å±€æ¡ä»¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚è¯¥æ–¹æ³•åœ¨Fashionpediaæ•°æ®é›†ä¸Šæ„å»ºå¹¶å‘å¸ƒäº†Sketchyæ•°æ®é›†ï¼Œå®ç°äº†é«˜æ°´å¹³çš„å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶å°šè®¾è®¡æ˜¯ä¸€ä¸ªå¤æ‚çš„åˆ›æ„è¿‡ç¨‹ï¼Œç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬è¡¨è¾¾ã€‚</li>
<li>LOTSæ–¹æ³•åˆ©ç”¨å…¨å±€æè¿°ä¸å±€éƒ¨è‰å›¾æ–‡æœ¬ä¿¡æ¯æ¥å®ç°æ—¶å°šè®¾è®¡çš„ç”Ÿæˆã€‚</li>
<li>æ¨¡å—åŒ–é…å¯¹è¡¨ç¤ºæ³•å°†è‰å›¾ä¸æ–‡æœ¬ç¼–ç åˆ°å…±äº«æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™ç‹¬ç«‹å±€éƒ¨ç‰¹å¾ã€‚</li>
<li>æ‰©æ•£é…å¯¹å¼•å¯¼é˜¶æ®µé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶é›†æˆå±€éƒ¨å’Œå…¨å±€æ¡ä»¶ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨Fashionpediaæ•°æ®é›†ä¸Šæ„å»ºå¹¶å‘å¸ƒäº†Sketchyæ•°æ®é›†ã€‚</li>
<li>LOTSæ–¹æ³•å®ç°äº†é«˜æ°´å¹³çš„å›¾åƒç”Ÿæˆæ€§èƒ½ï¼ŒåŒ…æ‹¬å…¨çƒå’Œå±€éƒ¨æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bf3e12cd37f93e0fde15ef1c2df63b95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b029cade73249cf1f07a0b5dec169137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c833ecb79f4d9c730af2e9f2f8516ed1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Diffusion-Model-Stability-for-Image-Restoration-via-Gradient-Management"><a href="#Enhancing-Diffusion-Model-Stability-for-Image-Restoration-via-Gradient-Management" class="headerlink" title="Enhancing Diffusion Model Stability for Image Restoration via Gradient   Management"></a>Enhancing Diffusion Model Stability for Image Restoration via Gradient   Management</h2><p><strong>Authors:Hongjie Wu, Mingqin Zhang, Linchao He, Ji-Zhe Zhou, Jiancheng Lv</strong></p>
<p>Diffusion models have shown remarkable promise for image restoration by leveraging powerful priors. Prominent methods typically frame the restoration problem within a Bayesian inference framework, which iteratively combines a denoising step with a likelihood guidance step. However, the interactions between these two components in the generation process remain underexplored. In this paper, we analyze the underlying gradient dynamics of these components and identify significant instabilities. Specifically, we demonstrate conflicts between the prior and likelihood gradient directions, alongside temporal fluctuations in the likelihood gradient itself. We show that these instabilities disrupt the generative process and compromise restoration performance. To address these issues, we propose Stabilized Progressive Gradient Diffusion (SPGD), a novel gradient management technique. SPGD integrates two synergistic components: (1) a progressive likelihood warm-up strategy to mitigate gradient conflicts; and (2) adaptive directional momentum (ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive experiments across diverse restoration tasks demonstrate that SPGD significantly enhances generation stability, leading to state-of-the-art performance in quantitative metrics and visually superior results. Code is available at <a target="_blank" rel="noopener" href="https://github.com/74587887/SPGD">https://github.com/74587887/SPGD</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†åœ¨å›¾åƒä¿®å¤é¢†åŸŸæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚æ˜¾è‘—çš„æ–¹æ³•é€šå¸¸åœ¨ä¸€ä¸ªè´å¶æ–¯æ¨æ–­æ¡†æ¶å†…æ„å»ºä¿®å¤é—®é¢˜ï¼Œè¯¥æ¡†æ¶è¿­ä»£åœ°ç»“åˆå»å™ªæ­¥éª¤å’Œå¯èƒ½æ€§æŒ‡å¯¼æ­¥éª¤ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿‡ç¨‹ä¸­è¿™ä¸¤ä¸ªç»„ä»¶ä¹‹é—´çš„ç›¸äº’ä½œç”¨ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›ç»„ä»¶çš„åº•å±‚æ¢¯åº¦åŠ¨æ€ï¼Œå¹¶å‘ç°äº†é‡å¤§ä¸ç¨³å®šç°è±¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å…ˆéªŒçŸ¥è¯†å’Œå¯èƒ½æ€§æ¢¯åº¦æ–¹å‘ä¹‹é—´çš„å†²çªï¼Œä»¥åŠå¯èƒ½æ€§æ¢¯åº¦æœ¬èº«çš„æš‚æ—¶æ³¢åŠ¨ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™äº›ä¸ç¨³å®šå› ç´ ç ´åäº†ç”Ÿæˆè¿‡ç¨‹å¹¶å½±å“äº†ä¿®å¤æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨³å®šçš„æ¸è¿›æ¢¯åº¦æ‰©æ•£ï¼ˆSPGDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ¢¯åº¦ç®¡ç†æŠ€æœ¯ã€‚SPGDé›†æˆä¸¤ä¸ªååŒç»„ä»¶ï¼šï¼ˆ1ï¼‰æ¸è¿›çš„å¯èƒ½æ€§é¢„çƒ­ç­–ç•¥ï¼Œä»¥ç¼“è§£æ¢¯åº¦å†²çªï¼›ï¼ˆ2ï¼‰è‡ªé€‚åº”æ–¹å‘åŠ¨é‡ï¼ˆADMï¼‰å¹³æ»‘ï¼Œä»¥å‡å°‘å¯èƒ½æ€§æ¢¯åº¦çš„æ³¢åŠ¨ã€‚åœ¨å¤šç§ä¿®å¤ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSPGDæ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆçš„ç¨³å®šæ€§ï¼Œå¹¶åœ¨å®šé‡æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè§†è§‰ä¸Šç»“æœä¹Ÿæ›´ä¸ºä¼˜è¶Šã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/74587887/SPGD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/74587887/SPGDè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06656v2">PDF</a> Accepted to ACM Multimedia 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†åœ¨å›¾åƒä¿®å¤æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‰æ™¯ã€‚ä¸»æµæ–¹æ³•é€šå¸¸å°†ä¿®å¤é—®é¢˜ç½®äºè´å¶æ–¯æ¨æ–­æ¡†æ¶å†…ï¼Œè¯¥æ¡†æ¶é€šè¿‡å»å™ªæ­¥éª¤å’Œå¯èƒ½æ€§å¼•å¯¼æ­¥éª¤çš„è¿­ä»£ç»„åˆæ¥å·¥ä½œã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿‡ç¨‹ä¸­è¿™ä¸¤ä¸ªç»„ä»¶ä¹‹é—´çš„ç›¸äº’ä½œç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›ç»„ä»¶çš„æ½œåœ¨æ¢¯åº¦åŠ¨æ€ï¼Œå¹¶å‘ç°äº†é‡å¤§ä¸ç¨³å®šç°è±¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº†å…ˆéªŒå’Œå¯èƒ½æ€§æ¢¯åº¦æ–¹å‘ä¹‹é—´çš„å†²çªï¼Œä»¥åŠå¯èƒ½æ€§æ¢¯åº¦æœ¬èº«çš„æš‚æ—¶æ³¢åŠ¨ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™äº›ä¸ç¨³å®šç°è±¡ç ´åäº†ç”Ÿæˆè¿‡ç¨‹å¹¶å½±å“äº†ä¿®å¤æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨³å®šçš„æ¸è¿›æ¢¯åº¦æ‰©æ•£ï¼ˆSPGDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¢¯åº¦ç®¡ç†æŠ€æœ¯ã€‚SPGDé›†æˆäº†ä¸¤ä¸ªååŒç»„ä»¶ï¼šï¼ˆ1ï¼‰æ¸è¿›çš„å¯èƒ½æ€§é¢„çƒ­ç­–ç•¥ï¼Œä»¥ç¼“è§£æ¢¯åº¦å†²çªï¼›ï¼ˆ2ï¼‰è‡ªé€‚åº”æ–¹å‘åŠ¨é‡ï¼ˆADMï¼‰å¹³æ»‘ï¼Œä»¥å‡å°‘å¯èƒ½æ€§æ¢¯åº¦çš„æ³¢åŠ¨ã€‚åœ¨å¤šç§ä¿®å¤ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSPGDæ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆç¨³å®šæ€§ï¼Œåœ¨å®šé‡æŒ‡æ ‡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨è§†è§‰ä¸Šäº§ç”Ÿäº†æ›´å¥½çš„ç»“æœã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—å‰æ™¯ï¼Œä¸»è¦ä½¿ç”¨è´å¶æ–¯æ¨æ–­æ¡†æ¶æ¥è§£å†³ä¿®å¤é—®é¢˜ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸­çš„ç”Ÿæˆè¿‡ç¨‹å­˜åœ¨é‡å¤§ä¸ç¨³å®šç°è±¡ï¼Œä¸»è¦ç”±äºå…ˆéªŒå’Œå¯èƒ½æ€§æ¢¯åº¦æ–¹å‘çš„å†²çªï¼Œä»¥åŠå¯èƒ½æ€§æ¢¯åº¦æœ¬èº«çš„æš‚æ—¶æ³¢åŠ¨ã€‚</li>
<li>è¿™äº›ä¸ç¨³å®šç°è±¡ä¼šå½±å“ç”Ÿæˆè¿‡ç¨‹å’Œä¿®å¤æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ç¨³å®šçš„æ¸è¿›æ¢¯åº¦æ‰©æ•£ï¼ˆSPGDï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>SPGDåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šæ¸è¿›çš„å¯èƒ½æ€§é¢„çƒ­ç­–ç•¥å’Œè‡ªé€‚åº”æ–¹å‘åŠ¨é‡ï¼ˆADMï¼‰å¹³æ»‘ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜SPGDèƒ½æ˜¾è‘—å¢å¼ºç”Ÿæˆç¨³å®šæ€§ï¼Œå¹¶è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-299da982081777524cb9aea87c906d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83bd010dc0821eb1fbfd34df76a5ea71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fabf07fd7884cdc9eb455f497a4b1de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce85bfb4ce435009352ea4758d364e65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f9f2f5e15b45cad5bd216b0748ee494.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LD-RPS-Zero-Shot-Unified-Image-Restoration-via-Latent-Diffusion-Recurrent-Posterior-Sampling"><a href="#LD-RPS-Zero-Shot-Unified-Image-Restoration-via-Latent-Diffusion-Recurrent-Posterior-Sampling" class="headerlink" title="LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion   Recurrent Posterior Sampling"></a>LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion   Recurrent Posterior Sampling</h2><p><strong>Authors:Huaqiu Li, Yong Wang, Tongwen Huang, Hailang Huang, Haoqian Wang, Xiangxiang Chu</strong></p>
<p>Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/LD-RPS">https://github.com/AMAP-ML/LD-RPS</a>. </p>
<blockquote>
<p>å›¾åƒç»Ÿä¸€æ¢å¤æ˜¯ä½çº§è§†è§‰ä¸­çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆé’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå®šåˆ¶è®¾è®¡ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨ä¸åŒç±»å‹é€€åŒ–ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¦ä¹ˆä¾èµ–äºé…å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå—åˆ°å°é—­é›†çº¦æŸçš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ã€æ— éœ€æ•°æ®é›†çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé€’å½’åé‡‡æ ·æ¥å®ç°å›¾åƒç»Ÿä¸€æ¢å¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¤šæ¨¡å¼ç†è§£æ¨¡å‹ï¼Œä¸ºä»»åŠ¡ç›²æ¡ä»¶ä¸‹çš„ç”Ÿæˆæ¨¡å‹æä¾›è¯­ä¹‰å…ˆéªŒã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åˆ©ç”¨äº†ä¸€ä¸ªè½»é‡çº§æ¨¡å—æ¥å¯¹é€€åŒ–è¾“å…¥ä¸æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåå¥½è¿›è¡Œå¯¹é½ï¼Œå¹¶é‡‡ç”¨äº†é€’å½’ç»†åŒ–è¿›è¡Œåé‡‡æ ·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/LD-RPS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/LD-RPSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00790v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€å›¾åƒæ¢å¤æ–¹æ³•ï¼Œæ— éœ€æ•°æ®é›†ã€‚è¯¥æ–¹æ³•é€šè¿‡é€’å½’åé‡‡æ ·æŠ€æœ¯ï¼Œç»“åˆå¤šæ¨¡æ€ç†è§£æ¨¡å‹ï¼Œä¸ºç”Ÿæˆæ¨¡å‹æä¾›è¯­ä¹‰å…ˆéªŒï¼ŒåŒæ—¶é‡‡ç”¨è½»é‡çº§æ¨¡å—å¯¹é½é€€åŒ–è¾“å…¥ä¸æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåå¥½ï¼Œå¹¶é€šè¿‡é€’å½’ä¼˜åŒ–è¿›è¡Œåé‡‡æ ·ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ç»Ÿä¸€å›¾åƒæ¢å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå…·æœ‰æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€å›¾åƒæ¢å¤æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨é€’å½’åé‡‡æ ·æŠ€æœ¯ï¼Œæ— éœ€ä½¿ç”¨æ•°æ®é›†ã€‚</li>
<li>ç»“åˆå¤šæ¨¡æ€ç†è§£æ¨¡å‹ï¼Œä¸ºç”Ÿæˆæ¨¡å‹æä¾›è¯­ä¹‰å…ˆéªŒã€‚</li>
<li>é‡‡ç”¨è½»é‡çº§æ¨¡å—å¯¹é½é€€åŒ–è¾“å…¥ä¸æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåå¥½ã€‚</li>
<li>é€šè¿‡é€’å½’ä¼˜åŒ–è¿›è¡Œåé‡‡æ ·ï¼Œæé«˜å›¾åƒæ¢å¤æ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ç»Ÿä¸€å›¾åƒæ¢å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00790">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4b44743101d6e0def28afe6d116de7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-107a307ad8c66246569b0a4ed2f4156f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc7448222e85fbeb4517d4534bf64612.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d79ef6f8754f4ab75da534ab70b9be6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55774104684831c5eb4d9a96ad5e0a29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c7887fe15c1eb5cbb673f497390ee43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d70f87d544706de6ddf281045bf9ead1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Integrating-Intermediate-Layer-Optimization-and-Projected-Gradient-Descent-for-Solving-Inverse-Problems-with-Diffusion-Models"><a href="#Integrating-Intermediate-Layer-Optimization-and-Projected-Gradient-Descent-for-Solving-Inverse-Problems-with-Diffusion-Models" class="headerlink" title="Integrating Intermediate Layer Optimization and Projected Gradient   Descent for Solving Inverse Problems with Diffusion Models"></a>Integrating Intermediate Layer Optimization and Projected Gradient   Descent for Solving Inverse Problems with Diffusion Models</h2><p><strong>Authors:Yang Zheng, Wen Li, Zhaoqiang Liu</strong></p>
<p>Inverse problems (IPs) involve reconstructing signals from noisy observations. Recently, diffusion models (DMs) have emerged as a powerful framework for solving IPs, achieving remarkable reconstruction performance. However, existing DM-based methods frequently encounter issues such as heavy computational demands and suboptimal convergence. In this work, building upon the idea of the recent work DMPlug, we propose two novel methods, DMILO and DMILO-PGD, to address these challenges. Our first method, DMILO, employs intermediate layer optimization (ILO) to alleviate the memory burden inherent in DMPlug. Additionally, by introducing sparse deviations, we expand the range of DMs, enabling the exploration of underlying signals that may lie outside the range of the diffusion model. We further propose DMILO-PGD, which integrates ILO with projected gradient descent (PGD), thereby reducing the risk of suboptimal convergence. We provide an intuitive theoretical analysis of our approaches under appropriate conditions and validate their superiority through extensive experiments on diverse image datasets, encompassing both linear and nonlinear IPs. Our results demonstrate significant performance gains over state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD in addressing common challenges in DM-based IP solvers. </p>
<blockquote>
<p>é€†é—®é¢˜ï¼ˆIPsï¼‰æ¶‰åŠä»å™ªå£°è§‚å¯Ÿä¸­é‡å»ºä¿¡å·ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºä¸€ç§è§£å†³IPsçš„å¼ºå¤§æ¡†æ¶å´­éœ²å¤´è§’ï¼Œå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„é‡å»ºæ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºDMçš„æ–¹æ³•ç»å¸¸é¢ä¸´è®¡ç®—é‡å¤§å’Œæ”¶æ•›ä¸ä½³ç­‰é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åŸºäºè¿‘æœŸå·¥ä½œDMPlugçš„æ€æƒ³ï¼Œæå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•DMILOå’ŒDMILO-PGDï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€ç§æ–¹æ³•DMILOé‡‡ç”¨ä¸­é—´å±‚ä¼˜åŒ–ï¼ˆILOï¼‰æ¥ç¼“è§£DMPlugæ‰€å›ºæœ‰çš„å†…å­˜è´Ÿæ‹…ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ç¨€ç–åå·®ï¼Œæˆ‘ä»¬æ‰©å¤§äº†DMsçš„èŒƒå›´ï¼Œèƒ½å¤Ÿæ¢ç´¢å¯èƒ½ä½äºæ‰©æ•£æ¨¡å‹èŒƒå›´ä¹‹å¤–çš„æ½œåœ¨ä¿¡å·ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†DMILO-PGDï¼Œå®ƒå°†ILOä¸æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰ç›¸ç»“åˆï¼Œä»è€Œé™ä½äº†æ”¶æ•›ä¸ä½³çš„é£é™©ã€‚æˆ‘ä»¬åœ¨é€‚å½“æ¡ä»¶ä¸‹å¯¹æ–¹æ³•è¿›è¡Œäº†ç›´è§‚çš„ç†è®ºåˆ†æï¼Œå¹¶é€šè¿‡å¯¹å„ç§å›¾åƒæ•°æ®é›†çš„å¤§é‡å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ï¼Œè¿™äº›å®éªŒæ¶µç›–äº†çº¿æ€§å’Œéçº¿æ€§IPsã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†DMILOå’ŒDMILO-PGDç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯æ–¹æ³•çš„æ˜¾è‘—æ€§èƒ½æå‡ï¼Œçªæ˜¾äº†å®ƒä»¬åœ¨è§£å†³åŸºäºDMçš„IPæ±‚è§£å™¨ä¸­çš„å¸¸è§æŒ‘æˆ˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20789v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨è§£å†³åé—®é¢˜ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å­˜åœ¨è®¡ç®—é‡å¤§å’Œæ”¶æ•›æ€§ä¸ä½³çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸¤ç§æ–°æ–¹æ³•DMILOå’ŒDMILO-PGDï¼Œå‰è€…é€šè¿‡ä¸­é—´å±‚ä¼˜åŒ–å’Œç¨€ç–åå·®æ¥æ‰©å±•æ¨¡å‹çš„æ¢ç´¢èŒƒå›´ï¼Œåè€…ç»“åˆäº†ä¸­é—´å±‚ä¼˜åŒ–ä¸æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼Œä»¥é™ä½æ¬¡ä¼˜æ”¶æ•›çš„é£é™©ã€‚å®éªŒè¯æ˜ï¼Œä¸¤ç§æ–¹æ³•åœ¨å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§£å†³åé—®é¢˜ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹æ–¹æ³•é¢ä¸´è®¡ç®—é‡å¤§å’Œæ”¶æ•›æ€§ä¸ä½³çš„æŒ‘æˆ˜ã€‚</li>
<li>DMILOæ–¹æ³•é€šè¿‡ä¸­é—´å±‚ä¼˜åŒ–å’Œç¨€ç–åå·®æ¥æ‰©å±•æ¨¡å‹çš„æ¢ç´¢èŒƒå›´ï¼Œå‡è½»å†…å­˜è´Ÿæ‹…ã€‚</li>
<li>DMILO-PGDç»“åˆäº†ä¸­é—´å±‚ä¼˜åŒ–ä¸æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼Œæé«˜æ”¶æ•›æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡å¯¹ä¸¤ç§æ–¹æ³•è¿›è¡Œäº†ç›´è§‚çš„ç†è®ºåˆ†æã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œä¸¤ç§æ–¹æ³•åœ¨å¤šç§å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c267f46e13650935f28dd88fe83e2629.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96e3b8e467a8db9b0a2a50ce4f540073.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13e0833b9302a3ad16fce71e030957bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-739d1d6742170fa5e7308a6a025205d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18dcb198023a5b623c25e20445002d04.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Res-MoCoDiff-Residual-guided-diffusion-models-for-motion-artifact-correction-in-brain-MRI"><a href="#Res-MoCoDiff-Residual-guided-diffusion-models-for-motion-artifact-correction-in-brain-MRI" class="headerlink" title="Res-MoCoDiff: Residual-guided diffusion models for motion artifact   correction in brain MRI"></a>Res-MoCoDiff: Residual-guided diffusion models for motion artifact   correction in brain MRI</h2><p><strong>Authors:Mojtaba Safari, Shansong Wang, Qiang Li, Zach Eidex, Richard L. J. Qiu, Chih-Wei Chang, Hui Mao, Xiaofeng Yang</strong></p>
<p>Objective. Motion artifacts in brain MRI, mainly from rigid head motion, degrade image quality and hinder downstream applications. Conventional methods to mitigate these artifacts, including repeated acquisitions or motion tracking, impose workflow burdens. This study introduces Res-MoCoDiff, an efficient denoising diffusion probabilistic model specifically designed for MRI motion artifact correction.Approach.Res-MoCoDiff exploits a novel residual error shifting mechanism during the forward diffusion process to incorporate information from motion-corrupted images. This mechanism allows the model to simulate the evolution of noise with a probability distribution closely matching that of the corrupted data, enabling a reverse diffusion process that requires only four steps. The model employs a U-net backbone, with attention layers replaced by Swin Transformer blocks, to enhance robustness across resolutions. Furthermore, the training process integrates a combined l1+l2 loss function, which promotes image sharpness and reduces pixel-level errors. Res-MoCoDiff was evaluated on both an in-silico dataset generated using a realistic motion simulation framework and an in-vivo MR-ART dataset. Comparative analyses were conducted against established methods, including CycleGAN, Pix2pix, and a diffusion model with a vision transformer backbone, using quantitative metrics such as PSNR, SSIM, and NMSE.Main results. The proposed method demonstrated superior performance in removing motion artifacts across minor, moderate, and heavy distortion levels. Res-MoCoDiff consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling time was reduced to 0.37 seconds per batch of two image slices, compared with 101.74 seconds for conventional approaches. </p>
<blockquote>
<p>ç›®æ ‡ï¼šè„‘MRIä¸­çš„è¿åŠ¨ä¼ªå½±ï¼Œä¸»è¦æ¥è‡ªå¤´éƒ¨åˆšæ€§è¿åŠ¨ï¼Œä¼šé™ä½å›¾åƒè´¨é‡å¹¶é˜»ç¢ä¸‹æ¸¸åº”ç”¨ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬é‡å¤é‡‡é›†æˆ–è¿åŠ¨è·Ÿè¸ªï¼Œä¼šç»™å·¥ä½œæµç¨‹å¸¦æ¥è´Ÿæ‹…ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†Res-MoCoDiffï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºMRIè¿åŠ¨ä¼ªå½±æ ¡æ­£è®¾è®¡çš„é«˜æ•ˆå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šRes-MoCoDiffåœ¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­åˆ©ç”¨æ–°é¢–æ®‹å·®è¯¯å·®è½¬ç§»æœºåˆ¶ï¼Œèå…¥è¿åŠ¨ä¼ªå½±å›¾åƒçš„ä¿¡æ¯ã€‚è¿™ç§æœºåˆ¶å…è®¸æ¨¡å‹æ¨¡æ‹Ÿå™ªå£°æ¼”å˜ï¼Œå…¶æ¦‚ç‡åˆ†å¸ƒä¸å—å¹²æ‰°æ•°æ®ç›¸åŒ¹é…ï¼Œå®ç°ä»…éœ€è¦å››ä¸ªæ­¥éª¤çš„åå‘æ‰©æ•£è¿‡ç¨‹ã€‚æ¨¡å‹é‡‡ç”¨U-netéª¨å¹²ç½‘ï¼Œç”¨Swin Transformerå—æ›¿æ¢æ³¨æ„åŠ›å±‚ï¼Œä»¥å¢å¼ºè·¨åˆ†è¾¨ç‡çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œè®­ç»ƒè¿‡ç¨‹ç»“åˆäº†l1+l2æŸå¤±å‡½æ•°ï¼Œè¿™æœ‰åŠ©äºæé«˜å›¾åƒæ¸…æ™°åº¦å’Œå‡å°‘åƒç´ çº§é”™è¯¯ã€‚Res-MoCoDiffåœ¨åˆ©ç”¨ç°å®è¿åŠ¨æ¨¡æ‹Ÿæ¡†æ¶ç”Ÿæˆçš„æ¨¡æ‹Ÿæ•°æ®é›†å’Œä½“å†…MR-ARTæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ä¸CycleGANã€Pix2pixå’Œå…·æœ‰è§†è§‰è½¬æ¢å™¨éª¨å¹²çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒæ€§åˆ†æï¼Œä½¿ç”¨äº†PSNRã€SSIMå’ŒNMSEç­‰å®šé‡æŒ‡æ ‡ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03498v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹MRIè¿åŠ¨ä¼ªå½±æ ¡æ­£çš„é«˜æ•ˆå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹â€”â€”Res-MoCoDiffã€‚è¯¥æ–¹æ³•é€šè¿‡æ–°é¢–çš„æ®‹å·®è¯¯å·®ç§»ä½æœºåˆ¶åœ¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­èå…¥è¿åŠ¨å—å¹²æ‰°å›¾åƒçš„ä¿¡æ¯ã€‚å®ƒèƒ½æ¨¡æ‹Ÿå™ªå£°æ¼”å˜ï¼Œå¹¶ä¸å—å¹²æ‰°æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒç›¸åŒ¹é…ï¼Œä½¿åå‘æ‰©æ•£è¿‡ç¨‹ä»…éœ€å››æ­¥ã€‚æ¨¡å‹é‡‡ç”¨U-netæ¶æ„ï¼Œå¹¶ç”¨Swin Transformerå—å¢å¼ºè·¨åˆ†è¾¨ç‡çš„ç¨³å¥æ€§ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®MRæ•°æ®ä¸­ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒRes-MoCoDiffåœ¨å»é™¤å„çº§è¿åŠ¨ä¼ªå½±æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œé‡‡æ ·æ—¶é—´å¤§å¤§å‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Res-MoCoDiffæ˜¯ä¸€ç§é’ˆå¯¹MRIè¿åŠ¨ä¼ªå½±æ ¡æ­£çš„æ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ–°é¢–çš„æ®‹å·®è¯¯å·®ç§»ä½æœºåˆ¶åœ¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­èå…¥è¿åŠ¨å—å¹²æ‰°å›¾åƒçš„ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨U-netæ¶æ„ï¼Œå¹¶ç”¨Swin Transformerå—æé«˜è·¨åˆ†è¾¨ç‡çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®MRæ•°æ®ä¸­ï¼ŒRes-MoCoDiffåœ¨å»é™¤å„çº§è¿åŠ¨ä¼ªå½±æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒRes-MoCoDiffçš„é‡‡æ ·æ—¶é—´å¤§å¤§å‡å°‘ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨ç»“åˆl1+l2æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œæœ‰åŠ©äºæé«˜å›¾åƒæ¸…æ™°åº¦å’Œå‡å°‘åƒç´ çº§è¯¯å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03498">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e768719b0bf8cc7459d1ec866b499e22.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LATINO-PRO-LAtent-consisTency-INverse-sOlver-with-PRompt-Optimization"><a href="#LATINO-PRO-LAtent-consisTency-INverse-sOlver-with-PRompt-Optimization" class="headerlink" title="LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization"></a>LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization</h2><p><strong>Authors:Alessio Spagnoletti, Jean Prost, AndrÃ©s Almansa, Nicolas Papadakis, Marcelo Pereyra</strong></p>
<p>Text-to-image latent diffusion models (LDMs) have recently emerged as powerful generative models with great potential for solving inverse problems in imaging. However, leveraging such models in a Plug &amp; Play (PnP), zero-shot manner remains challenging because it requires identifying a suitable text prompt for the unknown image of interest. Also, existing text-to-image PnP approaches are highly computationally expensive. We herein address these challenges by proposing a novel PnP inference paradigm specifically designed for embedding generative models within stochastic inverse solvers, with special attention to Latent Consistency Models (LCMs), which distill LDMs into fast generators. We leverage our framework to propose LAtent consisTency INverse sOlver (LATINO), the first zero-shot PnP framework to solve inverse problems with priors encoded by LCMs. Our conditioning mechanism avoids automatic differentiation and reaches SOTA quality in as little as 8 neural function evaluations. As a result, LATINO delivers remarkably accurate solutions and is significantly more memory and computationally efficient than previous approaches. We then embed LATINO within an empirical Bayesian framework that automatically calibrates the text prompt from the observed measurements by marginal maximum likelihood estimation. Extensive experiments show that prompt self-calibration greatly improves estimation, allowing LATINO with PRompt Optimization to define new SOTAs in image reconstruction quality and computational efficiency. The code is available at <a target="_blank" rel="noopener" href="https://latino-pro.github.io/">https://latino-pro.github.io</a> </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰æœ€è¿‘ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹å‡ºç°ï¼Œåœ¨æˆåƒä¸­çš„é€†é—®é¢˜è§£å†³æ–¹æ¡ˆæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä»¥Plug &amp; Playï¼ˆPnPï¼‰çš„æ–¹å¼åˆ©ç”¨è¿™äº›æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™éœ€è¦é’ˆå¯¹æ„Ÿå…´è¶£çš„æœªçŸ¥å›¾åƒç¡®å®šåˆé€‚çš„æ–‡æœ¬æç¤ºã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒPnPæ–¹æ³•åœ¨è®¡ç®—ä¸Šéå¸¸æ˜‚è´µã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§æ–°å‹PnPæ¨ç†èŒƒå¼æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥èŒƒå¼ä¸“é—¨è®¾è®¡ç”¨äºå°†ç”Ÿæˆæ¨¡å‹åµŒå…¥éšæœºé€†æ±‚è§£å™¨ä¸­ï¼Œç‰¹åˆ«å…³æ³¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMsï¼‰ï¼Œå®ƒå°†LDMè’¸é¦æˆå¿«é€Ÿç”Ÿæˆå™¨ã€‚æˆ‘ä»¬åˆ©ç”¨æˆ‘ä»¬çš„æ¡†æ¶æå‡ºäº†LAtent consisTency INverse sOlverï¼ˆLATINOï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»¥é›¶å°„å‡»æ–¹å¼è§£å†³é€†é—®é¢˜çš„PnPæ¡†æ¶ï¼Œå…¶å…ˆéªŒç”±LCMsç¼–ç ã€‚æˆ‘ä»¬çš„è°ƒèŠ‚æœºåˆ¶é¿å…äº†è‡ªåŠ¨å¾®åˆ†ï¼Œå¹¶åœ¨ä»…8ä¸ªç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚å› æ­¤ï¼ŒLATINOæä¾›äº†éå¸¸å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨å†…å­˜å’Œè®¡ç®—æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚ç„¶åæˆ‘ä»¬å°†LATINOåµŒå…¥ç»éªŒè´å¶æ¡†æ¶ä¸­ï¼Œè¯¥æ¡†æ¶é€šè¿‡è¾¹é™…æœ€å¤§ä¼¼ç„¶ä¼°è®¡è‡ªåŠ¨æ ¡å‡†æ–‡æœ¬æç¤ºæ¥è‡ªè§‚å¯Ÿåˆ°çš„æµ‹é‡å€¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæç¤ºè‡ªæˆ‘æ ¡å‡†æå¤§åœ°æé«˜äº†ä¼°è®¡å€¼ï¼Œä½¿å¾—LATINOä¸PRompt Optimizationä¸€èµ·åœ¨å›¾åƒé‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å®šä¹‰äº†æ–°çš„æœ€æ–°æŠ€æœ¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://latino-pro.github.ioæ‰¾åˆ°./">https://latino-pro.github.ioæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12615v2">PDF</a> 27 pages, 24 figures, International Conference on Computer Vision,   ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰æ˜¯æ–°å…´çš„å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰è§£å†³æˆåƒé€†é—®é¢˜çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä»¥Plug &amp; Playï¼ˆPnPï¼‰å³æ’å³ç”¨æ–¹å¼å’Œé›¶æ ·æœ¬æ–¹å¼åˆ©ç”¨æ­¤ç±»æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºéœ€è¦ä¸ºæœªçŸ¥å›¾åƒé€‰æ‹©åˆé€‚çš„æ–‡æœ¬æç¤ºã€‚ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒçš„PnPæ–¹æ³•è®¡ç®—é‡å¤§ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„PnPæ¨ç†èŒƒå¼ï¼Œä¸“ä¸ºå°†ç”Ÿæˆæ¨¡å‹åµŒå…¥éšæœºé€†æ±‚è§£å™¨è€Œè®¾è®¡ï¼Œç‰¹åˆ«å…³æ³¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMsï¼‰ï¼Œå®ƒå°†LDMè’¸é¦ä¸ºå¿«é€Ÿç”Ÿæˆå™¨ã€‚æˆ‘ä»¬åˆ©ç”¨è¯¥æ¡†æ¶æå‡ºLAtent consisTency INverse sOlverï¼ˆLATINOï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé›¶æ ·æœ¬PnPæ¡†æ¶ï¼Œä½¿ç”¨LCMsç¼–ç çš„å…ˆéªŒçŸ¥è¯†è§£å†³é€†é—®é¢˜ã€‚å…¶è°ƒèŠ‚æœºåˆ¶é¿å…äº†è‡ªåŠ¨å¾®åˆ†ï¼Œå¹¶åœ¨ä»…8ä¸ªç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€ä½³è´¨é‡ã€‚LATINOæä¾›äº†ç²¾å‡†è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨å†…å­˜å’Œè®¡ç®—æ–¹é¢æ¯”ä»¥å‰çš„æ–¹æ³•æ›´æœ‰æ•ˆç‡ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†LATINOåµŒå…¥ç»éªŒè´å¶æ–¯æ¡†æ¶ä¸­ï¼Œé€šè¿‡è¾¹é™…æœ€å¤§ä¼¼ç„¶ä¼°è®¡è‡ªåŠ¨æ ¡å‡†æ–‡æœ¬æç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæç¤ºè‡ªæˆ‘æ ¡å‡†å¤§å¤§æé«˜äº†ä¼°ç®—æ•ˆæœï¼Œä½¿LATINO with PRompt Optimizationåœ¨å›¾åƒé‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å®šä¹‰äº†æ–°çš„æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰æ˜¯å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½è§£å†³æˆåƒé€†é—®é¢˜ã€‚<br>2.Plug &amp; Playæ–¹å¼åˆ©ç”¨æ­¤ç±»æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€æ‰¾åˆ°åˆé€‚çš„æ–‡æœ¬æç¤ºã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çš„PnPæ¨ç†èŒƒå¼ï¼ŒåµŒå…¥éšæœºé€†æ±‚è§£å™¨ï¼Œç‰¹åˆ«å…³æ³¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMsï¼‰ã€‚</li>
<li>å¼•å…¥LAtent consisTency INverse sOlverï¼ˆLATINOï¼‰ï¼Œèƒ½åœ¨é›¶æ ·æœ¬PnPæ¡†æ¶ä¸‹è§£å†³é€†é—®é¢˜ã€‚</li>
<li>LATINOè°ƒèŠ‚æœºåˆ¶é¿å…è‡ªåŠ¨å¾®åˆ†ï¼Œè¾¾åˆ°é«˜è´¨é‡è§£å†³æ–¹æ¡ˆä¸”è®¡ç®—æ•ˆç‡é«˜ã€‚</li>
<li>å°†LATINOåµŒå…¥ç»éªŒè´å¶æ–¯æ¡†æ¶è¿›è¡Œæ–‡æœ¬æç¤ºçš„è‡ªæˆ‘æ ¡å‡†ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œè‡ªæˆ‘æ ¡å‡†çš„æç¤ºæé«˜äº†ä¼°ç®—æ•ˆæœï¼Œä½¿LATINOåœ¨å›¾åƒé‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¸Šè¾¾åˆ°æ–°çš„æœ€ä½³æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe479e3d4203843afbb8dffd427e8b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1be5b48a90b1ef2dee01d2988787990.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-caeaf30d136b204ce59a8566aac431c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-719028cb5e69dfd4ce467ca3e1af710b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MUNBa-Machine-Unlearning-via-Nash-Bargaining"><a href="#MUNBa-Machine-Unlearning-via-Nash-Bargaining" class="headerlink" title="MUNBa: Machine Unlearning via Nash Bargaining"></a>MUNBa: Machine Unlearning via Nash Bargaining</h2><p><strong>Authors:Jing Wu, Mehrtash Harandi</strong></p>
<p>Machine Unlearning (MU) aims to selectively erase harmful behaviors from models while retaining the overall utility of the model. As a multi-task learning problem, MU involves balancing objectives related to forgetting specific concepts&#x2F;data and preserving general performance. A naive integration of these forgetting and preserving objectives can lead to gradient conflicts and dominance, impeding MU algorithms from reaching optimal solutions. To address the gradient conflict and dominance issue, we reformulate MU as a two-player cooperative game, where the two players, namely, the forgetting player and the preservation player, contribute via their gradient proposals to maximize their overall gain and balance their contributions. To this end, inspired by the Nash bargaining theory, we derive a closed-form solution to guide the model toward the Pareto stationary point. Our formulation of MU guarantees an equilibrium solution, where any deviation from the final state would lead to a reduction in the overall objectives for both players, ensuring optimality in each objective. We evaluate our algorithmâ€™s effectiveness on a diverse set of tasks across image classification and image generation. Extensive experiments with ResNet, vision-language model CLIP, and text-to-image diffusion models demonstrate that our method outperforms state-of-the-art MU algorithms, achieving a better trade-off between forgetting and preserving. Our results also highlight improvements in forgetting precision, preservation of generalization, and robustness against adversarial attacks. </p>
<blockquote>
<p>æœºå™¨é—å¿˜ï¼ˆMUï¼‰æ—¨åœ¨é€‰æ‹©æ€§åœ°ä»æ¨¡å‹ä¸­åˆ é™¤æœ‰å®³è¡Œä¸ºï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„æ€»ä½“æ•ˆç”¨ã€‚ä½œä¸ºå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼ŒMUæ¶‰åŠå¹³è¡¡ä¸é—å¿˜ç‰¹å®šæ¦‚å¿µ&#x2F;æ•°æ®ç›¸å…³çš„ç›®æ ‡å’Œä¿æŒæ•´ä½“æ€§èƒ½çš„ç›®æ ‡ã€‚è¿™äº›é—å¿˜å’Œä¿ç•™ç›®æ ‡çš„ç®€å•é›†æˆå¯èƒ½å¯¼è‡´æ¢¯åº¦å†²çªå’Œä¸»å¯¼ï¼Œé˜»ç¢MUç®—æ³•è¾¾åˆ°æœ€ä¼˜è§£ã€‚ä¸ºäº†è§£å†³æ¢¯åº¦å†²çªå’Œä¸»å¯¼é—®é¢˜ï¼Œæˆ‘ä»¬å°†MUé‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªä¸¤äººåˆä½œæ¸¸æˆï¼Œå…¶ä¸­ä¸¤ä¸ªç©å®¶ï¼Œå³é—å¿˜ç©å®¶å’Œä¿ç•™ç©å®¶ï¼Œé€šè¿‡ä»–ä»¬çš„æ¢¯åº¦ææ¡ˆåšå‡ºè´¡çŒ®ï¼Œä»¥æœ€å¤§åŒ–ä»–ä»¬çš„æ•´ä½“æ”¶ç›Šå¹¶å¹³è¡¡ä»–ä»¬çš„è´¡çŒ®ã€‚ä¸ºæ­¤ï¼Œå—çº³ä»€è°ˆåˆ¤ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºä¸€ä¸ªå°é—­å½¢å¼çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥å¼•å¯¼æ¨¡å‹èµ°å‘å¸•ç´¯æ‰˜ç¨³å®šç‚¹ã€‚æˆ‘ä»¬å¯¹MUçš„è¡¨è¿°ä¿è¯äº†å‡è¡¡è§£ï¼Œä»»ä½•åç¦»æœ€ç»ˆçŠ¶æ€éƒ½ä¼šå¯¼è‡´ä¸¤ä¸ªç©å®¶çš„æ•´ä½“ç›®æ ‡å‡å°‘ï¼Œä»è€Œç¡®ä¿æ¯ä¸ªç›®æ ‡çš„æœ€ä¼˜æ€§ã€‚æˆ‘ä»¬åœ¨å›¾åƒåˆ†ç±»å’Œå›¾åƒç”Ÿæˆçš„å„ç§ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨ResNetã€è§†è§‰è¯­è¨€æ¨¡å‹CLIPå’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é—å¿˜å’Œä¿ç•™ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œä¼˜äºæœ€æ–°çš„MUç®—æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜çªå‡ºäº†åœ¨é—å¿˜ç²¾åº¦ã€ä¿æŒæ³›åŒ–èƒ½åŠ›å’Œå¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§æ–¹é¢çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15537v4">PDF</a> </p>
<p><strong>Summary</strong><br>     æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„æœºå™¨é—å¿˜ï¼ˆMUï¼‰æ—¨åœ¨é€‰æ‹©æ€§åˆ é™¤æ¨¡å‹ä¸­çš„æœ‰å®³è¡Œä¸ºï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„æ€»ä½“æ•ˆç”¨ã€‚MUä½œä¸ºä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼Œæ¶‰åŠå¹³è¡¡é—å¿˜ç‰¹å®šæ¦‚å¿µ&#x2F;æ•°æ®å’Œä¿æŒæ•´ä½“æ€§èƒ½çš„ç›®æ ‡ã€‚ç„¶è€Œï¼Œç®€å•åœ°å°†è¿™äº›é—å¿˜å’Œä¿ç•™ç›®æ ‡é›†æˆåœ¨ä¸€èµ·å¯èƒ½å¯¼è‡´æ¢¯åº¦å†²çªå’Œä¸»å¯¼é—®é¢˜ï¼Œé˜»ç¢MUç®—æ³•è¾¾åˆ°æœ€ä¼˜è§£ã€‚ä¸ºè§£å†³æ¢¯åº¦å†²çªå’Œä¸»å¯¼é—®é¢˜ï¼Œæˆ‘ä»¬å°†MUé‡æ–°æ„å»ºä¸ºä¸€ä¸ªåŒäººåˆä½œæ¸¸æˆï¼Œå…¶ä¸­é—å¿˜ç©å®¶å’Œä¿ç•™ç©å®¶é€šè¿‡å…¶æ¢¯åº¦ææ¡ˆæ¥æœ€å¤§åŒ–å…¶æ•´ä½“æ”¶ç›Šå¹¶å¹³è¡¡å…¶è´¡çŒ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å—åˆ°çº³ä»€è°ˆåˆ¤ç†è®ºçš„å¯å‘ï¼Œæ¨å¯¼å‡ºä¸€ä¸ªå°é—­å½¢å¼çš„è§£å†³æ–¹æ¡ˆæ¥æŒ‡å¯¼æ¨¡å‹èµ°å‘å¸•ç´¯æ‰˜ç¨³å®šç‚¹ã€‚æˆ‘ä»¬çš„MUå…¬å¼ä¿è¯äº†å‡è¡¡è§£ï¼Œä»»ä½•åç¦»æœ€ç»ˆçŠ¶æ€éƒ½ä¼šå¯¼è‡´æ•´ä½“ç›®æ ‡å‡å°‘ï¼Œä»è€Œç¡®ä¿æ¯ä¸ªç›®æ ‡çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨å›¾åƒåˆ†ç±»å’Œå›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ä¸Šè¯„ä¼°äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨ResNetã€è§†è§‰è¯­è¨€æ¨¡å‹CLIPå’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„MUç®—æ³•ï¼Œåœ¨é—å¿˜å’Œä¿ç•™ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜æ˜¾ç¤ºäº†åœ¨é—å¿˜ç²¾åº¦ã€ä¿æŒæ³›åŒ–èƒ½åŠ›å’Œå¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§æ–¹é¢çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨é—å¿˜ï¼ˆMUï¼‰æ—¨åœ¨ä»æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­åˆ é™¤æœ‰å®³è¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ•´ä½“æ•ˆç”¨ã€‚</li>
<li>MUé¢ä¸´æ¢¯åº¦å†²çªå’Œä¸»å¯¼é—®é¢˜çš„æŒ‘æˆ˜ã€‚</li>
<li>å°†MUé‡æ–°æ„å»ºä¸ºåŒäººåˆä½œæ¸¸æˆï¼Œå…¶ä¸­é—å¿˜ç©å®¶å’Œä¿ç•™ç©å®¶é€šè¿‡æ¢¯åº¦ææ¡ˆè¿›è¡Œåˆä½œã€‚</li>
<li>å—çº³ä»€è°ˆåˆ¤ç†è®ºå¯å‘ï¼Œæ¨å¯¼å‡ºä¸€ä¸ªå°é—­å½¢å¼çš„è§£å†³æ–¹æ¡ˆæ¥æŒ‡å¯¼æ¨¡å‹èµ°å‘å¸•ç´¯æ‰˜ç¨³å®šç‚¹ã€‚</li>
<li>æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»å’Œå›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸æœ€å…ˆè¿›çš„MUç®—æ³•ç›¸æ¯”ï¼Œåœ¨é—å¿˜å’Œä¿ç•™ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-04039d31686df04ae3953726abf9b91b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ac2870e24d1cc6241c80e2c95acc807.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52e9a49bdb0f38fefeeaa7cf435e49cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-510f7093481ccd57572760d85c6fde36.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Hardware-Friendly-Diffusion-Models-with-Fixed-Size-Reusable-Structures-for-On-Device-Image-Generation"><a href="#Hardware-Friendly-Diffusion-Models-with-Fixed-Size-Reusable-Structures-for-On-Device-Image-Generation" class="headerlink" title="Hardware-Friendly Diffusion Models with Fixed-Size Reusable Structures   for On-Device Image Generation"></a>Hardware-Friendly Diffusion Models with Fixed-Size Reusable Structures   for On-Device Image Generation</h2><p><strong>Authors:Sanchar Palit, Sathya Veera Reddy Dendi, Mallikarjuna Talluri, Raj Narayana Gadde</strong></p>
<p>Vision Transformers and U-Net architectures have been widely adopted in the implementation of Diffusion Models. However, each architecture presents specific challenges while realizing them on-device. Vision Transformers require positional embedding to maintain correspondence between the tokens processed by the transformer, although they offer the advantage of using fixed-size, reusable repetitive blocks following tokenization. The U-Net architecture lacks these attributes, as it utilizes variable-sized intermediate blocks for down-convolution and up-convolution in the noise estimation backbone for the diffusion process. To address these issues, we propose an architecture that utilizes a fixed-size, reusable transformer block as a core structure, making it more suitable for hardware implementation. Our architecture is characterized by low complexity, token-free design, absence of positional embeddings, uniformity, and scalability, making it highly suitable for deployment on mobile and resource-constrained devices. The proposed model exhibit competitive and consistent performance across both unconditional and conditional image generation tasks. The model achieved a state-of-the-art FID score of 1.6 on unconditional image generation with the CelebA. </p>
<blockquote>
<p>åœ¨Diffusion Modelsçš„å®ç°è¿‡ç¨‹ä¸­ï¼ŒVision Transformerså’ŒU-Netæ¶æ„å·²è¢«å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œæ¯ç§æ¶æ„åœ¨è®¾å¤‡ä¸Šå®ç°æ—¶éƒ½é¢ä¸´ç€ç‰¹å®šçš„æŒ‘æˆ˜ã€‚Vision Transformerséœ€è¦ä½ç½®åµŒå…¥æ¥ä¿æŒç»è¿‡å˜å‹å™¨å¤„ç†çš„ä»¤ç‰Œä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå°½ç®¡å®ƒä»¬æä¾›äº†ä½¿ç”¨å›ºå®šå¤§å°çš„ã€å¯é‡å¤ä½¿ç”¨çš„é‡å¤å—è¿›è¡Œä»¤ç‰ŒåŒ–åçš„ä¼˜åŠ¿ã€‚U-Netæ¶æ„ç¼ºå°‘è¿™äº›å±æ€§ï¼Œå› ä¸ºå®ƒä½¿ç”¨å¯å˜å¤§å°çš„ä¸­é—´å—æ¥è¿›è¡Œå»å·ç§¯å’Œæ‰©æ•£è¿‡ç¨‹ä¸­çš„å™ªå£°ä¼°è®¡ä¸»å¹²ä¸­çš„ä¸Šå·ç§¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å›ºå®šå¤§å°çš„ã€å¯é‡å¤ä½¿ç”¨çš„å˜å‹å™¨å—ä½œä¸ºæ ¸å¿ƒç»“æ„çš„æ¶æ„ï¼Œä½¿å…¶æ›´é€‚åˆç¡¬ä»¶å®ç°ã€‚æˆ‘ä»¬çš„æ¶æ„ç‰¹ç‚¹æ˜¯å¤æ‚åº¦ä½ã€æ— ä»¤ç‰Œè®¾è®¡ã€æ— éœ€ä½ç½®åµŒå…¥ã€é€šç”¨æ€§å¼ºå’Œå¯æ‰©å±•æ€§é«˜ï¼Œéå¸¸é€‚åˆåœ¨ç§»åŠ¨è®¾å¤‡å’Œèµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚æ‰€æå‡ºæ¨¡å‹çš„æ€§èƒ½åœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡å’Œæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­éƒ½å…·æœ‰ç«äº‰åŠ›å’Œä¸€è‡´æ€§ã€‚è¯¥æ¨¡å‹åœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„FIDåˆ†æ•°1.6ï¼Œä½¿ç”¨CelebAæ•°æ®é›†çš„æ¨¡å‹æ•ˆæœæœ€ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06119v2">PDF</a> presented at IJCNN 2025 poster track</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Vision Transformerså’ŒU-Netæ¶æ„åœ¨Diffusion Modelsä¸­çš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å›ºå®šå¤§å°ã€å¯é‡ç”¨transformerå—ä½œä¸ºæ ¸å¿ƒç»“æ„çš„æ–°æ¶æ„ï¼Œå…·æœ‰ä½å¤æ‚åº¦ã€æ— ä»¤ç‰Œè®¾è®¡ã€æ— éœ€ä½ç½®åµŒå…¥ã€é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ç­‰ç‰¹ç‚¹ï¼Œéå¸¸é€‚åˆåœ¨ç§»åŠ¨å’Œèµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚è¯¥æ¨¡å‹åœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„FIDåˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformerså’ŒU-Netæ¶æ„åœ¨Diffusion Modelsä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li>
<li>Vision Transformerséœ€è¦ä½ç½®åµŒå…¥æ¥ä¿æŒä»¤ç‰Œé—´çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>U-Netæ¶æ„ç¼ºä¹å›ºå®šå¤§å°çš„ä¸­é—´å—ï¼Œé€‚ç”¨äºå™ªå£°ä¼°è®¡ä¸»å¹²è¿›è¡Œæ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>æ–°æå‡ºçš„æ¶æ„åˆ©ç”¨å›ºå®šå¤§å°ã€å¯é‡ç”¨çš„transformerå—ä½œä¸ºæ ¸å¿ƒç»“æ„ã€‚</li>
<li>æ–°æ¶æ„å…·æœ‰ä½å¤æ‚åº¦ã€æ— ä»¤ç‰Œè®¾è®¡ã€æ— éœ€ä½ç½®åµŒå…¥ã€é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>æ–°æ¨¡å‹é€‚åˆåœ¨ç§»åŠ¨å’Œèµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.06119">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09f6c88b79c68bb321c37b4b98a2c53a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e24c70eaf4db2db8dcc984e50f84d04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c52e9e2a33b1e11a88eb36b321734e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-871a628bb247a238ab041124a1111bb2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b574c2adda4ff49049586c2e9cb7d274.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance"><a href="#Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance" class="headerlink" title="Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance"></a>Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance</h2><p><strong>Authors:Quang-Huy Che, Duc-Tri Le, Bich-Nga Pham, Duc-Khai Lam, Vinh-Tiep Nguyen</strong></p>
<p>Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using Class-Prompt Appending and Visual Prior Blending to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a class balancing algorithm to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance">https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance</a>. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºå¯¹äºåƒç´ çº§æ ‡æ³¨ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰è‡³å…³é‡è¦ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å¤§é‡æ ‡æ³¨å·¥ä½œã€‚ä¼ ç»Ÿçš„æ–¹æ³•ï¼Œå¦‚æ—‹è½¬å’Œç¿»è½¬ç­‰ç®€å•å˜æ¢ï¼Œè™½ç„¶å¯ä»¥ç”Ÿæˆæ–°å›¾åƒï¼Œä½†å¾€å¾€åœ¨å…³é”®çš„è¯­ä¹‰ç»´åº¦ä¸Šç¼ºä¹å¤šæ ·æ€§ï¼Œå¹¶ä¸”æ— æ³•æ”¹å˜é«˜çº§è¯­ä¹‰å±æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç”Ÿæˆæ¨¡å‹ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢å¼ºæ•°æ®ã€‚å¯æ§ç”Ÿæˆæ¨¡å‹é€šè¿‡åŸå§‹å›¾åƒçš„æç¤ºå’Œè§†è§‰å‚è€ƒï¼Œä¸ºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡æä¾›æ•°æ®å¢å¼ºæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®åæ˜ åŸå§‹å›¾åƒå†…å®¹å’Œç»“æ„çš„åˆæˆå›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºåˆ›å»ºæœ‰æ•ˆçš„æç¤ºå’Œè§†è§‰å‚è€ƒå…·æœ‰éš¾åº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä½¿ç”¨å¯æ§æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®å¢å¼ºç®¡é“ï¼Œç”¨äºè¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨ç±»æç¤ºé™„åŠ å’Œè§†è§‰å…ˆéªŒèåˆæ¥æœ‰æ•ˆç”Ÿæˆæç¤ºï¼Œä»¥å¢å¼ºå¯¹çœŸå®å›¾åƒä¸­æ ‡è®°ç±»çš„å…³æ³¨ï¼Œä½¿ç®¡é“èƒ½å¤Ÿåœ¨ä¿æŒåˆ†å‰²æ ‡è®°ç±»ç»“æ„çš„åŒæ—¶ï¼Œç”Ÿæˆç²¾ç¡®æ•°é‡çš„å¢å¼ºå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ç§ç±»å¹³è¡¡ç®—æ³•ï¼Œä»¥ç¡®ä¿åœ¨åˆå¹¶åˆæˆå›¾åƒå’ŒåŸå§‹å›¾åƒæ—¶ï¼Œè®­ç»ƒæ•°æ®é›†æ˜¯å¹³è¡¡çš„ã€‚åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²æ–¹é¢éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance">https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06002v5">PDF</a> Published in ICPRAM 2025, ISBN 978-989-758-730-6, ISSN 2184-4313</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åƒç´ çº§æ ‡æ³¨ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰ä¸­çš„æ•°æ®å¢å¼ºè‡³å…³é‡è¦ï¼Œä¼ ç»Ÿæ–¹æ³•ç®€å•è½¬æ¢å›¾åƒï¼Œä½†ç¼ºä¹å¤šæ ·æ€§ã€‚ç”Ÿæˆæ¨¡å‹å¯ç”Ÿæˆåˆæˆå›¾åƒï¼Œæˆä¸ºè§£å†³æ­¤é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚å¯æ§ç”Ÿæˆæ¨¡å‹å¯é€šè¿‡æç¤ºå’ŒåŸå§‹å›¾åƒçš„è§†è§‰å‚è€ƒè¿›è¡Œæ•°æ®å¢å¼ºã€‚æœ¬å·¥ä½œä½¿ç”¨å¯æ§æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡Class-Prompt Appendingå’ŒVisual Prior BlendingæŠ€æœ¯æé«˜çœŸå®å›¾åƒä¸­æ ‡è®°ç±»çš„å…³æ³¨åº¦ï¼Œç”Ÿæˆç²¾ç¡®æ•°é‡çš„å¢å¼ºå›¾åƒå¹¶ä¿ç•™åˆ†å‰²æ ‡è®°ç±»çš„ç»“æ„ã€‚åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºå¯¹åƒç´ çº§æ ‡æ³¨ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå°¤å…¶å¯¹äºéœ€è¦å¤§é‡æ ‡æ³¨çš„è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•å¦‚æ—‹è½¬å’Œç¿»è½¬è™½ç„¶èƒ½åˆ›å»ºæ–°å›¾åƒï¼Œä½†ç¼ºä¹å…³é”®è¯­ä¹‰ç»´åº¦çš„å¤šæ ·æ€§ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¯æ§ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒä¸ºè§£å†³æ•°æ®å¢å¼ºé—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å¼•å…¥å¯æ§æ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°æ®å¢å¼ºï¼Œä½¿ç”¨Class-Prompt Appendingå’ŒVisual Prior BlendingæŠ€æœ¯æé«˜çœŸå®å›¾åƒä¸­æ ‡è®°ç±»çš„å…³æ³¨åº¦ã€‚</li>
<li>æ–¹æ³•èƒ½ç”Ÿæˆç²¾ç¡®æ•°é‡çš„å¢å¼ºå›¾åƒï¼ŒåŒæ—¶ä¿ç•™åˆ†å‰²æ ‡è®°ç±»çš„ç»“æ„ã€‚</li>
<li>é€šè¿‡ç±»å¹³è¡¡ç®—æ³•ç¡®ä¿åˆæˆå’ŒåŸå§‹å›¾åƒçš„åˆå¹¶æ—¶æ•°æ®é›†å¹³è¡¡ã€‚</li>
<li>åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†è¯¥æ•°æ®å¢å¼ºç®¡é“çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad495f1fb36a8692ec602bfe07ab2570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f65bfd9b2b7e788b5f54c9643a7800c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91dd43e3d48806df3e2fa23090a86993.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b3c65fb7936a29984cc349b14779630.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b79cbb1b35476816ef186a87a8ee7de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c298870b43ce121bb270ff9dab12e16e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-07/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-97d2c1e109c4a374cf44348db26016a8.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  DeepSeek performs better than other Large Language Models in Dental   Cases
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cf5693abbe3c3c025881e809c9791603.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-07  SSGaussian Semantic-Aware and Structure-Preserving 3D Style Transfer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
