<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-11  Long-VITA Scaling Large Multi-modal Models to 1 Million Tokens with   Leading Short-Context Accuray">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-820e620c53c7c61f13abf341d6049f71.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-11-æ›´æ–°"><a href="#2025-02-11-æ›´æ–°" class="headerlink" title="2025-02-11 æ›´æ–°"></a>2025-02-11 æ›´æ–°</h1><h2 id="Long-VITA-Scaling-Large-Multi-modal-Models-to-1-Million-Tokens-with-Leading-Short-Context-Accuray"><a href="#Long-VITA-Scaling-Large-Multi-modal-Models-to-1-Million-Tokens-with-Leading-Short-Context-Accuray" class="headerlink" title="Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with   Leading Short-Context Accuray"></a>Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with   Leading Short-Context Accuray</h2><p><strong>Authors:Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Rongrong Ji, Xing Sun</strong></p>
<p>Establishing the long-context capability of large vision-language models is crucial for video understanding, high-resolution image understanding, multi-modal agents and reasoning. We introduce Long-VITA, a simple yet effective large multi-modal model for long-context visual-language understanding tasks. It is adept at concurrently processing and analyzing modalities of image, video, and text over 4K frames or 1M tokens while delivering advanced performances on short-context multi-modal tasks. We propose an effective multi-modal training schema that starts with large language models and proceeds through vision-language alignment, general knowledge learning, and two sequential stages of long-sequence fine-tuning. We further implement context-parallelism distributed inference and logits-masked language modeling head to scale Long-VITA to infinitely long inputs of images and texts during model inference. Regarding training data, Long-VITA is built on a mix of $17$M samples from public datasets only and demonstrates the state-of-the-art performance on various multi-modal benchmarks, compared against recent cutting-edge models with internal data. Long-VITA is fully reproducible and supports both NPU and GPU platforms for training and testing. We hope Long-VITA can serve as a competitive baseline and offer valuable insights for the open-source community in advancing long-context multi-modal understanding. </p>
<blockquote>
<p>å»ºç«‹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›æ˜¯è§†é¢‘ç†è§£ã€é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ã€å¤šæ¨¡æ€ä»£ç†å’Œæ¨ç†çš„å…³é”®ã€‚æˆ‘ä»¬ä»‹ç»äº†Long-VITAï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œç”¨äºé•¿ä¸Šä¸‹æ–‡è§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ã€‚å®ƒæ“…é•¿åŒæ—¶å¤„ç†å’Œåˆ†æå›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬çš„æ¨¡æ€ï¼Œå¯åœ¨4Kå¸§æˆ–1Mä»¤ç‰Œä¸Šè¿›è¡Œæ“ä½œï¼ŒåŒæ—¶åœ¨çŸ­ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå®ç°å…ˆè¿›æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¤šæ¨¡æ€è®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä»¥å¤§å‹è¯­è¨€æ¨¡å‹å¼€å§‹ï¼Œé€šè¿‡è§†è§‰è¯­è¨€å¯¹é½ã€ä¸€èˆ¬çŸ¥è¯†å­¦ä¹ ä»¥åŠä¸¤ä¸ªè¿ç»­çš„é•¿åºåˆ—å¾®è°ƒé˜¶æ®µè¿›è¡Œã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å®ç°äº†ä¸Šä¸‹æ–‡å¹¶è¡Œåˆ†å¸ƒå¼æ¨ç†å’Œlogits-maskedè¯­è¨€å»ºæ¨¡å¤´ï¼Œä»¥åœ¨æ¨¡å‹æ¨ç†æœŸé—´å°†Long-VITAæ‰©å±•åˆ°æ— é™é•¿çš„å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ã€‚å…³äºè®­ç»ƒæ•°æ®ï¼ŒLong-VITAä»…å»ºç«‹åœ¨å…¬å…±æ•°æ®é›†çš„1700ä¸‡ä¸ªæ ·æœ¬çš„æ··åˆä¸Šï¼Œä¸æœ€è¿‘ä½¿ç”¨å†…éƒ¨æ•°æ®çš„å°–ç«¯æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚Long-VITAå¯å®Œå…¨å¤åˆ¶ï¼Œæ”¯æŒNPUå’ŒGPUå¹³å°è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚æˆ‘ä»¬å¸Œæœ›Long-VITAå¯ä»¥ä½œä¸ºæœ‰ç«äº‰åŠ›çš„åŸºå‡†ï¼Œä¸ºå¼€æºç¤¾åŒºåœ¨æ¨è¿›é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ç†è§£æ–¹é¢æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05177v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/Long-VITA">https://github.com/VITA-MLLM/Long-VITA</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å»ºç«‹é•¿æœŸä¸Šä¸‹æ–‡èƒ½åŠ›æ˜¯è§†é¢‘ç†è§£ã€é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ã€å¤šæ¨¡æ€ä»£ç†å’Œæ¨ç†çš„å…³é”®ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„é•¿æœŸä¸Šä¸‹æ–‡è§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹Long-VITAï¼Œå®ƒèƒ½å¤ŸåŒæ—¶å¤„ç†å’Œåˆ†æå›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬çš„æ¨¡æ€ï¼Œå¹¶åœ¨çŸ­ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¤šæ¨¡æ€è®­ç»ƒæ¨¡å¼ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å¼€å§‹ï¼Œè¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ã€ä¸€èˆ¬çŸ¥è¯†å­¦ä¹ ä»¥åŠä¸¤ä¸ªè¿ç»­çš„é•¿åºåˆ—å¾®è°ƒé˜¶æ®µã€‚é€šè¿‡å®æ–½ä¸Šä¸‹æ–‡å¹¶è¡Œåˆ†å¸ƒå¼æ¨ç†å’Œlogitsæ©ç è¯­è¨€å»ºæ¨¡å¤´ï¼Œå°†Long-VITAæ‰©å±•åˆ°æ— é™é•¿çš„å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ã€‚è¯¥æ¨¡å‹åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¸å…¶ä»–ä½¿ç”¨å†…éƒ¨æ•°æ®çš„å…ˆè¿›æ¨¡å‹ç›¸æ¯”å…·æœ‰å“è¶Šæ€§èƒ½ã€‚Long-VITAå®Œå…¨å¯å¤ç°ï¼Œæ”¯æŒNPUå’ŒGPUå¹³å°è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œæ—¨åœ¨ä¸ºå¼€æºç¤¾åŒºæä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œæ¨åŠ¨é•¿æœŸä¸Šä¸‹æ–‡å¤šæ¨¡æ€ç†è§£çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Long-VITAæ˜¯ä¸€ç§ç”¨äºé•¿æœŸä¸Šä¸‹æ–‡è§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚</li>
<li>Long-VITAèƒ½å¤ŸåŒæ—¶å¤„ç†å’Œåˆ†æå›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬çš„æ¨¡æ€ã€‚</li>
<li>Long-VITAåœ¨çŸ­ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¤šæ¨¡æ€è®­ç»ƒæ¨¡å¼ï¼ŒåŒ…æ‹¬è§†è§‰è¯­è¨€å¯¹é½ã€ä¸€èˆ¬çŸ¥è¯†å­¦ä¹ å’Œé•¿åºåˆ—å¾®è°ƒé˜¶æ®µã€‚</li>
<li>é€šè¿‡ä¸Šä¸‹æ–‡å¹¶è¡Œåˆ†å¸ƒå¼æ¨ç†å’Œlogitsæ©ç è¯­è¨€å»ºæ¨¡å¤´ï¼ŒLong-VITAå¯æ‰©å±•åˆ°æ— é™é•¿çš„å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ã€‚</li>
<li>Long-VITAåœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1014fbee42dfed5b37bd4e2f2d86b042.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28d3037eb96bc1dafbd56268a54937d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faef1be7ea35928b587de000c7a2fdf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e93c89a155e961a6db58f66c4753be50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f368f7840b2a479ef15b22862d644fc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DuoGuard-A-Two-Player-RL-Driven-Framework-for-Multilingual-LLM-Guardrails"><a href="#DuoGuard-A-Two-Player-RL-Driven-Framework-for-Multilingual-LLM-Guardrails" class="headerlink" title="DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM   Guardrails"></a>DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM   Guardrails</h2><p><strong>Authors:Yihe Deng, Yu Yang, Junkai Zhang, Wei Wang, Bo Li</strong></p>
<p>The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at <a target="_blank" rel="noopener" href="https://github.com/yihedeng9/DuoGuard">https://github.com/yihedeng9/DuoGuard</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¢åŠ äº†å¯¹æŠ¤æ æ¨¡å‹çš„éœ€æ±‚ï¼Œä»¥ç¡®ä¿å…¶ä½¿ç”¨è´£ä»»ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹ä¸å®‰å…¨å’Œéæ³•å†…å®¹æ–¹é¢ã€‚è™½ç„¶è‹±è¯­ä¸­æœ‰å¤§é‡çš„å®‰å…¨æ•°æ®ï¼Œä½†ç”±äºå…¶ä»–è¯­è¨€ä¸­å¼€æºå®‰å…¨æ•°æ®çš„ç¨€ç¼ºï¼Œå¤šè¯­è¨€æŠ¤æ å»ºæ¨¡ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒäººå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œå…¶ä¸­ç”Ÿæˆå™¨å’ŒæŠ¤æ æ¨¡å‹å¯¹æŠ—æ€§åœ°å…±åŒè¿›åŒ–ï¼Œä»¥äº§ç”Ÿé«˜è´¨é‡åˆæˆæ•°æ®ï¼Œç”¨äºå¤šè¯­è¨€æŠ¤æ è®­ç»ƒã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šå°†è¿™ç§äº’åŠ¨å½¢å¼åŒ–ä¸ºä¸€ä¸ªåŒäººæ¸¸æˆï¼Œå¹¶è¯æ˜äº†å…¶æ”¶æ•›åˆ°çº³ä»€å‡è¡¡ç‚¹ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹â€œoursâ€ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œåœ¨è‹±è¯­åŸºå‡†æµ‹è¯•ä¸­ç›¸å¯¹äºLlamaGuard3ï¼ˆ8Bï¼‰æœ‰è¿‘10%çš„æå‡ï¼ŒåŒæ—¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¨¡å‹æ›´å°ï¼ˆ0.5Bï¼‰ä¸”é€Ÿåº¦æ›´å¿«ï¼ˆ4.5å€ï¼‰ã€‚æˆ‘ä»¬åœ¨å¤šè¯­è¨€å®‰å…¨ä»»åŠ¡æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³æ”¶é›†çš„çœŸå®æ•°æ®é›†ä¸­å¯¹ä½èµ„æºè¯­è¨€çš„ä¸å¹³è¡¡é—®é¢˜æ–¹é¢è¡¨ç°å°¤ä¸ºçªå‡ºã€‚æ¶ˆèç ”ç©¶å¼ºè°ƒäº†åˆæˆæ•°æ®ç”Ÿæˆåœ¨å¼¥åˆè‹±è¯­å’Œå…¶ä»–è¯­è¨€ä¹‹é—´å¼€æºæ•°æ®ä¸å¹³è¡¡ä¸­çš„å…³é”®ä½œç”¨ã€‚è¿™äº›å‘ç°å»ºç«‹äº†ä¸€ç§å¯æ‰©å±•å’Œé«˜æ•ˆçš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºæ”¹è¿›å¤šè¯­è¨€æŠ¤æ æ¨¡å‹ä»¥æé«˜LLMå®‰å…¨æ€§é“ºå¹³äº†é“è·¯ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/yihedeng9/DuoGuard%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/yihedeng9/DuoGuardä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05163v1">PDF</a> 24 pages, 9 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¢åŠ äº†å¯¹é˜²æŠ¤æ¨¡å‹çš„éœ€æ±‚ï¼Œä»¥ç¡®ä¿å…¶è´Ÿè´£ä»»çš„ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹ä¸å®‰å…¨å’Œéæ³•å†…å®¹æ–¹é¢ã€‚é’ˆå¯¹å¤šè¯­è¨€é˜²æŠ¤å»ºæ¨¡çš„ç¼ºä¹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤ç©å®¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œç”Ÿæˆå™¨å’Œé˜²æŠ¤æ¨¡å‹å¯¹æŠ—æ€§å…±åŒè¿›åŒ–ä»¥äº§ç”Ÿé«˜è´¨é‡åˆæˆæ•°æ®ç”¨äºå¤šè¯­è¨€é˜²æŠ¤è®­ç»ƒã€‚æœ¬æ–‡ç†è®ºå½¢å¼åŒ–è¿™ç§äº’åŠ¨ä¸ºä¸¤ç©å®¶æ¸¸æˆï¼Œå¹¶è¯æ˜æ”¶æ•›åˆ°çº³ä»€å‡è¡¡ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è‹±è¯­åŸºå‡†æµ‹è¯•ä¸­è¾ƒå½“å‰é¡¶å°–æ¨¡å‹æå‡äº†è¿‘10%ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æ›´å¿«ã€æ¨¡å‹æ›´å°ã€‚æˆ‘ä»¬åœ¨å¤šè¯­è¨€å®‰å…¨ä»»åŠ¡ä¸Šå–å¾—äº†é‡å¤§è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³æ”¶é›†çš„çœŸå®æ•°æ®é›†ä¸­å¯¹ä½èµ„æºè¯­è¨€çš„ä¸å¹³è¡¡é—®é¢˜æ–¹é¢ã€‚è¿™äº›å‘ç°å»ºç«‹äº†ä¸€ç§å¯æ‰©å±•å’Œé«˜æ•ˆåˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºæ”¹è¿›å¤šè¯­è¨€é˜²æŠ¤æ¨¡å‹ä»¥æé«˜LLMå®‰å…¨æ€§é“ºå¹³äº†é“è·¯ã€‚ç›¸å…³ä»£ç ã€æ¨¡å‹å’Œèµ„æ–™å·²å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/yihedeng9/DuoGuard%E3%80%82">https://github.com/yihedeng9/DuoGuardã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦é˜²æŠ¤æ¨¡å‹ä»¥ç¡®ä¿å…¶è´Ÿè´£ä»»çš„ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹æœ‰å®³å’Œéæ³•å†…å®¹æ–¹é¢ã€‚</li>
<li>é’ˆå¯¹å¤šè¯­è¨€é˜²æŠ¤å»ºæ¨¡çš„ç¼ºä¹ï¼Œæå‡ºäº†æ–°å‹çš„ä¸¤ç©å®¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ã€‚</li>
<li>ç”Ÿæˆå™¨å’Œé˜²æŠ¤æ¨¡å‹é€šè¿‡å¯¹æŠ—æ€§å…±åŒè¿›åŒ–äº§ç”Ÿé«˜è´¨é‡åˆæˆæ•°æ®ç”¨äºå¤šè¯­è¨€é˜²æŠ¤è®­ç»ƒã€‚</li>
<li>æœ¬æ–‡ç†è®ºå½¢å¼åŒ–è¯¥æ¡†æ¶ä¸ºä¸€ç§ä¸¤ç©å®¶æ¸¸æˆï¼Œè¯æ˜å…¶æ”¶æ•›è‡³çº³ä»€å‡è¡¡ã€‚</li>
<li>ç»éªŒè¯„ä¼°æ˜¾ç¤ºæ‰€ææ¨¡å‹æ€§èƒ½ä¼˜å¼‚ï¼ŒåŒ…æ‹¬æé«˜å‡†ç¡®æ€§ã€æ¨ç†é€Ÿåº¦å’Œæ¨¡å‹è§„æ¨¡æ–¹é¢ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šè¯­è¨€å®‰å…¨ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºï¼Œè§£å†³äº†ä½èµ„æºè¯­è¨€çš„ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4018754b0c11f68bb8a9f525e4342d65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8563978b08d7e048c1fa485f439daf95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ce588f10b36dd0d4b5f240c36dec80a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Refining-Integration-by-Parts-Reduction-of-Feynman-Integrals-with-Machine-Learning"><a href="#Refining-Integration-by-Parts-Reduction-of-Feynman-Integrals-with-Machine-Learning" class="headerlink" title="Refining Integration-by-Parts Reduction of Feynman Integrals with   Machine Learning"></a>Refining Integration-by-Parts Reduction of Feynman Integrals with   Machine Learning</h2><p><strong>Authors:Matt von Hippel, Matthias Wilhelm</strong></p>
<p>Integration-by-parts reductions of Feynman integrals pose a frequent bottle-neck in state-of-the-art calculations in theoretical particle and gravitational-wave physics, and rely on heuristic approaches for selecting integration-by-parts identities, whose quality heavily influences the performance. In this paper, we investigate the use of machine-learning techniques to find improved heuristics. We use funsearch, a genetic programming variant based on code generation by a Large Language Model, in order to explore possible approaches, then use strongly typed genetic programming to zero in on useful solutions. Both approaches manage to re-discover the state-of-the-art heuristics recently incorporated into integration-by-parts solvers, and in one example find a small advance on this state of the art. </p>
<blockquote>
<p>åœ¨ç†è®ºç²’å­ç‰©ç†å’Œå¼•åŠ›æ³¢ç‰©ç†çš„æœ€æ–°è®¡ç®—ä¸­ï¼Œåˆ†éƒ¨ç§¯åˆ†æ³•çš„Feynmanç§¯åˆ†æ„æˆäº†ä¸€ä¸ªç“¶é¢ˆé—®é¢˜ï¼Œå¹¶ä¾èµ–äºå¯å‘å¼æ–¹æ³•æ¥é€‰æ‹©åˆ†éƒ¨ç§¯åˆ†æ³•èº«ä»½ï¼Œå…¶è´¨é‡å¯¹æ€§èƒ½æœ‰ç€é‡å¤§å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ¥æ‰¾åˆ°æ”¹è¿›å¯å‘å¼æ–¹æ³•çš„åº”ç”¨ã€‚æˆ‘ä»¬ä½¿ç”¨äº†funsearchï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä»£ç ç”Ÿæˆçš„æ–°å‹é—ä¼ ç¨‹åºè®¾è®¡å˜ä½“ï¼Œæ¥æ¢ç´¢å¯èƒ½çš„æ–¹æ³•ï¼Œç„¶åä½¿ç”¨å¼ºç±»å‹é—ä¼ ç¨‹åºè®¾è®¡æ¥å¯»æ‰¾å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½å¤Ÿé‡æ–°å‘ç°æœ€è¿‘èå…¥åˆ†éƒ¨ç§¯åˆ†æ±‚è§£å™¨çš„æœ€æ–°å¯å‘å¼æŠ€æœ¯ï¼Œå¹¶åœ¨ä¸€ä¸ªä¾‹å­ä¸­å–å¾—äº†è½»å¾®çš„è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05121v1">PDF</a> 28 pages, 9 figures</p>
<p><strong>Summary</strong><br>é›†æˆ-åˆ†éƒ¨ç§¯åˆ†æ³•ï¼ˆintegration-by-partsï¼‰åœ¨ç†è®ºç²’å­å’Œå¼•åŠ›æ³¢ç‰©ç†å­¦çš„å…ˆè¿›è®¡ç®—ä¸­ç»å¸¸é‡åˆ°ç“¶é¢ˆï¼Œä¾èµ–äºå¯å‘å¼æ–¹æ³•æ¥é€‰æ‹©ç§¯åˆ†æ’ç­‰å¼ï¼Œå…¶è´¨é‡ä¸¥é‡å½±å“æ€§èƒ½ã€‚æœ¬æ–‡ç ”ç©¶ä½¿ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ¥æ”¹è¿›å¯å‘å¼æ–¹æ³•ï¼Œé‡‡ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç ç”Ÿæˆçš„é—ä¼ ç¼–ç¨‹å˜ä½“funsearchè¿›è¡Œæ¢ç´¢ï¼Œç„¶åä½¿ç”¨å¼ºç±»å‹é—ä¼ ç¼–ç¨‹æ¥é›†ä¸­å¯»æ‰¾æœ‰ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚ä¸¤ç§æ–¹æ³•éƒ½èƒ½å¤Ÿé‡æ–°å‘ç°ç›®å‰é›†æˆ-åˆ†éƒ¨ç§¯åˆ†æ±‚è§£å™¨ä¸­ä½¿ç”¨çš„æœ€æ–°å¯å‘å¼æŠ€æœ¯ï¼Œå¹¶åœ¨ä¸€ä¸ªä¾‹å­ä¸­å–å¾—äº†å¾®å°çš„è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›†æˆ-åˆ†éƒ¨ç§¯åˆ†æ³•æ˜¯ç†è®ºç²’å­å’Œå¼•åŠ›æ³¢ç‰©ç†å­¦ä¸­å…ˆè¿›è®¡ç®—çš„ä¸€ä¸ªç“¶é¢ˆã€‚</li>
<li>å¯å‘å¼æ–¹æ³•çš„é€‰æ‹©å¯¹é›†æˆ-åˆ†éƒ¨ç§¯åˆ†æ³•çš„æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>æœºå™¨å­¦ä¹ æŠ€æœ¯è¢«ç”¨äºæ”¹è¿›å¯å‘å¼æ–¹æ³•ä»¥æé«˜é›†æˆ-åˆ†éƒ¨ç§¯åˆ†æ³•çš„æ•ˆç‡ã€‚</li>
<li>funsearchæ˜¯ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é—ä¼ ç¼–ç¨‹å˜ä½“ï¼Œç”¨äºæ¢ç´¢å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¼ºç±»å‹é—ä¼ ç¼–ç¨‹è¢«ç”¨äºé›†ä¸­å¯»æ‰¾æœ‰ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä¸¤ç§æ–¹æ³•éƒ½èƒ½å¤Ÿé‡æ–°å‘ç°å½“å‰å¯å‘å¼æŠ€æœ¯çš„å‰æ²¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05121">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1a3b5dfe928fe133075007955fd68d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c31b711fa54b9466ba2c5f1932ccb465.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Flexible-and-Efficient-Grammar-Constrained-Decoding"><a href="#Flexible-and-Efficient-Grammar-Constrained-Decoding" class="headerlink" title="Flexible and Efficient Grammar-Constrained Decoding"></a>Flexible and Efficient Grammar-Constrained Decoding</h2><p><strong>Authors:Kanghee Park, Timothy Zhou, Loris Dâ€™Antoni</strong></p>
<p>Large Language Models (LLMs) are often asked to generate structured outputs that obey precise syntactic rules, such as code snippets or formatted data. Grammar-constrained decoding (GCD) can guarantee that LLM outputs matches such rules by masking out tokens that will provably lead to outputs that do not belong to a specified context-free grammar (CFG). To guarantee soundness, GCD algorithms have to compute how a given LLM subword tokenizer can align with the tokens used   by a given context-free grammar and compute token masks based on this information. Doing so efficiently is challenging and existing GCD algorithms require tens of minutes to preprocess common grammars. We present a new GCD algorithm together with an implementation that offers 17.71x faster offline preprocessing than existing approaches while preserving state-of-the-art efficiency in online mask computation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸è¢«è¦æ±‚ç”Ÿæˆéµå¾ªç²¾ç¡®å¥æ³•è§„åˆ™çš„ç»“æ„åŒ–è¾“å‡ºï¼Œä¾‹å¦‚ä»£ç ç‰‡æ®µæˆ–æ ¼å¼åŒ–æ•°æ®ã€‚è¯­æ³•çº¦æŸè§£ç ï¼ˆGCDï¼‰å¯ä»¥é€šè¿‡å±è”½é‚£äº›æ˜æ˜¾ä¼šå¯¼è‡´è¾“å‡ºä¸ç¬¦åˆç‰¹å®šä¸Šä¸‹æ–‡æ— å…³è¯­æ³•ï¼ˆCFGï¼‰çš„ä»¤ç‰Œï¼Œæ¥ä¿è¯LLMè¾“å‡ºç¬¦åˆè¿™äº›è§„åˆ™ã€‚ä¸ºäº†ä¿è¯æ­£ç¡®æ€§ï¼ŒGCDç®—æ³•å¿…é¡»è®¡ç®—ç»™å®šçš„LLMå­è¯æ ‡è®°å™¨å¦‚ä½•ä¸ç‰¹å®šçš„ä¸Šä¸‹æ–‡æ— å…³è¯­æ³•æ‰€ä½¿ç”¨çš„ä»¤ç‰Œå¯¹é½ï¼Œå¹¶åŸºäºè¿™äº›ä¿¡æ¯è®¡ç®—ä»¤ç‰Œæ©ç ã€‚é«˜æ•ˆåœ°å®Œæˆè¿™ä¸€è¿‡ç¨‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç°æœ‰çš„GCDç®—æ³•éœ€è¦æ•°åˆ†é’Ÿé¢„å¤„ç†å¸¸è§çš„è¯­æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„GCDç®—æ³•åŠå…¶å®ç°ï¼Œä¸ä¼ ç»Ÿçš„åšæ³•ç›¸æ¯”ï¼Œè¯¥ç®—æ³•çº¿ä¸‹é¢„å¤„ç†é€Ÿåº¦æé«˜äº†17.71å€ï¼ŒåŒæ—¶åœ¨çº¿æ©ç è®¡ç®—æ•ˆç‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05111v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆéœ€éµå¾ªç²¾ç¡®å¥æ³•è§„åˆ™çš„ç»“æ„åŒ–è¾“å‡ºï¼ˆå¦‚ä»£ç ç‰‡æ®µæˆ–æ ¼å¼åŒ–æ•°æ®ï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯­æ³•çº¦æŸè§£ç ï¼ˆGCDï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥é«˜æ•ˆåœ°ä¿è¯LLMè¾“å‡ºä¸æŒ‡å®šä¸Šä¸‹æ–‡æ— å…³è¯­æ³•ï¼ˆCFGï¼‰åŒ¹é…ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ–°ç®—æ³•çš„ç¦»çº¿é¢„å¤„ç†é€Ÿåº¦æé«˜äº†17.71å€ï¼ŒåŒæ—¶ä¿æŒåœ¨çº¿æ©ç è®¡ç®—çš„æœ€æ–°æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦ç”Ÿæˆéµå¾ªç²¾ç¡®å¥æ³•è§„åˆ™çš„ç»“æ„åŒ–è¾“å‡ºã€‚</li>
<li>è¯­æ³•çº¦æŸè§£ç ï¼ˆGCDï¼‰å¯ä¿è¯LLMè¾“å‡ºä¸ä¸Šä¸‹æ–‡æ— å…³è¯­æ³•ï¼ˆCFGï¼‰åŒ¹é…ã€‚</li>
<li>ç°æœ‰GCDç®—æ³•åœ¨é¢„å¤„ç†å¸¸è§è¯­æ³•æ—¶æ•ˆç‡è¾ƒä½ï¼Œéœ€è¦æ•°ååˆ†é’Ÿã€‚</li>
<li>æ–°GCDç®—æ³•æé«˜äº†ç¦»çº¿é¢„å¤„ç†é€Ÿåº¦ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•æé«˜äº†17.71å€ã€‚</li>
<li>æ–°ç®—æ³•åœ¨åœ¨çº¿æ©ç è®¡ç®—æ–¹é¢ä¿æŒæœ€æ–°æ•ˆç‡ã€‚</li>
<li>GCDç®—æ³•é€šè¿‡æ©ç è¾“å‡ºç¡®å®šä¼šå¯¼è‡´ä¸ç¬¦åˆæŒ‡å®šè¯­æ³•çš„æ ‡è®°ï¼Œä»è€Œä¿è¯å¥å…¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f62c26bf00e1d0f8e853b6c453e1322b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f696d1e04d3d4f9f8e4c956d0bec7bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c09369b8c86378b0ca5b4392e97edcca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1de9cd2f9855a249fcbd864ff372e39d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6e11915c5d2daa57c1c8a05875b6e50.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Adaptive-Graph-of-Thoughts-Test-Time-Adaptive-Reasoning-Unifying-Chain-Tree-and-Graph-Structures"><a href="#Adaptive-Graph-of-Thoughts-Test-Time-Adaptive-Reasoning-Unifying-Chain-Tree-and-Graph-Structures" class="headerlink" title="Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain,   Tree, and Graph Structures"></a>Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain,   Tree, and Graph Structures</h2><p><strong>Authors:Tushar Pandey, Ara Ghukasyan, Oktay Goktas, Santosh Kumar Radha</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, yet their performance is highly dependent on the prompting strategy and model scale. While reinforcement learning and fine-tuning have been deployed to boost reasoning, these approaches incur substantial computational and data overhead. In this work, we introduce Adaptive Graph of Thoughts (AGoT), a dynamic, graph-based inference framework that enhances LLM reasoning solely at test time. Rather than relying on fixed-step methods like Chain of Thought (CoT) or Tree of Thoughts (ToT), AGoT recursively decomposes complex queries into structured subproblems, forming an dynamic directed acyclic graph (DAG) of interdependent reasoning steps. By selectively expanding only those subproblems that require further analysis, AGoT unifies the strengths of chain, tree, and graph paradigms into a cohesive framework that allocates computation where it is most needed. We validate our approach on diverse benchmarks spanning multi-hop retrieval, scientific reasoning, and mathematical problem-solving, achieving up to 46.2% improvement on scientific reasoning tasks (GPQA) - comparable to gains achieved through computationally intensive reinforcement learning approaches and outperforming state-of-the-art iterative approaches. These results suggest that dynamic decomposition and structured recursion offer a scalable, cost-effective alternative to post-training modifications, paving the way for more robust, general-purpose reasoning in LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶æ€§èƒ½é«˜åº¦ä¾èµ–äºæç¤ºç­–ç•¥å’Œæ¨¡å‹è§„æ¨¡ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ å’Œå¾®è°ƒå·²è¢«ç”¨äºæå‡æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™äº›æ–¹æ³•äº§ç”Ÿäº†å¤§é‡çš„è®¡ç®—å’Œæ•°æ®å¼€é”€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”æ€ç»´å›¾ï¼ˆAGoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŠ¨æ€å›¾çš„æ¨ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¾èµ–å›ºå®šæ­¥éª¤çš„æ–¹æ³•ï¼ˆå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰æˆ–æ€ç»´æ ‘ï¼ˆToTï¼‰ï¼‰ä¸åŒï¼ŒAGoTé€’å½’åœ°å°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºç»“æ„åŒ–çš„å­é—®é¢˜ï¼Œå½¢æˆä¸€ä¸ªåŠ¨æ€çš„ã€æœ‰å‘çš„æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼ŒåŒ…å«ç›¸äº’ä¾èµ–çš„æ¨ç†æ­¥éª¤ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°æ‰©å±•é‚£äº›éœ€è¦è¿›ä¸€æ­¥åˆ†æçš„å­é—®é¢˜ï¼ŒAGoTå°†é“¾ã€æ ‘å’Œå›¾èŒƒå¼çš„ä¼˜ç‚¹ç»“åˆåœ¨ä¸€ä¸ªåè°ƒä¸€è‡´çš„æ¡†æ¶ä¸­ï¼Œåœ¨éœ€è¦çš„åœ°æ–¹åˆ†é…è®¡ç®—èµ„æºã€‚æˆ‘ä»¬åœ¨è·¨è¶Šå¤šè·³æ£€ç´¢ã€ç§‘å­¦æ¨ç†å’Œæ•°å­¦é—®é¢˜è§£å†³ç­‰å¤šç§åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨ç§‘å­¦æ¨ç†ä»»åŠ¡ï¼ˆGPQAï¼‰ä¸Šå®ç°äº†é«˜è¾¾46.2%çš„æ”¹è¿›ï¼Œè¿™ä¸€æˆç»©ä¸å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å¢ç›Šç›¸å½“ï¼Œå¹¶è¶…è¶Šäº†æœ€å…ˆè¿›çš„è¿­ä»£æ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€åˆ†è§£å’Œç»“æ„é€’å½’æ˜¯ä¸€ç§å¯æ‰©å±•ã€ç»æµé«˜æ•ˆçš„æ›¿ä»£æ–¹æ³•ï¼Œä¸ºè®­ç»ƒåçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹æ”¹è¿›äº†æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ›´ç¨³å¥ã€é€šç”¨çš„æ¨ç†èƒ½åŠ›é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05078v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶æ€§èƒ½é«˜åº¦ä¾èµ–äºæç¤ºç­–ç•¥å’Œæ¨¡å‹è§„æ¨¡ã€‚å½“å‰æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ å’Œå¾®è°ƒè™½ç„¶èƒ½æé«˜æ¨ç†èƒ½åŠ›ï¼Œä½†è®¡ç®—å’Œæ•°æ®æˆæœ¬è¾ƒé«˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŠ¨æ€çš„å›¾æ¨ç†æ¡†æ¶â€”â€”è‡ªé€‚åº”æ€ç»´å›¾ï¼ˆAGoTï¼‰ï¼Œåœ¨æµ‹è¯•æ—¶å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚AGoTé€šè¿‡é€’å½’åˆ†è§£å¤æ‚æŸ¥è¯¢åˆ°ç»“æ„åŒ–å­é—®é¢˜ï¼Œå½¢æˆåŠ¨æ€çš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œæ ¹æ®éœ€æ±‚é€‰æ‹©æ€§æ‰©å±•å­é—®é¢˜ã€‚è¯¥æ¡†æ¶èåˆäº†é“¾å¼æ€ç»´ã€æ ‘çŠ¶æ€ç»´å’Œå›¾æ€ç»´çš„ä¼˜åŠ¿ï¼Œåœ¨è®¡ç®—éœ€æ±‚å¤„åˆ†é…èµ„æºã€‚åœ¨è·¨è¶Šå¤šè·³æ£€ç´¢ã€ç§‘å­¦æ¨ç†å’Œæ•°å­¦é—®é¢˜è§£å†³ç­‰å¤šç§åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†åœ¨ç§‘å­¦æ¨ç†ä»»åŠ¡ï¼ˆGPQAï¼‰ä¸Šé«˜è¾¾46.2%çš„æ”¹è¿›â€”â€”ä¸å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„è®¡ç®—å¯†é›†å‹å¢ç›Šç›¸å½“ï¼Œå¹¶ä¼˜äºæœ€å…ˆè¿›çš„è¿­ä»£æ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€åˆ†è§£å’Œç»“æ„åŒ–é€’å½’æ˜¯ä¸€ç§å¯æ‰©å±•ä¸”ç»æµå®æƒ çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºLLMä¸­æ›´ç¨³å¥ã€é€šç”¨çš„æ¨ç†é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å—æç¤ºç­–ç•¥å’Œæ¨¡å‹è§„æ¨¡çš„å½±å“ã€‚</li>
<li>å½“å‰æé«˜LLMæ¨ç†èƒ½åŠ›çš„æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ å’Œå¾®è°ƒè®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>è‡ªé€‚åº”æ€ç»´å›¾ï¼ˆAGoTï¼‰æ˜¯ä¸€ç§æ–°å‹åŠ¨æ€çš„å›¾æ¨ç†æ¡†æ¶ï¼Œèƒ½åœ¨æµ‹è¯•æ—¶å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>AGoTé€šè¿‡é€’å½’åˆ†è§£å¤æ‚æŸ¥è¯¢åˆ°ç»“æ„åŒ–å­é—®é¢˜ï¼Œå½¢æˆåŠ¨æ€çš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ã€‚</li>
<li>AGoTèƒ½èåˆå¤šç§æ€ç»´æ¡†æ¶ï¼ˆå¦‚é“¾å¼æ€ç»´ã€æ ‘çŠ¶æ€ç»´å’Œå›¾æ€ç»´ï¼‰çš„ä¼˜åŠ¿ã€‚</li>
<li>åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šï¼ŒAGoTå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05078">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63e4813fc5d0c8c447d393ce70dcd0a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c187f1e61eb7567ef507d38847084f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-511b33c1476c6a563e322189d16154f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2964752e94669bfb0410f95fb27968ed.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="nvAgent-Automated-Data-Visualization-from-Natural-Language-via-Collaborative-Agent-Workflow"><a href="#nvAgent-Automated-Data-Visualization-from-Natural-Language-via-Collaborative-Agent-Workflow" class="headerlink" title="nvAgent: Automated Data Visualization from Natural Language via   Collaborative Agent Workflow"></a>nvAgent: Automated Data Visualization from Natural Language via   Collaborative Agent Workflow</h2><p><strong>Authors:Geliang Ouyang, Jingyao Chen, Zhihe Nie, Yi Gui, Yao Wan, Hongyu Zhang, Dongping Chen</strong></p>
<p>Natural Language to Visualization (NL2Vis) seeks to convert natural-language descriptions into visual representations of given tables, empowering users to derive insights from large-scale data. Recent advancements in Large Language Models (LLMs) show promise in automating code generation to transform tabular data into accessible visualizations. However, they often struggle with complex queries that require reasoning across multiple tables. To address this limitation, we propose a collaborative agent workflow, termed nvAgent, for NL2Vis. Specifically, nvAgent comprises three agents: a processor agent for database processing and context filtering, a composer agent for planning visualization generation, and a validator agent for code translation and output verification. Comprehensive evaluations on the new VisEval benchmark demonstrate that nvAgent consistently surpasses state-of-the-art baselines, achieving a 7.88% improvement in single-table and a 9.23% improvement in multi-table scenarios. Qualitative analyses further highlight that nvAgent maintains nearly a 20% performance margin over previous models, underscoring its capacity to produce high-quality visual representations from complex, heterogeneous data sources. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€è‡³å¯è§†åŒ–ï¼ˆNL2Visï¼‰æ—¨åœ¨å°†è‡ªç„¶è¯­è¨€æè¿°è½¬åŒ–ä¸ºç»™å®šè¡¨æ ¼çš„è§†è§‰è¡¨ç¤ºï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä»å¤§è§„æ¨¡æ•°æ®ä¸­è·å–æ´å¯Ÿã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æ˜¾ç¤ºï¼Œåœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆä»¥å°†è¡¨æ ¼æ•°æ®è½¬åŒ–ä¸ºå¯è®¿é—®çš„å¯è§†åŒ–æ–¹é¢å­˜åœ¨å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éš¾ä»¥å¤„ç†éœ€è¦åœ¨å¤šä¸ªè¡¨æ ¼ä¹‹é—´è¿›è¡Œæ¨ç†çš„å¤æ‚æŸ¥è¯¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºnvAgentçš„NL2VisååŒä»£ç†å·¥ä½œæµç¨‹ã€‚å…·ä½“æ¥è¯´ï¼ŒnvAgentåŒ…å«ä¸‰ä¸ªä»£ç†ï¼šç”¨äºæ•°æ®åº“å¤„ç†å’Œä¸Šä¸‹æ–‡è¿‡æ»¤çš„å¤„ç†å™¨ä»£ç†ã€ç”¨äºè§„åˆ’å¯è§†åŒ–ç”Ÿæˆçš„ä½œæ›²å®¶ä»£ç†ã€ä»¥åŠç”¨äºä»£ç ç¿»è¯‘å’Œè¾“å‡ºéªŒè¯çš„éªŒè¯å™¨ä»£ç†ã€‚åœ¨æ–°çš„VisEvalåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒnvAgentå§‹ç»ˆè¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œåœ¨å•è¡¨åœºæ™¯ä¸‹å®ç°äº†7.88%çš„æ”¹è¿›ï¼Œåœ¨å¤šè¡¨åœºæ™¯ä¸‹å®ç°äº†9.23%çš„æ”¹è¿›ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥å¼ºè°ƒï¼ŒnvAgentç›¸æ¯”ä»¥å‰çš„æ¨¡å‹ä¿æŒäº†è¿‘20%çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œçªå‡ºäº†å…¶ä»å¤æ‚ã€å¼‚ç±»æ•°æ®æºç”Ÿæˆé«˜è´¨é‡å¯è§†åŒ–è¡¨ç¤ºçš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05036v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªç„¶è¯­è¨€æè¿°è½¬æ¢ä¸ºå¯è§†åŒ–å›¾è¡¨æ˜¯ä¸€ä¸ªé‡è¦ç ”ç©¶æ–¹å‘ï¼Œæœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œè¡¨æ ¼æ•°æ®å¯è§†åŒ–æ–¹é¢å±•ç°æ½œåŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ–¹é¢ä»æœ‰ä¸è¶³ã€‚nvAgentæ˜¯ä¸€ç§æ–°å‹çš„åˆä½œå¼å·¥ä½œæµï¼Œé€šè¿‡å¤„ç†å™¨ã€ä½œæ›²å®¶å’ŒéªŒè¯å™¨ä¸‰ä¸ªä»£ç†å®ç°NL2Visï¼Œèƒ½å¤Ÿæ”¹å–„è¿™ä¸€çŠ¶å†µã€‚nvAgentåœ¨VisEvalåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨å•è¡¨å’Œå¤šè¡¨åœºæ™¯ä¸‹åˆ†åˆ«æé«˜äº†7.88%å’Œ9.23%ã€‚å®ƒèƒ½å¤Ÿå¤„ç†å¤æ‚æ•°æ®æºå¹¶ç”Ÿæˆé«˜è´¨é‡å¯è§†åŒ–è¡¨ç¤ºï¼Œæ€§èƒ½ä¼˜åŠ¿æ¥è¿‘è¾¾åˆ°ç™¾åˆ†ä¹‹äºŒåã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NL2Visæ—¨åœ¨å°†è‡ªç„¶è¯­è¨€æè¿°è½¬æ¢ä¸ºç»™å®šè¡¨æ ¼çš„è§†è§‰è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œè¡¨æ ¼æ•°æ®å¯è§†åŒ–æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>nvAgentæ˜¯ä¸€ç§æ–°å‹çš„åˆä½œå¼å·¥ä½œæµï¼ŒåŒ…æ‹¬å¤„ç†å™¨ã€ä½œæ›²å®¶å’ŒéªŒè¯å™¨ä¸‰ä¸ªä»£ç†ï¼Œç”¨äºå®ç°NL2Visã€‚</li>
<li>nvAgentåœ¨å•è¡¨å’Œå¤šè¡¨åœºæ™¯ä¸‹çš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>nvAgentèƒ½å¤Ÿå¤„ç†å¤æ‚æ•°æ®æºå¹¶ç”Ÿæˆé«˜è´¨é‡å¯è§†åŒ–è¡¨ç¤ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05036">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-851a285ed342aa5a214664f3e344a21f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae78b9090f60d981521ecaf25e3d9639.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4abc8ef7ef6e07fa28331dcf18bfcc58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e0b4f74a242e6de2c0031857c8fea84.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="QuEST-Stable-Training-of-LLMs-with-1-Bit-Weights-and-Activations"><a href="#QuEST-Stable-Training-of-LLMs-with-1-Bit-Weights-and-Activations" class="headerlink" title="QuEST: Stable Training of LLMs with 1-Bit Weights and Activations"></a>QuEST: Stable Training of LLMs with 1-Bit Weights and Activations</h2><p><strong>Authors:Andrei Panferov, Jiale Chen, Soroush Tabesh, Roberto L. Castro, Mahdi Nikdan, Dan Alistarh</strong></p>
<p>One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the â€œoptimalâ€ bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16&#x2F;BF16 precision, at 8-bits weights and activations.   We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the â€œtrueâ€ (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/QuEST">https://github.com/IST-DASLab/QuEST</a>. </p>
<blockquote>
<p>å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å·¨å¤§æˆæœ¬çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨é‡åŒ–æˆ–ç¨€ç–è¡¨ç¤ºè¿›è¡Œè®­ç»ƒæˆ–éƒ¨ç½²ã€‚è™½ç„¶åè®­ç»ƒå‹ç¼©æ–¹æ³•éå¸¸å—æ¬¢è¿ï¼Œä½†é€šè¿‡åœ¨ç›´æ¥åœ¨è¿™æ ·çš„è¡¨ç¤ºä¸Šè¿›è¡Œè®­ç»ƒè·å¾—æ›´ä¸ºå‡†ç¡®çš„å‹ç¼©æ¨¡å‹ï¼Œå³æ‰€è°“çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾é—®é¢˜ï¼šä¾‹å¦‚ï¼Œä¸€é¡¹è¿‘æœŸçš„ç ”ç©¶ï¼ˆarXiv:2411.04330v2ï¼‰ç»™å‡ºäº†ä½¿ç”¨QATè¿›è¡Œè®­ç»ƒæ—¶æ¨¡å‹çš„â€œæœ€ä½³â€ä½å®½ï¼ŒåŒæ—¶ä¿æŒä¸æ ‡å‡†FP16&#x2F;BF16ç²¾åº¦çš„ç«äº‰æ€§èƒ½ï¼Œè¯¥ä½å®½ä¸ºæƒé‡å’Œæ¿€æ´»å€¼ä½¿ç”¨8ä½ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§ç§°ä¸ºQuESTçš„æ–°æ–¹æ³•æé«˜äº†è¿™ä¸€æœ€æ–°æ°´å¹³ï¼Œè¯¥æ–¹æ³•ä¸FP16å…·æœ‰å¸•ç´¯æ‰˜ç«äº‰æ€§ï¼Œå³å®ƒåœ¨è¾ƒå°çš„æ¨¡å‹å¤§å°ä¸Šæä¾›æ›´é«˜çš„ç²¾åº¦ï¼ŒåŒæ—¶ä»¥æƒé‡å’Œæ¿€æ´»å€¼åœ¨4ä½æˆ–æ›´å°‘è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æ­¤å¤–ï¼ŒQuESTå…è®¸ä»¥æƒé‡å’Œæ¿€æ´»å€¼ä¸º1ä½è¿›è¡Œç¨³å®šè®­ç»ƒã€‚QuESTé€šè¿‡æ”¹è¿›QATæ–¹æ³•çš„ä¸¤ä¸ªå…³é”®æ–¹é¢æ¥å®ç°è¿™ä¸€ç‚¹ï¼šï¼ˆ1ï¼‰é€šè¿‡Hadamardå½’ä¸€åŒ–å’ŒMSEæœ€ä¼˜æ‹Ÿåˆå‡†ç¡®å¿«é€Ÿåœ°é‡åŒ–æƒé‡å’Œæ¿€æ´»å€¼çš„è¿ç»­åˆ†å¸ƒï¼›ï¼ˆ2ï¼‰åŸºäºæ˜ç¡®æœ€å°åŒ–é‡åŒ–çŠ¶æ€ä¸‹è®¡ç®—å¾—åˆ°çš„å™ªå£°æ¢¯åº¦ä¸â€œçœŸå®â€ï¼ˆä½†æœªçŸ¥ï¼‰å…¨ç²¾åº¦æ¢¯åº¦ä¹‹é—´çš„è¯¯å·®çš„æ„æƒ³ï¼Œæå‡ºä¸€ç§æ–°çš„ä¿¡ä»»æ¢¯åº¦ä¼°è®¡å™¨ã€‚åœ¨Llamaæ¶æ„ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒQuESTåœ¨æ•´ä¸ªç¡¬ä»¶æ”¯æŒçš„ç²¾åº¦èŒƒå›´å†…å®ç°äº†ç¨³å®šçš„ç¼©æ”¾å®šå¾‹ï¼Œå¹¶å¯æ‰©å±•åˆ°ç¨€ç–è¡¨ç¤ºã€‚æˆ‘ä»¬æä¾›äº†GPUå†…æ ¸æ”¯æŒï¼Œè¯æ˜ç”±QuESTäº§ç”Ÿçš„æ¨¡å‹å¯ä»¥é«˜æ•ˆæ‰§è¡Œã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/QuEST%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IST-DASLab/QuESTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05003v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä½¿ç”¨é‡åŒ–æˆ–ç¨€ç–è¡¨ç¤ºè¿›è¡Œè®­ç»ƒæˆ–éƒ¨ç½²æ˜¯é™ä½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆæœ¬çš„ä¸€ç§æ–¹æ³•ã€‚å°½ç®¡é’ˆå¯¹è®­ç»ƒåçš„å‹ç¼©æ–¹æ³•å¾ˆå—æ¬¢è¿ï¼Œä½†é€šè¿‡ç›´æ¥åœ¨è¿™äº›è¡¨ç¤ºä¸Šè¿›è¡Œè®­ç»ƒä»¥è·å¾—æ›´ç²¾ç¡®çš„å‹ç¼©æ¨¡å‹ï¼Œå³é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰çš„é—®é¢˜ä»ç„¶æ‚¬è€Œæœªå†³ã€‚ä¸€ç§æ–°æ–¹æ³•QuESTï¼Œä»¥4ä½ä»¥ä¸‹çš„æƒé‡å’Œæ¿€æ´»å€¼è¿›è¡Œè®­ç»ƒï¼Œæ¯”FP16å…·æœ‰æ›´å¥½çš„å‡†ç¡®æ€§å’Œæ›´å°çš„æ¨¡å‹å¤§å°ï¼Œå¹¶ä¸”å…è®¸ä½¿ç”¨1ä½æƒé‡å’Œæ¿€æ´»å€¼è¿›è¡Œç¨³å®šè®­ç»ƒã€‚QuESTé€šè¿‡æ”¹è¿›QATæ–¹æ³•çš„ä¸¤ä¸ªå…³é”®æ–¹é¢æ¥å®ç°è¿™ä¸€ç‚¹ï¼šï¼ˆ1ï¼‰é€šè¿‡Hadamardå½’ä¸€åŒ–å’ŒMSEæœ€ä¼˜æ‹Ÿåˆå‡†ç¡®å¿«é€Ÿåœ°é‡åŒ–æƒé‡å’Œæ¿€æ´»å€¼çš„è¿ç»­åˆ†å¸ƒï¼›ï¼ˆ2ï¼‰åŸºäºæœ€å°åŒ–é‡åŒ–çŠ¶æ€å’Œâ€œçœŸå®â€ï¼ˆä½†æœªçŸ¥ï¼‰å…¨ç²¾åº¦æ¢¯åº¦ä¹‹é—´çš„è¯¯å·®çš„æ–°ä¿¡ä»»æ¢¯åº¦ä¼°è®¡å™¨ã€‚åœ¨Llamaæ¶æ„çš„å®éªŒä¸­ï¼ŒQuESTæ˜¾ç¤ºå‡ºåœ¨æ•´ä¸ªç¡¬ä»¶æ”¯æŒç²¾åº¦èŒƒå›´å†…çš„ç¨³å®šç¼©æ”¾å®šå¾‹ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°ç¨€ç–è¡¨ç¤ºã€‚æˆ‘ä»¬æä¾›GPUå†…æ ¸æ”¯æŒä»¥è¯æ˜QuESTäº§ç”Ÿçš„æ¨¡å‹å¯ä»¥é«˜æ•ˆæ‰§è¡Œã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨IST-DASLabçš„GitHubå­˜å‚¨åº“ä¸­è·å–ã€‚æ€»ä¹‹ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ–¹æ³•QuESTï¼Œç”¨äºè®­ç»ƒLLMçš„æ›´ç²¾ç¡®å‹ç¼©æ¨¡å‹ï¼Œå¯é™ä½å†…å­˜éœ€æ±‚å’Œè®¡ç®—æˆæœ¬ã€‚QuESTæ—¨åœ¨å®ç°æ›´æœ‰æ•ˆçš„æƒé‡å’Œæ¿€æ´»é‡åŒ–å¹¶ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œå¯åº”ç”¨äºç¡¬ä»¶åŠ é€Ÿè¯­è¨€æ¨¡å‹çš„åº”ç”¨éƒ¨ç½²åœºæ™¯ã€‚å¯¹äºè®ºæ–‡ç ”ç©¶å†…å®¹çš„å…·ä½“å±•ç¤ºä¸åº”ç”¨æ•ˆæœå¯é€šè¿‡ä»£ç éªŒè¯ä½“éªŒå…¶å¯é æ€§åŠæ•ˆæœè¯„ä¼°è®ºè¯åˆç†æ€§é€‚ç”¨æ€§ç­‰çš„å®è¯è€ƒé‡æœ‰å…¶å®ç°ç‰¹æ®ŠæŠ€æœ¯çš„æ ‡å‡†åŒ–å±•ç°å’Œåˆ†ææ¨¡å—è½¯ä»¶çš„é«˜åº¦å‡†ç¡®æ€§å’Œæ™®é€‚æ€§ç­‰äº®ç‚¹ä»·å€¼å’Œæ½œåœ¨çš„ä¼˜è¶Šæ€§ç‰¹åˆ«å—åˆ°å·¥ä¸šé¢†åŸŸå¯¹å…¶é«˜åº¦é‡è§†å¹¶å±•ç°å‡ºå¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œå¸‚åœºæ½œåŠ›ã€‚æ­¤æŠ€æœ¯å…·æœ‰åˆ›æ–°æ€§å’Œå®ç”¨æ€§ã€‚å¯¹äºè¯¥æŠ€æœ¯çš„å®é™…åº”ç”¨æ•ˆæœéœ€è¦è¿›ä¸€æ­¥éªŒè¯ã€‚æ­¤æ‘˜è¦æ—¨åœ¨ç®€è¦æ¦‚æ‹¬æ–‡æœ¬å†…å®¹ã€‚å…·ä½“å®ç°ç»†èŠ‚éœ€è¦è¿›ä¸€æ­¥æŸ¥é˜…åŸæ–‡ä»¥è·å–æ›´å…¨é¢çš„ä¿¡æ¯ã€‚æ­¤æ‘˜è¦æ²¡æœ‰å¿½ç•¥ä»»ä½•é‡è¦ä¿¡æ¯ç‚¹æˆ–å¼•å…¥ä»»ä½•æ–°çš„è§‚ç‚¹æˆ–æ¦‚å¿µæ€§å†…å®¹æˆ–åè§æ€§çš„è§‚ç‚¹ç­‰ä¸å½“å†…å®¹ä¿è¯ä¿¡æ¯çš„çœŸå®æ€§å’Œå‡†ç¡®æ€§æœ‰åŠ©äºç†è§£æ–‡æœ¬ä¸»æ—¨å¹¶åšå‡ºå®¢è§‚è¯„ä»·ä¸ºæŠ€æœ¯å®æ–½è€…å’ŒæŠ€æœ¯ç ”ç©¶è€…æä¾›äº†æ–¹ä¾¿å‡†ç¡®çš„èƒŒæ™¯çŸ¥è¯†å’Œæ‘˜è¦è§†è§’å±•ç¤ºæœ€æ–°çš„å‰æ²¿ç ”ç©¶åŠå…¶è¶‹åŠ¿ç†è§£ç°æœ‰æ–‡çŒ®å¹¶æå‡ºå…·ä½“çš„åˆ›æ–°æ€§åˆ†æåŒæ—¶ä¹Ÿé’ˆå¯¹åº”ç”¨é¢†åŸŸä¸ºè¯¥æŠ€æœ¯æ™®åŠåº”ç”¨çš„å‡†ç¡®æ€§ç»™äºˆäº†è§£å’Œå‚ç…§çš„æ€è·¯ä»…ä¾›å‚è€ƒå…·ä½“å†…å®¹åœ¨å®é™…ä½¿ç”¨ä¸­å¯èƒ½ä¼šæœ‰ä¸€äº›åç¦»å½±å“æ ¹æ®éœ€æ±‚å’Œæƒ…å†µä¸åŒç»“æœç•¥æœ‰ä¸åŒå¿…è¦æ—¶å¯å‚è€ƒå…·ä½“çš„è®ºæ–‡åŸæ–‡è¿›è¡Œç†è§£å­¦ä¹ å¹¶çµæ´»åº”ç”¨æ€»ç»“åˆ†æçš„å†…å®¹ä»¥å®é™…æƒ…å†µä¸ºå‡†ã€‚æ€»ä¹‹ï¼Œè¿™é¡¹æ–°æŠ€æœ¯å°†æå¤§åœ°æ¨åŠ¨è¯­è¨€æ¨¡å‹çš„å‘å±•å¹¶å¸¦æ¥å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å±•æœ›æœªæ¥çš„å‘å±•æœŸå¾…è¿™é¡¹æŠ€æœ¯åœ¨å­¦æœ¯ç•Œå’Œäº§ä¸šç•Œçš„æ·±å…¥åˆä½œä¸­å®ç°æ›´å¹¿æ³›çš„æ¨å¹¿åº”ç”¨åœ¨ç‰¹å®šåœºæ™¯æˆ–é¢†åŸŸå†…å‘æŒ¥ä½œç”¨å‡å°‘æ¨¡å‹çš„å·¨å¤§æˆæœ¬å¸¦æ¥å…¨æ–°çš„è¡Œä¸šè§£å†³æ–¹æ¡ˆå’Œç ”ç©¶æŒ‘æˆ˜åŠå…¶æ›´æ·±å…¥çš„è§è§£æ€»ç»“æ‹“å±•ç›¸å…³ç ”ç©¶å†…æ¶µå¹¶è¿›ä¸€æ­¥ä¿ƒè¿›æŠ€æœ¯åˆ›æ–°å’Œè¡Œä¸šåº”ç”¨çš„æ·±å…¥å‘å±•ä¹Ÿæ¨è¿›LLMè¿™ä¸€é¢†åŸŸçš„ç›¸å…³æŠ€æœ¯å’Œäº§å“åœ¨å®é™…ç”Ÿäº§ä¸­çš„å®è·µè¿›ç¨‹æå‡ç›¸å…³äº§ä¸šçš„æŠ€æœ¯æ°´å¹³å’Œç¤¾ä¼šä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è®ºæ–‡çš„å…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ–¹æ³•QuESTï¼Œæ—¨åœ¨é™ä½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆæœ¬ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-93907b89fab5ce6bafd88abd995d22ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8940fdf680d0975bb2ca5a4f9bb3b91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51eb35c7359816f7621647f61be56a07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5fbe4f2df03349ec19943de56451347.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CoCoA-A-Generalized-Approach-to-Uncertainty-Quantification-by-Integrating-Confidence-and-Consistency-of-LLM-Outputs"><a href="#CoCoA-A-Generalized-Approach-to-Uncertainty-Quantification-by-Integrating-Confidence-and-Consistency-of-LLM-Outputs" class="headerlink" title="CoCoA: A Generalized Approach to Uncertainty Quantification by   Integrating Confidence and Consistency of LLM Outputs"></a>CoCoA: A Generalized Approach to Uncertainty Quantification by   Integrating Confidence and Consistency of LLM Outputs</h2><p><strong>Authors:Roman Vashurin, Maiya Goloburda, Preslav Nakov, Artem Shelmanov, Maxim Panov</strong></p>
<p>Uncertainty quantification (UQ) methods for Large Language Models (LLMs) encompasses a variety of approaches, with two major types being particularly prominent: information-based, which focus on model confidence expressed as token probabilities, and consistency-based, which assess the semantic relationship between multiple outputs generated using repeated sampling. Several recent methods have combined these two approaches and shown impressive performance in various applications. However, they sometimes fail to outperform much simpler baseline methods. Our investigation reveals distinctive characteristics of LLMs as probabilistic models, which help to explain why these UQ methods underperform in certain tasks. Based on these findings, we propose a new way of synthesizing model confidence and output consistency that leads to a family of efficient and robust UQ methods. We evaluate our approach across a variety of tasks such as question answering, abstractive summarization, and machine translation, demonstrating sizable improvements over state-of-the-art UQ approaches. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰æ–¹æ³•åŒ…æ‹¬å¤šç§é€”å¾„ï¼Œå…¶ä¸­ä¸¤ç§ä¸»è¦ç±»å‹ç‰¹åˆ«çªå‡ºï¼šåŸºäºä¿¡æ¯çš„ï¼Œä¾§é‡äºä»¥ç¬¦å·æ¦‚ç‡è¡¨è¾¾æ¨¡å‹ä¿¡å¿ƒï¼›åŸºäºä¸€è‡´æ€§çš„ï¼Œè¯„ä¼°ä½¿ç”¨é‡å¤é‡‡æ ·ç”Ÿæˆçš„å¤šä¸ªè¾“å‡ºä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚æœ€è¿‘çš„ä¸€äº›æ–¹æ³•ç»“åˆäº†è¿™ä¸¤ç§æ–¹æ³•ï¼Œåœ¨å„ç§åº”ç”¨ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬æœ‰æ—¶æœªèƒ½è¶…è¶Šæ›´ç®€å•çš„åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥æ­ç¤ºäº†LLMä½œä¸ºæ¦‚ç‡æ¨¡å‹çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œè¿™æœ‰åŠ©äºè§£é‡Šä¸ºä»€ä¹ˆè¿™äº›UQæ–¹æ³•åœ¨æŸäº›ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆæˆæ¨¡å‹ä¿¡å¿ƒå’Œè¾“å‡ºä¸€è‡´æ€§çš„æ–°æ–¹æ³•ï¼Œå¯¼è‡´ä¸€ç³»åˆ—é«˜æ•ˆä¸”ç¨³å¥çš„UQæ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å„ç§ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¦‚é—®ç­”ã€æ‘˜è¦ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ï¼Œè¯æ˜äº†æˆ‘ä»¬åœ¨æœ€å…ˆè¿›çš„UQæ–¹æ³•ä¸Šå–å¾—äº†å·¨å¤§çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04964v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¿¡æ¯å‹å’Œä¸€è‡´æ€§å‹ä¸¤ç§ä¸»è¦ç±»å‹ã€‚ä¿¡æ¯å‹å…³æ³¨æ¨¡å‹è¡¨è¾¾çš„ä»¤ç‰Œæ¦‚ç‡çš„ä¿¡å¿ƒï¼Œè€Œä¸€è‡´æ€§å‹åˆ™è¯„ä¼°å¤šæ¬¡é‡‡æ ·ç”Ÿæˆçš„å¤šä¸ªè¾“å‡ºä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚è¿‘æœŸç»“åˆè¿™ä¸¤ç§æ–¹æ³•çš„ç­–ç•¥åœ¨æŸäº›åº”ç”¨ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨æŸäº›ä»»åŠ¡ä¸­æœªèƒ½è¶…è¶Šç®€å•çš„åŸºçº¿æ–¹æ³•ã€‚ç ”ç©¶æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ¦‚ç‡æ¨¡å‹çš„ç‹¬ç‰¹ç‰¹å¾ï¼ŒåŸºäºæ­¤æå‡ºäº†åˆæˆæ¨¡å‹ä¿¡å¿ƒå’Œè¾“å‡ºä¸€è‡´æ€§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆä¸”ç¨³å¥ï¼Œå¹¶åœ¨é—®ç­”ã€æ‘˜è¦ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¿¡æ¯å‹å’Œä¸€è‡´æ€§å‹ä¸¤ç§ç­–ç•¥ã€‚</li>
<li>è¿‘æœŸæ–¹æ³•ç»“åˆä»¥ä¸Šä¸¤ç§æ–¹æ³•åœ¨æŸäº›åº”ç”¨ä¸­å±•ç°å‡ºå¼ºå¤§æ€§èƒ½ã€‚</li>
<li>æŸäº›ä»»åŠ¡ä¸­æœªèƒ½è¶…è¶Šç®€å•åŸºçº¿æ–¹æ³•å¯èƒ½æ˜¯å› ä¸ºå¯¹è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ç†è§£ä¸å¤Ÿæ·±å…¥ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ¦‚ç‡æ¨¡å‹çš„ç‹¬ç‰¹ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åˆæˆæ¨¡å‹ä¿¡å¿ƒå’Œè¾“å‡ºä¸€è‡´æ€§çš„æ–¹æ³•ï¼Œå…·æœ‰è‰¯å¥½çš„æ€§èƒ½å’Œç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨é—®ç­”ã€æ‘˜è¦ç”Ÿæˆå’Œæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7be9875df3b9d67c9625d2bafc512d3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3e3976cbe44ec55396b23ec7c5b08de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-820e620c53c7c61f13abf341d6049f71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-192b1abad4d14354c2e1587e5d45a99f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Beyond-Sample-Level-Feedback-Using-Reference-Level-Feedback-to-Guide-Data-Synthesis"><a href="#Beyond-Sample-Level-Feedback-Using-Reference-Level-Feedback-to-Guide-Data-Synthesis" class="headerlink" title="Beyond Sample-Level Feedback: Using Reference-Level Feedback to Guide   Data Synthesis"></a>Beyond Sample-Level Feedback: Using Reference-Level Feedback to Guide   Data Synthesis</h2><p><strong>Authors:Shuhaib Mehri, Xiusi Chen, Heng Ji, Dilek Hakkani-TÃ¼r</strong></p>
<p>LLMs demonstrate remarkable capabilities in following natural language instructions, largely due to instruction-tuning on high-quality datasets. While synthetic data generation has emerged as a scalable approach for creating such datasets, maintaining consistent quality standards remains challenging. Recent approaches incorporate feedback to improve data quality, but typically operate at the sample level, generating and applying feedback for each response individually. In this work, we propose Reference-Level Feedback, a novel methodology that instead collects feedback based on high-quality reference samples from carefully curated seed data. We use this feedback to capture rich signals of desirable characteristics that can be propagated to newly synthesized data. We present REFED, a dataset of 10K instruction-response pairs synthesized using such feedback. We demonstrate the effectiveness of our approach by showing that Llama-3.1-8B-Instruct finetuned on REFED achieves state-of-the-art performance among similar-sized SFT-based models on AlpacaEval 2.0 and strong results on Arena-Hard. Through extensive experiments, we show that our approach consistently outperforms traditional sample-level feedback methods with significantly fewer feedback collections and improves performance across different model architectures. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºéµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„æ˜¾è‘—èƒ½åŠ›ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºåœ¨é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´çš„ç»“æœã€‚è™½ç„¶åˆæˆæ•°æ®ç”Ÿæˆå·²ç»æˆä¸ºåˆ›å»ºæ­¤ç±»æ•°æ®é›†çš„ä¸€ç§å¯æ‰©å±•æ–¹æ³•ï¼Œä½†ä¿æŒä¸€è‡´çš„è´¨é‡æ ‡å‡†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘çš„æ–¹æ³•çº³å…¥äº†åé¦ˆæ¥æ”¹å–„æ•°æ®è´¨é‡ï¼Œä½†é€šå¸¸æ˜¯åœ¨æ ·æœ¬å±‚é¢æ“ä½œï¼Œä¸ºæ¯ä¸ªå“åº”å•ç‹¬ç”Ÿæˆå’Œåº”ç”¨åé¦ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å‚è€ƒçº§åé¦ˆï¼ˆReference-Level Feedbackï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåŸºäºç²¾å¿ƒç­›é€‰çš„ç§å­æ•°æ®ä¸­çš„é«˜è´¨é‡å‚è€ƒæ ·æœ¬æ”¶é›†åé¦ˆã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ç§åé¦ˆæ¥æ•è·ä¸°å¯Œçš„ç†æƒ³ç‰¹å¾ä¿¡å·ï¼Œè¿™äº›ä¿¡å·å¯ä»¥ä¼ æ’­åˆ°æ–°åˆæˆçš„æ•°æ®ä¸­ã€‚æˆ‘ä»¬æ¨å‡ºäº†REFEDæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±è¿™æ ·çš„åé¦ˆåˆæˆçš„åŒ…å«1ä¸‡ä¸ªæŒ‡ä»¤å“åº”å¯¹çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡å±•ç¤ºLLama-3.1-8B-Instructåœ¨REFEDä¸Šçš„å¾®è°ƒåœ¨AlpacaEval 2.0ä¸Šå®ç°äº†åŒç±»å¤§å°åŸºäºSFTçš„æ¨¡å‹çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨Arena-Hardä¸Šå–å¾—äº†è‰¯å¥½ç»“æœã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„æ ·æœ¬çº§åé¦ˆæ–¹æ³•ï¼Œä½¿ç”¨æ›´å°‘çš„åé¦ˆæ”¶é›†æ¬¡æ•°ï¼Œå¹¶åœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„ä¸Šæé«˜äº†æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04511v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMså±•ç°å¼ºå¤§çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œä¸»è¦å¾—ç›Šäºåœ¨é«˜è´¨é‡æ•°æ®é›†ä¸Šçš„æŒ‡ä»¤è°ƒä¼˜ã€‚å°½ç®¡åˆæˆæ•°æ®ç”Ÿæˆå·²æˆä¸ºåˆ›å»ºæ­¤ç±»æ•°æ®é›†çš„å¯æ‰©å±•æ–¹æ³•ï¼Œä½†ä¿æŒä¸€è‡´çš„è´¨é‡æ ‡å‡†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡å¼•å…¥åé¦ˆæ¥æ”¹å–„æ•°æ®è´¨é‡ï¼Œé€šå¸¸åœ¨æ ·æœ¬å±‚é¢æ“ä½œï¼Œä¸ºæ¯ä¸ªå“åº”ä¸ªåˆ«ç”Ÿæˆå’Œåº”ç”¨åé¦ˆã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºç²¾å¿ƒç­›é€‰çš„ç§å­æ•°æ®ä¸­çš„é«˜è´¨é‡å‚è€ƒæ ·æœ¬æ”¶é›†åé¦ˆçš„Reference-Level Feedbackæ–°æ–¹æ³•ã€‚åˆ©ç”¨è¿™ç§åé¦ˆæ•æ‰ä¸°å¯Œçš„ç†æƒ³ç‰¹å¾ä¿¡å·ï¼Œå¯ä»¥ä¼ æ’­åˆ°æ–°åˆæˆçš„æ•°æ®ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†ä½¿ç”¨è¿™ç§åé¦ˆåˆæˆçš„REFEDæ•°æ®é›†ï¼ŒåŒ…å«10KæŒ‡ä»¤-å“åº”å¯¹ã€‚é€šè¿‡å±•ç¤ºLlama-3.1-8B-Instructåœ¨REFEDä¸Šçš„å¾®è°ƒæ•ˆæœï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨AlpacaEval 2.0ä¸Šå®ç°åŒç±»äº§å“æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨Arena-Hardä¸Šå–å¾—å¼ºåŠ²ç»“æœã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨æ”¶é›†æ›´å°‘åé¦ˆçš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„æ ·æœ¬çº§åé¦ˆæ–¹æ³•ï¼Œå¹¶æ”¹å–„ä¸åŒæ¨¡å‹æ¶æ„çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsçš„ä¼˜å¼‚è¡¨ç°å¾—ç›Šäºåœ¨é«˜è´¨é‡æ•°æ®é›†ä¸Šçš„æŒ‡ä»¤è°ƒä¼˜ã€‚</li>
<li>åˆæˆæ•°æ®ç”Ÿæˆæ˜¯åˆ›å»ºæ­¤ç±»æ•°æ®é›†çš„å¯æ‰©å±•æ–¹æ³•ï¼Œä½†ä¿æŒè´¨é‡ä¸€è‡´æ€§å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥åé¦ˆæœºåˆ¶æ¥æ”¹å–„æ•°æ®è´¨é‡ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•ä¸»è¦åœ¨æ ·æœ¬å±‚é¢æ“ä½œï¼Œæ•ˆç‡æœ‰é™ã€‚</li>
<li>æå‡ºReference-Level Feedbackæ–°æ–¹æ³•ï¼ŒåŸºäºé«˜è´¨é‡å‚è€ƒæ ·æœ¬æ”¶é›†åé¦ˆã€‚</li>
<li>åˆ©ç”¨ä¸°å¯Œçš„ç†æƒ³ç‰¹å¾ä¿¡å·ï¼Œå°†åé¦ˆä¼ æ’­åˆ°æ–°çš„åˆæˆæ•°æ®ã€‚</li>
<li>å±•ç¤ºä½¿ç”¨æ­¤æ–¹æ³•åˆæˆçš„REFEDæ•°æ®é›†æ•ˆæœï¼Œåœ¨å¤šä¸ªè¯„ä¼°ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-32e89f8c2d52b84528fe0bfa54beed74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4840127179c7518f241f0c2241d705ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-137503d715c39f1a71835377ae3a4a4a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Kronecker-Mask-and-Interpretive-Prompts-are-Language-Action-Video-Learners"><a href="#Kronecker-Mask-and-Interpretive-Prompts-are-Language-Action-Video-Learners" class="headerlink" title="Kronecker Mask and Interpretive Prompts are Language-Action Video   Learners"></a>Kronecker Mask and Interpretive Prompts are Language-Action Video   Learners</h2><p><strong>Authors:Jingyi Yang, Zitong Yu, Xiuming Ni, Jia He, Hui Li</strong></p>
<p>Contrastive language-image pretraining (CLIP) has significantly advanced image-based vision learning. A pressing topic subsequently arises: how can we effectively adapt CLIP to the video domain? Recent studies have focused on adjusting either the textual or visual branch of CLIP for action recognition. However, we argue that adaptations of both branches are crucial. In this paper, we propose \textbf{CLAVER}: a \textbf{C}ontrastive \textbf{L}anguage-\textbf{A}ction \textbf{V}ideo Learn\textbf{er}, designed to shift CLIPâ€™s focus from the alignment of static visual objects and concrete nouns to the alignment of dynamic action behaviors and abstract verbs. Specifically, we introduce a novel Kronecker mask attention for temporal modeling. Our tailored Kronecker mask offers three benefits 1) it expands the temporal receptive field for each token, 2) it serves as an effective spatiotemporal heterogeneity inductive bias, mitigating the issue of spatiotemporal homogenization, and 3) it can be seamlessly plugged into transformer-based models. Regarding the textual branch, we leverage large language models to generate diverse, sentence-level and semantically rich interpretive prompts of actions, which shift the modelâ€™s focus towards the verb comprehension. Extensive experiments on various benchmarks and learning scenarios demonstrate the superiority and generality of our approach. The code will be available soon. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨åŸºäºå›¾åƒçš„è§†è§‰å­¦ä¹ æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚éšä¹‹è€Œæ¥å‡ºç°äº†ä¸€ä¸ªç´§è¿«çš„é—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•æœ‰æ•ˆåœ°å°†CLIPé€‚åº”åˆ°è§†é¢‘é¢†åŸŸï¼Ÿè¿‘æœŸçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è°ƒæ•´CLIPçš„æ–‡æœ¬æˆ–è§†è§‰åˆ†æ”¯æ¥è¿›è¡ŒåŠ¨ä½œè¯†åˆ«ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºä¸¤ä¸ªåˆ†æ”¯çš„é€‚åº”éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CLAVERï¼šä¸€ä¸ªå¯¹æ¯”è¯­è¨€-åŠ¨ä½œè§†é¢‘å­¦ä¹ å™¨ï¼ˆContrastive Language-Action Video Learnerï¼‰ï¼Œæ—¨åœ¨å°†CLIPçš„é‡ç‚¹ä»é™æ€è§†è§‰å¯¹è±¡å’Œå…·ä½“åè¯çš„å¯¹é½è½¬å‘åŠ¨æ€è¡Œä¸ºåŠ¨ä½œå’ŒæŠ½è±¡åŠ¨è¯çš„å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹å…‹ç½—å†…å…‹æ©è†œæ³¨æ„åŠ›æ¥å®ç°æ—¶åºå»ºæ¨¡ã€‚æˆ‘ä»¬çš„å®šåˆ¶å…‹ç½—å†…å…‹æ©è†œæä¾›äº†ä¸‰ä¸ªå¥½å¤„ï¼š1ï¼‰å®ƒæ‰©å¤§äº†æ¯ä¸ªæ ‡è®°çš„æ—¶åºæ„Ÿå—é‡ï¼›2ï¼‰å®ƒä½œä¸ºæœ‰æ•ˆçš„æ—¶ç©ºå¼‚è´¨æ€§å½’çº³åç½®ï¼Œå‡è½»äº†æ—¶ç©ºåŒè´¨åŒ–çš„é—®é¢˜ï¼›3ï¼‰å®ƒå¯ä»¥æ— ç¼åœ°æ’å…¥åŸºäºtransformerçš„æ¨¡å‹ã€‚å…³äºæ–‡æœ¬åˆ†æ”¯ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·ã€å¥å­çº§åˆ«ä¸”è¯­ä¹‰ä¸°å¯Œçš„åŠ¨ä½œè§£é‡Šæ€§æç¤ºï¼Œä½¿æ¨¡å‹å…³æ³¨åŠ¨è¯ç†è§£ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•å’Œå­¦ä¹ åœºæ™¯çš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚ä»£ç å¾ˆå¿«å°†å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03549v2">PDF</a> Accepted to ICLR2025</p>
<p><strong>Summary</strong><br>     åŸºäºCLIPçš„è§†é¢‘åŠ¨ä½œè¯†åˆ«ç ”ç©¶æå‡ºCLAVERæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è°ƒæ•´æ–‡æœ¬å’Œè§†è§‰åˆ†æ”¯ä»¥é€‚åº”åŠ¨æ€è¡Œä¸ºè¯†åˆ«ã€‚CLAVERå¼•å…¥Kronecker maskæ³¨æ„åŠ›è¿›è¡Œæ—¶åºå»ºæ¨¡ï¼Œæ‰©å¤§æ—¶åºæ„Ÿå—é‡ï¼Œæœ‰æ•ˆåº”å¯¹æ—¶ç©ºåŒè´¨åŒ–é—®é¢˜ï¼Œå¹¶å¯ä»¥æ— ç¼æ’å…¥åŸºäºtransformerçš„æ¨¡å‹ã€‚åŒæ—¶ï¼Œæ–‡æœ¬åˆ†æ”¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸°å¯Œè¯­ä¹‰çš„è§£è¯»æç¤ºï¼Œä½¿æ¨¡å‹æ›´ä¾§é‡äºåŠ¨è¯ç†è§£ã€‚å®éªŒè¯æ˜CLAVERæ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§å’Œæ³›åŒ–æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>CLAVERæ¨¡å‹æ—¨åœ¨å°†CLIPçš„ç„¦ç‚¹ä»é™æ€è§†è§‰å¯¹è±¡å’Œå…·ä½“åè¯çš„å¯¹é½è½¬ç§»åˆ°åŠ¨æ€è¡Œä¸ºè¡Œä¸ºå’ŒæŠ½è±¡åŠ¨è¯çš„å¯¹é½ã€‚</p>
</li>
<li><p>è¯¥æ¨¡å‹å¼•å…¥äº†åˆ›æ–°çš„Kronecker maskæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ—¶åºå»ºæ¨¡ï¼Œæ‰©å¤§äº†æ—¶åºæ„Ÿå—é‡ï¼Œå¹¶æœ‰æ•ˆåº”å¯¹æ—¶ç©ºåŒè´¨åŒ–é—®é¢˜ã€‚</p>
</li>
<li><p>Kronecker maskå¯ä»¥æ— ç¼é›†æˆåˆ°åŸºäºtransformerçš„æ¨¡å‹ä¸­ã€‚</p>
</li>
<li><p>å¯¹äºæ–‡æœ¬åˆ†æ”¯çš„è°ƒæ•´ï¼ŒCLAVERåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–ã€å¥å­çº§åˆ«çš„è¯­ä¹‰ä¸°å¯Œè§£è¯»æç¤ºï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£åŠ¨è¯ã€‚</p>
</li>
<li><p>é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†å’Œåœºæ™¯ä¸‹çš„å¹¿æ³›å®éªŒéªŒè¯ï¼ŒCLAVERæ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§å’Œæ³›åŒ–æ€§ã€‚</p>
</li>
<li><p>CLAVERæ¨¡å‹çš„ä»£ç å°†å¾ˆå¿«å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9af44d114b74db81f084a0c42ab7100.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74e837888e4c3df7534ca77280781c35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21c00327f43c091d3a1d441a734fb18b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68bc4c6543b88928f5a8c0e3c0dda734.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Simplifying-Formal-Proof-Generating-Models-with-ChatGPT-and-Basic-Searching-Techniques"><a href="#Simplifying-Formal-Proof-Generating-Models-with-ChatGPT-and-Basic-Searching-Techniques" class="headerlink" title="Simplifying Formal Proof-Generating Models with ChatGPT and Basic   Searching Techniques"></a>Simplifying Formal Proof-Generating Models with ChatGPT and Basic   Searching Techniques</h2><p><strong>Authors:Sangjun Han, Taeil Hur, Youngmi Hur, Kathy Sangkyung Lee, Myungyoon Lee, Hyojae Lim</strong></p>
<p>The challenge of formal proof generation has a rich history, but with modern techniques, we may finally be at the stage of making actual progress in real-life mathematical problems. This paper explores the integration of ChatGPT and basic searching techniques to simplify generating formal proofs, with a particular focus on the miniF2F dataset. We demonstrate how combining a large language model like ChatGPT with a formal language such as Lean, which has the added advantage of being verifiable, enhances the efficiency and accessibility of formal proof generation. Despite its simplicity, our best-performing Lean-based model surpasses all known benchmarks with a 31.15% pass rate. We extend our experiments to include other datasets and employ alternative language models, showcasing our modelsâ€™ comparable performance in diverse settings and allowing for a more nuanced analysis of our results. Our findings offer insights into AI-assisted formal proof generation, suggesting a promising direction for future research in formal mathematical proof. </p>
<blockquote>
<p>ç”Ÿæˆå½¢å¼åŒ–è¯æ˜çš„æŒ‘æˆ˜æœ‰ç€æ‚ ä¹…çš„å†å²ï¼Œä½†å€ŸåŠ©ç°ä»£æŠ€æœ¯ï¼Œæˆ‘ä»¬å¯èƒ½ç»ˆäºè¿›å…¥äº†åœ¨ç°å®ç”Ÿæ´»ä¸­çš„æ•°å­¦é—®é¢˜å–å¾—å®é™…è¿›å±•çš„é˜¶æ®µã€‚æœ¬æ–‡æ¢è®¨äº†å°†ChatGPTå’ŒåŸºæœ¬æœç´¢æŠ€æœ¯ç›¸ç»“åˆä»¥ç®€åŒ–ç”Ÿæˆå½¢å¼åŒ–è¯æ˜çš„è¿‡ç¨‹ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨miniF2Fæ•°æ®é›†ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†ChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸åƒLeanè¿™æ ·çš„å½¢å¼åŒ–è¯­è¨€ç›¸ç»“åˆï¼Œç”±äºå½¢å¼åŒ–è¯­è¨€å…·æœ‰å¯éªŒè¯æ€§çš„ä¼˜åŠ¿ï¼Œè¿™æé«˜äº†å½¢å¼åŒ–è¯æ˜çš„æ•ˆç‡å’Œå¯åŠæ€§ã€‚å°½ç®¡å…¶ç®€å•æ€§ï¼Œæˆ‘ä»¬è¡¨ç°æœ€ä½³çš„åŸºäºLeançš„æ¨¡å‹è¶…è¿‡äº†æ‰€æœ‰å·²çŸ¥åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ç‡é«˜è¾¾31.15%ã€‚æˆ‘ä»¬å°†å®éªŒæ‰©å±•åˆ°å…¶ä»–æ•°æ®é›†å¹¶ä½¿ç”¨å…¶ä»–è¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸­çš„å¯æ¯”æ€§èƒ½ï¼Œå¹¶å¯¹æˆ‘ä»¬çš„ç»“æœè¿›è¡Œäº†æ›´æ·±å…¥çš„åˆ†æã€‚æˆ‘ä»¬çš„å‘ç°å¯¹äººå·¥æ™ºèƒ½è¾…åŠ©çš„å½¢å¼åŒ–è¯æ˜ç”Ÿæˆæä¾›äº†æ·±åˆ»çš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„å½¢å¼åŒ–æ•°å­¦è¯æ˜ç ”ç©¶æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03321v2">PDF</a> Accepted to Computing Conference 2025</p>
<p><strong>Summary</strong><br>ChatGPTä¸åŸºæœ¬æœç´¢æŠ€æœ¯çš„ç»“åˆç®€åŒ–äº†æ­£å¼è¯æ˜ç”Ÿæˆè¿‡ç¨‹ï¼Œç‰¹åˆ«å…³æ³¨miniF2Fæ•°æ®é›†ã€‚é€šè¿‡é›†æˆå¯éªŒè¯çš„æ­£å¼è¯­è¨€Leanä¸å¤§å‹è¯­è¨€æ¨¡å‹ChatGPTï¼Œæé«˜äº†è¯æ˜ç”Ÿæˆæ•ˆç‡å’Œå¯åŠæ€§ã€‚æœ€ä½³æ€§èƒ½çš„Leanæ¨¡å‹è¶…è¶Šæ‰€æœ‰å·²çŸ¥åŸºå‡†æµ‹è¯•ï¼Œè¾¾åˆ°31.15%é€šè¿‡ç‡ï¼Œå¹¶åœ¨ä¸åŒæ•°æ®é›†å’Œè¯­è¨€æ¨¡å‹ä¸Šå±•ç¤ºå‡ºè‰²æ€§èƒ½ã€‚è¿™ä¸ºAIè¾…åŠ©å½¢å¼åŒ–è¯æ˜ç”Ÿæˆæä¾›äº†æ·±å…¥è§è§£å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ•´åˆChatGPTå’ŒåŸºæœ¬æœç´¢æŠ€æœ¯ç®€åŒ–äº†æ­£å¼è¯æ˜ç”Ÿæˆã€‚</li>
<li>ä½¿ç”¨miniF2Fæ•°æ®é›†å±•ç¤ºäº†ChatGPTä¸Leançš„ç»“åˆä¼˜åŠ¿ã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„Leanæ¨¡å‹è¡¨ç°è¶…è¶Šæ‰€æœ‰å·²çŸ¥åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†å’Œè¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°å±•ç¤ºå…¶é€šç”¨æ€§ã€‚</li>
<li>AIè¾…åŠ©å½¢å¼åŒ–è¯æ˜ç”Ÿæˆå…·æœ‰å¹¿é˜”å‰æ™¯å’Œæ½œåŠ›ã€‚</li>
<li>é›†æˆå¯éªŒè¯çš„æ­£å¼è¯­è¨€å¦‚Leanå¢å¼ºäº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-98cc4a391f3a3638f8bf368a7f5e5e4c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Longer-Attention-Span-Increasing-Transformer-Context-Length-with-Sparse-Graph-Processing-Techniques"><a href="#Longer-Attention-Span-Increasing-Transformer-Context-Length-with-Sparse-Graph-Processing-Techniques" class="headerlink" title="Longer Attention Span: Increasing Transformer Context Length with Sparse   Graph Processing Techniques"></a>Longer Attention Span: Increasing Transformer Context Length with Sparse   Graph Processing Techniques</h2><p><strong>Authors:Nathaniel Tomczak, Sanmukh Kuppannagari</strong></p>
<p>Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the inputâ€™s context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve â€œtrue sparsityâ€ are lacking.   In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB). </p>
<blockquote>
<p>Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿç‰©ä¿¡æ¯å­¦ç­‰é¢†åŸŸå–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚è¿™ç§æˆåŠŸæºäºè¿™äº›æ¨¡å‹é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥è¡¨ç¤ºå’Œä¼ æ’­åºåˆ—æ•°æ®ä¸­å•ä¸ªæ ‡è®°ä¹‹é—´çš„æˆå¯¹äº¤äº’ã€‚ç„¶è€Œï¼Œè¿™é¡¹æ“ä½œçš„ä¸»è¦å±€é™æ€§åœ¨äºå…¶ä¸è¾“å…¥ä¸Šä¸‹æ–‡é•¿åº¦ç›¸å…³çš„äºŒæ¬¡å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦â€”â€”éœ€è¦æ•è·äº¤äº’çš„åºåˆ—é•¿åº¦ã€‚è¿™æå¤§åœ°é™åˆ¶äº†è¿™äº›æ¨¡å‹å¯ä»¥æ¨æ–­çš„åºåˆ—é•¿åº¦ã€‚å·²ç»è¿›è¡Œäº†å¤§é‡ç ”ç©¶ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶çš„ç¨€ç–æ€§ï¼Œå°†æˆå¯¹çš„äº¤äº’æ•°é‡å‡å°‘åˆ°æ¬¡äºŒæ¬¡æ–¹çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚ç„¶è€Œï¼Œå®ç°â€œçœŸæ­£ç¨€ç–æ€§â€çš„æœ‰æ•ˆå®ç°æ–¹æ³•ä»ç„¶ç¼ºä¹ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§å›¾å½¢è®¡ç®—è§†è§’æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå…¶ä¸­æ ‡è®°è¢«è§†ä¸ºå›¾çš„èŠ‚ç‚¹ï¼Œæ³¨æ„åŠ›æ©ç ç¡®å®šå›¾çš„è¾¹ã€‚ä½¿ç”¨è¿™ç§è§†è§’ï¼Œæˆ‘ä»¬å¼€å‘å›¾å½¢å¤„ç†ç®—æ³•æ¥å®ç°æ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨ç†è®ºå’Œå®è·µä¸Šï¼Œæˆ‘ä»¬éƒ½è¯æ˜äº†æˆ‘ä»¬çš„ç®—æ³•åªæ‰§è¡Œæ‰€éœ€çš„è®¡ç®—ï¼Œå³å®ƒä»¬æ˜¯æœ€ä¼˜çš„ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æµè¡Œçš„æ³¨æ„åŠ›æ©ç è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥æ¢ç´¢ç¨€ç–æ€§å¯¹æ‰§è¡Œæ—¶é—´å’Œå¯å®ç°ä¸Šä¸‹æ–‡é•¿åº¦çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜ï¼Œä¸FlashAttentionç­‰æœ€å…ˆè¿›çš„æ³¨æ„åŠ›å®ç°ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨æ‰§è¡Œæ—¶é—´ä¸Šå®ç°äº†æ˜¾è‘—åŠ é€Ÿã€‚æˆ‘ä»¬è¿˜è¯æ˜æˆ‘ä»¬çš„ç®—æ³•èƒ½å¤Ÿåœ¨å•ä¸ªNVIDIA A100 GPUï¼ˆSXM4 80GBï¼‰ä¸Šå®ç°é«˜è¾¾1äº¿6åƒä¸‡çš„æé•¿åºåˆ—é•¿åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01659v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformeræ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤šä¸ªé¢†åŸŸçš„åº”ç”¨ï¼Œä½†å­˜åœ¨å¯¹é•¿åºåˆ—è¾“å…¥æ—¶å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦è¿‡é«˜çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºå›¾è®¡ç®—çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°æ–¹æ³•ï¼Œå°†ä»¤ç‰Œè§†ä¸ºå›¾ä¸­çš„èŠ‚ç‚¹ï¼Œæ³¨æ„åŠ›æ©ç ç¡®å®šå›¾çš„è¾¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç†è®ºä¸Šæ˜¯é«˜æ•ˆçš„ï¼Œå¹¶ä¸”åœ¨æ‰§è¡Œæ—¶é—´å’Œå¤„ç†é•¿åºåˆ—æ–¹é¢ç›¸æ¯”ç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶è™½åœ¨è®¸å¤šé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å¯¹äºé•¿åºåˆ—è¾“å…¥å­˜åœ¨å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦è¿‡é«˜çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶è€…å¼•å…¥ç¨€ç–æ³¨æ„åŠ›æ©ç ä»¥å‡å°‘æ³¨æ„åŠ›æ“ä½œçš„æˆå¯¹äº¤äº’æ•°é‡ä»¥é™ä½å¤æ‚åº¦ã€‚</li>
<li>å½“å‰ç¼ºä¹å®ç°â€œçœŸæ­£ç¨€ç–æ€§â€çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºå›¾è®¡ç®—çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°æ–¹æ³•ï¼Œå°†ä»¤ç‰Œè§†ä¸ºå›¾ä¸­çš„èŠ‚ç‚¹ï¼Œæ³¨æ„åŠ›æ©ç ç¡®å®šå›¾çš„è¾¹ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†æŒ‰éœ€è®¡ç®—ï¼Œè¯æ˜äº†å…¶é«˜æ•ˆæ€§ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ‰§è¡Œæ—¶é—´å¹¶å¤„ç†äº†é•¿åºåˆ—é•¿åº¦è¾¾åˆ°æé«˜çš„åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜å…¶åœ¨å•å—NVIDIA A100 GPUä¸Šçš„å¤„ç†èƒ½åŠ›å¯è¾¾åˆ°æ•°åäº¿çº§åˆ«çš„åºåˆ—é•¿åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-01505b4be493203a652e61930ef9e4e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-974f422fbac37f20b4a9f0e696ea183b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4fd3e28bce1a9a9608a254ef91291da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd52538dbf8f8420c2ca6e277e6fa44d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e77d89af2861289a5442841628f9a410.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-967394e4f57a94c4711387061765e038.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80fa2dd79c6e7116485d5360d1a61723.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Question-to-Question-Retrieval-for-Hallucination-Free-Knowledge-Access-An-Approach-for-Wikipedia-and-Wikidata-Question-Answering"><a href="#Question-to-Question-Retrieval-for-Hallucination-Free-Knowledge-Access-An-Approach-for-Wikipedia-and-Wikidata-Question-Answering" class="headerlink" title="Question-to-Question Retrieval for Hallucination-Free Knowledge Access:   An Approach for Wikipedia and Wikidata Question Answering"></a>Question-to-Question Retrieval for Hallucination-Free Knowledge Access:   An Approach for Wikipedia and Wikidata Question Answering</h2><p><strong>Authors:Santhosh Thottingal</strong></p>
<p>This paper introduces an approach to question answering over knowledge bases like Wikipedia and Wikidata by performing â€œquestion-to-questionâ€ matching and retrieval from a dense vector embedding store. Instead of embedding document content, we generate a comprehensive set of questions for each logical content unit using an instruction-tuned LLM. These questions are vector-embedded and stored, mapping to the corresponding content. Vector embedding of user queries are then matched against this question vector store. The highest similarity score leads to direct retrieval of the associated article content, eliminating the need for answer generation. Our method achieves high cosine similarity ( &gt; 0.9 ) for relevant question pairs, enabling highly precise retrieval. This approach offers several advantages including computational efficiency, rapid response times, and increased scalability. We demonstrate its effectiveness on Wikipedia and Wikidata, including multimedia content through structured fact retrieval from Wikidata, opening up new pathways for multimodal question answering. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åœ¨çŸ¥è¯†åº“ï¼ˆå¦‚Wikipediaå’ŒWikidataï¼‰ä¸Šè¿›è¡Œé—®ç­”çš„æ–¹æ³•ï¼Œé€šè¿‡æ‰§è¡Œâ€œé—®é¢˜å¯¹é—®é¢˜â€çš„åŒ¹é…å’Œä»å¯†é›†å‘é‡åµŒå…¥å­˜å‚¨åº“ä¸­è¿›è¡Œæ£€ç´¢æ¥å®ç°ã€‚æˆ‘ä»¬ä¸æ˜¯åµŒå…¥æ–‡æ¡£å†…å®¹ï¼Œè€Œæ˜¯ä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ¯ä¸ªé€»è¾‘å†…å®¹å•å…ƒç”Ÿæˆä¸€ç»„ç»¼åˆé—®é¢˜ã€‚è¿™äº›é—®é¢˜è¢«å‘é‡åµŒå…¥å¹¶å­˜å‚¨ï¼Œæ˜ å°„åˆ°ç›¸åº”çš„å†…å®¹ã€‚ç„¶åï¼Œå°†ç”¨æˆ·æŸ¥è¯¢çš„å‘é‡åµŒå…¥ä¸æ­¤é—®é¢˜å‘é‡å­˜å‚¨åº“è¿›è¡ŒåŒ¹é…ã€‚æœ€é«˜ç›¸ä¼¼åº¦å¾—åˆ†ç›´æ¥å¯¼è‡´ç›¸å…³æ–‡ç« çš„ç›´æ¥æ£€ç´¢ï¼Œä»è€Œæ¶ˆé™¤äº†ç­”æ¡ˆç”Ÿæˆçš„éœ€è¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†é«˜ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ&gt; 0.9ï¼‰çš„ç›¸å…³é—®é¢˜é…å¯¹ï¼Œå¯å®ç°é«˜åº¦ç²¾ç¡®çš„æ£€ç´¢ã€‚è¿™ç§æ–¹æ³•æä¾›äº†å‡ ä¸ªä¼˜ç‚¹ï¼ŒåŒ…æ‹¬è®¡ç®—æ•ˆç‡é«˜ã€å“åº”é€Ÿåº¦å¿«å’Œå¯æ‰©å±•æ€§å¼ºã€‚æˆ‘ä»¬åœ¨Wikipediaå’ŒWikidataä¸Šå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬é€šè¿‡ä»Wikidataè¿›è¡Œç»“æ„åŒ–äº‹å®æ£€ç´¢çš„å¤šåª’ä½“å†…å®¹ï¼Œä¸ºå¤šåª’ä½“é—®ç­”æ‰“å¼€äº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11301v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šè¿‡æ‰§è¡Œâ€œé—®é¢˜åˆ°é—®é¢˜â€åŒ¹é…å’Œä»å¯†é›†å‘é‡åµŒå…¥å­˜å‚¨ä¸­è¿›è¡Œæ£€ç´¢ï¼Œå®ç°åœ¨çŸ¥è¯†åº“ï¼ˆå¦‚Wikipediaå’ŒWikidataï¼‰ä¸Šè¿›è¡Œé—®ç­”çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸åµŒå…¥æ–‡æ¡£å†…å®¹ï¼Œè€Œæ˜¯ä¸ºé€»è¾‘å†…å®¹å•å…ƒç”Ÿæˆä¸€å¥—ç»¼åˆé—®é¢˜ï¼Œè¿™äº›é—®é¢˜é€šè¿‡æŒ‡ä»¤ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå‘é‡åµŒå…¥å¹¶å­˜å‚¨ï¼Œæ˜ å°„åˆ°ç›¸åº”å†…å®¹ã€‚ç„¶åï¼Œå°†ç”¨æˆ·æŸ¥è¯¢çš„å‘é‡åµŒå…¥ä¸é—®é¢˜å‘é‡å­˜å‚¨è¿›è¡ŒåŒ¹é…ã€‚æœ€é«˜ç›¸ä¼¼åº¦å¾—åˆ†ç›´æ¥æ£€ç´¢ç›¸å…³çš„æ–‡ç« å†…å®¹ï¼Œæ— éœ€ç”Ÿæˆç­”æ¡ˆã€‚è¯¥æ–¹æ³•å®ç°äº†é«˜ä½™å¼¦ç›¸ä¼¼æ€§ï¼ˆ&gt; 0.9ï¼‰ï¼Œé€‚ç”¨äºç›¸å…³é—®é¢˜å¯¹ï¼Œå¯å®ç°ç²¾ç¡®æ£€ç´¢ã€‚è¯¥æ–¹æ³•å…·æœ‰è®¡ç®—æ•ˆç‡é«˜ã€å“åº”é€Ÿåº¦å¿«å’Œå¯æ‰©å±•æ€§å¼ºç­‰ä¼˜ç‚¹ã€‚åœ¨Wikipediaå’ŒWikidataä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬é€šè¿‡ä»Wikidataæ£€ç´¢ç»“æ„åŒ–äº‹å®æ¥åŒ…å«å¤šåª’ä½“å†…å®¹ï¼Œä¸ºå¤šåª’ä½“é—®ç­”å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯¥æ–¹æ³•é€šè¿‡â€œé—®é¢˜åˆ°é—®é¢˜â€åŒ¹é…å’Œå¯†é›†å‘é‡åµŒå…¥å­˜å‚¨è¿›è¡ŒçŸ¥è¯†åº“é—®ç­”ã€‚</li>
<li>æ–¹æ³•ä¸ºé€»è¾‘å†…å®¹å•å…ƒç”Ÿæˆç»¼åˆé—®é¢˜ï¼Œé€šè¿‡æŒ‡ä»¤ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå‘é‡åµŒå…¥ã€‚</li>
<li>ç”¨æˆ·æŸ¥è¯¢ä¸é—®é¢˜å‘é‡å­˜å‚¨åŒ¹é…ï¼Œå®ç°ç›´æ¥å†…å®¹æ£€ç´¢ã€‚</li>
<li>æ–¹æ³•å®ç°é«˜ä½™å¼¦ç›¸ä¼¼æ€§ï¼Œé€‚ç”¨äºç›¸å…³é—®é¢˜å¯¹ï¼Œç²¾ç¡®æ£€ç´¢ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰è®¡ç®—æ•ˆç‡é«˜ã€å“åº”é€Ÿåº¦å¿«å’Œå¯æ‰©å±•æ€§å¼ºç­‰ä¼˜ç‚¹ã€‚</li>
<li>åœ¨Wikipediaå’ŒWikidataä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-94ad1c981f88990969c90aea5f39f0e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76ef7522c412876487ecaffe5967303a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fbf31b3077833cc743f71babeb914f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f19c43e7d089e772a6e72fba9da7d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-469faf5e4b1c3b47975943aa77b71fa9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Performance-of-ChatGPT-on-tasks-involving-physics-visual-representations-the-case-of-the-Brief-Electricity-and-Magnetism-Assessment"><a href="#Performance-of-ChatGPT-on-tasks-involving-physics-visual-representations-the-case-of-the-Brief-Electricity-and-Magnetism-Assessment" class="headerlink" title="Performance of ChatGPT on tasks involving physics visual   representations: the case of the Brief Electricity and Magnetism Assessment"></a>Performance of ChatGPT on tasks involving physics visual   representations: the case of the Brief Electricity and Magnetism Assessment</h2><p><strong>Authors:Giulia Polverini, Jakob Melin, Elias Onerud, Bor Gregorcic</strong></p>
<p>Artificial intelligence-based chatbots are increasingly influencing physics education due to their ability to interpret and respond to textual and visual inputs. This study evaluates the performance of two large multimodal model-based chatbots, ChatGPT-4 and ChatGPT-4o on the Brief Electricity and Magnetism Assessment (BEMA), a conceptual physics inventory rich in visual representations such as vector fields, circuit diagrams, and graphs. Quantitative analysis shows that ChatGPT-4o outperforms both ChatGPT-4 and a large sample of university students, and demonstrates improvements in ChatGPT-4oâ€™s vision interpretation ability over its predecessor ChatGPT-4. However, qualitative analysis of ChatGPT-4oâ€™s responses reveals persistent challenges. We identified three types of difficulties in the chatbotâ€™s responses to tasks on BEMA: (1) difficulties with visual interpretation, (2) difficulties in providing correct physics laws or rules, and (3) difficulties with spatial coordination and application of physics representations. Spatial reasoning tasks, particularly those requiring the use of the right-hand rule, proved especially problematic. These findings highlight that the most broadly used large multimodal model-based chatbot, ChatGPT-4o, still exhibits significant difficulties in engaging with physics tasks involving visual representations. While the chatbot shows potential for educational applications, including personalized tutoring and accessibility support for students who are blind or have low vision, its limitations necessitate caution. On the other hand, our findings can also be leveraged to design assessments that are difficult for chatbots to solve. </p>
<blockquote>
<p>åŸºäºäººå·¥æ™ºèƒ½çš„èŠå¤©æœºå™¨äººå› å…¶è§£é‡Šå’Œå“åº”æ–‡æœ¬å’Œè§†è§‰è¾“å…¥çš„èƒ½åŠ›ï¼Œæ­£è¶Šæ¥è¶Šå¤šåœ°å½±å“ç‰©ç†æ•™è‚²ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸¤ä¸ªå¤§å‹å¤šæ¨¡å¼æ¨¡å‹åŸºç¡€èŠå¤©æœºå™¨äººChatGPT-4å’ŒChatGPT-4oåœ¨â€œç®€çŸ­çš„ç”µä¸ç£è¯„ä¼°(BEMA)â€ä¸Šçš„è¡¨ç°ã€‚BEMAæ˜¯ä¸€ä¸ªæ¦‚å¿µä¸°å¯Œçš„ç‰©ç†é¢˜åº“ï¼ŒåŒ…å«çŸ¢é‡åœºã€ç”µè·¯å›¾å’Œå›¾è¡¨ç­‰è§†è§‰è¡¨ç¤ºå½¢å¼ã€‚å®šé‡åˆ†æè¡¨æ˜ï¼ŒChatGPT-4oåœ¨ChatGPT-4å’Œå¤§é‡å¤§å­¦ç”Ÿæ ·æœ¬ä¸­çš„è¡¨ç°æ›´ä¸ºå‡ºè‰²ï¼Œå¹¶å±•ç¤ºäº†å…¶ç›¸è¾ƒäºå‰èº«ChatGPT-4åœ¨è§†è§‰è§£é‡Šèƒ½åŠ›æ–¹é¢çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œå¯¹ChatGPT-4oçš„å›åº”è¿›è¡Œå®šæ€§åˆ†ææ­ç¤ºäº†æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç¡®å®šäº†èŠå¤©æœºå™¨äººåœ¨BEMAä»»åŠ¡å›åº”ä¸­çš„ä¸‰ç§å›°éš¾ç±»å‹ï¼šï¼ˆ1ï¼‰è§†è§‰è§£é‡Šå›°éš¾ï¼Œï¼ˆ2ï¼‰æä¾›æ­£ç¡®ç‰©ç†å®šå¾‹æˆ–è§„åˆ™å›°éš¾ï¼Œï¼ˆ3ï¼‰ç©ºé—´åè°ƒå’Œç‰©ç†è¡¨ç¤ºåº”ç”¨å›°éš¾ã€‚ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯é‚£äº›éœ€è¦ä½¿ç”¨å³æ‰‹å®šåˆ™çš„ä»»åŠ¡ï¼Œè¯æ˜å°¤å…¶å›°éš¾ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæœ€å¹¿æ³›ä½¿ç”¨çš„å¤§å‹å¤šæ¨¡å¼æ¨¡å‹åŸºç¡€èŠå¤©æœºå™¨äººChatGPT-4oåœ¨æ¶‰åŠè§†è§‰è¡¨è¾¾çš„ç‰©ç†ä»»åŠ¡ä¸­ä»å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚è™½ç„¶èŠå¤©æœºå™¨äººåœ¨æ•™è‚²åº”ç”¨æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–è¾…å¯¼å’Œå¤±æ˜æˆ–è§†åŠ›å—æŸå­¦ç”Ÿçš„è¾…åŠ©æ”¯æŒï¼Œä½†å…¶å±€é™æ€§éœ€è¦è°¨æ…å¯¹å¾…ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„å‘ç°ä¹Ÿå¯ä»¥ç”¨æ¥è®¾è®¡èŠå¤©æœºå™¨äººéš¾ä»¥è§£å†³çš„è¯„ä¼°é¢˜ç›®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10019v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººå¯¹ç‰©ç†æ•™è‚²çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬å¯¹è§†è§‰è¾“å…¥çš„è§£è¯»å’Œå“åº”èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”ä¸¤ä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹èŠå¤©æœºå™¨äººChatGPT-4å’ŒChatGPT-4oåœ¨æ¦‚å¿µç‰©ç†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œå‘ç°ChatGPT-4oåœ¨è§†è§‰è§£è¯»èƒ½åŠ›ä¸Šæœ‰æ‰€æ”¹è¿›ï¼Œä½†å…¶è§£ç­”ä»å­˜åœ¨æŒ‘æˆ˜ã€‚é’ˆå¯¹è§†è§‰è§£è¯»ã€ç‰©ç†å®šå¾‹æˆ–è§„åˆ™æä¾›æ­£ç¡®æ€§ä»¥åŠç©ºé—´åè°ƒå’Œç‰©ç†è¡¨ç¤ºåº”ç”¨ç­‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚è™½ç„¶è¯¥èŠå¤©æœºå™¨äººåœ¨æ•™è‚²åº”ç”¨æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶å±€é™æ€§ä»éœ€è°¨æ…å¯¹å¾…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººå¯¹ç‰©ç†æ•™è‚²äº§ç”Ÿå½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£è¯»å’Œå“åº”è§†è§‰è¾“å…¥æ–¹é¢ã€‚</li>
<li>ChatGPT-4oåœ¨è§†è§‰è§£è¯»èƒ½åŠ›ä¸Šç›¸è¾ƒäºChatGPT-4æœ‰æ‰€æå‡ã€‚</li>
<li>ChatGPT-4oåœ¨å›ç­”ç‰©ç†ä»»åŠ¡æ—¶ä»é¢ä¸´å›°éš¾ï¼Œå°¤å…¶åœ¨æ¶‰åŠè§†è§‰è§£è¯»ã€ç‰©ç†å®šå¾‹æˆ–è§„åˆ™çš„æ­£ç¡®æä¾›ä»¥åŠç©ºé—´åè°ƒç­‰æ–¹é¢ã€‚</li>
<li>ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯éœ€è¦ä½¿ç”¨å³æ‰‹è§„åˆ™çš„ä»»åŠ¡ï¼Œå¯¹èŠå¤©æœºå™¨äººæ¥è¯´ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>è™½ç„¶ChatGPT-4oåœ¨ä¸ªæ€§åŒ–è¾…å¯¼å’Œè§†åŠ›éšœç¢å­¦ç”Ÿæ”¯æŒæ–¹é¢å…·æœ‰æ•™è‚²åº”ç”¨æ½œåŠ›ï¼Œä½†å…¶å±€é™æ€§éœ€è¦è°¨æ…å¯¹å¾…ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯ç”¨äºè®¾è®¡é’ˆå¯¹èŠå¤©æœºå™¨äººçš„è¯„ä¼°éš¾é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-21e37b5c4d5f408a7f97db2948f9d3e4.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CollabEdit-Towards-Non-destructive-Collaborative-Knowledge-Editing"><a href="#CollabEdit-Towards-Non-destructive-Collaborative-Knowledge-Editing" class="headerlink" title="CollabEdit: Towards Non-destructive Collaborative Knowledge Editing"></a>CollabEdit: Towards Non-destructive Collaborative Knowledge Editing</h2><p><strong>Authors:Jiamu Zheng, Jinghuai Zhang, Tianyu Du, Xuhong Zhang, Jianwei Yin, Tao Lin</strong></p>
<p>Collaborative learning of large language models (LLMs) has emerged as a new paradigm for utilizing private data from different parties to guarantee efficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also garnered increased attention due to its ability to manipulate the behaviors of LLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits of multiple parties are aggregated in a privacy-preserving and continual manner) unexamined. To this end, this manuscript dives into the first investigation of collaborative KE, in which we start by carefully identifying the unique three challenges therein, including knowledge overlap, knowledge conflict, and knowledge forgetting. We then propose a non-destructive collaborative KE framework, COLLABEDIT, which employs a novel model merging mechanism to mimic the global KE behavior while preventing the severe performance drop. Extensive experiments on two canonical datasets demonstrate the superiority of COLLABEDIT compared to other destructive baselines, and results shed light on addressing three collaborative KE challenges and future applications. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LINs-lab/CollabEdit">https://github.com/LINs-lab/CollabEdit</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åä½œå­¦ä¹ å·²ç»æˆä¸ºåˆ©ç”¨ä¸åŒæ–¹çš„ç§æœ‰æ•°æ®çš„æ–°èŒƒå¼ï¼Œä»¥ä¿è¯æ•ˆç‡å’Œéšç§ã€‚ä¸æ­¤åŒæ—¶ï¼Œç”±äºèƒ½å¤Ÿæ˜ç¡®æ“æ§LLMçš„è¡Œä¸ºï¼ŒLLMçš„çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰ä¹Ÿå¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œç„¶è€Œåä½œKEçš„æƒ…å†µï¼ˆå³å¤šæ–¹çŸ¥è¯†ç¼–è¾‘ä»¥éšç§ä¿æŠ¤å’ŒæŒç»­çš„æ–¹å¼è¿›è¡Œèšåˆï¼‰å°šæœªè¢«ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ·±å…¥ç ”ç©¶äº†åä½œKEï¼Œé¦–å…ˆä»”ç»†ç¡®å®šäº†å…¶ä¸­çš„ä¸‰ä¸ªç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬çŸ¥è¯†é‡å ã€çŸ¥è¯†å†²çªå’ŒçŸ¥è¯†é—å¿˜ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†éç ´åæ€§çš„åä½œKEæ¡†æ¶COLLABEDITï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æ–°å‹æ¨¡å‹åˆå¹¶æœºåˆ¶æ¥æ¨¡æ‹Ÿå…¨å±€KEè¡Œä¸ºï¼ŒåŒæ—¶é˜²æ­¢æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚åœ¨ä¸¤ä¸ªå…¸å‹æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†COLLABEDITä¸å…¶ä»–ç ´åæ€§åŸºå‡†çº¿çš„ä¼˜è¶Šæ€§ï¼Œå®éªŒç»“æœä¸ºè§£å†³ä¸‰ä¸ªåä½œKEæŒ‘æˆ˜å’Œæœªæ¥åº”ç”¨æä¾›äº†å¯ç¤ºã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/LINs-lab/CollabEdit%E3%80%82">https://github.com/LINs-lab/CollabEditã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09508v3">PDF</a> 20 pages, 11 figures. Published as a conference paper at ICLR 2025.   Code at <a target="_blank" rel="noopener" href="https://github.com/LINs-lab/CollabEdit">https://github.com/LINs-lab/CollabEdit</a></p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åä½œå­¦ä¹ å·²æˆä¸ºåˆ©ç”¨å„æ–¹ç§æœ‰æ•°æ®çš„æ–°èŒƒå¼ï¼Œä¿è¯äº†æ•ˆç‡å’Œéšç§ã€‚çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰æŠ€æœ¯å¯¹äºLLMçš„æ“æ§èƒ½åŠ›å¤‡å—å…³æ³¨ï¼Œä½†åœ¨åä½œKEçš„æƒ…å†µä¸‹ï¼ˆå³å¤šæ–¹çŸ¥è¯†ç¼–è¾‘åœ¨ä¿æŠ¤éšç§å’ŒæŒç»­æ€§çš„æ–¹å¼è¿›è¡Œèšåˆï¼‰å°šæœªè¢«ç ”ç©¶ã€‚æœ¬æ–‡é¦–æ¬¡æ¢ç©¶åä½œKEï¼Œé¦–å…ˆæ˜ç¡®äº†å…¶ä¸­çš„ä¸‰å¤§ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬çŸ¥è¯†é‡å ã€çŸ¥è¯†å†²çªå’ŒçŸ¥è¯†é—å¿˜ã€‚éšåï¼Œæå‡ºäº†ä¸€ç§éç ´åæ€§çš„åä½œKEæ¡†æ¶COLLABEDITï¼Œé‡‡ç”¨æ–°å‹æ¨¡å‹åˆå¹¶æœºåˆ¶æ¥æ¨¡æ‹Ÿå…¨å±€KEè¡Œä¸ºï¼ŒåŒæ—¶é˜²æ­¢æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚åœ¨å…¸å‹æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCOLLABEDITä¸å…¶ä»–ç ´åæ€§åŸºçº¿ç›¸æ¯”å…·æœ‰ä¼˜è¶Šæ€§ï¼Œä¸ºè§£å†³ä¸‰é¡¹åä½œKEæŒ‘æˆ˜å’Œæœªæ¥åº”ç”¨æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åä½œå­¦ä¹ æˆä¸ºåˆ©ç”¨LLMçš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨ä¿è¯æ•ˆç‡å’Œéšç§ã€‚</li>
<li>çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰æŠ€æœ¯èƒ½å¤Ÿæ˜ç¡®æ“æ§LLMçš„è¡Œä¸ºã€‚</li>
<li>åä½œçŸ¥è¯†ç¼–è¾‘ï¼ˆCKEï¼‰å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨éšç§ä¿æŠ¤å’ŒæŒç»­æ€§æ–¹é¢ã€‚</li>
<li>CKEé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šçŸ¥è¯†é‡å ã€çŸ¥è¯†å†²çªå’ŒçŸ¥è¯†é—å¿˜ã€‚</li>
<li>COLLABEDITæ¡†æ¶è¢«æå‡ºç”¨äºè§£å†³CKEçš„æŒ‘æˆ˜ï¼Œé‡‡ç”¨æ–°å‹æ¨¡å‹åˆå¹¶æœºåˆ¶ã€‚</li>
<li>å®éªŒè¡¨æ˜COLLABEDITæ¡†æ¶åœ¨å…¸å‹æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d00acde680f1dac53364eeab13b36d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3668c21ab9c13cd23995f8c27ef61296.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Simplicity-Prevails-Rethinking-Negative-Preference-Optimization-for-LLM-Unlearning"><a href="#Simplicity-Prevails-Rethinking-Negative-Preference-Optimization-for-LLM-Unlearning" class="headerlink" title="Simplicity Prevails: Rethinking Negative Preference Optimization for LLM   Unlearning"></a>Simplicity Prevails: Rethinking Negative Preference Optimization for LLM   Unlearning</h2><p><strong>Authors:Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Ruiqi Zhang, Song Mei, Sijia Liu</strong></p>
<p>This work studies the problem of large language model (LLM) unlearning, aiming to remove unwanted data influences (e.g., copyrighted or harmful content) while preserving model utility. Despite the increasing demand for unlearning, a technically-grounded optimization framework is lacking. Gradient ascent (GA)-type methods, though widely used, are suboptimal as they reverse the learning process without controlling optimization divergence (i.e., deviation from the pre-trained state), leading to risks of over-forgetting and potential model collapse. Negative preference optimization (NPO) has been proposed to address this issue and is considered one of the state-of-the-art LLM unlearning approaches. In this work, we revisit NPO and identify another critical issue: reference model bias. This bias arises from using the reference model (i.e., the model prior to unlearning) to evaluate the unlearning success, which can compromise NPOâ€™s effectiveness. Specifically, it leads to (a) uneven allocation of optimization power across forget data with varying difficulty levels and (b) ineffective gradient weight smoothing during the early stages of unlearning optimization. To overcome these challenges, we propose a simple yet effective unlearning optimization framework, called SimNPO, showing that &#96;simplicityâ€™ in removing the reliance on a reference model (through the lens of simple preference optimization) benefits unlearning. We provide deeper insights into SimNPOâ€™s advantages through an analysis based on mixtures of Markov chains. Extensive experiments further validate SimNPOâ€™s efficacy on benchmarks like TOFU and MUSE, as well as its robustness against relearning attacks. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/OPTML-Group/Unlearn-Simple">https://github.com/OPTML-Group/Unlearn-Simple</a>. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å»å­¦ä¹ é—®é¢˜ï¼Œæ—¨åœ¨æ¶ˆé™¤ä¸å¿…è¦çš„æ•°æ®å½±å“ï¼ˆä¾‹å¦‚ç‰ˆæƒæˆ–æœ‰å®³å†…å®¹ï¼‰ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚å°½ç®¡å¯¹å»å­¦ä¹ çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œä½†ç¼ºä¹æŠ€æœ¯åŸºç¡€çš„ä¼˜åŒ–æ¡†æ¶ã€‚æ¢¯åº¦ä¸Šå‡ï¼ˆGAï¼‰ç±»æ–¹æ³•è™½ç„¶åº”ç”¨å¹¿æ³›ï¼Œä½†å¹¶ä¸ç†æƒ³ï¼Œå› ä¸ºå®ƒä»¬é€†è½¬å­¦ä¹ è¿‡ç¨‹ï¼ŒåŒæ—¶ä¸æ§åˆ¶ä¼˜åŒ–å‘æ•£ï¼ˆå³åç¦»é¢„è®­ç»ƒçŠ¶æ€ï¼‰ï¼Œå¯¼è‡´è¿‡åº¦é—å¿˜å’Œæ½œåœ¨æ¨¡å‹å´©æºƒçš„é£é™©ã€‚è´Ÿåå¥½ä¼˜åŒ–ï¼ˆNPOï¼‰å·²è¢«æå‡ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¢«è®¤ä¸ºæ˜¯æœ€æ–°çš„LLMå»å­¦ä¹ æ–¹æ³•ä¹‹ä¸€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†NPOå¹¶ç¡®å®šäº†å¦ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå‚è€ƒæ¨¡å‹åè§ã€‚è¿™ç§åè§æ¥è‡ªäºä½¿ç”¨å‚è€ƒæ¨¡å‹ï¼ˆå³å»å­¦ä¹ ä¹‹å‰çš„æ¨¡å‹ï¼‰æ¥è¯„ä¼°å»å­¦ä¹ çš„æˆåŠŸç¨‹åº¦ï¼Œè¿™å¯èƒ½ä¼šå±åŠNPOçš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä¼šå¯¼è‡´ï¼ˆaï¼‰åœ¨å…·æœ‰ä¸åŒéš¾åº¦çº§åˆ«çš„é—å¿˜æ•°æ®ä¸Šåˆ†é…ä¸å‡åŒ€çš„ä¼˜åŒ–èƒ½åŠ›ï¼›ï¼ˆbï¼‰åœ¨å»å­¦ä¹ çš„æ—©æœŸé˜¶æ®µï¼Œæ¢¯åº¦æƒé‡å¹³æ»‘æ— æ•ˆã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å»å­¦ä¹ ä¼˜åŒ–æ¡†æ¶ï¼Œç§°ä¸ºSimNPOï¼Œè¡¨æ˜åœ¨å»é™¤å¯¹å‚è€ƒæ¨¡å‹çš„ä¾èµ–ï¼ˆé€šè¿‡ç®€å•åå¥½ä¼˜åŒ–çš„è§†è§’ï¼‰æ–¹é¢ï¼Œâ€œç®€æ´æ€§â€æœ‰ç›Šäºå»å­¦ä¹ ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºé©¬å°”å¯å¤«é“¾æ··åˆçš„åˆ†ææä¾›äº†å¯¹SimNPOä¼˜åŠ¿çš„æ·±å…¥äº†è§£ã€‚å¹¿æ³›çš„å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†SimNPOåœ¨TOFUå’ŒMUSEç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå…¶å¯¹æŠ—é‡æ–°å­¦ä¹ æ”»å‡»çš„ç¨³å¥æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/OPTML-Group/Unlearn-Simple">https://github.com/OPTML-Group/Unlearn-Simple</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07163v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é—å¿˜å­¦ä¹ é—®é¢˜ï¼Œæ—¨åœ¨ç§»é™¤ä¸å¿…è¦çš„æ•°æ®å½±å“ï¼ˆå¦‚ç‰ˆæƒæˆ–æœ‰å®³å†…å®¹ï¼‰ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„å®ç”¨æ€§ã€‚é’ˆå¯¹é—å¿˜å­¦ä¹ ç¼ºä¹æŠ€æœ¯æ€§çš„ä¼˜åŒ–æ¡†æ¶ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„ä¼˜åŒ–æ¡†æ¶SimNPOï¼Œå®ƒç®€åŒ–äº†å¯¹å‚è€ƒæ¨¡å‹çš„ä¾èµ–ï¼Œé€šè¿‡ç®€å•çš„åå¥½ä¼˜åŒ–å®ç°é—å¿˜å­¦ä¹ ï¼Œå¹¶é€šè¿‡æ··åˆé©¬å°”å¯å¤«é“¾çš„åˆ†ææ·±å…¥æ¢è®¨äº†SimNPOçš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimNPOåœ¨TOFUå’ŒMUSEç­‰åŸºå‡†æµ‹è¯•ä¸Šå…·æœ‰è‰¯å¥½çš„æ•ˆæœï¼Œå¹¶ä¸”å¯¹äºé‡æ–°å­¦ä¹ æ”»å‡»å…·æœ‰é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é—å¿˜å­¦ä¹ æ˜¯ç§»é™¤ä¸å¿…è¦æ•°æ®å½±å“åŒæ—¶ä¿ç•™æ¨¡å‹å®ç”¨æ€§çš„ç ”ç©¶é—®é¢˜ã€‚</li>
<li>ç°æœ‰é—å¿˜å­¦ä¹ æ–¹æ³•å¦‚åŸºäºæ¢¯åº¦ä¸Šå‡çš„æ–¹æ³•å­˜åœ¨ä¼˜åŒ–å‘æ•£çš„é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´è¿‡åº¦é—å¿˜å’Œæ¨¡å‹å´©æºƒã€‚</li>
<li>è´Ÿåå¥½ä¼˜åŒ–ï¼ˆNPOï¼‰è¢«è®¤ä¸ºæ˜¯ç›®å‰å…ˆè¿›çš„LLMé—å¿˜å­¦ä¹ æ–¹æ³•ä¹‹ä¸€ï¼Œä½†å­˜åœ¨å‚è€ƒæ¨¡å‹åè§é—®é¢˜ã€‚</li>
<li>å‚è€ƒæ¨¡å‹åè§æºäºä½¿ç”¨æœªé—å¿˜å‰çš„æ¨¡å‹æ¥è¯„ä¼°é—å¿˜æˆåŠŸçš„ç¨‹åº¦ï¼Œå¯èƒ½å½±å“NPOçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„SimNPOæ¡†æ¶é€šè¿‡ç®€åŒ–åå¥½ä¼˜åŒ–ï¼Œå»é™¤å¯¹å‚è€ƒæ¨¡å‹çš„ä¾èµ–ï¼Œè§£å†³äº†å‚è€ƒæ¨¡å‹åè§é—®é¢˜ã€‚</li>
<li>SimNPOæ¡†æ¶åœ¨TOFUå’ŒMUSEç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆèƒ½ï¼Œä¸”å¯¹é‡æ–°å­¦ä¹ æ”»å‡»å…·æœ‰é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34e2da23b7a1fcb626502f034e0a053c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fd3d849b60de564cce9f241234059f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-299f175f33507434fc062a9a59528567.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-11/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-11/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-11/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-34f69da5a9fe52608df7b824c5373e42.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-11  nvAgent Automated Data Visualization from Natural Language via   Collaborative Agent Workflow
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5c7ad26b8fba8b43b39e61fddc5802f5.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-09  OmniBal Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
