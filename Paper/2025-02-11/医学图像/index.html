<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-11  Chest X-ray Foundation Model with Global and Local Representations   Integration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-41c9ab818dd1c1e9027abe02b39211c8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    53 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-11-更新"><a href="#2025-02-11-更新" class="headerlink" title="2025-02-11 更新"></a>2025-02-11 更新</h1><h2 id="Chest-X-ray-Foundation-Model-with-Global-and-Local-Representations-Integration"><a href="#Chest-X-ray-Foundation-Model-with-Global-and-Local-Representations-Integration" class="headerlink" title="Chest X-ray Foundation Model with Global and Local Representations   Integration"></a>Chest X-ray Foundation Model with Global and Local Representations   Integration</h2><p><strong>Authors:Zefan Yang, Xuanang Xu, Jiajin Zhang, Ge Wang, Mannudeep K. Kalra, Pingkun Yan</strong></p>
<p>Chest X-ray (CXR) is the most frequently ordered imaging test, supporting diverse clinical tasks from thoracic disease detection to postoperative monitoring. However, task-specific classification models are limited in scope, require costly labeled data, and lack generalizability to out-of-distribution datasets. To address these challenges, we introduce CheXFound, a self-supervised vision foundation model that learns robust CXR representations and generalizes effectively across a wide range of downstream tasks. We pretrain CheXFound on a curated CXR-1M dataset, comprising over one million unique CXRs from publicly available sources. We propose a Global and Local Representations Integration (GLoRI) module for downstream adaptations, by incorporating disease-specific local features with global image features for enhanced performance in multilabel classification. Our experimental results show that CheXFound outperforms state-of-the-art models in classifying 40 disease findings across different prevalence levels on the CXR-LT 24 dataset and exhibits superior label efficiency on downstream tasks with limited training data. Additionally, CheXFound achieved significant improvements on new tasks with out-of-distribution datasets, including opportunistic cardiovascular disease risk estimation and mortality prediction. These results highlight CheXFound’s strong generalization capabilities, enabling diverse adaptations with improved label efficiency. The project source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/RPIDIAL/CheXFound">https://github.com/RPIDIAL/CheXFound</a>. </p>
<blockquote>
<p>胸部X光（CXR）是最常进行的影像检查，支持从胸部疾病检测到术后监护的多种临床任务。然而，特定任务的分类模型在范围上有限，需要昂贵的标注数据，并且对超出分布范围的数据集缺乏通用性。为了应对这些挑战，我们引入了CheXFound，这是一种自我监督的视觉基础模型，能够学习稳健的CXR表示，并在广泛的下游任务中有效地进行推广。我们在定制的CXR-1M数据集上预训练CheXFound，该数据集包含来自公开来源的一百万多个独特的CXRs。我们提出了一个全局和局部表示集成（GLoRI）模块，用于下游适应，通过结合疾病特定的局部特征与全局图像特征，以提高多标签分类的性能。我们的实验结果表明，在CXR-LT 24数据集上，CheXFound在分类不同发病率的40种疾病发现方面优于最先进模型，并且在下游任务中表现出优越的标签效率，特别是在训练数据有限的情况下。此外，CheXFound在新任务上的表现也有显著改善，包括机会性心血管疾病风险评估和死亡率预测等超出分布范围的数据集。这些结果突显了CheXFound强大的通用性能力，能够实现多样化的适应并具有改进的标签效率。项目源代码可在<a target="_blank" rel="noopener" href="https://github.com/RPIDIAL/CheXFound%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/RPIDIAL/CheXFound获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05142v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CheXFound是一个基于自监督学习的通用医学图像基础模型，用于学习稳健的胸部X射线（CXR）表示，并广泛应用于多种下游任务。该模型在CXR-1M数据集上进行预训练，并通过结合全局图像特征和疾病特定的局部特征，使用一个名为Global and Local Representations Integration (GLoRI)的模块进行下游任务适配。实验结果表明，CheXFound在CXR-LT 24数据集上分类40种疾病的能力优于当前主流模型，并在具有有限训练数据的新任务上展现出出色的标签效率。此外，CheXFound在机会性心血管疾病风险评估和死亡率预测等离分布数据集的新任务上也取得了显著改进，证明了其强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CheXFound是一个自监督的医学图像基础模型，用于学习稳健的胸部X射线（CXR）表示。</li>
<li>CheXFound在广泛的下游任务中表现出强大的泛化能力。</li>
<li>通过预训练的CXR-1M数据集进行训练，包含超过一百万张独特的胸部X射线图像。</li>
<li>引入Global and Local Representations Integration (GLoRI)模块，结合全局图像特征和疾病特定的局部特征，提高下游任务的多标签分类性能。</li>
<li>在CXR-LT 24数据集上分类40种疾病的能力优于当前主流模型。</li>
<li>在具有有限训练数据的新任务上展现出出色的标签效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05142">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5df4763452a11fcb53557bc1a1904cc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ff754f3d8dbfa7115277cc15286d996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf697bcaabd67610dca9dd15afd78a6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be23a84fbe01b9cbcabc9de64d316d49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-072bcdf7cca794c3e130c6082440f09f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-130c0c80891fb409e3b3ab9b4c6af6bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecfd8a2964f6547100925afbe14f8580.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Graph-Contrastive-Learning-for-Connectome-Classification"><a href="#Graph-Contrastive-Learning-for-Connectome-Classification" class="headerlink" title="Graph Contrastive Learning for Connectome Classification"></a>Graph Contrastive Learning for Connectome Classification</h2><p><strong>Authors:Martín Schmidt, Sara Silva, Federico Larroca, Gonzalo Mateos, Pablo Musé</strong></p>
<p>With recent advancements in non-invasive techniques for measuring brain activity, such as magnetic resonance imaging (MRI), the study of structural and functional brain networks through graph signal processing (GSP) has gained notable prominence. GSP stands as a key tool in unraveling the interplay between the brain’s function and structure, enabling the analysis of graphs defined by the connections between regions of interest – referred to as connectomes in this context. Our work represents a further step in this direction by exploring supervised contrastive learning methods within the realm of graph representation learning. The main objective of this approach is to generate subject-level (i.e., graph-level) vector representations that bring together subjects sharing the same label while separating those with different labels. These connectome embeddings are derived from a graph neural network Encoder-Decoder architecture, which jointly considers structural and functional connectivity. By leveraging data augmentation techniques, the proposed framework achieves state-of-the-art performance in a gender classification task using Human Connectome Project data. More broadly, our connectome-centric methodological advances support the promising prospect of using GSP to discover more about brain function, with potential impact to understanding heterogeneity in the neurodegeneration for precision medicine and diagnosis. </p>
<blockquote>
<p>随着无创技术（如磁共振成像MRI）的最新发展，通过图形信号处理（GSP）研究脑的结构和功能网络已受到广泛关注。图形信号处理是揭示大脑功能和结构之间相互作用的关键工具，能够分析感兴趣区域之间连接所定义的图形，本文称之为连接组。我们的工作朝着这个方向迈出了一步，在图形表示学习的范围内探索了监督对比学习方法。这种方法的主要目标是生成主体级（即图级）的向量表示，这些表示能够将具有相同标签的主体聚集在一起，同时将具有不同标签的主体分开。这些连接组嵌入是从图形神经网络编码器-解码器架构中派生出来的，同时考虑了结构和功能连接。通过利用数据增强技术，所提出的框架在利用人类连接组项目数据的性别分类任务上达到了最先进的性能。更广泛地说，我们的以连接组为中心的方法论进步支持了使用图形信号处理来了解更多关于大脑功能的希望，这对精确医学和诊断中神经变异的异质性理解具有潜在影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05109v1">PDF</a> Submitted to EMBC ‘25</p>
<p><strong>Summary</strong><br>医学图像研究领域近期借助非侵入技术（如磁共振成像MRI）的进步，通过图信号处理（GSP）研究脑结构和功能网络变得尤为突出。本研究旨在通过图表示学习中的监督对比学习方法，生成主体级别的向量表示，使相同标签的主体聚集，不同标签的主体分离。这些脑连接组嵌入来自图神经网络Encoder-Decoder架构，该架构联合考虑结构和功能连接。利用数据增强技术，该框架实现了人类连接组项目数据中的性别分类任务的最新性能。该研究的连接组为中心的方法论进步支持了GSP在了解大脑功能方面的前景，具有对神经变性的异质性的深入了解的潜力，有助于精准医疗和诊断。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用非侵入技术（如MRI）进行脑结构和功能网络的研究成为当前焦点。</li>
<li>图信号处理（GSP）在解析脑功能结构交互中扮演重要角色。</li>
<li>监督对比学习方法用于生成主体级别的向量表示，实现相同标签主体的聚集和不同标签主体的分离。</li>
<li>脑连接组嵌入来源于图神经网络Encoder-Decoder架构，同时考虑结构和功能连接。</li>
<li>利用数据增强技术实现了性别分类任务的卓越性能。</li>
<li>研究方法在连接组为中心的图信号处理领域的进步对于深入了解大脑功能具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05109">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3e44c33b6cf15277c7c9e37b8a68565d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c27605f4336587ec4a8c249ca0b0d91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61d2060384662baa447142debc1b749f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86ff7f6592775a4e91922654d7324603.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Gaze-Guided-Robotic-Vascular-Ultrasound-Leveraging-Human-Intention-Estimation"><a href="#Gaze-Guided-Robotic-Vascular-Ultrasound-Leveraging-Human-Intention-Estimation" class="headerlink" title="Gaze-Guided Robotic Vascular Ultrasound Leveraging Human Intention   Estimation"></a>Gaze-Guided Robotic Vascular Ultrasound Leveraging Human Intention   Estimation</h2><p><strong>Authors:Yuan Bi, Yang Su, Nassir Navab, Zhongliang Jiang</strong></p>
<p>Medical ultrasound has been widely used to examine vascular structure in modern clinical practice. However, traditional ultrasound examination often faces challenges related to inter- and intra-operator variation. The robotic ultrasound system (RUSS) appears as a potential solution for such challenges because of its superiority in stability and reproducibility. Given the complex anatomy of human vasculature, multiple vessels often appear in ultrasound images, or a single vessel bifurcates into branches, complicating the examination process. To tackle this challenge, this work presents a gaze-guided RUSS for vascular applications. A gaze tracker captures the eye movements of the operator. The extracted gaze signal guides the RUSS to follow the correct vessel when it bifurcates. Additionally, a gaze-guided segmentation network is proposed to enhance segmentation robustness by exploiting gaze information. However, gaze signals are often noisy, requiring interpretation to accurately discern the operator’s true intentions. To this end, this study proposes a stabilization module to process raw gaze data. The inferred attention heatmap is utilized as a region proposal to aid segmentation and serve as a trigger signal when the operator needs to adjust the scanning target, such as when a bifurcation appears. To ensure appropriate contact between the probe and surface during scanning, an automatic ultrasound confidence-based orientation correction method is developed. In experiments, we demonstrated the efficiency of the proposed gaze-guided segmentation pipeline by comparing it with other methods. Besides, the performance of the proposed gaze-guided RUSS was also validated as a whole on a realistic arm phantom with an uneven surface. </p>
<blockquote>
<p>现代医学实践中，医用超声已广泛应用于血管结构的检查。然而，传统超声检查常常面临操作者间和操作者内部的挑战。由于其在稳定性和可重复性方面的优势，机器人超声系统（RUSS）的出现为解决这些挑战提供了潜在解决方案。考虑到人类血管的复杂结构，超声图像中经常会出现多条血管，或者单个血管分叉成多个分支，使检查过程复杂化。为解决这一挑战，本研究提出了一种用于血管应用的目光引导型RUSS。目光追踪器捕捉操作员的眼球运动。提取的目光信号引导RUSS在血管分叉时追踪正确的血管。此外，本研究还提出了一种目光引导分割网络，利用目光信息提高分割的稳健性。然而，目光信号往往存在噪声，需要解读以准确判断操作员的真正意图。为此，本研究提出了一个稳定模块来处理原始的目光数据。推断出的注意力热图被用作区域提案，以辅助分割，并在操作员需要调整扫描目标（例如出现分叉时）时作为触发信号。为确保扫描过程中探头与表面之间的适当接触，开发了一种基于自动超声置信度的方向校正方法。通过与其他方法进行比较，我们验证了所提出的目光引导分割流程的效率。此外，在实际的手臂模型（具有不均匀表面）上，也对所提出目光引导型RUSS的整体性能进行了验证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05053v1">PDF</a> </p>
<p><strong>摘要</strong><br>医学超声广泛应用于现代临床实践中血管结构的检查，但传统超声检查常面临操作者间和操作者内部差异的挑战。机器人超声系统（RUSS）因其稳定性和可重复性优势，成为解决这些挑战的潜在方案。针对人类血管复杂结构，超声图像中常出现多条血管或单条血管分叉，使检查过程复杂化。本研究提出了一种用于血管应用的目光引导型RUSS。目光追踪器捕捉操作者的眼神动作，提取的目光信号引导RUSS在血管分叉时跟踪正确的血管。同时，提出一种目光引导分割网络，利用目光信息提高分割稳健性。然而，目光信号往往存在噪声，需要解读以准确判断操作者的真正意图。因此，本研究提出一种稳定模块来处理原始目光数据。推断出的注意力热图被用作区域提案，辅助分割，并在出现分叉等操作时触发信号，提醒操作者调整扫描目标。为确保扫描过程中探头与表面之间的适当接触，开发了一种基于自动超声置信度的方向校正方法。实验表明，与其他方法相比，所提出的目光引导分割管道的效率较高。此外，在具有不平表面的真实手臂幻影上验证了所提出目光引导型RUSS的性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>医学超声在血管结构检查中具有广泛应用，但传统超声检查面临操作者差异和血管结构复杂的挑战。</li>
<li>机器人超声系统（RUSS）因其稳定性和可重复性，被视为解决这些挑战的有潜力的方案。</li>
<li>提出一种目光引导型RUSS，通过捕捉操作者的眼动来指导机器人系统在复杂血管结构中的操作。</li>
<li>利用目光信号来增强图像分割的稳健性，并提出一种稳定模块来处理含有噪声的目光数据。</li>
<li>注意力热图作为区域提案，在血管分叉等情况下辅助分割并触发操作调整信号。</li>
<li>开发了一种自动超声置信度方法，以确保探头在扫描过程中的方向校正。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05053">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cbe80b234da599b41b564e1e15cb4cca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6f9a386f1feb0c85f68ee6d9f7b6c8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-159a6f7af168db1a00cf45843c12740e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4afd2a836adf14948fa4ebacda565e7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18650567901b9534382a36ad2ffa7285.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="WGM-microprobe-device-for-high-sensitivity-and-broadband-ultrasound-detection"><a href="#WGM-microprobe-device-for-high-sensitivity-and-broadband-ultrasound-detection" class="headerlink" title="WGM microprobe device for high-sensitivity and broadband ultrasound   detection"></a>WGM microprobe device for high-sensitivity and broadband ultrasound   detection</h2><p><strong>Authors:Jialve Sun, Shengnan Huangfu, Tinglan Chen, Zijing Cai, Bowen Ruan, Fangxing Zhang</strong></p>
<p>Whispering-gallery-mode (WGM) microcavities have emerged as a promising alternative to traditional ultrasound probes, offering high sensitivity and wide bandwidth. In our research, we propose a novel silica WGM microprobe device, with impressive Q factors up to 10^7.The side-coupled approach and special encapsulation design make the device small, robust, and capable of utilizing in both gaseous and liquid environments.We have successfully conducted photoacoustic (PA) imaging on various samples using this device which demonstrates a high sensitivity of 5.4 mPa&#x2F;sqrt(Hz) and a board bandwidth of 41 MHz at -6 dB for ultrasound. What’s more, it’s capable of capturing the vibration spectrum of microparticles up to a few hundred megahertz. Our compact and lightweight device exhibits significant application potential in PA endoscopic detection, near-field ultrasound sensing and other aspects. </p>
<blockquote>
<p>低语型腔模式（WGM）微腔已经作为一种前景光明的超声探针替代方案出现，它提供了高灵敏度和宽带宽。在我们的研究中，我们提出了一种新型二氧化硅WGM微型探针设备，具有高达的Q因子（Q factor）达百万次方级。侧耦合方式和特殊的封装设计使得该设备体积小、稳定性高，能在气态和液态环境中都能应用自如。我们已经成功使用该设备对各种样本进行光声成像，证明了其灵敏度高达每赫兹的声压级为5.4米帕（mPa），超声波的带宽在-6分贝时达到41兆赫（MHz）。此外，它能够捕捉到高达数百兆赫兹的微粒子振动频谱。我们的紧凑且轻型的设备在光声内窥检测、近场超声感应等方面表现出显著的应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04627v1">PDF</a> </p>
<p><strong>Summary</strong><br>微腔中的回音壁模式（WGM）微腔作为一种有前景的超声探针替代方案，具有高灵敏度和宽带宽的特点。研究中提出了一种新型二氧化硅WGM微探针设备，具有高达10^7的Q因子。该设备的侧耦合方式和特殊封装设计使其体积小、稳健，既可用于气体环境也可用于液体环境。使用此设备进行的光声成像实验表明，其灵敏度高达5.4 mPa&#x2F;sqrt(Hz)，在-6 dB时的超声波带宽为41 MHz，并能捕捉到高达数百兆赫兹的微粒子振动光谱。此紧凑轻便的设备在光声内窥检测、近场超声传感等方面具有显著的应用潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WGM微腔作为超声探针的替代方案，具有高灵敏度和宽带宽特性。</li>
<li>新型二氧化硅WGM微探针设备具有高达10^7的Q因子。</li>
<li>设备采用侧耦合方式和特殊封装设计，体积小、稳健，适用于气体和液体环境。</li>
<li>光声成像实验证明了设备的高灵敏度和宽带宽性能。</li>
<li>设备能捕捉到高达数百兆赫兹的微粒子振动光谱。</li>
<li>该设备具有广泛的应用潜力，特别是在光声内窥检测、近场超声传感等领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-99c799c78ac2892ab5bf05ed48de471d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2934d60cb2c951db362fc60b1d1cef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07f6526b592d8d45cf1c225b5600863e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e053710ca7242a781bcd2ee1730b7b68.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generative-Autoregressive-Transformers-for-Model-Agnostic-Federated-MRI-Reconstruction"><a href="#Generative-Autoregressive-Transformers-for-Model-Agnostic-Federated-MRI-Reconstruction" class="headerlink" title="Generative Autoregressive Transformers for Model-Agnostic Federated MRI   Reconstruction"></a>Generative Autoregressive Transformers for Model-Agnostic Federated MRI   Reconstruction</h2><p><strong>Authors:Valiyeh A. Nezhad, Gokberk Elmas, Bilal Kabas, Fuat Arslan, Tolga Çukur</strong></p>
<p>Although learning-based models hold great promise for MRI reconstruction, single-site models built on limited local datasets often suffer from poor generalization. This challenge has spurred interest in collaborative model training on multi-site datasets via federated learning (FL) – a privacy-preserving framework that aggregates model updates instead of sharing imaging data. Conventional FL builds a global model by aggregating locally trained model weights, inherently constraining all sites to a homogeneous model architecture. This rigid homogeneity requirement forces sites to forgo architectures tailored to their compute infrastructure and application-specific demands. Consequently, existing FL methods for MRI reconstruction fail to support model-heterogeneous settings, where individual sites are allowed to use distinct architectures. To overcome this fundamental limitation, here we introduce FedGAT, a novel model-agnostic FL technique based on generative autoregressive transformers. FedGAT decentralizes the training of a global generative prior that captures the distribution of multi-site MR images. For enhanced fidelity, we propose a novel site-prompted GAT prior that controllably synthesizes MR images from desired sites via autoregressive prediction across spatial scales. Each site then trains its site-specific reconstruction model – using its preferred architecture – on a hybrid dataset comprising the local MRI dataset and GAT-generated synthetic MRI datasets for other sites. Comprehensive experiments on multi-institutional datasets demonstrate that FedGAT supports flexible collaborations while enjoying superior within-site and across-site reconstruction performance compared to state-of-the-art FL baselines. </p>
<blockquote>
<p>尽管基于学习的模型在MRI重建方面展现出巨大潜力，但仅依赖于有限本地数据集的单站点模型通常面临泛化能力差的挑战。这一挑战激发了对通过联邦学习（FL）进行多站点数据集协作模型训练的兴趣。联邦学习是一种隐私保护框架，它通过聚合模型更新而不是共享成像数据来进行训练。传统的联邦学习通过聚合本地训练的模型权重来构建全局模型，这固有地要求所有站点采用同质模型架构。这种刚性的同质化要求迫使站点放弃针对其计算基础设施和应用特定需求量身定制的架构。因此，现有的用于MRI重建的联邦学习方法无法支持模型异构设置，其中单个站点被允许使用不同的架构。为了克服这一基本限制，这里我们引入了FedGAT，这是一种基于生成自回归变压器的新型模型无关联邦学习技术。FedGAT去中心化全局生成先验的训练，该先验捕捉多站点MR图像的分布。为了提高保真度，我们提出了一种新型站点提示GAT先验，该先验可通过空间尺度的自回归预测，可控地合成来自所需站点的MR图像。然后，每个站点在其混合数据集（由本地MRI数据集和GAT生成的合成MRI数据集组成）上，使用其首选架构训练其特定的重建模型。在多机构数据集上的综合实验表明，FedGAT支持灵活的协作，同时在站内和跨站重建性能上优于最新的联邦学习基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04521v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于学习的模型在MRI重建中具有巨大潜力，但单一站点模型在有限的本地数据集上训练常常面临泛化性能不足的问题。为应对这一挑战，研究人员开始尝试使用联邦学习（FL）进行多站点数据集上的协同模型训练。然而，传统的联邦学习通过聚合本地训练的模型权重来建立全局模型，这要求所有站点必须使用相同架构，限制了站点的灵活性。本研究提出了一种新型的联邦学习技术——FedGAT，它基于生成式自回归转换器，打破了这一局限。FedGAT实现了生成式先验知识的去中心化训练，可以捕捉多站点MRI图像分布的特点。为提高图像质量，本研究还提出了一种新型的可控合成MRI图像的位点提示GAT先验技术。每个站点可以在本地数据集和GAT生成的合成MRI数据集组成的混合数据集上，使用其偏好的架构训练重建模型。在跨机构数据集上的综合实验表明，FedGAT在支持灵活协作的同时，其重建性能优于当前最先进的联邦学习基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>学习型模型在MRI重建中有巨大潜力，但单一站点模型的泛化能力受限。</li>
<li>联邦学习（FL）通过多站点数据集协同模型训练解决此问题。</li>
<li>传统联邦学习方法要求所有站点使用相同模型架构，限制了灵活性。</li>
<li>FedGAT是一种新型的联邦学习技术，基于生成式自回归转换器，支持模型异构设置。</li>
<li>FedGAT实现了生成式先验知识的去中心化训练，捕捉多站点MRI图像分布特点。</li>
<li>提出了一种新型的可控合成MRI图像的位点提示GAT先验技术，提高图像质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04521">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4507c80ec7dbc4331be781dc2746e3ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44f6f553538f63fee3c9c579c2dad6de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3662300e71e46f53c78f044f9b9fbba.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hybrid-Deep-Learning-Framework-for-Classification-of-Kidney-CT-Images-Diagnosis-of-Stones-Cysts-and-Tumors"><a href="#Hybrid-Deep-Learning-Framework-for-Classification-of-Kidney-CT-Images-Diagnosis-of-Stones-Cysts-and-Tumors" class="headerlink" title="Hybrid Deep Learning Framework for Classification of Kidney CT Images:   Diagnosis of Stones, Cysts, and Tumors"></a>Hybrid Deep Learning Framework for Classification of Kidney CT Images:   Diagnosis of Stones, Cysts, and Tumors</h2><p><strong>Authors:Kiran Sharma, Ziya Uddin, Adarsh Wadal, Dhruv Gupta</strong></p>
<p>Medical image classification is a vital research area that utilizes advanced computational techniques to improve disease diagnosis and treatment planning. Deep learning models, especially Convolutional Neural Networks (CNNs), have transformed this field by providing automated and precise analysis of complex medical images. This study introduces a hybrid deep learning model that integrates a pre-trained ResNet101 with a custom CNN to classify kidney CT images into four categories: normal, stone, cyst, and tumor. The proposed model leverages feature fusion to enhance classification accuracy, achieving 99.73% training accuracy and 100% testing accuracy. Using a dataset of 12,446 CT images and advanced feature mapping techniques, the hybrid CNN model outperforms standalone ResNet101. This architecture delivers a robust and efficient solution for automated kidney disease diagnosis, providing improved precision, recall, and reduced testing time, making it highly suitable for clinical applications. </p>
<blockquote>
<p>医学图像分类是一个重要的研究领域，它利用先进的计算技术来改善疾病诊断和治疗计划的制定。深度学习模型，尤其是卷积神经网络（CNN），已经通过提供复杂医学图像的自动化和精确分析，使该领域发生了变革。本研究介绍了一种混合深度学习模型，它将预训练的ResNet101与自定义CNN相结合，将肾脏CT图像分类为正常、结石、囊肿和肿瘤四类。所提出的模型利用特征融合来提高分类精度，实现99.73%的训练精度和100%的测试精度。使用包含12446张CT图像的数据库和先进的特征映射技术，混合CNN模型的表现优于单独的ResNet101。该架构为自动化肾脏疾病诊断提供了稳健高效的解决方案，提高了精确度、召回率，并缩短了测试时间，非常适合临床应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04367v1">PDF</a> </p>
<p><strong>Summary</strong><br>     医学图像分类是运用先进计算技术促进疾病诊断和治疗计划的重要研究领域。本研究提出一种混合深度学习模型，结合预训练的ResNet101和自定义CNN，对肾脏CT图像进行正常、结石、囊肿和肿瘤四类分类。该模型利用特征融合提高分类精度，实现99.73%的训练精度和100%的测试精度。混合CNN模型优于单独的ResNet101，为肾脏疾病的自动诊断提供了精确、高效、快速的解决方案，非常适合临床应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分类在疾病诊断和治疗计划中起重要作用。</li>
<li>深度学习模型，尤其是卷积神经网络（CNNs），已经彻底改变了医学图像分析领域。</li>
<li>本研究提出了一种混合深度学习模型，结合了ResNet101和自定义CNN进行肾脏CT图像分类。</li>
<li>该模型通过特征融合提高了分类精度。</li>
<li>混合模型的训练精度达到99.73%，测试精度达到100%。</li>
<li>混合CNN模型优于单独的ResNet101模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04367">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c72562ecc0503c0c6a8ca3d333989ee7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-706d2455639943bf6aff5c9106f27e4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87892531c2c7283c3d6fba0441f6c45e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41c9ab818dd1c1e9027abe02b39211c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f48f5c6ef740fa9fe816c30460b7f60.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Multi-Scale-Feature-Fusion-Framework-Integrating-Frequency-Domain-and-Cross-View-Attention-for-Dual-View-X-ray-Security-Inspections"><a href="#A-Multi-Scale-Feature-Fusion-Framework-Integrating-Frequency-Domain-and-Cross-View-Attention-for-Dual-View-X-ray-Security-Inspections" class="headerlink" title="A Multi-Scale Feature Fusion Framework Integrating Frequency Domain and   Cross-View Attention for Dual-View X-ray Security Inspections"></a>A Multi-Scale Feature Fusion Framework Integrating Frequency Domain and   Cross-View Attention for Dual-View X-ray Security Inspections</h2><p><strong>Authors:Shilong Hong, Yanzhou Zhou, Weichao Xu</strong></p>
<p>With the rapid development of modern transportation systems and the exponential growth of logistics volumes, intelligent X-ray-based security inspection systems play a crucial role in public safety. Although single-view X-ray equipment is widely deployed, it struggles to accurately identify contraband in complex stacking scenarios due to strong viewpoint dependency and inadequate feature representation. To address this, we propose an innovative multi-scale interactive feature fusion framework tailored for dual-view X-ray security inspection image classification. The framework comprises three core modules: the Frequency Domain Interaction Module (FDIM) enhances frequency-domain features through Fourier transform; the Multi-Scale Cross-View Feature Enhancement (MSCFE) leverages cross-view attention mechanisms to strengthen feature interactions; and the Convolutional Attention Fusion Module (CAFM) efficiently fuses features by integrating channel attention with depthwise-separable convolutions. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches across multiple backbone architectures, particularly excelling in complex scenarios with occlusions and object stacking. </p>
<blockquote>
<p>随着现代交通运输系统的快速发展和物流量的指数级增长，基于智能X射线的安全检查系统在公共安全中扮演着至关重要的角色。虽然单视图X射线设备已广泛部署，但由于其强烈的视角依赖性和特征表示的不足，它在复杂的堆叠场景中难以准确识别违禁品。为了解决这一问题，我们提出了一种针对双视图X射线安全检查图像分类的多尺度交互特征融合框架。该框架包括三个核心模块：频域交互模块（FDIM）通过傅里叶变换增强频域特征；多尺度跨视图特征增强（MSCFE）利用跨视图注意力机制加强特征交互；卷积注意力融合模块（CAFM）通过结合通道注意力和深度可分离卷积来有效地融合特征。实验结果表明，我们的方法在多种主干架构上超越了现有的最先进的方法，特别是在存在遮挡和物体堆叠的复杂场景中表现尤为出色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01710v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着现代交通运输系统的快速发展和物流量的指数级增长，智能X光安全检测系统在公共安全中发挥着至关重要的作用。针对单视角X光设备在复杂堆叠场景中难以准确识别违禁品的问题，提出了一种多尺度交互式特征融合框架，用于双视角X光安全检测图像分类。该框架包括三个核心模块：频率域交互模块（FDIM）通过傅立叶变换增强频率域特征；多尺度跨视图特征增强（MSCFE）利用跨视图注意力机制加强特征交互；卷积注意力融合模块（CAFM）通过结合通道注意力和深度可分离卷积来有效地融合特征。实验结果表明，该方法在多种主干架构上优于现有最先进的方法，尤其在存在遮挡和物体堆叠的复杂场景中表现卓越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>智能X光安全检测系统在公共安全中扮演重要角色，特别是面对日益增长的物流量和复杂场景的挑战。</li>
<li>单视角X光设备在复杂堆叠场景中识别违禁品存在困难，需要更先进的图像分类技术。</li>
<li>提出的多尺度交互式特征融合框架包含三个核心模块，分别增强频率域特征、利用跨视图注意力机制和加强特征交互、以及有效地融合特征。</li>
<li>框架通过傅立叶变换增强频率域特征，通过跨视图注意力机制处理不同视角下的特征，并结合通道注意力和深度可分离卷积来融合特征。</li>
<li>实验结果表明，该框架在多种主干架构上的表现优于现有方法，特别是在复杂场景中。</li>
<li>该框架特别擅长处理存在遮挡和物体堆叠的情况，这是现有方法的一个重大改进。</li>
<li>框架的优异性能使其成为X光安全检测领域的一个有前途的研究方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01710">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f8e861630402e46c07d18095c2f01edb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6200b9aabddab4bd72b77c12a1b4b43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1ec2a763901d523dba70ae66a41be91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a96735b420779f155f9b37050f06327.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5593da8868927c82e7fb7429a2276bd6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning"><a href="#Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning" class="headerlink" title="Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning"></a>Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning</h2><p><strong>Authors:Jun-En Ding, Chien-Chin Hsu, Chi-Hsiang Chu, Shuqiang Wang, Feng Liu</strong></p>
<p>The classification of medical images is a pivotal aspect of disease diagnosis, often enhanced by deep learning techniques. However, traditional approaches typically focus on unimodal medical image data, neglecting the integration of diverse non-image patient data. This paper proposes a novel Cross-Graph Modal Contrastive Learning (CGMCL) framework for multimodal structured data from different data domains to improve medical image classification. The model effectively integrates both image and non-image data by constructing cross-modality graphs and leveraging contrastive learning to align multimodal features in a shared latent space. An inter-modality feature scaling module further optimizes the representation learning process by reducing the gap between heterogeneous modalities. The proposed approach is evaluated on two datasets: a Parkinson’s disease (PD) dataset and a public melanoma dataset. Results demonstrate that CGMCL outperforms conventional unimodal methods in accuracy, interpretability, and early disease prediction. Additionally, the method shows superior performance in multi-class melanoma classification. The CGMCL framework provides valuable insights into medical image classification while offering improved disease interpretability and predictive capabilities. </p>
<blockquote>
<p>医学图像分类是疾病诊断的重要方面，通常可以通过深度学习技术得到增强。然而，传统方法主要关注单模态医学图像数据，忽略了不同非图像患者数据的整合。本文提出了一种新颖的跨图模态对比学习（CGMCL）框架，用于不同数据域的多模态结构化数据，以改进医学图像分类。该模型通过构建跨模态图并利用对比学习来对齐多模态特征在共享潜在空间，从而有效地整合图像和非图像数据。跨模态特征缩放模块进一步优化了表示学习过程，缩小了不同模态之间的差距。所提出的方法在两个数据集上进行了评估：帕金森病（PD）数据集和公共黑色素瘤数据集。结果表明，在准确性、可解释性和早期疾病预测方面，CGMCL优于传统单模态方法。此外，该方法在多类黑色素瘤分类方面表现出卓越性能。CGMCL框架为医学图像分类提供了有价值的见解，同时提高了疾病可解释性和预测能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17494v3">PDF</a> </p>
<p><strong>Summary</strong><br>     论文提出一种基于跨图模态对比学习（CGMCL）的多模态数据融合框架，用于医学图像分类。该框架能有效整合图像和非图像数据，通过构建跨模态图并利用对比学习对齐多模态特征，优化表示学习过程，缩小不同模态之间的差距。在帕金森病和公共黑色素瘤数据集上的实验结果表明，该框架在准确性、可解释性和早期疾病预测方面优于传统单模态方法，为多类黑色素瘤分类提供了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分类是疾病诊断的关键环节，通常可通过深度学习技术加强。</li>
<li>传统方法主要关注单模态医学图像数据，忽略了非图像患者数据的整合。</li>
<li>论文提出了一种基于跨图模态对比学习（CGMCL）的多模态数据融合框架。</li>
<li>CGMCL框架通过构建跨模态图，利用对比学习整合图像和非图像数据。</li>
<li>跨模态特征缩放模块缩小了不同模态之间的差距，优化了表示学习过程。</li>
<li>在帕金森病和黑色素瘤数据集上的实验验证了CGMCL框架的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17494">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1d9dbcde773730552ed74f85416b1857.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ac1d5ef9dd19ac3b7c89ecc314ff390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c3afe74232f885688a350ce87122b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6986fc293845fd61bc9ba737bf16d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f99f055faccc7f1c93c993d5ac326652.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Vec2Face-Scaling-Face-Dataset-Generation-with-Loosely-Constrained-Vectors"><a href="#Vec2Face-Scaling-Face-Dataset-Generation-with-Loosely-Constrained-Vectors" class="headerlink" title="Vec2Face: Scaling Face Dataset Generation with Loosely Constrained   Vectors"></a>Vec2Face: Scaling Face Dataset Generation with Loosely Constrained   Vectors</h2><p><strong>Authors:Haiyu Wu, Jaskirat Singh, Sicong Tian, Liang Zheng, Kevin W. Bowyer</strong></p>
<p>This paper studies how to synthesize face images of non-existent persons, to create a dataset that allows effective training of face recognition (FR) models. Besides generating realistic face images, two other important goals are: 1) the ability to generate a large number of distinct identities (inter-class separation), and 2) a proper variation in appearance of the images for each identity (intra-class variation). However, existing works 1) are typically limited in how many well-separated identities can be generated and 2) either neglect or use an external model for attribute augmentation. We propose Vec2Face, a holistic model that uses only a sampled vector as input and can flexibly generate and control the identity of face images and their attributes. Composed of a feature masked autoencoder and an image decoder, Vec2Face is supervised by face image reconstruction and can be conveniently used in inference. Using vectors with low similarity among themselves as inputs, Vec2Face generates well-separated identities. Randomly perturbing an input identity vector within a small range allows Vec2Face to generate faces of the same identity with proper variation in face attributes. It is also possible to generate images with designated attributes by adjusting vector values with a gradient descent method. Vec2Face has efficiently synthesized as many as 300K identities, whereas 60K is the largest number of identities created in the previous works. As for performance, FR models trained with the generated HSFace datasets, from 10k to 300k identities, achieve state-of-the-art accuracy, from 92% to 93.52%, on five real-world test sets (\emph{i.e.}, LFW, CFP-FP, AgeDB-30, CALFW, and CPLFW). For the first time, the FR model trained using our synthetic training set achieves higher accuracy than that trained using a same-scale training set of real face images on the CALFW, IJBB, and IJBC test sets. </p>
<blockquote>
<p>本文研究了如何合成不存在的人脸图像，以创建一个允许有效训练人脸识别（FR）模型的数据集。除了生成逼真的人脸图像外，另外两个重要目标是：1）能够生成大量不同的身份（类间分离），2）为每个身份的外观提供适当的变体（类内变化）。然而，现有工作1）通常可以生成的分离良好的身份数量有限，2）要么忽视要么使用外部模型进行属性增强。我们提出了Vec2Face，这是一个全面的模型，它只使用采样向量作为输入，可以灵活地生成和控制人脸图像的身份和属性。Vec2Face由特征掩码自动编码器和图像解码器组成，受到人脸图像重建的监督，可以方便地进行推理。使用彼此相似性低的向量作为输入，Vec2Face生成了分离良好的身份。在输入身份向量的小范围内进行随机扰动，允许Vec2Face生成具有适当变化面部属性的人脸图像。通过梯度下降法调整向量值，还可以生成具有指定属性的图像。Vec2Face已经有效地合成了多达30万个身份，而以前的工作最多只能创建6万个身份。在性能方面，使用生成的HSFace数据集训练的FR模型，从10k到30万身份，在五个真实世界测试集上达到了最先进的准确性，从92%到93.5%，即LFW、CFP-FP、AgeDB-30、CALFW和CPLFW测试集。首次尝试的是，使用我们合成训练集训练的FR模型在CALFW、IJBB和IJBC测试集上的准确性高于使用相同规模的现实人脸图像训练集训练的模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02979v4">PDF</a> Accepted at ICLR2025</p>
<p><strong>Summary</strong><br>该论文旨在合成非真实人物的脸部图像，创建一个用于训练脸部识别模型的优质数据集。论文的目标包括生成逼真的脸部图像、生成大量不同的身份以及为每个身份提供适当的外观变化。提出了一种全新的模型Vec2Face，只需通过输入采样向量，就能灵活生成并控制脸部图像的身份及其属性。Vec2Face包括特征掩码自编码器和图像解码器，通过脸部图像重建进行监督，便于推理使用。Vec2Face能够生成具有良好分离度的身份，并使用随机扰动输入身份向量在较小的范围内生成具有适当属性变化的脸部图像。此外，通过调整向量值使用梯度下降法，可以生成具有指定属性的图像。Vec2Face已成功合成多达30万个身份，而之前的工作最多只能创建6万个身份。使用生成的HSFace数据集训练的面部识别模型在五个真实世界测试集上达到了最先进的准确性，在某些测试集上的准确性甚至超过了使用相同规模的真实面部图像训练集训练的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文研究合成非真实人物脸部图像以创建数据集，用于训练脸部识别模型。</li>
<li>论文旨在生成大量不同的身份，同时保持良好的分离度，并为每个身份提供适当的外观变化。</li>
<li>论文提出了一种名为Vec2Face的模型，该模型使用采样向量作为输入，可以灵活生成和控制脸部图像的属性和身份。</li>
<li>Vec2Face包括特征掩码自编码器和图像解码器，通过脸部图像重建进行监督。</li>
<li>通过随机扰动输入身份向量，Vec2Face可以生成具有适当属性变化的同一身份的脸部图像。</li>
<li>Vec2Face能够生成具有指定属性的图像，这有助于在训练中引入更多变化和提高模型的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7bd8ca65382486e44524a7344fb2572c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8bf6d71eba88ca7dffe1171602f16acf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b67ee4e3b0155055789b5859155e61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16440a532ec98c1bec06dcd53ad12454.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CT-AGRG-Automated-Abnormality-Guided-Report-Generation-from-3D-Chest-CT-Volumes"><a href="#CT-AGRG-Automated-Abnormality-Guided-Report-Generation-from-3D-Chest-CT-Volumes" class="headerlink" title="CT-AGRG: Automated Abnormality-Guided Report Generation from 3D Chest CT   Volumes"></a>CT-AGRG: Automated Abnormality-Guided Report Generation from 3D Chest CT   Volumes</h2><p><strong>Authors:Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel</strong></p>
<p>The rapid increase of computed tomography (CT) scans and their time-consuming manual analysis have created an urgent need for robust automated analysis techniques in clinical settings. These aim to assist radiologists and help them managing their growing workload. Existing methods typically generate entire reports directly from 3D CT images, without explicitly focusing on observed abnormalities. This unguided approach often results in repetitive content or incomplete reports, failing to prioritize anomaly-specific descriptions. We propose a new anomaly-guided report generation model, which first predicts abnormalities and then generates targeted descriptions for each. Evaluation on a public dataset demonstrates significant improvements in report quality and clinical relevance. We extend our work by conducting an ablation study to demonstrate its effectiveness. </p>
<blockquote>
<p>计算机断层扫描（CT）扫描的迅速增加及其耗时的手动分析，为临床环境中稳健的自动化分析技术创造了迫切的需求。这些技术的目标是协助放射科医生，并帮助他们应对日益增长的工作量。现有方法通常直接从3D CT图像生成整个报告，而没有明确关注观察到的异常。这种无导向的方法常常导致内容重复或报告不完整，无法优先提供异常特定的描述。我们提出了一种新的异常导向的报告生成模型，该模型首先预测异常，然后为每一个生成有针对性的描述。在公共数据集上的评估显示，该模型在报告质量和临床相关性方面有明显的改进。我们通过进行消融研究来进一步证明其有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11965v6">PDF</a> Paper accepted to ISBI 2025</p>
<p><strong>Summary</strong></p>
<p>该文介绍了计算层析扫描（CT）扫描的快速增长及其耗时的人工分析所带来的问题，凸显了对临床环境中稳健自动化分析技术的迫切需求。现有方法通常直接从3D CT图像生成整个报告，没有特别强调观察到的异常。本文提出了一种新的异常引导报告生成模型，该模型首先预测异常，然后针对每个异常生成有针对性的描述。在公共数据集上的评估证明了该模型在提高报告质量和临床相关性方面的显著优势。通过进行消融研究进一步验证了其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CT扫描的广泛应用及其耗时的人工分析凸显了对自动化分析技术的需求。</li>
<li>现有方法直接从3D CT图像生成报告，缺乏明确的异常识别。</li>
<li>新的异常引导报告生成模型先预测异常，再针对每个异常生成描述。</li>
<li>提出的模型在公共数据集上的评估显示，报告质量和临床相关性显著提高。</li>
<li>模型通过消融研究验证了其有效性。</li>
<li>该模型有助于减轻放射科医生的工作负担。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e72dcab730cae53328f22d48c0655b89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac79443920f63d036961d0395423cd7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31ad4d276bd8edd9a8bb7b6674aebf91.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="multiGradICON-A-Foundation-Model-for-Multimodal-Medical-Image-Registration"><a href="#multiGradICON-A-Foundation-Model-for-Multimodal-Medical-Image-Registration" class="headerlink" title="multiGradICON: A Foundation Model for Multimodal Medical Image   Registration"></a>multiGradICON: A Foundation Model for Multimodal Medical Image   Registration</h2><p><strong>Authors:Basar Demir, Lin Tian, Thomas Hastings Greer, Roland Kwitt, Francois-Xavier Vialard, Raul San Jose Estepar, Sylvain Bouix, Richard Jarrett Rushmore, Ebrahim Ebrahim, Marc Niethammer</strong></p>
<p>Modern medical image registration approaches predict deformations using deep networks. These approaches achieve state-of-the-art (SOTA) registration accuracy and are generally fast. However, deep learning (DL) approaches are, in contrast to conventional non-deep-learning-based approaches, anatomy-specific. Recently, a universal deep registration approach, uniGradICON, has been proposed. However, uniGradICON focuses on monomodal image registration. In this work, we therefore develop multiGradICON as a first step towards universal <em>multimodal</em> medical image registration. Specifically, we show that 1) we can train a DL registration model that is suitable for monomodal <em>and</em> multimodal registration; 2) loss function randomization can increase multimodal registration accuracy; and 3) training a model with multimodal data helps multimodal generalization. Our code and the multiGradICON model are available at <a target="_blank" rel="noopener" href="https://github.com/uncbiag/uniGradICON">https://github.com/uncbiag/uniGradICON</a>. </p>
<blockquote>
<p>现代医学图像配准方法利用深度网络来预测变形。这些方法达到了最先进的配准精度，并且通常速度很快。然而，与传统的非深度学习方法相比，深度学习（DL）方法具有针对特定解剖结构的特点。最近，已经提出了一种通用的深度配准方法uniGradICON。然而，uniGradICON专注于单模态图像配准。因此，在这项工作中，我们开发了multiGradICON，作为通用多模态医学图像配准的第一步。具体来说，我们证明了以下几点：1）我们可以训练一个既适用于单模态又适用于多模态配准的深度学习配准模型；2）损失函数随机化可以提高多模态配准的精度；3）使用多模态数据进行模型训练有助于多模态泛化。我们的代码和multiGradICON模型可在<a target="_blank" rel="noopener" href="https://github.com/uncbiag/uniGradICON">https://github.com/uncbiag/uniGradICON</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.00221v2">PDF</a> </p>
<p><strong>Summary</strong><br>现代医疗图像配准方法使用深度网络预测变形，达到最新配准精度且通常快速。然而，深度学习方法与传统非深度学习方法相反，具有特定解剖结构的特点。最近提出了通用深度配准方法uniGradICON，但主要关注单模态图像配准。因此，我们开发了multiGradICON作为向通用多模态医学图像配准迈出的第一步。本工作证明了：1）我们可以训练一个适用于单模态和多模态配准的深度学习模型；2）损失函数随机化可以提高多模态配准精度；3）使用多模态数据进行模型训练有助于多模态泛化。我们的代码和multiGradICON模型可在以下网址获取：[链接地址]。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>现代医疗图像配准方法使用深度网络提高配准精度和速度。</li>
<li>深度学习方法与传统方法不同，具有特定解剖结构的特点。</li>
<li>uniGradICON方法主要关注单模态图像配准。</li>
<li>multiGradICON作为通用多模态医学图像配准的第一步被开发出来。</li>
<li>本研究证明可以训练一个适用于单模态和多模态配准的深度学习模型。</li>
<li>损失函数随机化能提高多模态配准精度。</li>
<li>使用多模态数据进行模型训练有助于多模态泛化。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.00221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d1681798caece933c0cceb000845848f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80d477c5acc001e4b972fafa3df9fdcd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e57e82a683c2385c1e782b96371c85d9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leveraging-Bi-Focal-Perspectives-and-Granular-Feature-Integration-for-Accurate-Reliable-Early-Alzheimer’s-Detection"><a href="#Leveraging-Bi-Focal-Perspectives-and-Granular-Feature-Integration-for-Accurate-Reliable-Early-Alzheimer’s-Detection" class="headerlink" title="Leveraging Bi-Focal Perspectives and Granular Feature Integration for   Accurate Reliable Early Alzheimer’s Detection"></a>Leveraging Bi-Focal Perspectives and Granular Feature Integration for   Accurate Reliable Early Alzheimer’s Detection</h2><p><strong>Authors:Pandiyaraju V, Shravan Venkatraman, Abeshek A, Pavan Kumar S, Aravintakshan S A</strong></p>
<p>Being the most commonly known neurodegeneration, Alzheimer’s Disease (AD) is annually diagnosed in millions of patients. The present medical scenario still finds the exact diagnosis and classification of AD through neuroimaging data as a challenging task. Traditional CNNs can extract a good amount of low-level information in an image while failing to extract high-level minuscule particles, which is a significant challenge in detecting AD from MRI scans. To overcome this, we propose a novel Granular Feature Integration method to combine information extraction at different scales along with an efficient information flow, enabling the model to capture both broad and fine-grained features simultaneously. We also propose a Bi-Focal Perspective mechanism to highlight the subtle neurofibrillary tangles and amyloid plaques in the MRI scans, ensuring that critical pathological markers are accurately identified. Our model achieved an F1-Score of 99.31%, precision of 99.24%, and recall of 99.51%. These scores prove that our model is significantly better than the state-of-the-art (SOTA) CNNs in existence. </p>
<blockquote>
<p>阿尔茨海默病（AD）是最常见的神经退行性疾病之一，每年有数百万患者被诊断出患有此病。当前医学界通过神经成像数据对阿尔茨海默病进行确切诊断和分类仍然是一项具有挑战性的任务。传统卷积神经网络（CNN）可以在图像中提取大量的低级信息，但无法提取高级微小颗粒，这对于从MRI扫描中检测阿尔茨海默病来说是一个巨大的挑战。为了克服这一问题，我们提出了一种新型的粒度特征集成方法，该方法能够结合不同尺度的信息提取和高效的信息流，使模型能够同时捕获广泛和精细的特征。我们还提出了一种双焦点透视机制，以突出MRI扫描中的神经原纤维缠结和淀粉样斑块等细微之处，确保准确识别关键病理标记物。我们的模型取得了F1分数为99.31%，精确度为99.24%，召回率为99.51%的成绩。这些分数证明我们的模型明显优于目前存在的最先进的CNN模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10921v6">PDF</a> 14 pages, 12 figures, 6 tables</p>
<p><strong>Summary</strong><br>     阿尔茨海默病（AD）的神经影像诊断仍具挑战。传统CNN难以提取高级微小颗粒信息，新型Granular Feature Integration方法结合不同尺度信息提取，同时捕捉广泛与精细特征。Bi-Focal Perspective机制能准确识别神经纤维缠结和淀粉样斑块等关键病理标记物。模型实现F1分数99.31%，精度99.24%，召回率99.51%，显著优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>阿尔茨海默病（AD）的神经影像诊断仍是医学界的一大挑战。</li>
<li>传统CNN在提取高级微小颗粒信息方面存在困难。</li>
<li>新型Granular Feature Integration方法能够结合不同尺度的信息提取，提高诊断准确性。</li>
<li>Bi-Focal Perspective机制能突出显示MRI扫描中的细微神经纤维缠结和淀粉样斑块。</li>
<li>该模型的F1分数达到99.31%，精度和召回率均超过99%，表现优异。</li>
<li>模型显著优于现有技术（SOTA）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.10921">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6cb755f14702c130691fe5d8eef7beab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39fef78c92f9c9e47b43c79af0261ab2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2de55f6b50cc4f1be34dddf67099914.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf218884d9171b85d27f244a06bc1cae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d85f5bf773ab563b2a84f3e0fd4f97e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ac2f6f0097e62acdffb0e056f8eb7b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a4cff75da89c44c3abe8e54c97f1bcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b1f2a66b77381eb475389ff8beb5e86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4be07e5eb47169df5eea32645d6c9dde.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Exploring-scalable-medical-image-encoders-beyond-text-supervision"><a href="#Exploring-scalable-medical-image-encoders-beyond-text-supervision" class="headerlink" title="Exploring scalable medical image encoders beyond text supervision"></a>Exploring scalable medical image encoders beyond text supervision</h2><p><strong>Authors:Fernando Pérez-García, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Matthew P. Lungren, Maria Teodora Wetscherek, Noel Codella, Stephanie L. Hyland, Javier Alvarez-Valle, Ozan Oktay</strong></p>
<p>Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains. However, the computed features are limited by the information contained in the text, which is particularly problematic in medical imaging, where the findings described by radiologists focus on specific observations. This challenge is compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information. In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general-purpose biomedical imaging encoders. We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language-supervised models on a diverse range of benchmarks. Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images). To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports. Finally, we conduct a series of ablations determining the factors in RAD-DINO’s performance; notably, we observe that RAD-DINO’s downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder. Model weights of RAD-DINO trained on publicly available datasets are available at <a target="_blank" rel="noopener" href="https://huggingface.co/microsoft/rad-dino">https://huggingface.co/microsoft/rad-dino</a>. </p>
<blockquote>
<p>语言监督的预训练已被证明是从图像中提取语义上有意义的特征的一种有价值的方法，作为计算机视觉和医学成像领域中的多模态系统的基础元素。然而，计算出的特征受到文本中所包含信息的限制，这在医学成像中尤其成问题，因为放射科医生所描述的发现主要关注特定的观察结果。这一挑战还因配对成像-文本数据的稀缺而加剧，这主要是由于担心泄露个人健康信息。在这项工作中，我们从根本上质疑对语言监督的普遍依赖，以学习通用生物医学成像编码器。我们引入了RAD-DINO，这是一种仅在一维生物医学成像数据上进行预训练的生物医学图像编码器，它在多种基准测试上达到了或超越了最新生物医学语言监督模型的表现。具体来说，通过在标准成像任务（分类和语义分割）和视觉语言对齐任务（根据图像生成文本报告）上评估所学表示的质量。为了进一步证明语言监督的局限性，我们展示了RAD-DINO的特征与其他医疗记录（如性别或年龄）的相关性优于语言监督模型，这些通常不会在放射学报告中被提及。最后，我们进行了一系列确定RAD-DINO性能因素的实验；值得注意的是，我们观察到RAD-DINO的下游性能随着训练数据数量和多样性的增加而表现良好，这表明仅使用图像监督是一种可扩展的方法来训练基础生物医学图像编码器。RAD-DINO在公开数据集上训练的模型权重可在<a target="_blank" rel="noopener" href="https://huggingface.co/microsoft/rad-dino">https://huggingface.co/microsoft/rad-dino</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.10815v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了在医学图像领域，通过纯图像预训练方法（RAD-DINO）来提取图像特征，其性能可以达到甚至超越依赖于语言监督的预训练模型。RAD-DINO能够在标准成像任务（分类和语义分割）以及跨模态任务（从图像生成文本报告）中表现优秀。相较于语言监督模型，RAD-DINO的特征能更好地与其他医疗记录（如性别、年龄等）相关联。此外，RAD-DINO的训练性能可通过增加训练数据量和多样性来进一步提升，证明了纯图像监督是一种可扩展的医学图像基础编码器训练方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言监督预训练在医学图像特征提取中存在局限性，尤其是在注重特定观察的医学成像领域。</li>
<li>RAD-DINO是一个纯图像预训练的医学图像编码器，能在多种基准测试中达到或超越先进语言监督模型的表现。</li>
<li>RAD-DINO学到的表示质量在标准成像任务（分类和语义分割）上得到评估，并展示了其在跨模态任务（从图像生成文本报告）中的有效性。</li>
<li>与语言监督模型相比，RAD-DINO的特征能更好地与其他医疗记录（如性别、年龄）关联。</li>
<li>RAD-DINO的训练性能可通过增加训练数据量和多样性来提升，证明了纯图像监督的可扩展性。</li>
<li>RAD-DINO模型权重可在公开数据集上进行训练，并可在huggingface.co&#x2F;microsoft&#x2F;rad-dino获取。</li>
<li>该研究通过一系列消融实验确定了RAD-DINO性能的关键因素。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.10815">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e485deb1ecf6ab6442d5400cad884a0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c98567d71c2a82688f23ddb2ccd9eef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c33078168e75d8a43b69c51cb3c8fad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ba7cb63a246cd84ab24def442b4ddc8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3f2dbb7b9fc4ae0a1234f5ee88840144.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-02-12  ReasonFlux Hierarchical LLM Reasoning via Scaling Thought Templates
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-11/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-79770b626976880b93ff148925e8c9de.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-11  Hummingbird High Fidelity Image Generation via Multimodal Context   Alignment
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
