<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-11  Chest X-ray Foundation Model with Global and Local Representations   Integration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-41c9ab818dd1c1e9027abe02b39211c8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    53 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-11-æ›´æ–°"><a href="#2025-02-11-æ›´æ–°" class="headerlink" title="2025-02-11 æ›´æ–°"></a>2025-02-11 æ›´æ–°</h1><h2 id="Chest-X-ray-Foundation-Model-with-Global-and-Local-Representations-Integration"><a href="#Chest-X-ray-Foundation-Model-with-Global-and-Local-Representations-Integration" class="headerlink" title="Chest X-ray Foundation Model with Global and Local Representations   Integration"></a>Chest X-ray Foundation Model with Global and Local Representations   Integration</h2><p><strong>Authors:Zefan Yang, Xuanang Xu, Jiajin Zhang, Ge Wang, Mannudeep K. Kalra, Pingkun Yan</strong></p>
<p>Chest X-ray (CXR) is the most frequently ordered imaging test, supporting diverse clinical tasks from thoracic disease detection to postoperative monitoring. However, task-specific classification models are limited in scope, require costly labeled data, and lack generalizability to out-of-distribution datasets. To address these challenges, we introduce CheXFound, a self-supervised vision foundation model that learns robust CXR representations and generalizes effectively across a wide range of downstream tasks. We pretrain CheXFound on a curated CXR-1M dataset, comprising over one million unique CXRs from publicly available sources. We propose a Global and Local Representations Integration (GLoRI) module for downstream adaptations, by incorporating disease-specific local features with global image features for enhanced performance in multilabel classification. Our experimental results show that CheXFound outperforms state-of-the-art models in classifying 40 disease findings across different prevalence levels on the CXR-LT 24 dataset and exhibits superior label efficiency on downstream tasks with limited training data. Additionally, CheXFound achieved significant improvements on new tasks with out-of-distribution datasets, including opportunistic cardiovascular disease risk estimation and mortality prediction. These results highlight CheXFoundâ€™s strong generalization capabilities, enabling diverse adaptations with improved label efficiency. The project source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/RPIDIAL/CheXFound">https://github.com/RPIDIAL/CheXFound</a>. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰æ˜¯æœ€å¸¸è¿›è¡Œçš„å½±åƒæ£€æŸ¥ï¼Œæ”¯æŒä»èƒ¸éƒ¨ç–¾ç—…æ£€æµ‹åˆ°æœ¯åç›‘æŠ¤çš„å¤šç§ä¸´åºŠä»»åŠ¡ã€‚ç„¶è€Œï¼Œç‰¹å®šä»»åŠ¡çš„åˆ†ç±»æ¨¡å‹åœ¨èŒƒå›´ä¸Šæœ‰é™ï¼Œéœ€è¦æ˜‚è´µçš„æ ‡æ³¨æ•°æ®ï¼Œå¹¶ä¸”å¯¹è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ•°æ®é›†ç¼ºä¹é€šç”¨æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CheXFoundï¼Œè¿™æ˜¯ä¸€ç§è‡ªæˆ‘ç›‘ç£çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿå­¦ä¹ ç¨³å¥çš„CXRè¡¨ç¤ºï¼Œå¹¶åœ¨å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­æœ‰æ•ˆåœ°è¿›è¡Œæ¨å¹¿ã€‚æˆ‘ä»¬åœ¨å®šåˆ¶çš„CXR-1Mæ•°æ®é›†ä¸Šé¢„è®­ç»ƒCheXFoundï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªå…¬å¼€æ¥æºçš„ä¸€ç™¾ä¸‡å¤šä¸ªç‹¬ç‰¹çš„CXRsã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨å±€å’Œå±€éƒ¨è¡¨ç¤ºé›†æˆï¼ˆGLoRIï¼‰æ¨¡å—ï¼Œç”¨äºä¸‹æ¸¸é€‚åº”ï¼Œé€šè¿‡ç»“åˆç–¾ç—…ç‰¹å®šçš„å±€éƒ¨ç‰¹å¾ä¸å…¨å±€å›¾åƒç‰¹å¾ï¼Œä»¥æé«˜å¤šæ ‡ç­¾åˆ†ç±»çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨CXR-LT 24æ•°æ®é›†ä¸Šï¼ŒCheXFoundåœ¨åˆ†ç±»ä¸åŒå‘ç—…ç‡çš„40ç§ç–¾ç—…å‘ç°æ–¹é¢ä¼˜äºæœ€å…ˆè¿›æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„æ ‡ç­¾æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼ŒCheXFoundåœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼ŒåŒ…æ‹¬æœºä¼šæ€§å¿ƒè¡€ç®¡ç–¾ç—…é£é™©è¯„ä¼°å’Œæ­»äº¡ç‡é¢„æµ‹ç­‰è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ•°æ®é›†ã€‚è¿™äº›ç»“æœçªæ˜¾äº†CheXFoundå¼ºå¤§çš„é€šç”¨æ€§èƒ½åŠ›ï¼Œèƒ½å¤Ÿå®ç°å¤šæ ·åŒ–çš„é€‚åº”å¹¶å…·æœ‰æ”¹è¿›çš„æ ‡ç­¾æ•ˆç‡ã€‚é¡¹ç›®æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RPIDIAL/CheXFound%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/RPIDIAL/CheXFoundè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05142v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CheXFoundæ˜¯ä¸€ä¸ªåŸºäºè‡ªç›‘ç£å­¦ä¹ çš„é€šç”¨åŒ»å­¦å›¾åƒåŸºç¡€æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ ç¨³å¥çš„èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰è¡¨ç¤ºï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åœ¨CXR-1Mæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡ç»“åˆå…¨å±€å›¾åƒç‰¹å¾å’Œç–¾ç—…ç‰¹å®šçš„å±€éƒ¨ç‰¹å¾ï¼Œä½¿ç”¨ä¸€ä¸ªåä¸ºGlobal and Local Representations Integration (GLoRI)çš„æ¨¡å—è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡é€‚é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCheXFoundåœ¨CXR-LT 24æ•°æ®é›†ä¸Šåˆ†ç±»40ç§ç–¾ç—…çš„èƒ½åŠ›ä¼˜äºå½“å‰ä¸»æµæ¨¡å‹ï¼Œå¹¶åœ¨å…·æœ‰æœ‰é™è®­ç»ƒæ•°æ®çš„æ–°ä»»åŠ¡ä¸Šå±•ç°å‡ºå‡ºè‰²çš„æ ‡ç­¾æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒCheXFoundåœ¨æœºä¼šæ€§å¿ƒè¡€ç®¡ç–¾ç—…é£é™©è¯„ä¼°å’Œæ­»äº¡ç‡é¢„æµ‹ç­‰ç¦»åˆ†å¸ƒæ•°æ®é›†çš„æ–°ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œè¯æ˜äº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CheXFoundæ˜¯ä¸€ä¸ªè‡ªç›‘ç£çš„åŒ»å­¦å›¾åƒåŸºç¡€æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ ç¨³å¥çš„èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰è¡¨ç¤ºã€‚</li>
<li>CheXFoundåœ¨å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„CXR-1Mæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡å¼ ç‹¬ç‰¹çš„èƒ¸éƒ¨Xå°„çº¿å›¾åƒã€‚</li>
<li>å¼•å…¥Global and Local Representations Integration (GLoRI)æ¨¡å—ï¼Œç»“åˆå…¨å±€å›¾åƒç‰¹å¾å’Œç–¾ç—…ç‰¹å®šçš„å±€éƒ¨ç‰¹å¾ï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>åœ¨CXR-LT 24æ•°æ®é›†ä¸Šåˆ†ç±»40ç§ç–¾ç—…çš„èƒ½åŠ›ä¼˜äºå½“å‰ä¸»æµæ¨¡å‹ã€‚</li>
<li>åœ¨å…·æœ‰æœ‰é™è®­ç»ƒæ•°æ®çš„æ–°ä»»åŠ¡ä¸Šå±•ç°å‡ºå‡ºè‰²çš„æ ‡ç­¾æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5df4763452a11fcb53557bc1a1904cc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ff754f3d8dbfa7115277cc15286d996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf697bcaabd67610dca9dd15afd78a6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be23a84fbe01b9cbcabc9de64d316d49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-072bcdf7cca794c3e130c6082440f09f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-130c0c80891fb409e3b3ab9b4c6af6bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecfd8a2964f6547100925afbe14f8580.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Graph-Contrastive-Learning-for-Connectome-Classification"><a href="#Graph-Contrastive-Learning-for-Connectome-Classification" class="headerlink" title="Graph Contrastive Learning for Connectome Classification"></a>Graph Contrastive Learning for Connectome Classification</h2><p><strong>Authors:MartÃ­n Schmidt, Sara Silva, Federico Larroca, Gonzalo Mateos, Pablo MusÃ©</strong></p>
<p>With recent advancements in non-invasive techniques for measuring brain activity, such as magnetic resonance imaging (MRI), the study of structural and functional brain networks through graph signal processing (GSP) has gained notable prominence. GSP stands as a key tool in unraveling the interplay between the brainâ€™s function and structure, enabling the analysis of graphs defined by the connections between regions of interest â€“ referred to as connectomes in this context. Our work represents a further step in this direction by exploring supervised contrastive learning methods within the realm of graph representation learning. The main objective of this approach is to generate subject-level (i.e., graph-level) vector representations that bring together subjects sharing the same label while separating those with different labels. These connectome embeddings are derived from a graph neural network Encoder-Decoder architecture, which jointly considers structural and functional connectivity. By leveraging data augmentation techniques, the proposed framework achieves state-of-the-art performance in a gender classification task using Human Connectome Project data. More broadly, our connectome-centric methodological advances support the promising prospect of using GSP to discover more about brain function, with potential impact to understanding heterogeneity in the neurodegeneration for precision medicine and diagnosis. </p>
<blockquote>
<p>éšç€æ— åˆ›æŠ€æœ¯ï¼ˆå¦‚ç£å…±æŒ¯æˆåƒMRIï¼‰çš„æœ€æ–°å‘å±•ï¼Œé€šè¿‡å›¾å½¢ä¿¡å·å¤„ç†ï¼ˆGSPï¼‰ç ”ç©¶è„‘çš„ç»“æ„å’ŒåŠŸèƒ½ç½‘ç»œå·²å—åˆ°å¹¿æ³›å…³æ³¨ã€‚å›¾å½¢ä¿¡å·å¤„ç†æ˜¯æ­ç¤ºå¤§è„‘åŠŸèƒ½å’Œç»“æ„ä¹‹é—´ç›¸äº’ä½œç”¨çš„å…³é”®å·¥å…·ï¼Œèƒ½å¤Ÿåˆ†ææ„Ÿå…´è¶£åŒºåŸŸä¹‹é—´è¿æ¥æ‰€å®šä¹‰çš„å›¾å½¢ï¼Œæœ¬æ–‡ç§°ä¹‹ä¸ºè¿æ¥ç»„ã€‚æˆ‘ä»¬çš„å·¥ä½œæœç€è¿™ä¸ªæ–¹å‘è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œåœ¨å›¾å½¢è¡¨ç¤ºå­¦ä¹ çš„èŒƒå›´å†…æ¢ç´¢äº†ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•çš„ä¸»è¦ç›®æ ‡æ˜¯ç”Ÿæˆä¸»ä½“çº§ï¼ˆå³å›¾çº§ï¼‰çš„å‘é‡è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºèƒ½å¤Ÿå°†å…·æœ‰ç›¸åŒæ ‡ç­¾çš„ä¸»ä½“èšé›†åœ¨ä¸€èµ·ï¼ŒåŒæ—¶å°†å…·æœ‰ä¸åŒæ ‡ç­¾çš„ä¸»ä½“åˆ†å¼€ã€‚è¿™äº›è¿æ¥ç»„åµŒå…¥æ˜¯ä»å›¾å½¢ç¥ç»ç½‘ç»œç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­æ´¾ç”Ÿå‡ºæ¥çš„ï¼ŒåŒæ—¶è€ƒè™‘äº†ç»“æ„å’ŒåŠŸèƒ½è¿æ¥ã€‚é€šè¿‡åˆ©ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨åˆ©ç”¨äººç±»è¿æ¥ç»„é¡¹ç›®æ•°æ®çš„æ€§åˆ«åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œæˆ‘ä»¬çš„ä»¥è¿æ¥ç»„ä¸ºä¸­å¿ƒçš„æ–¹æ³•è®ºè¿›æ­¥æ”¯æŒäº†ä½¿ç”¨å›¾å½¢ä¿¡å·å¤„ç†æ¥äº†è§£æ›´å¤šå…³äºå¤§è„‘åŠŸèƒ½çš„å¸Œæœ›ï¼Œè¿™å¯¹ç²¾ç¡®åŒ»å­¦å’Œè¯Šæ–­ä¸­ç¥ç»å˜å¼‚çš„å¼‚è´¨æ€§ç†è§£å…·æœ‰æ½œåœ¨å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05109v1">PDF</a> Submitted to EMBC â€˜25</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒç ”ç©¶é¢†åŸŸè¿‘æœŸå€ŸåŠ©éä¾µå…¥æŠ€æœ¯ï¼ˆå¦‚ç£å…±æŒ¯æˆåƒMRIï¼‰çš„è¿›æ­¥ï¼Œé€šè¿‡å›¾ä¿¡å·å¤„ç†ï¼ˆGSPï¼‰ç ”ç©¶è„‘ç»“æ„å’ŒåŠŸèƒ½ç½‘ç»œå˜å¾—å°¤ä¸ºçªå‡ºã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å›¾è¡¨ç¤ºå­¦ä¹ ä¸­çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œç”Ÿæˆä¸»ä½“çº§åˆ«çš„å‘é‡è¡¨ç¤ºï¼Œä½¿ç›¸åŒæ ‡ç­¾çš„ä¸»ä½“èšé›†ï¼Œä¸åŒæ ‡ç­¾çš„ä¸»ä½“åˆ†ç¦»ã€‚è¿™äº›è„‘è¿æ¥ç»„åµŒå…¥æ¥è‡ªå›¾ç¥ç»ç½‘ç»œEncoder-Decoderæ¶æ„ï¼Œè¯¥æ¶æ„è”åˆè€ƒè™‘ç»“æ„å’ŒåŠŸèƒ½è¿æ¥ã€‚åˆ©ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œè¯¥æ¡†æ¶å®ç°äº†äººç±»è¿æ¥ç»„é¡¹ç›®æ•°æ®ä¸­çš„æ€§åˆ«åˆ†ç±»ä»»åŠ¡çš„æœ€æ–°æ€§èƒ½ã€‚è¯¥ç ”ç©¶çš„è¿æ¥ç»„ä¸ºä¸­å¿ƒçš„æ–¹æ³•è®ºè¿›æ­¥æ”¯æŒäº†GSPåœ¨äº†è§£å¤§è„‘åŠŸèƒ½æ–¹é¢çš„å‰æ™¯ï¼Œå…·æœ‰å¯¹ç¥ç»å˜æ€§çš„å¼‚è´¨æ€§çš„æ·±å…¥äº†è§£çš„æ½œåŠ›ï¼Œæœ‰åŠ©äºç²¾å‡†åŒ»ç–—å’Œè¯Šæ–­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨éä¾µå…¥æŠ€æœ¯ï¼ˆå¦‚MRIï¼‰è¿›è¡Œè„‘ç»“æ„å’ŒåŠŸèƒ½ç½‘ç»œçš„ç ”ç©¶æˆä¸ºå½“å‰ç„¦ç‚¹ã€‚</li>
<li>å›¾ä¿¡å·å¤„ç†ï¼ˆGSPï¼‰åœ¨è§£æè„‘åŠŸèƒ½ç»“æ„äº¤äº’ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ç”¨äºç”Ÿæˆä¸»ä½“çº§åˆ«çš„å‘é‡è¡¨ç¤ºï¼Œå®ç°ç›¸åŒæ ‡ç­¾ä¸»ä½“çš„èšé›†å’Œä¸åŒæ ‡ç­¾ä¸»ä½“çš„åˆ†ç¦»ã€‚</li>
<li>è„‘è¿æ¥ç»„åµŒå…¥æ¥æºäºå›¾ç¥ç»ç½‘ç»œEncoder-Decoderæ¶æ„ï¼ŒåŒæ—¶è€ƒè™‘ç»“æ„å’ŒåŠŸèƒ½è¿æ¥ã€‚</li>
<li>åˆ©ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯å®ç°äº†æ€§åˆ«åˆ†ç±»ä»»åŠ¡çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æ–¹æ³•åœ¨è¿æ¥ç»„ä¸ºä¸­å¿ƒçš„å›¾ä¿¡å·å¤„ç†é¢†åŸŸçš„è¿›æ­¥å¯¹äºæ·±å…¥äº†è§£å¤§è„‘åŠŸèƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05109">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3e44c33b6cf15277c7c9e37b8a68565d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c27605f4336587ec4a8c249ca0b0d91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61d2060384662baa447142debc1b749f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86ff7f6592775a4e91922654d7324603.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Gaze-Guided-Robotic-Vascular-Ultrasound-Leveraging-Human-Intention-Estimation"><a href="#Gaze-Guided-Robotic-Vascular-Ultrasound-Leveraging-Human-Intention-Estimation" class="headerlink" title="Gaze-Guided Robotic Vascular Ultrasound Leveraging Human Intention   Estimation"></a>Gaze-Guided Robotic Vascular Ultrasound Leveraging Human Intention   Estimation</h2><p><strong>Authors:Yuan Bi, Yang Su, Nassir Navab, Zhongliang Jiang</strong></p>
<p>Medical ultrasound has been widely used to examine vascular structure in modern clinical practice. However, traditional ultrasound examination often faces challenges related to inter- and intra-operator variation. The robotic ultrasound system (RUSS) appears as a potential solution for such challenges because of its superiority in stability and reproducibility. Given the complex anatomy of human vasculature, multiple vessels often appear in ultrasound images, or a single vessel bifurcates into branches, complicating the examination process. To tackle this challenge, this work presents a gaze-guided RUSS for vascular applications. A gaze tracker captures the eye movements of the operator. The extracted gaze signal guides the RUSS to follow the correct vessel when it bifurcates. Additionally, a gaze-guided segmentation network is proposed to enhance segmentation robustness by exploiting gaze information. However, gaze signals are often noisy, requiring interpretation to accurately discern the operatorâ€™s true intentions. To this end, this study proposes a stabilization module to process raw gaze data. The inferred attention heatmap is utilized as a region proposal to aid segmentation and serve as a trigger signal when the operator needs to adjust the scanning target, such as when a bifurcation appears. To ensure appropriate contact between the probe and surface during scanning, an automatic ultrasound confidence-based orientation correction method is developed. In experiments, we demonstrated the efficiency of the proposed gaze-guided segmentation pipeline by comparing it with other methods. Besides, the performance of the proposed gaze-guided RUSS was also validated as a whole on a realistic arm phantom with an uneven surface. </p>
<blockquote>
<p>ç°ä»£åŒ»å­¦å®è·µä¸­ï¼ŒåŒ»ç”¨è¶…å£°å·²å¹¿æ³›åº”ç”¨äºè¡€ç®¡ç»“æ„çš„æ£€æŸ¥ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿè¶…å£°æ£€æŸ¥å¸¸å¸¸é¢ä¸´æ“ä½œè€…é—´å’Œæ“ä½œè€…å†…éƒ¨çš„æŒ‘æˆ˜ã€‚ç”±äºå…¶åœ¨ç¨³å®šæ€§å’Œå¯é‡å¤æ€§æ–¹é¢çš„ä¼˜åŠ¿ï¼Œæœºå™¨äººè¶…å£°ç³»ç»Ÿï¼ˆRUSSï¼‰çš„å‡ºç°ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚è€ƒè™‘åˆ°äººç±»è¡€ç®¡çš„å¤æ‚ç»“æ„ï¼Œè¶…å£°å›¾åƒä¸­ç»å¸¸ä¼šå‡ºç°å¤šæ¡è¡€ç®¡ï¼Œæˆ–è€…å•ä¸ªè¡€ç®¡åˆ†å‰æˆå¤šä¸ªåˆ†æ”¯ï¼Œä½¿æ£€æŸ¥è¿‡ç¨‹å¤æ‚åŒ–ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè¡€ç®¡åº”ç”¨çš„ç›®å…‰å¼•å¯¼å‹RUSSã€‚ç›®å…‰è¿½è¸ªå™¨æ•æ‰æ“ä½œå‘˜çš„çœ¼çƒè¿åŠ¨ã€‚æå–çš„ç›®å…‰ä¿¡å·å¼•å¯¼RUSSåœ¨è¡€ç®¡åˆ†å‰æ—¶è¿½è¸ªæ­£ç¡®çš„è¡€ç®¡ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç›®å…‰å¼•å¯¼åˆ†å‰²ç½‘ç»œï¼Œåˆ©ç”¨ç›®å…‰ä¿¡æ¯æé«˜åˆ†å‰²çš„ç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œç›®å…‰ä¿¡å·å¾€å¾€å­˜åœ¨å™ªå£°ï¼Œéœ€è¦è§£è¯»ä»¥å‡†ç¡®åˆ¤æ–­æ“ä½œå‘˜çš„çœŸæ­£æ„å›¾ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç¨³å®šæ¨¡å—æ¥å¤„ç†åŸå§‹çš„ç›®å…‰æ•°æ®ã€‚æ¨æ–­å‡ºçš„æ³¨æ„åŠ›çƒ­å›¾è¢«ç”¨ä½œåŒºåŸŸææ¡ˆï¼Œä»¥è¾…åŠ©åˆ†å‰²ï¼Œå¹¶åœ¨æ“ä½œå‘˜éœ€è¦è°ƒæ•´æ‰«æç›®æ ‡ï¼ˆä¾‹å¦‚å‡ºç°åˆ†å‰æ—¶ï¼‰æ—¶ä½œä¸ºè§¦å‘ä¿¡å·ã€‚ä¸ºç¡®ä¿æ‰«æè¿‡ç¨‹ä¸­æ¢å¤´ä¸è¡¨é¢ä¹‹é—´çš„é€‚å½“æ¥è§¦ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºè‡ªåŠ¨è¶…å£°ç½®ä¿¡åº¦çš„æ–¹å‘æ ¡æ­£æ–¹æ³•ã€‚é€šè¿‡ä¸å…¶ä»–æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬éªŒè¯äº†æ‰€æå‡ºçš„ç›®å…‰å¼•å¯¼åˆ†å‰²æµç¨‹çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œåœ¨å®é™…çš„æ‰‹è‡‚æ¨¡å‹ï¼ˆå…·æœ‰ä¸å‡åŒ€è¡¨é¢ï¼‰ä¸Šï¼Œä¹Ÿå¯¹æ‰€æå‡ºç›®å…‰å¼•å¯¼å‹RUSSçš„æ•´ä½“æ€§èƒ½è¿›è¡Œäº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05053v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŒ»å­¦è¶…å£°å¹¿æ³›åº”ç”¨äºç°ä»£ä¸´åºŠå®è·µä¸­è¡€ç®¡ç»“æ„çš„æ£€æŸ¥ï¼Œä½†ä¼ ç»Ÿè¶…å£°æ£€æŸ¥å¸¸é¢ä¸´æ“ä½œè€…é—´å’Œæ“ä½œè€…å†…éƒ¨å·®å¼‚çš„æŒ‘æˆ˜ã€‚æœºå™¨äººè¶…å£°ç³»ç»Ÿï¼ˆRUSSï¼‰å› å…¶ç¨³å®šæ€§å’Œå¯é‡å¤æ€§ä¼˜åŠ¿ï¼Œæˆä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜çš„æ½œåœ¨æ–¹æ¡ˆã€‚é’ˆå¯¹äººç±»è¡€ç®¡å¤æ‚ç»“æ„ï¼Œè¶…å£°å›¾åƒä¸­å¸¸å‡ºç°å¤šæ¡è¡€ç®¡æˆ–å•æ¡è¡€ç®¡åˆ†å‰ï¼Œä½¿æ£€æŸ¥è¿‡ç¨‹å¤æ‚åŒ–ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè¡€ç®¡åº”ç”¨çš„ç›®å…‰å¼•å¯¼å‹RUSSã€‚ç›®å…‰è¿½è¸ªå™¨æ•æ‰æ“ä½œè€…çš„çœ¼ç¥åŠ¨ä½œï¼Œæå–çš„ç›®å…‰ä¿¡å·å¼•å¯¼RUSSåœ¨è¡€ç®¡åˆ†å‰æ—¶è·Ÿè¸ªæ­£ç¡®çš„è¡€ç®¡ã€‚åŒæ—¶ï¼Œæå‡ºä¸€ç§ç›®å…‰å¼•å¯¼åˆ†å‰²ç½‘ç»œï¼Œåˆ©ç”¨ç›®å…‰ä¿¡æ¯æé«˜åˆ†å‰²ç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œç›®å…‰ä¿¡å·å¾€å¾€å­˜åœ¨å™ªå£°ï¼Œéœ€è¦è§£è¯»ä»¥å‡†ç¡®åˆ¤æ–­æ“ä½œè€…çš„çœŸæ­£æ„å›¾ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºä¸€ç§ç¨³å®šæ¨¡å—æ¥å¤„ç†åŸå§‹ç›®å…‰æ•°æ®ã€‚æ¨æ–­å‡ºçš„æ³¨æ„åŠ›çƒ­å›¾è¢«ç”¨ä½œåŒºåŸŸææ¡ˆï¼Œè¾…åŠ©åˆ†å‰²ï¼Œå¹¶åœ¨å‡ºç°åˆ†å‰ç­‰æ“ä½œæ—¶è§¦å‘ä¿¡å·ï¼Œæé†’æ“ä½œè€…è°ƒæ•´æ‰«æç›®æ ‡ã€‚ä¸ºç¡®ä¿æ‰«æè¿‡ç¨‹ä¸­æ¢å¤´ä¸è¡¨é¢ä¹‹é—´çš„é€‚å½“æ¥è§¦ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºè‡ªåŠ¨è¶…å£°ç½®ä¿¡åº¦çš„æ–¹å‘æ ¡æ­£æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„ç›®å…‰å¼•å¯¼åˆ†å‰²ç®¡é“çš„æ•ˆç‡è¾ƒé«˜ã€‚æ­¤å¤–ï¼Œåœ¨å…·æœ‰ä¸å¹³è¡¨é¢çš„çœŸå®æ‰‹è‡‚å¹»å½±ä¸ŠéªŒè¯äº†æ‰€æå‡ºç›®å…‰å¼•å¯¼å‹RUSSçš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦è¶…å£°åœ¨è¡€ç®¡ç»“æ„æ£€æŸ¥ä¸­å…·æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†ä¼ ç»Ÿè¶…å£°æ£€æŸ¥é¢ä¸´æ“ä½œè€…å·®å¼‚å’Œè¡€ç®¡ç»“æ„å¤æ‚çš„æŒ‘æˆ˜ã€‚</li>
<li>æœºå™¨äººè¶…å£°ç³»ç»Ÿï¼ˆRUSSï¼‰å› å…¶ç¨³å®šæ€§å’Œå¯é‡å¤æ€§ï¼Œè¢«è§†ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜çš„æœ‰æ½œåŠ›çš„æ–¹æ¡ˆã€‚</li>
<li>æå‡ºä¸€ç§ç›®å…‰å¼•å¯¼å‹RUSSï¼Œé€šè¿‡æ•æ‰æ“ä½œè€…çš„çœ¼åŠ¨æ¥æŒ‡å¯¼æœºå™¨äººç³»ç»Ÿåœ¨å¤æ‚è¡€ç®¡ç»“æ„ä¸­çš„æ“ä½œã€‚</li>
<li>åˆ©ç”¨ç›®å…‰ä¿¡å·æ¥å¢å¼ºå›¾åƒåˆ†å‰²çš„ç¨³å¥æ€§ï¼Œå¹¶æå‡ºä¸€ç§ç¨³å®šæ¨¡å—æ¥å¤„ç†å«æœ‰å™ªå£°çš„ç›®å…‰æ•°æ®ã€‚</li>
<li>æ³¨æ„åŠ›çƒ­å›¾ä½œä¸ºåŒºåŸŸææ¡ˆï¼Œåœ¨è¡€ç®¡åˆ†å‰ç­‰æƒ…å†µä¸‹è¾…åŠ©åˆ†å‰²å¹¶è§¦å‘æ“ä½œè°ƒæ•´ä¿¡å·ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§è‡ªåŠ¨è¶…å£°ç½®ä¿¡åº¦æ–¹æ³•ï¼Œä»¥ç¡®ä¿æ¢å¤´åœ¨æ‰«æè¿‡ç¨‹ä¸­çš„æ–¹å‘æ ¡æ­£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05053">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cbe80b234da599b41b564e1e15cb4cca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6f9a386f1feb0c85f68ee6d9f7b6c8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-159a6f7af168db1a00cf45843c12740e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4afd2a836adf14948fa4ebacda565e7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18650567901b9534382a36ad2ffa7285.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="WGM-microprobe-device-for-high-sensitivity-and-broadband-ultrasound-detection"><a href="#WGM-microprobe-device-for-high-sensitivity-and-broadband-ultrasound-detection" class="headerlink" title="WGM microprobe device for high-sensitivity and broadband ultrasound   detection"></a>WGM microprobe device for high-sensitivity and broadband ultrasound   detection</h2><p><strong>Authors:Jialve Sun, Shengnan Huangfu, Tinglan Chen, Zijing Cai, Bowen Ruan, Fangxing Zhang</strong></p>
<p>Whispering-gallery-mode (WGM) microcavities have emerged as a promising alternative to traditional ultrasound probes, offering high sensitivity and wide bandwidth. In our research, we propose a novel silica WGM microprobe device, with impressive Q factors up to 10^7.The side-coupled approach and special encapsulation design make the device small, robust, and capable of utilizing in both gaseous and liquid environments.We have successfully conducted photoacoustic (PA) imaging on various samples using this device which demonstrates a high sensitivity of 5.4 mPa&#x2F;sqrt(Hz) and a board bandwidth of 41 MHz at -6 dB for ultrasound. Whatâ€™s more, itâ€™s capable of capturing the vibration spectrum of microparticles up to a few hundred megahertz. Our compact and lightweight device exhibits significant application potential in PA endoscopic detection, near-field ultrasound sensing and other aspects. </p>
<blockquote>
<p>ä½è¯­å‹è…”æ¨¡å¼ï¼ˆWGMï¼‰å¾®è…”å·²ç»ä½œä¸ºä¸€ç§å‰æ™¯å…‰æ˜çš„è¶…å£°æ¢é’ˆæ›¿ä»£æ–¹æ¡ˆå‡ºç°ï¼Œå®ƒæä¾›äº†é«˜çµæ•åº¦å’Œå®½å¸¦å®½ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹äºŒæ°§åŒ–ç¡…WGMå¾®å‹æ¢é’ˆè®¾å¤‡ï¼Œå…·æœ‰é«˜è¾¾çš„Qå› å­ï¼ˆQ factorï¼‰è¾¾ç™¾ä¸‡æ¬¡æ–¹çº§ã€‚ä¾§è€¦åˆæ–¹å¼å’Œç‰¹æ®Šçš„å°è£…è®¾è®¡ä½¿å¾—è¯¥è®¾å¤‡ä½“ç§¯å°ã€ç¨³å®šæ€§é«˜ï¼Œèƒ½åœ¨æ°”æ€å’Œæ¶²æ€ç¯å¢ƒä¸­éƒ½èƒ½åº”ç”¨è‡ªå¦‚ã€‚æˆ‘ä»¬å·²ç»æˆåŠŸä½¿ç”¨è¯¥è®¾å¤‡å¯¹å„ç§æ ·æœ¬è¿›è¡Œå…‰å£°æˆåƒï¼Œè¯æ˜äº†å…¶çµæ•åº¦é«˜è¾¾æ¯èµ«å…¹çš„å£°å‹çº§ä¸º5.4ç±³å¸•ï¼ˆmPaï¼‰ï¼Œè¶…å£°æ³¢çš„å¸¦å®½åœ¨-6åˆ†è´æ—¶è¾¾åˆ°41å…†èµ«ï¼ˆMHzï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒèƒ½å¤Ÿæ•æ‰åˆ°é«˜è¾¾æ•°ç™¾å…†èµ«å…¹çš„å¾®ç²’å­æŒ¯åŠ¨é¢‘è°±ã€‚æˆ‘ä»¬çš„ç´§å‡‘ä¸”è½»å‹çš„è®¾å¤‡åœ¨å…‰å£°å†…çª¥æ£€æµ‹ã€è¿‘åœºè¶…å£°æ„Ÿåº”ç­‰æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04627v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¾®è…”ä¸­çš„å›éŸ³å£æ¨¡å¼ï¼ˆWGMï¼‰å¾®è…”ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è¶…å£°æ¢é’ˆæ›¿ä»£æ–¹æ¡ˆï¼Œå…·æœ‰é«˜çµæ•åº¦å’Œå®½å¸¦å®½çš„ç‰¹ç‚¹ã€‚ç ”ç©¶ä¸­æå‡ºäº†ä¸€ç§æ–°å‹äºŒæ°§åŒ–ç¡…WGMå¾®æ¢é’ˆè®¾å¤‡ï¼Œå…·æœ‰é«˜è¾¾10^7çš„Qå› å­ã€‚è¯¥è®¾å¤‡çš„ä¾§è€¦åˆæ–¹å¼å’Œç‰¹æ®Šå°è£…è®¾è®¡ä½¿å…¶ä½“ç§¯å°ã€ç¨³å¥ï¼Œæ—¢å¯ç”¨äºæ°”ä½“ç¯å¢ƒä¹Ÿå¯ç”¨äºæ¶²ä½“ç¯å¢ƒã€‚ä½¿ç”¨æ­¤è®¾å¤‡è¿›è¡Œçš„å…‰å£°æˆåƒå®éªŒè¡¨æ˜ï¼Œå…¶çµæ•åº¦é«˜è¾¾5.4 mPa&#x2F;sqrt(Hz)ï¼Œåœ¨-6 dBæ—¶çš„è¶…å£°æ³¢å¸¦å®½ä¸º41 MHzï¼Œå¹¶èƒ½æ•æ‰åˆ°é«˜è¾¾æ•°ç™¾å…†èµ«å…¹çš„å¾®ç²’å­æŒ¯åŠ¨å…‰è°±ã€‚æ­¤ç´§å‡‘è½»ä¾¿çš„è®¾å¤‡åœ¨å…‰å£°å†…çª¥æ£€æµ‹ã€è¿‘åœºè¶…å£°ä¼ æ„Ÿç­‰æ–¹é¢å…·æœ‰æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WGMå¾®è…”ä½œä¸ºè¶…å£°æ¢é’ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…·æœ‰é«˜çµæ•åº¦å’Œå®½å¸¦å®½ç‰¹æ€§ã€‚</li>
<li>æ–°å‹äºŒæ°§åŒ–ç¡…WGMå¾®æ¢é’ˆè®¾å¤‡å…·æœ‰é«˜è¾¾10^7çš„Qå› å­ã€‚</li>
<li>è®¾å¤‡é‡‡ç”¨ä¾§è€¦åˆæ–¹å¼å’Œç‰¹æ®Šå°è£…è®¾è®¡ï¼Œä½“ç§¯å°ã€ç¨³å¥ï¼Œé€‚ç”¨äºæ°”ä½“å’Œæ¶²ä½“ç¯å¢ƒã€‚</li>
<li>å…‰å£°æˆåƒå®éªŒè¯æ˜äº†è®¾å¤‡çš„é«˜çµæ•åº¦å’Œå®½å¸¦å®½æ€§èƒ½ã€‚</li>
<li>è®¾å¤‡èƒ½æ•æ‰åˆ°é«˜è¾¾æ•°ç™¾å…†èµ«å…¹çš„å¾®ç²’å­æŒ¯åŠ¨å…‰è°±ã€‚</li>
<li>è¯¥è®¾å¤‡å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‰å£°å†…çª¥æ£€æµ‹ã€è¿‘åœºè¶…å£°ä¼ æ„Ÿç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-99c799c78ac2892ab5bf05ed48de471d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2934d60cb2c951db362fc60b1d1cef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07f6526b592d8d45cf1c225b5600863e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e053710ca7242a781bcd2ee1730b7b68.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generative-Autoregressive-Transformers-for-Model-Agnostic-Federated-MRI-Reconstruction"><a href="#Generative-Autoregressive-Transformers-for-Model-Agnostic-Federated-MRI-Reconstruction" class="headerlink" title="Generative Autoregressive Transformers for Model-Agnostic Federated MRI   Reconstruction"></a>Generative Autoregressive Transformers for Model-Agnostic Federated MRI   Reconstruction</h2><p><strong>Authors:Valiyeh A. Nezhad, Gokberk Elmas, Bilal Kabas, Fuat Arslan, Tolga Ã‡ukur</strong></p>
<p>Although learning-based models hold great promise for MRI reconstruction, single-site models built on limited local datasets often suffer from poor generalization. This challenge has spurred interest in collaborative model training on multi-site datasets via federated learning (FL) â€“ a privacy-preserving framework that aggregates model updates instead of sharing imaging data. Conventional FL builds a global model by aggregating locally trained model weights, inherently constraining all sites to a homogeneous model architecture. This rigid homogeneity requirement forces sites to forgo architectures tailored to their compute infrastructure and application-specific demands. Consequently, existing FL methods for MRI reconstruction fail to support model-heterogeneous settings, where individual sites are allowed to use distinct architectures. To overcome this fundamental limitation, here we introduce FedGAT, a novel model-agnostic FL technique based on generative autoregressive transformers. FedGAT decentralizes the training of a global generative prior that captures the distribution of multi-site MR images. For enhanced fidelity, we propose a novel site-prompted GAT prior that controllably synthesizes MR images from desired sites via autoregressive prediction across spatial scales. Each site then trains its site-specific reconstruction model â€“ using its preferred architecture â€“ on a hybrid dataset comprising the local MRI dataset and GAT-generated synthetic MRI datasets for other sites. Comprehensive experiments on multi-institutional datasets demonstrate that FedGAT supports flexible collaborations while enjoying superior within-site and across-site reconstruction performance compared to state-of-the-art FL baselines. </p>
<blockquote>
<p>å°½ç®¡åŸºäºå­¦ä¹ çš„æ¨¡å‹åœ¨MRIé‡å»ºæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ä»…ä¾èµ–äºæœ‰é™æœ¬åœ°æ•°æ®é›†çš„å•ç«™ç‚¹æ¨¡å‹é€šå¸¸é¢ä¸´æ³›åŒ–èƒ½åŠ›å·®çš„æŒ‘æˆ˜ã€‚è¿™ä¸€æŒ‘æˆ˜æ¿€å‘äº†å¯¹é€šè¿‡è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰è¿›è¡Œå¤šç«™ç‚¹æ•°æ®é›†åä½œæ¨¡å‹è®­ç»ƒçš„å…´è¶£ã€‚è”é‚¦å­¦ä¹ æ˜¯ä¸€ç§éšç§ä¿æŠ¤æ¡†æ¶ï¼Œå®ƒé€šè¿‡èšåˆæ¨¡å‹æ›´æ–°è€Œä¸æ˜¯å…±äº«æˆåƒæ•°æ®æ¥è¿›è¡Œè®­ç»ƒã€‚ä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ é€šè¿‡èšåˆæœ¬åœ°è®­ç»ƒçš„æ¨¡å‹æƒé‡æ¥æ„å»ºå…¨å±€æ¨¡å‹ï¼Œè¿™å›ºæœ‰åœ°è¦æ±‚æ‰€æœ‰ç«™ç‚¹é‡‡ç”¨åŒè´¨æ¨¡å‹æ¶æ„ã€‚è¿™ç§åˆšæ€§çš„åŒè´¨åŒ–è¦æ±‚è¿«ä½¿ç«™ç‚¹æ”¾å¼ƒé’ˆå¯¹å…¶è®¡ç®—åŸºç¡€è®¾æ–½å’Œåº”ç”¨ç‰¹å®šéœ€æ±‚é‡èº«å®šåˆ¶çš„æ¶æ„ã€‚å› æ­¤ï¼Œç°æœ‰çš„ç”¨äºMRIé‡å»ºçš„è”é‚¦å­¦ä¹ æ–¹æ³•æ— æ³•æ”¯æŒæ¨¡å‹å¼‚æ„è®¾ç½®ï¼Œå…¶ä¸­å•ä¸ªç«™ç‚¹è¢«å…è®¸ä½¿ç”¨ä¸åŒçš„æ¶æ„ã€‚ä¸ºäº†å…‹æœè¿™ä¸€åŸºæœ¬é™åˆ¶ï¼Œè¿™é‡Œæˆ‘ä»¬å¼•å…¥äº†FedGATï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç”Ÿæˆè‡ªå›å½’å˜å‹å™¨çš„æ–°å‹æ¨¡å‹æ— å…³è”é‚¦å­¦ä¹ æŠ€æœ¯ã€‚FedGATå»ä¸­å¿ƒåŒ–å…¨å±€ç”Ÿæˆå…ˆéªŒçš„è®­ç»ƒï¼Œè¯¥å…ˆéªŒæ•æ‰å¤šç«™ç‚¹MRå›¾åƒçš„åˆ†å¸ƒã€‚ä¸ºäº†æé«˜ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç«™ç‚¹æç¤ºGATå…ˆéªŒï¼Œè¯¥å…ˆéªŒå¯é€šè¿‡ç©ºé—´å°ºåº¦çš„è‡ªå›å½’é¢„æµ‹ï¼Œå¯æ§åœ°åˆæˆæ¥è‡ªæ‰€éœ€ç«™ç‚¹çš„MRå›¾åƒã€‚ç„¶åï¼Œæ¯ä¸ªç«™ç‚¹åœ¨å…¶æ··åˆæ•°æ®é›†ï¼ˆç”±æœ¬åœ°MRIæ•°æ®é›†å’ŒGATç”Ÿæˆçš„åˆæˆMRIæ•°æ®é›†ç»„æˆï¼‰ä¸Šï¼Œä½¿ç”¨å…¶é¦–é€‰æ¶æ„è®­ç»ƒå…¶ç‰¹å®šçš„é‡å»ºæ¨¡å‹ã€‚åœ¨å¤šæœºæ„æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒFedGATæ”¯æŒçµæ´»çš„åä½œï¼ŒåŒæ—¶åœ¨ç«™å†…å’Œè·¨ç«™é‡å»ºæ€§èƒ½ä¸Šä¼˜äºæœ€æ–°çš„è”é‚¦å­¦ä¹ åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04521v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå­¦ä¹ çš„æ¨¡å‹åœ¨MRIé‡å»ºä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å•ä¸€ç«™ç‚¹æ¨¡å‹åœ¨æœ‰é™çš„æœ¬åœ°æ•°æ®é›†ä¸Šè®­ç»ƒå¸¸å¸¸é¢ä¸´æ³›åŒ–æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜å¼€å§‹å°è¯•ä½¿ç”¨è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰è¿›è¡Œå¤šç«™ç‚¹æ•°æ®é›†ä¸Šçš„ååŒæ¨¡å‹è®­ç»ƒã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ é€šè¿‡èšåˆæœ¬åœ°è®­ç»ƒçš„æ¨¡å‹æƒé‡æ¥å»ºç«‹å…¨å±€æ¨¡å‹ï¼Œè¿™è¦æ±‚æ‰€æœ‰ç«™ç‚¹å¿…é¡»ä½¿ç”¨ç›¸åŒæ¶æ„ï¼Œé™åˆ¶äº†ç«™ç‚¹çš„çµæ´»æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„è”é‚¦å­¦ä¹ æŠ€æœ¯â€”â€”FedGATï¼Œå®ƒåŸºäºç”Ÿæˆå¼è‡ªå›å½’è½¬æ¢å™¨ï¼Œæ‰“ç ´äº†è¿™ä¸€å±€é™ã€‚FedGATå®ç°äº†ç”Ÿæˆå¼å…ˆéªŒçŸ¥è¯†çš„å»ä¸­å¿ƒåŒ–è®­ç»ƒï¼Œå¯ä»¥æ•æ‰å¤šç«™ç‚¹MRIå›¾åƒåˆ†å¸ƒçš„ç‰¹ç‚¹ã€‚ä¸ºæé«˜å›¾åƒè´¨é‡ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯æ§åˆæˆMRIå›¾åƒçš„ä½ç‚¹æç¤ºGATå…ˆéªŒæŠ€æœ¯ã€‚æ¯ä¸ªç«™ç‚¹å¯ä»¥åœ¨æœ¬åœ°æ•°æ®é›†å’ŒGATç”Ÿæˆçš„åˆæˆMRIæ•°æ®é›†ç»„æˆçš„æ··åˆæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨å…¶åå¥½çš„æ¶æ„è®­ç»ƒé‡å»ºæ¨¡å‹ã€‚åœ¨è·¨æœºæ„æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒFedGATåœ¨æ”¯æŒçµæ´»åä½œçš„åŒæ—¶ï¼Œå…¶é‡å»ºæ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„è”é‚¦å­¦ä¹ åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ å‹æ¨¡å‹åœ¨MRIé‡å»ºä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å•ä¸€ç«™ç‚¹æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚</li>
<li>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰é€šè¿‡å¤šç«™ç‚¹æ•°æ®é›†ååŒæ¨¡å‹è®­ç»ƒè§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿè”é‚¦å­¦ä¹ æ–¹æ³•è¦æ±‚æ‰€æœ‰ç«™ç‚¹ä½¿ç”¨ç›¸åŒæ¨¡å‹æ¶æ„ï¼Œé™åˆ¶äº†çµæ´»æ€§ã€‚</li>
<li>FedGATæ˜¯ä¸€ç§æ–°å‹çš„è”é‚¦å­¦ä¹ æŠ€æœ¯ï¼ŒåŸºäºç”Ÿæˆå¼è‡ªå›å½’è½¬æ¢å™¨ï¼Œæ”¯æŒæ¨¡å‹å¼‚æ„è®¾ç½®ã€‚</li>
<li>FedGATå®ç°äº†ç”Ÿæˆå¼å…ˆéªŒçŸ¥è¯†çš„å»ä¸­å¿ƒåŒ–è®­ç»ƒï¼Œæ•æ‰å¤šç«™ç‚¹MRIå›¾åƒåˆ†å¸ƒç‰¹ç‚¹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯æ§åˆæˆMRIå›¾åƒçš„ä½ç‚¹æç¤ºGATå…ˆéªŒæŠ€æœ¯ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4507c80ec7dbc4331be781dc2746e3ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44f6f553538f63fee3c9c579c2dad6de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3662300e71e46f53c78f044f9b9fbba.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hybrid-Deep-Learning-Framework-for-Classification-of-Kidney-CT-Images-Diagnosis-of-Stones-Cysts-and-Tumors"><a href="#Hybrid-Deep-Learning-Framework-for-Classification-of-Kidney-CT-Images-Diagnosis-of-Stones-Cysts-and-Tumors" class="headerlink" title="Hybrid Deep Learning Framework for Classification of Kidney CT Images:   Diagnosis of Stones, Cysts, and Tumors"></a>Hybrid Deep Learning Framework for Classification of Kidney CT Images:   Diagnosis of Stones, Cysts, and Tumors</h2><p><strong>Authors:Kiran Sharma, Ziya Uddin, Adarsh Wadal, Dhruv Gupta</strong></p>
<p>Medical image classification is a vital research area that utilizes advanced computational techniques to improve disease diagnosis and treatment planning. Deep learning models, especially Convolutional Neural Networks (CNNs), have transformed this field by providing automated and precise analysis of complex medical images. This study introduces a hybrid deep learning model that integrates a pre-trained ResNet101 with a custom CNN to classify kidney CT images into four categories: normal, stone, cyst, and tumor. The proposed model leverages feature fusion to enhance classification accuracy, achieving 99.73% training accuracy and 100% testing accuracy. Using a dataset of 12,446 CT images and advanced feature mapping techniques, the hybrid CNN model outperforms standalone ResNet101. This architecture delivers a robust and efficient solution for automated kidney disease diagnosis, providing improved precision, recall, and reduced testing time, making it highly suitable for clinical applications. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†ç±»æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é¢†åŸŸï¼Œå®ƒåˆ©ç”¨å…ˆè¿›çš„è®¡ç®—æŠ€æœ¯æ¥æ”¹å–„ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’çš„åˆ¶å®šã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œå·²ç»é€šè¿‡æä¾›å¤æ‚åŒ»å­¦å›¾åƒçš„è‡ªåŠ¨åŒ–å’Œç²¾ç¡®åˆ†æï¼Œä½¿è¯¥é¢†åŸŸå‘ç”Ÿäº†å˜é©ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ··åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå®ƒå°†é¢„è®­ç»ƒçš„ResNet101ä¸è‡ªå®šä¹‰CNNç›¸ç»“åˆï¼Œå°†è‚¾è„CTå›¾åƒåˆ†ç±»ä¸ºæ­£å¸¸ã€ç»“çŸ³ã€å›Šè‚¿å’Œè‚¿ç˜¤å››ç±»ã€‚æ‰€æå‡ºçš„æ¨¡å‹åˆ©ç”¨ç‰¹å¾èåˆæ¥æé«˜åˆ†ç±»ç²¾åº¦ï¼Œå®ç°99.73%çš„è®­ç»ƒç²¾åº¦å’Œ100%çš„æµ‹è¯•ç²¾åº¦ã€‚ä½¿ç”¨åŒ…å«12446å¼ CTå›¾åƒçš„æ•°æ®åº“å’Œå…ˆè¿›çš„ç‰¹å¾æ˜ å°„æŠ€æœ¯ï¼Œæ··åˆCNNæ¨¡å‹çš„è¡¨ç°ä¼˜äºå•ç‹¬çš„ResNet101ã€‚è¯¥æ¶æ„ä¸ºè‡ªåŠ¨åŒ–è‚¾è„ç–¾ç—…è¯Šæ–­æä¾›äº†ç¨³å¥é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæé«˜äº†ç²¾ç¡®åº¦ã€å¬å›ç‡ï¼Œå¹¶ç¼©çŸ­äº†æµ‹è¯•æ—¶é—´ï¼Œéå¸¸é€‚åˆä¸´åºŠåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04367v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†ç±»æ˜¯è¿ç”¨å…ˆè¿›è®¡ç®—æŠ€æœ¯ä¿ƒè¿›ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’çš„é‡è¦ç ”ç©¶é¢†åŸŸã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ··åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç»“åˆé¢„è®­ç»ƒçš„ResNet101å’Œè‡ªå®šä¹‰CNNï¼Œå¯¹è‚¾è„CTå›¾åƒè¿›è¡Œæ­£å¸¸ã€ç»“çŸ³ã€å›Šè‚¿å’Œè‚¿ç˜¤å››ç±»åˆ†ç±»ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç‰¹å¾èåˆæé«˜åˆ†ç±»ç²¾åº¦ï¼Œå®ç°99.73%çš„è®­ç»ƒç²¾åº¦å’Œ100%çš„æµ‹è¯•ç²¾åº¦ã€‚æ··åˆCNNæ¨¡å‹ä¼˜äºå•ç‹¬çš„ResNet101ï¼Œä¸ºè‚¾è„ç–¾ç—…çš„è‡ªåŠ¨è¯Šæ–­æä¾›äº†ç²¾ç¡®ã€é«˜æ•ˆã€å¿«é€Ÿçš„è§£å†³æ–¹æ¡ˆï¼Œéå¸¸é€‚åˆä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­èµ·é‡è¦ä½œç”¨ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ï¼Œå·²ç»å½»åº•æ”¹å˜äº†åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç»“åˆäº†ResNet101å’Œè‡ªå®šä¹‰CNNè¿›è¡Œè‚¾è„CTå›¾åƒåˆ†ç±»ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ç‰¹å¾èåˆæé«˜äº†åˆ†ç±»ç²¾åº¦ã€‚</li>
<li>æ··åˆæ¨¡å‹çš„è®­ç»ƒç²¾åº¦è¾¾åˆ°99.73%ï¼Œæµ‹è¯•ç²¾åº¦è¾¾åˆ°100%ã€‚</li>
<li>æ··åˆCNNæ¨¡å‹ä¼˜äºå•ç‹¬çš„ResNet101æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c72562ecc0503c0c6a8ca3d333989ee7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-706d2455639943bf6aff5c9106f27e4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87892531c2c7283c3d6fba0441f6c45e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41c9ab818dd1c1e9027abe02b39211c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f48f5c6ef740fa9fe816c30460b7f60.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Multi-Scale-Feature-Fusion-Framework-Integrating-Frequency-Domain-and-Cross-View-Attention-for-Dual-View-X-ray-Security-Inspections"><a href="#A-Multi-Scale-Feature-Fusion-Framework-Integrating-Frequency-Domain-and-Cross-View-Attention-for-Dual-View-X-ray-Security-Inspections" class="headerlink" title="A Multi-Scale Feature Fusion Framework Integrating Frequency Domain and   Cross-View Attention for Dual-View X-ray Security Inspections"></a>A Multi-Scale Feature Fusion Framework Integrating Frequency Domain and   Cross-View Attention for Dual-View X-ray Security Inspections</h2><p><strong>Authors:Shilong Hong, Yanzhou Zhou, Weichao Xu</strong></p>
<p>With the rapid development of modern transportation systems and the exponential growth of logistics volumes, intelligent X-ray-based security inspection systems play a crucial role in public safety. Although single-view X-ray equipment is widely deployed, it struggles to accurately identify contraband in complex stacking scenarios due to strong viewpoint dependency and inadequate feature representation. To address this, we propose an innovative multi-scale interactive feature fusion framework tailored for dual-view X-ray security inspection image classification. The framework comprises three core modules: the Frequency Domain Interaction Module (FDIM) enhances frequency-domain features through Fourier transform; the Multi-Scale Cross-View Feature Enhancement (MSCFE) leverages cross-view attention mechanisms to strengthen feature interactions; and the Convolutional Attention Fusion Module (CAFM) efficiently fuses features by integrating channel attention with depthwise-separable convolutions. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches across multiple backbone architectures, particularly excelling in complex scenarios with occlusions and object stacking. </p>
<blockquote>
<p>éšç€ç°ä»£äº¤é€šè¿è¾“ç³»ç»Ÿçš„å¿«é€Ÿå‘å±•å’Œç‰©æµé‡çš„æŒ‡æ•°çº§å¢é•¿ï¼ŒåŸºäºæ™ºèƒ½Xå°„çº¿çš„å®‰å…¨æ£€æŸ¥ç³»ç»Ÿåœ¨å…¬å…±å®‰å…¨ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚è™½ç„¶å•è§†å›¾Xå°„çº¿è®¾å¤‡å·²å¹¿æ³›éƒ¨ç½²ï¼Œä½†ç”±äºå…¶å¼ºçƒˆçš„è§†è§’ä¾èµ–æ€§å’Œç‰¹å¾è¡¨ç¤ºçš„ä¸è¶³ï¼Œå®ƒåœ¨å¤æ‚çš„å †å åœºæ™¯ä¸­éš¾ä»¥å‡†ç¡®è¯†åˆ«è¿ç¦å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹åŒè§†å›¾Xå°„çº¿å®‰å…¨æ£€æŸ¥å›¾åƒåˆ†ç±»çš„å¤šå°ºåº¦äº¤äº’ç‰¹å¾èåˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šé¢‘åŸŸäº¤äº’æ¨¡å—ï¼ˆFDIMï¼‰é€šè¿‡å‚…é‡Œå¶å˜æ¢å¢å¼ºé¢‘åŸŸç‰¹å¾ï¼›å¤šå°ºåº¦è·¨è§†å›¾ç‰¹å¾å¢å¼ºï¼ˆMSCFEï¼‰åˆ©ç”¨è·¨è§†å›¾æ³¨æ„åŠ›æœºåˆ¶åŠ å¼ºç‰¹å¾äº¤äº’ï¼›å·ç§¯æ³¨æ„åŠ›èåˆæ¨¡å—ï¼ˆCAFMï¼‰é€šè¿‡ç»“åˆé€šé“æ³¨æ„åŠ›å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯æ¥æœ‰æ•ˆåœ°èåˆç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ä¸»å¹²æ¶æ„ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨é®æŒ¡å’Œç‰©ä½“å †å çš„å¤æ‚åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01710v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€ç°ä»£äº¤é€šè¿è¾“ç³»ç»Ÿçš„å¿«é€Ÿå‘å±•å’Œç‰©æµé‡çš„æŒ‡æ•°çº§å¢é•¿ï¼Œæ™ºèƒ½Xå…‰å®‰å…¨æ£€æµ‹ç³»ç»Ÿåœ¨å…¬å…±å®‰å…¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚é’ˆå¯¹å•è§†è§’Xå…‰è®¾å¤‡åœ¨å¤æ‚å †å åœºæ™¯ä¸­éš¾ä»¥å‡†ç¡®è¯†åˆ«è¿ç¦å“çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤šå°ºåº¦äº¤äº’å¼ç‰¹å¾èåˆæ¡†æ¶ï¼Œç”¨äºåŒè§†è§’Xå…‰å®‰å…¨æ£€æµ‹å›¾åƒåˆ†ç±»ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šé¢‘ç‡åŸŸäº¤äº’æ¨¡å—ï¼ˆFDIMï¼‰é€šè¿‡å‚…ç«‹å¶å˜æ¢å¢å¼ºé¢‘ç‡åŸŸç‰¹å¾ï¼›å¤šå°ºåº¦è·¨è§†å›¾ç‰¹å¾å¢å¼ºï¼ˆMSCFEï¼‰åˆ©ç”¨è·¨è§†å›¾æ³¨æ„åŠ›æœºåˆ¶åŠ å¼ºç‰¹å¾äº¤äº’ï¼›å·ç§¯æ³¨æ„åŠ›èåˆæ¨¡å—ï¼ˆCAFMï¼‰é€šè¿‡ç»“åˆé€šé“æ³¨æ„åŠ›å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯æ¥æœ‰æ•ˆåœ°èåˆç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä¸»å¹²æ¶æ„ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨å­˜åœ¨é®æŒ¡å’Œç‰©ä½“å †å çš„å¤æ‚åœºæ™¯ä¸­è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½Xå…‰å®‰å…¨æ£€æµ‹ç³»ç»Ÿåœ¨å…¬å…±å®‰å…¨ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹æ—¥ç›Šå¢é•¿çš„ç‰©æµé‡å’Œå¤æ‚åœºæ™¯çš„æŒ‘æˆ˜ã€‚</li>
<li>å•è§†è§’Xå…‰è®¾å¤‡åœ¨å¤æ‚å †å åœºæ™¯ä¸­è¯†åˆ«è¿ç¦å“å­˜åœ¨å›°éš¾ï¼Œéœ€è¦æ›´å…ˆè¿›çš„å›¾åƒåˆ†ç±»æŠ€æœ¯ã€‚</li>
<li>æå‡ºçš„å¤šå°ºåº¦äº¤äº’å¼ç‰¹å¾èåˆæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼Œåˆ†åˆ«å¢å¼ºé¢‘ç‡åŸŸç‰¹å¾ã€åˆ©ç”¨è·¨è§†å›¾æ³¨æ„åŠ›æœºåˆ¶å’ŒåŠ å¼ºç‰¹å¾äº¤äº’ã€ä»¥åŠæœ‰æ•ˆåœ°èåˆç‰¹å¾ã€‚</li>
<li>æ¡†æ¶é€šè¿‡å‚…ç«‹å¶å˜æ¢å¢å¼ºé¢‘ç‡åŸŸç‰¹å¾ï¼Œé€šè¿‡è·¨è§†å›¾æ³¨æ„åŠ›æœºåˆ¶å¤„ç†ä¸åŒè§†è§’ä¸‹çš„ç‰¹å¾ï¼Œå¹¶ç»“åˆé€šé“æ³¨æ„åŠ›å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯æ¥èåˆç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§ä¸»å¹²æ¶æ„ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸­ã€‚</li>
<li>è¯¥æ¡†æ¶ç‰¹åˆ«æ“…é•¿å¤„ç†å­˜åœ¨é®æŒ¡å’Œç‰©ä½“å †å çš„æƒ…å†µï¼Œè¿™æ˜¯ç°æœ‰æ–¹æ³•çš„ä¸€ä¸ªé‡å¤§æ”¹è¿›ã€‚</li>
<li>æ¡†æ¶çš„ä¼˜å¼‚æ€§èƒ½ä½¿å…¶æˆä¸ºXå…‰å®‰å…¨æ£€æµ‹é¢†åŸŸçš„ä¸€ä¸ªæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8e861630402e46c07d18095c2f01edb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6200b9aabddab4bd72b77c12a1b4b43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1ec2a763901d523dba70ae66a41be91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a96735b420779f155f9b37050f06327.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5593da8868927c82e7fb7429a2276bd6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning"><a href="#Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning" class="headerlink" title="Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning"></a>Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning</h2><p><strong>Authors:Jun-En Ding, Chien-Chin Hsu, Chi-Hsiang Chu, Shuqiang Wang, Feng Liu</strong></p>
<p>The classification of medical images is a pivotal aspect of disease diagnosis, often enhanced by deep learning techniques. However, traditional approaches typically focus on unimodal medical image data, neglecting the integration of diverse non-image patient data. This paper proposes a novel Cross-Graph Modal Contrastive Learning (CGMCL) framework for multimodal structured data from different data domains to improve medical image classification. The model effectively integrates both image and non-image data by constructing cross-modality graphs and leveraging contrastive learning to align multimodal features in a shared latent space. An inter-modality feature scaling module further optimizes the representation learning process by reducing the gap between heterogeneous modalities. The proposed approach is evaluated on two datasets: a Parkinsonâ€™s disease (PD) dataset and a public melanoma dataset. Results demonstrate that CGMCL outperforms conventional unimodal methods in accuracy, interpretability, and early disease prediction. Additionally, the method shows superior performance in multi-class melanoma classification. The CGMCL framework provides valuable insights into medical image classification while offering improved disease interpretability and predictive capabilities. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†ç±»æ˜¯ç–¾ç—…è¯Šæ–­çš„é‡è¦æ–¹é¢ï¼Œé€šå¸¸å¯ä»¥é€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯å¾—åˆ°å¢å¼ºã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å•æ¨¡æ€åŒ»å­¦å›¾åƒæ•°æ®ï¼Œå¿½ç•¥äº†ä¸åŒéå›¾åƒæ‚£è€…æ•°æ®çš„æ•´åˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨å›¾æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼ˆCGMCLï¼‰æ¡†æ¶ï¼Œç”¨äºä¸åŒæ•°æ®åŸŸçš„å¤šæ¨¡æ€ç»“æ„åŒ–æ•°æ®ï¼Œä»¥æ”¹è¿›åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾å¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¥å¯¹é½å¤šæ¨¡æ€ç‰¹å¾åœ¨å…±äº«æ½œåœ¨ç©ºé—´ï¼Œä»è€Œæœ‰æ•ˆåœ°æ•´åˆå›¾åƒå’Œéå›¾åƒæ•°æ®ã€‚è·¨æ¨¡æ€ç‰¹å¾ç¼©æ”¾æ¨¡å—è¿›ä¸€æ­¥ä¼˜åŒ–äº†è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œç¼©å°äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šå¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ•°æ®é›†å’Œå…¬å…±é»‘è‰²ç´ ç˜¤æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ—©æœŸç–¾ç—…é¢„æµ‹æ–¹é¢ï¼ŒCGMCLä¼˜äºä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç±»é»‘è‰²ç´ ç˜¤åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚CGMCLæ¡†æ¶ä¸ºåŒ»å­¦å›¾åƒåˆ†ç±»æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼ŒåŒæ—¶æé«˜äº†ç–¾ç—…å¯è§£é‡Šæ€§å’Œé¢„æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17494v3">PDF</a> </p>
<p><strong>Summary</strong><br>     è®ºæ–‡æå‡ºä¸€ç§åŸºäºè·¨å›¾æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼ˆCGMCLï¼‰çš„å¤šæ¨¡æ€æ•°æ®èåˆæ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ã€‚è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæ•´åˆå›¾åƒå’Œéå›¾åƒæ•°æ®ï¼Œé€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾å¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ å¯¹é½å¤šæ¨¡æ€ç‰¹å¾ï¼Œä¼˜åŒ–è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œç¼©å°ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚åœ¨å¸•é‡‘æ£®ç—…å’Œå…¬å…±é»‘è‰²ç´ ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ—©æœŸç–¾ç—…é¢„æµ‹æ–¹é¢ä¼˜äºä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•ï¼Œä¸ºå¤šç±»é»‘è‰²ç´ ç˜¤åˆ†ç±»æä¾›äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»æ˜¯ç–¾ç—…è¯Šæ–­çš„å…³é”®ç¯èŠ‚ï¼Œé€šå¸¸å¯é€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯åŠ å¼ºã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å•æ¨¡æ€åŒ»å­¦å›¾åƒæ•°æ®ï¼Œå¿½ç•¥äº†éå›¾åƒæ‚£è€…æ•°æ®çš„æ•´åˆã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè·¨å›¾æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼ˆCGMCLï¼‰çš„å¤šæ¨¡æ€æ•°æ®èåˆæ¡†æ¶ã€‚</li>
<li>CGMCLæ¡†æ¶é€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ•´åˆå›¾åƒå’Œéå›¾åƒæ•°æ®ã€‚</li>
<li>è·¨æ¨¡æ€ç‰¹å¾ç¼©æ”¾æ¨¡å—ç¼©å°äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ï¼Œä¼˜åŒ–äº†è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>åœ¨å¸•é‡‘æ£®ç—…å’Œé»‘è‰²ç´ ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†CGMCLæ¡†æ¶çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d9dbcde773730552ed74f85416b1857.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ac1d5ef9dd19ac3b7c89ecc314ff390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c3afe74232f885688a350ce87122b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6986fc293845fd61bc9ba737bf16d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f99f055faccc7f1c93c993d5ac326652.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Vec2Face-Scaling-Face-Dataset-Generation-with-Loosely-Constrained-Vectors"><a href="#Vec2Face-Scaling-Face-Dataset-Generation-with-Loosely-Constrained-Vectors" class="headerlink" title="Vec2Face: Scaling Face Dataset Generation with Loosely Constrained   Vectors"></a>Vec2Face: Scaling Face Dataset Generation with Loosely Constrained   Vectors</h2><p><strong>Authors:Haiyu Wu, Jaskirat Singh, Sicong Tian, Liang Zheng, Kevin W. Bowyer</strong></p>
<p>This paper studies how to synthesize face images of non-existent persons, to create a dataset that allows effective training of face recognition (FR) models. Besides generating realistic face images, two other important goals are: 1) the ability to generate a large number of distinct identities (inter-class separation), and 2) a proper variation in appearance of the images for each identity (intra-class variation). However, existing works 1) are typically limited in how many well-separated identities can be generated and 2) either neglect or use an external model for attribute augmentation. We propose Vec2Face, a holistic model that uses only a sampled vector as input and can flexibly generate and control the identity of face images and their attributes. Composed of a feature masked autoencoder and an image decoder, Vec2Face is supervised by face image reconstruction and can be conveniently used in inference. Using vectors with low similarity among themselves as inputs, Vec2Face generates well-separated identities. Randomly perturbing an input identity vector within a small range allows Vec2Face to generate faces of the same identity with proper variation in face attributes. It is also possible to generate images with designated attributes by adjusting vector values with a gradient descent method. Vec2Face has efficiently synthesized as many as 300K identities, whereas 60K is the largest number of identities created in the previous works. As for performance, FR models trained with the generated HSFace datasets, from 10k to 300k identities, achieve state-of-the-art accuracy, from 92% to 93.52%, on five real-world test sets (\emph{i.e.}, LFW, CFP-FP, AgeDB-30, CALFW, and CPLFW). For the first time, the FR model trained using our synthetic training set achieves higher accuracy than that trained using a same-scale training set of real face images on the CALFW, IJBB, and IJBC test sets. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•åˆæˆä¸å­˜åœ¨çš„äººè„¸å›¾åƒï¼Œä»¥åˆ›å»ºä¸€ä¸ªå…è®¸æœ‰æ•ˆè®­ç»ƒäººè„¸è¯†åˆ«ï¼ˆFRï¼‰æ¨¡å‹çš„æ•°æ®é›†ã€‚é™¤äº†ç”Ÿæˆé€¼çœŸçš„äººè„¸å›¾åƒå¤–ï¼Œå¦å¤–ä¸¤ä¸ªé‡è¦ç›®æ ‡æ˜¯ï¼š1ï¼‰èƒ½å¤Ÿç”Ÿæˆå¤§é‡ä¸åŒçš„èº«ä»½ï¼ˆç±»é—´åˆ†ç¦»ï¼‰ï¼Œ2ï¼‰ä¸ºæ¯ä¸ªèº«ä»½çš„å¤–è§‚æä¾›é€‚å½“çš„å˜ä½“ï¼ˆç±»å†…å˜åŒ–ï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œ1ï¼‰é€šå¸¸å¯ä»¥ç”Ÿæˆçš„åˆ†ç¦»è‰¯å¥½çš„èº«ä»½æ•°é‡æœ‰é™ï¼Œ2ï¼‰è¦ä¹ˆå¿½è§†è¦ä¹ˆä½¿ç”¨å¤–éƒ¨æ¨¡å‹è¿›è¡Œå±æ€§å¢å¼ºã€‚æˆ‘ä»¬æå‡ºäº†Vec2Faceï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„æ¨¡å‹ï¼Œå®ƒåªä½¿ç”¨é‡‡æ ·å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå¯ä»¥çµæ´»åœ°ç”Ÿæˆå’Œæ§åˆ¶äººè„¸å›¾åƒçš„èº«ä»½å’Œå±æ€§ã€‚Vec2Faceç”±ç‰¹å¾æ©ç è‡ªåŠ¨ç¼–ç å™¨å’Œå›¾åƒè§£ç å™¨ç»„æˆï¼Œå—åˆ°äººè„¸å›¾åƒé‡å»ºçš„ç›‘ç£ï¼Œå¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œæ¨ç†ã€‚ä½¿ç”¨å½¼æ­¤ç›¸ä¼¼æ€§ä½çš„å‘é‡ä½œä¸ºè¾“å…¥ï¼ŒVec2Faceç”Ÿæˆäº†åˆ†ç¦»è‰¯å¥½çš„èº«ä»½ã€‚åœ¨è¾“å…¥èº«ä»½å‘é‡çš„å°èŒƒå›´å†…è¿›è¡Œéšæœºæ‰°åŠ¨ï¼Œå…è®¸Vec2Faceç”Ÿæˆå…·æœ‰é€‚å½“å˜åŒ–é¢éƒ¨å±æ€§çš„äººè„¸å›¾åƒã€‚é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•è°ƒæ•´å‘é‡å€¼ï¼Œè¿˜å¯ä»¥ç”Ÿæˆå…·æœ‰æŒ‡å®šå±æ€§çš„å›¾åƒã€‚Vec2Faceå·²ç»æœ‰æ•ˆåœ°åˆæˆäº†å¤šè¾¾30ä¸‡ä¸ªèº«ä»½ï¼Œè€Œä»¥å‰çš„å·¥ä½œæœ€å¤šåªèƒ½åˆ›å»º6ä¸‡ä¸ªèº«ä»½ã€‚åœ¨æ€§èƒ½æ–¹é¢ï¼Œä½¿ç”¨ç”Ÿæˆçš„HSFaceæ•°æ®é›†è®­ç»ƒçš„FRæ¨¡å‹ï¼Œä»10kåˆ°30ä¸‡èº«ä»½ï¼Œåœ¨äº”ä¸ªçœŸå®ä¸–ç•Œæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œä»92%åˆ°93.5%ï¼Œå³LFWã€CFP-FPã€AgeDB-30ã€CALFWå’ŒCPLFWæµ‹è¯•é›†ã€‚é¦–æ¬¡å°è¯•çš„æ˜¯ï¼Œä½¿ç”¨æˆ‘ä»¬åˆæˆè®­ç»ƒé›†è®­ç»ƒçš„FRæ¨¡å‹åœ¨CALFWã€IJBBå’ŒIJBCæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®æ€§é«˜äºä½¿ç”¨ç›¸åŒè§„æ¨¡çš„ç°å®äººè„¸å›¾åƒè®­ç»ƒé›†è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02979v4">PDF</a> Accepted at ICLR2025</p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡æ—¨åœ¨åˆæˆéçœŸå®äººç‰©çš„è„¸éƒ¨å›¾åƒï¼Œåˆ›å»ºä¸€ä¸ªç”¨äºè®­ç»ƒè„¸éƒ¨è¯†åˆ«æ¨¡å‹çš„ä¼˜è´¨æ•°æ®é›†ã€‚è®ºæ–‡çš„ç›®æ ‡åŒ…æ‹¬ç”Ÿæˆé€¼çœŸçš„è„¸éƒ¨å›¾åƒã€ç”Ÿæˆå¤§é‡ä¸åŒçš„èº«ä»½ä»¥åŠä¸ºæ¯ä¸ªèº«ä»½æä¾›é€‚å½“çš„å¤–è§‚å˜åŒ–ã€‚æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ¨¡å‹Vec2Faceï¼Œåªéœ€é€šè¿‡è¾“å…¥é‡‡æ ·å‘é‡ï¼Œå°±èƒ½çµæ´»ç”Ÿæˆå¹¶æ§åˆ¶è„¸éƒ¨å›¾åƒçš„èº«ä»½åŠå…¶å±æ€§ã€‚Vec2FaceåŒ…æ‹¬ç‰¹å¾æ©ç è‡ªç¼–ç å™¨å’Œå›¾åƒè§£ç å™¨ï¼Œé€šè¿‡è„¸éƒ¨å›¾åƒé‡å»ºè¿›è¡Œç›‘ç£ï¼Œä¾¿äºæ¨ç†ä½¿ç”¨ã€‚Vec2Faceèƒ½å¤Ÿç”Ÿæˆå…·æœ‰è‰¯å¥½åˆ†ç¦»åº¦çš„èº«ä»½ï¼Œå¹¶ä½¿ç”¨éšæœºæ‰°åŠ¨è¾“å…¥èº«ä»½å‘é‡åœ¨è¾ƒå°çš„èŒƒå›´å†…ç”Ÿæˆå…·æœ‰é€‚å½“å±æ€§å˜åŒ–çš„è„¸éƒ¨å›¾åƒã€‚æ­¤å¤–ï¼Œé€šè¿‡è°ƒæ•´å‘é‡å€¼ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼Œå¯ä»¥ç”Ÿæˆå…·æœ‰æŒ‡å®šå±æ€§çš„å›¾åƒã€‚Vec2Faceå·²æˆåŠŸåˆæˆå¤šè¾¾30ä¸‡ä¸ªèº«ä»½ï¼Œè€Œä¹‹å‰çš„å·¥ä½œæœ€å¤šåªèƒ½åˆ›å»º6ä¸‡ä¸ªèº«ä»½ã€‚ä½¿ç”¨ç”Ÿæˆçš„HSFaceæ•°æ®é›†è®­ç»ƒçš„é¢éƒ¨è¯†åˆ«æ¨¡å‹åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œåœ¨æŸäº›æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®æ€§ç”šè‡³è¶…è¿‡äº†ä½¿ç”¨ç›¸åŒè§„æ¨¡çš„çœŸå®é¢éƒ¨å›¾åƒè®­ç»ƒé›†è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ç ”ç©¶åˆæˆéçœŸå®äººç‰©è„¸éƒ¨å›¾åƒä»¥åˆ›å»ºæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒè„¸éƒ¨è¯†åˆ«æ¨¡å‹ã€‚</li>
<li>è®ºæ–‡æ—¨åœ¨ç”Ÿæˆå¤§é‡ä¸åŒçš„èº«ä»½ï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½çš„åˆ†ç¦»åº¦ï¼Œå¹¶ä¸ºæ¯ä¸ªèº«ä»½æä¾›é€‚å½“çš„å¤–è§‚å˜åŒ–ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVec2Faceçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨é‡‡æ ·å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå¯ä»¥çµæ´»ç”Ÿæˆå’Œæ§åˆ¶è„¸éƒ¨å›¾åƒçš„å±æ€§å’Œèº«ä»½ã€‚</li>
<li>Vec2FaceåŒ…æ‹¬ç‰¹å¾æ©ç è‡ªç¼–ç å™¨å’Œå›¾åƒè§£ç å™¨ï¼Œé€šè¿‡è„¸éƒ¨å›¾åƒé‡å»ºè¿›è¡Œç›‘ç£ã€‚</li>
<li>é€šè¿‡éšæœºæ‰°åŠ¨è¾“å…¥èº«ä»½å‘é‡ï¼ŒVec2Faceå¯ä»¥ç”Ÿæˆå…·æœ‰é€‚å½“å±æ€§å˜åŒ–çš„åŒä¸€èº«ä»½çš„è„¸éƒ¨å›¾åƒã€‚</li>
<li>Vec2Faceèƒ½å¤Ÿç”Ÿæˆå…·æœ‰æŒ‡å®šå±æ€§çš„å›¾åƒï¼Œè¿™æœ‰åŠ©äºåœ¨è®­ç»ƒä¸­å¼•å…¥æ›´å¤šå˜åŒ–å’Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7bd8ca65382486e44524a7344fb2572c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8bf6d71eba88ca7dffe1171602f16acf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b67ee4e3b0155055789b5859155e61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16440a532ec98c1bec06dcd53ad12454.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CT-AGRG-Automated-Abnormality-Guided-Report-Generation-from-3D-Chest-CT-Volumes"><a href="#CT-AGRG-Automated-Abnormality-Guided-Report-Generation-from-3D-Chest-CT-Volumes" class="headerlink" title="CT-AGRG: Automated Abnormality-Guided Report Generation from 3D Chest CT   Volumes"></a>CT-AGRG: Automated Abnormality-Guided Report Generation from 3D Chest CT   Volumes</h2><p><strong>Authors:Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel</strong></p>
<p>The rapid increase of computed tomography (CT) scans and their time-consuming manual analysis have created an urgent need for robust automated analysis techniques in clinical settings. These aim to assist radiologists and help them managing their growing workload. Existing methods typically generate entire reports directly from 3D CT images, without explicitly focusing on observed abnormalities. This unguided approach often results in repetitive content or incomplete reports, failing to prioritize anomaly-specific descriptions. We propose a new anomaly-guided report generation model, which first predicts abnormalities and then generates targeted descriptions for each. Evaluation on a public dataset demonstrates significant improvements in report quality and clinical relevance. We extend our work by conducting an ablation study to demonstrate its effectiveness. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ‰«æçš„è¿…é€Ÿå¢åŠ åŠå…¶è€—æ—¶çš„æ‰‹åŠ¨åˆ†æï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸­ç¨³å¥çš„è‡ªåŠ¨åŒ–åˆ†ææŠ€æœ¯åˆ›é€ äº†è¿«åˆ‡çš„éœ€æ±‚ã€‚è¿™äº›æŠ€æœ¯çš„ç›®æ ‡æ˜¯ååŠ©æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œå¹¶å¸®åŠ©ä»–ä»¬åº”å¯¹æ—¥ç›Šå¢é•¿çš„å·¥ä½œé‡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥ä»3D CTå›¾åƒç”Ÿæˆæ•´ä¸ªæŠ¥å‘Šï¼Œè€Œæ²¡æœ‰æ˜ç¡®å…³æ³¨è§‚å¯Ÿåˆ°çš„å¼‚å¸¸ã€‚è¿™ç§æ— å¯¼å‘çš„æ–¹æ³•å¸¸å¸¸å¯¼è‡´å†…å®¹é‡å¤æˆ–æŠ¥å‘Šä¸å®Œæ•´ï¼Œæ— æ³•ä¼˜å…ˆæä¾›å¼‚å¸¸ç‰¹å®šçš„æè¿°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¼‚å¸¸å¯¼å‘çš„æŠ¥å‘Šç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é¦–å…ˆé¢„æµ‹å¼‚å¸¸ï¼Œç„¶åä¸ºæ¯ä¸€ä¸ªç”Ÿæˆæœ‰é’ˆå¯¹æ€§çš„æè¿°ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æŠ¥å‘Šè´¨é‡å’Œä¸´åºŠç›¸å…³æ€§æ–¹é¢æœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚æˆ‘ä»¬é€šè¿‡è¿›è¡Œæ¶ˆèç ”ç©¶æ¥è¿›ä¸€æ­¥è¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11965v6">PDF</a> Paper accepted to ISBI 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†è®¡ç®—å±‚ææ‰«æï¼ˆCTï¼‰æ‰«æçš„å¿«é€Ÿå¢é•¿åŠå…¶è€—æ—¶çš„äººå·¥åˆ†ææ‰€å¸¦æ¥çš„é—®é¢˜ï¼Œå‡¸æ˜¾äº†å¯¹ä¸´åºŠç¯å¢ƒä¸­ç¨³å¥è‡ªåŠ¨åŒ–åˆ†ææŠ€æœ¯çš„è¿«åˆ‡éœ€æ±‚ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥ä»3D CTå›¾åƒç”Ÿæˆæ•´ä¸ªæŠ¥å‘Šï¼Œæ²¡æœ‰ç‰¹åˆ«å¼ºè°ƒè§‚å¯Ÿåˆ°çš„å¼‚å¸¸ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼‚å¸¸å¼•å¯¼æŠ¥å‘Šç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é¦–å…ˆé¢„æµ‹å¼‚å¸¸ï¼Œç„¶åé’ˆå¯¹æ¯ä¸ªå¼‚å¸¸ç”Ÿæˆæœ‰é’ˆå¯¹æ€§çš„æè¿°ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†è¯¥æ¨¡å‹åœ¨æé«˜æŠ¥å‘Šè´¨é‡å’Œä¸´åºŠç›¸å…³æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚é€šè¿‡è¿›è¡Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CTæ‰«æçš„å¹¿æ³›åº”ç”¨åŠå…¶è€—æ—¶çš„äººå·¥åˆ†æå‡¸æ˜¾äº†å¯¹è‡ªåŠ¨åŒ–åˆ†ææŠ€æœ¯çš„éœ€æ±‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç›´æ¥ä»3D CTå›¾åƒç”ŸæˆæŠ¥å‘Šï¼Œç¼ºä¹æ˜ç¡®çš„å¼‚å¸¸è¯†åˆ«ã€‚</li>
<li>æ–°çš„å¼‚å¸¸å¼•å¯¼æŠ¥å‘Šç”Ÿæˆæ¨¡å‹å…ˆé¢„æµ‹å¼‚å¸¸ï¼Œå†é’ˆå¯¹æ¯ä¸ªå¼‚å¸¸ç”Ÿæˆæè¿°ã€‚</li>
<li>æå‡ºçš„æ¨¡å‹åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒæŠ¥å‘Šè´¨é‡å’Œä¸´åºŠç›¸å…³æ€§æ˜¾è‘—æé«˜ã€‚</li>
<li>æ¨¡å‹é€šè¿‡æ¶ˆèç ”ç©¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ¨¡å‹æœ‰åŠ©äºå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e72dcab730cae53328f22d48c0655b89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac79443920f63d036961d0395423cd7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31ad4d276bd8edd9a8bb7b6674aebf91.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="multiGradICON-A-Foundation-Model-for-Multimodal-Medical-Image-Registration"><a href="#multiGradICON-A-Foundation-Model-for-Multimodal-Medical-Image-Registration" class="headerlink" title="multiGradICON: A Foundation Model for Multimodal Medical Image   Registration"></a>multiGradICON: A Foundation Model for Multimodal Medical Image   Registration</h2><p><strong>Authors:Basar Demir, Lin Tian, Thomas Hastings Greer, Roland Kwitt, Francois-Xavier Vialard, Raul San Jose Estepar, Sylvain Bouix, Richard Jarrett Rushmore, Ebrahim Ebrahim, Marc Niethammer</strong></p>
<p>Modern medical image registration approaches predict deformations using deep networks. These approaches achieve state-of-the-art (SOTA) registration accuracy and are generally fast. However, deep learning (DL) approaches are, in contrast to conventional non-deep-learning-based approaches, anatomy-specific. Recently, a universal deep registration approach, uniGradICON, has been proposed. However, uniGradICON focuses on monomodal image registration. In this work, we therefore develop multiGradICON as a first step towards universal <em>multimodal</em> medical image registration. Specifically, we show that 1) we can train a DL registration model that is suitable for monomodal <em>and</em> multimodal registration; 2) loss function randomization can increase multimodal registration accuracy; and 3) training a model with multimodal data helps multimodal generalization. Our code and the multiGradICON model are available at <a target="_blank" rel="noopener" href="https://github.com/uncbiag/uniGradICON">https://github.com/uncbiag/uniGradICON</a>. </p>
<blockquote>
<p>ç°ä»£åŒ»å­¦å›¾åƒé…å‡†æ–¹æ³•åˆ©ç”¨æ·±åº¦ç½‘ç»œæ¥é¢„æµ‹å˜å½¢ã€‚è¿™äº›æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é…å‡†ç²¾åº¦ï¼Œå¹¶ä¸”é€šå¸¸é€Ÿåº¦å¾ˆå¿«ã€‚ç„¶è€Œï¼Œä¸ä¼ ç»Ÿçš„éæ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•å…·æœ‰é’ˆå¯¹ç‰¹å®šè§£å‰–ç»“æ„çš„ç‰¹ç‚¹ã€‚æœ€è¿‘ï¼Œå·²ç»æå‡ºäº†ä¸€ç§é€šç”¨çš„æ·±åº¦é…å‡†æ–¹æ³•uniGradICONã€‚ç„¶è€Œï¼ŒuniGradICONä¸“æ³¨äºå•æ¨¡æ€å›¾åƒé…å‡†ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†multiGradICONï¼Œä½œä¸ºé€šç”¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒé…å‡†çš„ç¬¬ä¸€æ­¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº†ä»¥ä¸‹å‡ ç‚¹ï¼š1ï¼‰æˆ‘ä»¬å¯ä»¥è®­ç»ƒä¸€ä¸ªæ—¢é€‚ç”¨äºå•æ¨¡æ€åˆé€‚ç”¨äºå¤šæ¨¡æ€é…å‡†çš„æ·±åº¦å­¦ä¹ é…å‡†æ¨¡å‹ï¼›2ï¼‰æŸå¤±å‡½æ•°éšæœºåŒ–å¯ä»¥æé«˜å¤šæ¨¡æ€é…å‡†çš„ç²¾åº¦ï¼›3ï¼‰ä½¿ç”¨å¤šæ¨¡æ€æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒæœ‰åŠ©äºå¤šæ¨¡æ€æ³›åŒ–ã€‚æˆ‘ä»¬çš„ä»£ç å’ŒmultiGradICONæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/uncbiag/uniGradICON">https://github.com/uncbiag/uniGradICON</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.00221v2">PDF</a> </p>
<p><strong>Summary</strong><br>ç°ä»£åŒ»ç–—å›¾åƒé…å‡†æ–¹æ³•ä½¿ç”¨æ·±åº¦ç½‘ç»œé¢„æµ‹å˜å½¢ï¼Œè¾¾åˆ°æœ€æ–°é…å‡†ç²¾åº¦ä¸”é€šå¸¸å¿«é€Ÿã€‚ç„¶è€Œï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•ä¸ä¼ ç»Ÿéæ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸åï¼Œå…·æœ‰ç‰¹å®šè§£å‰–ç»“æ„çš„ç‰¹ç‚¹ã€‚æœ€è¿‘æå‡ºäº†é€šç”¨æ·±åº¦é…å‡†æ–¹æ³•uniGradICONï¼Œä½†ä¸»è¦å…³æ³¨å•æ¨¡æ€å›¾åƒé…å‡†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†multiGradICONä½œä¸ºå‘é€šç”¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒé…å‡†è¿ˆå‡ºçš„ç¬¬ä¸€æ­¥ã€‚æœ¬å·¥ä½œè¯æ˜äº†ï¼š1ï¼‰æˆ‘ä»¬å¯ä»¥è®­ç»ƒä¸€ä¸ªé€‚ç”¨äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€é…å‡†çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼›2ï¼‰æŸå¤±å‡½æ•°éšæœºåŒ–å¯ä»¥æé«˜å¤šæ¨¡æ€é…å‡†ç²¾åº¦ï¼›3ï¼‰ä½¿ç”¨å¤šæ¨¡æ€æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒæœ‰åŠ©äºå¤šæ¨¡æ€æ³›åŒ–ã€‚æˆ‘ä»¬çš„ä»£ç å’ŒmultiGradICONæ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°ä»£åŒ»ç–—å›¾åƒé…å‡†æ–¹æ³•ä½¿ç”¨æ·±åº¦ç½‘ç»œæé«˜é…å‡†ç²¾åº¦å’Œé€Ÿåº¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œå…·æœ‰ç‰¹å®šè§£å‰–ç»“æ„çš„ç‰¹ç‚¹ã€‚</li>
<li>uniGradICONæ–¹æ³•ä¸»è¦å…³æ³¨å•æ¨¡æ€å›¾åƒé…å‡†ã€‚</li>
<li>multiGradICONä½œä¸ºé€šç”¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒé…å‡†çš„ç¬¬ä¸€æ­¥è¢«å¼€å‘å‡ºæ¥ã€‚</li>
<li>æœ¬ç ”ç©¶è¯æ˜å¯ä»¥è®­ç»ƒä¸€ä¸ªé€‚ç”¨äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€é…å‡†çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>æŸå¤±å‡½æ•°éšæœºåŒ–èƒ½æé«˜å¤šæ¨¡æ€é…å‡†ç²¾åº¦ã€‚</li>
<li>ä½¿ç”¨å¤šæ¨¡æ€æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒæœ‰åŠ©äºå¤šæ¨¡æ€æ³›åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.00221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1681798caece933c0cceb000845848f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80d477c5acc001e4b972fafa3df9fdcd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e57e82a683c2385c1e782b96371c85d9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leveraging-Bi-Focal-Perspectives-and-Granular-Feature-Integration-for-Accurate-Reliable-Early-Alzheimerâ€™s-Detection"><a href="#Leveraging-Bi-Focal-Perspectives-and-Granular-Feature-Integration-for-Accurate-Reliable-Early-Alzheimerâ€™s-Detection" class="headerlink" title="Leveraging Bi-Focal Perspectives and Granular Feature Integration for   Accurate Reliable Early Alzheimerâ€™s Detection"></a>Leveraging Bi-Focal Perspectives and Granular Feature Integration for   Accurate Reliable Early Alzheimerâ€™s Detection</h2><p><strong>Authors:Pandiyaraju V, Shravan Venkatraman, Abeshek A, Pavan Kumar S, Aravintakshan S A</strong></p>
<p>Being the most commonly known neurodegeneration, Alzheimerâ€™s Disease (AD) is annually diagnosed in millions of patients. The present medical scenario still finds the exact diagnosis and classification of AD through neuroimaging data as a challenging task. Traditional CNNs can extract a good amount of low-level information in an image while failing to extract high-level minuscule particles, which is a significant challenge in detecting AD from MRI scans. To overcome this, we propose a novel Granular Feature Integration method to combine information extraction at different scales along with an efficient information flow, enabling the model to capture both broad and fine-grained features simultaneously. We also propose a Bi-Focal Perspective mechanism to highlight the subtle neurofibrillary tangles and amyloid plaques in the MRI scans, ensuring that critical pathological markers are accurately identified. Our model achieved an F1-Score of 99.31%, precision of 99.24%, and recall of 99.51%. These scores prove that our model is significantly better than the state-of-the-art (SOTA) CNNs in existence. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯æœ€å¸¸è§çš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…ä¹‹ä¸€ï¼Œæ¯å¹´æœ‰æ•°ç™¾ä¸‡æ‚£è€…è¢«è¯Šæ–­å‡ºæ‚£æœ‰æ­¤ç—…ã€‚å½“å‰åŒ»å­¦ç•Œé€šè¿‡ç¥ç»æˆåƒæ•°æ®å¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…è¿›è¡Œç¡®åˆ‡è¯Šæ–­å’Œåˆ†ç±»ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯ä»¥åœ¨å›¾åƒä¸­æå–å¤§é‡çš„ä½çº§ä¿¡æ¯ï¼Œä½†æ— æ³•æå–é«˜çº§å¾®å°é¢—ç²’ï¼Œè¿™å¯¹äºä»MRIæ‰«æä¸­æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…æ¥è¯´æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç²’åº¦ç‰¹å¾é›†æˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç»“åˆä¸åŒå°ºåº¦çš„ä¿¡æ¯æå–å’Œé«˜æ•ˆçš„ä¿¡æ¯æµï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶æ•è·å¹¿æ³›å’Œç²¾ç»†çš„ç‰¹å¾ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŒç„¦ç‚¹é€è§†æœºåˆ¶ï¼Œä»¥çªå‡ºMRIæ‰«æä¸­çš„ç¥ç»åŸçº¤ç»´ç¼ ç»“å’Œæ·€ç²‰æ ·æ–‘å—ç­‰ç»†å¾®ä¹‹å¤„ï¼Œç¡®ä¿å‡†ç¡®è¯†åˆ«å…³é”®ç—…ç†æ ‡è®°ç‰©ã€‚æˆ‘ä»¬çš„æ¨¡å‹å–å¾—äº†F1åˆ†æ•°ä¸º99.31%ï¼Œç²¾ç¡®åº¦ä¸º99.24%ï¼Œå¬å›ç‡ä¸º99.51%çš„æˆç»©ã€‚è¿™äº›åˆ†æ•°è¯æ˜æˆ‘ä»¬çš„æ¨¡å‹æ˜æ˜¾ä¼˜äºç›®å‰å­˜åœ¨çš„æœ€å…ˆè¿›çš„CNNæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10921v6">PDF</a> 14 pages, 12 figures, 6 tables</p>
<p><strong>Summary</strong><br>     é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„ç¥ç»å½±åƒè¯Šæ–­ä»å…·æŒ‘æˆ˜ã€‚ä¼ ç»ŸCNNéš¾ä»¥æå–é«˜çº§å¾®å°é¢—ç²’ä¿¡æ¯ï¼Œæ–°å‹Granular Feature Integrationæ–¹æ³•ç»“åˆä¸åŒå°ºåº¦ä¿¡æ¯æå–ï¼ŒåŒæ—¶æ•æ‰å¹¿æ³›ä¸ç²¾ç»†ç‰¹å¾ã€‚Bi-Focal Perspectiveæœºåˆ¶èƒ½å‡†ç¡®è¯†åˆ«ç¥ç»çº¤ç»´ç¼ ç»“å’Œæ·€ç²‰æ ·æ–‘å—ç­‰å…³é”®ç—…ç†æ ‡è®°ç‰©ã€‚æ¨¡å‹å®ç°F1åˆ†æ•°99.31%ï¼Œç²¾åº¦99.24%ï¼Œå¬å›ç‡99.51%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„ç¥ç»å½±åƒè¯Šæ–­ä»æ˜¯åŒ»å­¦ç•Œçš„ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»ŸCNNåœ¨æå–é«˜çº§å¾®å°é¢—ç²’ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æ–°å‹Granular Feature Integrationæ–¹æ³•èƒ½å¤Ÿç»“åˆä¸åŒå°ºåº¦çš„ä¿¡æ¯æå–ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>Bi-Focal Perspectiveæœºåˆ¶èƒ½çªå‡ºæ˜¾ç¤ºMRIæ‰«æä¸­çš„ç»†å¾®ç¥ç»çº¤ç»´ç¼ ç»“å’Œæ·€ç²‰æ ·æ–‘å—ã€‚</li>
<li>è¯¥æ¨¡å‹çš„F1åˆ†æ•°è¾¾åˆ°99.31%ï¼Œç²¾åº¦å’Œå¬å›ç‡å‡è¶…è¿‡99%ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ˆSOTAï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.10921">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6cb755f14702c130691fe5d8eef7beab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39fef78c92f9c9e47b43c79af0261ab2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2de55f6b50cc4f1be34dddf67099914.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf218884d9171b85d27f244a06bc1cae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d85f5bf773ab563b2a84f3e0fd4f97e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ac2f6f0097e62acdffb0e056f8eb7b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a4cff75da89c44c3abe8e54c97f1bcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b1f2a66b77381eb475389ff8beb5e86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4be07e5eb47169df5eea32645d6c9dde.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Exploring-scalable-medical-image-encoders-beyond-text-supervision"><a href="#Exploring-scalable-medical-image-encoders-beyond-text-supervision" class="headerlink" title="Exploring scalable medical image encoders beyond text supervision"></a>Exploring scalable medical image encoders beyond text supervision</h2><p><strong>Authors:Fernando PÃ©rez-GarcÃ­a, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Matthew P. Lungren, Maria Teodora Wetscherek, Noel Codella, Stephanie L. Hyland, Javier Alvarez-Valle, Ozan Oktay</strong></p>
<p>Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains. However, the computed features are limited by the information contained in the text, which is particularly problematic in medical imaging, where the findings described by radiologists focus on specific observations. This challenge is compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information. In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general-purpose biomedical imaging encoders. We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language-supervised models on a diverse range of benchmarks. Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images). To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports. Finally, we conduct a series of ablations determining the factors in RAD-DINOâ€™s performance; notably, we observe that RAD-DINOâ€™s downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder. Model weights of RAD-DINO trained on publicly available datasets are available at <a target="_blank" rel="noopener" href="https://huggingface.co/microsoft/rad-dino">https://huggingface.co/microsoft/rad-dino</a>. </p>
<blockquote>
<p>è¯­è¨€ç›‘ç£çš„é¢„è®­ç»ƒå·²è¢«è¯æ˜æ˜¯ä»å›¾åƒä¸­æå–è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç‰¹å¾çš„ä¸€ç§æœ‰ä»·å€¼çš„æ–¹æ³•ï¼Œä½œä¸ºè®¡ç®—æœºè§†è§‰å’ŒåŒ»å­¦æˆåƒé¢†åŸŸä¸­çš„å¤šæ¨¡æ€ç³»ç»Ÿçš„åŸºç¡€å…ƒç´ ã€‚ç„¶è€Œï¼Œè®¡ç®—å‡ºçš„ç‰¹å¾å—åˆ°æ–‡æœ¬ä¸­æ‰€åŒ…å«ä¿¡æ¯çš„é™åˆ¶ï¼Œè¿™åœ¨åŒ»å­¦æˆåƒä¸­å°¤å…¶æˆé—®é¢˜ï¼Œå› ä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæ‰€æè¿°çš„å‘ç°ä¸»è¦å…³æ³¨ç‰¹å®šçš„è§‚å¯Ÿç»“æœã€‚è¿™ä¸€æŒ‘æˆ˜è¿˜å› é…å¯¹æˆåƒ-æ–‡æœ¬æ•°æ®çš„ç¨€ç¼ºè€ŒåŠ å‰§ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ‹…å¿ƒæ³„éœ²ä¸ªäººå¥åº·ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»æ ¹æœ¬ä¸Šè´¨ç–‘å¯¹è¯­è¨€ç›‘ç£çš„æ™®éä¾èµ–ï¼Œä»¥å­¦ä¹ é€šç”¨ç”Ÿç‰©åŒ»å­¦æˆåƒç¼–ç å™¨ã€‚æˆ‘ä»¬å¼•å…¥äº†RAD-DINOï¼Œè¿™æ˜¯ä¸€ç§ä»…åœ¨ä¸€ç»´ç”Ÿç‰©åŒ»å­¦æˆåƒæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ç”Ÿç‰©åŒ»å­¦å›¾åƒç¼–ç å™¨ï¼Œå®ƒåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æˆ–è¶…è¶Šäº†æœ€æ–°ç”Ÿç‰©åŒ»å­¦è¯­è¨€ç›‘ç£æ¨¡å‹çš„è¡¨ç°ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡åœ¨æ ‡å‡†æˆåƒä»»åŠ¡ï¼ˆåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ï¼‰å’Œè§†è§‰è¯­è¨€å¯¹é½ä»»åŠ¡ï¼ˆæ ¹æ®å›¾åƒç”Ÿæˆæ–‡æœ¬æŠ¥å‘Šï¼‰ä¸Šè¯„ä¼°æ‰€å­¦è¡¨ç¤ºçš„è´¨é‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯æ˜è¯­è¨€ç›‘ç£çš„å±€é™æ€§ï¼Œæˆ‘ä»¬å±•ç¤ºäº†RAD-DINOçš„ç‰¹å¾ä¸å…¶ä»–åŒ»ç–—è®°å½•ï¼ˆå¦‚æ€§åˆ«æˆ–å¹´é¾„ï¼‰çš„ç›¸å…³æ€§ä¼˜äºè¯­è¨€ç›‘ç£æ¨¡å‹ï¼Œè¿™äº›é€šå¸¸ä¸ä¼šåœ¨æ”¾å°„å­¦æŠ¥å‘Šä¸­è¢«æåŠã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—ç¡®å®šRAD-DINOæ€§èƒ½å› ç´ çš„å®éªŒï¼›å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°RAD-DINOçš„ä¸‹æ¸¸æ€§èƒ½éšç€è®­ç»ƒæ•°æ®æ•°é‡å’Œå¤šæ ·æ€§çš„å¢åŠ è€Œè¡¨ç°è‰¯å¥½ï¼Œè¿™è¡¨æ˜ä»…ä½¿ç”¨å›¾åƒç›‘ç£æ˜¯ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•æ¥è®­ç»ƒåŸºç¡€ç”Ÿç‰©åŒ»å­¦å›¾åƒç¼–ç å™¨ã€‚RAD-DINOåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/microsoft/rad-dino">https://huggingface.co/microsoft/rad-dino</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.10815v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸï¼Œé€šè¿‡çº¯å›¾åƒé¢„è®­ç»ƒæ–¹æ³•ï¼ˆRAD-DINOï¼‰æ¥æå–å›¾åƒç‰¹å¾ï¼Œå…¶æ€§èƒ½å¯ä»¥è¾¾åˆ°ç”šè‡³è¶…è¶Šä¾èµ–äºè¯­è¨€ç›‘ç£çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚RAD-DINOèƒ½å¤Ÿåœ¨æ ‡å‡†æˆåƒä»»åŠ¡ï¼ˆåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ä»¥åŠè·¨æ¨¡æ€ä»»åŠ¡ï¼ˆä»å›¾åƒç”Ÿæˆæ–‡æœ¬æŠ¥å‘Šï¼‰ä¸­è¡¨ç°ä¼˜ç§€ã€‚ç›¸è¾ƒäºè¯­è¨€ç›‘ç£æ¨¡å‹ï¼ŒRAD-DINOçš„ç‰¹å¾èƒ½æ›´å¥½åœ°ä¸å…¶ä»–åŒ»ç–—è®°å½•ï¼ˆå¦‚æ€§åˆ«ã€å¹´é¾„ç­‰ï¼‰ç›¸å…³è”ã€‚æ­¤å¤–ï¼ŒRAD-DINOçš„è®­ç»ƒæ€§èƒ½å¯é€šè¿‡å¢åŠ è®­ç»ƒæ•°æ®é‡å’Œå¤šæ ·æ€§æ¥è¿›ä¸€æ­¥æå‡ï¼Œè¯æ˜äº†çº¯å›¾åƒç›‘ç£æ˜¯ä¸€ç§å¯æ‰©å±•çš„åŒ»å­¦å›¾åƒåŸºç¡€ç¼–ç å™¨è®­ç»ƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€ç›‘ç£é¢„è®­ç»ƒåœ¨åŒ»å­¦å›¾åƒç‰¹å¾æå–ä¸­å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ³¨é‡ç‰¹å®šè§‚å¯Ÿçš„åŒ»å­¦æˆåƒé¢†åŸŸã€‚</li>
<li>RAD-DINOæ˜¯ä¸€ä¸ªçº¯å›¾åƒé¢„è®­ç»ƒçš„åŒ»å­¦å›¾åƒç¼–ç å™¨ï¼Œèƒ½åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¶Šå…ˆè¿›è¯­è¨€ç›‘ç£æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>RAD-DINOå­¦åˆ°çš„è¡¨ç¤ºè´¨é‡åœ¨æ ‡å‡†æˆåƒä»»åŠ¡ï¼ˆåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ä¸Šå¾—åˆ°è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è·¨æ¨¡æ€ä»»åŠ¡ï¼ˆä»å›¾åƒç”Ÿæˆæ–‡æœ¬æŠ¥å‘Šï¼‰ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸è¯­è¨€ç›‘ç£æ¨¡å‹ç›¸æ¯”ï¼ŒRAD-DINOçš„ç‰¹å¾èƒ½æ›´å¥½åœ°ä¸å…¶ä»–åŒ»ç–—è®°å½•ï¼ˆå¦‚æ€§åˆ«ã€å¹´é¾„ï¼‰å…³è”ã€‚</li>
<li>RAD-DINOçš„è®­ç»ƒæ€§èƒ½å¯é€šè¿‡å¢åŠ è®­ç»ƒæ•°æ®é‡å’Œå¤šæ ·æ€§æ¥æå‡ï¼Œè¯æ˜äº†çº¯å›¾åƒç›‘ç£çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>RAD-DINOæ¨¡å‹æƒé‡å¯åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶å¯åœ¨huggingface.co&#x2F;microsoft&#x2F;rad-dinoè·å–ã€‚</li>
<li>è¯¥ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—æ¶ˆèå®éªŒç¡®å®šäº†RAD-DINOæ€§èƒ½çš„å…³é”®å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.10815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e485deb1ecf6ab6442d5400cad884a0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c98567d71c2a82688f23ddb2ccd9eef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c33078168e75d8a43b69c51cb3c8fad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ba7cb63a246cd84ab24def442b4ddc8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3f2dbb7b9fc4ae0a1234f5ee88840144.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-12  ReasonFlux Hierarchical LLM Reasoning via Scaling Thought Templates
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-11/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-79770b626976880b93ff148925e8c9de.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-11  Hummingbird High Fidelity Image Generation via Multimodal Context   Alignment
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
