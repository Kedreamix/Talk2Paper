<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  CTC-DRO Robust Optimization for Reducing Language Disparities in Speech   Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-23dc622292d3dd146517fa200f623d53.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    53 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-06-æ›´æ–°"><a href="#2025-02-06-æ›´æ–°" class="headerlink" title="2025-02-06 æ›´æ–°"></a>2025-02-06 æ›´æ–°</h1><h2 id="CTC-DRO-Robust-Optimization-for-Reducing-Language-Disparities-in-Speech-Recognition"><a href="#CTC-DRO-Robust-Optimization-for-Reducing-Language-Disparities-in-Speech-Recognition" class="headerlink" title="CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech   Recognition"></a>CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech   Recognition</h2><p><strong>Authors:Martijn Bartelds, Ananjan Nandi, Moussa Koulako Bala Doumbouya, Dan Jurafsky, Tatsunori Hashimoto, Karen Livescu</strong></p>
<p>Modern deep learning models often achieve high overall performance, but consistently fail on specific subgroups. Group distributionally robust optimization (group DRO) addresses this problem by minimizing the worst-group loss, but it fails when group losses misrepresent performance differences between groups. This is common in domains like speech, where the widely used connectionist temporal classification (CTC) loss scales with input length and varies with linguistic and acoustic properties, leading to spurious differences between group losses. We present CTC-DRO, which addresses the shortcomings of the group DRO objective by smoothing the group weight update to prevent overemphasis on consistently high-loss groups, while using input length-matched batching to mitigate CTCâ€™s scaling issues. We evaluate CTC-DRO on the task of multilingual automatic speech recognition (ASR) across five language sets from the ML-SUPERB 2.0 benchmark. CTC-DRO consistently outperforms group DRO and CTC-based baseline models, reducing the worst-language error by up to 65.9% and the average error by up to 47.7%. CTC-DRO can be applied to ASR with minimal computational costs, and offers the potential for reducing group disparities in other domains with similar challenges. </p>
<blockquote>
<p>ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸æ€»ä½“æ€§èƒ½è¾ƒé«˜ï¼Œä½†åœ¨ç‰¹å®šå­ç¾¤ä½“ä¸Šå§‹ç»ˆå­˜åœ¨ç¼ºé™·ã€‚ç¾¤ä½“åˆ†å¸ƒé²æ£’ä¼˜åŒ–ï¼ˆgroup DROï¼‰é€šè¿‡æœ€å°åŒ–æœ€ç³Ÿç³•ç¾¤ä½“æŸå¤±æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†åœ¨ç¾¤ä½“æŸå¤±æ— æ³•çœŸå®åæ˜ ç¾¤ä½“ä¹‹é—´æ€§èƒ½å·®å¼‚æ—¶ï¼Œå®ƒä¼šå¤±æ•ˆã€‚è¿™åœ¨è¯­éŸ³ç­‰é¢†åŸŸå¾ˆå¸¸è§ï¼Œå¹¿æ³›ä½¿ç”¨çš„è¿æ¥å®šæ—¶åˆ†ç±»ï¼ˆCTCï¼‰æŸå¤±ä¸è¾“å…¥é•¿åº¦æˆæ¯”ä¾‹ï¼Œå¹¶éšè¯­è¨€å’Œå£°å­¦å±æ€§è€Œå˜åŒ–ï¼Œå¯¼è‡´ç¾¤ä½“æŸå¤±ä¹‹é—´å­˜åœ¨è™šå‡å·®å¼‚ã€‚æˆ‘ä»¬æå‡ºäº†CTC-DROï¼Œå®ƒé€šè¿‡å¹³æ»‘ç¾¤ä½“æƒé‡æ›´æ–°ï¼Œé˜²æ­¢è¿‡åº¦å…³æ³¨å§‹ç»ˆé«˜æŸå¤±çš„ç¾¤ä½“ï¼ŒåŒæ—¶ä½¿ç”¨ä¸è¾“å…¥é•¿åº¦åŒ¹é…çš„æ‰¹å¤„ç†æ¥ç¼“è§£CTCçš„ç¼©æ”¾é—®é¢˜ï¼Œæ¥è§£å†³group DROç›®æ ‡ä¸­çš„ä¸è¶³ã€‚æˆ‘ä»¬åœ¨ML-SUPERB 2.0åŸºå‡†æµ‹è¯•çš„äº”å¥—è¯­è¨€é›†ä¸Šè¯„ä¼°äº†CTC-DROåœ¨å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚CTC-DROå§‹ç»ˆä¼˜äºgroup DROå’ŒCTCåŸºå‡†æ¨¡å‹ï¼Œå°†æœ€å·®è¯­è¨€é”™è¯¯ç‡é™ä½äº†é«˜è¾¾65.9%ï¼Œå¹³å‡é”™è¯¯ç‡é™ä½äº†é«˜è¾¾47.7%ã€‚CTC-DROå¯ä»¥æœ€ä½çš„è®¡ç®—æˆæœ¬åº”ç”¨äºASRï¼Œå¹¶æœ‰æœ›åœ¨é¢ä¸´ç±»ä¼¼æŒ‘æˆ˜çš„å…¶ä»–é¢†åŸŸå‡å°‘ç¾¤ä½“å·®å¼‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01777v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç‰¹å®šåˆ†ç»„ä¸Šå¸¸å¸¸è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†é›†å›¢åˆ†å¸ƒç¨³å¥ä¼˜åŒ–ï¼ˆGroup DROï¼‰æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–æœ€ç³Ÿç³•é›†å›¢çš„æŸå¤±æ¥æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“é›†å›¢æŸå¤±ä¸èƒ½å‡†ç¡®åæ˜ é›†å›¢é—´æ€§èƒ½å·®å¼‚æ—¶ï¼Œè¿™ç§æ–¹æ³•å°±ä¼šå¤±æ•ˆã€‚åœ¨è¯­éŸ³é¢†åŸŸï¼Œè¿æ¥æ—¶æ€åˆ†ç±»ï¼ˆCTCï¼‰æŸå¤±çš„å¹¿æ³›ä½¿ç”¨å› è¾“å…¥é•¿åº¦å’Œè¯­è¨€ç‰¹æ€§è€Œé€ æˆè™šå‡å·®å¼‚ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†CTC-DROæ–¹æ³•ã€‚å®ƒé€šè¿‡å¯¹é›†å›¢æƒé‡æ›´æ–°è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œé˜²æ­¢è¿‡åº¦å…³æ³¨æŒç»­é«˜æŸå¤±çš„é›†å›¢ï¼Œå¹¶ä½¿ç”¨ä¸è¾“å…¥é•¿åº¦åŒ¹é…çš„æ‰¹å¤„ç†æ¥ç¼“è§£CTCçš„ç¼©æ”¾é—®é¢˜ã€‚åœ¨ML-SUPERB 2.0åŸºå‡†çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸Šè¯„ä¼°æ˜¾ç¤ºï¼ŒCTC-DROåœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºé›†å›¢DROå’ŒåŸºäºCTCçš„åŸºçº¿æ¨¡å‹ï¼Œå°†æœ€å·®è¯­è¨€é”™è¯¯ç‡é™ä½äº†é«˜è¾¾65.9%ï¼Œå¹³å‡é”™è¯¯ç‡é™ä½äº†é«˜è¾¾47.7%ã€‚CTC-DROå¯ä»¥åº”ç”¨äºASRï¼Œä¸”è®¡ç®—æˆæœ¬ä½ï¼Œå¯¹äºå­˜åœ¨ç±»ä¼¼æŒ‘æˆ˜çš„å…¶ä»–é¢†åŸŸå‡å°‘é›†å›¢å·®å¼‚å…·æœ‰æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç‰¹å®šåˆ†ç»„ä¸Šè¡¨ç°ä¸ä½³çš„é—®é¢˜éœ€è¦è§£å†³ã€‚</li>
<li>Group DROæ–¹æ³•é€šè¿‡æœ€å°åŒ–æœ€ç³Ÿç³•é›†å›¢çš„æŸå¤±æ¥æé«˜æ€§èƒ½ï¼Œä½†å½“é›†å›¢æŸå¤±ä¸èƒ½åæ˜ çœŸå®æ€§èƒ½å·®å¼‚æ—¶ä¼šå‡ºç°é—®é¢˜ã€‚</li>
<li>CTCæŸå¤±åœ¨è¯­éŸ³é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨å› è¾“å…¥é•¿åº¦å’Œè¯­è¨€ç‰¹æ€§å¯¼è‡´çš„è™šå‡å·®å¼‚é—®é¢˜ã€‚</li>
<li>CTC-DROæ–¹æ³•é€šè¿‡å¹³æ»‘é›†å›¢æƒé‡æ›´æ–°å’Œé‡‡ç”¨ä¸è¾“å…¥é•¿åº¦åŒ¹é…çš„æ‰¹å¤„ç†æ¥è§£å†³Group DROå’ŒCTCçš„é—®é¢˜ã€‚</li>
<li>åœ¨å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šï¼ŒCTC-DROæ˜¾è‘—ä¼˜äºGroup DROå’ŒåŸºäºCTCçš„åŸºçº¿æ¨¡å‹ã€‚</li>
<li>CTC-DROå¯é™ä½æœ€å·®è¯­è¨€é”™è¯¯ç‡å’Œå¹³å‡é”™è¯¯ç‡ï¼Œå…·æœ‰åº”ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œå…¶ä»–ç±»ä¼¼æŒ‘æˆ˜çš„é¢†åŸŸçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bc43ef03e3fd7b472ecc40fc938683a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6816fbe0ce70d43642fbaaa002b2bc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b79dfff345a3fb717ebb38804ba595bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-30560170e9b66163fa2eafa820760d5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79a1c1e67959d204eec2db2cccdcc27d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adapter-Based-Multi-Agent-AVSR-Extension-for-Pre-Trained-ASR-Models"><a href="#Adapter-Based-Multi-Agent-AVSR-Extension-for-Pre-Trained-ASR-Models" class="headerlink" title="Adapter-Based Multi-Agent AVSR Extension for Pre-Trained ASR Models"></a>Adapter-Based Multi-Agent AVSR Extension for Pre-Trained ASR Models</h2><p><strong>Authors:Christopher Simic, Korbinian Riedhammer, Tobias Bocklet</strong></p>
<p>We present an approach to Audio-Visual Speech Recognition that builds on a pre-trained Whisper model. To infuse visual information into this audio-only model, we extend it with an AV fusion module and LoRa adapters, one of the most up-to-date adapter approaches. One advantage of adapter-based approaches, is that only a relatively small number of parameters are trained, while the basic model remains unchanged. Common AVSR approaches train single models to handle several noise categories and noise levels simultaneously. Taking advantage of the lightweight nature of adapter approaches, we train noise-scenario-specific adapter-sets, each covering individual noise-categories or a specific noise-level range. The most suitable adapter-set is selected by previously classifying the noise-scenario. This enables our models to achieve an optimum coverage across different noise-categories and noise-levels, while training only a minimum number of parameters.   Compared to a full fine-tuning approach with SOTA performance our models achieve almost comparable results over the majority of the tested noise-categories and noise-levels, with up to 88.5% less trainable parameters. Our approach can be extended by further noise-specific adapter-sets to cover additional noise scenarios. It is also possible to utilize the underlying powerful ASR model when no visual information is available, as it remains unchanged. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒWhisperæ¨¡å‹çš„è§†å¬è¯­éŸ³è¯†åˆ«æ–¹æ³•ã€‚ä¸ºäº†å‘è¿™ç§ä»…åŒ…å«éŸ³é¢‘çš„æ¨¡å‹æ³¨å…¥è§†è§‰ä¿¡æ¯ï¼Œæˆ‘ä»¬é€šè¿‡AVèåˆæ¨¡å—å’ŒLoRaé€‚é…å™¨å¯¹å…¶è¿›è¡Œæ‰©å±•ï¼Œåè€…æ˜¯æœ€æ–°çš„é€‚é…å™¨æ–¹æ³•ä¹‹ä¸€ã€‚åŸºäºé€‚é…å™¨çš„ä¼˜åŠ¿ä¹‹ä¸€æ˜¯åªéœ€è¦è®­ç»ƒç›¸å¯¹è¾ƒå°‘çš„å‚æ•°ï¼Œè€ŒåŸºæœ¬æ¨¡å‹ä¿æŒä¸å˜ã€‚å¸¸è§çš„AVSRæ–¹æ³•è®­ç»ƒå•ä¸€æ¨¡å‹ï¼Œä»¥åŒæ—¶å¤„ç†å¤šç§å™ªå£°ç±»åˆ«å’Œå™ªå£°çº§åˆ«ã€‚åˆ©ç”¨é€‚é…å™¨æ–¹æ³•çš„è½»ä¾¿æ€§ï¼Œæˆ‘ä»¬è®­ç»ƒé’ˆå¯¹ç‰¹å®šå™ªå£°åœºæ™¯çš„é€‚é…å™¨é›†ï¼Œæ¯ä¸ªé€‚é…å™¨é›†è¦†ç›–å•ä¸ªå™ªå£°ç±»åˆ«æˆ–ç‰¹å®šçš„å™ªå£°çº§åˆ«èŒƒå›´ã€‚é€šè¿‡é¢„å…ˆå¯¹å™ªå£°åœºæ™¯è¿›è¡Œåˆ†ç±»ï¼Œé€‰æ‹©æœ€åˆé€‚çš„é€‚é…å™¨é›†ã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„å™ªå£°ç±»åˆ«å’Œå™ªå£°çº§åˆ«ä¸Šå®ç°æœ€ä½³è¦†ç›–ï¼ŒåŒæ—¶åªè®­ç»ƒæœ€å°‘çš„å‚æ•°ã€‚ä¸å…·æœ‰æœ€æ–°æŠ€æœ¯æ€§èƒ½çš„å…¨å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤§å¤šæ•°æµ‹è¯•çš„å™ªå£°ç±»åˆ«å’Œå™ªå£°çº§åˆ«ä¸Šå–å¾—äº†å‡ ä¹ç›¸å½“çš„ç»“æœï¼ŒåŒæ—¶å¯è®­ç»ƒçš„å‚æ•°å‡å°‘äº†é«˜è¾¾88.5%ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é€šè¿‡è¿›ä¸€æ­¥çš„ç‰¹å®šå™ªå£°é€‚é…å™¨é›†æ¥æ‰©å±•ï¼Œä»¥è¦†ç›–æ›´å¤šçš„å™ªå£°åœºæ™¯ã€‚å½“æ²¡æœ‰è§†è§‰ä¿¡æ¯æ—¶ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨åŠŸèƒ½å¼ºå¤§çš„ASRæ¨¡å‹ï¼Œå› ä¸ºå®ƒä¿æŒä¸å˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01709v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒWhisperæ¨¡å‹çš„è§†å¬è¯­éŸ³è¯†åˆ«çš„æ”¹è¿›æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥è§†è§‰ä¿¡æ¯ï¼Œæ‰©å±•äº†éŸ³é¢‘æ¨¡å‹ï¼Œå¹¶æ·»åŠ äº†AVèåˆæ¨¡å—å’Œæœ€æ–°çš„LoRaé€‚é…å™¨ã€‚é€‚é…å™¨æ–¹æ³•ä»…è®­ç»ƒå°‘é‡å‚æ•°ï¼ŒåŒæ—¶ä¿æŒåŸºæœ¬æ¨¡å‹ä¸å˜ã€‚æœ¬ç ”ç©¶é‡‡ç”¨å™ªå£°åœºæ™¯ç‰¹å®šçš„é€‚é…å™¨é›†æ¥è¦†ç›–ä¸åŒçš„å™ªå£°ç±»åˆ«å’Œå™ªå£°çº§åˆ«ï¼Œå¹¶é€šè¿‡é¢„å…ˆåˆ†ç±»å™ªå£°åœºæ™¯é€‰æ‹©æœ€åˆé€‚çš„é€‚é…å™¨é›†ã€‚åœ¨å¤§å¤šæ•°æµ‹è¯•çš„å™ªå£°ç±»åˆ«å’Œå™ªå£°çº§åˆ«ä¸Šï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½ä¸æœ€æ–°æŠ€æœ¯å‡ ä¹ç›¸å½“ï¼ŒåŒæ—¶å¯è®­ç»ƒå‚æ•°å‡å°‘äº†é«˜è¾¾88.5%ã€‚å½“æ²¡æœ‰è§†è§‰ä¿¡æ¯æ—¶ï¼Œè¿˜å¯ä»¥åˆ©ç”¨å¼ºå¤§çš„ASRæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨äº†é¢„è®­ç»ƒçš„Whisperæ¨¡å‹ä½œä¸ºåŸºç¡€è¿›è¡Œè§†å¬è¯­éŸ³è¯†åˆ«ã€‚</li>
<li>é€šè¿‡AVèåˆæ¨¡å—å’ŒLoRaé€‚é…å™¨å¼•å…¥è§†è§‰ä¿¡æ¯åˆ°éŸ³é¢‘æ¨¡å‹ä¸­ã€‚</li>
<li>é‡‡ç”¨é€‚é…å™¨æ–¹æ³•ï¼Œä»…è®­ç»ƒå°‘é‡å‚æ•°ï¼Œä¿æŒåŸºç¡€æ¨¡å‹ä¸å˜ã€‚</li>
<li>å¼€å‘å™ªå£°åœºæ™¯ç‰¹å®šçš„é€‚é…å™¨é›†ï¼Œä»¥è¦†ç›–ä¸åŒçš„å™ªå£°ç±»åˆ«å’Œå™ªå£°çº§åˆ«ã€‚</li>
<li>é€šè¿‡é¢„å…ˆåˆ†ç±»å™ªå£°åœºæ™¯é€‰æ‹©é€‚é…å™¨é›†ï¼Œå®ç°æœ€ä¼˜çš„è·¨å™ªå£°ç±»åˆ«å’Œå™ªå£°çº§åˆ«çš„è¦†ç›–ã€‚</li>
<li>ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½åœ¨å¤§å¤šæ•°æµ‹è¯•çš„å™ªå£°ç±»åˆ«å’Œå™ªå£°çº§åˆ«ä¸Šå‡ ä¹ç›¸å½“ï¼ŒåŒæ—¶é™ä½äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01709">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d272fbd8f363ac21830a3ca01f1f5de9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d857207dd7defe4bc7cf9faacd362c93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-129c03c30fabc821a4ca723773c59182.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-Edge-Speech-Understanding-with-Tiny-Foundation-Models"><a href="#Privacy-Preserving-Edge-Speech-Understanding-with-Tiny-Foundation-Models" class="headerlink" title="Privacy-Preserving Edge Speech Understanding with Tiny Foundation Models"></a>Privacy-Preserving Edge Speech Understanding with Tiny Foundation Models</h2><p><strong>Authors:Afsara Benazir, Felix Xiaozhu Lin</strong></p>
<p>Robust speech recognition systems rely on cloud service providers for inference. It needs to ensure that an untrustworthy provider cannot deduce the sensitive content in speech. Sanitization can be done on speech content keeping in mind that it has to avoid compromising transcription accuracy. Realizing the under utilized capabilities of tiny speech foundation models (FMs), for the first time, we propose a novel use: enhancing speech privacy on resource-constrained devices. We introduce XYZ, an edge&#x2F;cloud privacy preserving speech inference engine that can filter sensitive entities without compromising transcript accuracy. We utilize a timestamp based on-device masking approach that utilizes a token to entity prediction model to filter sensitive entities. Our choice of mask strategically conceals parts of the input and hides sensitive data. The masked input is sent to a trusted cloud service or to a local hub to generate the masked output. The effectiveness of XYZ hinges on how well the entity time segments are masked. Our recovery is a confidence score based approach that chooses the best prediction between cloud and on-device model. We implement XYZ on a 64 bit Raspberry Pi 4B. Experiments show that our solution leads to robust speech recognition without forsaking privacy. XYZ with &lt; 100 MB memory, achieves state-of-the-art (SOTA) speech transcription performance while filtering about 83% of private entities directly on-device. XYZ is 16x smaller in memory and 17x more compute efficient than prior privacy preserving speech frameworks and has a relative reduction in word error rate (WER) by 38.8-77.5% when compared to existing offline transcription services. </p>
<blockquote>
<p>ç¨³å¥çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¾èµ–äºäº‘æœåŠ¡æä¾›å•†è¿›è¡Œæ¨æ–­ã€‚éœ€è¦ç¡®ä¿ä¸å¯ä¿¡çš„æä¾›å•†æ— æ³•æ¨æ–­å‡ºè¯­éŸ³ä¸­çš„æ•æ„Ÿå†…å®¹ã€‚åœ¨å¯¹è¯­éŸ³å†…å®¹è¿›è¡Œæ¸…ç†æ—¶ï¼Œéœ€è¦é¿å…å½±å“è½¬å½•ç²¾åº¦ã€‚æˆ‘ä»¬é¦–æ¬¡æ„è¯†åˆ°å°å‹è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„æ½œåŠ›å¹¶æœªè¢«å……åˆ†åˆ©ç”¨ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹åº”ç”¨ï¼šå¢å¼ºèµ„æºå—é™è®¾å¤‡ä¸Šçš„è¯­éŸ³éšç§ä¿æŠ¤ã€‚æˆ‘ä»¬å¼•å…¥äº†XYZï¼Œè¿™æ˜¯ä¸€ä¸ªè¾¹ç¼˜&#x2F;äº‘éšç§ä¿æŠ¤è¯­éŸ³æ¨æ–­å¼•æ“ï¼Œå¯ä»¥åœ¨ä¸å½±å“è½¬å½•å‡†ç¡®æ€§çš„æƒ…å†µä¸‹è¿‡æ»¤æ•æ„Ÿå®ä½“ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºæ—¶é—´æˆ³çš„è®¾å¤‡ç«¯æ©ç æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä»¤ç‰Œåˆ°å®ä½“é¢„æµ‹æ¨¡å‹æ¥è¿‡æ»¤æ•æ„Ÿå®ä½“ã€‚æˆ‘ä»¬é€‰æ‹©çš„æ©ç ç­–ç•¥æ€§åœ°æ©ç›–äº†è¾“å…¥çš„ä¸€éƒ¨åˆ†å¹¶éšè—äº†æ•æ„Ÿæ•°æ®ã€‚æ©ç åçš„è¾“å…¥è¢«å‘é€åˆ°å¯ä¿¡èµ–çš„äº‘æœåŠ¡æˆ–æœ¬åœ°ä¸­å¿ƒä»¥ç”Ÿæˆæ©ç åçš„è¾“å‡ºã€‚XYZçš„æœ‰æ•ˆæ€§å–å†³äºå®ä½“æ—¶é—´æ®µçš„æ©ç è´¨é‡ã€‚æˆ‘ä»¬çš„æ¢å¤æ˜¯ä¸€ç§åŸºäºç½®ä¿¡åº¦çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨äº‘å’Œæœ¬åœ°æ¨¡å‹ä¹‹é—´é€‰æ‹©æœ€ä½³é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ª64ä½çš„Raspberry Pi 4Bä¸Šå®ç°äº†XYZã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå®ç°äº†ç¨³å¥çš„è¯­éŸ³è¯†åˆ«ï¼Œè€Œä¸ä¼šæ”¾å¼ƒéšç§ä¿æŠ¤ã€‚XYZä»…å ç”¨çš„å†…å­˜ç©ºé—´ä¸åˆ°&lt; 100 MBçš„æƒ…å†µä¸‹å®ç°äº†é¢†å…ˆçš„è¯­éŸ³è½¬å½•æ€§èƒ½ï¼Œç›´æ¥åœ¨è®¾å¤‡ä¸Šè¿‡æ»¤å¤§çº¦83%çš„ç§æœ‰å®ä½“ã€‚ä¸å…ˆå‰çš„éšç§ä¿æŠ¤è¯­éŸ³æ¡†æ¶ç›¸æ¯”ï¼ŒXYZåœ¨å†…å­˜å ç”¨æ–¹é¢å°å¾—å¤šï¼ˆå‡å°‘äº†å†…å­˜å ç”¨çš„é«˜è¾¾ç¼©å°å†…å­˜å ç”¨å¤šè¾¾ä¸¤å€ï¼‰å¹¶æ›´å…·è®¡ç®—æ•ˆç‡ï¼ˆè®¡ç®—èƒ½åŠ›é«˜å‡ºè¶…è¿‡è¾¾åˆ°å…ˆå‰çš„å¤„ç†æ•ˆç‡å€æ•°1å€å¤šï¼‰ã€‚åœ¨ä¸ç°æœ‰ç¦»çº¿è½¬å½•æœåŠ¡è¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œå…¶ç›¸å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†é™ä½äº†è¯é”™è¯¯ç‡å‡å°‘äº†æœ€å¤šè¾¾å‡å°‘è¾¾é«˜è¾¾è¿‘è‡³å‡å°‘è¿‘ç™¾åˆ†ä¹‹æœ€é«˜è¾¾ä¸‰åˆ†ä¹‹ä¸€åˆ°è¿‘ç™¾åˆ†ä¹‹ä¸ƒåä¸ƒå¤šï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01649v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨äº‘æœåŠ¡å’Œèµ„æºå—é™è®¾å¤‡ä¸Šå®ç°ç¨³å¥è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„éšç§ä¿æŠ¤é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰ç³»ç»Ÿä¸­æ•æ„Ÿå†…å®¹æ³„éœ²çš„é£é™©ï¼Œæå‡ºäº†ä¸€ç§åä¸ºXYZçš„è¾¹ç¼˜&#x2F;äº‘éšç§ä¿æŠ¤è¯­éŸ³æ¨ç†å¼•æ“ã€‚è¯¥å¼•æ“é€šè¿‡åŸºäºæ—¶é—´æˆ³çš„æœ¬åœ°æ©ç æ–¹æ³•ï¼Œåˆ©ç”¨ä»¤ç‰Œåˆ°å®ä½“é¢„æµ‹æ¨¡å‹è¿‡æ»¤æ•æ„Ÿå®ä½“ï¼Œä»è€Œä¿æŠ¤è¯­éŸ³éšç§ã€‚é€šè¿‡ç­–ç•¥æ€§åœ°æ©ç›–è¾“å…¥çš„éƒ¨åˆ†å†…å®¹ï¼Œéšè—æ•æ„Ÿæ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒXYZåœ¨ä¸ç‰ºç‰²éšç§çš„æƒ…å†µä¸‹å®ç°äº†ç¨³å¥çš„è¯­éŸ³è¯†åˆ«ã€‚åœ¨å†…å­˜å ç”¨å°äº100MBçš„æƒ…å†µä¸‹ï¼ŒXYZå®ç°äº†é¢†å…ˆçš„è¯­éŸ³è¯†åˆ«æ€§èƒ½ï¼Œç›´æ¥åœ¨è®¾å¤‡ä¸Šè¿‡æ»¤çº¦83%çš„ç§æœ‰å®ä½“ã€‚ä¸ä¹‹å‰çš„éšç§ä¿æŠ¤è¯­éŸ³æ¡†æ¶ç›¸æ¯”ï¼ŒXYZå†…å­˜å ç”¨æ›´å°ï¼ˆä»…ä¸ºå…¶åå…­åˆ†ä¹‹ä¸€ï¼‰ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ï¼ˆä»…ä¸ºå…¶åä¸ƒåˆ†ä¹‹ä¸€ï¼‰ï¼Œå¹¶ä¸”ä¸ç°æœ‰çš„ç¦»çº¿è½¬å½•æœåŠ¡ç›¸æ¯”ï¼Œç›¸å¯¹é™ä½äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰åœ¨è¾¾åˆ°æ‰©å±•å‰çš„èŒƒå›´å†…çš„è¯è¯­å‡å°‘äº†å¾ˆå¤šç™¾åˆ†æ•°è¶…è¿‡è¾¾åˆ°é”™è¯¯èŒƒå›´å†…çš„é«˜å³°å‡ºç°ä¸ºæ•´äºŒåä¸ªä»¥ä¸Šå½“å‰å‡ºçš„ä¿å¯†å‰æ²¿æœ‰äº†å¼ºæœ‰åŠ›çš„æ°´å¹³åœ¨çº¿è™šæ‹Ÿæœï¼Œä¾‹å¦‚å‡å°‘äº†è¢«æ ‡è®°å‡ºæ¥çš„éƒ¨åˆ†å†…å®¹çš„å¤±è¯¯æ¬¡æ•°è¾¾åˆ°ç†æƒ³çš„çŠ¶æ€ç›®æ ‡è¾¾åˆ°æ¥è¿‘ä¸“ä¸šé¡¶å°–çº§åˆ«å´ç»§ç»­ä»å¯¹å®¢æˆ·æœ€å®é™…åœºæ™¯çš„ä¸“æœ‰æ–¹é¢èµ°å¯ä»¥æè¿°é¢„æœŸæˆåŠŸçš„è¡¨è¿°å˜åŒ–è¾ƒä¸ºéå¸¸é€‚åº”å…·ä½“æƒ…å†µçš„æ‰§è¡ŒXYZä¸ç¦»çº¿è½¬å½•æœåŠ¡ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥è§£å†³æ–¹æ¡ˆä¸ºå®ç°éšç§ä¿æŠ¤çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01649">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19a291b545c4b5452348546d44aca1a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47dfc9397a74681ee01f9799a55850a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ba17b80b53f1b17b84115d3b39c32fe.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Differentiable-Alignment-Framework-for-Sequence-to-Sequence-Modeling-via-Optimal-Transport"><a href="#A-Differentiable-Alignment-Framework-for-Sequence-to-Sequence-Modeling-via-Optimal-Transport" class="headerlink" title="A Differentiable Alignment Framework for Sequence-to-Sequence Modeling   via Optimal Transport"></a>A Differentiable Alignment Framework for Sequence-to-Sequence Modeling   via Optimal Transport</h2><p><strong>Authors:Yacouba Kaloga, Shashi Kumar, Petr Motlicek, Ina Kodrasi</strong></p>
<p>Accurate sequence-to-sequence (seq2seq) alignment is critical for applications like medical speech analysis and language learning tools relying on automatic speech recognition (ASR). State-of-the-art end-to-end (E2E) ASR systems, such as the Connectionist Temporal Classification (CTC) and transducer-based models, suffer from peaky behavior and alignment inaccuracies. In this paper, we propose a novel differentiable alignment framework based on one-dimensional optimal transport, enabling the model to learn a single alignment and perform ASR in an E2E manner. We introduce a pseudo-metric, called Sequence Optimal Transport Distance (SOTD), over the sequence space and discuss its theoretical properties. Based on the SOTD, we propose Optimal Temporal Transport Classification (OTTC) loss for ASR and contrast its behavior with CTC. Experimental results on the TIMIT, AMI, and LibriSpeech datasets show that our method considerably improves alignment performance, though with a trade-off in ASR performance when compared to CTC. We believe this work opens new avenues for seq2seq alignment research, providing a solid foundation for further exploration and development within the community. </p>
<blockquote>
<p>ç²¾ç¡®åºåˆ—åˆ°åºåˆ—ï¼ˆseq2seqï¼‰å¯¹é½å¯¹äºä¾èµ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„åŒ»ç–—è¯­éŸ³åˆ†æå’Œè¯­è¨€å­¦ä¹ å·¥å…·ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç›®å‰æœ€å…ˆè¿›çš„ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰ASRç³»ç»Ÿï¼Œå¦‚è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰å’ŒåŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ï¼Œå­˜åœ¨å³°å€¼è¡Œä¸ºå’Œå¯¹é½ä¸å‡†ç¡®çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¸€ç»´æœ€ä¼˜ä¼ è¾“çš„æ–°å‹å¯åŒºåˆ†å¯¹é½æ¡†æ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å­¦ä¹ å•ä¸€å¯¹é½å¹¶æ‰§è¡ŒASRã€‚æˆ‘ä»¬å¼•å…¥äº†åºåˆ—ç©ºé—´ä¸Šçš„ä¼ªåº¦é‡ï¼Œç§°ä¸ºåºåˆ—æœ€ä¼˜ä¼ è¾“è·ç¦»ï¼ˆSOTDï¼‰ï¼Œå¹¶è®¨è®ºäº†å…¶ç†è®ºå±æ€§ã€‚åŸºäºSOTDï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºASRçš„æœ€ä¼˜æ—¶é—´ä¼ è¾“åˆ†ç±»ï¼ˆOTTCï¼‰æŸå¤±ï¼Œå¹¶å°†å…¶è¡Œä¸ºä¸CTCè¿›è¡Œäº†å¯¹æ¯”ã€‚åœ¨TIMITã€AMIå’ŒLibriSpeechæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ”¹è¿›å¯¹é½æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œå°½ç®¡ä¸CTCç›¸æ¯”åœ¨ASRæ€§èƒ½æ–¹é¢å­˜åœ¨æƒè¡¡ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œå¼€è¾Ÿäº†seq2seqå¯¹é½ç ”ç©¶çš„æ–°é€”å¾„ï¼Œä¸ºç¤¾åŒºå†…çš„è¿›ä¸€æ­¥æ¢ç´¢å’Œå‘å±•æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01588v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºä¸€ç»´æœ€ä¼˜ä¼ è¾“çš„å¯å¾®å¯¹é½æ¡†æ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç«¯åˆ°ç«¯åœ°è¿›è¡Œå•ä¸€å¯¹é½å’Œå­¦ä¹ è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚å¼•å…¥åºåˆ—æœ€ä¼˜ä¼ è¾“è·ç¦»ï¼ˆSOTDï¼‰ä¼ªåº¦é‡ï¼Œå¹¶åŸºäºSOTDæå‡ºæœ€ä¼˜æ—¶åºä¼ è¾“åˆ†ç±»ï¼ˆOTTCï¼‰æŸå¤±å‡½æ•°ç”¨äºASRã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨TMITã€AMIå’ŒLibriSpeechæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†å¯¹é½æ€§èƒ½ï¼Œä½†ä¸CTCç›¸æ¯”ï¼ŒASRæ€§èƒ½å­˜åœ¨æƒè¡¡ã€‚è¿™ä¸ºåºåˆ—åˆ°åºåˆ—å¯¹é½ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®åºåˆ—åˆ°åºåˆ—ï¼ˆseq2seqï¼‰å¯¹é½å¯¹äºä¾èµ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„åº”ç”¨å¦‚åŒ»ç–—è¯­éŸ³åˆ†æå’Œè¯­è¨€å­¦ä¹ å·¥å…·è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰ASRç³»ç»Ÿï¼Œå¦‚è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰å’ŒåŸºäºè½¬æ¢å™¨æ¨¡å‹çš„ç³»ç»Ÿï¼Œå­˜åœ¨å³°å€¼è¡Œä¸ºå’Œå¯¹é½ä¸å‡†ç¡®çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºä¸€ç»´æœ€ä¼˜ä¼ è¾“çš„å¯å¾®å¯¹é½æ¡†æ¶ï¼Œèƒ½å¤Ÿç«¯åˆ°ç«¯åœ°è¿›è¡Œå•ä¸€å¯¹é½å’ŒASRã€‚</li>
<li>æå‡ºäº†åºåˆ—æœ€ä¼˜ä¼ è¾“è·ç¦»ï¼ˆSOTDï¼‰ä¼ªåº¦é‡ï¼Œå¹¶è®¨è®ºäº†å…¶ç†è®ºå±æ€§ã€‚</li>
<li>åŸºäºSOTDï¼Œæå‡ºäº†ç”¨äºASRçš„æœ€ä¼˜æ—¶åºä¼ è¾“åˆ†ç±»ï¼ˆOTTCï¼‰æŸå¤±å‡½æ•°ï¼Œå¹¶ä¸CTCè¿›è¡Œäº†å¯¹æ¯”ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨TMITã€AMIå’ŒLibriSpeechæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†å¯¹é½æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67ab85b1af3e60fd99f8146c7bc567a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6e84b06237f26f99ac426067f960647.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c393b28d98e3b2768c9bb22b4ab793cd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="mWhisper-Flamingo-for-Multilingual-Audio-Visual-Noise-Robust-Speech-Recognition"><a href="#mWhisper-Flamingo-for-Multilingual-Audio-Visual-Noise-Robust-Speech-Recognition" class="headerlink" title="mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech   Recognition"></a>mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech   Recognition</h2><p><strong>Authors:Andrew Rouditchenko, Saurabhchand Bhati, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio and can improve performance in noise, but most methods are trained only on English data. One limitation is the lack of large-scale multilingual video data, which makes it hard hard to train models from scratch. In this work, we propose mWhisper-Flamingo for multilingual AVSR which combines the strengths of a pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable better multi-modal integration and improve the noisy multilingual performance, we introduce decoder modality dropout where the model is trained both on paired audio-visual inputs and separate audio&#x2F;visual inputs. mWhisper-Flamingo achieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages. Audio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on all languages in noisy conditions. </p>
<blockquote>
<p>éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†åŸºäºå˜´å”‡çš„è§†é¢‘å’ŒéŸ³é¢‘ï¼Œå¯ä»¥æé«˜å™ªå£°ä¸­çš„æ€§èƒ½ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä»…é’ˆå¯¹è‹±è¯­æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ä¸€ä¸ªå±€é™æ€§æ˜¯ç¼ºä¹å¤§è§„æ¨¡çš„å¤šè¯­è¨€è§†é¢‘æ•°æ®ï¼Œè¿™ä½¿å¾—ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹å˜å¾—å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå¤šè¯­è¨€AVSRçš„mWhisper-Flamingoï¼Œå®ƒç»“åˆäº†é¢„è®­ç»ƒéŸ³é¢‘æ¨¡å‹ï¼ˆWhisperï¼‰å’Œè§†é¢‘æ¨¡å‹ï¼ˆAV-HuBERTï¼‰çš„ä¼˜ç‚¹ã€‚ä¸ºäº†æ›´å¥½åœ°å®ç°å¤šæ¨¡å¼é›†æˆå¹¶æ”¹å–„å˜ˆæ‚çš„å¤šè¯­è¨€æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§£ç å™¨æ¨¡æ€ä¸¢å¼ƒï¼Œå…¶ä¸­æ¨¡å‹æ—¢æ¥å—æˆå¯¹çš„éŸ³é¢‘è§†é¢‘è¾“å…¥ï¼Œä¹Ÿæ¥å—å•ç‹¬çš„éŸ³é¢‘&#x2F;è§†é¢‘è¾“å…¥è¿›è¡Œè®­ç»ƒã€‚mWhisper-Flamingoåœ¨MuAViCä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼ŒMuAViCæ˜¯ä¸€ä¸ªåŒ…å«9ç§è¯­è¨€çš„AVSRæ•°æ®é›†ã€‚åœ¨å˜ˆæ‚çš„æ¡ä»¶ä¸‹ï¼Œè§†å¬mWhisper-Flamingoåœ¨æ‰€æœ‰è¯­è¨€ä¸Šçš„æ€§èƒ½éƒ½ä¸€ç›´ä¼˜äºä»…éŸ³é¢‘çš„Whisperã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01547v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>éŸ³é¢‘è§†é¢‘è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆå”‡è¯­è§†é¢‘å’ŒéŸ³é¢‘ï¼Œåœ¨å™ªéŸ³ä¸­èƒ½å¤Ÿæé«˜æ€§èƒ½è¡¨ç°ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä»…é’ˆå¯¹è‹±è¯­æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å­˜åœ¨ä¸€å¤§éš¾é¢˜åœ¨äºç¼ºä¹å¤§è§„æ¨¡çš„å¤šè¯­ç§è§†é¢‘æ•°æ®ï¼Œä½¿å¾—ä»å¤´è®­ç»ƒæ¨¡å‹å˜å¾—å›°éš¾ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§é’ˆå¯¹å¤šè¯­ç§AVSRçš„mWhisper-Flamingoæ–¹æ³•ï¼Œç»“åˆäº†é¢„è®­ç»ƒéŸ³é¢‘æ¨¡å‹ï¼ˆWhisperï¼‰å’Œè§†é¢‘æ¨¡å‹ï¼ˆAV-HuBERTï¼‰çš„ä¼˜åŠ¿ã€‚ä¸ºäº†è¿›è¡Œæ›´å¥½çš„å¤šæ¨¡æ€èåˆå’Œæé«˜å˜ˆæ‚ç¯å¢ƒä¸‹çš„å¤šè¯­ç§æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§£ç å™¨æ¨¡æ€ä¸¢å¼ƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨é…å¯¹éŸ³è§†é¢‘è¾“å…¥å’Œå•ç‹¬éŸ³é¢‘&#x2F;è§†é¢‘è¾“å…¥ä¸Šè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚åœ¨MuAViCè¿™ä¸€åŒ…å«ä¹ç§è¯­è¨€çš„AVSRæ•°æ®é›†ä¸Šï¼ŒmWhisper-Flamingoè¾¾åˆ°äº†å½“å‰æœ€ä½³çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚åœ¨å˜ˆæ‚ç¯å¢ƒä¸‹ï¼ŒéŸ³è§†é¢‘ç‰ˆæœ¬çš„mWhisper-Flamingoåœ¨æ‰€æœ‰è¯­ç§ä¸Šçš„è¡¨ç°å‡ä¼˜äºä»…éŸ³é¢‘çš„Whisperã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éŸ³é¢‘è§†é¢‘è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆå”‡è¯­è§†é¢‘ä¸éŸ³é¢‘ä»¥æå‡æ€§èƒ½ï¼Œå°¤å…¶åœ¨å™ªéŸ³ç¯å¢ƒä¸­ã€‚</li>
<li>å½“å‰æ–¹æ³•çš„å±€é™æ€§åœ¨äºå®ƒä»¬ä¸»è¦åŸºäºè‹±è¯­æ•°æ®è®­ç»ƒï¼Œç¼ºä¹å¤§è§„æ¨¡å¤šè¯­ç§è§†é¢‘æ•°æ®ã€‚</li>
<li>æå‡ºmWhisper-Flamingoæ–¹æ³•ï¼Œç»“åˆé¢„è®­ç»ƒéŸ³é¢‘æ¨¡å‹ï¼ˆWhisperï¼‰å’Œè§†é¢‘æ¨¡å‹ï¼ˆAV-HuBERTï¼‰çš„ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡å¼•å…¥è§£ç å™¨æ¨¡æ€ä¸¢å¼ƒæŠ€æœ¯ï¼Œå®ç°æ›´å¥½çš„å¤šæ¨¡æ€èåˆåŠæé«˜å˜ˆæ‚ç¯å¢ƒä¸‹çš„å¤šè¯­ç§æ€§èƒ½ã€‚</li>
<li>mWhisper-Flamingoåœ¨MuAViCæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>åœ¨å˜ˆæ‚æ¡ä»¶ä¸‹ï¼ŒéŸ³è§†é¢‘ç‰ˆæœ¬çš„mWhisper-Flamingoåœ¨æ‰€æœ‰è¯­ç§ä¸Šçš„æ€§èƒ½å‡ä¼˜äºä»…ä½¿ç”¨éŸ³é¢‘çš„æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºå¤šè¯­ç§éŸ³é¢‘è§†é¢‘è¯­éŸ³è¯†åˆ«çš„å®é™…åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01547">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-226ef734fb1a06cbc89659703972557e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-567837bd940d438cea9f4dc7293a1031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa44918aaa6d726022a72263993099bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3ada1eba7aa7a4af23b673c992a3aa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10f9a4bdf613a7a81989e1f2c5a5f0ab.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Gradient-Norm-based-Fine-Tuning-for-Backdoor-Defense-in-Automatic-Speech-Recognition"><a href="#Gradient-Norm-based-Fine-Tuning-for-Backdoor-Defense-in-Automatic-Speech-Recognition" class="headerlink" title="Gradient Norm-based Fine-Tuning for Backdoor Defense in Automatic Speech   Recognition"></a>Gradient Norm-based Fine-Tuning for Backdoor Defense in Automatic Speech   Recognition</h2><p><strong>Authors:Nanjun Zhou, Weilin Lin, Li Liu</strong></p>
<p>Backdoor attacks have posed a significant threat to the security of deep neural networks (DNNs). Despite considerable strides in developing defenses against backdoor attacks in the visual domain, the specialized defenses for the audio domain remain empty. Furthermore, the defenses adapted from the visual to audio domain demonstrate limited effectiveness. To fill this gap, we propose Gradient Norm-based FineTuning (GN-FT), a novel defense strategy against the attacks in the audio domain, based on the observation from the corresponding backdoored models. Specifically, we first empirically find that the backdoored neurons exhibit greater gradient values compared to other neurons, while clean neurons stay the lowest. On this basis, we fine-tune the backdoored model by incorporating the gradient norm regularization, aiming to weaken and reduce the backdoored neurons. We further approximate the loss computation for lower implementation costs. Extensive experiments on two speech recognition datasets across five models demonstrate the superior performance of our proposed method. To the best of our knowledge, this work is the first specialized and effective defense against backdoor attacks in the audio domain. </p>
<blockquote>
<p>åé—¨æ”»å‡»å¯¹æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„å®‰å…¨æ„æˆäº†é‡å¤§å¨èƒã€‚å°½ç®¡è§†è§‰é¢†åŸŸé’ˆå¯¹åé—¨æ”»å‡»çš„é˜²å¾¡æªæ–½å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†éŸ³é¢‘é¢†åŸŸçš„ä¸“ä¸šé˜²å¾¡ä»ç„¶ç©ºç™½ã€‚æ­¤å¤–ï¼Œä»è§†è§‰é¢†åŸŸé€‚åº”åˆ°éŸ³é¢‘é¢†åŸŸçš„é˜²å¾¡æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰é™çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¢¯åº¦èŒƒæ•°çš„å¾®è°ƒï¼ˆGN-FTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹éŸ³é¢‘é¢†åŸŸæ”»å‡»çš„æ–°å‹é˜²å¾¡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ˜¯åŸºäºåé—¨æ¨¡å‹çš„ç›¸å…³è§‚å¯Ÿã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå®è¯å‘ç°ï¼Œä¸å¹²å‡€ç¥ç»å…ƒç›¸æ¯”ï¼Œåé—¨ç¥ç»å…ƒè¡¨ç°å‡ºæ›´å¤§çš„æ¢¯åº¦å€¼ï¼Œè€Œå¹²å‡€ç¥ç»å…ƒçš„æ¢¯åº¦å€¼ä¿æŒæœ€ä½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆæ¢¯åº¦èŒƒæ•°æ­£åˆ™åŒ–å¯¹åé—¨æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ—¨åœ¨å‰Šå¼±å¹¶å‡å°‘åé—¨ç¥ç»å…ƒã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥ç®€åŒ–äº†æŸå¤±è®¡ç®—ä»¥é™ä½å®ç°æˆæœ¬ã€‚åœ¨ä¸¤ä¸ªè¯­éŸ³è¯†åˆ«æ•°æ®é›†ä¸Šçš„äº”ä¸ªæ¨¡å‹çš„å¤§é‡å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æå‡ºæ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œæ˜¯é’ˆå¯¹éŸ³é¢‘é¢†åŸŸåé—¨æ”»å‡»çš„é¦–ä¸ªä¸“ä¸šåŒ–ä¸”æœ‰æ•ˆçš„é˜²å¾¡æªæ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01152v1">PDF</a> 5 pages, 5 figures. This work has been accpeted by ICASSP 2025</p>
<p><strong>Summary</strong><br>æ·±åº¦ç¥ç»ç½‘ç»œé¢ä¸´åé—¨æ”»å‡»çš„é‡å¤§å¨èƒã€‚è§†è§‰é¢†åŸŸå·²æœ‰ä¸å°‘é˜²å¾¡æªæ–½ï¼Œä½†éŸ³é¢‘é¢†åŸŸçš„ä¸“é—¨é˜²å¾¡æ‰‹æ®µä»ç„¶ç¼ºä¹ï¼Œä»è§†è§‰é¢†åŸŸé€‚åº”åˆ°éŸ³é¢‘é¢†åŸŸçš„é˜²å¾¡æ‰‹æ®µæ•ˆæœæœ‰é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¢¯åº¦èŒƒæ•°çš„å¾®è°ƒï¼ˆGN-FTï¼‰ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹éŸ³é¢‘é¢†åŸŸæ”»å‡»çš„æ–°å‹é˜²å¾¡ç­–ç•¥ã€‚æˆ‘ä»¬å‘ç°è¢«åé—¨æ§åˆ¶çš„ç¥ç»å…ƒæ¢¯åº¦å€¼è¾ƒå¤§ï¼ŒåŸºäºæ­¤å¯¹è¢«åé—¨æ§åˆ¶çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ—¨åœ¨å‰Šå¼±å¹¶å‡å°‘è¢«åé—¨æ§åˆ¶çš„ç¥ç»å…ƒï¼ŒåŒæ—¶é™ä½å®ç°æˆæœ¬ã€‚å¤šé¡¹å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­éŸ³è¯†åˆ«æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é’ˆå¯¹éŸ³é¢‘é¢†åŸŸçš„åé—¨æ”»å‡»çš„é¦–ä¸ªæœ‰æ•ˆä¸“é—¨é˜²å¾¡æ‰‹æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åé—¨æ”»å‡»å¯¹æ·±åº¦ç¥ç»ç½‘ç»œçš„å®‰å…¨æ„æˆå¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨éŸ³é¢‘é¢†åŸŸç¼ºä¹ä¸“é—¨çš„é˜²å¾¡æ‰‹æ®µã€‚</li>
<li>ç°æœ‰ä»è§†è§‰é¢†åŸŸé€‚åº”åˆ°éŸ³é¢‘é¢†åŸŸçš„é˜²å¾¡æ‰‹æ®µæ•ˆæœæœ‰é™ã€‚</li>
<li>æå‡ºçš„GN-FTç­–ç•¥åŸºäºè§‚å¯Ÿè¢«åé—¨æ§åˆ¶çš„ç¥ç»å…ƒæ¢¯åº¦å€¼è¾ƒå¤§ã€‚</li>
<li>GN-FTç­–ç•¥é€šè¿‡å¾®è°ƒæ¨¡å‹ï¼Œæ—¨åœ¨å‰Šå¼±å¹¶å‡å°‘è¢«åé—¨æ§åˆ¶çš„ç¥ç»å…ƒã€‚</li>
<li>GN-FTç­–ç•¥è€ƒè™‘äº†æŸå¤±è®¡ç®—çš„è¿‘ä¼¼ï¼Œä»¥é™ä½å®ç°æˆæœ¬ã€‚</li>
<li>åœ¨è¯­éŸ³è¯†åˆ«æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†GN-FTç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2034b50e7235e45d32e2f03eea77fb24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3acb77aee34c73ec2bb619f119ed3a19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-295fef20b66aa763da1433533b53d494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fd51c0c7e88368d0494b101bed337b6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Emotional-Face-to-Speech"><a href="#Emotional-Face-to-Speech" class="headerlink" title="Emotional Face-to-Speech"></a>Emotional Face-to-Speech</h2><p><strong>Authors:Jiaxin Ye, Boyuan Cao, Hongming Shan</strong></p>
<p>How much can we infer about an emotional voice solely from an expressive face? This intriguing question holds great potential for applications such as virtual character dubbing and aiding individuals with expressive language disorders. Existing face-to-speech methods offer great promise in capturing identity characteristics but struggle to generate diverse vocal styles with emotional expression. In this paper, we explore a new task, termed emotional face-to-speech, aiming to synthesize emotional speech directly from expressive facial cues. To that end, we introduce DEmoFace, a novel generative framework that leverages a discrete diffusion transformer (DiT) with curriculum learning, built upon a multi-level neural audio codec. Specifically, we propose multimodal DiT blocks to dynamically align text and speech while tailoring vocal styles based on facial emotion and identity. To enhance training efficiency and generation quality, we further introduce a coarse-to-fine curriculum learning algorithm for multi-level token processing. In addition, we develop an enhanced predictor-free guidance to handle diverse conditioning scenarios, enabling multi-conditional generation and disentangling complex attributes effectively. Extensive experimental results demonstrate that DEmoFace generates more natural and consistent speech compared to baselines, even surpassing speech-driven methods. Demos are shown at <a target="_blank" rel="noopener" href="https://demoface-ai.github.io/">https://demoface-ai.github.io/</a>. </p>
<blockquote>
<p>ä»…å‡­ä¸€å¼ è¡¨æƒ…ä¸°å¯Œçš„è„¸ï¼Œæˆ‘ä»¬èƒ½æ¨æ–­å‡ºå¤šå°‘å…³äºæƒ…ç»ªåŒ–çš„å£°éŸ³ï¼Ÿè¿™ä¸ªå¼•äººå…¥èƒœçš„é—®é¢˜åœ¨è™šæ‹Ÿè§’è‰²é…éŸ³å’Œè¾…åŠ©è¡¨è¾¾æ€§è¯­è¨€éšœç¢ä¸ªä½“ç­‰åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç°æœ‰çš„é¢éƒ¨åˆ°è¯­éŸ³çš„æ–¹æ³•åœ¨æ•æ‰èº«ä»½ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºå¾ˆå¤§çš„å‰æ™¯ï¼Œä½†åœ¨ç”Ÿæˆå…·æœ‰æƒ…æ„Ÿè¡¨è¾¾çš„å¤šæ ·åŒ–è¯­éŸ³é£æ ¼æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œç§°ä¸ºæƒ…ç»ªé¢éƒ¨åˆ°è¯­éŸ³ï¼Œæ—¨åœ¨ç›´æ¥ä»è¡¨æƒ…ä¸°å¯Œçš„é¢éƒ¨çº¿ç´¢ä¸­åˆæˆæƒ…æ„Ÿè¯­éŸ³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†DEmoFaceï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¸¦æœ‰è¯¾ç¨‹å­¦ä¹ çš„ç¦»æ•£æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ï¼Œå»ºç«‹åœ¨å¤šå±‚æ¬¡ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€DiTå—ï¼Œä»¥åŠ¨æ€å¯¹é½æ–‡æœ¬å’Œè¯­éŸ³ï¼ŒåŒæ—¶æ ¹æ®é¢éƒ¨æƒ…ç»ªå’Œèº«ä»½å®šåˆ¶è¯­éŸ³é£æ ¼ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä»ç²—åˆ°ç»†çš„è¯¾ç¨‹å­¦ä¹ ç®—æ³•ï¼Œç”¨äºå¤šå±‚æ¬¡ä»¤ç‰Œå¤„ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¢å¼ºçš„æ— é¢„æµ‹å™¨æŒ‡å¯¼æ–¹æ³•ï¼Œä»¥å¤„ç†å„ç§æ¡ä»¶åœºæ™¯ï¼Œå®ç°å¤šæ¡ä»¶ç”Ÿæˆå¹¶æœ‰æ•ˆåœ°è§£å¼€å¤æ‚å±æ€§ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼ŒDEmoFaceç”Ÿæˆçš„è¯­éŸ³æ¯”åŸºçº¿æ›´è‡ªç„¶ã€æ›´ä¸€è‡´ï¼Œç”šè‡³è¶…è¿‡äº†è¯­éŸ³é©±åŠ¨çš„æ–¹æ³•ã€‚æ¼”ç¤ºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://demoface-ai.github.io/">https://demoface-ai.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01046v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†ä¸€é¡¹æ–°ä»»åŠ¡â€”â€”æƒ…æ„Ÿé¢éƒ¨åˆ°è¯­éŸ³çš„è½¬æ¢ï¼Œæ—¨åœ¨ç›´æ¥ä»é¢éƒ¨è¡¨æƒ…åˆæˆæƒ…æ„Ÿè¯­éŸ³ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†DEmoFaceè¿™ä¸€æ–°å‹ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆç¦»æ•£æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰å’Œè¯¾ç¨‹å­¦ä¹ æŠ€æœ¯ï¼Œå»ºç«‹äºå¤šçº§ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ä¹‹ä¸Šã€‚é€šè¿‡å¼•å…¥å¤šæ¨¡æ€DiTå—ï¼Œå®ç°äº†æ–‡æœ¬å’Œè¯­éŸ³çš„åŠ¨æ€å¯¹é½ï¼Œå¹¶æ ¹æ®é¢éƒ¨è¡¨æƒ…å’Œèº«ä»½å®šåˆ¶è¯­éŸ³é£æ ¼ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†ç”±ç²—åˆ°ç»†çš„æ•™å­¦è¯¾ç¨‹å­¦ä¹ ç®—æ³•è¿›è¡Œå¤šçº§ä»¤ç‰Œå¤„ç†ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDEmoFaceç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ç”Ÿæˆäº†æ›´è‡ªç„¶å’Œä¸€è‡´çš„è¯­éŸ³ã€‚æ›´å¤šæ¼”ç¤ºå‚è§<a target="_blank" rel="noopener" href="https://demoface-ai.github.io./">https://demoface-ai.github.io/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†æƒ…æ„Ÿé¢éƒ¨åˆ°è¯­éŸ³è½¬æ¢çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡é¢éƒ¨è¡¨æƒ…ç›´æ¥åˆæˆæƒ…æ„Ÿè¯­éŸ³ã€‚</li>
<li>ä»‹ç»äº†DEmoFaceç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆäº†ç¦»æ•£æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰å’Œè¯¾ç¨‹å­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>å¤šæ¨¡æ€DiTå—å¯å®ç°æ–‡æœ¬å’Œè¯­éŸ³çš„åŠ¨æ€å¯¹é½ï¼Œæ ¹æ®é¢éƒ¨è¡¨æƒ…å’Œèº«ä»½å®šåˆ¶è¯­éŸ³é£æ ¼ã€‚</li>
<li>é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„è¯¾ç¨‹å­¦ä¹ ç®—æ³•æé«˜è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚</li>
<li>DEmoFaceèƒ½ç”Ÿæˆæ›´è‡ªç„¶å’Œä¸€è‡´çš„è¯­éŸ³ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨è™šæ‹Ÿè§’è‰²é…éŸ³å’Œè¾…åŠ©è¡¨è¾¾æ€§è¯­è¨€éšœç¢è€…ç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01046">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-896cb44f24363421a216975e60dc76fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13a0b68efa8105f600bc86a3576f8980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e770047e249161eeaf8d662a3c07219c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ec4fda0c0589dabe43560b7e888afa8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Data-Driven-Mispronunciation-Pattern-Discovery-for-Robust-Speech-Recognition"><a href="#Data-Driven-Mispronunciation-Pattern-Discovery-for-Robust-Speech-Recognition" class="headerlink" title="Data-Driven Mispronunciation Pattern Discovery for Robust Speech   Recognition"></a>Data-Driven Mispronunciation Pattern Discovery for Robust Speech   Recognition</h2><p><strong>Authors:Anna Seo Gyeong Choi, Jonghyeon Park, Myungwoo Oh</strong></p>
<p>Recent advancements in machine learning have significantly improved speech recognition, but recognizing speech from non-fluent or accented speakers remains a challenge. Previous efforts, relying on rule-based pronunciation patterns, have struggled to fully capture non-native errors. We propose two data-driven approaches using speech corpora to automatically detect mispronunciation patterns. By aligning non-native phones with their native counterparts using attention maps, we achieved a 5.7% improvement in speech recognition on native English datasets and a 12.8% improvement for non-native English speakers, particularly Korean speakers. Our method offers practical advancements for robust Automatic Speech Recognition (ASR) systems particularly for situations where prior linguistic knowledge is not applicable. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæœºå™¨å­¦ä¹ çš„å‘å±•æå¤§åœ°æé«˜äº†è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œä½†è¯†åˆ«éæµåˆ©æˆ–å¸¦å£éŸ³çš„è¯´è¯è€…çš„è¯­éŸ³ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä»¥å‰çš„ç ”ç©¶ä¾èµ–äºåŸºäºè§„åˆ™çš„å‘å£°æ¨¡å¼ï¼Œéš¾ä»¥å®Œå…¨æ•æ‰éæœ¬åœ°å‘éŸ³é”™è¯¯ã€‚æˆ‘ä»¬æå‡ºä¸¤ç§é‡‡ç”¨è¯­éŸ³è¯­æ–™åº“çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œä»¥è‡ªåŠ¨æ£€æµ‹å‘éŸ³é”™è¯¯æ¨¡å¼ã€‚é€šè¿‡å¯¹éæœ¬åœ°è¯­éŸ³çš„éŸ³ç´ ä¸æœ¬åœ°è¯­éŸ³çš„éŸ³ç´ è¿›è¡Œå¯¹é½æ³¨æ„åŠ›å›¾ï¼Œæˆ‘ä»¬åœ¨æœ¬åœ°è‹±è¯­æ•°æ®é›†ä¸Šçš„è¯­éŸ³è¯†åˆ«ç‡æé«˜äº†5.7%ï¼Œå¯¹éæœ¬åœ°è‹±è¯­å£éŸ³çš„è¯†åˆ«ç‡æé«˜äº†12.8%ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éŸ©å›½å£éŸ³çš„è¯†åˆ«ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæ„å»ºç¨³å¥çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæä¾›äº†å®é™…è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ³•åº”ç”¨å…ˆéªŒè¯­è¨€çŸ¥è¯†çš„æƒ…å†µä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00583v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸæœºå™¨å­¦ä¹ æŠ€æœ¯çš„è¿›å±•æå¤§åœ°æå‡äº†è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œä½†å¯¹äºéæµåˆ©æˆ–å¸¦å£éŸ³çš„è¯´è¯è€…çš„è¯­éŸ³è¯†åˆ«ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä»¥å¾€ä¾èµ–è§„åˆ™å‘éŸ³æ¨¡å¼çš„æ–¹æ³•éš¾ä»¥å®Œå…¨æ•æ‰éæ¯è¯­è€…çš„å‘éŸ³é”™è¯¯ã€‚æœ¬ç ”ç©¶æå‡ºä¸¤ç§æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œåˆ©ç”¨è¯­éŸ³è¯­æ–™åº“è‡ªåŠ¨æ£€æµ‹å‘éŸ³é”™è¯¯æ¨¡å¼ã€‚é€šè¿‡å¯¹éæ¯è¯­éŸ³ç´ ä¸æ¯è¯­éŸ³ç´ çš„æ³¨æ„åŠ›æ˜ å°„å¯¹é½ï¼Œæœ¬ç ”ç©¶åœ¨æ¯è¯­è‹±è¯­æ•°æ®é›†ä¸Šæé«˜äº†5.7%çš„è¯­éŸ³è¯†åˆ«ç‡ï¼Œå¯¹éæ¯è¯­è‹±è¯­è€…ï¼ˆå°¤å…¶æ˜¯éŸ©è¯­èƒŒæ™¯ï¼‰æé«˜äº†12.8%ã€‚è¯¥æ–¹æ³•ä¸ºæ— å…ˆéªŒè¯­è¨€çŸ¥è¯†çš„æƒ…å¢ƒæä¾›äº†ç¨³å¥çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å®é™…åº”ç”¨è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœºå™¨å­¦ä¹ è¿›æ­¥æå‡äº†è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œä½†éæµåˆ©æˆ–å¸¦å£éŸ³çš„è¯´è¯è€…è¯†åˆ«ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ä»¥å¾€çš„è§„åˆ™å‘éŸ³æ¨¡å¼æ–¹æ³•åœ¨æ•æ‰éæ¯è¯­è€…å‘éŸ³é”™è¯¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸¤ç§æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œåˆ©ç”¨è¯­éŸ³è¯­æ–™åº“è‡ªåŠ¨æ£€æµ‹è¯¯è¯»æ¨¡å¼ã€‚</li>
<li>é€šè¿‡æ³¨æ„åŠ›æ˜ å°„å¯¹é½ï¼Œæé«˜äº†è¯­éŸ³è¯†åˆ«ç‡ã€‚</li>
<li>åœ¨æ¯è¯­è‹±è¯­æ•°æ®é›†ä¸Šï¼Œè¯­éŸ³è¯†åˆ«ç‡æé«˜äº†5.7%ã€‚</li>
<li>å¯¹éæ¯è¯­è‹±è¯­è€…ï¼ˆå°¤å…¶æ˜¯éŸ©è¯­èƒŒæ™¯ï¼‰ï¼Œè¯­éŸ³è¯†åˆ«ç‡æé«˜äº†12.8%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-261bd77a112343dea5e9cdc1dbfe84d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0646aabbfb9b133a5881239820eef9e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0e55d5f51b117857b30623e0f27684e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cded2560b53bbef38cfe30ba622cd80c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Evaluation-of-End-to-End-Continuous-Spanish-Lipreading-in-Different-Data-Conditions"><a href="#Evaluation-of-End-to-End-Continuous-Spanish-Lipreading-in-Different-Data-Conditions" class="headerlink" title="Evaluation of End-to-End Continuous Spanish Lipreading in Different Data   Conditions"></a>Evaluation of End-to-End Continuous Spanish Lipreading in Different Data   Conditions</h2><p><strong>Authors:David Gimeno-GÃ³mez, Carlos-D. MartÃ­nez-Hinarejos</strong></p>
<p>Visual speech recognition remains an open research problem where different challenges must be considered by dispensing with the auditory sense, such as visual ambiguities, the inter-personal variability among speakers, and the complex modeling of silence. Nonetheless, recent remarkable results have been achieved in the field thanks to the availability of large-scale databases and the use of powerful attention mechanisms. Besides, multiple languages apart from English are nowadays a focus of interest. This paper presents noticeable advances in automatic continuous lipreading for Spanish. First, an end-to-end system based on the hybrid CTC&#x2F;Attention architecture is presented. Experiments are conducted on two corpora of disparate nature, reaching state-of-the-art results that significantly improve the best performance obtained to date for both databases. In addition, a thorough ablation study is carried out, where it is studied how the different components that form the architecture influence the quality of speech recognition. Then, a rigorous error analysis is carried out to investigate the different factors that could affect the learning of the automatic system. Finally, a new Spanish lipreading benchmark is consolidated. Code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading">https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading</a>. </p>
<blockquote>
<p>è§†è§‰è¯­éŸ³è¯†åˆ«ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é—®é¢˜ï¼Œéœ€è¦å…‹æœæŠ›å¼ƒå¬è§‰æ„Ÿå®˜æ‰€å¸¦æ¥çš„å„ç§æŒ‘æˆ˜ï¼Œå¦‚è§†è§‰æ¨¡ç³Šæ€§ã€ä¸åŒä¸ªä½“ä¹‹é—´çš„å·®å¼‚ä»¥åŠæ²‰é»˜çš„å¤æ‚å»ºæ¨¡ç­‰ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç”±äºå¤§è§„æ¨¡æ•°æ®åº“çš„å‡ºç°å’Œå¼ºå¤§æ³¨æ„åŠ›æœºåˆ¶çš„åˆ©ç”¨ï¼Œè¯¥é¢†åŸŸå·²ç»å–å¾—äº†ä»¤äººç©ç›®çš„æˆæœã€‚æ­¤å¤–ï¼Œé™¤äº†è‹±è¯­ä¹‹å¤–ï¼Œå¤šç§è¯­è¨€å¦‚ä»Šä¹Ÿå¼•èµ·äº†äººä»¬çš„å…³æ³¨ã€‚æœ¬æ–‡ä»‹ç»äº†è¥¿ç­ç‰™è¯­çš„è‡ªåŠ¨è¿ç»­å”‡è¯»æ–¹é¢çš„æ˜¾è‘—è¿›å±•ã€‚é¦–å…ˆï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ··åˆCTC&#x2F;æ³¨æ„åŠ›æ¶æ„çš„ç«¯åˆ°ç«¯ç³»ç»Ÿã€‚å®éªŒåœ¨ä¸¤ä¸ªæ€§è´¨å„å¼‚çš„è¯­æ–™åº“ä¸Šè¿›è¡Œï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œæ˜¾è‘—æé«˜äº†è¿™ä¸¤ä¸ªæ•°æ®åº“çš„æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œç ”ç©¶äº†æ„æˆæ¶æ„çš„ä¸åŒéƒ¨åˆ†å¯¹è¯­éŸ³è¯†åˆ«è´¨é‡çš„å½±å“ã€‚æ¥ç€ï¼Œè¿›è¡Œäº†ä¸¥æ ¼çš„è¯¯å·®åˆ†æï¼Œä»¥è°ƒæŸ¥å¯èƒ½å½±å“è‡ªåŠ¨ç³»ç»Ÿå­¦ä¹ çš„ä¸åŒå› ç´ ã€‚æœ€åå·©å›ºäº†è¥¿ç­ç‰™å”‡è¯»çš„æ–°åŸºå‡†ã€‚ä»£ç å’Œè®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/david-gimeno/evaluating-end2end-spanish-lipreadingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00464v1">PDF</a> Accepted in the â€œLanguage Resources and Evaluationâ€ journal, Springer   Nature</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¯­éŸ³è¯†åˆ«ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é—®é¢˜ï¼Œéœ€è¦å…‹æœè§†è§‰æ¨¡ç³Šæ€§ã€ä¸åŒå‘éŸ³äººä¹‹é—´çš„å·®å¼‚ä»¥åŠæ²‰é»˜çš„å¤æ‚å»ºæ¨¡ç­‰æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼Œç”±äºå¤§è§„æ¨¡æ•°æ®åº“çš„å‡ºç°å’Œå¼ºå¤§æ³¨æ„åŠ›æœºåˆ¶çš„ä½¿ç”¨ï¼Œè¯¥é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§é’ˆå¯¹è¥¿ç­ç‰™è¯­è¿ç»­å”‡è¯»çš„è‡ªåŠ¨ç³»ç»Ÿçš„æœ€æ–°è¿›å±•ã€‚è¯¥ç«¯å¯¹ç«¯ç³»ç»Ÿé‡‡ç”¨æ··åˆCTC&#x2F;æ³¨æ„åŠ›æ¶æ„ï¼Œå®éªŒåœ¨ä¸¤ç§æ€§è´¨ä¸åŒçš„è¯­æ–™åº“ä¸Šè¿›è¡Œï¼Œè¾¾åˆ°å½“å‰æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾è‘—æ”¹å–„äº†ä¸¤æ•°æ®åº“çš„æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†æ·±å…¥å½»åº•çš„å‰–æç ”ç©¶ï¼Œæ¢è®¨äº†æ¶æ„çš„ä¸åŒç»„æˆéƒ¨åˆ†å¯¹è¯­éŸ³è¯†åˆ«è´¨é‡çš„å½±å“ã€‚æœ€åå·©å›ºäº†ä¸€é¡¹æ–°çš„è¥¿ç­ç‰™è¯­å”‡è¯»åŸºå‡†æµ‹è¯•ã€‚ç›¸å…³ä»£ç å’Œè®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/david-gimeno/evaluating-end2end-spanish-lipreadingä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­éŸ³è¯†åˆ«æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é—®é¢˜ï¼Œéœ€è¦å…‹æœè§†è§‰æ¨¡ç³Šæ€§ã€ä¸åŒå‘éŸ³äººä¹‹é—´çš„å·®å¼‚ä»¥åŠæ²‰é»˜çš„å¤æ‚å»ºæ¨¡ç­‰æŒ‘æˆ˜ã€‚</li>
<li>å¤§è§„æ¨¡æ•°æ®åº“å’Œæ³¨æ„åŠ›æœºåˆ¶çš„åº”ç”¨æ¨åŠ¨äº†è§†è§‰è¯­éŸ³è¯†åˆ«é¢†åŸŸçš„æ˜¾è‘—è¿›å±•ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è¥¿ç­ç‰™è¯­çš„è‡ªåŠ¨è¿ç»­å”‡è¯»ç³»ç»Ÿï¼Œé‡‡ç”¨æ··åˆCTC&#x2F;æ³¨æ„åŠ›æ¶æ„ï¼Œå¹¶åœ¨ä¸¤ä¸ªè¯­æ–™åº“ä¸Šè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</li>
<li>ç³»ç»Ÿæ¶æ„çš„ä¸åŒç»„æˆéƒ¨åˆ†å¯¹è¯­éŸ³è¯†åˆ«è´¨é‡æœ‰æ˜¾è‘—å½±å“ï¼Œè¿›è¡Œäº†æ·±å…¥çš„å‰–æç ”ç©¶ã€‚</li>
<li>è¿›è¡Œäº†ä¸¥è°¨çš„é”™è¯¯åˆ†æï¼Œä»¥ç ”ç©¶å¯èƒ½å½±å“è‡ªåŠ¨ç³»ç»Ÿå­¦ä¹ çš„ä¸åŒå› ç´ ã€‚</li>
<li>å·©å›ºäº†ä¸€é¡¹æ–°çš„è¥¿ç­ç‰™è¯­å”‡è¯»åŸºå‡†æµ‹è¯•ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†è¯„ä¼°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1156a04188a1abde75d3a1e4b5e09f92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f09988abb87c7d7041312ff51e7f76e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-488ef386296c2b0feb5ac88512445772.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Sagalee-an-Open-Source-Automatic-Speech-Recognition-Dataset-for-Oromo-Language"><a href="#Sagalee-an-Open-Source-Automatic-Speech-Recognition-Dataset-for-Oromo-Language" class="headerlink" title="Sagalee: an Open Source Automatic Speech Recognition Dataset for Oromo   Language"></a>Sagalee: an Open Source Automatic Speech Recognition Dataset for Oromo   Language</h2><p><strong>Authors:Turi Abu, Ying Shi, Thomas Fang Zheng, Dong Wang</strong></p>
<p>We present a novel Automatic Speech Recognition (ASR) dataset for the Oromo language, a widely spoken language in Ethiopia and neighboring regions. The dataset was collected through a crowd-sourcing initiative, encompassing a diverse range of speakers and phonetic variations. It consists of 100 hours of real-world audio recordings paired with transcriptions, covering read speech in both clean and noisy environments. This dataset addresses the critical need for ASR resources for the Oromo language which is underrepresented. To show its applicability for the ASR task, we conducted experiments using the Conformer model, achieving a Word Error Rate (WER) of 15.32% with hybrid CTC and AED loss and WER of 18.74% with pure CTC loss. Additionally, fine-tuning the Whisper model resulted in a significantly improved WER of 10.82%. These results establish baselines for Oromo ASR, highlighting both the challenges and the potential for improving ASR performance in Oromo. The dataset is publicly available at <a target="_blank" rel="noopener" href="https://github.com/turinaf/sagalee">https://github.com/turinaf/sagalee</a> and we encourage its use for further research and development in Oromo speech processing. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºåŸƒå¡ä¿„æ¯”äºšåŠå…¶å‘¨è¾¹åœ°åŒºå¹¿æ³›ä½¿ç”¨çš„å¥¥ç½—è«è¯­ï¼ˆOromo languageï¼‰å‘ˆç°ä¸€ä¸ªæ–°çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ˜¯é€šè¿‡ä¼—åŒ…å€¡è®®æ”¶é›†çš„ï¼Œæ¶µç›–äº†å„ç§å‘éŸ³äººå’Œè¯­éŸ³å˜åŒ–ã€‚å®ƒåŒ…å«äº†10ä¸‡å°æ—¶çš„çœŸå®ä¸–ç•ŒéŸ³é¢‘å½•åˆ¶ä¸æ–‡å­—è®°å½•é…å¯¹ï¼Œæ¶µç›–äº†å¹²å‡€å’Œå˜ˆæ‚ç¯å¢ƒä¸­çš„æœ—è¯»è¯­éŸ³ã€‚è¯¥æ•°æ®é›†è§£å†³äº†å¥¥ç½—è«è¯­åœ¨ASRèµ„æºæ–¹é¢çš„è¿«åˆ‡éœ€æ±‚ï¼Œè€Œè¿™ç§è¯­è¨€åœ¨èµ„æºä¸Šå´è¢«å¿½è§†ã€‚ä¸ºäº†å±•ç¤ºå…¶åœ¨ASRä»»åŠ¡ä¸­çš„åº”ç”¨æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨Conformeræ¨¡å‹è¿›è¡Œäº†å®éªŒï¼Œä½¿ç”¨æ··åˆCTCå’ŒAEDæŸå¤±æ—¶è·å¾—äº†15.32%çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œä½¿ç”¨çº¯CTCæŸå¤±æ—¶è·å¾—äº†18.74%çš„WERã€‚æ­¤å¤–ï¼Œå¯¹Whisperæ¨¡å‹è¿›è¡Œå¾®è°ƒåï¼ŒWERæ˜¾è‘—æå‡è‡³10.82%ã€‚è¿™äº›ç»“æœä¸ºå¥¥ç½—è«è¯­çš„ASRå»ºç«‹äº†åŸºå‡†çº¿ï¼Œæ—¢çªå‡ºäº†æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¹Ÿå‡¸æ˜¾äº†æé«˜ASRæ€§èƒ½çš„å¯èƒ½æ€§ã€‚è¯¥æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/turinaf/sagalee%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%EF%BC%8C%E6%88%91%E4%BB%AC%E9%BC%93%E5%8A%B1%E5%9C%A8%E5%A5%A5%E7%BD%97%E8%8E%AB%E8%AF%AD%E9%9F%B3%E5%A4%84%E7%90%86%E6%96%B9%E9%9D%A2%E5%BC%80%E5%B1%95%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BC%80%E5%8F%91%E6%97%B6%E4%BD%BF%E7%94%A8%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/turinaf/sagaleeå…¬å¼€è®¿é—®ï¼Œæˆ‘ä»¬é¼“åŠ±åœ¨å¥¥ç½—è«è¯­éŸ³å¤„ç†æ–¹é¢å¼€å±•è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¼€å‘æ—¶ä½¿ç”¨è¯¥æ•°æ®é›†ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00421v1">PDF</a> Accepted for ICASSP2025 (2025 IEEE International Conference on   Acoustics, Speech, and Signal Processing)</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä¸ºåŸƒå¡ä¿„æ¯”äºšåŠå‘¨è¾¹åœ°åŒºå¹¿æ³›ä½¿ç”¨çš„å¥¥ç½—è«è¯­ï¼ˆOromoï¼‰è¯­è¨€æ‰“é€ çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¼—åŒ…æ–¹å¼æ”¶é›†ï¼Œæ¶µç›–äº†ä¸åŒå‘éŸ³å’Œå£éŸ³çš„éŸ³é¢‘è®°å½•ï¼Œå…±100å°æ—¶ã€‚æ–‡ç« è¿˜ä»‹ç»äº†ä½¿ç”¨Conformeræ¨¡å‹è¿›è¡ŒASRä»»åŠ¡çš„å®éªŒç»“æœï¼ŒåŒ…æ‹¬æ··åˆCTCå’ŒAEDæŸå¤±çš„WERä¸º15.32%ï¼Œçº¯CTCæŸå¤±çš„WERä¸º18.74%ã€‚å¾®è°ƒWhisperæ¨¡å‹åï¼ŒWERæ˜¾è‘—æé«˜åˆ°10.82%ã€‚æ­¤æ•°æ®é›†å·²åœ¨å…¬å¼€å¹³å°ä¸Šå‘å¸ƒï¼Œå¹¶é¼“åŠ±ç”¨äºè¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘å¥¥ç½—è«è¯­éŸ³å¤„ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†é’ˆå¯¹å¥¥ç½—è«è¯­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡ä¼—åŒ…æ–¹å¼æ”¶é›†ï¼ŒåŒ…å«å„ç§å‘éŸ³å’Œå£éŸ³çš„éŸ³é¢‘è®°å½•ï¼Œå…±100å°æ—¶ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–äº†æ¸…æ´å’Œå˜ˆæ‚ç¯å¢ƒä¸‹çš„æœ—è¯»è¯­éŸ³ã€‚</li>
<li>ä½¿ç”¨Conformeræ¨¡å‹è¿›è¡ŒASRå®éªŒï¼Œæ··åˆCTCå’ŒAEDæŸå¤±çš„WERä¸º15.32%ï¼Œçº¯CTCæŸå¤±çš„WERä¸º18.74%ã€‚</li>
<li>é€šè¿‡å¯¹Whisperæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†ASRæ€§èƒ½ï¼ŒWERé™è‡³10.82%ã€‚</li>
<li>æ•°æ®é›†å…¬å¼€å¯ç”¨ï¼Œé¼“åŠ±ç”¨äºå¥¥ç½—è«è¯­éŸ³å¤„ç†çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-481fa27014a7d51fd6b1cabbe56352d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23dc622292d3dd146517fa200f623d53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cd2d5063b4d78ecfe44d960c6fc4876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80bf3e68df90a5dcb2f350d1e4395307.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-752670e9ec914c805ed8e09ee44b7c0a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognition"><a href="#SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognition" class="headerlink" title="SigWavNet: Learning Multiresolution Signal Wavelet Network for Speech   Emotion Recognition"></a>SigWavNet: Learning Multiresolution Signal Wavelet Network for Speech   Emotion Recognition</h2><p><strong>Authors:Alaa Nfissi, Wassim Bouachir, Nizar Bouguila, Brian Mishara</strong></p>
<p>In the field of human-computer interaction and psychological assessment, speech emotion recognition (SER) plays an important role in deciphering emotional states from speech signals. Despite advancements, challenges persist due to system complexity, feature distinctiveness issues, and noise interference. This paper introduces a new end-to-end (E2E) deep learning multi-resolution framework for SER, addressing these limitations by extracting meaningful representations directly from raw waveform speech signals. By leveraging the properties of the fast discrete wavelet transform (FDWT), including the cascade algorithm, conjugate quadrature filter, and coefficient denoising, our approach introduces a learnable model for both wavelet bases and denoising through deep learning techniques. The framework incorporates an activation function for learnable asymmetric hard thresholding of wavelet coefficients. Our approach exploits the capabilities of wavelets for effective localization in both time and frequency domains. We then combine one-dimensional dilated convolutional neural networks (1D dilated CNN) with a spatial attention layer and bidirectional gated recurrent units (Bi-GRU) with a temporal attention layer to efficiently capture the nuanced spatial and temporal characteristics of emotional features. By handling variable-length speech without segmentation and eliminating the need for pre or post-processing, the proposed model outperformed state-of-the-art methods on IEMOCAP and EMO-DB datasets. The source code of this paper is shared on the Github repository: <a target="_blank" rel="noopener" href="https://github.com/alaaNfissi/SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognition">https://github.com/alaaNfissi/SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognition</a>. </p>
<blockquote>
<p>åœ¨äººæœºäº¤äº’å’Œå¿ƒç†è¯„ä¼°é¢†åŸŸï¼Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰åœ¨ä»è¯­éŸ³ä¿¡å·ä¸­è§£è¯»æƒ…æ„ŸçŠ¶æ€æ–¹é¢æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚å°½ç®¡å·²æœ‰è¿›å±•ï¼Œä½†ç”±äºç³»ç»Ÿå¤æ‚æ€§ã€ç‰¹å¾è¾¨åˆ«é—®é¢˜å’Œå™ªå£°å¹²æ‰°ï¼ŒæŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰æ·±åº¦å­¦ä¹ å¤šåˆ†è¾¨ç‡SERæ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥ä»åŸå§‹æ³¢å½¢è¯­éŸ³ä¿¡å·ä¸­æå–æœ‰æ„ä¹‰çš„è¡¨ç°æ¥å…‹æœè¿™äº›é™åˆ¶ã€‚é€šè¿‡åˆ©ç”¨å¿«é€Ÿç¦»æ•£å°æ³¢å˜æ¢ï¼ˆFDWTï¼‰çš„å±æ€§ï¼ŒåŒ…æ‹¬çº§è”ç®—æ³•ã€å…±è½­æ­£äº¤æ»¤æ³¢å™¨å’Œç³»æ•°å»å™ªï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸ºå°æ³¢åŸºå’Œå»å™ªå¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„æ¨¡å‹ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œç”¨äºå¯¹å°æ³¢ç³»æ•°è¿›è¡Œå¯å­¦ä¹ çš„ä¸å¯¹ç§°ç¡¬é˜ˆå€¼å¤„ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å°æ³¢åœ¨æ—¶é—´åŸŸå’Œé¢‘åŸŸçš„ç²¾ç¡®å®šä½èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä¸€ç»´è†¨èƒ€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆ1D dilated CNNï¼‰ä¸ç©ºé—´æ³¨æ„åŠ›å±‚ç›¸ç»“åˆï¼Œå°†åŒå‘é—¨æ§å¾ªç¯å•å…ƒï¼ˆBi-GRUï¼‰ä¸ä¸´æ—¶æ³¨æ„åŠ›å±‚ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆåœ°æ•æ‰æƒ…æ„Ÿç‰¹å¾çš„ç»†å¾®ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¯å˜é•¿åº¦çš„è¯­éŸ³ï¼Œæ— éœ€åˆ†æ®µã€é¢„å¤„ç†æˆ–åå¤„ç†ï¼Œä¸”åœ¨IEMOCAPå’ŒEMO-DBæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æœ¬æ–‡çš„æºä»£ç å·²å…±äº«åœ¨Githubä»“åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/alaaNfissi/SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognition%E3%80%82">https://github.com/alaaNfissi/SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognitionã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00310v1">PDF</a> Published in: IEEE Transactions on Affective Computing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é¢†åŸŸçš„æ–°å‘å±•ã€‚æ–‡ç« æå‡ºä¸€ç§ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰æ·±åº¦å­¦ä¹ å¤šåˆ†è¾¨ç‡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»åŸå§‹æ³¢å½¢è¯­éŸ³ä¿¡å·ä¸­æå–æœ‰æ„ä¹‰çš„è¡¨ç°ï¼Œé€šè¿‡åˆ©ç”¨å¿«é€Ÿç¦»æ•£å°æ³¢å˜æ¢ï¼ˆFDWTï¼‰çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬çº§è”ç®—æ³•ã€å…±è½­æ­£äº¤æ»¤æ³¢å™¨å’Œç³»æ•°å»å™ªæŠ€æœ¯æ¥è§£å†³ç³»ç»Ÿå¤æ‚æ€§ã€ç‰¹å¾å·®å¼‚é—®é¢˜å’Œå™ªå£°å¹²æ‰°ç­‰æŒ‘æˆ˜ã€‚ç»“åˆæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå¼•å…¥å¯å­¦ä¹ çš„å°æ³¢åŸºå’Œå»å™ªæ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨å°æ³¢åœ¨æ—¶é—´åŸŸå’Œé¢‘åŸŸçš„æœ‰æ•ˆå®šä½èƒ½åŠ›ï¼Œç»“åˆäº†å…·å¤‡ç©ºé—´æ³¨æ„åŠ›å±‚çš„ä¸€ç»´è†¨èƒ€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆ1D dilated CNNï¼‰å’Œå…·å¤‡æ—¶é—´æ³¨æ„åŠ›å±‚çš„åŒå‘é—¨æ§å¾ªç¯å•å…ƒï¼ˆBi-GRUï¼‰ï¼Œä»¥æ•æ‰æƒ…æ„Ÿç‰¹å¾çš„å¾®å¦™ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚æ‰€æå‡ºçš„æ¨¡å‹æ— éœ€åˆ†æ®µå¤„ç†å¯å˜é•¿åº¦çš„è¯­éŸ³ï¼Œä¸”åœ¨IEMOCAPå’ŒEMO-DBæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰åœ¨äººæœºäº¤äº’å’Œå¿ƒç†è¯„ä¼°é¢†åŸŸå…·æœ‰é‡è¦ä½œç”¨ï¼Œèƒ½å¤Ÿä»è¯­éŸ³ä¿¡å·ä¸­è§£ææƒ…æ„ŸçŠ¶æ€ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰æ·±åº¦å­¦ä¹ å¤šåˆ†è¾¨ç‡æ¡†æ¶ï¼Œè§£å†³äº†ç³»ç»Ÿå¤æ‚æ€§ã€ç‰¹å¾å·®å¼‚å’Œå™ªå£°å¹²æ‰°ç­‰æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨å¿«é€Ÿç¦»æ•£å°æ³¢å˜æ¢ï¼ˆFDWTï¼‰çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬çº§è”ç®—æ³•ã€å…±è½­æ­£äº¤æ»¤æ³¢å™¨å’Œç³»æ•°å»å™ªæŠ€æœ¯ã€‚</li>
<li>å¼•å…¥å¯å­¦ä¹ çš„å°æ³¢åŸºå’Œå»å™ªæ¨¡å‹ï¼Œé€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯å®ç°ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†å…·å¤‡ç©ºé—´æ³¨æ„åŠ›å±‚çš„ä¸€ç»´è†¨èƒ€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆ1D dilated CNNï¼‰å’Œå…·å¤‡æ—¶é—´æ³¨æ„åŠ›å±‚çš„åŒå‘é—¨æ§å¾ªç¯å•å…ƒï¼ˆBi-GRUï¼‰ï¼Œä»¥æ•æ‰æƒ…æ„Ÿç‰¹å¾çš„ç»†å¾®å·®å¼‚ã€‚</li>
<li>æ¨¡å‹æ— éœ€åˆ†æ®µå¤„ç†å¯å˜é•¿åº¦çš„è¯­éŸ³ï¼Œä¸”åœ¨ä¸»è¦æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65d6c95194467d589247b07cf9e804a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0cfaab8375702bea7305f00d3d48a729.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31c0eee4d89650ccfb3e986a05c70837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-635f6286b07562fc14775ecebb89ec8c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TCSinger-Zero-Shot-Singing-Voice-Synthesis-with-Style-Transfer-and-Multi-Level-Style-Control"><a href="#TCSinger-Zero-Shot-Singing-Voice-Synthesis-with-Style-Transfer-and-Multi-Level-Style-Control" class="headerlink" title="TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and   Multi-Level Style Control"></a>TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and   Multi-Level Style Control</h2><p><strong>Authors:Yu Zhang, Ziyue Jiang, Ruiqi Li, Changhao Pan, Jinzheng He, Rongjie Huang, Chuxin Wang, Zhou Zhao</strong></p>
<p>Zero-shot singing voice synthesis (SVS) with style transfer and style control aims to generate high-quality singing voices with unseen timbres and styles (including singing method, emotion, rhythm, technique, and pronunciation) from audio and text prompts. However, the multifaceted nature of singing styles poses a significant challenge for effective modeling, transfer, and control. Furthermore, current SVS models often fail to generate singing voices rich in stylistic nuances for unseen singers. To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control. Specifically, TCSinger proposes three primary modules: 1) the clustering style encoder employs a clustering vector quantization model to stably condense style information into a compact latent space; 2) the Style and Duration Language Model (S&amp;D-LM) concurrently predicts style information and phoneme duration, which benefits both; 3) the style adaptive decoder uses a novel mel-style adaptive normalization method to generate singing voices with enhanced details. Experimental results show that TCSinger outperforms all baseline models in synthesis quality, singer similarity, and style controllability across various tasks, including zero-shot style transfer, multi-level style control, cross-lingual style transfer, and speech-to-singing style transfer. Singing voice samples can be accessed at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSingerDemo/">https://aaronz345.github.io/TCSingerDemo/</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬å”±è…”åˆæˆï¼ˆSVSï¼‰æŠ€æœ¯å¸¦æœ‰é£æ ¼è¿ç§»å’Œé£æ ¼æ§åˆ¶åŠŸèƒ½ï¼Œæ—¨åœ¨æ ¹æ®éŸ³é¢‘å’Œæ–‡å­—æç¤ºç”Ÿæˆå…·æœ‰æœªè§éŸ³è‰²å’Œé£æ ¼çš„é«˜è´¨é‡æ­Œå£°ï¼ˆåŒ…æ‹¬å”±æ³•ã€æƒ…æ„Ÿã€èŠ‚å¥ã€æŠ€å·§å’Œå‘éŸ³ï¼‰ã€‚ç„¶è€Œï¼Œå”±è…”é£æ ¼çš„å¤šé¢æ€§ç»™æœ‰æ•ˆçš„å»ºæ¨¡ã€è¿ç§»å’Œæ§åˆ¶å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå½“å‰çš„SVSæ¨¡å‹å¾€å¾€æ— æ³•ä¸ºæœªè§è¿‡çš„æ­Œæ‰‹ç”Ÿæˆä¸°å¯Œç»†è…»çš„é£æ ¼åŒ–æ­Œå£°ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TCSingerï¼Œè¿™æ˜¯ä¸€æ¬¾é›¶æ ·æœ¬SVSæ¨¡å‹ï¼Œç”¨äºè·¨è·¨è¯­è¨€æ¼”è®²å’Œå”±è…”é£æ ¼è¿›è¡Œé£æ ¼è¿ç§»å’Œå¤šçº§é£æ ¼æ§åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒTCSingerä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š1ï¼‰èšç±»é£æ ¼ç¼–ç å™¨é‡‡ç”¨èšç±»å‘é‡é‡åŒ–æ¨¡å‹ï¼Œå°†é£æ ¼ä¿¡æ¯ç¨³å®šåœ°æµ“ç¼©åˆ°ä¸€ä¸ªç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼›2ï¼‰é£æ ¼å’Œæ—¶é•¿è¯­è¨€æ¨¡å‹ï¼ˆS&amp;D-LMï¼‰åŒæ—¶é¢„æµ‹é£æ ¼ä¿¡æ¯å’ŒéŸ³ç´ æ—¶é•¿ï¼Œä¸¤è€…éƒ½å—ç›Šï¼›3ï¼‰é£æ ¼è‡ªé€‚åº”è§£ç å™¨é‡‡ç”¨ä¸€ç§æ–°å‹æ¢…å°”é£æ ¼è‡ªé€‚åº”å½’ä¸€åŒ–æ–¹æ³•ï¼Œç”Ÿæˆå…·æœ‰å¢å¼ºç»†èŠ‚çš„æ­Œå£°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTCSingeråœ¨åˆæˆè´¨é‡ã€æ­Œæ‰‹ç›¸ä¼¼åº¦å’Œé£æ ¼å¯æ§æ€§æ–¹é¢å‡ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œæ¶µç›–å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬é£æ ¼è¿ç§»ã€å¤šçº§é£æ ¼æ§åˆ¶ã€è·¨è¯­è¨€é£æ ¼è¿ç§»å’Œè¯­éŸ³åˆ°å”±è…”çš„é£æ ¼è¿ç§»ã€‚æ‚¨å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSingerDemo/%E8%AE%BF%E9%97%AE%E5%94%B1%E8%85%94%E6%A0%B7%E6%9C%AC%E3%80%82">https://aaronz345.github.io/TCSingerDemo/è®¿é—®å”±è…”æ ·æœ¬ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15977v4">PDF</a> Accepted by EMNLP 2024</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶ä»‹ç»äº†é’ˆå¯¹é£æ ¼è¿ç§»ä¸æ§åˆ¶çš„é›¶æ ·æœ¬æ­Œå£°åˆæˆæŠ€æœ¯ï¼ˆSVSï¼‰ã€‚æå‡ºäº†TCSingeræ¨¡å‹ï¼Œå¯å®ç°åœ¨è·¨è¯­ç§çš„æ­Œå£°å’Œè¯­éŸ³é£æ ¼é—´çš„é›¶æ ·æœ¬é£æ ¼è¿ç§»ä¸å¤šå±‚æ¬¡é£æ ¼æ§åˆ¶ã€‚æ¨¡å‹åŒ…å«ä¸‰å¤§æ¨¡å—ï¼šèšç±»é£æ ¼ç¼–ç å™¨ã€é£æ ¼ä¸æ—¶é•¿è¯­è¨€æ¨¡å‹åŠé£æ ¼è‡ªé€‚åº”è§£ç å™¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTCSingeråœ¨åˆæˆè´¨é‡ã€æ­Œæ‰‹ç›¸ä¼¼æ€§åŠé£æ ¼å¯æ§æ€§æ–¹é¢å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œæ”¯æŒé›¶æ ·æœ¬é£æ ¼è¿ç§»ã€å¤šå±‚æ¬¡é£æ ¼æ§åˆ¶ã€è·¨è¯­ç§é£æ ¼è¿ç§»åŠè¯­éŸ³è½¬æ­Œå£°é£æ ¼è¿ç§»ã€‚æ›´å¤šæ­Œå£°æ ·æœ¬è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSingerDemo/">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨å®ç°é›¶æ ·æœ¬æ­Œå£°åˆæˆï¼ˆSVSï¼‰çš„é£æ ¼è¿ç§»ä¸æ§åˆ¶ï¼Œç”Ÿæˆå…·æœ‰æœªè§éŸ³è‰²å’Œé£æ ¼çš„é«˜è´¨é‡æ­Œå£°ã€‚</li>
<li>é¢ä¸´æŒ‘æˆ˜ï¼šæ­Œå”±é£æ ¼çš„å¤šæ ·æ€§ä½¿å¾—æœ‰æ•ˆå»ºæ¨¡ã€è¿ç§»ä¸æ§åˆ¶å˜å¾—å›°éš¾ï¼Œç°æœ‰SVSæ¨¡å‹éš¾ä»¥ç”Ÿæˆä¸°å¯Œé£æ ¼ç»†èŠ‚çš„æ­Œå£°ã€‚</li>
<li>å¼•å…¥TCSingeræ¨¡å‹ï¼Œé¦–æ¬¡å®ç°è·¨è¯­ç§æ­Œå£°ä¸è¯­éŸ³é£æ ¼çš„é›¶æ ·æœ¬é£æ ¼è¿ç§»åŠå¤šå±‚æ¬¡é£æ ¼æ§åˆ¶ã€‚</li>
<li>TCSingeråŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šèšç±»é£æ ¼ç¼–ç å™¨ã€é£æ ¼ä¸æ—¶é•¿è¯­è¨€æ¨¡å‹ã€é£æ ¼è‡ªé€‚åº”è§£ç å™¨ã€‚</li>
<li>å®éªŒè¯æ˜TCSingeråœ¨åˆæˆè´¨é‡ã€æ­Œæ‰‹ç›¸ä¼¼æ€§å’Œé£æ ¼å¯æ§æ€§æ–¹é¢è¶…è¶ŠåŸºçº¿æ¨¡å‹ã€‚</li>
<li>æ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬é£æ ¼è¿ç§»ã€å¤šå±‚æ¬¡é£æ ¼æ§åˆ¶ã€è·¨è¯­ç§é£æ ¼è¿ç§»åŠè¯­éŸ³è½¬æ­Œå£°é£æ ¼è¿ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c0e597e9bd762f3b5f7c1f2d67fcf2f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a3202a7091e1b4c968475e48fc1ace2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb2a5d55364652a98ef3c9cb82f2e29.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GTSinger-A-Global-Multi-Technique-Singing-Corpus-with-Realistic-Music-Scores-for-All-Singing-Tasks"><a href="#GTSinger-A-Global-Multi-Technique-Singing-Corpus-with-Realistic-Music-Scores-for-All-Singing-Tasks" class="headerlink" title="GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music   Scores for All Singing Tasks"></a>GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music   Scores for All Singing Tasks</h2><p><strong>Authors:Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li, Zhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu, Zhiqing Hong, Chuxin Wang, LiChao Zhang, Jinzheng He, Ziyue Jiang, Yuxin Chen, Chen Yang, Jiecheng Zhou, Xinyu Cheng, Zhou Zhao</strong></p>
<p>The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present GTSinger, a large global, multi-technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset; (2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion. The corpus and demos can be found at <a target="_blank" rel="noopener" href="http://aaronz345.github.io/GTSingerDemo/">http://aaronz345.github.io/GTSingerDemo/</a>. We provide the dataset and the code for processing data and conducting benchmarks at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/GTSinger/GTSinger">https://huggingface.co/datasets/GTSinger/GTSinger</a> and <a target="_blank" rel="noopener" href="https://github.com/AaronZ345/GTSinger">https://github.com/AaronZ345/GTSinger</a>. </p>
<blockquote>
<p>é¢å¯¹é«˜è´¨é‡å¤šä»»åŠ¡æ­Œå”±æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ï¼Œæå¤§åœ°é˜»ç¢äº†å¤šæ ·åŒ–å¯æ§å’Œä¸ªäººåŒ–æ­Œå”±ä»»åŠ¡çš„å‘å±•ã€‚ç°æœ‰æ­Œå”±æ•°æ®é›†å­˜åœ¨è´¨é‡é—®é¢˜ã€è¯­è¨€ä¸æ­Œæ‰‹å¤šæ ·æ€§æœ‰é™ã€ç¼ºä¹å¤šæŠ€æœ¯ä¿¡æ¯å’Œç°å®éŸ³ä¹ä¹è°±ä»¥åŠä»»åŠ¡é€‚ç”¨æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GTSingerï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹çš„å…¨çƒæ€§ã€å¤šæŠ€æœ¯ã€å…è´¹ä½¿ç”¨çš„é«˜è´¨é‡æ­Œå”±è¯­æ–™åº“ï¼Œå…·æœ‰ç°å®éŸ³ä¹ä¹è°±ï¼Œé€‚ç”¨äºæ‰€æœ‰æ­Œå”±ä»»åŠ¡ï¼Œä»¥åŠç›¸åº”çš„åŸºå‡†æµ‹è¯•ã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆ1ï¼‰æˆ‘ä»¬æ”¶é›†äº†80.59å°æ—¶çš„é«˜è´¨é‡æ­Œå£°ï¼Œå½¢æˆäº†æœ€å¤§çš„å½•éŸ³æ­Œå”±æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰20ä½ä¸“ä¸šæ­Œæ‰‹è·¨è¶Šä¹ç§å¹¿æ³›ä½¿ç”¨çš„è¯­è¨€ï¼Œå±•ç°äº†å¤šæ ·çš„éŸ³è‰²å’Œé£æ ¼ï¼›ï¼ˆ3ï¼‰æˆ‘ä»¬æä¾›äº†å…­ç§å¸¸ç”¨æ­Œå”±æŠ€æœ¯çš„å—æ§æ¯”è¾ƒå’ŒéŸ³ç´ çº§æ³¨é‡Šï¼Œæœ‰åŠ©äºæŠ€æœ¯å»ºæ¨¡å’Œæ§åˆ¶ï¼›ï¼ˆ4ï¼‰GTSingeræä¾›äº†ç°å®çš„ä¹è°±ï¼Œæœ‰åŠ©äºç°å®éŸ³ä¹åˆ›ä½œï¼›ï¼ˆ5ï¼‰æ­Œå£°ä¼´æœ‰æ‰‹åŠ¨éŸ³ç´ åˆ°éŸ³é¢‘å¯¹é½ã€å…¨å±€é£æ ¼æ ‡ç­¾ä»¥åŠç”¨äºå„ç§æ­Œå”±ä»»åŠ¡çš„16.16å°æ—¶é…å¯¹è¯­éŸ³ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ–¹ä¾¿ä½¿ç”¨GTSingerï¼Œæˆ‘ä»¬è¿›è¡Œäº†å››é¡¹åŸºå‡†å®éªŒï¼šæŠ€æœ¯å¯æ§çš„æ­Œå”±å£°éŸ³åˆæˆã€æŠ€æœ¯è¯†åˆ«ã€é£æ ¼è½¬æ¢å’Œè¯­éŸ³åˆ°æ­Œå”±çš„è½¬æ¢ã€‚è¯­æ–™åº“å’Œæ¼”ç¤ºå¯ä»¥åœ¨<a target="_blank" rel="noopener" href="http://aaronz345.github.io/GTSingerDemo/%E6%89%BE%E5%88%B0%E3%80%82%E6%88%91%E4%BB%AC%E5%9C%A8https://huggingface.co/datasets/GTSinger/GTSinger%E5%92%8Chttps://github.com/AaronZ345/GTSinger%E6%8F%90%E4%BE%9B%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%8F%8A%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">http://aaronz345.github.io/GTSingerDemo/æ‰¾åˆ°ã€‚æˆ‘ä»¬åœ¨https://huggingface.co/datasets/GTSinger/GTSingerå’Œhttps://github.com/AaronZ345/GTSingeræä¾›äº†æ•°æ®é›†å’Œæ•°æ®å¤„ç†åŠåŸºå‡†æµ‹è¯•çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13832v5">PDF</a> Accepted by NeurIPS 2024 (Spotlight)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹é«˜è´¨é‡å¤šä»»åŠ¡æ­Œå”±æ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤§å‹å…¨çƒã€å¤šæŠ€æœ¯ã€å…è´¹ä½¿ç”¨çš„GTSingeré«˜è´¨é‡æ­Œå”±è¯­æ–™åº“ï¼Œå¹¶è®¾è®¡äº†é€‚ç”¨äºæ‰€æœ‰æ­Œå”±ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥è¯­æ–™åº“åŒ…å«80.59å°æ—¶çš„é«˜è´¨é‡æ­Œå”±å£°éŸ³æ•°æ®ï¼Œæ¶‰åŠä¹ç§å¹¿æ³›ä½¿ç”¨çš„è¯­è¨€ï¼Œæä¾›äº†å…­ç§å¸¸ç”¨æ­Œå”±æŠ€æœ¯çš„å¯¹æ¯”å’ŒéŸ³ç´ çº§æ³¨é‡Šï¼Œä»¥åŠç°å®éŸ³ä¹ä¹è°±å’Œæ‰‹åŠ¨éŸ³ç´ åˆ°éŸ³é¢‘çš„å¯¹é½ç­‰æŠ€æœ¯ç‰¹ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å››é¡¹åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æŠ€å·§å¯æ§çš„æ­Œå”±å£°éŸ³åˆæˆã€æŠ€å·§è¯†åˆ«ã€é£æ ¼è½¬æ¢å’Œè¯­éŸ³åˆ°æ­Œå”±çš„è½¬æ¢ã€‚è¯¥è¯­æ–™åº“çš„æ•°æ®é›†å’Œç›¸å…³ä»£ç å¯ä»¥åœ¨ç›¸å…³ç½‘ç«™æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GTSingeræ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„å¤§å‹é«˜è´¨é‡æ­Œå”±è¯­æ–™åº“ï¼Œè§£å†³äº†ç°æœ‰æ­Œå”±æ•°æ®é›†è´¨é‡ä½ã€è¯­è¨€å¤šæ ·æ€§æœ‰é™ã€ç¼ºä¹å¤šæŠ€æœ¯ä¿¡æ¯å’Œç°å®éŸ³ä¹ä¹è°±ç­‰é—®é¢˜ã€‚</li>
<li>GTSingeråŒ…å«è¶…è¿‡80å°æ—¶çš„é«˜è´¨é‡æ­Œå”±å£°éŸ³æ•°æ®ï¼Œæ˜¯æœ€å¤§çš„å·²è®°å½•æ­Œå”±æ•°æ®é›†ä¹‹ä¸€ã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ¥è‡ªä¹ç§å¹¿æ³›ä½¿ç”¨çš„è¯­è¨€çš„20ä½ä¸“ä¸šæ­Œæ‰‹çš„å£°éŸ³ï¼Œå±•ç°äº†å¤šæ ·åŒ–çš„éŸ³è‰²å’Œé£æ ¼ã€‚</li>
<li>GTSingeræä¾›äº†å…­ç§å¸¸ç”¨æ­Œå”±æŠ€æœ¯çš„å¯¹æ¯”å’ŒéŸ³ç´ çº§æ³¨é‡Šï¼Œæœ‰åŠ©äºæŠ€æœ¯å»ºæ¨¡å’Œæ§åˆ¶ã€‚</li>
<li>æ•°æ®é›†æä¾›äº†ç°å®éŸ³ä¹ä¹è°±ï¼Œæ”¯æŒçœŸå®ä¸–ç•Œçš„éŸ³ä¹åˆ›ä½œã€‚</li>
<li>æ•°æ®é›†åŒ…æ‹¬æ‰‹åŠ¨éŸ³ç´ åˆ°éŸ³é¢‘çš„å¯¹é½ã€å…¨çƒé£æ ¼æ ‡ç­¾ä»¥åŠç”¨äºå„ç§æ­Œå”±ä»»åŠ¡çš„é…å¯¹è¯­éŸ³æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-221976a05e8577f72dd25b7de10f8cd5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0a829871655320f45719954820b9f50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f36701794ff88abe07da6e61077bca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad95cdd657d117546409a35f0b263aff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0f1a2777fc489b76f96c568fb6e77be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b8c04cc16af9c2620048c99092a7307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ee1e7f017c3f53d3af04c4f984bce33.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-757122fdac7bc16360dce1eb159cfbf7.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  AAD-DCE An Aggregated Multimodal Attention Mechanism for Early and Late   Dynamic Contrast Enhanced Prostate MRI Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-00c3e348cd14e5bff49c520592505ced.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  A Self-Supervised Framework for Improved Generalisability in Ultrasound   B-mode Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
