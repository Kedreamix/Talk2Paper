<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-06  Calibrated Multi-Preference Optimization for Aligning Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8846ef6514f9aa6d25a3aa58d18ef713.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    62 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-06-更新"><a href="#2025-02-06-更新" class="headerlink" title="2025-02-06 更新"></a>2025-02-06 更新</h1><h2 id="Calibrated-Multi-Preference-Optimization-for-Aligning-Diffusion-Models"><a href="#Calibrated-Multi-Preference-Optimization-for-Aligning-Diffusion-Models" class="headerlink" title="Calibrated Multi-Preference Optimization for Aligning Diffusion Models"></a>Calibrated Multi-Preference Optimization for Aligning Diffusion Models</h2><p><strong>Authors:Kyungmin Lee, Xiaohang Li, Qifei Wang, Junfeng He, Junjie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin, Feng Yang, Yinxiao Li</strong></p>
<p>Aligning text-to-image (T2I) diffusion models with preference optimization is valuable for human-annotated datasets, but the heavy cost of manual data collection limits scalability. Using reward models offers an alternative, however, current preference optimization methods fall short in exploiting the rich information, as they only consider pairwise preference distribution. Furthermore, they lack generalization to multi-preference scenarios and struggle to handle inconsistencies between rewards. To address this, we present Calibrated Preference Optimization (CaPO), a novel method to align T2I diffusion models by incorporating the general preference from multiple reward models without human annotated data. The core of our approach involves a reward calibration method to approximate the general preference by computing the expected win-rate against the samples generated by the pretrained models. Additionally, we propose a frontier-based pair selection method that effectively manages the multi-preference distribution by selecting pairs from Pareto frontiers. Finally, we use regression loss to fine-tune diffusion models to match the difference between calibrated rewards of a selected pair. Experimental results show that CaPO consistently outperforms prior methods, such as Direct Preference Optimization (DPO), in both single and multi-reward settings validated by evaluation on T2I benchmarks, including GenEval and T2I-Compbench. </p>
<blockquote>
<p>将文本到图像（T2I）扩散模型与偏好优化对齐对于人类注释数据集是有价值的，但手动数据收集的沉重成本限制了可扩展性。使用奖励模型提供了一种替代方案，然而，当前的偏好优化方法未能充分利用丰富信息，因为它们只考虑成对偏好分布。此外，它们缺乏在多种偏好场景下的通用性，并且难以处理奖励之间的一致性问题。为了解决这个问题，我们提出了校准偏好优化（CaPO）这一新方法，通过结合来自多个奖励模型的通用偏好，无需人类注释数据即可对齐T2I扩散模型。我们的方法的核心在于奖励校准方法，通过计算预训练模型生成的样本的预期胜率来近似通用偏好。此外，我们提出了一种基于前沿的配对选择方法，通过从帕累托前沿选择配对来有效地管理多偏好分布。最后，我们使用回归损失对扩散模型进行微调，以匹配选定配对的校准奖励之间的差异。实验结果表明，无论是在单一奖励还是多奖励设置下，CaPO在GenEval和T2I-Compbench等T2I基准测试上的表现均优于先前的方法，如直接偏好优化（DPO）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02588v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本到图像（T2I）扩散模型与偏好优化的对齐对于人类注释数据集具有价值，但手动数据收集的昂贵成本限制了其可扩展性。使用奖励模型作为替代方案，但当前偏好优化方法未能充分利用丰富信息，仅考虑成对偏好分布。此外，它们缺乏在多种偏好场景下的通用性，并难以处理奖励间的不一致性。为解决这些问题，我们提出了校准偏好优化（CaPO）方法，通过结合多个奖励模型的通用偏好，无需人类注释数据即可对齐T2I扩散模型。核心在于奖励校准方法，通过计算预期胜率来近似通用偏好，对抗由预训练模型生成的样本。同时，我们提出了基于前沿的配对选择方法，通过选择帕累托前沿的配对来有效管理多偏好分布。最后，使用回归损失对扩散模型进行微调，以匹配选定配对的校准奖励差异。实验结果表明，CaPO在单奖励和多奖励设置下均优于Direct Preference Optimization（DPO）方法，并在GenEval和T2I-Compbench等T2I基准测试中得到了验证。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像（T2I）扩散模型的偏好优化对于人类注释数据集至关重要，但手动数据收集成本高昂。</li>
<li>当前偏好优化方法仅考虑成对偏好分布，无法充分利用丰富信息。</li>
<li>现有方法缺乏在多种偏好场景下的通用性，难以处理奖励间的不一致性。</li>
<li>提出了校准偏好优化（CaPO）方法，结合多个奖励模型的通用偏好，无需人类注释数据即可对齐T2I扩散模型。</li>
<li>CaPO通过奖励校准方法和基于前沿的配对选择方法来管理多偏好分布。</li>
<li>CaPO使用回归损失微调扩散模型，以匹配选定配对的校准奖励差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02588">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-32b3c1027aad8bfef221257a011449a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dde4b5d77edebfb190e64f9b638cd715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8846ef6514f9aa6d25a3aa58d18ef713.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-404a4678fec2b5c96685b6cc2cd4fb1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-101789e96f34212aadcc1fe2b0055486.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73e29fdca2bbeea6c19ad2bc338f5330.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Privacy-Attacks-on-Image-AutoRegressive-Models"><a href="#Privacy-Attacks-on-Image-AutoRegressive-Models" class="headerlink" title="Privacy Attacks on Image AutoRegressive Models"></a>Privacy Attacks on Image AutoRegressive Models</h2><p><strong>Authors:Antoni Kowalczuk, Jan Dubiński, Franziska Boenisch, Adam Dziedzic</strong></p>
<p>Image autoregressive (IAR) models have surpassed diffusion models (DMs) in both image quality (FID: 1.48 vs. 1.58) and generation speed. However, their privacy risks remain largely unexplored. To address this, we conduct a comprehensive privacy analysis comparing IARs to DMs. We develop a novel membership inference attack (MIA) that achieves a significantly higher success rate in detecting training images (TPR@FPR&#x3D;1%: 86.38% for IARs vs. 4.91% for DMs). Using this MIA, we perform dataset inference (DI) and find that IARs require as few as six samples to detect dataset membership, compared to 200 for DMs, indicating higher information leakage. Additionally, we extract hundreds of training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight a fundamental privacy-utility trade-off: while IARs excel in generation quality and speed, they are significantly more vulnerable to privacy attacks. This suggests that incorporating techniques from DMs, such as per-token probability modeling using diffusion, could help mitigate IARs’ privacy risks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars">https://github.com/sprintml/privacy_attacks_against_iars</a>. </p>
<blockquote>
<p>图像自回归（IAR）模型在图像质量（FID：1.48 vs 1.58）和生成速度方面都超越了扩散模型（DMs）。然而，它们的隐私风险尚未得到充分探索。为了解决这个问题，我们对IARs和DMs进行了全面的隐私分析比较。我们开发了一种新型的成员推理攻击（MIA），它在检测训练图像时成功率显著提高（TPR@FPR&#x3D;1%：IARs为86.38%，DMs为4.91%）。使用这种MIA，我们执行数据集推理（DI），发现IARs仅需六个样本即可检测数据集成员身份，而DMs则需要200个样本，这表明信息泄露更高。此外，我们从IAR中提取了数百张训练图像（例如，从VAR-d30中提取了698张）。我们的研究结果凸显了隐私效用之间的基本权衡：虽然IAR在生成质量和速度方面表现出色，但它们更容易受到隐私攻击。这表明结合DMs的技术，如使用扩散的每令牌概率建模，可能有助于缓解IARs的隐私风险。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sprintml/privacy_attacks_against_iars上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02514v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars">https://github.com/sprintml/privacy_attacks_against_iars</a></p>
<p><strong>Summary</strong></p>
<p>图像自回归（IAR）模型在图像质量和生成速度上超越了扩散模型（DM），但其隐私风险尚未得到充分探索。本研究对IAR和DM进行了全面的隐私分析，开发了一种新型成员推理攻击（MIA），在检测训练图像方面取得了较高的成功率。研究表明，IAR在数据集推理（DI）方面所需样本数量较少，且能从IAR中提取大量训练图像。研究指出，虽然IAR在生成质量和速度方面表现出色，但更易受到隐私攻击。结合扩散模型的某些技术可能有助于缓解IAR的隐私风险。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IAR模型在图像质量和生成速度上超越了DM，但隐私风险尚未得到足够研究。</li>
<li>开发了一种新型的MIA，能够成功检测IAR中训练图像的成功率远高于DM。</li>
<li>在数据集推理方面，IAR所需样本数量较少，仅需六个样本即可检测数据集成员身份，而DM需要200个样本。</li>
<li>IAR模型更容易泄露训练图像信息，可以从IAR中提取大量训练图像。</li>
<li>IAR和DM之间存在隐私实用性的权衡：IAR在生成质量和速度方面表现出色，但隐私风险更高。</li>
<li>结合扩散模型的某些技术，如通过扩散进行每令牌概率建模，可能有助于缓解IAR的隐私风险。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-58262df3bab208c8603af49257cd4247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2f1e8909482d612842d72e7423ef90e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InterLCM-Low-Quality-Images-as-Intermediate-States-of-Latent-Consistency-Models-for-Effective-Blind-Face-Restoration"><a href="#InterLCM-Low-Quality-Images-as-Intermediate-States-of-Latent-Consistency-Models-for-Effective-Blind-Face-Restoration" class="headerlink" title="InterLCM: Low-Quality Images as Intermediate States of Latent   Consistency Models for Effective Blind Face Restoration"></a>InterLCM: Low-Quality Images as Intermediate States of Latent   Consistency Models for Effective Blind Face Restoration</h2><p><strong>Authors:Senmao Li, Kai Wang, Joost van de Weijer, Fahad Shahbaz Khan, Chun-Le Guo, Shiqi Yang, Yaxing Wang, Jian Yang, Ming-Ming Cheng</strong></p>
<p>Diffusion priors have been used for blind face restoration (BFR) by fine-tuning diffusion models (DMs) on restoration datasets to recover low-quality images. However, the naive application of DMs presents several key limitations. (i) The diffusion prior has inferior semantic consistency (e.g., ID, structure and color.), increasing the difficulty of optimizing the BFR model; (ii) reliance on hundreds of denoising iterations, preventing the effective cooperation with perceptual losses, which is crucial for faithful restoration. Observing that the latent consistency model (LCM) learns consistency noise-to-data mappings on the ODE-trajectory and therefore shows more semantic consistency in the subject identity, structural information and color preservation, we propose InterLCM to leverage the LCM for its superior semantic consistency and efficiency to counter the above issues. Treating low-quality images as the intermediate state of LCM, InterLCM achieves a balance between fidelity and quality by starting from earlier LCM steps. LCM also allows the integration of perceptual loss during training, leading to improved restoration quality, particularly in real-world scenarios. To mitigate structural and semantic uncertainties, InterLCM incorporates a Visual Module to extract visual features and a Spatial Encoder to capture spatial details, enhancing the fidelity of restored images. Extensive experiments demonstrate that InterLCM outperforms existing approaches in both synthetic and real-world datasets while also achieving faster inference speed. </p>
<blockquote>
<p>扩散先验已被用于盲脸修复（BFR）中，通过对扩散模型（DMs）进行微调以恢复低质量图像。然而，扩散模型的直接应用存在几个关键局限性。（i）扩散先验的语义一致性较差（例如，身份、结构和颜色），增加了优化BFR模型的难度；（ii）依赖于数百次的去噪迭代，阻碍了与感知损失的协同工作，这对于忠实恢复至关重要。观察到潜在一致性模型（LCM）学习噪声到数据的一致性映射在ODE轨迹上，因此在对主体身份、结构信息和颜色保留方面显示出更高的语义一致性，我们提出InterLCM，利用LCM的优异语义一致性和效率来解决上述问题。将低质量图像视为LCM的中间状态，InterLCM通过从较早的LCM步骤开始，在保真度和质量之间取得平衡。LCM还允许在训练过程中整合感知损失，从而提高恢复质量，特别是在现实场景应用中。为了减轻结构和语义不确定性，InterLCM结合了视觉模块来提取视觉特征和一个空间编码器来捕捉空间细节，增强了恢复图像的保真度。大量实验表明，InterLCM在合成和真实世界数据集上均优于现有方法，同时实现了更快的推理速度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02215v1">PDF</a> Accepted at ICLR2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散先验在盲脸修复（BFR）中的应用，通过对扩散模型（DMs）进行微调以恢复低质量图像。然而，直接使用DMs存在语义一致性差和需要大量去噪迭代等局限性。为此，提出了InterLCM方法，该方法结合了潜在一致性模型（LCM）的优势和效率，改善了语义一致性和优化速度。InterLCM通过将低质量图像视为LCM的中间状态，实现了保真度和质量之间的平衡。此外，它引入了视觉模块和空间编码器来缓解结构和语义不确定性，提高了修复图像的保真度。实验表明，InterLCM在合成和真实世界数据集上均优于现有方法，同时推理速度更快。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散先验被用于盲脸修复（BFR），通过微调扩散模型（DMs）来恢复低质量图像。</li>
<li>单纯应用DMs存在语义一致性差和需要大量去噪迭代的关键局限。</li>
<li>InterLCM方法结合潜在一致性模型（LCM）的优势和效率，改善语义一致性和优化速度。</li>
<li>InterLCM通过将低质量图像视为LCM的中间状态，实现保真度和质量间的平衡。</li>
<li>InterLCM引入视觉模块和空间编码器，增强图像修复的保真度。</li>
<li>InterLCM在合成和真实世界数据集上的表现均优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02215">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8aa50c2dbaacc49707c9b24d9a5162e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bef06ff7f2376c949f68b73b4cd157d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b361ce2d671c20aa99ae4d9fb4e729.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e40b560a2a4d80034d038b128ab577a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="On-the-Guidance-of-Flow-Matching"><a href="#On-the-Guidance-of-Flow-Matching" class="headerlink" title="On the Guidance of Flow Matching"></a>On the Guidance of Flow Matching</h2><p><strong>Authors:Ruiqi Feng, Tailin Wu, Chenglei Yu, Wenhao Deng, Peiyan Hu</strong></p>
<p>Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where guided generation is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at <a target="_blank" rel="noopener" href="https://github.com/AI4Science-WestlakeU/flow_guidance">https://github.com/AI4Science-WestlakeU/flow_guidance</a>. </p>
<blockquote>
<p>流匹配在各种生成任务中表现出了卓越的性能，从图像生成到决策制定，其中引导生成是关键。然而，流匹配的指导更加通用，因此与其前身扩散模型有着本质的区别。因此，通用流匹配的指导挑战仍然被大大忽视。在本文中，我们提出了流匹配通用指导框架。基于此框架，我们开发了一系列可应用于通用流匹配的指导技术。这包括无需训练即可渐近精确指导的新技术、基于训练指导的新训练损失，以及涵盖经典梯度指导方法为特例的两类近似指导。我们从理论上探讨了这些方法，为不同场景选择适当的方法提供了实用指南。在合成数据集、图像反问题和离线强化学习上的实验证明了我们提出的指导方法的有效性，并验证了我们的流匹配指导框架的正确性。可在此找到重现实验的代码：<a target="_blank" rel="noopener" href="https://github.com/AI4Science-WestlakeU/flow_guidance%E3%80%82">https://github.com/AI4Science-WestlakeU/flow_guidance。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02150v1">PDF</a> 35 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了流匹配通用指导框架，解决了流匹配在生成任务中的指导问题。该框架涵盖了一系列指导技术，包括无训练渐进精确指导、新型训练损失训练指导以及涵盖经典梯度指导方法的两类近似指导。通过理论研究和实验验证，本文展示了该框架在不同场景下的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>流匹配在生成任务中展现出卓越性能，特别是在需要引导生成的任务中。</li>
<li>流匹配的指导与其前身扩散模型相比更为通用且有所差异。</li>
<li>提出首个流匹配通用指导框架，涵盖多种指导技术。</li>
<li>框架包括无训练渐进精确指导，新型训练损失训练指导以及涵盖经典梯度指导方法的两类近似指导。</li>
<li>对不同指导方法进行了理论研究，为不同场景选择合适方法提供了实践指南。</li>
<li>在合成数据集、图像反问题以及离线强化学习上的实验验证了所提指导方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02150">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b8f0923b3549ef32d3ba229ff6bf9f31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18e872942ce45433ea1422c4b0f24260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8a3e41427cae22bb2efb6e00eeeb8eb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="One-Diffusion-Step-to-Real-World-Super-Resolution-via-Flow-Trajectory-Distillation"><a href="#One-Diffusion-Step-to-Real-World-Super-Resolution-via-Flow-Trajectory-Distillation" class="headerlink" title="One Diffusion Step to Real-World Super-Resolution via Flow Trajectory   Distillation"></a>One Diffusion Step to Real-World Super-Resolution via Flow Trajectory   Distillation</h2><p><strong>Authors:Jianze Li, Jiezhang Cao, Yong Guo, Wenbo Li, Yulun Zhang</strong></p>
<p>Diffusion models (DMs) have significantly advanced the development of real-world image super-resolution (Real-ISR), but the computational cost of multi-step diffusion models limits their application. One-step diffusion models generate high-quality images in a one sampling step, greatly reducing computational overhead and inference latency. However, most existing one-step diffusion methods are constrained by the performance of the teacher model, where poor teacher performance results in image artifacts. To address this limitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique based on flow matching models. We use the state-of-the-art diffusion model FLUX.1-dev as both the teacher model and the base model. First, we introduce Flow Trajectory Distillation (FTD) to distill a multi-step flow matching model into a one-step Real-ISR. Second, to improve image realism and address high-frequency artifact issues in generated images, we propose TV-LPIPS as a perceptual loss and introduce Attention Diversification Loss (ADL) as a regularization term to reduce token similarity in transformer, thereby eliminating high-frequency artifacts. Comprehensive experiments demonstrate that our method outperforms existing one-step diffusion-based Real-ISR methods. The code and model will be released at <a target="_blank" rel="noopener" href="https://github.com/JianzeLi-114/FluxSR">https://github.com/JianzeLi-114/FluxSR</a>. </p>
<blockquote>
<p>扩散模型（DMs）显著推动了真实世界图像超分辨率（Real-ISR）的发展，但多步扩散模型的计算成本限制了其应用。一步扩散模型在一步采样过程中生成高质量图像，大大降低了计算开销和推理延迟。然而，大多数现有的一步扩散方法受到教师模型性能的制约，教师模型性能不佳会导致图像出现伪影。为了解决这一局限性，我们提出了FluxSR，这是一种基于流匹配模型的新型一步扩散Real-ISR技术。我们使用最先进的扩散模型FLUX.1-dev作为教师模型和基础模型。首先，我们引入流轨迹蒸馏（FTD）技术，将多步流匹配模型转化为一步Real-ISR。其次，为了提高图像的真实性和解决生成图像中的高频伪影问题，我们提出将TV-LPIPS作为感知损失，并引入注意力多样化损失（ADL）作为正则化项，以减少transformer中的令牌相似性，从而消除高频伪影。综合实验表明，我们的方法优于现有的基于一步扩散的Real-ISR方法。代码和模型将在<a target="_blank" rel="noopener" href="https://github.com/JianzeLi-114/FluxSR">https://github.com/JianzeLi-114/FluxSR</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01993v1">PDF</a> </p>
<p><strong>Summary</strong><br>     扩散模型在真实图像超分辨率（Real-ISR）领域取得显著进展，但多步扩散模型计算成本高。本文提出一种基于流匹配模型的新型一步扩散Real-ISR技术FluxSR，通过引入Flow Trajectory Distillation（FTD）将多步流匹配模型转化为一步Real-ISR，并引入TV-LPIPS作为感知损失和注意力多样化损失（ADL）作为正则化项来减少变换器中的令牌相似性，消除高频伪影。实验表明，该方法优于现有的一步扩散Real-ISR方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型（DMs）在真实图像超分辨率（Real-ISR）方面取得进展。</li>
<li>多步扩散模型计算成本高，限制了其应用。</li>
<li>FluxSR是一种新型一步扩散Real-ISR技术，基于流匹配模型。</li>
<li>FluxSR通过Flow Trajectory Distillation（FTD）将多步流匹配模型转化为一步Real-ISR。</li>
<li>为提高图像真实性和解决生成图像的高频伪影问题，FluxSR引入TV-LPIPS作为感知损失。</li>
<li>FluxSR提出Attention Diversification Loss（ADL）作为正则化项，以减少变换器中的令牌相似性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01993">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d34e81f169f2698c31f398362442ea22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e4e92b60bad3b5ed861202f4366bc3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1816359a806fb7dd7fc83bdc5c9d42c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9e96df63fefd28452a7f3fada5c1f7b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation"><a href="#Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation" class="headerlink" title="Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation"></a>Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation</h2><p><strong>Authors:Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong</strong></p>
<p>Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an \textbf{online} algorithm capable of collecting data during runtime and supporting a \textbf{black-box} objective function. Moreover, the \textbf{query efficiency} of the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm, \textbf{Fast Direct}, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a> </p>
<blockquote>
<p>引导式扩散模型生成方向对于定制预训练的扩散模型的生成过程以解决特定的下游任务具有广阔前景。现有的引导式扩散模型要么依赖于使用预先收集的数据集对引导模型进行训练，要么需要目标函数可微。然而，对于大多数现实世界任务而言，离线数据集通常不可用，并且其目标函数通常不可微分，例如具有人类偏好的图像生成、用于药物发现的分子生成以及材料设计。因此，我们需要一种<strong>在线</strong>算法，该算法能够在运行时收集数据并支持<strong>黑箱</strong>目标函数。此外，算法的<strong>查询效率</strong>也至关重要，因为在现实场景中，目标查询的评估往往成本高昂。在这项工作中，我们提出了一种新颖而简单的算法——<strong>Fast Direct</strong>，用于高效查询在线黑箱目标生成。我们的Fast Direct在数据流形上构建伪目标，以通用方向更新扩散模型的噪声序列，这在执行高效查询引导生成方面显示出巨大潜力。在十二个高分辨率（$\small {1024 \times 1024}$）图像目标生成任务和六个3D分子目标生成任务上的广泛实验显示，查询效率提高了$\textbf{6}$倍至$\textbf{10}$倍，以及分别提高了$\textbf{11}$倍至$\textbf{44}$倍。我们的实现可在以下网址公开访问：<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01692v1">PDF</a> </p>
<p><strong>摘要</strong><br>    引导扩散模型生成是定制预训练扩散模型的生成过程以应对特定下游任务的有前途的方向。现有引导扩散模型依赖于使用预先收集的数据集训练的指导模型，或者需要目标函数可微。然而，对于大多数现实世界任务，离线数据集通常不可用，且其目标函数通常不可微分，如基于人类偏好的图像生成、用于药物发现的分子生成和材料设计。因此，我们需要一种能够在运行时收集数据的在线算法，并支持黑箱目标函数。此外，算法的查询效率也至关重要，因为在现实场景中目标查询的评估往往很昂贵。本研究提出了一种新颖而简单的算法Fast Direct，用于查询高效的在线黑箱目标生成。我们的Fast Direct在数据流形上构建伪目标，以通用方向更新扩散模型的噪声序列，有望在查询高效的引导生成方面表现出色。在十二个高分辨率（$1024 \times 1024$）图像目标生成任务和六个3D分子目标生成任务上的广泛实验显示，查询效率提高了6至10倍和11至44倍。我们的实现可在<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct公开访问。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>引导扩散模型生成是定制预训练扩散模型的有力方向，尤其适用于特定下游任务。</li>
<li>现有引导扩散模型依赖于离线数据集或可微目标函数，这在现实任务中往往不可用或不可微。</li>
<li>提出了一种新颖算法Fast Direct，支持在线黑箱目标生成，可在运行时收集数据。</li>
<li>Fast Direct通过构建伪目标在数据流形上更新噪声序列，具有查询高效的潜力。</li>
<li>在图像和分子生成等任务上，Fast Direct显示出显著的查询效率改进。</li>
<li>Fast Direct算法的实现已公开可用，方便公众访问和使用。</li>
<li>该方法具有潜力解决现实世界中目标查询评估昂贵的问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01692">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e583201520afad6bd1a558d3cc9e80c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8e6b3245a8fea1168ea1a33d9883dd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b02e30e6862644daeb69fdee46ecbd0f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HuViDPO-Enhancing-Video-Generation-through-Direct-Preference-Optimization-for-Human-Centric-Alignment"><a href="#HuViDPO-Enhancing-Video-Generation-through-Direct-Preference-Optimization-for-Human-Centric-Alignment" class="headerlink" title="HuViDPO:Enhancing Video Generation through Direct Preference   Optimization for Human-Centric Alignment"></a>HuViDPO:Enhancing Video Generation through Direct Preference   Optimization for Human-Centric Alignment</h2><p><strong>Authors:Lifan Jiang, Boxi Wu, Jiahui Zhang, Xiaotong Guan, Shuang Chen</strong></p>
<p>With the rapid development of AIGC technology, significant progress has been made in diffusion model-based technologies for text-to-image (T2I) and text-to-video (T2V). In recent years, a few studies have introduced the strategy of Direct Preference Optimization (DPO) into T2I tasks, significantly enhancing human preferences in generated images. However, existing T2V generation methods lack a well-formed pipeline with exact loss function to guide the alignment of generated videos with human preferences using DPO strategies. Additionally, challenges such as the scarcity of paired video preference data hinder effective model training. At the same time, the lack of training datasets poses a risk of insufficient flexibility and poor video generation quality in the generated videos. Based on those problems, our work proposes three targeted solutions in sequence. 1) Our work is the first to introduce the DPO strategy into the T2V tasks. By deriving a carefully structured loss function, we utilize human feedback to align video generation with human preferences. We refer to this new method as HuViDPO. 2) Our work constructs small-scale human preference datasets for each action category and fine-tune this model, improving the aesthetic quality of the generated videos while reducing training costs. 3) We adopt a First-Frame-Conditioned strategy, leveraging the rich in formation from the first frame to guide the generation of subsequent frames, enhancing flexibility in video generation. At the same time, we employ a SparseCausal Attention mechanism to enhance the quality of the generated videos.More details and examples can be accessed on our website: <a target="_blank" rel="noopener" href="https://tankowa.github.io/HuViDPO">https://tankowa.github.io/HuViDPO</a>. github.io&#x2F;. </p>
<blockquote>
<p>随着AIGC技术的快速发展，基于扩散模型的文本到图像（T2I）和文本到视频（T2V）技术在近年来取得了显著进展。一些研究已将直接偏好优化（DPO）策略引入T2I任务，显著提高了生成图像的人类偏好。然而，现有的T2V生成方法缺乏完善的管道和精确的损失函数，无法使用DPO策略将生成视频与人类偏好对齐。此外，配对视频偏好数据的稀缺性给模型的有效训练带来了挑战。同时，训练数据集的缺乏可能导致生成视频的灵活性和质量不足。基于这些问题，我们的工作提出了三个有针对性的解决方案。1）我们的工作是首次将DPO策略引入T2V任务。通过精心构建的损失函数，我们利用人类反馈将视频生成与人类偏好对齐。我们将这种新方法称为HuViDPO。2）我们的工作为每个动作类别构建了小规模的人类偏好数据集，并对模型进行了微调，提高了生成视频的美学质量，同时降低了训练成本。3）我们采用First-Frame-Conditioned策略，利用第一帧的丰富信息来引导后续帧的生成，提高视频生成的灵活性。同时，我们采用SparseCausal Attention机制，以提高生成视频的质量。更多详情和示例请访问我们的网站：[<a target="_blank" rel="noopener" href="https://tankowa.github.io/HuViDPO/]">https://tankowa.github.io/HuViDPO/]</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01690v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>随着AIGC技术的快速发展，文本转图像（T2I）和文本转视频（T2V）的扩散模型技术取得了显著进展。虽然Direct Preference Optimization（DPO）策略已应用于T2I任务并提升了生成图像的人类偏好，但在T2V生成方法中，应用DPO策略的管道尚未完善，缺乏精确的损失函数来指导生成视频与人类偏好的对齐。此外，缺乏配对视频偏好数据等挑战影响了模型的有效训练。针对这些问题，我们的工作提出了三项针对性解决方案。首先，我们首次将DPO策略引入T2V任务，通过构建精细结构的损失函数，利用人类反馈将视频生成与人类偏好对齐，我们称之为HuViDPO。其次，我们构建了每个动作类别的小规模人类偏好数据集进行微调，提高了生成视频的美学质量并降低了训练成本。最后，我们采用First-Frame-Conditioned策略，利用第一帧的丰富信息来指导后续帧的生成，增强了视频生成的灵活性。同时，我们采用了SparseCausal Attention机制来提升生成视频的质量。更多详情和示例可访问我们的网站：<a target="_blank" rel="noopener" href="https://tankowa.github.io/HuViDPO">HuViDPO官网链接</a>。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>首次将Direct Preference Optimization（DPO）策略引入文本转视频（T2V）任务，提出HuViDPO方法。</li>
<li>通过构建损失函数，利用人类反馈将视频生成与人类偏好对齐。</li>
<li>构建小规模人类偏好数据集进行模型微调，提高生成视频的美学质量并降低训练成本。</li>
<li>采用First-Frame-Conditioned策略，利用第一帧信息提升视频生成的灵活性和质量。</li>
<li>引入SparseCausal Attention机制，进一步增强生成视频的质量。</li>
<li>提供了网站链接，可获取更多关于HuViDPO的细节和示例。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01690">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-54d98a51ec56fb6f2177b7ff19a21b3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3702cd8f2dae45966cd4d0609eb5d01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b362613fb71f98f85dbd89f89daa67d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3227cf25eed143ba397c230ab0dbeb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c228874b7e0e03a41750a92a269c1799.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf1d6fd1691d1accd80fe38c5ab29d4e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="BD-Diff-Generative-Diffusion-Model-for-Image-Deblurring-on-Unknown-Domains-with-Blur-Decoupled-Learning"><a href="#BD-Diff-Generative-Diffusion-Model-for-Image-Deblurring-on-Unknown-Domains-with-Blur-Decoupled-Learning" class="headerlink" title="BD-Diff: Generative Diffusion Model for Image Deblurring on Unknown   Domains with Blur-Decoupled Learning"></a>BD-Diff: Generative Diffusion Model for Image Deblurring on Unknown   Domains with Blur-Decoupled Learning</h2><p><strong>Authors:Junhao Cheng, Wei-Ting Chen, Xi Lu, Ming-Hsuan Yang</strong></p>
<p>Generative diffusion models trained on large-scale datasets have achieved remarkable progress in image synthesis. In favor of their ability to supplement missing details and generate aesthetically pleasing contents, recent works have applied them to image deblurring tasks via training an adapter on blurry-sharp image pairs to provide structural conditions for restoration. However, acquiring substantial amounts of realistic paired data is challenging and costly in real-world scenarios. On the other hand, relying solely on synthetic data often results in overfitting, leading to unsatisfactory performance when confronted with unseen blur patterns. To tackle this issue, we propose BD-Diff, a generative-diffusion-based model designed to enhance deblurring performance on unknown domains by decoupling structural features and blur patterns through joint training on three specially designed tasks. We employ two Q-Formers as structural representations and blur patterns extractors separately. The features extracted by them will be used for the supervised deblurring task on synthetic data and the unsupervised blur-transfer task by leveraging unpaired blurred images from the target domain simultaneously. Furthermore, we introduce a reconstruction task to make the structural features and blur patterns complementary. This blur-decoupled learning process enhances the generalization capabilities of BD-Diff when encountering unknown domain blur patterns. Experiments on real-world datasets demonstrate that BD-Diff outperforms existing state-of-the-art methods in blur removal and structural preservation in various challenging scenarios. The codes will be released in <a target="_blank" rel="noopener" href="https://github.com/donahowe/BD-Diff">https://github.com/donahowe/BD-Diff</a> </p>
<blockquote>
<p>基于大规模数据集训练的生成扩散模型在图像合成方面取得了显著的进展。由于其能够补充缺失的细节并生成美观的内容，最近的研究将其应用于图像去模糊任务，通过训练适配器对模糊和清晰图像对进行训练，为恢复提供结构条件。然而，在现实世界场景中获取大量真实的配对数据具有挑战性和成本高昂。另一方面，仅依赖合成数据往往会导致过度拟合，在面对未知的模糊模式时，性能往往不尽如人意。为了解决这一问题，我们提出了BD-Diff，这是一个基于生成扩散的模型，通过联合训练三个专门设计的任务，旨在提高未知领域的去模糊性能。我们采用两个Q-Formers分别作为结构特征和模糊模式的表示和提取器。它们提取的特征将用于合成数据上的监督去模糊任务，并同时利用来自目标域的无配对模糊图像进行无监督的模糊转移任务。此外，我们引入了一个重建任务，使结构特征和模糊模式相互补充。这种去模糊解耦的学习过程提高了BD-Diff在遇到未知领域模糊模式时的泛化能力。在真实世界数据集上的实验表明，BD-Diff在多种具有挑战性的场景中，在模糊去除和结构保留方面超越了现有的先进方法。相关代码将发布在：<a target="_blank" rel="noopener" href="https://github.com/donahowe/BD-Diff">https://github.com/donahowe/BD-Diff</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01522v1">PDF</a> We propose BD-Diff to integrate generative diffusion model into   unpaired deblurring tasks</p>
<p><strong>Summary</strong></p>
<p>基于大型数据集训练的生成扩散模型已在图像合成方面取得显著进展。为利用其在补充缺失细节和生成美观内容方面的能力，近期研究尝试将其应用于图像去模糊任务。然而，获取大量真实配对数据具有挑战性和成本高昂。为解决这个问题，本文提出BD-Diff模型，通过联合训练三个特别设计的任务，增强在未知域上的去模糊性能。该模型采用两个Q-Formers分别作为结构特征表示和模糊模式提取器。实验证明，BD-Diff在真实世界数据集上优于现有先进方法，在各种挑战场景下实现更好的去模糊和结构保留效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成扩散模型在图像合成方面取得显著进展，可应用于图像去模糊任务。</li>
<li>获取真实配对数据具有挑战性和成本，单纯依赖合成数据可能导致过度拟合。</li>
<li>BD-Diff模型通过联合训练三个特别设计的任务，旨在增强在未知域上的去模糊性能。</li>
<li>BD-Diff模型采用两个Q-Formers分别提取结构特征和模糊模式。</li>
<li>BD-Diff在真实世界数据集上实现优越的去模糊效果，优于现有方法。</li>
<li>BD-Diff模型具有更好的结构保留效果，在各种挑战场景下表现突出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01522">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-742747dd6cda7b068eb5a5a1aa629672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04335becfcab3b9663a4679620a54415.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c35380521116a3063a6d6830e56ff246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f52c11ea436f2676ad1518ef71c9a7e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84df65be90783c1a02467fc3745dff7e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Improved-Training-Technique-for-Latent-Consistency-Models"><a href="#Improved-Training-Technique-for-Latent-Consistency-Models" class="headerlink" title="Improved Training Technique for Latent Consistency Models"></a>Improved Training Technique for Latent Consistency Models</h2><p><strong>Authors:Quan Dao, Khanh Doan, Di Liu, Trung Le, Dimitris Metaxas</strong></p>
<p>Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-$c$ scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: <a target="_blank" rel="noopener" href="https://github.com/quandao10/sLCT/">https://github.com/quandao10/sLCT/</a> </p>
<blockquote>
<p>一致性模型是一种新型生成模型家族，能够在单步或多步中生成高质量样本。最近，一致性模型表现出了令人印象深刻的性能，在像素空间达到了与扩散模型相当的结果。然而，将一致性训练扩展到大规模数据集的成功，特别是在文本到图像和视频生成任务中，取决于其在潜在空间中的性能。</p>
</blockquote>
<p>在这项工作中，我们分析了像素和潜在空间之间的统计差异，发现潜在数据经常包含高度冲动的异常值，这会显著降级潜在空间中的iCT性能。为了解决这一问题，我们用Cauchy损失替换了Pseudo-Huber损失，有效地减轻了异常值的影响。此外，我们在早期时间步长中引入了扩散损失，并采用最优传输（OT）耦合来进一步提高性能。最后，我们引入了自适应缩放-c调度器来管理稳健的训练过程，并在架构中采用非缩放LayerNorm，以更好地捕获特征的统计信息并减少异常值的影响。</p>
<p>通过这些策略，我们成功训练了能够在一步或两步内生成高质量样本的潜在一致性模型，显著缩小了潜在一致性与扩散模型之间的性能差距。相关实现已在此处发布：<a target="_blank" rel="noopener" href="https://github.com/quandao10/sLCT/%E3%80%82">https://github.com/quandao10/sLCT/。</a></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01441v1">PDF</a> Accepted at ICLR2025</p>
<p><strong>摘要</strong><br>    一致性模型是新一代生成模型，能一步或多步生成高质量样本。近期，它在像素空间的表现已接近扩散模型。然而，在大规模数据集上，特别是在文本到图像和视频生成任务中，一致性模型在潜在空间的性能决定其成功。本研究分析了像素和潜在空间之间的统计差异，发现潜在数据常含有冲动异常值，会显著影响iCT在潜在空间的性能。为此，我们用Cauchy损失替代Pseudo-Huber损失，有效减轻异常值的影响。此外，我们在早期时间步长引入扩散损失，并采用最优传输耦合进一步增强性能。最后，我们引入自适应缩放-c调度器管理稳健训练过程，并在架构中采用非缩放LayerNorm，以更好地捕捉特征统计信息并减少异常值影响。这些策略使我们能训练出能在一步或两步内生成高质量样本的潜在一致性模型，显著缩小了其与扩散模型之间的性能差距。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>一致性模型是生成高质量样本的新模型，能在单步或多步中生成样本。</li>
<li>在大规模数据集上，一致性模型在潜在空间的性能对其成功至关重要。</li>
<li>潜在数据包含冲动异常值，这些异常值会显著影响iCT的性能。</li>
<li>使用Cauchy损失替代Pseudo-Huber损失以减轻异常值的影响。</li>
<li>在早期时间步长引入扩散损失和采用最优传输耦合增强性能。</li>
<li>引入自适应缩放-c调度器管理稳健训练过程。</li>
<li>在架构中使用非缩放LayerNorm，以捕捉特征统计信息并减少异常值影响。这些策略帮助缩小了潜在一致性模型和扩散模型之间的性能差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01441">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-48a19ef7a2a2e3f49d8be79bd8aa186c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f171726621e8d3ada6e0bce8c5bf4338.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5beb9b320f3321fc861e60764fed4043.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Assessing-the-use-of-Diffusion-models-for-motion-artifact-correction-in-brain-MRI"><a href="#Assessing-the-use-of-Diffusion-models-for-motion-artifact-correction-in-brain-MRI" class="headerlink" title="Assessing the use of Diffusion models for motion artifact correction in   brain MRI"></a>Assessing the use of Diffusion models for motion artifact correction in   brain MRI</h2><p><strong>Authors:Paolo Angella, Vito Paolo Pastore, Matteo Santacesaria</strong></p>
<p>Magnetic Resonance Imaging generally requires long exposure times, while being sensitive to patient motion, resulting in artifacts in the acquired images, which may hinder their diagnostic relevance. Despite research efforts to decrease the acquisition time, and designing efficient acquisition sequences, motion artifacts are still a persistent problem, pushing toward the need for the development of automatic motion artifact correction techniques. Recently, diffusion models have been proposed as a solution for the task at hand. While diffusion models can produce high-quality reconstructions, they are also susceptible to hallucination, which poses risks in diagnostic applications. In this study, we critically evaluate the use of diffusion models for correcting motion artifacts in 2D brain MRI scans. Using a popular benchmark dataset, we compare a diffusion model-based approach with state-of-the-art methods consisting of Unets trained in a supervised fashion on motion-affected images to reconstruct ground truth motion-free images. Our findings reveal mixed results: diffusion models can produce accurate predictions or generate harmful hallucinations in this context, depending on data heterogeneity and the acquisition planes considered as input. </p>
<blockquote>
<p>磁共振成像通常需要长时间的曝光，同时对患者动作敏感，导致采集的图像出现伪影，可能会降低其诊断价值。尽管已付出努力减少采集时间并设计有效的采集序列，但运动伪影仍是一个持续存在的问题，推动了对开发自动运动伪影校正技术的需求。最近，扩散模型已被提议作为解决手头任务的一种解决方案。虽然扩散模型可以产生高质量的重建，但它们也容易出现幻觉，这在诊断应用中构成风险。在这项研究中，我们全面评估了使用扩散模型校正二维脑部MRI扫描中的运动伪影的使用情况。我们使用流行的大型数据集，将基于扩散模型的方法与最先进的Unets方法进行对比，Unets方法在有运动影响图像的监督训练下重建出真实无运动图像。我们的研究结果喜忧参半：扩散模型可以根据数据异质性和输入的采集平面产生准确的预测或产生有害的幻觉。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01418v1">PDF</a> Accepted at IEEE International Symposium for Biomedical Imaging   (ISBI) 2025</p>
<p><strong>Summary</strong></p>
<p>扩散模型在纠正二维脑核磁共振扫描中的运动伪影方面具有潜力，但存在产生幻觉的风险。本研究采用基准数据集对比扩散模型与当前先进方法，结果显示混合结果，取决于数据异质性和输入采集平面。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型可用于纠正MRI中的运动伪影。</li>
<li>扩散模型能生成高质量重建，但也存在产生幻觉的风险。</li>
<li>研究采用基准数据集对扩散模型与当前先进方法进行了比较。</li>
<li>数据异质性和输入采集平面会影响扩散模型的预测结果。</li>
<li>扩散模型在某些情况下可能产生准确预测，而在其他情况下可能生成有害的幻觉。</li>
<li>需要进一步研究和改进扩散模型，以提高其在MRI运动伪影纠正中的性能和稳定性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01418">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1c0ff2db24ef0b3d27b3a157481e6bf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3bc7df182465c9abeb71c16ac4d4dca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6e4a2022804a3d3286943ebf6437e7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e19a444759c1f0bbc204de4ce2fcc7ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d60df56725baa625acfb04fb81fdfea4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef540aa18af7f90553241da98e76b27b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Human-Body-Restoration-with-One-Step-Diffusion-Model-and-A-New-Benchmark"><a href="#Human-Body-Restoration-with-One-Step-Diffusion-Model-and-A-New-Benchmark" class="headerlink" title="Human Body Restoration with One-Step Diffusion Model and A New Benchmark"></a>Human Body Restoration with One-Step Diffusion Model and A New Benchmark</h2><p><strong>Authors:Jue Gong, Jingkai Wang, Zheng Chen, Xing Liu, Hong Gu, Yulun Zhang, Xiaokang Yang</strong></p>
<p>Human body restoration, as a specific application of image restoration, is widely applied in practice and plays a vital role across diverse fields. However, thorough research remains difficult, particularly due to the lack of benchmark datasets. In this study, we propose a high-quality dataset automated cropping and filtering (HQ-ACF) pipeline. This pipeline leverages existing object detection datasets and other unlabeled images to automatically crop and filter high-quality human images. Using this pipeline, we constructed a person-based restoration with sophisticated objects and natural activities (\emph{PERSONA}) dataset, which includes training, validation, and test sets. The dataset significantly surpasses other human-related datasets in both quality and content richness. Finally, we propose \emph{OSDHuman}, a novel one-step diffusion model for human body restoration. Specifically, we propose a high-fidelity image embedder (HFIE) as the prompt generator to better guide the model with low-quality human image information, effectively avoiding misleading prompts. Experimental results show that OSDHuman outperforms existing methods in both visual quality and quantitative metrics. The dataset and code will at <a target="_blank" rel="noopener" href="https://github.com/gobunu/OSDHuman">https://github.com/gobunu/OSDHuman</a>. </p>
<blockquote>
<p>人体修复作为图像修复的一种特定应用，在实践中得到广泛应用，并在各个领域发挥着重要作用。然而，由于其难度较高，尤其是缺乏基准数据集，相关研究仍然困难重重。在本研究中，我们提出了一种高质量数据集自动裁剪和过滤（HQ-ACF）管道。该管道利用现有的目标检测数据集和其他未标记的图像，自动裁剪和过滤高质量的人体图像。使用该管道，我们构建了一个基于人的修复与复杂目标和自然活动（PERSONA）数据集，包括训练集、验证集和测试集。该数据集在质量和内容丰富程度上都大大超过了其他与人体相关的数据集。最后，我们提出了一种用于人体修复的新型一步扩散模型OSDHuman。具体来说，我们提出了一种高保真图像嵌入器（HFIE）作为提示生成器，以更好地利用低质量人体图像信息来指导模型，有效避免误导性提示。实验结果表明，OSDHuman在视觉质量和定量指标上均优于现有方法。数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/gobunu/OSDHuman%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/gobunu/OSDHuman公开。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01411v1">PDF</a> 8 pages, 9 figures. The code and model will be available at   <a target="_blank" rel="noopener" href="https://github.com/gobunu/OSDHuman">https://github.com/gobunu/OSDHuman</a></p>
<p><strong>Summary</strong><br>     该研究针对人体恢复图像修复的一个具体应用，提出一个高质量数据集自动化裁剪和过滤（HQ-ACF）管道，利用现有目标检测数据集和其他未标记图像自动裁剪和过滤高质量的人体图像。基于此管道，构建了包含训练、验证和测试集的人员恢复与复杂物体和自然活动（PERSONA）数据集。此外，提出了一种新型一步扩散模型OSDHuman用于人体恢复，并提出高保真图像嵌入器（HFIE）作为提示生成器，更好地引导模型利用低质量人体图像信息，避免误导提示。实验结果表明，OSDHuman在视觉质量和定量指标上均优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人体恢复作为图像修复的一个特定应用，在实践中得到广泛应用，并在多个领域中发挥重要作用。</li>
<li>缺乏基准数据集使深入研究变得困难。</li>
<li>提出一个高质量数据集自动化裁剪和过滤（HQ-ACF）管道，用于自动裁剪和过滤高质量的人体图像。</li>
<li>构建了一个包括训练、验证和测试集的人员恢复与复杂物体和自然活动（PERSONA）数据集。</li>
<li>提出了一种新型一步扩散模型OSDHuman用于人体恢复。</li>
<li>高保真图像嵌入器（HFIE）作为提示生成器，更好地引导模型利用低质量人体图像信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01411">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cbc60712e9585aea63b5adc93497ae47.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0639a76eae63a99b28103fa40400d6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11c1aefa331e77938629aa84b2e80d29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4abf9396d3335ac760e1a6862d74653c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c31eeb926795cf8d47598d15e2d63509.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90de7fda2c8f3c4a0ae9a1015cee091f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Heterogeneous-Image-GNN-Graph-Conditioned-Diffusion-for-Image-Synthesis"><a href="#Heterogeneous-Image-GNN-Graph-Conditioned-Diffusion-for-Image-Synthesis" class="headerlink" title="Heterogeneous Image GNN: Graph-Conditioned Diffusion for Image Synthesis"></a>Heterogeneous Image GNN: Graph-Conditioned Diffusion for Image Synthesis</h2><p><strong>Authors:Rupert Menneer, Christos Margadji, Sebastian W. Pattinson</strong></p>
<p>We introduce a novel method for conditioning diffusion-based image synthesis models with heterogeneous graph data. Existing approaches typically incorporate conditioning variables directly into model architectures, either through cross-attention layers that attend to text latents or image concatenation that spatially restrict generation. However, these methods struggle to handle complex scenarios involving diverse, relational conditioning variables, which are more naturally represented as unstructured graphs. This paper presents Heterogeneous Image Graphs (HIG), a novel representation that models conditioning variables and target images as two interconnected graphs, enabling efficient handling of variable-length conditioning inputs and their relationships. We also propose a magnitude-preserving GNN that integrates the HIG into the existing EDM2 diffusion model using a ControlNet approach. Our approach improves upon the SOTA on a variety of conditioning inputs for the COCO-stuff and Visual Genome datasets, and showcases the ability to condition on graph attributes and relationships represented by edges in the HIG. </p>
<blockquote>
<p>我们介绍了一种基于异质图数据对扩散图像合成模型进行条件控制的新方法。现有方法通常直接将条件变量纳入模型架构中，无论是通过关注文本潜在特征的交叉注意力层，还是通过空间限制生成的图像拼接。然而，这些方法在处理涉及多样化、关系型条件变量的复杂场景时面临困难，这些场景更自然地表现为无结构的图形。本文提出了异质图像图（HIG）这一新表示方法，它将条件变量和目标图像建模为两个相互连接的图，能够高效处理可变长度的条件输入及其关系。我们还提出了一种幅度保持图神经网络（GNN），采用ControlNet方法将HIG集成到现有的EDM2扩散模型中。我们的方法在COCO-stuff和Visual Genome数据集的各种条件输入上改进了最新技术水平，并展示了在HIG中根据图形属性和边缘表示的关系进行条件设置的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01309v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种将扩散图像合成模型与异质图数据进行条件化结合的新方法。现有方法通常直接将条件变量纳入模型架构中，通过关注文本潜在变量或图像拼接来限制生成过程。然而，这些方法在处理涉及多样化和关系性条件变量的复杂场景时遇到困难，这些场景更适合用无结构的图来表示。本文提出了异质图像图（HIG）这一新表示法，将条件变量和目标图像建模为两个相互连接的图，能够高效处理可变长度的条件输入及其关系。此外，我们还提出了一种幅度保持图神经网络（GNN），采用ControlNet方法将HIG集成到现有的EDM2扩散模型中。我们的方法在COCO-stuff和Visual Genome数据集上的多种条件输入上超越了现有最佳水平，并展示了在HIG中由边缘表示的图形属性和关系进行条件设置的能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了一种新的方法，使用异质图像图（HIG）对扩散图像合成模型进行条件化，以处理复杂和多样化的条件变量。</li>
<li>HIG建模将条件变量和目标图像表示为相互连接的图，能够处理可变长度的条件输入及其关系。</li>
<li>幅度保持图神经网络（GNN）被用来集成HIG到现有的扩散模型中。</li>
<li>该方法在多个数据集上超越了现有最佳水平，包括COCO-stuff和Visual Genome。</li>
<li>展示了在HIG中利用图形属性和关系进行条件设置的能力，这是以前方法难以处理的。</li>
<li>这种新方法为处理涉及复杂和多样化条件变量的图像生成任务提供了新的视角和工具。</li>
<li>通过整合异质图数据和扩散模型，提高了图像生成的灵活性和质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01309">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-659f37e4c5160b191072b05238c237e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-783d0f4774238c2d43627d7da44c3e90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87572860c05c11e2c9b5541d4147214f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Compressed-Image-Generation-with-Denoising-Diffusion-Codebook-Models"><a href="#Compressed-Image-Generation-with-Denoising-Diffusion-Codebook-Models" class="headerlink" title="Compressed Image Generation with Denoising Diffusion Codebook Models"></a>Compressed Image Generation with Denoising Diffusion Codebook Models</h2><p><strong>Authors:Guy Ohayon, Hila Manor, Tomer Michaeli, Michael Elad</strong></p>
<p>We present a novel generative approach based on Denoising Diffusion Models (DDMs), which produces high-quality image samples along with their losslessly compressed bit-stream representations. This is obtained by replacing the standard Gaussian noise sampling in the reverse diffusion with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors. Surprisingly, we find that our method, termed Denoising Diffusion Codebook Model (DDCM), retains sample quality and diversity of standard DDMs, even for extremely small codebooks. We leverage DDCM and pick the noises from the codebooks that best match a given image, converting our generative model into a highly effective lossy image codec achieving state-of-the-art perceptual image compression results. More generally, by setting other noise selections rules, we extend our compression method to any conditional image generation task (e.g., image restoration), where the generated images are produced jointly with their condensed bit-stream representations. Our work is accompanied by a mathematical interpretation of the proposed compressed conditional generation schemes, establishing a connection with score-based approximations of posterior samplers for the tasks considered. </p>
<blockquote>
<p>我们提出了一种基于去噪扩散模型（DDMs）的新型生成方法，该方法能够生成高质量的图像样本及其无损压缩的位流表示形式。这是通过将反向扩散中的标准高斯噪声采样替换为从预定义的固定独立同分布高斯向量的代码库中选择的噪声样本而获得的。令人惊讶的是，我们发现我们的方法，即去噪扩散代码本模型（DDCM），即使在极小的代码本中，也能保持标准DDM的样本质量和多样性。我们利用DDCM，从代码库中挑选与给定图像最匹配的噪声，将我们的生成模型转化为一种高效的有损图像编码，实现了先进的感知图像压缩结果。更一般地说，通过设置其他噪声选择规则，我们将我们的压缩方法扩展到任何条件图像生成任务（例如图像修复），其中生成的图像与其压缩的位流表示形式联合生成。我们的工作还提供了对所提出的压缩条件生成方案的数学解释，与所考虑任务的基于分数的后采样器的近似建立了联系。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01189v2">PDF</a> Code and demo are available at <a target="_blank" rel="noopener" href="https://ddcm-2025.github.io/">https://ddcm-2025.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>基于去噪扩散模型（DDM），我们提出了一种新的生成方法，该方法可以生成高质量图像样本及其无损压缩比特流表示。通过用预定义的固定独立同分布高斯向量的代码本中的噪声样本替换反向扩散中的标准高斯噪声样本，我们实现了称为去噪扩散代码本模型（DDCM）的方法。令人惊讶的是，即使对于极小的代码本，DDCM也能保持标准DDM的样本质量和多样性。我们利用DDCM，从代码本中选择与给定图像最佳匹配的噪声，将我们的生成模型转化为一种高效的有损图像编码，实现了最先进的感知图像压缩结果。更一般地说，通过设置其他噪声选择规则，我们将压缩方法扩展到任何条件图像生成任务（例如图像恢复），其中生成的图像与其浓缩的比特流表示一起产生。我们的工作还提供了对所提出的压缩条件生成方案进行数学解释，与所考虑任务的基于分数的后验采样器建立了联系。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于Denoising Diffusion Models（DDM）提出一种新的生成方法，能够生成高质量图像样本及其无损压缩表示。</li>
<li>引入Denoising Diffusion Codebook Model（DDCM），在极小的代码本下仍能保持样本质量和多样性。</li>
<li>利用DDCM将生成模型转化为高效的有损图像编码，实现先进感知图像压缩结果。</li>
<li>压缩方法可扩展到任何条件图像生成任务，如图像恢复。</li>
<li>所提出的方法具有数学解释，与基于分数的后验采样器建立联系。</li>
<li>DDCM通过选择最佳匹配给定图像的噪声，实现了图像生成与压缩的联合过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01189">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77f33794600b21a079e9234cbf259269.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b156377b6710535d8011d6b79b01dc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-519e9ecc2b78356a426d316212a5f22e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-828aa2a9e51d182813a6e49ccf80dc1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d44c768fd5d5949b00e7a5c9a9d3cbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c1a3c539c371e9d848e26186f5a4ca.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-as-a-Noise-Aware-Latent-Reward-Model-for-Step-Level-Preference-Optimization"><a href="#Diffusion-Model-as-a-Noise-Aware-Latent-Reward-Model-for-Step-Level-Preference-Optimization" class="headerlink" title="Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization"></a>Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization</h2><p><strong>Authors:Tao Zhang, Cheng Da, Kun Ding, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, Chunhong Pan</strong></p>
<p>Preference optimization for diffusion models aims to align them with human preferences for images. Previous methods typically leverage Vision-Language Models (VLMs) as pixel-level reward models to approximate human preferences. However, when used for step-level preference optimization, these models face challenges in handling noisy images of different timesteps and require complex transformations into pixel space. In this work, we demonstrate that diffusion models are inherently well-suited for step-level reward modeling in the latent space, as they can naturally extract features from noisy latent images. Accordingly, we propose the Latent Reward Model (LRM), which repurposes components of diffusion models to predict preferences of latent images at various timesteps. Building on LRM, we introduce Latent Preference Optimization (LPO), a method designed for step-level preference optimization directly in the latent space. Experimental results indicate that LPO not only significantly enhances performance in aligning diffusion models with general, aesthetic, and text-image alignment preferences, but also achieves 2.5-28$\times$ training speedup compared to existing preference optimization methods. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/casiatao/LPO">https://github.com/casiatao/LPO</a>. </p>
<blockquote>
<p>扩散模型的偏好优化旨在使图像与人类偏好相一致。之前的方法通常利用视觉语言模型（VLMs）作为像素级奖励模型来近似人类偏好。然而，当用于步骤级偏好优化时，这些模型在处理不同时间步长的噪声图像时面临挑战，并需要将复杂转换应用于像素空间。在这项工作中，我们证明了扩散模型天然适合在潜在空间中进行步骤级奖励建模，因为它们可以从噪声潜在图像中自然提取特征。因此，我们提出了潜在奖励模型（LRM），该模型重新利用扩散模型的组件来预测不同时间步长的潜在图像的偏好。基于LRM，我们引入了潜在偏好优化（LPO），这是一种专为潜在空间中的步骤级偏好优化设计的算法。实验结果表明，LPO不仅显著提高了扩散模型与一般偏好、美学偏好和文本图像对齐偏好的对齐性能，而且与现有的偏好优化方法相比实现了2.5-28倍的训练速度提升。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/casiatao/LPO%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/casiatao/LPO上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01051v1">PDF</a> 20 pages, 14 tables, 15 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Latent Reward Model（LRM）的方法，利用扩散模型的固有特性在潜在空间进行步骤级别的奖励建模，从而预测不同时间步长潜在图像的偏好。在此基础上，进一步引入了Latent Preference Optimization（LPO）方法，直接在潜在空间进行步骤级别的偏好优化。实验结果表明，LPO不仅显著提高了扩散模型与通用、美学和文本-图像对齐偏好的对齐性能，而且与现有偏好优化方法相比，实现了2.5-28倍的训练速度提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在潜在空间进行步骤级别的奖励建模具有天然优势，能够提取噪声潜在图像的特征。</li>
<li>提出Latent Reward Model（LRM）方法，利用扩散模型的组件预测不同时间步长潜在图像的偏好。</li>
<li>基于LRM，引入Latent Preference Optimization（LPO）方法，直接在潜在空间进行步骤级别的偏好优化。</li>
<li>LPO不仅能显著提高扩散模型与各种偏好的对齐性能，而且实现了显著的训练速度提升。</li>
<li>LPO方法适用于通用、美学和文本-图像对齐等多种偏好场景。</li>
<li>该研究解决了使用Vision-Language Models（VLMs）在处理不同时间步长噪声图像时的复杂转换问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01051">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-93882ab0ff0d6f68f2f1619e95f0d00c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee307810b1e611e60b713b096e7a2935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6088490955f4353a0574dad9b3a16909.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bbb79a5459f145937d74958321af3cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41404ecbf428f8fca083cd14d3f9d6a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4c1bf4f272373ac7db9ffda5d9946a9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Diffusion-Transformer-Captures-Spatial-Temporal-Dependencies-A-Theory-for-Gaussian-Process-Data"><a href="#Diffusion-Transformer-Captures-Spatial-Temporal-Dependencies-A-Theory-for-Gaussian-Process-Data" class="headerlink" title="Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory   for Gaussian Process Data"></a>Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory   for Gaussian Process Data</h2><p><strong>Authors:Hengyu Fu, Zehao Dou, Jiawei Guo, Mengdi Wang, Minshuo Chen</strong></p>
<p>Diffusion Transformer, the backbone of Sora for video generation, successfully scales the capacity of diffusion models, pioneering new avenues for high-fidelity sequential data generation. Unlike static data such as images, sequential data consists of consecutive data frames indexed by time, exhibiting rich spatial and temporal dependencies. These dependencies represent the underlying dynamic model and are critical to validate the generated data. In this paper, we make the first theoretical step towards bridging diffusion transformers for capturing spatial-temporal dependencies. Specifically, we establish score approximation and distribution estimation guarantees of diffusion transformers for learning Gaussian process data with covariance functions of various decay patterns. We highlight how the spatial-temporal dependencies are captured and affect learning efficiency. Our study proposes a novel transformer approximation theory, where the transformer acts to unroll an algorithm. We support our theoretical results by numerical experiments, providing strong evidence that spatial-temporal dependencies are captured within attention layers, aligning with our approximation theory. </p>
<blockquote>
<p>扩散模型的核心——Diffusion Transformer，为视频生成提供了强大的支撑，成功扩大了扩散模型的容量，为高质量序列数据生成开辟了新途径。不同于静态数据（如图像），序列数据由时间索引的连续数据帧组成，具有丰富的空间和时间依赖性。这些依赖关系代表了基础动态模型，对验证生成的数据至关重要。在本文中，我们朝着将扩散模型应用于捕捉时空依赖性的方向迈出了理论上的第一步。具体来说，我们为学习具有各种衰减模式的协方差函数的高斯过程数据建立扩散模型的得分近似和分布估计保证。我们强调了时空依赖性是如何被捕获并影响学习效率的。我们的研究提出了一种新的转换器近似理论，其中转换器起到展开算法的作用。我们通过数值实验支持我们的理论结果，提供强有力的证据表明时空依赖性被注意力层捕获，这与我们的近似理论相符。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16134v2">PDF</a> 56 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>扩散Transformer作为Sora视频生成的核心，成功扩展了扩散模型的容量，为高质量序列数据生成开辟了新途径。与静态数据（如图像）不同，序列数据由时间索引的连续数据帧组成，表现出丰富的空间和时间依赖性。本文首次从理论上探讨了扩散Transformer在捕捉时空依赖性方面的应用。我们建立了扩散Transformer对具有各种衰减模式的高斯过程数据进行评分逼近和分布估计的保证。本文强调了时空依赖性如何影响学习效率和捕捉方式，并提出了新的Transformer逼近理论，其中Transformer扮演展开算法的角色。数值实验支持我们的理论结果，提供有力证据表明时空依赖性被捕获在注意力层中，与我们的逼近理论相符。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散Transformer成功扩展了扩散模型的容量，为高质量序列数据生成提供了新途径。</li>
<li>序列数据具有时空依赖性，这是生成数据验证的关键。</li>
<li>本文首次从理论上探讨了扩散Transformer在捕捉时空依赖性方面的应用。</li>
<li>建立了扩散Transformer对具有不同衰减模式的高斯过程数据的评分逼近和分布估计的保证。</li>
<li>扩散Transformer能够捕捉和学习时空依赖性，这影响了学习效率和模型性能。</li>
<li>提出新的Transformer逼近理论，其中Transformer扮演算法展开的角色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16134">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4910d39f76cf8a62b462ef4deca7edec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2dc12d397058760db71b3108b58aba5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6a56125eb6031be96d23bc242dfb904.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-53e99a612b943d39cdf14c613335ec94.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-06  AAD-DCE An Aggregated Multimodal Attention Mechanism for Early and Late   Dynamic Contrast Enhanced Prostate MRI Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6f258dda41d56b1266669aaaaa56ea0d.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-02-06  MaintaAvatar A Maintainable Avatar Based on Neural Radiance Fields by   Continual Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11770.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
