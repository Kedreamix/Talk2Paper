<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  Calibrated Multi-Preference Optimization for Aligning Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8846ef6514f9aa6d25a3aa58d18ef713.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    62 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-06-æ›´æ–°"><a href="#2025-02-06-æ›´æ–°" class="headerlink" title="2025-02-06 æ›´æ–°"></a>2025-02-06 æ›´æ–°</h1><h2 id="Calibrated-Multi-Preference-Optimization-for-Aligning-Diffusion-Models"><a href="#Calibrated-Multi-Preference-Optimization-for-Aligning-Diffusion-Models" class="headerlink" title="Calibrated Multi-Preference Optimization for Aligning Diffusion Models"></a>Calibrated Multi-Preference Optimization for Aligning Diffusion Models</h2><p><strong>Authors:Kyungmin Lee, Xiaohang Li, Qifei Wang, Junfeng He, Junjie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin, Feng Yang, Yinxiao Li</strong></p>
<p>Aligning text-to-image (T2I) diffusion models with preference optimization is valuable for human-annotated datasets, but the heavy cost of manual data collection limits scalability. Using reward models offers an alternative, however, current preference optimization methods fall short in exploiting the rich information, as they only consider pairwise preference distribution. Furthermore, they lack generalization to multi-preference scenarios and struggle to handle inconsistencies between rewards. To address this, we present Calibrated Preference Optimization (CaPO), a novel method to align T2I diffusion models by incorporating the general preference from multiple reward models without human annotated data. The core of our approach involves a reward calibration method to approximate the general preference by computing the expected win-rate against the samples generated by the pretrained models. Additionally, we propose a frontier-based pair selection method that effectively manages the multi-preference distribution by selecting pairs from Pareto frontiers. Finally, we use regression loss to fine-tune diffusion models to match the difference between calibrated rewards of a selected pair. Experimental results show that CaPO consistently outperforms prior methods, such as Direct Preference Optimization (DPO), in both single and multi-reward settings validated by evaluation on T2I benchmarks, including GenEval and T2I-Compbench. </p>
<blockquote>
<p>å°†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸åå¥½ä¼˜åŒ–å¯¹é½å¯¹äºäººç±»æ³¨é‡Šæ•°æ®é›†æ˜¯æœ‰ä»·å€¼çš„ï¼Œä½†æ‰‹åŠ¨æ•°æ®æ”¶é›†çš„æ²‰é‡æˆæœ¬é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ä½¿ç”¨å¥–åŠ±æ¨¡å‹æä¾›äº†ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œç„¶è€Œï¼Œå½“å‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ä¸°å¯Œä¿¡æ¯ï¼Œå› ä¸ºå®ƒä»¬åªè€ƒè™‘æˆå¯¹åå¥½åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç¼ºä¹åœ¨å¤šç§åå¥½åœºæ™¯ä¸‹çš„é€šç”¨æ€§ï¼Œå¹¶ä¸”éš¾ä»¥å¤„ç†å¥–åŠ±ä¹‹é—´çš„ä¸€è‡´æ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ ¡å‡†åå¥½ä¼˜åŒ–ï¼ˆCaPOï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆæ¥è‡ªå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„é€šç”¨åå¥½ï¼Œæ— éœ€äººç±»æ³¨é‡Šæ•°æ®å³å¯å¯¹é½T2Iæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå¥–åŠ±æ ¡å‡†æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬çš„é¢„æœŸèƒœç‡æ¥è¿‘ä¼¼é€šç”¨åå¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå‰æ²¿çš„é…å¯¹é€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡ä»å¸•ç´¯æ‰˜å‰æ²¿é€‰æ‹©é…å¯¹æ¥æœ‰æ•ˆåœ°ç®¡ç†å¤šåå¥½åˆ†å¸ƒã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å›å½’æŸå¤±å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥åŒ¹é…é€‰å®šé…å¯¹çš„æ ¡å‡†å¥–åŠ±ä¹‹é—´çš„å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å•ä¸€å¥–åŠ±è¿˜æ˜¯å¤šå¥–åŠ±è®¾ç½®ä¸‹ï¼ŒCaPOåœ¨GenEvalå’ŒT2I-Compbenchç­‰T2IåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02588v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸åå¥½ä¼˜åŒ–çš„å¯¹é½å¯¹äºäººç±»æ³¨é‡Šæ•°æ®é›†å…·æœ‰ä»·å€¼ï¼Œä½†æ‰‹åŠ¨æ•°æ®æ”¶é›†çš„æ˜‚è´µæˆæœ¬é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚ä½¿ç”¨å¥–åŠ±æ¨¡å‹ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œä½†å½“å‰åå¥½ä¼˜åŒ–æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ä¸°å¯Œä¿¡æ¯ï¼Œä»…è€ƒè™‘æˆå¯¹åå¥½åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç¼ºä¹åœ¨å¤šç§åå¥½åœºæ™¯ä¸‹çš„é€šç”¨æ€§ï¼Œå¹¶éš¾ä»¥å¤„ç†å¥–åŠ±é—´çš„ä¸ä¸€è‡´æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ ¡å‡†åå¥½ä¼˜åŒ–ï¼ˆCaPOï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„é€šç”¨åå¥½ï¼Œæ— éœ€äººç±»æ³¨é‡Šæ•°æ®å³å¯å¯¹é½T2Iæ‰©æ•£æ¨¡å‹ã€‚æ ¸å¿ƒåœ¨äºå¥–åŠ±æ ¡å‡†æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—é¢„æœŸèƒœç‡æ¥è¿‘ä¼¼é€šç”¨åå¥½ï¼Œå¯¹æŠ—ç”±é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå‰æ²¿çš„é…å¯¹é€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©å¸•ç´¯æ‰˜å‰æ²¿çš„é…å¯¹æ¥æœ‰æ•ˆç®¡ç†å¤šåå¥½åˆ†å¸ƒã€‚æœ€åï¼Œä½¿ç”¨å›å½’æŸå¤±å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥åŒ¹é…é€‰å®šé…å¯¹çš„æ ¡å‡†å¥–åŠ±å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCaPOåœ¨å•å¥–åŠ±å’Œå¤šå¥–åŠ±è®¾ç½®ä¸‹å‡ä¼˜äºDirect Preference Optimizationï¼ˆDPOï¼‰æ–¹æ³•ï¼Œå¹¶åœ¨GenEvalå’ŒT2I-Compbenchç­‰T2IåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„åå¥½ä¼˜åŒ–å¯¹äºäººç±»æ³¨é‡Šæ•°æ®é›†è‡³å…³é‡è¦ï¼Œä½†æ‰‹åŠ¨æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>å½“å‰åå¥½ä¼˜åŒ–æ–¹æ³•ä»…è€ƒè™‘æˆå¯¹åå¥½åˆ†å¸ƒï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ä¸°å¯Œä¿¡æ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹åœ¨å¤šç§åå¥½åœºæ™¯ä¸‹çš„é€šç”¨æ€§ï¼Œéš¾ä»¥å¤„ç†å¥–åŠ±é—´çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºäº†æ ¡å‡†åå¥½ä¼˜åŒ–ï¼ˆCaPOï¼‰æ–¹æ³•ï¼Œç»“åˆå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„é€šç”¨åå¥½ï¼Œæ— éœ€äººç±»æ³¨é‡Šæ•°æ®å³å¯å¯¹é½T2Iæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>CaPOé€šè¿‡å¥–åŠ±æ ¡å‡†æ–¹æ³•å’ŒåŸºäºå‰æ²¿çš„é…å¯¹é€‰æ‹©æ–¹æ³•æ¥ç®¡ç†å¤šåå¥½åˆ†å¸ƒã€‚</li>
<li>CaPOä½¿ç”¨å›å½’æŸå¤±å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥åŒ¹é…é€‰å®šé…å¯¹çš„æ ¡å‡†å¥–åŠ±å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-32b3c1027aad8bfef221257a011449a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dde4b5d77edebfb190e64f9b638cd715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8846ef6514f9aa6d25a3aa58d18ef713.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-404a4678fec2b5c96685b6cc2cd4fb1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-101789e96f34212aadcc1fe2b0055486.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73e29fdca2bbeea6c19ad2bc338f5330.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Privacy-Attacks-on-Image-AutoRegressive-Models"><a href="#Privacy-Attacks-on-Image-AutoRegressive-Models" class="headerlink" title="Privacy Attacks on Image AutoRegressive Models"></a>Privacy Attacks on Image AutoRegressive Models</h2><p><strong>Authors:Antoni Kowalczuk, Jan DubiÅ„ski, Franziska Boenisch, Adam Dziedzic</strong></p>
<p>Image autoregressive (IAR) models have surpassed diffusion models (DMs) in both image quality (FID: 1.48 vs. 1.58) and generation speed. However, their privacy risks remain largely unexplored. To address this, we conduct a comprehensive privacy analysis comparing IARs to DMs. We develop a novel membership inference attack (MIA) that achieves a significantly higher success rate in detecting training images (TPR@FPR&#x3D;1%: 86.38% for IARs vs. 4.91% for DMs). Using this MIA, we perform dataset inference (DI) and find that IARs require as few as six samples to detect dataset membership, compared to 200 for DMs, indicating higher information leakage. Additionally, we extract hundreds of training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight a fundamental privacy-utility trade-off: while IARs excel in generation quality and speed, they are significantly more vulnerable to privacy attacks. This suggests that incorporating techniques from DMs, such as per-token probability modeling using diffusion, could help mitigate IARsâ€™ privacy risks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars">https://github.com/sprintml/privacy_attacks_against_iars</a>. </p>
<blockquote>
<p>å›¾åƒè‡ªå›å½’ï¼ˆIARï¼‰æ¨¡å‹åœ¨å›¾åƒè´¨é‡ï¼ˆFIDï¼š1.48 vs 1.58ï¼‰å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢éƒ½è¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„éšç§é£é™©å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹IARså’ŒDMsè¿›è¡Œäº†å…¨é¢çš„éšç§åˆ†ææ¯”è¾ƒã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹çš„æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ï¼Œå®ƒåœ¨æ£€æµ‹è®­ç»ƒå›¾åƒæ—¶æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼ˆTPR@FPR&#x3D;1%ï¼šIARsä¸º86.38%ï¼ŒDMsä¸º4.91%ï¼‰ã€‚ä½¿ç”¨è¿™ç§MIAï¼Œæˆ‘ä»¬æ‰§è¡Œæ•°æ®é›†æ¨ç†ï¼ˆDIï¼‰ï¼Œå‘ç°IARsä»…éœ€å…­ä¸ªæ ·æœ¬å³å¯æ£€æµ‹æ•°æ®é›†æˆå‘˜èº«ä»½ï¼Œè€ŒDMsåˆ™éœ€è¦200ä¸ªæ ·æœ¬ï¼Œè¿™è¡¨æ˜ä¿¡æ¯æ³„éœ²æ›´é«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»IARä¸­æå–äº†æ•°ç™¾å¼ è®­ç»ƒå›¾åƒï¼ˆä¾‹å¦‚ï¼Œä»VAR-d30ä¸­æå–äº†698å¼ ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå‡¸æ˜¾äº†éšç§æ•ˆç”¨ä¹‹é—´çš„åŸºæœ¬æƒè¡¡ï¼šè™½ç„¶IARåœ¨ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬æ›´å®¹æ˜“å—åˆ°éšç§æ”»å‡»ã€‚è¿™è¡¨æ˜ç»“åˆDMsçš„æŠ€æœ¯ï¼Œå¦‚ä½¿ç”¨æ‰©æ•£çš„æ¯ä»¤ç‰Œæ¦‚ç‡å»ºæ¨¡ï¼Œå¯èƒ½æœ‰åŠ©äºç¼“è§£IARsçš„éšç§é£é™©ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sprintml/privacy_attacks_against_iarsä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02514v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars">https://github.com/sprintml/privacy_attacks_against_iars</a></p>
<p><strong>Summary</strong></p>
<p>å›¾åƒè‡ªå›å½’ï¼ˆIARï¼‰æ¨¡å‹åœ¨å›¾åƒè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦ä¸Šè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ï¼Œä½†å…¶éšç§é£é™©å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶å¯¹IARå’ŒDMè¿›è¡Œäº†å…¨é¢çš„éšç§åˆ†æï¼Œå¼€å‘äº†ä¸€ç§æ–°å‹æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ï¼Œåœ¨æ£€æµ‹è®­ç»ƒå›¾åƒæ–¹é¢å–å¾—äº†è¾ƒé«˜çš„æˆåŠŸç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒIARåœ¨æ•°æ®é›†æ¨ç†ï¼ˆDIï¼‰æ–¹é¢æ‰€éœ€æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼Œä¸”èƒ½ä»IARä¸­æå–å¤§é‡è®­ç»ƒå›¾åƒã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶IARåœ¨ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æ›´æ˜“å—åˆ°éšç§æ”»å‡»ã€‚ç»“åˆæ‰©æ•£æ¨¡å‹çš„æŸäº›æŠ€æœ¯å¯èƒ½æœ‰åŠ©äºç¼“è§£IARçš„éšç§é£é™©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IARæ¨¡å‹åœ¨å›¾åƒè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦ä¸Šè¶…è¶Šäº†DMï¼Œä½†éšç§é£é™©å°šæœªå¾—åˆ°è¶³å¤Ÿç ”ç©¶ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹çš„MIAï¼Œèƒ½å¤ŸæˆåŠŸæ£€æµ‹IARä¸­è®­ç»ƒå›¾åƒçš„æˆåŠŸç‡è¿œé«˜äºDMã€‚</li>
<li>åœ¨æ•°æ®é›†æ¨ç†æ–¹é¢ï¼ŒIARæ‰€éœ€æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼Œä»…éœ€å…­ä¸ªæ ·æœ¬å³å¯æ£€æµ‹æ•°æ®é›†æˆå‘˜èº«ä»½ï¼Œè€ŒDMéœ€è¦200ä¸ªæ ·æœ¬ã€‚</li>
<li>IARæ¨¡å‹æ›´å®¹æ˜“æ³„éœ²è®­ç»ƒå›¾åƒä¿¡æ¯ï¼Œå¯ä»¥ä»IARä¸­æå–å¤§é‡è®­ç»ƒå›¾åƒã€‚</li>
<li>IARå’ŒDMä¹‹é—´å­˜åœ¨éšç§å®ç”¨æ€§çš„æƒè¡¡ï¼šIARåœ¨ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†éšç§é£é™©æ›´é«˜ã€‚</li>
<li>ç»“åˆæ‰©æ•£æ¨¡å‹çš„æŸäº›æŠ€æœ¯ï¼Œå¦‚é€šè¿‡æ‰©æ•£è¿›è¡Œæ¯ä»¤ç‰Œæ¦‚ç‡å»ºæ¨¡ï¼Œå¯èƒ½æœ‰åŠ©äºç¼“è§£IARçš„éšç§é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58262df3bab208c8603af49257cd4247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2f1e8909482d612842d72e7423ef90e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InterLCM-Low-Quality-Images-as-Intermediate-States-of-Latent-Consistency-Models-for-Effective-Blind-Face-Restoration"><a href="#InterLCM-Low-Quality-Images-as-Intermediate-States-of-Latent-Consistency-Models-for-Effective-Blind-Face-Restoration" class="headerlink" title="InterLCM: Low-Quality Images as Intermediate States of Latent   Consistency Models for Effective Blind Face Restoration"></a>InterLCM: Low-Quality Images as Intermediate States of Latent   Consistency Models for Effective Blind Face Restoration</h2><p><strong>Authors:Senmao Li, Kai Wang, Joost van de Weijer, Fahad Shahbaz Khan, Chun-Le Guo, Shiqi Yang, Yaxing Wang, Jian Yang, Ming-Ming Cheng</strong></p>
<p>Diffusion priors have been used for blind face restoration (BFR) by fine-tuning diffusion models (DMs) on restoration datasets to recover low-quality images. However, the naive application of DMs presents several key limitations. (i) The diffusion prior has inferior semantic consistency (e.g., ID, structure and color.), increasing the difficulty of optimizing the BFR model; (ii) reliance on hundreds of denoising iterations, preventing the effective cooperation with perceptual losses, which is crucial for faithful restoration. Observing that the latent consistency model (LCM) learns consistency noise-to-data mappings on the ODE-trajectory and therefore shows more semantic consistency in the subject identity, structural information and color preservation, we propose InterLCM to leverage the LCM for its superior semantic consistency and efficiency to counter the above issues. Treating low-quality images as the intermediate state of LCM, InterLCM achieves a balance between fidelity and quality by starting from earlier LCM steps. LCM also allows the integration of perceptual loss during training, leading to improved restoration quality, particularly in real-world scenarios. To mitigate structural and semantic uncertainties, InterLCM incorporates a Visual Module to extract visual features and a Spatial Encoder to capture spatial details, enhancing the fidelity of restored images. Extensive experiments demonstrate that InterLCM outperforms existing approaches in both synthetic and real-world datasets while also achieving faster inference speed. </p>
<blockquote>
<p>æ‰©æ•£å…ˆéªŒå·²è¢«ç”¨äºç›²è„¸ä¿®å¤ï¼ˆBFRï¼‰ä¸­ï¼Œé€šè¿‡å¯¹æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è¿›è¡Œå¾®è°ƒä»¥æ¢å¤ä½è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹çš„ç›´æ¥åº”ç”¨å­˜åœ¨å‡ ä¸ªå…³é”®å±€é™æ€§ã€‚ï¼ˆiï¼‰æ‰©æ•£å…ˆéªŒçš„è¯­ä¹‰ä¸€è‡´æ€§è¾ƒå·®ï¼ˆä¾‹å¦‚ï¼Œèº«ä»½ã€ç»“æ„å’Œé¢œè‰²ï¼‰ï¼Œå¢åŠ äº†ä¼˜åŒ–BFRæ¨¡å‹çš„éš¾åº¦ï¼›ï¼ˆiiï¼‰ä¾èµ–äºæ•°ç™¾æ¬¡çš„å»å™ªè¿­ä»£ï¼Œé˜»ç¢äº†ä¸æ„ŸçŸ¥æŸå¤±çš„ååŒå·¥ä½œï¼Œè¿™å¯¹äºå¿ å®æ¢å¤è‡³å…³é‡è¦ã€‚è§‚å¯Ÿåˆ°æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMï¼‰å­¦ä¹ å™ªå£°åˆ°æ•°æ®çš„ä¸€è‡´æ€§æ˜ å°„åœ¨ODEè½¨è¿¹ä¸Šï¼Œå› æ­¤åœ¨å¯¹ä¸»ä½“èº«ä»½ã€ç»“æ„ä¿¡æ¯å’Œé¢œè‰²ä¿ç•™æ–¹é¢æ˜¾ç¤ºå‡ºæ›´é«˜çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºInterLCMï¼Œåˆ©ç”¨LCMçš„ä¼˜å¼‚è¯­ä¹‰ä¸€è‡´æ€§å’Œæ•ˆç‡æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å°†ä½è´¨é‡å›¾åƒè§†ä¸ºLCMçš„ä¸­é—´çŠ¶æ€ï¼ŒInterLCMé€šè¿‡ä»è¾ƒæ—©çš„LCMæ­¥éª¤å¼€å§‹ï¼Œåœ¨ä¿çœŸåº¦å’Œè´¨é‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚LCMè¿˜å…è®¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ•´åˆæ„ŸçŸ¥æŸå¤±ï¼Œä»è€Œæé«˜æ¢å¤è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®åœºæ™¯åº”ç”¨ä¸­ã€‚ä¸ºäº†å‡è½»ç»“æ„å’Œè¯­ä¹‰ä¸ç¡®å®šæ€§ï¼ŒInterLCMç»“åˆäº†è§†è§‰æ¨¡å—æ¥æå–è§†è§‰ç‰¹å¾å’Œä¸€ä¸ªç©ºé—´ç¼–ç å™¨æ¥æ•æ‰ç©ºé—´ç»†èŠ‚ï¼Œå¢å¼ºäº†æ¢å¤å›¾åƒçš„ä¿çœŸåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒInterLCMåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02215v1">PDF</a> Accepted at ICLR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£å…ˆéªŒåœ¨ç›²è„¸ä¿®å¤ï¼ˆBFRï¼‰ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡å¯¹æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è¿›è¡Œå¾®è°ƒä»¥æ¢å¤ä½è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨DMså­˜åœ¨è¯­ä¹‰ä¸€è‡´æ€§å·®å’Œéœ€è¦å¤§é‡å»å™ªè¿­ä»£ç­‰å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†InterLCMæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMï¼‰çš„ä¼˜åŠ¿å’Œæ•ˆç‡ï¼Œæ”¹å–„äº†è¯­ä¹‰ä¸€è‡´æ€§å’Œä¼˜åŒ–é€Ÿåº¦ã€‚InterLCMé€šè¿‡å°†ä½è´¨é‡å›¾åƒè§†ä¸ºLCMçš„ä¸­é—´çŠ¶æ€ï¼Œå®ç°äº†ä¿çœŸåº¦å’Œè´¨é‡ä¹‹é—´çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼Œå®ƒå¼•å…¥äº†è§†è§‰æ¨¡å—å’Œç©ºé—´ç¼–ç å™¨æ¥ç¼“è§£ç»“æ„å’Œè¯­ä¹‰ä¸ç¡®å®šæ€§ï¼Œæé«˜äº†ä¿®å¤å›¾åƒçš„ä¿çœŸåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒInterLCMåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æ›´å¿«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£å…ˆéªŒè¢«ç”¨äºç›²è„¸ä¿®å¤ï¼ˆBFRï¼‰ï¼Œé€šè¿‡å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æ¥æ¢å¤ä½è´¨é‡å›¾åƒã€‚</li>
<li>å•çº¯åº”ç”¨DMså­˜åœ¨è¯­ä¹‰ä¸€è‡´æ€§å·®å’Œéœ€è¦å¤§é‡å»å™ªè¿­ä»£çš„å…³é”®å±€é™ã€‚</li>
<li>InterLCMæ–¹æ³•ç»“åˆæ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMï¼‰çš„ä¼˜åŠ¿å’Œæ•ˆç‡ï¼Œæ”¹å–„è¯­ä¹‰ä¸€è‡´æ€§å’Œä¼˜åŒ–é€Ÿåº¦ã€‚</li>
<li>InterLCMé€šè¿‡å°†ä½è´¨é‡å›¾åƒè§†ä¸ºLCMçš„ä¸­é—´çŠ¶æ€ï¼Œå®ç°ä¿çœŸåº¦å’Œè´¨é‡é—´çš„å¹³è¡¡ã€‚</li>
<li>InterLCMå¼•å…¥è§†è§‰æ¨¡å—å’Œç©ºé—´ç¼–ç å™¨ï¼Œå¢å¼ºå›¾åƒä¿®å¤çš„ä¿çœŸåº¦ã€‚</li>
<li>InterLCMåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8aa50c2dbaacc49707c9b24d9a5162e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bef06ff7f2376c949f68b73b4cd157d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b361ce2d671c20aa99ae4d9fb4e729.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e40b560a2a4d80034d038b128ab577a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="On-the-Guidance-of-Flow-Matching"><a href="#On-the-Guidance-of-Flow-Matching" class="headerlink" title="On the Guidance of Flow Matching"></a>On the Guidance of Flow Matching</h2><p><strong>Authors:Ruiqi Feng, Tailin Wu, Chenglei Yu, Wenhao Deng, Peiyan Hu</strong></p>
<p>Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where guided generation is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at <a target="_blank" rel="noopener" href="https://github.com/AI4Science-WestlakeU/flow_guidance">https://github.com/AI4Science-WestlakeU/flow_guidance</a>. </p>
<blockquote>
<p>æµåŒ¹é…åœ¨å„ç§ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä»å›¾åƒç”Ÿæˆåˆ°å†³ç­–åˆ¶å®šï¼Œå…¶ä¸­å¼•å¯¼ç”Ÿæˆæ˜¯å…³é”®ã€‚ç„¶è€Œï¼ŒæµåŒ¹é…çš„æŒ‡å¯¼æ›´åŠ é€šç”¨ï¼Œå› æ­¤ä¸å…¶å‰èº«æ‰©æ•£æ¨¡å‹æœ‰ç€æœ¬è´¨çš„åŒºåˆ«ã€‚å› æ­¤ï¼Œé€šç”¨æµåŒ¹é…çš„æŒ‡å¯¼æŒ‘æˆ˜ä»ç„¶è¢«å¤§å¤§å¿½è§†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æµåŒ¹é…é€šç”¨æŒ‡å¯¼æ¡†æ¶ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—å¯åº”ç”¨äºé€šç”¨æµåŒ¹é…çš„æŒ‡å¯¼æŠ€æœ¯ã€‚è¿™åŒ…æ‹¬æ— éœ€è®­ç»ƒå³å¯æ¸è¿‘ç²¾ç¡®æŒ‡å¯¼çš„æ–°æŠ€æœ¯ã€åŸºäºè®­ç»ƒæŒ‡å¯¼çš„æ–°è®­ç»ƒæŸå¤±ï¼Œä»¥åŠæ¶µç›–ç»å…¸æ¢¯åº¦æŒ‡å¯¼æ–¹æ³•ä¸ºç‰¹ä¾‹çš„ä¸¤ç±»è¿‘ä¼¼æŒ‡å¯¼ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šæ¢è®¨äº†è¿™äº›æ–¹æ³•ï¼Œä¸ºä¸åŒåœºæ™¯é€‰æ‹©é€‚å½“çš„æ–¹æ³•æä¾›äº†å®ç”¨æŒ‡å—ã€‚åœ¨åˆæˆæ•°æ®é›†ã€å›¾åƒåé—®é¢˜å’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æŒ‡å¯¼æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶éªŒè¯äº†æˆ‘ä»¬çš„æµåŒ¹é…æŒ‡å¯¼æ¡†æ¶çš„æ­£ç¡®æ€§ã€‚å¯åœ¨æ­¤æ‰¾åˆ°é‡ç°å®éªŒçš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/AI4Science-WestlakeU/flow_guidance%E3%80%82">https://github.com/AI4Science-WestlakeU/flow_guidanceã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02150v1">PDF</a> 35 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†æµåŒ¹é…é€šç”¨æŒ‡å¯¼æ¡†æ¶ï¼Œè§£å†³äº†æµåŒ¹é…åœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„æŒ‡å¯¼é—®é¢˜ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†ä¸€ç³»åˆ—æŒ‡å¯¼æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ— è®­ç»ƒæ¸è¿›ç²¾ç¡®æŒ‡å¯¼ã€æ–°å‹è®­ç»ƒæŸå¤±è®­ç»ƒæŒ‡å¯¼ä»¥åŠæ¶µç›–ç»å…¸æ¢¯åº¦æŒ‡å¯¼æ–¹æ³•çš„ä¸¤ç±»è¿‘ä¼¼æŒ‡å¯¼ã€‚é€šè¿‡ç†è®ºç ”ç©¶å’Œå®éªŒéªŒè¯ï¼Œæœ¬æ–‡å±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµåŒ¹é…åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¼•å¯¼ç”Ÿæˆçš„ä»»åŠ¡ä¸­ã€‚</li>
<li>æµåŒ¹é…çš„æŒ‡å¯¼ä¸å…¶å‰èº«æ‰©æ•£æ¨¡å‹ç›¸æ¯”æ›´ä¸ºé€šç”¨ä¸”æœ‰æ‰€å·®å¼‚ã€‚</li>
<li>æå‡ºé¦–ä¸ªæµåŒ¹é…é€šç”¨æŒ‡å¯¼æ¡†æ¶ï¼Œæ¶µç›–å¤šç§æŒ‡å¯¼æŠ€æœ¯ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬æ— è®­ç»ƒæ¸è¿›ç²¾ç¡®æŒ‡å¯¼ï¼Œæ–°å‹è®­ç»ƒæŸå¤±è®­ç»ƒæŒ‡å¯¼ä»¥åŠæ¶µç›–ç»å…¸æ¢¯åº¦æŒ‡å¯¼æ–¹æ³•çš„ä¸¤ç±»è¿‘ä¼¼æŒ‡å¯¼ã€‚</li>
<li>å¯¹ä¸åŒæŒ‡å¯¼æ–¹æ³•è¿›è¡Œäº†ç†è®ºç ”ç©¶ï¼Œä¸ºä¸åŒåœºæ™¯é€‰æ‹©åˆé€‚æ–¹æ³•æä¾›äº†å®è·µæŒ‡å—ã€‚</li>
<li>åœ¨åˆæˆæ•°æ®é›†ã€å›¾åƒåé—®é¢˜ä»¥åŠç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸Šçš„å®éªŒéªŒè¯äº†æ‰€ææŒ‡å¯¼æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b8f0923b3549ef32d3ba229ff6bf9f31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18e872942ce45433ea1422c4b0f24260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8a3e41427cae22bb2efb6e00eeeb8eb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="One-Diffusion-Step-to-Real-World-Super-Resolution-via-Flow-Trajectory-Distillation"><a href="#One-Diffusion-Step-to-Real-World-Super-Resolution-via-Flow-Trajectory-Distillation" class="headerlink" title="One Diffusion Step to Real-World Super-Resolution via Flow Trajectory   Distillation"></a>One Diffusion Step to Real-World Super-Resolution via Flow Trajectory   Distillation</h2><p><strong>Authors:Jianze Li, Jiezhang Cao, Yong Guo, Wenbo Li, Yulun Zhang</strong></p>
<p>Diffusion models (DMs) have significantly advanced the development of real-world image super-resolution (Real-ISR), but the computational cost of multi-step diffusion models limits their application. One-step diffusion models generate high-quality images in a one sampling step, greatly reducing computational overhead and inference latency. However, most existing one-step diffusion methods are constrained by the performance of the teacher model, where poor teacher performance results in image artifacts. To address this limitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique based on flow matching models. We use the state-of-the-art diffusion model FLUX.1-dev as both the teacher model and the base model. First, we introduce Flow Trajectory Distillation (FTD) to distill a multi-step flow matching model into a one-step Real-ISR. Second, to improve image realism and address high-frequency artifact issues in generated images, we propose TV-LPIPS as a perceptual loss and introduce Attention Diversification Loss (ADL) as a regularization term to reduce token similarity in transformer, thereby eliminating high-frequency artifacts. Comprehensive experiments demonstrate that our method outperforms existing one-step diffusion-based Real-ISR methods. The code and model will be released at <a target="_blank" rel="noopener" href="https://github.com/JianzeLi-114/FluxSR">https://github.com/JianzeLi-114/FluxSR</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æ˜¾è‘—æ¨åŠ¨äº†çœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰çš„å‘å±•ï¼Œä½†å¤šæ­¥æ‰©æ•£æ¨¡å‹çš„è®¡ç®—æˆæœ¬é™åˆ¶äº†å…¶åº”ç”¨ã€‚ä¸€æ­¥æ‰©æ•£æ¨¡å‹åœ¨ä¸€æ­¥é‡‡æ ·è¿‡ç¨‹ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œå¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€å’Œæ¨ç†å»¶è¿Ÿã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„ä¸€æ­¥æ‰©æ•£æ–¹æ³•å—åˆ°æ•™å¸ˆæ¨¡å‹æ€§èƒ½çš„åˆ¶çº¦ï¼Œæ•™å¸ˆæ¨¡å‹æ€§èƒ½ä¸ä½³ä¼šå¯¼è‡´å›¾åƒå‡ºç°ä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FluxSRï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæµåŒ¹é…æ¨¡å‹çš„æ–°å‹ä¸€æ­¥æ‰©æ•£Real-ISRæŠ€æœ¯ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹FLUX.1-devä½œä¸ºæ•™å¸ˆæ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥æµè½¨è¿¹è’¸é¦ï¼ˆFTDï¼‰æŠ€æœ¯ï¼Œå°†å¤šæ­¥æµåŒ¹é…æ¨¡å‹è½¬åŒ–ä¸ºä¸€æ­¥Real-ISRã€‚å…¶æ¬¡ï¼Œä¸ºäº†æé«˜å›¾åƒçš„çœŸå®æ€§å’Œè§£å†³ç”Ÿæˆå›¾åƒä¸­çš„é«˜é¢‘ä¼ªå½±é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå°†TV-LPIPSä½œä¸ºæ„ŸçŸ¥æŸå¤±ï¼Œå¹¶å¼•å…¥æ³¨æ„åŠ›å¤šæ ·åŒ–æŸå¤±ï¼ˆADLï¼‰ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œä»¥å‡å°‘transformerä¸­çš„ä»¤ç‰Œç›¸ä¼¼æ€§ï¼Œä»è€Œæ¶ˆé™¤é«˜é¢‘ä¼ªå½±ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„åŸºäºä¸€æ­¥æ‰©æ•£çš„Real-ISRæ–¹æ³•ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JianzeLi-114/FluxSR">https://github.com/JianzeLi-114/FluxSR</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01993v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å¤šæ­¥æ‰©æ•£æ¨¡å‹è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæµåŒ¹é…æ¨¡å‹çš„æ–°å‹ä¸€æ­¥æ‰©æ•£Real-ISRæŠ€æœ¯FluxSRï¼Œé€šè¿‡å¼•å…¥Flow Trajectory Distillationï¼ˆFTDï¼‰å°†å¤šæ­¥æµåŒ¹é…æ¨¡å‹è½¬åŒ–ä¸ºä¸€æ­¥Real-ISRï¼Œå¹¶å¼•å…¥TV-LPIPSä½œä¸ºæ„ŸçŸ¥æŸå¤±å’Œæ³¨æ„åŠ›å¤šæ ·åŒ–æŸå¤±ï¼ˆADLï¼‰ä½œä¸ºæ­£åˆ™åŒ–é¡¹æ¥å‡å°‘å˜æ¢å™¨ä¸­çš„ä»¤ç‰Œç›¸ä¼¼æ€§ï¼Œæ¶ˆé™¤é«˜é¢‘ä¼ªå½±ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„ä¸€æ­¥æ‰©æ•£Real-ISRæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰æ–¹é¢å–å¾—è¿›å±•ã€‚</li>
<li>å¤šæ­¥æ‰©æ•£æ¨¡å‹è®¡ç®—æˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚</li>
<li>FluxSRæ˜¯ä¸€ç§æ–°å‹ä¸€æ­¥æ‰©æ•£Real-ISRæŠ€æœ¯ï¼ŒåŸºäºæµåŒ¹é…æ¨¡å‹ã€‚</li>
<li>FluxSRé€šè¿‡Flow Trajectory Distillationï¼ˆFTDï¼‰å°†å¤šæ­¥æµåŒ¹é…æ¨¡å‹è½¬åŒ–ä¸ºä¸€æ­¥Real-ISRã€‚</li>
<li>ä¸ºæé«˜å›¾åƒçœŸå®æ€§å’Œè§£å†³ç”Ÿæˆå›¾åƒçš„é«˜é¢‘ä¼ªå½±é—®é¢˜ï¼ŒFluxSRå¼•å…¥TV-LPIPSä½œä¸ºæ„ŸçŸ¥æŸå¤±ã€‚</li>
<li>FluxSRæå‡ºAttention Diversification Lossï¼ˆADLï¼‰ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œä»¥å‡å°‘å˜æ¢å™¨ä¸­çš„ä»¤ç‰Œç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d34e81f169f2698c31f398362442ea22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e4e92b60bad3b5ed861202f4366bc3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1816359a806fb7dd7fc83bdc5c9d42c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9e96df63fefd28452a7f3fada5c1f7b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation"><a href="#Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation" class="headerlink" title="Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation"></a>Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation</h2><p><strong>Authors:Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong</strong></p>
<p>Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an \textbf{online} algorithm capable of collecting data during runtime and supporting a \textbf{black-box} objective function. Moreover, the \textbf{query efficiency} of the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm, \textbf{Fast Direct}, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a> </p>
<blockquote>
<p>å¼•å¯¼å¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–¹å‘å¯¹äºå®šåˆ¶é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ä»¥è§£å†³ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚ç°æœ‰çš„å¼•å¯¼å¼æ‰©æ•£æ¨¡å‹è¦ä¹ˆä¾èµ–äºä½¿ç”¨é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†å¯¹å¼•å¯¼æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¦ä¹ˆéœ€è¦ç›®æ ‡å‡½æ•°å¯å¾®ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§å¤šæ•°ç°å®ä¸–ç•Œä»»åŠ¡è€Œè¨€ï¼Œç¦»çº¿æ•°æ®é›†é€šå¸¸ä¸å¯ç”¨ï¼Œå¹¶ä¸”å…¶ç›®æ ‡å‡½æ•°é€šå¸¸ä¸å¯å¾®åˆ†ï¼Œä¾‹å¦‚å…·æœ‰äººç±»åå¥½çš„å›¾åƒç”Ÿæˆã€ç”¨äºè¯ç‰©å‘ç°çš„åˆ†å­ç”Ÿæˆä»¥åŠææ–™è®¾è®¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§<strong>åœ¨çº¿</strong>ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿåœ¨è¿è¡Œæ—¶æ”¶é›†æ•°æ®å¹¶æ”¯æŒ<strong>é»‘ç®±</strong>ç›®æ ‡å‡½æ•°ã€‚æ­¤å¤–ï¼Œç®—æ³•çš„<strong>æŸ¥è¯¢æ•ˆç‡</strong>ä¹Ÿè‡³å…³é‡è¦ï¼Œå› ä¸ºåœ¨ç°å®åœºæ™¯ä¸­ï¼Œç›®æ ‡æŸ¥è¯¢çš„è¯„ä¼°å¾€å¾€æˆæœ¬é«˜æ˜‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–è€Œç®€å•çš„ç®—æ³•â€”â€”<strong>Fast Direct</strong>ï¼Œç”¨äºé«˜æ•ˆæŸ¥è¯¢åœ¨çº¿é»‘ç®±ç›®æ ‡ç”Ÿæˆã€‚æˆ‘ä»¬çš„Fast Directåœ¨æ•°æ®æµå½¢ä¸Šæ„å»ºä¼ªç›®æ ‡ï¼Œä»¥é€šç”¨æ–¹å‘æ›´æ–°æ‰©æ•£æ¨¡å‹çš„å™ªå£°åºåˆ—ï¼Œè¿™åœ¨æ‰§è¡Œé«˜æ•ˆæŸ¥è¯¢å¼•å¯¼ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚åœ¨åäºŒä¸ªé«˜åˆ†è¾¨ç‡ï¼ˆ$\small {1024 \times 1024}$ï¼‰å›¾åƒç›®æ ‡ç”Ÿæˆä»»åŠ¡å’Œå…­ä¸ª3Dåˆ†å­ç›®æ ‡ç”Ÿæˆä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒæ˜¾ç¤ºï¼ŒæŸ¥è¯¢æ•ˆç‡æé«˜äº†$\textbf{6}$å€è‡³$\textbf{10}$å€ï¼Œä»¥åŠåˆ†åˆ«æé«˜äº†$\textbf{11}$å€è‡³$\textbf{44}$å€ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01692v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ˜¯å®šåˆ¶é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ä»¥åº”å¯¹ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚ç°æœ‰å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¾èµ–äºä½¿ç”¨é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†è®­ç»ƒçš„æŒ‡å¯¼æ¨¡å‹ï¼Œæˆ–è€…éœ€è¦ç›®æ ‡å‡½æ•°å¯å¾®ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§å¤šæ•°ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œç¦»çº¿æ•°æ®é›†é€šå¸¸ä¸å¯ç”¨ï¼Œä¸”å…¶ç›®æ ‡å‡½æ•°é€šå¸¸ä¸å¯å¾®åˆ†ï¼Œå¦‚åŸºäºäººç±»åå¥½çš„å›¾åƒç”Ÿæˆã€ç”¨äºè¯ç‰©å‘ç°çš„åˆ†å­ç”Ÿæˆå’Œææ–™è®¾è®¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§èƒ½å¤Ÿåœ¨è¿è¡Œæ—¶æ”¶é›†æ•°æ®çš„åœ¨çº¿ç®—æ³•ï¼Œå¹¶æ”¯æŒé»‘ç®±ç›®æ ‡å‡½æ•°ã€‚æ­¤å¤–ï¼Œç®—æ³•çš„æŸ¥è¯¢æ•ˆç‡ä¹Ÿè‡³å…³é‡è¦ï¼Œå› ä¸ºåœ¨ç°å®åœºæ™¯ä¸­ç›®æ ‡æŸ¥è¯¢çš„è¯„ä¼°å¾€å¾€å¾ˆæ˜‚è´µã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–è€Œç®€å•çš„ç®—æ³•Fast Directï¼Œç”¨äºæŸ¥è¯¢é«˜æ•ˆçš„åœ¨çº¿é»‘ç®±ç›®æ ‡ç”Ÿæˆã€‚æˆ‘ä»¬çš„Fast Directåœ¨æ•°æ®æµå½¢ä¸Šæ„å»ºä¼ªç›®æ ‡ï¼Œä»¥é€šç”¨æ–¹å‘æ›´æ–°æ‰©æ•£æ¨¡å‹çš„å™ªå£°åºåˆ—ï¼Œæœ‰æœ›åœ¨æŸ¥è¯¢é«˜æ•ˆçš„å¼•å¯¼ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨åäºŒä¸ªé«˜åˆ†è¾¨ç‡ï¼ˆ$1024 \times 1024$ï¼‰å›¾åƒç›®æ ‡ç”Ÿæˆä»»åŠ¡å’Œå…­ä¸ª3Dåˆ†å­ç›®æ ‡ç”Ÿæˆä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒæ˜¾ç¤ºï¼ŒæŸ¥è¯¢æ•ˆç‡æé«˜äº†6è‡³10å€å’Œ11è‡³44å€ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-directå…¬å¼€è®¿é—®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ˜¯å®šåˆ¶é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æœ‰åŠ›æ–¹å‘ï¼Œå°¤å…¶é€‚ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¾èµ–äºç¦»çº¿æ•°æ®é›†æˆ–å¯å¾®ç›®æ ‡å‡½æ•°ï¼Œè¿™åœ¨ç°å®ä»»åŠ¡ä¸­å¾€å¾€ä¸å¯ç”¨æˆ–ä¸å¯å¾®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–ç®—æ³•Fast Directï¼Œæ”¯æŒåœ¨çº¿é»‘ç®±ç›®æ ‡ç”Ÿæˆï¼Œå¯åœ¨è¿è¡Œæ—¶æ”¶é›†æ•°æ®ã€‚</li>
<li>Fast Directé€šè¿‡æ„å»ºä¼ªç›®æ ‡åœ¨æ•°æ®æµå½¢ä¸Šæ›´æ–°å™ªå£°åºåˆ—ï¼Œå…·æœ‰æŸ¥è¯¢é«˜æ•ˆçš„æ½œåŠ›ã€‚</li>
<li>åœ¨å›¾åƒå’Œåˆ†å­ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šï¼ŒFast Directæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æŸ¥è¯¢æ•ˆç‡æ”¹è¿›ã€‚</li>
<li>Fast Directç®—æ³•çš„å®ç°å·²å…¬å¼€å¯ç”¨ï¼Œæ–¹ä¾¿å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ½œåŠ›è§£å†³ç°å®ä¸–ç•Œä¸­ç›®æ ‡æŸ¥è¯¢è¯„ä¼°æ˜‚è´µçš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e583201520afad6bd1a558d3cc9e80c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8e6b3245a8fea1168ea1a33d9883dd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b02e30e6862644daeb69fdee46ecbd0f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HuViDPO-Enhancing-Video-Generation-through-Direct-Preference-Optimization-for-Human-Centric-Alignment"><a href="#HuViDPO-Enhancing-Video-Generation-through-Direct-Preference-Optimization-for-Human-Centric-Alignment" class="headerlink" title="HuViDPO:Enhancing Video Generation through Direct Preference   Optimization for Human-Centric Alignment"></a>HuViDPO:Enhancing Video Generation through Direct Preference   Optimization for Human-Centric Alignment</h2><p><strong>Authors:Lifan Jiang, Boxi Wu, Jiahui Zhang, Xiaotong Guan, Shuang Chen</strong></p>
<p>With the rapid development of AIGC technology, significant progress has been made in diffusion model-based technologies for text-to-image (T2I) and text-to-video (T2V). In recent years, a few studies have introduced the strategy of Direct Preference Optimization (DPO) into T2I tasks, significantly enhancing human preferences in generated images. However, existing T2V generation methods lack a well-formed pipeline with exact loss function to guide the alignment of generated videos with human preferences using DPO strategies. Additionally, challenges such as the scarcity of paired video preference data hinder effective model training. At the same time, the lack of training datasets poses a risk of insufficient flexibility and poor video generation quality in the generated videos. Based on those problems, our work proposes three targeted solutions in sequence. 1) Our work is the first to introduce the DPO strategy into the T2V tasks. By deriving a carefully structured loss function, we utilize human feedback to align video generation with human preferences. We refer to this new method as HuViDPO. 2) Our work constructs small-scale human preference datasets for each action category and fine-tune this model, improving the aesthetic quality of the generated videos while reducing training costs. 3) We adopt a First-Frame-Conditioned strategy, leveraging the rich in formation from the first frame to guide the generation of subsequent frames, enhancing flexibility in video generation. At the same time, we employ a SparseCausal Attention mechanism to enhance the quality of the generated videos.More details and examples can be accessed on our website: <a target="_blank" rel="noopener" href="https://tankowa.github.io/HuViDPO">https://tankowa.github.io/HuViDPO</a>. github.io&#x2F;. </p>
<blockquote>
<p>éšç€AIGCæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰å’Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ä¸€äº›ç ”ç©¶å·²å°†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­–ç•¥å¼•å…¥T2Iä»»åŠ¡ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„äººç±»åå¥½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„T2Vç”Ÿæˆæ–¹æ³•ç¼ºä¹å®Œå–„çš„ç®¡é“å’Œç²¾ç¡®çš„æŸå¤±å‡½æ•°ï¼Œæ— æ³•ä½¿ç”¨DPOç­–ç•¥å°†ç”Ÿæˆè§†é¢‘ä¸äººç±»åå¥½å¯¹é½ã€‚æ­¤å¤–ï¼Œé…å¯¹è§†é¢‘åå¥½æ•°æ®çš„ç¨€ç¼ºæ€§ç»™æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒå¸¦æ¥äº†æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œè®­ç»ƒæ•°æ®é›†çš„ç¼ºä¹å¯èƒ½å¯¼è‡´ç”Ÿæˆè§†é¢‘çš„çµæ´»æ€§å’Œè´¨é‡ä¸è¶³ã€‚åŸºäºè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸‰ä¸ªæœ‰é’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆã€‚1ï¼‰æˆ‘ä»¬çš„å·¥ä½œæ˜¯é¦–æ¬¡å°†DPOç­–ç•¥å¼•å…¥T2Vä»»åŠ¡ã€‚é€šè¿‡ç²¾å¿ƒæ„å»ºçš„æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬åˆ©ç”¨äººç±»åé¦ˆå°†è§†é¢‘ç”Ÿæˆä¸äººç±»åå¥½å¯¹é½ã€‚æˆ‘ä»¬å°†è¿™ç§æ–°æ–¹æ³•ç§°ä¸ºHuViDPOã€‚2ï¼‰æˆ‘ä»¬çš„å·¥ä½œä¸ºæ¯ä¸ªåŠ¨ä½œç±»åˆ«æ„å»ºäº†å°è§„æ¨¡çš„äººç±»åå¥½æ•°æ®é›†ï¼Œå¹¶å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œæé«˜äº†ç”Ÿæˆè§†é¢‘çš„ç¾å­¦è´¨é‡ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚3ï¼‰æˆ‘ä»¬é‡‡ç”¨First-Frame-Conditionedç­–ç•¥ï¼Œåˆ©ç”¨ç¬¬ä¸€å¸§çš„ä¸°å¯Œä¿¡æ¯æ¥å¼•å¯¼åç»­å¸§çš„ç”Ÿæˆï¼Œæé«˜è§†é¢‘ç”Ÿæˆçš„çµæ´»æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨SparseCausal Attentionæœºåˆ¶ï¼Œä»¥æé«˜ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚æ›´å¤šè¯¦æƒ…å’Œç¤ºä¾‹è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ï¼š[<a target="_blank" rel="noopener" href="https://tankowa.github.io/HuViDPO/]">https://tankowa.github.io/HuViDPO/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01690v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€AIGCæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œæ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰å’Œæ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰çš„æ‰©æ•£æ¨¡å‹æŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è™½ç„¶Direct Preference Optimizationï¼ˆDPOï¼‰ç­–ç•¥å·²åº”ç”¨äºT2Iä»»åŠ¡å¹¶æå‡äº†ç”Ÿæˆå›¾åƒçš„äººç±»åå¥½ï¼Œä½†åœ¨T2Vç”Ÿæˆæ–¹æ³•ä¸­ï¼Œåº”ç”¨DPOç­–ç•¥çš„ç®¡é“å°šæœªå®Œå–„ï¼Œç¼ºä¹ç²¾ç¡®çš„æŸå¤±å‡½æ•°æ¥æŒ‡å¯¼ç”Ÿæˆè§†é¢‘ä¸äººç±»åå¥½çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œç¼ºä¹é…å¯¹è§†é¢‘åå¥½æ•°æ®ç­‰æŒ‘æˆ˜å½±å“äº†æ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸‰é¡¹é’ˆå¯¹æ€§è§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é¦–æ¬¡å°†DPOç­–ç•¥å¼•å…¥T2Vä»»åŠ¡ï¼Œé€šè¿‡æ„å»ºç²¾ç»†ç»“æ„çš„æŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨äººç±»åé¦ˆå°†è§†é¢‘ç”Ÿæˆä¸äººç±»åå¥½å¯¹é½ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºHuViDPOã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ„å»ºäº†æ¯ä¸ªåŠ¨ä½œç±»åˆ«çš„å°è§„æ¨¡äººç±»åå¥½æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†ç”Ÿæˆè§†é¢‘çš„ç¾å­¦è´¨é‡å¹¶é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨First-Frame-Conditionedç­–ç•¥ï¼Œåˆ©ç”¨ç¬¬ä¸€å¸§çš„ä¸°å¯Œä¿¡æ¯æ¥æŒ‡å¯¼åç»­å¸§çš„ç”Ÿæˆï¼Œå¢å¼ºäº†è§†é¢‘ç”Ÿæˆçš„çµæ´»æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†SparseCausal Attentionæœºåˆ¶æ¥æå‡ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚æ›´å¤šè¯¦æƒ…å’Œç¤ºä¾‹å¯è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://tankowa.github.io/HuViDPO">HuViDPOå®˜ç½‘é“¾æ¥</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¦–æ¬¡å°†Direct Preference Optimizationï¼ˆDPOï¼‰ç­–ç•¥å¼•å…¥æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰ä»»åŠ¡ï¼Œæå‡ºHuViDPOæ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ„å»ºæŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨äººç±»åé¦ˆå°†è§†é¢‘ç”Ÿæˆä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>æ„å»ºå°è§„æ¨¡äººç±»åå¥½æ•°æ®é›†è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œæé«˜ç”Ÿæˆè§†é¢‘çš„ç¾å­¦è´¨é‡å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>é‡‡ç”¨First-Frame-Conditionedç­–ç•¥ï¼Œåˆ©ç”¨ç¬¬ä¸€å¸§ä¿¡æ¯æå‡è§†é¢‘ç”Ÿæˆçš„çµæ´»æ€§å’Œè´¨é‡ã€‚</li>
<li>å¼•å…¥SparseCausal Attentionæœºåˆ¶ï¼Œè¿›ä¸€æ­¥å¢å¼ºç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚</li>
<li>æä¾›äº†ç½‘ç«™é“¾æ¥ï¼Œå¯è·å–æ›´å¤šå…³äºHuViDPOçš„ç»†èŠ‚å’Œç¤ºä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54d98a51ec56fb6f2177b7ff19a21b3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3702cd8f2dae45966cd4d0609eb5d01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b362613fb71f98f85dbd89f89daa67d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3227cf25eed143ba397c230ab0dbeb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c228874b7e0e03a41750a92a269c1799.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf1d6fd1691d1accd80fe38c5ab29d4e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="BD-Diff-Generative-Diffusion-Model-for-Image-Deblurring-on-Unknown-Domains-with-Blur-Decoupled-Learning"><a href="#BD-Diff-Generative-Diffusion-Model-for-Image-Deblurring-on-Unknown-Domains-with-Blur-Decoupled-Learning" class="headerlink" title="BD-Diff: Generative Diffusion Model for Image Deblurring on Unknown   Domains with Blur-Decoupled Learning"></a>BD-Diff: Generative Diffusion Model for Image Deblurring on Unknown   Domains with Blur-Decoupled Learning</h2><p><strong>Authors:Junhao Cheng, Wei-Ting Chen, Xi Lu, Ming-Hsuan Yang</strong></p>
<p>Generative diffusion models trained on large-scale datasets have achieved remarkable progress in image synthesis. In favor of their ability to supplement missing details and generate aesthetically pleasing contents, recent works have applied them to image deblurring tasks via training an adapter on blurry-sharp image pairs to provide structural conditions for restoration. However, acquiring substantial amounts of realistic paired data is challenging and costly in real-world scenarios. On the other hand, relying solely on synthetic data often results in overfitting, leading to unsatisfactory performance when confronted with unseen blur patterns. To tackle this issue, we propose BD-Diff, a generative-diffusion-based model designed to enhance deblurring performance on unknown domains by decoupling structural features and blur patterns through joint training on three specially designed tasks. We employ two Q-Formers as structural representations and blur patterns extractors separately. The features extracted by them will be used for the supervised deblurring task on synthetic data and the unsupervised blur-transfer task by leveraging unpaired blurred images from the target domain simultaneously. Furthermore, we introduce a reconstruction task to make the structural features and blur patterns complementary. This blur-decoupled learning process enhances the generalization capabilities of BD-Diff when encountering unknown domain blur patterns. Experiments on real-world datasets demonstrate that BD-Diff outperforms existing state-of-the-art methods in blur removal and structural preservation in various challenging scenarios. The codes will be released in <a target="_blank" rel="noopener" href="https://github.com/donahowe/BD-Diff">https://github.com/donahowe/BD-Diff</a> </p>
<blockquote>
<p>åŸºäºå¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç”±äºå…¶èƒ½å¤Ÿè¡¥å……ç¼ºå¤±çš„ç»†èŠ‚å¹¶ç”Ÿæˆç¾è§‚çš„å†…å®¹ï¼Œæœ€è¿‘çš„ç ”ç©¶å°†å…¶åº”ç”¨äºå›¾åƒå»æ¨¡ç³Šä»»åŠ¡ï¼Œé€šè¿‡è®­ç»ƒé€‚é…å™¨å¯¹æ¨¡ç³Šå’Œæ¸…æ™°å›¾åƒå¯¹è¿›è¡Œè®­ç»ƒï¼Œä¸ºæ¢å¤æä¾›ç»“æ„æ¡ä»¶ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­è·å–å¤§é‡çœŸå®çš„é…å¯¹æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚å¦ä¸€æ–¹é¢ï¼Œä»…ä¾èµ–åˆæˆæ•°æ®å¾€å¾€ä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆï¼Œåœ¨é¢å¯¹æœªçŸ¥çš„æ¨¡ç³Šæ¨¡å¼æ—¶ï¼Œæ€§èƒ½å¾€å¾€ä¸å°½å¦‚äººæ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BD-Diffï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç”Ÿæˆæ‰©æ•£çš„æ¨¡å‹ï¼Œé€šè¿‡è”åˆè®­ç»ƒä¸‰ä¸ªä¸“é—¨è®¾è®¡çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æé«˜æœªçŸ¥é¢†åŸŸçš„å»æ¨¡ç³Šæ€§èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤ä¸ªQ-Formersåˆ†åˆ«ä½œä¸ºç»“æ„ç‰¹å¾å’Œæ¨¡ç³Šæ¨¡å¼çš„è¡¨ç¤ºå’Œæå–å™¨ã€‚å®ƒä»¬æå–çš„ç‰¹å¾å°†ç”¨äºåˆæˆæ•°æ®ä¸Šçš„ç›‘ç£å»æ¨¡ç³Šä»»åŠ¡ï¼Œå¹¶åŒæ—¶åˆ©ç”¨æ¥è‡ªç›®æ ‡åŸŸçš„æ— é…å¯¹æ¨¡ç³Šå›¾åƒè¿›è¡Œæ— ç›‘ç£çš„æ¨¡ç³Šè½¬ç§»ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé‡å»ºä»»åŠ¡ï¼Œä½¿ç»“æ„ç‰¹å¾å’Œæ¨¡ç³Šæ¨¡å¼ç›¸äº’è¡¥å……ã€‚è¿™ç§å»æ¨¡ç³Šè§£è€¦çš„å­¦ä¹ è¿‡ç¨‹æé«˜äº†BD-Diffåœ¨é‡åˆ°æœªçŸ¥é¢†åŸŸæ¨¡ç³Šæ¨¡å¼æ—¶çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBD-Diffåœ¨å¤šç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ï¼Œåœ¨æ¨¡ç³Šå»é™¤å’Œç»“æ„ä¿ç•™æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚ç›¸å…³ä»£ç å°†å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/donahowe/BD-Diff">https://github.com/donahowe/BD-Diff</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01522v1">PDF</a> We propose BD-Diff to integrate generative diffusion model into   unpaired deblurring tasks</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹æ•°æ®é›†è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹å·²åœ¨å›¾åƒåˆæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚ä¸ºåˆ©ç”¨å…¶åœ¨è¡¥å……ç¼ºå¤±ç»†èŠ‚å’Œç”Ÿæˆç¾è§‚å†…å®¹æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿‘æœŸç ”ç©¶å°è¯•å°†å…¶åº”ç”¨äºå›¾åƒå»æ¨¡ç³Šä»»åŠ¡ã€‚ç„¶è€Œï¼Œè·å–å¤§é‡çœŸå®é…å¯¹æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºBD-Diffæ¨¡å‹ï¼Œé€šè¿‡è”åˆè®­ç»ƒä¸‰ä¸ªç‰¹åˆ«è®¾è®¡çš„ä»»åŠ¡ï¼Œå¢å¼ºåœ¨æœªçŸ¥åŸŸä¸Šçš„å»æ¨¡ç³Šæ€§èƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤ä¸ªQ-Formersåˆ†åˆ«ä½œä¸ºç»“æ„ç‰¹å¾è¡¨ç¤ºå’Œæ¨¡ç³Šæ¨¡å¼æå–å™¨ã€‚å®éªŒè¯æ˜ï¼ŒBD-Diffåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œåœ¨å„ç§æŒ‘æˆ˜åœºæ™¯ä¸‹å®ç°æ›´å¥½çš„å»æ¨¡ç³Šå’Œç»“æ„ä¿ç•™æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå¯åº”ç”¨äºå›¾åƒå»æ¨¡ç³Šä»»åŠ¡ã€‚</li>
<li>è·å–çœŸå®é…å¯¹æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬ï¼Œå•çº¯ä¾èµ–åˆæˆæ•°æ®å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>BD-Diffæ¨¡å‹é€šè¿‡è”åˆè®­ç»ƒä¸‰ä¸ªç‰¹åˆ«è®¾è®¡çš„ä»»åŠ¡ï¼Œæ—¨åœ¨å¢å¼ºåœ¨æœªçŸ¥åŸŸä¸Šçš„å»æ¨¡ç³Šæ€§èƒ½ã€‚</li>
<li>BD-Diffæ¨¡å‹é‡‡ç”¨ä¸¤ä¸ªQ-Formersåˆ†åˆ«æå–ç»“æ„ç‰¹å¾å’Œæ¨¡ç³Šæ¨¡å¼ã€‚</li>
<li>BD-Diffåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°ä¼˜è¶Šçš„å»æ¨¡ç³Šæ•ˆæœï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>BD-Diffæ¨¡å‹å…·æœ‰æ›´å¥½çš„ç»“æ„ä¿ç•™æ•ˆæœï¼Œåœ¨å„ç§æŒ‘æˆ˜åœºæ™¯ä¸‹è¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-742747dd6cda7b068eb5a5a1aa629672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04335becfcab3b9663a4679620a54415.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c35380521116a3063a6d6830e56ff246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f52c11ea436f2676ad1518ef71c9a7e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84df65be90783c1a02467fc3745dff7e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Improved-Training-Technique-for-Latent-Consistency-Models"><a href="#Improved-Training-Technique-for-Latent-Consistency-Models" class="headerlink" title="Improved Training Technique for Latent Consistency Models"></a>Improved Training Technique for Latent Consistency Models</h2><p><strong>Authors:Quan Dao, Khanh Doan, Di Liu, Trung Le, Dimitris Metaxas</strong></p>
<p>Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-$c$ scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: <a target="_blank" rel="noopener" href="https://github.com/quandao10/sLCT/">https://github.com/quandao10/sLCT/</a> </p>
<blockquote>
<p>ä¸€è‡´æ€§æ¨¡å‹æ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ¨¡å‹å®¶æ—ï¼Œèƒ½å¤Ÿåœ¨å•æ­¥æˆ–å¤šæ­¥ä¸­ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚æœ€è¿‘ï¼Œä¸€è‡´æ€§æ¨¡å‹è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œåœ¨åƒç´ ç©ºé—´è¾¾åˆ°äº†ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚ç„¶è€Œï¼Œå°†ä¸€è‡´æ€§è®­ç»ƒæ‰©å±•åˆ°å¤§è§„æ¨¡æ•°æ®é›†çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå–å†³äºå…¶åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†åƒç´ å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´çš„ç»Ÿè®¡å·®å¼‚ï¼Œå‘ç°æ½œåœ¨æ•°æ®ç»å¸¸åŒ…å«é«˜åº¦å†²åŠ¨çš„å¼‚å¸¸å€¼ï¼Œè¿™ä¼šæ˜¾è‘—é™çº§æ½œåœ¨ç©ºé—´ä¸­çš„iCTæ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ç”¨CauchyæŸå¤±æ›¿æ¢äº†Pseudo-HuberæŸå¤±ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†å¼‚å¸¸å€¼çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ—©æœŸæ—¶é—´æ­¥é•¿ä¸­å¼•å…¥äº†æ‰©æ•£æŸå¤±ï¼Œå¹¶é‡‡ç”¨æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰è€¦åˆæ¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”ç¼©æ”¾-cè°ƒåº¦å™¨æ¥ç®¡ç†ç¨³å¥çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶åœ¨æ¶æ„ä¸­é‡‡ç”¨éç¼©æ”¾LayerNormï¼Œä»¥æ›´å¥½åœ°æ•è·ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯å¹¶å‡å°‘å¼‚å¸¸å€¼çš„å½±å“ã€‚</p>
<p>é€šè¿‡è¿™äº›ç­–ç•¥ï¼Œæˆ‘ä»¬æˆåŠŸè®­ç»ƒäº†èƒ½å¤Ÿåœ¨ä¸€æ­¥æˆ–ä¸¤æ­¥å†…ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬çš„æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼Œæ˜¾è‘—ç¼©å°äº†æ½œåœ¨ä¸€è‡´æ€§ä¸æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚ç›¸å…³å®ç°å·²åœ¨æ­¤å¤„å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/quandao10/sLCT/%E3%80%82">https://github.com/quandao10/sLCT/ã€‚</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01441v1">PDF</a> Accepted at ICLR2025</p>
<p><strong>æ‘˜è¦</strong><br>    ä¸€è‡´æ€§æ¨¡å‹æ˜¯æ–°ä¸€ä»£ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½ä¸€æ­¥æˆ–å¤šæ­¥ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚è¿‘æœŸï¼Œå®ƒåœ¨åƒç´ ç©ºé—´çš„è¡¨ç°å·²æ¥è¿‘æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä¸€è‡´æ€§æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´çš„æ€§èƒ½å†³å®šå…¶æˆåŠŸã€‚æœ¬ç ”ç©¶åˆ†æäº†åƒç´ å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´çš„ç»Ÿè®¡å·®å¼‚ï¼Œå‘ç°æ½œåœ¨æ•°æ®å¸¸å«æœ‰å†²åŠ¨å¼‚å¸¸å€¼ï¼Œä¼šæ˜¾è‘—å½±å“iCTåœ¨æ½œåœ¨ç©ºé—´çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç”¨CauchyæŸå¤±æ›¿ä»£Pseudo-HuberæŸå¤±ï¼Œæœ‰æ•ˆå‡è½»å¼‚å¸¸å€¼çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ—©æœŸæ—¶é—´æ­¥é•¿å¼•å…¥æ‰©æ•£æŸå¤±ï¼Œå¹¶é‡‡ç”¨æœ€ä¼˜ä¼ è¾“è€¦åˆè¿›ä¸€æ­¥å¢å¼ºæ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥è‡ªé€‚åº”ç¼©æ”¾-cè°ƒåº¦å™¨ç®¡ç†ç¨³å¥è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶åœ¨æ¶æ„ä¸­é‡‡ç”¨éç¼©æ”¾LayerNormï¼Œä»¥æ›´å¥½åœ°æ•æ‰ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯å¹¶å‡å°‘å¼‚å¸¸å€¼å½±å“ã€‚è¿™äº›ç­–ç•¥ä½¿æˆ‘ä»¬èƒ½è®­ç»ƒå‡ºèƒ½åœ¨ä¸€æ­¥æˆ–ä¸¤æ­¥å†…ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬çš„æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼Œæ˜¾è‘—ç¼©å°äº†å…¶ä¸æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¸€è‡´æ€§æ¨¡å‹æ˜¯ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬çš„æ–°æ¨¡å‹ï¼Œèƒ½åœ¨å•æ­¥æˆ–å¤šæ­¥ä¸­ç”Ÿæˆæ ·æœ¬ã€‚</li>
<li>åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼Œä¸€è‡´æ€§æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´çš„æ€§èƒ½å¯¹å…¶æˆåŠŸè‡³å…³é‡è¦ã€‚</li>
<li>æ½œåœ¨æ•°æ®åŒ…å«å†²åŠ¨å¼‚å¸¸å€¼ï¼Œè¿™äº›å¼‚å¸¸å€¼ä¼šæ˜¾è‘—å½±å“iCTçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨CauchyæŸå¤±æ›¿ä»£Pseudo-HuberæŸå¤±ä»¥å‡è½»å¼‚å¸¸å€¼çš„å½±å“ã€‚</li>
<li>åœ¨æ—©æœŸæ—¶é—´æ­¥é•¿å¼•å…¥æ‰©æ•£æŸå¤±å’Œé‡‡ç”¨æœ€ä¼˜ä¼ è¾“è€¦åˆå¢å¼ºæ€§èƒ½ã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”ç¼©æ”¾-cè°ƒåº¦å™¨ç®¡ç†ç¨³å¥è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>åœ¨æ¶æ„ä¸­ä½¿ç”¨éç¼©æ”¾LayerNormï¼Œä»¥æ•æ‰ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯å¹¶å‡å°‘å¼‚å¸¸å€¼å½±å“ã€‚è¿™äº›ç­–ç•¥å¸®åŠ©ç¼©å°äº†æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48a19ef7a2a2e3f49d8be79bd8aa186c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f171726621e8d3ada6e0bce8c5bf4338.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5beb9b320f3321fc861e60764fed4043.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Assessing-the-use-of-Diffusion-models-for-motion-artifact-correction-in-brain-MRI"><a href="#Assessing-the-use-of-Diffusion-models-for-motion-artifact-correction-in-brain-MRI" class="headerlink" title="Assessing the use of Diffusion models for motion artifact correction in   brain MRI"></a>Assessing the use of Diffusion models for motion artifact correction in   brain MRI</h2><p><strong>Authors:Paolo Angella, Vito Paolo Pastore, Matteo Santacesaria</strong></p>
<p>Magnetic Resonance Imaging generally requires long exposure times, while being sensitive to patient motion, resulting in artifacts in the acquired images, which may hinder their diagnostic relevance. Despite research efforts to decrease the acquisition time, and designing efficient acquisition sequences, motion artifacts are still a persistent problem, pushing toward the need for the development of automatic motion artifact correction techniques. Recently, diffusion models have been proposed as a solution for the task at hand. While diffusion models can produce high-quality reconstructions, they are also susceptible to hallucination, which poses risks in diagnostic applications. In this study, we critically evaluate the use of diffusion models for correcting motion artifacts in 2D brain MRI scans. Using a popular benchmark dataset, we compare a diffusion model-based approach with state-of-the-art methods consisting of Unets trained in a supervised fashion on motion-affected images to reconstruct ground truth motion-free images. Our findings reveal mixed results: diffusion models can produce accurate predictions or generate harmful hallucinations in this context, depending on data heterogeneity and the acquisition planes considered as input. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒé€šå¸¸éœ€è¦é•¿æ—¶é—´çš„æ›å…‰ï¼ŒåŒæ—¶å¯¹æ‚£è€…åŠ¨ä½œæ•æ„Ÿï¼Œå¯¼è‡´é‡‡é›†çš„å›¾åƒå‡ºç°ä¼ªå½±ï¼Œå¯èƒ½ä¼šé™ä½å…¶è¯Šæ–­ä»·å€¼ã€‚å°½ç®¡å·²ä»˜å‡ºåŠªåŠ›å‡å°‘é‡‡é›†æ—¶é—´å¹¶è®¾è®¡æœ‰æ•ˆçš„é‡‡é›†åºåˆ—ï¼Œä½†è¿åŠ¨ä¼ªå½±ä»æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„é—®é¢˜ï¼Œæ¨åŠ¨äº†å¯¹å¼€å‘è‡ªåŠ¨è¿åŠ¨ä¼ªå½±æ ¡æ­£æŠ€æœ¯çš„éœ€æ±‚ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹å·²è¢«æè®®ä½œä¸ºè§£å†³æ‰‹å¤´ä»»åŠ¡çš„ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹å¯ä»¥äº§ç”Ÿé«˜è´¨é‡çš„é‡å»ºï¼Œä½†å®ƒä»¬ä¹Ÿå®¹æ˜“å‡ºç°å¹»è§‰ï¼Œè¿™åœ¨è¯Šæ–­åº”ç”¨ä¸­æ„æˆé£é™©ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å…¨é¢è¯„ä¼°äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ ¡æ­£äºŒç»´è„‘éƒ¨MRIæ‰«æä¸­çš„è¿åŠ¨ä¼ªå½±çš„ä½¿ç”¨æƒ…å†µã€‚æˆ‘ä»¬ä½¿ç”¨æµè¡Œçš„å¤§å‹æ•°æ®é›†ï¼Œå°†åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„Unetsæ–¹æ³•è¿›è¡Œå¯¹æ¯”ï¼ŒUnetsæ–¹æ³•åœ¨æœ‰è¿åŠ¨å½±å“å›¾åƒçš„ç›‘ç£è®­ç»ƒä¸‹é‡å»ºå‡ºçœŸå®æ— è¿åŠ¨å›¾åƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå–œå¿§å‚åŠï¼šæ‰©æ•£æ¨¡å‹å¯ä»¥æ ¹æ®æ•°æ®å¼‚è´¨æ€§å’Œè¾“å…¥çš„é‡‡é›†å¹³é¢äº§ç”Ÿå‡†ç¡®çš„é¢„æµ‹æˆ–äº§ç”Ÿæœ‰å®³çš„å¹»è§‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01418v1">PDF</a> Accepted at IEEE International Symposium for Biomedical Imaging   (ISBI) 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨çº æ­£äºŒç»´è„‘æ ¸ç£å…±æŒ¯æ‰«æä¸­çš„è¿åŠ¨ä¼ªå½±æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å­˜åœ¨äº§ç”Ÿå¹»è§‰çš„é£é™©ã€‚æœ¬ç ”ç©¶é‡‡ç”¨åŸºå‡†æ•°æ®é›†å¯¹æ¯”æ‰©æ•£æ¨¡å‹ä¸å½“å‰å…ˆè¿›æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºæ··åˆç»“æœï¼Œå–å†³äºæ•°æ®å¼‚è´¨æ€§å’Œè¾“å…¥é‡‡é›†å¹³é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¯ç”¨äºçº æ­£MRIä¸­çš„è¿åŠ¨ä¼ªå½±ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆé«˜è´¨é‡é‡å»ºï¼Œä½†ä¹Ÿå­˜åœ¨äº§ç”Ÿå¹»è§‰çš„é£é™©ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨åŸºå‡†æ•°æ®é›†å¯¹æ‰©æ•£æ¨¡å‹ä¸å½“å‰å…ˆè¿›æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
<li>æ•°æ®å¼‚è´¨æ€§å’Œè¾“å…¥é‡‡é›†å¹³é¢ä¼šå½±å“æ‰©æ•£æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½äº§ç”Ÿå‡†ç¡®é¢„æµ‹ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹å¯èƒ½ç”Ÿæˆæœ‰å®³çš„å¹»è§‰ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›æ‰©æ•£æ¨¡å‹ï¼Œä»¥æé«˜å…¶åœ¨MRIè¿åŠ¨ä¼ªå½±çº æ­£ä¸­çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1c0ff2db24ef0b3d27b3a157481e6bf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3bc7df182465c9abeb71c16ac4d4dca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6e4a2022804a3d3286943ebf6437e7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e19a444759c1f0bbc204de4ce2fcc7ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d60df56725baa625acfb04fb81fdfea4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef540aa18af7f90553241da98e76b27b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Human-Body-Restoration-with-One-Step-Diffusion-Model-and-A-New-Benchmark"><a href="#Human-Body-Restoration-with-One-Step-Diffusion-Model-and-A-New-Benchmark" class="headerlink" title="Human Body Restoration with One-Step Diffusion Model and A New Benchmark"></a>Human Body Restoration with One-Step Diffusion Model and A New Benchmark</h2><p><strong>Authors:Jue Gong, Jingkai Wang, Zheng Chen, Xing Liu, Hong Gu, Yulun Zhang, Xiaokang Yang</strong></p>
<p>Human body restoration, as a specific application of image restoration, is widely applied in practice and plays a vital role across diverse fields. However, thorough research remains difficult, particularly due to the lack of benchmark datasets. In this study, we propose a high-quality dataset automated cropping and filtering (HQ-ACF) pipeline. This pipeline leverages existing object detection datasets and other unlabeled images to automatically crop and filter high-quality human images. Using this pipeline, we constructed a person-based restoration with sophisticated objects and natural activities (\emph{PERSONA}) dataset, which includes training, validation, and test sets. The dataset significantly surpasses other human-related datasets in both quality and content richness. Finally, we propose \emph{OSDHuman}, a novel one-step diffusion model for human body restoration. Specifically, we propose a high-fidelity image embedder (HFIE) as the prompt generator to better guide the model with low-quality human image information, effectively avoiding misleading prompts. Experimental results show that OSDHuman outperforms existing methods in both visual quality and quantitative metrics. The dataset and code will at <a target="_blank" rel="noopener" href="https://github.com/gobunu/OSDHuman">https://github.com/gobunu/OSDHuman</a>. </p>
<blockquote>
<p>äººä½“ä¿®å¤ä½œä¸ºå›¾åƒä¿®å¤çš„ä¸€ç§ç‰¹å®šåº”ç”¨ï¼Œåœ¨å®è·µä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œå¹¶åœ¨å„ä¸ªé¢†åŸŸå‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºå…¶éš¾åº¦è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯ç¼ºä¹åŸºå‡†æ•°æ®é›†ï¼Œç›¸å…³ç ”ç©¶ä»ç„¶å›°éš¾é‡é‡ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜è´¨é‡æ•°æ®é›†è‡ªåŠ¨è£å‰ªå’Œè¿‡æ»¤ï¼ˆHQ-ACFï¼‰ç®¡é“ã€‚è¯¥ç®¡é“åˆ©ç”¨ç°æœ‰çš„ç›®æ ‡æ£€æµ‹æ•°æ®é›†å’Œå…¶ä»–æœªæ ‡è®°çš„å›¾åƒï¼Œè‡ªåŠ¨è£å‰ªå’Œè¿‡æ»¤é«˜è´¨é‡çš„äººä½“å›¾åƒã€‚ä½¿ç”¨è¯¥ç®¡é“ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºäºäººçš„ä¿®å¤ä¸å¤æ‚ç›®æ ‡å’Œè‡ªç„¶æ´»åŠ¨ï¼ˆPERSONAï¼‰æ•°æ®é›†ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚è¯¥æ•°æ®é›†åœ¨è´¨é‡å’Œå†…å®¹ä¸°å¯Œç¨‹åº¦ä¸Šéƒ½å¤§å¤§è¶…è¿‡äº†å…¶ä»–ä¸äººä½“ç›¸å…³çš„æ•°æ®é›†ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºäººä½“ä¿®å¤çš„æ–°å‹ä¸€æ­¥æ‰©æ•£æ¨¡å‹OSDHumanã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜ä¿çœŸå›¾åƒåµŒå…¥å™¨ï¼ˆHFIEï¼‰ä½œä¸ºæç¤ºç”Ÿæˆå™¨ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨ä½è´¨é‡äººä½“å›¾åƒä¿¡æ¯æ¥æŒ‡å¯¼æ¨¡å‹ï¼Œæœ‰æ•ˆé¿å…è¯¯å¯¼æ€§æç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSDHumanåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/gobunu/OSDHuman%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/gobunu/OSDHumanå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01411v1">PDF</a> 8 pages, 9 figures. The code and model will be available at   <a target="_blank" rel="noopener" href="https://github.com/gobunu/OSDHuman">https://github.com/gobunu/OSDHuman</a></p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶é’ˆå¯¹äººä½“æ¢å¤å›¾åƒä¿®å¤çš„ä¸€ä¸ªå…·ä½“åº”ç”¨ï¼Œæå‡ºä¸€ä¸ªé«˜è´¨é‡æ•°æ®é›†è‡ªåŠ¨åŒ–è£å‰ªå’Œè¿‡æ»¤ï¼ˆHQ-ACFï¼‰ç®¡é“ï¼Œåˆ©ç”¨ç°æœ‰ç›®æ ‡æ£€æµ‹æ•°æ®é›†å’Œå…¶ä»–æœªæ ‡è®°å›¾åƒè‡ªåŠ¨è£å‰ªå’Œè¿‡æ»¤é«˜è´¨é‡çš„äººä½“å›¾åƒã€‚åŸºäºæ­¤ç®¡é“ï¼Œæ„å»ºäº†åŒ…å«è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†çš„äººå‘˜æ¢å¤ä¸å¤æ‚ç‰©ä½“å’Œè‡ªç„¶æ´»åŠ¨ï¼ˆPERSONAï¼‰æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹ä¸€æ­¥æ‰©æ•£æ¨¡å‹OSDHumanç”¨äºäººä½“æ¢å¤ï¼Œå¹¶æå‡ºé«˜ä¿çœŸå›¾åƒåµŒå…¥å™¨ï¼ˆHFIEï¼‰ä½œä¸ºæç¤ºç”Ÿæˆå™¨ï¼Œæ›´å¥½åœ°å¼•å¯¼æ¨¡å‹åˆ©ç”¨ä½è´¨é‡äººä½“å›¾åƒä¿¡æ¯ï¼Œé¿å…è¯¯å¯¼æç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSDHumanåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººä½“æ¢å¤ä½œä¸ºå›¾åƒä¿®å¤çš„ä¸€ä¸ªç‰¹å®šåº”ç”¨ï¼Œåœ¨å®è·µä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œå¹¶åœ¨å¤šä¸ªé¢†åŸŸä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚</li>
<li>ç¼ºä¹åŸºå‡†æ•°æ®é›†ä½¿æ·±å…¥ç ”ç©¶å˜å¾—å›°éš¾ã€‚</li>
<li>æå‡ºä¸€ä¸ªé«˜è´¨é‡æ•°æ®é›†è‡ªåŠ¨åŒ–è£å‰ªå’Œè¿‡æ»¤ï¼ˆHQ-ACFï¼‰ç®¡é“ï¼Œç”¨äºè‡ªåŠ¨è£å‰ªå’Œè¿‡æ»¤é«˜è´¨é‡çš„äººä½“å›¾åƒã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåŒ…æ‹¬è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†çš„äººå‘˜æ¢å¤ä¸å¤æ‚ç‰©ä½“å’Œè‡ªç„¶æ´»åŠ¨ï¼ˆPERSONAï¼‰æ•°æ®é›†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ä¸€æ­¥æ‰©æ•£æ¨¡å‹OSDHumanç”¨äºäººä½“æ¢å¤ã€‚</li>
<li>é«˜ä¿çœŸå›¾åƒåµŒå…¥å™¨ï¼ˆHFIEï¼‰ä½œä¸ºæç¤ºç”Ÿæˆå™¨ï¼Œæ›´å¥½åœ°å¼•å¯¼æ¨¡å‹åˆ©ç”¨ä½è´¨é‡äººä½“å›¾åƒä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cbc60712e9585aea63b5adc93497ae47.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0639a76eae63a99b28103fa40400d6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11c1aefa331e77938629aa84b2e80d29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4abf9396d3335ac760e1a6862d74653c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c31eeb926795cf8d47598d15e2d63509.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90de7fda2c8f3c4a0ae9a1015cee091f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Heterogeneous-Image-GNN-Graph-Conditioned-Diffusion-for-Image-Synthesis"><a href="#Heterogeneous-Image-GNN-Graph-Conditioned-Diffusion-for-Image-Synthesis" class="headerlink" title="Heterogeneous Image GNN: Graph-Conditioned Diffusion for Image Synthesis"></a>Heterogeneous Image GNN: Graph-Conditioned Diffusion for Image Synthesis</h2><p><strong>Authors:Rupert Menneer, Christos Margadji, Sebastian W. Pattinson</strong></p>
<p>We introduce a novel method for conditioning diffusion-based image synthesis models with heterogeneous graph data. Existing approaches typically incorporate conditioning variables directly into model architectures, either through cross-attention layers that attend to text latents or image concatenation that spatially restrict generation. However, these methods struggle to handle complex scenarios involving diverse, relational conditioning variables, which are more naturally represented as unstructured graphs. This paper presents Heterogeneous Image Graphs (HIG), a novel representation that models conditioning variables and target images as two interconnected graphs, enabling efficient handling of variable-length conditioning inputs and their relationships. We also propose a magnitude-preserving GNN that integrates the HIG into the existing EDM2 diffusion model using a ControlNet approach. Our approach improves upon the SOTA on a variety of conditioning inputs for the COCO-stuff and Visual Genome datasets, and showcases the ability to condition on graph attributes and relationships represented by edges in the HIG. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºå¼‚è´¨å›¾æ•°æ®å¯¹æ‰©æ•£å›¾åƒåˆæˆæ¨¡å‹è¿›è¡Œæ¡ä»¶æ§åˆ¶çš„æ–°æ–¹æ³•ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥å°†æ¡ä»¶å˜é‡çº³å…¥æ¨¡å‹æ¶æ„ä¸­ï¼Œæ— è®ºæ˜¯é€šè¿‡å…³æ³¨æ–‡æœ¬æ½œåœ¨ç‰¹å¾çš„äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œè¿˜æ˜¯é€šè¿‡ç©ºé—´é™åˆ¶ç”Ÿæˆçš„å›¾åƒæ‹¼æ¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†æ¶‰åŠå¤šæ ·åŒ–ã€å…³ç³»å‹æ¡ä»¶å˜é‡çš„å¤æ‚åœºæ™¯æ—¶é¢ä¸´å›°éš¾ï¼Œè¿™äº›åœºæ™¯æ›´è‡ªç„¶åœ°è¡¨ç°ä¸ºæ— ç»“æ„çš„å›¾å½¢ã€‚æœ¬æ–‡æå‡ºäº†å¼‚è´¨å›¾åƒå›¾ï¼ˆHIGï¼‰è¿™ä¸€æ–°è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒå°†æ¡ä»¶å˜é‡å’Œç›®æ ‡å›¾åƒå»ºæ¨¡ä¸ºä¸¤ä¸ªç›¸äº’è¿æ¥çš„å›¾ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¯å˜é•¿åº¦çš„æ¡ä»¶è¾“å…¥åŠå…¶å…³ç³»ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¹…åº¦ä¿æŒå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œé‡‡ç”¨ControlNetæ–¹æ³•å°†HIGé›†æˆåˆ°ç°æœ‰çš„EDM2æ‰©æ•£æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨COCO-stuffå’ŒVisual Genomeæ•°æ®é›†çš„å„ç§æ¡ä»¶è¾“å…¥ä¸Šæ”¹è¿›äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶å±•ç¤ºäº†åœ¨HIGä¸­æ ¹æ®å›¾å½¢å±æ€§å’Œè¾¹ç¼˜è¡¨ç¤ºçš„å…³ç³»è¿›è¡Œæ¡ä»¶è®¾ç½®çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01309v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å°†æ‰©æ•£å›¾åƒåˆæˆæ¨¡å‹ä¸å¼‚è´¨å›¾æ•°æ®è¿›è¡Œæ¡ä»¶åŒ–ç»“åˆçš„æ–°æ–¹æ³•ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥å°†æ¡ä»¶å˜é‡çº³å…¥æ¨¡å‹æ¶æ„ä¸­ï¼Œé€šè¿‡å…³æ³¨æ–‡æœ¬æ½œåœ¨å˜é‡æˆ–å›¾åƒæ‹¼æ¥æ¥é™åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†æ¶‰åŠå¤šæ ·åŒ–å’Œå…³ç³»æ€§æ¡ä»¶å˜é‡çš„å¤æ‚åœºæ™¯æ—¶é‡åˆ°å›°éš¾ï¼Œè¿™äº›åœºæ™¯æ›´é€‚åˆç”¨æ— ç»“æ„çš„å›¾æ¥è¡¨ç¤ºã€‚æœ¬æ–‡æå‡ºäº†å¼‚è´¨å›¾åƒå›¾ï¼ˆHIGï¼‰è¿™ä¸€æ–°è¡¨ç¤ºæ³•ï¼Œå°†æ¡ä»¶å˜é‡å’Œç›®æ ‡å›¾åƒå»ºæ¨¡ä¸ºä¸¤ä¸ªç›¸äº’è¿æ¥çš„å›¾ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¯å˜é•¿åº¦çš„æ¡ä»¶è¾“å…¥åŠå…¶å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¹…åº¦ä¿æŒå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œé‡‡ç”¨ControlNetæ–¹æ³•å°†HIGé›†æˆåˆ°ç°æœ‰çš„EDM2æ‰©æ•£æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨COCO-stuffå’ŒVisual Genomeæ•°æ®é›†ä¸Šçš„å¤šç§æ¡ä»¶è¾“å…¥ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€ä½³æ°´å¹³ï¼Œå¹¶å±•ç¤ºäº†åœ¨HIGä¸­ç”±è¾¹ç¼˜è¡¨ç¤ºçš„å›¾å½¢å±æ€§å’Œå…³ç³»è¿›è¡Œæ¡ä»¶è®¾ç½®çš„èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨å¼‚è´¨å›¾åƒå›¾ï¼ˆHIGï¼‰å¯¹æ‰©æ•£å›¾åƒåˆæˆæ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»¥å¤„ç†å¤æ‚å’Œå¤šæ ·åŒ–çš„æ¡ä»¶å˜é‡ã€‚</li>
<li>HIGå»ºæ¨¡å°†æ¡ä»¶å˜é‡å’Œç›®æ ‡å›¾åƒè¡¨ç¤ºä¸ºç›¸äº’è¿æ¥çš„å›¾ï¼Œèƒ½å¤Ÿå¤„ç†å¯å˜é•¿åº¦çš„æ¡ä»¶è¾“å…¥åŠå…¶å…³ç³»ã€‚</li>
<li>å¹…åº¦ä¿æŒå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰è¢«ç”¨æ¥é›†æˆHIGåˆ°ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€ä½³æ°´å¹³ï¼ŒåŒ…æ‹¬COCO-stuffå’ŒVisual Genomeã€‚</li>
<li>å±•ç¤ºäº†åœ¨HIGä¸­åˆ©ç”¨å›¾å½¢å±æ€§å’Œå…³ç³»è¿›è¡Œæ¡ä»¶è®¾ç½®çš„èƒ½åŠ›ï¼Œè¿™æ˜¯ä»¥å‰æ–¹æ³•éš¾ä»¥å¤„ç†çš„ã€‚</li>
<li>è¿™ç§æ–°æ–¹æ³•ä¸ºå¤„ç†æ¶‰åŠå¤æ‚å’Œå¤šæ ·åŒ–æ¡ä»¶å˜é‡çš„å›¾åƒç”Ÿæˆä»»åŠ¡æä¾›äº†æ–°çš„è§†è§’å’Œå·¥å…·ã€‚</li>
<li>é€šè¿‡æ•´åˆå¼‚è´¨å›¾æ•°æ®å’Œæ‰©æ•£æ¨¡å‹ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆçš„çµæ´»æ€§å’Œè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01309">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-659f37e4c5160b191072b05238c237e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-783d0f4774238c2d43627d7da44c3e90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87572860c05c11e2c9b5541d4147214f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Compressed-Image-Generation-with-Denoising-Diffusion-Codebook-Models"><a href="#Compressed-Image-Generation-with-Denoising-Diffusion-Codebook-Models" class="headerlink" title="Compressed Image Generation with Denoising Diffusion Codebook Models"></a>Compressed Image Generation with Denoising Diffusion Codebook Models</h2><p><strong>Authors:Guy Ohayon, Hila Manor, Tomer Michaeli, Michael Elad</strong></p>
<p>We present a novel generative approach based on Denoising Diffusion Models (DDMs), which produces high-quality image samples along with their losslessly compressed bit-stream representations. This is obtained by replacing the standard Gaussian noise sampling in the reverse diffusion with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors. Surprisingly, we find that our method, termed Denoising Diffusion Codebook Model (DDCM), retains sample quality and diversity of standard DDMs, even for extremely small codebooks. We leverage DDCM and pick the noises from the codebooks that best match a given image, converting our generative model into a highly effective lossy image codec achieving state-of-the-art perceptual image compression results. More generally, by setting other noise selections rules, we extend our compression method to any conditional image generation task (e.g., image restoration), where the generated images are produced jointly with their condensed bit-stream representations. Our work is accompanied by a mathematical interpretation of the proposed compressed conditional generation schemes, establishing a connection with score-based approximations of posterior samplers for the tasks considered. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMsï¼‰çš„æ–°å‹ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒæ ·æœ¬åŠå…¶æ— æŸå‹ç¼©çš„ä½æµè¡¨ç¤ºå½¢å¼ã€‚è¿™æ˜¯é€šè¿‡å°†åå‘æ‰©æ•£ä¸­çš„æ ‡å‡†é«˜æ–¯å™ªå£°é‡‡æ ·æ›¿æ¢ä¸ºä»é¢„å®šä¹‰çš„å›ºå®šç‹¬ç«‹åŒåˆ†å¸ƒé«˜æ–¯å‘é‡çš„ä»£ç åº“ä¸­é€‰æ‹©çš„å™ªå£°æ ·æœ¬è€Œè·å¾—çš„ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå³å»å™ªæ‰©æ•£ä»£ç æœ¬æ¨¡å‹ï¼ˆDDCMï¼‰ï¼Œå³ä½¿åœ¨æå°çš„ä»£ç æœ¬ä¸­ï¼Œä¹Ÿèƒ½ä¿æŒæ ‡å‡†DDMçš„æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨DDCMï¼Œä»ä»£ç åº“ä¸­æŒ‘é€‰ä¸ç»™å®šå›¾åƒæœ€åŒ¹é…çš„å™ªå£°ï¼Œå°†æˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºä¸€ç§é«˜æ•ˆçš„æœ‰æŸå›¾åƒç¼–ç ï¼Œå®ç°äº†å…ˆè¿›çš„æ„ŸçŸ¥å›¾åƒå‹ç¼©ç»“æœã€‚æ›´ä¸€èˆ¬åœ°è¯´ï¼Œé€šè¿‡è®¾ç½®å…¶ä»–å™ªå£°é€‰æ‹©è§„åˆ™ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„å‹ç¼©æ–¹æ³•æ‰©å±•åˆ°ä»»ä½•æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒä¿®å¤ï¼‰ï¼Œå…¶ä¸­ç”Ÿæˆçš„å›¾åƒä¸å…¶å‹ç¼©çš„ä½æµè¡¨ç¤ºå½¢å¼è”åˆç”Ÿæˆã€‚æˆ‘ä»¬çš„å·¥ä½œè¿˜æä¾›äº†å¯¹æ‰€æå‡ºçš„å‹ç¼©æ¡ä»¶ç”Ÿæˆæ–¹æ¡ˆçš„æ•°å­¦è§£é‡Šï¼Œä¸æ‰€è€ƒè™‘ä»»åŠ¡çš„åŸºäºåˆ†æ•°çš„åé‡‡æ ·å™¨çš„è¿‘ä¼¼å»ºç«‹äº†è”ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01189v2">PDF</a> Code and demo are available at <a target="_blank" rel="noopener" href="https://ddcm-2025.github.io/">https://ddcm-2025.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ ·æœ¬åŠå…¶æ— æŸå‹ç¼©æ¯”ç‰¹æµè¡¨ç¤ºã€‚é€šè¿‡ç”¨é¢„å®šä¹‰çš„å›ºå®šç‹¬ç«‹åŒåˆ†å¸ƒé«˜æ–¯å‘é‡çš„ä»£ç æœ¬ä¸­çš„å™ªå£°æ ·æœ¬æ›¿æ¢åå‘æ‰©æ•£ä¸­çš„æ ‡å‡†é«˜æ–¯å™ªå£°æ ·æœ¬ï¼Œæˆ‘ä»¬å®ç°äº†ç§°ä¸ºå»å™ªæ‰©æ•£ä»£ç æœ¬æ¨¡å‹ï¼ˆDDCMï¼‰çš„æ–¹æ³•ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿å¯¹äºæå°çš„ä»£ç æœ¬ï¼ŒDDCMä¹Ÿèƒ½ä¿æŒæ ‡å‡†DDMçš„æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨DDCMï¼Œä»ä»£ç æœ¬ä¸­é€‰æ‹©ä¸ç»™å®šå›¾åƒæœ€ä½³åŒ¹é…çš„å™ªå£°ï¼Œå°†æˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºä¸€ç§é«˜æ•ˆçš„æœ‰æŸå›¾åƒç¼–ç ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ„ŸçŸ¥å›¾åƒå‹ç¼©ç»“æœã€‚æ›´ä¸€èˆ¬åœ°è¯´ï¼Œé€šè¿‡è®¾ç½®å…¶ä»–å™ªå£°é€‰æ‹©è§„åˆ™ï¼Œæˆ‘ä»¬å°†å‹ç¼©æ–¹æ³•æ‰©å±•åˆ°ä»»ä½•æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒæ¢å¤ï¼‰ï¼Œå…¶ä¸­ç”Ÿæˆçš„å›¾åƒä¸å…¶æµ“ç¼©çš„æ¯”ç‰¹æµè¡¨ç¤ºä¸€èµ·äº§ç”Ÿã€‚æˆ‘ä»¬çš„å·¥ä½œè¿˜æä¾›äº†å¯¹æ‰€æå‡ºçš„å‹ç¼©æ¡ä»¶ç”Ÿæˆæ–¹æ¡ˆè¿›è¡Œæ•°å­¦è§£é‡Šï¼Œä¸æ‰€è€ƒè™‘ä»»åŠ¡çš„åŸºäºåˆ†æ•°çš„åéªŒé‡‡æ ·å™¨å»ºç«‹äº†è”ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºDenoising Diffusion Modelsï¼ˆDDMï¼‰æå‡ºä¸€ç§æ–°çš„ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒæ ·æœ¬åŠå…¶æ— æŸå‹ç¼©è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥Denoising Diffusion Codebook Modelï¼ˆDDCMï¼‰ï¼Œåœ¨æå°çš„ä»£ç æœ¬ä¸‹ä»èƒ½ä¿æŒæ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
<li>åˆ©ç”¨DDCMå°†ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºé«˜æ•ˆçš„æœ‰æŸå›¾åƒç¼–ç ï¼Œå®ç°å…ˆè¿›æ„ŸçŸ¥å›¾åƒå‹ç¼©ç»“æœã€‚</li>
<li>å‹ç¼©æ–¹æ³•å¯æ‰©å±•åˆ°ä»»ä½•æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¦‚å›¾åƒæ¢å¤ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•å…·æœ‰æ•°å­¦è§£é‡Šï¼Œä¸åŸºäºåˆ†æ•°çš„åéªŒé‡‡æ ·å™¨å»ºç«‹è”ç³»ã€‚</li>
<li>DDCMé€šè¿‡é€‰æ‹©æœ€ä½³åŒ¹é…ç»™å®šå›¾åƒçš„å™ªå£°ï¼Œå®ç°äº†å›¾åƒç”Ÿæˆä¸å‹ç¼©çš„è”åˆè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77f33794600b21a079e9234cbf259269.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b156377b6710535d8011d6b79b01dc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-519e9ecc2b78356a426d316212a5f22e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-828aa2a9e51d182813a6e49ccf80dc1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d44c768fd5d5949b00e7a5c9a9d3cbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c1a3c539c371e9d848e26186f5a4ca.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-as-a-Noise-Aware-Latent-Reward-Model-for-Step-Level-Preference-Optimization"><a href="#Diffusion-Model-as-a-Noise-Aware-Latent-Reward-Model-for-Step-Level-Preference-Optimization" class="headerlink" title="Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization"></a>Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization</h2><p><strong>Authors:Tao Zhang, Cheng Da, Kun Ding, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, Chunhong Pan</strong></p>
<p>Preference optimization for diffusion models aims to align them with human preferences for images. Previous methods typically leverage Vision-Language Models (VLMs) as pixel-level reward models to approximate human preferences. However, when used for step-level preference optimization, these models face challenges in handling noisy images of different timesteps and require complex transformations into pixel space. In this work, we demonstrate that diffusion models are inherently well-suited for step-level reward modeling in the latent space, as they can naturally extract features from noisy latent images. Accordingly, we propose the Latent Reward Model (LRM), which repurposes components of diffusion models to predict preferences of latent images at various timesteps. Building on LRM, we introduce Latent Preference Optimization (LPO), a method designed for step-level preference optimization directly in the latent space. Experimental results indicate that LPO not only significantly enhances performance in aligning diffusion models with general, aesthetic, and text-image alignment preferences, but also achieves 2.5-28$\times$ training speedup compared to existing preference optimization methods. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/casiatao/LPO">https://github.com/casiatao/LPO</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„åå¥½ä¼˜åŒ–æ—¨åœ¨ä½¿å›¾åƒä¸äººç±»åå¥½ç›¸ä¸€è‡´ã€‚ä¹‹å‰çš„æ–¹æ³•é€šå¸¸åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºåƒç´ çº§å¥–åŠ±æ¨¡å‹æ¥è¿‘ä¼¼äººç±»åå¥½ã€‚ç„¶è€Œï¼Œå½“ç”¨äºæ­¥éª¤çº§åå¥½ä¼˜åŒ–æ—¶ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†ä¸åŒæ—¶é—´æ­¥é•¿çš„å™ªå£°å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶éœ€è¦å°†å¤æ‚è½¬æ¢åº”ç”¨äºåƒç´ ç©ºé—´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰©æ•£æ¨¡å‹å¤©ç„¶é€‚åˆåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ­¥éª¤çº§å¥–åŠ±å»ºæ¨¡ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä»å™ªå£°æ½œåœ¨å›¾åƒä¸­è‡ªç„¶æå–ç‰¹å¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨å¥–åŠ±æ¨¡å‹ï¼ˆLRMï¼‰ï¼Œè¯¥æ¨¡å‹é‡æ–°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç»„ä»¶æ¥é¢„æµ‹ä¸åŒæ—¶é—´æ­¥é•¿çš„æ½œåœ¨å›¾åƒçš„åå¥½ã€‚åŸºäºLRMï¼Œæˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºæ½œåœ¨ç©ºé—´ä¸­çš„æ­¥éª¤çº§åå¥½ä¼˜åŒ–è®¾è®¡çš„ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLPOä¸ä»…æ˜¾è‘—æé«˜äº†æ‰©æ•£æ¨¡å‹ä¸ä¸€èˆ¬åå¥½ã€ç¾å­¦åå¥½å’Œæ–‡æœ¬å›¾åƒå¯¹é½åå¥½çš„å¯¹é½æ€§èƒ½ï¼Œè€Œä¸”ä¸ç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”å®ç°äº†2.5-28å€çš„è®­ç»ƒé€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/casiatao/LPO%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/casiatao/LPOä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01051v1">PDF</a> 20 pages, 14 tables, 15 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºLatent Reward Modelï¼ˆLRMï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å›ºæœ‰ç‰¹æ€§åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ­¥éª¤çº§åˆ«çš„å¥–åŠ±å»ºæ¨¡ï¼Œä»è€Œé¢„æµ‹ä¸åŒæ—¶é—´æ­¥é•¿æ½œåœ¨å›¾åƒçš„åå¥½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å¼•å…¥äº†Latent Preference Optimizationï¼ˆLPOï¼‰æ–¹æ³•ï¼Œç›´æ¥åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ­¥éª¤çº§åˆ«çš„åå¥½ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLPOä¸ä»…æ˜¾è‘—æé«˜äº†æ‰©æ•£æ¨¡å‹ä¸é€šç”¨ã€ç¾å­¦å’Œæ–‡æœ¬-å›¾åƒå¯¹é½åå¥½çš„å¯¹é½æ€§èƒ½ï¼Œè€Œä¸”ä¸ç°æœ‰åå¥½ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†2.5-28å€çš„è®­ç»ƒé€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ­¥éª¤çº§åˆ«çš„å¥–åŠ±å»ºæ¨¡å…·æœ‰å¤©ç„¶ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæå–å™ªå£°æ½œåœ¨å›¾åƒçš„ç‰¹å¾ã€‚</li>
<li>æå‡ºLatent Reward Modelï¼ˆLRMï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç»„ä»¶é¢„æµ‹ä¸åŒæ—¶é—´æ­¥é•¿æ½œåœ¨å›¾åƒçš„åå¥½ã€‚</li>
<li>åŸºäºLRMï¼Œå¼•å…¥Latent Preference Optimizationï¼ˆLPOï¼‰æ–¹æ³•ï¼Œç›´æ¥åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ­¥éª¤çº§åˆ«çš„åå¥½ä¼˜åŒ–ã€‚</li>
<li>LPOä¸ä»…èƒ½æ˜¾è‘—æé«˜æ‰©æ•£æ¨¡å‹ä¸å„ç§åå¥½çš„å¯¹é½æ€§èƒ½ï¼Œè€Œä¸”å®ç°äº†æ˜¾è‘—çš„è®­ç»ƒé€Ÿåº¦æå‡ã€‚</li>
<li>LPOæ–¹æ³•é€‚ç”¨äºé€šç”¨ã€ç¾å­¦å’Œæ–‡æœ¬-å›¾åƒå¯¹é½ç­‰å¤šç§åå¥½åœºæ™¯ã€‚</li>
<li>è¯¥ç ”ç©¶è§£å†³äº†ä½¿ç”¨Vision-Language Modelsï¼ˆVLMsï¼‰åœ¨å¤„ç†ä¸åŒæ—¶é—´æ­¥é•¿å™ªå£°å›¾åƒæ—¶çš„å¤æ‚è½¬æ¢é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-93882ab0ff0d6f68f2f1619e95f0d00c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee307810b1e611e60b713b096e7a2935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6088490955f4353a0574dad9b3a16909.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bbb79a5459f145937d74958321af3cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41404ecbf428f8fca083cd14d3f9d6a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4c1bf4f272373ac7db9ffda5d9946a9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Diffusion-Transformer-Captures-Spatial-Temporal-Dependencies-A-Theory-for-Gaussian-Process-Data"><a href="#Diffusion-Transformer-Captures-Spatial-Temporal-Dependencies-A-Theory-for-Gaussian-Process-Data" class="headerlink" title="Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory   for Gaussian Process Data"></a>Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory   for Gaussian Process Data</h2><p><strong>Authors:Hengyu Fu, Zehao Dou, Jiawei Guo, Mengdi Wang, Minshuo Chen</strong></p>
<p>Diffusion Transformer, the backbone of Sora for video generation, successfully scales the capacity of diffusion models, pioneering new avenues for high-fidelity sequential data generation. Unlike static data such as images, sequential data consists of consecutive data frames indexed by time, exhibiting rich spatial and temporal dependencies. These dependencies represent the underlying dynamic model and are critical to validate the generated data. In this paper, we make the first theoretical step towards bridging diffusion transformers for capturing spatial-temporal dependencies. Specifically, we establish score approximation and distribution estimation guarantees of diffusion transformers for learning Gaussian process data with covariance functions of various decay patterns. We highlight how the spatial-temporal dependencies are captured and affect learning efficiency. Our study proposes a novel transformer approximation theory, where the transformer acts to unroll an algorithm. We support our theoretical results by numerical experiments, providing strong evidence that spatial-temporal dependencies are captured within attention layers, aligning with our approximation theory. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒâ€”â€”Diffusion Transformerï¼Œä¸ºè§†é¢‘ç”Ÿæˆæä¾›äº†å¼ºå¤§çš„æ”¯æ’‘ï¼ŒæˆåŠŸæ‰©å¤§äº†æ‰©æ•£æ¨¡å‹çš„å®¹é‡ï¼Œä¸ºé«˜è´¨é‡åºåˆ—æ•°æ®ç”Ÿæˆå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚ä¸åŒäºé™æ€æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰ï¼Œåºåˆ—æ•°æ®ç”±æ—¶é—´ç´¢å¼•çš„è¿ç»­æ•°æ®å¸§ç»„æˆï¼Œå…·æœ‰ä¸°å¯Œçš„ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§ã€‚è¿™äº›ä¾èµ–å…³ç³»ä»£è¡¨äº†åŸºç¡€åŠ¨æ€æ¨¡å‹ï¼Œå¯¹éªŒè¯ç”Ÿæˆçš„æ•°æ®è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æœç€å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºæ•æ‰æ—¶ç©ºä¾èµ–æ€§çš„æ–¹å‘è¿ˆå‡ºäº†ç†è®ºä¸Šçš„ç¬¬ä¸€æ­¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºå­¦ä¹ å…·æœ‰å„ç§è¡°å‡æ¨¡å¼çš„åæ–¹å·®å‡½æ•°çš„é«˜æ–¯è¿‡ç¨‹æ•°æ®å»ºç«‹æ‰©æ•£æ¨¡å‹çš„å¾—åˆ†è¿‘ä¼¼å’Œåˆ†å¸ƒä¼°è®¡ä¿è¯ã€‚æˆ‘ä»¬å¼ºè°ƒäº†æ—¶ç©ºä¾èµ–æ€§æ˜¯å¦‚ä½•è¢«æ•è·å¹¶å½±å“å­¦ä¹ æ•ˆç‡çš„ã€‚æˆ‘ä»¬çš„ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è½¬æ¢å™¨è¿‘ä¼¼ç†è®ºï¼Œå…¶ä¸­è½¬æ¢å™¨èµ·åˆ°å±•å¼€ç®—æ³•çš„ä½œç”¨ã€‚æˆ‘ä»¬é€šè¿‡æ•°å€¼å®éªŒæ”¯æŒæˆ‘ä»¬çš„ç†è®ºç»“æœï¼Œæä¾›å¼ºæœ‰åŠ›çš„è¯æ®è¡¨æ˜æ—¶ç©ºä¾èµ–æ€§è¢«æ³¨æ„åŠ›å±‚æ•è·ï¼Œè¿™ä¸æˆ‘ä»¬çš„è¿‘ä¼¼ç†è®ºç›¸ç¬¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16134v2">PDF</a> 56 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£Transformerä½œä¸ºSoraè§†é¢‘ç”Ÿæˆçš„æ ¸å¿ƒï¼ŒæˆåŠŸæ‰©å±•äº†æ‰©æ•£æ¨¡å‹çš„å®¹é‡ï¼Œä¸ºé«˜è´¨é‡åºåˆ—æ•°æ®ç”Ÿæˆå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚ä¸é™æ€æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰ä¸åŒï¼Œåºåˆ—æ•°æ®ç”±æ—¶é—´ç´¢å¼•çš„è¿ç»­æ•°æ®å¸§ç»„æˆï¼Œè¡¨ç°å‡ºä¸°å¯Œçš„ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§ã€‚æœ¬æ–‡é¦–æ¬¡ä»ç†è®ºä¸Šæ¢è®¨äº†æ‰©æ•£Transformeråœ¨æ•æ‰æ—¶ç©ºä¾èµ–æ€§æ–¹é¢çš„åº”ç”¨ã€‚æˆ‘ä»¬å»ºç«‹äº†æ‰©æ•£Transformerå¯¹å…·æœ‰å„ç§è¡°å‡æ¨¡å¼çš„é«˜æ–¯è¿‡ç¨‹æ•°æ®è¿›è¡Œè¯„åˆ†é€¼è¿‘å’Œåˆ†å¸ƒä¼°è®¡çš„ä¿è¯ã€‚æœ¬æ–‡å¼ºè°ƒäº†æ—¶ç©ºä¾èµ–æ€§å¦‚ä½•å½±å“å­¦ä¹ æ•ˆç‡å’Œæ•æ‰æ–¹å¼ï¼Œå¹¶æå‡ºäº†æ–°çš„Transformeré€¼è¿‘ç†è®ºï¼Œå…¶ä¸­Transformeræ‰®æ¼”å±•å¼€ç®—æ³•çš„è§’è‰²ã€‚æ•°å€¼å®éªŒæ”¯æŒæˆ‘ä»¬çš„ç†è®ºç»“æœï¼Œæä¾›æœ‰åŠ›è¯æ®è¡¨æ˜æ—¶ç©ºä¾èµ–æ€§è¢«æ•è·åœ¨æ³¨æ„åŠ›å±‚ä¸­ï¼Œä¸æˆ‘ä»¬çš„é€¼è¿‘ç†è®ºç›¸ç¬¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£TransformeræˆåŠŸæ‰©å±•äº†æ‰©æ•£æ¨¡å‹çš„å®¹é‡ï¼Œä¸ºé«˜è´¨é‡åºåˆ—æ•°æ®ç”Ÿæˆæä¾›äº†æ–°é€”å¾„ã€‚</li>
<li>åºåˆ—æ•°æ®å…·æœ‰æ—¶ç©ºä¾èµ–æ€§ï¼Œè¿™æ˜¯ç”Ÿæˆæ•°æ®éªŒè¯çš„å…³é”®ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡ä»ç†è®ºä¸Šæ¢è®¨äº†æ‰©æ•£Transformeråœ¨æ•æ‰æ—¶ç©ºä¾èµ–æ€§æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>å»ºç«‹äº†æ‰©æ•£Transformerå¯¹å…·æœ‰ä¸åŒè¡°å‡æ¨¡å¼çš„é«˜æ–¯è¿‡ç¨‹æ•°æ®çš„è¯„åˆ†é€¼è¿‘å’Œåˆ†å¸ƒä¼°è®¡çš„ä¿è¯ã€‚</li>
<li>æ‰©æ•£Transformerèƒ½å¤Ÿæ•æ‰å’Œå­¦ä¹ æ—¶ç©ºä¾èµ–æ€§ï¼Œè¿™å½±å“äº†å­¦ä¹ æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æå‡ºæ–°çš„Transformeré€¼è¿‘ç†è®ºï¼Œå…¶ä¸­Transformeræ‰®æ¼”ç®—æ³•å±•å¼€çš„è§’è‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4910d39f76cf8a62b462ef4deca7edec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2dc12d397058760db71b3108b58aba5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6a56125eb6031be96d23bc242dfb904.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-53e99a612b943d39cdf14c613335ec94.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  AAD-DCE An Aggregated Multimodal Attention Mechanism for Early and Late   Dynamic Contrast Enhanced Prostate MRI Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6f258dda41d56b1266669aaaaa56ea0d.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  MaintaAvatar A Maintainable Avatar Based on Neural Radiance Fields by   Continual Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">11770.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
