<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  AAD-DCE An Aggregated Multimodal Attention Mechanism for Early and Late   Dynamic Contrast Enhanced Prostate MRI Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-53e99a612b943d39cdf14c613335ec94.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-06-æ›´æ–°"><a href="#2025-02-06-æ›´æ–°" class="headerlink" title="2025-02-06 æ›´æ–°"></a>2025-02-06 æ›´æ–°</h1><h2 id="AAD-DCE-An-Aggregated-Multimodal-Attention-Mechanism-for-Early-and-Late-Dynamic-Contrast-Enhanced-Prostate-MRI-Synthesis"><a href="#AAD-DCE-An-Aggregated-Multimodal-Attention-Mechanism-for-Early-and-Late-Dynamic-Contrast-Enhanced-Prostate-MRI-Synthesis" class="headerlink" title="AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late   Dynamic Contrast Enhanced Prostate MRI Synthesis"></a>AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late   Dynamic Contrast Enhanced Prostate MRI Synthesis</h2><p><strong>Authors:Divya Bharti, Sriprabha Ramanarayanan, Sadhana S, Kishore Kumar M, Keerthi Ram, Harsh Agarwal, Ramesh Venkatesan, Mohanasankar Sivaprakasam</strong></p>
<p>Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) is a medical imaging technique that plays a crucial role in the detailed visualization and identification of tissue perfusion in abnormal lesions and radiological suggestions for biopsy. However, DCE-MRI involves the administration of a Gadolinium based (Gad) contrast agent, which is associated with a risk of toxicity in the body. Previous deep learning approaches that synthesize DCE-MR images employ unimodal non-contrast or low-dose contrast MRI images lacking focus on the local perfusion information within the anatomy of interest. We propose AAD-DCE, a generative adversarial network (GAN) with an aggregated attention discriminator module consisting of global and local discriminators. The discriminators provide a spatial embedded attention map to drive the generator to synthesize early and late response DCE-MRI images. Our method employs multimodal inputs - T2 weighted (T2W), Apparent Diffusion Coefficient (ADC), and T1 pre-contrast for image synthesis. Extensive comparative and ablation studies on the ProstateX dataset show that our model (i) is agnostic to various generator benchmarks and (ii) outperforms other DCE-MRI synthesis approaches with improvement margins of +0.64 dB PSNR, +0.0518 SSIM, -0.015 MAE for early response and +0.1 dB PSNR, +0.0424 SSIM, -0.021 MAE for late response, and (ii) emphasize the importance of attention ensembling. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/bhartidivya/AAD-DCE">https://github.com/bhartidivya/AAD-DCE</a>. </p>
<blockquote>
<p>åŠ¨æ€å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰æ˜¯ä¸€ç§åŒ»å­¦æˆåƒæŠ€æœ¯ï¼Œåœ¨å¼‚å¸¸ç—…å˜çš„è¡€æµçŒæ³¨è¯¦ç»†å¯è§†åŒ–ä»¥åŠé’ˆå¯¹æ´»æ£€çš„æ”¾å°„å­¦å»ºè®®ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼ŒDCE-MRIæ¶‰åŠä½¿ç”¨åŸºäºé’†ï¼ˆGadï¼‰çš„é€ å½±å‰‚ï¼Œè¿™å¸¦æ¥äº†ä½“å†…æ¯’æ€§é£é™©ã€‚ä¹‹å‰åˆæˆDCE-MRå›¾åƒçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸»è¦é‡‡ç”¨éå¯¹æ¯”æˆ–ä½å‰‚é‡å¯¹æ¯”MRIå›¾åƒçš„å•æ¨¡æ€ï¼Œç¼ºä¹å…³æ³¨è§£å‰–ç»“æ„å†…å±€éƒ¨çŒæ³¨ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†AAD-DCEï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼ŒåŒ…å«ä¸€ä¸ªèšåˆæ³¨æ„åŠ›é‰´åˆ«å™¨æ¨¡å—ï¼Œç”±å…¨å±€å’Œå±€éƒ¨é‰´åˆ«å™¨ç»„æˆã€‚é‰´åˆ«å™¨æä¾›ç©ºé—´åµŒå…¥æ³¨æ„åŠ›å›¾ï¼Œä»¥é©±åŠ¨ç”Ÿæˆå™¨åˆæˆæ—©æœŸå’Œæ™šæœŸå“åº”DCE-MRIå›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¤šæ¨¡æ€è¾“å…¥ï¼ŒåŒ…æ‹¬T2åŠ æƒï¼ˆT2Wï¼‰ã€è¡¨è§‚æ‰©æ•£ç³»æ•°ï¼ˆADCï¼‰å’ŒT1é¢„å¯¹æ¯”ç”¨äºå›¾åƒåˆæˆã€‚åœ¨ProstateXæ•°æ®é›†ä¸Šçš„å¹¿æ³›æ¯”è¾ƒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ï¼ˆiï¼‰å¯¹å„ç§ç”Ÿæˆå™¨åŸºå‡†æµ‹è¯•è¡¨ç°ç¨³å®šï¼›ï¼ˆiiï¼‰åœ¨æ—©æœŸå“åº”å’Œæ™šæœŸå“åº”çš„PSNRã€SSIMå’ŒMAEæŒ‡æ ‡ä¸Šåˆ†åˆ«æé«˜äº†+0.64 dBã€+0.0518å’Œ-0.015ä»¥åŠ+0.1 dBã€+0.0424å’Œ-0.021ï¼Œç›¸è¾ƒäºå…¶ä»–DCE-MRIåˆæˆæ–¹æ³•è¡¨ç°æ›´ä¼˜å¼‚ï¼›ï¼ˆiiiï¼‰å¼ºè°ƒäº†æ³¨æ„åŠ›é›†æˆçš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bhartidivya/AAD-DCE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/bhartidivya/AAD-DCEè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02555v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¸€ç§åä¸ºAAD-DCEçš„åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æŠ€æœ¯ï¼Œè¢«ç”¨äºåˆæˆåŠ¨æ€å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰ã€‚è¯¥æŠ€æœ¯é‡‡ç”¨èšåˆæ³¨æ„åŠ›é‰´åˆ«å™¨æ¨¡å—ï¼ŒåŒ…å«å…¨å±€å’Œå±€éƒ¨é‰´åˆ«å™¨ï¼Œå¹¶æä¾›ç©ºé—´åµŒå…¥æ³¨æ„åŠ›å›¾ä»¥æŒ‡å¯¼ç”Ÿæˆå™¨åˆæˆæ—©æœŸå’Œæ™šæœŸå“åº”çš„DCE-MRIå›¾åƒã€‚é‡‡ç”¨å¤šæ¨¡æ€è¾“å…¥ï¼Œå¦‚T2åŠ æƒã€è¡¨è§‚æ‰©æ•£ç³»æ•°å’ŒT1é¢„å¯¹æ¯”å›¾åƒè¿›è¡Œå›¾åƒåˆæˆã€‚åœ¨ProstateXæ•°æ®é›†ä¸Šçš„å¹¿æ³›æ¯”è¾ƒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–DCE-MRIåˆæˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AAD-DCEæ˜¯ä¸€ç§åŸºäºGANçš„DCE-MRIå›¾åƒåˆæˆæŠ€æœ¯ã€‚</li>
<li>è¯¥æŠ€æœ¯ä½¿ç”¨èšåˆæ³¨æ„åŠ›é‰´åˆ«å™¨æ¨¡å—ï¼ŒåŒ…å«å…¨å±€å’Œå±€éƒ¨é‰´åˆ«å™¨ã€‚</li>
<li>é‰´åˆ«å™¨æä¾›ç©ºé—´åµŒå…¥æ³¨æ„åŠ›å›¾ï¼ŒæŒ‡å¯¼ç”Ÿæˆå™¨åˆæˆæ—©æœŸå’Œæ™šæœŸå“åº”çš„DCE-MRIå›¾åƒã€‚</li>
<li>è¯¥æŠ€æœ¯é‡‡ç”¨å¤šæ¨¡æ€è¾“å…¥ï¼ŒåŒ…æ‹¬T2åŠ æƒã€ADCå’ŒT1é¢„å¯¹æ¯”å›¾åƒã€‚</li>
<li>åœ¨ProstateXæ•°æ®é›†ä¸Šçš„ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒAAD-DCEåœ¨å„ç§ç”Ÿæˆå™¨æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>AAD-DCEçš„åˆæˆå›¾åƒåœ¨PSNRã€SSIMå’ŒMAEç­‰è¯„ä»·æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–DCE-MRIåˆæˆæ–¹æ³•ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æ³¨æ„åŠ›é›†æˆçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cf0995ebd82f85ee7edfb015bc4baa16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10923e0595e1d3960897a4f7d6c07a84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8f697724877fc4e629c9434f5612c2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0dc7540a7dcd2bb35fa94a97dc61e18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3bb2725ff790ad8b53b472e16b72185b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-450b6799898d335223e789206f61cc60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01b678f867eb0bdcc0aa62ec3ed15a84.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mosaic3D-Foundation-Dataset-and-Model-for-Open-Vocabulary-3D-Segmentation"><a href="#Mosaic3D-Foundation-Dataset-and-Model-for-Open-Vocabulary-3D-Segmentation" class="headerlink" title="Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D   Segmentation"></a>Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D   Segmentation</h2><p><strong>Authors:Junha Lee, Chunghyun Park, Jaesung Choe, Yu-Chiang Frank Wang, Jan Kautz, Minsu Cho, Chris Choy</strong></p>
<p>We tackle open-vocabulary 3D scene understanding by introducing a novel data generation pipeline and training framework. Our method addresses three critical requirements for effective training: precise 3D region segmentation, comprehensive textual descriptions, and sufficient dataset scale. By leveraging state-of-the-art open-vocabulary image segmentation models and region-aware Vision-Language Models, we develop an automatic pipeline that generates high-quality 3D mask-text pairs. Applying this pipeline to multiple 3D scene datasets, we create Mosaic3D-5.6M, a dataset of over 30K annotated scenes with 5.6M mask-text pairs, significantly larger than existing datasets. Building upon this data, we propose Mosaic3D, a foundation model combining a 3D encoder trained with contrastive learning and a lightweight mask decoder for open-vocabulary 3D semantic and instance segmentation. Our approach achieves state-of-the-art results on open-vocabulary 3D semantic and instance segmentation tasks including ScanNet200, Matterport3D, and ScanNet++, with ablation studies validating the effectiveness of our large-scale training data. </p>
<blockquote>
<p>æˆ‘ä»¬é€šè¿‡å¯¹æ–°å‹æ•°æ®ç”Ÿæˆæµç¨‹å’Œè®­ç»ƒæ¡†æ¶çš„å¼•å…¥ï¼Œè§£å†³äº†å¼€æ”¾å¼è¯æ±‡è¡¨ä¸‹çš„3Dåœºæ™¯ç†è§£é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ»¡è¶³äº†æœ‰æ•ˆè®­ç»ƒçš„ä¸‰é¡¹å…³é”®éœ€æ±‚ï¼šç²¾ç¡®çš„3DåŒºåŸŸåˆ†å‰²ã€å…¨é¢çš„æ–‡æœ¬æè¿°å’Œè¶³å¤Ÿè§„æ¨¡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å€ŸåŠ©æœ€å…ˆè¿›çš„å¼€æ”¾å¼è¯æ±‡è¡¨å›¾åƒåˆ†å‰²æ¨¡å‹å’ŒåŒºåŸŸæ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ç§è‡ªåŠ¨æµç¨‹ï¼Œç”Ÿæˆäº†é«˜è´¨é‡3D mask-textå¯¹ã€‚å°†æ­¤æµç¨‹åº”ç”¨äºå¤šä¸ª3Dåœºæ™¯æ•°æ®é›†ï¼Œæˆ‘ä»¬åˆ›å»ºäº†Mosaic3D-5.6Mæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡3ä¸‡ä¸ªæ³¨é‡Šåœºæ™¯å’Œ560ä¸‡ä¸ªmask-textå¯¹ï¼Œæ˜¾è‘—å¤§äºç°æœ‰æ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†Mosaic3Dæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†é€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„3Dç¼–ç å™¨å’Œç”¨äºå¼€æ”¾å¼è¯æ±‡è¡¨çš„è½»é‡çº§maskè§£ç å™¨çš„åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¼€æ”¾å¼è¯æ±‡è¡¨ä¸‹çš„3Dè¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°ç»“æœï¼ŒåŒ…æ‹¬ScanNet200ã€Matterport3Då’ŒScanNet++ç­‰ä»»åŠ¡ï¼Œé€šè¿‡æ¶ˆèç ”ç©¶éªŒè¯äº†å¤§è§„æ¨¡è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02548v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://nvlabs.github.io/Mosaic3D/">https://nvlabs.github.io/Mosaic3D/</a></p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶è§£å†³äº†å¼€æ”¾è¯æ±‡è¡¨ä¸­çš„ä¸‰ç»´åœºæ™¯ç†è§£é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥æ–°çš„æ•°æ®ç”Ÿæˆæµç¨‹å’Œè®­ç»ƒæ¡†æ¶ï¼Œæ»¡è¶³äº†ç²¾ç¡®çš„ä¸‰ç»´åŒºåŸŸåˆ†å‰²ã€å…¨é¢çš„æ–‡æœ¬æè¿°å’Œè¶³å¤Ÿçš„æ•°æ®é›†è§„æ¨¡ä¸‰ä¸ªå…³é”®è®­ç»ƒè¦æ±‚ã€‚ç ”ç©¶å›¢é˜Ÿä½¿ç”¨å…ˆè¿›çš„å¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²æ¨¡å‹å’ŒåŒºåŸŸæ„ŸçŸ¥çš„è§†è¯­è¨€æ¨¡å‹ï¼Œå¼€å‘å‡ºè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ä¸‰ç»´æ©è†œ-æ–‡æœ¬å¯¹çš„ç®¡é“ã€‚è¯¥ç®¡é“åº”ç”¨äºå¤šä¸ªä¸‰ç»´åœºæ™¯æ•°æ®é›†ï¼Œåˆ›å»ºäº†Mosaic3D-5.6Mæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡3ä¸‡æ ‡æ³¨åœºæ™¯å’Œ560ä¸‡æ©è†œ-æ–‡æœ¬å¯¹ï¼Œæ˜¾è‘—å¤§äºç°æœ‰æ•°æ®é›†ã€‚åŸºäºè¿™äº›æ•°æ®ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†Mosaic3Dæ¨¡å‹ï¼Œç»“åˆä¸‰ç»´ç¼–ç å™¨å¯¹æ¯”å­¦ä¹ å’Œè½»é‡çº§æ©è†œè§£ç å™¨è¿›è¡Œå¼€æ”¾è¯æ±‡ä¸‰ç»´è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ã€‚è¯¥æ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡çš„ä¸‰ç»´è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°å‹æ•°æ®ç”Ÿæˆæµç¨‹å’Œè®­ç»ƒæ¡†æ¶ä»¥è§£å†³å¼€æ”¾è¯æ±‡ä¸‰ç»´åœºæ™¯ç†è§£é—®é¢˜ã€‚</li>
<li>æ»¡è¶³ç²¾ç¡®ä¸‰ç»´åŒºåŸŸåˆ†å‰²ã€å…¨é¢æ–‡æœ¬æè¿°å’Œè¶³å¤Ÿæ•°æ®é›†è§„æ¨¡ä¸‰ä¸ªå…³é”®è®­ç»ƒè¦æ±‚ã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›å›¾åƒåˆ†å‰²æ¨¡å‹å’ŒåŒºåŸŸæ„ŸçŸ¥è§†è¯­è¨€æ¨¡å‹ï¼Œåˆ›å»ºè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ä¸‰ç»´æ©è†œ-æ–‡æœ¬å¯¹çš„ç®¡é“ã€‚</li>
<li>å¼€å‘å‡ºå¤§è§„æ¨¡æ•°æ®é›†Mosaic3D-5.6Mï¼ŒåŒ…å«è¶…è¿‡3ä¸‡æ ‡æ³¨åœºæ™¯å’Œ560ä¸‡æ©è†œ-æ–‡æœ¬å¯¹ã€‚</li>
<li>æå‡ºMosaic3Dæ¨¡å‹ï¼Œç»“åˆä¸‰ç»´ç¼–ç å™¨å’Œè½»é‡çº§æ©è†œè§£ç å™¨è¿›è¡Œå¼€æ”¾è¯æ±‡ä¸‰ç»´è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ã€‚</li>
<li>Mosaic3Dæ¨¡å‹åœ¨å¤šä¸ªå¼€æ”¾è¯æ±‡ä¸‰ç»´è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—æœ€æ–°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-411196b20c4c0c2b150020218b36154d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9821fd57338ff12c94253f52010cc508.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3ff4ffaf147e46b4f8f5a2f65148c49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5713a565341905a70fb25649f18836c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fef7f7a41c938aecddeb9961a5ed5f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1543e1f7363ffe3a8956fb0986f326e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a34175a8be2cff87a694d341290ea029.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Self-Supervised-Framework-for-Improved-Generalisability-in-Ultrasound-B-mode-Image-Segmentation"><a href="#A-Self-Supervised-Framework-for-Improved-Generalisability-in-Ultrasound-B-mode-Image-Segmentation" class="headerlink" title="A Self-Supervised Framework for Improved Generalisability in Ultrasound   B-mode Image Segmentation"></a>A Self-Supervised Framework for Improved Generalisability in Ultrasound   B-mode Image Segmentation</h2><p><strong>Authors:Edward Ellis, Andrew Bulpitt, Nasim Parsa, Michael F Byrne, Sharib Ali</strong></p>
<p>Ultrasound (US) imaging is clinically invaluable due to its noninvasive and safe nature. However, interpreting US images is challenging, requires significant expertise, and time, and is often prone to errors. Deep learning offers assistive solutions such as segmentation. Supervised methods rely on large, high-quality, and consistently labeled datasets, which are challenging to curate. Moreover, these methods tend to underperform on out-of-distribution data, limiting their clinical utility. Self-supervised learning (SSL) has emerged as a promising alternative, leveraging unlabeled data to enhance model performance and generalisability. We introduce a contrastive SSL approach tailored for B-mode US images, incorporating a novel Relation Contrastive Loss (RCL). RCL encourages learning of distinct features by differentiating positive and negative sample pairs through a learnable metric. Additionally, we propose spatial and frequency-based augmentation strategies for the representation learning on US images. Our approach significantly outperforms traditional supervised segmentation methods across three public breast US datasets, particularly in data-limited scenarios. Notable improvements on the Dice similarity metric include a 4% increase on 20% and 50% of the BUSI dataset, nearly 6% and 9% improvements on 20% and 50% of the BrEaST dataset, and 6.4% and 3.7% improvements on 20% and 50% of the UDIAT dataset, respectively. Furthermore, we demonstrate superior generalisability on the out-of-distribution UDIAT dataset with performance boosts of 20.6% and 13.6% compared to the supervised baseline using 20% and 50% of the BUSI and BrEaST training data, respectively. Our research highlights that domain-inspired SSL can improve US segmentation, especially under data-limited conditions. </p>
<blockquote>
<p>è¶…å£°ï¼ˆUSï¼‰æˆåƒå› å…¶æ— åˆ›ä¸”å®‰å…¨çš„ç‰¹ç‚¹è€Œåœ¨ä¸´åºŠä¸Šå…·æœ‰å·¨å¤§ä»·å€¼ã€‚ç„¶è€Œï¼Œè§£è¯»è¶…å£°å›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œå¤§é‡æ—¶é—´ï¼Œå¹¶ä¸”å®¹æ˜“å‡ºé”™ã€‚æ·±åº¦å­¦ä¹ æä¾›äº†è¾…åŠ©è§£å†³æ–¹æ¡ˆï¼Œå¦‚åˆ†å‰²æŠ€æœ¯ã€‚ç›‘ç£å­¦ä¹ æ–¹æ³•ä¾èµ–äºå¤§é‡é«˜è´¨é‡ä¸”æŒç»­æ ‡æ³¨çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†éš¾ä»¥æ•´ç†ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠä¸Šçš„å®ç”¨æ€§ã€‚è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ³•åº”è¿è€Œç”Ÿï¼Œå®ƒåˆ©ç”¨æœªæ ‡è®°çš„æ•°æ®æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œé€šç”¨æ€§ã€‚æˆ‘ä»¬é’ˆå¯¹Bæ¨¡å¼è¶…å£°å›¾åƒå¼•å…¥äº†ä¸€ç§å¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå¹¶ç»“åˆäº†ä¸€ç§æ–°å‹çš„å…³ç³»å¯¹æ¯”æŸå¤±ï¼ˆRCLï¼‰ã€‚RCLé€šè¿‡å¯å­¦ä¹ çš„æŒ‡æ ‡æ¥åŒºåˆ†æ­£è´Ÿæ ·æœ¬å¯¹ï¼Œé¼“åŠ±å­¦ä¹ ç‹¬ç‰¹ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºç©ºé—´å’Œé¢‘ç‡çš„å¢å¼ºç­–ç•¥ï¼Œç”¨äºè¶…å£°å›¾åƒä¸Šçš„è¡¨ç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å…±ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šçš„è¡¨ç°è¿œè¶…ä¼ ç»Ÿçš„ç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚åœ¨è¿ªæ°ç›¸ä¼¼åº¦æŒ‡æ ‡ä¸Šçš„æ˜¾è‘—æ”¹è¿›åŒ…æ‹¬åœ¨BUSIæ•°æ®é›†20%å’Œ50%çš„æ•°æ®ä¸Šåˆ†åˆ«æé«˜äº†4%å’Œè¿‘6%å’Œè¿‘9%çš„æ”¹è¿›ï¼›åœ¨BrEaSTæ•°æ®é›†ä¸Šä¹Ÿæœ‰äº†ç›¸åº”çš„æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨åˆ†å¸ƒå¤–çš„UDIATæ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ä½¿ç”¨BUSIå’ŒBrEaSTè®­ç»ƒæ•°æ®20%å’Œ50%çš„åŸºçº¿ç›¸æ¯”ï¼Œæ€§èƒ½åˆ†åˆ«æé«˜äº†20.6%å’Œ13.6%ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†é¢†åŸŸå¯å‘å¼çš„è‡ªç›‘ç£å­¦ä¹ èƒ½å¤Ÿæ”¹å–„è¶…å£°åˆ†å‰²æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02489v1">PDF</a> 12</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¶…å£°ï¼ˆUSï¼‰æˆåƒçš„é‡è¦æ€§åŠå…¶è§£è¯»çš„æŒ‘æˆ˜æ€§ã€‚æ·±åº¦å­¦ä¹ ä¸­çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ–¹æ³•åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®æé«˜æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§é’ˆå¯¹Bæ¨¡å¼è¶…å£°å›¾åƒçš„å¯¹æ¯”SSLæ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„å…³ç³»å¯¹æ¯”æŸå¤±ï¼ˆRCLï¼‰ã€‚æ­¤æ–¹æ³•åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹æ”¹å–„æ˜æ˜¾ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å±•ç¤ºäº†å…¶åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¶…å£°ï¼ˆUSï¼‰æˆåƒåœ¨ä¸´åºŠä¸­éå¸¸é‡è¦ï¼Œä½†å…¶å›¾åƒè§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ä¸­çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ˜¯è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æœ‰æ•ˆæ–¹æ³•ä¹‹ä¸€ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§é’ˆå¯¹Bæ¨¡å¼è¶…å£°å›¾åƒçš„å¯¹æ¯”SSLæ–¹æ³•ï¼ŒåŒ…æ‹¬å…³ç³»å¯¹æ¯”æŸå¤±ï¼ˆRCLï¼‰å’ŒåŸºäºç©ºé—´å’Œé¢‘ç‡çš„å¢å¼ºç­–ç•¥ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹æ”¹å–„æ˜æ˜¾ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨è·¨æ•°æ®é›†çš„åº”ç”¨ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02489">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53f8c94b3885a617ef10eceb8aa87ec1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0640f368db2aed1a8401c8e8dae30d52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b05990c1dfa9823b545a94eaceb009f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00c3e348cd14e5bff49c520592505ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4b1af83f358d2e70636b3e8641d4071.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9498dfe9c72c6baa847ee9f9467d7371.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Medical-Multimodal-Model-Stealing-Attacks-via-Adversarial-Domain-Alignment"><a href="#Medical-Multimodal-Model-Stealing-Attacks-via-Adversarial-Domain-Alignment" class="headerlink" title="Medical Multimodal Model Stealing Attacks via Adversarial Domain   Alignment"></a>Medical Multimodal Model Stealing Attacks via Adversarial Domain   Alignment</h2><p><strong>Authors:Yaling Shen, Zhixiong Zhuang, Kun Yuan, Maria-Irina Nicolae, Nassir Navab, Nicolas Padoy, Mario Fritz</strong></p>
<p>Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-STEAL), the first stealing attack against medical MLLMs. ADA-STEAL relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data. </p>
<blockquote>
<p>åŒ»ç–—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£æˆä¸ºåŒ»ç–—ä¿å¥ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼ŒååŠ©åŒ»ç–—äººå‘˜è¿›è¡Œå†³ç­–åˆ¶å®šå’Œç»“æœåˆ†æã€‚ç”¨äºç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šçš„æ¨¡å‹èƒ½å¤Ÿè§£é‡ŠåŒ»å­¦å›¾åƒï¼Œä»è€Œå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚ç”±äºåŒ»ç–—æ•°æ®ç¨€ç¼ºä¸”å—éšç§æ³•è§„çš„ä¿æŠ¤ï¼ŒåŒ»ç–—MLLMsä»£è¡¨äº†å®è´µçš„çŸ¥è¯†äº§æƒã€‚ç„¶è€Œï¼Œè¿™äº›èµ„äº§å¯èƒ½é¢ä¸´æ¨¡å‹çªƒå–çš„é£é™©ï¼Œæ”»å‡»è€…é€šè¿‡é»‘ç®±è®¿é—®è¯•å›¾å¤åˆ¶å…¶åŠŸèƒ½ã€‚è¿„ä»Šä¸ºæ­¢ï¼ŒåŒ»ç–—é¢†åŸŸçš„æ¨¡å‹çªƒå–ä¸»è¦é›†ä¸­åœ¨åˆ†ç±»é¢†åŸŸï¼Œä½†ç°æœ‰çš„æ”»å‡»å¯¹MLLMså¹¶ä¸æœ‰æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—MLLMsçš„é¦–ä¸ªçªƒå–æ”»å‡»â€”â€”å¯¹æŠ—åŸŸå¯¹é½ï¼ˆADA-STEALï¼‰ã€‚ADA-STEALä¾èµ–äºå…¬å…±ä¸”å¹¿æ³›å¯ç”¨çš„è‡ªç„¶å›¾åƒï¼Œè€ŒéåŒ»ç–—å›¾åƒã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡å¯¹æŠ—æ€§å™ªå£°è¿›è¡Œæ•°æ®å¢å¼ºè¶³ä»¥å…‹æœè‡ªç„¶å›¾åƒå’Œå—å®³è€…MLLMé¢†åŸŸç‰¹å®šåˆ†å¸ƒä¹‹é—´çš„æ•°æ®åˆ†å¸ƒå·®è·ã€‚åœ¨IU Xå°„çº¿ä»¥åŠMIMIC-CXRæ”¾å°„å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¯¹æŠ—åŸŸå¯¹é½ä½¿æ”»å‡»è€…æ— éœ€è®¿é—®åŒ»ç–—æ•°æ®å³å¯çªƒå–åŒ»ç–—MLLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02438v1">PDF</a> Accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŒ»ç–—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥ç³»ç»Ÿä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼ŒååŠ©åŒ»ç–—äººå‘˜è¿›è¡Œå†³ç­–å’Œç»“æœåˆ†æã€‚é’ˆå¯¹æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„æ¨¡å‹èƒ½å¤Ÿè§£è¯»åŒ»å­¦å›¾åƒï¼Œä»è€Œå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚ç”±äºåŒ»å­¦æ•°æ®ç¨€ç¼ºä¸”å—éšç§æ³•è§„ä¿æŠ¤ï¼ŒåŒ»å­¦MLLMsè¢«è§†ä¸ºå®è´µçš„çŸ¥è¯†äº§æƒã€‚ç„¶è€Œï¼Œè¿™äº›èµ„äº§å­˜åœ¨æ½œåœ¨çš„æ¨¡å‹çªƒå–é£é™©ï¼Œæ”»å‡»è€…é€šè¿‡é»‘ç®±è®¿é—®è¯•å›¾å¤åˆ¶å…¶åŠŸèƒ½ã€‚ç›®å‰é’ˆå¯¹åŒ»å­¦é¢†åŸŸçš„æ¨¡å‹çªƒå–ä¸»è¦é›†ä¸­åœ¨åˆ†ç±»æ–¹é¢ï¼Œä½†ç°æœ‰æ”»å‡»å¯¹MLLMså¹¶ä¸å¥æ•ˆã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹åŒ»å­¦MLLMsçš„é¦–ä¸ªçªƒå–æ”»å‡»â€”â€”å¯¹æŠ—åŸŸå¯¹é½ï¼ˆADA-STEALï¼‰ã€‚ADA-STEALä¾èµ–äºå…¬å¼€ä¸”å¹¿æ³›å¯ç”¨çš„è‡ªç„¶å›¾åƒï¼Œè€ŒéåŒ»å­¦å›¾åƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æ•°æ®å¢å¼ºæ·»åŠ å¯¹æŠ—æ€§å™ªå£°è¶³ä»¥å…‹æœè‡ªç„¶å›¾åƒä¸å—å®³è€…MLLMçš„ç‰¹å®šé¢†åŸŸåˆ†å¸ƒä¹‹é—´çš„æ•°æ®åˆ†å¸ƒå·®è·ã€‚åœ¨IU Xå…‰ç‰‡å’ŒMIMIC-CXRæ”¾å°„å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¯¹æŠ—åŸŸå¯¹é½ä½¿æ”»å‡»è€…æ— éœ€è®¿é—®åŒ»å­¦æ•°æ®å³å¯çªƒå–åŒ»ç–—MLLMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥ç³»ç»Ÿä¸­ä½œç”¨æ˜¾è‘—ï¼Œè¾…åŠ©å†³ç­–å’Œç»“æœåˆ†æã€‚</li>
<li>åŒ»å­¦MLLMsèƒ½å¤Ÿè§£è¯»åŒ»å­¦å›¾åƒï¼Œå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿå·¥ä½œè´Ÿæ‹…ã€‚</li>
<li>åŒ»å­¦æ•°æ®ç¨€ç¼ºæ€§å’Œéšç§æ³•è§„ä½¿åŒ»å­¦MLLMsæˆä¸ºå®è´µçš„çŸ¥è¯†äº§æƒã€‚</li>
<li>åŒ»å­¦MLLMså­˜åœ¨æ¨¡å‹çªƒå–é£é™©ï¼Œæ”»å‡»è€…è¯•å›¾é€šè¿‡é»‘ç®±è®¿é—®å¤åˆ¶å…¶åŠŸèƒ½ã€‚</li>
<li>ç›®å‰é’ˆå¯¹åŒ»å­¦é¢†åŸŸçš„æ¨¡å‹çªƒå–ä¸»è¦é›†ä¸­åœ¨åˆ†ç±»æ¨¡å‹ä¸Šï¼Œå¯¹MLLMsçš„æ”»å‡»å¹¶ä¸æˆåŠŸã€‚</li>
<li>å¼•å…¥ä¸€ç§æ–°çš„çªƒå–æ”»å‡»æ–¹æ³•â€”â€”å¯¹æŠ—åŸŸå¯¹é½ï¼ˆADA-STEALï¼‰ï¼Œå®ƒåˆ©ç”¨å…¬å¼€çš„è‡ªç„¶å›¾åƒè¿›è¡Œæ¨¡å‹çªƒå–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd6d59ed3da1f14128b437f75a369c08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76bdec8905bda7c4423a6013ebb9f5ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a8e12f8702934fb75d56f7af98a0842.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b8bac7d4eddee4ec3cf16f63d763390.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3dedd89a79d4ce15d55537342ef4f32d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e46681d2d988c00397279d28fe083a71.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Extending-SEEDS-to-a-Supervoxel-Algorithm-for-Medical-Image-Analysis"><a href="#Extending-SEEDS-to-a-Supervoxel-Algorithm-for-Medical-Image-Analysis" class="headerlink" title="Extending SEEDS to a Supervoxel Algorithm for Medical Image Analysis"></a>Extending SEEDS to a Supervoxel Algorithm for Medical Image Analysis</h2><p><strong>Authors:Chenhui Zhao, Yan Jiang, Todd C. Hollon</strong></p>
<p>In this work, we extend the SEEDS superpixel algorithm from 2D images to 3D volumes, resulting in 3D SEEDS, a faster, better, and open-source supervoxel algorithm for medical image analysis. We compare 3D SEEDS with the widely used supervoxel algorithm SLIC on 13 segmentation tasks across 10 organs. 3D SEEDS accelerates supervoxel generation by a factor of 10, improves the achievable Dice score by +6.5%, and reduces the under-segmentation error by -0.16%. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Zch0414/3d_seeds">https://github.com/Zch0414/3d_seeds</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†SEEDSè¶…åƒç´ ç®—æ³•ä»2Då›¾åƒæ‰©å±•åˆ°3Dä½“ç§¯ï¼Œä»è€Œå¾—åˆ°3D SEEDSï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´å¿«ã€æ›´å¥½ã€å¼€æºçš„ç”¨äºåŒ»å­¦å›¾åƒåˆ†æçš„Supervoxelç®—æ³•ã€‚æˆ‘ä»¬åœ¨10ä¸ªå™¨å®˜çš„13ä¸ªåˆ†å‰²ä»»åŠ¡ä¸Šï¼Œå°†3D SEEDSä¸å¹¿æ³›ä½¿ç”¨çš„Supervoxelç®—æ³•SLICè¿›è¡Œäº†æ¯”è¾ƒã€‚3D SEEDSå°†Supervoxelçš„ç”Ÿæˆé€Ÿåº¦æé«˜äº†10å€ï¼Œæé«˜äº†Diceç³»æ•°çš„å¾—åˆ†+6.5%ï¼Œå¹¶é™ä½äº†æ¬ åˆ†å‰²è¯¯å·®-0.16%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Zch0414/3d_seeds%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Zch0414/3d_seedsä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02409v1">PDF</a> Tech report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ‰©å±•äº†SEEDSè¶…åƒç´ ç®—æ³•ï¼Œä»2Då›¾åƒåˆ°3Dä½“ç§¯ï¼Œæå‡ºäº†æ›´å¿«çš„ã€æ›´å¥½çš„ã€å¼€æºçš„ç”¨äºåŒ»å­¦å›¾åƒåˆ†æçš„3D SEEDSè¶…ä½“ç´ ç®—æ³•ã€‚åœ¨è·¨è¶Š10ä¸ªå™¨å®˜çš„13ä¸ªåˆ†å‰²ä»»åŠ¡ä¸Šï¼Œä¸å¹¿æ³›ä½¿ç”¨çš„SLICè¶…ä½“ç´ ç®—æ³•ç›¸æ¯”ï¼Œ3D SEEDSåŠ é€Ÿè¶…ä½“ç´ ç”Ÿæˆé€Ÿåº¦æé«˜10å€ï¼Œæé«˜äº†Diceè¯„åˆ†ï¼Œå¹¶é™ä½äº†æ¬ åˆ†å‰²è¯¯å·®ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Zch0414/3d_seeds%E3%80%82">https://github.com/Zch0414/3d_seedsã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æˆåŠŸå°†SEEDSè¶…åƒç´ ç®—æ³•ä»2Då›¾åƒæ‰©å±•åˆ°3Dä½“ç§¯ï¼Œå½¢æˆäº†æ–°çš„è¶…ä½“ç´ ç®—æ³•â€”â€”3D SEEDSã€‚</li>
<li>3D SEEDSåœ¨å¤šä¸ªå™¨å®˜ï¼ˆå…±è·¨è¶Šåä¸ªï¼‰çš„å¤šä¸ªåˆ†å‰²ä»»åŠ¡ï¼ˆå…±13ä¸ªï¼‰ä¸Šä¸å¹¿æ³›ä½¿ç”¨çš„SLICç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
<li>ç›¸è¾ƒäºSLICç®—æ³•ï¼Œ3D SEEDSå¤§å¤§åŠ é€Ÿäº†è¶…ä½“ç´ çš„ç”Ÿæˆé€Ÿåº¦ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>ç›¸è¾ƒäºSLICç®—æ³•ï¼Œä½¿ç”¨3D SEEDSåDiceè¯„åˆ†æé«˜äº†çº¦+6.5%ã€‚è¿™è¡¨æ˜äº†åœ¨åŒ»ç–—å›¾åƒåˆ†æä¸­å¯èƒ½æé«˜çš„æ•ˆæœã€‚</li>
<li>ç›¸è¾ƒäºSLICç®—æ³•ï¼Œä½¿ç”¨3D SEEDSåæ¬ åˆ†å‰²è¯¯å·®é™ä½äº†çº¦-0.16%ï¼Œè¡¨æ˜å…¶åœ¨æŸäº›æƒ…å†µä¸‹èƒ½æ›´å¥½åœ°å¤„ç†å¤æ‚çš„å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>ä»£ç å·²ç»å¼€æºï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚å…·ä½“é“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/Zch0414/3d_seeds">https://github.com/Zch0414/3d_seeds</a>ã€‚è¿™å°†åŠ é€Ÿç®—æ³•çš„æ¨å¹¿å’Œå®é™…åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02409">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b570162880f12c7a6c68204d08d13206.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9792333d7617224affdffd31f870baf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8881f1c2937aafde4565ca8003bf0d3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db3c004546f0e46e6631e522d8b5f751.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3ffdd38a8ef715c4eb81834515c9942.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ce7c875f68442b7594e6c28894b7e0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a13dcd7a6b3e0fd728273b5a3859ea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3974dbcd348c44ae01bd1a26dc7ee808.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Deep-Ensemble-approach-for-Enhancing-Brain-Tumor-Segmentation-in-Resource-Limited-Settings"><a href="#Deep-Ensemble-approach-for-Enhancing-Brain-Tumor-Segmentation-in-Resource-Limited-Settings" class="headerlink" title="Deep Ensemble approach for Enhancing Brain Tumor Segmentation in   Resource-Limited Settings"></a>Deep Ensemble approach for Enhancing Brain Tumor Segmentation in   Resource-Limited Settings</h2><p><strong>Authors:Jeremiah Fadugba, Isabel Lieberman, Olabode Ajayi, Mansour Osman, Solomon Oluwole Akinola, Tinashe Mustvangwa, Dong Zhang, Udunna C Anazondo, Raymond Confidence</strong></p>
<p>Segmentation of brain tumors is a critical step in treatment planning, yet manual segmentation is both time-consuming and subjective, relying heavily on the expertise of radiologists. In Sub-Saharan Africa, this challenge is magnified by overburdened medical systems and limited access to advanced imaging modalities and expert radiologists. Automating brain tumor segmentation using deep learning offers a promising solution. Convolutional Neural Networks (CNNs), especially the U-Net architecture, have shown significant potential. However, a major challenge remains: achieving generalizability across different datasets. This study addresses this gap by developing a deep learning ensemble that integrates UNet3D, V-Net, and MSA-VNet models for the semantic segmentation of gliomas. By initially training on the BraTS-GLI dataset and fine-tuning with the BraTS-SSA dataset, we enhance model performance. Our ensemble approach significantly outperforms individual models, achieving DICE scores of 0.8358 for Tumor Core, 0.8521 for Whole Tumor, and 0.8167 for Enhancing Tumor. These results underscore the potential of ensemble methods in improving the accuracy and reliability of automated brain tumor segmentation, particularly in resource-limited settings. </p>
<blockquote>
<p>è„‘éƒ¨è‚¿ç˜¤çš„åˆ†å‰²æ˜¯æ²»ç–—æ–¹æ¡ˆåˆ¶å®šä¸­çš„å…³é”®æ­¥éª¤ï¼Œç„¶è€Œæ‰‹åŠ¨åˆ†å‰²æ—¢è€—æ—¶åˆå­˜åœ¨ä¸»è§‚æ€§ï¼Œå¹¶é«˜åº¦ä¾èµ–äºæ”¾å°„ç§‘ä¸“å®¶çš„ä¸“ä¸šçŸ¥è¯†ã€‚åœ¨æ’’å“ˆæ‹‰ä»¥å—éæ´²åœ°åŒºï¼Œç”±äºåŒ»ç–—ç³»ç»Ÿè´Ÿæ‹…è¿‡é‡ä»¥åŠå…ˆè¿›çš„æˆåƒæ¨¡å¼å’Œä¸“å®¶çº§æ”¾å°„ç§‘åŒ»ç”Ÿèµ„æºæœ‰é™ï¼Œè¿™ä¸€æŒ‘æˆ˜è¿›ä¸€æ­¥åŠ å‰§ã€‚ä½¿ç”¨æ·±åº¦å­¦ä¹ è‡ªåŠ¨è¿›è¡Œè„‘éƒ¨è‚¿ç˜¤åˆ†å‰²æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å°¤å…¶æ˜¯U-Netæ¶æ„æ˜¾ç¤ºå‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåœ¨ä¸åŒæ•°æ®é›†ä¸Šå®ç°æ³›åŒ–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼€å‘ä¸€ä¸ªæ·±åº¦å­¦ä¹ é›†æˆæ¥è§£å†³è¿™ä¸€å·®è·ï¼Œè¯¥é›†æˆç»“åˆäº†UNet3Dã€V-Netå’ŒMSA-VNetæ¨¡å‹è¿›è¡Œèƒ¶è´¨ç˜¤çš„è¯­ä¹‰åˆ†å‰²ã€‚é€šè¿‡é¦–å…ˆåœ¨BraTS-GLIæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨BraTS-SSAæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„é›†æˆæ–¹æ³•æ˜¾è‘—ä¼˜äºå•ä¸ªæ¨¡å‹ï¼Œè‚¿ç˜¤æ ¸å¿ƒçš„DICEåˆ†æ•°ä¸º0.8358ï¼Œæ•´ä¸ªè‚¿ç˜¤çš„DICEåˆ†æ•°ä¸º0.8521ï¼Œå¢å¼ºè‚¿ç˜¤çš„DICEåˆ†æ•°ä¸º0.8167ã€‚è¿™äº›ç»“æœçªæ˜¾äº†é›†æˆæ–¹æ³•åœ¨æ”¹è¿›è‡ªåŠ¨è„‘éƒ¨è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­æ›´æ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02179v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è‡ªåŠ¨åŒ–è„‘è‚¿ç˜¤åˆ†å‰²çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ’’å“ˆæ‹‰ä»¥å—éæ´²åœ°åŒºé¢ä¸´æŒ‘æˆ˜çš„æƒ…å†µä¸‹ã€‚è¯¥ç ”ç©¶é‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰çš„U-Netæ¶æ„å¹¶ç»“åˆUNet3Dã€V-Netå’ŒMSA-VNetæ¨¡å‹ï¼Œå¯¹èƒ¶è´¨ç˜¤è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚é€šè¿‡åˆå§‹è®­ç»ƒåœ¨BraTS-GLIæ•°æ®é›†ä¸Šï¼Œå¹¶ä½¿ç”¨BraTS-SSAæ•°æ®é›†å¾®è°ƒï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚è¯¥ç ”ç©¶çš„ç»“æœè¡¨æ˜ï¼Œé›†æˆæ–¹æ³•æ˜¾è‘—ä¼˜äºå•ä¸ªæ¨¡å‹ï¼Œè‚¿ç˜¤æ ¸å¿ƒã€æ•´ä¸ªè‚¿ç˜¤å’Œå¢å¼ºè‚¿ç˜¤çš„DICEå¾—åˆ†åˆ†åˆ«ä¸º0.8358ã€0.8521å’Œ0.8167ã€‚è¿™çªæ˜¾äº†é›†æˆæ–¹æ³•åœ¨æ”¹å–„è‡ªåŠ¨åŒ–è„‘è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤çš„åˆ†å‰²æ˜¯æ²»ç–—è®¡åˆ’ä¸­çš„å…³é”®æ­¥éª¤ï¼Œä½†æ‰‹åŠ¨åˆ†å‰²è€—æ—¶ä¸”ä¸»è§‚ï¼Œä¾èµ–äºæ”¾å°„ç§‘ä¸“å®¶çš„ä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>åœ¨æ’’å“ˆæ‹‰ä»¥å—éæ´²åœ°åŒºï¼Œç”±äºåŒ»ç–—ç³»ç»Ÿè´Ÿæ‹…è¿‡é‡ã€å…ˆè¿›çš„æˆåƒæ¨¡å¼å’Œä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿæœ‰é™ï¼Œè¿™ä¸€æŒ‘æˆ˜è¢«æ”¾å¤§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ è‡ªåŠ¨è„‘è‚¿ç˜¤åˆ†å‰²æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚å…¶ä¸­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ç‰¹åˆ«æ˜¯U-Netæ¶æ„æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>é›†æˆå¤šç§æ¨¡å‹ï¼ˆå¦‚UNet3Dã€V-Netå’ŒMSA-VNetï¼‰èƒ½å¤Ÿæé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ºè¯­ä¹‰åˆ†å‰²æä¾›æ›´å‡†ç¡®çš„é¢„æµ‹ã€‚</li>
<li>é€šè¿‡åœ¨BraTS-GLIæ•°æ®é›†ä¸Šè¿›è¡Œåˆå§‹è®­ç»ƒå¹¶ä½¿ç”¨BraTS-SSAæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é›†æˆæ–¹æ³•æ˜¾è‘—ä¼˜äºå•ä¸€æ¨¡å‹ï¼Œåœ¨è‚¿ç˜¤æ ¸å¿ƒã€æ•´ä¸ªè‚¿ç˜¤å’Œå¢å¼ºè‚¿ç˜¤çš„DICEå¾—åˆ†ä¸Šè¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea3955297186e4df842b411482acf60c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab90921be3d6536d824ffc314cef85e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-171d5d4f172b6874793c774b233a149e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60ece2691225586beb25a8e2c6f4f76d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41fb9f7e2d78627eb472850e8d59d567.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0afd5658eb43d68ae7869831ada69b51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f53c24193c08d0cbf5eb33f4787344d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VerteNet-â€“-A-Multi-Context-Hybrid-CNN-Transformer-for-Accurate-Vertebral-Landmark-Localization-in-Lateral-Spine-DXA-Images"><a href="#VerteNet-â€“-A-Multi-Context-Hybrid-CNN-Transformer-for-Accurate-Vertebral-Landmark-Localization-in-Lateral-Spine-DXA-Images" class="headerlink" title="VerteNet â€“ A Multi-Context Hybrid CNN Transformer for Accurate   Vertebral Landmark Localization in Lateral Spine DXA Images"></a>VerteNet â€“ A Multi-Context Hybrid CNN Transformer for Accurate   Vertebral Landmark Localization in Lateral Spine DXA Images</h2><p><strong>Authors:Zaid Ilyas, Arooba Maqsood, Afsah Saleem, Erchuan Zhang, David Suter, Parminder Raina, Jonathan M. Hodgson, John T. Schousboe, William D. Leslie, Joshua R. Lewis, Syed Zulqarnain Gilani</strong></p>
<p>Lateral Spine Image (LSI) analysis is important for medical diagnosis, treatment planning, and detailed spinal health assessments. Although modalities like Computed Tomography and Digital X-ray Imaging are commonly used, Dual Energy X-ray Absorptiometry (DXA) is often preferred due to lower radiation exposure, seamless capture, and cost-effectiveness. Accurate Vertebral Landmark Localization (VLL) on LSIs is important to detect spinal conditions like kyphosis and lordosis, as well as assessing Abdominal Aortic Calcification (AAC) using Inter-Vertebral Guides (IVGs). Nonetheless, few automated VLL methodologies have concentrated on DXA LSIs. We present VerteNet, a hybrid CNN-Transformer model featuring a novel dual-resolution attention mechanism in self and cross-attention domains, referred to as Dual Resolution Self-Attention (DRSA) and Dual Resolution Cross-Attention (DRCA). These mechanisms capture the diverse frequencies in DXA images by operating at two different feature map resolutions. Additionally, we design a Multi-Context Feature Fusion Block (MCFB) that efficiently integrates the features using DRSA and DRCA. We train VerteNet on 620 DXA LSIs from various machines and achieve superior results compared to existing methods. We also design an algorithm that utilizes VerteNetâ€™s predictions in estimating the Region of Interest (ROI) to detect potential abdominal aorta cropping, where inadequate soft tissue hinders calcification assessment. Additionally, we present a small proof-of-concept study to show that IVGs generated from VLL information can improve inter-reader correlation in AAC scoring, addressing two key areas of disagreement in expert AAC-24 scoring: IVG placement and quality control for full abdominal aorta assessment. The code for this work can be found at <a target="_blank" rel="noopener" href="https://github.com/zaidilyas89/VerteNet">https://github.com/zaidilyas89/VerteNet</a>. </p>
<blockquote>
<p>ä¾§ä½è„ŠæŸ±å›¾åƒï¼ˆLSIï¼‰åˆ†æåœ¨åŒ»å­¦è¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œè¯¦ç»†çš„è„ŠæŸ±å¥åº·è¯„ä¼°ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚å°½ç®¡è®¡ç®—æœºæ–­å±‚æ‰«æå’Œæ•°å­—Xå°„çº¿æˆåƒç­‰æ¨¡å¼å¸¸ç”¨ï¼Œä½†ç”±äºè¾ƒä½çš„è¾å°„æš´éœ²ã€æ— ç¼æ•è·å’Œæˆæœ¬æ•ˆç›Šï¼ŒåŒèƒ½Xå°„çº¿å¸æ”¶æ³•ï¼ˆDXAï¼‰å¾€å¾€æ›´å—æ¬¢è¿ã€‚åœ¨LSIsä¸Šè¿›è¡Œå‡†ç¡®çš„æ¤ä½“åœ°æ ‡å®šä½ï¼ˆVLLï¼‰å¯¹äºæ£€æµ‹è„ŠæŸ±ç–¾ç—…å¦‚é©¼èƒŒå’Œè…°æ¤å‰å‡¸å¾ˆé‡è¦ï¼ŒåŒæ—¶è¿˜éœ€ä½¿ç”¨æ¤é—´æŒ‡å—ï¼ˆIVGsï¼‰è¯„ä¼°è…¹éƒ¨ä¸»åŠ¨è„‰é’™åŒ–ï¼ˆAACï¼‰ã€‚ç„¶è€Œï¼Œå¾ˆå°‘æœ‰è‡ªåŠ¨åŒ–VLLæ–¹æ³•é›†ä¸­äºDXA LSIsã€‚æˆ‘ä»¬æå‡ºäº†VerteNetï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆCNN-Transformeræ¨¡å‹ï¼Œå…·æœ‰ä¸€ç§æ–°å‹çš„åŒåˆ†è¾¨ç‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨è‡ªæˆ‘å’Œäº¤å‰æ³¨æ„åŠ›åŸŸä¸­è¢«ç§°ä¸ºåŒåˆ†è¾¨ç‡è‡ªæ³¨æ„åŠ›ï¼ˆDRSAï¼‰å’ŒåŒåˆ†è¾¨ç‡äº¤å‰æ³¨æ„åŠ›ï¼ˆDRCAï¼‰ã€‚è¿™äº›æœºåˆ¶é€šè¿‡åœ¨ä¸¤ä¸ªä¸åŒçš„ç‰¹å¾å›¾åˆ†è¾¨ç‡ä¸Šæ“ä½œæ¥æ•æ‰DXAå›¾åƒä¸­çš„ä¸åŒé¢‘ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šä¸Šä¸‹æ–‡ç‰¹å¾èåˆå—ï¼ˆMCFBï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°ç»“åˆäº†ä½¿ç”¨DRSAå’ŒDRCAçš„ç‰¹å¾ã€‚æˆ‘ä»¬å¯¹æ¥è‡ªå„ç§æœºå™¨çš„620ä¸ªDXA LSIè¿›è¡Œäº†VerteNetè®­ç»ƒï¼Œå¹¶å–å¾—äº†æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½çš„ç»“æœã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§ç®—æ³•ï¼Œåˆ©ç”¨VerteNetçš„é¢„æµ‹æ¥ä¼°è®¡æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ï¼Œä»¥æ£€æµ‹æ½œåœ¨çš„è…¹éƒ¨ä¸»åŠ¨è„‰å‰ªè£ï¼Œå…¶ä¸­è½¯ç»„ç»‡ä¸è¶³ä¼šå¦¨ç¢é’™åŒ–è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€é¡¹å°å‹æ¦‚å¿µéªŒè¯ç ”ç©¶ï¼Œä»¥è¡¨æ˜ç”±VLLä¿¡æ¯ç”Ÿæˆçš„IVGå¯ä»¥æé«˜AACè¯„åˆ†çš„è¯»è€…é—´ç›¸å…³æ€§ï¼Œè§£å†³ä¸“å®¶AAC-24è¯„åˆ†ä¸­çš„ä¸¤ä¸ªä¸»è¦åˆ†æ­§é¢†åŸŸï¼šIVGæ”¾ç½®å’Œå¯¹æ•´ä¸ªè…¹éƒ¨ä¸»åŠ¨è„‰è¯„ä¼°çš„è´¨é‡æ§åˆ¶ã€‚è¯¥å·¥ä½œçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zaidilyas89/VerteNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zaidilyas89/VerteNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02097v1">PDF</a> 10 pages with 7 figures</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒä¸­çš„ä¾§ä½è„ŠæŸ±å›¾åƒï¼ˆLSIï¼‰åˆ†æå¯¹äºåŒ»å­¦è¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œè¯¦ç»†çš„è„ŠæŸ±å¥åº·è¯„ä¼°è‡³å…³é‡è¦ã€‚è™½ç„¶è®¡ç®—æœºæ–­å±‚æ‰«æå’Œæ•°å­—Xå°„çº¿æˆåƒç­‰æ¨¡æ€æ˜¯å¸¸ç”¨çš„ï¼Œä½†ç”±äºè¾ƒä½çš„è¾å°„æš´éœ²ã€æ— ç¼æ•è·å’Œæˆæœ¬æ•ˆç›Šï¼ŒåŒèƒ½Xå°„çº¿å¸æ”¶æ³•ï¼ˆDXAï¼‰å¸¸è¢«ä¼˜å…ˆé€‰æ‹©ã€‚å‡†ç¡®çš„æ¤ä½“å®šä½ï¼ˆVLLï¼‰å¯¹äºæ£€æµ‹è„ŠæŸ±ç–¾ç—…å¦‚é©¼èƒŒå’Œè„ŠæŸ±ä¾§å‡¸ä»¥åŠè¯„ä¼°è…¹éƒ¨ä¸»åŠ¨è„‰é’™åŒ–ï¼ˆAACï¼‰éå¸¸é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆCNN-Transformeræ¨¡å‹VerteNetï¼Œå…·æœ‰æ–°å‹çš„åŒåˆ†è¾¨ç‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºDXA LSIsçš„VLLã€‚è¯¥æ¨¡å‹åœ¨å¤šç§æœºå™¨ä¸Šçš„620å¼ DXA LSIå›¾åƒä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶å–å¾—äº†ä¼˜è¶Šçš„ç»“æœã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§åˆ©ç”¨VerteNeté¢„æµ‹ä¼°è®¡æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„ç®—æ³•ï¼Œä»¥æ£€æµ‹å› è½¯ç»„ç»‡ä¸è¶³å¯¼è‡´çš„è…¹éƒ¨ä¸»åŠ¨è„‰è£å‰ªé—®é¢˜ã€‚åŒæ—¶ï¼Œæœ¬ç ”ç©¶è¿˜å±•ç¤ºäº†é€šè¿‡VLLä¿¡æ¯ç”Ÿæˆçš„IVGå¯ä»¥æé«˜AACè¯„åˆ†çš„è¯»ç‰‡é—´ç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> 1. Lateral Spine Image (LSI)åˆ†æåœ¨åŒ»å­¦è¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œè„ŠæŸ±å¥åº·è¯„ä¼°ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚
 2. è™½ç„¶å­˜åœ¨å¤šç§æˆåƒæ–¹å¼ï¼Œä½†Dual Energy X-ray Absorptiometry (DXA)å› å…¶ä½è¾å°„æš´éœ²ã€æ— ç¼æ•è·å’Œæˆæœ¬æ•ˆç›Šè€Œå¸¸è¢«é‡‡ç”¨ã€‚
 3. å‡†ç¡®çš„æ¤ä½“å®šä½ï¼ˆVLLï¼‰å¯¹äºè¯Šæ–­è„ŠæŸ±ç–¾ç—…å’Œè¯„ä¼°è…¹éƒ¨ä¸»åŠ¨è„‰é’™åŒ–ï¼ˆAACï¼‰è‡³å…³é‡è¦ã€‚
 4. æå‡ºäº†ä¸€ç§æ··åˆCNN-Transformeræ¨¡å‹VerteNetï¼Œå…·æœ‰åŒåˆ†è¾¨ç‡æ³¨æ„åŠ›æœºåˆ¶ç”¨äºDXA LSIsçš„VLLåˆ†æï¼Œå–å¾—äº†ä¼˜è¶Šçš„ç»“æœã€‚
 5. VerteNetè®¾è®¡äº†ä¸€ç§ç®—æ³•ç”¨äºä¼°è®¡æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ï¼Œä»¥è§£å†³å› è½¯ç»„ç»‡ä¸è¶³å¯¼è‡´çš„è…¹éƒ¨ä¸»åŠ¨è„‰è£å‰ªé—®é¢˜ã€‚
 6. IVGçš„ä½¿ç”¨èƒ½æé«˜AACè¯„åˆ†çš„è¯»ç‰‡é—´ç›¸å…³æ€§ï¼Œæœ‰åŠ©äºè§£å†³ä¸“å®¶è¯„ä¼°ä¸­çš„ä¸¤ä¸ªé—®é¢˜ï¼šIVGæ”¾ç½®å’Œè…¹éƒ¨ä¸»åŠ¨è„‰å…¨é¢è¯„ä¼°çš„è´¨é‡æ§åˆ¶ã€‚ 
 7. è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€åˆ†äº«äºGitHubä¸Šã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02097">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-500a46c4ed88bcd44de74660788937e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d896170acd3e553d6589911a8d941c59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6451518b1368b1d168875bf765229ddc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e978332fe46737728ff0731d29a3c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f6c5e4fd3e7028971299a8820a04486.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LEAD-Large-Foundation-Model-for-EEG-Based-Alzheimerâ€™s-Disease-Detection"><a href="#LEAD-Large-Foundation-Model-for-EEG-Based-Alzheimerâ€™s-Disease-Detection" class="headerlink" title="LEAD: Large Foundation Model for EEG-Based Alzheimerâ€™s Disease Detection"></a>LEAD: Large Foundation Model for EEG-Based Alzheimerâ€™s Disease Detection</h2><p><strong>Authors:Yihe Wang, Nan Huang, Nadia Mammone, Marco Cecchi, Xiang Zhang</strong></p>
<p>Electroencephalogram (EEG) provides a non-invasive, highly accessible, and cost-effective solution for Alzheimerâ€™s Disease (AD) detection. However, existing methods, whether based on manual feature extraction or deep learning, face two major challenges: the lack of large-scale datasets for robust feature learning and evaluation, and poor detection performance due to inter-subject variations. To address these challenges, we curate an EEG-AD corpus containing 813 subjects, which forms the worldâ€™s largest EEG-AD dataset to the best of our knowledge. Using this unique dataset, we propose LEAD, the first large foundation model for EEG-based AD detection. Our method encompasses an entire pipeline, from data selection and preprocessing to self-supervised contrastive pretraining, fine-tuning, and key setups such as subject-independent evaluation and majority voting for subject-level detection. We pre-train the model on 11 EEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervised pre-training design includes sample-level and subject-level contrasting to extract useful general EEG features. Fine-tuning is performed on 5 channel-aligned datasets together. The backbone encoder incorporates temporal and channel embeddings to capture features across both temporal and spatial dimensions. Our method demonstrates outstanding AD detection performance, achieving up to a 9.86% increase in F1 score at the sample-level and up to a 9.31% at the subject-level compared to state-of-the-art methods. The results of our model strongly confirm the effectiveness of contrastive pre-training and channel-aligned unified fine-tuning for addressing inter-subject variation. The source code is at <a target="_blank" rel="noopener" href="https://github.com/DL4mHealth/LEAD">https://github.com/DL4mHealth/LEAD</a>. </p>
<blockquote>
<p>è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¸ºé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„æ£€æµ‹æä¾›äº†ä¸€ç§éä¾µå…¥æ€§ã€æ˜“äºè·å–ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ï¼Œæ— è®ºæ˜¯åŸºäºæ‰‹åŠ¨ç‰¹å¾æå–è¿˜æ˜¯æ·±åº¦å­¦ä¹ ï¼Œéƒ½é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç¼ºä¹ç”¨äºç¨³å¥ç‰¹å¾å­¦ä¹ å’Œè¯„ä¼°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥åŠå› å—è¯•è€…é—´å·®å¼‚è€Œå¯¼è‡´çš„æ£€æµ‹æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç¼–çº‚äº†ä¸€ä¸ªåŒ…å«813åå—è¯•è€…çš„EEG-ADè¯­æ–™åº“ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ä¸–ç•Œä¸Šæœ€å¤§çš„EEG-ADæ•°æ®é›†ã€‚ä½¿ç”¨è¯¥ç‹¬ç‰¹æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†LEADâ€”â€”é¦–ä¸ªç”¨äºåŸºäºEEGçš„ADæ£€æµ‹çš„å¤§å‹åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶µç›–äº†ä»æ•°æ®é€‰æ‹©ã€é¢„å¤„ç†åˆ°è‡ªæˆ‘ç›‘ç£å¯¹æ¯”é¢„è®­ç»ƒã€å¾®è°ƒä»¥åŠä¸»ä½“çº§æ£€æµ‹çš„ç‹¬ç«‹è¯„ä¼°å’Œå¤šæŠ•ç¥¨ç­‰å…³é”®è®¾ç½®çš„æ•´ä¸ªæµç¨‹ã€‚æˆ‘ä»¬åœ¨11ä¸ªEEGæ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨5ä¸ªADæ•°æ®é›†ä¸Šè¿›è¡Œç»Ÿä¸€å¾®è°ƒã€‚æˆ‘ä»¬çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒè®¾è®¡åŒ…æ‹¬æ ·æœ¬çº§å’Œå—è¯•è€…çº§çš„å¯¹æ¯”ï¼Œä»¥æå–æœ‰ç”¨çš„é€šç”¨EEGç‰¹å¾ã€‚å¾®è°ƒæ˜¯åœ¨5ä¸ªé€šé“å¯¹é½çš„æ•°æ®é›†ä¸Šä¸€èµ·è¿›è¡Œçš„ã€‚ä¸»å¹²ç¼–ç å™¨ç»“åˆæ—¶é—´å’Œé€šé“åµŒå…¥ï¼Œä»¥æ•è·æ—¶é—´å’Œç©ºé—´ä¸¤ä¸ªç»´åº¦çš„ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ADæ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ ·æœ¬çº§åˆ«çš„F1åˆ†æ•°æé«˜äº†9.86%ï¼Œä¸»ä½“çº§åˆ«æé«˜äº†9.31%ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»“æœå¼ºçƒˆè¯æ˜äº†å¯¹æ¯”é¢„è®­ç»ƒå’Œé€šé“å¯¹é½ç»Ÿä¸€å¾®è°ƒåœ¨è§£å†³å—è¯•è€…é—´å·®å¼‚æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/DL4mHealth/LEAD%E3%80%82">https://github.com/DL4mHealth/LEADã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01678v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åˆ©ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†å’Œä¸»ä½“é—´å·®å¼‚å¯¼è‡´çš„æ£€æµ‹æ€§èƒ½ä¸ä½³ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«813åå—è¯•è€…çš„EEG-ADæ•°æ®é›†ï¼Œå¹¶åŸºäºæ­¤æå‡ºäº†é¦–ä¸ªç”¨äºEEG-ADæ£€æµ‹çš„LEADå¤§å‹åŸºç¡€æ¨¡å‹ã€‚é€šè¿‡æ ·æœ¬çº§åˆ«å’Œä¸»ä½“çº§åˆ«çš„å¯¹æ¯”è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå®ç°äº†å‡ºè‰²çš„ADæ£€æµ‹æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒF1å¾—åˆ†æœ€é«˜å¯æé«˜9.86%ï¼ˆæ ·æœ¬çº§åˆ«ï¼‰å’Œ9.31%ï¼ˆä¸»ä½“çº§åˆ«ï¼‰ã€‚è¯¥æ¨¡å‹å¯æœ‰æ•ˆåœ°å¤„ç†ä¸»ä½“é—´çš„å·®å¼‚ï¼Œå¹¶å…·æœ‰æ½œåŠ›æ¨åŠ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„å‘å±•ã€‚ç›¸å…³æºä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³é”®è§‚ç‚¹çš„ç²¾ç®€æ‘˜è¦ï¼š</p>
<ul>
<li>è¯¥ç ”ç©¶é€šè¿‡æ„å»ºåŒ…å«å¤§é‡å—è¯•è€…çš„EEG-ADæ•°æ®é›†è§£å†³äº†ç°æœ‰EEGæ£€æµ‹ADæ–¹æ³•çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºLEADæ¨¡å‹ï¼Œå®ç°äº†EEGä¿¡å·çš„å…¨é¢å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®é€‰æ‹©ã€é¢„å¤„ç†ã€è‡ªç›‘ç£å¯¹æ¯”é¢„è®­ç»ƒç­‰ã€‚</li>
<li>è‡ªç›‘ç£é¢„è®­ç»ƒè®¾è®¡åŒ…å«æ ·æœ¬çº§åˆ«å’Œä¸»ä½“çº§åˆ«çš„å¯¹æ¯”ï¼Œæœ‰æ•ˆæå–é€šç”¨EEGç‰¹å¾ã€‚</li>
<li>ç»Ÿä¸€å¾®è°ƒæŠ€æœ¯æé«˜äº†æ¨¡å‹åœ¨äº”ä¸ªADæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚é‡‡ç”¨åç«¯ç¼–ç å™¨å®ç°æ—¶é—´åµŒå…¥å’Œé€šé“åµŒå…¥çš„èåˆå¤„ç†ï¼Œä¼˜åŒ–äº†æ—¶é—´ã€ç©ºé—´ç»´åº¦ç‰¹å¾æå–ã€‚</li>
<li>LEADæ¨¡å‹åœ¨ADæ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æ˜¾è‘—æé«˜F1å¾—åˆ†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-157d2bad57213c513ea753d9e1cfc077.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-273fcbb0b316691e801d4464deffeb68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39f61a3c6ff6fd15b509d8b00c376866.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Efficient-Brain-Tumor-Classification-with-Lightweight-CNN-Architecture-A-Novel-Approach"><a href="#Efficient-Brain-Tumor-Classification-with-Lightweight-CNN-Architecture-A-Novel-Approach" class="headerlink" title="Efficient Brain Tumor Classification with Lightweight CNN Architecture:   A Novel Approach"></a>Efficient Brain Tumor Classification with Lightweight CNN Architecture:   A Novel Approach</h2><p><strong>Authors:Priyam Ganguly, Akhilbaran Ghosh</strong></p>
<p>Brain tumor classification using MRI images is critical in medical diagnostics, where early and accurate detection significantly impacts patient outcomes. While recent advancements in deep learning (DL), particularly CNNs, have shown promise, many models struggle with balancing accuracy and computational efficiency and often lack robustness across diverse datasets. To address these challenges, we propose a novel model architecture integrating separable convolutions and squeeze and excitation (SE) blocks, designed to enhance feature extraction while maintaining computational efficiency. Our model further incorporates batch normalization and dropout to prevent overfitting, ensuring stable and reliable performance. The proposed model is lightweight because it uses separable convolutions, which reduce the number of parameters, and incorporates global average pooling instead of fully connected layers to minimize computational complexity while maintaining high accuracy. Our model does better than other models by about 0.5% to 1.0% in accuracy and 1.5% to 2.5% in loss reduction, as shown by many experiments. It has a validation accuracy of 99.22% and a test accuracy of 98.44%. These results highlight the modelâ€™s ability to generalize effectively across different brain tumour types, offering a robust tools for clinical applications. Our work sets a new benchmark in the field, providing a foundation for future research in optimizing the accuracy and efficiency of DL models for medical image analysis. </p>
<blockquote>
<p>ä½¿ç”¨MRIå›¾åƒå¯¹è„‘è‚¿ç˜¤è¿›è¡Œåˆ†ç±»åœ¨åŒ»å­¦è¯Šæ–­ä¸­è‡³å…³é‡è¦ï¼Œæ—©æœŸå’Œå‡†ç¡®çš„æ£€æµ‹å¯¹æ‚£è€…çš„æ²»ç–—æ•ˆæœå…·æœ‰é‡å¤§å½±å“ã€‚è™½ç„¶æœ€è¿‘æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ï¼Œä½†è®¸å¤šæ¨¡å‹åœ¨å¹³è¡¡å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ•°æ®é›†ä¹‹é—´ç¼ºä¹ç¨³å¥æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ¨¡å‹æ¶æ„ï¼Œèåˆäº†å¯åˆ†ç¦»å·ç§¯å’ŒæŒ¤å‹æ¿€å‘ï¼ˆSEï¼‰å—ï¼Œæ—¨åœ¨æé«˜ç‰¹å¾æå–èƒ½åŠ›çš„åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹è¿˜ç»“åˆäº†æ‰¹é‡å½’ä¸€åŒ–å’Œä¸¢å¼ƒæ³•ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œç¡®ä¿ç¨³å®šå’Œå¯é çš„æ€§èƒ½ã€‚æ‰€æå‡ºæ¨¡å‹çš„é‡é‡è¾ƒè½»ï¼Œå› ä¸ºå®ƒä½¿ç”¨äº†å¯åˆ†ç¦»å·ç§¯ï¼Œå‡å°‘äº†å‚æ•°æ•°é‡ï¼Œå¹¶é‡‡ç”¨äº†å…¨å±€å¹³å‡æ± åŒ–è€Œä¸æ˜¯å…¨è¿æ¥å±‚ï¼Œä»¥æœ€å°åŒ–è®¡ç®—å¤æ‚æ€§åŒæ—¶ä¿æŒé«˜å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸Šæ¯”å…¶ä»–æ¨¡å‹é«˜å‡ºçº¦0.5%è‡³1%ï¼Œåœ¨æŸå¤±å‡å°‘æ–¹é¢é«˜å‡ºçº¦1.5%è‡³2.5%ï¼Œè¿™äº›ç»“æœç»è¿‡å¤šæ¬¡å®éªŒéªŒè¯ã€‚å®ƒå…·æœ‰99.22%çš„éªŒè¯å‡†ç¡®ç‡å’Œ98.44%çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœçªæ˜¾äº†è¯¥æ¨¡å‹åœ¨ä¸åŒè„‘è‚¿ç˜¤ç±»å‹ä¹‹é—´çš„æœ‰æ•ˆæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºä¸´åºŠåº”ç”¨æä¾›äº†ç¨³å¥çš„å·¥å…·ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè¯¥é¢†åŸŸè®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œä¸ºä¼˜åŒ–åŒ»å­¦å›¾åƒåˆ†ææ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡æä¾›äº†ç ”ç©¶åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01674v1">PDF</a> Accepted in FMLDS 2024</p>
<p><strong>Summary</strong><br>     åˆ©ç”¨MRIå›¾åƒè¿›è¡Œè„‘è‚¿ç˜¤åˆ†ç±»æ˜¯åŒ»å­¦è¯Šæ–­ä¸­çš„å…³é”®ï¼Œæ—©æœŸå‡†ç¡®æ£€æµ‹å¯¹ç—…äººé¢„åæœ‰é‡è¦å½±å“ã€‚é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¹³è¡¡å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹æ¶æ„ï¼Œé›†æˆå¯åˆ†å·ç§¯å’ŒæŒ¤å‹æ¿€å‘ï¼ˆSEï¼‰å—ï¼Œæ—¨åœ¨å¢å¼ºç‰¹å¾æå–çš„åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚æ¨¡å‹è¿˜èå…¥æ‰¹é‡å½’ä¸€åŒ–å’Œdropoutä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œç¡®ä¿ç¨³å®šå’Œå¯é çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸Šæé«˜äº†çº¦0.5%~1.0%ï¼Œåœ¨æŸå¤±å‡å°‘ä¸Šæé«˜äº†çº¦1.5%~2.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤åˆ†ç±»å¯¹åŒ»å­¦è¯Šæ–­å’Œæ‚£è€…é¢„åè‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ï¼Œå°¤å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¹³è¡¡å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„æ–°å‹æ¨¡å‹æ¶æ„é›†æˆå¯åˆ†å·ç§¯å’ŒæŒ¤å‹æ¿€å‘ï¼ˆSEï¼‰å—ï¼Œä»¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›å¹¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ‰¹é‡å½’ä¸€åŒ–å’ŒdropoutæŠ€æœ¯ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œç¡®ä¿ç¨³å®šå’Œå¯é æ€§èƒ½ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å‡†ç¡®æ€§å’ŒæŸå¤±å‡å°‘æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5263e59177d6d128c05e2574fcbdb810.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-330ec3c9653c217b6becc637fbe2fae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f3971bc2ef1ce9bd179b297d6b4a7ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-109535d5fd7d3b683ef04412c4ba9091.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-301e5a4b51c9f74c12e9c88dc70a5543.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multi-Resolution-SAR-and-Optical-Remote-Sensing-Image-Registration-Methods-A-Review-Datasets-and-Future-Perspectives"><a href="#Multi-Resolution-SAR-and-Optical-Remote-Sensing-Image-Registration-Methods-A-Review-Datasets-and-Future-Perspectives" class="headerlink" title="Multi-Resolution SAR and Optical Remote Sensing Image Registration   Methods: A Review, Datasets, and Future Perspectives"></a>Multi-Resolution SAR and Optical Remote Sensing Image Registration   Methods: A Review, Datasets, and Future Perspectives</h2><p><strong>Authors:Wenfei Zhang, Ruipeng Zhao, Yongxiang Yao, Yi Wan, Peihao Wu, Jiayuan Li, Yansheng Li, Yongjun Zhang</strong></p>
<p>Synthetic Aperture Radar (SAR) and optical image registration is essential for remote sensing data fusion, with applications in military reconnaissance, environmental monitoring, and disaster management. However, challenges arise from differences in imaging mechanisms, geometric distortions, and radiometric properties between SAR and optical images. As image resolution increases, fine SAR textures become more significant, leading to alignment issues and 3D spatial discrepancies. Two major gaps exist: the lack of a publicly available multi-resolution, multi-scene registration dataset and the absence of systematic analysis of current methods. To address this, the MultiResSAR dataset was created, containing over 10k pairs of multi-source, multi-resolution, and multi-scene SAR and optical images. Sixteen state-of-the-art algorithms were tested. Results show no algorithm achieves 100% success, and performance decreases as resolution increases, with most failing on sub-meter data. XoFTR performs best among deep learning methods (40.58%), while RIFT performs best among traditional methods (66.51%). Future research should focus on noise suppression, 3D geometric fusion, cross-view transformation modeling, and deep learning optimization for robust registration of high-resolution SAR and optical images. The dataset is available at <a target="_blank" rel="noopener" href="https://github.com/betterlll/Multi-Resolution-SAR-dataset-">https://github.com/betterlll/Multi-Resolution-SAR-dataset-</a>. </p>
<blockquote>
<p>åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å’Œå…‰å­¦å›¾åƒé…å‡†å¯¹äºé¥æ„Ÿæ•°æ®èåˆè‡³å…³é‡è¦ï¼Œåœ¨å†›äº‹ä¾¦å¯Ÿã€ç¯å¢ƒç›‘æµ‹å’Œç¾å®³ç®¡ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºSARå’Œå…‰å­¦å›¾åƒåœ¨æˆåƒæœºåˆ¶ã€å‡ ä½•å¤±çœŸå’Œè¾å°„ç‰¹æ€§ç­‰æ–¹é¢çš„å·®å¼‚ï¼Œå¸¦æ¥äº†æŒ‘æˆ˜ã€‚éšç€å›¾åƒåˆ†è¾¨ç‡çš„æé«˜ï¼ŒSARçš„ç²¾ç»†çº¹ç†å˜å¾—æ›´åŠ é‡è¦ï¼Œå¯¼è‡´å¯¹é½é—®é¢˜å’Œ3Dç©ºé—´å·®å¼‚ã€‚ç›®å‰å­˜åœ¨ä¸¤ä¸ªä¸»è¦ç©ºç™½ï¼šç¼ºä¹å¯å…¬å¼€è®¿é—®çš„å¤šåˆ†è¾¨ç‡ã€å¤šåœºæ™¯é…å‡†æ•°æ®é›†ï¼Œä»¥åŠç¼ºä¹å¯¹å½“å‰æ–¹æ³•çš„ç³»ç»Ÿåˆ†æã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œåˆ›å»ºäº†MultiResSARæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡1håƒå¯¹å¤šæºã€å¤šåˆ†è¾¨ç‡ã€å¤šåœºæ™¯çš„SARå’Œå…‰å­¦å›¾åƒã€‚å¯¹åå…­ç§æœ€å…ˆè¿›ç®—æ³•è¿›è¡Œäº†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰ç®—æ³•èƒ½è¾¾åˆ°ç™¾åˆ†ä¹‹ç™¾çš„æˆåŠŸç‡ï¼Œéšç€åˆ†è¾¨ç‡çš„æé«˜ï¼Œæ€§èƒ½ä¸‹é™ï¼Œå¤§å¤šæ•°ç®—æ³•åœ¨äºšç±³çº§æ•°æ®ä¸Šå¤±è´¥ã€‚åœ¨æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸­ï¼ŒXoFTRè¡¨ç°æœ€ä½³ï¼ˆ40.58%ï¼‰ï¼Œè€Œåœ¨ä¼ ç»Ÿæ–¹æ³•ä¸­ï¼ŒRIFTè¡¨ç°æœ€ä½³ï¼ˆ66.51%ï¼‰ã€‚æœªæ¥çš„ç ”ç©¶åº”ä¸“æ³¨äºå™ªå£°æŠ‘åˆ¶ã€3Då‡ ä½•èåˆã€è·¨è§†å›¾è½¬æ¢å»ºæ¨¡ä»¥åŠæ·±åº¦å­¦ä¹ ä¼˜åŒ–ï¼Œä»¥å®ç°é«˜åˆ†è¾¨ç‡SARå’Œå…‰å­¦å›¾åƒçš„ç¨³å¥é…å‡†ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/betterlll/Multi-Resolution-SAR-dataset-%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/betterlll/Multi-Resolution-SAR-dataset-ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01002v1">PDF</a> 48 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å’Œå…‰å­¦å›¾åƒé…å‡†åœ¨é¥æ„Ÿæ•°æ®èåˆä¸­çš„å…³é”®ä½œç”¨ï¼Œä»¥åŠå…¶åœ¨å†›äº‹ä¾¦å¯Ÿã€ç¯å¢ƒç›‘æµ‹å’Œç¾å®³ç®¡ç†ä¸­çš„åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºäº†SARå’Œå…‰å­¦å›¾åƒä¹‹é—´çš„å·®å¼‚æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¦‚æˆåƒæœºåˆ¶ã€å‡ ä½•å¤±çœŸå’Œè¾å°„ç‰¹æ€§ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œåˆ›å»ºäº†MultiResSARæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10kå¯¹çš„SARå’Œå…‰å­¦å›¾åƒã€‚æµ‹è¯•äº†16ç§æœ€æ–°ç®—æ³•ï¼Œç»“æœæ˜¾ç¤ºæ²¡æœ‰ç®—æ³•è¾¾åˆ°100%çš„æˆåŠŸç‡ï¼Œéšç€åˆ†è¾¨ç‡çš„æé«˜ï¼Œæ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨äºšç±³çº§æ•°æ®ä¸Šå¤§å¤šæ•°ç®—æ³•å¤±è´¥ã€‚æœ€å¥½çš„æ·±åº¦å­¦ä¹ æ–¹æ³•æ˜¯XoFTRï¼ˆ40.58%ï¼‰ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•ä¸­RIFTè¡¨ç°æœ€ä½³ï¼ˆ66.51%ï¼‰ã€‚æœªæ¥ç ”ç©¶åº”å…³æ³¨å™ªå£°æŠ‘åˆ¶ã€3Då‡ ä½•èåˆã€è·¨è§†å›¾è½¬æ¢å»ºæ¨¡ä»¥åŠæ·±åº¦å­¦ä¹ ä¼˜åŒ–ï¼Œä»¥å®ç°é«˜åˆ†è¾¨ç‡SARå’Œå…‰å­¦å›¾åƒçš„ç¨³å¥é…å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SARå’Œå…‰å­¦å›¾åƒé…å‡†å¯¹äºé¥æ„Ÿæ•°æ®èåˆè‡³å…³é‡è¦ï¼Œåœ¨å†›äº‹ä¾¦å¯Ÿã€ç¯å¢ƒç›‘æµ‹å’Œç¾å®³ç®¡ç†ä¸­å…·æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>SARå’Œå…‰å­¦å›¾åƒä¹‹é—´çš„å·®å¼‚å¯¼è‡´é…å‡†æŒ‘æˆ˜ï¼Œå¦‚æˆåƒæœºåˆ¶ã€å‡ ä½•å¤±çœŸå’Œè¾å°„ç‰¹æ€§ã€‚</li>
<li>ç¼ºå°‘å¤šåˆ†è¾¨ç‡ã€å¤šåœºæ™¯æ³¨å†Œæ•°æ®é›†ä»¥åŠå¯¹å½“å‰æ–¹æ³•çš„ç³»ç»Ÿåˆ†ææ˜¯ç°æœ‰çš„ä¸»è¦é—®é¢˜ã€‚</li>
<li>åˆ›å»ºäº†MultiResSARæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10kå¯¹çš„SARå’Œå…‰å­¦å›¾åƒã€‚</li>
<li>æµ‹è¯•äº†16ç§æœ€æ–°ç®—æ³•ï¼Œæ²¡æœ‰ä¸€ç§è¾¾åˆ°100%çš„æˆåŠŸç‡ã€‚</li>
<li>éšç€å›¾åƒåˆ†è¾¨ç‡çš„æé«˜ï¼Œé…å‡†æ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨äºšç±³çº§æ•°æ®ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6848dfb2e7886d945975edc09973ac80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2adcc12a358abf5753ada302033a37a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-802ec2d9aa1a09fcce9bd0653a1b4f68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fbf3b7cac5d8f5e192c18d06284601c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adapting-Foundation-Models-for-Few-Shot-Medical-Image-Segmentation-Actively-and-Sequentially"><a href="#Adapting-Foundation-Models-for-Few-Shot-Medical-Image-Segmentation-Actively-and-Sequentially" class="headerlink" title="Adapting Foundation Models for Few-Shot Medical Image Segmentation:   Actively and Sequentially"></a>Adapting Foundation Models for Few-Shot Medical Image Segmentation:   Actively and Sequentially</h2><p><strong>Authors:Jingyun Yang, Guoqing Zhang, Jingge Wang, Yang Li</strong></p>
<p>Recent advances in foundation models have brought promising results in computer vision, including medical image segmentation. Fine-tuning foundation models on specific low-resource medical tasks has become a standard practice. However, ensuring reliable and robust model adaptation when the target task has a large domain gap and few annotated samples remains a challenge. Previous few-shot domain adaptation (FSDA) methods seek to bridge the distribution gap between source and target domains by utilizing auxiliary data. The selection and scheduling of auxiliaries are often based on heuristics, which can easily cause negative transfer. In this work, we propose an Active and Sequential domain AdaPtation (ASAP) framework for dynamic auxiliary dataset selection in FSDA. We formulate FSDA as a multi-armed bandit problem and derive an efficient reward function to prioritize training on auxiliary datasets that align closely with the target task, through a single-round fine-tuning. Empirical validation on diverse medical segmentation datasets demonstrates that our method achieves favorable segmentation performance, significantly outperforming the state-of-the-art FSDA methods, achieving an average gain of 27.75% on MRI and 7.52% on CT datasets in Dice score. Code is available at the git repository: <a target="_blank" rel="noopener" href="https://github.com/techicoco/ASAP">https://github.com/techicoco/ASAP</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„è¿›å±•ä¸ºåŸºäºæ¨¡å‹çš„è®¡ç®—æœºè§†è§‰é¢†åŸŸå¸¦æ¥äº†æœ‰å¸Œæœ›çš„ç»“æœï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚å¯¹ç‰¹å®šä½èµ„æºåŒ»å­¦ä»»åŠ¡è¿›è¡Œå¾®è°ƒåŸºç¡€æ¨¡å‹å·²ç»æˆä¸ºä¸€ç§æ ‡å‡†åšæ³•ã€‚ç„¶è€Œï¼Œå½“ç›®æ ‡ä»»åŠ¡å…·æœ‰è¾ƒå¤§çš„é¢†åŸŸå·®è·å’Œè¾ƒå°‘çš„æ ‡æ³¨æ ·æœ¬æ—¶ï¼Œç¡®ä¿å¯é å’Œç¨³å¥çš„æ¨¡å‹é€‚åº”ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¹‹å‰çš„å°‘æ ·æœ¬åŸŸè‡ªé€‚åº”ï¼ˆFSDAï¼‰æ–¹æ³•è¯•å›¾é€šè¿‡åˆ©ç”¨è¾…åŠ©æ•°æ®æ¥ç¼©å°æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„åˆ†å¸ƒå·®è·ã€‚è¾…åŠ©æ•°æ®çš„é€‰æ‹©å’Œè°ƒåº¦é€šå¸¸åŸºäºå¯å‘å¼æ–¹æ³•ï¼Œè¿™å¾ˆå®¹æ˜“å¯¼è‡´è´Ÿè¿ç§»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºFSDAä¸­åŠ¨æ€è¾…åŠ©æ•°æ®é›†é€‰æ‹©çš„ä¸»åŠ¨åºè´¯åŸŸé€‚åº”ï¼ˆASAPï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬å°†FSDAåˆ¶å®šä¸ºå¤šè‡‚è€è™æœºé—®é¢˜ï¼Œå¹¶åˆ¶å®šäº†æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°ï¼Œä»¥ä¼˜å…ˆè®­ç»ƒä¸ç›®æ ‡ä»»åŠ¡ç´§å¯†ç›¸å…³çš„è¾…åŠ©æ•°æ®é›†ï¼Œé€šè¿‡ä¸€è½®å¾®è°ƒå®ç°è¿™ä¸€ç›®æ ‡ã€‚åœ¨å¤šæ ·åŒ–çš„åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„ç»éªŒéªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ‰åˆ©çš„åˆ†å‰²æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°çš„FSDAæ–¹æ³•ï¼Œåœ¨MRIå’ŒCTæ•°æ®é›†ä¸Šçš„Diceå¾—åˆ†å¹³å‡æé«˜äº†27.75%å’Œ7.52%ã€‚ä»£ç å¯åœ¨gitä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/techicoco/ASAP%E3%80%82">https://github.com/techicoco/ASAPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01000v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶äº†åŸºäºè®¡ç®—æœºè§†è§‰çš„åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„åŸŸé€‚åº”é—®é¢˜ã€‚é’ˆå¯¹å…·æœ‰å¤§é‡åŸŸé—´éš™å’Œå°‘é‡æ ‡æ³¨æ ·æœ¬çš„ç›®æ ‡ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä¸»åŠ¨åºè´¯åŸŸé€‚åº”ï¼ˆASAPï¼‰æ¡†æ¶è¿›è¡ŒåŠ¨æ€è¾…åŠ©æ•°æ®é›†é€‰æ‹©ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ¶å®šå¤šè‡‚è€è™æœºé—®é¢˜æ¥åŠ¨æ€é€‰æ‹©è¾…åŠ©æ•°æ®é›†ï¼Œå¹¶é€šè¿‡å•è½®å¾®è°ƒå®ç°ä¸ç›®æ ‡ä»»åŠ¡ç´§å¯†å¯¹é½çš„è¾…åŠ©è®­ç»ƒï¼Œå®ç°äº†è¾ƒå¥½çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ€§èƒ½ã€‚è¯¥æ–¹æ³•å¹³å‡æå‡äº†MRIæ•°æ®é›†çš„Diceåˆ†æ•°è¾¾åˆ°27.75%ï¼Œå¹¶æé«˜äº†CTæ•°æ®é›†çš„Diceåˆ†æ•°è¾¾åˆ°7.52%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡çš„åŸŸé€‚åº”é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä¸»åŠ¨åºè´¯åŸŸé€‚åº”ï¼ˆASAPï¼‰æ¡†æ¶è¿›è¡ŒåŠ¨æ€è¾…åŠ©æ•°æ®é›†é€‰æ‹©ã€‚</li>
<li>å°†é—®é¢˜å»ºæ¨¡ä¸ºå¤šè‡‚è€è™æœºé—®é¢˜ï¼Œåˆ¶å®šé«˜æ•ˆå¥–åŠ±å‡½æ•°é€‰æ‹©ä¸ç›®æ ‡ä»»åŠ¡å¯¹é½çš„è¾…åŠ©æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å•è½®å¾®è°ƒå®ç°äº†ä¸ç›®æ ‡ä»»åŠ¡ç´§å¯†å¯¹é½çš„è¾…åŠ©è®­ç»ƒã€‚</li>
<li>åœ¨å¤šæ ·çš„åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®è¯ç ”ç©¶ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº†è¾ƒå¥½çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ€§èƒ½ï¼Œå¹³å‡æå‡äº†MRIæ•°æ®é›†çš„Diceåˆ†æ•°è¾¾åˆ°27.75%ï¼Œå¹¶æé«˜äº†CTæ•°æ®é›†çš„Diceåˆ†æ•°è¾¾åˆ°7.52%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-77c603db060814eaf9ad257b20c2c345.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89b0b6b74805eb25ac43ab1065640119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53e99a612b943d39cdf14c613335ec94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90a3a21de93eebcf344c8b734f827b9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12e021bed4a914256af45bb32af29753.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Registration-Enhanced-Segmentation-Method-for-Prostate-Cancer-in-Ultrasound-Images"><a href="#Registration-Enhanced-Segmentation-Method-for-Prostate-Cancer-in-Ultrasound-Images" class="headerlink" title="Registration-Enhanced Segmentation Method for Prostate Cancer in   Ultrasound Images"></a>Registration-Enhanced Segmentation Method for Prostate Cancer in   Ultrasound Images</h2><p><strong>Authors:Shengtian Sang, Hassan Jahanandish, Cynthia Xinran Li, Indrani Bhattachary, Jeong Hoon Lee, Lichun Zhang, Sulaiman Vesal, Pejman Ghanouni, Richard Fan, Geoffrey A. Sonn, Mirabela Rusu</strong></p>
<p>Prostate cancer is a major cause of cancer-related deaths in men, where early detection greatly improves survival rates. Although MRI-TRUS fusion biopsy offers superior accuracy by combining MRIâ€™s detailed visualization with TRUSâ€™s real-time guidance, it is a complex and time-intensive procedure that relies heavily on manual annotations, leading to potential errors. To address these challenges, we propose a fully automatic MRI-TRUS fusion-based segmentation method that identifies prostate tumors directly in TRUS images without requiring manual annotations. Unlike traditional multimodal fusion approaches that rely on naive data concatenation, our method integrates a registration-segmentation framework to align and leverage spatial information between MRI and TRUS modalities. This alignment enhances segmentation accuracy and reduces reliance on manual effort. Our approach was validated on a dataset of 1,747 patients from Stanford Hospital, achieving an average Dice coefficient of 0.212, outperforming TRUS-only (0.117) and naive MRI-TRUS fusion (0.132) methods, with significant improvements (p $&lt;$ 0.01). This framework demonstrates the potential for reducing the complexity of prostate cancer diagnosis and provides a flexible architecture applicable to other multimodal medical imaging tasks. </p>
<blockquote>
<p>å‰åˆ—è…ºç™Œæ˜¯ç”·æ€§ç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œæ—©æœŸå‘ç°å¯ä»¥å¤§å¤§æé«˜å­˜æ´»ç‡ã€‚è™½ç„¶MRI-TRUSèåˆæ´»æ£€é€šè¿‡ç»“åˆMRIçš„è¯¦ç»†å¯è§†åŒ–å’ŒTRUSçš„å®æ—¶æŒ‡å¯¼æä¾›äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªå¤æ‚ä¸”è€—æ—¶çš„ç¨‹åºï¼Œä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨æ³¨é‡Šï¼Œä»è€Œå¯¼è‡´æ½œåœ¨è¯¯å·®ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå…¨è‡ªåŠ¨MRI-TRUSèåˆçš„åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ç›´æ¥åœ¨TRUSå›¾åƒä¸­è¯†åˆ«å‰åˆ—è…ºè‚¿ç˜¤ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šã€‚ä¸ä¼ ç»Ÿçš„å¤šæ¨¡æ€èåˆæ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå¹¼ç¨šçš„æ•°æ®æ‹¼æ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†æ³¨å†Œåˆ†å‰²æ¡†æ¶ï¼Œä»¥å¯¹é½å’Œåˆ©ç”¨MRIå’ŒTRUSæ¨¡æ€ä¹‹é—´çš„ç©ºé—´ä¿¡æ¯ã€‚è¿™ç§å¯¹é½æé«˜äº†åˆ†å‰²ç²¾åº¦ï¼Œå¹¶å‡å°‘äº†å¯¹æ‰‹åŠ¨æ“ä½œçš„ä¾èµ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•å·²åœ¨æ–¯å¦ç¦åŒ»é™¢çš„1747ä¾‹æ‚£è€…æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹³å‡Diceç³»æ•°ä¸º0.212ï¼Œä¼˜äºä»…ä½¿ç”¨TRUSï¼ˆ0.117ï¼‰å’Œç®€å•MRI-TRUSèåˆï¼ˆ0.132ï¼‰çš„æ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—æ”¹è¿›ï¼ˆp&lt;0.01ï¼‰ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†é™ä½å‰åˆ—è…ºç™Œè¯Šæ–­å¤æ‚æ€§çš„æ½œåŠ›ï¼Œå¹¶æä¾›äº†é€‚ç”¨äºå…¶ä»–å¤šæ¨¡æ€åŒ»å­¦æˆåƒä»»åŠ¡çš„çµæ´»æ¶æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00712v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å…¨è‡ªåŠ¨MRI-TRUSèåˆåˆ†å‰²æ–¹æ³•ï¼Œç”¨äºç›´æ¥è¯†åˆ«TRUSå›¾åƒä¸­çš„å‰åˆ—è…ºè‚¿ç˜¤ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šã€‚è¯¥æ–¹æ³•é€šè¿‡æ³¨å†Œåˆ†å‰²æ¡†æ¶å®ç°å¯¹MRIå’ŒTRUSæ¨¡æ€çš„ç©ºé—´ä¿¡æ¯å¯¹é½å’Œåˆ©ç”¨ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ï¼Œå‡å°‘å¯¹äººå·¥æ“ä½œçš„ä¾èµ–ã€‚åœ¨StanfordåŒ»é™¢çš„æ‚£è€…æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå¹³å‡Diceç³»æ•°ä¸º0.212ï¼Œä¼˜äºä»…ä½¿ç”¨TRUSï¼ˆ0.117ï¼‰å’Œç®€å•çš„MRI-TRUSèåˆæ–¹æ³•ï¼ˆ0.132ï¼‰ï¼Œå…·æœ‰æ˜¾è‘—çš„æ”¹è¿›ï¼ˆp &lt; 0.01ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶é’ˆå¯¹å‰åˆ—è…ºç™Œè¯Šæ–­ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨MRI-TRUSèåˆåˆ†å‰²æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨TRUSå›¾åƒä¸­ç›´æ¥è¯†åˆ«å‰åˆ—è…ºè‚¿ç˜¤ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šã€‚</li>
<li>é€šè¿‡æ³¨å†Œåˆ†å‰²æ¡†æ¶å®ç°å¯¹MRIå’ŒTRUSæ¨¡æ€çš„ç©ºé—´ä¿¡æ¯å¯¹é½å’Œåˆ©ç”¨ã€‚</li>
<li>æ–¹æ³•åœ¨StanfordåŒ»é™¢çš„æ‚£è€…æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹³å‡Diceç³»æ•°ä¸º0.212ã€‚</li>
<li>ä¸ä»…ä½¿ç”¨TRUSå’Œç®€å•çš„MRI-TRUSèåˆæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´é«˜çš„åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„æ”¹è¿›æ•ˆæœï¼ˆp &lt; 0.01ï¼‰ï¼Œä¸ºå‡å°‘å‰åˆ—è…ºç™Œè¯Šæ–­çš„å¤æ‚æ€§æä¾›äº†æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ef530541997a3572ebb0e25b7a49da00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe80a1fad0be4044df76024d2ae8b99f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d689040bce0a4ff2c6e7ad97ee3e9fc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Self-Prompt-SAM-Medical-Image-Segmentation-via-Automatic-Prompt-SAM-Adaptation"><a href="#Self-Prompt-SAM-Medical-Image-Segmentation-via-Automatic-Prompt-SAM-Adaptation" class="headerlink" title="Self-Prompt SAM: Medical Image Segmentation via Automatic Prompt SAM   Adaptation"></a>Self-Prompt SAM: Medical Image Segmentation via Automatic Prompt SAM   Adaptation</h2><p><strong>Authors:Bin Xie, Hao Tang, Dawen Cai, Yan Yan, Gady Agam</strong></p>
<p>Segment Anything Model (SAM) has demonstrated impressive zero-shot performance and brought a range of unexplored capabilities to natural image segmentation tasks. However, as a very important branch of image segmentation, the performance of SAM remains uncertain when applied to medical image segmentation due to the significant differences between natural images and medical images. Meanwhile, it is harsh to meet the SAMâ€™s requirements of extra prompts provided, such as points or boxes to specify medical regions. In this paper, we propose a novel self-prompt SAM adaptation framework for medical image segmentation, named Self-Prompt-SAM. We design a multi-scale prompt generator combined with the image encoder in SAM to generate auxiliary masks. Then, we use the auxiliary masks to generate bounding boxes as box prompts and use Distance Transform to select the most central points as point prompts. Meanwhile, we design a 3D depth-fused adapter (DfusedAdapter) and inject the DFusedAdapter into each transformer in the image encoder and mask decoder to enable pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images. Extensive experiments demonstrate that our method achieves state-of-the-art performance and outperforms nnUNet by 2.3% on AMOS2022, 1.6% on ACDCand 0.5% on Synapse datasets. </p>
<blockquote>
<p>Segment Anything Modelï¼ˆSAMï¼‰å·²ç»å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹¶ä¸ºè‡ªç„¶å›¾åƒåˆ†å‰²ä»»åŠ¡å¸¦æ¥äº†ä¸€ç³»åˆ—æœªæ¢ç´¢çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä½œä¸ºå›¾åƒåˆ†å‰²çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼ŒSAMåœ¨åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²æ—¶çš„è¡¨ç°ä»ç„¶ä¸ç¡®å®šï¼Œå› ä¸ºè‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åŒæ—¶ï¼Œæ»¡è¶³SAMå¯¹é¢å¤–æç¤ºçš„è¦æ±‚æ˜¯å¾ˆä¸¥æ ¼çš„ï¼Œå¦‚ç”¨äºæŒ‡å®šåŒ»ç–—åŒºåŸŸçš„ç‚¹æˆ–æ¡†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹Self-Prompt-SAMè‡ªæç¤ºSAMé€‚åº”æ¡†æ¶ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦æç¤ºç”Ÿæˆå™¨ï¼Œç»“åˆSAMä¸­çš„å›¾åƒç¼–ç å™¨ç”Ÿæˆè¾…åŠ©æ©ç ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¾…åŠ©æ©ç ç”Ÿæˆè¾¹ç•Œæ¡†ä½œä¸ºæ¡†æç¤ºï¼Œå¹¶ä½¿ç”¨è·ç¦»å˜æ¢é€‰æ‹©æœ€ä¸­å¿ƒç‚¹ä½œä¸ºç‚¹æç¤ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ª3Dæ·±åº¦èåˆé€‚é…å™¨ï¼ˆDfusedAdapterï¼‰ï¼Œå¹¶å°†å…¶æ³¨å…¥å›¾åƒç¼–ç å™¨å’Œæ©ç è§£ç å™¨ä¸­çš„æ¯ä¸ªè½¬æ¢å™¨ï¼Œä½¿é¢„è®­ç»ƒçš„2D SAMæ¨¡å‹èƒ½å¤Ÿæå–3Dä¿¡æ¯å¹¶é€‚åº”3DåŒ»å­¦å›¾åƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œåœ¨AMOS2022æ•°æ®é›†ä¸Šæ¯”nnUNeté«˜å‡º2.3%ï¼Œåœ¨ACDCæ•°æ®é›†ä¸Šé«˜å‡º1.6%ï¼Œåœ¨Synapseæ•°æ®é›†ä¸Šé«˜å‡º0.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00630v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAMæ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å±•ç°å‡ºé›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†å…¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨æ€§èƒ½å°šä¸ç¡®å®šã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSelf-Prompt-SAMçš„æ–°å‹è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡å¤šå°ºåº¦æç¤ºç”Ÿæˆå™¨ä¸SAMçš„å›¾åƒç¼–ç å™¨ç»“åˆï¼Œç”Ÿæˆè¾…åŠ©æ©è†œæ¥äº§ç”Ÿè¾¹ç•Œæ¡†å’Œç‚¹æç¤ºã€‚åŒæ—¶ï¼Œè®¾è®¡3Dæ·±åº¦èåˆé€‚é…å™¨ï¼ˆDfusedAdapterï¼‰ï¼Œæ³¨å…¥å›¾åƒç¼–ç å™¨å’Œæ©è†œè§£ç å™¨çš„æ¯ä¸ªè½¬æ¢å™¨ä¸­ï¼Œä½¿é¢„è®­ç»ƒçš„2D SAMæ¨¡å‹èƒ½å¤Ÿæå–3Dä¿¡æ¯å¹¶é€‚åº”3DåŒ»å­¦å›¾åƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³å¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¿‡äº†nnUNetã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAMæ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºé›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>SAMåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨æ€§èƒ½å°šä¸ç¡®å®šã€‚</li>
<li>æå‡ºäº†åä¸ºSelf-Prompt-SAMçš„æ–°å‹è‡ªé€‚åº”æ¡†æ¶ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>è®¾è®¡äº†å¤šå°ºåº¦æç¤ºç”Ÿæˆå™¨ä»¥ç”Ÿæˆè¾…åŠ©æ©è†œï¼Œä»è€Œäº§ç”Ÿè¾¹ç•Œæ¡†å’Œç‚¹æç¤ºã€‚</li>
<li>ä½¿ç”¨äº†DfusedAdapterï¼Œä½¿SAMæ¨¡å‹èƒ½å¤Ÿé€‚åº”3DåŒ»å­¦å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†nnUNetã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a05502fdfe65eb753b940b889c3d2be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9957a8822f1fe2c5d7510620bb63d6b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4e8aecb803ed0bc6466c50483744400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c79a58f8feb6a109e0b3bd474badd83e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Distribution-aware-Fairness-Learning-in-Medical-Image-Segmentation-From-A-Control-Theoretic-Perspective"><a href="#Distribution-aware-Fairness-Learning-in-Medical-Image-Segmentation-From-A-Control-Theoretic-Perspective" class="headerlink" title="Distribution-aware Fairness Learning in Medical Image Segmentation From   A Control-Theoretic Perspective"></a>Distribution-aware Fairness Learning in Medical Image Segmentation From   A Control-Theoretic Perspective</h2><p><strong>Authors:Yujin Oh, Pengfei Jin, Sangjoon Park, Sekeun Kim, Siyeop Yoon, Kyungsang Kim, Jin Sung Kim, Xiang Li, Quanzheng Li</strong></p>
<p>Ensuring fairness in medical image segmentation is critical due to biases in imbalanced clinical data acquisition caused by demographic attributes (e.g., age, sex, race) and clinical factors (e.g., disease severity). To address these challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired by optimal control theory. We provide a comprehensive analysis of its underlying mechanisms and clarify dMoEâ€™s role in adapting to heterogeneous distributions in medical image segmentation. Furthermore, we integrate dMoE into multiple network architectures, demonstrating its broad applicability across diverse medical image analysis tasks. By incorporating demographic and clinical factors, dMoE achieves state-of-the-art performance on two 2D benchmark datasets and a 3D in-house dataset. Our results highlight the effectiveness of dMoE in mitigating biases from imbalanced distributions, offering a promising approach to bridging control theory and medical image segmentation within fairness learning paradigms. The source code will be made available. </p>
<blockquote>
<p>ç¡®ä¿åŒ»å­¦å›¾åƒåˆ†å‰²çš„å…¬å¹³æ€§æ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºç”±äºäººå£ç»Ÿè®¡å­¦å±æ€§ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€ç§æ—ï¼‰å’Œä¸´åºŠå› ç´ ï¼ˆå¦‚ç–¾ç—…ä¸¥é‡ç¨‹åº¦ï¼‰å¯¼è‡´çš„ä¸å¹³è¡¡ä¸´åºŠæ•°æ®é‡‡é›†ä¸­çš„åè§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å—æœ€ä¼˜æ§åˆ¶ç†è®ºå¯å‘çš„â€œåˆ†å¸ƒæ„ŸçŸ¥ä¸“å®¶æ··åˆï¼ˆdMoEï¼‰â€æ–¹æ³•ã€‚æˆ‘ä»¬å¯¹dMoEçš„å†…åœ¨æœºåˆ¶è¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œå¹¶æ˜ç¡®äº†å…¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é€‚åº”å¼‚è´¨åˆ†å¸ƒçš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†dMoEé›†æˆåˆ°å¤šç§ç½‘ç»œæ¶æ„ä¸­ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒåŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨æ€§ã€‚é€šè¿‡ç»“åˆäººå£ç»Ÿè®¡å­¦å’Œä¸´åºŠå› ç´ ï¼ŒdMoEåœ¨ä¸¤ä¸ªäºŒç»´åŸºå‡†æ•°æ®é›†å’Œä¸€ä¸ªä¸‰ç»´å†…éƒ¨æ•°æ®é›†ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†dMoEåœ¨ç¼“è§£ä¸å¹³è¡¡åˆ†å¸ƒä¸­çš„åè§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåœ¨å…¬å¹³å­¦ä¹ èŒƒå¼å†…å°†æ§åˆ¶ç†è®ºä¸åŒ»å­¦å›¾åƒåˆ†å‰²ä¹‹é—´æ¶èµ·æ¡¥æ¢æä¾›äº†æœ‰å‰é€”çš„æ–¹æ³•ã€‚æºä»£ç å°†æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00619v1">PDF</a> 12 pages, 3 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ç¡®ä¿å…¬å¹³æ€§çš„é‡è¦æ€§ï¼Œå› ä¸ºä¸´åºŠæ•°æ®è·å–ä¸­çš„ä¸å¹³è¡¡ä¼šå¯¼è‡´ç”±äººå£ç»Ÿè®¡å­¦å±æ€§ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€ç§æ—ï¼‰å’Œä¸´åºŠå› ç´ ï¼ˆå¦‚ç–¾ç—…ä¸¥é‡ç¨‹åº¦ï¼‰äº§ç”Ÿçš„åè§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæœ€ä¼˜æ§åˆ¶ç†è®ºçš„åˆ†å¸ƒæ„ŸçŸ¥æ··åˆä¸“å®¶ï¼ˆdMoEï¼‰ã€‚æœ¬æ–‡å¯¹å…¶å†…åœ¨æœºåˆ¶è¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œå¹¶æ˜ç¡®äº†dMoEåœ¨é€‚åº”åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å¼‚è´¨åˆ†å¸ƒä¸­çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†dMoEé›†æˆåˆ°å¤šç§ç½‘ç»œæ¶æ„ä¸­ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒåŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨æ€§ã€‚é€šè¿‡ç»“åˆäººå£ç»Ÿè®¡å­¦å’Œä¸´åºŠå› ç´ ï¼ŒdMoEåœ¨ä¸¤ä¸ª2DåŸºå‡†æ•°æ®é›†å’Œä¸€ä¸ª3Då†…éƒ¨æ•°æ®é›†ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒdMoEåœ¨ç¼“è§£ç”±åˆ†å¸ƒä¸å¹³è¡¡å¯¼è‡´çš„åè§æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä¸ºæ§åˆ¶ç†è®ºä¸åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨å…¬å¹³å­¦ä¹ èŒƒå¼ä¸­çš„èåˆæä¾›äº†æœ‰å‰é€”çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å­˜åœ¨ç”±æ•°æ®ä¸å¹³è¡¡å¯¼è‡´çš„åè§é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¯èƒ½æºäºäººå£ç»Ÿè®¡å­¦å±æ€§å’Œä¸´åºŠå› ç´ ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†åˆ†å¸ƒæ„ŸçŸ¥æ··åˆä¸“å®¶ï¼ˆdMoEï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å—åˆ°æœ€ä¼˜æ§åˆ¶ç†è®ºçš„å¯å‘ã€‚</li>
<li>dMoEèƒ½å¤Ÿç»¼åˆå¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å¼‚è´¨åˆ†å¸ƒé—®é¢˜ã€‚</li>
<li>dMoEè¢«æˆåŠŸé›†æˆåˆ°å¤šç§ç½‘ç»œæ¶æ„ä¸­ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒåŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨æ€§ã€‚</li>
<li>é€šè¿‡ç»“åˆäººå£ç»Ÿè®¡å­¦å’Œä¸´åºŠå› ç´ ï¼ŒdMoEåœ¨å¤šä¸ªæ•°æ®é›†ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>dMoEåœ¨ç¼“è§£ç”±åˆ†å¸ƒä¸å¹³è¡¡å¯¼è‡´çš„åè§æ–¹é¢éå¸¸æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-df2bf3c86da58c13aeaba1929d59c576.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48d79360fdc705f1a89cb2c2d658158c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Generating-crossmodal-gene-expression-from-cancer-histopathology-improves-multimodal-AI-predictions"><a href="#Generating-crossmodal-gene-expression-from-cancer-histopathology-improves-multimodal-AI-predictions" class="headerlink" title="Generating crossmodal gene expression from cancer histopathology   improves multimodal AI predictions"></a>Generating crossmodal gene expression from cancer histopathology   improves multimodal AI predictions</h2><p><strong>Authors:Samiran Dey, Christopher R. S. Banerji, Partha Basuchowdhuri, Sanjoy K. Saha, Deepak Parashar, Tapabrata Chakraborti</strong></p>
<p>Emerging research has highlighted that artificial intelligence based multimodal fusion of digital pathology and transcriptomic features can improve cancer diagnosis (grading&#x2F;subtyping) and prognosis (survival risk) prediction. However, such direct fusion for joint decision is impractical in real clinical settings, where histopathology is still the gold standard for diagnosis and transcriptomic tests are rarely requested, at least in the public healthcare system. With our novel diffusion based crossmodal generative AI model PathoGen, we show that genomic expressions synthesized from digital histopathology jointly predicts cancer grading and patient survival risk with high accuracy (state-of-the-art performance), certainty (through conformal coverage guarantee) and interpretability (through distributed attention maps). PathoGen code is available for open use by the research community through GitHub at <a target="_blank" rel="noopener" href="https://github.com/Samiran-Dey/PathoGen">https://github.com/Samiran-Dey/PathoGen</a>. </p>
<blockquote>
<p>æœ€æ–°çš„ç ”ç©¶å·²ç»å¼ºè°ƒï¼ŒåŸºäºäººå·¥æ™ºèƒ½çš„æ•°å­—ç—…ç†å’Œè½¬å½•ç»„ç‰¹å¾çš„å¤šæ¨¡å¼èåˆå¯ä»¥æé«˜ç™Œç—‡è¯Šæ–­ï¼ˆåˆ†çº§&#x2F;äºšå‹ï¼‰å’Œé¢„åï¼ˆç”Ÿå­˜é£é™©ï¼‰é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨å®é™…çš„ä¸´åºŠç¯å¢ƒä¸­ï¼Œè¿™ç§ç›´æ¥èåˆè¿›è¡Œè”åˆå†³ç­–å¹¶ä¸å®ç”¨ã€‚åœ¨å…¬å…±åŒ»ç–—ä½“ç³»ä¸­ï¼Œç»„ç»‡ç—…ç†å­¦ä»ç„¶æ˜¯è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œè½¬å½•ç»„æµ‹è¯•å¾ˆå°‘è¢«è¦æ±‚ã€‚é€šè¿‡æˆ‘ä»¬çš„æ–°å‹åŸºäºæ‰©æ•£çš„è·¨æ¨¡æ€ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹PathoGenï¼Œæˆ‘ä»¬è¯æ˜äº†ç”±æ•°å­—ç—…ç†å­¦åˆæˆçš„åŸºå› è¡¨è¾¾èƒ½å¤Ÿè”åˆé¢„æµ‹ç™Œç—‡åˆ†çº§å’Œæ‚£è€…ç”Ÿå­˜é£é™©ï¼Œå…·æœ‰é«˜ç²¾åº¦ï¼ˆæœ€å…ˆè¿›çš„æ€§èƒ½ï¼‰ã€ç¡®å®šæ€§ï¼ˆé€šè¿‡ç½®ä¿¡åŒºé—´ä¿è¯ï¼‰å’Œå¯è§£é‡Šæ€§ï¼ˆé€šè¿‡åˆ†å¸ƒå¼æ³¨æ„åŠ›å›¾ï¼‰ã€‚PathoGenä»£ç å¯é€šè¿‡GitHubä¾›ç ”ç©¶ç•Œå¼€æ”¾ä½¿ç”¨ï¼Œç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/Samiran-Dey/PathoGen%E3%80%82">https://github.com/Samiran-Dey/PathoGenã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00568v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°å…´ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºäººå·¥æ™ºèƒ½çš„å¤šæ¨¡æ€èåˆæ•°å­—ç—…ç†ä¸è½¬å½•ç»„ç‰¹å¾èƒ½æé«˜ç™Œç—‡è¯Šæ–­ï¼ˆåˆ†çº§&#x2F;äºšå‹ï¼‰å’Œé¢„åï¼ˆç”Ÿå­˜é£é™©ï¼‰é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­ç›´æ¥èåˆè¿›è¡Œè”åˆå†³ç­–å¹¶ä¸ç°å®ã€‚æˆ‘ä»¬çš„æ–°å‹æ‰©æ•£å¼è·¨æ¨¡æ€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹PathoGenï¼Œèƒ½å¤Ÿåœ¨æ•°å­—ç—…ç†ä¸­åˆæˆåŸºå› è¡¨è¾¾ï¼Œä»¥é«˜ç²¾ç¡®åº¦ã€é«˜å¯è§£é‡Šæ€§è”åˆé¢„æµ‹ç™Œç—‡åˆ†çº§å’Œæ‚£è€…ç”Ÿå­˜é£é™©ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚PathoGenä»£ç å·²å¼€æºä¾›ç ”ç©¶ç¤¾åŒºä½¿ç”¨ï¼Œå¯é€šè¿‡GitHubè®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨å¤šæ¨¡æ€èåˆæ•°å­—ç—…ç†å’Œè½¬å½•ç»„ç‰¹å¾æ–¹é¢å±•ç°å‡ºæé«˜ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>å½“å‰ç›´æ¥èåˆå¤šæ¨¡æ€æ•°æ®åœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­çš„å®ç”¨æ€§æœ‰é™ã€‚</li>
<li>PathoGenæ¨¡å‹èƒ½å¤Ÿåœ¨æ•°å­—ç—…ç†ä¸­åˆæˆåŸºå› è¡¨è¾¾ï¼Œè¿›è¡Œç™Œç—‡åˆ†çº§å’Œæ‚£è€…ç”Ÿå­˜é£é™©çš„é¢„æµ‹ã€‚</li>
<li>PathoGenæ¨¡å‹å…·æœ‰é«˜ç²¾åº¦ã€é«˜å¯è§£é‡Šæ€§å’Œé«˜å¯é æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
<li>PathoGenä»£ç å·²å¼€æºï¼Œä¾›ç ”ç©¶ç¤¾åŒºä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5dfd353e0ff35ec55a82ce43d3cf1753.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f33394a83fddd37af5fb9cee46ef029.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b8b5c480999519c5c61b2c6c02d142c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3779e081e955aeed5bc098a958af6b95.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CAD-Confidence-Aware-Adaptive-Displacement-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#CAD-Confidence-Aware-Adaptive-Displacement-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="CAD: Confidence-Aware Adaptive Displacement for Semi-Supervised Medical   Image Segmentation"></a>CAD: Confidence-Aware Adaptive Displacement for Semi-Supervised Medical   Image Segmentation</h2><p><strong>Authors:Wenbo Xiao, Zhihao Xu, Guiping Liang, Yangjun Deng, Yi Xiao</strong></p>
<p>Semi-supervised medical image segmentation aims to leverage minimal expert annotations, yet remains confronted by challenges in maintaining high-quality consistency learning. Excessive perturbations can degrade alignment and hinder precise decision boundaries, especially in regions with uncertain predictions. In this paper, we introduce Confidence-Aware Adaptive Displacement (CAD), a framework that selectively identifies and replaces the largest low-confidence regions with high-confidence patches. By dynamically adjusting both the maximum allowable replacement size and the confidence threshold throughout training, CAD progressively refines the segmentation quality without overwhelming the learning process. Experimental results on public medical datasets demonstrate that CAD effectively enhances segmentation quality, establishing new state-of-the-art accuracy in this field. The source code will be released after the paper is published. </p>
<blockquote>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨åˆ©ç”¨æœ€å°‘çš„ä¸“å®¶æ ‡æ³¨ï¼Œä½†ä»é¢ä¸´ä¿æŒé«˜è´¨é‡ä¸€è‡´æ€§å­¦ä¹ çš„æŒ‘æˆ˜ã€‚è¿‡å¤šçš„æ‰°åŠ¨å¯èƒ½ä¼šé™ä½å¯¹é½ç²¾åº¦ï¼Œé˜»ç¢ç²¾ç¡®å†³ç­–è¾¹ç•Œçš„å½¢æˆï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æµ‹ä¸ç¡®å®šçš„åŒºåŸŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¿¡å¿ƒæ„ŸçŸ¥è‡ªé€‚åº”ä½ç§»ï¼ˆCADï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰é€‰æ‹©åœ°è¯†åˆ«å’Œç”¨é«˜ä¿¡å¿ƒè¡¥ä¸æ›¿æ¢æœ€å¤§çš„ä½ä¿¡å¿ƒåŒºåŸŸã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´è®­ç»ƒè¿‡ç¨‹ä¸­å…è®¸çš„æœ€å¤§æ›¿æ¢å¤§å°å’Œç½®ä¿¡é˜ˆå€¼ï¼ŒCADå¯ä»¥é€æ­¥æé«˜åˆ†å‰²è´¨é‡ï¼Œè€Œä¸ä¼šä½¿å­¦ä¹ è¿‡ç¨‹è¿‡äºå¤æ‚ã€‚åœ¨å…¬å…±åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCADæœ‰æ•ˆæé«˜äº†åˆ†å‰²è´¨é‡ï¼Œåœ¨æ­¤é¢†åŸŸå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„ç²¾åº¦ã€‚è®ºæ–‡å‘å¸ƒåå°†å…¬å¸ƒæºä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00536v1">PDF</a> 9 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåŠç›‘ç£åˆ†å‰²æ—¨åœ¨åˆ©ç”¨æœ€å°‘çš„ä¸“å®¶æ ‡æ³¨ï¼Œä½†ä»é¢ä¸´ä¿æŒé«˜è´¨é‡ä¸€è‡´æ€§å­¦ä¹ çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºConfidence-Aware Adaptive Displacementï¼ˆCADï¼‰çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé€‰æ‹©æ€§è¯†åˆ«å¹¶æ›¿æ¢æœ€å¤§ä½ç½®ä¿¡åº¦åŒºåŸŸä¸ºé«˜ç½®ä¿¡åº¦è¡¥ä¸ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´æœ€å¤§å…è®¸æ›¿æ¢å¤§å°å’Œç½®ä¿¡é˜ˆå€¼ï¼ŒCADåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥ä¼˜åŒ–åˆ†å‰²è´¨é‡ï¼Œè€Œä¸ä¼šä½¿å­¦ä¹ è¿‡ç¨‹è¿‡äºå¤æ‚ã€‚åœ¨å…¬å…±åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCADèƒ½æœ‰æ•ˆæé«˜åˆ†å‰²è´¨é‡ï¼Œåœ¨è¯¥é¢†åŸŸå»ºç«‹æ–°çš„æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨åˆ©ç”¨å°‘é‡ä¸“å®¶æ ‡æ³¨ï¼Œä½†é¢ä¸´ä¿æŒé«˜è´¨é‡ä¸€è‡´æ€§å­¦ä¹ çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„Confidence-Aware Adaptive Displacementï¼ˆCADï¼‰æ¡†æ¶èƒ½è¯†åˆ«å¹¶æ›¿æ¢ä½ç½®ä¿¡åº¦åŒºåŸŸã€‚</li>
<li>CADé€šè¿‡åŠ¨æ€è°ƒæ•´æœ€å¤§å…è®¸æ›¿æ¢å¤§å°å’Œç½®ä¿¡é˜ˆå€¼ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥ä¼˜åŒ–åˆ†å‰²è´¨é‡ã€‚</li>
<li>CADåœ¨å…¬å…±åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½æœ‰æ•ˆæé«˜åˆ†å‰²è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å…·æœ‰ä¸ç¡®å®šé¢„æµ‹çš„åŒºåŸŸæ—¶ã€‚</li>
<li>CADæ¡†æ¶çš„åº”ç”¨æœ‰åŠ©äºæ”¹å–„è¿‡åº¦æ‰°åŠ¨å¯¼è‡´çš„å¯¹é½é—®é¢˜ï¼Œæé«˜å†³ç­–è¾¹ç•Œçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be105b76fa0431699c0b70663d64d0be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca6e2b8984adf778ae0ac1a1eaf4b7da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55e0803e8964ae58fcc088aff9cf816a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5aeb79c0a00bb935711d82a1719c2782.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Deep-Ensembling-with-Multimodal-Image-Fusion-for-Efficient-Classification-of-Lung-Cancer"><a href="#Deep-Ensembling-with-Multimodal-Image-Fusion-for-Efficient-Classification-of-Lung-Cancer" class="headerlink" title="Deep Ensembling with Multimodal Image Fusion for Efficient   Classification of Lung Cancer"></a>Deep Ensembling with Multimodal Image Fusion for Efficient   Classification of Lung Cancer</h2><p><strong>Authors:Surochita Pal, Sushmita Mitra</strong></p>
<p>This study focuses on the classification of cancerous and healthy slices from multimodal lung images. The data used in the research comprises Computed Tomography (CT) and Positron Emission Tomography (PET) images. The proposed strategy achieves the fusion of PET and CT images by utilizing Principal Component Analysis (PCA) and an Autoencoder. Subsequently, a new ensemble-based classifier developed, Deep Ensembled Multimodal Fusion (DEMF), employing majority voting to classify the sample images under examination. Gradient-weighted Class Activation Mapping (Grad-CAM) employed to visualize the classification accuracy of cancer-affected images. Given the limited sample size, a random image augmentation strategy employed during the training phase. The DEMF network helps mitigate the challenges of scarce data in computer-aided medical image analysis. The proposed network compared with state-of-the-art networks across three publicly available datasets. The network outperforms others based on the metrics - Accuracy, F1-Score, Precision, and Recall. The investigation results highlight the effectiveness of the proposed network. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å…³æ³¨å¤šæ¨¡æ€è‚ºéƒ¨å›¾åƒä¸­çš„ç™Œå˜ä¸æ­£å¸¸åˆ‡ç‰‡çš„åˆ†ç±»ã€‚ç ”ç©¶ä¸­ä½¿ç”¨çš„æ•°æ®åŒ…æ‹¬è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å’Œæ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒã€‚æå‡ºçš„ç­–ç•¥é€šè¿‡åˆ©ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å’Œè‡ªåŠ¨ç¼–ç å™¨å®ç°PETå’ŒCTå›¾åƒçš„èåˆã€‚éšåï¼Œå¼€å‘äº†ä¸€ç§æ–°çš„åŸºäºé›†æˆåˆ†ç±»å™¨çš„å¤šæ¨¡æ€èåˆé›†æˆç½‘ç»œï¼ˆDEMFï¼‰ï¼Œé‡‡ç”¨å¤šæ•°æŠ•ç¥¨æ–¹å¼å¯¹æ£€æŸ¥æ ·æœ¬å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚é‡‡ç”¨æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰å¯è§†åŒ–ç™Œç—‡å—å½±å“å›¾åƒçš„åˆ†ç±»å‡†ç¡®æ€§ã€‚è€ƒè™‘åˆ°æ ·æœ¬é‡æœ‰é™ï¼Œåœ¨è®­ç»ƒé˜¶æ®µé‡‡ç”¨äº†éšæœºå›¾åƒå¢å¼ºç­–ç•¥ã€‚DEMFç½‘ç»œæœ‰åŠ©äºç¼“è§£è®¡ç®—æœºè¾…åŠ©åŒ»å­¦å›¾åƒåˆ†æä¸­æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚ä¸ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„æœ€å…ˆè¿›çš„ç½‘ç»œç›¸æ¯”ï¼Œæ‰€æå‡ºçš„ç½‘ç»œåœ¨å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€ç²¾ç¡®ç‡å’Œå¬å›ç‡ç­‰æŒ‡æ ‡ä¸Šè¡¨ç°è¾ƒå¥½ã€‚è°ƒæŸ¥ç»“æœçªå‡ºäº†æ‰€æå‡ºç½‘ç»œçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00078v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶åˆ©ç”¨å¤šæ¨¡æ€è‚ºéƒ¨å›¾åƒå¯¹ç™Œå˜å’Œæ­£å¸¸åˆ‡ç‰‡è¿›è¡Œåˆ†ç±»ã€‚æ•°æ®åŒ…æ‹¬è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å’Œæ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒã€‚é€šè¿‡ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å’Œè‡ªç¼–ç å™¨å®ç°PETå’ŒCTå›¾åƒçš„èåˆã€‚å¼€å‘äº†ä¸€ç§åŸºäºé›†æˆçš„æ–°åˆ†ç±»å™¨â€”â€”æ·±åº¦é›†æˆå¤šæ¨¡æ€èåˆï¼ˆDEMFï¼‰ï¼Œé‡‡ç”¨æŠ•ç¥¨å¤šæ•°åˆ¶å¯¹æ£€æŸ¥æ ·æœ¬å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚åˆ©ç”¨æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰å¯è§†åŒ–ç™Œç—‡å—å½±å“å›¾åƒçš„åˆ†ç±»å‡†ç¡®æ€§ã€‚é‰´äºæ ·æœ¬é‡æœ‰é™ï¼Œè®­ç»ƒé˜¶æ®µé‡‡ç”¨äº†éšæœºå›¾åƒå¢å¼ºç­–ç•¥ã€‚DEMFç½‘ç»œæœ‰åŠ©äºè§£å†³è®¡ç®—æœºè¾…åŠ©åŒ»å­¦å›¾åƒåˆ†æä¸­æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šï¼Œè¯¥ç½‘ç»œä¸æœ€æ–°ç½‘ç»œè¿›è¡Œäº†æ¯”è¾ƒï¼Œåœ¨å‡†ç¡®æ€§ã€F1åˆ†æ•°ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡ç­‰æŒ‡æ ‡ä¸Šè¡¨ç°è¾ƒå¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨å¤šæ¨¡æ€è‚ºéƒ¨å›¾åƒï¼ˆåŒ…æ‹¬CTå’ŒPETå›¾åƒï¼‰çš„ç™Œå˜å’Œæ­£å¸¸åˆ‡ç‰‡åˆ†ç±»ã€‚</li>
<li>ç ”ç©¶é€šè¿‡PCAå’Œè‡ªç¼–ç å™¨å®ç°äº†PETå’ŒCTå›¾åƒçš„èåˆã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºé›†æˆçš„åˆ†ç±»å™¨DEMFï¼Œé€šè¿‡å¤šæ•°æŠ•ç¥¨æœºåˆ¶å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚</li>
<li>ä½¿ç”¨Grad-CAMå¯è§†åŒ–ç™Œç—‡å›¾åƒåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç”±äºæ ·æœ¬é‡æœ‰é™ï¼Œé‡‡ç”¨äº†éšæœºå›¾åƒå¢å¼ºç­–ç•¥ä»¥æ”¹å–„æ¨¡å‹è®­ç»ƒã€‚</li>
<li>DEMFç½‘ç»œåœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†æä¸­æ•°æ®ç¨€ç¼ºé—®é¢˜æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šï¼ŒDEMFç½‘ç»œåœ¨åˆ†ç±»æ€§èƒ½ä¸Šè¶…è¿‡äº†å…¶ä»–ç½‘ç»œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00078">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e0cffbb9b76e2c050e836499ff2f150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04809083347df0765596bb946b631931.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6597cdb04137e94cb5fc7e3cce4cf2bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50c1a0fcab80f9732214e70f1388f51b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3788efad5a8b54efb9fb68e5f85385ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5860f563d54647ed46aa02c061a6d95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1016ee46c24f18bf69a66b270caa2781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abf617ccfb54a6853ebabcc6430f19bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ec0ccb64a6f3e8ee581915cfcc2db8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7823415987dfe2f8ec9dd1e98946cac.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LSU-Net-Lightweight-Automatic-Organs-Segmentation-Network-For-Medical-Images"><a href="#LSU-Net-Lightweight-Automatic-Organs-Segmentation-Network-For-Medical-Images" class="headerlink" title="LSU-Net: Lightweight Automatic Organs Segmentation Network For Medical   Images"></a>LSU-Net: Lightweight Automatic Organs Segmentation Network For Medical   Images</h2><p><strong>Authors:Yujie Ding, Shenghua Teng, Zuoyong Li, Xiao Chen</strong></p>
<p>UNet and its variants have widespread applications in medical image segmentation. However, the substantial number of parameters and computational complexity of these models make them less suitable for use in clinical settings with limited computational resources. To address this limitation, we propose a novel Lightweight Shift U-Net (LSU-Net). We integrate the Light Conv Block and the Tokenized Shift Block in a lightweight manner, combining them with a dynamic weight multi-loss design for efficient dynamic weight allocation. The Light Conv Block effectively captures features with a low parameter count by combining standard convolutions with depthwise separable convolutions. The Tokenized Shift Block optimizes feature representation by shifting and capturing deep features through a combination of the Spatial Shift Block and depthwise separable convolutions. Dynamic adjustment of the loss weights at each layer approaches the optimal solution and enhances training stability. We validated LSU-Net on the UWMGI and MSD Colon datasets, and experimental results demonstrate that LSU-Net outperforms most state-of-the-art segmentation architectures. </p>
<blockquote>
<p>UNetåŠå…¶å˜ä½“åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æœ‰ç€å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å‚æ•°ä¼—å¤šï¼Œè®¡ç®—å¤æ‚åº¦è¾ƒé«˜ï¼Œå¯¹äºè®¡ç®—èµ„æºæœ‰é™çš„ä¸´åºŠç¯å¢ƒæ¥è¯´ä¸å¤ªé€‚ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è½»é‡åŒ–ç§»ä½U-Netï¼ˆLSU-Netï¼‰ã€‚æˆ‘ä»¬é€šè¿‡è½»é‡çº§çš„æ–¹å¼é›†æˆäº†Light Conv Blockå’ŒTokenized Shift Blockï¼Œå¹¶ç»“åˆåŠ¨æ€æƒé‡å¤šæŸå¤±è®¾è®¡ï¼Œå®ç°äº†æœ‰æ•ˆçš„åŠ¨æ€æƒé‡åˆ†é…ã€‚Light Conv Blocké€šè¿‡ç»“åˆæ ‡å‡†å·ç§¯å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼Œä»¥è¾ƒä½çš„å‚æ•°æ•°é‡æœ‰æ•ˆåœ°æ•æ‰ç‰¹å¾ã€‚Tokenized Shift Blocké€šè¿‡ä¼˜åŒ–ç©ºé—´ç§»ä½å—å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„ç»„åˆï¼Œå®ç°äº†ç‰¹å¾çš„ç§»ä½å’Œæ·±å±‚ç‰¹å¾çš„æ•æ‰ï¼Œä»è€Œä¼˜åŒ–äº†ç‰¹å¾è¡¨ç¤ºã€‚åŠ¨æ€è°ƒæ•´æ¯å±‚çš„æŸå¤±æƒé‡æ¥è¿‘æœ€ä¼˜è§£ï¼Œæé«˜äº†è®­ç»ƒç¨³å®šæ€§ã€‚æˆ‘ä»¬åœ¨UWMGIå’ŒMSDç»“è‚ æ•°æ®é›†ä¸Šå¯¹LSU-Netè¿›è¡Œäº†éªŒè¯ï¼Œå®éªŒç»“æœè¡¨æ˜LSU-Netåœ¨å¤§å¤šæ•°æœ€å…ˆè¿›çš„åˆ†å‰²æ¶æ„ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00042v1">PDF</a> 5 pages, 3 figures, 4 tables. Accepted at ICASSP 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­UNetåŠå…¶å˜ä½“æ¨¡å‹å‚æ•°å¤šã€è®¡ç®—å¤æ‚åº¦é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†è½»é‡çº§ç§»ä½U-Netï¼ˆLSU-Netï¼‰ã€‚é€šè¿‡é›†æˆè½»é‡çº§å·ç§¯å—å’Œæ ‡è®°ç§»ä½å—ï¼Œå¹¶ç»“åˆåŠ¨æ€æƒé‡å¤šæŸå¤±è®¾è®¡ï¼Œå®ç°é«˜æ•ˆåŠ¨æ€æƒé‡åˆ†é…ã€‚åœ¨UWMGIå’ŒMSDç»“è‚ æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLSU-Netä¼˜äºå¤§å¤šæ•°æœ€å…ˆè¿›çš„åˆ†å‰²æ¶æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UNetåŠå…¶å˜ä½“åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨å‚æ•°å¤šã€è®¡ç®—å¤æ‚åº¦é«˜çš„é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†è½»é‡çº§ç§»ä½U-Netï¼ˆLSU-Netï¼‰ã€‚</li>
<li>LSU-Neté›†æˆäº†è½»é‡çº§å·ç§¯å—å’Œæ ‡è®°ç§»ä½å—ã€‚</li>
<li>è½»é‡çº§å·ç§¯å—é€šè¿‡ç»“åˆæ ‡å‡†å·ç§¯å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼Œå®ç°ä½å‚æ•°ç‰¹å¾æ•è·ã€‚</li>
<li>æ ‡è®°ç§»ä½å—é€šè¿‡ç§»ä½æ“ä½œå’Œä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œå®ç°äº†æ·±å±‚ç‰¹å¾çš„æ•è·ã€‚</li>
<li>LSU-Neté‡‡ç”¨åŠ¨æ€æƒé‡å¤šæŸå¤±è®¾è®¡ï¼Œå®ç°æ¯å±‚æŸå¤±æƒé‡çš„åŠ¨æ€è°ƒæ•´ï¼Œæ¥è¿‘æœ€ä¼˜è§£ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6edb17687da9cd5d9ab17ae3f2b56c7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-395b81eb9814bebbfc9a73a38b1bba49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce192651d322c766c0bf6ad3c68c995e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87a0313d8bdc91d51e38c94d9c7353b8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Hanle-Effect-for-Lifetime-Analysis-Li-like-Ions"><a href="#Hanle-Effect-for-Lifetime-Analysis-Li-like-Ions" class="headerlink" title="Hanle Effect for Lifetime Analysis: Li-like Ions"></a>Hanle Effect for Lifetime Analysis: Li-like Ions</h2><p><strong>Authors:Jan Richter, Moto Togawa, JosÃ© R. Crespo LÃ³pez-Urrutia, Andrey Surzhykov</strong></p>
<p>Accurate lifetime measurements of excited states of highly charged ions (HCIs) are essential for advancing diagnostics in both laboratory and astrophysical plasmas, especially in the X-ray regime. The Hanle effect, which utilizes external magnetic fields to modify photon scattering patterns, provides a powerful technique for these measurements. Previously, this method has been successfully employed for He-like ions. Here, we present a theoretical study of the prospects of the Hanle effect for lifetime determinations of Li-like ions. Our results highlight the potential for plasma diagnostics and X-ray spectral analysis. </p>
<blockquote>
<p>å¯¹é«˜ç”µè·ç¦»å­ï¼ˆHCIsï¼‰æ¿€å‘æ€çš„ç²¾ç¡®å¯¿å‘½æµ‹é‡å¯¹äºæ¨è¿›å®éªŒå®¤å’Œå¤©æ–‡ç­‰ç¦»å­ä½“è¯Šæ–­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨Xå°„çº¿é¢†åŸŸã€‚Hanleæ•ˆåº”æ˜¯ä¸€ç§åˆ©ç”¨å¤–éƒ¨ç£åœºæ”¹å˜å…‰å­æ•£å°„æ¨¡å¼çš„å¼ºå¤§æŠ€æœ¯ï¼Œä¸ºè¿™äº›æµ‹é‡æä¾›äº†æœ‰åŠ›æ‰‹æ®µã€‚ä»¥å‰ï¼Œè¿™ç§æ–¹æ³•å·²æˆåŠŸåº”ç”¨äºç±»æ°¦ç¦»å­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯¹ç±»é”‚ç¦»å­å¯¿å‘½ç¡®å®šçš„Hanleæ•ˆåº”è¿›è¡Œäº†ç†è®ºç ”ç©¶ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†ç­‰ç¦»å­ä½“è¯Šæ–­å’ŒXå°„çº¿å…‰è°±åˆ†ææ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01509v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤–éƒ¨ç£åœºå¯¹å…‰å­æ•£å°„æ¨¡å¼çš„ä¿®æ”¹ä½œç”¨ï¼Œæ±‰å‹’æ•ˆåº”ä¸ºé«˜åº¦ç”µç¦»ç¦»å­æ¿€å‘æ€å¯¿å‘½çš„ç²¾ç¡®æµ‹é‡æä¾›äº†æœ‰åŠ›æ‰‹æ®µã€‚æœ¬æ–‡ç†è®ºç ”ç©¶äº†æ±‰å‹’æ•ˆåº”åœ¨Liç±»ç¦»å­å¯¿å‘½æµ‹å®šä¸­çš„åº”ç”¨å‰æ™¯ï¼Œä¸ºç­‰ç¦»å­ä½“è¯Šæ–­å’ŒXå°„çº¿å…‰è°±åˆ†ææä¾›äº†æ½œåœ¨å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜åº¦ç”µç¦»ç¦»å­ï¼ˆHCIsï¼‰æ¿€å‘æ€çš„ç²¾ç¡®å¯¿å‘½æµ‹é‡å¯¹äºæ¨è¿›å®éªŒå®¤å’Œå¤©æ–‡ç­‰ç¦»å­ä½“è¯Šæ–­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨Xå°„çº¿é¢†åŸŸã€‚</li>
<li>æ±‰å‹’æ•ˆåº”åˆ©ç”¨å¤–éƒ¨ç£åœºä¿®æ”¹å…‰å­æ•£å°„æ¨¡å¼ï¼Œä¸ºè¿™äº›æµ‹é‡æä¾›äº†æœ‰åŠ›æŠ€æœ¯ã€‚</li>
<li>æ­¤æ–¹æ³•å·²æˆåŠŸåº”ç”¨äºç±»æ°¦ç¦»å­ã€‚</li>
<li>æœ¬æ–‡ç†è®ºç ”ç©¶äº†æ±‰å‹’æ•ˆåº”åœ¨Liç±»ç¦»å­å¯¿å‘½æµ‹å®šä¸­çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>ç ”ç©¶ç»“æœçªå‡ºäº†æ±‰å‹’æ•ˆåº”åœ¨ç­‰ç¦»å­ä½“è¯Šæ–­å’ŒXå°„çº¿å…‰è°±åˆ†æä¸­çš„æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
<li>æ­¤ç ”ç©¶æœ‰æœ›æé«˜ç­‰ç¦»å­ä½“ç‰©ç†å’Œå…‰è°±åˆ†æé¢†åŸŸçš„å‡†ç¡®æ€§å’Œç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8aa5325ff9d99cb83d3b31b6e3a50de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2484b1ecdc82e8a1657315ebbce6b8be.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="VisionTS-Visual-Masked-Autoencoders-Are-Free-Lunch-Zero-Shot-Time-Series-Forecasters"><a href="#VisionTS-Visual-Masked-Autoencoders-Are-Free-Lunch-Zero-Shot-Time-Series-Forecasters" class="headerlink" title="VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time   Series Forecasters"></a>VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time   Series Forecasters</h2><p><strong>Authors:Mouxiang Chen, Lefei Shen, Zhuo Li, Xiaoyun Joy Wang, Jianling Sun, Chenghao Liu</strong></p>
<p>Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either repurpose large language models (LLMs) or build large-scale time series datasets to develop TSF foundation models for universal forecasting. However, these methods face challenges due to the severe cross-domain gap or in-domain heterogeneity. This paper explores a new road to building a TSF foundation model from rich, high-quality natural images. Our key insight is that a visual masked autoencoder, pre-trained on the ImageNet dataset, can naturally be a numeric series forecaster. By reformulating TSF as an image reconstruction task, we bridge the gap between image pre-training and TSF downstream tasks. Surprisingly, without further adaptation in the time series domain, the proposed VisionTS could achieve better zero-shot forecast performance than existing TSF foundation models. With fine-tuning for one epoch, VisionTS could further improve the forecasting and achieve state-of-the-art performance in most cases. Extensive experiments reveal intrinsic similarities between images and real-world time series, suggesting that visual models may offer a â€œfree lunchâ€ for TSF and highlight the potential for future cross-modality research. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Keytoyze/VisionTS">https://github.com/Keytoyze/VisionTS</a>. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ä¸­çš„åŸºç¡€æ¨¡å‹å·²æˆä¸ºä¸€ç§å‰æ™¯å¹¿é˜”çš„æ–¹æ³•ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆé‡æ–°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¦ä¹ˆæ„å»ºå¤§è§„æ¨¡æ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œä»¥å¼€å‘ç”¨äºé€šç”¨é¢„æµ‹çš„æ—¶é—´åºåˆ—é¢„æµ‹åŸºç¡€æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´ç€è·¨åŸŸå·®è·å¤§æˆ–åŸŸå†…å¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€æ¡æ–°çš„æ„å»ºæ—¶é—´åºåˆ—é¢„æµ‹åŸºç¡€æ¨¡å‹çš„é€”å¾„ï¼Œå³ä»ä¸°å¯Œã€é«˜è´¨é‡çš„è‡ªç„¶å›¾åƒä¸­æ„å»ºã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„è§†è§‰æ©ç è‡ªåŠ¨ç¼–ç å™¨å¯ä»¥è‡ªç„¶åœ°æˆä¸ºæ•°å€¼åºåˆ—é¢„æµ‹å™¨ã€‚é€šè¿‡å°†æ—¶é—´åºåˆ—é¢„æµ‹é‡æ–°æ„å»ºä¸ºå›¾åƒé‡å»ºä»»åŠ¡ï¼Œæˆ‘ä»¬å¼¥åˆäº†å›¾åƒé¢„è®­ç»ƒå’Œä¸‹æ¸¸æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ä¹‹é—´çš„å·®è·ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ— éœ€åœ¨æ—¶åºé¢†åŸŸè¿›ä¸€æ­¥é€‚åº”ï¼Œæˆ‘ä»¬æå‡ºçš„VisionTSå¯ä»¥åœ¨é›¶æ ·æœ¬é¢„æµ‹æ–¹é¢å®ç°ä¼˜äºç°æœ‰æ—¶é—´åºåˆ—é¢„æµ‹åŸºç¡€æ¨¡å‹çš„è¡¨ç°ã€‚ç»è¿‡ä¸€ä¸ªå‘¨æœŸçš„å¾®è°ƒåï¼ŒVisionTSå¯ä»¥è¿›ä¸€æ­¥æé«˜é¢„æµ‹èƒ½åŠ›ï¼Œå¹¶åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚å¤§é‡å®éªŒæ­ç¤ºäº†å›¾åƒå’Œç°å®ä¸–ç•Œæ—¶é—´åºåˆ—ä¹‹é—´çš„å†…åœ¨ç›¸ä¼¼æ€§ï¼Œè¿™è¡¨æ˜è§†è§‰æ¨¡å‹å¯èƒ½ä¸ºæ—¶é—´åºåˆ—é¢„æµ‹æä¾›äº†â€œå…è´¹åˆé¤â€ï¼Œå¹¶çªæ˜¾äº†æœªæ¥è·¨æ¨¡æ€ç ”ç©¶çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/Keytoyze/VisionTS">https://github.com/Keytoyze/VisionTS</a> ä¸Šå…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.17253v3">PDF</a> v3: add GIFT-EVAL results</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡å›¾åƒé¢„è®­ç»ƒæ¨¡å‹æ„å»ºæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰åŸºç¡€æ¨¡å‹æ˜¯ä¸€ç§æ–°å…´ä¸”æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶é€šè¿‡å›¾åƒé‡å»ºä»»åŠ¡é‡æ–°æ„å»ºTSFï¼Œå®ç°å›¾åƒé¢„è®­ç»ƒä¸TSFä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„æ¡¥æ¢ï¼Œæ— éœ€è¿›ä¸€æ­¥çš„æ—¶é—´åºåˆ—åŸŸé€‚åº”ï¼Œå³å¯å®ç°é›¶æ ·æœ¬é¢„æµ‹æ€§èƒ½ä¼˜äºç°æœ‰TSFåŸºç¡€æ¨¡å‹ã€‚é€šè¿‡å¾®è°ƒä¸€ä¸ªå‘¨æœŸï¼Œè¯¥æ¨¡å‹å¯ä»¥è¿›ä¸€æ­¥æé«˜é¢„æµ‹æ€§èƒ½ï¼Œå¹¶åœ¨å¤šæ•°æƒ…å†µä¸‹è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†å›¾åƒä¸çœŸå®ä¸–ç•Œæ—¶é—´åºåˆ—ä¹‹é—´çš„å†…åœ¨ç›¸ä¼¼æ€§ï¼Œæ˜¾ç¤ºå‡ºè§†è§‰æ¨¡å‹å¯¹TSFçš„æ½œåœ¨è´¡çŒ®ï¼Œå¹¶ä¸ºæœªæ¥çš„è·¨æ¨¡æ€ç ”ç©¶æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºå¤§è§„æ¨¡å›¾åƒé¢„è®­ç»ƒæ¨¡å‹æ„å»ºæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰åŸºç¡€æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å›¾åƒé‡å»ºä»»åŠ¡é‡æ–°æ„å»ºTSFï¼Œå®ç°äº†å›¾åƒé¢„è®­ç»ƒä¸TSFä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„æ¡¥æ¢ã€‚</li>
<li>è¯¥æ¨¡å‹æ— éœ€è¿›ä¸€æ­¥é€‚åº”æ—¶é—´åºåˆ—åŸŸï¼Œå³å¯å®ç°é›¶æ ·æœ¬é¢„æµ‹æ€§èƒ½ä¼˜äºç°æœ‰TSFåŸºç¡€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¾®è°ƒä¸€ä¸ªå‘¨æœŸï¼Œè¯¥æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æé«˜ï¼Œå¹¶åœ¨å¤šæ•°æƒ…å†µä¸‹è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å›¾åƒä¸çœŸå®ä¸–ç•Œæ—¶é—´åºåˆ—ä¹‹é—´çš„å†…åœ¨ç›¸ä¼¼æ€§ã€‚</li>
<li>è§†è§‰æ¨¡å‹å¯¹TSFå…·æœ‰æ½œåœ¨è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.17253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5da6443cb2237e7b9de925bffabbb51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dcb1b05b3835bf21ed27c50a1e847ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f17842a00cde6cf8f35541552e0bc27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9bd87077a799a5953bbcbbbf870dd0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-764f3706e1271e1c4fa9bf502bbbe4d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38fd8f1136be00a360633f3432845081.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b08739982c2e6b6b9ed2ba69773cfb8b.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  Design and Simulation of the Adaptive Continuous Entanglement Generation   Protocol
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8846ef6514f9aa6d25a3aa58d18ef713.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  Calibrated Multi-Preference Optimization for Aligning Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
