<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  FewTopNER Integrating Few-Shot Learning with Topic Modeling and Named   Entity Recognition in a Multilingual Framework">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2ab57b78ba775a0e866360773d4766c2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-06-æ›´æ–°"><a href="#2025-02-06-æ›´æ–°" class="headerlink" title="2025-02-06 æ›´æ–°"></a>2025-02-06 æ›´æ–°</h1><h2 id="FewTopNER-Integrating-Few-Shot-Learning-with-Topic-Modeling-and-Named-Entity-Recognition-in-a-Multilingual-Framework"><a href="#FewTopNER-Integrating-Few-Shot-Learning-with-Topic-Modeling-and-Named-Entity-Recognition-in-a-Multilingual-Framework" class="headerlink" title="FewTopNER: Integrating Few-Shot Learning with Topic Modeling and Named   Entity Recognition in a Multilingual Framework"></a>FewTopNER: Integrating Few-Shot Learning with Topic Modeling and Named   Entity Recognition in a Multilingual Framework</h2><p><strong>Authors:Ibrahim Bouabdallaoui, Fatima Guerouate, Samya Bouhaddour, Chaimae Saadi, Mohammed Sbihi</strong></p>
<p>We introduce FewTopNER, a novel framework that integrates few-shot named entity recognition (NER) with topic-aware contextual modeling to address the challenges of cross-lingual and low-resource scenarios. FewTopNER leverages a shared multilingual encoder based on XLM-RoBERTa, augmented with language-specific calibration mechanisms, to generate robust contextual embeddings. The architecture comprises a prototype-based entity recognition branch, employing BiLSTM and Conditional Random Fields for sequence labeling, and a topic modeling branch that extracts document-level semantic features through hybrid probabilistic and neural methods. A cross-task bridge facilitates dynamic bidirectional attention and feature fusion between entity and topic representations, thereby enhancing entity disambiguation by incorporating global semantic context. Empirical evaluations on multilingual benchmarks across English, French, Spanish, German, and Italian demonstrate that FewTopNER significantly outperforms existing state-of-the-art few-shot NER models. In particular, the framework achieves improvements of 2.5-4.0 percentage points in F1 score and exhibits enhanced topic coherence, as measured by normalized pointwise mutual information. Ablation studies further confirm the critical contributions of the shared encoder and cross-task integration mechanisms to the overall performance. These results underscore the efficacy of incorporating topic-aware context into few-shot NER and highlight the potential of FewTopNER for robust cross-lingual applications in low-resource settings. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†FewTopNERï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒå°†å°‘é‡å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸ä¸»é¢˜æ„ŸçŸ¥ä¸Šä¸‹æ–‡å»ºæ¨¡ç›¸ç»“åˆï¼Œä»¥è§£å†³è·¨è¯­è¨€å’Œä½èµ„æºåœºæ™¯çš„æŒ‘æˆ˜ã€‚FewTopNERåˆ©ç”¨åŸºäºXLM-RoBERTaçš„å…±äº«å¤šè¯­è¨€ç¼–ç å™¨ï¼Œè¾…ä»¥è¯­è¨€ç‰¹å®šçš„æ ¡å‡†æœºåˆ¶ï¼Œç”Ÿæˆç¨³å¥çš„ä¸Šä¸‹æ–‡åµŒå…¥ã€‚æ¶æ„åŒ…æ‹¬ä¸€ä¸ªåŸºäºåŸå‹çš„å®ä½“è¯†åˆ«åˆ†æ”¯ï¼Œé‡‡ç”¨BiLSTMå’Œæ¡ä»¶éšæœºåœºè¿›è¡Œåºåˆ—æ ‡æ³¨ï¼Œä»¥åŠä¸€ä¸ªä¸»é¢˜å»ºæ¨¡åˆ†æ”¯ï¼Œé€šè¿‡æ··åˆæ¦‚ç‡å’Œç¥ç»æ–¹æ³•æå–æ–‡æ¡£çº§è¯­ä¹‰ç‰¹å¾ã€‚è·¨ä»»åŠ¡æ¡¥æ¢ä¿ƒè¿›äº†å®ä½“å’Œä¸»é¢˜è¡¨ç¤ºä¹‹é—´çš„åŠ¨æ€åŒå‘æ³¨æ„åŠ›å’Œç‰¹å¾èåˆï¼Œä»è€Œé€šè¿‡èå…¥å…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡å¢å¼ºå®ä½“æ¶ˆæ­§ã€‚åœ¨è‹±è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€å¾·è¯­å’Œæ„å¤§åˆ©è¯­çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒFewTopNERæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„å°‘é‡å‘½åå®ä½“è¯†åˆ«æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯¥æ¡†æ¶åœ¨F1åˆ†æ•°ä¸Šæé«˜äº†2.5-4.0ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ä¸”è¡¨ç°å‡ºå¢å¼ºçš„ä¸»é¢˜è¿è´¯æ€§ï¼Œå¦‚é€šè¿‡å½’ä¸€åŒ–ç‚¹äº’ä¿¡æ¯æ‰€è¡¡é‡ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†å…±äº«ç¼–ç å™¨å’Œè·¨ä»»åŠ¡æ•´åˆæœºåˆ¶å¯¹æ•´ä½“æ€§èƒ½çš„å…³é”®è´¡çŒ®ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å°†ä¸»é¢˜æ„ŸçŸ¥ä¸Šä¸‹æ–‡çº³å…¥å°‘é‡å‘½åå®ä½“è¯†åˆ«çš„æœ‰æ•ˆæ€§ï¼Œå¹¶çªå‡ºäº†FewTopNERåœ¨ä½èµ„æºè®¾ç½®ä¸­çš„ç¨³å¥è·¨è¯­è¨€åº”ç”¨çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02391v1">PDF</a> Code source : <a target="_blank" rel="noopener" href="https://github.com/ibrahimself/FewTopNER/">https://github.com/ibrahimself/FewTopNER/</a></p>
<p><strong>Summary</strong></p>
<p>FewTopNERæ˜¯ä¸€ä¸ªèåˆå°‘æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œä¸»é¢˜æ„ŸçŸ¥ä¸Šä¸‹æ–‡å»ºæ¨¡çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹è·¨è¯­è¨€å’Œä½èµ„æºåœºæ™¯çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡å…±äº«çš„å¤šè¯­è¨€ç¼–ç å™¨ç”Ÿæˆç¨³å¥çš„ä¸Šä¸‹æ–‡åµŒå…¥ï¼Œå¹¶ç»“åˆåŸå‹åŸºç¡€çš„å®ä½“è¯†åˆ«åˆ†æ”¯å’Œä¸»é¢˜å»ºæ¨¡åˆ†æ”¯ï¼Œå®ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶åœ¨è·¨ä»»åŠ¡æ¡¥æ¢çš„ååŠ©ä¸‹ï¼Œå®ç°äº†å®ä½“è¡¨ç¤ºå’Œä¸»é¢˜è¡¨ç¤ºä¹‹é—´çš„åŠ¨æ€åŒå‘æ³¨æ„åŠ›å’Œç‰¹å¾èåˆï¼Œä»è€Œé€šè¿‡å¼•å…¥å…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡å¢å¼ºäº†å®ä½“æ¶ˆæ­§ã€‚åœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒFewTopNERæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„å°‘æ ·æœ¬NERæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨F1åˆ†æ•°ä¸Šæé«˜äº†2.5-4.0ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FewTopNERæ¡†æ¶èåˆäº†å°‘æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œä¸»é¢˜æ„ŸçŸ¥ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚</li>
<li>é‡‡ç”¨å…±äº«çš„å¤šè¯­è¨€ç¼–ç å™¨ï¼ŒåŸºäºXLM-RoBERTaï¼Œå¹¶å¢åŠ äº†è¯­è¨€ç‰¹å®šçš„æ ¡å‡†æœºåˆ¶ï¼Œç”Ÿæˆç¨³å¥çš„ä¸Šä¸‹æ–‡åµŒå…¥ã€‚</li>
<li>æ¶æ„åŒ…æ‹¬åŸå‹åŸºç¡€çš„å®ä½“è¯†åˆ«åˆ†æ”¯å’Œä¸»é¢˜å»ºæ¨¡åˆ†æ”¯ã€‚</li>
<li>é€šè¿‡è·¨ä»»åŠ¡æ¡¥æ¢å®ç°å®ä½“è¡¨ç¤ºå’Œä¸»é¢˜è¡¨ç¤ºä¹‹é—´çš„åŠ¨æ€åŒå‘æ³¨æ„åŠ›å’Œç‰¹å¾èåˆã€‚</li>
<li>åœ¨å¤šä¸ªè¯­è¨€çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å°‘æ ·æœ¬NERæ¨¡å‹ã€‚</li>
<li>æ¡†æ¶æé«˜äº†F1åˆ†æ•°ï¼Œå¢å¼ºäº†ä¸»é¢˜è¿è´¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3b3afeaa14b2334ab87736549e7d7c2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Effectiveness-of-LLMs-in-Fixing-Maintainability-Issues-in-Real-World-Projects"><a href="#Evaluating-the-Effectiveness-of-LLMs-in-Fixing-Maintainability-Issues-in-Real-World-Projects" class="headerlink" title="Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in   Real-World Projects"></a>Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in   Real-World Projects</h2><p><strong>Authors:Henrique Nunes, Eduardo Figueiredo, Larissa Rocha, Sarah Nadi, Fischer Ferreira, Geanderson Esteves</strong></p>
<p>Large Language Models (LLMs) have gained attention for addressing coding problems, but their effectiveness in fixing code maintainability remains unclear. This study evaluates LLMs capability to resolve 127 maintainability issues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat and Llama 3.1, and few-shot prompting with Llama only. The LLM-generated solutions are assessed for compilation errors, test failures, and new maintainability problems. Llama with few-shot prompting successfully fixed 44.9% of the methods, while Copilot Chat and Llama zero-shot fixed 32.29% and 30%, respectively. However, most solutions introduced errors or new maintainability issues. We also conducted a human study with 45 participants to evaluate the readability of 51 LLM-generated solutions. The human study showed that 68.63% of participants observed improved readability. Overall, while LLMs show potential for fixing maintainability issues, their introduction of errors highlights their current limitations. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ç¼–ç¨‹é—®é¢˜æ–¹é¢å¼•èµ·äº†äººä»¬çš„å…³æ³¨ï¼Œä½†å®ƒä»¬åœ¨è§£å†³ä»£ç å¯ç»´æŠ¤æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§å°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°LLMè§£å†³æ¥è‡ªGitHubä¸Š10ä¸ªä»“åº“çš„127ä¸ªå¯ç»´æŠ¤æ€§é—®é¢˜çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹Copilot Chatå’ŒLlama 3.1ä½¿ç”¨é›¶æ ·æœ¬æç¤ºï¼Œåªå¯¹Llamaä½¿ç”¨å°æ ·æœ¬æç¤ºã€‚è¯„ä¼°LLMç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆæ˜¯å¦å­˜åœ¨ç¼–è¯‘é”™è¯¯ã€æµ‹è¯•å¤±è´¥ä»¥åŠæ–°çš„å¯ç»´æŠ¤æ€§é—®é¢˜ã€‚ä½¿ç”¨å°æ ·æœ¬æç¤ºçš„LlamaæˆåŠŸä¿®å¤äº†44.9%çš„æ–¹æ³•ï¼Œè€ŒCopilot Chatå’ŒLlamaçš„é›¶æ ·æœ¬ä¿®å¤åˆ†åˆ«ä¸º32.29%å’Œ30%ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è§£å†³æ–¹æ¡ˆéƒ½å¼•å…¥äº†é”™è¯¯æˆ–æ–°çš„å¯ç»´æŠ¤æ€§é—®é¢˜ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†æ¶‰åŠ45åå‚ä¸è€…çš„ç ”ç©¶ï¼Œä»¥è¯„ä¼°LLMç”Ÿæˆçš„51ä¸ªè§£å†³æ–¹æ¡ˆçš„å¯è¯»æ€§ã€‚äººç±»ç ”ç©¶è¡¨æ˜ï¼Œ68.63%çš„å‚ä¸è€…è§‚å¯Ÿåˆ°å¯è¯»æ€§æœ‰æ‰€æé«˜ã€‚æ€»ä½“è€Œè¨€ï¼Œè™½ç„¶LLMåœ¨è§£å†³å¯ç»´æŠ¤æ€§é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¼•å…¥çš„é”™è¯¯çªå‡ºäº†å½“å‰çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02368v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³ç¼–ç é—®é¢˜æ–¹é¢å¤‡å—å…³æ³¨ï¼Œä½†å…¶å¯¹ä»£ç å¯ç»´æŠ¤æ€§çš„ä¿®å¤æ•ˆæœå°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†LLMsè§£å†³æ¥è‡ªGitHubä»“åº“çš„127ä¸ªå¯ç»´æŠ¤é—®é¢˜çš„èƒ½åŠ›ã€‚é€šè¿‡Copilot Chatçš„é›¶å°„æç¤ºå’ŒLlama 3.1ä»¥åŠåªæœ‰Llamaçš„å°‘é‡å°„å‡»æç¤ºæ¥è¯„ä¼°ã€‚LLMç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆä¼šç»è¿‡ç¼–è¯‘é”™è¯¯æµ‹è¯•ã€æµ‹è¯•å¤±è´¥ä»¥åŠæ–°çš„å¯ç»´æŠ¤æ€§é—®é¢˜çš„è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å°‘é‡å°„å‡»æç¤ºçš„LlamaæˆåŠŸä¿®å¤äº†44.9%çš„æ–¹æ³•ï¼Œè€ŒCopilot Chatå’ŒLlamaçš„é›¶å°„ä¿®å¤äº†32.29%å’Œ30%ã€‚ä½†å¤§å¤šæ•°è§£å†³æ–¹æ¡ˆéƒ½å¼•å…¥äº†é”™è¯¯æˆ–æ–°çš„å¯ç»´æŠ¤æ€§é—®é¢˜ã€‚åŒæ—¶ï¼Œæœ¬ç ”ç©¶è¿˜è¿›è¡Œäº†åŒ…å«45åå‚ä¸è€…çš„äººç±»ç ”ç©¶ï¼Œä»¥è¯„ä¼°LLMç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆçš„å¯è¯»æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ‰68.63%çš„å‚ä¸è€…è®¤ä¸ºå¯è¯»æ€§æœ‰æ‰€æé«˜ã€‚æ€»ä½“è€Œè¨€ï¼Œè™½ç„¶LLMsåœ¨ä¿®å¤å¯ç»´æŠ¤æ€§é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¼•å…¥çš„é”™è¯¯ä¹Ÿå‡¸æ˜¾äº†å½“å‰å­˜åœ¨çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«ç”¨æ¥è§£å†³ç¼–ç é—®é¢˜ä¸­çš„å¯ç»´æŠ¤æ€§é—®é¢˜ã€‚</li>
<li>LLMsåœ¨è§£å†³GitHubä»“åº“ä¸­çš„å¯ç»´æŠ¤æ€§é—®é¢˜ä¸Šçš„æˆåŠŸç‡è¾¾åˆ°äº†ä¸€å®šçš„æ°´å¹³ã€‚</li>
<li>ä½¿ç”¨ä¸åŒçš„æç¤ºæ–¹æ³•ï¼ˆå¦‚é›¶å°„å’Œå°‘é‡å°„å‡»æç¤ºï¼‰å¯¹LLMsçš„æ•ˆæœæœ‰æ‰€å½±å“ã€‚</li>
<li>LLMç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆæœ‰æ—¶ä¼šå¯¼è‡´æ–°çš„é”™è¯¯æˆ–å¯ç»´æŠ¤æ€§é—®é¢˜ã€‚</li>
<li>äººç±»ç ”ç©¶ç»“æœè¡¨æ˜LLMç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆåœ¨æŸäº›æƒ…å†µä¸‹çš„å¯è¯»æ€§æœ‰æ‰€æé«˜ã€‚</li>
<li>LLMsåœ¨ä¿®å¤å¯ç»´æŠ¤æ€§é—®é¢˜æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e0cabc32ff58749a52ca25fd5d098a59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d253ecebdbb6db5a3c6177d81a62835f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5fde5ddc95e9358c05b341ceb19c22b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c751a09070ec9e6c5dd320fa55a4086.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18eeb4ffa66b115003ebc08ff4987195.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e285befc86ebc6358d4a61f83ca2dbd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Adaptive-Observation-Cost-Control-for-Variational-Quantum-Eigensolvers"><a href="#Adaptive-Observation-Cost-Control-for-Variational-Quantum-Eigensolvers" class="headerlink" title="Adaptive Observation Cost Control for Variational Quantum Eigensolvers"></a>Adaptive Observation Cost Control for Variational Quantum Eigensolvers</h2><p><strong>Authors:Christopher J. Anders, Kim A. Nicoli, Bingting Wu, Naima Elosegui, Samuele Pedrielli, Lena Funcke, Karl Jansen, Stefan KÃ¼hn, Shinichi Nakajima</strong></p>
<p>The objective to be minimized in the variational quantum eigensolver (VQE) has a restricted form, which allows a specialized sequential minimal optimization (SMO) that requires only a few observations in each iteration. However, the SMO iteration is still costly due to the observation noise â€“ one observation at a point typically requires averaging over hundreds to thousands of repeated quantum measurement shots for achieving a reasonable noise level. In this paper, we propose an adaptive cost control method, named subspace in confident region (SubsCoRe), for SMO. SubsCoRe uses the Gaussian process (GP) surrogate, and requires it to have low uncertainty over the subspace being updated, so that optimization in each iteration is performed with guaranteed accuracy. The adaptive cost control is performed by first setting the required accuracy according to the progress of the optimization, and then choosing the minimum number of measurement shots and their distribution such that the required accuracy is satisfied. We demonstrate that SubsCoRe significantly improves the efficiency of SMO, and outperforms the state-of-the-art methods. </p>
<blockquote>
<p>åœ¨å˜åˆ†é‡å­æœ¬å¾è§£ç®—å™¨ï¼ˆVQEï¼‰ä¸­ï¼Œè¦æœ€å°åŒ–çš„ç›®æ ‡å…·æœ‰å—é™å½¢å¼ï¼Œè¿™å…è®¸ä¸“é—¨çš„é¡ºåºæœ€å°ä¼˜åŒ–ï¼ˆSMOï¼‰ï¼Œæ¯æ¬¡è¿­ä»£åªéœ€è¦å‡ æ¬¡è§‚å¯Ÿã€‚ç„¶è€Œï¼Œç”±äºè§‚æµ‹å™ªå£°ï¼ŒSMOè¿­ä»£ä»ç„¶æˆæœ¬é«˜æ˜‚â€”â€”åœ¨ä¸€ä¸ªç‚¹ä¸Šçš„ä¸€æ¬¡è§‚æµ‹é€šå¸¸éœ€è¦é€šè¿‡æ•°ç™¾åˆ°æ•°åƒæ¬¡çš„é‡å¤é‡å­æµ‹é‡æ¥è·å¾—åˆç†çš„å™ªå£°æ°´å¹³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºSMOçš„è‡ªé€‚åº”æˆæœ¬æ§åˆ¶æ–¹æ³•ï¼Œåä¸ºå­ç©ºé—´ç½®ä¿¡åŒºåŸŸï¼ˆSubsCoReï¼‰ã€‚SubsCoReä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ä»£ç†ï¼Œè¦æ±‚å…¶åœ¨æ›´æ–°çš„å­ç©ºé—´ä¸Šå…·æœ‰ä½ä¸ç¡®å®šæ€§ï¼Œä»¥ç¡®ä¿æ¯æ¬¡è¿­ä»£çš„ä¼˜åŒ–å…·æœ‰ä¿è¯çš„å‡†ç¡®æ€§ã€‚è‡ªé€‚åº”æˆæœ¬æ§åˆ¶æ˜¯é€šè¿‡é¦–å…ˆæ ¹æ®ä¼˜åŒ–çš„è¿›åº¦è®¾ç½®æ‰€éœ€çš„å‡†ç¡®æ€§ï¼Œç„¶åé€‰æ‹©æœ€å°çš„æµ‹é‡æ¬¡æ•°åŠå…¶åˆ†å¸ƒä»¥æ»¡è¶³æ‰€éœ€çš„å‡†ç¡®æ€§æ¥å®ç°çš„ã€‚æˆ‘ä»¬è¯æ˜äº†SubsCoReå¯ä»¥æ˜¾è‘—æé«˜SMOçš„æ•ˆç‡å¹¶ä¼˜äºæœ€æ–°çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01704v1">PDF</a> 9 pages, 6 figures, 41st International Conference on Machine Learning   (ICML 2024)</p>
<p><strong>Summary</strong></p>
<p>åœ¨å˜åˆ†é‡å­æœ¬å¾æ±‚è§£å™¨ï¼ˆVQEï¼‰çš„ç›®æ ‡å‡½æ•°ä¸­ï¼Œæœ‰ä¸€ç§ç‰¹æ®Šå½¢å¼å…è®¸é‡‡ç”¨åªéœ€å‡ æ¬¡è§‚å¯Ÿçš„åºåˆ—æœ€å°ä¼˜åŒ–ï¼ˆSMOï¼‰ã€‚ç„¶è€Œï¼Œç”±äºè§‚æµ‹å™ªå£°ï¼ŒSMOè¿­ä»£ä»ç„¶æˆæœ¬é«˜æ˜‚â€”â€”åœ¨è¾¾åˆ°åˆç†çš„å™ªå£°æ°´å¹³æ—¶ï¼Œä¸€ä¸ªè§‚æµ‹ç‚¹é€šå¸¸éœ€è¦é‡å¤æ•°ç™¾è‡³æ•°åƒæ¬¡çš„é‡å­æµ‹é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå­ç©ºé—´ç½®ä¿¡åŒºåŸŸï¼ˆSubsCoReï¼‰çš„é€‚åº”æ€§æˆæœ¬æ§åˆ¶æ–¹æ³•ï¼Œç”¨äºSMOã€‚SubsCoReä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ä»£ç†ï¼Œè¦æ±‚å…¶åœ¨æ›´æ–°çš„å­ç©ºé—´ä¸Šå…·æœ‰ä½ä¸ç¡®å®šæ€§ï¼Œä»¥ç¡®ä¿æ¯æ¬¡è¿­ä»£çš„ä¼˜åŒ–å‡†ç¡®æ€§ã€‚é€‚åº”æ€§æˆæœ¬æ§åˆ¶æ˜¯æ ¹æ®ä¼˜åŒ–çš„è¿›åº¦æ¥è®¾å®šæ‰€éœ€çš„ç²¾åº¦ï¼Œç„¶åé€‰æ‹©æœ€å°‘çš„æµ‹é‡æ¬¡æ•°åŠå…¶åˆ†å¸ƒæ¥æ»¡è¶³æ‰€éœ€çš„ç²¾åº¦è¦æ±‚ã€‚æˆ‘ä»¬è¯æ˜SubsCoReæ˜¾è‘—æé«˜äº†SMOçš„æ•ˆç‡ï¼Œå¹¶ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜åˆ†é‡å­æœ¬å¾æ±‚è§£å™¨ï¼ˆVQEï¼‰çš„ç›®æ ‡å‡½æ•°å…·æœ‰ç‰¹å®šå½¢å¼ï¼Œå…è®¸åºåˆ—æœ€å°ä¼˜åŒ–ï¼ˆSMOï¼‰ã€‚</li>
<li>ç”±äºè§‚æµ‹å™ªå£°ï¼ŒSMOè¿­ä»£æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åä¸ºå­ç©ºé—´ç½®ä¿¡åŒºåŸŸï¼ˆSubsCoReï¼‰çš„é€‚åº”æ€§æˆæœ¬æ§åˆ¶æ–¹æ³•ã€‚</li>
<li>SubsCoReä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ä»£ç†æ¥æé«˜ä¼˜åŒ–çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ ¹æ®ä¼˜åŒ–çš„è¿›åº¦è®¾å®šæ‰€éœ€çš„ç²¾åº¦ã€‚</li>
<li>é€šè¿‡é€‰æ‹©æœ€å°‘çš„æµ‹é‡æ¬¡æ•°åŠå…¶åˆ†å¸ƒæ¥æ»¡è¶³æ‰€éœ€çš„ç²¾åº¦è¦æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-41c52dce243bf17f9ca4a563ef1a43cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15ba17608a03191b2eafce2b1998ae13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aa993d460cda537ef9fc77a3cf95abe.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BARE-Combining-Base-and-Instruction-Tuned-Language-Models-for-Better-Synthetic-Data-Generation"><a href="#BARE-Combining-Base-and-Instruction-Tuned-Language-Models-for-Better-Synthetic-Data-Generation" class="headerlink" title="BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation"></a>BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation</h2><p><strong>Authors:Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia</strong></p>
<p>As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs-a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT. </p>
<blockquote>
<p>éšç€æ¨¡å‹è®­ç»ƒä¸­å¯¹äºé«˜è´¨é‡æ•°æ®éœ€æ±‚çš„å¢é•¿ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ­£è¶Šæ¥è¶Šå¤šåœ°ç”Ÿæˆåˆæˆæ•°æ®æ¥è°ƒæ•´å¹¶è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚å…³äºåˆæˆæ•°æ®çš„ä¸€ä¸ªæ™®éå‡è®¾æ˜¯ï¼Œä»æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸­é‡‡æ ·å°±è¶³å¤Ÿäº†ï¼›ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨äº§ç”Ÿå¤šæ ·åŒ–è¾“å‡ºæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™æ˜¯æ³›åŒ–çš„ä¸€ä¸ªå…³é”®è¦æ±‚ã€‚å°½ç®¡æœ‰å„ç§æç¤ºæ–¹æ³•ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ä»æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸­å®ç°æœ‰æ„ä¹‰çš„å¤šæ ·æ€§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å‘ç°æ²¡æœ‰ç»è¿‡åè®­ç»ƒçš„åŸºå‡†æ¨¡å‹è¡¨ç°å‡ºæ›´å¤§çš„å¤šæ ·æ€§ï¼Œä½†åœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›è¾ƒå¼±ï¼Œå› æ­¤è´¨é‡è¾ƒä½ã€‚åˆ©ç”¨è¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Base-Refineï¼ˆBAREï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ç»“åˆäº†åŸºå‡†æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„è´¨é‡ã€‚é€šè¿‡å°‘é‡çš„ç¤ºä¾‹å’Œç­›é€‰ï¼ŒBAREå¯ä»¥ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬å±•ç¤ºä½¿ç”¨ä»…1000ä¸ªBAREç”Ÿæˆæ ·æœ¬è¿›è¡Œå¾®è°ƒæ—¶ï¼Œå…¶åœ¨LiveCodeBenchä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯ä»¥è¾¾åˆ°ä¸æœ€ä½³ç›¸ä¼¼è§„æ¨¡æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œä½¿ç”¨BAREç”Ÿæˆçš„æ•°æ®è¿›è¡Œå¾®è°ƒåœ¨GSM8Kä¸Šå®ç°äº†æ¯”ä»…ä½¿ç”¨æŒ‡ä»¤æ•°æ®æé«˜101%çš„æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨RAFTä¸Šç›¸å¯¹äºæœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†18.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01697v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç ”ç©¶è€…å‘ç°å•çº¯ä¾èµ–åŸºäºæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®å­˜åœ¨è¾“å‡ºå¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œè€ŒåŸºç¡€æ¨¡å‹è™½ç„¶è¾“å‡ºå¤šæ ·ä½†éš¾ä»¥éµå¾ªæŒ‡ä»¤ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºBase-Refineï¼ˆBAREï¼‰çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆåŸºç¡€æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„è´¨é‡ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ç”Ÿæˆæ—¢å¤šæ ·åˆé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œèƒ½æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ç ”ç©¶æ˜¾ç¤ºï¼Œä½¿ç”¨BAREç”Ÿæˆçš„æ•°æ®é›†åªéœ€å°‘é‡æ ·æœ¬å³å¯å®ç°è‰¯å¥½çš„æ€§èƒ½ã€‚ç›¸è¾ƒäºåªä½¿ç”¨æŒ‡ä»¤ç”Ÿæˆçš„æ•°æ®é›†åœ¨GSM8Kä¸Šæ€§èƒ½æå‡è¾¾101%ï¼Œç›¸è¾ƒäºRAFTæ•°æ®é›†ä¸Šçš„ç°æœ‰æœ€ä½³æ–¹æ³•æ€§èƒ½æå‡è¾¾18.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆæ•°æ®åœ¨æ¨¡å‹è®­ç»ƒä¸­çš„éœ€æ±‚å¢é•¿ï¼Œç ”ç©¶è€…å°è¯•é€šè¿‡ä¸åŒæ–¹æ³•ç”Ÿæˆåˆæˆæ•°æ®ä»¥è®­ç»ƒLLMsã€‚</li>
<li>åŸºäºæŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨ç”Ÿæˆåˆæˆæ•°æ®æ—¶é¢ä¸´è¾“å‡ºå¤šæ ·æ€§æŒ‘æˆ˜ã€‚</li>
<li>åŸºç¡€æ¨¡å‹è™½èƒ½äº§å‡ºå¤šæ ·è¾“å‡ºï¼Œä½†éš¾ä»¥éµå¾ªæŒ‡ä»¤ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºBase-Refineï¼ˆBAREï¼‰çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆäº†åŸºç¡€æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„è´¨é‡ã€‚</li>
<li>BAREé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ç”Ÿæˆæ—¢å¤šæ ·åˆé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œèƒ½æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨BAREç”Ÿæˆçš„æ•°æ®é›†åªéœ€å°‘é‡æ ·æœ¬å³å¯å®ç°è‰¯å¥½çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c0281da528061c674534a3eec671a51a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2aedf490ff7ee0cbb3ff7b1063b933f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ecc96ecda174238e70bc680753559e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81d93849a1a7b4bb94a319e97c5973c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a2a1fce24198ceb9f7f7f2a168d9850.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Exploring-Few-Shot-Defect-Segmentation-in-General-Industrial-Scenarios-with-Metric-Learning-and-Vision-Foundation-Models"><a href="#Exploring-Few-Shot-Defect-Segmentation-in-General-Industrial-Scenarios-with-Metric-Learning-and-Vision-Foundation-Models" class="headerlink" title="Exploring Few-Shot Defect Segmentation in General Industrial Scenarios   with Metric Learning and Vision Foundation Models"></a>Exploring Few-Shot Defect Segmentation in General Industrial Scenarios   with Metric Learning and Vision Foundation Models</h2><p><strong>Authors:Tongkun Liu, Bing Li, Xiao Jin, Yupeng Shi, Qiuying Li, Xiang Wei</strong></p>
<p>Industrial defect segmentation is critical for manufacturing quality control. Due to the scarcity of training defect samples, few-shot semantic segmentation (FSS) holds significant value in this field. However, existing studies mostly apply FSS to tackle defects on simple textures, without considering more diverse scenarios. This paper aims to address this gap by exploring FSS in broader industrial products with various defect types. To this end, we contribute a new real-world dataset and reorganize some existing datasets to build a more comprehensive few-shot defect segmentation (FDS) benchmark. On this benchmark, we thoroughly investigate metric learning-based FSS methods, including those based on meta-learning and those based on Vision Foundation Models (VFMs). We observe that existing meta-learning-based methods are generally not well-suited for this task, while VFMs hold great potential. We further systematically study the applicability of various VFMs in this task, involving two paradigms: feature matching and the use of Segment Anything (SAM) models. We propose a novel efficient FDS method based on feature matching. Meanwhile, we find that SAM2 is particularly effective for addressing FDS through its video track mode. The contributed dataset and code will be available at: <a target="_blank" rel="noopener" href="https://github.com/liutongkun/GFDS">https://github.com/liutongkun/GFDS</a>. </p>
<blockquote>
<p>å·¥ä¸šç¼ºé™·åˆ†å‰²å¯¹äºåˆ¶é€ è´¨é‡æ§åˆ¶è‡³å…³é‡è¦ã€‚ç”±äºè®­ç»ƒç¼ºé™·æ ·æœ¬çš„ç¨€ç¼ºæ€§ï¼Œå°æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰åœ¨è¯¥é¢†åŸŸå…·æœ‰é‡å¤§æ„ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¤§å¤šå°†FSSåº”ç”¨äºç®€å•çº¹ç†ä¸Šçš„ç¼ºé™·æ£€æµ‹ï¼Œå¹¶æœªè€ƒè™‘æ›´å¤šæ ·åŒ–çš„åœºæ™¯ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æ¢ç´¢FSSåœ¨æ›´å¹¿æ³›çš„å·¥ä¸šäº§å“ä¸­çš„å¤šç§ç¼ºé™·ç±»å‹æ¥è§£å†³è¿™ä¸€å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªæ–°çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œå¹¶é‡ç»„äº†ä¸€äº›ç°æœ‰æ•°æ®é›†æ¥å»ºç«‹ä¸€ä¸ªæ›´å…¨é¢çš„å°‘æ ·æœ¬ç¼ºé™·åˆ†å‰²ï¼ˆFDSï¼‰åŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬å…¨é¢ç ”ç©¶äº†åŸºäºåº¦é‡å­¦ä¹ çš„FSSæ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºå…ƒå­¦ä¹ å’ŒåŸºäºè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„åŸºäºå…ƒå­¦ä¹ çš„æ–¹æ³•é€šå¸¸ä¸é€‚åˆè¿™é¡¹ä»»åŠ¡ï¼Œè€ŒVFMså…·æœ‰å·¨å¤§æ½œåŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç³»ç»Ÿåœ°ç ”ç©¶äº†å„ç§VFMsåœ¨æ­¤ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œæ¶‰åŠç‰¹å¾åŒ¹é…å’Œä½¿ç”¨Segment Anythingï¼ˆSAMï¼‰æ¨¡å‹ä¸¤ç§èŒƒå¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾åŒ¹é…çš„æ–°å‹é«˜æ•ˆFDSæ–¹æ³•ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‘ç°SAM2é€šè¿‡å…¶è§†é¢‘è·Ÿè¸ªæ¨¡å¼åœ¨è§£å†³FDSæ–¹é¢ç‰¹åˆ«æœ‰æ•ˆã€‚ç›¸å…³çš„æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/liutongkun/GFDS%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/liutongkun/GFDSä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01216v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å·¥ä¸šç¼ºé™·åˆ†å‰²é¢†åŸŸä¸­çš„å°æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®€å•çº¹ç†ç¼ºé™·ä¸Šï¼Œè€Œå¿½ç•¥æ›´å¤šæ ·åœºæ™¯çš„é—®é¢˜ï¼Œæœ¬æ–‡æ—¨åœ¨é€šè¿‡æ¢ç´¢FSSåœ¨æ›´å¹¿æ³›çš„å·¥ä¸šäº§å“ä¸­çš„å¤šç§ç¼ºé™·ç±»å‹æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚ä¸ºæ­¤ï¼Œä½œè€…è´¡çŒ®äº†ä¸€ä¸ªæ–°çš„çœŸå®ä¸–ç•Œæ•°æ®é›†å¹¶é‡æ–°ç»„ç»‡äº†ç°æœ‰çš„æ•°æ®é›†ä»¥å»ºç«‹ä¸€ä¸ªæ›´å…¨é¢çš„å°‘æ ·æœ¬ç¼ºé™·åˆ†å‰²ï¼ˆFDSï¼‰åŸºå‡†æµ‹è¯•ã€‚ä½œè€…æ·±å…¥ç ”ç©¶äº†åŸºäºåº¦é‡å­¦ä¹ çš„FSSæ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºå…ƒå­¦ä¹ å’ŒåŸºäºè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„åŸºäºå…ƒå­¦ä¹ çš„æ–¹æ³•é€šå¸¸ä¸é€‚ç”¨äºè¿™é¡¹ä»»åŠ¡ï¼Œè€ŒVFMså…·æœ‰å·¨å¤§æ½œåŠ›ã€‚æ­¤å¤–ï¼Œä½œè€…ç³»ç»Ÿåœ°ç ”ç©¶äº†å„ç§VFMsåœ¨æ­¤ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¹¶åŸºäºç‰¹å¾åŒ¹é…æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆFDSæ–¹æ³•ã€‚åŒæ—¶ï¼Œå‘ç°SAM2æ¨¡å‹é€šè¿‡è§†é¢‘è·Ÿè¸ªæ¨¡å¼åœ¨è§£å†³FDSæ–¹é¢ç‰¹åˆ«æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä¸šç¼ºé™·åˆ†å‰²ä¸­ï¼Œå°æ ·è¯­ä¹‰æœ¬åˆ†å‰²ï¼ˆFSSï¼‰å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› è®­ç»ƒç¼ºé™·æ ·æœ¬ç¨€ç¼ºã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ç®€å•çº¹ç†ç¼ºé™·çš„FSSï¼Œç¼ºä¹å¤šæ ·åœºæ™¯è€ƒè™‘ã€‚</li>
<li>æœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„å°‘æ ·æœ¬ç¼ºé™·åˆ†å‰²ï¼ˆFDSï¼‰åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ–°çš„çœŸå®ä¸–ç•Œæ•°æ®é›†å’Œé‡ç»„çš„ç°æœ‰æ•°æ®é›†ã€‚</li>
<li>åŸºäºåº¦é‡å­¦ä¹ çš„FSSæ–¹æ³•è¢«æ·±å…¥ç ”ç©¶ï¼ŒåŒ…æ‹¬åŸºäºå…ƒå­¦ä¹ å’ŒåŸºäºè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„æ–¹æ³•ã€‚</li>
<li>ç°æœ‰åŸºäºå…ƒå­¦ä¹ çš„æ–¹æ³•åœ¨FDSä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€ŒVFMså±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ä½œè€…æå‡ºäº†åŸºäºç‰¹å¾åŒ¹é…çš„æ–°é¢–é«˜æ•ˆFDSæ–¹æ³•ï¼Œå¹¶å‘ç°SAM2æ¨¡å‹é€šè¿‡è§†é¢‘è·Ÿè¸ªæ¨¡å¼ç‰¹åˆ«æœ‰æ•ˆã€‚</li>
<li>è´¡çŒ®çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d36c2a5713763f32ee7c2beec59a0ceb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26a49516f1ea298ffc6792c8c7f06249.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a45d236ddc333feeb85a3dfe270c270.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ba32e10977d52affb1a6c7557092f6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86b0811ed7d18764134633c28defc75d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ca6fa42cbe8ed3589cbe7474478af93.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="One-to-Normal-Anomaly-Personalization-for-Few-shot-Anomaly-Detection"><a href="#One-to-Normal-Anomaly-Personalization-for-Few-shot-Anomaly-Detection" class="headerlink" title="One-to-Normal: Anomaly Personalization for Few-shot Anomaly Detection"></a>One-to-Normal: Anomaly Personalization for Few-shot Anomaly Detection</h2><p><strong>Authors:Yiyue Li, Shaoting Zhang, Kang Li, Qicheng Lao</strong></p>
<p>Traditional Anomaly Detection (AD) methods have predominantly relied on unsupervised learning from extensive normal data. Recent AD methods have evolved with the advent of large pre-trained vision-language models, enhancing few-shot anomaly detection capabilities. However, these latest AD methods still exhibit limitations in accuracy improvement. One contributing factor is their direct comparison of a query imageâ€™s features with those of few-shot normal images. This direct comparison often leads to a loss of precision and complicates the extension of these techniques to more complex domainsâ€“an area that remains underexplored in a more refined and comprehensive manner. To address these limitations, we introduce the anomaly personalization method, which performs a personalized one-to-normal transformation of query images using an anomaly-free customized generation model, ensuring close alignment with the normal manifold. Moreover, to further enhance the stability and robustness of prediction results, we propose a triplet contrastive anomaly inference strategy, which incorporates a comprehensive comparison between the query and generated anomaly-free data pool and prompt information. Extensive evaluations across eleven datasets in three domains demonstrate our modelâ€™s effectiveness compared to the latest AD methods. Additionally, our method has been proven to transfer flexibly to other AD methods, with the generated image data effectively improving the performance of other AD methods. </p>
<blockquote>
<p>ä¼ ç»Ÿå¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰æ–¹æ³•ä¸»è¦ä¾èµ–äºä»å¤§é‡æ­£å¸¸æ•°æ®ä¸­è¿›è¡Œçš„æ— ç›‘ç£å­¦ä¹ ã€‚éšç€å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œæœ€è¿‘çš„ADæ–¹æ³•å·²ç»å‘å±•å¹¶å¢å¼ºäº†å°æ ·æœ¬å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æœ€æ–°çš„ADæ–¹æ³•åœ¨ç²¾åº¦æé«˜æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚ä¸€ä¸ªåŸå› æ˜¯å®ƒä»¬ç›´æ¥å°†æŸ¥è¯¢å›¾åƒçš„ç‰¹å¾ä¸å°‘æ•°æ­£å¸¸å›¾åƒè¿›è¡Œæ¯”è¾ƒã€‚è¿™ç§ç›´æ¥æ¯”è¾ƒå¾€å¾€ä¼šå¯¼è‡´ç²¾åº¦æŸå¤±ï¼Œå¹¶ä½¿å¾—å°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°æ›´å¤æ‚é¢†åŸŸæ›´åŠ å¤æ‚â€”â€”è¿™ä¸€é¢†åŸŸä»éœ€è¦ä»¥æ›´ç²¾ç»†å’Œå…¨é¢çš„æ–¹å¼è¿›è¡Œæ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¼‚å¸¸ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ— å¼‚å¸¸å®šåˆ¶ç”Ÿæˆæ¨¡å‹å¯¹æŸ¥è¯¢å›¾åƒè¿›è¡Œä¸ªæ€§åŒ–çš„ä¸€å¯¹ä¸€æ­£å¸¸è½¬æ¢ï¼Œç¡®ä¿ä¸æ­£å¸¸æµå½¢ç´§å¯†å¯¹é½ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºé¢„æµ‹ç»“æœçš„ç¨³å®šæ€§å’Œé²æ£’æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰å…ƒå¯¹æ¯”å¼‚å¸¸æ¨ç†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨æŸ¥è¯¢å’Œç”Ÿæˆçš„å¼‚å¸¸æ•°æ®æ± ä»¥åŠæç¤ºä¿¡æ¯ä¹‹é—´è¿›è¡Œå…¨é¢çš„æ¯”è¾ƒã€‚åœ¨ä¸‰ä¸ªé¢†åŸŸçš„åä¸€ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†æˆ‘ä»¬æ¨¡å‹ä¸æœ€æ–°ADæ–¹æ³•ç›¸æ¯”çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¢«è¯æ˜å¯ä»¥çµæ´»åœ°è½¬ç§»åˆ°å…¶ä»–ADæ–¹æ³•ï¼Œç”Ÿæˆçš„å›¾åƒæ•°æ®æœ‰æ•ˆåœ°æé«˜äº†å…¶ä»–ADæ–¹æ³•çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01201v1">PDF</a> In The Thirty-eighth Annual Conference on Neural Information   Processing Systems (NeurIPS2024)</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰æ–¹æ³•å·²ç»éšç€å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å‡ºç°è€Œå‘å±•ï¼Œæå‡äº†å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨ç²¾åº¦æå‡æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦æ˜¯ç”±äºå®ƒä»¬ç›´æ¥æ¯”è¾ƒæŸ¥è¯¢å›¾åƒçš„ç‰¹å¾ä¸å°‘é‡æ­£å¸¸å›¾åƒçš„ç‰¹å¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¼‚å¸¸ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨æ— å¼‚å¸¸å®šåˆ¶ç”Ÿæˆæ¨¡å‹å¯¹æŸ¥è¯¢å›¾åƒè¿›è¡Œä¸ªæ€§åŒ–æ­£å¸¸è½¬æ¢ï¼ŒåŒæ—¶é‡‡ç”¨ä¸‰å…ƒå¯¹æ¯”å¼‚å¸¸æ¨ç†ç­–ç•¥ï¼Œæé«˜é¢„æµ‹ç»“æœçš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚åœ¨ä¸‰ä¸ªé¢†åŸŸçš„åä¸€ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯çµæ´»è¿ç§»è‡³å…¶ä»–ADæ–¹æ³•ï¼Œç”Ÿæˆçš„å›¾åƒæ•°æ®å¯æœ‰æ•ˆæå‡å…¶ä»–ADæ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸADæ–¹æ³•ä¾èµ–é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>ç›´æ¥æ¯”è¾ƒæŸ¥è¯¢å›¾åƒä¸å°‘é‡æ­£å¸¸å›¾åƒçš„ç‰¹å¾å­˜åœ¨ç²¾åº¦å±€é™æ€§ã€‚</li>
<li>å¼•å…¥å¼‚å¸¸ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œé€šè¿‡æ— å¼‚å¸¸å®šåˆ¶ç”Ÿæˆæ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–æ­£å¸¸è½¬æ¢ã€‚</li>
<li>é‡‡ç”¨ä¸‰å…ƒå¯¹æ¯”å¼‚å¸¸æ¨ç†ç­–ç•¥ï¼Œæé«˜é¢„æµ‹ç»“æœçš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•å¯ä»¥çµæ´»è¿ç§»è‡³å…¶ä»–ADæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01201">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdb32c196d02217e52df7421d13a8436.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8962c4cb00e3ebf0ddc639072439d4f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-Environmental-Robustness-in-Few-shot-Learning-via-Conditional-Representation-Learning"><a href="#Enhancing-Environmental-Robustness-in-Few-shot-Learning-via-Conditional-Representation-Learning" class="headerlink" title="Enhancing Environmental Robustness in Few-shot Learning via Conditional   Representation Learning"></a>Enhancing Environmental Robustness in Few-shot Learning via Conditional   Representation Learning</h2><p><strong>Authors:Qianyu Guo, Jingrong Wu, Tianxing Wu, Haofen Wang, Weifeng Ge, Wenqiang Zhang</strong></p>
<p>Few-shot learning (FSL) has recently been extensively utilized to overcome the scarcity of training data in domain-specific visual recognition. In real-world scenarios, environmental factors such as complex backgrounds, varying lighting conditions, long-distance shooting, and moving targets often cause test images to exhibit numerous incomplete targets or noise disruptions. However, current research on evaluation datasets and methodologies has largely ignored the concept of â€œenvironmental robustnessâ€, which refers to maintaining consistent performance in complex and diverse physical environments. This neglect has led to a notable decline in the performance of FSL models during practical testing compared to their training performance. To bridge this gap, we introduce a new real-world multi-domain few-shot learning (RD-FSL) benchmark, which includes four domains and six evaluation datasets. The test images in this benchmark feature various challenging elements, such as camouflaged objects, small targets, and blurriness. Our evaluation experiments reveal that existing methods struggle to utilize training images effectively to generate accurate feature representations for challenging test images. To address this problem, we propose a novel conditional representation learning network (CRLNet) that integrates the interactions between training and testing images as conditional information in their respective representation processes. The main goal is to reduce intra-class variance or enhance inter-class variance at the feature representation level. Finally, comparative experiments reveal that CRLNet surpasses the current state-of-the-art methods, achieving performance improvements ranging from 6.83% to 16.98% across diverse settings and backbones. The source code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/guoqianyu-alberta/Conditional-Representation-Learning">https://github.com/guoqianyu-alberta/Conditional-Representation-Learning</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æœ€è¿‘å·²è¢«å¹¿æ³›ç”¨åº”ç”¨äºå…‹æœç‰¹å®šé¢†åŸŸçš„è§†è§‰è¯†åˆ«ä¸­è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ã€‚åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œç¯å¢ƒå› ç´ å¦‚å¤æ‚èƒŒæ™¯ã€ä¸åŒç…§æ˜æ¡ä»¶ã€è¿œè·ç¦»æ‹æ‘„å’Œç§»åŠ¨ç›®æ ‡ç­‰å¸¸å¯¼è‡´æµ‹è¯•å›¾åƒå‡ºç°è®¸å¤šä¸å®Œæ•´çš„ç›®æ ‡æˆ–å™ªå£°å¹²æ‰°ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶åœ¨è¯„ä¼°æ•°æ®é›†å’Œæ–¹æ³•è®ºæ—¶ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†â€œç¯å¢ƒç¨³å¥æ€§â€çš„æ¦‚å¿µï¼Œå³æŒ‡åœ¨å¤æ‚å’Œå¤šæ ·åŒ–çš„ç‰©ç†ç¯å¢ƒä¸­ä¿æŒä¸€è‡´æ€§è¡¨ç°ã€‚è¿™ç§å¿½è§†å¯¼è‡´äº†FSLæ¨¡å‹åœ¨å®é™…æµ‹è¯•ä¸­çš„æ€§èƒ½ä¸è®­ç»ƒæ€§èƒ½ç›¸æ¯”å‡ºç°äº†æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„çœŸå®ä¸–ç•Œå¤šåŸŸå°‘é‡å­¦ä¹ ï¼ˆRD-FSLï¼‰åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬å››ä¸ªåŸŸå’Œå…­ä¸ªè¯„ä¼°æ•°æ®é›†ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸­çš„æµ‹è¯•å›¾åƒå…·æœ‰å„ç§æŒ‘æˆ˜æ€§çš„å…ƒç´ ï¼Œä¾‹å¦‚éšè”½ç‰©ä½“ã€å°ç›®æ ‡å’Œæ¨¡ç³Šæ€§ç­‰ã€‚æˆ‘ä»¬çš„è¯„ä¼°å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨è®­ç»ƒå›¾åƒä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å›¾åƒç”Ÿæˆå‡†ç¡®çš„ç‰¹å¾è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ¡ä»¶è¡¨ç¤ºå­¦ä¹ ç½‘ç»œï¼ˆCRLNetï¼‰ï¼Œå®ƒå°†è®­ç»ƒå›¾åƒå’Œæµ‹è¯•å›¾åƒä¹‹é—´çš„äº¤äº’ä½œä¸ºå„è‡ªè¡¨ç¤ºè¿‡ç¨‹ä¸­çš„æ¡ä»¶ä¿¡æ¯ã€‚ä¸»è¦ç›®æ ‡æ˜¯åœ¨ç‰¹å¾è¡¨ç¤ºå±‚é¢é™ä½ç±»å†…æ–¹å·®æˆ–å¢å¼ºç±»é—´æ–¹å·®ã€‚æœ€åï¼Œå¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒCRLNetè¶…è¶Šäº†å½“å‰æœ€å‰æ²¿çš„æ–¹æ³•ï¼Œåœ¨å¤šç§è®¾ç½®å’Œä¸»å¹²ç½‘ç»œä¸Šå®ç°äº†æ€§èƒ½æå‡ï¼ŒèŒƒå›´ä»6.8 é—´æ¥ç¿»è¯‘å¯èƒ½å¼•èµ·è¯­ä¹‰ä¸Šçš„æ··æ·†å’Œè¯¯è§£ï¼Œä»¥ä¸Šç¿»è¯‘ä»…ä¾›å‚è€ƒã€‚å¦‚æœæ‚¨æœ‰å…¶ä»–æ›´å…·ä½“çš„è¯­å¢ƒæˆ–éœ€æ±‚ï¼Œæˆ‘ä¼šæ›´ä¹æ„è¿›ä¸€æ­¥å¸®åŠ©æ‚¨å®Œæˆç¿»è¯‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01183v1">PDF</a> 15 pages, 8 figures, Accepted by IEEE Transactions on Image   Processing</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æŒ‘æˆ˜ï¼Œå¦‚ç¯å¢ƒå¤šæ ·æ€§å’Œå¤æ‚æ€§å¯¼è‡´çš„æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†æ–°çš„çœŸå®ä¸–ç•Œå¤šåŸŸå°‘æ ·æœ¬å­¦ä¹ ï¼ˆRD-FSLï¼‰åŸºå‡†æµ‹è¯•ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ¡ä»¶è¡¨ç¤ºå­¦ä¹ ç½‘ç»œï¼ˆCRLNetï¼‰ã€‚CRLNeté€šè¿‡æ•´åˆè®­ç»ƒå’Œæµ‹è¯•å›¾åƒä¹‹é—´çš„äº¤äº’ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼Œæ—¨åœ¨é™ä½ç±»å†…æ–¹å·®æˆ–å¢å¼ºç±»é—´æ–¹å·®ï¼Œä»è€Œæé«˜ç‰¹å¾è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCRLNetåœ¨å¤šç§è®¾ç½®å’Œä¸»å¹²ç½‘ç»œä¸Šå‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†ä»6.83%åˆ°16.98%çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰åœ¨ç‰¹å®šé¢†åŸŸçš„è§†è§‰è¯†åˆ«ä¸­ï¼Œå› è®­ç»ƒæ•°æ®ç¨€ç¼ºè€Œå—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç°å®åœºæ™¯ä¸­çš„ç¯å¢ƒå› ç´ å½±å“æµ‹è¯•å›¾åƒçš„è´¨é‡ï¼Œå¯¼è‡´ç°æœ‰FSLæ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>å¼•å…¥æ–°çš„çœŸå®ä¸–ç•Œå¤šåŸŸå°‘æ ·æœ¬å­¦ä¹ ï¼ˆRD-FSLï¼‰åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å››ä¸ªåŸŸå’Œå…­ä¸ªè¯„ä¼°æ•°æ®é›†ã€‚</li>
<li>æµ‹è¯•å›¾åƒå…·æœ‰å¤šç§æŒ‘æˆ˜å…ƒç´ ï¼Œå¦‚éšè”½ç‰©ä½“ã€å°ç›®æ ‡å’Œæ¨¡ç³Šæ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è®­ç»ƒå›¾åƒä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å›¾åƒç”Ÿæˆå‡†ç¡®ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>æå‡ºæ–°çš„æ¡ä»¶è¡¨ç¤ºå­¦ä¹ ç½‘ç»œï¼ˆCRLNetï¼‰ï¼Œé€šè¿‡æ•´åˆè®­ç»ƒå’Œæµ‹è¯•å›¾åƒé—´çš„äº¤äº’æ¥æå‡ç‰¹å¾è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚</li>
<li>CRLNetåœ¨å¤šç§è®¾ç½®å’Œä¸»å¹²ç½‘ç»œä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-417bf18b665c581ab773158301208acf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-421c5c25582498609a5b67ebab002425.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b59f82e9ce70f9dab5587fd60e2a6b10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a2e7f516e7c0d95e6d1712638caf50b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5840aded1655914389d966783e313f91.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Learning-to-Learn-Weight-Generation-via-Trajectory-Diffusion"><a href="#Learning-to-Learn-Weight-Generation-via-Trajectory-Diffusion" class="headerlink" title="Learning to Learn Weight Generation via Trajectory Diffusion"></a>Learning to Learn Weight Generation via Trajectory Diffusion</h2><p><strong>Authors:Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Serge Belongie, Jenq-Neng Hwang, Lei Li</strong></p>
<p>Diffusion-based algorithms have emerged as promising techniques for weight generation, particularly in scenarios like multi-task learning that require frequent weight updates. However, existing solutions suffer from limited cross-task transferability. In addition, they only utilize optimal weights as training samples, ignoring the value of other weights in the optimization process. To address these issues, we propose Lt-Di, which integrates the diffusion algorithm with meta-learning to generate weights for unseen tasks. Furthermore, we extend the vanilla diffusion algorithm into a trajectory diffusion algorithm to utilize other weights along the optimization trajectory. Trajectory diffusion decomposes the entire diffusion chain into multiple shorter ones, improving training and inference efficiency. We analyze the convergence properties of the weight generation paradigm and improve convergence efficiency without additional time overhead. Our experiments demonstrate Lt-Diâ€™s higher accuracy while reducing computational overhead across various tasks, including zero-shot and few-shot learning, multi-domain generalization, and large-scale language model fine-tuning.Our code is released at <a target="_blank" rel="noopener" href="https://github.com/tuantuange/Lt-Di">https://github.com/tuantuange/Lt-Di</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç®—æ³•å·²æˆä¸ºæƒé‡ç”Ÿæˆçš„æœ‰å‰é€”çš„æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é¢‘ç¹æƒé‡æ›´æ–°çš„å¤šä»»åŠ¡å­¦ä¹ ç­‰åœºæ™¯ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆå­˜åœ¨è·¨ä»»åŠ¡è¿ç§»èƒ½åŠ›æœ‰é™çš„ç¼ºé™·ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä»…å°†æœ€ä½³æƒé‡ä½œä¸ºè®­ç»ƒæ ·æœ¬ï¼Œå¿½ç•¥äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­å…¶ä»–æƒé‡çš„ä»·å€¼ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Lt-Diï¼Œå®ƒå°†æ‰©æ•£ç®—æ³•ä¸å…ƒå­¦ä¹ ç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆæœªè§ä»»åŠ¡çš„æƒé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ™®é€šçš„æ‰©æ•£ç®—æ³•æ‰©å±•ä¸ºè½¨è¿¹æ‰©æ•£ç®—æ³•ï¼Œä»¥åˆ©ç”¨ä¼˜åŒ–è½¨è¿¹ä¸­çš„å…¶ä»–æƒé‡ã€‚è½¨è¿¹æ‰©æ•£å°†æ•´ä¸ªæ‰©æ•£é“¾åˆ†è§£ä¸ºå¤šä¸ªè¾ƒçŸ­çš„é“¾ï¼Œæé«˜äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚æˆ‘ä»¬åˆ†æäº†æƒé‡ç”ŸæˆèŒƒå¼çš„æ”¶æ•›ç‰¹æ€§ï¼Œæé«˜äº†æ”¶æ•›æ•ˆç‡ï¼Œä¸”æ²¡æœ‰é¢å¤–çš„æ—¶é—´å¼€é”€ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLt-Diåœ¨å„ç§ä»»åŠ¡ä¸­å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†è®¡ç®—å¼€é”€ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ã€å¤šåŸŸæ³›åŒ–å’Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¾®è°ƒã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/tuantuange/Lt-Di%E3%80%82">https://github.com/tuantuange/Lt-Diã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01117v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£ç®—æ³•å’Œå…ƒå­¦ä¹ çš„æ–°æ–¹æ³•Lt-Diï¼Œç”¨äºä¸ºæœªè§ä»»åŠ¡ç”Ÿæˆæƒé‡ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰è§£å†³æ–¹æ¡ˆä¸­è·¨ä»»åŠ¡è¿ç§»èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†è½¨è¿¹æ‰©æ•£ç®—æ³•ï¼Œåˆ©ç”¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å…¶ä»–æƒé‡ã€‚Lt-Dièƒ½æé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶æ”¹å–„æƒé‡ç”ŸæˆèŒƒå¼çš„æ”¶æ•›æ€§èƒ½ï¼Œé™ä½è®¡ç®—å¼€é”€ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§ä»»åŠ¡ä¸Šå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lt-Diç»“åˆäº†æ‰©æ•£ç®—æ³•å’Œå…ƒå­¦ä¹ ï¼Œç”¨äºç”Ÿæˆæœªè§ä»»åŠ¡çš„æƒé‡ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆå­˜åœ¨è·¨ä»»åŠ¡è¿ç§»èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œè€ŒLt-Diè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>Lt-Diå¼•å…¥äº†è½¨è¿¹æ‰©æ•£ç®—æ³•ï¼Œåˆ©ç”¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å…¶ä»–æƒé‡ã€‚</li>
<li>Lt-Dié€šè¿‡åˆ†è§£æ•´ä¸ªæ‰©æ•£é“¾ä¸ºå¤šä¸ªè¾ƒçŸ­çš„é“¾ï¼Œæé«˜äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚</li>
<li>Lt-Dièƒ½æ”¹å–„æƒé‡ç”ŸæˆèŒƒå¼çš„æ”¶æ•›æ€§èƒ½ï¼Œä¸”ä¸ä¼šå¢åŠ é¢å¤–çš„æ—¶é—´å¼€é”€ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLt-Diåœ¨å¤šç§ä»»åŠ¡ä¸Šå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ã€å¤šåŸŸæ³›åŒ–å’Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e023d3e79787553343dc79a4e922d2af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a166195fcd2db168a50d21d1d59850b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4c91ab2b9694eb4eb2032630d28a4d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-628969ad983cf3a78ca149979decb7a5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Adapting-Foundation-Models-for-Few-Shot-Medical-Image-Segmentation-Actively-and-Sequentially"><a href="#Adapting-Foundation-Models-for-Few-Shot-Medical-Image-Segmentation-Actively-and-Sequentially" class="headerlink" title="Adapting Foundation Models for Few-Shot Medical Image Segmentation:   Actively and Sequentially"></a>Adapting Foundation Models for Few-Shot Medical Image Segmentation:   Actively and Sequentially</h2><p><strong>Authors:Jingyun Yang, Guoqing Zhang, Jingge Wang, Yang Li</strong></p>
<p>Recent advances in foundation models have brought promising results in computer vision, including medical image segmentation. Fine-tuning foundation models on specific low-resource medical tasks has become a standard practice. However, ensuring reliable and robust model adaptation when the target task has a large domain gap and few annotated samples remains a challenge. Previous few-shot domain adaptation (FSDA) methods seek to bridge the distribution gap between source and target domains by utilizing auxiliary data. The selection and scheduling of auxiliaries are often based on heuristics, which can easily cause negative transfer. In this work, we propose an Active and Sequential domain AdaPtation (ASAP) framework for dynamic auxiliary dataset selection in FSDA. We formulate FSDA as a multi-armed bandit problem and derive an efficient reward function to prioritize training on auxiliary datasets that align closely with the target task, through a single-round fine-tuning. Empirical validation on diverse medical segmentation datasets demonstrates that our method achieves favorable segmentation performance, significantly outperforming the state-of-the-art FSDA methods, achieving an average gain of 27.75% on MRI and 7.52% on CT datasets in Dice score. Code is available at the git repository: <a target="_blank" rel="noopener" href="https://github.com/techicoco/ASAP">https://github.com/techicoco/ASAP</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„è¿›å±•åœ¨åŸºç¡€æ¨¡å‹æ–¹é¢å·²ç»åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆåŒ…æ‹¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼‰ä¸­å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚å¯¹ç‰¹å®šä½èµ„æºåŒ»å­¦ä»»åŠ¡è¿›è¡Œå¾®è°ƒåŸºç¡€æ¨¡å‹å·²ç»æˆä¸ºä¸€ç§æ ‡å‡†åšæ³•ã€‚ç„¶è€Œï¼Œå½“ç›®æ ‡ä»»åŠ¡å…·æœ‰è¾ƒå¤§çš„é¢†åŸŸå·®è·å’Œå°‘é‡çš„æ ‡æ³¨æ ·æœ¬æ—¶ï¼Œç¡®ä¿å¯é å’Œç¨³å¥çš„æ¨¡å‹é€‚åº”ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¹‹å‰çš„å°‘æ ·æœ¬åŸŸé€‚åº”ï¼ˆFSDAï¼‰æ–¹æ³•è¯•å›¾é€šè¿‡åˆ©ç”¨è¾…åŠ©æ•°æ®æ¥ç¼©å°æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„åˆ†å¸ƒå·®è·ã€‚è¾…åŠ©æ•°æ®çš„é€‰æ‹©å’Œæ—¶é—´å®‰æ’é€šå¸¸åŸºäºå¯å‘å¼æ–¹æ³•ï¼Œè¿™å¾ˆå®¹æ˜“å¯¼è‡´è´Ÿè¿ç§»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘å°‘æ ·æœ¬åŸŸé€‚åº”ï¼ˆFSDAï¼‰çš„åŠ¨æ€è¾…åŠ©æ•°æ®é›†é€‰æ‹©çš„ä¸»åŠ¨åºè´¯åŸŸé€‚åº”ï¼ˆASAPï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬å°†FSDAåˆ¶å®šä¸ºå¤šè·¯èµŒåšé—®é¢˜ï¼Œå¹¶æ¨å¯¼å‡ºä¸€ä¸ªæœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°ï¼Œä»¥ä¼˜å…ˆè®­ç»ƒä¸ç›®æ ‡ä»»åŠ¡ç´§å¯†å¯¹é½çš„è¾…åŠ©æ•°æ®é›†ï¼Œé€šè¿‡ä¸€è½®å¾®è°ƒæ¥å®ç°ã€‚åœ¨å¤šæ ·åŒ–çš„åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„ç»éªŒéªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ‰åˆ©çš„åˆ†å‰²æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°çš„FSDAæ–¹æ³•ï¼Œåœ¨MRIå’ŒCTæ•°æ®é›†ä¸Šçš„Diceå¾—åˆ†å¹³å‡æé«˜äº†27.75%å’Œ7.52%ã€‚ä»£ç å¯åœ¨gitä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/techicoco/ASAP%E3%80%82">https://github.com/techicoco/ASAPã€‚</a></p>
</blockquote>
<p><strong>ç®€åŒ–</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01000v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°è¿›å±•çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚é’ˆå¯¹ç›®æ ‡ä»»åŠ¡å…·æœ‰è¾ƒå¤§é¢†åŸŸå·®è·å’Œå°‘é‡æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹çš„æ¨¡å‹é€‚åº”æ€§é—®é¢˜ï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºåŠ¨æ€è¾…åŠ©æ•°æ®é›†é€‰æ‹©çš„ä¸»åŠ¨åºè´¯åŸŸé€‚åº”ï¼ˆASAPï¼‰æ¡†æ¶ã€‚æœ¬æ–‡å°†few-shoté¢†åŸŸé€‚åº”é—®é¢˜å»ºæ¨¡ä¸ºmulti-armed bandité—®é¢˜ï¼Œå¹¶è®¾è®¡äº†é«˜æ•ˆçš„å¥–åŠ±å‡½æ•°ä»¥ä¼˜å…ˆé€‰æ‹©ä¸ç›®æ ‡ä»»åŠ¡å¯†åˆ‡ç›¸å…³çš„è¾…åŠ©æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚åœ¨å¤šæ ·åŒ–çš„åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®è¯éªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†ä¼˜è¶Šçš„åˆ†å‰²æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°çš„few-shoté¢†åŸŸé€‚åº”æ–¹æ³•ï¼Œåœ¨MRIå’ŒCTæ•°æ®é›†ä¸Šçš„Diceå¾—åˆ†å¹³å‡æé«˜äº†27.75%å’Œ7.52%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨fine-tuningç‰¹å®šä½èµ„æºåŒ»ç–—ä»»åŠ¡æ–¹é¢ã€‚</li>
<li>åœ¨ç›®æ ‡ä»»åŠ¡ä¸æºä»»åŠ¡é¢†åŸŸå·®è·å¤§ã€æ ‡æ³¨æ ·æœ¬å°‘çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹é€‚åº”ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰few-shoté¢†åŸŸé€‚åº”æ–¹æ³•ä¸»è¦åˆ©ç”¨è¾…åŠ©æ•°æ®ç¼©å°æºåŸŸä¸ç›®æ ‡åŸŸä¹‹é—´çš„å·®è·ï¼Œä½†è¾…åŠ©æ•°æ®çš„é€‰æ‹©å’Œè°ƒåº¦é€šå¸¸åŸºäºå¯å‘å¼è§„åˆ™ï¼Œå®¹æ˜“é€ æˆè´Ÿè¿ç§»ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„Active and Sequential domain AdaPtation (ASAP)æ¡†æ¶ï¼Œç”¨äºåŠ¨æ€é€‰æ‹©è¾…åŠ©æ•°æ®é›†ã€‚</li>
<li>å°†few-shoté¢†åŸŸé€‚åº”é—®é¢˜å»ºæ¨¡ä¸ºmulti-armed bandité—®é¢˜ï¼Œå¹¶è®¾è®¡é«˜æ•ˆçš„å¥–åŠ±å‡½æ•°ä»¥é€‰æ‹©ä¸ç›®æ ‡ä»»åŠ¡ç›¸å…³çš„è¾…åŠ©æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨å¤šæ ·åŒ–åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®è¯éªŒè¯æ˜¾ç¤ºï¼ŒASAPæ¡†æ¶åœ¨åˆ†å‰²æ€§èƒ½ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77c603db060814eaf9ad257b20c2c345.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89b0b6b74805eb25ac43ab1065640119.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53e99a612b943d39cdf14c613335ec94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90a3a21de93eebcf344c8b734f827b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12e021bed4a914256af45bb32af29753.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Predicting-potentially-unfair-clauses-in-Chilean-terms-of-services-with-natural-language-processing"><a href="#Predicting-potentially-unfair-clauses-in-Chilean-terms-of-services-with-natural-language-processing" class="headerlink" title="Predicting potentially unfair clauses in Chilean terms of services with   natural language processing"></a>Predicting potentially unfair clauses in Chilean terms of services with   natural language processing</h2><p><strong>Authors:Christoffer Loeffler, Andrea MartÃ­nez Freile, TomÃ¡s Rey Pizarro</strong></p>
<p>This study addresses the growing concern of information asymmetry in consumer contracts, exacerbated by the proliferation of online services with complex Terms of Service that are rarely even read. Even though research on automatic analysis methods is conducted, the problem is aggravated by the general focus on English-language Machine Learning approaches and on major jurisdictions, such as the European Union. We introduce a new methodology and a substantial dataset addressing this gap. We propose a novel annotation scheme with four categories and a total of 20 classes, and apply it on 50 online Terms of Service used in Chile. Our evaluation of transformer-based models highlights how factors like language- and&#x2F;or domain-specific pre-training, few-shot sample size, and model architecture affect the detection and classification of potentially abusive clauses. Results show a large variability in performance for the different tasks and models, with the highest macro-F1 scores for the detection task ranging from 79% to 89% and micro-F1 scores up to 96%, while macro-F1 scores for the classification task range from 60% to 70% and micro-F1 scores from 64% to 80%. Notably, this is the first Spanish-language multi-label classification dataset for legal clauses, applying Chilean law and offering a comprehensive evaluation of Spanish-language models in the legal domain. Our work lays the ground for future research in method development for rarely considered legal analysis and potentially leads to practical applications to support consumers in Chile and Latin America as a whole. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å…³æ³¨æ¶ˆè´¹è€…åˆåŒä¸­ä¿¡æ¯ä¸å¯¹ç§°é—®é¢˜çš„æ—¥ç›Šä¸¥é‡ï¼Œè¿™ä¸€é—®é¢˜å› åœ¨çº¿æœåŠ¡çš„æ™®åŠè€ŒåŠ å‰§ï¼Œè¿™äº›æœåŠ¡å…·æœ‰å¤æ‚çš„ã€ŠæœåŠ¡æ¡æ¬¾ã€‹ï¼Œè¿™äº›æ¡æ¬¾å¾ˆå°‘è¢«é˜…è¯»ã€‚å°½ç®¡å·²ç»å¼€å±•äº†å…³äºè‡ªåŠ¨åˆ†ææ–¹æ³•çš„ç ”ç©¶ï¼Œä½†é—®é¢˜æ˜¯ç”±äºæ™®éå…³æ³¨è‹±è¯­æœºå™¨å­¦ä¹ æ–¹æ³•å’Œä¸»è¦å¸æ³•ç®¡è¾–åŒºï¼ˆå¦‚æ¬§æ´²è”ç›Ÿï¼‰ï¼Œè¿™ä¸€é—®é¢˜å˜å¾—æ›´åŠ ä¸¥é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•å’Œå¤§é‡çš„æ•°æ®é›†æ¥è§£å†³è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ ‡æ³¨æ–¹æ¡ˆï¼ŒåŒ…å«å››ä¸ªç±»åˆ«å’Œæ€»å…±äºŒåä¸ªç±»åˆ«ï¼Œå¹¶åº”ç”¨äºæ™ºåˆ©ä½¿ç”¨çš„äº”åé¡¹åœ¨çº¿ã€ŠæœåŠ¡æ¡æ¬¾ã€‹ã€‚æˆ‘ä»¬å¯¹åŸºäºå˜å‹å™¨çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œçªå‡ºäº†è¯­è¨€æˆ–é¢†åŸŸç‰¹å®šçš„é¢„è®­ç»ƒã€å°‘é‡æ ·æœ¬å¤§å°å’Œæ¨¡å‹æ¶æ„ç­‰å› ç´ å¦‚ä½•å½±å“æ½œåœ¨æ»¥ç”¨æ¡æ¬¾çš„æ£€æµ‹å’Œåˆ†ç±»ã€‚ç»“æœè¡¨æ˜ï¼Œä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œæ£€æµ‹ä»»åŠ¡çš„æœ€é«˜å®F1åˆ†æ•°åœ¨79%è‡³89%ä¹‹é—´ï¼Œå¾®F1åˆ†æ•°é«˜è¾¾96%ï¼Œè€Œåˆ†ç±»ä»»åŠ¡çš„å®F1åˆ†æ•°åœ¨60%è‡³70%ä¹‹é—´ï¼Œå¾®F1åˆ†æ•°åœ¨64%è‡³80%ä¹‹é—´ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæ³•å¾‹æ¡æ¬¾çš„è¥¿ç­ç‰™è¯­å¤šæ ‡ç­¾åˆ†ç±»æ•°æ®é›†ï¼Œé€‚ç”¨äºæ™ºåˆ©æ³•å¾‹ï¼Œå¹¶å¯¹è¥¿ç­ç‰™è¯­æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæœªæ¥åœ¨æ³•å¾‹åˆ†ææ–¹é¢çš„æ–¹æ³•å¼€å‘ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œå¹¶æœ‰æœ›ä¸ºæ™ºåˆ©å’Œæ•´ä¸ªæ‹‰ä¸ç¾æ´²çš„æ¶ˆè´¹è€…æä¾›å®é™…æ”¯æŒåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00865v1">PDF</a> 37 pages, 2 figures, under review</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶å…³æ³¨æ¶ˆè´¹è€…åˆåŒä¸­ä¿¡æ¯ä¸å¯¹ç­‰é—®é¢˜çš„æ—¥ç›Šä¸¥é‡ï¼Œç‰¹åˆ«æ˜¯ç”±äºåœ¨çº¿æœåŠ¡æ™®åŠå’Œå…¶å¤æ‚çš„æœåŠ¡æ¡æ¬¾å¾ˆå°‘è¢«é˜…è¯»è€ŒåŠ å‰§çš„é—®é¢˜ã€‚å°½ç®¡å·²æœ‰å…³äºè‡ªåŠ¨åˆ†ææ–¹æ³•çš„ç ”ç©¶ï¼Œä½†é—®é¢˜ä¾ç„¶ä¸¥å³»ï¼Œå› ä¸ºç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­æœºå™¨å­¦ä¹ æ–¹æ³•ä»¥åŠæ¬§æ´²è”ç›Ÿç­‰ä¸»è¦å¸æ³•ç®¡è¾–åŒºã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•å’Œå¤§è§„æ¨¡æ•°æ®é›†æ¥è§£å†³è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ³¨é‡Šæ–¹æ¡ˆï¼ŒåŒ…æ‹¬å››ä¸ªç±»åˆ«å’Œæ€»å…±20ä¸ªå­ç±»åˆ«ï¼Œå¹¶åº”ç”¨äºæ™ºåˆ©ä½¿ç”¨çš„50é¡¹åœ¨çº¿æœåŠ¡æ¡æ¬¾ã€‚æˆ‘ä»¬è¯„ä¼°äº†åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ï¼Œæ¢è®¨äº†è¯­è¨€ã€é¢†åŸŸç‰¹å®šçš„é¢„è®­ç»ƒã€å°‘æ•°æ ·æœ¬å¤§å°å’Œæ¨¡å‹æ¶æ„ç­‰å› ç´ å¯¹æ½œåœ¨æ»¥ç”¨æ¡æ¬¾çš„æ£€æµ‹å’Œåˆ†ç±»çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºå„é¡¹ä»»åŠ¡å’Œæ¨¡å‹çš„è¡¨ç°å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œæ£€æµ‹ä»»åŠ¡çš„å®è§‚F1åˆ†æ•°æœ€é«˜è¾¾åˆ°89%ï¼Œå¾®è§‚F1åˆ†æ•°æœ€é«˜è¾¾åˆ°96%ï¼Œè€Œåˆ†ç±»ä»»åŠ¡çš„å®è§‚å’Œå¾®è§‚F1åˆ†æ•°åˆ†åˆ«åœ¨60%è‡³70%å’Œ64%è‡³80%ä¹‹é—´ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæ³•å¾‹æ¡æ¬¾çš„è¥¿ç­ç‰™è¯­å¤šæ ‡ç­¾åˆ†ç±»æ•°æ®é›†ï¼Œä»¥æ™ºåˆ©æ³•å¾‹ä¸ºä¾æ®ï¼Œå…¨é¢è¯„ä¼°äº†è¥¿ç­ç‰™è¯­æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„è¡¨ç°ã€‚æœ¬ç ”ç©¶ä¸ºæœªæ¥åœ¨è¾ƒå°‘è€ƒè™‘çš„æ³•å¾‹åˆ†æä¸­çš„æ–¹æ³•å¼€å‘å¥ å®šäº†åŸºç¡€ï¼Œå¹¶æœ‰æœ›ä¸ºæ¶ˆè´¹è€…åœ¨æ™ºåˆ©ä¹ƒè‡³æ‹‰ä¸ç¾æ´²æä¾›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æœ¬ç ”ç©¶å…³æ³¨åœ¨çº¿æœåŠ¡åˆåŒä¸­ä¿¡æ¯ä¸å¯¹ç§°çš„é—®é¢˜ã€‚</li>
<li>ç”±äºå¤æ‚çš„æœåŠ¡æ¡æ¬¾é²œå°‘è¢«é˜…è¯»ï¼Œè¯¥é—®é¢˜è¿›ä¸€æ­¥åŠ å‰§ã€‚</li>
<li>å°½ç®¡æœ‰è‡ªåŠ¨åˆ†ææ–¹æ³•çš„ç ”ç©¶ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­å’Œæ¬§æ´²å¸æ³•ç®¡è¾–åŒºã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ³¨é‡Šæ–¹æ¡ˆå’Œæ•°æ®é›†æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œç‰¹åˆ«å…³æ³¨æ™ºåˆ©ä½¿ç”¨çš„åœ¨çº¿æœåŠ¡æ¡æ¬¾ã€‚</li>
<li>åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹è¢«è¯„ä¼°ï¼Œæ¢è®¨ä¸åŒå› ç´ å¯¹æ¡æ¬¾æ£€æµ‹å’Œåˆ†ç±»çš„å½±å“ã€‚</li>
<li>æ£€æµ‹å’Œåˆ†ç±»ä»»åŠ¡çš„è¡¨ç°å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œå…¶ä¸­æ£€æµ‹ä»»åŠ¡è¡¨ç°è¾ƒå¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f4ea1bcaafb5b0b65578d29ab0d2b3c8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Vision-and-Language-Reference-Prompt-into-SAM-for-Few-shot-Segmentation"><a href="#Vision-and-Language-Reference-Prompt-into-SAM-for-Few-shot-Segmentation" class="headerlink" title="Vision and Language Reference Prompt into SAM for Few-shot Segmentation"></a>Vision and Language Reference Prompt into SAM for Few-shot Segmentation</h2><p><strong>Authors:Kosuke Sakurai, Ryotaro Shimizu, Masayuki Goto</strong></p>
<p>Segment Anything Model (SAM) represents a large-scale segmentation model that enables powerful zero-shot capabilities with flexible prompts. While SAM can segment any object in zero-shot, it requires user-provided prompts for each target image and does not attach any label information to masks. Few-shot segmentation models addressed these issues by inputting annotated reference images as prompts to SAM and can segment specific objects in target images without user-provided prompts. Previous SAM-based few-shot segmentation models only use annotated reference images as prompts, resulting in limited accuracy due to a lack of reference information. In this paper, we propose a novel few-shot segmentation model, Vision and Language reference Prompt into SAM (VLP-SAM), that utilizes the visual information of the reference images and the semantic information of the text labels by inputting not only images but also language as reference information. In particular, VLP-SAM is a simple and scalable structure with minimal learnable parameters, which inputs prompt embeddings with vision-language information into SAM using a multimodal vision-language model. To demonstrate the effectiveness of VLP-SAM, we conducted experiments on the PASCAL-5i and COCO-20i datasets, and achieved high performance in the few-shot segmentation task, outperforming the previous state-of-the-art model by a large margin (6.3% and 9.5% in mIoU, respectively). Furthermore, VLP-SAM demonstrates its generality in unseen objects that are not included in the training data. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/kosukesakurai1/VLP-SAM">https://github.com/kosukesakurai1/VLP-SAM</a>. </p>
<blockquote>
<p>Segment Anything Modelï¼ˆSAMï¼‰ä»£è¡¨äº†ä¸€ç§å¤§è§„æ¨¡åˆ†å‰²æ¨¡å‹ï¼Œå®ƒé€šè¿‡çµæ´»çš„æç¤ºå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚è™½ç„¶SAMå¯ä»¥åœ¨é›¶æ ·æœ¬ä¸­åˆ†å‰²ä»»ä½•å¯¹è±¡ï¼Œä½†å®ƒéœ€è¦é’ˆå¯¹æ¯ä¸ªç›®æ ‡å›¾åƒæä¾›ç”¨æˆ·æç¤ºï¼Œå¹¶ä¸”ä¸ä¼šåœ¨è’™ç‰ˆä¸Šé™„åŠ ä»»ä½•æ ‡ç­¾ä¿¡æ¯ã€‚å°‘æ ·æœ¬åˆ†å‰²æ¨¡å‹é€šè¿‡è¾“å…¥å¸¦æœ‰æ ‡æ³¨çš„å‚è€ƒå›¾åƒä½œä¸ºæç¤ºæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œèƒ½å¤Ÿåœ¨ç›®æ ‡å›¾åƒä¸­åˆ†å‰²ç‰¹å®šå¯¹è±¡ï¼Œè€Œæ— éœ€ç”¨æˆ·æä¾›æç¤ºã€‚ä¹‹å‰çš„åŸºäºSAMçš„å°‘æ ·æœ¬åˆ†å‰²æ¨¡å‹ä»…ä½¿ç”¨å¸¦æœ‰æ ‡æ³¨çš„å‚è€ƒå›¾åƒä½œä¸ºæç¤ºï¼Œç”±äºç¼ºå°‘å‚è€ƒä¿¡æ¯ï¼Œå¯¼è‡´å‡†ç¡®æ€§å—é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å°‘æ ·æœ¬åˆ†å‰²æ¨¡å‹â€”â€”Vision and Language reference Prompt into SAMï¼ˆVLP-SAMï¼‰ï¼Œå®ƒåˆ©ç”¨å‚è€ƒå›¾åƒä¸­çš„è§†è§‰ä¿¡æ¯å’Œæ–‡æœ¬æ ‡ç­¾çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä¸ä»…è¾“å…¥å›¾åƒï¼Œè¿˜è¾“å…¥è¯­è¨€ä½œä¸ºå‚è€ƒä¿¡æ¯ã€‚ç‰¹åˆ«æ˜¯ï¼ŒVLP-SAMå…·æœ‰ç®€å•å¯æ‰©å±•çš„ç»“æ„å’Œæœ€å°çš„å¯å­¦ä¹ å‚æ•°ï¼Œä½¿ç”¨å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹å°†å¸¦æœ‰è§†è§‰è¯­è¨€ä¿¡æ¯çš„æç¤ºåµŒå…¥SAMä¸­ã€‚ä¸ºäº†è¯æ˜VLP-SAMçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨PASCAL-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œåœ¨å°‘æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†é«˜æ€§èƒ½ï¼Œå¤§å¹…è¶…è¶Šäº†ä¹‹å‰çš„æœ€æ–°æ¨¡å‹ï¼ˆåˆ†åˆ«æé«˜äº†6.3%å’Œ9.5%çš„mIoUï¼‰ã€‚æ­¤å¤–ï¼ŒVLP-SAMåœ¨æœªè§è¿‡çš„å¯¹è±¡ä¸Šè¡¨ç°å‡ºäº†å…¶æ™®éæ€§ï¼Œè¿™äº›å¯¹è±¡ä¸åŒ…æ‹¬åœ¨è®­ç»ƒæ•°æ®ä¸­ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kosukesakurai1/VLP-SAM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kosukesakurai1/VLP-SAMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00719v1">PDF</a> 8 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å°‘æ ·æœ¬åˆ†å‰²æ¨¡å‹VLP-SAMï¼Œå®ƒå°†è§†è§‰ä¿¡æ¯ä¸æ–‡æœ¬æ ‡ç­¾çš„è¯­ä¹‰ä¿¡æ¯ç»“åˆï¼Œé€šè¿‡è¾“å…¥å›¾åƒå’Œæ–‡æœ¬ä½œä¸ºå‚è€ƒä¿¡æ¯æ¥ä¼˜åŒ–SAMæ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ç®€å•ã€å¯æ‰©å±•ï¼Œå‚æ•°å¯å­¦ä¹ æ€§ä½ã€‚åœ¨PASCAL-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨å°‘æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†é«˜æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—ä¼˜äºå…ˆå‰çš„ä¸»æµæ¨¡å‹ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹å¯¹äºæœªè§å¯¹è±¡ä¹Ÿè¡¨ç°å‡ºå…¶æ™®éæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLP-SAMæ¨¡å‹ç»“åˆäº†è§†è§‰ä¿¡æ¯å’Œæ–‡æœ¬æ ‡ç­¾çš„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡è¾“å…¥å›¾åƒå’Œæ–‡æœ¬ä½œä¸ºå‚è€ƒä¿¡æ¯ï¼Œä¼˜åŒ–äº†SAMæ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</li>
<li>VLP-SAMæ¨¡å‹å…·æœ‰ç®€å•ã€å¯æ‰©å±•çš„ç‰¹æ€§ï¼Œä¸”å‚æ•°å¯å­¦ä¹ æ€§è¾ƒä½ã€‚</li>
<li>åœ¨PASCAL-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†VLP-SAMæ¨¡å‹åœ¨å°‘æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸Šçš„é«˜æ€§èƒ½ã€‚</li>
<li>VLP-SAMæ¨¡å‹æ˜¾è‘—ä¼˜äºå…ˆå‰çš„ä¸»æµæ¨¡å‹ï¼Œåœ¨mIoUæŒ‡æ ‡ä¸Šåˆ†åˆ«æå‡äº†6.3%å’Œ9.5%ã€‚</li>
<li>VLP-SAMæ¨¡å‹å¯¹äºæœªè§å¯¹è±¡ä¹Ÿè¡¨ç°å‡ºå…¶æ™®éæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00719">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-744e826eba0b92f9dd0b11bc5af31209.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21173187b7284b7e753bfcb120ccefcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbbe7fe65e9d1bfff876bb8e37679304.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6870058217bd10845a497c61263eac9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Mitigating-the-Modality-Gap-Few-Shot-Out-of-Distribution-Detection-with-Multi-modal-Prototypes-and-Image-Bias-Estimation"><a href="#Mitigating-the-Modality-Gap-Few-Shot-Out-of-Distribution-Detection-with-Multi-modal-Prototypes-and-Image-Bias-Estimation" class="headerlink" title="Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with   Multi-modal Prototypes and Image Bias Estimation"></a>Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with   Multi-modal Prototypes and Image Bias Estimation</h2><p><strong>Authors:Yimu Wang, Evelien Riddell, Adrian Chow, Sean Sedwards, Krzysztof Czarnecki</strong></p>
<p>Existing vision-language model (VLM)-based methods for out-of-distribution (OOD) detection typically rely on similarity scores between input images and in-distribution (ID) text prototypes. However, the modality gap between image and text often results in high false positive rates, as OOD samples can exhibit high similarity to ID text prototypes. To mitigate the impact of this modality gap, we propose incorporating ID image prototypes along with ID text prototypes. We present theoretical analysis and empirical evidence indicating that this approach enhances VLM-based OOD detection performance without any additional training. To further reduce the gap between image and text, we introduce a novel few-shot tuning framework, SUPREME, comprising biased prompts generation (BPG) and image-text consistency (ITC) modules. BPG enhances image-text fusion and improves generalization by conditioning ID text prototypes on the Gaussian-based estimated image domain bias; ITC reduces the modality gap by minimizing intra- and inter-modal distances. Moreover, inspired by our theoretical and empirical findings, we introduce a novel OOD score $S_{\textit{GMP}}$, leveraging uni- and cross-modal similarities. Finally, we present extensive experiments to demonstrate that SUPREME consistently outperforms existing VLM-based OOD detection methods. </p>
<blockquote>
<p>ç°æœ‰çš„åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç¦»ç¾¤å€¼ï¼ˆOODï¼‰æ£€æµ‹é€šå¸¸ä¾èµ–äºè¾“å…¥å›¾åƒä¸åˆ†å¸ƒå†…ï¼ˆIDï¼‰æ–‡æœ¬åŸå‹ä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚ç„¶è€Œï¼Œå›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€å·®è·å¸¸å¸¸å¯¼è‡´é«˜è¯¯æŠ¥ç‡ï¼Œå› ä¸ºç¦»ç¾¤æ ·æœ¬å¯èƒ½è¡¨ç°å‡ºä¸IDæ–‡æœ¬åŸå‹çš„é«˜åº¦ç›¸ä¼¼æ€§ã€‚ä¸ºäº†å‡è½»è¿™ç§æ¨¡æ€å·®è·çš„å½±å“ï¼Œæˆ‘ä»¬æå‡ºç»“åˆIDå›¾åƒåŸå‹å’ŒIDæ–‡æœ¬åŸå‹ã€‚æˆ‘ä»¬æä¾›ç†è®ºåˆ†æå’Œå®è¯è¯æ®è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¸è¿›è¡Œä»»ä½•é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜äº†åŸºäºVLMçš„OODæ£€æµ‹æ€§èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç¼©å°å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬è°ƒæ•´æ¡†æ¶SUPREMEï¼ŒåŒ…æ‹¬åå‘æç¤ºç”Ÿæˆï¼ˆBPGï¼‰å’Œå›¾åƒæ–‡æœ¬ä¸€è‡´æ€§ï¼ˆITCï¼‰æ¨¡å—ã€‚BPGé€šè¿‡åŸºäºé«˜æ–¯ä¼°è®¡çš„å›¾åƒåŸŸåå·®å¯¹IDæ–‡æœ¬åŸå‹è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œå¢å¼ºå›¾åƒæ–‡æœ¬çš„èåˆå¹¶æé«˜æ³›åŒ–èƒ½åŠ›ï¼›ITCé€šè¿‡æœ€å°åŒ–è·¨æ¨¡æ€è·ç¦»æ¥ç¼©å°æ¨¡æ€å·®è·ã€‚æ­¤å¤–ï¼Œå—åˆ°ç†è®ºå’Œå®è¯ç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„OODå¾—åˆ†å…¬å¼$S_{\text{GMP}}$ï¼Œåˆ©ç”¨å•æ¨¡æ€å’Œè·¨æ¨¡æ€ç›¸ä¼¼æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒè¯æ˜ï¼ŒSUPREMEåœ¨æ€§èƒ½ä¸ŠæŒç»­è¶…è¶Šç°æœ‰åŸºäºVLMçš„OODæ£€æµ‹æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00662v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå›¾åƒå’Œæ–‡æœ¬åŸå‹çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å°‘æ•°æ ·æœ¬å¾®è°ƒæ¡†æ¶SUPREMEï¼ŒåŒ…æ‹¬åå·®æç¤ºç”Ÿæˆï¼ˆBPGï¼‰å’Œå›¾åƒæ–‡æœ¬ä¸€è‡´æ€§ï¼ˆITCï¼‰æ¨¡å—ï¼Œç¼©å°å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€å·®è·ï¼Œæé«˜åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æœªçŸ¥åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹æ€§èƒ½ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æœªçŸ¥åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹æ–¹æ³•é€šå¸¸ä¾èµ–äºè¾“å…¥å›¾åƒå’Œå·²çŸ¥åˆ†å¸ƒï¼ˆIDï¼‰æ–‡æœ¬åŸå‹ä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œä½†å­˜åœ¨æ¨¡æ€å·®è·å¯¼è‡´é«˜è¯¯æŠ¥ç‡ã€‚</li>
<li>æå‡ºç»“åˆIDå›¾åƒåŸå‹å’ŒIDæ–‡æœ¬åŸå‹çš„æ–¹æ³•ï¼Œä»¥ç¼©å°æ¨¡æ€å·®è·å¹¶å¢å¼ºVLMçš„OODæ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥å°‘æ•°æ ·æœ¬å¾®è°ƒæ¡†æ¶SUPREMEï¼ŒåŒ…æ‹¬åå·®æç¤ºç”Ÿæˆï¼ˆBPGï¼‰å’Œå›¾åƒæ–‡æœ¬ä¸€è‡´æ€§ï¼ˆITCï¼‰æ¨¡å—ï¼Œè¿›ä¸€æ­¥æé«˜å›¾åƒå’Œæ–‡æœ¬çš„èåˆå’Œä¸€è‡´æ€§ã€‚</li>
<li>BPGé€šè¿‡åŸºäºé«˜æ–¯ä¼°è®¡çš„å›¾åƒåŸŸåå·®å¯¹IDæ–‡æœ¬åŸå‹è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œå¢å¼ºäº†å›¾åƒæ–‡æœ¬èåˆå¹¶æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ITCé€šè¿‡æœ€å°åŒ–è·¨æ¨¡æ€è·ç¦»æ¥ç¼©å°æ¨¡æ€å·®è·ã€‚</li>
<li>æå‡ºä¸€ç§æ–°çš„OODå¾—åˆ†å…¬å¼$S_{\textit{GMP}}$ï¼Œåˆ©ç”¨å•æ¨¡æ€å’Œè·¨æ¨¡æ€ç›¸ä¼¼æ€§è¿›è¡Œæ›´å‡†ç¡®çš„OODæ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-61ac9c2d536dd9a4687ccbb8130ed121.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3268114fb57933e596c6f64007de163a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TeST-V-TEst-time-Support-set-Tuning-for-Zero-shot-Video-Classification"><a href="#TeST-V-TEst-time-Support-set-Tuning-for-Zero-shot-Video-Classification" class="headerlink" title="TeST-V: TEst-time Support-set Tuning for Zero-shot Video Classification"></a>TeST-V: TEst-time Support-set Tuning for Zero-shot Video Classification</h2><p><strong>Authors:Rui Yan, Jin Wang, Hongyu Qu, Xiaoyu Du, Dong Zhang, Jinhui Tang, Tieniu Tan</strong></p>
<p>Recently, adapting Vision Language Models (VLMs) to zero-shot visual classification by tuning class embedding with a few prompts (Test-time Prompt Tuning, TPT) or replacing class names with generated visual samples (support-set) has shown promising results. However, TPT cannot avoid the semantic gap between modalities while the support-set cannot be tuned. To this end, we draw on each otherâ€™s strengths and propose a novel framework namely TEst-time Support-set Tuning for zero-shot Video Classification (TEST-V). It first dilates the support-set with multiple prompts (Multi-prompting Support-set Dilation, MSD) and then erodes the support-set via learnable weights to mine key cues dynamically (Temporal-aware Support-set Erosion, TSE). Specifically, i) MSD expands the support samples for each class based on multiple prompts enquired from LLMs to enrich the diversity of the support-set. ii) TSE tunes the support-set with factorized learnable weights according to the temporal prediction consistency in a self-supervised manner to dig pivotal supporting cues for each class. $\textbf{TEST-V}$ achieves state-of-the-art results across four benchmarks and has good interpretability for the support-set dilation and erosion. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé€šè¿‡å¾®è°ƒç±»åˆ«åµŒå…¥ï¼ˆclass embeddingï¼‰ä½¿ç”¨å°‘é‡æç¤ºï¼ˆTest-time Prompt Tuningï¼ŒTPTï¼‰æˆ–å°†ç±»åˆ«åç§°æ›¿æ¢ä¸ºç”Ÿæˆçš„è§†è§‰æ ·æœ¬ï¼ˆæ”¯æŒé›†ï¼‰æ¥é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œé›¶æ ·æœ¬è§†è§‰åˆ†ç±»ï¼Œå·²ç»å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼ŒTPTæ— æ³•é¿å…ä¸åŒæ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿï¼Œè€Œæ”¯æŒé›†æ— æ³•è°ƒæ•´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å–é•¿è¡¥çŸ­ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåä¸ºTEST-Vï¼ˆç”¨äºé›¶æ ·æœ¬è§†é¢‘åˆ†ç±»çš„æµ‹è¯•æ—¶æ”¯æŒé›†è°ƒæ•´ï¼‰ã€‚å®ƒé¦–å…ˆé€šè¿‡å¤šä¸ªæç¤ºï¼ˆMulti-prompting Support-set Dilationï¼ŒMSDï¼‰è†¨èƒ€æ”¯æŒé›†ï¼Œç„¶åé€šè¿‡å¯å­¦ä¹ æƒé‡å¯¹æ”¯æŒé›†è¿›è¡Œä¾µèš€ï¼Œä»¥åŠ¨æ€æŒ–æ˜å…³é”®çº¿ç´¢ï¼ˆæ—¶åºæ„ŸçŸ¥æ”¯æŒé›†ä¾µèš€ï¼ŒTSEï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œiï¼‰MSDæ ¹æ®ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è·å¾—çš„å¤šä¸ªæç¤ºæ‰©å±•æ¯ä¸ªç±»åˆ«çš„æ”¯æŒæ ·æœ¬ï¼Œä»¥ä¸°å¯Œæ”¯æŒé›†çš„å¤šæ ·æ€§ã€‚iiï¼‰TSEä½¿ç”¨å¯å­¦ä¹ æƒé‡è‡ªé€‚åº”åœ°è°ƒæ•´æ”¯æŒé›†ï¼Œä»¥æ—¶åºé¢„æµ‹ä¸€è‡´æ€§ä¸ºå‡†åˆ™æŒ–æ˜æ¯ä¸ªç±»åˆ«çš„å…³é”®æ”¯æŒçº¿ç´¢ã€‚TEST-Våœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°æˆæœï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„è§£é‡Šæ€§æ¥æ”¯æŒé›†è†¨èƒ€å’Œä¾µèš€çš„è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00426v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶TEST-Vï¼Œç”¨äºé›¶æ ·æœ¬è§†é¢‘åˆ†ç±»ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æµ‹è¯•æ—¶æç¤ºè°ƒæ•´ï¼ˆTPTï¼‰å’Œæ”¯æŒé›†çš„ä¼˜åŠ¿ï¼Œé€šè¿‡å¤šæç¤ºæ”¯æŒé›†è†¨èƒ€ï¼ˆMSDï¼‰å’Œæ—¶é—´æ„ŸçŸ¥æ”¯æŒé›†ä¾µèš€ï¼ˆTSEï¼‰æ¥ä¼˜åŒ–æ”¯æŒé›†ã€‚MSDé€šè¿‡ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è·å–å¤šä¸ªæç¤ºæ¥ä¸°å¯Œæ”¯æŒé›†çš„å¤šæ ·æ€§ï¼Œè€ŒTSEåˆ™é€šè¿‡å¯å­¦ä¹ çš„æƒé‡æ¥æŒ–æ˜å…³é”®çº¿ç´¢ã€‚TEST-Våœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ”¯æŒé›†è†¨èƒ€å’Œä¾µèš€å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TEST-Vç»“åˆäº†æµ‹è¯•æ—¶æç¤ºè°ƒæ•´ï¼ˆTPTï¼‰å’Œæ”¯æŒé›†çš„ä¼˜åŠ¿ï¼Œç”¨äºé›¶æ ·æœ¬è§†é¢‘åˆ†ç±»ã€‚</li>
<li>å¤šæç¤ºæ”¯æŒé›†è†¨èƒ€ï¼ˆMSDï¼‰é€šè¿‡ä»å¤§å‹è¯­è¨€æ¨¡å‹è·å–å¤šä¸ªæç¤ºæ¥ä¸°å¯Œæ”¯æŒé›†çš„å¤šæ ·æ€§ã€‚</li>
<li>æ—¶é—´æ„ŸçŸ¥æ”¯æŒé›†ä¾µèš€ï¼ˆTSEï¼‰é€šè¿‡å¯å­¦ä¹ çš„æƒé‡æŒ–æ˜å…³é”®çº¿ç´¢ï¼Œæ ¹æ®æ—¶é—´é¢„æµ‹ä¸€è‡´æ€§è¿›è¡Œè‡ªæˆ‘ç›‘ç£ã€‚</li>
<li>TEST-Vé€šè¿‡ç»“åˆMSDå’ŒTSEï¼Œå®ç°äº†åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>TEST-Vå…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼Œæ”¯æŒé›†è†¨èƒ€å’Œä¾µèš€çš„è¿‡ç¨‹å¯ä»¥æ¸…æ™°åœ°ç†è§£å’Œåˆ†æã€‚</li>
<li>TEST-Væ¡†æ¶æé«˜äº†é›¶æ ·æœ¬è§†é¢‘åˆ†ç±»çš„å‡†ç¡®æ€§å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-291cd3d8ead003446d06c092d72b094e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e91e44c43f59f731078740f24091dcab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fea1db53a95ab42e86ba663622590c94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26a0c22e540a8a5ddb5844f9c8d56c80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4167a8343bac6a77d1912b8ed8caabcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5fddd1a83aaa7811d1dbbaf240214cc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="From-Few-to-Many-Self-Improving-Many-Shot-Reasoners-Through-Iterative-Optimization-and-Generation"><a href="#From-Few-to-Many-Self-Improving-Many-Shot-Reasoners-Through-Iterative-Optimization-and-Generation" class="headerlink" title="From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative   Optimization and Generation"></a>From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative   Optimization and Generation</h2><p><strong>Authors:Xingchen Wan, Han Zhou, Ruoxi Sun, Hootan Nakhost, Ke Jiang, Sercan Ã–. ArÄ±k</strong></p>
<p>Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However, despite its promise, it is unclear what aspects dominate the benefits and whether simply scaling to more examples is the most effective way of improving many-shot ICL. In this work, we first provide an analysis of the factors driving many-shot ICL, and we find that 1) many-shot performance can still be attributed to often a few disproportionately influential examples and 2) identifying such influential examples (â€œoptimizeâ€) and using them as demonstrations to regenerate new examples (â€œgenerateâ€) can lead to further improvements. Inspired by the findings, we propose BRIDGE, an algorithm that alternates between the optimize step with Bayesian optimization to discover the influential sets of examples and the generate step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of different sizes, we show that BRIDGE to significant improvements across a diverse set of tasks, including symbolic reasoning, numerical reasoning, and code generation. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œé•¿è¯­å¢ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å‚¬ç”Ÿäº†å¤šç¤ºä¾‹ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„èŒƒå¼å…´èµ·ã€‚åœ¨è¯¥èŒƒå¼ä¸­ï¼Œè§‚å¯Ÿåˆ°åœ¨è¯­å¢ƒä¸­æ‰©å¤§è¿œè¶…ä¼ ç»Ÿå°æ ·æœ¬è®¾ç½®çš„è®¸å¤šç¤ºä¾‹ï¼Œå¯ä»¥å¸¦æ¥æ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå°½ç®¡å…·æœ‰æ½œåŠ›ï¼Œä½†å°šä¸æ¸…æ¥šå“ªäº›æ–¹é¢ä¸»å¯¼äº†è¿™äº›å¥½å¤„ï¼Œä»¥åŠå•çº¯æ‰©å¤§ç¤ºä¾‹è§„æ¨¡æ˜¯å¦æ˜¯æ”¹è¿›å¤šç¤ºä¾‹ICLçš„æœ€æœ‰æ•ˆæ–¹å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†æäº†æ¨åŠ¨å¤šç¤ºä¾‹ICLå‘å±•çš„å› ç´ ï¼Œå¹¶å‘ç°ï¼š1ï¼‰å¤šç¤ºä¾‹æ€§èƒ½ä»ç„¶å¯ä»¥å½’åŠŸäºå°‘æ•°ä¸æˆæ¯”ä¾‹åœ°å…·æœ‰å½±å“åŠ›çš„ç¤ºä¾‹ï¼›2ï¼‰è¯†åˆ«è¿™äº›æœ‰å½±å“åŠ›çš„ç¤ºä¾‹ï¼ˆâ€œä¼˜åŒ–â€ï¼‰ï¼Œå¹¶æŠŠå®ƒä»¬ä½œä¸ºç¤ºèŒƒæ¥ç”Ÿæˆæ–°ç¤ºä¾‹ï¼ˆâ€œç”Ÿæˆâ€ï¼‰ï¼Œå¯ä»¥å¯¼è‡´è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚å—è¿™äº›å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†BRIDGEç®—æ³•ï¼Œè¯¥ç®—æ³•äº¤æ›¿è¿›è¡Œä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æ¥å‘ç°å…·æœ‰å½±å“åŠ›çš„ç¤ºä¾‹é›†ï¼ˆâ€œä¼˜åŒ–â€ï¼‰å’Œé‡ç”¨æ­¤é›†æ¥è‡ªåŠ¨æ‰©å±•ç¤ºä¾‹çš„æ¨ç†è·¯å¾„åˆ°å¤šç¤ºä¾‹èŒƒå›´ï¼ˆâ€œç”Ÿæˆâ€ï¼‰ã€‚æˆ‘ä»¬åœ¨Geminiã€Claudeå’ŒMistralçš„ä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå±•ç¤ºäº†BRIDGEç®—æ³•åœ¨åŒ…æ‹¬ç¬¦å·æ¨ç†ã€æ•°å€¼æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å¤šæ ·åŒ–ä»»åŠ¡ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00330v1">PDF</a> Expanded version of the ICLR 2025 paper</p>
<p><strong>Summary</strong><br>     è¿‘æœŸé•¿æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†å¤šç¤ºä¾‹ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰èŒƒå¼çš„å…´èµ·ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å¸¸è§„å°‘ç¤ºä¾‹è®¾ç½®çš„åŸºç¡€ä¸Šæ‰©å±•æ›´å¤šç¤ºä¾‹ä¸Šä¸‹æ–‡çš„è§„æ¨¡å¯ä»¥å¸¦æ¥æ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šå“ªäº›å› ç´ ä¸»å¯¼äº†è¿™ç§ä¼˜åŠ¿ï¼Œä»¥åŠå•çº¯æ‰©å¤§ç¤ºä¾‹è§„æ¨¡æ˜¯å¦æ˜¯æœ€æœ‰æ•ˆçš„æ”¹è¿›æ–¹å¼ã€‚æœ¬æ–‡é¦–å…ˆåˆ†æäº†æ¨åŠ¨å¤šç¤ºä¾‹ä¸Šä¸‹æ–‡å­¦ä¹ å‘å±•çš„å› ç´ ï¼Œå‘ç°ï¼šä¸€æ˜¯å¤šç¤ºä¾‹æ€§èƒ½çš„æå‡ä»æºäºå°‘æ•°æå…·å½±å“åŠ›çš„ç¤ºä¾‹ï¼›äºŒæ˜¯é€šè¿‡è¯†åˆ«è¿™äº›æœ‰å½±å“åŠ›çš„ç¤ºä¾‹å¹¶è¿›è¡Œä¼˜åŒ–ï¼Œå†åˆ©ç”¨å®ƒä»¬ç”Ÿæˆæ–°çš„ç¤ºä¾‹ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›æ€§èƒ½ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæœ¬æ–‡æå‡ºäº†åä¸ºBRIDGEçš„ç®—æ³•ï¼Œé€šè¿‡è´å¶æ–¯ä¼˜åŒ–æ¥å‘ç°å¹¶ä¼˜åŒ–æœ€å…·å½±å“åŠ›çš„ç¤ºä¾‹é›†ï¼Œå†åˆ©ç”¨è¿™äº›ç¤ºä¾‹è‡ªåŠ¨æ‰©å±•æ¨ç†è·¯å¾„å›åˆ°å¤šç¤ºä¾‹çŠ¶æ€ã€‚åœ¨ä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¯ä»¥é€šè¿‡æ‰©å¤§ä¸Šä¸‹æ–‡ä¸­çš„ç¤ºä¾‹æ•°é‡æ¥è¿›ä¸€æ­¥æå‡ã€‚</li>
<li>æ€§èƒ½çš„æå‡ä¸»è¦æºäºå°‘æ•°æå…·å½±å“åŠ›çš„ç¤ºä¾‹ã€‚</li>
<li>é€šè¿‡è¯†åˆ«å’Œä¼˜åŒ–è¿™äº›æœ‰å½±å“åŠ›çš„ç¤ºä¾‹ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
<li>BRIDGEç®—æ³•ç»“åˆäº†ä¼˜åŒ–å’Œç”Ÿæˆæ­¥éª¤ï¼Œé€šè¿‡è´å¶æ–¯ä¼˜åŒ–å‘ç°æœ€å…·å½±å“åŠ›çš„ç¤ºä¾‹é›†ï¼Œå¹¶ç”¨äºæ‰©å±•æ¨ç†è·¯å¾„ã€‚</li>
<li>åœ¨ä¸åŒè§„æ¨¡çš„è¯­è¨€æ¨¡å‹å’Œå¤šç§ä»»åŠ¡ä¸Šï¼ŒBRIDGEç®—æ³•éƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>é™¤äº†æ€§èƒ½å’Œç®—æ³•å±‚é¢çš„å‘ç°å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼ºè°ƒäº†åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­ç†è§£å’Œè¯†åˆ«å…³é”®ç¤ºä¾‹çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00330">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6877815ff565608eb7ffeec918a5615b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2acd09cfac296252e5c60bbe133d252c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5523c00c8e7dad104ac684876e71233.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-are-Few-shot-Multivariate-Time-Series-Classifiers"><a href="#Large-Language-Models-are-Few-shot-Multivariate-Time-Series-Classifiers" class="headerlink" title="Large Language Models are Few-shot Multivariate Time Series Classifiers"></a>Large Language Models are Few-shot Multivariate Time Series Classifiers</h2><p><strong>Authors:Yakun Chen, Zihao Li, Chao Yang, Xianzhi Wang, Guandong Xu</strong></p>
<p>Large Language Models (LLMs) have been extensively applied in time series analysis. Yet, their utility in the few-shot classification (i.e., a crucial training scenario due to the limited training data available in industrial applications) concerning multivariate time series data remains underexplored. We aim to leverage the extensive pre-trained knowledge in LLMs to overcome the data scarcity problem within multivariate time series. Specifically, we propose LLMFew, an LLM-enhanced framework to investigate the feasibility and capacity of LLMs for few-shot multivariate time series classification. This model introduces a Patch-wise Temporal Convolution Encoder (PTCEnc) to align time series data with the textual embedding input of LLMs. We further fine-tune the pre-trained LLM decoder with Low-rank Adaptations (LoRA) to enhance its feature representation learning ability in time series data. Experimental results show that our model outperformed state-of-the-art baselines by a large margin, achieving 125.2% and 50.2% improvement in classification accuracy on Handwriting and EthanolConcentration datasets, respectively. Moreover, our experimental results demonstrate that LLM-based methods perform well across a variety of datasets in few-shot MTSC, delivering reliable results compared to traditional models. This success paves the way for their deployment in industrial environments where data are limited. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶é—´åºåˆ—åˆ†ææ–¹é¢å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå…³äºå¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®çš„å°‘æ ·æœ¬åˆ†ç±»ï¼ˆç”±äºå·¥ä¸šåº”ç”¨ä¸­å¯ç”¨è®­ç»ƒæ•°æ®æœ‰é™ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„è®­ç»ƒåœºæ™¯ï¼‰ï¼Œå…¶åœ¨LLMä¸­çš„æ•ˆç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ©ç”¨LLMä¸­ä¸°å¯Œçš„é¢„è®­ç»ƒçŸ¥è¯†æ¥è§£å†³å¤šå…ƒæ—¶é—´åºåˆ—ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†LLMFewæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨LLMè¿›è¡Œå°‘æ ·æœ¬å¤šå…ƒæ—¶é—´åºåˆ—åˆ†ç±»çš„å¢å¼ºæ¡†æ¶ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªPatch-wise Temporal Convolution Encoderï¼ˆPTCEncï¼‰ï¼Œå°†æ—¶é—´åºåˆ—æ•°æ®ä¸LLMçš„æ–‡æœ¬åµŒå…¥è¾“å…¥è¿›è¡Œå¯¹é½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨Low-rank Adaptationsï¼ˆLoRAï¼‰å¯¹é¢„è®­ç»ƒçš„LLMè§£ç å™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥å¢å¼ºå…¶åœ¨æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„ç‰¹å¾è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šå¤§å¹…åº¦è¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹ï¼Œåœ¨æ‰‹å†™å’ŒEthanolConcentrationæ•°æ®é›†ä¸Šçš„åˆ†ç±»å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†125.2%å’Œ50.2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„æ–¹æ³•åœ¨å°‘æ ·æœ¬MTSCä¸­è¡¨ç°è‰¯å¥½ï¼Œä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”æä¾›äº†å¯é çš„ç»“æœã€‚è¿™ä¸€æˆåŠŸä¸ºåœ¨å·¥ä¸šç¯å¢ƒä¸­éƒ¨ç½²è¿™äº›æ¨¡å‹é“ºå¹³äº†é“è·¯ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00059v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨å¤šå…ƒæ—¶é—´åºåˆ—åˆ†ç±»æ–¹é¢çš„æ½œåŠ›å·¨å¤§ï¼Œå°¤å…¶æ˜¯å°‘æ ·æœ¬åœºæ™¯ä¸‹ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§LLMå¢å¼ºçš„æ¡†æ¶LLMFewï¼Œé€šè¿‡Patch-wise Temporal Convolution Encoderï¼ˆPTCEncï¼‰å’ŒLow-rank Adaptationsï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œå®ç°äº†å¯¹LLMçš„å¾®è°ƒï¼Œå¤§å¹…æå‡äº†åˆ†ç±»å‡†ç¡®åº¦ã€‚å®éªŒè¯æ˜ï¼ŒLLMFewæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¿™ä¸€è¿›å±•ä¸ºå·¥ä¸šç¯å¢ƒä¸­æ•°æ®æœ‰é™çš„æƒ…å†µæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤šå…ƒæ—¶é—´åºåˆ—åˆ†æä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨å°‘æ ·æœ¬åˆ†ç±»æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºäº†LLMFewæ¡†æ¶ï¼Œåˆ©ç”¨LLMçš„é¢„è®­ç»ƒçŸ¥è¯†æ¥è§£å†³å¤šå…ƒæ—¶é—´åºåˆ—çš„å°‘æ ·æœ¬åˆ†ç±»é—®é¢˜ã€‚</li>
<li>LLMFewæ¡†æ¶é€šè¿‡PTCEncæŠ€æœ¯å°†æ—¶é—´åºåˆ—æ•°æ®ä¸LLMçš„æ–‡æœ¬åµŒå…¥è¾“å…¥å¯¹é½ã€‚</li>
<li>é‡‡ç”¨LoRAæŠ€æœ¯å¯¹LLMçš„é¢„è®­ç»ƒè§£ç å™¨è¿›è¡Œå¾®è°ƒï¼Œå¢å¼ºäº†å…¶åœ¨æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„ç‰¹å¾è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMFewæ¨¡å‹åœ¨Handwritingå’ŒEthanolConcentrationæ•°æ®é›†ä¸Šçš„åˆ†ç±»å‡†ç¡®åº¦åˆ†åˆ«æé«˜äº†125.2%å’Œ50.2%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>LLMåœ¨å°‘æ ·æœ¬å¤šå…ƒæ—¶é—´åºåˆ—åˆ†ç±»ä¸­çš„è¡¨ç°å¯é ï¼Œé€‚ç”¨äºå¤šç§æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b7da6f94dbc82e6ba9661a3ff60210e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16de2f9f1d4e150a3d8e46e6013cdb4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13ae35225af91619bc061c0522078c67.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Monocular-Per-Object-Distance-Estimation-with-Masked-Object-Modeling"><a href="#Monocular-Per-Object-Distance-Estimation-with-Masked-Object-Modeling" class="headerlink" title="Monocular Per-Object Distance Estimation with Masked Object Modeling"></a>Monocular Per-Object Distance Estimation with Masked Object Modeling</h2><p><strong>Authors:Aniello Panariello, Gianluca Mancusi, Fedy Haj Ali, Angelo Porrello, Simone Calderara, Rita Cucchiara</strong></p>
<p>Per-object distance estimation is critical in surveillance and autonomous driving, where safety is crucial. While existing methods rely on geometric or deep supervised features, only a few attempts have been made to leverage self-supervised learning. In this respect, our paper draws inspiration from Masked Image Modeling (MiM) and extends it to multi-object tasks. While MiM focuses on extracting global image-level representations, it struggles with individual objects within the image. This is detrimental for distance estimation, as objects far away correspond to negligible portions of the image. Conversely, our strategy, termed Masked Object Modeling (MoM), enables a novel application of masking techniques. In a few words, we devise an auxiliary objective that reconstructs the portions of the image pertaining to the objects detected in the scene. The training phase is performed in a single unified stage, simultaneously optimizing the masking objective and the downstream loss (i.e., distance estimation).   We evaluate the effectiveness of MoM on a novel reference architecture (DistFormer) on the standard KITTI, NuScenes, and MOTSynth datasets. Our evaluation reveals that our framework surpasses the SoTA and highlights its robust regularization properties. The MoM strategy enhances both zero-shot and few-shot capabilities, from synthetic to real domain. Finally, it furthers the robustness of the model in the presence of occluded or poorly detected objects. Code is available at <a target="_blank" rel="noopener" href="https://github.com/apanariello4/DistFormer">https://github.com/apanariello4/DistFormer</a> </p>
<blockquote>
<p>åŸºäºæ¯ä¸ªå¯¹è±¡çš„è·ç¦»ä¼°è®¡æ˜¯ç›‘æ§å’Œè‡ªåŠ¨é©¾é©¶ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå®‰å…¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•å¤§å¤šä¾èµ–äºå‡ ä½•ç‰¹å¾æˆ–æ·±åº¦ç›‘ç£ç‰¹å¾ï¼Œåªæœ‰å°‘æ•°å°è¯•åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ã€‚åœ¨è¿™æ–¹é¢ï¼Œæˆ‘ä»¬çš„è®ºæ–‡ä»è¢«é®æŒ¡çš„å›¾åƒå»ºæ¨¡ï¼ˆMiMï¼‰ä¸­æ±²å–çµæ„Ÿï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°å¤šå¯¹è±¡ä»»åŠ¡ã€‚è™½ç„¶MiMä¾§é‡äºæå–å…¨å±€å›¾åƒçº§åˆ«çš„è¡¨ç¤ºï¼Œä½†å®ƒå¯¹å›¾åƒä¸­çš„å•ä¸ªå¯¹è±¡å¤„ç†èµ·æ¥æ¯”è¾ƒå›°éš¾ã€‚è¿™å¯¹äºè·ç¦»ä¼°è®¡æ˜¯æœ‰å®³çš„ï¼Œå› ä¸ºè¿œå¤„çš„å¯¹è±¡åœ¨å›¾åƒä¸­åªå å¾ˆå°ä¸€éƒ¨åˆ†ã€‚ç›¸åï¼Œæˆ‘ä»¬çš„ç­–ç•¥ç§°ä¸ºè¢«é®æŒ¡çš„å¯¹è±¡å»ºæ¨¡ï¼ˆMoMï¼‰ï¼Œå®ç°äº†é®æŒ¡æŠ€æœ¯çš„åˆ›æ–°åº”ç”¨ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¾…åŠ©ç›®æ ‡æ¥é‡å»ºåœºæ™¯ä¸­æ£€æµ‹åˆ°çš„å¯¹è±¡çš„å›¾åƒéƒ¨åˆ†ã€‚è®­ç»ƒé˜¶æ®µæ˜¯åœ¨ä¸€ä¸ªç»Ÿä¸€çš„å•ä¸€é˜¶æ®µå®Œæˆçš„ï¼ŒåŒæ—¶ä¼˜åŒ–é®æŒ¡ç›®æ ‡å’Œä¸‹æ¸¸æŸå¤±ï¼ˆå³è·ç¦»ä¼°è®¡ï¼‰ã€‚æˆ‘ä»¬åœ¨æ–°å‹å‚è€ƒæ¶æ„ï¼ˆDistFormerï¼‰ä¸Šè¯„ä¼°MoMçš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ ‡å‡†çš„KITTIã€NuSceneså’ŒMOTSynthæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯å¹¶çªå‡ºäº†å…¶ç¨³å¥çš„æ­£åˆ™åŒ–å±æ€§ã€‚MoMç­–ç•¥å¢å¼ºäº†é›¶é•œå¤´å’Œå°‘é•œå¤´èƒ½åŠ›ï¼Œä»åˆæˆåˆ°çœŸå®é¢†åŸŸã€‚æœ€åï¼Œå®ƒåœ¨å­˜åœ¨é®æŒ¡æˆ–æ£€æµ‹ä¸è‰¯çš„å¯¹è±¡çš„æƒ…å†µä¸‹è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/apanariello4/DistFormer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/apanariello4/DistFormeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.03191v2">PDF</a> Accepted for publication in Computer Vision and Image Understanding   (CVIU) 2025</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŠ€æœ¯ï¼Œç§°ä¸ºMasked Object Modelingï¼ˆMoMï¼‰ï¼Œç”¨äºæ”¹å–„ç›®æ ‡è·ç¦»ä¼°ç®—çš„é—®é¢˜ã€‚ç›¸è¾ƒäºå…ˆå‰åŸºäºå…¨å±€å›¾åƒçº§çš„ç‰¹å¾æå–æŠ€æœ¯ï¼Œä¾‹å¦‚Masked Image Modelingï¼ˆMiMï¼‰ï¼ŒMoMå¯æ›´æœ‰æ•ˆåœ°é’ˆå¯¹å¤šç›®æ ‡ä»»åŠ¡è¿›è¡Œå¤„ç†ã€‚é€šè¿‡å¼€å‘ä¸€ä¸ªè¾…åŠ©ç›®æ ‡æ¥é‡å»ºåœºæ™¯ä¸­æ£€æµ‹åˆ°çš„å¯¹è±¡çš„å›¾åƒéƒ¨åˆ†ï¼ŒMoMä¼˜åŒ–äº†è®­ç»ƒé˜¶æ®µï¼Œå¹¶æé«˜äº†è·ç¦»ä¼°ç®—çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åœ¨DistFormeræ¶æ„ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>Masked Object Modeling (MoM)è¢«å¼•å…¥ä»¥è§£å†³ç›‘æ§å’Œè‡ªåŠ¨é©¾é©¶ä¸­çš„ç›®æ ‡è·ç¦»ä¼°ç®—é—®é¢˜ã€‚</li>
<li>MoMæŠ€æœ¯é€šè¿‡é‡å»ºåœºæ™¯ä¸­æ£€æµ‹åˆ°çš„å¯¹è±¡çš„å›¾åƒéƒ¨åˆ†æ¥ä¼˜åŒ–è®­ç»ƒé˜¶æ®µã€‚</li>
<li>MoMç­–ç•¥ç»“åˆäº†é®æŒ¡æŠ€æœ¯ä¸ä¸‹æ¸¸ä»»åŠ¡æŸå¤±ï¼ˆå¦‚è·ç¦»ä¼°ç®—ï¼‰ï¼Œå®ç°äº†å•ä¸€ç»Ÿä¸€é˜¶æ®µçš„è®­ç»ƒã€‚</li>
<li>MoMåœ¨DistFormeræ¶æ„ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„ç¨³å¥æ€§ã€‚</li>
<li>MoMç­–ç•¥å¢å¼ºäº†æ¨¡å‹çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶å…·æœ‰ä»åˆæˆåˆ°çœŸå®åŸŸçš„é€‚åº”æ€§ã€‚</li>
<li>MoMæé«˜äº†æ¨¡å‹å¯¹é®æŒ¡æˆ–æ£€æµ‹ä¸è‰¯ç›®æ ‡çš„ç¨³å¥æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.03191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67f12d55fcffb118b25a8fb7acb579f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d036316cdf8bdb7ef15ce274b500e307.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f00ce5f81e8ff3232a63cd27b1ac527.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25b400b205d47a5a830428a298f73a99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ab57b78ba775a0e866360773d4766c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8db20cd3be336a2912cae35891673f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4271a79955737993860882e7629a5fab.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8eb7cec5a3046c6ac84ae266dfef4c6f.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  VideoWebArena Evaluating Long Context Multimodal Agents with Video   Understanding Web Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-129c03c30fabc821a4ca723773c59182.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  DHP Discrete Hierarchical Planning for Hierarchical Reinforcement   Learning Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">11004.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
