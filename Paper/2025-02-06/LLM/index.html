<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  A comparison of translation performance between DeepL and Supertext">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-67c4ed3d47a60b3d126b11e3d1c258e2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-06-æ›´æ–°"><a href="#2025-02-06-æ›´æ–°" class="headerlink" title="2025-02-06 æ›´æ–°"></a>2025-02-06 æ›´æ–°</h1><h2 id="A-comparison-of-translation-performance-between-DeepL-and-Supertext"><a href="#A-comparison-of-translation-performance-between-DeepL-and-Supertext" class="headerlink" title="A comparison of translation performance between DeepL and Supertext"></a>A comparison of translation performance between DeepL and Supertext</h2><p><strong>Authors:Alex FlÃ¼ckiger, Chantal Amrhein, Tim Graf, Philippe SchlÃ¤pfer, Florian Schottmann, Samuel LÃ¤ubli</strong></p>
<p>As strong machine translation (MT) systems are increasingly based on large language models (LLMs), reliable quality benchmarking requires methods that capture their ability to leverage extended context. This study compares two commercial MT systems â€“ DeepL and Supertext â€“ by assessing their performance on unsegmented texts. We evaluate translation quality across four language directions with professional translators assessing segments with full document-level context. While segment-level assessments indicate no strong preference between the systems in most cases, document-level analysis reveals a preference for Supertext in three out of four language directions, suggesting superior consistency across longer texts. We advocate for more context-sensitive evaluation methodologies to ensure that MT quality assessments reflect real-world usability. We release all evaluation data and scripts for further analysis and reproduction at <a target="_blank" rel="noopener" href="https://github.com/supertext/evaluation_deepl_supertext">https://github.com/supertext/evaluation_deepl_supertext</a>. </p>
<blockquote>
<p>éšç€å¼ºå¤§çš„æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¯é çš„è´¨é‡è¯„ä¼°éœ€è¦èƒ½å¤Ÿæ•æ‰å…¶åœ¨æ‰©å±•è¯­å¢ƒä¸‹èƒ½åŠ›çš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶é€šè¿‡è¯„ä¼°å®ƒä»¬åœ¨æœªåˆ†æ®µæ–‡æœ¬ä¸Šçš„è¡¨ç°ï¼Œå¯¹æ¯”äº†DeepLå’ŒSupertextä¸¤ä¸ªå•†ä¸šæœºå™¨ç¿»è¯‘ç³»ç»Ÿã€‚æˆ‘ä»¬è¯„ä¼°äº†å››ä¸ªè¯­è¨€æ–¹å‘çš„ç¿»è¯‘è´¨é‡ï¼Œä¸“ä¸šç¿»è¯‘äººå‘˜ä¼šè¯„ä¼°å…·æœ‰å…¨æ–‡è¯­å¢ƒçš„ç‰‡æ®µã€‚è™½ç„¶åˆ†æ®µçº§åˆ«çš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¿™ä¸¤ä¸ªç³»ç»Ÿä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„åå¥½ï¼Œä½†æ–‡æ¡£çº§åˆ«çš„åˆ†ææ­ç¤ºäº†åœ¨å››ä¸ªè¯­è¨€æ–¹å‘ä¸­æœ‰ä¸‰ä¸ªæ–¹å‘æ›´å€¾å‘äºSupertextï¼Œè¿™è¡¨æ˜å…¶åœ¨è¾ƒé•¿æ–‡æœ¬ä¸­çš„ä¸€è‡´æ€§æ›´ä½³ã€‚æˆ‘ä»¬æå€¡é‡‡ç”¨æ›´æ•æ„Ÿäºè¯­å¢ƒçš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥ç¡®ä¿æœºå™¨ç¿»è¯‘çš„è´¨é‡è¯„ä¼°èƒ½å¤Ÿåæ˜ çœŸå®ä¸–ç•Œçš„å¯ç”¨æ€§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/supertext/evaluation_deepl_supertext">https://github.com/supertext/evaluation_deepl_supertext</a>ä¸Šå…¬å¼€äº†æ‰€æœ‰è¯„ä¼°æ•°æ®å’Œè„šæœ¬ï¼Œä¾›è¿›ä¸€æ­¥åˆ†æå’Œå¤åˆ¶ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02577v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶é€šè¿‡æ–‡æ¡£çº§åˆ«è¯„ä¼°DeepLå’ŒSupertextä¸¤ä¸ªå•†ä¸šæœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œå‘ç°åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹çš„åˆ†æ®µè¯„ä¼°è¡¨ç°ç›¸è¿‘ï¼Œä½†åœ¨å››ä¸ªè¯­è¨€æ–¹å‘ä¸­çš„ä¸‰ä¸ªæ–¹å‘ä¸­ï¼ŒSupertextåœ¨æ–‡æ¡£çº§åˆ«çš„è¡¨ç°æ›´ä½³ï¼Œå±•ç°å‡ºæ›´ä¼˜ç§€çš„é•¿æ–‡æœ¬ä¸€è‡´æ€§ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶å‘¼åé‡‡ç”¨æ›´è¯­å¢ƒåŒ–çš„è¯„ä¼°æ–¹æ³•æ¥ç¡®ä¿æœºå™¨ç¿»è¯‘çš„è´¨é‡è¯„ä¼°åæ˜ çœŸå®ä¸–ç•Œçš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æœ¬ç ”ç©¶å¯¹æ¯”äº†DeepLå’ŒSupertextä¸¤ä¸ªå•†ä¸šæœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œè¯„ä¼°å…¶åœ¨æœªåˆ†å‰²æ–‡æœ¬ä¸Šçš„è¡¨ç°ã€‚</li>
<li>åœ¨å››ä¸ªè¯­è¨€æ–¹å‘ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶‰åŠä¸“ä¸šç¿»è¯‘äººå‘˜å¯¹å…·æœ‰å…¨æ–‡ä¸Šä¸‹æ–‡çš„ç‰‡æ®µçš„è¯„ä¼°ã€‚</li>
<li>åœ¨åˆ†æ®µè¯„ä¼°ä¸­ï¼Œä¸¤ä¸ªç³»ç»Ÿçš„è¡¨ç°ç›¸å½“ã€‚</li>
<li>åœ¨æ–‡æ¡£çº§åˆ«ä¸Šï¼ŒSupertextåœ¨ä¸‰ä¸ªè¯­è¨€æ–¹å‘ä¸Šçš„è¡¨ç°ä¼˜äºDeepLï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨é•¿æ–‡æœ¬ä¸­çš„ä¸€è‡´æ€§æ›´å¥½ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†éœ€è¦æ›´è¯­å¢ƒåŒ–çš„è¯„ä¼°æ–¹æ³•æ¥ç¡®ä¿æœºå™¨ç¿»è¯‘çš„è´¨é‡è¯„ä¼°èƒ½å¤Ÿåæ˜ çœŸå®ä¸–ç•Œçš„å®ç”¨æ€§ã€‚</li>
<li>æ‰€æœ‰è¯„ä¼°æ•°æ®å’Œè„šæœ¬å‡å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¾›è¿›ä¸€æ­¥åˆ†æå’Œå¤åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e134c8f7a8f126d5b3450f21e45fe868.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49dfe6b0e3cf294d1abef54ec607db1d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-891e2468bf58f14921f1657de4481927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0796ec379ea3c8a747ce52797f9f200.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LLMs-for-Generation-of-Architectural-Components-An-Exploratory-Empirical-Study-in-the-Serverless-World"><a href="#LLMs-for-Generation-of-Architectural-Components-An-Exploratory-Empirical-Study-in-the-Serverless-World" class="headerlink" title="LLMs for Generation of Architectural Components: An Exploratory   Empirical Study in the Serverless World"></a>LLMs for Generation of Architectural Components: An Exploratory   Empirical Study in the Serverless World</h2><p><strong>Authors:Shrikara Arun, Meghana Tedla, Karthik Vaidhyanathan</strong></p>
<p>Recently, the exponential growth in capability and pervasiveness of Large Language Models (LLMs) has led to significant work done in the field of code generation. However, this generation has been limited to code snippets. Going one step further, our desideratum is to automatically generate architectural components. This would not only speed up development time, but would also enable us to eventually completely skip the development phase, moving directly from design decisions to deployment. To this end, we conduct an exploratory study on the capability of LLMs to generate architectural components for Functions as a Service (FaaS), commonly known as serverless functions. The small size of their architectural components make this architectural style amenable for generation using current LLMs compared to other styles like monoliths and microservices. We perform the study by systematically selecting open source serverless repositories, masking a serverless function and utilizing state of the art LLMs provided with varying levels of context information about the overall system to generate the masked function. We evaluate correctness through existing tests present in the repositories and use metrics from the Software Engineering (SE) and Natural Language Processing (NLP) domains to evaluate code quality and the degree of similarity between human and LLM generated code respectively. Along with our findings, we also present a discussion on the path forward for using GenAI in architectural component generation. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›å’Œæ™®åŠæ€§çš„æŒ‡æ•°çº§å¢é•¿ï¼Œä»£ç ç”Ÿæˆé¢†åŸŸå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™ç§ç”Ÿæˆä»…é™äºä»£ç ç‰‡æ®µã€‚æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬çš„æ„¿æœ›æ˜¯è‡ªåŠ¨ç”Ÿæˆæ¶æ„ç»„ä»¶ã€‚è¿™ä¸ä»…å¯ä»¥åŠ å¿«å¼€å‘æ—¶é—´ï¼Œè¿˜å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿæœ€ç»ˆå®Œå…¨è·³è¿‡å¼€å‘é˜¶æ®µï¼Œç›´æ¥ä»è®¾è®¡å†³ç­–è½¬ç§»åˆ°éƒ¨ç½²ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹LLMç”Ÿæˆä½œä¸ºæœåŠ¡ï¼ˆFaaSï¼‰çš„æ¶æ„ç»„ä»¶çš„èƒ½åŠ›è¿›è¡Œäº†æ¢ç´¢æ€§ç ”ç©¶ï¼ŒFaaSé€šå¸¸è¢«ç§°ä¸ºæ— æœåŠ¡å™¨å‡½æ•°ã€‚ä¸å…¶ä»–é£æ ¼ï¼ˆå¦‚å•ä½“åº”ç”¨å’Œå¾®æœåŠ¡ï¼‰ç›¸æ¯”ï¼Œå…¶æ¶æ„ç»„ä»¶çš„ä½“ç§¯è¾ƒå°ï¼Œä½¿å¾—è¿™ç§æ¶æ„é£æ ¼æ›´é€‚åˆä½¿ç”¨å½“å‰çš„LLMè¿›è¡Œç”Ÿæˆã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°é€‰æ‹©å¼€æºçš„æ— æœåŠ¡å™¨ä»“åº“è¿›è¡Œç ”ç©¶ï¼Œå¯¹æ— æœåŠ¡å™¨å‡½æ•°è¿›è¡Œæ©ç›–ï¼Œå¹¶åˆ©ç”¨æœ€å…ˆè¿›çš„LLMæŠ€æœ¯ï¼Œä¸ºæ€»ä½“ç³»ç»Ÿæä¾›ä¸åŒçº§åˆ«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç”Ÿæˆè¢«æ©ç›–çš„å‡½æ•°ã€‚æˆ‘ä»¬é€šè¿‡ä»“åº“ä¸­ç°æœ‰çš„æµ‹è¯•æ¥è¯„ä¼°æ­£ç¡®æ€§ï¼Œå¹¶ä½¿ç”¨è½¯ä»¶å·¥ç¨‹ï¼ˆSEï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„æŒ‡æ ‡æ¥è¯„ä¼°ä»£ç è´¨é‡å’Œäººç±»ä¸LLMç”Ÿæˆä»£ç ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚é™¤äº†æˆ‘ä»¬çš„å‘ç°ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å°±æœªæ¥åœ¨æ¶æ„ç»„ä»¶ç”Ÿæˆä¸­ä½¿ç”¨GenAIçš„è·¯å¾„è¿›è¡Œäº†è®¨è®ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02539v1">PDF</a> Accepted to IEEE International Conference on Software Architecture   (ICSA) 2025 Main Track (<a target="_blank" rel="noopener" href="https://conf.researchr.org/home/icsa-2025">https://conf.researchr.org/home/icsa-2025</a>)</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›å’Œæ™®åŠæ€§å‘ˆç°æŒ‡æ•°çº§å¢é•¿ï¼Œä»£ç ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶ä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»£ç ç”Ÿæˆä¸»è¦é›†ä¸­åœ¨ä»£ç ç‰‡æ®µå±‚é¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¿›ä¸€æ­¥æ¢ç´¢LLMè‡ªåŠ¨ç”Ÿæˆæ¶æ„ç»„ä»¶çš„å¯èƒ½æ€§ã€‚è¿™é¡¹ç ”ç©¶ä»¥å‡½æ•°å³æœåŠ¡ï¼ˆFaaSï¼‰çš„æ¶æ„é£æ ¼ä¸ºä¾‹ï¼Œå‘ç°å…¶æ¶æ„ç»„ä»¶è¾ƒå°ï¼Œæ›´é€‚åˆä½¿ç”¨å½“å‰çš„LLMè¿›è¡Œç”Ÿæˆã€‚ç ”ç©¶é€šè¿‡é€‰æ‹©å¼€æºçš„æ— æœåŠ¡å™¨å­˜å‚¨åº“ï¼Œå¯¹å…¶ä¸­æŸä¸ªå‡½æ•°è¿›è¡Œæ©ç›–ï¼Œå¹¶åˆ©ç”¨å…ˆè¿›çš„LLMï¼Œåœ¨æä¾›ä¸åŒçº§åˆ«çš„ç³»ç»Ÿä¸Šä¸‹æ–‡ä¿¡æ¯çš„æƒ…å†µä¸‹ç”Ÿæˆæ©ç›–çš„å‡½æ•°ã€‚æœ¬ç ”ç©¶é€šè¿‡ç°æœ‰æµ‹è¯•è¯„ä¼°æ­£ç¡®æ€§ï¼Œå¹¶ä½¿ç”¨è½¯ä»¶å·¥ç¨‹å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŒ‡æ ‡è¯„ä¼°ä»£ç è´¨é‡å’ŒLLMç”Ÿæˆä»£ç ä¸äººç±»ç”Ÿæˆä»£ç çš„ç›¸ä¼¼åº¦ã€‚æ­¤å¤–ï¼Œè¿˜è®¨è®ºäº†æœªæ¥åœ¨æ¶æ„ç»„ä»¶ç”Ÿæˆé¢†åŸŸä½¿ç”¨GenAIçš„å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä¸»è¦é›†ä¸­åœ¨ä»£ç ç‰‡æ®µçš„ç”Ÿæˆã€‚</li>
<li>ç ”ç©¶æ—¨åœ¨æ¢ç´¢LLMè‡ªåŠ¨ç”Ÿæˆæ¶æ„ç»„ä»¶çš„å¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡½æ•°å³æœåŠ¡ï¼ˆFaaSï¼‰é¢†åŸŸã€‚</li>
<li>FaaSçš„æ¶æ„é£æ ¼ç”±äºå…¶è¾ƒå°çš„æ¶æ„ç»„ä»¶ï¼Œæ›´é€‚åˆä½¿ç”¨å½“å‰çš„LLMè¿›è¡Œç”Ÿæˆã€‚</li>
<li>ç ”ç©¶é€šè¿‡é€‰æ‹©å¼€æºçš„æ— æœåŠ¡å™¨å­˜å‚¨åº“è¿›è¡Œæ©ç›–å‡½æ•°å®éªŒï¼Œå¹¶åˆ©ç”¨å…ˆè¿›çš„LLMç”Ÿæˆè¿™äº›å‡½æ•°ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†ç”Ÿæˆçš„æ­£ç¡®æ€§ã€ä»£ç è´¨é‡ä»¥åŠLLMç”Ÿæˆä»£ç ä¸äººç±»ç”Ÿæˆä»£ç çš„ç›¸ä¼¼åº¦ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•åŒ…æ‹¬ç°æœ‰æµ‹è¯•ã€è½¯ä»¶å·¥ç¨‹å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02539">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8aec6f506a377f602b52a2b16bb33e73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0683e4b0630f79a6dad4c1885bc15c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-720af3f53a168a340c53852ffef60f3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c402d780efe7b8bf8ecd236c7cd03fed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-670ed1f7eeffd92c73d45e9178818596.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Satori-Reinforcement-Learning-with-Chain-of-Action-Thought-Enhances-LLM-Reasoning-via-Autoregressive-Search"><a href="#Satori-Reinforcement-Learning-with-Chain-of-Action-Thought-Enhances-LLM-Reasoning-via-Autoregressive-Search" class="headerlink" title="Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM   Reasoning via Autoregressive Search"></a>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM   Reasoning via Autoregressive Search</h2><p><strong>Authors:Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, Chuang Gan</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMsâ€™ reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†æƒŠäººçš„æ¨ç†èƒ½åŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¢åŠ æµ‹è¯•æ—¶çš„è®¡ç®—é‡å¯ä»¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚è¿™é€šå¸¸æ¶‰åŠåœ¨æ¨ç†æ—¶é—´æ—¶ç”±å¤–éƒ¨LLMéªŒè¯å™¨å¼•å¯¼çš„å¹¿æ³›é‡‡æ ·ï¼Œä»è€Œå½¢æˆäº†ä¸€ä¸ªåŒäººç³»ç»Ÿã€‚å°½ç®¡æœ‰å¤–éƒ¨æŒ‡å¯¼ï¼Œä½†è¯¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§å±•ç¤ºäº†å•ä¸ªLLMè§£å†³å¤æ‚ä»»åŠ¡çš„æ½œåŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ç ”ç©¶é—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦å°†æœç´¢èƒ½åŠ›å†…åœ¨åŒ–ï¼Œä»è€Œä»æ ¹æœ¬ä¸Šæé«˜å•ä¸ªLLMçš„æ¨ç†èƒ½åŠ›ï¼Ÿè¿™é¡¹å·¥ä½œæ¢ç´¢äº†ä¸€ä¸ªæ­£äº¤æ–¹å‘ï¼Œä¸“æ³¨äºå¯¹è®­ç»ƒåçš„LLMè¿›è¡Œè‡ªå›å½’æœç´¢ï¼ˆå³ï¼Œå…·æœ‰è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘æ¢ç´¢æ–°ç­–ç•¥çš„æ‰©å±•æ¨ç†è¿‡ç¨‹ï¼‰ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†è¡ŒåŠ¨æ€ç»´é“¾ï¼ˆCOATï¼‰æ¨ç†å’Œä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼š1ï¼‰å°è§„æ¨¡æ ¼å¼è°ƒæ•´é˜¶æ®µï¼Œä»¥å†…åœ¨åŒ–COATæ¨ç†æ ¼å¼ï¼›2ï¼‰åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„å¤§è§„æ¨¡è‡ªæˆ‘æ”¹è¿›é˜¶æ®µã€‚æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿäº†Satoriï¼Œä¸€ä¸ªä»¥å¼€æºæ¨¡å‹å’Œæ•°æ®è®­ç»ƒçš„7B LLMã€‚å¹¿æ³›çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒSatoriåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒæ—¶åœ¨åŸŸå¤–ä»»åŠ¡ä¸­å±•ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†å®Œå…¨å¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02508v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰è·¨åŸŸçš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°åœ¨æµ‹è¯•æ—¶å¢åŠ è®¡ç®—æ—¶é—´èƒ½æé«˜LLMçš„æ¨ç†èƒ½åŠ›ï¼Œé€šå¸¸é‡‡ç”¨åœ¨æ¨ç†æ—¶é—´è¿›è¡Œå¤§é‡é‡‡æ ·å¹¶åœ¨å¤–éƒ¨LLMéªŒè¯å™¨çš„æŒ‡å¯¼ä¸‹å®Œæˆï¼Œå½¢æˆäº†ä¸€ç§ä¸¤ç©å®¶ç³»ç»Ÿã€‚ç ”ç©¶æå‡ºæ–°çš„é—®é¢˜ï¼šèƒ½å¦å°†æœç´¢èƒ½åŠ›å†…åœ¨åŒ–ï¼Œä»æ ¹æœ¬ä¸Šæé«˜å•ä¸€LLMçš„æ¨ç†èƒ½åŠ›ï¼Ÿè¯¥ç ”ç©¶æ¢ç´¢äº†ä¸€ä¸ªæ­£äº¤æ–¹å‘ï¼Œä¸“æ³¨äºå¯¹LLMè¿›è¡Œåè®­ç»ƒä»¥å®ç°è‡ªä¸»æ¨ç†æœç´¢ï¼ˆå³å…·æœ‰è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘æ¢ç´¢æ–°ç­–ç•¥çš„æ‰©å±•æ¨ç†è¿‡ç¨‹ï¼‰ã€‚é€šè¿‡æå‡ºçš„è¡ŒåŠ¨æ€ç»´é“¾ï¼ˆCOATï¼‰æ¨ç†å’Œä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒèŒƒå¼ï¼Œç ”ç©¶è®­ç»ƒäº†ä¸€ä¸ªåä¸ºSatoriçš„7B LLMã€‚å…¨é¢å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒSatoriåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨åŸŸå¤–ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMså±•ç°è·¨é¢†åŸŸçš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¢åŠ æµ‹è¯•æ—¶çš„è®¡ç®—æ—¶é—´å¯ä»¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤–éƒ¨éªŒè¯å™¨æŒ‡å¯¼ä¸‹çš„é‡‡æ ·å¯¹LLMç³»ç»Ÿæœ‰æ•ˆã€‚</li>
<li>ç ”ç©¶æ¢ç´¢å†…åœ¨åŒ–æœç´¢èƒ½åŠ›ä»¥æé«˜å•ä¸€LLMçš„æ¨ç†èƒ½åŠ›çš„æ–°æ–¹å‘ã€‚</li>
<li>COATæ¨ç†æ–¹æ³•ç”¨äºå®ç°è‡ªä¸»æ¨ç†æœç´¢ã€‚</li>
<li>Satoriæ¨¡å‹åœ¨ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒä¸‹è¡¨ç°ä¼˜ç§€ï¼Œå¹¶åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9552ecefcf2a4a6bbfc7eb1ba8aab501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be02399355b276134cd40cca534f64b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-032dcd9d607c4e81ee766cf6236ce6fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-477295ff18c1dbfedd8c11044e9593e8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multilingual-Machine-Translation-with-Open-Large-Language-Models-at-Practical-Scale-An-Empirical-Study"><a href="#Multilingual-Machine-Translation-with-Open-Large-Language-Models-at-Practical-Scale-An-Empirical-Study" class="headerlink" title="Multilingual Machine Translation with Open Large Language Models at   Practical Scale: An Empirical Study"></a>Multilingual Machine Translation with Open Large Language Models at   Practical Scale: An Empirical Study</h2><p><strong>Authors:Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan,  BinWang</strong></p>
<p>Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving top-tier multilingual translation performance across 28 languages. Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA) models such as TowerInstruct and XALMA and achieves competitive performance with Google Translate and GPT-4-turbo. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šè¯­è¨€å¤„ç†èƒ½åŠ›æ­£åœ¨æŒç»­è¿›æ­¥ï¼Œå³ä½¿æ˜¯å°å‹å¼€æºæ¨¡å‹ä¹Ÿè¡¨ç°å‡ºäº†å¿«é€Ÿæ€§èƒ½çš„æå‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†å…·æœ‰ä¸åˆ°åäº¿å‚æ•°çš„å¼€æºLLMåœ¨å¤„ç†å¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å…­ä¸ªæµè¡Œçš„LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°Gemma2-9Bç­‰æ¨¡å‹è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„å¤šè¯­è¨€ç¿»è¯‘èƒ½åŠ›ã€‚éšåï¼Œæˆ‘ä»¬åœ¨æŒç»­é¢„è®­ç»ƒé˜¶æ®µå¼•å…¥äº†Parallel-First Monolingual-Secondï¼ˆPFMSï¼‰æ•°æ®æ··åˆç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥æé«˜MTæ€§èƒ½ï¼Œå¹¶æ¨å‡ºäº†GemmaX2-28ï¼Œè¿™æ˜¯ä¸€æ¬¾åœ¨28ç§è¯­è¨€ä¸Šå®ç°é¡¶çº§å¤šè¯­è¨€ç¿»è¯‘æ€§èƒ½çš„9Bæ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒGemmaX2-28å§‹ç»ˆä¼˜äºå½“å‰æŠ€æœ¯æ°´å¹³æ¨¡å‹ï¼ˆSOTAï¼‰ï¼Œå¦‚TowerInstructå’ŒXALMAï¼Œä¸Google Translateå’ŒGPT-4-turboç­‰é¡¶å°–æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02481v1">PDF</a> Accept to NAACL2025 Main Conference</p>
<p><strong>Summary</strong><br>LLMåœ¨å¤šè¯­ç§æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œé€šè¿‡å°äºåäº¿å‚æ•°çš„å¼€æºæ¨¡å‹å³å¯å®Œæˆé«˜è´¨é‡ç¿»è¯‘ã€‚ç ”ç©¶é€šè¿‡å…­ç§æµè¡ŒLLMçš„ç»¼åˆè¯„ä¼°å‘ç°ï¼Œå¦‚Gemma2-9Bç­‰æ¨¡å‹å…·å¤‡å‡ºè‰²çš„å¤šè¯­ç§ç¿»è¯‘èƒ½åŠ›ã€‚é‡‡ç”¨Parallel-First Monolingual-Secondï¼ˆPFMSï¼‰æ•°æ®æ··åˆç­–ç•¥è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ŒæˆåŠŸæ¨å‡ºGemmaX2-28æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨28ç§è¯­è¨€ä¹‹é—´å®ç°é¡¶å°–çš„å¤šè¯­ç§ç¿»è¯‘æ€§èƒ½ï¼Œå¹¶è¶…è¶Šç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œä¸Google Translateå’ŒGPT-4-turboç­‰æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šè¯­ç§æœºå™¨ç¿»è¯‘é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå³ä½¿æ˜¯å°è§„æ¨¡å¼€æºæ¨¡å‹ä¹Ÿèƒ½å®ç°é«˜è´¨é‡ç¿»è¯‘ã€‚</li>
<li>Gemma2-9Bç­‰æ¨¡å‹å…·å¤‡å‡ºè‰²çš„å¤šè¯­ç§ç¿»è¯‘èƒ½åŠ›ï¼Œå¯å¤„ç†å¤šç§è¯­è¨€é—´çš„ç¿»è¯‘ä»»åŠ¡ã€‚</li>
<li>Parallel-First Monolingual-Secondï¼ˆPFMSï¼‰æ•°æ®æ··åˆç­–ç•¥è¢«ç”¨äºå¢å¼ºæœºå™¨ç¿»è¯‘æ€§èƒ½ã€‚</li>
<li>GemmaX2-28æ¨¡å‹æ˜¯ä¸€ä¸ª9Bå‚æ•°æ¨¡å‹ï¼Œå…·å¤‡é¡¶å°–çš„å¤šè¯­ç§ç¿»è¯‘æ€§èƒ½ï¼Œè¦†ç›–28ç§è¯­è¨€ã€‚</li>
<li>GemmaX2-28æ¨¡å‹è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨å¤šè¯­ç§ç¿»è¯‘é¢†åŸŸè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>GemmaX2-28æ¨¡å‹çš„æ€§èƒ½ä¸Google Translateå’ŒGPT-4-turboç­‰å…ˆè¿›æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25a57e9857d9e451b56769acb35f4f09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeb944eda4d04e3d17bded5e134aa68f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5107970c9313d3d0f852d160c6ddf6f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61b387235b50bccd93990b2f1d816be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53f1c2c79bd40358885cb88e5df93d32.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SAISA-Towards-Multimodal-Large-Language-Models-with-Both-Training-and-Inference-Efficiency"><a href="#SAISA-Towards-Multimodal-Large-Language-Models-with-Both-Training-and-Inference-Efficiency" class="headerlink" title="SAISA: Towards Multimodal Large Language Models with Both Training and   Inference Efficiency"></a>SAISA: Towards Multimodal Large Language Models with Both Training and   Inference Efficiency</h2><p><strong>Authors:Qianhao Yuan, Yanjiang Liu, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun</strong></p>
<p>Multimodal Large Language Models (MLLMs) mainly fall into two architectures, each involving a trade-off between training and inference efficiency: embedding space alignment (e.g., LLaVA-1.5) is inefficient during inference, while cross-attention space alignment (e.g., Flamingo) is inefficient in training. In this paper, we compare these two architectures and identify the key factors for building efficient MLLMs. A primary difference between them lies in how attention is applied to visual tokens, particularly in their interactions with each other. To investigate whether attention among visual tokens is necessary, we propose a new self-attention mechanism, NAAViT (\textbf{N}o \textbf{A}ttention \textbf{A}mong \textbf{Vi}sual \textbf{T}okens), which eliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that attention among visual tokens is highly redundant. Based on these insights, we introduce SAISA (\textbf{S}elf-\textbf{A}ttention \textbf{I}nput \textbf{S}pace \textbf{A}lignment), a novel architecture that enhance both training and inference efficiency. SAISA directly aligns visual features with the input spaces of NAAViT self-attention blocks, reducing computational overhead in both self-attention blocks and feed-forward networks (FFNs). Using the same configuration as LLaVA-1.5, SAISA reduces inference FLOPs by 66% and training budget by 26%, while achieving superior performance in terms of accuracy. Comprehensive ablation studies further validate the effectiveness of SAISA across various LLMs and visual encoders. The code and model will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/icip-cas/SAISA">https://github.com/icip-cas/SAISA</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦å¯åˆ†ä¸ºä¸¤ç§æ¶æ„ï¼Œæ¯ç§æ¶æ„åœ¨è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ä¹‹é—´éƒ½å­˜åœ¨æƒè¡¡ï¼šåµŒå…¥ç©ºé—´å¯¹é½ï¼ˆä¾‹å¦‚LLaVA-1.5ï¼‰åœ¨æ¨ç†æ—¶æ•ˆç‡è¾ƒä½ï¼Œè€Œäº¤å‰æ³¨æ„åŠ›ç©ºé—´å¯¹é½ï¼ˆä¾‹å¦‚Flamingoï¼‰åœ¨è®­ç»ƒæ—¶æ•ˆç‡è¾ƒä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†è¿™ä¸¤ç§æ¶æ„ï¼Œå¹¶ç¡®å®šäº†æ„å»ºé«˜æ•ˆMLLMçš„å…³é”®å› ç´ ã€‚å®ƒä»¬ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºå¦‚ä½•å°†æ³¨æ„åŠ›åº”ç”¨äºè§†è§‰æ ‡è®°ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ƒä»¬ä¹‹é—´çš„äº¤äº’ä¸Šã€‚ä¸ºäº†è°ƒæŸ¥è§†è§‰æ ‡è®°ä¹‹é—´çš„æ³¨æ„åŠ›æ˜¯å¦å¿…è¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶NAAViTï¼ˆ\textbf{N}o \textbf{A}ttention \textbf{A}mong \textbf{Vi}sual \textbf{T}okensï¼‰ï¼Œè¯¥æœºåˆ¶æ¶ˆé™¤äº†è¿™ç§ç±»å‹çš„æ³¨æ„åŠ›ã€‚æˆ‘ä»¬å¯¹LLaVA-1.5çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼Œè§†è§‰æ ‡è®°ä¹‹é—´çš„æ³¨æ„åŠ›é«˜åº¦å†—ä½™ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†SAISAï¼ˆ\textbf{S}elf-\textbf{A}ttention \textbf{I}nput \textbf{S}pace \textbf{A}lignmentï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨æé«˜è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚SAISAç›´æ¥å°†è§†è§‰ç‰¹å¾ä¸NAAViTè‡ªæ³¨æ„åŠ›å—çš„è¾“å…¥ç©ºé—´å¯¹é½ï¼Œå‡å°‘äº†è‡ªæ³¨æ„åŠ›å—å’Œå‰é¦ˆç½‘ç»œï¼ˆFFNsï¼‰ä¸­çš„è®¡ç®—å¼€é”€ã€‚ä¸LLaVA-1.5ç›¸åŒçš„é…ç½®ä¸‹ï¼ŒSAISAå°†æ¨ç†FLOPså‡å°‘äº†66%ï¼Œè®­ç»ƒé¢„ç®—å‡å°‘äº†26%ï¼ŒåŒæ—¶åœ¨å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†SAISAåœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/icip-cas/SAISA%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/icip-cas/SAISAå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02458v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡å¯¹æ¯”äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¸¤ç§æ¶æ„ï¼Œå‘ç°å®ƒä»¬åœ¨è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ä¸Šå­˜åœ¨æƒè¡¡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶NAAViTï¼Œå¹¶åŸºäºæ­¤å¼•å…¥äº†SAISAæ¶æ„ï¼Œè¯¥æ¶æ„åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶å‡å…·æœ‰è¾ƒé«˜çš„æ•ˆç‡ï¼Œå¹¶é€šè¿‡ç›´æ¥å¯¹é½è§†è§‰ç‰¹å¾ä¸NAAViTè‡ªæ³¨æ„åŠ›å—çš„è¾“å…¥ç©ºé—´ï¼Œå‡å°‘äº†è‡ªæ³¨æ„åŠ›å—å’Œå‰é¦ˆç½‘ç»œï¼ˆFFNsï¼‰çš„è®¡ç®—å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦æœ‰ä¸¤ç§æ¶æ„ï¼šåµŒå…¥ç©ºé—´å¯¹é½å’Œè·¨æ³¨æ„åŠ›ç©ºé—´å¯¹é½ï¼Œåˆ†åˆ«å­˜åœ¨æ¨ç†å’Œè®­ç»ƒæ•ˆç‡ä¸Šçš„æƒè¡¡ã€‚</li>
<li>è§†è§‰æ ‡è®°ä¹‹é—´çš„æ³¨æ„åŠ›å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡å¯èƒ½æ˜¯å†—ä½™çš„ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶NAAViTï¼Œæ¶ˆé™¤äº†è§†è§‰æ ‡è®°ä¹‹é—´çš„æ³¨æ„åŠ›ã€‚</li>
<li>åŸºäºNAAViTï¼Œå¼•å…¥äº†SAISAæ¶æ„ï¼Œè¯¥æ¶æ„ç›´æ¥å¯¹é½è§†è§‰ç‰¹å¾ä¸è‡ªæ³¨æ„åŠ›å—çš„è¾“å…¥ç©ºé—´ï¼Œæé«˜äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚</li>
<li>SAISAæ¶æ„åœ¨ç›¸åŒé…ç½®ä¸‹ç›¸æ¯”LLaVA-1.5ï¼Œæ¨ç†FLOPså‡å°‘äº†66%ï¼Œè®­ç»ƒé¢„ç®—å‡å°‘äº†26%ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç»¼åˆçš„æ¶ˆèç ”ç©¶éªŒè¯äº†SAISAåœ¨å„ç§å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-40cd2b35afac6eac8c766af3fd2dd415.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad1b0e9d59f27ad30bc390e90030d8cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-923fd447d2f0d59c82e8fea3817dc467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67c4ed3d47a60b3d126b11e3d1c258e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-137dfe8d3f94c3c8a6af93eb53c1b55e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLMER-Crafting-Interactive-Extended-Reality-Worlds-with-JSON-Data-Generated-by-Large-Language-Models"><a href="#LLMER-Crafting-Interactive-Extended-Reality-Worlds-with-JSON-Data-Generated-by-Large-Language-Models" class="headerlink" title="LLMER: Crafting Interactive Extended Reality Worlds with JSON Data   Generated by Large Language Models"></a>LLMER: Crafting Interactive Extended Reality Worlds with JSON Data   Generated by Large Language Models</h2><p><strong>Authors:Jiangong Chen, Xiaoyi Wu, Tian Lan, Bin Li</strong></p>
<p>The integration of Large Language Models (LLMs) like GPT-4 with Extended Reality (XR) technologies offers the potential to build truly immersive XR environments that interact with human users through natural language, e.g., generating and animating 3D scenes from audio inputs. However, the complexity of XR environments makes it difficult to accurately extract relevant contextual data and scene&#x2F;object parameters from an overwhelming volume of XR artifacts. It leads to not only increased costs with pay-per-use models, but also elevated levels of generation errors. Moreover, existing approaches focusing on coding script generation are often prone to generation errors, resulting in flawed or invalid scripts, application crashes, and ultimately a degraded user experience. To overcome these challenges, we introduce LLMER, a novel framework that creates interactive XR worlds using JSON data generated by LLMs. Unlike prior approaches focusing on coding script generation, LLMER translates natural language inputs into JSON data, significantly reducing the likelihood of application crashes and processing latency. It employs a multi-stage strategy to supply only the essential contextual information adapted to the userâ€™s request and features multiple modules designed for various XR tasks. Our preliminary user study reveals the effectiveness of the proposed system, with over 80% reduction in consumed tokens and around 60% reduction in task completion time compared to state-of-the-art approaches. The analysis of usersâ€™ feedback also illuminates a series of directions for further optimization. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰ä¸æ‰©å±•ç°å®ï¼ˆXRï¼‰æŠ€æœ¯çš„é›†æˆï¼Œæœ‰æœ›æ„å»ºçœŸæ­£æ²‰æµ¸å¼XRç¯å¢ƒï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€ä¸äººç±»ç”¨æˆ·è¿›è¡Œäº¤äº’ï¼Œä¾‹å¦‚ä»éŸ³é¢‘è¾“å…¥ç”Ÿæˆå’ŒåŠ¨ç”»åŒ–3Dåœºæ™¯ã€‚ç„¶è€Œï¼ŒXRç¯å¢ƒçš„å¤æ‚æ€§ä½¿å¾—ä»å¤§é‡XRæ–‡ç‰©ä¸­å‡†ç¡®æå–ç›¸å…³ä¸Šä¸‹æ–‡æ•°æ®å’Œç¯å¢ƒ&#x2F;å¯¹è±¡å‚æ•°å˜å¾—å›°éš¾ã€‚è¿™ä¸ä»…å¢åŠ äº†æŒ‰ä½¿ç”¨ä»˜è´¹æ¨¡å‹çš„æˆæœ¬ï¼Œè¿˜æé«˜äº†ç”Ÿæˆé”™è¯¯çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œç°æœ‰ä¸“æ³¨äºç¼–ç è„šæœ¬ç”Ÿæˆçš„æ–¹æ³•å¾€å¾€å®¹æ˜“å‡ºç°ç”Ÿæˆé”™è¯¯ï¼Œå¯¼è‡´æœ‰ç¼ºé™·æˆ–æ— æ•ˆçš„è„šæœ¬ã€åº”ç”¨ç¨‹åºå´©æºƒï¼Œå¹¶æœ€ç»ˆé™ä½ç”¨æˆ·ä½“éªŒã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»‹ç»äº†LLMERï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨LLMç”ŸæˆJSONæ•°æ®åˆ›å»ºäº¤äº’å¼XRä¸–ç•Œçš„æ–°å‹æ¡†æ¶ã€‚ä¸ä»¥å¾€ä¸“æ³¨äºç¼–ç è„šæœ¬ç”Ÿæˆçš„æ–¹æ³•ä¸åŒï¼ŒLLMERå°†è‡ªç„¶è¯­è¨€è¾“å…¥è½¬æ¢ä¸ºJSONæ•°æ®ï¼Œå¤§å¤§é™ä½äº†åº”ç”¨ç¨‹åºå´©æºƒå’Œå¤„ç†å»¶è¿Ÿçš„å¯èƒ½æ€§ã€‚å®ƒé‡‡ç”¨å¤šé˜¶æ®µç­–ç•¥ï¼Œåªæä¾›é€‚åº”äºç”¨æˆ·è¯·æ±‚çš„å¿…è¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶è®¾æœ‰é’ˆå¯¹å„ç§XRä»»åŠ¡è®¾è®¡çš„å¤šä¸ªæ¨¡å—ã€‚æˆ‘ä»¬çš„åˆæ­¥ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºç³»ç»Ÿçš„æœ‰æ•ˆæ€§ä½“ç°åœ¨æ¶ˆè€—çš„ä»¤ç‰Œå‡å°‘äº†80%ä»¥ä¸Šï¼Œä»»åŠ¡å®Œæˆæ—¶é—´å‡å°‘äº†çº¦60%ã€‚ç”¨æˆ·åé¦ˆçš„åˆ†æè¿˜æ­ç¤ºäº†ä¸€ç³»åˆ—è¿›ä¸€æ­¥ä¼˜åŒ–çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02441v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4ä¸æ‰©å±•ç°å®ï¼ˆXRï¼‰æŠ€æœ¯çš„èåˆï¼Œæ„å»ºçœŸæ­£æ²‰æµ¸å¼ã€å¯é€šè¿‡è‡ªç„¶è¯­è¨€äº’åŠ¨çš„XRç¯å¢ƒå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼ŒXRç¯å¢ƒçš„å¤æ‚æ€§å¯¼è‡´ä»å¤§é‡XRä¼ªå½±ä¸­æå–ç›¸å…³ä¸Šä¸‹æ–‡æ•°æ®å’Œåœºæ™¯&#x2F;å¯¹è±¡å‚æ•°å˜å¾—å›°éš¾ã€‚è¿™å¢åŠ äº†æŒ‰ä½¿ç”¨ä»˜è´¹æ¨¡å‹çš„æˆæœ¬å¹¶æé«˜äº†ç”Ÿæˆé”™è¯¯çš„æ°´å¹³ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è„šæœ¬ç”Ÿæˆï¼Œå®¹æ˜“å‡ºç°ç”Ÿæˆé”™è¯¯ï¼Œå¯¼è‡´è„šæœ¬ç¼ºé™·ã€åº”ç”¨ç¨‹åºå´©æºƒå¹¶æœ€ç»ˆé™ä½ç”¨æˆ·ä½“éªŒã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LLMERæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨LLMç”Ÿæˆçš„JSONæ•°æ®åˆ›å»ºäº¤äº’å¼XRä¸–ç•Œã€‚å®ƒé‡‡ç”¨å¤šé˜¶æ®µç­–ç•¥ï¼Œä»…æä¾›é€‚åº”äºç”¨æˆ·è¯·æ±‚çš„å¿…è¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ä¸ºå„ç§XRä»»åŠ¡è®¾è®¡å¤šä¸ªæ¨¡å—ã€‚åˆæ­¥ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ•ˆæœæ˜¾è‘—ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ¶ˆè€—çš„ä»¤ç‰Œå‡å°‘äº†80%ï¼Œä»»åŠ¡å®Œæˆæ—¶é—´å‡å°‘äº†çº¦60%ã€‚ç”¨æˆ·åé¦ˆåˆ†æè¿˜ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä¸XRæŠ€æœ¯çš„ç»“åˆå¯åˆ›å»ºçœŸæ­£æ²‰æµ¸å¼ã€è‡ªç„¶è¯­è¨€äº’åŠ¨çš„XRç¯å¢ƒã€‚</li>
<li>XRç¯å¢ƒçš„å¤æ‚æ€§å¯¼è‡´æå–ç›¸å…³æ•°æ®çš„å›°éš¾ï¼Œå¢åŠ æˆæœ¬å’Œç”Ÿæˆé”™è¯¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è„šæœ¬ç”Ÿæˆï¼Œå®¹æ˜“å‡ºé”™ï¼Œå½±å“ç”¨æˆ·ä½“éªŒã€‚</li>
<li>LLMERæ¡†æ¶ä½¿ç”¨LLMç”Ÿæˆçš„JSONæ•°æ®åˆ›å»ºäº¤äº’å¼XRä¸–ç•Œï¼Œå‡å°‘åº”ç”¨å´©æºƒå’Œå¤„ç†å»¶è¿Ÿã€‚</li>
<li>LLMERé‡‡ç”¨å¤šé˜¶æ®µç­–ç•¥æä¾›å¿…è¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ä¸ºXRä»»åŠ¡è®¾è®¡å¤šä¸ªæ¨¡å—ã€‚</li>
<li>åˆæ­¥ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒLLMERç³»ç»Ÿæ•ˆæœæ˜¾è‘—ï¼Œå¤§å¹…å‡å°‘æ¶ˆè€—èµ„æºå¹¶æå‡æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5309cc9c11d876c7723e0ba007fc9503.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97898513ae4674906f4dfdeb642b34f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13ac128983f2a1727b0e63f1a251d833.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1643a5ddc31f9b298989bdaff7df65c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fe7173ccf2a899b1ceacd6c9679ae85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-297b3c8f2070efe61cefa5c1487dba1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdc8b8ba889bbfee29bd746873376365.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Medical-Multimodal-Model-Stealing-Attacks-via-Adversarial-Domain-Alignment"><a href="#Medical-Multimodal-Model-Stealing-Attacks-via-Adversarial-Domain-Alignment" class="headerlink" title="Medical Multimodal Model Stealing Attacks via Adversarial Domain   Alignment"></a>Medical Multimodal Model Stealing Attacks via Adversarial Domain   Alignment</h2><p><strong>Authors:Yaling Shen, Zhixiong Zhuang, Kun Yuan, Maria-Irina Nicolae, Nassir Navab, Nicolas Padoy, Mario Fritz</strong></p>
<p>Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-STEAL), the first stealing attack against medical MLLMs. ADA-STEAL relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data. </p>
<blockquote>
<p>åŒ»ç–—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£åœ¨æˆä¸ºåŒ»ç–—ä¿å¥ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼ŒååŠ©åŒ»ç–—äººå‘˜è¿›è¡Œå†³ç­–å’Œç»“æœåˆ†æã€‚ç”¨äºç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šçš„æ¨¡å‹èƒ½å¤Ÿè§£é‡ŠåŒ»å­¦å›¾åƒï¼Œä»è€Œå‡è½»æ”¾å°„ç§‘åŒ»å¸ˆçš„å·¥ä½œé‡ã€‚ç”±äºåŒ»ç–—æ•°æ®ç¨€ç¼ºä¸”å—éšç§æ³•è§„çš„ä¿æŠ¤ï¼ŒåŒ»ç–—MLLMsä»£è¡¨äº†å®è´µçš„æ™ºåŠ›è´¢äº§ã€‚ç„¶è€Œï¼Œè¿™äº›èµ„äº§å¯èƒ½é¢ä¸´æ¨¡å‹çªƒå–çš„é£é™©ï¼Œæ”»å‡»è€…è¯•å›¾é€šè¿‡é»‘ç®±è®¿é—®æ¥å¤åˆ¶å…¶åŠŸèƒ½ã€‚è¿„ä»Šä¸ºæ­¢ï¼ŒåŒ»ç–—é¢†åŸŸçš„æ¨¡å‹çªƒå–ä¸»è¦é›†ä¸­åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œä½†ç°æœ‰çš„æ”»å‡»å¯¹MLLMså¹¶ä¸æœ‰æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—MLLMsçš„é¦–ä¸ªçªƒå–æ”»å‡»â€”â€”å¯¹æŠ—åŸŸå¯¹é½ï¼ˆADA-STEALï¼‰ã€‚ADA-STEALä¾èµ–äºå…¬å¼€ä¸”å¹¿æ³›å¯ç”¨çš„è‡ªç„¶å›¾åƒï¼Œè€ŒéåŒ»ç–—å›¾åƒã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡æ·»åŠ å¯¹æŠ—å™ªå£°è¿›è¡Œæ•°æ®å¢å¼ºè¶³ä»¥å…‹æœè‡ªç„¶å›¾åƒä¸å—å®³è€…MLLMçš„ç‰¹å®šé¢†åŸŸåˆ†å¸ƒä¹‹é—´çš„æ•°æ®åˆ†å¸ƒå·®è·ã€‚åœ¨IU Xå°„çº¿ä»¥åŠMIMIC-CXRæ”¾å°„å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¯¹æŠ—åŸŸå¯¹é½ä½¿æ”»å‡»è€…æ— éœ€è®¿é—®ä»»ä½•åŒ»ç–—æ•°æ®å³å¯çªƒå–åŒ»ç–—MLLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02438v1">PDF</a> Accepted at AAAI 2025</p>
<p><strong>Summary</strong><br>åŒ»ç–—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥ç³»ç»Ÿä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼ŒååŠ©åŒ»ç–—äººå‘˜è¿›è¡Œå†³ç­–å’Œç»“æœåˆ†æã€‚æ¨¡å‹å¯ç”¨äºç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šï¼Œè§£è¯»åŒ»å­¦å›¾åƒï¼Œä»è€Œå‡è½»æ”¾å°„ç§‘åŒ»å¸ˆçš„å·¥ä½œé‡ã€‚ç”±äºåŒ»ç–—æ•°æ®ç¨€ç¼ºä¸”å—éšç§æ³•è§„ä¿æŠ¤ï¼ŒåŒ»ç–—MLLMsæˆä¸ºé‡è¦çš„çŸ¥è¯†äº§æƒã€‚ç„¶è€Œï¼Œè¿™äº›èµ„äº§å¯èƒ½é¢ä¸´æ¨¡å‹çªƒå–çš„é£é™©ï¼Œæ”»å‡»è€…é€šè¿‡é»‘ç®±è®¿é—®è¯•å›¾å¤åˆ¶å…¶åŠŸèƒ½ã€‚å°½ç®¡é’ˆå¯¹åŒ»ç–—é¢†åŸŸçš„æ¨¡å‹çªƒå–å·²æœ‰ç ”ç©¶ï¼Œä½†ç°æœ‰æ”»å‡»å¯¹MLLMså¹¶ä¸æœ‰æ•ˆã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹åŒ»ç–—MLLMsçš„çªƒå–æ”»å‡»â€”â€”å¯¹æŠ—åŸŸå¯¹é½ï¼ˆADA-STEALï¼‰ã€‚ADA-STEALä¾èµ–äºå…¬å¼€ä¸”å¹¿æ³›å¯ç”¨çš„è‡ªç„¶å›¾åƒï¼Œå…‹æœäº†è‡ªç„¶å›¾åƒä¸å—å®³è€…MLLMçš„ç‰¹å®šé¢†åŸŸåˆ†å¸ƒä¹‹é—´çš„æ•°æ®åˆ†å¸ƒå·®è·ã€‚åœ¨IU X-RAYå’ŒMIMIC-CXRæ”¾å°„å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¯¹æŠ—åŸŸå¯¹é½ä½¿æ”»å‡»è€…èƒ½å¤Ÿåœ¨æ— éœ€è®¿é—®åŒ»ç–—æ•°æ®çš„æƒ…å†µä¸‹çªƒå–åŒ»ç–—MLLMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥ç³»ç»Ÿä¸­èµ·é‡è¦ä½œç”¨ï¼Œè¾…åŠ©å†³ç­–å’Œç»“æœåˆ†æã€‚</li>
<li>MLLMsèƒ½å¤Ÿç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šï¼Œå‡è½»æ”¾å°„ç§‘åŒ»å¸ˆçš„å·¥ä½œè´Ÿæ‹…ã€‚</li>
<li>åŒ»ç–—æ•°æ®å› å…¶ç¨€ç¼ºæ€§å’Œéšç§ä¿æŠ¤è€Œå…·æœ‰çŸ¥è¯†äº§æƒä»·å€¼ã€‚</li>
<li>MLLMsé¢ä¸´æ¨¡å‹çªƒå–é£é™©ï¼Œæ”»å‡»è€…è¯•å›¾é€šè¿‡é»‘ç®±è®¿é—®å¤åˆ¶å…¶åŠŸèƒ½ã€‚</li>
<li>ç°æœ‰æ”»å‡»å¯¹MLLMsæ•ˆæœä¸ä½³ï¼Œéœ€è¦æ–°çš„æ”»å‡»æ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„é’ˆå¯¹åŒ»ç–—MLLMsçš„çªƒå–æ”»å‡»æ–¹æ³•â€”â€”å¯¹æŠ—åŸŸå¯¹é½ï¼ˆADA-STEALï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd6d59ed3da1f14128b437f75a369c08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76bdec8905bda7c4423a6013ebb9f5ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a8e12f8702934fb75d56f7af98a0842.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b8bac7d4eddee4ec3cf16f63d763390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dedd89a79d4ce15d55537342ef4f32d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e46681d2d988c00397279d28fe083a71.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Flatten-Graphs-as-Sequences-Transformers-are-Scalable-Graph-Generators"><a href="#Flatten-Graphs-as-Sequences-Transformers-are-Scalable-Graph-Generators" class="headerlink" title="Flatten Graphs as Sequences: Transformers are Scalable Graph Generators"></a>Flatten Graphs as Sequences: Transformers are Scalable Graph Generators</h2><p><strong>Authors:Dexiong Chen, Markus Krimmel, Karsten Borgwardt</strong></p>
<p>We introduce AutoGraph, a novel autoregressive framework for generating large attributed graphs using decoder-only transformers. At the core of our approach is a reversible â€œflatteningâ€ process that transforms graphs into random sequences. By sampling and learning from these sequences, AutoGraph enables transformers to model and generate complex graph structures in a manner akin to natural language. In contrast to diffusion models that rely on computationally intensive node features, our approach operates exclusively on these sequences. The sampling complexity and sequence length scale linearly with the number of edges, making AutoGraph highly scalable for generating large sparse graphs. Empirically, AutoGraph achieves state-of-the-art performance across diverse synthetic and molecular graph generation benchmarks, while delivering a 100-fold generation and a 3-fold training speedup compared to leading diffusion models. Additionally, it demonstrates promising transfer capabilities and supports substructure-conditioned generation without additional fine-tuning. By extending language modeling techniques to graph generation, this work paves the way for developing graph foundation models. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†AutoGraphï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ä»…è§£ç å™¨è½¬æ¢å™¨ç”Ÿæˆå¤§å‹å±æ€§å›¾çš„æ–°å‹è‡ªå›å½’æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¯é€†çš„â€œå±•å¹³â€è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹å°†å›¾è½¬æ¢ä¸ºéšæœºåºåˆ—ã€‚é€šè¿‡ä»è¿™äº›åºåˆ—ä¸­è¿›è¡Œé‡‡æ ·å’Œå­¦ä¹ ï¼ŒAutoGraphèƒ½å¤Ÿè®©è½¬æ¢å™¨ä»¥ç±»ä¼¼äºè‡ªç„¶è¯­è¨€çš„æ–¹å¼å¯¹å¤æ‚çš„å›¾ç»“æ„è¿›è¡Œå»ºæ¨¡å’Œç”Ÿæˆã€‚ä¸ä¾èµ–è®¡ç®—å¯†é›†å‹èŠ‚ç‚¹ç‰¹å¾çš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…åœ¨è¿™äº›åºåˆ—ä¸Šè¿è¡Œã€‚é‡‡æ ·å¤æ‚æ€§å’Œåºåˆ—é•¿åº¦ä¸è¾¹çš„æ•°é‡å‘ˆçº¿æ€§å…³ç³»ï¼Œè¿™ä½¿å¾—AutoGraphåœ¨ç”Ÿæˆå¤§å‹ç¨€ç–å›¾æ—¶å…·æœ‰é«˜åº¦çš„å¯æ‰©å±•æ€§ã€‚ç»éªŒä¸Šï¼ŒAutoGraphåœ¨å¤šç§åˆæˆå’Œå›¾åˆ†å­ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸é¢†å…ˆçš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œç”Ÿæˆé€Ÿåº¦æé«˜äº†100å€ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜äº†3å€ã€‚æ­¤å¤–ï¼Œå®ƒæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„è¿ç§»èƒ½åŠ›ï¼Œå¹¶æ”¯æŒå­ç»“æ„æ¡ä»¶ä¸‹çš„ç”Ÿæˆè€Œæ— éœ€é¢å¤–çš„å¾®è°ƒã€‚é€šè¿‡å°†è¯­è¨€å»ºæ¨¡æŠ€æœ¯æ‰©å±•åˆ°å›¾å½¢ç”Ÿæˆï¼Œè¿™é¡¹å·¥ä½œä¸ºå¼€å‘å›¾å½¢åŸºç¡€æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02216v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AutoGraphæ˜¯ä¸€ç§æ–°å‹çš„åŸºäºè‡ªåŠ¨å›å½’æ¡†æ¶çš„å›¾ç”Ÿæˆæ–¹æ³•ï¼Œä½¿ç”¨ä»…è§£ç å™¨å˜æ¢å™¨å°†å›¾è½¬åŒ–ä¸ºéšæœºåºåˆ—è¿›è¡Œç”Ÿæˆã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¯é€†çš„â€œæ‰å¹³åŒ–â€è¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—æ•ˆç‡å’Œç”Ÿæˆå¤§è§„æ¨¡ç¨€ç–å›¾æ–¹é¢è¡¨ç°å‡ºé«˜å¯æ‰©å±•æ€§ã€‚AutoGraphåœ¨åˆæˆå’Œåˆ†å­å›¾ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œç›¸æ¯”é¢†å…ˆçš„æ‰©æ•£æ¨¡å‹å®ç°äº†ç™¾å€ç”Ÿæˆé€Ÿåº¦å’Œä¸‰å€è®­ç»ƒé€Ÿåº¦æå‡ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„è¿ç§»èƒ½åŠ›å’Œå­ç»“æ„æ¡ä»¶ä¸‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å·¥ä½œå°†è¯­è¨€å»ºæ¨¡æŠ€æœ¯æ‰©å±•åˆ°å›¾ç”Ÿæˆé¢†åŸŸï¼Œä¸ºå›¾åŸºç¡€æ¨¡å‹çš„å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AutoGraphæ˜¯ä¸€ç§åŸºäºè‡ªåŠ¨å›å½’æ¡†æ¶çš„å›¾ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨å¯é€†çš„â€œæ‰å¹³åŒ–â€è¿‡ç¨‹å°†å›¾è½¬åŒ–ä¸ºéšæœºåºåˆ—è¿›è¡Œç”Ÿæˆã€‚</li>
<li>é‡‡æ ·å’Œä»è¿™äº›åºåˆ—ä¸­å­¦ä¹ ï¼Œä½¿å˜æ¢å™¨èƒ½å¤Ÿç”Ÿæˆå¤æ‚çš„å›¾ç»“æ„ã€‚</li>
<li>ä¸ä¾èµ–è®¡ç®—å¯†é›†å‹èŠ‚ç‚¹ç‰¹å¾çš„æ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒAutoGraphä»…åœ¨è¿™äº›åºåˆ—ä¸Šæ“ä½œã€‚</li>
<li>é‡‡æ ·å¤æ‚åº¦å’Œåºåˆ—é•¿åº¦ä¸è¾¹çš„æ•°é‡çº¿æ€§ç›¸å…³ï¼Œä½¿å¾—AutoGraphåœ¨ç”Ÿæˆå¤§è§„æ¨¡ç¨€ç–å›¾æ—¶å…·æœ‰é«˜åº¦çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>AutoGraphåœ¨å¤šä¸ªåˆæˆå’Œåˆ†å­å›¾ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”ç›¸æ¯”å…¶ä»–é¢†å…ˆæ¨¡å‹å…·æœ‰æ˜¾è‘—çš„é€Ÿåº¦ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc16ccc200eace613a88281cd83e68df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9273d663856c74e97f11e4772d2ddd4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79cd3c75dcb1f437919da0be82971ef0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d81164705974fe7264d525883997eb05.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mass-Editing-Memory-with-Attention-in-Transformers-A-cross-lingual-exploration-of-knowledge"><a href="#Mass-Editing-Memory-with-Attention-in-Transformers-A-cross-lingual-exploration-of-knowledge" class="headerlink" title="Mass-Editing Memory with Attention in Transformers: A cross-lingual   exploration of knowledge"></a>Mass-Editing Memory with Attention in Transformers: A cross-lingual   exploration of knowledge</h2><p><strong>Authors:Daniel Tamayo, Aitor Gonzalez-Agirre, Javier Hernando, Marta Villegas</strong></p>
<p>Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at <a target="_blank" rel="noopener" href="https://github.com/dtamayo-nlp/MEMAT">https://github.com/dtamayo-nlp/MEMAT</a>. </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶æ¢è®¨äº†æ›´æ–°å’Œä¿®æ”¹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­äº‹å®çŸ¥è¯†çš„æ–¹æ³•ï¼Œé€šå¸¸èšç„¦äºç‰¹å®šçš„å¤šå±‚æ„ŸçŸ¥å™¨å—ã€‚æœ¬ç ”ç©¶åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè€ƒå¯Ÿäº†ç°æœ‰çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨ä¸åŒè¯­è¨€ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†æ³¨æ„åŠ›æœºåˆ¶åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚æ ¹æ®æ‰€è·å¾—çš„è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºTransformersçš„æ³¨æ„åŠ›å¤§è§„æ¨¡ç¼–è¾‘å†…å­˜ï¼ˆMEMATï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†é‡å¤§æ”¹è¿›ï¼ŒåŒæ—¶éœ€è¦çš„å‚æ•°ä¿®æ”¹å¾ˆå°‘ã€‚MEMATåœ¨å¹…åº¦æŒ‡æ ‡ä¸Šå®ç°äº†æƒŠäººçš„10%çš„æå‡ï¼Œå¯¹è®­ç»ƒæ•°æ®æœªåŒ…å«çš„è¯­è¨€ä¹Ÿæœ‰å¥½å¤„ï¼Œå¹¶è¡¨ç°å‡ºå¾ˆé«˜çš„å¯ç§»æ¤æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ä½äº<a target="_blank" rel="noopener" href="https://github.com/dtamayo-nlp/MEMAT%E3%80%82">https://github.com/dtamayo-nlp/MEMATã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02173v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ›´æ–°å’Œä¿®æ”¹äº‹å®çŸ¥è¯†çš„æ–¹æ³•ï¼Œé‡ç‚¹æ¢è®¨äº†è·¨è¯­è¨€çš„ç°æœ‰çŸ¥è¯†ç¼–è¾‘æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†æ³¨æ„åŠ›æœºåˆ¶åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæå‡ºäº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å˜å‹å™¨å¤§è§„æ¨¡ç¼–è¾‘å†…å­˜ï¼ˆMEMATï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒæ—¶éœ€è¦æœ€å°‘çš„å‚æ•°ä¿®æ”¹ã€‚MEMATå®ç°äº†å¹…åº¦æŒ‡æ ‡çš„æ˜¾è‘—å¢é•¿ï¼Œå¹¶æœ‰ç›ŠäºæœªåŒ…å«åœ¨è®­ç»ƒæ•°æ®ä¸­çš„è¯­è¨€ï¼Œè¿˜è¡¨ç°å‡ºé«˜åº¦çš„å¯ç§»æ¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ›´æ–°å’Œä¿®æ”¹äº‹å®çŸ¥è¯†çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¤šå±‚æ„ŸçŸ¥å™¨å—çš„ç‰¹å®šæ–¹æ³•ã€‚</li>
<li>æ¢è®¨äº†è·¨è¯­è¨€çš„ç°æœ‰çŸ¥è¯†ç¼–è¾‘æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ·±å…¥ç ”ç©¶äº†æ³¨æ„åŠ›æœºåˆ¶åœ¨çŸ¥è¯†ç¼–è¾‘è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚</li>
<li>æå‡ºäº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å˜å‹å™¨å¤§è§„æ¨¡ç¼–è¾‘å†…å­˜ï¼ˆMEMATï¼‰æ–¹æ³•ã€‚</li>
<li>MEMATåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>MEMATå®ç°äº†å¹…åº¦æŒ‡æ ‡çš„10%å¢é•¿ã€‚</li>
<li>MEMATæœ‰ç›ŠäºæœªåŒ…å«åœ¨è®­ç»ƒæ•°æ®ä¸­çš„è¯­è¨€ï¼Œè¡¨ç°å‡ºé«˜åº¦çš„å¯ç§»æ¤æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd156f87060dab520f1c947c312329a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18bfce4ef90b849e017f986b2590e4f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06c84f1cc4c7d08c5d3c9d896ae39cb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31a6c960ecbf83b856f155a7c40cddff.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="M2R2-Mixture-of-Multi-Rate-Residuals-for-Efficient-Transformer-Inference"><a href="#M2R2-Mixture-of-Multi-Rate-Residuals-for-Efficient-Transformer-Inference" class="headerlink" title="M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer   Inference"></a>M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer   Inference</h2><p><strong>Authors:Nikhil Bhendawade, Mahyar Najibi, Devang Naik, Irina Belousova</strong></p>
<p>Residual transformations enhance the representational depth and expressive power of large language models (LLMs). However, applying static residual transformations across all tokens in auto-regressive generation leads to a suboptimal trade-off between inference efficiency and generation fidelity. Existing methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth address this by modulating the residual transformation based on token-level complexity. Nevertheless, these approaches predominantly consider the distance traversed by tokens through the model layers, neglecting the underlying velocity of residual evolution. We introduce Mixture of Multi-rate Residuals (M2R2), a framework that dynamically modulates residual velocity to improve early alignment, enhancing inference efficiency. Evaluations on reasoning oriented tasks such as Koala, Self-Instruct, WizardLM, and MT-Bench show M2R2 surpasses state-of-the-art distance-based strategies, balancing generation quality and speedup. In self-speculative decoding setup, M2R2 achieves up to 2.8x speedups on MT-Bench, outperforming methods like 2-model speculative decoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE) architectures, integrating early residual alignment with ahead-of-time expert loading into high-bandwidth memory (HBM) accelerates decoding, reduces expert-switching bottlenecks, and achieves a 2.9x speedup, making it highly effective in resource-constrained environments. </p>
<blockquote>
<p>æ®‹å·®è½¬æ¢å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¡¨ç¤ºæ·±åº¦å’Œè¡¨è¾¾èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œå¯¹æ‰€æœ‰ä»¤ç‰Œåº”ç”¨é™æ€æ®‹å·®è½¬æ¢ä¼šå¯¼è‡´æ¨ç†æ•ˆç‡å’Œç”Ÿæˆä¿çœŸåº¦ä¹‹é—´çš„æ¬¡ä¼˜æƒè¡¡ã€‚ç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬æ—©æœŸé€€å‡ºã€è·³è¿‡è§£ç å’Œæ·±åº¦æ··åˆç­‰ï¼Œé€šè¿‡æ ¹æ®ä»¤ç‰Œçº§å¤æ‚åº¦è°ƒåˆ¶æ®‹å·®è½¬æ¢æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦è€ƒè™‘ä»¤ç‰Œé€šè¿‡æ¨¡å‹å±‚æ‰€ç©¿è¶Šçš„è·ç¦»ï¼Œè€Œå¿½ç•¥äº†æ®‹å·®æ¼”åŒ–çš„å†…åœ¨é€Ÿåº¦ã€‚æˆ‘ä»¬å¼•å…¥äº†å¤šé€Ÿç‡æ®‹å·®æ··åˆï¼ˆM2R2ï¼‰æ¡†æ¶ï¼Œå®ƒåŠ¨æ€è°ƒåˆ¶æ®‹å·®é€Ÿåº¦ï¼Œä»¥æ”¹è¿›æ—©æœŸå¯¹é½ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚åœ¨é¢å‘æ¨ç†çš„ä»»åŠ¡ï¼ˆå¦‚Koalaã€Self-Instructã€WizardLMå’ŒMT-Benchï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒM2R2è¶…è¶Šäº†åŸºäºè·ç¦»çš„æœ€æ–°ç­–ç•¥ï¼Œå¹³è¡¡äº†ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ã€‚åœ¨è‡ªæˆ‘æ¨æµ‹è§£ç è®¾ç½®ä¸­ï¼ŒM2R2åœ¨MT-Benchä¸Šå®ç°äº†é«˜è¾¾2.8å€çš„åŠ é€Ÿï¼Œä¼˜äºè¯¸å¦‚åŒæ¨¡å‹æ¨æµ‹è§£ç ã€Medusaã€è¶…å‰è§£ç å’ŒDEEDç­‰æ–¹æ³•ã€‚åœ¨ä¸“ä¸šæ··åˆï¼ˆMoEï¼‰æ¶æ„ä¸­ï¼Œå°†æ—©æœŸæ®‹å·®å¯¹é½ä¸æå‰çš„ä¸“å®¶åŠ è½½åˆ°é«˜é€Ÿç¼“å†²å­˜å‚¨å™¨ï¼ˆHBMï¼‰ç›¸ç»“åˆï¼Œå¯ä»¥åŠ é€Ÿè§£ç ï¼Œå‡å°‘ä¸“å®¶åˆ‡æ¢ç“¶é¢ˆï¼Œå®ç°2.9å€çš„åŠ é€Ÿï¼Œä½¿å…¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02040v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ®‹å·®è½¬æ¢èƒ½å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„è¡¨è¾¾æ·±åº¦å’Œèƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œå¯¹æ‰€æœ‰tokenåº”ç”¨é™æ€æ®‹å·®è½¬æ¢ä¼šåœ¨æ¨ç†æ•ˆç‡å’Œç”Ÿæˆä¿çœŸåº¦ä¹‹é—´äº§ç”Ÿæƒè¡¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é€šè¿‡åŸºäºtokençº§åˆ«å¤æ‚åº¦è°ƒæ•´æ®‹å·®è½¬æ¢æ¥å®ç°ä¼˜åŒ–ã€‚æœ¬æ–‡æå‡ºMixture of Multi-rate Residualsï¼ˆM2R2ï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŠ¨æ€è°ƒæ•´æ®‹å·®é€Ÿåº¦æ¥æ”¹è¿›æ—©æœŸå¯¹é½ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚åœ¨é¢å‘ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼Œå¦‚Koalaã€Self-Instructã€WizardLMå’ŒMT-Benchç­‰ä»»åŠ¡ä¸Šï¼ŒM2R2è¶…è¶Šäº†åŸºäºè·ç¦»çš„ç­–ç•¥ï¼Œå¹³è¡¡äº†ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ã€‚åœ¨è‡ªæˆ‘æ¢ç´¢è§£ç è®¾ç½®ä¸­ï¼ŒM2R2åœ¨MT-Benchä¸Šå®ç°äº†é«˜è¾¾2.8å€çš„åŠ é€Ÿï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚åœ¨æ··åˆä¸“å®¶æ¶æ„ä¸­ï¼Œå°†æ—©æœŸæ®‹å·®å¯¹é½ä¸é¢„å…ˆåŠ è½½ä¸“å®¶åˆ°é«˜é€Ÿç¼“å­˜å­˜å‚¨å™¨ç›¸ç»“åˆï¼Œå¯ä»¥åŠ é€Ÿè§£ç ï¼Œå‡å°‘ä¸“å®¶åˆ‡æ¢ç“¶é¢ˆï¼Œå®ç°2.9å€çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ®‹å·®è½¬æ¢å¯¹äºæé«˜å¤§è¯­è¨€æ¨¡å‹çš„è¡¨è¾¾æ·±åº¦å’Œèƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>å¯¹æ‰€æœ‰tokenåº”ç”¨é™æ€æ®‹å·®è½¬æ¢ä¼šå¯¼è‡´æ¨ç†æ•ˆç‡å’Œç”Ÿæˆä¿çœŸåº¦ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡åŸºäºtokençº§åˆ«å¤æ‚åº¦è°ƒæ•´æ®‹å·®è½¬æ¢è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>M2R2æ¡†æ¶é€šè¿‡åŠ¨æ€è°ƒæ•´æ®‹å·®é€Ÿåº¦æ”¹è¿›æ—©æœŸå¯¹é½ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>M2R2åœ¨å¤šç§é¢å‘ä»»åŠ¡çš„è¯„ä¼°ä¸­è¶…è¶ŠåŸºäºè·ç¦»çš„ç­–ç•¥ï¼Œå¹³è¡¡ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ã€‚</li>
<li>åœ¨è‡ªæˆ‘æ¢ç´¢è§£ç è®¾ç½®ä¸­ï¼ŒM2R2å®ç°æ˜¾è‘—åŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e69dc3ae406b928a417f7d88e5d3e0dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d1950b66a7a58e0b2dea5b1a0417f27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebcc52f45219956fe0c58400fd65cfd5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a297493906d70b968605ec6f123a6bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6565d95ae655a6313dbfbe7ea09b7ff.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BARE-Combining-Base-and-Instruction-Tuned-Language-Models-for-Better-Synthetic-Data-Generation"><a href="#BARE-Combining-Base-and-Instruction-Tuned-Language-Models-for-Better-Synthetic-Data-Generation" class="headerlink" title="BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation"></a>BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation</h2><p><strong>Authors:Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia</strong></p>
<p>As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs-a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT. </p>
<blockquote>
<p>éšç€æ¨¡å‹è®­ç»ƒä¸­å¯¹äºé«˜è´¨é‡æ•°æ®éœ€æ±‚çš„å¢é•¿ï¼Œç ”ç©¶è€…å’Œå¼€å‘è€…æ­£åœ¨è¶Šæ¥è¶Šå¤šåœ°ç”Ÿæˆåˆæˆæ•°æ®ä»¥è°ƒæ•´å’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å…³äºåˆæˆæ•°æ®çš„å¸¸è§å‡è®¾æ˜¯ï¼Œä»ç»è¿‡æŒ‡ä»¤è°ƒæ•´ï¼ˆinstruct-tunedï¼‰çš„æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·å°±è¶³å¤Ÿäº†ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå¤šæ ·åŒ–è¾“å‡ºæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™æ˜¯å®ç°æ³›åŒ–çš„å…³é”®è¦æ±‚ã€‚å°½ç®¡æœ‰å„ç§æç¤ºæ–¹æ³•ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­æˆ‘ä»¬è¡¨æ˜ï¼Œä»ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„æ¨¡å‹ä¸­å®ç°æœ‰æ„ä¹‰çš„å¤šæ ·æ€§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å‘ç°æ²¡æœ‰ç»è¿‡åè®­ç»ƒçš„åŸºå‡†æ¨¡å‹è¡¨ç°å‡ºæ›´å¤§çš„å¤šæ ·æ€§ï¼Œä½†åœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›è¾ƒå¼±ï¼Œå› æ­¤è´¨é‡è¾ƒä½ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Base-Refineï¼ˆBAREï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ç»“åˆåŸºå‡†æ¨¡å‹çš„å¤šæ ·æ€§å’Œç»è¿‡æŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„è´¨é‡ã€‚é€šè¿‡å°‘é‡ç¤ºä¾‹å’Œç­›é€‰ï¼ŒBAREèƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œä½¿ç”¨BAREç”Ÿæˆçš„æ ·æœ¬ä»…éœ€è¿›è¡Œå¾®è°ƒè‡³å¤š1000ä¸ªæ ·æœ¬å³å¯è¾¾åˆ°LiveCodeBenchä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³çš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œä½¿ç”¨BAREç”Ÿæˆçš„æ•°æ®è¿›è¡Œå¾®è°ƒåœ¨GSM8Kæ•°æ®é›†ä¸Šçš„è¡¨ç°æ¯”ä»…ä½¿ç”¨æŒ‡ä»¤æ•°æ®æé«˜äº†101%ï¼Œåœ¨RAFTæ•°æ®é›†ä¸Šçš„è¡¨ç°æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†18.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01697v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†éšç€æ¨¡å‹è®­ç»ƒä¸­å¯¹é«˜è´¨é‡æ•°æ®éœ€æ±‚çš„å¢é•¿ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜å¦‚ä½•åˆ©ç”¨åˆæˆæ•°æ®æ¥è°ƒæ•´å’Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡é‡‡ç”¨å„ç§æç¤ºæ–¹æ³•ï¼ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆæ•°æ®æ—¶ä»éš¾ä»¥å®ç°æœ‰æ„ä¹‰çš„å¤šæ ·æ€§ã€‚ç›¸åï¼ŒåŸºç¡€æ¨¡å‹å±•ç°å‡ºæ›´å¤§çš„å¤šæ ·æ€§ï¼Œä½†åœ¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›æ–¹é¢è¾ƒå·®ã€‚åŸºäºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºBase-Refineï¼ˆBAREï¼‰çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ç»“åˆåŸºç¡€æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„è´¨é‡ã€‚åˆ©ç”¨å°‘é‡çš„ç¤ºä¾‹å’Œç­›é€‰ï¼ŒBAREèƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨BAREç”Ÿæˆçš„æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œåœ¨LiveCodeBenchä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯ä¸æœ€ä½³ç›¸ä¼¼è§„æ¨¡æ¨¡å‹ç›¸å½“ï¼Œå¹¶ä¸”åœ¨GSM8Kä¸Šç›¸å¯¹äºä»…ä½¿ç”¨æŒ‡ä»¤æ•°æ®å®ç°äº†101%çš„æ”¹è¿›ï¼Œåœ¨RAFTä¸Šç›¸å¯¹äºæœ€æ–°æ–¹æ³•å®ç°äº†18.4%çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨åˆæˆæ•°æ®åœ¨è®­ç»ƒLLMä¸­çš„ä½œç”¨ï¼Œå‘ç°æŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨ç”Ÿæˆå¤šæ ·è¾“å‡ºæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>åŸºç¡€æ¨¡å‹å±•ç°å‡ºæ›´å¤§çš„å¤šæ ·æ€§ï¼Œä½†åœ¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›æ–¹é¢è¾ƒå·®ã€‚</li>
<li>æå‡ºäº†Base-Refineï¼ˆBAREï¼‰æ–¹æ³•ï¼Œç»“åˆåŸºç¡€æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„è´¨é‡ã€‚</li>
<li>BAREé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨BAREç”Ÿæˆçš„æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæ€§èƒ½å¯ä¸æœ€ä½³æ¨¡å‹ç›¸å½“ã€‚</li>
<li>BAREåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œç›¸å¯¹äºä»…ä½¿ç”¨æŒ‡ä»¤æ•°æ®åœ¨GSM8Kä¸Šå®ç°äº†101%çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0281da528061c674534a3eec671a51a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2aedf490ff7ee0cbb3ff7b1063b933f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ecc96ecda174238e70bc680753559e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81d93849a1a7b4bb94a319e97c5973c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a2a1fce24198ceb9f7f7f2a168d9850.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Longer-Attention-Span-Increasing-Transformer-Context-Length-with-Sparse-Graph-Processing-Techniques"><a href="#Longer-Attention-Span-Increasing-Transformer-Context-Length-with-Sparse-Graph-Processing-Techniques" class="headerlink" title="Longer Attention Span: Increasing Transformer Context Length with Sparse   Graph Processing Techniques"></a>Longer Attention Span: Increasing Transformer Context Length with Sparse   Graph Processing Techniques</h2><p><strong>Authors:Nathaniel Tomczak, Sanmukh Kuppannagari</strong></p>
<p>Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the inputâ€™s context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve â€œtrue sparsityâ€ are lacking.   In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB). </p>
<blockquote>
<p>Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿç‰©ä¿¡æ¯å­¦ç­‰å¤šä¸ªé¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚è¿™ä¸€æˆåŠŸæºäºè¿™äº›æ¨¡å‹é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥è¡¨ç¤ºå¹¶ä¼ æ’­åºåˆ—æ•°æ®ä¸­å„ä¸ªæ ‡è®°ä¹‹é—´çš„é…å¯¹äº¤äº’ä½œç”¨ã€‚ç„¶è€Œï¼Œè¿™é¡¹æ“ä½œçš„ä¸»è¦å±€é™æ€§åœ¨äºå…¶ç›¸å¯¹äºè¾“å…¥ä¸Šä¸‹æ–‡é•¿åº¦çš„äºŒæ¬¡å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦â€”â€”éœ€è¦æ•è·äº¤äº’ä½œç”¨çš„åºåˆ—é•¿åº¦ã€‚è¿™æå¤§åœ°é™åˆ¶äº†è¿™äº›æ¨¡å‹å¯ä»¥æ¨æ–­çš„åºåˆ—é•¿åº¦ã€‚ä¸ºäº†å°†é…å¯¹äº¤äº’çš„æ•°é‡å‡å°‘åˆ°ç›¸å¯¹äºä¸Šä¸‹æ–‡é•¿åº¦çš„æ¬¡äºŒæ¬¡ï¼Œå·²ç»è¿›è¡Œäº†å¤§é‡ç ”ç©¶ï¼Œé€šè¿‡åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­å¼•å…¥ç¨€ç–æ€§æ¥å¼€å‘ç¨€ç–æ³¨æ„åŠ›æ©ç ã€‚ç„¶è€Œï¼Œå®ç°â€œçœŸæ­£ç¨€ç–æ€§â€çš„é«˜æ•ˆå®ç°ä»ç„¶ç¼ºä¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01659v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformeræ¨¡å‹åœ¨åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œåºåˆ—æ•°æ®çš„æ ‡è®°é—´ç›¸äº’ä½œç”¨è¡¨ç¤ºå’Œä¼ æ’­æ–¹é¢çš„æˆåŠŸåº”ç”¨ï¼Œä½†é¢ä¸´è¾“å…¥ä¸Šä¸‹æ–‡é•¿åº¦å¼•èµ·çš„äºŒæ¬¡å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦é™åˆ¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŸºäºå›¾çš„æ³¨æ„åŠ›è®¡ç®—è§‚ç‚¹ï¼Œé€šè¿‡å›¾å½¢å¤„ç†ç®—æ³•å®ç°æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»…æ‰§è¡Œå¿…éœ€çš„è®¡ç®—ä»¥å®ç°é«˜æ•ˆå·¥ä½œã€‚åŒæ—¶è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œä¸æœ€æ–°æ³¨æ„åŠ›å®ç°æ–¹æ³•ç›¸æ¯”ï¼Œå¤§å¹…æé«˜äº†æ‰§è¡Œé€Ÿåº¦ï¼Œå¹¶åœ¨å•ä¸ªNVIDIA A100 GPUä¸Šå®ç°äº†é«˜è¾¾1äº¿å…­åƒä¸‡çš„æé•¿åºåˆ—é•¿åº¦ã€‚æ­¤æŠ€æœ¯ä¸ºå¤„ç†é•¿åºåˆ—æ•°æ®æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹çš„æˆåŠŸå¾—ç›Šäºå…¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰åºåˆ—æ•°æ®çš„æ ‡è®°é—´ç›¸äº’ä½œç”¨çš„èƒ½åŠ›ã€‚</li>
<li>Transformeræ¨¡å‹é¢ä¸´çš„ä¸»è¦é™åˆ¶æ˜¯äºŒæ¬¡å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦ï¼Œé™åˆ¶äº†å¯ä»¥æ¨æ–­çš„åºåˆ—é•¿åº¦ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥ç¨€ç–æ³¨æ„åŠ›æ©ç ä»¥å‡å°‘æˆå¯¹äº¤äº’çš„æ•°é‡ã€‚ç„¶è€Œï¼Œå®ç°çœŸæ­£çš„ç¨€ç–æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æ³¨æ„åŠ›è®¡ç®—è§‚ç‚¹ï¼Œå°†æ ‡è®°è§†ä¸ºå›¾çš„èŠ‚ç‚¹ï¼Œæ³¨æ„åŠ›æ©ç ç¡®å®šå›¾çš„è¾¹ç¼˜ã€‚</li>
<li>åˆ©ç”¨å›¾å½¢å¤„ç†ç®—æ³•å®ç°æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»…æ‰§è¡Œå¿…éœ€çš„è®¡ç®—ä»¥å®ç°é«˜æ•ˆå·¥ä½œã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æŠ€æœ¯æ˜¾è‘—æé«˜äº†æ‰§è¡Œé€Ÿåº¦ï¼Œå¹¶åœ¨å•ä¸ªGPUä¸Šå®ç°äº†æé•¿çš„åºåˆ—é•¿åº¦å¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-66e0b2a3c1e97f3bff9abb727fb3c3bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-974f422fbac37f20b4a9f0e696ea183b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4fd3e28bce1a9a9608a254ef91291da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd52538dbf8f8420c2ca6e277e6fa44d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e77d89af2861289a5442841628f9a410.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-967394e4f57a94c4711387061765e038.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80fa2dd79c6e7116485d5360d1a61723.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Probabilistic-Inference-Approach-to-Inference-Time-Scaling-of-LLMs-using-Particle-Based-Monte-Carlo-Methods"><a href="#A-Probabilistic-Inference-Approach-to-Inference-Time-Scaling-of-LLMs-using-Particle-Based-Monte-Carlo-Methods" class="headerlink" title="A Probabilistic Inference Approach to Inference-Time Scaling of LLMs   using Particle-Based Monte Carlo Methods"></a>A Probabilistic Inference Approach to Inference-Time Scaling of LLMs   using Particle-Based Monte Carlo Methods</h2><p><strong>Authors:Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, Akash Srivastava</strong></p>
<p>Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and&#x2F;or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code and further information is available at <a target="_blank" rel="noopener" href="https://probabilistic-inference-scaling.github.io/">https://probabilistic-inference-scaling.github.io</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„è¯æ®è¡¨æ˜ï¼Œä»è¿™äº›æ–¹æ³•ä¸­è·å¾—çš„æ”¶ç›Šæ­£åœ¨å‡å°‘ï¼Œå› æ­¤æ¿€å‘äº†åœ¨æ¨ç†æ—¶é—´æ¶ˆè€—çš„è®¡ç®—ä¸Šè¿›è¡Œæ‰©å±•çš„åŠ¨æœºã€‚ç°æœ‰çš„æ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•é€šå¸¸ä½¿ç”¨å¥–åŠ±æ¨¡å‹ï¼Œå°†ä»»åŠ¡è½¬åŒ–ä¸ºæœç´¢é—®é¢˜ï¼Œè¿™å¾€å¾€å®¹æ˜“å—åˆ°å¥–åŠ±æ¨¡å‹çš„è¿‘ä¼¼è¯¯å·®å¯¼è‡´çš„å¥–åŠ±ç ´è§£çš„å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¨ç†æ—¶é—´æ‰©å±•è½¬å˜ä¸ºæ¦‚ç‡æ¨ç†ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨é‡‡æ ·æŠ€æœ¯æ¢ç´¢çŠ¶æ€ç©ºé—´æ¨¡å‹çš„å…¸å‹çŠ¶æ€åˆ†å¸ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œå³é€šè¿‡åŸºäºç²’å­çš„è’™ç‰¹å¡æ´›æ–¹æ³•é€‚åº”è¿™ä¸€ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•çš„æ‰©å±•ç‡æ˜¯ç¡®å®šæ€§æœç´¢æ–¹æ³•çš„4-16å€ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è¯æ˜äº†Qwen2.5-Math-1.5B-Instructåœ¨ä»…4æ¬¡è¿­ä»£ä¸­å°±èƒ½è¶…è¶ŠGPT-4oçš„å‡†ç¡®æ€§ï¼Œè€ŒQwen2.5-Math-7B-Instructåœ¨ä»…32æ¬¡è¿­ä»£ä¸­å°±èƒ½è¾¾åˆ°o1çº§åˆ«çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ä»…æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œè¿˜å°†æ¦‚ç‡æ¨ç†çš„ä¸°å¯Œæ–‡çŒ®ä¸LLMçš„æ¨ç†æ—¶é—´æ‰©å±•ç›¸ç»“åˆï¼Œä¸ºæœªæ¥å·¥ä½œå¼€å‘æ›´ç¨³å¥çš„ç®—æ³•æä¾›äº†åŸºç¡€ã€‚ç›¸å…³ä»£ç å’Œæ›´å¤šä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://probabilistic-inference-scaling.github.io./">https://probabilistic-inference-scaling.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01618v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡æˆ–æ•°æ®é‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æ˜“äº§ç”Ÿè¾¹é™…æ•ˆç›Šé€’å‡ç°è±¡ï¼Œä¸ºæ­¤æå‡ºäº†é€šè¿‡è®¡ç®—èµ„æºè¿›è¡Œæ¨æ–­æ—¶é—´çš„ç¼©æ”¾ä½œä¸ºè§£å†³æ€è·¯ã€‚ç°æœ‰çš„æ¨æ–­æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œå€¾å‘äºæŠŠä»»åŠ¡ä½œä¸ºæœç´¢é—®é¢˜å¹¶åˆ©ç”¨å¥–åŠ±æ¨¡å‹æ¥å¤„ç†ï¼Œä½†ç”±äºå¥–åŠ±æ¨¡å‹çš„è¿‘ä¼¼è¯¯å·®å®¹æ˜“é­å—å¥–åŠ±ä½œå¼Šçš„å½±å“ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¦‚ç‡æ¨æ–­çš„æ¨æ–­æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œåˆ©ç”¨é‡‡æ ·æŠ€æœ¯æ¢ç´¢çŠ¶æ€ç©ºé—´æ¨¡å‹çš„å…¸å‹çŠ¶æ€åˆ†å¸ƒè¿‘ä¼¼æ¦‚ç‡ï¼Œè€Œéç›´æ¥ä¼˜åŒ–å…¶æ¨¡æ€ã€‚é€šè¿‡ç²’å­è’™ç‰¹å¡æ´›æ–¹æ³•çš„æ”¹è¿›é€‚ç”¨äºè¯¥ä»»åŠ¡ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å¤šç§æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†4-16å€çš„ç¼©æ”¾ç‡æå‡ã€‚ä½¿ç”¨æ­¤æ–¹æ³•ï¼ŒQwen2.5-Math-1.5B-Instruct ä»…éœ€4æ¬¡è¿­ä»£å³å¯è¶…è¶ŠGPT-4oçš„å‡†ç¡®åº¦ï¼Œè€ŒQwen2.5-Math-7B-Instructåœ¨ä»…32æ¬¡è¿­ä»£æ—¶å³å¯è¾¾åˆ°O1çº§åˆ«ç²¾åº¦ã€‚æœ¬æ–‡ä¸ä»…æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ¨æ–­æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œè¿˜å°†æ¦‚ç‡æ¨æ–­ä¸LLMçš„æ¨æ–­æ—¶é—´ç¼©æ”¾è”ç³»èµ·æ¥ï¼Œä¸ºæœªæ¥å¼€å‘æ›´ç¨³å¥çš„ç®—æ³•æ‰“ä¸‹åŸºç¡€ã€‚æ›´å¤šä¿¡æ¯åŠä»£ç è¯·å‚è§ï¼š<a target="_blank" rel="noopener" href="https://probabilistic-inference-scaling.github.io/">https://probabilistic-inference-scaling.github.io</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´æ€§èƒ½æå‡è¾¹é™…æ•ˆç›Šé€’å‡é—®é¢˜ï¼Œéœ€è¦å¯»æ‰¾æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰æ¨æ–­æ—¶é—´ç¼©æ”¾æ–¹æ³•æ˜“é­å—å¥–åŠ±ä½œå¼Šå½±å“ï¼Œå€¾å‘äºæŠŠä»»åŠ¡ä½œä¸ºæœç´¢é—®é¢˜å¤„ç†ã€‚</li>
<li>æœ¬æ–‡æå‡ºåŸºäºæ¦‚ç‡æ¨æ–­çš„æ¨æ–­æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œæ¢ç´¢çŠ¶æ€ç©ºé—´æ¨¡å‹çš„å…¸å‹çŠ¶æ€åˆ†å¸ƒã€‚</li>
<li>é‡‡ç”¨ç²’å­è’™ç‰¹å¡æ´›æ–¹æ³•çš„æ”¹è¿›å®ç°ï¼Œåœ¨å¤šç§æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>Qwen2.5ç³»åˆ—æ¨¡å‹åœ¨ä»…æ•°æ¬¡è¿­ä»£åä¾¿èƒ½è¾¾åˆ°è¾ƒé«˜çš„å‡†ç¡®æ€§æ°´å¹³ã€‚</li>
<li>æœ¬æ–‡ä¸ä»…æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ¨æ–­æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œä¹Ÿä¸ºæœªæ¥å¼€å‘æ›´ç¨³å¥çš„ç®—æ³•æä¾›äº†è”ç³»æ¦‚ç‡æ¨æ–­ä¸LLMçš„æ¨æ–­æ—¶é—´ç¼©æ”¾çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4469260c4c7acde26a0baf093d8bec5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92d325cfb140bc935078b8388fc7f3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9ff6cf4c7e0ecd9490d6eb9fd95d367.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c55419ab83ae46e8bf56f5a07927aca6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-for-Long-Horizon-Interactive-LLM-Agents"><a href="#Reinforcement-Learning-for-Long-Horizon-Interactive-LLM-Agents" class="headerlink" title="Reinforcement Learning for Long-Horizon Interactive LLM Agents"></a>Reinforcement Learning for Long-Horizon Interactive LLM Agents</h2><p><strong>Authors:Kevin Chen, Marco Cusumano-Towner, Brody Huval, Aleksei Petrenko, Jackson Hamburger, Vladlen Koltun, Philipp KrÃ¤henbÃ¼hl</strong></p>
<p>Interactive digital agents (IDAs) leverage APIs of stateful digital environments to perform tasks in response to user requests. While IDAs powered by instruction-tuned large language models (LLMs) can react to feedback from interface invocations in multi-step exchanges, they have not been trained in their respective digital environments. Prior methods accomplish less than half of tasks in sophisticated benchmarks such as AppWorld. We present a reinforcement learning (RL) approach that trains IDAs directly in their target environments. We formalize this training as a partially observable Markov decision process and derive LOOP, a data- and memory-efficient variant of proximal policy optimization. LOOP uses no value network and maintains exactly one copy of the underlying LLM in memory, making its implementation straightforward and as memory-efficient as fine-tuning a single LLM. A 32-billion-parameter agent trained with LOOP in the AppWorld environment outperforms the much larger OpenAI o1 agent by 9 percentage points (15% relative). To our knowledge, this is the first reported application of RL to IDAs that interact with a stateful, multi-domain, multi-app environment via direct API calls. Our analysis sheds light on the effectiveness of RL in this area, showing that the agent learns to consult the API documentation, avoid unwarranted assumptions, minimize confabulation, and recover from setbacks. </p>
<blockquote>
<p>äº¤äº’å¼æ•°å­—ä»£ç†ï¼ˆIDAsï¼‰åˆ©ç”¨æœ‰çŠ¶æ€æ•°å­—ç¯å¢ƒçš„APIæ¥æ‰§è¡Œç”¨æˆ·è¯·æ±‚çš„ä»»åŠ¡ã€‚è™½ç„¶ç”±æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„IDAså¯ä»¥å“åº”ç•Œé¢è°ƒç”¨ä¸­çš„åé¦ˆè¿›è¡Œå¤šæ­¥éª¤äº¤äº’ï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰åœ¨å…¶å„è‡ªçš„æ•°å­—ç¯å¢ƒä¸­æ¥å—è¿‡è®­ç»ƒã€‚ä¹‹å‰çš„æ–¹æ³•åœ¨å®ŒæˆåƒAppWorldè¿™æ ·çš„å¤æ‚åŸºå‡†æµ‹è¯•ä»»åŠ¡æ—¶ï¼ŒæˆåŠŸç‡ä¸åˆ°ä¸€åŠã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥åœ¨ç›®æ ‡ç¯å¢ƒä¸­è®­ç»ƒIDAsã€‚æˆ‘ä»¬å°†è¿™ç§è®­ç»ƒå½¢å¼åŒ–ä¸ºéƒ¨åˆ†å¯è§‚æµ‹çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶æ¨å¯¼å‡ºLOOPï¼Œè¿™æ˜¯è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–çš„ä¸€ç§æ•°æ®å’Œå†…å­˜æ•ˆç‡é«˜çš„å˜ä½“ã€‚LOOPä¸ä½¿ç”¨ä»·å€¼ç½‘ç»œï¼Œå¹¶ä¸”åœ¨å†…å­˜ä¸­åªä¿ç•™ä¸€ä¸ªåº•å±‚LLMçš„å‰¯æœ¬ï¼Œä½¿å…¶å®ç°ç®€å•ä¸”å†…å­˜ä½¿ç”¨æ•ˆç‡ä¸å¾®è°ƒå•ä¸ªLLMç›¸å½“ã€‚åœ¨AppWorldç¯å¢ƒä¸­ä½¿ç”¨LOOPè®­ç»ƒçš„32äº¿å‚æ•°ä»£ç†æ¯”OpenAI o1ä»£ç†é«˜å‡º9ä¸ªç™¾åˆ†ç‚¹ï¼ˆç›¸å¯¹æé«˜15%ï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡æŠ¥å‘Šå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºIDAsï¼Œè¿™äº›ä»£ç†é€šè¿‡ç›´æ¥çš„APIè°ƒç”¨ä¸æœ‰çŠ¶æ€ã€å¤šé¢†åŸŸã€å¤šåº”ç”¨ç¨‹åºç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨è¿™ä¸ªé¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºä»£ç†å­¦ä¼šäº†å‚è€ƒAPIæ–‡æ¡£ï¼Œé¿å…ä¸å¿…è¦çš„å‡è®¾ï¼Œå°½é‡å‡å°‘è™šæ„ï¼Œå¹¶ä»æŒ«æŠ˜ä¸­æ¢å¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01600v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºäº¤äº’å¼æ•°å­—ä»£ç†ï¼ˆIDAï¼‰åˆ©ç”¨çŠ¶æ€åŒ–æ•°å­—ç¯å¢ƒçš„APIæ‰§è¡Œä»»åŠ¡ä»¥å“åº”ç”¨æˆ·è¯·æ±‚çš„ç‰¹ç‚¹ï¼Œæå‡ºä¸€ç§åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒIDAçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†æ•°æ®å†…å­˜é«˜æ•ˆä¸”ç²¾å‡†åœ°å®šä¹‰ä¸ºéƒ¨åˆ†è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ï¼Œå¹¶æå‡ºLOOPç®—æ³•ï¼Œè¯¥ç®—æ³•æ˜¯ä¸€ç§åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–çš„å˜ä½“ã€‚LOOPæ— éœ€ä»·å€¼ç½‘ç»œï¼Œä»…ç»´æŠ¤ä¸€ä¸ªåº•å±‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‰¯æœ¬ï¼Œä½¿å…¶å®ç°ç®€å•ä¸”å†…å­˜é«˜æ•ˆã€‚åœ¨AppWorldç¯å¢ƒä¸­è®­ç»ƒçš„åŒ…å«32äº¿å‚æ•°çš„ä»£ç†æ€§èƒ½è¶…è¿‡äº†OpenAI o1ä»£ç†ï¼Œç›¸å¯¹æé«˜äº†çº¦9ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™æ˜¯é¦–æ¬¡å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºäº¤äº’å¼æ•°å­—ä»£ç†é¢†åŸŸï¼Œè¯¥ä»£ç†é€šè¿‡ç›´æ¥APIè°ƒç”¨ä¸çŠ¶æ€åŒ–ã€å¤šé¢†åŸŸã€å¤šåº”ç”¨ç¨‹åºç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚æœ¬ç ”ç©¶åˆ†ææ­ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨è¯¥é¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºä»£ç†å­¦ä¼šäº†æŸ¥é˜…APIæ–‡æ¡£ã€é¿å…æ— æ•ˆå‡è®¾ã€å‡å°‘æ··æ·†å¹¶ä»æŒ«æŠ˜ä¸­æ¢å¤çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IDAsåˆ©ç”¨APIæ‰§è¡Œç”¨æˆ·è¯·æ±‚çš„ä»»åŠ¡ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨æ‰§è¡Œå¤æ‚ä»»åŠ¡æ—¶çš„æˆåŠŸç‡ä½äºä¸€åŠã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒIDAçš„æ–¹æ³•ï¼Œç›´æ¥åœ¨ç›®æ ‡ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚</li>
<li>LOOPç®—æ³•æ˜¯ä¸€ç§æ•°æ®å†…å­˜é«˜æ•ˆçš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–å˜ä½“ã€‚</li>
<li>LOOPæ— éœ€ä»·å€¼ç½‘ç»œï¼Œåªç»´æŠ¤ä¸€ä¸ªLLMå‰¯æœ¬ï¼Œå®ç°ç®€å•ä¸”å†…å­˜é«˜æ•ˆã€‚</li>
<li>åœ¨AppWorldç¯å¢ƒä¸­è®­ç»ƒçš„ä»£ç†æ€§èƒ½è¶…è¿‡OpenAI o1ä»£ç†çº¦9ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2d74ed148876ebd8d5cdf7898bafc004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee14e63dda5bfb2f34608be0f157415a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-851ebeb82e6d77658d36b5c970d64faf.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="On-the-Robustness-of-Temporal-Factual-Knowledge-in-Language-Models"><a href="#On-the-Robustness-of-Temporal-Factual-Knowledge-in-Language-Models" class="headerlink" title="On the Robustness of Temporal Factual Knowledge in Language Models"></a>On the Robustness of Temporal Factual Knowledge in Language Models</h2><p><strong>Authors:Hichem Ammar Khodja, FrÃ©dÃ©ric BÃ©chet, Quentin Brabant, Alexis Nasr, GwÃ©nolÃ© LecorvÃ©</strong></p>
<p>This paper explores the temporal robustness of language models (LMs) in handling factual knowledge. While LMs can often complete simple factual statements, their ability to manage temporal facts (those valid only within specific timeframes) remains uncertain. We design a controlled experiment to test the robustness of temporal factual knowledge inside LMs, which we use to evaluate several pretrained and instruction-tuned models using prompts on popular Wikidata facts, assessing their performance across different temporal granularities (Day, Month, and Year). Our findings indicate that even very large state-of-the-art models, such as Llama-3.1-70B, vastly lack robust knowledge of temporal facts. In addition, they are incapable of generalizing their knowledge from one granularity to another. These results highlight the inherent limitations of using LMs as temporal knowledge bases. The source code and data to reproduce our experiments will be released. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨å¤„ç†äº‹å®çŸ¥è¯†æ—¶çš„æ—¶åºç¨³å¥æ€§ã€‚è™½ç„¶è¯­è¨€æ¨¡å‹é€šå¸¸å¯ä»¥å®Œæˆç®€å•çš„é™ˆè¿°ï¼Œä½†å®ƒä»¬å¤„ç†æ—¶åºäº‹å®ï¼ˆä»…åœ¨ç‰¹å®šæ—¶é—´èŒƒå›´å†…æœ‰æ•ˆçš„äº‹å®ï¼‰çš„èƒ½åŠ›ä»ç„¶ä¸ç¡®å®šã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå—æ§å®éªŒæ¥æµ‹è¯•è¯­è¨€æ¨¡å‹å†…éƒ¨æ—¶åºäº‹å®çŸ¥è¯†çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨æ­¤å®éªŒæ¥è¯„ä¼°ä½¿ç”¨æµè¡ŒWikidataäº‹å®æç¤ºçš„å¤šä¸ªé¢„è®­ç»ƒå’ŒæŒ‡å¯¼æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨ä¸åŒçš„æ—¶é—´ç²’åº¦ï¼ˆæ—¥ã€æœˆå’Œå¹´ï¼‰ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€æ–°ã€æœ€å¤§çš„æ¨¡å‹ï¼Œå¦‚Llama-3.1-70Bï¼Œå¯¹æ—¶åºäº‹å®çš„çŸ¥è¯†ç¨³å¥æ€§ä¹Ÿä¸¥é‡ä¸è¶³ã€‚æ­¤å¤–ï¼Œå®ƒä»¬æ— æ³•å°†çŸ¥è¯†ä»ä¸€ä¸ªç²’åº¦æ¨å¹¿åˆ°å¦ä¸€ä¸ªç²’åº¦ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å°†è¯­è¨€æ¨¡å‹ç”¨ä½œä¸´æ—¶çŸ¥è¯†åº“çš„å†…åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬å°†å‘å¸ƒç”¨äºé‡ç°æˆ‘ä»¬å®éªŒçš„æºä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01220v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨å¤„ç†äº‹å®çŸ¥è¯†æ—¶çš„æ—¶åºç¨³å¥æ€§ã€‚è™½ç„¶è¯­è¨€æ¨¡å‹é€šå¸¸å¯ä»¥å®Œæˆç®€å•çš„é™ˆè¿°ï¼Œä½†å®ƒä»¬å¤„ç†æ—¶æ•ˆæ€§äº‹å®ï¼ˆä»…åœ¨ç‰¹å®šæ—¶é—´èŒƒå›´å†…æœ‰æ•ˆçš„äº‹å®ï¼‰çš„èƒ½åŠ›å°šä¸ç¡®å®šã€‚æœ¬æ–‡é€šè¿‡è®¾è®¡å®éªŒæµ‹è¯•äº†è¯­è¨€æ¨¡å‹å†…éƒ¨å¯¹æ—¶æ•ˆæ€§äº‹å®ç¨³å¥æ€§çš„è¯„ä¼°ï¼Œä½¿ç”¨é’ˆå¯¹æµè¡ŒWikidataäº‹å®çš„æç¤ºæ¥è¯„ä¼°å¤šä¸ªé¢„è®­ç»ƒå’ŒæŒ‡å¯¼è°ƒæ•´æ¨¡å‹çš„æ€§èƒ½ï¼Œè¯„ä¼°å…¶åœ¨ä¸åŒæ—¶é—´ç²’åº¦ï¼ˆæ—¥ã€æœˆå’Œå¹´ï¼‰çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å¤§çš„æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚Llama-3.1-70Bï¼Œå¯¹æ—¶æ•ˆæ€§çŸ¥è¯†çš„ç¨³å¥æ€§ä¹Ÿä¸¥é‡ä¸è¶³ã€‚æ­¤å¤–ï¼Œå®ƒä»¬æ— æ³•å°†çŸ¥è¯†ä»ä¸€ä¸ªç²’åº¦æ¨å¹¿åˆ°å¦ä¸€ä¸ªç²’åº¦ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å°†è¯­è¨€æ¨¡å‹ç”¨ä½œä¸´æ—¶çŸ¥è¯†åº“çš„å†…åœ¨å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨å¤„ç†äº‹å®çŸ¥è¯†æ—¶ï¼Œå°¤å…¶æ˜¯æ—¶æ•ˆæ€§äº‹å®ï¼Œå­˜åœ¨ç¨³å¥æ€§é—®é¢˜ã€‚</li>
<li>å¤§å‹æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹å¦‚Llama-3.1-70Båœ¨æ—¶æ•ˆæ€§çŸ¥è¯†æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ—¶æ•ˆæ€§äº‹å®æ—¶ï¼Œæ— æ³•å°†çŸ¥è¯†ä»ä¸€ä¸ªæ—¶é—´ç²’åº¦æ¨å¹¿åˆ°å¦ä¸€ä¸ªç²’åº¦ã€‚</li>
<li>è¯­è¨€æ¨¡å‹ä½œä¸ºä¸´æ—¶çŸ¥è¯†åº“çš„å†…åœ¨å±€é™æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯é’ˆå¯¹æµè¡ŒWikidataäº‹å®çš„æç¤ºï¼Œè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¹Ÿå‚å·®ä¸é½ã€‚</li>
<li>å®éªŒä¸­è®¾è®¡çš„æ§åˆ¶æµ‹è¯•å¯ä»¥æœ‰æ•ˆè¯„ä¼°è¯­è¨€æ¨¡å‹å¤„ç†æ—¶æ•ˆæ€§äº‹å®çš„èƒ½åŠ›ã€‚</li>
<li>æºç å’Œæ•°æ®å°†å…¬å¼€ï¼Œä»¥ä¾¿ä»–äººé‡ç°å®éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99e53cd5be26c73c3986cda89ec9613c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeb2708abf5bb6e26580a128c9d24586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3725b90733067bb25be94ecd1e552ae.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AtmosSci-Bench-Evaluating-the-Recent-Advance-of-Large-Language-Model-for-Atmospheric-Science"><a href="#AtmosSci-Bench-Evaluating-the-Recent-Advance-of-Large-Language-Model-for-Atmospheric-Science" class="headerlink" title="AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science"></a>AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science</h2><p><strong>Authors:Chenyue Li, Wen Deng, Mengqian Lu, Binhang Yuan</strong></p>
<p>The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. To address this need, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. We employ a template-based question generation framework, enabling scalable and diverse multiple-choice questions curated from graduate-level atmospheric science problems. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate service by offering a standard and rigorous evaluation framework. Our source codes are currently available at <a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench">https://github.com/Relaxed-System-Lab/AtmosSci-Bench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶æ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œä¸ºè§£å†³å¤§æ°”ç§‘å­¦ä¸­çš„å¤æ‚æŒ‘æˆ˜å¸¦æ¥äº†å˜é©æ€§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¦åœ¨è¿™ä¸€é¢†åŸŸæœ‰æ•ˆåœ°åˆ©ç”¨LLMï¼Œéœ€è¦ç¨³å¥è€Œå…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AtmosSci-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°LLMåœ¨äº”ä¸ªå¤§æ°”ç§‘å­¦æ ¸å¿ƒç±»åˆ«çš„é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼šæ°´æ–‡ã€å¤§æ°”åŠ¨åŠ›å­¦ã€å¤§æ°”ç‰©ç†å­¦ã€åœ°çƒç‰©ç†å­¦å’Œç‰©ç†æµ·æ´‹å­¦ã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿè§„æ¨¡åŒ–åœ°ä»ç ”ç©¶ç”Ÿå±‚æ¬¡çš„å¤§æ°”ç§‘å­¦é—®é¢˜ä¸­ç”Ÿæˆå¤šæ ·ä¸”ä¸°å¯Œçš„é€‰æ‹©é¢˜ã€‚æˆ‘ä»¬å¯¹å…·æœ‰ä»£è¡¨æ€§çš„LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå°†å…¶åˆ†ä¸ºå››ç»„ï¼šæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€é«˜çº§æ¨ç†æ¨¡å‹ã€æ•°å­¦å¢å¼ºæ¨¡å‹å’Œç‰¹å®šé¢†åŸŸçš„æ°”å€™æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æå¯¹LLMåœ¨å¤§æ°”ç§‘å­¦ä¸­çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›æä¾›äº†ä¸€äº›æœ‰è¶£çš„è§è§£ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œé€šè¿‡æä¾›ä¸€ä¸ªæ ‡å‡†ä¸”ä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼ŒAtmosSci-Benchå¯ä»¥æˆä¸ºæ¨è¿›LLMåœ¨æ°”å€™æœåŠ¡ä¸­åº”ç”¨çš„å…³é”®æ­¥éª¤ã€‚æˆ‘ä»¬çš„æºä»£ç ç›®å‰å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench">https://github.com/Relaxed-System-Lab/AtmosSci-Bench</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01159v1">PDF</a> 16 pages, 3 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„å¿«é€Ÿè¿›æ­¥ï¼Œä¸ºåº”å¯¹å¤§æ°”ç§‘å­¦ä¸­çš„å¤æ‚æŒ‘æˆ˜å¸¦æ¥äº†å˜é©æ€§æ½œåŠ›ã€‚ä¸ºè§£å†³åœ¨åŸŸä¸­æœ‰æ•ˆåˆ©ç”¨LLMçš„éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AtmosSci-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨äº”ä¸ªå¤§æ°”ç§‘å­¦æ ¸å¿ƒç±»åˆ«çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼šæ°´æ–‡å­¦ã€å¤§æ°”åŠ¨åŠ›å­¦ã€å¤§æ°”ç‰©ç†å­¦ã€åœ°çƒç‰©ç†å­¦å’Œç‰©ç†æµ·æ´‹å­¦ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿä»ç ”ç©¶ç”Ÿçº§åˆ«çš„å¤§æ°”ç§‘å­¦é—®é¢˜ä¸­ç”Ÿæˆå¯æ‰©å±•å’Œå¤šæ ·åŒ–çš„å¤šé¡¹é€‰æ‹©é¢˜ã€‚æˆ‘ä»¬å¯¹ä»£è¡¨æ€§LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œåˆ†ä¸ºå››ç±»ï¼šæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€é«˜çº§æ¨ç†æ¨¡å‹ã€æ•°å­¦å¢å¼ºæ¨¡å‹å’Œé¢†åŸŸç‰¹å®šæ°”å€™æ¨¡å‹ã€‚æ­¤åˆ†æä¸ºLLMåœ¨å¤§æ°”ç§‘å­¦ä¸­çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›æä¾›äº†æœ‰è¶£è§è§£ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒAtmosSci-Benchå¯ä»¥ä½œä¸ºæ¨è¿›LLMåœ¨æ°”å€™æœåŠ¡ä¸­åº”ç”¨çš„å…³é”®æ­¥éª¤ï¼Œé€šè¿‡æä¾›æ ‡å‡†å’Œä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶æ¥æœåŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„è¿…é€Ÿè¿›æ­¥ä¸ºåº”å¯¹å¤§æ°”ç§‘å­¦ä¸­çš„å¤æ‚æŒ‘æˆ˜å¸¦æ¥äº†é‡è¦æœºä¼šã€‚</li>
<li>AtmosSci-Benchæ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LLMè§£å†³å¤§æ°”ç§‘å­¦ä¸­äº”ä¸ªæ ¸å¿ƒç±»åˆ«é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>AtmosSci-Benché‡‡ç”¨åŸºäºæ¨¡æ¿çš„é—®é¢˜ç”Ÿæˆæ¡†æ¶ï¼Œå¯ç”Ÿæˆå¤šæ ·åŒ–ã€å¯æ‰©å±•çš„å¤šé¡¹é€‰æ‹©é¢˜ã€‚</li>
<li>ç»¼åˆæ€§è¯„ä¼°æ¶µç›–äº†ä¸åŒç±»å‹çš„LLMï¼ŒåŒ…æ‹¬æŒ‡ä»¤è°ƒæ•´ã€é«˜çº§æ¨ç†ã€æ•°å­¦å¢å¼ºå’Œé¢†åŸŸç‰¹å®šæ°”å€™æ¨¡å‹ã€‚</li>
<li>LLMåœ¨å¤§æ°”ç§‘å­¦ä¸­çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›è¡¨ç°å‡ºæœ‰è¶£ä¸”å…·æ´å¯ŸåŠ›çš„æ€§èƒ½ã€‚</li>
<li>AtmosSci-Benché€šè¿‡æä¾›æ ‡å‡†å’Œä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼Œè¢«è§†ä¸ºæ¨è¿›LLMåœ¨æ°”å€™æœåŠ¡ä¸­åº”ç”¨çš„å…³é”®æ­¥éª¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-abe26b0c19cb580060f06a7a9878bf0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b915b23aa83a9d8f4542f116a568d3a4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Omni-Mol-Exploring-Universal-Convergent-Space-for-Omni-Molecular-Tasks"><a href="#Omni-Mol-Exploring-Universal-Convergent-Space-for-Omni-Molecular-Tasks" class="headerlink" title="Omni-Mol: Exploring Universal Convergent Space for Omni-Molecular Tasks"></a>Omni-Mol: Exploring Universal Convergent Space for Omni-Molecular Tasks</h2><p><strong>Authors:Chengxin Hu, Hao Li, Yihe Yuan, Zezheng Song, Haixin Wang</strong></p>
<p>Building generalist models has recently demonstrated remarkable capabilities in diverse scientific domains. Within the realm of molecular learning, several studies have explored unifying diverse tasks across diverse domains. However, negative conflicts and interference between molecules and knowledge from different domain may have a worse impact in threefold. First, conflicting molecular representations can lead to optimization difficulties for the models. Second, mixing and scaling up training data across diverse tasks is inherently challenging. Third, the computational cost of refined pretraining is prohibitively high. To address these limitations, this paper presents Omni-Mol, a scalable and unified LLM-based framework for direct instruction tuning. Omni-Mol builds on three key components to tackles conflicts: (1) a unified encoding mechanism for any task input; (2) an active-learning-driven data selection strategy that significantly reduces dataset size; (3) a novel design of the adaptive gradient stabilization module and anchor-and-reconcile MoE framework that ensures stable convergence. Experimentally, Omni-Mol achieves state-of-the-art performance across 15 molecular tasks, demonstrates the presence of scaling laws in the molecular domain, and is supported by extensive ablation studies and analyses validating the effectiveness of its design. The code and weights of the powerful AI-driven chemistry generalist are open-sourced at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Omni-Mol-8EDB">https://anonymous.4open.science/r/Omni-Mol-8EDB</a>. </p>
<blockquote>
<p>æ„å»ºé€šç”¨æ¨¡å‹æœ€è¿‘åœ¨å¤šä¸ªç§‘å­¦é¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚åœ¨åˆ†å­å­¦ä¹ é¢†åŸŸï¼Œä¸€äº›ç ”ç©¶å·²ç»æ¢ç´¢äº†åœ¨ä¸åŒçš„é¢†åŸŸä¸­ç»Ÿä¸€å„ç§ä»»åŠ¡çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œä¸åŒé¢†åŸŸçš„åˆ†å­å’ŒçŸ¥è¯†ä¹‹é—´çš„è´Ÿé¢å†²çªå’Œå¹²æ‰°å¯èƒ½ä¼šäº§ç”Ÿä¸‰é‡çš„è´Ÿé¢å½±å“ã€‚é¦–å…ˆï¼Œå†²çªçš„åˆ†å­è¡¨ç¤ºå½¢å¼å¯èƒ½å¯¼è‡´æ¨¡å‹ä¼˜åŒ–å›°éš¾ã€‚å…¶æ¬¡ï¼Œåœ¨ä¸åŒä»»åŠ¡ä¹‹é—´æ··åˆå’Œæ‰©å¤§è®­ç»ƒæ•°æ®æœ¬èº«å°±å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç¬¬ä¸‰ï¼Œç²¾ç»†é¢„è®­ç»ƒçš„è®¡ç®—æˆæœ¬éå¸¸é«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Omni-Molï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„ã€åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç›´æ¥æŒ‡ä»¤è°ƒæ•´ã€‚Omni-Molå»ºç«‹åœ¨ä¸‰ä¸ªå…³é”®ç»„ä»¶ä¸Šä»¥è§£å†³å†²çªï¼šï¼ˆ1ï¼‰ä»»ä½•ä»»åŠ¡è¾“å…¥çš„ç»Ÿä¸€ç¼–ç æœºåˆ¶ï¼›ï¼ˆ2ï¼‰ä»¥ä¸»åŠ¨å­¦ä¹ é©±åŠ¨çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå¯æ˜¾è‘—å‡å°‘æ•°æ®é›†å¤§å°ï¼›ï¼ˆ3ï¼‰è‡ªé€‚åº”æ¢¯åº¦ç¨³å®šæ¨¡å—å’Œé”šç‚¹ä¸å’Œè§£MoEæ¡†æ¶çš„æ–°é¢–è®¾è®¡ï¼Œç¡®ä¿ç¨³å®šçš„æ”¶æ•›ã€‚é€šè¿‡å®éªŒï¼ŒOmni-Molåœ¨15ä¸ªåˆ†å­ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†åˆ†å­é¢†åŸŸçš„è§„æ¨¡åŒ–æ³•åˆ™çš„å­˜åœ¨ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶å’Œåˆ†æéªŒè¯äº†å…¶è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚è¯¥å¼ºå¤§çš„AIé©±åŠ¨åŒ–å­¦é€šç”¨æ¨¡å‹çš„ä»£ç å’Œæƒé‡å·²å¼€æºï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Omni-Mol-8EDB%E3%80%82">https://anonymous.4open.science/r/Omni-Mol-8EDBã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01074v1">PDF</a> 29 pages, 13 figures, 7 tables, paper under review</p>
<p><strong>Summary</strong></p>
<p>Omni-Molæ¡†æ¶é€šè¿‡ç»Ÿä¸€ç¼–ç æœºåˆ¶ã€ä¸»åŠ¨å­¦ä¹ çš„æ•°æ®é€‰æ‹©ç­–ç•¥å’Œæ¢¯åº¦ç¨³å®šæ¨¡å—ä¸å’Œè§£MoEæ¡†æ¶çš„è®¾è®¡ï¼Œè§£å†³äº†åœ¨åˆ†å­å­¦ä¹ é¢†åŸŸå¤šä»»åŠ¡å­¦ä¹ çš„å†²çªé—®é¢˜ï¼Œå®ç°äº†è·¨15ä¸ªåˆ†å­ä»»åŠ¡çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Omni-Molæ˜¯ä¸€ä¸ªåŸºäºLLMçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³åˆ†å­å­¦ä¹ ä¸­çš„å¤šä»»åŠ¡å­¦ä¹ å†²çªé—®é¢˜ã€‚</li>
<li>å®ƒé€šè¿‡ç»Ÿä¸€ç¼–ç æœºåˆ¶é€‚åº”å„ç§ä»»åŠ¡è¾“å…¥ã€‚</li>
<li>Omni-Molé‡‡ç”¨ä¸»åŠ¨å­¦ä¹ çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘æ•°æ®é›†å¤§å°ã€‚</li>
<li>è¯¥æ¡†æ¶è®¾è®¡äº†è‡ªé€‚åº”æ¢¯åº¦ç¨³å®šæ¨¡å—å’Œé”šå®šä¸å’Œè§£MoEæ¡†æ¶ï¼Œç¡®ä¿æ¨¡å‹ç¨³å®šæ”¶æ•›ã€‚</li>
<li>Omni-Molåœ¨15ä¸ªåˆ†å­ä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶å±•ç¤ºäº†åˆ†å­é¢†åŸŸçš„è§„æ¨¡æ³•åˆ™ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶å’Œåˆ†æéªŒè¯äº†å…¶è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a73dad8414e0eae73ffdf458946756aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-208f3fbc35750a318aa341c0d6870eb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-456d579badae8d91abc83093d2f10cd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69a650b7c53e15bb5ebe3086da775b9.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SimPER-A-Minimalist-Approach-to-Preference-Alignment-without-Hyperparameters"><a href="#SimPER-A-Minimalist-Approach-to-Preference-Alignment-without-Hyperparameters" class="headerlink" title="SimPER: A Minimalist Approach to Preference Alignment without   Hyperparameters"></a>SimPER: A Minimalist Approach to Preference Alignment without   Hyperparameters</h2><p><strong>Authors:Teng Xiao, Yige Yuan, Zhengyu Chen, Mingxiao Li, Shangsong Liang, Zhaochun Ren, Vasant G Honavar</strong></p>
<p>Existing preference optimization objectives for language model alignment require additional hyperparameters that must be extensively tuned to achieve optimal performance, increasing both the complexity and time required for fine-tuning large language models. In this paper, we propose a simple yet effective hyperparameter-free preference optimization algorithm for alignment. We observe that promising performance can be achieved simply by optimizing inverse perplexity, which is calculated as the inverse of the exponentiated average log-likelihood of the chosen and rejected responses in the preference dataset. The resulting simple learning objective, SimPER, is easy to implement and eliminates the need for expensive hyperparameter tuning and a reference model, making it both computationally and memory efficient. Extensive experiments on widely used real-world benchmarks, including MT-Bench, AlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base models, demonstrate that SimPER consistently and significantly outperforms existing approaches-even without any hyperparameters or a reference model . For example, despite its simplicity, SimPER outperforms state-of-the-art methods by up to 5.7 points on AlpacaEval 2 and achieves the highest average ranking across 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/tengxiao1/SimPER">https://github.com/tengxiao1/SimPER</a>. </p>
<blockquote>
<p>ç°æœ‰è¯­è¨€æ¨¡å‹å¯¹é½çš„åå¥½ä¼˜åŒ–ç›®æ ‡éœ€è¦é¢å¤–çš„è¶…å‚æ•°ï¼Œä¸”å¿…é¡»è¿›è¡Œå…¨é¢è°ƒæ•´æ‰èƒ½å®ç°æœ€ä½³æ€§èƒ½ï¼Œè¿™å¢åŠ äº†å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ€§å’Œæ—¶é—´è¦æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ— è¶…å‚æ•°åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºå¯¹é½ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡ä¼˜åŒ–é€†å›°æƒ‘åº¦ï¼ˆå³æ‰€é€‰å’Œæ‹’ç»å“åº”çš„åå¥½æ•°æ®é›†çš„æŒ‡æ•°å¹³å‡å¯¹æ•°ä¼¼ç„¶åº¦çš„å€’æ•°ï¼‰å¯ä»¥è¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚ç”±æ­¤äº§ç”Ÿçš„ç®€å•å­¦ä¹ ç›®æ ‡SimPERæ˜“äºå®ç°ï¼Œæ— éœ€æ˜‚è´µçš„è¶…å‚æ•°è°ƒæ•´å’Œå‚è€ƒæ¨¡å‹ï¼Œä½¿å…¶åœ¨è®¡ç®—å’Œå†…å­˜æ–¹é¢éƒ½éå¸¸é«˜æ•ˆã€‚åœ¨åŒ…æ‹¬MT-Benchã€AlpacaEval 2å’ŒOpen LLM Leaderboardçš„10ä¸ªå…³é”®åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒï¼Œä½¿ç”¨5ä¸ªåŸºç¡€æ¨¡å‹è¿›è¡ŒéªŒè¯ï¼Œç»“æœè¡¨æ˜SimPERæŒç»­ä¸”æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå³ä½¿æ²¡æœ‰ä»»ä½•è¶…å‚æ•°æˆ–å‚è€ƒæ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå°½ç®¡SimPERç®€å•ï¼Œä½†åœ¨AlpacaEval 2ä¸Šå®ƒçš„æ€§èƒ½æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•é«˜å‡ºé«˜è¾¾5.7ä¸ªç‚¹ï¼Œå¹¶åœ¨Open LLM Leaderboardçš„10ä¸ªåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†æœ€é«˜å¹³å‡æ’åã€‚SimPERçš„æºä»£ç å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/tengxiao1/SimPER%E3%80%82">https://github.com/tengxiao1/SimPERã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00883v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè¯­è¨€æ¨¡å‹å¯¹é½çš„ç®€æ´è€Œæœ‰æ•ˆçš„æ— è¶…å‚æ•°åå¥½ä¼˜åŒ–ç®—æ³•ã€‚é€šè¿‡ä¼˜åŒ–é€‰æ‹©å“åº”å’Œæ‹’ç»å“åº”çš„é€†å›°æƒ‘åº¦ï¼Œå®ç°äº†è‰¯å¥½çš„æ€§èƒ½ã€‚è¯¥ç®—æ³•ä¸ä»…æ˜“äºå®ç°ï¼Œè€Œä¸”æ— éœ€æ˜‚è´µçš„è¶…å‚æ•°è°ƒæ•´å’Œå‚è€ƒæ¨¡å‹ï¼Œå…·æœ‰è¾ƒé«˜çš„è®¡ç®—æ•ˆç‡å’Œå†…å­˜æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimPERåœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸ŠæŒç»­ä¸”æ˜¾è‘—åœ°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è¯­è¨€æ¨¡å‹å¯¹é½çš„åå¥½ä¼˜åŒ–ç›®æ ‡éœ€è¦é¢å¤–çš„è¶…å‚æ•°ï¼Œå¢åŠ äº†å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ€§å’Œæ—¶é—´ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è¶…å‚æ•°çš„åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡ä¼˜åŒ–é€†å›°æƒ‘åº¦å®ç°å¯¹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆå¯¹é½ã€‚</li>
<li>SimPERç®—æ³•ç®€å•æ˜“è¡Œï¼Œä¸éœ€è¦æ˜‚è´µçš„è¶…å‚æ•°è°ƒæ•´å’Œå‚è€ƒæ¨¡å‹ã€‚</li>
<li>SimPERåœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>SimPERåœ¨MT-Benchã€AlpacaEval 2ä»¥åŠOpen LLM Leaderboardçš„10ä¸ªå…³é”®åŸºå‡†æµ‹è¯•ä¸Šéƒ½å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>SimPERç®—æ³•å·²å…¬å¼€å¯ç”¨ï¼Œå¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/tengxiao1/SimPER%E8%8E%B7%E5%8F%96%E6%BA%90%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/tengxiao1/SimPERè·å–æºä»£ç ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9ccba258b30d834d03207bfd7b4210e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9368517360d4c67cceafe433df8324b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee03fb7c929e3983c1b28bd62c70867c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf8ba98161734315ee8817bc2cfac83.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Beyond-the-Permutation-Symmetry-of-Transformers-The-Role-of-Rotation-for-Model-Fusion"><a href="#Beyond-the-Permutation-Symmetry-of-Transformers-The-Role-of-Rotation-for-Model-Fusion" class="headerlink" title="Beyond the Permutation Symmetry of Transformers: The Role of Rotation   for Model Fusion"></a>Beyond the Permutation Symmetry of Transformers: The Role of Rotation   for Model Fusion</h2><p><strong>Authors:Binchi Zhang, Zaiyi Zheng, Zhengzhang Chen, Jundong Li</strong></p>
<p>Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on <a target="_blank" rel="noopener" href="https://github.com/zhengzaiyi/RotationSymmetry">https://github.com/zhengzaiyi/RotationSymmetry</a>. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å‚æ•°ç©ºé—´ä¸­çš„å¯¹ç§°æ€§å·²è¢«è¯æ˜å¯¹å„ç§æ·±åº¦å­¦ä¹ åº”ç”¨æœ‰ç›Šã€‚ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„ä¾‹å­æ˜¯å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä¸­çš„ç½®æ¢å¯¹ç§°æ€§ï¼Œå…¶ä¸­å¯¹ä¸€å±‚ä¸­çš„æƒé‡çŸ©é˜µçš„è¡Œè¿›è¡Œç½®æ¢ï¼Œå¹¶å¯¹ç›¸é‚»å±‚åº”ç”¨é€†ç½®æ¢ï¼Œäº§ç”ŸåŠŸèƒ½ä¸Šç­‰æ•ˆçš„æ¨¡å‹ã€‚è™½ç„¶ç½®æ¢å¯¹ç§°æ€§å®Œå…¨è¡¨å¾äº†MLPçš„ç­‰ä»·é›†ï¼Œä½†å…¶ç¦»æ•£æ€§è´¨é™åˆ¶äº†å…¶åœ¨å˜æ¢å™¨ä¸­çš„åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—‹è½¬å¯¹ç§°æ€§ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å‚æ•°ç©ºé—´å¯¹ç§°æ€§ï¼Œé€šè¿‡æ—‹è½¬è‡ªæ³¨æ„åŠ›å±‚çš„å‚æ•°çŸ©é˜µæ¥æ¨å¹¿ç½®æ¢å¯¹ç§°æ€§ã€‚ä¸ç½®æ¢å¯¹ç§°æ€§ä¸åŒï¼Œæ—‹è½¬å¯¹ç§°æ€§åœ¨ä¸€ä¸ªè¿ç»­åŸŸä¸­è¿è¡Œï¼Œä»è€Œæ˜¾è‘—æ‰©å¤§äº†å˜æ¢å™¨çš„ç­‰ä»·é›†ã€‚åŸºäºè¿™ä¸€å±æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç†è®ºä¸Šçš„æœ€ä¼˜å‚æ•°åŒ¹é…ç®—æ³•ï¼Œä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—æ¥å¢å¼ºæ¨¡å‹èåˆã€‚æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„å˜æ¢å™¨åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†å’Œè§†è§‰ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºæ—‹è½¬å¯¹ç§°æ€§çš„åŒ¹é…ç®—æ³•æå¤§åœ°æé«˜äº†æ¨¡å‹èåˆï¼Œçªå‡ºäº†å‚æ•°ç©ºé—´å¯¹ç§°æ€§åœ¨ä¿ƒè¿›æ¨¡å‹èåˆæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhengzaiyi/RotationSymmetry%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhengzaiyi/RotationSymmetryä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00264v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ ç½‘ç»œå‚æ•°ç©ºé—´çš„å¯¹ç§°æ€§å¯¹äºå¤šç§æ·±åº¦å­¦ä¹ åº”ç”¨å…·æœ‰ç›Šå¤„ã€‚è®ºæ–‡ä»‹ç»äº†æ—‹è½¬å¯¹ç§°æ€§è¿™ä¸€æ–°å‹å‚æ•°ç©ºé—´å¯¹ç§°æ€§å½¢å¼ï¼Œå¹¶å°†å…¶åº”ç”¨äºå˜å‹å™¨æ¨¡å‹ã€‚æ—‹è½¬å¯¹ç§°æ€§é€šè¿‡æ—‹è½¬è‡ªæ³¨æ„åŠ›å±‚çš„å‚æ•°çŸ©é˜µå®ç°ï¼Œä¸åŒäºç¦»æ•£æ’åˆ—å¯¹ç§°æ€§ï¼Œå…¶åœ¨è¿ç»­åŸŸå†…è¿è¡Œï¼Œæ˜¾è‘—æ‰©å±•äº†å˜å‹å™¨çš„ç­‰ä»·é›†åˆã€‚åŸºäºæ­¤å±æ€§ï¼Œè®ºæ–‡æå‡ºä¸€ç§ç†è®ºä¸Šçš„æœ€ä½³å‚æ•°åŒ¹é…ç®—æ³•ï¼Œä½œä¸ºå¢å¼ºæ¨¡å‹èåˆçš„å³æ’å³ç”¨æ¨¡å—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥åŸºäºæ—‹è½¬å¯¹ç§°æ€§çš„åŒ¹é…ç®—æ³•èƒ½æ˜¾è‘—æé«˜æ¨¡å‹èåˆæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‚æ•°ç©ºé—´å¯¹ç§°åœ¨æ·±åº¦å­¦ä¹ ä¸­æœ‰ç›Šã€‚</li>
<li>è®ºæ–‡å¼•å…¥æ—‹è½¬å¯¹ç§°æ€§ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å‚æ•°ç©ºé—´å¯¹ç§°æ€§å½¢å¼ã€‚</li>
<li>æ—‹è½¬å¯¹ç§°æ€§åº”ç”¨äºå˜å‹å™¨æ¨¡å‹ï¼Œé€šè¿‡æ—‹è½¬è‡ªæ³¨æ„åŠ›å±‚çš„å‚æ•°çŸ©é˜µå®ç°ã€‚</li>
<li>æ—‹è½¬å¯¹ç§°æ€§åœ¨è¿ç»­åŸŸå†…è¿è¡Œï¼Œæ‰©å±•äº†å˜å‹å™¨çš„ç­‰ä»·é›†åˆã€‚</li>
<li>æå‡ºä¸€ç§ç†è®ºä¸Šçš„æœ€ä½³å‚æ•°åŒ¹é…ç®—æ³•ï¼Œå¢å¼ºæ¨¡å‹èåˆã€‚</li>
<li>åŸºäºæ—‹è½¬å¯¹ç§°æ€§çš„åŒ¹é…ç®—æ³•èƒ½æ˜¾è‘—æé«˜æ¨¡å‹èåˆæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6e49df1c83cda86bcc4ca6fbc306ee2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72d277e77e40f33727666ac304117a22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-438a952d3265d835f29fa8e82735ae80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56f2561d502749811270bbf4aa58c791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b72214b4d18da8a269009ef99824774b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="The-TIP-of-the-Iceberg-Revealing-a-Hidden-Class-of-Task-in-Prompt-Adversarial-Attacks-on-LLMs"><a href="#The-TIP-of-the-Iceberg-Revealing-a-Hidden-Class-of-Task-in-Prompt-Adversarial-Attacks-on-LLMs" class="headerlink" title="The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt   Adversarial Attacks on LLMs"></a>The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt   Adversarial Attacks on LLMs</h2><p><strong>Authors:Sergey Berezin, Reza Farahbakhsh, Noel Crespi</strong></p>
<p>We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the modelâ€™s prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies.   Warning: this paper contains examples of unethical inquiries used solely for research purposes. </p>
<blockquote>
<p>æˆ‘ä»¬é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå‡ºäº†ä¸€ç±»æ–°å‹è¶Šç‹±å¯¹æŠ—æ”»å‡»ï¼Œç§°ä¸ºTask-in-Promptï¼ˆTIPï¼‰æ”»å‡»ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼ˆå¦‚å¯†ç è§£ç ã€è°œè¯­ã€ä»£ç æ‰§è¡Œï¼‰åµŒå…¥åˆ°æ¨¡å‹çš„æç¤ºä¸­ï¼Œä»¥é—´æ¥ç”Ÿæˆç¦æ­¢çš„è¾“å…¥ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›æ”»å‡»çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†PHRYGEåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯æˆåŠŸåœ°ç»•è¿‡äº†å…­ç§æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹çš„å®‰å…¨ä¿éšœæªæ–½ï¼ŒåŒ…æ‹¬GPT-4oå’ŒLLaMA 3.2ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†å½“å‰LLMå®‰å…¨å¯¹é½ä¸­çš„å…³é”®å¼±ç‚¹ï¼Œå¹¶å¼ºè°ƒäº†æ›´å¤æ‚çš„é˜²å¾¡ç­–ç•¥çš„ç´§è¿«éœ€æ±‚ã€‚è­¦å‘Šï¼šæœ¬è®ºæ–‡åŒ…å«çš„è¯¢é—®ç¤ºä¾‹ä»…ç”¨äºç ”ç©¶ç›®çš„ï¼Œå¹¶éå‡ºäºé“å¾·ç«‹åœºè€Œæå‡ºæ”»å‡»å»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18626v3">PDF</a> </p>
<p><strong>Summary</strong><br>ä»»åŠ¡åµŒå…¥æç¤ºæ”»å‡»ï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°å‹è¶Šç‹±å¯¹æŠ—æ”»å‡»ã€‚é€šè¿‡åµŒå…¥åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼ˆå¦‚å¯†ç è§£ç ã€è§£è°œã€ä»£ç æ‰§è¡Œï¼‰åˆ°æ¨¡å‹æç¤ºæ¥é—´æ¥ç”Ÿæˆç¦æ­¢è¾“å…¥ï¼Œå¹¶å¯¹å½“å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¼”ç¤ºè¯„ä¼°ã€‚å±•ç¤ºäº†å¯¹æŠ—æ–¹æ³•å¯ç»•è¿‡å®‰å…¨é˜²èŒƒæªæ–½æ”»å‡»å‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹å¹¶æŒ‡å‡ºæ¨¡å‹å®‰å…¨æ¼æ´åŠé˜²èŒƒè¿«åˆ‡æ€§ã€‚åŒ…å«ä¸ºç ”ç©¶ç›®çš„ä»…ç”¨çš„éé“å¾·æ¡ˆä¾‹è¯¢é—®ç¤ºä¾‹ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§æŒ‘æˆ˜äºŸéœ€åº”å¯¹å’Œè§£å†³ã€‚ç ”ç©¶äº®ç‚¹åœ¨äºå…¶çªç ´æ€§çš„è¯„ä¼°æ–¹æ³•å’Œæ–°çš„å®‰å…¨æ¼æ´çš„å‘ç°ï¼ŒåŒæ—¶è­¦å‘Šäº†é’ˆå¯¹è¿™äº›æ¼æ´å¯èƒ½å­˜åœ¨çš„æ½œåœ¨é£é™©ã€‚å¼ºè°ƒäº†ä¼¦ç†å’Œé“å¾·åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦æ€§ï¼Œå¹¶å‘¼åä¸šç•Œå…³æ³¨å¹¶é‡è§†è¿™ä¸€é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ”»å‡»æ–¹æ³•â€”â€”ä»»åŠ¡åµŒå…¥æç¤ºï¼ˆTIPï¼‰æ”»å‡»ã€‚</li>
<li>é€šè¿‡åœ¨æ¨¡å‹æç¤ºä¸­åµŒå…¥åºåˆ—åˆ°åºåˆ—ä»»åŠ¡æ¥é—´æ¥ç”Ÿæˆç¦æ­¢è¾“å…¥ï¼Œä»è€Œç»•è¿‡å®‰å…¨é˜²èŒƒæªæ–½æ”»å‡»å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>å±•ç¤ºäº†TIPæ”»å‡»åœ¨å…­ç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬GPT-4oå’ŒLLaMA 3.2ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffd8552147cd8625dfbd99d6ed52a466.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ad600a12d86d1c1c57649ede0a528e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d075c69d70beaa4059b93faa482363e8.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-129c03c30fabc821a4ca723773c59182.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  DHP Discrete Hierarchical Planning for Hierarchical Reinforcement   Learning Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5f4ca5ae820d1405e3f7f4ca095f44ac.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-05  Exploring total fission cross sections of neutron- and proton-induced   reactions of exotic nuclei at relativistic energies using the INCL-ABLA++   models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
