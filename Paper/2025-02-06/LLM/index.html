<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-02-06  A comparison of translation performance between DeepL and Supertext">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-67c4ed3d47a60b3d126b11e3d1c258e2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    77 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-06-更新"><a href="#2025-02-06-更新" class="headerlink" title="2025-02-06 更新"></a>2025-02-06 更新</h1><h2 id="A-comparison-of-translation-performance-between-DeepL-and-Supertext"><a href="#A-comparison-of-translation-performance-between-DeepL-and-Supertext" class="headerlink" title="A comparison of translation performance between DeepL and Supertext"></a>A comparison of translation performance between DeepL and Supertext</h2><p><strong>Authors:Alex Flückiger, Chantal Amrhein, Tim Graf, Philippe Schläpfer, Florian Schottmann, Samuel Läubli</strong></p>
<p>As strong machine translation (MT) systems are increasingly based on large language models (LLMs), reliable quality benchmarking requires methods that capture their ability to leverage extended context. This study compares two commercial MT systems – DeepL and Supertext – by assessing their performance on unsegmented texts. We evaluate translation quality across four language directions with professional translators assessing segments with full document-level context. While segment-level assessments indicate no strong preference between the systems in most cases, document-level analysis reveals a preference for Supertext in three out of four language directions, suggesting superior consistency across longer texts. We advocate for more context-sensitive evaluation methodologies to ensure that MT quality assessments reflect real-world usability. We release all evaluation data and scripts for further analysis and reproduction at <a target="_blank" rel="noopener" href="https://github.com/supertext/evaluation_deepl_supertext">https://github.com/supertext/evaluation_deepl_supertext</a>. </p>
<blockquote>
<p>随着强大的机器翻译（MT）系统越来越多地依赖于大型语言模型（LLM），可靠的质量评估需要能够捕捉其在扩展语境下能力的方法。本研究通过评估它们在未分段文本上的表现，对比了DeepL和Supertext两个商业机器翻译系统。我们评估了四个语言方向的翻译质量，专业翻译人员会评估具有全文语境的片段。虽然分段级别的评估显示，在大多数情况下这两个系统之间没有明显的偏好，但文档级别的分析揭示了在四个语言方向中有三个方向更倾向于Supertext，这表明其在较长文本中的一致性更佳。我们提倡采用更敏感于语境的评估方法，以确保机器翻译的质量评估能够反映真实世界的可用性。我们在<a target="_blank" rel="noopener" href="https://github.com/supertext/evaluation_deepl_supertext">https://github.com/supertext/evaluation_deepl_supertext</a>上公开了所有评估数据和脚本，供进一步分析和复制使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02577v1">PDF</a> </p>
<p><strong>Summary</strong>：本研究通过文档级别评估DeepL和Supertext两个商业机器翻译系统，发现在大多数情况下的分段评估表现相近，但在四个语言方向中的三个方向中，Supertext在文档级别的表现更佳，展现出更优秀的长文本一致性。因此，本研究呼吁采用更语境化的评估方法来确保机器翻译的质量评估反映真实世界的实用性。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>本研究对比了DeepL和Supertext两个商业机器翻译系统，评估其在未分割文本上的表现。</li>
<li>在四个语言方向上进行了评估，涉及专业翻译人员对具有全文上下文的片段的评估。</li>
<li>在分段评估中，两个系统的表现相当。</li>
<li>在文档级别上，Supertext在三个语言方向上的表现优于DeepL，显示出其在长文本中的一致性更好。</li>
<li>研究强调了需要更语境化的评估方法来确保机器翻译的质量评估能够反映真实世界的实用性。</li>
<li>所有评估数据和脚本均已公开发布，以供进一步分析和复制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02577">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e134c8f7a8f126d5b3450f21e45fe868.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49dfe6b0e3cf294d1abef54ec607db1d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-891e2468bf58f14921f1657de4481927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0796ec379ea3c8a747ce52797f9f200.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LLMs-for-Generation-of-Architectural-Components-An-Exploratory-Empirical-Study-in-the-Serverless-World"><a href="#LLMs-for-Generation-of-Architectural-Components-An-Exploratory-Empirical-Study-in-the-Serverless-World" class="headerlink" title="LLMs for Generation of Architectural Components: An Exploratory   Empirical Study in the Serverless World"></a>LLMs for Generation of Architectural Components: An Exploratory   Empirical Study in the Serverless World</h2><p><strong>Authors:Shrikara Arun, Meghana Tedla, Karthik Vaidhyanathan</strong></p>
<p>Recently, the exponential growth in capability and pervasiveness of Large Language Models (LLMs) has led to significant work done in the field of code generation. However, this generation has been limited to code snippets. Going one step further, our desideratum is to automatically generate architectural components. This would not only speed up development time, but would also enable us to eventually completely skip the development phase, moving directly from design decisions to deployment. To this end, we conduct an exploratory study on the capability of LLMs to generate architectural components for Functions as a Service (FaaS), commonly known as serverless functions. The small size of their architectural components make this architectural style amenable for generation using current LLMs compared to other styles like monoliths and microservices. We perform the study by systematically selecting open source serverless repositories, masking a serverless function and utilizing state of the art LLMs provided with varying levels of context information about the overall system to generate the masked function. We evaluate correctness through existing tests present in the repositories and use metrics from the Software Engineering (SE) and Natural Language Processing (NLP) domains to evaluate code quality and the degree of similarity between human and LLM generated code respectively. Along with our findings, we also present a discussion on the path forward for using GenAI in architectural component generation. </p>
<blockquote>
<p>最近，随着大型语言模型（LLM）的能力和普及性的指数级增长，代码生成领域已经取得了重大进展。然而，这种生成仅限于代码片段。更进一步，我们的愿望是自动生成架构组件。这不仅可以加快开发时间，还将使我们能够最终完全跳过开发阶段，直接从设计决策转移到部署。为此，我们对LLM生成作为服务（FaaS）的架构组件的能力进行了探索性研究，FaaS通常被称为无服务器函数。与其他风格（如单体应用和微服务）相比，其架构组件的体积较小，使得这种架构风格更适合使用当前的LLM进行生成。我们通过系统地选择开源的无服务器仓库进行研究，对无服务器函数进行掩盖，并利用最先进的LLM技术，为总体系统提供不同级别的上下文信息来生成被掩盖的函数。我们通过仓库中现有的测试来评估正确性，并使用软件工程（SE）和自然语言处理（NLP）领域的指标来评估代码质量和人类与LLM生成代码之间的相似度。除了我们的发现之外，我们还就未来在架构组件生成中使用GenAI的路径进行了讨论。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02539v1">PDF</a> Accepted to IEEE International Conference on Software Architecture   (ICSA) 2025 Main Track (<a target="_blank" rel="noopener" href="https://conf.researchr.org/home/icsa-2025">https://conf.researchr.org/home/icsa-2025</a>)</p>
<p><strong>Summary</strong></p>
<p>近期，大型语言模型（LLM）的能力和普及性呈现指数级增长，代码生成领域的研究也取得了显著进展。然而，现有的代码生成主要集中在代码片段层面。本研究旨在进一步探索LLM自动生成架构组件的可能性。这项研究以函数即服务（FaaS）的架构风格为例，发现其架构组件较小，更适合使用当前的LLM进行生成。研究通过选择开源的无服务器存储库，对其中某个函数进行掩盖，并利用先进的LLM，在提供不同级别的系统上下文信息的情况下生成掩盖的函数。本研究通过现有测试评估正确性，并使用软件工程和自然语言处理领域的指标评估代码质量和LLM生成代码与人类生成代码的相似度。此外，还讨论了未来在架构组件生成领域使用GenAI的前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在代码生成领域取得显著进展，但主要集中在代码片段的生成。</li>
<li>研究旨在探索LLM自动生成架构组件的可能性，特别是在函数即服务（FaaS）领域。</li>
<li>FaaS的架构风格由于其较小的架构组件，更适合使用当前的LLM进行生成。</li>
<li>研究通过选择开源的无服务器存储库进行掩盖函数实验，并利用先进的LLM生成这些函数。</li>
<li>本研究评估了生成的正确性、代码质量以及LLM生成代码与人类生成代码的相似度。</li>
<li>研究使用的评估方法包括现有测试、软件工程和自然语言处理领域的指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02539">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8aec6f506a377f602b52a2b16bb33e73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0683e4b0630f79a6dad4c1885bc15c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-720af3f53a168a340c53852ffef60f3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c402d780efe7b8bf8ecd236c7cd03fed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-670ed1f7eeffd92c73d45e9178818596.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Satori-Reinforcement-Learning-with-Chain-of-Action-Thought-Enhances-LLM-Reasoning-via-Autoregressive-Search"><a href="#Satori-Reinforcement-Learning-with-Chain-of-Action-Thought-Enhances-LLM-Reasoning-via-Autoregressive-Search" class="headerlink" title="Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM   Reasoning via Autoregressive Search"></a>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM   Reasoning via Autoregressive Search</h2><p><strong>Authors:Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, Chuang Gan</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs’ reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced. </p>
<blockquote>
<p>大型语言模型（LLM）在各个领域表现出了惊人的推理能力。最近的研究表明，增加测试时的计算量可以提高LLM的推理能力。这通常涉及在推理时间时由外部LLM验证器引导的广泛采样，从而形成了一个双人系统。尽管有外部指导，但该系统的有效性展示了单个LLM解决复杂任务的潜力。因此，我们提出了一个新的研究问题：我们能否将搜索能力内在化，从而从根本上提高单个LLM的推理能力？这项工作探索了一个正交方向，专注于对训练后的LLM进行自回归搜索（即，具有自我反思和自我探索新策略的扩展推理过程）。为了实现这一点，我们提出了行动思维链（COAT）推理和两阶段训练范式：1）小规模格式调整阶段，以内在化COAT推理格式；2）利用强化学习的大规模自我改进阶段。我们的方法产生了Satori，一个以开源模型和数据训练的7B LLM。广泛的实证评估表明，Satori在数学推理基准测试上达到了最新技术水平，同时在域外任务中展现出了强大的泛化能力。代码、数据和模型将完全开源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02508v1">PDF</a> </p>
<p><strong>Summary</strong>：大型语言模型（LLM）具有跨域的强大推理能力。研究发现在测试时增加计算时间能提高LLM的推理能力，通常采用在推理时间进行大量采样并在外部LLM验证器的指导下完成，形成了一种两玩家系统。研究提出新的问题：能否将搜索能力内在化，从根本上提高单一LLM的推理能力？该研究探索了一个正交方向，专注于对LLM进行后训练以实现自主推理搜索（即具有自我反思和自我探索新策略的扩展推理过程）。通过提出的行动思维链（COAT）推理和两个阶段的训练范式，研究训练了一个名为Satori的7B LLM。全面实证评估表明，Satori在数学推理基准测试中达到了最新技术水平，并在域外任务中表现出强大的泛化能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLMs展现跨领域的强大推理能力。</li>
<li>增加测试时的计算时间可以提高LLM的推理能力。</li>
<li>外部验证器指导下的采样对LLM系统有效。</li>
<li>研究探索内在化搜索能力以提高单一LLM的推理能力的新方向。</li>
<li>COAT推理方法用于实现自主推理搜索。</li>
<li>Satori模型在两个阶段训练下表现优秀，并在数学推理基准测试中达到最新技术水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02508">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9552ecefcf2a4a6bbfc7eb1ba8aab501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be02399355b276134cd40cca534f64b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-032dcd9d607c4e81ee766cf6236ce6fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-477295ff18c1dbfedd8c11044e9593e8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multilingual-Machine-Translation-with-Open-Large-Language-Models-at-Practical-Scale-An-Empirical-Study"><a href="#Multilingual-Machine-Translation-with-Open-Large-Language-Models-at-Practical-Scale-An-Empirical-Study" class="headerlink" title="Multilingual Machine Translation with Open Large Language Models at   Practical Scale: An Empirical Study"></a>Multilingual Machine Translation with Open Large Language Models at   Practical Scale: An Empirical Study</h2><p><strong>Authors:Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan,  BinWang</strong></p>
<p>Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving top-tier multilingual translation performance across 28 languages. Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA) models such as TowerInstruct and XALMA and achieves competitive performance with Google Translate and GPT-4-turbo. </p>
<blockquote>
<p>大型语言模型（LLM）的多语言处理能力正在持续进步，即使是小型开源模型也表现出了快速性能的提升。在本文中，我们系统地探索了具有不到十亿参数的开源LLM在处理多语言机器翻译（MT）任务方面的能力。我们对六个流行的LLM进行了全面评估，发现Gemma2-9B等模型表现出令人印象深刻的多语言翻译能力。随后，我们在持续预训练阶段引入了Parallel-First Monolingual-Second（PFMS）数据混合策略，以进一步提高MT性能，并推出了GemmaX2-28，这是一款在28种语言上实现顶级多语言翻译性能的9B模型。具体而言，GemmaX2-28始终优于当前技术水平模型（SOTA），如TowerInstruct和XALMA，与Google Translate和GPT-4-turbo等顶尖模型表现相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02481v1">PDF</a> Accept to NAACL2025 Main Conference</p>
<p><strong>Summary</strong><br>LLM在多语种机器翻译任务中展现出强大的能力，通过小于十亿参数的开源模型即可完成高质量翻译。研究通过六种流行LLM的综合评估发现，如Gemma2-9B等模型具备出色的多语种翻译能力。采用Parallel-First Monolingual-Second（PFMS）数据混合策略进行持续预训练，成功推出GemmaX2-28模型，该模型在28种语言之间实现顶尖的多语种翻译性能，并超越现有先进技术，与Google Translate和GPT-4-turbo等模型表现相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在多语种机器翻译领域展现出强大的能力，即使是小规模开源模型也能实现高质量翻译。</li>
<li>Gemma2-9B等模型具备出色的多语种翻译能力，可处理多种语言间的翻译任务。</li>
<li>Parallel-First Monolingual-Second（PFMS）数据混合策略被用于增强机器翻译性能。</li>
<li>GemmaX2-28模型是一个9B参数模型，具备顶尖的多语种翻译性能，覆盖28种语言。</li>
<li>GemmaX2-28模型超越了现有先进技术，在多语种翻译领域表现优异。</li>
<li>GemmaX2-28模型的性能与Google Translate和GPT-4-turbo等先进模型相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-25a57e9857d9e451b56769acb35f4f09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeb944eda4d04e3d17bded5e134aa68f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5107970c9313d3d0f852d160c6ddf6f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61b387235b50bccd93990b2f1d816be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53f1c2c79bd40358885cb88e5df93d32.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SAISA-Towards-Multimodal-Large-Language-Models-with-Both-Training-and-Inference-Efficiency"><a href="#SAISA-Towards-Multimodal-Large-Language-Models-with-Both-Training-and-Inference-Efficiency" class="headerlink" title="SAISA: Towards Multimodal Large Language Models with Both Training and   Inference Efficiency"></a>SAISA: Towards Multimodal Large Language Models with Both Training and   Inference Efficiency</h2><p><strong>Authors:Qianhao Yuan, Yanjiang Liu, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun</strong></p>
<p>Multimodal Large Language Models (MLLMs) mainly fall into two architectures, each involving a trade-off between training and inference efficiency: embedding space alignment (e.g., LLaVA-1.5) is inefficient during inference, while cross-attention space alignment (e.g., Flamingo) is inefficient in training. In this paper, we compare these two architectures and identify the key factors for building efficient MLLMs. A primary difference between them lies in how attention is applied to visual tokens, particularly in their interactions with each other. To investigate whether attention among visual tokens is necessary, we propose a new self-attention mechanism, NAAViT (\textbf{N}o \textbf{A}ttention \textbf{A}mong \textbf{Vi}sual \textbf{T}okens), which eliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that attention among visual tokens is highly redundant. Based on these insights, we introduce SAISA (\textbf{S}elf-\textbf{A}ttention \textbf{I}nput \textbf{S}pace \textbf{A}lignment), a novel architecture that enhance both training and inference efficiency. SAISA directly aligns visual features with the input spaces of NAAViT self-attention blocks, reducing computational overhead in both self-attention blocks and feed-forward networks (FFNs). Using the same configuration as LLaVA-1.5, SAISA reduces inference FLOPs by 66% and training budget by 26%, while achieving superior performance in terms of accuracy. Comprehensive ablation studies further validate the effectiveness of SAISA across various LLMs and visual encoders. The code and model will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/icip-cas/SAISA">https://github.com/icip-cas/SAISA</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）主要可分为两种架构，每种架构在训练和推理效率之间都存在权衡：嵌入空间对齐（例如LLaVA-1.5）在推理时效率较低，而交叉注意力空间对齐（例如Flamingo）在训练时效率较低。在本文中，我们比较了这两种架构，并确定了构建高效MLLM的关键因素。它们之间的主要区别在于如何将注意力应用于视觉标记，特别是在它们之间的交互上。为了调查视觉标记之间的注意力是否必要，我们提出了一种新的自注意力机制NAAViT（\textbf{N}o \textbf{A}ttention \textbf{A}mong \textbf{Vi}sual \textbf{T}okens），该机制消除了这种类型的注意力。我们对LLaVA-1.5的初步实验表明，视觉标记之间的注意力高度冗余。基于这些见解，我们引入了SAISA（\textbf{S}elf-\textbf{A}ttention \textbf{I}nput \textbf{S}pace \textbf{A}lignment），这是一种新型架构，旨在提高训练和推理的效率。SAISA直接将视觉特征与NAAViT自注意力块的输入空间对齐，减少了自注意力块和前馈网络（FFNs）中的计算开销。与LLaVA-1.5相同的配置下，SAISA将推理FLOPs减少了66%，训练预算减少了26%，同时在准确性方面表现出卓越的性能。全面的消融研究进一步验证了SAISA在不同的大型语言模型和视觉编码器中的有效性。代码和模型将在<a target="_blank" rel="noopener" href="https://github.com/icip-cas/SAISA%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/icip-cas/SAISA公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02458v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文对比了多模态大型语言模型（MLLMs）的两种架构，发现它们在训练和推理效率上存在权衡。为此，本文提出了一种新的自注意力机制NAAViT，并基于此引入了SAISA架构，该架构在训练和推理时均具有较高的效率，并通过直接对齐视觉特征与NAAViT自注意力块的输入空间，减少了自注意力块和前馈网络（FFNs）的计算开销。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）主要有两种架构：嵌入空间对齐和跨注意力空间对齐，分别存在推理和训练效率上的权衡。</li>
<li>视觉标记之间的注意力对于大型语言模型的效率可能是冗余的。</li>
<li>本文提出了一种新的自注意力机制NAAViT，消除了视觉标记之间的注意力。</li>
<li>基于NAAViT，引入了SAISA架构，该架构直接对齐视觉特征与自注意力块的输入空间，提高了训练和推理效率。</li>
<li>SAISA架构在相同配置下相比LLaVA-1.5，推理FLOPs减少了66%，训练预算减少了26%，同时实现了更高的准确性。</li>
<li>综合的消融研究验证了SAISA在各种大型语言模型和视觉编码器中的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02458">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-40cd2b35afac6eac8c766af3fd2dd415.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad1b0e9d59f27ad30bc390e90030d8cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-923fd447d2f0d59c82e8fea3817dc467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67c4ed3d47a60b3d126b11e3d1c258e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-137dfe8d3f94c3c8a6af93eb53c1b55e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLMER-Crafting-Interactive-Extended-Reality-Worlds-with-JSON-Data-Generated-by-Large-Language-Models"><a href="#LLMER-Crafting-Interactive-Extended-Reality-Worlds-with-JSON-Data-Generated-by-Large-Language-Models" class="headerlink" title="LLMER: Crafting Interactive Extended Reality Worlds with JSON Data   Generated by Large Language Models"></a>LLMER: Crafting Interactive Extended Reality Worlds with JSON Data   Generated by Large Language Models</h2><p><strong>Authors:Jiangong Chen, Xiaoyi Wu, Tian Lan, Bin Li</strong></p>
<p>The integration of Large Language Models (LLMs) like GPT-4 with Extended Reality (XR) technologies offers the potential to build truly immersive XR environments that interact with human users through natural language, e.g., generating and animating 3D scenes from audio inputs. However, the complexity of XR environments makes it difficult to accurately extract relevant contextual data and scene&#x2F;object parameters from an overwhelming volume of XR artifacts. It leads to not only increased costs with pay-per-use models, but also elevated levels of generation errors. Moreover, existing approaches focusing on coding script generation are often prone to generation errors, resulting in flawed or invalid scripts, application crashes, and ultimately a degraded user experience. To overcome these challenges, we introduce LLMER, a novel framework that creates interactive XR worlds using JSON data generated by LLMs. Unlike prior approaches focusing on coding script generation, LLMER translates natural language inputs into JSON data, significantly reducing the likelihood of application crashes and processing latency. It employs a multi-stage strategy to supply only the essential contextual information adapted to the user’s request and features multiple modules designed for various XR tasks. Our preliminary user study reveals the effectiveness of the proposed system, with over 80% reduction in consumed tokens and around 60% reduction in task completion time compared to state-of-the-art approaches. The analysis of users’ feedback also illuminates a series of directions for further optimization. </p>
<blockquote>
<p>将大型语言模型（如GPT-4）与扩展现实（XR）技术的集成，有望构建真正沉浸式XR环境，通过自然语言与人类用户进行交互，例如从音频输入生成和动画化3D场景。然而，XR环境的复杂性使得从大量XR文物中准确提取相关上下文数据和环境&#x2F;对象参数变得困难。这不仅增加了按使用付费模型的成本，还提高了生成错误的水平。此外，现有专注于编码脚本生成的方法往往容易出现生成错误，导致有缺陷或无效的脚本、应用程序崩溃，并最终降低用户体验。为了克服这些挑战，我们介绍了LLMER，这是一个使用LLM生成JSON数据创建交互式XR世界的新型框架。与以往专注于编码脚本生成的方法不同，LLMER将自然语言输入转换为JSON数据，大大降低了应用程序崩溃和处理延迟的可能性。它采用多阶段策略，只提供适应于用户请求的必要上下文信息，并设有针对各种XR任务设计的多个模块。我们的初步用户研究表明，与最新方法相比，所提出系统的有效性体现在消耗的令牌减少了80%以上，任务完成时间减少了约60%。用户反馈的分析还揭示了一系列进一步优化的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02441v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）如GPT-4与扩展现实（XR）技术的融合，构建真正沉浸式、可通过自然语言互动的XR环境具有巨大潜力。然而，XR环境的复杂性导致从大量XR伪影中提取相关上下文数据和场景&#x2F;对象参数变得困难。这增加了按使用付费模型的成本并提高了生成错误的水平。现有方法主要关注脚本生成，容易出现生成错误，导致脚本缺陷、应用程序崩溃并最终降低用户体验。为解决这些挑战，我们引入了LLMER框架，该框架使用LLM生成的JSON数据创建交互式XR世界。它采用多阶段策略，仅提供适应于用户请求的必要上下文信息，并为各种XR任务设计多个模块。初步用户研究表明，该系统效果显著，与现有方法相比，消耗的令牌减少了80%，任务完成时间减少了约60%。用户反馈分析还为进一步优化指明了方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM与XR技术的结合可创建真正沉浸式、自然语言互动的XR环境。</li>
<li>XR环境的复杂性导致提取相关数据的困难，增加成本和生成错误。</li>
<li>现有方法主要关注脚本生成，容易出错，影响用户体验。</li>
<li>LLMER框架使用LLM生成的JSON数据创建交互式XR世界，减少应用崩溃和处理延迟。</li>
<li>LLMER采用多阶段策略提供必要上下文信息，并为XR任务设计多个模块。</li>
<li>初步用户研究表明，LLMER系统效果显著，大幅减少消耗资源并提升效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02441">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5309cc9c11d876c7723e0ba007fc9503.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97898513ae4674906f4dfdeb642b34f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13ac128983f2a1727b0e63f1a251d833.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1643a5ddc31f9b298989bdaff7df65c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fe7173ccf2a899b1ceacd6c9679ae85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-297b3c8f2070efe61cefa5c1487dba1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdc8b8ba889bbfee29bd746873376365.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Medical-Multimodal-Model-Stealing-Attacks-via-Adversarial-Domain-Alignment"><a href="#Medical-Multimodal-Model-Stealing-Attacks-via-Adversarial-Domain-Alignment" class="headerlink" title="Medical Multimodal Model Stealing Attacks via Adversarial Domain   Alignment"></a>Medical Multimodal Model Stealing Attacks via Adversarial Domain   Alignment</h2><p><strong>Authors:Yaling Shen, Zhixiong Zhuang, Kun Yuan, Maria-Irina Nicolae, Nassir Navab, Nicolas Padoy, Mario Fritz</strong></p>
<p>Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-STEAL), the first stealing attack against medical MLLMs. ADA-STEAL relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data. </p>
<blockquote>
<p>医疗多模态大型语言模型（MLLMs）正在成为医疗保健系统的重要组成部分，协助医疗人员进行决策和结果分析。用于生成放射学报告的模型能够解释医学图像，从而减轻放射科医师的工作量。由于医疗数据稀缺且受隐私法规的保护，医疗MLLMs代表了宝贵的智力财产。然而，这些资产可能面临模型窃取的风险，攻击者试图通过黑箱访问来复制其功能。迄今为止，医疗领域的模型窃取主要集中在分类任务上，但现有的攻击对MLLMs并不有效。在本文中，我们介绍了针对医疗MLLMs的首个窃取攻击——对抗域对齐（ADA-STEAL）。ADA-STEAL依赖于公开且广泛可用的自然图像，而非医疗图像。我们表明，通过添加对抗噪声进行数据增强足以克服自然图像与受害者MLLM的特定领域分布之间的数据分布差距。在IU X射线以及MIMIC-CXR放射学数据集上的实验表明，对抗域对齐使攻击者无需访问任何医疗数据即可窃取医疗MLLM。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02438v1">PDF</a> Accepted at AAAI 2025</p>
<p><strong>Summary</strong><br>医疗多模态大型语言模型（MLLMs）在医疗保健系统中发挥着重要作用，协助医疗人员进行决策和结果分析。模型可用于生成放射学报告，解读医学图像，从而减轻放射科医师的工作量。由于医疗数据稀缺且受隐私法规保护，医疗MLLMs成为重要的知识产权。然而，这些资产可能面临模型窃取的风险，攻击者通过黑箱访问试图复制其功能。尽管针对医疗领域的模型窃取已有研究，但现有攻击对MLLMs并不有效。本文介绍了一种针对医疗MLLMs的窃取攻击——对抗域对齐（ADA-STEAL）。ADA-STEAL依赖于公开且广泛可用的自然图像，克服了自然图像与受害者MLLM的特定领域分布之间的数据分布差距。在IU X-RAY和MIMIC-CXR放射学数据集上的实验表明，对抗域对齐使攻击者能够在无需访问医疗数据的情况下窃取医疗MLLM。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗多模态大型语言模型（MLLMs）在医疗保健系统中起重要作用，辅助决策和结果分析。</li>
<li>MLLMs能够生成放射学报告，减轻放射科医师的工作负担。</li>
<li>医疗数据因其稀缺性和隐私保护而具有知识产权价值。</li>
<li>MLLMs面临模型窃取风险，攻击者试图通过黑箱访问复制其功能。</li>
<li>现有攻击对MLLMs效果不佳，需要新的攻击方法。</li>
<li>引入了一种新的针对医疗MLLMs的窃取攻击方法——对抗域对齐（ADA-STEAL）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02438">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fd6d59ed3da1f14128b437f75a369c08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76bdec8905bda7c4423a6013ebb9f5ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a8e12f8702934fb75d56f7af98a0842.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b8bac7d4eddee4ec3cf16f63d763390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dedd89a79d4ce15d55537342ef4f32d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e46681d2d988c00397279d28fe083a71.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Flatten-Graphs-as-Sequences-Transformers-are-Scalable-Graph-Generators"><a href="#Flatten-Graphs-as-Sequences-Transformers-are-Scalable-Graph-Generators" class="headerlink" title="Flatten Graphs as Sequences: Transformers are Scalable Graph Generators"></a>Flatten Graphs as Sequences: Transformers are Scalable Graph Generators</h2><p><strong>Authors:Dexiong Chen, Markus Krimmel, Karsten Borgwardt</strong></p>
<p>We introduce AutoGraph, a novel autoregressive framework for generating large attributed graphs using decoder-only transformers. At the core of our approach is a reversible “flattening” process that transforms graphs into random sequences. By sampling and learning from these sequences, AutoGraph enables transformers to model and generate complex graph structures in a manner akin to natural language. In contrast to diffusion models that rely on computationally intensive node features, our approach operates exclusively on these sequences. The sampling complexity and sequence length scale linearly with the number of edges, making AutoGraph highly scalable for generating large sparse graphs. Empirically, AutoGraph achieves state-of-the-art performance across diverse synthetic and molecular graph generation benchmarks, while delivering a 100-fold generation and a 3-fold training speedup compared to leading diffusion models. Additionally, it demonstrates promising transfer capabilities and supports substructure-conditioned generation without additional fine-tuning. By extending language modeling techniques to graph generation, this work paves the way for developing graph foundation models. </p>
<blockquote>
<p>我们介绍了AutoGraph，这是一个利用仅解码器转换器生成大型属性图的新型自回归框架。我们的方法的核心是一个可逆的“展平”过程，该过程将图转换为随机序列。通过从这些序列中进行采样和学习，AutoGraph能够让转换器以类似于自然语言的方式对复杂的图结构进行建模和生成。与依赖计算密集型节点特征的扩散模型相比，我们的方法仅在这些序列上运行。采样复杂性和序列长度与边的数量呈线性关系，这使得AutoGraph在生成大型稀疏图时具有高度的可扩展性。经验上，AutoGraph在多种合成和图分子生成基准测试中达到了最先进的性能，与领先的扩散模型相比，生成速度提高了100倍，训练速度提高了3倍。此外，它显示出有希望的迁移能力，并支持子结构条件下的生成而无需额外的微调。通过将语言建模技术扩展到图形生成，这项工作为开发图形基础模型铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02216v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AutoGraph是一种新型的基于自动回归框架的图生成方法，使用仅解码器变换器将图转化为随机序列进行生成。该方法采用可逆的“扁平化”过程，能够在计算效率和生成大规模稀疏图方面表现出高可扩展性。AutoGraph在合成和分子图生成基准测试中达到最佳性能，相比领先的扩散模型实现了百倍生成速度和三倍训练速度提升，并展现出良好的迁移能力和子结构条件下的生成能力。此工作将语言建模技术扩展到图生成领域，为图基础模型的发展铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AutoGraph是一种基于自动回归框架的图生成方法。</li>
<li>使用可逆的“扁平化”过程将图转化为随机序列进行生成。</li>
<li>采样和从这些序列中学习，使变换器能够生成复杂的图结构。</li>
<li>与依赖计算密集型节点特征的扩散模型不同，AutoGraph仅在这些序列上操作。</li>
<li>采样复杂度和序列长度与边的数量线性相关，使得AutoGraph在生成大规模稀疏图时具有高度的可扩展性。</li>
<li>AutoGraph在多个合成和分子图生成基准测试中达到最佳性能，并且相比其他领先模型具有显著的速度优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02216">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dc16ccc200eace613a88281cd83e68df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9273d663856c74e97f11e4772d2ddd4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79cd3c75dcb1f437919da0be82971ef0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d81164705974fe7264d525883997eb05.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mass-Editing-Memory-with-Attention-in-Transformers-A-cross-lingual-exploration-of-knowledge"><a href="#Mass-Editing-Memory-with-Attention-in-Transformers-A-cross-lingual-exploration-of-knowledge" class="headerlink" title="Mass-Editing Memory with Attention in Transformers: A cross-lingual   exploration of knowledge"></a>Mass-Editing Memory with Attention in Transformers: A cross-lingual   exploration of knowledge</h2><p><strong>Authors:Daniel Tamayo, Aitor Gonzalez-Agirre, Javier Hernando, Marta Villegas</strong></p>
<p>Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at <a target="_blank" rel="noopener" href="https://github.com/dtamayo-nlp/MEMAT">https://github.com/dtamayo-nlp/MEMAT</a>. </p>
<blockquote>
<p>近期研究探讨了更新和修改大型语言模型中事实知识的方法，通常聚焦于特定的多层感知器块。本研究在此基础上，考察了现有知识编辑方法在不同语言中的有效性，并深入探讨了注意力机制在这一过程中的作用。根据所获得的见解，我们提出了基于Transformers的注意力大规模编辑内存（MEMAT）方法，该方法在所有指标上都取得了重大改进，同时需要的参数修改很少。MEMAT在幅度指标上实现了惊人的10%的提升，对训练数据未包含的语言也有好处，并表现出很高的可移植性。我们的代码和数据位于<a target="_blank" rel="noopener" href="https://github.com/dtamayo-nlp/MEMAT%E3%80%82">https://github.com/dtamayo-nlp/MEMAT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02173v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了在大型语言模型中更新和修改事实知识的方法，重点探讨了跨语言的现有知识编辑方法的有效性，并深入探讨了注意力机制在这一过程中的作用。基于这些见解，提出了基于注意力机制的变压器大规模编辑内存（MEMAT）方法，该方法在各项指标上取得了显著改进，同时需要最少的参数修改。MEMAT实现了幅度指标的显著增长，并有益于未包含在训练数据中的语言，还表现出高度的可移植性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了大型语言模型中更新和修改事实知识的方法，特别是多层感知器块的特定方法。</li>
<li>探讨了跨语言的现有知识编辑方法的有效性。</li>
<li>深入研究了注意力机制在知识编辑过程中的作用。</li>
<li>提出了基于注意力机制的变压器大规模编辑内存（MEMAT）方法。</li>
<li>MEMAT在各项指标上取得了显著改进。</li>
<li>MEMAT实现了幅度指标的10%增长。</li>
<li>MEMAT有益于未包含在训练数据中的语言，表现出高度的可移植性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02173">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd156f87060dab520f1c947c312329a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18bfce4ef90b849e017f986b2590e4f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06c84f1cc4c7d08c5d3c9d896ae39cb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31a6c960ecbf83b856f155a7c40cddff.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="M2R2-Mixture-of-Multi-Rate-Residuals-for-Efficient-Transformer-Inference"><a href="#M2R2-Mixture-of-Multi-Rate-Residuals-for-Efficient-Transformer-Inference" class="headerlink" title="M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer   Inference"></a>M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer   Inference</h2><p><strong>Authors:Nikhil Bhendawade, Mahyar Najibi, Devang Naik, Irina Belousova</strong></p>
<p>Residual transformations enhance the representational depth and expressive power of large language models (LLMs). However, applying static residual transformations across all tokens in auto-regressive generation leads to a suboptimal trade-off between inference efficiency and generation fidelity. Existing methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth address this by modulating the residual transformation based on token-level complexity. Nevertheless, these approaches predominantly consider the distance traversed by tokens through the model layers, neglecting the underlying velocity of residual evolution. We introduce Mixture of Multi-rate Residuals (M2R2), a framework that dynamically modulates residual velocity to improve early alignment, enhancing inference efficiency. Evaluations on reasoning oriented tasks such as Koala, Self-Instruct, WizardLM, and MT-Bench show M2R2 surpasses state-of-the-art distance-based strategies, balancing generation quality and speedup. In self-speculative decoding setup, M2R2 achieves up to 2.8x speedups on MT-Bench, outperforming methods like 2-model speculative decoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE) architectures, integrating early residual alignment with ahead-of-time expert loading into high-bandwidth memory (HBM) accelerates decoding, reduces expert-switching bottlenecks, and achieves a 2.9x speedup, making it highly effective in resource-constrained environments. </p>
<blockquote>
<p>残差转换增强了大型语言模型（LLM）的表示深度和表达能力。然而，在自回归生成中，对所有令牌应用静态残差转换会导致推理效率和生成保真度之间的次优权衡。现有方法，包括早期退出、跳过解码和深度混合等，通过根据令牌级复杂度调制残差转换来解决这一问题。然而，这些方法主要考虑令牌通过模型层所穿越的距离，而忽略了残差演化的内在速度。我们引入了多速率残差混合（M2R2）框架，它动态调制残差速度，以改进早期对齐，提高推理效率。在面向推理的任务（如Koala、Self-Instruct、WizardLM和MT-Bench）上的评估表明，M2R2超越了基于距离的最新策略，平衡了生成质量和速度。在自我推测解码设置中，M2R2在MT-Bench上实现了高达2.8倍的加速，优于诸如双模型推测解码、Medusa、超前解码和DEED等方法。在专业混合（MoE）架构中，将早期残差对齐与提前的专家加载到高速缓冲存储器（HBM）相结合，可以加速解码，减少专家切换瓶颈，实现2.9倍的加速，使其在资源受限的环境中非常有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02040v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>残差转换能增强大语言模型的表达深度和能力。然而，在自回归生成中，对所有token应用静态残差转换会在推理效率和生成保真度之间产生权衡。为解决这一问题，现有方法通过基于token级别复杂度调整残差转换来实现优化。本文提出Mixture of Multi-rate Residuals（M2R2）框架，它通过动态调整残差速度来改进早期对齐，从而提高推理效率。在面向任务的评估中，如Koala、Self-Instruct、WizardLM和MT-Bench等任务上，M2R2超越了基于距离的策略，平衡了生成质量和速度。在自我探索解码设置中，M2R2在MT-Bench上实现了高达2.8倍的加速，优于其他方法。在混合专家架构中，将早期残差对齐与预先加载专家到高速缓存存储器相结合，可以加速解码，减少专家切换瓶颈，实现2.9倍的加速。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>残差转换对于提高大语言模型的表达深度和能力至关重要。</li>
<li>对所有token应用静态残差转换会导致推理效率和生成保真度之间的权衡。</li>
<li>现有方法通过基于token级别复杂度调整残差转换进行优化。</li>
<li>M2R2框架通过动态调整残差速度改进早期对齐，提高推理效率。</li>
<li>M2R2在多种面向任务的评估中超越基于距离的策略，平衡生成质量和速度。</li>
<li>在自我探索解码设置中，M2R2实现显著加速。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e69dc3ae406b928a417f7d88e5d3e0dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d1950b66a7a58e0b2dea5b1a0417f27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebcc52f45219956fe0c58400fd65cfd5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a297493906d70b968605ec6f123a6bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6565d95ae655a6313dbfbe7ea09b7ff.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BARE-Combining-Base-and-Instruction-Tuned-Language-Models-for-Better-Synthetic-Data-Generation"><a href="#BARE-Combining-Base-and-Instruction-Tuned-Language-Models-for-Better-Synthetic-Data-Generation" class="headerlink" title="BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation"></a>BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation</h2><p><strong>Authors:Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia</strong></p>
<p>As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs-a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT. </p>
<blockquote>
<p>随着模型训练中对于高质量数据需求的增长，研究者和开发者正在越来越多地生成合成数据以调整和优化大型语言模型（LLM）。关于合成数据的常见假设是，从经过指令调整（instruct-tuned）的模型中进行采样就足够了。然而，这些模型在生成多样化输出方面存在困难，这是实现泛化的关键要求。尽管有各种提示方法，但在这项工作中我们表明，从经过指令调整的模型中实现有意义的多样性仍然具有挑战性。相比之下，我们发现没有经过后训练的基准模型表现出更大的多样性，但在遵循指令方面的能力较弱，因此质量较低。基于这一发现，我们提出了Base-Refine（BARE）方法，这是一种合成数据生成方法，它通过两阶段过程结合基准模型的多样性和经过指令调整模型的质量。通过少量示例和筛选，BARE能够生成多样且高质量的数据集，提高下游任务性能。我们展示，使用BARE生成的样本仅需进行微调至多1000个样本即可达到LiveCodeBench任务上表现最佳的类似规模模型的性能水平。此外，使用BARE生成的数据进行微调在GSM8K数据集上的表现比仅使用指令数据提高了101%，在RAFT数据集上的表现比现有最佳方法提高了18.4%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01697v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究探讨了随着模型训练中对高质量数据需求的增长，研究人员和开发人员如何利用合成数据来调整和训练大型语言模型（LLM）。研究发现，尽管采用各种提示方法，指令调整模型在生成合成数据时仍难以实现有意义的多样性。相反，基础模型展现出更大的多样性，但在指令遵循能力方面较差。基于此，该研究提出了一种名为Base-Refine（BARE）的合成数据生成方法，该方法通过两阶段过程结合基础模型的多样性和指令调整模型的质量。利用少量的示例和筛选，BARE能够生成多样且高质量的数据集，提高下游任务性能。研究结果表明，使用BARE生成的数据进行微调，在LiveCodeBench任务上的性能可与最佳相似规模模型相当，并且在GSM8K上相对于仅使用指令数据实现了101%的改进，在RAFT上相对于最新方法实现了18.4%的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究关注合成数据在训练LLM中的作用，发现指令调整模型在生成多样输出方面存在挑战。</li>
<li>基础模型展现出更大的多样性，但在指令遵循能力方面较差。</li>
<li>提出了Base-Refine（BARE）方法，结合基础模型的多样性和指令调整模型的质量。</li>
<li>BARE通过两阶段过程生成多样且高质量的数据集，提高下游任务性能。</li>
<li>使用BARE生成的数据进行微调，性能可与最佳模型相当。</li>
<li>BARE在多个任务上实现了显著的性能改进，相对于仅使用指令数据在GSM8K上实现了101%的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01697">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c0281da528061c674534a3eec671a51a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2aedf490ff7ee0cbb3ff7b1063b933f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ecc96ecda174238e70bc680753559e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81d93849a1a7b4bb94a319e97c5973c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a2a1fce24198ceb9f7f7f2a168d9850.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Longer-Attention-Span-Increasing-Transformer-Context-Length-with-Sparse-Graph-Processing-Techniques"><a href="#Longer-Attention-Span-Increasing-Transformer-Context-Length-with-Sparse-Graph-Processing-Techniques" class="headerlink" title="Longer Attention Span: Increasing Transformer Context Length with Sparse   Graph Processing Techniques"></a>Longer Attention Span: Increasing Transformer Context Length with Sparse   Graph Processing Techniques</h2><p><strong>Authors:Nathaniel Tomczak, Sanmukh Kuppannagari</strong></p>
<p>Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input’s context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve “true sparsity” are lacking.   In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB). </p>
<blockquote>
<p>Transformer在自然语言处理和生物信息学等多个领域取得了巨大成功。这一成功源于这些模型采用注意力机制，以表示并传播序列数据中各个标记之间的配对交互作用。然而，这项操作的主要局限性在于其相对于输入上下文长度的二次内存和时间复杂度——需要捕获交互作用的序列长度。这极大地限制了这些模型可以推断的序列长度。为了将配对交互的数量减少到相对于上下文长度的次二次，已经进行了大量研究，通过在注意力机制中引入稀疏性来开发稀疏注意力掩码。然而，实现“真正稀疏性”的高效实现仍然缺乏。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01659v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了Transformer模型在利用注意力机制进行序列数据的标记间相互作用表示和传播方面的成功应用，但面临输入上下文长度引起的二次内存和时间复杂度限制。为解决此问题，本文提出一种基于图的注意力计算观点，通过图形处理算法实现注意力机制，仅执行必需的计算以实现高效工作。同时进行了广泛实验，与最新注意力实现方法相比，大幅提高了执行速度，并在单个NVIDIA A100 GPU上实现了高达1亿六千万的极长序列长度。此技术为处理长序列数据提供了有力支持。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer模型的成功得益于其利用注意力机制来捕捉序列数据的标记间相互作用的能力。</li>
<li>Transformer模型面临的主要限制是二次内存和时间复杂度，限制了可以推断的序列长度。</li>
<li>为解决此问题，引入稀疏注意力掩码以减少成对交互的数量。然而，实现真正的稀疏性仍然是一个挑战。</li>
<li>本文提出了一种基于图的注意力计算观点，将标记视为图的节点，注意力掩码确定图的边缘。</li>
<li>利用图形处理算法实现注意力机制，仅执行必需的计算以实现高效工作。</li>
<li>与现有技术相比，该技术显著提高了执行速度，并在单个GPU上实现了极长的序列长度处理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01659">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-66e0b2a3c1e97f3bff9abb727fb3c3bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-974f422fbac37f20b4a9f0e696ea183b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4fd3e28bce1a9a9608a254ef91291da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd52538dbf8f8420c2ca6e277e6fa44d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e77d89af2861289a5442841628f9a410.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-967394e4f57a94c4711387061765e038.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80fa2dd79c6e7116485d5360d1a61723.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Probabilistic-Inference-Approach-to-Inference-Time-Scaling-of-LLMs-using-Particle-Based-Monte-Carlo-Methods"><a href="#A-Probabilistic-Inference-Approach-to-Inference-Time-Scaling-of-LLMs-using-Particle-Based-Monte-Carlo-Methods" class="headerlink" title="A Probabilistic Inference Approach to Inference-Time Scaling of LLMs   using Particle-Based Monte Carlo Methods"></a>A Probabilistic Inference Approach to Inference-Time Scaling of LLMs   using Particle-Based Monte Carlo Methods</h2><p><strong>Authors:Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, Akash Srivastava</strong></p>
<p>Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and&#x2F;or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code and further information is available at <a target="_blank" rel="noopener" href="https://probabilistic-inference-scaling.github.io/">https://probabilistic-inference-scaling.github.io</a>. </p>
<blockquote>
<p>大规模语言模型（LLM）通过扩大模型规模和数据量取得了显著的性能提升。然而，最近的证据表明，从这些方法中获得的收益正在减少，因此激发了在推理时间消耗的计算上进行扩展的动机。现有的推理时间扩展方法通常使用奖励模型，将任务转化为搜索问题，这往往容易受到奖励模型的近似误差导致的奖励破解的影响。在本文中，我们将推理时间扩展转变为概率推理任务，并利用采样技术探索状态空间模型的典型状态分布。我们提出了一种新的推理时间扩展方法，即通过基于粒子的蒙特卡洛方法适应这一任务。我们的经验评估表明，在各种具有挑战性的数学推理任务上，我们的方法的扩展率是确定性搜索方法的4-16倍。使用我们的方法，我们证明了Qwen2.5-Math-1.5B-Instruct在仅4次迭代中就能超越GPT-4o的准确性，而Qwen2.5-Math-7B-Instruct在仅32次迭代中就能达到o1级别的准确性。我们的工作不仅提出了一种有效的推理时间扩展方法，还将概率推理的丰富文献与LLM的推理时间扩展相结合，为未来工作开发更稳健的算法提供了基础。相关代码和更多信息请访问：<a target="_blank" rel="noopener" href="https://probabilistic-inference-scaling.github.io./">https://probabilistic-inference-scaling.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01618v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过扩大模型规模或数据量取得了显著的性能提升。然而，现有方法易产生边际效益递减现象，为此提出了通过计算资源进行推断时间的缩放作为解决思路。现有的推断时间缩放方法，倾向于把任务作为搜索问题并利用奖励模型来处理，但由于奖励模型的近似误差容易遭受奖励作弊的影响。本文提出一种基于概率推断的推断时间缩放方法，利用采样技术探索状态空间模型的典型状态分布近似概率，而非直接优化其模态。通过粒子蒙特卡洛方法的改进适用于该任务，本文方法在多种数学推理任务上实现了4-16倍的缩放率提升。使用此方法，Qwen2.5-Math-1.5B-Instruct 仅需4次迭代即可超越GPT-4o的准确度，而Qwen2.5-Math-7B-Instruct在仅32次迭代时即可达到O1级别精度。本文不仅提供了一种有效的推断时间缩放方法，还将概率推断与LLM的推断时间缩放联系起来，为未来开发更稳健的算法打下基础。更多信息及代码请参见：<a target="_blank" rel="noopener" href="https://probabilistic-inference-scaling.github.io/">https://probabilistic-inference-scaling.github.io</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）面临性能提升边际效益递减问题，需要寻找新的解决方案。</li>
<li>现有推断时间缩放方法易遭受奖励作弊影响，倾向于把任务作为搜索问题处理。</li>
<li>本文提出基于概率推断的推断时间缩放方法，探索状态空间模型的典型状态分布。</li>
<li>采用粒子蒙特卡洛方法的改进实现，在多种数学推理任务上实现显著性能提升。</li>
<li>Qwen2.5系列模型在仅数次迭代后便能达到较高的准确性水平。</li>
<li>本文不仅提供了一种有效的推断时间缩放方法，也为未来开发更稳健的算法提供了联系概率推断与LLM的推断时间缩放的视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01618">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f4469260c4c7acde26a0baf093d8bec5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92d325cfb140bc935078b8388fc7f3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9ff6cf4c7e0ecd9490d6eb9fd95d367.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c55419ab83ae46e8bf56f5a07927aca6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-for-Long-Horizon-Interactive-LLM-Agents"><a href="#Reinforcement-Learning-for-Long-Horizon-Interactive-LLM-Agents" class="headerlink" title="Reinforcement Learning for Long-Horizon Interactive LLM Agents"></a>Reinforcement Learning for Long-Horizon Interactive LLM Agents</h2><p><strong>Authors:Kevin Chen, Marco Cusumano-Towner, Brody Huval, Aleksei Petrenko, Jackson Hamburger, Vladlen Koltun, Philipp Krähenbühl</strong></p>
<p>Interactive digital agents (IDAs) leverage APIs of stateful digital environments to perform tasks in response to user requests. While IDAs powered by instruction-tuned large language models (LLMs) can react to feedback from interface invocations in multi-step exchanges, they have not been trained in their respective digital environments. Prior methods accomplish less than half of tasks in sophisticated benchmarks such as AppWorld. We present a reinforcement learning (RL) approach that trains IDAs directly in their target environments. We formalize this training as a partially observable Markov decision process and derive LOOP, a data- and memory-efficient variant of proximal policy optimization. LOOP uses no value network and maintains exactly one copy of the underlying LLM in memory, making its implementation straightforward and as memory-efficient as fine-tuning a single LLM. A 32-billion-parameter agent trained with LOOP in the AppWorld environment outperforms the much larger OpenAI o1 agent by 9 percentage points (15% relative). To our knowledge, this is the first reported application of RL to IDAs that interact with a stateful, multi-domain, multi-app environment via direct API calls. Our analysis sheds light on the effectiveness of RL in this area, showing that the agent learns to consult the API documentation, avoid unwarranted assumptions, minimize confabulation, and recover from setbacks. </p>
<blockquote>
<p>交互式数字代理（IDAs）利用有状态数字环境的API来执行用户请求的任务。虽然由指令调优的大型语言模型（LLM）驱动的IDAs可以响应界面调用中的反馈进行多步骤交互，但它们并没有在其各自的数字环境中接受过训练。之前的方法在完成像AppWorld这样的复杂基准测试任务时，成功率不到一半。我们提出了一种强化学习（RL）的方法，该方法直接在目标环境中训练IDAs。我们将这种训练形式化为部分可观测的马尔可夫决策过程，并推导出LOOP，这是近端策略优化的一种数据和内存效率高的变体。LOOP不使用价值网络，并且在内存中只保留一个底层LLM的副本，使其实现简单且内存使用效率与微调单个LLM相当。在AppWorld环境中使用LOOP训练的32亿参数代理比OpenAI o1代理高出9个百分点（相对提高15%）。据我们所知，这是首次报告将强化学习应用于IDAs，这些代理通过直接的API调用与有状态、多领域、多应用程序环境进行交互。我们的分析揭示了强化学习在这个领域的有效性，显示代理学会了参考API文档，避免不必要的假设，尽量减少虚构，并从挫折中恢复。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01600v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于交互式数字代理（IDA）利用状态化数字环境的API执行任务以响应用户请求的特点，提出一种利用强化学习（RL）训练IDA的方法。该方法将数据内存高效且精准地定义为部分观察马尔可夫决策过程（POMDP），并提出LOOP算法，该算法是一种基于近端策略优化的变体。LOOP无需价值网络，仅维护一个底层大型语言模型（LLM）的副本，使其实现简单且内存高效。在AppWorld环境中训练的包含32亿参数的代理性能超过了OpenAI o1代理，相对提高了约9个百分点。这是首次将强化学习应用于交互式数字代理领域，该代理通过直接API调用与状态化、多领域、多应用程序环境进行交互。本研究分析揭示了强化学习在该领域的有效性，展示代理学会了查阅API文档、避免无效假设、减少混淆并从挫折中恢复的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IDAs利用API执行用户请求的任务。</li>
<li>当前方法在执行复杂任务时的成功率低于一半。</li>
<li>提出一种基于强化学习的训练IDA的方法，直接在目标环境中进行训练。</li>
<li>LOOP算法是一种数据内存高效的近端策略优化变体。</li>
<li>LOOP无需价值网络，只维护一个LLM副本，实现简单且内存高效。</li>
<li>在AppWorld环境中训练的代理性能超过OpenAI o1代理约9个百分点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01600">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2d74ed148876ebd8d5cdf7898bafc004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee14e63dda5bfb2f34608be0f157415a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-851ebeb82e6d77658d36b5c970d64faf.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="On-the-Robustness-of-Temporal-Factual-Knowledge-in-Language-Models"><a href="#On-the-Robustness-of-Temporal-Factual-Knowledge-in-Language-Models" class="headerlink" title="On the Robustness of Temporal Factual Knowledge in Language Models"></a>On the Robustness of Temporal Factual Knowledge in Language Models</h2><p><strong>Authors:Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé</strong></p>
<p>This paper explores the temporal robustness of language models (LMs) in handling factual knowledge. While LMs can often complete simple factual statements, their ability to manage temporal facts (those valid only within specific timeframes) remains uncertain. We design a controlled experiment to test the robustness of temporal factual knowledge inside LMs, which we use to evaluate several pretrained and instruction-tuned models using prompts on popular Wikidata facts, assessing their performance across different temporal granularities (Day, Month, and Year). Our findings indicate that even very large state-of-the-art models, such as Llama-3.1-70B, vastly lack robust knowledge of temporal facts. In addition, they are incapable of generalizing their knowledge from one granularity to another. These results highlight the inherent limitations of using LMs as temporal knowledge bases. The source code and data to reproduce our experiments will be released. </p>
<blockquote>
<p>本文探讨了语言模型（LMs）在处理事实知识时的时序稳健性。虽然语言模型通常可以完成简单的陈述，但它们处理时序事实（仅在特定时间范围内有效的事实）的能力仍然不确定。我们设计了一个受控实验来测试语言模型内部时序事实知识的稳健性，我们使用此实验来评估使用流行Wikidata事实提示的多个预训练和指导模型的性能，并评估它们在不同的时间粒度（日、月和年）上的表现。我们的研究结果表明，即使是最新、最大的模型，如Llama-3.1-70B，对时序事实的知识稳健性也严重不足。此外，它们无法将知识从一个粒度推广到另一个粒度。这些结果突显了将语言模型用作临时知识库的内在局限性。我们将发布用于重现我们实验的源代码和数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01220v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文探讨了语言模型在处理事实知识时的时序稳健性。虽然语言模型通常可以完成简单的陈述，但它们处理时效性事实（仅在特定时间范围内有效的事实）的能力尚不确定。本文通过设计实验测试了语言模型内部对时效性事实稳健性的评估，使用针对流行Wikidata事实的提示来评估多个预训练和指导调整模型的性能，评估其在不同时间粒度（日、月和年）的表现。研究发现，即使是最大的最先进的模型，如Llama-3.1-70B，对时效性知识的稳健性也严重不足。此外，它们无法将知识从一个粒度推广到另一个粒度。这些结果突显了将语言模型用作临时知识库的内在局限性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>语言模型在处理事实知识时，尤其是时效性事实，存在稳健性问题。</li>
<li>大型最先进的语言模型如Llama-3.1-70B在时效性知识方面存在显著缺陷。</li>
<li>语言模型在处理时效性事实时，无法将知识从一个时间粒度推广到另一个粒度。</li>
<li>语言模型作为临时知识库的内在局限性。</li>
<li>实验结果表明，即使是针对流行Wikidata事实的提示，语言模型的性能也参差不齐。</li>
<li>实验中设计的控制测试可以有效评估语言模型处理时效性事实的能力。</li>
<li>源码和数据将公开，以便他人重现实验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01220">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-99e53cd5be26c73c3986cda89ec9613c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeb2708abf5bb6e26580a128c9d24586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3725b90733067bb25be94ecd1e552ae.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AtmosSci-Bench-Evaluating-the-Recent-Advance-of-Large-Language-Model-for-Atmospheric-Science"><a href="#AtmosSci-Bench-Evaluating-the-Recent-Advance-of-Large-Language-Model-for-Atmospheric-Science" class="headerlink" title="AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science"></a>AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science</h2><p><strong>Authors:Chenyue Li, Wen Deng, Mengqian Lu, Binhang Yuan</strong></p>
<p>The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. To address this need, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. We employ a template-based question generation framework, enabling scalable and diverse multiple-choice questions curated from graduate-level atmospheric science problems. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate service by offering a standard and rigorous evaluation framework. Our source codes are currently available at <a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench">https://github.com/Relaxed-System-Lab/AtmosSci-Bench</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的快速发展，特别是在其推理能力方面，为解决大气科学中的复杂挑战带来了变革性潜力。然而，要在这一领域有效地利用LLM，需要稳健而全面的评估基准。为了解决这一需求，我们推出了AtmosSci-Bench，这是一个新颖的基准测试，旨在系统评估LLM在五个大气科学核心类别的问题上的性能：水文、大气动力学、大气物理学、地球物理学和物理海洋学。我们采用基于模板的问题生成框架，能够规模化地从研究生层次的大气科学问题中生成多样且丰富的选择题。我们对具有代表性的LLM进行了全面评估，将其分为四组：指令调整模型、高级推理模型、数学增强模型和特定领域的气候模型。我们的分析对LLM在大气科学中的推理和问题解决能力提供了一些有趣的见解。我们相信，通过提供一个标准且严格的评估框架，AtmosSci-Bench可以成为推进LLM在气候服务中应用的关键步骤。我们的源代码目前可在<a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench">https://github.com/Relaxed-System-Lab/AtmosSci-Bench</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01159v1">PDF</a> 16 pages, 3 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在推理能力上的快速进步，为应对大气科学中的复杂挑战带来了变革性潜力。为解决在域中有效利用LLM的需求，我们推出了AtmosSci-Bench，这是一个新的评估基准，旨在系统地评估LLM在五个大气科学核心类别的问题解决能力：水文学、大气动力学、大气物理学、地球物理学和物理海洋学。我们使用基于模板的问题生成框架，能够从研究生级别的大气科学问题中生成可扩展和多样化的多项选择题。我们对代表性LLM进行了全面评估，分为四类：指令调整模型、高级推理模型、数学增强模型和领域特定气候模型。此分析为LLM在大气科学中的推理和问题解决能力提供了有趣见解。我们相信，AtmosSci-Bench可以作为推进LLM在气候服务中应用的关键步骤，通过提供标准和严格的评估框架来服务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在推理能力上的迅速进步为应对大气科学中的复杂挑战带来了重要机会。</li>
<li>AtmosSci-Bench是一个新的评估基准，旨在评估LLM解决大气科学中五个核心类别问题的能力。</li>
<li>AtmosSci-Bench采用基于模板的问题生成框架，可生成多样化、可扩展的多项选择题。</li>
<li>综合性评估涵盖了不同类型的LLM，包括指令调整、高级推理、数学增强和领域特定气候模型。</li>
<li>LLM在大气科学中的推理和问题解决能力表现出有趣且具洞察力的性能。</li>
<li>AtmosSci-Bench通过提供标准和严格的评估框架，被视为推进LLM在气候服务中应用的关键步骤。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01159">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-abe26b0c19cb580060f06a7a9878bf0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b915b23aa83a9d8f4542f116a568d3a4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Omni-Mol-Exploring-Universal-Convergent-Space-for-Omni-Molecular-Tasks"><a href="#Omni-Mol-Exploring-Universal-Convergent-Space-for-Omni-Molecular-Tasks" class="headerlink" title="Omni-Mol: Exploring Universal Convergent Space for Omni-Molecular Tasks"></a>Omni-Mol: Exploring Universal Convergent Space for Omni-Molecular Tasks</h2><p><strong>Authors:Chengxin Hu, Hao Li, Yihe Yuan, Zezheng Song, Haixin Wang</strong></p>
<p>Building generalist models has recently demonstrated remarkable capabilities in diverse scientific domains. Within the realm of molecular learning, several studies have explored unifying diverse tasks across diverse domains. However, negative conflicts and interference between molecules and knowledge from different domain may have a worse impact in threefold. First, conflicting molecular representations can lead to optimization difficulties for the models. Second, mixing and scaling up training data across diverse tasks is inherently challenging. Third, the computational cost of refined pretraining is prohibitively high. To address these limitations, this paper presents Omni-Mol, a scalable and unified LLM-based framework for direct instruction tuning. Omni-Mol builds on three key components to tackles conflicts: (1) a unified encoding mechanism for any task input; (2) an active-learning-driven data selection strategy that significantly reduces dataset size; (3) a novel design of the adaptive gradient stabilization module and anchor-and-reconcile MoE framework that ensures stable convergence. Experimentally, Omni-Mol achieves state-of-the-art performance across 15 molecular tasks, demonstrates the presence of scaling laws in the molecular domain, and is supported by extensive ablation studies and analyses validating the effectiveness of its design. The code and weights of the powerful AI-driven chemistry generalist are open-sourced at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Omni-Mol-8EDB">https://anonymous.4open.science/r/Omni-Mol-8EDB</a>. </p>
<blockquote>
<p>构建通用模型最近在多个科学领域表现出了显著的能力。在分子学习领域，一些研究已经探索了在不同的领域中统一各种任务的方法。然而，不同领域的分子和知识之间的负面冲突和干扰可能会产生三重的负面影响。首先，冲突的分子表示形式可能导致模型优化困难。其次，在不同任务之间混合和扩大训练数据本身就具有挑战性。第三，精细预训练的计算成本非常高。为了解决这些局限性，本文提出了Omni-Mol，这是一个可扩展的、基于大型语言模型的统一框架，用于直接指令调整。Omni-Mol建立在三个关键组件上以解决冲突：（1）任何任务输入的统一编码机制；（2）以主动学习驱动的数据选择策略，可显著减少数据集大小；（3）自适应梯度稳定模块和锚点与和解MoE框架的新颖设计，确保稳定的收敛。通过实验，Omni-Mol在15个分子任务上达到了最先进的性能，证明了分子领域的规模化法则的存在，并通过广泛的消融研究和分析验证了其设计的有效性。该强大的AI驱动化学通用模型的代码和权重已开源：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Omni-Mol-8EDB%E3%80%82">https://anonymous.4open.science/r/Omni-Mol-8EDB。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01074v1">PDF</a> 29 pages, 13 figures, 7 tables, paper under review</p>
<p><strong>Summary</strong></p>
<p>Omni-Mol框架通过统一编码机制、主动学习的数据选择策略和梯度稳定模块与和解MoE框架的设计，解决了在分子学习领域多任务学习的冲突问题，实现了跨15个分子任务的卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Omni-Mol是一个基于LLM的框架，用于解决分子学习中的多任务学习冲突问题。</li>
<li>它通过统一编码机制适应各种任务输入。</li>
<li>Omni-Mol采用主动学习的数据选择策略，显著减少数据集大小。</li>
<li>该框架设计了自适应梯度稳定模块和锚定与和解MoE框架，确保模型稳定收敛。</li>
<li>Omni-Mol在15个分子任务上实现卓越性能。</li>
<li>该框架展示了分子领域的规模法则，并通过广泛的消融研究和分析验证了其设计的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01074">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a73dad8414e0eae73ffdf458946756aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-208f3fbc35750a318aa341c0d6870eb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-456d579badae8d91abc83093d2f10cd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69a650b7c53e15bb5ebe3086da775b9.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SimPER-A-Minimalist-Approach-to-Preference-Alignment-without-Hyperparameters"><a href="#SimPER-A-Minimalist-Approach-to-Preference-Alignment-without-Hyperparameters" class="headerlink" title="SimPER: A Minimalist Approach to Preference Alignment without   Hyperparameters"></a>SimPER: A Minimalist Approach to Preference Alignment without   Hyperparameters</h2><p><strong>Authors:Teng Xiao, Yige Yuan, Zhengyu Chen, Mingxiao Li, Shangsong Liang, Zhaochun Ren, Vasant G Honavar</strong></p>
<p>Existing preference optimization objectives for language model alignment require additional hyperparameters that must be extensively tuned to achieve optimal performance, increasing both the complexity and time required for fine-tuning large language models. In this paper, we propose a simple yet effective hyperparameter-free preference optimization algorithm for alignment. We observe that promising performance can be achieved simply by optimizing inverse perplexity, which is calculated as the inverse of the exponentiated average log-likelihood of the chosen and rejected responses in the preference dataset. The resulting simple learning objective, SimPER, is easy to implement and eliminates the need for expensive hyperparameter tuning and a reference model, making it both computationally and memory efficient. Extensive experiments on widely used real-world benchmarks, including MT-Bench, AlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base models, demonstrate that SimPER consistently and significantly outperforms existing approaches-even without any hyperparameters or a reference model . For example, despite its simplicity, SimPER outperforms state-of-the-art methods by up to 5.7 points on AlpacaEval 2 and achieves the highest average ranking across 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/tengxiao1/SimPER">https://github.com/tengxiao1/SimPER</a>. </p>
<blockquote>
<p>现有语言模型对齐的偏好优化目标需要额外的超参数，且必须进行全面调整才能实现最佳性能，这增加了微调大型语言模型的复杂性和时间要求。在本文中，我们提出了一种简单而有效的无超参数偏好优化算法，用于对齐。我们发现，通过优化逆困惑度（即所选和拒绝响应的偏好数据集的指数平均对数似然度的倒数）可以达到良好的性能。由此产生的简单学习目标SimPER易于实现，无需昂贵的超参数调整和参考模型，使其在计算和内存方面都非常高效。在包括MT-Bench、AlpacaEval 2和Open LLM Leaderboard的10个关键基准测试上的广泛实验，使用5个基础模型进行验证，结果表明SimPER持续且显著优于现有方法，即使没有任何超参数或参考模型。例如，尽管SimPER简单，但在AlpacaEval 2上它的性能比最先进的方法高出高达5.7个点，并在Open LLM Leaderboard的10个基准测试中获得了最高平均排名。SimPER的源代码公开可用：<a target="_blank" rel="noopener" href="https://github.com/tengxiao1/SimPER%E3%80%82">https://github.com/tengxiao1/SimPER。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00883v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种用于语言模型对齐的简洁而有效的无超参数偏好优化算法。通过优化选择响应和拒绝响应的逆困惑度，实现了良好的性能。该算法不仅易于实现，而且无需昂贵的超参数调整和参考模型，具有较高的计算效率和内存效率。实验结果表明，SimPER在多个真实世界基准测试上持续且显著地优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有语言模型对齐的偏好优化目标需要额外的超参数，增加了微调大型语言模型的复杂性和时间。</li>
<li>本文提出了一种无超参数的偏好优化算法，通过优化逆困惑度实现对语言模型的有效对齐。</li>
<li>SimPER算法简单易行，不需要昂贵的超参数调整和参考模型。</li>
<li>SimPER在多个真实世界基准测试上表现出卓越性能，显著优于现有方法。</li>
<li>SimPER在MT-Bench、AlpacaEval 2以及Open LLM Leaderboard的10个关键基准测试上都展示了其有效性。</li>
<li>SimPER算法已公开可用，可访问<a target="_blank" rel="noopener" href="https://github.com/tengxiao1/SimPER%E8%8E%B7%E5%8F%96%E6%BA%90%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/tengxiao1/SimPER获取源代码。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00883">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9ccba258b30d834d03207bfd7b4210e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9368517360d4c67cceafe433df8324b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee03fb7c929e3983c1b28bd62c70867c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf8ba98161734315ee8817bc2cfac83.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Beyond-the-Permutation-Symmetry-of-Transformers-The-Role-of-Rotation-for-Model-Fusion"><a href="#Beyond-the-Permutation-Symmetry-of-Transformers-The-Role-of-Rotation-for-Model-Fusion" class="headerlink" title="Beyond the Permutation Symmetry of Transformers: The Role of Rotation   for Model Fusion"></a>Beyond the Permutation Symmetry of Transformers: The Role of Rotation   for Model Fusion</h2><p><strong>Authors:Binchi Zhang, Zaiyi Zheng, Zhengzhang Chen, Jundong Li</strong></p>
<p>Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on <a target="_blank" rel="noopener" href="https://github.com/zhengzaiyi/RotationSymmetry">https://github.com/zhengzaiyi/RotationSymmetry</a>. </p>
<blockquote>
<p>深度神经网络（DNN）参数空间中的对称性已被证明对各种深度学习应用有益。一个众所周知的例子是多层感知器（MLP）中的置换对称性，其中对一层中的权重矩阵的行进行置换，并对相邻层应用逆置换，产生功能上等效的模型。虽然置换对称性完全表征了MLP的等价集，但其离散性质限制了其在变换器中的应用。在本文中，我们引入了旋转对称性，这是一种新型参数空间对称性，通过旋转自注意力层的参数矩阵来推广置换对称性。与置换对称性不同，旋转对称性在一个连续域中运行，从而显著扩大了变换器的等价集。基于这一属性，我们提出了一种理论上的最优参数匹配算法，作为一个即插即用的模块来增强模型融合。我们使用预训练的变换器在多种自然语言处理和视觉任务上评估了我们的方法。实验结果表明，我们的基于旋转对称性的匹配算法极大地提高了模型融合，突出了参数空间对称性在促进模型融合方面的潜力。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/zhengzaiyi/RotationSymmetry%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhengzaiyi/RotationSymmetry上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00264v1">PDF</a> </p>
<p><strong>Summary</strong><br>深度学习网络参数空间的对称性对于多种深度学习应用具有益处。论文介绍了旋转对称性这一新型参数空间对称性形式，并将其应用于变压器模型。旋转对称性通过旋转自注意力层的参数矩阵实现，不同于离散排列对称性，其在连续域内运行，显著扩展了变压器的等价集合。基于此属性，论文提出一种理论上的最佳参数匹配算法，作为增强模型融合的即插即用模块。实验结果表明，该基于旋转对称性的匹配算法能显著提高模型融合效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>参数空间对称在深度学习中有益。</li>
<li>论文引入旋转对称性，这是一种新型参数空间对称性形式。</li>
<li>旋转对称性应用于变压器模型，通过旋转自注意力层的参数矩阵实现。</li>
<li>旋转对称性在连续域内运行，扩展了变压器的等价集合。</li>
<li>提出一种理论上的最佳参数匹配算法，增强模型融合。</li>
<li>基于旋转对称性的匹配算法能显著提高模型融合效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00264">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6e49df1c83cda86bcc4ca6fbc306ee2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72d277e77e40f33727666ac304117a22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-438a952d3265d835f29fa8e82735ae80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56f2561d502749811270bbf4aa58c791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b72214b4d18da8a269009ef99824774b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="The-TIP-of-the-Iceberg-Revealing-a-Hidden-Class-of-Task-in-Prompt-Adversarial-Attacks-on-LLMs"><a href="#The-TIP-of-the-Iceberg-Revealing-a-Hidden-Class-of-Task-in-Prompt-Adversarial-Attacks-on-LLMs" class="headerlink" title="The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt   Adversarial Attacks on LLMs"></a>The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt   Adversarial Attacks on LLMs</h2><p><strong>Authors:Sergey Berezin, Reza Farahbakhsh, Noel Crespi</strong></p>
<p>We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model’s prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies.   Warning: this paper contains examples of unethical inquiries used solely for research purposes. </p>
<blockquote>
<p>我们针对大型语言模型（LLMs）提出了一类新型越狱对抗攻击，称为Task-in-Prompt（TIP）攻击。我们的方法将序列到序列任务（如密码解码、谜语、代码执行）嵌入到模型的提示中，以间接生成禁止的输入。为了系统地评估这些攻击的有效性，我们引入了PHRYGE基准测试。我们证明，我们的技术成功地绕过了六种最先进的语言模型的安全保障措施，包括GPT-4o和LLaMA 3.2。我们的研究结果突出了当前LLM安全对齐中的关键弱点，并强调了更复杂的防御策略的紧迫需求。警告：本论文包含的询问示例仅用于研究目的，并非出于道德立场而提出攻击建议。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18626v3">PDF</a> </p>
<p><strong>Summary</strong><br>任务嵌入提示攻击：针对大型语言模型的新型越狱对抗攻击。通过嵌入序列到序列任务（如密码解码、解谜、代码执行）到模型提示来间接生成禁止输入，并对当前最先进的大型语言模型进行演示评估。展示了对抗方法可绕过安全防范措施攻击前沿大型语言模型并指出模型安全漏洞及防范迫切性。包含为研究目的仅用的非道德案例询问示例。针对大型语言模型的安全性挑战亟需应对和解决。研究亮点在于其突破性的评估方法和新的安全漏洞的发现，同时警告了针对这些漏洞可能存在的潜在风险。强调了伦理和道德在人工智能领域的重要性，并呼吁业界关注并重视这一问题。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一种新型的对大型语言模型（LLM）的攻击方法——任务嵌入提示（TIP）攻击。</li>
<li>通过在模型提示中嵌入序列到序列任务来间接生成禁止输入，从而绕过安全防范措施攻击前沿的大型语言模型。</li>
<li>展示了TIP攻击在六种最先进的大型语言模型上的有效性，包括GPT-4o和LLaMA 3.2。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18626">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ffd8552147cd8625dfbd99d6ed52a466.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ad600a12d86d1c1c57649ede0a528e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d075c69d70beaa4059b93faa482363e8.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-06/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-129c03c30fabc821a4ca723773c59182.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-06  DHP Discrete Hierarchical Planning for Hierarchical Reinforcement   Learning Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5f4ca5ae820d1405e3f7f4ca095f44ac.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-02-05  Exploring total fission cross sections of neutron- and proton-induced   reactions of exotic nuclei at relativistic energies using the INCL-ABLA++   models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
