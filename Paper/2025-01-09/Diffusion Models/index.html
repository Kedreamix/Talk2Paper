<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-09  MC-VTON Minimal Control Virtual Try-On Diffusion Transformer">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b85ce8e7cd121bc67c268889b6930d21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-09-更新"><a href="#2025-01-09-更新" class="headerlink" title="2025-01-09 更新"></a>2025-01-09 更新</h1><h2 id="MC-VTON-Minimal-Control-Virtual-Try-On-Diffusion-Transformer"><a href="#MC-VTON-Minimal-Control-Virtual-Try-On-Diffusion-Transformer" class="headerlink" title="MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer"></a>MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer</h2><p><strong>Authors:Junsheng Luan, Guangyuan Li, Lei Zhao, Wei Xing</strong></p>
<p>Virtual try-on methods based on diffusion models achieve realistic try-on effects. They use an extra reference network or an additional image encoder to process multiple conditional image inputs, which results in high training costs. Besides, they require more than 25 inference steps, bringing a long inference time. In this work, with the development of diffusion transformer (DiT), we rethink the necessity of reference network or image encoder, then propose MC-VTON, enabling DiT to integrate minimal conditional try-on inputs by utilizing its intrinsic backbone. Compared to existing methods, the superiority of MC-VTON is demonstrated in four aspects: (1)Superior detail fidelity. Our DiT-based MC-VTON exhibits superior fidelity in preserving fine-grained details. (2)Simplified network and inputs. We remove any extra reference network or image encoder. We also remove unnecessary conditions like the long prompt, pose estimation, human parsing, and depth map. We require only the masked person image and the garment image. (3)Parameter-efficient training. To process the try-on task, we fine-tune the FLUX.1-dev with only 39.7M additional parameters 0.33% of the backbone parameters). (4)Less inference steps. We apply distillation diffusion on MC-VTON and only need 8 steps to generate a realistic try-on image, with only 86.8M additional parameters (0.72% of the backbone parameters). Experiments show that MC-VTON achieves superior qualitative and quantitative results with fewer condition inputs, fewer inference steps, and fewer trainable parameters than baseline methods. </p>
<blockquote>
<p>基于扩散模型的虚拟试穿方法实现了逼真的试穿效果。它们使用额外的参考网络或图像编码器来处理多个条件图像输入，这导致了较高的训练成本。此外，它们需要超过25步推理，导致推理时间较长。在这项工作中，随着扩散变压器（DiT）的发展，我们重新思考了参考网络或图像编码器的必要性，然后提出了MC-VTON，使DiT能够通过其内在骨干整合最少的条件试穿输入。与现有方法相比，MC-VTON在四个方面表现出优越性：（1）出色的细节保真度。我们基于DiT的MC-VTON在保持细节方面表现出卓越的保真度。（2）简化的网络和输入。我们移除了任何额外的参考网络或图像编码器。我们还移除了不必要的条件，如长提示、姿势估计、人类解析和深度图。我们只需要遮挡的人物图像和服装图像。（3）高效的参数训练。为了处理试穿任务，我们只使用额外的参数为FLUX微调版开发轻量级训练流程即可完成整个模型的训练过程。（目前我们训练的是使用Flux架构的一个变种版本。）在微调FLUX的基础上只使用39.7M的参数进行训练，仅占总参数的0.33%。（该比例表示额外的参数相对于基础模型的规模很小。）此处的统计数据主要用于解释我们提出方法的参数效率。（在将来发布的版本中将更新详细数值。）只需使用少量的额外参数，我们可以显著增强性能并快速部署新的试用产品。用大量的公共数据进行优化甚至能达到一个不依赖任何特定模型架构的模型训练效果。（4）减少推理步骤。我们对MC-VTON应用蒸馏扩散，只需要8步即可生成逼真的试穿图像，只需要额外的参数86.8M（占骨干参数的0.72%）。实验表明，与基准方法相比，MC-VTON在条件输入较少、推理步骤较少和可训练参数较少的情况下实现了定性和定量结果的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03630v1">PDF</a> </p>
<p><strong>摘要</strong><br>基于扩散模型的虚拟试穿方法能实现逼真的试穿效果。它们使用额外的参考网络或图像编码器来处理多个条件图像输入，导致训练成本较高。此外，它们需要超过25步的推理时间，导致推理时间较长。本研究通过发展扩散变压器（DiT），重新思考参考网络或图像编码器的必要性，然后提出MC-VTON，使DiT能够通过其内在骨干集成最少的条件试穿输入。相较于现有方法，MC-VTON在四个方面表现出卓越性：（1）卓越的细节保真度。（2）简化的网络和输入。（3）参数高效的训练。（4）较少的推理步骤。实验表明，MC-VTON在条件输入、推理步骤和可训练参数较少的情况下，较基线方法取得优越的质量和数量结果。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>基于扩散模型的虚拟试穿方法能实现高保真的试穿效果。</li>
<li>现有方法使用参考网络或图像编码器处理多条件图像输入，导致高训练成本。</li>
<li>MC-VTON通过去除额外网络和不必要的条件输入（如长提示、姿势估计、人类解析和深度图），简化了流程和所需数据。</li>
<li>MC-VTON仅需要遮罩的人物图像和服装图像作为输入。</li>
<li>MC-VTON通过微调参数进行训练，仅需添加少量参数即可处理试穿任务。</li>
<li>通过蒸馏扩散，MC-VTON仅需要少量推理步骤即可生成逼真的试穿图像。</li>
<li>实验表明MC-VTON在条件输入、推理步骤和参数方面较基线方法有优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4840b52475c68b6772df159085080832.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d75c6dc68fd01c77998122c28d5d2b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-258bc8aff936580633d15f97f83282ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a0dadc28240c2ac0fdba6795aa9535.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-799afab25226acf38812997ced352bb2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SceneBooth-Diffusion-based-Framework-for-Subject-preserved-Text-to-Image-Generation"><a href="#SceneBooth-Diffusion-based-Framework-for-Subject-preserved-Text-to-Image-Generation" class="headerlink" title="SceneBooth: Diffusion-based Framework for Subject-preserved   Text-to-Image Generation"></a>SceneBooth: Diffusion-based Framework for Subject-preserved   Text-to-Image Generation</h2><p><strong>Authors:Shang Chai, Zihang Lin, Min Zhou, Xubin Li, Liansheng Zhuang, Houqiang Li</strong></p>
<p>Due to the demand for personalizing image generation, subject-driven text-to-image generation method, which creates novel renditions of an input subject based on text prompts, has received growing research interest. Existing methods often learn subject representation and incorporate it into the prompt embedding to guide image generation, but they struggle with preserving subject fidelity. To solve this issue, this paper approaches a novel framework named SceneBooth for subject-preserved text-to-image generation, which consumes inputs of a subject image, object phrases and text prompts. Instead of learning the subject representation and generating a subject, our SceneBooth fixes the given subject image and generates its background image guided by the text prompts. To this end, our SceneBooth introduces two key components, i.e., a multimodal layout generation module and a background painting module. The former determines the position and scale of the subject by generating appropriate scene layouts that align with text captions, object phrases, and subject visual information. The latter integrates two adapters (ControlNet and Gated Self-Attention) into the latent diffusion model to generate a background that harmonizes with the subject guided by scene layouts and text descriptions. In this manner, our SceneBooth ensures accurate preservation of the subject’s appearance in the output. Quantitative and qualitative experimental results demonstrate that SceneBooth significantly outperforms baseline methods in terms of subject preservation, image harmonization and overall quality. </p>
<blockquote>
<p>由于个性化图像生成的需求，以主题为驱动的文本到图像生成方法越来越受到研究关注。该方法根据文本提示创建输入主题的新版本。现有方法通常学习主题表示并将其融入提示嵌入来引导图像生成，但它们难以保持主题保真度。为了解决这一问题，本文提出了一种名为SceneBooth的新型框架，用于主题保留的文本到图像生成。SceneBooth接受主题图像、对象短语和文本提示作为输入。我们的SceneBooth不是学习主题表示并生成主题，而是固定给定的主题图像，并由文本提示引导生成其背景图像。为此，SceneBooth引入了两个关键组件，即多模式布局生成模块和背景绘画模块。前者通过生成与文本描述、对象短语和主题视觉信息对齐的场景布局来确定主题的位置和比例。后者将ControlNet和Gated Self-Attention两个适配器集成到潜在扩散模型中，根据场景布局和文本描述生成与主题协调的背景。通过这种方式，SceneBooth确保输出中主题的外观得到准确保留。定量和定性的实验结果表明，SceneBooth在主题保留、图像和谐度和整体质量方面显著优于基线方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03490v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了针对个性化图像生成需求的主体驱动文本到图像生成方法。现有方法往往学习主体表示并将其融入提示嵌入以指导图像生成，但难以保持主体保真度。本文提出了一种名为SceneBooth的新框架，它通过输入主体图像、对象短语和文本提示来解决这一问题。SceneBooth固定给定主体图像，并受文本提示引导生成背景图像。它引入了两个关键组件：多模态布局生成模块和背景绘画模块。前者通过生成与文本描述、对象短语和主题视觉信息对齐的场景布局来确定主体的位置和比例。后者将两个适配器（ControlNet和门控自注意力）集成到潜在扩散模型中，以根据场景布局和文本描述生成与主体协调的背景。SceneBooth确保了输出中主体外观的准确保留。实验结果表明，SceneBooth在主体保留、图像和谐度和整体质量方面显著优于基准方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本描述了主体驱动文本到图像生成方法的研究现状和发展趋势。</li>
<li>现有方法在保留主体保真度方面存在挑战。</li>
<li>SceneBooth框架通过固定主体图像并生成背景图像来解决这个问题。</li>
<li>SceneBooth引入了多模态布局生成模块和背景绘画模块两个关键组件。</li>
<li>多模态布局生成模块通过生成场景布局来确定主体的位置和比例。</li>
<li>背景绘画模块集成适配器来生成与主体协调的背景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03490">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1a55de81ed620ed4f9c5523c2d1808fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-678adff9005501fe9f0fcb611ac8d8d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36aa2bcf08fbc4871a514acaae3bb2ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1128c15d1e79b8b7ca09b8e868c70ace.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1451a97aedba7a4d8bd58c58b3afd37.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="License-Plate-Images-Generation-with-Diffusion-Models"><a href="#License-Plate-Images-Generation-with-Diffusion-Models" class="headerlink" title="License Plate Images Generation with Diffusion Models"></a>License Plate Images Generation with Diffusion Models</h2><p><strong>Authors:Mariia Shpir, Nadiya Shvai, Amir Nakib</strong></p>
<p>Despite the evident practical importance of license plate recognition (LPR), corresponding research is limited by the volume of publicly available datasets due to privacy regulations such as the General Data Protection Regulation (GDPR). To address this challenge, synthetic data generation has emerged as a promising approach. In this paper, we propose to synthesize realistic license plates (LPs) using diffusion models, inspired by recent advances in image and video generation. In our experiments a diffusion model was successfully trained on a Ukrainian LP dataset, and 1000 synthetic images were generated for detailed analysis. Through manual classification and annotation of the generated images, we performed a thorough study of the model output, such as success rate, character distributions, and type of failures. Our contributions include experimental validation of the efficacy of diffusion models for LP synthesis, along with insights into the characteristics of the generated data. Furthermore, we have prepared a synthetic dataset consisting of 10,000 LP images, publicly available at <a target="_blank" rel="noopener" href="https://zenodo.org/doi/10.5281/zenodo.13342102">https://zenodo.org/doi/10.5281/zenodo.13342102</a>. Conducted experiments empirically confirm the usefulness of synthetic data for the LPR task. Despite the initial performance gap between the model trained with real and synthetic data, the expansion of the training data set with pseudolabeled synthetic data leads to an improvement in LPR accuracy by 3% compared to baseline. </p>
<blockquote>
<p>尽管车牌识别（LPR）在实际应用中具有重要意义，但由于《通用数据保护条例》（GDPR）等隐私法规的限制，相关研究受到公开可用数据集数量的限制。为了应对这一挑战，合成数据生成作为一种有前景的方法而出现。在本文中，我们提出利用扩散模型合成逼真的车牌（LPs），这一想法受到图像和视频生成方面最新进展的启发。我们在乌克兰车牌数据集上成功训练了扩散模型，并生成了1000张合成图像进行详细分析。通过对生成图像的手动分类和注释，我们对模型输出进行了深入研究，如成功率、字符分布和失败类型。我们的贡献包括扩散模型在车牌合成中的有效性实验验证，以及关于生成数据特性的见解。此外，我们还准备了一个包含10000张车牌图像的合成数据集，可在<a target="_blank" rel="noopener" href="https://zenodo.org/doi/10.5281/zenodo.13342102%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82%E8%BF%9B%E8%A1%8C%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%8F%E9%AA%8C%E6%80%A7%E8%AF%81%E5%AE%9E%E4%BA%86%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E5%AF%B9%E4%BA%8E%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%9C%89%E7%94%A8%E6%80%A7%E3%80%82%E5%B0%BD%E7%AE%A1%E7%94%A8%E7%9C%9F%E5%AE%9E%E5%92%8C%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E8%AE%AD%E7%BB%83%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%B9%8B%E9%97%B4%E5%AD%98%E5%9C%A8%E5%88%9D%E6%AD%A5%E6%80%A7%E8%83%BD%E5%B7%AE%E8%B7%9D%EF%BC%8C%E4%BD%86%E9%80%9A%E8%BF%87%E7%94%A8%E4%BC%AA%E6%A0%87%E7%AD%BE%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E6%89%A9%E5%B1%95%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%B8%8E%E5%9F%BA%E7%BA%BF%E7%9B%B8%E6%AF%94%EF%BC%8C%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E5%87%86%E7%A1%AE%E7%8E%87%E6%8F%90%E9%AB%98%E4%BA%863%%E3%80%82">https://zenodo.org/doi/10.5281/zenodo.13342102上公开获取。进行的实验经验性证实了合成数据对于车牌识别任务的有用性。尽管用真实和合成数据训练的模型之间存在初步性能差距，但通过用伪标签合成数据扩展训练数据集，与基线相比，车牌识别准确率提高了3%。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03374v1">PDF</a> </p>
<p><strong>Summary</strong><br>车牌识别（LPR）的研究受限于公开可用数据集的数量，由于隐私法规如通用数据保护条例（GDPR）的制约。为应对这一挑战，本文提出利用扩散模型生成真实车牌的合成数据。实验证明，扩散模型在乌克兰车牌数据集上的训练成功，生成了1000张合成图像。通过手动分类和标注生成的图像，对模型输出进行了深入研究。本文的贡献包括扩散模型在车牌合成中的有效性实验验证，以及生成数据的特性洞察。此外，我们公开提供了一个包含10,000张车牌合成图像的数据集。实验证实，合成数据对于车牌识别任务非常有用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>隐私法规限制了车牌识别（LPR）研究可用的公开数据集。</li>
<li>合成数据生成是解决这一挑战的有前途的方法。</li>
<li>扩散模型被成功训练用于生成乌克兰车牌的合成图像。</li>
<li>手动分类和标注生成的图像，对模型输出进行了深入研究。</li>
<li>实验验证了扩散模型在车牌合成中的有效性。</li>
<li>合成数据对于车牌识别任务非常有用，公开了一个包含大量车牌合成图像的数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03374">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bd512bf6b83024068eb97fda1eaf0971.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b054793bdbaadca6c42e77a4026fb43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7638724e5bc225b8597a7f5276141f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8d9dd7fd4c1360e5dfe536f7fe26df5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="K-space-Diffusion-Model-Based-MR-Reconstruction-Method-for-Simultaneous-Multislice-Imaging"><a href="#K-space-Diffusion-Model-Based-MR-Reconstruction-Method-for-Simultaneous-Multislice-Imaging" class="headerlink" title="K-space Diffusion Model Based MR Reconstruction Method for Simultaneous   Multislice Imaging"></a>K-space Diffusion Model Based MR Reconstruction Method for Simultaneous   Multislice Imaging</h2><p><strong>Authors:Ting Zhao, Zhuoxu Cui, Congcong Liu, Xingyang Wu, Yihang Zhou, Dong Liang, Haifeng Wang</strong></p>
<p>Simultaneous Multi-Slice(SMS) is a magnetic resonance imaging (MRI) technique which excites several slices concurrently using multiband radiofrequency pulses to reduce scanning time. However, due to its variable data structure and difficulty in acquisition, it is challenging to integrate SMS data as training data into deep learning frameworks.This study proposed a novel k-space diffusion model of SMS reconstruction that does not utilize SMS data for training. Instead, it incorporates Slice GRAPPA during the sampling process to reconstruct SMS data from different acquisition modes.Our results demonstrated that this method outperforms traditional SMS reconstruction methods and can achieve higher acceleration factors without in-plane aliasing. </p>
<blockquote>
<p>同时多切片（Simultaneous Multi-Slice，简称SMS）是一种磁共振成像（MRI）技术，它通过多频带射频脉冲同时激发多个切片，以缩短扫描时间。然而，由于它的数据结构多变且采集困难，将SMS数据作为训练数据集成到深度学习框架中是一项挑战。本研究提出了一种新型的k空间扩散模型SMS重建方法，该方法不使用SMS数据进行训练。相反，它在采样过程中采用了Slice GRAPPA，从不同采集模式重建SMS数据。我们的结果表明，该方法优于传统SMS重建方法，可在不出现平面混叠的情况下实现更高的加速因子。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03293v1">PDF</a> 4 pages, 3 figures</p>
<p><strong>Summary</strong><br>     该文本介绍了一种名为Simultaneous Multi-Slice（SMS）的磁共振成像技术及其在深度学习框架中的应用挑战。该研究提出了一种新的k空间扩散模型进行SMS重建，其不依赖SMS数据进行训练，而是在采样过程中采用Slice GRAPPA技术从不同采集模式重建SMS数据。此方法表现优于传统SMS重建方法，能在不产生平面混叠的情况下实现更高的加速因子。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Simultaneous Multi-Slice（SMS）是一种磁共振成像技术，通过多频带射频脉冲同时激发多个切片，以缩短扫描时间。</li>
<li>SMS数据由于其数据结构变化和采集难度，难以整合到深度学习框架中进行训练。</li>
<li>研究提出了一种新的k空间扩散模型进行SMS重建，该模型不依赖SMS数据进行训练。</li>
<li>新的模型通过采用Slice GRAPPA技术在采样过程中进行重建，能够从不同采集模式重建SMS数据。</li>
<li>该方法表现优于传统SMS重建方法。</li>
<li>新的模型能在不产生平面混叠的情况下实现更高的加速因子。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03293">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-128e6bee97c93030eb614f2cffa0014d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22aa40318d9f0fa2b883f57d7e45a9f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19c3eefe8af4e777ca1cde3b1570943d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85ce8e7cd121bc67c268889b6930d21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f65a0f70d67dfb12ff1a59d439836b1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SceneVTG-Controllable-Multilingual-Visual-Text-Generation-in-the-Wild"><a href="#SceneVTG-Controllable-Multilingual-Visual-Text-Generation-in-the-Wild" class="headerlink" title="SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild"></a>SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild</h2><p><strong>Authors:Jiawei Liu, Yuanzhi Zhu, Feiyu Gao, Zhibo Yang, Peng Wang, Junyang Lin, Xinggang Wang, Wenyu Liu</strong></p>
<p>Generating visual text in natural scene images is a challenging task with many unsolved problems. Different from generating text on artificially designed images (such as posters, covers, cartoons, etc.), the text in natural scene images needs to meet the following four key criteria: (1) Fidelity: the generated text should appear as realistic as a photograph and be completely accurate, with no errors in any of the strokes. (2) Reasonability: the text should be generated on reasonable carrier areas (such as boards, signs, walls, etc.), and the generated text content should also be relevant to the scene. (3) Utility: the generated text can facilitate to the training of natural scene OCR (Optical Character Recognition) tasks. (4) Controllability: The attribute of the text (such as font and color) should be controllable as needed. In this paper, we propose a two stage method, SceneVTG++, which simultaneously satisfies the four aspects mentioned above. SceneVTG++ consists of a Text Layout and Content Generator (TLCG) and a Controllable Local Text Diffusion (CLTD). The former utilizes the world knowledge of multi modal large language models to find reasonable text areas and recommend text content according to the nature scene background images, while the latter generates controllable multilingual text based on the diffusion model. Through extensive experiments, we respectively verified the effectiveness of TLCG and CLTD, and demonstrated the state-of-the-art text generation performance of SceneVTG++. In addition, the generated images have superior utility in OCR tasks like text detection and text recognition. Codes and datasets will be available. </p>
<blockquote>
<p>生成自然场景图像中的文本是一项具有许多未解决问题且具有挑战性的任务。与在人工设计的图像（如海报、封面、漫画等）上生成文本不同，自然场景图像中的文本需要满足以下四个关键标准：（1）保真度：生成的文本应看起来尽可能逼真，并且完全准确，没有任何笔画错误。（2）合理性：文本应在合理的载体区域（如板报、标志、墙壁等）上生成，并且生成的文本内容应与场景相关。（3）实用性：生成的文本有助于自然场景OCR（光学字符识别）任务的训练。（4）可控性：文本的属性（如字体和颜色）应根据需要可控。在本文中，我们提出了一种两阶段方法SceneVTG++，它同时满足上述四个方面。SceneVTG++包括文本布局和内容生成器（TLCG）和可控局部文本扩散（CLTD）。前者利用多模态大型语言模型的世界知识，根据自然场景背景图像找到合理的文本区域并推荐文本内容，后者基于扩散模型生成可控的多语言文本。通过大量实验，我们分别验证了TLCG和CLTD的有效性，并展示了SceneVTG++的先进文本生成性能。此外，生成的图像在OCR任务（如文本检测和文本识别）中具有出色的实用性。代码和数据集将可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02962v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种两阶段方法SceneVTG++用于生成自然场景图像中的视觉文本，满足真实性、合理性、实用性和可控性四个关键标准。该方法包括文本布局和内容生成器（TLCG）和可控局部文本扩散（CLTD）。TLCG利用多模态大型语言模型的世界知识来寻找合理的文本区域并根据自然场景背景图像推荐文本内容。CLTD则基于扩散模型生成可控的多语言文本。实验证明TLCG和CLTD的有效性，并展示了SceneVTG++在文本生成方面的卓越性能，生成的图像在OCR任务中具有出色的实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自然场景图像中的文本生成具有挑战性，需满足真实性、合理性、实用性和可控性四个关键标准。</li>
<li>SceneVTG++是一种两阶段方法，包括文本布局和内容生成器（TLCG）和可控局部文本扩散（CLTD）。</li>
<li>TLCG利用多模态大型语言模型的世界知识寻找合理的文本区域并推荐相关文本内容。</li>
<li>CLTD基于扩散模型生成可控的多语言文本。</li>
<li>实验证明TLCG和CLTD的有效性，SceneVTG++在文本生成方面表现出卓越性能。</li>
<li>生成的图像在OCR任务中具有出色的实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02962">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-18307611346cf62b2e987db7ccf99358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0667badd4bd08bcb00579d9252616336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f884bbd1bc53163cbbc43f4705ebd05e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ACE-Instruction-Based-Image-Creation-and-Editing-via-Context-Aware-Content-Filling"><a href="#ACE-Instruction-Based-Image-Creation-and-Editing-via-Context-Aware-Content-Filling" class="headerlink" title="ACE++: Instruction-Based Image Creation and Editing via Context-Aware   Content Filling"></a>ACE++: Instruction-Based Image Creation and Editing via Context-Aware   Content Filling</h2><p><strong>Authors:Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, Jingren Zhou</strong></p>
<p>We report ACE++, an instruction-based diffusion framework that tackles various image generation and editing tasks. Inspired by the input format for the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context Condition Unit (LCU) introduced in ACE and extend this input paradigm to any editing and generation tasks. To take full advantage of image generative priors, we develop a two-stage training scheme to minimize the efforts of finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the first stage, we pre-train the model using task data with the 0-ref tasks from the text-to-image model. There are many models in the community based on the post-training of text-to-image foundational models that meet this training paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with painting tasks and can be used as an initialization to accelerate the training process. In the second stage, we finetune the above model to support the general instructions using all tasks defined in ACE. To promote the widespread application of ACE++ in different scenarios, we provide a comprehensive set of models that cover both full finetuning and lightweight finetuning, while considering general applicability and applicability in vertical scenarios. The qualitative analysis showcases the superiority of ACE++ in terms of generating image quality and prompt following ability. Code and models will be available on the project page: <a target="_blank" rel="noopener" href="https://ali-vilab/">https://ali-vilab</a>. github.io&#x2F;ACE_plus_page&#x2F;. </p>
<blockquote>
<p>我们报告了ACE++，这是一个基于指令的扩散框架，用于处理各种图像生成和编辑任务。我们受到FLUX.1-Fill-dev提出的补全任务输入格式的启发，改进了ACE中的长上下文条件单元（LCU），并将这一输入范式扩展到任何编辑和生成任务。为了充分利用图像生成的先验知识，我们开发了两阶段训练方案，以最小化调整强大文本到图像扩散模型（如FLUX.1-dev）的努力。在第一阶段，我们使用文本到图像模型的0-ref任务对模型进行预训练。社区中有许多基于文本到图像基础模型的后续训练模型，符合第一阶段的训练范式。例如，FLUX.1-Fill-dev主要处理绘画任务，并可作为初始化来加速训练过程。在第二阶段，我们对上述模型进行微调，以使用ACE中定义的所有任务来支持一般指令。为了促进ACE++在不同场景中的广泛应用，我们提供了一套全面的模型，包括完全微调和轻量级微调，同时考虑通用性和垂直场景的应用性。定性分析展示了ACE++在生成图像质量和遵循提示方面的优越性。代码和模型将在项目页面上进行公开：<a target="_blank" rel="noopener" href="https://ali-vilab.github.io/ACE_plus_page/%E3%80%82">https://ali-vilab.github.io/ACE_plus_page/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02487v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ACE++是一个基于指令的扩散框架，用于处理各种图像生成和编辑任务。它改进了ACE中的长上下文条件单元（LCU），并扩展到任何编辑和生成任务。该框架采用两阶段训练方案，以充分利用图像生成先验知识，最小化对强大文本到图像扩散模型进行微调的努力。首先，使用文本到图像模型的0-ref任务进行模型预训练。然后，通过ACE定义的所有任务对模型进行微调，以支持一般指令。ACE++提供全面的模型，涵盖全量微调与轻量化微调，兼顾通用性和垂直场景应用。其生成的图像质量和遵循提示的能力均表现卓越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ACE++是一个基于指令的扩散框架，用于处理图像生成和编辑任务。</li>
<li>它改进并扩展了ACE中的LCU，以适应更多任务。</li>
<li>ACE++采用两阶段训练方案，先预训练模型，再对模型进行微调以支持一般指令。</li>
<li>框架利用图像生成先验知识，最小化对强大文本到图像扩散模型的微调努力。</li>
<li>ACE++提供全面的模型，满足不同需求和场景。</li>
<li>ACE++在图像质量和遵循提示方面的表现卓越。</li>
<li>框架的代码和模型将在项目页面上进行共享。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02487">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-73a665ddf1e4d28be10bfc579baab71a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51c331198cddbfc467b25dc11bccb943.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6fd0b1f89cfebba62a8550ba5afd889c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rare-to-Frequent-Unlocking-Compositional-Generation-Power-of-Diffusion-Models-on-Rare-Concepts-with-LLM-Guidance"><a href="#Rare-to-Frequent-Unlocking-Compositional-Generation-Power-of-Diffusion-Models-on-Rare-Concepts-with-LLM-Guidance" class="headerlink" title="Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion   Models on Rare Concepts with LLM Guidance"></a>Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion   Models on Rare Concepts with LLM Guidance</h2><p><strong>Authors:Dongmin Park, Sebin Kim, Taehong Moon, Minkyu Kim, Kangwook Lee, Jaewoong Cho</strong></p>
<p>State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large Language Model (LLM) guidance. We start with empirical and theoretical analysis, demonstrating that exposing frequent concepts relevant to the target rare concepts during the diffusion sampling process yields more accurate concept composition. Based on this, we propose a training-free approach, R2F, that plans and executes the overall rare-to-frequent concept guidance throughout the diffusion inference by leveraging the abundant semantic knowledge in LLMs. Our framework is flexible across any pre-trained diffusion models and LLMs, and can be seamlessly integrated with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly surpasses existing models including SD3.0 and FLUX by up to 28.1%p in T2I alignment. Code is available at <a target="_blank" rel="noopener" href="https://github.com/krafton-ai/Rare-to-Frequent">https://github.com/krafton-ai/Rare-to-Frequent</a>. </p>
<blockquote>
<p>当前先进的文本到图像（T2I）扩散模型在生成罕见概念组合方面经常遇到困难，例如具有不寻常属性的物体。在本文中，我们展示了大语言模型（LLM）指导可以显著增强扩散模型在这种罕见概念上的组合生成能力。我们首先进行实证和理论分析，证明在扩散采样过程中暴露与目标罕见概念相关的频繁概念可以产生更精确的概念组合。基于此，我们提出了一种无需训练的方法R2F，它利用LLM中的丰富语义知识，通过扩散推理，规划和执行从罕见到频繁的整体概念指导。我们的框架灵活适用于任何预训练的扩散模型和语言大模型，并能无缝集成到区域引导的扩散方法中。在包括我们新提出的基准测试集RareBench在内的三个数据集上进行的大量实验表明，R2F在各种包含罕见概念组合的提示下，显著超越了SD3.0和FLUX模型，文本到图像的匹配度提高了高达28.1%。代码可在<a target="_blank" rel="noopener" href="https://github.com/krafton-ai/Rare-to-Frequent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/krafton-ai/Rare-to-Frequent找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22376v2">PDF</a> </p>
<p><strong>Summary</strong><br>     文本到图像（T2I）扩散模型在生成罕见概念组合时面临挑战。本文展示通过大型语言模型（LLM）指导，可以显著提高扩散模型在罕见概念上的组合生成能力。通过实证和理论分析，本文发现扩散采样过程中暴露与目标罕见概念相关的频繁概念，可以更准确地进行概念组合。基于此，本文提出了一种无需训练的方法R2F，利用LLM中的丰富语义知识，通过扩散推断，实现罕见到频繁的概念指导。该方法灵活适用于任何预训练的扩散模型和LLM，并可无缝集成到区域引导扩散方法中。在包括新提出的基准测试集RareBench在内的三个数据集上进行的广泛实验表明，R2F在T2I对齐方面显著超越了现有模型，包括SD3.0和FLUX，提高了高达28.1%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像（T2I）扩散模型在生成罕见概念组合时存在挑战。</li>
<li>大型语言模型（LLM）指导有助于提高扩散模型在罕见概念上的组合生成能力。</li>
<li>在扩散采样过程中暴露与目标罕见概念相关的频繁概念，能更准确地进行概念组合。</li>
<li>提出了一种无需训练的方法R2F，通过利用LLM的丰富语义知识，实现罕见到频繁的概念指导。</li>
<li>R2F方法灵活适用于各种预训练的扩散模型和LLM，并可集成到区域引导扩散方法中。</li>
<li>在多个数据集上的实验表明，R2F在T2I对齐方面显著优于现有模型。</li>
<li>R2F方法的代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22376">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f38bfa2aa23976e69d80156a347001c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32bf1a62a31a0a65b6d454b9ac0c3ed2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a965b9e72af2fb57f65e2d7e8ce31095.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edc7c07638dde3f387d795fc2f62f7b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be8ec4db92f65e4e056ca5766c9e89ec.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Wavelet-Driven-Generalizable-Framework-for-Deepfake-Face-Forgery-Detection"><a href="#Wavelet-Driven-Generalizable-Framework-for-Deepfake-Face-Forgery-Detection" class="headerlink" title="Wavelet-Driven Generalizable Framework for Deepfake Face Forgery   Detection"></a>Wavelet-Driven Generalizable Framework for Deepfake Face Forgery   Detection</h2><p><strong>Authors:Lalith Bharadwaj Baru, Rohit Boddeda, Shilhora Akshay Patel, Sai Mohan Gajapaka</strong></p>
<p>The evolution of digital image manipulation, particularly with the advancement of deep generative models, significantly challenges existing deepfake detection methods, especially when the origin of the deepfake is obscure. To tackle the increasing complexity of these forgeries, we propose \textbf{Wavelet-CLIP}, a deepfake detection framework that integrates wavelet transforms with features derived from the ViT-L&#x2F;14 architecture, pre-trained in the CLIP fashion. Wavelet-CLIP utilizes Wavelet Transforms to deeply analyze both spatial and frequency features from images, thus enhancing the model’s capability to detect sophisticated deepfakes. To verify the effectiveness of our approach, we conducted extensive evaluations against existing state-of-the-art methods for cross-dataset generalization and detection of unseen images generated by standard diffusion models. Our method showcases outstanding performance, achieving an average AUC of 0.749 for cross-data generalization and 0.893 for robustness against unseen deepfakes, outperforming all compared methods. The code can be reproduced from the repo: \url{<a target="_blank" rel="noopener" href="https://github.com/lalithbharadwajbaru/Wavelet-CLIP%7D">https://github.com/lalithbharadwajbaru/Wavelet-CLIP}</a> </p>
<blockquote>
<p>数字图像操作的演变，尤其是随着深度生成模型的进步，对现有深度伪造检测方法的挑战日益显著，尤其是在深度伪造的来源不明确的情况下。为了应对这些伪造品日益复杂的特性，我们提出了<strong>Wavelet-CLIP</strong>，这是一个深度伪造检测框架，它将小波变换与从ViT-L&#x2F;14架构中派生的特征相结合，以CLIP方式进行预训练。Wavelet-CLIP利用小波变换深入分析图像的空间和频率特征，从而增强模型检测复杂深度伪造的能力。为了验证我们方法的有效性，我们对现有的最先进方法进行了广泛的评估，以实现对跨数据集生成的未见图像进行泛化和检测。我们的方法表现出卓越的性能，在跨数据泛化方面平均AUC达到0.749，在对抗未见深度伪造时达到0.893的AUC，优于所有比较的方法。代码可从仓库重现：<a target="_blank" rel="noopener" href="https://github.com/lalithbharadwajbaru/Wavelet-CLIP">https://github.com/lalithbharadwajbaru/Wavelet-CLIP</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18301v3">PDF</a> 9 Pages, 2 Figures, 3 Tables</p>
<p><strong>Summary</strong><br>     针对数字图像操纵的进化，特别是深度生成模型的进步，对现有的深度伪造检测方法提出了重大挑战，特别是在深度伪造来源不明朗的情况下。为解决这些伪造品日益复杂的局面，我们提出了结合小波变换与ViT-L&#x2F;14架构特征的深度伪造检测框架——Wavelet-CLIP，该框架采用CLIP预训练方式。Wavelet-CLIP利用小波变换深入图像的空间和频率特征分析，从而增强模型检测高级深度伪造的能力。实验证明，我们的方法相较于最新的先进方法，在跨数据集推广和检测标准扩散模型生成的未见图像方面表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数字图像操纵的进化对现有深度伪造检测方法构成挑战。</li>
<li>小波变换用于深入图像的空间和频率特征分析以增强检测能力。</li>
<li>提出了一种名为Wavelet-CLIP的深度伪造检测框架，结合了ViT-L&#x2F;14架构特征和小波变换。</li>
<li>Wavelet-CLIP采用了CLIP预训练方式。</li>
<li>Wavelet-CLIP在跨数据集推广和检测未见图像方面表现出卓越性能。</li>
<li>与现有先进方法相比，Wavelet-CLIP在检测深度伪造方面表现出更高的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e738969b06e87e433b434fbacad88627.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94dff75ac98e22f56576d5f499ff9a59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0404383553ac4256516b6815706e7ba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a142a7fb2a939f9ceea4a23a575b98d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4154a0d6d1d871bf9e4872b01ccfad27.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SpotDiffusion-A-Fast-Approach-For-Seamless-Panorama-Generation-Over-Time"><a href="#SpotDiffusion-A-Fast-Approach-For-Seamless-Panorama-Generation-Over-Time" class="headerlink" title="SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over   Time"></a>SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over   Time</h2><p><strong>Authors:Stanislav Frolov, Brian B. Moser, Andreas Dengel</strong></p>
<p>Generating high-resolution images with generative models has recently been made widely accessible by leveraging diffusion models pre-trained on large-scale datasets. Various techniques, such as MultiDiffusion and SyncDiffusion, have further pushed image generation beyond training resolutions, i.e., from square images to panorama, by merging multiple overlapping diffusion paths or employing gradient descent to maintain perceptual coherence. However, these methods suffer from significant computational inefficiencies due to generating and averaging numerous predictions, which is required in practice to produce high-quality and seamless images. This work addresses this limitation and presents a novel approach that eliminates the need to generate and average numerous overlapping denoising predictions. Our method shifts non-overlapping denoising windows over time, ensuring that seams in one timestep are corrected in the next. This results in coherent, high-resolution images with fewer overall steps. We demonstrate the effectiveness of our approach through qualitative and quantitative evaluations, comparing it with MultiDiffusion, SyncDiffusion, and StitchDiffusion. Our method offers several key benefits, including improved computational efficiency and faster inference times while producing comparable or better image quality. Link to code <a target="_blank" rel="noopener" href="https://github.com/stanifrolov/spotdiffusion">https://github.com/stanifrolov/spotdiffusion</a> </p>
<blockquote>
<p>利用在大规模数据集上预训练的扩散模型，生成模型生成高分辨率图像的方法近期已经变得广泛可用。通过各种技术，如MultiDiffusion和SyncDiffusion，图像生成已经超越了训练分辨率，例如从方形图像到全景图像，通过合并多个重叠的扩散路径或采用梯度下降法来维持感知连贯性。然而，这些方法由于需要生成和平均大量预测来产生高质量、无缝图像，因此存在显著的计算效率低下问题。本研究解决了这一限制，提出了一种新方法，消除了生成和平均多个重叠的去噪预测的需要。我们的方法随时间移动非重叠的去噪窗口，确保一个时间步长的接缝在下一个时间步长中得到修正。这导致在更少的总体步骤中产生连贯的高分辨率图像。我们通过定性和定量评估来展示我们方法的有效性，并将其与MultiDiffusion、SyncDiffusion和StitchDiffusion进行比较。我们的方法提供了几个关键优势，包括提高计算效率和更快的推理时间，同时产生相当或更好的图像质量。代码链接 <a target="_blank" rel="noopener" href="https://github.com/stanifrolov/spotdiffusion">https://github.com/stanifrolov/spotdiffusion</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15507v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://spotdiffusion.github.io/">https://spotdiffusion.github.io/</a></p>
<p><strong>Summary</strong><br>     基于扩散模型预训练的大规模数据集，生成高质量图像已变得广泛可行。近期，通过消除对生成和平均大量重叠降噪预测的需求，一种新方法解决了现有技术如MultiDiffusion和SyncDiffusion的计算效率低下的问题。该方法通过随时间移动非重叠的降噪窗口，确保一个时间步长的接缝在下一个时间步长中得到修正，从而生成连贯的高分辨率图像，总体步骤更少。通过与MultiDiffusion、SyncDiffusion和StitchDiffusion的比较，证明了该方法在计算效率和推理速度方面提供了几个关键优势，同时产生了相当或更好的图像质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型预训练的大规模数据集使得生成高质量图像变得广泛可行。</li>
<li>现有技术如MultiDiffusion和SyncDiffusion虽然能够生成高分辨率图像，但存在计算效率低下的问题。</li>
<li>新方法通过消除对生成和平均大量重叠降噪预测的需求，提高了计算效率。</li>
<li>该方法通过移动非重叠的降噪窗口，确保图像生成的连贯性，并减少总体步骤。</li>
<li>该方法与现有技术相比，具有更高的计算效率和更快的推理速度。</li>
<li>新方法生成的图像质量相当或更好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15507">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bbcdcdfafccf98bb721a511936a7668c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e65c52b05f22694b8c642616003092bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82e163d7182da59d558eedbfe863351b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2398949c8a6a90322150d808e576aec2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d867b73a9d27c096e7b8b35d73bc53ab.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-09/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-09/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-06ae655fe7bce8faac4aa64ec52359e7.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-09  LM-Net A Light-weight and Multi-scale Network for Medical Image   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-09/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2fb4d2f96d8e3cb99e5fb273598e548c.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-01-09  NeuralSVG An Implicit Representation for Text-to-Vector Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">8926.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
