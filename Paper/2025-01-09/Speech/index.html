<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-01-09  Effective and Efficient Mixed Precision Quantization of Speech   Foundation Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5aafc2f07ed1af74324b88f06ced2acc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    38 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-09-更新"><a href="#2025-01-09-更新" class="headerlink" title="2025-01-09 更新"></a>2025-01-09 更新</h1><h2 id="Effective-and-Efficient-Mixed-Precision-Quantization-of-Speech-Foundation-Models"><a href="#Effective-and-Efficient-Mixed-Precision-Quantization-of-Speech-Foundation-Models" class="headerlink" title="Effective and Efficient Mixed Precision Quantization of Speech   Foundation Models"></a>Effective and Efficient Mixed Precision Quantization of Speech   Foundation Models</h2><p><strong>Authors:Haoning Xu, Zhaoqing Li, Zengrui Jin, Huimeng Wang, Youjun Chen, Guinan Li, Mengzhe Geng, Shujie Hu, Jiajun Deng, Xunying Liu</strong></p>
<p>This paper presents a novel mixed-precision quantization approach for speech foundation models that tightly integrates mixed-precision learning and quantized model parameter estimation into one single model compression stage. Experiments conducted on LibriSpeech dataset with fine-tuned wav2vec2.0-base and HuBERT-large models suggest the resulting mixed-precision quantized models increased the lossless compression ratio by factors up to 1.7x and 1.9x over the respective uniform-precision and two-stage mixed-precision quantized baselines that perform precision learning and model parameters quantization in separate and disjointed stages, while incurring no statistically word error rate (WER) increase over the 32-bit full-precision models. The system compression time of wav2vec2.0-base and HuBERT-large models is reduced by up to 1.9 and 1.5 times over the two-stage mixed-precision baselines, while both produce lower WERs. The best-performing 3.5-bit mixed-precision quantized HuBERT-large model produces a lossless compression ratio of 8.6x over the 32-bit full-precision system. </p>
<blockquote>
<p>本文提出了一种用于语音基础模型的新型混合精度量化方法，该方法将混合精度学习和量化模型参数估计紧密集成到一个单一的模型压缩阶段中。在LibriSpeech数据集上进行的实验，对fine-tuned的wav2vec2.0-base和HuBERT-large模型显示，所得混合精度量化模型相对于各自的均匀精度和两阶段混合精度量化基线，无损压缩比提高了最高达1.7倍和1.9倍。这些基线在不同的阶段进行精度学习和模型参数量化，彼此独立。同时，相对于32位全精度模型，混合精度量化模型没有产生统计上的词错误率（WER）增加。对于wav2vec2.0-base和HuBERT-large模型的系统压缩时间，相对于两阶段混合精度基线减少了最多达1.9倍和1.5倍，同时两者都产生了更低的WER。表现最佳的3.5位混合精度量化HuBERT-large模型相对于32位全精度系统实现了8.6倍的无损压缩比。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03643v1">PDF</a> To appear at IEEE ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新颖的混合精度量化方法，用于语音基础模型。该方法将混合精度学习和量化模型参数估计紧密集成到一个单一的模型压缩阶段。实验表明，该方法的压缩比例最高可达均匀精度方法的1.7倍和两阶段混合精度量化方法的1.9倍，且并未导致单词错误率（WER）的显著提高。此外，此方法还能有效减少模型的系统压缩时间。对于最好的表现的混合精度量化的HuBERT-large模型，相对于32位全精度系统，其无损压缩比例达到了惊人的8.6倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文提出了一种新的混合精度量化方法，结合了混合精度学习与模型参数量化的单一压缩阶段。</li>
<li>在LibriSpeech数据集上进行的实验验证了这种方法的效率，无损压缩比例显著提高。</li>
<li>对比均匀精度方法和两阶段混合精度量化方法，新方法表现出更高的压缩效率和更低的WER。</li>
<li>对于HuBERT-large模型，最佳表现的混合精度量化模型实现了高达8.6倍的无损压缩比例。</li>
<li>该方法减少了模型的系统压缩时间。</li>
<li>新方法在提高压缩效率的同时，未增加统计上的WER。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03643">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cd30d7f14b7d1a80352f08fb38f0dc2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7da3f13c4780c202a3b7fe195778edfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fabf51ae269118ac4aadb58bc578922.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-for-Pathological-Speech-A-Survey"><a href="#Deep-Learning-for-Pathological-Speech-A-Survey" class="headerlink" title="Deep Learning for Pathological Speech: A Survey"></a>Deep Learning for Pathological Speech: A Survey</h2><p><strong>Authors:Shakeel A. Sheikh, Md. Sahidullah, Ina Kodrasi</strong></p>
<p>Advancements in spoken language technologies for neurodegenerative speech disorders are crucial for meeting both clinical and technological needs. This overview paper is vital for advancing the field, as it presents a comprehensive review of state-of-the-art methods in pathological speech detection, automatic speech recognition, pathological speech intelligibility enhancement, intelligibility and severity assessment, and data augmentation approaches for pathological speech. It also high-lights key challenges, such as ensuring robustness, privacy, and interpretability. The paper concludes by exploring promising future directions, including the adoption of multimodal approaches and the integration of graph neural networks and large language models to further advance speech technology for neurodegenerative speech disorders </p>
<blockquote>
<p>自然语言技术针对神经退行性疾病言语障碍的进步对于满足临床和技术需求至关重要。这篇综述文章对推进该领域至关重要，因为它全面回顾了病理性言语检测、自动语音识别、病理性言语清晰度增强、清晰度和严重性评估以及病理性言语数据增强方法的最新方法。它还强调了关键挑战，如确保稳健性、隐私性和可解释性。文章最后探讨了有前景的未来方向，包括采用多模式方法和整合图神经网络和大型语言模型，以进一步推动针对神经退行性疾病的言语技术的提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03536v1">PDF</a> Submitted to IEEE JSTSP Special Issue on Modelling and Processing   Language and Speech in Neurodegenerative Disorders</p>
<p><strong>Summary</strong><br>随着神经退行性疾病导致的言语障碍的日益普遍，针对此领域的口语技术进展尤为重要。本文全面综述了当前最先进的方法，包括病理性言语检测、自动语音识别、病理性言语清晰度提升等，同时强调了鲁棒性、隐私和可解释性等关键挑战。未来研究方向包括采用多模态方法和集成图神经网络和大型语言模型等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经退行性疾病导致的言语障碍需要先进的口语技术来满足临床和技术需求。</li>
<li>论文全面综述了病理性言语检测、自动语音识别等领域的最新方法。</li>
<li>论文强调了提高技术鲁棒性、保护隐私和增强可解释性等方面的关键挑战。</li>
<li>多模态方法在未来的口语技术发展中具有潜力。</li>
<li>图神经网络和大型语言模型的集成有望进一步提高神经退行性疾病的口语技术水平。</li>
<li>论文强调了数据扩充方法在病理性言语研究中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03536">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0341a79dfaad280c995b0976531d9f07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d827968e8962ba62fc3e1167fd9690d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26c87e9c9e7b4b4b84a56fd315bdd27f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1325eb5b8384b8cbd8956373fe1b3caf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-586f612c9e7a407596ad42ee6556db8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e24039f9fdf4919e4a3e20b55633c614.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Breaking-Through-the-Spike-Spike-Window-Decoding-for-Accelerated-and-Precise-Automatic-Speech-Recognition"><a href="#Breaking-Through-the-Spike-Spike-Window-Decoding-for-Accelerated-and-Precise-Automatic-Speech-Recognition" class="headerlink" title="Breaking Through the Spike: Spike Window Decoding for Accelerated and   Precise Automatic Speech Recognition"></a>Breaking Through the Spike: Spike Window Decoding for Accelerated and   Precise Automatic Speech Recognition</h2><p><strong>Authors:Wei Zhang, Tian-Hao Zhang, Chao Luo, Hui Zhou, Chao Yang, Xinyuan Qian, Xu-Cheng Yin</strong></p>
<p>Recently, end-to-end automatic speech recognition has become the mainstream approach in both industry and academia. To optimize system performance in specific scenarios, the Weighted Finite-State Transducer (WFST) is extensively used to integrate acoustic and language models, leveraging its capacity to implicitly fuse language models within static graphs, thereby ensuring robust recognition while also facilitating rapid error correction. However, WFST necessitates a frame-by-frame search of CTC posterior probabilities through autoregression, which significantly hampers inference speed. In this work, we thoroughly investigate the spike property of CTC outputs and further propose the conjecture that adjacent frames to non-blank spikes carry semantic information beneficial to the model. Building on this, we propose the Spike Window Decoding algorithm, which greatly improves the inference speed by making the number of frames decoded in WFST linearly related to the number of spiking frames in the CTC output, while guaranteeing the recognition performance. Our method achieves SOTA recognition accuracy with significantly accelerates decoding speed, proven across both AISHELL-1 and large-scale In-House datasets, establishing a pioneering approach for integrating CTC output with WFST. </p>
<blockquote>
<p>最近，端到端的自动语音识别已成为业界和学术界的主流方法。为了优化特定场景的系统性能，广泛采用加权有限状态转换器（WFST）来整合声学模型和语言模型，利用其隐式融合静态图内语言模型的能力，从而确保稳健的识别并促进快速错误纠正。然而，WFST需要通过自回归对CTC后验概率进行逐帧搜索，这显著阻碍了推理速度。在这项工作中，我们深入研究了CTC输出的突发特性，并进一步提出了猜想，即非空白突发的相邻帧携带对模型有益的语义信息。在此基础上，我们提出了突发窗口解码算法，通过使WFST中解码的帧数线性相关于CTC输出中的突发帧数，大大提高了推理速度，同时保证识别性能。我们的方法在AI壳牌语音1号语料库以及大规模内部数据集中均实现了最先进的识别精度和显著加速的解码速度，为整合CTC输出和WFST建立了开创性的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03257v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了端到端的自动语音识别技术在特定场景下的优化问题。文章指出，尽管加权有限状态转换器（WFST）能够融合声学模型和语言模型，保证识别的稳健性和错误修正的迅速性，但其需要进行帧到帧的CTC后验概率搜索，严重影响了推理速度。研究提出了Spike Window解码算法，通过基于CTC输出中的Spike属性进行解码，在保证识别性能的同时，极大地提高了推理速度。该算法在AISHELL-1和大规模内部数据集上实现了最先进的识别精度和快速解码速度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>端到端的自动语音识别技术已成为行业与学术界的主流。</li>
<li>加权有限状态转换器（WFST）能够融合声学模型和语言模型，确保稳健的识别并促进快速错误修正。</li>
<li>WFST的帧到帧搜索限制了推理速度。</li>
<li>研究提出了Spike Window解码算法，该算法基于CTC输出的Spike属性进行解码。</li>
<li>Spike Window解码算法在保证识别性能的同时，显著提高了解码速度。</li>
<li>该方法在AISHELL-1和大规模内部数据集上实现了最先进的识别精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03257">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3b206ffa405ea400c6d516835443d56f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd56cd3489c4aa7a9559b0aef88bdde9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37eff64472d40f3e5aaea579cd2a88d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8e67586aae890ef489c95561cdff654.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Samba-ASR-State-Of-The-Art-Speech-Recognition-Leveraging-Structured-State-Space-Models"><a href="#Samba-ASR-State-Of-The-Art-Speech-Recognition-Leveraging-Structured-State-Space-Models" class="headerlink" title="Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured   State-Space Models"></a>Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured   State-Space Models</h2><p><strong>Authors:Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi</strong></p>
<p>We propose Samba ASR,the first state of the art Automatic Speech Recognition(ASR)model leveraging the novel Mamba architecture as both encoder and decoder,built on the foundation of state space models(SSMs).Unlike transformerbased ASR models,which rely on self-attention mechanisms to capture dependencies,Samba ASR effectively models both local and global temporal dependencies using efficient statespace dynamics,achieving remarkable performance gains.By addressing the limitations of transformers,such as quadratic scaling with input length and difficulty in handling longrange dependencies,Samba ASR achieves superior accuracy and efficiency.Experimental results demonstrate that Samba ASR surpasses existing opensource transformerbased ASR models across various standard benchmarks,establishing it as the new state of theart in ASR.Extensive evaluations on the benchmark dataset show significant improvements in Word Error Rate(WER),with competitive performance even in lowresource scenarios.Furthermore,the inherent computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.Our contributions include the development of a new Samba ASR architecture for automatic speech recognition(ASR),demonstrating the superiority of structured statespace models(SSMs)over transformer based models for speech sequence processing.We provide a comprehensive evaluation on public benchmarks,showcasing stateoftheart(SOTA)performance,and present an indepth analysis of computational efficiency,robustness to noise,and sequence generalization.This work highlights the viability of Mamba SSMs as a transformerfree alternative for efficient and accurate ASR.By leveraging the advancements of statespace modeling,Samba ASR redefines ASR performance standards and sets a new benchmark for future research in this field. </p>
<blockquote>
<p>我们提出Samba ASR，这是一个利用新型Mamba架构作为编解码器的最先进的语音识别（ASR）模型。它建立在状态空间模型（SSMs）的基础上。不同于依赖自注意力机制捕捉依赖性的基于变压器的ASR模型，Samba ASR使用高效的状态空间动力学有效地对局部和全局时间依赖性进行建模，实现了显著的性能提升。通过解决变压器模型的局限性，如输入长度的二次扩展和难以处理长距离依赖关系，Samba ASR在准确性方面更胜一筹，且效率更高。实验结果表明，Samba ASR在各种标准基准测试中超越了现有的开源基于变压器的ASR模型，成为ASR领域的新技术标杆。在基准数据集上的全面评估显示，其在词错误率（WER）方面取得了显著的改进，即使在资源有限的情况下也表现出竞争力。此外，Mamba架构的固有计算效率和参数优化使得Samba ASR成为各种ASR任务的可扩展和稳健解决方案。我们的贡献包括为自动语音识别（ASR）开发新的Samba ASR架构，证明了结构化状态空间模型（SSMs）在语音序列处理方面优于基于变压器的模型。我们在公共基准测试上进行了全面评估，展示了其卓越的性能，并深入分析了其计算效率、对噪声的鲁棒性和序列泛化能力。这项工作强调了Mamba SSMs作为无变压器的高效准确ASR的可行性。通过利用状态空间建模的进展，Samba ASR重新定义了ASR的性能标准，并为该领域的未来研究设定了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02832v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出一种新型的自动语音识别（ASR）模型——Samba ASR，它采用新颖的Mamba架构作为编码器和解码器，建立在状态空间模型（SSMs）的基础上。与其他基于变压器的ASR模型不同，Samba ASR通过状态空间动态有效地建模局部和全局时间依赖性，实现了显著的性能提升。实验结果表明，Samba ASR在各项标准基准测试中超越了现有的开源基于变压器的ASR模型，成为ASR领域的新技术领先者。它在基准数据集上的词错误率（WER）有显著改善，甚至在低资源场景中也有竞争力。此外，Mamba架构的固有计算效率和参数优化使Samba ASR成为多样化ASR任务的可扩展和稳健解决方案。本研究贡献包括为自动语音识别（ASR）开发新的Samba ASR架构，展示结构化状态空间模型（SSMs）在语音序列处理方面优于基于变压器的模型。我们在公共基准测试上进行了全面评估，展示了其卓越性能，并深入分析了其计算效率、对噪声的鲁棒性和序列泛化能力。这项工作证明了Mamba SSMs作为无变压器的高效、准确ASR的可行性。通过利用状态空间建模的进步，Samba ASR重新定义了ASR的性能标准，为这一领域未来的研究设定了新的基准。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Samba ASR是基于状态空间模型（SSMs）的自动语音识别（ASR）模型，采用Mamba架构作为编码器和解码器。</li>
<li>与基于变压器的ASR模型不同，Samba ASR通过状态空间动态建模局部和全局时间依赖性，实现性能提升。</li>
<li>Samba ASR在各项标准基准测试中表现出卓越性能，超越了现有的开源基于变压器的ASR模型。</li>
<li>它在基准数据集上的词错误率（WER）显著改善，且在低资源场景中也有竞争力。</li>
<li>Mamba架构的固有计算效率和参数优化使Samba ASR适用于多样化ASR任务。</li>
<li>深入研究展示结构化状态空间模型（SSMs）在语音序列处理方面的优势，为ASR设定了新的性能标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-991da57500cd943a6a7f32885c4e172a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improving-Speech-Emotion-Recognition-in-Under-Resourced-Languages-via-Speech-to-Speech-Translation-with-Bootstrapping-Data-Selection"><a href="#Improving-Speech-Emotion-Recognition-in-Under-Resourced-Languages-via-Speech-to-Speech-Translation-with-Bootstrapping-Data-Selection" class="headerlink" title="Improving Speech Emotion Recognition in Under-Resourced Languages via   Speech-to-Speech Translation with Bootstrapping Data Selection"></a>Improving Speech Emotion Recognition in Under-Resourced Languages via   Speech-to-Speech Translation with Bootstrapping Data Selection</h2><p><strong>Authors:Hsi-Che Lin, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee</strong></p>
<p>Speech Emotion Recognition (SER) is a crucial component in developing general-purpose AI agents capable of natural human-computer interaction. However, building robust multilingual SER systems remains challenging due to the scarcity of labeled data in languages other than English and Chinese. In this paper, we propose an approach to enhance SER performance in low SER resource languages by leveraging data from high-resource languages. Specifically, we employ expressive Speech-to-Speech translation (S2ST) combined with a novel bootstrapping data selection pipeline to generate labeled data in the target language. Extensive experiments demonstrate that our method is both effective and generalizable across different upstream models and languages. Our results suggest that this approach can facilitate the development of more scalable and robust multilingual SER systems. </p>
<blockquote>
<p>语音情绪识别（SER）是开发能够与人类进行自然交互的通用人工智能代理的关键组成部分。然而，由于除英语和中文之外的其他语言的标记数据稀缺，构建稳健的多语言SER系统仍然是一个挑战。在本文中，我们提出了一种方法，通过利用高资源语言的数据来提高低资源语言SER的性能。具体来说，我们采用表达性语音到语音翻译（S2ST）结合一种新的自举数据选择管道，以在目标语言中生成标记数据。广泛的实验表明，我们的方法既有效又可在不同的上游模型和语言中通用化。我们的结果表明，这种方法可以促进更可扩展和稳健的多语言SER系统的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10985v2">PDF</a> 5 pages, 2 figures, Accepted to ICASSP 2025</p>
<p><strong>Summary</strong><br>本文提出了通过利用高资源语言的数据来提高低资源语言语音情感识别性能的方法。具体而言，通过采用表现力强的语音到语音翻译结合新颖的自举数据选择流程，在目标语言中生成标记数据。实验证明该方法有效且可跨不同上游模型和语言进行推广，有助于开发更可扩展和稳健的多语种语音情感识别系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音情感识别是开发通用人工智能代理实现自然人机交互的关键技术。</li>
<li>构建多语种语音情感识别系统面临的主要挑战是除英语和中文以外其他语言的标记数据稀缺。</li>
<li>本文提出利用高资源语言的数据来提升低资源语言语音情感识别的性能。</li>
<li>采用表达性强的语音到语音翻译技术结合自举数据选择流程，在目标语言中生成标记数据。</li>
<li>实验证明该方法既有效，又可在不同上游模型和语言间推广。</li>
<li>此方法有助于开发更可扩展和稳健的多语种语音情感识别系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10985">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0b306e42061f71413f090306659eba33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d70c38c564a1fd3e82708589107ac276.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-155d21249ea0621b3ddeb321b4f47dd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e6357eb9478fb3ba1172d6ec612b649.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83c96e4211a79515b45dc3eff7748895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-176d6424a3dce3d801ae296d7d717fe4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Apollo-Band-sequence-Modeling-for-High-Quality-Audio-Restoration"><a href="#Apollo-Band-sequence-Modeling-for-High-Quality-Audio-Restoration" class="headerlink" title="Apollo: Band-sequence Modeling for High-Quality Audio Restoration"></a>Apollo: Band-sequence Modeling for High-Quality Audio Restoration</h2><p><strong>Authors:Kai Li, Yi Luo</strong></p>
<p>Audio restoration has become increasingly significant in modern society, not only due to the demand for high-quality auditory experiences enabled by advanced playback devices, but also because the growing capabilities of generative audio models necessitate high-fidelity audio. Typically, audio restoration is defined as a task of predicting undistorted audio from damaged input, often trained using a GAN framework to balance perception and distortion. Since audio degradation is primarily concentrated in mid- and high-frequency ranges, especially due to codecs, a key challenge lies in designing a generator capable of preserving low-frequency information while accurately reconstructing high-quality mid- and high-frequency content. Inspired by recent advancements in high-sample-rate music separation, speech enhancement, and audio codec models, we propose Apollo, a generative model designed for high-sample-rate audio restoration. Apollo employs an explicit frequency band split module to model the relationships between different frequency bands, allowing for more coherent and higher-quality restored audio. Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently outperforms existing SR-GAN models across various bit rates and music genres, particularly excelling in complex scenarios involving mixtures of multiple instruments and vocals. Apollo significantly improves music restoration quality while maintaining computational efficiency. The source code for Apollo is publicly available at <a target="_blank" rel="noopener" href="https://github.com/JusperLee/Apollo">https://github.com/JusperLee/Apollo</a>. </p>
<blockquote>
<p>音频修复在现代社会变得越来越重要，这不仅仅是因为先进播放设备带来的对高质量听觉体验的需求，也因为生成式音频模型的日益强大的能力需要高保真音频。通常，音频修复被定义为从损坏的输入预测无损音频的任务，通常使用生成对抗网络（GAN）框架进行训练，以平衡感知和失真。由于音频退化主要集中在中频和高频范围内，特别是在编解码器中，一个关键挑战在于设计一种生成器，能够保留低频信息，同时准确重建高质量的中频和高频内容。受高采样率音乐分离、语音增强和音频编解码器模型的最新进展的启发，我们提出了Apollo，这是一个用于高采样率音频修复的生成模型。Apollo采用明确的频带分割模块来模拟不同频带之间的关系，从而生成更连贯、更高质量的修复音频。在MUSDB18-HQ和MoisesDB数据集上评估，Apollo在各种比特率和音乐风格上均优于现有的SR-GAN模型，特别是在涉及多种乐器和人声混合的复杂场景中表现尤为出色。Apollo在提升音乐修复质量的同时保持了计算效率。Apollo的源代码可在<a target="_blank" rel="noopener" href="https://github.com/JusperLee/Apollo">https://github.com/JusperLee/Apollo</a>公开获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08514v2">PDF</a> Accepted by ICASSP 2025, Demo Page: <a target="_blank" rel="noopener" href="https://cslikai.cn/Apollo">https://cslikai.cn/Apollo</a></p>
<p><strong>摘要</strong></p>
<p>随着现代社会对高质量听觉体验的需求日益增长，音频修复技术变得尤为重要。音频修复的任务在于从损坏的输入中预测无损音频，通常使用生成对抗网络（GAN）框架进行训练，以平衡感知和失真。针对音频降质主要集中在中高频范围的问题，尤其是编码器的降质，设计出一个能够保存低频信息并准确重建高质量中高频内容的生成器成为一大挑战。受高采样率音乐分离、语音增强和音频编码模型最新进展的启发，我们提出了Apollo，一个用于高采样率音频修复的生成模型。Apollo采用明确的频带分割模块，模拟不同频带之间的关系，从而生成更为连贯和高质量的音频。在MUSDB18-HQ和MoisesDB数据集上的评估结果表明，Apollo在各种比特率和音乐类型上均优于现有的SR-GAN模型，特别是在涉及多种乐器和人声混合的复杂场景中表现尤为出色。Apollo在提升音乐修复质量的同时，也保持了计算效率。Apollo的源代码已公开在<a target="_blank" rel="noopener" href="https://github.com/JusperLee/Apollo">https://github.com/JusperLee/Apollo</a>。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>音频修复在现代社会中具有重要性，以满足高质量听觉体验的需求。</li>
<li>音频修复的任务是预测从损坏输入中的无损音频，通常使用GAN框架进行训练。</li>
<li>音频降质主要集中在中高频范围，设计能够保存低频信息并重建中高频内容的生成器是挑战之一。</li>
<li>Apollo是一个用于高采样率音频修复的生成模型，采用频带分割模块来提高音频质量。</li>
<li>Apollo在多个数据集上的表现优于现有SR-GAN模型，尤其在复杂场景中表现突出。</li>
<li>Apollo在提高音乐修复质量的同时，也保持了计算效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-449c3f8021d9e93300079fcd694ccf32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dada42a783abd96397de5997152e00b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a806a41f4d7740594ecb558fa1c59a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aafc2f07ed1af74324b88f06ced2acc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9024d03fcacbfc9998b0da74127fec10.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-Faetar-Benchmark-Speech-Recognition-in-a-Very-Under-Resourced-Language"><a href="#The-Faetar-Benchmark-Speech-Recognition-in-a-Very-Under-Resourced-Language" class="headerlink" title="The Faetar Benchmark: Speech Recognition in a Very Under-Resourced   Language"></a>The Faetar Benchmark: Speech Recognition in a Very Under-Resourced   Language</h2><p><strong>Authors:Michael Ong, Sean Robertson, Leo Peckham, Alba Jorquera Jimenez de Aberasturi, Paula Arkhangorodsky, Robin Huo, Aman Sakhardande, Mark Hallap, Naomi Nagy, Ewan Dunbar</strong></p>
<p>We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark corpus designed to push the limits of current approaches to low-resource speech recognition. Faetar, a Franco-Proven\c{c}al variety spoken primarily in Italy, has no standard orthography, has virtually no existing textual or speech resources other than what is included in the benchmark, and is quite different from other forms of Franco-Proven\c{c}al. The corpus comes from field recordings, most of which are noisy, for which only 5 hrs have matching transcriptions, and for which forced alignment is of variable quality. The corpus contains an additional 20 hrs of unlabelled speech. We report baseline results from state-of-the-art multilingual speech foundation models with a best phone error rate of 30.4%, using a pipeline that continues pre-training on the foundation model using the unlabelled set. </p>
<blockquote>
<p>我们介绍了Faetar自动语音识别基准测试，这是一个基准测试语料库，旨在推动当前方法在资源匮乏语音识别方面的极限。Faetar是一种主要在意大利使用的法罗-普罗旺斯方言，它没有标准的正字法，除了基准测试中包含的内容外，几乎没有现有的文本或语音资源，而且与其他形式的法罗-普罗旺斯方言有很大的不同。该语料库来自现场录音，其中大部分是嘈杂的，只有5小时的语音有匹配的转录，而且其强制对齐的质量也是时好时坏。语料库还包含额外的20小时未标注的语音。我们报告了使用最先进的多语言语音基础模型的基线结果，最佳语音错误率为30.4%，使用在基础模型上继续使用未标注集进行预训练的管道。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08103v3">PDF</a> </p>
<p><strong>总结</strong></p>
<p>本文介绍了Faetar自动语音识别基准测试集，这是一个为挑战当前低资源语音识别方法而设计的基准测试集。Faetar是一种主要在意大利使用的法意混合方言，没有标准的正字法，除了基准测试集中包含的之外，几乎没有现有的文本或语音资源，并且与其他形式的法意混合方言有很大差异。该语料库来自现场录音，其中大部分是嘈杂的，只有5小时的录音有匹配的转录文本，强制对齐的质量不一。此外，语料库还包含20小时的未标记语音。报告了使用前沿的多语言语音基础模型的基线结果，最佳音素错误率为30.4%，使用在基础模型上继续对未标记集进行预训练的管道。</p>
<p><strong>要点</strong></p>
<ol>
<li>Faetar自动语音识别基准测试集是为了挑战低资源语音识别方法而设计的。</li>
<li>Faetar是一种特殊的法意混合方言，缺乏标准正字法，资源匮乏。</li>
<li>该语料库主要来自现场录音，部分语音环境嘈杂。</li>
<li>只有5小时的语音有匹配的转录文本，强制对齐的质量存在差异。</li>
<li>语料库还包含20小时的未标记语音。</li>
<li>报告了使用前沿多语言语音基础模型的基线结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08103">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0e674056e547e5230654066362c2c0f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbd387af0af3463a56cec4dec344ef04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99dd7f0a2311b0e2928143a41d6f2c8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-902705ddcaf0b05a0c78dc7607dc89b4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Neural-Speech-and-Audio-Coding-Modern-AI-Technology-Meets-Traditional-Codecs"><a href="#Neural-Speech-and-Audio-Coding-Modern-AI-Technology-Meets-Traditional-Codecs" class="headerlink" title="Neural Speech and Audio Coding: Modern AI Technology Meets Traditional   Codecs"></a>Neural Speech and Audio Coding: Modern AI Technology Meets Traditional   Codecs</h2><p><strong>Authors:Minje Kim, Jan Skoglund</strong></p>
<p>This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs’ output, along with the autoencoder-based end-to-end models and LPCNet–hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques. </p>
<blockquote>
<p>本文探讨了神经网络语音和音频编码系统中基于模型和基于数据的方法的融合。文章重点介绍了语音和音频编解码器的主观评估过程所面临的挑战，并讨论了纯数据驱动方法的局限性，这些方法通常需要构建庞大且低效的架构才能达到基于模型的方法的性能。该研究提出了混合系统作为一个可行的解决方案，通过精心选择的设计增强措施，对常规编解码器的性能进行了重大改进。具体来说，它引入了一种基于神经网络的信号增强器，用于对现有的编解码器输出进行后处理，以及基于自编码器的端到端模型和LPCNet混合系统，该系统将线性预测编码（LPC）与神经网络相结合。此外，文章还深入研究了在自定义特征空间（TF-Codec）或预定义变换域（MDCTNet）内运行的预测模型，并探讨了使用心理声学校准损失函数来训练端到端神经音频编解码器。通过这些研究，文章展示了混合系统在缩小传统基于模型的方法和现代基于数据的技术之间的差距方面所具有的发展潜力，从而推动语音和音频编码领域的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.06954v2">PDF</a> Published in IEEE Signal Processing Magazine</p>
<p><strong>Summary</strong></p>
<p>本文探讨了神经语音和音频编码系统中基于模型和数据驱动方法的融合。文章指出了语音和音频编码主观评估过程所带来的挑战，并讨论了纯粹数据驱动方法的局限性，这些方法通常需要构建庞大的架构才能达到基于模型的方法的性能。研究提出混合系统作为解决方案，通过精心设计改进，对传统编码器的性能进行了显着提高。特别是，它引入了一种基于神经网络的信号增强器，用于对现有编码器的输出进行后处理，以及基于自编码器的端到端模型和LPCNet混合系统，结合线性预测编码（LPC）与神经网络。此外，本文还研究了在自定义特征空间（TF-Codec）或预定义变换域（MDCTNet）内运行的预测模型，并探讨了使用心理声学校准损失函数来训练端到端神经音频编码器的潜力。本文通过混合系统的研究，展示了其缩小传统基于模型的方法和现代数据驱动技术之间差距，推动语音和音频编码领域发展的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>探讨了模型与数据驱动方法在神经语音和音频编码中的融合。</li>
<li>强调了语音和音频编码主观评估过程的挑战。</li>
<li>指出纯粹数据驱动方法的局限性以及为何需要混合系统。</li>
<li>介绍了基于神经网络的信号增强器及其作用。</li>
<li>探讨了结合线性预测编码（LPC）与神经网络的混合系统（LPCNet）。</li>
<li>研究了在自定义特征空间或预定义变换域内运行的预测模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.06954">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db9116f6ee323ca0b3495fb49da010ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67f7b3fd9f255e0da5c144176a3fffda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cb7605d7010b24d8867ea60cd1a9cfa.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Continuously-Learning-New-Words-in-Automatic-Speech-Recognition"><a href="#Continuously-Learning-New-Words-in-Automatic-Speech-Recognition" class="headerlink" title="Continuously Learning New Words in Automatic Speech Recognition"></a>Continuously Learning New Words in Automatic Speech Recognition</h2><p><strong>Authors:Christian Huber, Alexander Waibel</strong></p>
<p>Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities, and domain-specific special words for which little or no labeled data is available. To address the problem of recognizing these words, we propose a self-supervised continual learning approach: Given the audio of a lecture talk with the corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from the literature. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation data set. Continual learning is then performed by training adaptation weights added to the model on this data set. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model. </p>
<blockquote>
<p>尽管最近取得了进展，但自动语音识别（ASR）系统仍然远非完美。典型的错误包括缩写词、命名实体和特定领域的特殊词汇，这些词汇很少或几乎没有相应的标记数据。为了解决识别这些词汇的问题，我们提出了一种自监督的连续学习方法：给定带有相应幻灯片的讲座音频，我们通过使用文献中的记忆增强ASR模型，偏向模型以从幻灯片中解码新词汇。然后，我们对讲座进行推理，收集包含检测到的新词汇的片段，形成一个适应数据集。通过在此数据集上训练模型的附加适应权重，连续学习得以进行。整个过程多次迭代讲座内容。我们表明，通过这种方法，当新词汇出现得更频繁时，我们在这些词汇上的性能有所提高（召回率超过80%），同时保持模型的整体性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.04482v3">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>这篇文本主要讨论的是自动语音识别（ASR）系统存在的问题及其改进方法。针对ASR系统在识别缩写、命名实体和特定领域特殊词汇时出现的典型错误，提出了一种基于自监督的持续学习方法。该方法利用带有相应幻灯片的讲座音频，通过增强记忆的ASR模型，偏向解码幻灯片中的新词汇。然后，对讲座进行推理，收集包含检测到的新词汇的片段，形成一个适应数据集。通过在此数据集上训练模型的适应权重，实现持续学习。该方法能够提高模型对新词的识别性能，并且随着新词出现频率的增加，召回率超过80%，同时保持模型的总体性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音识别（ASR）系统仍存在识别缩写、命名实体和特定领域特殊词汇的挑战。</li>
<li>提出了一种基于自监督的持续学习方法来解决这些问题。</li>
<li>方法利用讲座音频和相应幻灯片，通过增强记忆的ASR模型偏向解码幻灯片中的新词汇。</li>
<li>通过收集包含新词汇的片段形成适应数据集，并进行持续学习。</li>
<li>该方法能够提高模型对新词的识别性能。</li>
<li>随着新词出现频率的增加，召回率超过80%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.04482">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3f4d69edadde240621de0865cfbab705.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98ba785a9f45c2a978dc727172b53d5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24b908eb16062b2c6b7a72537e50d20b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Harnessing-the-Zero-Shot-Power-of-Instruction-Tuned-Large-Language-Model-in-End-to-End-Speech-Recognition"><a href="#Harnessing-the-Zero-Shot-Power-of-Instruction-Tuned-Large-Language-Model-in-End-to-End-Speech-Recognition" class="headerlink" title="Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model   in End-to-End Speech Recognition"></a>Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model   in End-to-End Speech Recognition</h2><p><strong>Authors:Yosuke Higuchi, Tetsuji Ogawa, Tetsunori Kobayashi</strong></p>
<p>We propose to utilize an instruction-tuned large language model (LLM) for guiding the text generation process in automatic speech recognition (ASR). Modern large language models (LLMs) are adept at performing various text generation tasks through zero-shot learning, prompted with instructions designed for specific objectives. This paper explores the potential of LLMs to derive linguistic information that can facilitate text generation in end-to-end ASR models. Specifically, we instruct an LLM to correct grammatical errors in an ASR hypothesis and use the LLM-derived representations to refine the output further. The proposed model is built on the joint CTC and attention architecture, with the LLM serving as a front-end feature extractor for the decoder. The ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding and fed into the LLM along with a specific instruction. The decoder subsequently takes as input the LLM output to perform token predictions, combining acoustic information from the encoder and the powerful linguistic information provided by the LLM. Experimental results show that the proposed LLM-guided model achieves a relative gain of approximately 13% in word error rates across major benchmarks. </p>
<blockquote>
<p>我们提议在自动语音识别（ASR）的文本生成过程中，利用经过指令训练的大型语言模型（LLM）进行引导。现代大型语言模型（LLM）擅长通过零样本学习完成各种文本生成任务，这些任务通过针对特定目标的指令来提示。本文探讨了LLM在端到端ASR模型中用于促进文本生成的潜力。具体来说，我们指导LLM纠正ASR假设中的语法错误，并使用LLM生成的表示来进一步完善输出。所提出的模型建立在连接时序分类（CTC）和注意力架构之上，LLM作为解码器的前端特征提取器。需要纠正的ASR假设是通过CTC解码从编码器获得的，并与特定指令一起输入到LLM中。解码器随后以LLM输出作为输入来进行令牌预测，结合编码器提供的音频信息和LLM提供的强大的语言信息。实验结果表明，所提出的LLM引导模型在主要基准测试上实现了约13%的词错误率相对增益。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10524v3">PDF</a> Accepted to ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>利用指令优化的大型语言模型（LLM）指导自动语音识别（ASR）中的文本生成过程。LLM能够通过零样本学习执行各种文本生成任务，通过为特定目标设计的指令进行提示。本文探讨了LLM在端到端ASR模型中用于文本生成的潜力。具体来说，我们指导LLM纠正ASR假设中的语法错误，并使用LLM生成的表示来进一步完善输出。提出的模型建立在联合CTC和注意力架构上，LLM作为解码器的前端特征提取器。ASR假设（需进行纠正）由编码器通过CTC解码获得，并与特定指令一起输入LLM。解码器随后以LLM输出作为输入进行令牌预测，结合编码器的声音信息和LLM提供的强大语言信息。实验结果表明，所提出的LLM指导模型在主要基准测试上相对提高了约13％的单词错误率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）可以用于指导自动语音识别（ASR）中的文本生成过程。</li>
<li>LLM能够通过零样本学习执行文本生成任务，并通过特定指令进行优化。</li>
<li>LLM在端到端ASR模型中的潜力在于纠正ASR假设中的语法错误并进一步完善输出。</li>
<li>提出的模型结合CTC和注意力架构，其中LLM作为解码器的前端特征提取器。</li>
<li>ASR假设通过CTC解码获得，并与指令一起输入LLM进行进一步处理。</li>
<li>LLM的输出与编码器的声音信息结合，用于解码器的令牌预测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.10524">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-be53b67817602b3e3c2c7795746ef361.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6e915597c776147881ec29999ece777.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2a67cbc5de1e0b44da8a1f55d0ae8ff.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-09/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-09/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-09/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-25031f9f6a43ea24be70c8a51b2d803d.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-01-09  Diverse Rare Sample Generation with Pretrained GANs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-09/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b8194d2dc72cc85a4663039d2cecc517.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-01-09  CL3DOR Contrastive Learning for 3D Large Multimodal Models via Odds   Ratio on High-Resolution Point Clouds
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
