<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-09  Effective and Efficient Mixed Precision Quantization of Speech   Foundation Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5aafc2f07ed1af74324b88f06ced2acc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    38 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-09-æ›´æ–°"><a href="#2025-01-09-æ›´æ–°" class="headerlink" title="2025-01-09 æ›´æ–°"></a>2025-01-09 æ›´æ–°</h1><h2 id="Effective-and-Efficient-Mixed-Precision-Quantization-of-Speech-Foundation-Models"><a href="#Effective-and-Efficient-Mixed-Precision-Quantization-of-Speech-Foundation-Models" class="headerlink" title="Effective and Efficient Mixed Precision Quantization of Speech   Foundation Models"></a>Effective and Efficient Mixed Precision Quantization of Speech   Foundation Models</h2><p><strong>Authors:Haoning Xu, Zhaoqing Li, Zengrui Jin, Huimeng Wang, Youjun Chen, Guinan Li, Mengzhe Geng, Shujie Hu, Jiajun Deng, Xunying Liu</strong></p>
<p>This paper presents a novel mixed-precision quantization approach for speech foundation models that tightly integrates mixed-precision learning and quantized model parameter estimation into one single model compression stage. Experiments conducted on LibriSpeech dataset with fine-tuned wav2vec2.0-base and HuBERT-large models suggest the resulting mixed-precision quantized models increased the lossless compression ratio by factors up to 1.7x and 1.9x over the respective uniform-precision and two-stage mixed-precision quantized baselines that perform precision learning and model parameters quantization in separate and disjointed stages, while incurring no statistically word error rate (WER) increase over the 32-bit full-precision models. The system compression time of wav2vec2.0-base and HuBERT-large models is reduced by up to 1.9 and 1.5 times over the two-stage mixed-precision baselines, while both produce lower WERs. The best-performing 3.5-bit mixed-precision quantized HuBERT-large model produces a lossless compression ratio of 8.6x over the 32-bit full-precision system. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè¯­éŸ³åŸºç¡€æ¨¡å‹çš„æ–°å‹æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ··åˆç²¾åº¦å­¦ä¹ å’Œé‡åŒ–æ¨¡å‹å‚æ•°ä¼°è®¡ç´§å¯†é›†æˆåˆ°ä¸€ä¸ªå•ä¸€çš„æ¨¡å‹å‹ç¼©é˜¶æ®µä¸­ã€‚åœ¨LibriSpeechæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒï¼Œå¯¹fine-tunedçš„wav2vec2.0-baseå’ŒHuBERT-largeæ¨¡å‹æ˜¾ç¤ºï¼Œæ‰€å¾—æ··åˆç²¾åº¦é‡åŒ–æ¨¡å‹ç›¸å¯¹äºå„è‡ªçš„å‡åŒ€ç²¾åº¦å’Œä¸¤é˜¶æ®µæ··åˆç²¾åº¦é‡åŒ–åŸºçº¿ï¼Œæ— æŸå‹ç¼©æ¯”æé«˜äº†æœ€é«˜è¾¾1.7å€å’Œ1.9å€ã€‚è¿™äº›åŸºçº¿åœ¨ä¸åŒçš„é˜¶æ®µè¿›è¡Œç²¾åº¦å­¦ä¹ å’Œæ¨¡å‹å‚æ•°é‡åŒ–ï¼Œå½¼æ­¤ç‹¬ç«‹ã€‚åŒæ—¶ï¼Œç›¸å¯¹äº32ä½å…¨ç²¾åº¦æ¨¡å‹ï¼Œæ··åˆç²¾åº¦é‡åŒ–æ¨¡å‹æ²¡æœ‰äº§ç”Ÿç»Ÿè®¡ä¸Šçš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å¢åŠ ã€‚å¯¹äºwav2vec2.0-baseå’ŒHuBERT-largeæ¨¡å‹çš„ç³»ç»Ÿå‹ç¼©æ—¶é—´ï¼Œç›¸å¯¹äºä¸¤é˜¶æ®µæ··åˆç²¾åº¦åŸºçº¿å‡å°‘äº†æœ€å¤šè¾¾1.9å€å’Œ1.5å€ï¼ŒåŒæ—¶ä¸¤è€…éƒ½äº§ç”Ÿäº†æ›´ä½çš„WERã€‚è¡¨ç°æœ€ä½³çš„3.5ä½æ··åˆç²¾åº¦é‡åŒ–HuBERT-largeæ¨¡å‹ç›¸å¯¹äº32ä½å…¨ç²¾åº¦ç³»ç»Ÿå®ç°äº†8.6å€çš„æ— æŸå‹ç¼©æ¯”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03643v1">PDF</a> To appear at IEEE ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œç”¨äºè¯­éŸ³åŸºç¡€æ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†æ··åˆç²¾åº¦å­¦ä¹ å’Œé‡åŒ–æ¨¡å‹å‚æ•°ä¼°è®¡ç´§å¯†é›†æˆåˆ°ä¸€ä¸ªå•ä¸€çš„æ¨¡å‹å‹ç¼©é˜¶æ®µã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„å‹ç¼©æ¯”ä¾‹æœ€é«˜å¯è¾¾å‡åŒ€ç²¾åº¦æ–¹æ³•çš„1.7å€å’Œä¸¤é˜¶æ®µæ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•çš„1.9å€ï¼Œä¸”å¹¶æœªå¯¼è‡´å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰çš„æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œæ­¤æ–¹æ³•è¿˜èƒ½æœ‰æ•ˆå‡å°‘æ¨¡å‹çš„ç³»ç»Ÿå‹ç¼©æ—¶é—´ã€‚å¯¹äºæœ€å¥½çš„è¡¨ç°çš„æ··åˆç²¾åº¦é‡åŒ–çš„HuBERT-largeæ¨¡å‹ï¼Œç›¸å¯¹äº32ä½å…¨ç²¾åº¦ç³»ç»Ÿï¼Œå…¶æ— æŸå‹ç¼©æ¯”ä¾‹è¾¾åˆ°äº†æƒŠäººçš„8.6å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œç»“åˆäº†æ··åˆç²¾åº¦å­¦ä¹ ä¸æ¨¡å‹å‚æ•°é‡åŒ–çš„å•ä¸€å‹ç¼©é˜¶æ®µã€‚</li>
<li>åœ¨LibriSpeechæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†è¿™ç§æ–¹æ³•çš„æ•ˆç‡ï¼Œæ— æŸå‹ç¼©æ¯”ä¾‹æ˜¾è‘—æé«˜ã€‚</li>
<li>å¯¹æ¯”å‡åŒ€ç²¾åº¦æ–¹æ³•å’Œä¸¤é˜¶æ®µæ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œæ–°æ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„å‹ç¼©æ•ˆç‡å’Œæ›´ä½çš„WERã€‚</li>
<li>å¯¹äºHuBERT-largeæ¨¡å‹ï¼Œæœ€ä½³è¡¨ç°çš„æ··åˆç²¾åº¦é‡åŒ–æ¨¡å‹å®ç°äº†é«˜è¾¾8.6å€çš„æ— æŸå‹ç¼©æ¯”ä¾‹ã€‚</li>
<li>è¯¥æ–¹æ³•å‡å°‘äº†æ¨¡å‹çš„ç³»ç»Ÿå‹ç¼©æ—¶é—´ã€‚</li>
<li>æ–°æ–¹æ³•åœ¨æé«˜å‹ç¼©æ•ˆç‡çš„åŒæ—¶ï¼Œæœªå¢åŠ ç»Ÿè®¡ä¸Šçš„WERã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd30d7f14b7d1a80352f08fb38f0dc2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7da3f13c4780c202a3b7fe195778edfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fabf51ae269118ac4aadb58bc578922.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-for-Pathological-Speech-A-Survey"><a href="#Deep-Learning-for-Pathological-Speech-A-Survey" class="headerlink" title="Deep Learning for Pathological Speech: A Survey"></a>Deep Learning for Pathological Speech: A Survey</h2><p><strong>Authors:Shakeel A. Sheikh, Md. Sahidullah, Ina Kodrasi</strong></p>
<p>Advancements in spoken language technologies for neurodegenerative speech disorders are crucial for meeting both clinical and technological needs. This overview paper is vital for advancing the field, as it presents a comprehensive review of state-of-the-art methods in pathological speech detection, automatic speech recognition, pathological speech intelligibility enhancement, intelligibility and severity assessment, and data augmentation approaches for pathological speech. It also high-lights key challenges, such as ensuring robustness, privacy, and interpretability. The paper concludes by exploring promising future directions, including the adoption of multimodal approaches and the integration of graph neural networks and large language models to further advance speech technology for neurodegenerative speech disorders </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€æŠ€æœ¯é’ˆå¯¹ç¥ç»é€€è¡Œæ€§ç–¾ç—…è¨€è¯­éšœç¢çš„è¿›æ­¥å¯¹äºæ»¡è¶³ä¸´åºŠå’ŒæŠ€æœ¯éœ€æ±‚è‡³å…³é‡è¦ã€‚è¿™ç¯‡ç»¼è¿°æ–‡ç« å¯¹æ¨è¿›è¯¥é¢†åŸŸè‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå…¨é¢å›é¡¾äº†ç—…ç†æ€§è¨€è¯­æ£€æµ‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€ç—…ç†æ€§è¨€è¯­æ¸…æ™°åº¦å¢å¼ºã€æ¸…æ™°åº¦å’Œä¸¥é‡æ€§è¯„ä¼°ä»¥åŠç—…ç†æ€§è¨€è¯­æ•°æ®å¢å¼ºæ–¹æ³•çš„æœ€æ–°æ–¹æ³•ã€‚å®ƒè¿˜å¼ºè°ƒäº†å…³é”®æŒ‘æˆ˜ï¼Œå¦‚ç¡®ä¿ç¨³å¥æ€§ã€éšç§æ€§å’Œå¯è§£é‡Šæ€§ã€‚æ–‡ç« æœ€åæ¢è®¨äº†æœ‰å‰æ™¯çš„æœªæ¥æ–¹å‘ï¼ŒåŒ…æ‹¬é‡‡ç”¨å¤šæ¨¡å¼æ–¹æ³•å’Œæ•´åˆå›¾ç¥ç»ç½‘ç»œå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥æ¨åŠ¨é’ˆå¯¹ç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„è¨€è¯­æŠ€æœ¯çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03536v1">PDF</a> Submitted to IEEE JSTSP Special Issue on Modelling and Processing   Language and Speech in Neurodegenerative Disorders</p>
<p><strong>Summary</strong><br>éšç€ç¥ç»é€€è¡Œæ€§ç–¾ç—…å¯¼è‡´çš„è¨€è¯­éšœç¢çš„æ—¥ç›Šæ™®éï¼Œé’ˆå¯¹æ­¤é¢†åŸŸçš„å£è¯­æŠ€æœ¯è¿›å±•å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç—…ç†æ€§è¨€è¯­æ£€æµ‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€ç—…ç†æ€§è¨€è¯­æ¸…æ™°åº¦æå‡ç­‰ï¼ŒåŒæ—¶å¼ºè°ƒäº†é²æ£’æ€§ã€éšç§å’Œå¯è§£é‡Šæ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚æœªæ¥ç ”ç©¶æ–¹å‘åŒ…æ‹¬é‡‡ç”¨å¤šæ¨¡æ€æ–¹æ³•å’Œé›†æˆå›¾ç¥ç»ç½‘ç»œå’Œå¤§å‹è¯­è¨€æ¨¡å‹ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»é€€è¡Œæ€§ç–¾ç—…å¯¼è‡´çš„è¨€è¯­éšœç¢éœ€è¦å…ˆè¿›çš„å£è¯­æŠ€æœ¯æ¥æ»¡è¶³ä¸´åºŠå’ŒæŠ€æœ¯éœ€æ±‚ã€‚</li>
<li>è®ºæ–‡å…¨é¢ç»¼è¿°äº†ç—…ç†æ€§è¨€è¯­æ£€æµ‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸçš„æœ€æ–°æ–¹æ³•ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†æé«˜æŠ€æœ¯é²æ£’æ€§ã€ä¿æŠ¤éšç§å’Œå¢å¼ºå¯è§£é‡Šæ€§ç­‰æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>å¤šæ¨¡æ€æ–¹æ³•åœ¨æœªæ¥çš„å£è¯­æŠ€æœ¯å‘å±•ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å›¾ç¥ç»ç½‘ç»œå’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆæœ‰æœ›è¿›ä¸€æ­¥æé«˜ç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„å£è¯­æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†æ•°æ®æ‰©å……æ–¹æ³•åœ¨ç—…ç†æ€§è¨€è¯­ç ”ç©¶ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0341a79dfaad280c995b0976531d9f07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d827968e8962ba62fc3e1167fd9690d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26c87e9c9e7b4b4b84a56fd315bdd27f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1325eb5b8384b8cbd8956373fe1b3caf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-586f612c9e7a407596ad42ee6556db8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e24039f9fdf4919e4a3e20b55633c614.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Breaking-Through-the-Spike-Spike-Window-Decoding-for-Accelerated-and-Precise-Automatic-Speech-Recognition"><a href="#Breaking-Through-the-Spike-Spike-Window-Decoding-for-Accelerated-and-Precise-Automatic-Speech-Recognition" class="headerlink" title="Breaking Through the Spike: Spike Window Decoding for Accelerated and   Precise Automatic Speech Recognition"></a>Breaking Through the Spike: Spike Window Decoding for Accelerated and   Precise Automatic Speech Recognition</h2><p><strong>Authors:Wei Zhang, Tian-Hao Zhang, Chao Luo, Hui Zhou, Chao Yang, Xinyuan Qian, Xu-Cheng Yin</strong></p>
<p>Recently, end-to-end automatic speech recognition has become the mainstream approach in both industry and academia. To optimize system performance in specific scenarios, the Weighted Finite-State Transducer (WFST) is extensively used to integrate acoustic and language models, leveraging its capacity to implicitly fuse language models within static graphs, thereby ensuring robust recognition while also facilitating rapid error correction. However, WFST necessitates a frame-by-frame search of CTC posterior probabilities through autoregression, which significantly hampers inference speed. In this work, we thoroughly investigate the spike property of CTC outputs and further propose the conjecture that adjacent frames to non-blank spikes carry semantic information beneficial to the model. Building on this, we propose the Spike Window Decoding algorithm, which greatly improves the inference speed by making the number of frames decoded in WFST linearly related to the number of spiking frames in the CTC output, while guaranteeing the recognition performance. Our method achieves SOTA recognition accuracy with significantly accelerates decoding speed, proven across both AISHELL-1 and large-scale In-House datasets, establishing a pioneering approach for integrating CTC output with WFST. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç«¯åˆ°ç«¯çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å·²æˆä¸ºä¸šç•Œå’Œå­¦æœ¯ç•Œçš„ä¸»æµæ–¹æ³•ã€‚ä¸ºäº†ä¼˜åŒ–ç‰¹å®šåœºæ™¯çš„ç³»ç»Ÿæ€§èƒ½ï¼Œå¹¿æ³›é‡‡ç”¨åŠ æƒæœ‰é™çŠ¶æ€è½¬æ¢å™¨ï¼ˆWFSTï¼‰æ¥æ•´åˆå£°å­¦æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨å…¶éšå¼èåˆé™æ€å›¾å†…è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»è€Œç¡®ä¿ç¨³å¥çš„è¯†åˆ«å¹¶ä¿ƒè¿›å¿«é€Ÿé”™è¯¯çº æ­£ã€‚ç„¶è€Œï¼ŒWFSTéœ€è¦é€šè¿‡è‡ªå›å½’å¯¹CTCåéªŒæ¦‚ç‡è¿›è¡Œé€å¸§æœç´¢ï¼Œè¿™æ˜¾è‘—é˜»ç¢äº†æ¨ç†é€Ÿåº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†CTCè¾“å‡ºçš„çªå‘ç‰¹æ€§ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†çŒœæƒ³ï¼Œå³éç©ºç™½çªå‘çš„ç›¸é‚»å¸§æºå¸¦å¯¹æ¨¡å‹æœ‰ç›Šçš„è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†çªå‘çª—å£è§£ç ç®—æ³•ï¼Œé€šè¿‡ä½¿WFSTä¸­è§£ç çš„å¸§æ•°çº¿æ€§ç›¸å…³äºCTCè¾“å‡ºä¸­çš„çªå‘å¸§æ•°ï¼Œå¤§å¤§æé«˜äº†æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿è¯è¯†åˆ«æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨AIå£³ç‰Œè¯­éŸ³1å·è¯­æ–™åº“ä»¥åŠå¤§è§„æ¨¡å†…éƒ¨æ•°æ®é›†ä¸­å‡å®ç°äº†æœ€å…ˆè¿›çš„è¯†åˆ«ç²¾åº¦å’Œæ˜¾è‘—åŠ é€Ÿçš„è§£ç é€Ÿåº¦ï¼Œä¸ºæ•´åˆCTCè¾“å‡ºå’ŒWFSTå»ºç«‹äº†å¼€åˆ›æ€§çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03257v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç«¯åˆ°ç«¯çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„ä¼˜åŒ–é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡åŠ æƒæœ‰é™çŠ¶æ€è½¬æ¢å™¨ï¼ˆWFSTï¼‰èƒ½å¤Ÿèåˆå£°å­¦æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹ï¼Œä¿è¯è¯†åˆ«çš„ç¨³å¥æ€§å’Œé”™è¯¯ä¿®æ­£çš„è¿…é€Ÿæ€§ï¼Œä½†å…¶éœ€è¦è¿›è¡Œå¸§åˆ°å¸§çš„CTCåéªŒæ¦‚ç‡æœç´¢ï¼Œä¸¥é‡å½±å“äº†æ¨ç†é€Ÿåº¦ã€‚ç ”ç©¶æå‡ºäº†Spike Windowè§£ç ç®—æ³•ï¼Œé€šè¿‡åŸºäºCTCè¾“å‡ºä¸­çš„Spikeå±æ€§è¿›è¡Œè§£ç ï¼Œåœ¨ä¿è¯è¯†åˆ«æ€§èƒ½çš„åŒæ—¶ï¼Œæå¤§åœ°æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚è¯¥ç®—æ³•åœ¨AISHELL-1å’Œå¤§è§„æ¨¡å†…éƒ¨æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¯†åˆ«ç²¾åº¦å’Œå¿«é€Ÿè§£ç é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«¯åˆ°ç«¯çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯å·²æˆä¸ºè¡Œä¸šä¸å­¦æœ¯ç•Œçš„ä¸»æµã€‚</li>
<li>åŠ æƒæœ‰é™çŠ¶æ€è½¬æ¢å™¨ï¼ˆWFSTï¼‰èƒ½å¤Ÿèåˆå£°å­¦æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹ï¼Œç¡®ä¿ç¨³å¥çš„è¯†åˆ«å¹¶ä¿ƒè¿›å¿«é€Ÿé”™è¯¯ä¿®æ­£ã€‚</li>
<li>WFSTçš„å¸§åˆ°å¸§æœç´¢é™åˆ¶äº†æ¨ç†é€Ÿåº¦ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†Spike Windowè§£ç ç®—æ³•ï¼Œè¯¥ç®—æ³•åŸºäºCTCè¾“å‡ºçš„Spikeå±æ€§è¿›è¡Œè§£ç ã€‚</li>
<li>Spike Windowè§£ç ç®—æ³•åœ¨ä¿è¯è¯†åˆ«æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†è§£ç é€Ÿåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨AISHELL-1å’Œå¤§è§„æ¨¡å†…éƒ¨æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¯†åˆ«ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b206ffa405ea400c6d516835443d56f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd56cd3489c4aa7a9559b0aef88bdde9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37eff64472d40f3e5aaea579cd2a88d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8e67586aae890ef489c95561cdff654.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Samba-ASR-State-Of-The-Art-Speech-Recognition-Leveraging-Structured-State-Space-Models"><a href="#Samba-ASR-State-Of-The-Art-Speech-Recognition-Leveraging-Structured-State-Space-Models" class="headerlink" title="Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured   State-Space Models"></a>Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured   State-Space Models</h2><p><strong>Authors:Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi</strong></p>
<p>We propose Samba ASR,the first state of the art Automatic Speech Recognition(ASR)model leveraging the novel Mamba architecture as both encoder and decoder,built on the foundation of state space models(SSMs).Unlike transformerbased ASR models,which rely on self-attention mechanisms to capture dependencies,Samba ASR effectively models both local and global temporal dependencies using efficient statespace dynamics,achieving remarkable performance gains.By addressing the limitations of transformers,such as quadratic scaling with input length and difficulty in handling longrange dependencies,Samba ASR achieves superior accuracy and efficiency.Experimental results demonstrate that Samba ASR surpasses existing opensource transformerbased ASR models across various standard benchmarks,establishing it as the new state of theart in ASR.Extensive evaluations on the benchmark dataset show significant improvements in Word Error Rate(WER),with competitive performance even in lowresource scenarios.Furthermore,the inherent computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.Our contributions include the development of a new Samba ASR architecture for automatic speech recognition(ASR),demonstrating the superiority of structured statespace models(SSMs)over transformer based models for speech sequence processing.We provide a comprehensive evaluation on public benchmarks,showcasing stateoftheart(SOTA)performance,and present an indepth analysis of computational efficiency,robustness to noise,and sequence generalization.This work highlights the viability of Mamba SSMs as a transformerfree alternative for efficient and accurate ASR.By leveraging the advancements of statespace modeling,Samba ASR redefines ASR performance standards and sets a new benchmark for future research in this field. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºSamba ASRï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ–°å‹Mambaæ¶æ„ä½œä¸ºç¼–è§£ç å™¨çš„æœ€å…ˆè¿›çš„è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚å®ƒå»ºç«‹åœ¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„åŸºç¡€ä¸Šã€‚ä¸åŒäºä¾èµ–è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰ä¾èµ–æ€§çš„åŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ï¼ŒSamba ASRä½¿ç”¨é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦æœ‰æ•ˆåœ°å¯¹å±€éƒ¨å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡è§£å†³å˜å‹å™¨æ¨¡å‹çš„å±€é™æ€§ï¼Œå¦‚è¾“å…¥é•¿åº¦çš„äºŒæ¬¡æ‰©å±•å’Œéš¾ä»¥å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼ŒSamba ASRåœ¨å‡†ç¡®æ€§æ–¹é¢æ›´èƒœä¸€ç­¹ï¼Œä¸”æ•ˆç‡æ›´é«˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSamba ASRåœ¨å„ç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼€æºåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ï¼Œæˆä¸ºASRé¢†åŸŸçš„æ–°æŠ€æœ¯æ ‡æ†ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶åœ¨è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå³ä½¿åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒMambaæ¶æ„çš„å›ºæœ‰è®¡ç®—æ•ˆç‡å’Œå‚æ•°ä¼˜åŒ–ä½¿å¾—Samba ASRæˆä¸ºå„ç§ASRä»»åŠ¡çš„å¯æ‰©å±•å’Œç¨³å¥è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¼€å‘æ–°çš„Samba ASRæ¶æ„ï¼Œè¯æ˜äº†ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨è¯­éŸ³åºåˆ—å¤„ç†æ–¹é¢ä¼˜äºåŸºäºå˜å‹å™¨çš„æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶æ·±å…¥åˆ†æäº†å…¶è®¡ç®—æ•ˆç‡ã€å¯¹å™ªå£°çš„é²æ£’æ€§å’Œåºåˆ—æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†Mamba SSMsä½œä¸ºæ— å˜å‹å™¨çš„é«˜æ•ˆå‡†ç¡®ASRçš„å¯è¡Œæ€§ã€‚é€šè¿‡åˆ©ç”¨çŠ¶æ€ç©ºé—´å»ºæ¨¡çš„è¿›å±•ï¼ŒSamba ASRé‡æ–°å®šä¹‰äº†ASRçš„æ€§èƒ½æ ‡å‡†ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02832v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹â€”â€”Samba ASRï¼Œå®ƒé‡‡ç”¨æ–°é¢–çš„Mambaæ¶æ„ä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå»ºç«‹åœ¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„åŸºç¡€ä¸Šã€‚ä¸å…¶ä»–åŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ä¸åŒï¼ŒSamba ASRé€šè¿‡çŠ¶æ€ç©ºé—´åŠ¨æ€æœ‰æ•ˆåœ°å»ºæ¨¡å±€éƒ¨å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSamba ASRåœ¨å„é¡¹æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼€æºåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ï¼Œæˆä¸ºASRé¢†åŸŸçš„æ–°æŠ€æœ¯é¢†å…ˆè€…ã€‚å®ƒåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æœ‰æ˜¾è‘—æ”¹å–„ï¼Œç”šè‡³åœ¨ä½èµ„æºåœºæ™¯ä¸­ä¹Ÿæœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒMambaæ¶æ„çš„å›ºæœ‰è®¡ç®—æ•ˆç‡å’Œå‚æ•°ä¼˜åŒ–ä½¿Samba ASRæˆä¸ºå¤šæ ·åŒ–ASRä»»åŠ¡çš„å¯æ‰©å±•å’Œç¨³å¥è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶è´¡çŒ®åŒ…æ‹¬ä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¼€å‘æ–°çš„Samba ASRæ¶æ„ï¼Œå±•ç¤ºç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨è¯­éŸ³åºåˆ—å¤„ç†æ–¹é¢ä¼˜äºåŸºäºå˜å‹å™¨çš„æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶å“è¶Šæ€§èƒ½ï¼Œå¹¶æ·±å…¥åˆ†æäº†å…¶è®¡ç®—æ•ˆç‡ã€å¯¹å™ªå£°çš„é²æ£’æ€§å’Œåºåˆ—æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†Mamba SSMsä½œä¸ºæ— å˜å‹å™¨çš„é«˜æ•ˆã€å‡†ç¡®ASRçš„å¯è¡Œæ€§ã€‚é€šè¿‡åˆ©ç”¨çŠ¶æ€ç©ºé—´å»ºæ¨¡çš„è¿›æ­¥ï¼ŒSamba ASRé‡æ–°å®šä¹‰äº†ASRçš„æ€§èƒ½æ ‡å‡†ï¼Œä¸ºè¿™ä¸€é¢†åŸŸæœªæ¥çš„ç ”ç©¶è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Samba ASRæ˜¯åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œé‡‡ç”¨Mambaæ¶æ„ä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨ã€‚</li>
<li>ä¸åŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ä¸åŒï¼ŒSamba ASRé€šè¿‡çŠ¶æ€ç©ºé—´åŠ¨æ€å»ºæ¨¡å±€éƒ¨å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§ï¼Œå®ç°æ€§èƒ½æå‡ã€‚</li>
<li>Samba ASRåœ¨å„é¡¹æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ã€‚</li>
<li>å®ƒåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ˜¾è‘—æ”¹å–„ï¼Œä¸”åœ¨ä½èµ„æºåœºæ™¯ä¸­ä¹Ÿæœ‰ç«äº‰åŠ›ã€‚</li>
<li>Mambaæ¶æ„çš„å›ºæœ‰è®¡ç®—æ•ˆç‡å’Œå‚æ•°ä¼˜åŒ–ä½¿Samba ASRé€‚ç”¨äºå¤šæ ·åŒ–ASRä»»åŠ¡ã€‚</li>
<li>æ·±å…¥ç ”ç©¶å±•ç¤ºç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨è¯­éŸ³åºåˆ—å¤„ç†æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä¸ºASRè®¾å®šäº†æ–°çš„æ€§èƒ½æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-991da57500cd943a6a7f32885c4e172a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improving-Speech-Emotion-Recognition-in-Under-Resourced-Languages-via-Speech-to-Speech-Translation-with-Bootstrapping-Data-Selection"><a href="#Improving-Speech-Emotion-Recognition-in-Under-Resourced-Languages-via-Speech-to-Speech-Translation-with-Bootstrapping-Data-Selection" class="headerlink" title="Improving Speech Emotion Recognition in Under-Resourced Languages via   Speech-to-Speech Translation with Bootstrapping Data Selection"></a>Improving Speech Emotion Recognition in Under-Resourced Languages via   Speech-to-Speech Translation with Bootstrapping Data Selection</h2><p><strong>Authors:Hsi-Che Lin, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee</strong></p>
<p>Speech Emotion Recognition (SER) is a crucial component in developing general-purpose AI agents capable of natural human-computer interaction. However, building robust multilingual SER systems remains challenging due to the scarcity of labeled data in languages other than English and Chinese. In this paper, we propose an approach to enhance SER performance in low SER resource languages by leveraging data from high-resource languages. Specifically, we employ expressive Speech-to-Speech translation (S2ST) combined with a novel bootstrapping data selection pipeline to generate labeled data in the target language. Extensive experiments demonstrate that our method is both effective and generalizable across different upstream models and languages. Our results suggest that this approach can facilitate the development of more scalable and robust multilingual SER systems. </p>
<blockquote>
<p>è¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰æ˜¯å¼€å‘èƒ½å¤Ÿä¸äººç±»è¿›è¡Œè‡ªç„¶äº¤äº’çš„é€šç”¨äººå·¥æ™ºèƒ½ä»£ç†çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œç”±äºé™¤è‹±è¯­å’Œä¸­æ–‡ä¹‹å¤–çš„å…¶ä»–è¯­è¨€çš„æ ‡è®°æ•°æ®ç¨€ç¼ºï¼Œæ„å»ºç¨³å¥çš„å¤šè¯­è¨€SERç³»ç»Ÿä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é«˜èµ„æºè¯­è¨€çš„æ•°æ®æ¥æé«˜ä½èµ„æºè¯­è¨€SERçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨è¡¨è¾¾æ€§è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼ˆS2STï¼‰ç»“åˆä¸€ç§æ–°çš„è‡ªä¸¾æ•°æ®é€‰æ‹©ç®¡é“ï¼Œä»¥åœ¨ç›®æ ‡è¯­è¨€ä¸­ç”Ÿæˆæ ‡è®°æ•°æ®ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¢æœ‰æ•ˆåˆå¯åœ¨ä¸åŒçš„ä¸Šæ¸¸æ¨¡å‹å’Œè¯­è¨€ä¸­é€šç”¨åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥ä¿ƒè¿›æ›´å¯æ‰©å±•å’Œç¨³å¥çš„å¤šè¯­è¨€SERç³»ç»Ÿçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10985v2">PDF</a> 5 pages, 2 figures, Accepted to ICASSP 2025</p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†é€šè¿‡åˆ©ç”¨é«˜èµ„æºè¯­è¨€çš„æ•°æ®æ¥æé«˜ä½èµ„æºè¯­è¨€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½çš„æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡é‡‡ç”¨è¡¨ç°åŠ›å¼ºçš„è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ç»“åˆæ–°é¢–çš„è‡ªä¸¾æ•°æ®é€‰æ‹©æµç¨‹ï¼Œåœ¨ç›®æ ‡è¯­è¨€ä¸­ç”Ÿæˆæ ‡è®°æ•°æ®ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•æœ‰æ•ˆä¸”å¯è·¨ä¸åŒä¸Šæ¸¸æ¨¡å‹å’Œè¯­è¨€è¿›è¡Œæ¨å¹¿ï¼Œæœ‰åŠ©äºå¼€å‘æ›´å¯æ‰©å±•å’Œç¨³å¥çš„å¤šè¯­ç§è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ˜¯å¼€å‘é€šç”¨äººå·¥æ™ºèƒ½ä»£ç†å®ç°è‡ªç„¶äººæœºäº¤äº’çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>æ„å»ºå¤šè¯­ç§è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯é™¤è‹±è¯­å’Œä¸­æ–‡ä»¥å¤–å…¶ä»–è¯­è¨€çš„æ ‡è®°æ•°æ®ç¨€ç¼ºã€‚</li>
<li>æœ¬æ–‡æå‡ºåˆ©ç”¨é«˜èµ„æºè¯­è¨€çš„æ•°æ®æ¥æå‡ä½èµ„æºè¯­è¨€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨è¡¨è¾¾æ€§å¼ºçš„è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘æŠ€æœ¯ç»“åˆè‡ªä¸¾æ•°æ®é€‰æ‹©æµç¨‹ï¼Œåœ¨ç›®æ ‡è¯­è¨€ä¸­ç”Ÿæˆæ ‡è®°æ•°æ®ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•æ—¢æœ‰æ•ˆï¼Œåˆå¯åœ¨ä¸åŒä¸Šæ¸¸æ¨¡å‹å’Œè¯­è¨€é—´æ¨å¹¿ã€‚</li>
<li>æ­¤æ–¹æ³•æœ‰åŠ©äºå¼€å‘æ›´å¯æ‰©å±•å’Œç¨³å¥çš„å¤šè¯­ç§è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b306e42061f71413f090306659eba33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d70c38c564a1fd3e82708589107ac276.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-155d21249ea0621b3ddeb321b4f47dd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e6357eb9478fb3ba1172d6ec612b649.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83c96e4211a79515b45dc3eff7748895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-176d6424a3dce3d801ae296d7d717fe4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Apollo-Band-sequence-Modeling-for-High-Quality-Audio-Restoration"><a href="#Apollo-Band-sequence-Modeling-for-High-Quality-Audio-Restoration" class="headerlink" title="Apollo: Band-sequence Modeling for High-Quality Audio Restoration"></a>Apollo: Band-sequence Modeling for High-Quality Audio Restoration</h2><p><strong>Authors:Kai Li, Yi Luo</strong></p>
<p>Audio restoration has become increasingly significant in modern society, not only due to the demand for high-quality auditory experiences enabled by advanced playback devices, but also because the growing capabilities of generative audio models necessitate high-fidelity audio. Typically, audio restoration is defined as a task of predicting undistorted audio from damaged input, often trained using a GAN framework to balance perception and distortion. Since audio degradation is primarily concentrated in mid- and high-frequency ranges, especially due to codecs, a key challenge lies in designing a generator capable of preserving low-frequency information while accurately reconstructing high-quality mid- and high-frequency content. Inspired by recent advancements in high-sample-rate music separation, speech enhancement, and audio codec models, we propose Apollo, a generative model designed for high-sample-rate audio restoration. Apollo employs an explicit frequency band split module to model the relationships between different frequency bands, allowing for more coherent and higher-quality restored audio. Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently outperforms existing SR-GAN models across various bit rates and music genres, particularly excelling in complex scenarios involving mixtures of multiple instruments and vocals. Apollo significantly improves music restoration quality while maintaining computational efficiency. The source code for Apollo is publicly available at <a target="_blank" rel="noopener" href="https://github.com/JusperLee/Apollo">https://github.com/JusperLee/Apollo</a>. </p>
<blockquote>
<p>éŸ³é¢‘ä¿®å¤åœ¨ç°ä»£ç¤¾ä¼šå˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œè¿™ä¸ä»…ä»…æ˜¯å› ä¸ºå…ˆè¿›æ’­æ”¾è®¾å¤‡å¸¦æ¥çš„å¯¹é«˜è´¨é‡å¬è§‰ä½“éªŒçš„éœ€æ±‚ï¼Œä¹Ÿå› ä¸ºç”Ÿæˆå¼éŸ³é¢‘æ¨¡å‹çš„æ—¥ç›Šå¼ºå¤§çš„èƒ½åŠ›éœ€è¦é«˜ä¿çœŸéŸ³é¢‘ã€‚é€šå¸¸ï¼ŒéŸ³é¢‘ä¿®å¤è¢«å®šä¹‰ä¸ºä»æŸåçš„è¾“å…¥é¢„æµ‹æ— æŸéŸ³é¢‘çš„ä»»åŠ¡ï¼Œé€šå¸¸ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œä»¥å¹³è¡¡æ„ŸçŸ¥å’Œå¤±çœŸã€‚ç”±äºéŸ³é¢‘é€€åŒ–ä¸»è¦é›†ä¸­åœ¨ä¸­é¢‘å’Œé«˜é¢‘èŒƒå›´å†…ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼–è§£ç å™¨ä¸­ï¼Œä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºè®¾è®¡ä¸€ç§ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿä¿ç•™ä½é¢‘ä¿¡æ¯ï¼ŒåŒæ—¶å‡†ç¡®é‡å»ºé«˜è´¨é‡çš„ä¸­é¢‘å’Œé«˜é¢‘å†…å®¹ã€‚å—é«˜é‡‡æ ·ç‡éŸ³ä¹åˆ†ç¦»ã€è¯­éŸ³å¢å¼ºå’ŒéŸ³é¢‘ç¼–è§£ç å™¨æ¨¡å‹çš„æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Apolloï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜é‡‡æ ·ç‡éŸ³é¢‘ä¿®å¤çš„ç”Ÿæˆæ¨¡å‹ã€‚Apolloé‡‡ç”¨æ˜ç¡®çš„é¢‘å¸¦åˆ†å‰²æ¨¡å—æ¥æ¨¡æ‹Ÿä¸åŒé¢‘å¸¦ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œç”Ÿæˆæ›´è¿è´¯ã€æ›´é«˜è´¨é‡çš„ä¿®å¤éŸ³é¢‘ã€‚åœ¨MUSDB18-HQå’ŒMoisesDBæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒApolloåœ¨å„ç§æ¯”ç‰¹ç‡å’ŒéŸ³ä¹é£æ ¼ä¸Šå‡ä¼˜äºç°æœ‰çš„SR-GANæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤šç§ä¹å™¨å’Œäººå£°æ··åˆçš„å¤æ‚åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚Apolloåœ¨æå‡éŸ³ä¹ä¿®å¤è´¨é‡çš„åŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚Apolloçš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JusperLee/Apollo">https://github.com/JusperLee/Apollo</a>å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08514v2">PDF</a> Accepted by ICASSP 2025, Demo Page: <a target="_blank" rel="noopener" href="https://cslikai.cn/Apollo">https://cslikai.cn/Apollo</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€ç°ä»£ç¤¾ä¼šå¯¹é«˜è´¨é‡å¬è§‰ä½“éªŒçš„éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼ŒéŸ³é¢‘ä¿®å¤æŠ€æœ¯å˜å¾—å°¤ä¸ºé‡è¦ã€‚éŸ³é¢‘ä¿®å¤çš„ä»»åŠ¡åœ¨äºä»æŸåçš„è¾“å…¥ä¸­é¢„æµ‹æ— æŸéŸ³é¢‘ï¼Œé€šå¸¸ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œä»¥å¹³è¡¡æ„ŸçŸ¥å’Œå¤±çœŸã€‚é’ˆå¯¹éŸ³é¢‘é™è´¨ä¸»è¦é›†ä¸­åœ¨ä¸­é«˜é¢‘èŒƒå›´çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç¼–ç å™¨çš„é™è´¨ï¼Œè®¾è®¡å‡ºä¸€ä¸ªèƒ½å¤Ÿä¿å­˜ä½é¢‘ä¿¡æ¯å¹¶å‡†ç¡®é‡å»ºé«˜è´¨é‡ä¸­é«˜é¢‘å†…å®¹çš„ç”Ÿæˆå™¨æˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚å—é«˜é‡‡æ ·ç‡éŸ³ä¹åˆ†ç¦»ã€è¯­éŸ³å¢å¼ºå’ŒéŸ³é¢‘ç¼–ç æ¨¡å‹æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Apolloï¼Œä¸€ä¸ªç”¨äºé«˜é‡‡æ ·ç‡éŸ³é¢‘ä¿®å¤çš„ç”Ÿæˆæ¨¡å‹ã€‚Apolloé‡‡ç”¨æ˜ç¡®çš„é¢‘å¸¦åˆ†å‰²æ¨¡å—ï¼Œæ¨¡æ‹Ÿä¸åŒé¢‘å¸¦ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œç”Ÿæˆæ›´ä¸ºè¿è´¯å’Œé«˜è´¨é‡çš„éŸ³é¢‘ã€‚åœ¨MUSDB18-HQå’ŒMoisesDBæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒApolloåœ¨å„ç§æ¯”ç‰¹ç‡å’ŒéŸ³ä¹ç±»å‹ä¸Šå‡ä¼˜äºç°æœ‰çš„SR-GANæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤šç§ä¹å™¨å’Œäººå£°æ··åˆçš„å¤æ‚åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚Apolloåœ¨æå‡éŸ³ä¹ä¿®å¤è´¨é‡çš„åŒæ—¶ï¼Œä¹Ÿä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚Apolloçš„æºä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/JusperLee/Apollo">https://github.com/JusperLee/Apollo</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éŸ³é¢‘ä¿®å¤åœ¨ç°ä»£ç¤¾ä¼šä¸­å…·æœ‰é‡è¦æ€§ï¼Œä»¥æ»¡è¶³é«˜è´¨é‡å¬è§‰ä½“éªŒçš„éœ€æ±‚ã€‚</li>
<li>éŸ³é¢‘ä¿®å¤çš„ä»»åŠ¡æ˜¯é¢„æµ‹ä»æŸåè¾“å…¥ä¸­çš„æ— æŸéŸ³é¢‘ï¼Œé€šå¸¸ä½¿ç”¨GANæ¡†æ¶è¿›è¡Œè®­ç»ƒã€‚</li>
<li>éŸ³é¢‘é™è´¨ä¸»è¦é›†ä¸­åœ¨ä¸­é«˜é¢‘èŒƒå›´ï¼Œè®¾è®¡èƒ½å¤Ÿä¿å­˜ä½é¢‘ä¿¡æ¯å¹¶é‡å»ºä¸­é«˜é¢‘å†…å®¹çš„ç”Ÿæˆå™¨æ˜¯æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>Apolloæ˜¯ä¸€ä¸ªç”¨äºé«˜é‡‡æ ·ç‡éŸ³é¢‘ä¿®å¤çš„ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨é¢‘å¸¦åˆ†å‰²æ¨¡å—æ¥æé«˜éŸ³é¢‘è´¨é‡ã€‚</li>
<li>Apolloåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰SR-GANæ¨¡å‹ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°çªå‡ºã€‚</li>
<li>Apolloåœ¨æé«˜éŸ³ä¹ä¿®å¤è´¨é‡çš„åŒæ—¶ï¼Œä¹Ÿä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-449c3f8021d9e93300079fcd694ccf32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dada42a783abd96397de5997152e00b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a806a41f4d7740594ecb558fa1c59a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aafc2f07ed1af74324b88f06ced2acc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9024d03fcacbfc9998b0da74127fec10.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-Faetar-Benchmark-Speech-Recognition-in-a-Very-Under-Resourced-Language"><a href="#The-Faetar-Benchmark-Speech-Recognition-in-a-Very-Under-Resourced-Language" class="headerlink" title="The Faetar Benchmark: Speech Recognition in a Very Under-Resourced   Language"></a>The Faetar Benchmark: Speech Recognition in a Very Under-Resourced   Language</h2><p><strong>Authors:Michael Ong, Sean Robertson, Leo Peckham, Alba Jorquera Jimenez de Aberasturi, Paula Arkhangorodsky, Robin Huo, Aman Sakhardande, Mark Hallap, Naomi Nagy, Ewan Dunbar</strong></p>
<p>We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark corpus designed to push the limits of current approaches to low-resource speech recognition. Faetar, a Franco-Proven\c{c}al variety spoken primarily in Italy, has no standard orthography, has virtually no existing textual or speech resources other than what is included in the benchmark, and is quite different from other forms of Franco-Proven\c{c}al. The corpus comes from field recordings, most of which are noisy, for which only 5 hrs have matching transcriptions, and for which forced alignment is of variable quality. The corpus contains an additional 20 hrs of unlabelled speech. We report baseline results from state-of-the-art multilingual speech foundation models with a best phone error rate of 30.4%, using a pipeline that continues pre-training on the foundation model using the unlabelled set. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Faetarè‡ªåŠ¨è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•è¯­æ–™åº“ï¼Œæ—¨åœ¨æ¨åŠ¨å½“å‰æ–¹æ³•åœ¨èµ„æºåŒ®ä¹è¯­éŸ³è¯†åˆ«æ–¹é¢çš„æé™ã€‚Faetaræ˜¯ä¸€ç§ä¸»è¦åœ¨æ„å¤§åˆ©ä½¿ç”¨çš„æ³•ç½—-æ™®ç½—æ—ºæ–¯æ–¹è¨€ï¼Œå®ƒæ²¡æœ‰æ ‡å‡†çš„æ­£å­—æ³•ï¼Œé™¤äº†åŸºå‡†æµ‹è¯•ä¸­åŒ…å«çš„å†…å®¹å¤–ï¼Œå‡ ä¹æ²¡æœ‰ç°æœ‰çš„æ–‡æœ¬æˆ–è¯­éŸ³èµ„æºï¼Œè€Œä¸”ä¸å…¶ä»–å½¢å¼çš„æ³•ç½—-æ™®ç½—æ—ºæ–¯æ–¹è¨€æœ‰å¾ˆå¤§çš„ä¸åŒã€‚è¯¥è¯­æ–™åº“æ¥è‡ªç°åœºå½•éŸ³ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†æ˜¯å˜ˆæ‚çš„ï¼Œåªæœ‰5å°æ—¶çš„è¯­éŸ³æœ‰åŒ¹é…çš„è½¬å½•ï¼Œè€Œä¸”å…¶å¼ºåˆ¶å¯¹é½çš„è´¨é‡ä¹Ÿæ˜¯æ—¶å¥½æ—¶åã€‚è¯­æ–™åº“è¿˜åŒ…å«é¢å¤–çš„20å°æ—¶æœªæ ‡æ³¨çš„è¯­éŸ³ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†ä½¿ç”¨æœ€å…ˆè¿›çš„å¤šè¯­è¨€è¯­éŸ³åŸºç¡€æ¨¡å‹çš„åŸºçº¿ç»“æœï¼Œæœ€ä½³è¯­éŸ³é”™è¯¯ç‡ä¸º30.4%ï¼Œä½¿ç”¨åœ¨åŸºç¡€æ¨¡å‹ä¸Šç»§ç»­ä½¿ç”¨æœªæ ‡æ³¨é›†è¿›è¡Œé¢„è®­ç»ƒçš„ç®¡é“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08103v3">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Faetarè‡ªåŠ¨è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæŒ‘æˆ˜å½“å‰ä½èµ„æºè¯­éŸ³è¯†åˆ«æ–¹æ³•è€Œè®¾è®¡çš„åŸºå‡†æµ‹è¯•é›†ã€‚Faetaræ˜¯ä¸€ç§ä¸»è¦åœ¨æ„å¤§åˆ©ä½¿ç”¨çš„æ³•æ„æ··åˆæ–¹è¨€ï¼Œæ²¡æœ‰æ ‡å‡†çš„æ­£å­—æ³•ï¼Œé™¤äº†åŸºå‡†æµ‹è¯•é›†ä¸­åŒ…å«çš„ä¹‹å¤–ï¼Œå‡ ä¹æ²¡æœ‰ç°æœ‰çš„æ–‡æœ¬æˆ–è¯­éŸ³èµ„æºï¼Œå¹¶ä¸”ä¸å…¶ä»–å½¢å¼çš„æ³•æ„æ··åˆæ–¹è¨€æœ‰å¾ˆå¤§å·®å¼‚ã€‚è¯¥è¯­æ–™åº“æ¥è‡ªç°åœºå½•éŸ³ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†æ˜¯å˜ˆæ‚çš„ï¼Œåªæœ‰5å°æ—¶çš„å½•éŸ³æœ‰åŒ¹é…çš„è½¬å½•æ–‡æœ¬ï¼Œå¼ºåˆ¶å¯¹é½çš„è´¨é‡ä¸ä¸€ã€‚æ­¤å¤–ï¼Œè¯­æ–™åº“è¿˜åŒ…å«20å°æ—¶çš„æœªæ ‡è®°è¯­éŸ³ã€‚æŠ¥å‘Šäº†ä½¿ç”¨å‰æ²¿çš„å¤šè¯­è¨€è¯­éŸ³åŸºç¡€æ¨¡å‹çš„åŸºçº¿ç»“æœï¼Œæœ€ä½³éŸ³ç´ é”™è¯¯ç‡ä¸º30.4%ï¼Œä½¿ç”¨åœ¨åŸºç¡€æ¨¡å‹ä¸Šç»§ç»­å¯¹æœªæ ‡è®°é›†è¿›è¡Œé¢„è®­ç»ƒçš„ç®¡é“ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>Faetarè‡ªåŠ¨è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•é›†æ˜¯ä¸ºäº†æŒ‘æˆ˜ä½èµ„æºè¯­éŸ³è¯†åˆ«æ–¹æ³•è€Œè®¾è®¡çš„ã€‚</li>
<li>Faetaræ˜¯ä¸€ç§ç‰¹æ®Šçš„æ³•æ„æ··åˆæ–¹è¨€ï¼Œç¼ºä¹æ ‡å‡†æ­£å­—æ³•ï¼Œèµ„æºåŒ®ä¹ã€‚</li>
<li>è¯¥è¯­æ–™åº“ä¸»è¦æ¥è‡ªç°åœºå½•éŸ³ï¼Œéƒ¨åˆ†è¯­éŸ³ç¯å¢ƒå˜ˆæ‚ã€‚</li>
<li>åªæœ‰5å°æ—¶çš„è¯­éŸ³æœ‰åŒ¹é…çš„è½¬å½•æ–‡æœ¬ï¼Œå¼ºåˆ¶å¯¹é½çš„è´¨é‡å­˜åœ¨å·®å¼‚ã€‚</li>
<li>è¯­æ–™åº“è¿˜åŒ…å«20å°æ—¶çš„æœªæ ‡è®°è¯­éŸ³ã€‚</li>
<li>æŠ¥å‘Šäº†ä½¿ç”¨å‰æ²¿å¤šè¯­è¨€è¯­éŸ³åŸºç¡€æ¨¡å‹çš„åŸºçº¿ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08103">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0e674056e547e5230654066362c2c0f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbd387af0af3463a56cec4dec344ef04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99dd7f0a2311b0e2928143a41d6f2c8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-902705ddcaf0b05a0c78dc7607dc89b4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Neural-Speech-and-Audio-Coding-Modern-AI-Technology-Meets-Traditional-Codecs"><a href="#Neural-Speech-and-Audio-Coding-Modern-AI-Technology-Meets-Traditional-Codecs" class="headerlink" title="Neural Speech and Audio Coding: Modern AI Technology Meets Traditional   Codecs"></a>Neural Speech and Audio Coding: Modern AI Technology Meets Traditional   Codecs</h2><p><strong>Authors:Minje Kim, Jan Skoglund</strong></p>
<p>This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecsâ€™ output, along with the autoencoder-based end-to-end models and LPCNetâ€“hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†ç¥ç»ç½‘ç»œè¯­éŸ³å’ŒéŸ³é¢‘ç¼–ç ç³»ç»Ÿä¸­åŸºäºæ¨¡å‹å’ŒåŸºäºæ•°æ®çš„æ–¹æ³•çš„èåˆã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†è¯­éŸ³å’ŒéŸ³é¢‘ç¼–è§£ç å™¨çš„ä¸»è§‚è¯„ä¼°è¿‡ç¨‹æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶è®¨è®ºäº†çº¯æ•°æ®é©±åŠ¨æ–¹æ³•çš„å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦æ„å»ºåºå¤§ä¸”ä½æ•ˆçš„æ¶æ„æ‰èƒ½è¾¾åˆ°åŸºäºæ¨¡å‹çš„æ–¹æ³•çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶æå‡ºäº†æ··åˆç³»ç»Ÿä½œä¸ºä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„è®¾è®¡å¢å¼ºæªæ–½ï¼Œå¯¹å¸¸è§„ç¼–è§£ç å™¨çš„æ€§èƒ½è¿›è¡Œäº†é‡å¤§æ”¹è¿›ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå¼•å…¥äº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„ä¿¡å·å¢å¼ºå™¨ï¼Œç”¨äºå¯¹ç°æœ‰çš„ç¼–è§£ç å™¨è¾“å‡ºè¿›è¡Œåå¤„ç†ï¼Œä»¥åŠåŸºäºè‡ªç¼–ç å™¨çš„ç«¯åˆ°ç«¯æ¨¡å‹å’ŒLPCNetæ··åˆç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå°†çº¿æ€§é¢„æµ‹ç¼–ç ï¼ˆLPCï¼‰ä¸ç¥ç»ç½‘ç»œç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ·±å…¥ç ”ç©¶äº†åœ¨è‡ªå®šä¹‰ç‰¹å¾ç©ºé—´ï¼ˆTF-Codecï¼‰æˆ–é¢„å®šä¹‰å˜æ¢åŸŸï¼ˆMDCTNetï¼‰å†…è¿è¡Œçš„é¢„æµ‹æ¨¡å‹ï¼Œå¹¶æ¢è®¨äº†ä½¿ç”¨å¿ƒç†å£°å­¦æ ¡å‡†æŸå¤±å‡½æ•°æ¥è®­ç»ƒç«¯åˆ°ç«¯ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ã€‚é€šè¿‡è¿™äº›ç ”ç©¶ï¼Œæ–‡ç« å±•ç¤ºäº†æ··åˆç³»ç»Ÿåœ¨ç¼©å°ä¼ ç»ŸåŸºäºæ¨¡å‹çš„æ–¹æ³•å’Œç°ä»£åŸºäºæ•°æ®çš„æŠ€æœ¯ä¹‹é—´çš„å·®è·æ–¹é¢æ‰€å…·æœ‰çš„å‘å±•æ½œåŠ›ï¼Œä»è€Œæ¨åŠ¨è¯­éŸ³å’ŒéŸ³é¢‘ç¼–ç é¢†åŸŸçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.06954v2">PDF</a> Published in IEEE Signal Processing Magazine</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç¥ç»è¯­éŸ³å’ŒéŸ³é¢‘ç¼–ç ç³»ç»Ÿä¸­åŸºäºæ¨¡å‹å’Œæ•°æ®é©±åŠ¨æ–¹æ³•çš„èåˆã€‚æ–‡ç« æŒ‡å‡ºäº†è¯­éŸ³å’ŒéŸ³é¢‘ç¼–ç ä¸»è§‚è¯„ä¼°è¿‡ç¨‹æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¹¶è®¨è®ºäº†çº¯ç²¹æ•°æ®é©±åŠ¨æ–¹æ³•çš„å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦æ„å»ºåºå¤§çš„æ¶æ„æ‰èƒ½è¾¾åˆ°åŸºäºæ¨¡å‹çš„æ–¹æ³•çš„æ€§èƒ½ã€‚ç ”ç©¶æå‡ºæ··åˆç³»ç»Ÿä½œä¸ºè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡æ”¹è¿›ï¼Œå¯¹ä¼ ç»Ÿç¼–ç å™¨çš„æ€§èƒ½è¿›è¡Œäº†æ˜¾ç€æé«˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œå®ƒå¼•å…¥äº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„ä¿¡å·å¢å¼ºå™¨ï¼Œç”¨äºå¯¹ç°æœ‰ç¼–ç å™¨çš„è¾“å‡ºè¿›è¡Œåå¤„ç†ï¼Œä»¥åŠåŸºäºè‡ªç¼–ç å™¨çš„ç«¯åˆ°ç«¯æ¨¡å‹å’ŒLPCNetæ··åˆç³»ç»Ÿï¼Œç»“åˆçº¿æ€§é¢„æµ‹ç¼–ç ï¼ˆLPCï¼‰ä¸ç¥ç»ç½‘ç»œã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ç ”ç©¶äº†åœ¨è‡ªå®šä¹‰ç‰¹å¾ç©ºé—´ï¼ˆTF-Codecï¼‰æˆ–é¢„å®šä¹‰å˜æ¢åŸŸï¼ˆMDCTNetï¼‰å†…è¿è¡Œçš„é¢„æµ‹æ¨¡å‹ï¼Œå¹¶æ¢è®¨äº†ä½¿ç”¨å¿ƒç†å£°å­¦æ ¡å‡†æŸå¤±å‡½æ•°æ¥è®­ç»ƒç«¯åˆ°ç«¯ç¥ç»éŸ³é¢‘ç¼–ç å™¨çš„æ½œåŠ›ã€‚æœ¬æ–‡é€šè¿‡æ··åˆç³»ç»Ÿçš„ç ”ç©¶ï¼Œå±•ç¤ºäº†å…¶ç¼©å°ä¼ ç»ŸåŸºäºæ¨¡å‹çš„æ–¹æ³•å’Œç°ä»£æ•°æ®é©±åŠ¨æŠ€æœ¯ä¹‹é—´å·®è·ï¼Œæ¨åŠ¨è¯­éŸ³å’ŒéŸ³é¢‘ç¼–ç é¢†åŸŸå‘å±•çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¢è®¨äº†æ¨¡å‹ä¸æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨ç¥ç»è¯­éŸ³å’ŒéŸ³é¢‘ç¼–ç ä¸­çš„èåˆã€‚</li>
<li>å¼ºè°ƒäº†è¯­éŸ³å’ŒéŸ³é¢‘ç¼–ç ä¸»è§‚è¯„ä¼°è¿‡ç¨‹çš„æŒ‘æˆ˜ã€‚</li>
<li>æŒ‡å‡ºçº¯ç²¹æ•°æ®é©±åŠ¨æ–¹æ³•çš„å±€é™æ€§ä»¥åŠä¸ºä½•éœ€è¦æ··åˆç³»ç»Ÿã€‚</li>
<li>ä»‹ç»äº†åŸºäºç¥ç»ç½‘ç»œçš„ä¿¡å·å¢å¼ºå™¨åŠå…¶ä½œç”¨ã€‚</li>
<li>æ¢è®¨äº†ç»“åˆçº¿æ€§é¢„æµ‹ç¼–ç ï¼ˆLPCï¼‰ä¸ç¥ç»ç½‘ç»œçš„æ··åˆç³»ç»Ÿï¼ˆLPCNetï¼‰ã€‚</li>
<li>ç ”ç©¶äº†åœ¨è‡ªå®šä¹‰ç‰¹å¾ç©ºé—´æˆ–é¢„å®šä¹‰å˜æ¢åŸŸå†…è¿è¡Œçš„é¢„æµ‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.06954">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db9116f6ee323ca0b3495fb49da010ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67f7b3fd9f255e0da5c144176a3fffda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cb7605d7010b24d8867ea60cd1a9cfa.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Continuously-Learning-New-Words-in-Automatic-Speech-Recognition"><a href="#Continuously-Learning-New-Words-in-Automatic-Speech-Recognition" class="headerlink" title="Continuously Learning New Words in Automatic Speech Recognition"></a>Continuously Learning New Words in Automatic Speech Recognition</h2><p><strong>Authors:Christian Huber, Alexander Waibel</strong></p>
<p>Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities, and domain-specific special words for which little or no labeled data is available. To address the problem of recognizing these words, we propose a self-supervised continual learning approach: Given the audio of a lecture talk with the corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from the literature. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation data set. Continual learning is then performed by training adaptation weights added to the model on this data set. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä»ç„¶è¿œéå®Œç¾ã€‚å…¸å‹çš„é”™è¯¯åŒ…æ‹¬ç¼©å†™è¯ã€å‘½åå®ä½“å’Œç‰¹å®šé¢†åŸŸçš„ç‰¹æ®Šè¯æ±‡ï¼Œè¿™äº›è¯æ±‡å¾ˆå°‘æˆ–å‡ ä¹æ²¡æœ‰ç›¸åº”çš„æ ‡è®°æ•°æ®ã€‚ä¸ºäº†è§£å†³è¯†åˆ«è¿™äº›è¯æ±‡çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªç›‘ç£çš„è¿ç»­å­¦ä¹ æ–¹æ³•ï¼šç»™å®šå¸¦æœ‰ç›¸åº”å¹»ç¯ç‰‡çš„è®²åº§éŸ³é¢‘ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ–‡çŒ®ä¸­çš„è®°å¿†å¢å¼ºASRæ¨¡å‹ï¼Œåå‘æ¨¡å‹ä»¥ä»å¹»ç¯ç‰‡ä¸­è§£ç æ–°è¯æ±‡ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹è®²åº§è¿›è¡Œæ¨ç†ï¼Œæ”¶é›†åŒ…å«æ£€æµ‹åˆ°çš„æ–°è¯æ±‡çš„ç‰‡æ®µï¼Œå½¢æˆä¸€ä¸ªé€‚åº”æ•°æ®é›†ã€‚é€šè¿‡åœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹çš„é™„åŠ é€‚åº”æƒé‡ï¼Œè¿ç»­å­¦ä¹ å¾—ä»¥è¿›è¡Œã€‚æ•´ä¸ªè¿‡ç¨‹å¤šæ¬¡è¿­ä»£è®²åº§å†…å®¹ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå½“æ–°è¯æ±‡å‡ºç°å¾—æ›´é¢‘ç¹æ—¶ï¼Œæˆ‘ä»¬åœ¨è¿™äº›è¯æ±‡ä¸Šçš„æ€§èƒ½æœ‰æ‰€æé«˜ï¼ˆå¬å›ç‡è¶…è¿‡80%ï¼‰ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.04482v3">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä¸»è¦è®¨è®ºçš„æ˜¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå­˜åœ¨çš„é—®é¢˜åŠå…¶æ”¹è¿›æ–¹æ³•ã€‚é’ˆå¯¹ASRç³»ç»Ÿåœ¨è¯†åˆ«ç¼©å†™ã€å‘½åå®ä½“å’Œç‰¹å®šé¢†åŸŸç‰¹æ®Šè¯æ±‡æ—¶å‡ºç°çš„å…¸å‹é”™è¯¯ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£çš„æŒç»­å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¸¦æœ‰ç›¸åº”å¹»ç¯ç‰‡çš„è®²åº§éŸ³é¢‘ï¼Œé€šè¿‡å¢å¼ºè®°å¿†çš„ASRæ¨¡å‹ï¼Œåå‘è§£ç å¹»ç¯ç‰‡ä¸­çš„æ–°è¯æ±‡ã€‚ç„¶åï¼Œå¯¹è®²åº§è¿›è¡Œæ¨ç†ï¼Œæ”¶é›†åŒ…å«æ£€æµ‹åˆ°çš„æ–°è¯æ±‡çš„ç‰‡æ®µï¼Œå½¢æˆä¸€ä¸ªé€‚åº”æ•°æ®é›†ã€‚é€šè¿‡åœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹çš„é€‚åº”æƒé‡ï¼Œå®ç°æŒç»­å­¦ä¹ ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜æ¨¡å‹å¯¹æ–°è¯çš„è¯†åˆ«æ€§èƒ½ï¼Œå¹¶ä¸”éšç€æ–°è¯å‡ºç°é¢‘ç‡çš„å¢åŠ ï¼Œå¬å›ç‡è¶…è¿‡80%ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä»å­˜åœ¨è¯†åˆ«ç¼©å†™ã€å‘½åå®ä½“å’Œç‰¹å®šé¢†åŸŸç‰¹æ®Šè¯æ±‡çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£çš„æŒç»­å­¦ä¹ æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨è®²åº§éŸ³é¢‘å’Œç›¸åº”å¹»ç¯ç‰‡ï¼Œé€šè¿‡å¢å¼ºè®°å¿†çš„ASRæ¨¡å‹åå‘è§£ç å¹»ç¯ç‰‡ä¸­çš„æ–°è¯æ±‡ã€‚</li>
<li>é€šè¿‡æ”¶é›†åŒ…å«æ–°è¯æ±‡çš„ç‰‡æ®µå½¢æˆé€‚åº”æ•°æ®é›†ï¼Œå¹¶è¿›è¡ŒæŒç»­å­¦ä¹ ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜æ¨¡å‹å¯¹æ–°è¯çš„è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>éšç€æ–°è¯å‡ºç°é¢‘ç‡çš„å¢åŠ ï¼Œå¬å›ç‡è¶…è¿‡80%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.04482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f4d69edadde240621de0865cfbab705.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98ba785a9f45c2a978dc727172b53d5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24b908eb16062b2c6b7a72537e50d20b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Harnessing-the-Zero-Shot-Power-of-Instruction-Tuned-Large-Language-Model-in-End-to-End-Speech-Recognition"><a href="#Harnessing-the-Zero-Shot-Power-of-Instruction-Tuned-Large-Language-Model-in-End-to-End-Speech-Recognition" class="headerlink" title="Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model   in End-to-End Speech Recognition"></a>Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model   in End-to-End Speech Recognition</h2><p><strong>Authors:Yosuke Higuchi, Tetsuji Ogawa, Tetsunori Kobayashi</strong></p>
<p>We propose to utilize an instruction-tuned large language model (LLM) for guiding the text generation process in automatic speech recognition (ASR). Modern large language models (LLMs) are adept at performing various text generation tasks through zero-shot learning, prompted with instructions designed for specific objectives. This paper explores the potential of LLMs to derive linguistic information that can facilitate text generation in end-to-end ASR models. Specifically, we instruct an LLM to correct grammatical errors in an ASR hypothesis and use the LLM-derived representations to refine the output further. The proposed model is built on the joint CTC and attention architecture, with the LLM serving as a front-end feature extractor for the decoder. The ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding and fed into the LLM along with a specific instruction. The decoder subsequently takes as input the LLM output to perform token predictions, combining acoustic information from the encoder and the powerful linguistic information provided by the LLM. Experimental results show that the proposed LLM-guided model achieves a relative gain of approximately 13% in word error rates across major benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬æè®®åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨ç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¼•å¯¼ã€‚ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿é€šè¿‡é›¶æ ·æœ¬å­¦ä¹ å®Œæˆå„ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡é€šè¿‡é’ˆå¯¹ç‰¹å®šç›®æ ‡çš„æŒ‡ä»¤æ¥æç¤ºã€‚æœ¬æ–‡æ¢è®¨äº†LLMåœ¨ç«¯åˆ°ç«¯ASRæ¨¡å‹ä¸­ç”¨äºä¿ƒè¿›æ–‡æœ¬ç”Ÿæˆçš„æ½œåŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æŒ‡å¯¼LLMçº æ­£ASRå‡è®¾ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶ä½¿ç”¨LLMç”Ÿæˆçš„è¡¨ç¤ºæ¥è¿›ä¸€æ­¥å®Œå–„è¾“å‡ºã€‚æ‰€æå‡ºçš„æ¨¡å‹å»ºç«‹åœ¨è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰å’Œæ³¨æ„åŠ›æ¶æ„ä¹‹ä¸Šï¼ŒLLMä½œä¸ºè§£ç å™¨çš„å‰ç«¯ç‰¹å¾æå–å™¨ã€‚éœ€è¦çº æ­£çš„ASRå‡è®¾æ˜¯é€šè¿‡CTCè§£ç ä»ç¼–ç å™¨è·å¾—çš„ï¼Œå¹¶ä¸ç‰¹å®šæŒ‡ä»¤ä¸€èµ·è¾“å…¥åˆ°LLMä¸­ã€‚è§£ç å™¨éšåä»¥LLMè¾“å‡ºä½œä¸ºè¾“å…¥æ¥è¿›è¡Œä»¤ç‰Œé¢„æµ‹ï¼Œç»“åˆç¼–ç å™¨æä¾›çš„éŸ³é¢‘ä¿¡æ¯å’ŒLLMæä¾›çš„å¼ºå¤§çš„è¯­è¨€ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„LLMå¼•å¯¼æ¨¡å‹åœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†çº¦13%çš„è¯é”™è¯¯ç‡ç›¸å¯¹å¢ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10524v3">PDF</a> Accepted to ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>åˆ©ç”¨æŒ‡ä»¤ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡å¯¼è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ã€‚LLMèƒ½å¤Ÿé€šè¿‡é›¶æ ·æœ¬å­¦ä¹ æ‰§è¡Œå„ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡ä¸ºç‰¹å®šç›®æ ‡è®¾è®¡çš„æŒ‡ä»¤è¿›è¡Œæç¤ºã€‚æœ¬æ–‡æ¢è®¨äº†LLMåœ¨ç«¯åˆ°ç«¯ASRæ¨¡å‹ä¸­ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„æ½œåŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æŒ‡å¯¼LLMçº æ­£ASRå‡è®¾ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶ä½¿ç”¨LLMç”Ÿæˆçš„è¡¨ç¤ºæ¥è¿›ä¸€æ­¥å®Œå–„è¾“å‡ºã€‚æå‡ºçš„æ¨¡å‹å»ºç«‹åœ¨è”åˆCTCå’Œæ³¨æ„åŠ›æ¶æ„ä¸Šï¼ŒLLMä½œä¸ºè§£ç å™¨çš„å‰ç«¯ç‰¹å¾æå–å™¨ã€‚ASRå‡è®¾ï¼ˆéœ€è¿›è¡Œçº æ­£ï¼‰ç”±ç¼–ç å™¨é€šè¿‡CTCè§£ç è·å¾—ï¼Œå¹¶ä¸ç‰¹å®šæŒ‡ä»¤ä¸€èµ·è¾“å…¥LLMã€‚è§£ç å™¨éšåä»¥LLMè¾“å‡ºä½œä¸ºè¾“å…¥è¿›è¡Œä»¤ç‰Œé¢„æµ‹ï¼Œç»“åˆç¼–ç å™¨çš„å£°éŸ³ä¿¡æ¯å’ŒLLMæä¾›çš„å¼ºå¤§è¯­è¨€ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„LLMæŒ‡å¯¼æ¨¡å‹åœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šç›¸å¯¹æé«˜äº†çº¦13ï¼…çš„å•è¯é”™è¯¯ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥ç”¨äºæŒ‡å¯¼è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>LLMèƒ½å¤Ÿé€šè¿‡é›¶æ ·æœ¬å­¦ä¹ æ‰§è¡Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶é€šè¿‡ç‰¹å®šæŒ‡ä»¤è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>LLMåœ¨ç«¯åˆ°ç«¯ASRæ¨¡å‹ä¸­çš„æ½œåŠ›åœ¨äºçº æ­£ASRå‡è®¾ä¸­çš„è¯­æ³•é”™è¯¯å¹¶è¿›ä¸€æ­¥å®Œå–„è¾“å‡ºã€‚</li>
<li>æå‡ºçš„æ¨¡å‹ç»“åˆCTCå’Œæ³¨æ„åŠ›æ¶æ„ï¼Œå…¶ä¸­LLMä½œä¸ºè§£ç å™¨çš„å‰ç«¯ç‰¹å¾æå–å™¨ã€‚</li>
<li>ASRå‡è®¾é€šè¿‡CTCè§£ç è·å¾—ï¼Œå¹¶ä¸æŒ‡ä»¤ä¸€èµ·è¾“å…¥LLMè¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ã€‚</li>
<li>LLMçš„è¾“å‡ºä¸ç¼–ç å™¨çš„å£°éŸ³ä¿¡æ¯ç»“åˆï¼Œç”¨äºè§£ç å™¨çš„ä»¤ç‰Œé¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.10524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be53b67817602b3e3c2c7795746ef361.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6e915597c776147881ec29999ece777.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2a67cbc5de1e0b44da8a1f55d0ae8ff.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-09/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-09/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-09/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-25031f9f6a43ea24be70c8a51b2d803d.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-09  Diverse Rare Sample Generation with Pretrained GANs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-09/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b8194d2dc72cc85a4663039d2cecc517.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-09  CL3DOR Contrastive Learning for 3D Large Multimodal Models via Odds   Ratio on High-Resolution Point Clouds
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
