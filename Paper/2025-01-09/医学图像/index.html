<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-09  LM-Net A Light-weight and Multi-scale Network for Medical Image   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-06ae655fe7bce8faac4aa64ec52359e7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-09-æ›´æ–°"><a href="#2025-01-09-æ›´æ–°" class="headerlink" title="2025-01-09 æ›´æ–°"></a>2025-01-09 æ›´æ–°</h1><h2 id="LM-Net-A-Light-weight-and-Multi-scale-Network-for-Medical-Image-Segmentation"><a href="#LM-Net-A-Light-weight-and-Multi-scale-Network-for-Medical-Image-Segmentation" class="headerlink" title="LM-Net: A Light-weight and Multi-scale Network for Medical Image   Segmentation"></a>LM-Net: A Light-weight and Multi-scale Network for Medical Image   Segmentation</h2><p><strong>Authors:Zhenkun Lu, Chaoyin She, Wei Wang, Qinghua Huang</strong></p>
<p>Current medical image segmentation approaches have limitations in deeply exploring multi-scale information and effectively combining local detail textures with global contextual semantic information. This results in over-segmentation, under-segmentation, and blurred segmentation boundaries. To tackle these challenges, we explore multi-scale feature representations from different perspectives, proposing a novel, lightweight, and multi-scale architecture (LM-Net) that integrates advantages of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to enhance segmentation accuracy. LM-Net employs a lightweight multi-branch module to capture multi-scale features at the same level. Furthermore, we introduce two modules to concurrently capture local detail textures and global semantics with multi-scale features at different levels: the Local Feature Transformer (LFT) and Global Feature Transformer (GFT). The LFT integrates local window self-attention to capture local detail textures, while the GFT leverages global self-attention to capture global contextual semantics. By combining these modules, our model achieves complementarity between local and global representations, alleviating the problem of blurred segmentation boundaries in medical image segmentation. To evaluate the feasibility of LM-Net, extensive experiments have been conducted on three publicly available datasets with different modalities. Our proposed model achieves state-of-the-art results, surpassing previous methods, while only requiring 4.66G FLOPs and 5.4M parameters. These state-of-the-art results on three datasets with different modalities demonstrate the effectiveness and adaptability of our proposed LM-Net for various medical image segmentation tasks. </p>
<blockquote>
<p>å½“å‰åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•åœ¨å¤šå°ºåº¦ä¿¡æ¯æ·±åº¦æ¢ç´¢ä»¥åŠå±€éƒ¨ç»†èŠ‚çº¹ç†ä¸å…¨å±€ä¸Šä¸‹æ–‡è¯­ä¹‰ä¿¡æ¯æœ‰æ•ˆç»“åˆæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè¿™å¯¼è‡´äº†è¿‡åˆ†å‰²ã€æ¬ åˆ†å‰²å’Œåˆ†å‰²è¾¹ç•Œæ¨¡ç³Šçš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»ä¸åŒçš„è§’åº¦æ¢ç´¢å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–ã€è½»é‡çº§ã€å¤šå°ºåº¦çš„æ¶æ„ï¼ˆLM-Netï¼‰ï¼Œå®ƒç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„ä¼˜ç‚¹ï¼Œä»¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚LM-Neté‡‡ç”¨è½»é‡çº§çš„å¤šåˆ†æ”¯æ¨¡å—ï¼Œåœ¨åŒä¸€å±‚æ¬¡ä¸Šæ•æ‰å¤šå°ºåº¦ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªæ¨¡å—ï¼Œå³å±€éƒ¨ç‰¹å¾è½¬æ¢å™¨ï¼ˆLFTï¼‰å’Œå…¨å±€ç‰¹å¾è½¬æ¢å™¨ï¼ˆGFTï¼‰ï¼Œä»¥åœ¨ä¸åŒå±‚æ¬¡ä¸ŠåŒæ—¶æ•æ‰å±€éƒ¨ç»†èŠ‚çº¹ç†å’Œå…¨å±€è¯­ä¹‰ä»¥åŠå¤šå°ºåº¦ç‰¹å¾ã€‚LFTé€šè¿‡å±€éƒ¨çª—å£è‡ªæ³¨æ„åŠ›æ•æ‰å±€éƒ¨ç»†èŠ‚çº¹ç†ï¼Œè€ŒGFTåˆ©ç”¨å…¨å±€è‡ªæ³¨æ„åŠ›æ¥æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚é€šè¿‡ç»“åˆè¿™äº›æ¨¡å—ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†å±€éƒ¨å’Œå…¨å±€è¡¨ç¤ºä¹‹é—´çš„äº’è¡¥æ€§ï¼Œç¼“è§£äº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ¨¡ç³Šåˆ†å‰²è¾¹ç•Œçš„é—®é¢˜ã€‚ä¸ºäº†è¯„ä¼°LM-Netçš„å¯è¡Œæ€§ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒæ¨¡æ€çš„å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚æ‰€æå‡ºçš„æ¨¡å‹å–å¾—äº†æœ€æ–°ç»“æœï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œä»…éœ€4.66G FLOPså’Œ5.4Må‚æ•°ã€‚åœ¨ä¸‰ä¸ªä¸åŒæ¨¡æ€æ•°æ®é›†ä¸Šçš„è¿™äº›æœ€æ–°ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æå‡ºçš„LM-Netåœ¨å„ç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03838v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å½“å‰åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•åœ¨å¤šå°ºåº¦ä¿¡æ¯æ¢ç´¢åŠå±€éƒ¨ç»†èŠ‚çº¹ç†ä¸å…¨å±€ä¸Šä¸‹æ–‡è¯­ä¹‰ä¿¡æ¯ç»“åˆæ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºä¸€ç§æ–°å‹è½»é‡çº§å¤šå°ºåº¦æ¶æ„ï¼ˆLM-Netï¼‰ï¼Œç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„ä¼˜ç‚¹ï¼Œæé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚é€šè¿‡é‡‡ç”¨å¤šåˆ†æ”¯æ¨¡å—å’Œå±€éƒ¨ç‰¹å¾è½¬æ¢å™¨ï¼ˆLFTï¼‰ä¸å…¨å±€ç‰¹å¾è½¬æ¢å™¨ï¼ˆGFTï¼‰ï¼Œå®ç°å±€éƒ¨ä¸å…¨å±€è¡¨ç¤ºçš„äº’è¡¥ï¼Œç¼“è§£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è¾¹ç•Œæ¨¡ç³Šé—®é¢˜ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œæ‰€ææ¨¡å‹å‚æ•°å°‘ã€è®¡ç®—é‡å°ï¼Œè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•å­˜åœ¨å¤šå°ºåº¦ä¿¡æ¯æ¢ç´¢ä¸è¶³åŠå±€éƒ¨ç»†èŠ‚çº¹ç†ä¸å…¨å±€ä¸Šä¸‹æ–‡è¯­ä¹‰ä¿¡æ¯ç»“åˆä¸æœ‰æ•ˆçš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹è½»é‡çº§å¤šå°ºåº¦æ¶æ„ï¼ˆLM-Netï¼‰ï¼Œç»“åˆCNNå’ŒViTçš„ä¼˜ç‚¹ã€‚</li>
<li>é‡‡ç”¨å¤šåˆ†æ”¯æ¨¡å—æ•æ‰åŒä¸€å±‚æ¬¡çš„å¤šå°ºåº¦ç‰¹å¾ã€‚</li>
<li>å¼•å…¥LFTå’ŒGFTæ¨¡å—ï¼Œåˆ†åˆ«åœ¨ä¸åŒçš„å±‚æ¬¡ä¸ŠåŒæ—¶æ•æ‰å±€éƒ¨ç»†èŠ‚çº¹ç†å’Œå…¨å±€è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>LFTæ¨¡å—é€šè¿‡å±€éƒ¨çª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰å±€éƒ¨ç»†èŠ‚çº¹ç†ã€‚</li>
<li>GFTæ¨¡å—åˆ©ç”¨å…¨å±€è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-22bda771644518b01ccb15c78b547f39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6db107e517a90159a1cb4dcd53f7cb94.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis"><a href="#SCC-YOLO-An-Improved-Object-Detector-for-Assisting-in-Brain-Tumor-Diagnosis" class="headerlink" title="SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis"></a>SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor   Diagnosis</h2><p><strong>Authors:Runci Bai</strong></p>
<p>Brain tumors can result in neurological dysfunction, alterations in cognitive and psychological states, increased intracranial pressure, and the occurrence of seizures, thereby presenting a substantial risk to human life and health. The You Only Look Once(YOLO) series models have demonstrated superior accuracy in object detection for medical imaging. In this paper, we develop a novel SCC-YOLO architecture by integrating the SCConv attention mechanism into YOLOv9. The SCConv module reconstructs an efficient convolutional module by reducing spatial and channel redundancy among features, thereby enhancing the learning of image features. We investigate the impact of intergrating different attention mechanisms with the YOLOv9 model on brain tumor image detection using both the Br35H dataset and our self-made dataset(Brain_Tumor_Dataset). Experimental results show that on the Br35H dataset, SCC-YOLO achieved a 0.3% improvement in mAp50 compared to YOLOv9, while on our self-made dataset, SCC-YOLO exhibited a 0.5% improvement over YOLOv9. SCC-YOLO has reached state-of-the-art performance in brain tumor detection. Source code is available at : <a target="_blank" rel="noopener" href="https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master">https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master</a> </p>
<blockquote>
<p>è„‘è‚¿ç˜¤å¯èƒ½å¯¼è‡´ç¥ç»åŠŸèƒ½éšœç¢ã€è®¤çŸ¥å’Œå¿ƒç†çŠ¶æ€æ”¹å˜ã€é¢…å†…å‹å‡é«˜ä»¥åŠç™«ç—«å‘ä½œï¼Œä»è€Œå¯¹äººç±»ç”Ÿå‘½å’Œå¥åº·æ„æˆé‡å¤§é£é™©ã€‚You Only Look Onceï¼ˆYOLOï¼‰ç³»åˆ—æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒçš„ç›®æ ‡æ£€æµ‹ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„å‡†ç¡®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†SCConvæ³¨æ„åŠ›æœºåˆ¶èå…¥YOLOv9ï¼Œå¼€å‘äº†ä¸€ç§æ–°å‹çš„SCC-YOLOæ¶æ„ã€‚SCConvæ¨¡å—é€šè¿‡å‡å°‘ç‰¹å¾ä¹‹é—´çš„ç©ºé—´å†—ä½™å’Œé€šé“å†—ä½™ï¼Œä»è€Œé‡æ–°æ„å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„å·ç§¯æ¨¡å—ï¼Œå¢å¼ºäº†å›¾åƒç‰¹å¾çš„å­¦ä¹ ã€‚æˆ‘ä»¬ä½¿ç”¨Br35Hæ•°æ®é›†å’Œæˆ‘ä»¬è‡ªåˆ¶çš„Brain_Tumor_Datasetæ•°æ®é›†ï¼Œç ”ç©¶äº†å°†ä¸åŒæ³¨æ„åŠ›æœºåˆ¶ä¸YOLOv9æ¨¡å‹é›†æˆå¯¹è„‘è‚¿ç˜¤å›¾åƒæ£€æµ‹çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Br35Hæ•°æ®é›†ä¸Šï¼ŒSCC-YOLOç›¸è¾ƒäºYOLOv9åœ¨mAp50ä¸Šæé«˜äº†0.3%ï¼›è€Œåœ¨æˆ‘ä»¬è‡ªåˆ¶çš„æ•°æ®é›†ä¸Šï¼ŒSCC-YOLOç›¸è¾ƒäºYOLOv9æé«˜äº†0.5%ã€‚SCC-YOLOåœ¨è„‘è‚¿ç˜¤æ£€æµ‹æ–¹é¢å·²ç»è¾¾åˆ°äº†ä¸šç•Œé¡¶å°–çš„æ€§èƒ½ã€‚æºä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master%E3%80%82">https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/masterã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03836v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡å¼€å‘äº†ä¸€ç§æ–°å‹çš„SCC-YOLOæ¶æ„ï¼Œé€šè¿‡å°†SCConvæ³¨æ„åŠ›æœºåˆ¶èå…¥YOLOv9æ¨¡å‹ï¼Œæå‡äº†è„‘éƒ¨è‚¿ç˜¤å›¾åƒçš„è¯†åˆ«èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒï¼Œæ˜¾ç¤ºå‡ºè¯¥æ¨¡å‹åœ¨è„‘éƒ¨è‚¿ç˜¤æ£€æµ‹æ–¹é¢çš„ä¼˜å¼‚æ€§èƒ½ï¼Œè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SCC-YOLOæ¨¡å‹ç»“åˆäº†SCConvæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºäº†è„‘éƒ¨è‚¿ç˜¤å›¾åƒçš„è¯†åˆ«ç²¾åº¦ã€‚</li>
<li>SCConvæ¨¡å—èƒ½æœ‰æ•ˆå‡å°‘ç‰¹å¾çš„ç©ºé—´å’Œé€šé“å†—ä½™ï¼Œæå‡å›¾åƒç‰¹å¾çš„å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>åœ¨Br35Hæ•°æ®é›†ä¸Šï¼ŒSCC-YOLOç›¸è¾ƒäºYOLOv9æ¨¡å‹æé«˜äº†0.3%çš„mAp50æŒ‡æ ‡ã€‚</li>
<li>åœ¨è‡ªåˆ¶æ•°æ®é›†Brain_Tumor_Datasetä¸Šï¼ŒSCC-YOLOç›¸è¾ƒäºYOLOv9æ¨¡å‹æé«˜äº†0.5%çš„è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>SCC-YOLOæ¨¡å‹åœ¨è„‘éƒ¨è‚¿ç˜¤æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
<li>è¯¥æ¨¡å‹çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master%E3%80%82">https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/masterã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84321bdf7bb9547b06b2e695ffc11379.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41cdd4d727bb4ba92f1634652c0f9934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-069708ddce20a50ef27882a7a761751c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bccb4fa4abbfcf3f64fd648ee7380995.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0f2429e5f375e25501f6152510bedad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3df7519f4c04b6e423a9134b580c51c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1599e41c38c1a1767eefa2800a1cb828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56d6f27ce6aa50752ba3c588a5bc210a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c35bb56ee86dfdd0e48f1b116a88dc4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Characterization-of-Markarian-421-during-its-most-violent-year-Multiwavelength-variability-and-correlations"><a href="#Characterization-of-Markarian-421-during-its-most-violent-year-Multiwavelength-variability-and-correlations" class="headerlink" title="Characterization of Markarian 421 during its most violent year:   Multiwavelength variability and correlations"></a>Characterization of Markarian 421 during its most violent year:   Multiwavelength variability and correlations</h2><p><strong>Authors:K. Abe, S. Abe, J. Abhir, A. Abhishek, V. A. Acciari, A. Aguasca-Cabot, I. Agudo, T. Aniello, S. Ansoldi, L. A. Antonelli, A. Arbet Engels, C. Arcaro, K. Asano, D. Baack, A. BabiÄ‡, U. Barres de Almeida, J. A. Barrio, I. BatkoviÄ‡, A. Bautista, J. Baxter, J. Becerra GonzÃ¡lez, W. Bednarek, E. Bernardini, J. Bernete, A. Berti, J. Besenrieder, C. Bigongiari, A. Biland, O. Blanch, G. Bonnoli, Å½. BoÅ¡njak, E. Bronzini, I. Burelli, A. Campoy-Ordaz, R. Carosi, M. Carretero-Castrillo, A. J. Castro-Tirado, D. Cerasole, G. Ceribella, Y. Chai, A. Cifuentes, E. Colombo, J. L. Contreras, J. Cortina, S. Covino, G. Dâ€™Amico, F. Dâ€™Ammando, V. Dâ€™Elia, P. Da Vela, F. Dazzi, A. De Angelis, B. De Lotto, R. de Menezes, M. Delfino, J. Delgado, C. Delgado Mendez, F. Di Pierro, R. Di Tria, L. Di Venere, D. Dominis Prester, A. Donini, D. Dorner, M. Doro, L. Eisenberger, D. Elsaesser, J. Escudero, L. FariÃ±a, A. Fattorini, L. Foffano, L. Font, S. FrÃ¶se, S. Fukami, Y. Fukazawa, R. J. GarcÃ­a LÃ³pez, M. Garczarczyk, S. Gasparyan, M. Gaug, J. G. Giesbrecht Paiva, N. Giglietto, F. Giordano, P. Gliwny, N. GodinoviÄ‡, T. Gradetzke, R. Grau, D. Green, J. G. Green, P. GÃ¼nther, D. Hadasch, A. Hahn, T. Hassan, L. Heckmann, J. Herrera Llorente, D. Hrupec, R. Imazawa, K. Ishio, I. JimÃ©nez MartÃ­nez, J. Jormanainen, S. Kankkunen, T. Kayanoki, D. Kerszberg, G. W. Kluge, P. M. Kouch, H. Kubo, J. Kushida, M. LÃ¡inez, A. Lamastra, F. Leone, E. Lindfors, S. Lombardi, F. Longo, R. LÃ³pez-Coto, M. LÃ³pez-Moya, A. LÃ³pez-Oramas, S. Loporchio, A. Lorini, P. Majumdar, M. Makariev, G. Maneva, M. Manganaro, S. Mangano, K. Mannheim, M. Mariotti, M. MartÃ­nez, M. MartÃ­nez-Chicharro, A. Mas-Aguilar, D. Mazin, S. Menchiari, S. Mender, D. Miceli, T. Miener, J. M. Miranda, R. Mirzoyan, M. Molero GonzÃ¡lez, E. Molina, H. A. Mondal, A. Moralejo, D. Morcuende, T. Nakamori, C. Nanci, V. Neustroev, L. Nickel, C. Nigro, L. NikoliÄ‡, K. Nilsson, K. Nishijima, T. Njoh Ekoume, K. Noda, S. Nozaki, A. Okumura, J. Otero-Santos, S. Paiano, D. Paneque, R. Paoletti, J. M. Paredes, M. Peresano, M. Persic, M. Pihet, G. Pirola, F. Podobnik, P. G. Prada Moroni, E. Prandini, G. Principe, W. Rhode, M. RibÃ³, J. Rico, C. Righi, N. Sahakyan, T. Saito, F. G. Saturni, K. Schmidt, F. Schmuckermaier, J. L. Schubert, T. Schweizer, A. Sciaccaluga, G. Silvestri, J. Sitarek, D. Sobczynska, A. Stamerra, J. StriÅ¡koviÄ‡, D. Strom, Y. Suda, H. Tajima, M. Takahashi, R. Takeishi, F. Tavecchio, P. Temnikov, K. Terauchi, T. TerziÄ‡, M. Teshima, S. Truzzi, A. Tutone, S. Ubach, J. van Scherpenberg, S. Ventura, G. Verna, I. Viale, C. F. Vigorito, V. Vitale, I. Vovk, R. Walter, F. Wersig, M. Will, T. Yamamoto, S. G. Jorstad, A. P. Marscher, M. Perri, C. Leto, F. Verrecchia, M. Aller, W. Max-Moerbeck, A. C. S. Readhead, A. LÃ¤hteenmÃ¤ki, M. Tornikoski, M. A. Gurwell, A. E. Wehrle</strong></p>
<p>Mrk 421 was in its most active state around early 2010, which led to the highest TeV gamma-ray flux ever recorded from any active galactic nuclei. We aim to characterize the multiwavelength behavior during this exceptional year for Mrk 421, and evaluate whether it is consistent with the picture derived with data from other less exceptional years. We investigated the period from November 5, 2009, (MJD 55140) until July 3, 2010, (MJD 55380) with extensive coverage from very-high-energy (VHE; E$,&gt;,$100$,$GeV) gamma rays to radio with MAGIC, VERITAS, Fermi-LAT, RXTE, Swift, GASP-WEBT, VLBA, and a variety of additional optical and radio telescopes. We investigated the variability and correlation behavior among different energy bands in great detail. We find the strongest variability in X-rays and VHE gamma rays, and PSDs compatible with power-law functions. We observe strong correlations between X-rays and VHE gamma rays. We also report a marginally significant positive correlation between high-energy (HE; E$,&gt;,$100$,$MeV) gamma rays and the ultraviolet band. We detected marginally significant correlations between the HE and VHE gamma rays, and between HE gamma rays and the X-ray, that disappear when the large flare in February 2010 is excluded from the correlation study. The activity of Mrk 421 also yielded the first ejection of features in the VLBA images of the jet of Mrk 421. Yet the large uncertainties in the ejection times of these radio features prevent us from firmly associating them to the specific flares recorded during the campaign. We also show that the collected multi-instrument data are consistent with a scenario where the emission is dominated by two regions, a compact and extended zone, which could be considered as a simplified implementation of an energy-stratified jet as suggested by recent IXPE observations. </p>
<blockquote>
<p>Mrk 421åœ¨å¤§çº¦2010å¹´åˆæ—¶å¤„äºæœ€æ´»è·ƒçŠ¶æ€ï¼Œäº§ç”Ÿäº†è¿„ä»Šä¸ºæ­¢è®°å½•åˆ°çš„æ¥è‡ªä»»ä½•æ´»åŠ¨æ˜Ÿç³»æ ¸çš„æœ€é«˜TeVä¼½é©¬å°„çº¿æµé‡ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é’ˆå¯¹Mrk 421åœ¨è¿™ä¸åŒå¯»å¸¸çš„ä¸€å¹´ä¸­çš„å¤šæ³¢é•¿è¡Œä¸ºè¿›è¡Œç ”ç©¶ï¼Œå¹¶è¯„ä¼°å…¶æ˜¯å¦ä¸å…¶ä»–ä¸é‚£ä¹ˆç‰¹æ®Šçš„å¹´ä»½çš„æ•°æ®å¾—å‡ºçš„å›¾åƒä¸€è‡´ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä»2009å¹´11æœˆ5æ—¥ï¼ˆMJD 55140ï¼‰è‡³2010å¹´7æœˆ3æ—¥ï¼ˆMJD 55380ï¼‰æœŸé—´çš„æ—¶æœŸï¼Œå€ŸåŠ©MAGICã€VERITASã€Fermi-LATã€RXTEã€Swiftã€GASP-WEBTã€VLBAä»¥åŠå„ç§å…¶ä»–å…‰å­¦å’Œå°„ç”µæœ›è¿œé•œä»æé«˜èƒ½ï¼ˆVHEï¼›Eï¼100 GeVï¼‰ä¼½é©¬å°„çº¿åˆ°å°„ç”µæ³¢æ®µçš„å¹¿æ³›è¦†ç›–ã€‚æˆ‘ä»¬è¯¦ç»†ç ”ç©¶äº†ä¸åŒæ³¢æ®µä¹‹é—´çš„å˜åŒ–å’Œç›¸å…³è¡Œä¸ºã€‚æˆ‘ä»¬å‘ç°Xå°„çº¿å’ŒVHEä¼½é©¬å°„çº¿çš„å˜åŒ–æœ€ä¸ºå¼ºçƒˆï¼ŒåŠŸç‡è°±å¯†åº¦ä¸å¹‚å¾‹å‡½æ•°ç›¸ç¬¦ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°Xå°„çº¿å’ŒVHEä¼½é©¬å°„çº¿ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜æŠ¥å‘Šäº†åœ¨é«˜èƒ½ï¼ˆHEï¼›Eï¼100 MeVï¼‰ä¼½é©¬å°„çº¿å’Œç´«å¤–æ³¢æ®µä¹‹é—´å‡ºç°è¾¹ç¼˜æ˜¾è‘—çš„æ­£ç›¸å…³ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†é«˜èƒ½ä¼½é©¬å°„çº¿å’Œç”šé«˜èƒ½ä¼½é©¬å°„çº¿ä¹‹é—´ä»¥åŠé«˜èƒ½ä¼½é©¬å°„çº¿å’ŒXå°„çº¿ä¹‹é—´è¾¹ç¼˜æ˜¾è‘—çš„å…³è”æ€§ï¼Œè¿™äº›å…³è”åœ¨æ’é™¤äº†å‘ç”Ÿåœ¨2010å¹´2æœˆçš„ç‰¹å¤§çˆ†å‘ä¹‹åæ¶ˆå¤±ã€‚Mrk 421çš„æ´»åŠ¨è¿˜äº§ç”Ÿäº†Mrk 421å–·å°„æµVLBAå›¾åƒä¸­çš„ç‰¹å¾é¦–æ¬¡å–·å°„ã€‚ç„¶è€Œï¼Œè¿™äº›å°„ç”µç‰¹å¾å–·å°„æ—¶é—´çš„å·¨å¤§ä¸ç¡®å®šæ€§ä½¿æˆ‘ä»¬æ— æ³•å°†å®ƒä»¬ä¸æ´»åŠ¨æœŸé—´çš„ç‰¹å®šçˆ†å‘æ˜ç¡®åœ°è”ç³»èµ·æ¥ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ”¶é›†çš„å¤šä»ªå™¨æ•°æ®ä¸ä¸€ç§å‘å°„æƒ…æ™¯ç›¸ä¸€è‡´ï¼Œè¯¥æƒ…æ™¯ç”±ä¸¤ä¸ªåŒºåŸŸä¸»å¯¼ï¼Œå³ç´§å‡‘åŒºåŸŸå’Œæ‰©å±•åŒºåŸŸï¼Œè¿™å¯ä»¥è¢«è§†ä¸ºç”±æœ€è¿‘çš„IXPEè§‚æµ‹æ‰€æç¤ºçš„èƒ½é‡åˆ†å±‚å–·å°„çš„ç®€åŒ–å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03831v1">PDF</a> Accepted for publication in Astronomy &amp; Astrophysics. Corresponding   authors: Felix Schmuckermaier, David Paneque, Axel Arbet Engels</p>
<p><strong>Summary</strong><br>    Mrk 421åœ¨å¤§çº¦æ—©æœŸæ´»è·ƒçš„TeVä¼½é©¬å°„çº¿æµé‡æ—¶æ®µè¡¨ç°å‡ºå…¶æœ€æ´»è·ƒçŠ¶æ€ã€‚ç ”ç©¶è€…å¯¹è¯¥æ—¶æ®µè¿›è¡Œäº†å¤šæ³¢é•¿è¡Œä¸ºç‰¹å¾åˆ†æï¼Œå‘ç°å…¶åœ¨Xå°„çº¿å’Œç”šé«˜èƒ½ä¼½é©¬å°„çº¿ä¸Šçš„å˜åŒ–æœ€ä¸ºæ˜¾è‘—ï¼Œå¹¶è§‚å¯Ÿåˆ°è¿™äº›æ³¢æ®µé—´çš„å¼ºç›¸å…³æ€§ã€‚æ­¤å¤–ï¼ŒMrk 421çš„æ´»åŠ¨è¿˜äº§ç”Ÿäº†é¦–æ¬¡çš„å–·æµç‰¹å¾å–·å°„ç°è±¡ï¼Œä½†æ— æ³•ç¡®å®šå…¶ä¸ç‰¹å®šè€€æ–‘ä¹‹é—´çš„ç›´æ¥è”ç³»ã€‚ç»¼åˆå¤šä»ªå™¨æ•°æ®è¡¨æ˜ï¼Œå…¶å‘å°„ä¸»è¦ç”±ä¸¤ä¸ªåŒºåŸŸä¸»å¯¼ï¼Œä¸€ä¸ªç´§å‡‘åŒºåŸŸå’Œä¸€ä¸ªæ‰©å±•åŒºåŸŸï¼Œç¬¦åˆç®€åŒ–çš„èƒ½é‡åˆ†å±‚å–·å°„æƒ…æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mrk 421åœ¨å¤§çº¦æ—©æœŸæ´»è·ƒçŠ¶æ€æœŸé—´è¡¨ç°å‡ºæœ€é«˜çš„TeVä¼½é©¬å°„çº¿æµé‡è®°å½•ã€‚</li>
<li>å¤šæ³¢é•¿è¡Œä¸ºåˆ†ææ­ç¤ºäº†è¯¥æ—¶æ®µå†…å¼ºçƒˆçš„Xå°„çº¿å’Œç”šé«˜èƒ½ä¼½é©¬å°„çº¿å˜åŒ–ã€‚</li>
<li>Xå°„çº¿å’Œç”šé«˜èƒ½ä¼½é©¬å°„çº¿ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚</li>
<li>é«˜èƒ½ä¼½é©¬å°„çº¿ä¸ç´«å¤–æ³¢æ®µä¹‹é—´å­˜åœ¨è¾¹ç¼˜æ˜¾è‘—æ­£ç›¸å…³ã€‚</li>
<li>æ’é™¤å¤§å‹è€€æ–‘åï¼Œé«˜èƒ½ä¼½é©¬å°„çº¿ä¸ç”šé«˜èƒ½ä¼½é©¬å°„çº¿åŠé«˜èƒ½ä¼½é©¬å°„çº¿ä¸Xå°„çº¿çš„ç›¸å…³æ€§è¾¹ç¼˜æ˜¾è‘—ï¼Œä½†å¹¶ä¸æ˜¾è‘—ã€‚</li>
<li>Mrk 421çš„æ´»åŠ¨äº§ç”Ÿäº†é¦–æ¬¡çš„å–·æµç‰¹å¾å–·å°„ç°è±¡ï¼Œä½†æ— æ³•ç¡®å®šå…¶ä¸ç‰¹å®šè€€æ–‘çš„ç›´æ¥å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61139bc4e33e4e7991ab84eec1478141.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-577fa31983325394d15d9f1ef41331b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cfa7ade038b3f1450c26c1957f938fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e7db322e8b71516c9e1a715d6a0ff98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e378064c8b999369d2f7625cae6cda4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Deep-Sylvester-Posterior-Inference-for-Adaptive-Compressed-Sensing-in-Ultrasound-Imaging"><a href="#Deep-Sylvester-Posterior-Inference-for-Adaptive-Compressed-Sensing-in-Ultrasound-Imaging" class="headerlink" title="Deep Sylvester Posterior Inference for Adaptive Compressed Sensing in   Ultrasound Imaging"></a>Deep Sylvester Posterior Inference for Adaptive Compressed Sensing in   Ultrasound Imaging</h2><p><strong>Authors:Simon W. Penninga, Hans van Gorp, Ruud J. G. van Sloun</strong></p>
<p>Ultrasound images are commonly formed by sequential acquisition of beam-steered scan-lines. Minimizing the number of required scan-lines can significantly enhance frame rate, field of view, energy efficiency, and data transfer speeds. Existing approaches typically use static subsampling schemes in combination with sparsity-based or, more recently, deep-learning-based recovery. In this work, we introduce an adaptive subsampling method that maximizes intrinsic information gain in-situ, employing a Sylvester Normalizing Flow encoder to infer an approximate Bayesian posterior under partial observation in real-time. Using the Bayesian posterior and a deep generative model for future observations, we determine the subsampling scheme that maximizes the mutual information between the subsampled observations, and the next frame of the video. We evaluate our approach using the EchoNet cardiac ultrasound video dataset and demonstrate that our active sampling method outperforms competitive baselines, including uniform and variable-density random sampling, as well as equidistantly spaced scan-lines, improving mean absolute reconstruction error by 15%. Moreover, posterior inference and the sampling scheme generation are performed in just 0.015 seconds (66Hz), making it fast enough for real-time 2D ultrasound imaging applications. </p>
<blockquote>
<p>è¶…å£°æ³¢å›¾åƒé€šå¸¸æ˜¯é€šè¿‡è¿ç»­è·å–æ³¢æŸè½¬å‘æ‰«æçº¿å½¢æˆçš„ã€‚å‡å°‘æ‰€éœ€çš„æ‰«æçº¿æ•°é‡å¯ä»¥æ˜¾è‘—æé«˜å¸§ç‡ã€è§†é‡ã€èƒ½æ•ˆå’Œæ•°æ®ä¼ è¾“é€Ÿåº¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨é™æ€å­é‡‡æ ·æ–¹æ¡ˆï¼Œç»“åˆåŸºäºç¨€ç–æ€§æˆ–æœ€è¿‘å…´èµ·çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ¢å¤æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å­é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ€å¤§é™åº¦åœ°æé«˜äº†ç°åœºå†…åœ¨ä¿¡æ¯å¢ç›Šï¼Œé‡‡ç”¨Sylvesteræ­£è§„åŒ–æµç¼–ç å™¨å®æ—¶æ¨æ–­éƒ¨åˆ†è§‚æµ‹ä¸‹çš„è¿‘ä¼¼è´å¶æ–¯åéªŒæ¦‚ç‡ã€‚åˆ©ç”¨è´å¶æ–¯åéªŒæ¦‚ç‡å’Œç”¨äºæœªæ¥è§‚æµ‹çš„æ·±å±‚ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä½¿å­é‡‡æ ·è§‚æµ‹ä¸è§†é¢‘çš„ä¸‹ä¸€å¸§ä¹‹é—´çš„äº’ä¿¡æ¯æœ€å¤§åŒ–çš„å­é‡‡æ ·æ–¹æ¡ˆã€‚æˆ‘ä»¬ä½¿ç”¨EchoNetå¿ƒè„è¶…å£°è§†é¢‘æ•°æ®é›†è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„ä¸»åŠ¨é‡‡æ ·æ–¹æ³•åœ¨å‡åŒ€å’Œå¯å˜å¯†åº¦éšæœºé‡‡æ ·ä»¥åŠç­‰è·é—´éš”æ‰«æçº¿ç­‰ç«äº‰åŸºçº¿ä¹‹ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ï¼Œå¹³å‡ç»å¯¹é‡å»ºè¯¯å·®æé«˜äº†15%ã€‚æ­¤å¤–ï¼ŒåéªŒæ¨æ–­å’Œé‡‡æ ·æ–¹æ¡ˆç”Ÿæˆä»…éœ€0.015ç§’ï¼ˆ66Hzï¼‰ï¼Œè¶³ä»¥ç”¨äºå®æ—¶2Dè¶…å£°æˆåƒåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03825v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æè¿°äº†ä¸€ç§åŸºäºè¶…å£°æ³¢å›¾åƒè‡ªé€‚åº”å­é‡‡æ ·æ–¹æ³•çš„æå‡ºï¼Œé€šè¿‡å®æ—¶æ¨ç†æ¨æ–­è¿‘ä¼¼çš„è´å¶æ–¯åéªŒåˆ†å¸ƒå¹¶åˆ©ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹é¢„æµ‹æœªæ¥è§‚æµ‹å€¼ï¼Œæœ€å¤§åŒ–å­é‡‡æ ·è§‚æµ‹ä¸ä¸‹ä¸€å¸§è§†é¢‘ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åœ¨EchoNetå¿ƒè„è¶…å£°è§†é¢‘æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸æ¯”äºå…¶ä»–åŸºçº¿é‡‡æ ·ç­–ç•¥æå‡äº†å›¾åƒé‡å»ºç²¾åº¦å¹¶ä¼˜åŒ–äº†è¿è¡Œé€Ÿåº¦ï¼Œä¸ºåç»­å®æ—¶äºŒç»´è¶…å£°æˆåƒåº”ç”¨æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°æ³¢å›¾åƒé€šå¸¸é€šè¿‡è¿ç»­è·å–å…‰æŸæ‰«æçº¿å½¢æˆã€‚</li>
<li>å‡å°‘æ‰€éœ€æ‰«æçº¿çš„æ•°é‡å¯ä»¥æ˜¾è‘—æé«˜å¸§ç‡ã€è§†é‡ã€èƒ½é‡æ•ˆç‡å’Œæ•°æ®ä¼ è¾“é€Ÿåº¦ã€‚</li>
<li>å½“å‰æ–¹æ³•é€šå¸¸é‡‡ç”¨é™æ€å­é‡‡æ ·æ–¹æ¡ˆç»“åˆç¨€ç–æ€§æˆ–æ·±åº¦å­¦ä¹ æ¢å¤æŠ€æœ¯ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å­é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ€å¤§åŒ–å†…åœ¨ä¿¡æ¯å¢ç›Šï¼Œå¹¶é‡‡ç”¨Sylvester Normalizing Flowç¼–ç å™¨å®æ—¶æ¨æ–­éƒ¨åˆ†è§‚æµ‹çš„è¿‘ä¼¼è´å¶æ–¯åéªŒåˆ†å¸ƒã€‚</li>
<li>åˆ©ç”¨è´å¶æ–¯åéªŒåˆ†å¸ƒå’Œæ·±åº¦ç”Ÿæˆæ¨¡å‹é¢„æµ‹æœªæ¥è§‚æµ‹å€¼ï¼Œç¡®å®šèƒ½æœ€å¤§åŒ–å­é‡‡æ ·è§‚æµ‹ä¸ä¸‹ä¸€å¸§ä¹‹é—´çš„äº’ä¿¡æ¯çš„å­é‡‡æ ·æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-986cfd1f351e7e22805a4394c91ac8de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-428b5b3bc2a40026a661b0763ad1d19a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2704679423b47930008f90aa80be9b31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a75459e9de76e3885073fb7a16864b90.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-occurrence-of-powerful-flares-stronger-than-X10-class-in-Solar-Cycles"><a href="#The-occurrence-of-powerful-flares-stronger-than-X10-class-in-Solar-Cycles" class="headerlink" title="The occurrence of powerful flares stronger than X10 class in Solar   Cycles"></a>The occurrence of powerful flares stronger than X10 class in Solar   Cycles</h2><p><strong>Authors:Baolin Tan, Yin Zhang, Jing Huang, Kaifan Ji</strong></p>
<p>Solar flares stronger than X10 (S-flares, &gt;X10) are the highest class flares which significantly impact on the Sunâ€™s evolution and space weather. Based on observations of Geostationary Orbiting Environmental Satellites (GOES) at soft X-ray (SXR) wavelength and the daily sunspot numbers (DSNs) since 1975, we obtained some interesting and heuristic conclusions: (1) Both S-flares and the more powerful extremely strong flares (ES-flares, &gt;X14.3) mostly occur in the late phases of solar cycles and low-latitude regions on the solar disk; (2) Similar to X-class flares, the occurrence of S-flares in each solar cycle is somewhat random, but the occurrence of ES-flares seems to be dominated by the mean DSN (Vm) and its root-mean-square deviation during the valley phase (Vd) before the cycle: the ES-flare number is strongly correlated with Vd, and the occurrence time of the first ES-flare is anti-correlated with Vd and Vm. These facts indicate that the higher the Vm and Vd, the stronger the solar cycle, the more the ES-flares and the earlier they occurred. We proposed that the Sun may have a low-latitude active zone (LAZ), and most ES-flares are generated from the interaction between LAZ and the newly emerging active regions. The correlations and the linear regression functions may provide an useful method to predict the occurrence of ES-flares in an upcoming solar cycle, which derives that solar cycle 25 will have about 2 ES-flares after the spring of 2027. </p>
<blockquote>
<p>å¤ªé˜³è€€æ–‘ä¸­å¼ºåº¦è¶…è¿‡X10ï¼ˆSçº§è€€æ–‘ï¼Œ&gt;X10ï¼‰çš„è€€æ–‘æ˜¯å¯¹å¤ªé˜³æ¼”åŒ–å’Œå¤ªç©ºå¤©æ°”äº§ç”Ÿé‡å¤§å½±å“çš„æœ€é«˜çº§åˆ«è€€æ–‘ã€‚åŸºäºå¯¹åœ°çƒé™æ­¢è½¨é“ç¯å¢ƒå«æ˜Ÿï¼ˆGOESï¼‰åœ¨è½¯Xå°„çº¿ï¼ˆSXRï¼‰æ³¢é•¿ä¸‹çš„è§‚æµ‹ä»¥åŠè‡ª1975å¹´ä»¥æ¥çš„æ¯æ—¥å¤ªé˜³é»‘å­æ•°ï¼ˆDSNï¼‰ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€äº›æœ‰è¶£ä¸”å…·æœ‰å¯å‘æ€§çš„ç»“è®ºï¼šï¼ˆ1ï¼‰Sçº§è€€æ–‘å’Œæ›´å¼ºå¤§çš„æç«¯å¼ºè€€æ–‘ï¼ˆESçº§è€€æ–‘ï¼Œ&gt;X14.3ï¼‰å¤§å¤šå‘ç”Ÿåœ¨å¤ªé˜³å‘¨æœŸåæœŸå’Œå¤ªé˜³åœ†ç›˜ä½çº¬åº¦åŒºåŸŸï¼›ï¼ˆ2ï¼‰ä¸Xçº§è€€æ–‘ç±»ä¼¼ï¼ŒSçº§è€€æ–‘åœ¨æ¯ä¸ªå¤ªé˜³å‘¨æœŸä¸­çš„å‘ç”Ÿæœ‰äº›éšæœºï¼Œä½†ESçº§è€€æ–‘çš„å‘ç”Ÿä¼¼ä¹å—åˆ°å‘¨æœŸå‰ä½è°·é˜¶æ®µçš„å¹³å‡DSNï¼ˆVmï¼‰åŠå…¶å‡æ–¹æ ¹åå·®ï¼ˆVdï¼‰çš„ä¸»å¯¼ï¼šESçº§è€€æ–‘æ•°ä¸Vdå¼ºçƒˆç›¸å…³ï¼Œè€Œé¦–ä¸ªESçº§è€€æ–‘çš„å‘ç”Ÿæ—¶é—´ä¸Vdå’ŒVmå‘ˆè´Ÿç›¸å…³ã€‚è¿™äº›äº‹å®è¡¨æ˜ï¼ŒVmå’ŒVdè¶Šé«˜ï¼Œå¤ªé˜³å‘¨æœŸè¶Šå¼ºï¼ŒESçº§è€€æ–‘è¶Šå¤šä¸”å‘ç”Ÿæ—¶é—´è¶Šæ—©ã€‚æˆ‘ä»¬æå‡ºï¼Œå¤ªé˜³å¯èƒ½æœ‰ä¸€ä¸ªä½çº¬åº¦æ´»è·ƒåŒºï¼ˆLAZï¼‰ï¼Œå¤§å¤šæ•°ESçº§è€€æ–‘äº§ç”ŸäºLAZä¸æ–°å…´æ´»è·ƒåŒºåŸŸçš„ç›¸äº’ä½œç”¨ã€‚ç›¸å…³æ€§å’Œçº¿æ€§å›å½’å‡½æ•°å¯èƒ½æä¾›äº†ä¸€ç§é¢„æµ‹å³å°†æ¥ä¸´çš„å¤ªé˜³å‘¨æœŸå†…ESçº§è€€æ–‘å‘ç”Ÿçš„æ–¹æ³•ï¼Œç”±æ­¤æ¨æ–­ï¼Œå¤ªé˜³å‘¨æœŸ25å°†åœ¨2027å¹´æ˜¥å­£åæœ‰å¤§çº¦2æ¬¡ESçº§è€€æ–‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03791v1">PDF</a> 10 pages, 4 figures, 3 tables, accepted by ApJ Letters</p>
<p><strong>Summary</strong><br>    å¤ªé˜³è€€æ–‘ä¸­å¼ºåº¦è¶…è¿‡X10çš„è€€æ–‘ï¼ˆS-flaresï¼‰å¯¹å¤ªé˜³æ¼”åŒ–å’Œå¤ªç©ºå¤©æ°”äº§ç”Ÿé‡è¦å½±å“ã€‚é€šè¿‡è§‚æµ‹åœ°çƒåŒæ­¥è½¨é“ç¯å¢ƒå«æ˜Ÿï¼ˆGOESï¼‰çš„è½¯Xå°„çº¿ï¼ˆSXRï¼‰æ³¢é•¿å’Œè‡ª1975å¹´ä»¥æ¥çš„æ—¥é¢æš—æ–‘æ•°ï¼ˆDSNsï¼‰ï¼Œå‘ç°S-flareså’Œæ›´å¼ºå¤§çš„æç«¯å¼ºè€€æ–‘ï¼ˆES-flaresï¼‰å¤§å¤šå‡ºç°åœ¨å¤ªé˜³å‘¨æœŸåæœŸå’Œä½çº¬åº¦åŒºåŸŸï¼›ES-flaresçš„å‘ç”Ÿä¸ä½è°·æœŸçš„å¹³å‡æ—¥é¢æš—æ–‘æ•°ï¼ˆVmï¼‰å’Œå‡æ–¹æ ¹åå·®ï¼ˆVdï¼‰å¯†åˆ‡ç›¸å…³ã€‚æå‡ºå¤ªé˜³å¯èƒ½å­˜åœ¨ä¸€ä¸ªä½çº¬åº¦æ´»åŠ¨åŒºï¼ˆLAZï¼‰ï¼Œå¤šæ•°ES-flaresæºäºLAZä¸æ–°ç”Ÿæ´»åŠ¨åŒºçš„ç›¸äº’ä½œç”¨ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºé¢„æµ‹æœªæ¥å¤ªé˜³å‘¨æœŸä¸­ES-flaresçš„å‘ç”Ÿï¼Œé¢„æµ‹å¤ªé˜³å‘¨æœŸ25å°†åœ¨2027å¹´æ˜¥å­£åå‘ç”Ÿçº¦2æ¬¡ES-flaresã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>S-flareså’ŒES-flaresä¸»è¦å‡ºç°åœ¨å¤ªé˜³å‘¨æœŸåæœŸå’Œä½çº¬åº¦åŒºåŸŸã€‚</li>
<li>ES-flaresçš„å‘ç”Ÿä¸Vmå’ŒVdå¯†åˆ‡ç›¸å…³ï¼Œå…¶ä¸­ä¸Vdçš„å…³è”å°¤ä¸ºå¼ºçƒˆã€‚</li>
<li>ç¬¬ä¸€åœºES-flareçš„å‘ç”Ÿæ—¶é—´ä¸Vmå’ŒVdå‘ˆè´Ÿç›¸å…³ã€‚</li>
<li>å¤ªé˜³å¯èƒ½å­˜åœ¨ä¸€ä¸ªä½çº¬åº¦æ´»åŠ¨åŒºï¼ˆLAZï¼‰ã€‚</li>
<li>LAZä¸æ–°ç”Ÿæ´»åŠ¨åŒºçš„ç›¸äº’ä½œç”¨æ˜¯äº§ç”Ÿå¤šæ•°ES-flaresçš„åŸå› ã€‚</li>
<li>å‘ç°äº†é¢„æµ‹æœªæ¥å¤ªé˜³å‘¨æœŸä¸­ES-flareså‘ç”Ÿçš„æ–¹æ³•å’ŒæŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03791">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b870646a22a2fc6abda878e9006a2e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca3992bd3b584435dc258dbde9580b84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3454ee265fdeac46a957fd1dd864604f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1a4cb4d8553fbd559055f829e868079.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b00bd068ad56f78fed2e6300779cfdf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Re-Visible-Dual-Domain-Self-Supervised-Deep-Unfolding-Network-for-MRI-Reconstruction"><a href="#Re-Visible-Dual-Domain-Self-Supervised-Deep-Unfolding-Network-for-MRI-Reconstruction" class="headerlink" title="Re-Visible Dual-Domain Self-Supervised Deep Unfolding Network for MRI   Reconstruction"></a>Re-Visible Dual-Domain Self-Supervised Deep Unfolding Network for MRI   Reconstruction</h2><p><strong>Authors:Hao Zhang, Qi Wang, Jian Sun, Zhijie Wen, Jun Shi, Shihui Ying</strong></p>
<p>Magnetic Resonance Imaging (MRI) is widely used in clinical practice, but suffered from prolonged acquisition time. Although deep learning methods have been proposed to accelerate acquisition and demonstrate promising performance, they rely on high-quality fully-sampled datasets for training in a supervised manner. However, such datasets are time-consuming and expensive-to-collect, which constrains their broader applications. On the other hand, self-supervised methods offer an alternative by enabling learning from under-sampled data alone, but most existing methods rely on further partitioned under-sampled k-space data as modelâ€™s input for training, resulting in a loss of valuable information. Additionally, their models have not fully incorporated image priors, leading to degraded reconstruction performance. In this paper, we propose a novel re-visible dual-domain self-supervised deep unfolding network to address these issues when only under-sampled datasets are available. Specifically, by incorporating re-visible dual-domain loss, all under-sampled k-space data are utilized during training to mitigate information loss caused by further partitioning. This design enables the model to implicitly adapt to all under-sampled k-space data as input. Additionally, we design a deep unfolding network based on Chambolle and Pock Proximal Point Algorithm (DUN-CP-PPA) to achieve end-to-end reconstruction, incorporating imaging physics and image priors to guide the reconstruction process. By employing a Spatial-Frequency Feature Extraction (SFFE) block to capture global and local feature representation, we enhance the modelâ€™s efficiency to learn comprehensive image priors. Experiments conducted on the fastMRI and IXI datasets demonstrate that our method significantly outperforms state-of-the-art approaches in terms of reconstruction performance. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¸´åºŠå®è·µä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨é‡‡é›†æ—¶é—´é•¿çš„é—®é¢˜ã€‚è™½ç„¶å·²æå‡ºæ·±åº¦å­¦ä¹ æ–¹æ³•æ¥åŠ é€Ÿé‡‡é›†å¹¶å±•ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä¾èµ–äºé«˜è´¨é‡çš„å…¨é‡‡æ ·æ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒã€‚ç„¶è€Œï¼Œè¿™ç§æ•°æ®é›†è€—æ—¶ä¸”æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶æ›´å¹¿æ³›çš„åº”ç”¨ã€‚å¦ä¸€æ–¹é¢ï¼Œè‡ªç›‘ç£æ–¹æ³•èƒ½å¤Ÿé€šè¿‡ä»…ä»æ¬ é‡‡æ ·æ•°æ®ä¸­è¿›è¡Œå­¦ä¹ æ¥æä¾›æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºè¿›ä¸€æ­¥åˆ†åŒºçš„æ¬ é‡‡æ ·k-ç©ºé—´æ•°æ®ä½œä¸ºæ¨¡å‹è®­ç»ƒçš„è¾“å…¥ï¼Œå¯¼è‡´æœ‰ä»·å€¼çš„ä¿¡æ¯ä¸¢å¤±ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„æ¨¡å‹æ²¡æœ‰å®Œå…¨èå…¥å›¾åƒå…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´é‡å»ºæ€§èƒ½ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯è§†åŒåŸŸè‡ªç›‘ç£æ·±åº¦å±•å¼€ç½‘ç»œï¼Œä»¥è§£å†³ä»…æœ‰æ¬ é‡‡æ ·æ•°æ®é›†å¯ç”¨æ—¶çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡èå…¥å¯è§†åŒåŸŸæŸå¤±ï¼Œæ‰€æœ‰æ¬ é‡‡æ ·k-ç©ºé—´æ•°æ®åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éƒ½å¾—åˆ°åˆ©ç”¨ï¼Œä»¥å‡è½»å› è¿›ä¸€æ­¥åˆ†åŒºè€Œé€ æˆçš„ä¿¡æ¯æŸå¤±ã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹èƒ½å¤Ÿéšå¼é€‚åº”æ‰€æœ‰æ¬ é‡‡æ ·k-ç©ºé—´æ•°æ®ä½œä¸ºè¾“å…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºChambolleå’ŒPockè¿‘ç«¯ç‚¹ç®—æ³•ï¼ˆDUN-CP-PPAï¼‰è®¾è®¡äº†ä¸€ä¸ªæ·±åº¦å±•å¼€ç½‘ç»œï¼Œä»¥å®ç°ç«¯åˆ°ç«¯çš„é‡å»ºï¼Œèå…¥æˆåƒç‰©ç†å’Œå›¾åƒå…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼é‡å»ºè¿‡ç¨‹ã€‚é€šè¿‡é‡‡ç”¨ç©ºé—´é¢‘ç‡ç‰¹å¾æå–å—æ¥æ•è·å…¨å±€å’Œå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œæˆ‘ä»¬æé«˜äº†æ¨¡å‹å­¦ä¹ å…¨é¢å›¾åƒå…ˆéªŒçŸ¥è¯†çš„æ•ˆç‡ã€‚åœ¨fastMRIå’ŒIXIæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯çš„é‡å»ºæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03737v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¯è§†åŒé‡é¢†åŸŸçš„è‡ªç›‘ç£æ·±åº¦å±•å¼€ç½‘ç»œï¼Œç”¨äºè§£å†³ä»…ä½¿ç”¨æ¬ é‡‡æ ·æ•°æ®é›†æ—¶çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å¯è§†åŒé‡é¢†åŸŸæŸå¤±ï¼Œå……åˆ†åˆ©ç”¨æ¬ é‡‡æ ·kç©ºé—´æ•°æ®ï¼Œå‡å°‘å› è¿›ä¸€æ­¥åˆ†åŒºè€Œäº§ç”Ÿçš„ä¿¡æ¯æŸå¤±ã€‚ç»“åˆæˆåƒç‰©ç†å’Œå›¾åƒå…ˆéªŒçš„ç«¯åˆ°ç«¯é‡å»ºè¿‡ç¨‹ï¼Œå®ç°äº†é«˜æ•ˆçš„é‡å»ºæ•ˆæœã€‚åœ¨fastMRIå’ŒIXIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯è§†åŒé‡é¢†åŸŸçš„è‡ªç›‘ç£æ·±åº¦å±•å¼€ç½‘ç»œæ¥ä¼˜åŒ–MRIå›¾åƒé‡å»ºè¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¯è§†åŒé‡é¢†åŸŸæŸå¤±ï¼Œå……åˆ†åˆ©ç”¨æ¬ é‡‡æ ·æ•°æ®ï¼Œå‡å°‘ä¿¡æ¯æŸå¤±ã€‚</li>
<li>ç»“åˆæˆåƒç‰©ç†å’Œå›¾åƒå…ˆéªŒï¼Œå®ç°ç«¯åˆ°ç«¯çš„é‡å»ºè¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨ç©ºé—´é¢‘ç‡ç‰¹å¾æå–å—ï¼ˆSFFEï¼‰ï¼Œå¢å¼ºäº†å­¦ä¹ å›¾åƒå…ˆéªŒçš„æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dd65d65a07c2d2d8d925bf447d59494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6526e3831c8fc0211168584c4f6c75bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ea76ccf8835ee91acda79e78cc07a4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ef4ed3b5d9d65668c178b5f3a573436.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Self-adaptive-vision-language-model-for-3D-segmentation-of-pulmonary-artery-and-vein"><a href="#Self-adaptive-vision-language-model-for-3D-segmentation-of-pulmonary-artery-and-vein" class="headerlink" title="Self-adaptive vision-language model for 3D segmentation of pulmonary   artery and vein"></a>Self-adaptive vision-language model for 3D segmentation of pulmonary   artery and vein</h2><p><strong>Authors:Xiaotong Guo, Deqian Yang, Dan Wang, Haochen Zhao, Yuan Li, Zhilin Sui, Tao Zhou, Lijun Zhang, Yanda Meng</strong></p>
<p>Accurate segmentation of pulmonary structures iscrucial in clinical diagnosis, disease study, and treatment planning. Significant progress has been made in deep learning-based segmentation techniques, but most require much labeled data for training. Consequently, developing precise segmentation methods that demand fewer labeled datasets is paramount in medical image analysis. The emergence of pre-trained vision-language foundation models, such as CLIP, recently opened the door for universal computer vision tasks. Exploiting the generalization ability of these pre-trained foundation models on downstream tasks, such as segmentation, leads to unexpected performance with a relatively small amount of labeled data. However, exploring these models for pulmonary artery-vein segmentation is still limited. This paper proposes a novel framework called Language-guided self-adaptive Cross-Attention Fusion Framework. Our method adopts pre-trained CLIP as a strong feature extractor for generating the segmentation of 3D CT scans, while adaptively aggregating the cross-modality of text and image representations. We propose a s pecially designed adapter module to fine-tune pre-trained CLIP with a self-adaptive learning strategy to effectively fuse the two modalities of embeddings. We extensively validate our method on a local dataset, which is the largest pulmonary artery-vein CT dataset to date and consists of 718 labeled data in total. The experiments show that our method outperformed other state-of-the-art methods by a large margin. Our data and code will be made publicly available upon acceptance. </p>
<blockquote>
<p>è‚ºç»“æ„çš„ç²¾ç¡®åˆ†å‰²åœ¨ä¸´åºŠè¯Šæ–­ã€ç–¾ç—…ç ”ç©¶ã€å’Œæ²»ç–—æ–¹æ¡ˆåˆ¶å®šä¸­è‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²æŠ€æœ¯å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å¤§å¤šæ•°æŠ€æœ¯éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®æ¥è¿›è¡Œè®­ç»ƒã€‚å› æ­¤ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œå¼€å‘éœ€è¦è¾ƒå°‘æ ‡æ³¨æ•°æ®é›†çš„é«˜ç²¾åº¦åˆ†å‰²æ–¹æ³•è‡³å…³é‡è¦ã€‚æœ€è¿‘å‡ºç°çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä¸ºé€šç”¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡æ‰“å¼€äº†å¤§é—¨ã€‚åˆ©ç”¨è¿™äº›é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åœ¨ç›¸å¯¹è¾ƒå°‘çš„æ ‡æ³¨æ•°æ®ä¸Šå®ç°æ„æƒ³ä¸åˆ°çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹ç”¨äºè‚ºåŠ¨è„‰-é™è„‰åˆ†å‰²çš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè¯­è¨€å¼•å¯¼çš„è‡ªé€‚åº”äº¤å‰æ³¨æ„åŠ›èåˆæ¡†æ¶çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒçš„CLIPä½œä¸ºå¼ºå¤§çš„ç‰¹å¾æå–å™¨ï¼Œç”¨äºç”Ÿæˆ3D CTæ‰«æçš„åˆ†å‰²ï¼ŒåŒæ—¶è‡ªé€‚åº”åœ°èšåˆæ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºçš„è·¨æ¨¡æ€ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨è®¾è®¡çš„é€‚é…å™¨æ¨¡å—ï¼Œé‡‡ç”¨è‡ªé€‚åº”å­¦ä¹ ç­–ç•¥å¯¹é¢„è®­ç»ƒçš„CLIPè¿›è¡Œå¾®è°ƒï¼Œä»¥æœ‰æ•ˆåœ°èåˆä¸¤ç§æ¨¡æ€çš„åµŒå…¥ã€‚æˆ‘ä»¬åœ¨æœ¬åœ°æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼Œè¯¥æ•°æ®é›†æ˜¯ç›®å‰æœ€å¤§çš„è‚ºåŠ¨è„‰-é™è„‰CTæ•°æ®é›†ï¼Œæ€»å…±åŒ…å«718ä¸ªæ ‡æ³¨æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å°†åœ¨æ¥å—åå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03722v1">PDF</a> 8 pages,3 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†å‰²æŠ€æœ¯åœ¨ä¸´åºŠè¯Šç–—ã€ç–¾ç—…ç ”ç©¶å’Œæ²»ç–—è®¡åˆ’ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ã€‚é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å‡ºç°ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„æ–¹å‘ï¼Œå…·æœ‰å°‘æ ‡æ³¨æ•°æ®ä¸‹è‰¯å¥½è¡¨ç°çš„å¯èƒ½ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºè¯­è¨€å¼•å¯¼è‡ªé€‚åº”è·¨æ³¨æ„åŠ›èåˆæ¡†æ¶çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒCLIPæ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œå¯¹ä¸‰ç»´CTæ‰«æè¿›è¡Œåˆ†å‰²ï¼Œå¹¶è‡ªé€‚åº”èåˆæ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºã€‚é€šè¿‡æœ¬åœ°æ•°æ®é›†éªŒè¯ï¼Œè¯¥æ–¹æ³•å¤§å¹…ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠè¯Šç–—ã€ç–¾ç—…ç ”ç©¶å’Œæ²»ç–—è®¡åˆ’ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ã€‚</li>
<li>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æ–°æ–¹å‘ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§è¯­è¨€å¼•å¯¼è‡ªé€‚åº”è·¨æ³¨æ„åŠ›èåˆæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒCLIPæ¨¡å‹è¿›è¡Œè‚ºåŠ¨è„‰è¡€ç®¡åˆ†å‰²ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬è‡ªé€‚åº”åœ°èåˆæ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºçš„ç‰¹æ®Šé€‚é…å™¨æ¨¡å—ã€‚</li>
<li>åœ¨æœ¬åœ°æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1362e31da1a7e93fc15b54a0c4af8330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06ae655fe7bce8faac4aa64ec52359e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e32facf68ee6a0c3dc2dcfb8718d8b0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26d2d73ac85683afe329339f6d2186b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-600f0c8409349a685597961ffc7e2adb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c27765113351a12056a418054e2546e2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CFFormer-Cross-CNN-Transformer-Channel-Attention-and-Spatial-Feature-Fusion-for-Improved-Segmentation-of-Low-Quality-Medical-Images"><a href="#CFFormer-Cross-CNN-Transformer-Channel-Attention-and-Spatial-Feature-Fusion-for-Improved-Segmentation-of-Low-Quality-Medical-Images" class="headerlink" title="CFFormer: Cross CNN-Transformer Channel Attention and Spatial Feature   Fusion for Improved Segmentation of Low Quality Medical Images"></a>CFFormer: Cross CNN-Transformer Channel Attention and Spatial Feature   Fusion for Improved Segmentation of Low Quality Medical Images</h2><p><strong>Authors:Jiaxuan Li, Qing Xu, Xiangjian He, Ziyu Liu, Daokun Zhang, Ruili Wang, Rong Qu, Guoping Qiu</strong></p>
<p>Hybrid CNN-Transformer models are designed to combine the advantages of Convolutional Neural Networks (CNNs) and Transformers to efficiently model both local information and long-range dependencies. However, most research tends to focus on integrating the spatial features of CNNs and Transformers, while overlooking the critical importance of channel features. This is particularly significant for model performance in low-quality medical image segmentation. Effective channel feature extraction can significantly enhance the modelâ€™s ability to capture contextual information and improve its representation capabilities. To address this issue, we propose a hybrid CNN-Transformer model, CFFormer, and introduce two modules: the Cross Feature Channel Attention (CFCA) module and the X-Spatial Feature Fusion (XFF) module. The model incorporates dual encoders, with the CNN encoder focusing on capturing local features and the Transformer encoder modeling global features. The CFCA module filters and facilitates interactions between the channel features from the two encoders, while the XFF module effectively reduces the significant semantic information differences in spatial features, enabling a smooth and cohesive spatial feature fusion. We evaluate our model across eight datasets covering five modalities to test its generalization capability. Experimental results demonstrate that our model outperforms current state-of-the-art (SOTA) methods, with particularly superior performance on datasets characterized by blurry boundaries and low contrast. </p>
<blockquote>
<p>æ··åˆCNN-Transformeræ¨¡å‹æ—¨åœ¨ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒTransformerçš„ä¼˜åŠ¿ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹å±€éƒ¨ä¿¡æ¯å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶å€¾å‘äºå…³æ³¨CNNå’ŒTransformerçš„ç©ºé—´ç‰¹å¾çš„èåˆï¼Œè€Œå¿½ç•¥äº†é€šé“ç‰¹å¾çš„å…³é”®é‡è¦æ€§ã€‚è¿™å¯¹äºä½è´¨é‡åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ¨¡å‹æ€§èƒ½å°¤ä¸ºé‡è¦ã€‚æœ‰æ•ˆçš„é€šé“ç‰¹å¾æå–å¯ä»¥æ˜¾è‘—å¢å¼ºæ¨¡å‹æ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæé«˜å…¶è¡¨ç¤ºèƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆCNN-Transformeræ¨¡å‹CFFormerï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªæ¨¡å—ï¼šè·¨ç‰¹å¾é€šé“æ³¨æ„åŠ›ï¼ˆCFCAï¼‰æ¨¡å—å’ŒXç©ºé—´ç‰¹å¾èåˆï¼ˆXFFï¼‰æ¨¡å—ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŒç¼–ç å™¨ç»“æ„ï¼ŒCNNç¼–ç å™¨ä¸“æ³¨äºæ•è·å±€éƒ¨ç‰¹å¾ï¼Œè€ŒTransformerç¼–ç å™¨åˆ™å¯¹å…¨å±€ç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚CFCAæ¨¡å—è¿‡æ»¤å¹¶ä¿ƒè¿›ä¸¤ä¸ªç¼–ç å™¨ä¹‹é—´é€šé“ç‰¹å¾çš„äº¤äº’ï¼Œè€ŒXFFæ¨¡å—æœ‰æ•ˆåœ°å‡å°‘äº†ç©ºé—´ç‰¹å¾ä¸­çš„é‡å¤§è¯­ä¹‰ä¿¡æ¯å·®å¼‚ï¼Œå®ç°äº†å¹³æ»‘å’Œè¿è´¯çš„ç©ºé—´ç‰¹å¾èåˆã€‚æˆ‘ä»¬åœ¨å…«ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†äº”ç§æ¨¡æ€ï¼Œä»¥æµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆSOTAï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾¹ç•Œæ¨¡ç³Šã€å¯¹æ¯”åº¦ä½çš„æ•°æ®é›†ä¸Šè¡¨ç°æ›´ä¸ºä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03629v1">PDF</a> The article consists of 15 pages, including 10 figures and 7 tables.   The code will be made open-source once the article is accepted by the journal</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ··åˆCNN-Transformeræ¨¡å‹ï¼Œåä¸ºCFFormerï¼Œç”¨äºå¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥Cross Feature Channel Attentionï¼ˆCFCAï¼‰æ¨¡å—å’ŒX-Spatial Feature Fusionï¼ˆXFFï¼‰æ¨¡å—ï¼Œå®ç°äº†å¯¹CNNå’ŒTransformeråŒé‡ç¼–ç å™¨çš„æœ‰æ•ˆç»“åˆï¼Œèƒ½å¤ŸåŒæ—¶æ•æ‰å±€éƒ¨ç‰¹å¾å’Œå…¨å±€ç‰¹å¾ï¼Œå¹¶é€šè¿‡äº¤äº’å¢å¼ºé€šé“ç‰¹å¾æå–ï¼Œæé«˜æ¨¡å‹æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›å’Œå¯¹ä½è´¨é‡åŒ»å­¦å›¾åƒçš„è¡¨ç¤ºèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ¨¡ç³Šè¾¹ç•Œå’Œä½å¯¹æ¯”åº¦æ•°æ®é›†ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œä¼˜äºå½“å‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ··åˆCNN-Transformeræ¨¡å‹ç»“åˆäº†CNNå’ŒTransformerçš„ä¼˜ç‚¹ï¼Œèƒ½åŒæ—¶å»ºæ¨¡å±€éƒ¨ä¿¡æ¯å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šå…³æ³¨ç©ºé—´ç‰¹å¾çš„æ•´åˆï¼Œå¿½è§†äº†é€šé“ç‰¹å¾çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½è´¨é‡åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ã€‚</li>
<li>æå‡ºçš„CFFormeræ¨¡å‹é€šè¿‡å¼•å…¥CFCAå’ŒXFFæ¨¡å—è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>CFCAæ¨¡å—ä¿ƒè¿›CNNå’ŒTransformerç¼–ç å™¨ä¹‹é—´çš„é€šé“ç‰¹å¾äº¤äº’ã€‚</li>
<li>XFFæ¨¡å—æœ‰æ•ˆå‡å°‘ç©ºé—´ç‰¹å¾çš„è¯­ä¹‰ä¿¡æ¯å·®å¼‚ï¼Œå®ç°å¹³æ»‘ä¸€è‡´çš„ç©ºé—´ç‰¹å¾èåˆã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶è‰¯å¥½çš„é€šç”¨æ€§å’Œå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd7750d8f14aa99fc7a7daa1581edd5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40f127a64ad88301185cf774bf16d7dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a7606d6a0df29cc4676ec3c901ee1eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a930abbac693895ee96450687ec78cc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Bridged-Semantic-Alignment-for-Zero-shot-3D-Medical-Image-Diagnosis"><a href="#Bridged-Semantic-Alignment-for-Zero-shot-3D-Medical-Image-Diagnosis" class="headerlink" title="Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis"></a>Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis</h2><p><strong>Authors:Haoran Lai, Zihang Jiang, Qingsong Yao, Rongsheng Wang, Zhiyang He, Xiaodong Tao, Wei Wei, Weifu Lv, S. Kevin Zhou</strong></p>
<p>3D medical images such as Computed tomography (CT) are widely used in clinical practice, offering a great potential for automatic diagnosis. Supervised learning-based approaches have achieved significant progress but rely heavily on extensive manual annotations, limited by the availability of training data and the diversity of abnormality types. Vision-language alignment (VLA) offers a promising alternative by enabling zero-shot learning without additional annotations. However, we empirically discover that the visual and textural embeddings after alignment endeavors from existing VLA methods form two well-separated clusters, presenting a wide gap to be bridged. To bridge this gap, we propose a Bridged Semantic Alignment (BrgSA) framework. First, we utilize a large language model to perform semantic summarization of reports, extracting high-level semantic information. Second, we design a Cross-Modal Knowledge Interaction (CMKI) module that leverages a cross-modal knowledge bank as a semantic bridge, facilitating interaction between the two modalities, narrowing the gap, and improving their alignment. To comprehensively evaluate our method, we construct a benchmark dataset that includes 15 underrepresented abnormalities as well as utilize two existing benchmark datasets. Experimental results demonstrate that BrgSA achieves state-of-the-art performances on both public benchmark datasets and our custom-labeled dataset, with significant improvements in zero-shot diagnosis of underrepresented abnormalities. </p>
<blockquote>
<p>ä¸‰ç»´åŒ»å­¦å›¾åƒï¼Œå¦‚è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ï¼Œåœ¨ä¸´åºŠå®è·µä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä¸ºè‡ªåŠ¨è¯Šæ–­æä¾›äº†å·¨å¤§çš„æ½œåŠ›ã€‚åŸºäºç›‘ç£å­¦ä¹ çš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä¸¥é‡ä¾èµ–äºå¤§é‡çš„æ‰‹åŠ¨æ ‡æ³¨ï¼Œå—é™äºè®­ç»ƒæ•°æ®çš„å¯ç”¨æ€§å’Œå¼‚å¸¸ç±»å‹çš„å¤šæ ·æ€§ã€‚è§†è§‰è¯­è¨€å¯¹é½ï¼ˆVLAï¼‰æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå®ç°æ— éœ€é¢å¤–æ ‡æ³¨çš„é›¶æ ·æœ¬å­¦ä¹ ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œç°æœ‰VLAæ–¹æ³•å¯¹é½åçš„è§†è§‰å’Œçº¹ç†åµŒå…¥å½¢æˆä¸¤ä¸ªåˆ†ç¦»è‰¯å¥½çš„èšç±»ï¼Œå­˜åœ¨ä¸€ä¸ªè¾ƒå¤§çš„å·®è·éœ€è¦å¼¥åˆã€‚ä¸ºäº†å¼¥åˆè¿™ä¸ªå·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºBridged Semantic Alignmentï¼ˆBrgSAï¼‰çš„æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æŠ¥å‘Šè¿›è¡Œè¯­ä¹‰æ‘˜è¦ï¼Œæå–é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè·¨æ¨¡æ€çŸ¥è¯†äº¤äº’ï¼ˆCMKIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨è·¨æ¨¡æ€çŸ¥è¯†åº“ä½œä¸ºè¯­ä¹‰æ¡¥æ¢ï¼Œä¿ƒè¿›ä¸¤ç§æ¨¡æ€ä¹‹é—´çš„äº¤äº’ï¼Œç¼©å°å·®è·ï¼Œæé«˜å¯¹é½æ•ˆæœã€‚ä¸ºäº†å…¨é¢è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«15ç§ä»£è¡¨æ€§è¾ƒå·®çš„å¼‚å¸¸çš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨äº†ä¸¤ä¸ªç°æœ‰çš„åŸºå‡†æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBrgSAåœ¨å…¬å…±åŸºå‡†æ•°æ®é›†å’Œæˆ‘ä»¬è‡ªå®šä¹‰æ ‡è®°çš„æ•°æ®é›†ä¸Šéƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä»£è¡¨æ€§è¾ƒå·®å¼‚å¸¸çš„é›¶æ ·æœ¬è¯Šæ–­æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03565v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå¦‚CTåœ¨ä¸´åºŠå®è·µä¸­åº”ç”¨å¹¿æ³›ï¼Œè‡ªåŠ¨è¯Šæ–­æ½œåŠ›å·¨å¤§ã€‚åŸºäºç›‘ç£å­¦ä¹ çš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å—é™äºè®­ç»ƒæ•°æ®çš„å¯ç”¨æ€§å’Œå¼‚å¸¸ç±»å‹çš„å¤šæ ·æ€§ã€‚è§†è§‰è¯­è¨€å¯¹é½ï¼ˆVLAï¼‰ä¸ºå®ç°é›¶æ ·æœ¬å­¦ä¹ æä¾›äº†æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰çš„VLAæ–¹æ³•åœ¨è§†è§‰å’Œçº¹ç†åµŒå…¥å¯¹é½åå½¢æˆä¸¤ä¸ªåˆ†ç¦»æ˜æ˜¾çš„é›†ç¾¤ï¼Œå­˜åœ¨è¾ƒå¤§å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Bridged Semantic Alignmentï¼ˆBrgSAï¼‰æ¡†æ¶ã€‚é¦–å…ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æŠ¥å‘Šè¿›è¡Œè¯­ä¹‰æ‘˜è¦ï¼Œæå–é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†ä¸€ç§è·¨æ¨¡æ€çŸ¥è¯†äº¤äº’ï¼ˆCMKIï¼‰æ¨¡å—ï¼Œåˆ©ç”¨è·¨æ¨¡æ€çŸ¥è¯†åº“ä½œä¸ºè¯­ä¹‰æ¡¥æ¢ï¼Œä¿ƒè¿›ä¸¤ç§æ¨¡æ€çš„äº’åŠ¨ï¼Œç¼©å°å·®è·å¹¶æ”¹è¿›å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œä¸”åœ¨ä»£è¡¨æ€§ä¸è¶³çš„å¼‚å¸¸ç–¾ç—…çš„é›¶æ ·æœ¬è¯Šæ–­ä¸­å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒå¦‚CTåœ¨ä¸´åºŠå®è·µä¸­å¯¹è‡ªåŠ¨è¯Šæ–­æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>åŸºäºç›‘ç£å­¦ä¹ çš„æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒè¯Šæ–­ä¸­å—é™äºè®­ç»ƒæ•°æ®çš„å¯ç”¨æ€§å’Œå¼‚å¸¸ç±»å‹çš„å¤šæ ·æ€§ã€‚</li>
<li>è§†è§‰è¯­è¨€å¯¹é½ï¼ˆVLAï¼‰ä¸ºåŒ»å­¦å›¾åƒè‡ªåŠ¨è¯Šæ–­æä¾›äº†é›¶æ ·æœ¬å­¦ä¹ çš„å¯èƒ½æ€§ã€‚</li>
<li>ç°æœ‰çš„VLAæ–¹æ³•åœ¨è§†è§‰å’Œçº¹ç†åµŒå…¥å¯¹é½åå­˜åœ¨æ˜æ˜¾çš„å·®è·ã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†Bridged Semantic Alignmentï¼ˆBrgSAï¼‰æ¡†æ¶æ¥ç¼©å°è¿™ä¸€å·®è·ã€‚</li>
<li>BrgSAæ¡†æ¶åŒ…æ‹¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­ä¹‰æ‘˜è¦å’Œè·¨æ¨¡æ€çŸ¥è¯†äº¤äº’æ¨¡å—ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-357a1691a9f2be9039f5d1154196d836.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9a0ee0e3b9b9aa64cee20ffdcba63a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd8aebabcead4e900dbfb00ff552552d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfc6edd503d9c2a75252a5ff6a6edce4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Salient-Region-Matching-for-Fully-Automated-MR-TRUS-Registration"><a href="#Salient-Region-Matching-for-Fully-Automated-MR-TRUS-Registration" class="headerlink" title="Salient Region Matching for Fully Automated MR-TRUS Registration"></a>Salient Region Matching for Fully Automated MR-TRUS Registration</h2><p><strong>Authors:Zetian Feng, Dong Ni, Yi Wang</strong></p>
<p>Prostate cancer is a leading cause of cancer-related mortality in men. The registration of magnetic resonance (MR) and transrectal ultrasound (TRUS) can provide guidance for the targeted biopsy of prostate cancer. In this study, we propose a salient region matching framework for fully automated MR-TRUS registration. The framework consists of prostate segmentation, rigid alignment and deformable registration. Prostate segmentation is performed using two segmentation networks on MR and TRUS respectively, and the predicted salient regions are used for the rigid alignment. The rigidly-aligned MR and TRUS images serve as initialization for the deformable registration. The deformable registration network has a dual-stream encoder with cross-modal spatial attention modules to facilitate multi-modality feature learning, and a salient region matching loss to consider both structure and intensity similarity within the prostate region. Experiments on a public MR-TRUS dataset demonstrate that our method achieves satisfactory registration results, outperforming several cutting-edge methods. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/mock1ngbrd/salient-region-matching">https://github.com/mock1ngbrd/salient-region-matching</a>. </p>
<blockquote>
<p>å‰åˆ—è…ºç™Œæ˜¯ç”·æ€§ç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚ç£å…±æŒ¯ï¼ˆMRï¼‰å’Œç»ç›´è‚ è¶…å£°ï¼ˆTRUSï¼‰çš„æ³¨å†Œå¯ä»¥ä¸ºå‰åˆ—è…ºç™Œçš„é¶å‘æ´»æ£€æä¾›æŒ‡å¯¼ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå…¨è‡ªåŠ¨MR-TRUSæ³¨å†Œçš„æ˜¾è‘—åŒºåŸŸåŒ¹é…æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å‰åˆ—è…ºåˆ†æ®µã€åˆšæ€§å¯¹é½å’Œå¯å˜å½¢æ³¨å†Œã€‚å‰åˆ—è…ºåˆ†æ®µåˆ†åˆ«åœ¨MRå’ŒTRUSä¸Šé‡‡ç”¨ä¸¤ä¸ªåˆ†æ®µç½‘ç»œè¿›è¡Œï¼Œé¢„æµ‹çš„æ˜¾è‘—åŒºåŸŸç”¨äºåˆšæ€§å¯¹é½ã€‚åˆšæ€§å¯¹é½çš„MRå’ŒTRUSå›¾åƒä¸ºå¯å˜å½¢æ³¨å†Œæä¾›åˆå§‹åŒ–ã€‚å¯å˜å½¢æ³¨å†Œç½‘ç»œå…·æœ‰åŒæµç¼–ç å™¨ï¼Œé…å¤‡è·¨æ¨¡æ€ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ ï¼Œå¹¶è€ƒè™‘å‰åˆ—è…ºåŒºåŸŸå†…ç»“æ„å’Œå¼ºåº¦çš„ç›¸ä¼¼æ€§ï¼Œé‡‡ç”¨æ˜¾è‘—åŒºåŸŸåŒ¹é…æŸå¤±ã€‚åœ¨å…¬å…±MR-TRUSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†ä»¤äººæ»¡æ„çš„æ³¨å†Œç»“æœï¼Œä¼˜äºå‡ ç§å‰æ²¿æ–¹æ³•ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/mock1ngbrd/salient-region-matching">https://github.com/mock1ngbrd/salient-region-matching</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03510v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ˜¾è‘—åŒºåŸŸåŒ¹é…çš„MR-TRUSè‡ªåŠ¨åŒ–æ³¨å†Œæ¡†æ¶ï¼Œç”¨äºå‰åˆ—è…ºç™Œçš„é¶å‘æ´»æ£€ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å‰åˆ—è…ºåˆ†å‰²ã€åˆšæ€§å¯¹é½å’Œå¯å˜å½¢æ³¨å†Œã€‚é€šè¿‡MRå’ŒTRUSä¸Šçš„ä¸¤ä¸ªåˆ†å‰²ç½‘ç»œè¿›è¡Œå‰åˆ—è…ºåˆ†å‰²ï¼Œé¢„æµ‹æ˜¾è‘—åŒºåŸŸç”¨äºåˆšæ€§å¯¹é½ã€‚åˆšæ€§å¯¹é½çš„MRå’ŒTRUSå›¾åƒä¸ºå¯å˜å½¢æ³¨å†Œæä¾›åˆå§‹åŒ–ã€‚å¯å˜å½¢æ³¨å†Œç½‘ç»œå…·æœ‰å¸¦æœ‰è·¨æ¨¡æ€ç©ºé—´æ³¨æ„åŠ›æ¨¡å—çš„åŒé‡æµç¼–ç å™¨ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ ï¼Œå¹¶è€ƒè™‘å‰åˆ—è…ºåŒºåŸŸå†…çš„ç»“æ„å’Œå¼ºåº¦ç›¸ä¼¼æ€§ã€‚åœ¨å…¬å…±MR-TRUSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†ä»¤äººæ»¡æ„çš„æ³¨å†Œç»“æœï¼Œä¼˜äºä¸€äº›å‰æ²¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‰åˆ—è…ºç™Œæ˜¯ç”·æ€§ç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼ŒMR-TRUSæ³¨å†Œå¯¹äºé¶å‘æ´»æ£€å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ˜¾è‘—åŒºåŸŸåŒ¹é…çš„è‡ªåŠ¨åŒ–MR-TRUSæ³¨å†Œæ¡†æ¶ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬å‰åˆ—è…ºåˆ†å‰²ã€åˆšæ€§å¯¹é½å’Œå¯å˜å½¢æ³¨å†Œä¸‰ä¸ªä¸»è¦æ­¥éª¤ã€‚</li>
<li>ä½¿ç”¨ä¸¤ä¸ªåˆ†å‰²ç½‘ç»œåœ¨MRå’ŒTRUSä¸Šè¿›è¡Œå‰åˆ—è…ºåˆ†å‰²ï¼Œé¢„æµ‹æ˜¾è‘—åŒºåŸŸç”¨äºåˆšæ€§å¯¹é½ã€‚</li>
<li>åˆšæ€§å¯¹é½çš„MRå’ŒTRUSå›¾åƒä¸ºå¯å˜å½¢æ³¨å†Œæä¾›åˆå§‹åŒ–ã€‚</li>
<li>å¯å˜å½¢æ³¨å†Œç½‘ç»œå…·æœ‰åŒé‡æµç¼–ç å™¨å’Œè·¨æ¨¡æ€ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-34b1cb8a6c5e758783d493a3493e3f7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74d42ef6a64b3e4a31ed02ba63a73451.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8cabf1871d7e681ca61e842c2adac7e.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-09\./crop_åŒ»å­¦å›¾åƒ/2501.03510v1/page_3_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-405e4a779cff365192fa51a2246c0886.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a612378090783bd38685b6f32c41a8d6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="VOILA-Complexity-Aware-Universal-Segmentation-of-CT-images-by-Voxel-Interacting-with-Language"><a href="#VOILA-Complexity-Aware-Universal-Segmentation-of-CT-images-by-Voxel-Interacting-with-Language" class="headerlink" title="VOILA: Complexity-Aware Universal Segmentation of CT images by Voxel   Interacting with Language"></a>VOILA: Complexity-Aware Universal Segmentation of CT images by Voxel   Interacting with Language</h2><p><strong>Authors:Zishuo Wan, Yu Gao, Wanyuan Pang, Dawei Ding</strong></p>
<p>Satisfactory progress has been achieved recently in universal segmentation of CT images. Following the success of vision-language methods, there is a growing trend towards utilizing text prompts and contrastive learning to develop universal segmentation models. However, there exists a significant imbalance in information density between 3D images and text prompts. Moreover, the standard fully connected layer segmentation approach faces significant challenges in handling multiple classes and exhibits poor generalizability. To address these challenges, we propose the VOxel Interacting with LAnguage method (VOILA) for universal CT image segmentation. Initially, we align voxels and language into a shared representation space and classify voxels on the basis of cosine similarity. Subsequently, we develop the Voxel-Language Interaction framework to mitigate the impact of class imbalance caused by foreground-background discrepancies and variations in target volumes. Furthermore, a Complexity-Aware Sampling method is proposed to focus on region hard to segment, achieved by generating pseudo-heatmaps from a trainable Gaussian mixture distribution. Our results indicate the proposed VOILA is capable to achieve improved performance with reduced parameters and computational cost during training. Furthermore, it demonstrates significant generalizability across diverse datasets without additional fine-tuning. </p>
<blockquote>
<p>è¿‘æœŸè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒé€šç”¨åˆ†å‰²å–å¾—äº†ä»¤äººæ»¡æ„çš„è¿›å±•ã€‚éšç€è§†è§‰è¯­è¨€æ–¹æ³•çš„æˆåŠŸï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºå’Œå¯¹æ¯”å­¦ä¹ å¼€å‘é€šç”¨åˆ†å‰²æ¨¡å‹çš„è¶‹åŠ¿æ—¥ç›Šæ˜æ˜¾ã€‚ç„¶è€Œï¼Œ3Då›¾åƒå’Œæ–‡æœ¬æç¤ºä¹‹é—´å­˜åœ¨ä¿¡æ¯å¯†åº¦çš„ä¸å¹³è¡¡ç°è±¡ã€‚æ­¤å¤–ï¼Œæ ‡å‡†å…¨è¿æ¥å±‚åˆ†å‰²æ–¹æ³•åœ¨å¤„ç†å¤šç±»åˆ«æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå…¶æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†VOxelä¸LAnguageäº¤äº’æ–¹æ³•ï¼ˆVOILAï¼‰ç”¨äºé€šç”¨CTå›¾åƒåˆ†å‰²ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä½“ç´ å’Œè¯­è¨€å¯¹é½åˆ°ä¸€ä¸ªå…±äº«è¡¨ç¤ºç©ºé—´ï¼Œå¹¶æ ¹æ®ä½™å¼¦ç›¸ä¼¼æ€§å¯¹ä½“ç´ è¿›è¡Œåˆ†ç±»ã€‚éšåï¼Œæˆ‘ä»¬å¼€å‘äº†ä½“ç´ è¯­è¨€äº¤äº’æ¡†æ¶ï¼Œä»¥å‡è½»ç”±å‰æ™¯èƒŒæ™¯å·®å¼‚å’Œç›®æ ‡ä½“ç§¯å˜åŒ–å¼•èµ·çš„ç±»åˆ«ä¸å¹³è¡¡çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§å¤æ‚åº¦æ„ŸçŸ¥é‡‡æ ·æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆæ¥è‡ªå¯è®­ç»ƒé«˜æ–¯æ··åˆåˆ†å¸ƒçš„ä¼ªçƒ­å›¾æ¥é‡ç‚¹å…³æ³¨éš¾ä»¥åˆ†å‰²çš„åŒºåŸŸã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„VOILAæ–¹æ³•èƒ½å¤Ÿåœ¨å‡å°‘å‚æ•°å’Œè®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„ä¸€èˆ¬æ€§ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03482v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸCTå›¾åƒé€šç”¨åˆ†å‰²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œè¶‹åŠ¿æ˜¯ç»“åˆè§†è§‰å’Œè¯­è¨€æ–¹æ³•ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºå’Œå¯¹æ¯”å­¦ä¹ å¼€å‘é€šç”¨åˆ†å‰²æ¨¡å‹ã€‚ä½†å­˜åœ¨å›¾åƒä¸æ–‡æœ¬æç¤ºä¿¡æ¯å¯†åº¦ä¸å¹³è¡¡é—®é¢˜ï¼Œä¸”å…¨è¿æ¥å±‚åˆ†å‰²æ–¹æ³•åœ¨å¤„ç†å¤šç±»åˆ«æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œé€šç”¨æ€§è¾ƒå·®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºVOxelä¸è¯­è¨€äº¤äº’ï¼ˆVOILAï¼‰æ–¹æ³•ï¼Œé€šè¿‡å…±äº«è¡¨ç¤ºç©ºé—´åˆ†ç±»ä½“ç´ ã€å»ºç«‹ä½“ç´ è¯­è¨€äº¤äº’æ¡†æ¶ã€ä½¿ç”¨å¤æ‚åº¦æ„ŸçŸ¥é‡‡æ ·ç­‰æ–¹æ³•è§£å†³æŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVOILAèƒ½æé«˜æ€§èƒ½ã€å‡å°‘å‚æ•°å’Œè®¡ç®—æˆæœ¬ï¼Œå¹¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¿‘æœŸCTå›¾åƒé€šç”¨åˆ†å‰²è¿›å±•æ˜¾è‘—ï¼Œç»“åˆè§†è§‰å’Œè¯­è¨€æ–¹æ³•æˆä¸ºè¶‹åŠ¿ã€‚</li>
<li>æ–‡æœ¬æç¤ºå’Œå¯¹æ¯”å­¦ä¹ åœ¨å¼€å‘é€šç”¨åˆ†å‰²æ¨¡å‹ä¸­çš„åº”ç”¨é€æ¸å¢å¤šã€‚</li>
<li>å­˜åœ¨å›¾åƒä¸æ–‡æœ¬æç¤ºä¿¡æ¯å¯†åº¦ä¸å¹³è¡¡çš„é—®é¢˜ã€‚</li>
<li>å…¨è¿æ¥å±‚åˆ†å‰²æ–¹æ³•å¤„ç†å¤šç±»åˆ«æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œé€šç”¨æ€§è¾ƒå·®ã€‚</li>
<li>VOILAæ–¹æ³•é€šè¿‡å…±äº«è¡¨ç¤ºç©ºé—´åˆ†ç±»ä½“ç´ ï¼Œå»ºç«‹ä½“ç´ ä¸è¯­è¨€çš„äº¤äº’æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>VOILAé‡‡ç”¨å¤æ‚åº¦æ„ŸçŸ¥é‡‡æ ·ï¼Œå…³æ³¨éš¾ä»¥åˆ†å‰²çš„åŒºåŸŸã€‚</li>
<li>VOILAèƒ½æé«˜æ€§èƒ½ï¼Œå‡å°‘è®­ç»ƒå‚æ•°å’Œè®¡ç®—æˆæœ¬ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e92d6bafcb55ec935c882b0a77b70aaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7061b9b93561cb52980f0b5bb7ccad22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a740adec1b59a244fc851604ab441335.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31bdd2645db00505fd506b343d3a2ae1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f687b5adcf0b3da8b3ab1c6156549a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7df13253cfef0b59fa8a8c4499e52479.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DGSSA-Domain-generalization-with-structural-and-stylistic-augmentation-for-retinal-vessel-segmentation"><a href="#DGSSA-Domain-generalization-with-structural-and-stylistic-augmentation-for-retinal-vessel-segmentation" class="headerlink" title="DGSSA: Domain generalization with structural and stylistic augmentation   for retinal vessel segmentation"></a>DGSSA: Domain generalization with structural and stylistic augmentation   for retinal vessel segmentation</h2><p><strong>Authors:Bo Liu, Yudong Zhang, Shuihua Wang, Siyue Li, Jin Hong</strong></p>
<p>Retinal vascular morphology is crucial for diagnosing diseases such as diabetes, glaucoma, and hypertension, making accurate segmentation of retinal vessels essential for early intervention. Traditional segmentation methods assume that training and testing data share similar distributions, which can lead to poor performance on unseen domains due to domain shifts caused by variations in imaging devices and patient demographics. This paper presents a novel approach, DGSSA, for retinal vessel image segmentation that enhances model generalization by combining structural and style augmentation strategies. We utilize a space colonization algorithm to generate diverse vascular-like structures that closely mimic actual retinal vessels, which are then used to generate pseudo-retinal images with an improved Pix2Pix model, allowing the segmentation model to learn a broader range of structure distributions. Additionally, we utilize PixMix to implement random photometric augmentations and introduce uncertainty perturbations, thereby enriching stylistic diversity and significantly enhancing the modelâ€™s adaptability to varying imaging conditions. Our framework has been rigorously evaluated on four challenging datasets-DRIVE, CHASEDB, HRF, and STARE-demonstrating state-of-the-art performance that surpasses existing methods. This validates the effectiveness of our proposed approach, highlighting its potential for clinical application in automated retinal vessel analysis. </p>
<blockquote>
<p>è§†ç½‘è†œè¡€ç®¡å½¢æ€å¯¹äºè¯Šæ–­ç³–å°¿ç—…ã€é’å…‰çœ¼å’Œé«˜è¡€å‹ç­‰ç–¾ç—…è‡³å…³é‡è¦ï¼Œå› æ­¤ï¼Œå¯¹è§†ç½‘è†œè¡€ç®¡è¿›è¡Œå‡†ç¡®åˆ†å‰²å¯¹äºæ—©æœŸå¹²é¢„è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„åˆ†å‰²æ–¹æ³•å‡è®¾è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å…·æœ‰ç›¸ä¼¼çš„åˆ†å¸ƒï¼Œä½†ç”±äºæˆåƒè®¾å¤‡å’Œæ‚£è€…äººå£ç»Ÿè®¡ä¿¡æ¯çš„å·®å¼‚å¯¼è‡´çš„åŸŸåç§»ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨æœªè§è¿‡çš„åŸŸä¸Šè¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè§†ç½‘è†œè¡€ç®¡å›¾åƒåˆ†å‰²çš„æ–°æ–¹æ³•DGSSAï¼Œé€šè¿‡ç»“åˆç»“æ„å’Œé£æ ¼å¢å¼ºç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ©ç”¨ç©ºé—´æ®–æ°‘åŒ–ç®—æ³•ç”Ÿæˆå¤šæ ·åŒ–çš„è¡€ç®¡çŠ¶ç»“æ„ï¼Œè¿™äº›ç»“æ„ç´§å¯†æ¨¡ä»¿å®é™…çš„è§†ç½‘è†œè¡€ç®¡ï¼Œç„¶åç”¨äºç”Ÿæˆæ”¹è¿›çš„Pix2Pixæ¨¡å‹çš„ä¼ªè§†ç½‘è†œå›¾åƒï¼Œä»è€Œä½¿åˆ†å‰²æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¹¿æ³›çš„ç»“æ„åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨PixMixå®ç°éšæœºå…‰åº¦å¢å¼ºå¹¶å¼•å…¥ä¸ç¡®å®šæ€§æ‰°åŠ¨ï¼Œä»è€Œä¸°å¯Œäº†é£æ ¼å¤šæ ·æ€§ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹ä¸åŒæˆåƒæ¡ä»¶çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼ˆDRIVEã€CHASEDBã€HRFå’ŒSTAREï¼‰ä¸Šè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œè¡¨ç°å‡ºäº†è¶…è¶Šç°æœ‰æ–¹æ³•çš„æœ€æ–°æ€§èƒ½ï¼Œè¿™éªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶çªå‡ºäº†å…¶åœ¨ä¸´åºŠè‡ªåŠ¨è§†ç½‘è†œè¡€ç®¡åˆ†æä¸­çš„æ½œåœ¨åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03466v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è§†ç½‘è†œè¡€ç®¡å›¾åƒåˆ†å‰²æ–¹æ³•DGSSAï¼Œé€šè¿‡ç»“åˆç»“æ„å’Œé£æ ¼å¢å¼ºç­–ç•¥æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç©ºé—´æ®–æ°‘ç®—æ³•ç”Ÿæˆæ¨¡æ‹Ÿè§†ç½‘è†œè¡€ç®¡çš„è¡€ç®¡çŠ¶ç»“æ„ï¼Œå†é€šè¿‡æ”¹è¿›çš„Pix2Pixæ¨¡å‹ç”Ÿæˆä¼ªè§†ç½‘è†œå›¾åƒã€‚åŒæ—¶ï¼Œå¼•å…¥PixMixå®ç°éšæœºå…‰åº¦å¢å¼ºå’Œä¸ç¡®å®šæ€§æ‰°åŠ¨ï¼Œä»è€Œä¸°å¯Œäº†é£æ ¼å¤šæ ·æ€§ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹ä¸åŒæˆåƒæ¡ä»¶çš„é€‚åº”èƒ½åŠ›ã€‚åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½å“è¶Šï¼Œè¶…è¿‡ç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶ä¸´åºŠåº”ç”¨äºè‡ªåŠ¨åŒ–è§†ç½‘è†œè¡€ç®¡åˆ†æçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†ç½‘è†œè¡€ç®¡å½¢æ€å¯¹äºè¯Šæ–­ç³–å°¿ç—…ã€é’å…‰çœ¼å’Œé«˜è¡€å‹ç­‰ç–¾ç—…è‡³å…³é‡è¦ï¼Œå› æ­¤å‡†ç¡®çš„è§†ç½‘è†œè¡€ç®¡åˆ†å‰²å¯¹äºæ—©æœŸå¹²é¢„è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿåˆ†å‰²æ–¹æ³•å‡è®¾è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å…·æœ‰ç›¸ä¼¼çš„åˆ†å¸ƒï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨ç”±äºæˆåƒè®¾å¤‡å’Œæ‚£è€…äººå£ç»Ÿè®¡å­¦ç‰¹å¾å˜åŒ–è€Œå¼•èµ·çš„æœªè§é¢†åŸŸä¸Šè¡¨ç°ä¸ä½³ã€‚</li>
<li>DGSSAæ–¹æ³•é€šè¿‡ç»“åˆç»“æ„å’Œé£æ ¼å¢å¼ºç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹åœ¨è§†ç½‘è†œè¡€ç®¡å›¾åƒåˆ†å‰²ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨ç©ºé—´æ®–æ°‘ç®—æ³•ç”Ÿæˆæ¨¡æ‹Ÿè§†ç½‘è†œè¡€ç®¡çš„è¡€ç®¡çŠ¶ç»“æ„ï¼Œç„¶åä½¿ç”¨æ”¹è¿›çš„Pix2Pixæ¨¡å‹ç”Ÿæˆä¼ªè§†ç½‘è†œå›¾åƒã€‚</li>
<li>é€šè¿‡PixMixå®æ–½éšæœºå…‰åº¦å¢å¼ºå’Œä¸ç¡®å®šæ€§æ‰°åŠ¨ï¼Œä»¥ä¸°å¯Œé£æ ¼å¤šæ ·æ€§å’Œæé«˜æ¨¡å‹å¯¹ä¸åŒæˆåƒæ¡ä»¶çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03466">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2cf3b56ddd1119471e5f0ffa626b546f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20d7f8ca996e567fde6145c7558cae2d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Activating-Associative-Disease-Aware-Vision-Token-Memory-for-LLM-Based-X-ray-Report-Generation"><a href="#Activating-Associative-Disease-Aware-Vision-Token-Memory-for-LLM-Based-X-ray-Report-Generation" class="headerlink" title="Activating Associative Disease-Aware Vision Token Memory for LLM-Based   X-ray Report Generation"></a>Activating Associative Disease-Aware Vision Token Memory for LLM-Based   X-ray Report Generation</h2><p><strong>Authors:Xiao Wang, Fuling Wang, Haowen Wang, Bo Jiang, Chuanfu Li, Yaowei Wang, Yonghong Tian, Jin Tang</strong></p>
<p>X-ray image based medical report generation achieves significant progress in recent years with the help of the large language model, however, these models have not fully exploited the effective information in visual image regions, resulting in reports that are linguistically sound but insufficient in describing key diseases. In this paper, we propose a novel associative memory-enhanced X-ray report generation model that effectively mimics the process of professional doctors writing medical reports. It considers both the mining of global and local visual information and associates historical report information to better complete the writing of the current report. Specifically, given an X-ray image, we first utilize a classification model along with its activation maps to accomplish the mining of visual regions highly associated with diseases and the learning of disease query tokens. Then, we employ a visual Hopfield network to establish memory associations for disease-related tokens, and a report Hopfield network to retrieve report memory information. This process facilitates the generation of high-quality reports based on a large language model and achieves state-of-the-art performance on multiple benchmark datasets, including the IU X-ray, MIMIC-CXR, and Chexpert Plus. The source code of this work is released on \url{<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/Medical_Image_Analysis%7D">https://github.com/Event-AHU/Medical_Image_Analysis}</a>. </p>
<blockquote>
<p>åŸºäºXå…‰å›¾åƒçš„åŒ»å­¦æŠ¥å‘Šç”Ÿæˆè¿‘å¹´æ¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¸®åŠ©ä¸‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å°šæœªå……åˆ†åˆ©ç”¨å›¾åƒåŒºåŸŸä¸­çš„æœ‰æ•ˆä¿¡æ¯ï¼Œå¯¼è‡´ç”Ÿæˆçš„æŠ¥å‘Šè™½ç„¶åœ¨è¯­è¨€ä¸Šé€šé¡ºï¼Œä½†åœ¨æè¿°å…³é”®ç–¾ç—…æ–¹é¢å´ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å…³è”è®°å¿†å¢å¼ºå‹Xå…‰æŠ¥å‘Šç”Ÿæˆæ¨¡å‹ï¼Œæœ‰æ•ˆæ¨¡æ‹Ÿäº†ä¸“ä¸šåŒ»ç”Ÿæ’°å†™åŒ»å­¦æŠ¥å‘Šçš„è¿‡ç¨‹ã€‚å®ƒå…¼é¡¾å…¨å±€å’Œå±€éƒ¨è§†è§‰ä¿¡æ¯çš„æŒ–æ˜ï¼Œå¹¶å…³è”å†å²æŠ¥å‘Šä¿¡æ¯ä»¥æ›´å¥½åœ°å®Œæˆå½“å‰æŠ¥å‘Šçš„æ’°å†™ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€å¼ Xå…‰ç‰‡ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨åˆ†ç±»æ¨¡å‹åŠå…¶æ¿€æ´»å›¾æ¥å®Œæˆä¸ç–¾ç—…é«˜åº¦ç›¸å…³çš„è§†è§‰åŒºåŸŸçš„æŒ–æ˜å’Œå­¦ä¹ ç–¾ç—…æŸ¥è¯¢ä»¤ç‰Œã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨è§†è§‰Hopfieldç½‘ç»œå»ºç«‹ä¸ç–¾ç—…ç›¸å…³çš„ä»¤ç‰Œçš„å†…å­˜å…³è”ï¼Œå¹¶é‡‡ç”¨æŠ¥å‘ŠHopfieldç½‘ç»œæ¥æ£€ç´¢æŠ¥å‘Šå†…å­˜ä¿¡æ¯ã€‚æ­¤è¿‡ç¨‹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Šï¼Œå¹¶åœ¨IU Xå…‰ã€MIMIC-CXRå’ŒChexpert Plusç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥å·¥ä½œçš„æºä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/Medical_Image_Analysis%E4%B8%8A%E3%80%82">https://github.com/Event-AHU/Medical_Image_Analysisä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03458v1">PDF</a> In Peer Review</p>
<p><strong>Summary</strong><br>     æå‡ºäº†ä¸€ç§æ–°å‹å…³è”è®°å¿†å¢å¼ºçš„Xå…‰æŠ¥å‘Šç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æœ‰æ•ˆæ¨¡æ‹Ÿä¸“ä¸šåŒ»ç”Ÿæ’°å†™åŒ»ç–—æŠ¥å‘Šçš„è¿‡ç¨‹ï¼ŒåŒæ—¶æŒ–æ˜å…¨å±€å’Œå±€éƒ¨è§†è§‰ä¿¡æ¯ï¼Œå¹¶å…³è”å†å²æŠ¥å‘Šä¿¡æ¯ï¼Œä»¥å®Œæˆå½“å‰æŠ¥å‘Šçš„æ’°å†™ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å“è¶Šï¼ŒåŒ…æ‹¬IU X-rayã€MIMIC-CXRå’ŒChexpert Plusã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å…³è”è®°å¿†å¢å¼ºXå…‰æŠ¥å‘Šç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æé«˜åŒ»ç–—æŠ¥å‘Šçš„è´¨é‡ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†å…¨å±€å’Œå±€éƒ¨è§†è§‰ä¿¡æ¯çš„æŒ–æ˜ã€‚</li>
<li>æ¨¡å‹é€šè¿‡å…³è”å†å²æŠ¥å‘Šä¿¡æ¯ï¼Œæ¨¡æ‹Ÿäº†ä¸“ä¸šåŒ»ç”Ÿæ’°å†™æŠ¥å‘Šçš„è¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬IU X-rayã€MIMIC-CXRå’ŒChexpert Plusï¼Œå¹¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é€šè¿‡åˆ†ç±»æ¨¡å‹å’Œæ¿€æ´»å›¾æ¥å®Œæˆä¸ç–¾ç—…é«˜åº¦ç›¸å…³çš„è§†è§‰åŒºåŸŸçš„æŒ–æ˜å’Œå­¦ä¹ ç–¾ç—…æŸ¥è¯¢ä»¤ç‰Œã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨è§†è§‰Hopfieldç½‘ç»œä¸ºç–¾ç—…ç›¸å…³ä»¤ç‰Œå»ºç«‹å†…å­˜å…³è”ï¼Œå¹¶é‡‡ç”¨æŠ¥å‘ŠHopfieldç½‘ç»œæ£€ç´¢æŠ¥å‘Šå†…å­˜ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4a2fa54b74c4cd33712be23d6bae491c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5edc49f2d35847e3e20360e442883c92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-247934025ec5667988753034e2e28bee.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ScaleMAI-Accelerating-the-Development-of-Trusted-Datasets-and-AI-Models"><a href="#ScaleMAI-Accelerating-the-Development-of-Trusted-Datasets-and-AI-Models" class="headerlink" title="ScaleMAI: Accelerating the Development of Trusted Datasets and AI Models"></a>ScaleMAI: Accelerating the Development of Trusted Datasets and AI Models</h2><p><strong>Authors:Wenxuan Li, Pedro R. A. S. Bassi, Tianyu Lin, Yu-Cheng Chou, Xinze Zhou, Yucheng Tang, Fabian Isensee, Kang Wang, Qi Chen, Xiaowei Xu, Xiaoxi Chen, Lizhou Wu, Qilong Wu, Yannick Kirchhoff, Maximilian Rokuss, Saikat Roy, Yuxuan Zhao, Dexin Yu, Kai Ding, Constantin Ulrich, Klaus Maier-Hein, Yang Yang, Alan L. Yuille, Zongwei Zhou</strong></p>
<p>Building trusted datasets is critical for transparent and responsible Medical AI (MAI) research, but creating even small, high-quality datasets can take years of effort from multidisciplinary teams. This process often delays AI benefits, as human-centric data creation and AI-centric model development are treated as separate, sequential steps. To overcome this, we propose ScaleMAI, an agent of AI-integrated data curation and annotation, allowing data quality and AI performance to improve in a self-reinforcing cycle and reducing development time from years to months. We adopt pancreatic tumor detection as an example. First, ScaleMAI progressively creates a dataset of 25,362 CT scans, including per-voxel annotations for benign&#x2F;malignant tumors and 24 anatomical structures. Second, through progressive human-in-the-loop iterations, ScaleMAI provides Flagship AI Model that can approach the proficiency of expert annotators (30-year experience) in detecting pancreatic tumors. Flagship Model significantly outperforms models developed from smaller, fixed-quality datasets, with substantial gains in tumor detection (+14%), segmentation (+5%), and classification (72%) on three prestigious benchmarks. In summary, ScaleMAI transforms the speed, scale, and reliability of medical dataset creation, paving the way for a variety of impactful, data-driven applications. </p>
<blockquote>
<p>æ„å»ºå¯ä¿¡èµ–çš„æ•°æ®é›†å¯¹äºé€æ˜å’Œè´Ÿè´£ä»»çš„åŒ»ç–—äººå·¥æ™ºèƒ½ï¼ˆMAIï¼‰ç ”ç©¶è‡³å…³é‡è¦ã€‚ä½†æ˜¯ï¼Œå³ä½¿æ˜¯å°è§„æ¨¡çš„é«˜è´¨é‡æ•°æ®é›†ä¹Ÿéœ€è¦å¤šå­¦ç§‘å›¢é˜Ÿæ•°å¹´çš„åŠªåŠ›ã€‚è¿™ä¸€è¿‡ç¨‹å¾€å¾€å»¶ç¼“äº†äººå·¥æ™ºèƒ½çš„ä¼˜åŠ¿ï¼Œå› ä¸ºä»¥äººç±»ä¸ºä¸­å¿ƒçš„æ•°æ®åˆ›å»ºå’Œä»¥äººå·¥æ™ºèƒ½ä¸ºä¸­å¿ƒçš„æ¨¡å‹å¼€å‘è¢«è§†ä¸ºç‹¬ç«‹ä¸”è¿ç»­çš„å‡ ä¸ªæ­¥éª¤ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ScaleMAIï¼Œè¿™æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½é›†æˆæ•°æ®æ•´ç†å’Œæ³¨é‡Šçš„ä»£ç†ï¼Œå…è®¸æ•°æ®è´¨é‡å’Œäººå·¥æ™ºèƒ½æ€§èƒ½åœ¨ä¸€ä¸ªè‡ªæˆ‘åŠ å¼ºçš„å¾ªç¯ä¸­ä¸æ–­æé«˜ï¼Œå¹¶å°†å¼€å‘æ—¶é—´ä»æ•°å¹´ç¼©çŸ­åˆ°æ•°æœˆã€‚æˆ‘ä»¬ä»¥èƒ°è…ºè‚¿ç˜¤æ£€æµ‹ä¸ºä¾‹ã€‚é¦–å…ˆï¼ŒScaleMAIé€æ­¥åˆ›å»ºä¸€ä¸ªåŒ…å«25362æ¬¡CTæ‰«æçš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ¯ä¸ªä½“ç´ çš„è‰¯æ€§&#x2F;æ¶æ€§è‚¿ç˜¤å’Œ24ä¸ªè§£å‰–ç»“æ„çš„æ³¨é‡Šã€‚å…¶æ¬¡ï¼Œé€šè¿‡é€æ­¥çš„äººç±»åœ¨å¾ªç¯ä¸­çš„è¿­ä»£ï¼ŒScaleMAIæä¾›äº†æ——èˆ°äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ¥è¿‘å…·æœ‰30å¹´ç»éªŒçš„ä¸“å®¶æ³¨é‡Šè€…åœ¨æ£€æµ‹èƒ°è…ºè‚¿ç˜¤æ–¹é¢çš„ç†Ÿç»ƒç¨‹åº¦ã€‚æ——èˆ°æ¨¡å‹åœ¨ä¸‰ä¸ªæƒå¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„è‚¿ç˜¤æ£€æµ‹ä¼˜åŠ¿ï¼ˆ+ 14%ï¼‰ï¼Œåˆ†å‰²ï¼ˆ+ 5%ï¼‰å’Œåˆ†ç±»ï¼ˆå‡†ç¡®ç‡æé«˜äº†72%ï¼‰ï¼Œå…¶æ€§èƒ½è¿œä¼˜äºä»å°è§„æ¨¡å›ºå®šè´¨é‡æ•°æ®é›†ä¸­å¼€å‘çš„æ¨¡å‹ã€‚æ€»è€Œè¨€ä¹‹ï¼ŒScaleMAIè½¬å˜äº†åŒ»ç–—æ•°æ®é›†åˆ›å»ºçš„é€Ÿåº¦ã€è§„æ¨¡å’Œå¯é æ€§ï¼Œä¸ºå„ç§æœ‰å½±å“åŠ›çš„æ•°æ®é©±åŠ¨åº”ç”¨ç¨‹åºé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03410v1">PDF</a> </p>
<p><strong>Summary</strong><br>    ScaleMAIé€šè¿‡é›†æˆäººå·¥æ™ºèƒ½è¿›è¡Œæ•°æ®æ•´ç†å’Œæ ‡æ³¨ï¼Œæé«˜äº†åŒ»ç–—æ•°æ®é›†çš„è´¨é‡ä¸åˆ›å»ºé€Ÿåº¦ï¼Œå®ç°äº†ä»å¤šå¹´åˆ°æ•°æœˆçš„é£è·ƒã€‚ä»¥èƒ°è…ºç™Œæ£€æµ‹ä¸ºä¾‹ï¼ŒScaleMAIåˆ›å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†å’Œæ——èˆ°AIæ¨¡å‹æ˜¾è‘—æé«˜äº†è‚¿ç˜¤æ£€æµ‹çš„å‡†ç¡®æ€§ã€åˆ†å‰²çš„ç²¾ç»†åº¦å’Œåˆ†ç±»çš„å¯é æ€§ã€‚è¿™ä¸€åˆ›æ–°ä¸ºåŒ»ç–—é¢†åŸŸçš„æ•°æ®é©±åŠ¨åº”ç”¨å¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ScaleMAIæ˜¯ä¸€ä¸ªé›†æˆäººå·¥æ™ºèƒ½çš„æ•°æ®æ•´ç†å’Œæ ‡æ³¨å·¥å…·ï¼Œèƒ½æ˜¾è‘—æé«˜åŒ»ç–—æ•°æ®é›†çš„è´¨é‡å’Œåˆ›å»ºé€Ÿåº¦ã€‚</li>
<li>é€šè¿‡ScaleMAIï¼Œæ•°æ®é›†åˆ›å»ºæ—¶é—´ä»å¤šå¹´ç¼©çŸ­åˆ°æ•°æœˆã€‚</li>
<li>ä»¥èƒ°è…ºç™Œæ£€æµ‹ä¸ºä¾‹ï¼ŒScaleMAIåˆ›å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†åŒ…å«æ¯ä½“ç´ è‰¯æ€§&#x2F;æ¶æ€§è‚¿ç˜¤å’Œ24ä¸ªè§£å‰–ç»“æ„çš„æ³¨é‡Šã€‚</li>
<li>ScaleMAIæä¾›æ——èˆ°AIæ¨¡å‹ï¼Œåœ¨èƒ°è…ºç™Œæ£€æµ‹æ–¹é¢æ¥è¿‘æ‹¥æœ‰30å¹´ç»éªŒçš„ä¸“å®¶æ ‡æ³¨è€…çš„ä¸“ä¸šæ°´å¹³ã€‚</li>
<li>æ——èˆ°AIæ¨¡å‹åœ¨ä¸‰ä¸ªé‡è¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œè‚¿ç˜¤æ£€æµ‹å‡†ç¡®ç‡æé«˜14%ï¼Œåˆ†å‰²ç²¾ç»†åº¦æé«˜5%ï¼Œåˆ†ç±»å¯é æ€§è¾¾åˆ°72%ã€‚</li>
<li>ScaleMAIçš„å¼•å…¥æ”¹å˜äº†åŒ»ç–—æ•°æ®é›†åˆ›å»ºçš„é€Ÿåº¦ã€è§„æ¨¡å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e57e85cbfd70ff245597f3032ab78584.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9480f7ad15582492aa0be5096b0920d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48e1babdb53da7e56fd6d276fab3da66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f3da00d64392316de4f19541fe53014.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bd0b685cb2f1073f33d1dac6993728d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Plant-Leaf-Disease-Detection-and-Classification-Using-Deep-Learning-A-Review-and-A-Proposed-System-on-Bangladeshâ€™s-Perspective"><a href="#Plant-Leaf-Disease-Detection-and-Classification-Using-Deep-Learning-A-Review-and-A-Proposed-System-on-Bangladeshâ€™s-Perspective" class="headerlink" title="Plant Leaf Disease Detection and Classification Using Deep Learning: A   Review and A Proposed System on Bangladeshâ€™s Perspective"></a>Plant Leaf Disease Detection and Classification Using Deep Learning: A   Review and A Proposed System on Bangladeshâ€™s Perspective</h2><p><strong>Authors:Md. Jalal Uddin Chowdhury, Zumana Islam Mou, Rezwana Afrin, Shafkat Kibria</strong></p>
<p>A very crucial part of Bangladeshi peopleâ€™s employment, GDP contribution, and mainly livelihood is agriculture. It plays a vital role in decreasing poverty and ensuring food security. Plant diseases are a serious stumbling block in agricultural production in Bangladesh. At times, humans canâ€™t detect the disease from an infected leaf with the naked eye. Using inorganic chemicals or pesticides in plants when itâ€™s too late leads in vain most of the time, deposing all the previous labor. The deep-learning technique of leaf-based image classification, which has shown impressive results, can make the work of recognizing and classifying all diseases trouble-less and more precise. In this paper, weâ€™ve mainly proposed a better model for the detection of leaf diseases. Our proposed paper includes the collection of data on three different kinds of crops: bell peppers, tomatoes, and potatoes. For training and testing the proposed CNN model, the plant leaf disease dataset collected from Kaggle is used, which has 17,430 images. The images are labeled with 14 separate classes of damage. The developed CNN model performs efficiently and could successfully detect and classify the tested diseases. The proposed CNN model may have great potency in crop disease management. </p>
<blockquote>
<p>å†œä¸šåœ¨å­ŸåŠ æ‹‰å›½äººæ°‘çš„å°±ä¸šã€å›½å†…ç”Ÿäº§æ€»å€¼è´¡çŒ®ä»¥åŠä¸»è¦ç”Ÿè®¡ä¸­æ‰®æ¼”ç€éå¸¸é‡è¦çš„è§’è‰²ã€‚å®ƒå¯¹å‡å°‘è´«å›°å’Œç¡®ä¿ç²®é£Ÿå®‰å…¨èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ¤ç‰©ç–¾ç—…æ˜¯å­ŸåŠ æ‹‰å›½å†œä¸šç”Ÿäº§ä¸­çš„ä¸€ä¸ªä¸¥é‡éšœç¢ã€‚æœ‰æ—¶ï¼Œäººç±»æ— æ³•ç”¨è‚‰çœ¼ä»å—æ„ŸæŸ“çš„å¶ç‰‡ä¸Šæ£€æµ‹åˆ°ç–¾ç—…ã€‚åœ¨ç–¾ç—…å·²ç»ä¸¥é‡æ—¶æ‰ä½¿ç”¨æ— æœºåŒ–å­¦ç‰©è´¨æˆ–å†œè¯é€šå¸¸ä¼šå¯¼è‡´ä¹‹å‰æ‰€æœ‰çš„åŠªåŠ›ä»˜è¯¸ä¸œæµã€‚åŸºäºå¶ç‰‡å›¾åƒçš„æ·±åº¦å­¦ä¹ åˆ†ç±»æŠ€æœ¯å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œå¯ä»¥ä½¿è¯†åˆ«å’Œåˆ†ç±»æ‰€æœ‰ç–¾ç—…çš„å·¥ä½œæ›´åŠ æ— å¿§å’Œç²¾ç¡®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦æå‡ºäº†ä¸€ç§ç”¨äºæ£€æµ‹å¶ç‰‡ç–¾ç—…çš„æ›´å¥½æ¨¡å‹ã€‚æˆ‘ä»¬çš„è®ºæ–‡æ¶µç›–äº†ä¸‰ç§ä¸åŒä½œç‰©çš„æ•°æ®æ”¶é›†ï¼šç”œæ¤’ã€ç•ªèŒ„å’ŒåœŸè±†ã€‚ä¸ºäº†è®­ç»ƒå’Œæµ‹è¯•æ‰€æå‡ºçš„CNNæ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä»Kaggleæ”¶é›†çš„åŒ…å«17430å¼ å›¾åƒçš„æ¤ç‰©å¶ç‰‡ç–¾ç—…æ•°æ®é›†ï¼Œè¿™äº›å›¾åƒè¢«æ ‡è®°ä¸ºåŒ…å«æœ‰14ç§ä¸åŒçš„æŸå®³ç±»åˆ«ã€‚æ‰€å¼€å‘çš„CNNæ¨¡å‹è¿è¡Œæ•ˆç‡é«˜ï¼Œå¹¶èƒ½æˆåŠŸæ£€æµ‹å’Œåˆ†ç±»æµ‹è¯•ä¸­çš„ç–¾ç—…ã€‚æ‰€æå‡ºçš„CNNæ¨¡å‹åœ¨å†œä½œç‰©ç–¾ç—…ç®¡ç†ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03305v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å†œä½œç‰©å¶ç‰‡å›¾åƒåˆ†ç±»ä¸Šçš„åº”ç”¨å¯¹äºå­ŸåŠ æ‹‰å›½å†œä¸šé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œèƒ½æœ‰æ•ˆè¯†åˆ«ä¸åˆ†ç±»æ¤ç‰©ç—…å®³ï¼Œæé«˜å†œä¸šç”Ÿäº§æ•ˆç‡ï¼Œé™ä½æŸå¤±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†œä¸šåœ¨å­ŸåŠ æ‹‰å›½äººæ°‘çš„å°±ä¸šã€GDPè´¡çŒ®ä»¥åŠç”Ÿè®¡ä¸­å æ®é‡è¦åœ°ä½ï¼Œå¯¹äºå‡å°‘è´«å›°å’Œä¿éšœç²®é£Ÿå®‰å…¨è‡³å…³é‡è¦ã€‚</li>
<li>æ¤ç‰©ç—…å®³æ˜¯å­ŸåŠ æ‹‰å›½å†œä¸šç”Ÿäº§ä¸­çš„ä¸€å¤§éšœç¢ã€‚</li>
<li>æœ‰æ—¶äººç±»æ— æ³•ä»…å‡­è‚‰çœ¼ä»å—æ„ŸæŸ“çš„å¶ç‰‡ä¸­æ£€æµ‹å‡ºç—…å®³ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å¶ç‰‡å›¾åƒåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½æ›´ç²¾ç¡®ã€é«˜æ•ˆåœ°è¯†åˆ«ä¸åˆ†ç±»æ‰€æœ‰ç—…å®³ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å¶ç‰‡ç—…å®³æ£€æµ‹çš„æ”¹è¿›æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶äººå‘˜ä½¿ç”¨äº†ä¸‰ç§ä¸åŒä½œç‰©çš„æ•°æ®ï¼šè¾£æ¤’ã€ç•ªèŒ„å’ŒåœŸè±†ï¼Œå¹¶ä½¿ç”¨äº†æ¥è‡ªKaggleçš„æ¤ç‰©å¶ç‰‡ç—…å®³æ•°æ®é›†ï¼ŒåŒ…å«17,430å¼ å›¾åƒï¼Œåˆ†ä¸º14ä¸ªç‹¬ç«‹çš„æŸä¼¤ç±»åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03305">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-da68d9b3c5193a6ac3a012285f58a695.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0feceda4d42191ee8dc0e58a357de58f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Regulating-radiology-AI-medical-devices-that-evolve-in-their-lifecycle"><a href="#Regulating-radiology-AI-medical-devices-that-evolve-in-their-lifecycle" class="headerlink" title="Regulating radiology AI medical devices that evolve in their lifecycle"></a>Regulating radiology AI medical devices that evolve in their lifecycle</h2><p><strong>Authors:Camila GonzÃ¡lez, Moritz Fuchs, Daniel Pinto dos Santos, Philipp Matthies, Manuel Trenz, Maximilian GrÃ¼ning, Akshay Chaudhari, David B. Larson, Ahmed Othman, Moon Kim, Felix Nensa, Anirban Mukhopadhyay</strong></p>
<p>Over time, the distribution of medical image data drifts due to multiple factors, including shifts in patient demographics, acquisition devices, and disease manifestation. While human radiologists can extrapolate their knowledge to such changes, AI systems cannot. In fact, deep learning models are highly susceptible to even slight variations in image characteristics. Therefore, manufacturers must update their models with new data to ensure that they remain safe and effective. Until recently, conducting such model updates in the USA and European Union meant applying for re-approval. Given the time and monetary costs associated with these processes, updates were infrequent, and obsolete systems continued functioning for too long. During 2024, several developments in the regulatory frameworks of these regions have taken place that promise to streamline the process of rolling out model updates safely: The European Artificial Intelligence Act came into effect last August, and the Food and Drug Administration (FDA) released the final marketing submission recommendations for a Predetermined Change Control Plan (PCCP) in December. We give an overview of the requirements and objectives of recent regulatory efforts and summarize the building blocks needed for successfully deploying dynamic systems. At the center of these pieces of regulation - and as prerequisites for manufacturers to conduct model updates without re-approval - are the need to describe the data collection and re-training processes and to establish real-world quality monitoring mechanisms. </p>
<blockquote>
<p>éšç€æ—¶é—´çš„æ¨ç§»ï¼Œç”±äºæ‚£è€…äººå£ç»Ÿè®¡å­¦ã€é‡‡é›†è®¾å¤‡å’Œç–¾ç—…è¡¨ç°çš„å˜åŒ–ç­‰å¤šä¸ªå› ç´ ï¼ŒåŒ»å­¦å›¾åƒæ•°æ®çš„åˆ†å¸ƒä¼šå‘ç”Ÿå˜åŒ–ã€‚è™½ç„¶äººç±»æ”¾å°„ç§‘åŒ»ç”Ÿå¯ä»¥æ¨æ–­ä»–ä»¬çš„çŸ¥è¯†ä»¥é€‚åº”è¿™äº›å˜åŒ–ï¼Œä½†äººå·¥æ™ºèƒ½ç³»ç»Ÿå´æ— æ³•åšåˆ°ã€‚äº‹å®ä¸Šï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ç”šè‡³å¯¹å›¾åƒç‰¹å¾ä¸­çš„è½»å¾®å˜åŒ–éƒ½é«˜åº¦æ•æ„Ÿã€‚å› æ­¤ï¼Œåˆ¶é€ å•†å¿…é¡»ä½¿ç”¨æ–°æ•°æ®æ›´æ–°ä»–ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿å®ƒä»¬ä»ç„¶å®‰å…¨æœ‰æ•ˆã€‚ç›´åˆ°æœ€è¿‘ï¼Œåœ¨ç¾å›½å’Œæ¬§ç›Ÿè¿›è¡Œæ­¤ç±»æ¨¡å‹æ›´æ–°æ„å‘³ç€éœ€è¦é‡æ–°ç”³è¯·æ‰¹å‡†ã€‚ç”±äºè¿™äº›æµç¨‹æ¶‰åŠçš„æ—¶é—´å’Œé‡‘é’±æˆæœ¬ï¼Œæ›´æ–°å¹¶ä¸é¢‘ç¹ï¼Œä¸”è¿‡æ—¶çš„ç³»ç»Ÿç»§ç»­è¿è¡Œäº†å¤ªé•¿æ—¶é—´ã€‚åœ¨2024å¹´æœŸé—´ï¼Œè¿™äº›åœ°åŒºç›‘ç®¡æ¡†æ¶çš„è‹¥å¹²å‘å±•äº‹ä»¶æœ‰æœ›ç®€åŒ–å®‰å…¨æ¨å‡ºæ¨¡å‹æ›´æ–°çš„æµç¨‹ï¼šæ¬§æ´²ã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹å·²äºå»å¹´å…«æœˆç”Ÿæ•ˆï¼Œç¾å›½é£Ÿå“å’Œè¯ç‰©ç®¡ç†å±€ï¼ˆFDAï¼‰åœ¨åäºŒæœˆå‘å¸ƒäº†é¢„å®šçš„å˜æ›´æ§åˆ¶è®¡åˆ’ï¼ˆPCCPï¼‰çš„æœ€ç»ˆå¸‚åœºæ¨å¹¿å»ºè®®ã€‚æˆ‘ä»¬æ¦‚è¿°äº†è¿‘æœŸç›‘ç®¡å·¥ä½œçš„è¦æ±‚å’Œç›®æ ‡ï¼Œå¹¶æ€»ç»“äº†æˆåŠŸéƒ¨ç½²åŠ¨æ€ç³»ç»Ÿæ‰€éœ€çš„æ„å»ºæ¨¡å—ã€‚è¿™äº›æ³•è§„çš„æ ¸å¿ƒâ€”â€”ä»¥åŠåˆ¶é€ å•†è¿›è¡Œæ¨¡å‹æ›´æ–°è€Œä¸éœ€é‡æ–°æ‰¹å‡†çš„å…ˆå†³æ¡ä»¶â€”â€”æ˜¯éœ€è¦æè¿°æ•°æ®æ”¶é›†å’Œå†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å»ºç«‹ç°å®ä¸–ç•Œçš„è´¨é‡ç›‘æ§æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20498v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€åŒ»å­¦å›¾åƒæ•°æ®çš„åˆ†å¸ƒå› æ‚£è€…äººå£å˜åŒ–ã€é‡‡é›†è®¾å¤‡å’Œç–¾ç—…è¡¨ç°ç­‰å› ç´ è€Œé€æ¸æ¼‚ç§»ï¼Œäººå·¥æ™ºèƒ½ç³»ç»Ÿéš¾ä»¥é€‚åº”è¿™ç§å˜åŒ–ã€‚åˆ¶é€ å•†å¿…é¡»æ›´æ–°å…¶æ¨¡å‹ä»¥ç¡®ä¿å…¶å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚è¿‘æœŸç¾å›½å’Œæ¬§ç›Ÿçš„æ³•è§„å‘å±•ç®€åŒ–äº†æ¨¡å‹æ›´æ–°çš„æµç¨‹ã€‚è¿™äº›æ³•è§„è¦æ±‚æè¿°æ•°æ®é‡‡é›†å’Œå†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å»ºç«‹äº†ç°å®ä¸–ç•Œçš„è´¨é‡ç›‘æ§æœºåˆ¶ï¼Œä¸ºåˆ¶é€ å•†è¿›è¡Œæ— éœ€é‡æ–°å®¡æ‰¹çš„æ¨¡å‹æ›´æ–°æä¾›äº†å‰æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ•°æ®çš„åˆ†å¸ƒå› å¤šç§å› ç´ è€Œå‘ç”Ÿå˜åŒ–ï¼Œè¿™åŒ…æ‹¬æ‚£è€…äººå£å˜åŒ–ã€é‡‡é›†è®¾å¤‡å’Œç–¾ç—…è¡¨ç°ç­‰ã€‚</li>
<li>ä¸äººç±»æ”¾å°„ç§‘åŒ»ç”Ÿç›¸æ¯”ï¼Œäººå·¥æ™ºèƒ½ç³»ç»Ÿéš¾ä»¥é€‚åº”åŒ»å­¦å›¾åƒæ•°æ®çš„è¿™ç§å˜åŒ–ã€‚</li>
<li>åˆ¶é€ å•†éœ€è¦å®šæœŸæ›´æ–°å…¶æ¨¡å‹ä»¥ç¡®ä¿å…¶å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨ç¾å›½å’Œæ¬§æ´²è”ç›Ÿï¼Œæœ€è¿‘çš„æ³•è§„å‘å±•ç®€åŒ–äº†æ¨¡å‹æ›´æ–°çš„æµç¨‹ã€‚</li>
<li>è¿™äº›æ–°æ³•è§„çš„æ ¸å¿ƒè¦æ±‚åŒ…æ‹¬æè¿°æ•°æ®é‡‡é›†å’Œå†è®­ç»ƒè¿‡ç¨‹ï¼Œä»¥åŠå»ºç«‹ç°å®ä¸–ç•Œçš„è´¨é‡ç›‘æ§æœºåˆ¶ã€‚</li>
<li>è¿™äº›æ³•è§„ä¸ºåˆ¶é€ å•†è¿›è¡Œæ— éœ€é‡æ–°å®¡æ‰¹çš„æ¨¡å‹æ›´æ–°æä¾›äº†å‰æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20498">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41cc14b33a52fb36cec0670a7fffe138.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d00a48abbbd427955703dd588ead8ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cc187764ce95bab5e590923ffdceeb5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Flemme-A-Flexible-and-Modular-Learning-Platform-for-Medical-Images"><a href="#Flemme-A-Flexible-and-Modular-Learning-Platform-for-Medical-Images" class="headerlink" title="Flemme: A Flexible and Modular Learning Platform for Medical Images"></a>Flemme: A Flexible and Modular Learning Platform for Medical Images</h2><p><strong>Authors:Guoqing Zhang, Jingyun Yang, Yang Li</strong></p>
<p>As the rapid development of computer vision and the emergence of powerful network backbones and architectures, the application of deep learning in medical imaging has become increasingly significant. Unlike natural images, medical images lack huge volumes of data but feature more modalities, making it difficult to train a general model that has satisfactory performance across various datasets. In practice, practitioners often suffer from manually creating and testing models combining independent backbones and architectures, which is a laborious and time-consuming process. We propose Flemme, a FLExible and Modular learning platform for MEdical images. Our platform separates encoders from the model architectures so that different models can be constructed via various combinations of supported encoders and architectures. We construct encoders using building blocks based on convolution, transformer, and state-space model (SSM) to process both 2D and 3D image patches. A base architecture is implemented following an encoder-decoder style, with several derived architectures for image segmentation, reconstruction, and generation tasks. In addition, we propose a general hierarchical architecture incorporating a pyramid loss to optimize and fuse vertical features. Experiments demonstrate that this simple design leads to an average improvement of 5.60% in Dice score and 7.81% in mean interaction of units (mIoU) for segmentation models, as well as an enhancement of 5.57% in peak signal-to-noise ratio (PSNR) and 8.22% in structural similarity (SSIM) for reconstruction models. We further utilize Flemme as an analytical tool to assess the effectiveness and efficiency of various encoders across different tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wlsdzyzl/flemme">https://github.com/wlsdzyzl/flemme</a>. </p>
<blockquote>
<p>éšç€è®¡ç®—æœºè§†è§‰çš„å¿«é€Ÿå‘å±•ä»¥åŠå¼ºå¤§çš„ç½‘ç»œä¸»å¹²å’Œæ¶æ„çš„å‡ºç°ï¼Œæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„åº”ç”¨å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ä¸è‡ªç„¶å›¾åƒä¸åŒï¼ŒåŒ»å­¦å›¾åƒè™½ç„¶æ•°æ®é‡ä¸å¤§ï¼Œä½†å…·æœ‰æ›´å¤šçš„æ¨¡æ€ï¼Œè¿™ä½¿å¾—è®­ç»ƒåœ¨å„ç§æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½çš„é€šç”¨æ¨¡å‹å˜å¾—å›°éš¾ã€‚åœ¨å®è·µä¸­ï¼Œå®è·µè€…ç»å¸¸é¢ä¸´æ‰‹åŠ¨åˆ›å»ºå’Œæµ‹è¯•ç»“åˆç‹¬ç«‹ä¸»å¹²å’Œæ¶æ„çš„æ¨¡å‹çš„é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¢ç¹çåˆè€—æ—¶çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†Flemmeï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦å›¾åƒçš„çµæ´»æ¨¡å—åŒ–å­¦ä¹ å¹³å°ã€‚æˆ‘ä»¬çš„å¹³å°å°†ç¼–ç å™¨ä»æ¨¡å‹æ¶æ„ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œä»¥ä¾¿å¯ä»¥é€šè¿‡æ”¯æŒçš„ç¼–ç å™¨å’Œæ¶æ„çš„å„ç§ç»„åˆæ¥æ„å»ºä¸åŒçš„æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºå·ç§¯ã€å˜å‹å™¨å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„æ„å»ºå—æ¥æ„å»ºç¼–ç å™¨ï¼Œä»¥å¤„ç†2Då’Œ3Då›¾åƒè¡¥ä¸ã€‚æˆ‘ä»¬é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨é£æ ¼çš„åŸºå‡†æ¶æ„ï¼Œå¹¶ä¸ºå…¶è¡ç”Ÿå‡ºç”¨äºå›¾åƒåˆ†å‰²ã€é‡å»ºå’Œç”Ÿæˆä»»åŠ¡çš„å‡ ç§æ¶æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„å±‚æ¬¡åŒ–æ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†é‡‘å­—å¡”æŸå¤±æ¥ä¼˜åŒ–å’Œèåˆå‚ç›´ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§ç®€å•çš„è®¾è®¡å¯¼è‡´åˆ†å‰²æ¨¡å‹çš„Diceå¾—åˆ†å¹³å‡æé«˜äº†5.60%ï¼Œå¹³å‡å•å…ƒäº¤äº’ï¼ˆmIoUï¼‰æé«˜äº†7.81%ï¼Œé‡å»ºæ¨¡å‹çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜äº†5.57%ï¼Œç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰æé«˜äº†8.22%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†Flemmeç”¨ä½œåˆ†æå·¥å…·ï¼Œä»¥è¯„ä¼°ä¸åŒç¼–ç å™¨åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wlsdzyzl/flemme%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wlsdzyzl/flemmeä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09369v2">PDF</a> 8 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong><br>    åŒ»å­¦å›¾åƒå¤„ç†é¢†åŸŸï¼Œæå‡ºä¸€ç§çµæ´»æ¨¡å—åŒ–çš„å­¦ä¹ å¹³å°Flemmeï¼Œå¯æ„å»ºå¤šç§æ¨¡å‹ã€‚å¹³å°å°†ç¼–ç å™¨ä¸æ¨¡å‹æ¶æ„åˆ†ç¦»ï¼Œåˆ©ç”¨å·ç§¯ã€å˜å‹å™¨å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ç­‰æ„å»ºå—å¤„ç†2Då’Œ3Då›¾åƒè¡¥ä¸ã€‚å®ç°åŸºç¡€æ¶æ„åï¼Œè¿›è¡Œå›¾åƒåˆ†å‰²ã€é‡å»ºå’Œç”Ÿæˆç­‰ä»»åŠ¡ã€‚é‡‡ç”¨é‡‘å­—å¡”æŸå¤±ä¼˜åŒ–å’Œèåˆå‚ç›´ç‰¹å¾ï¼Œå®éªŒè¯æ˜è¯¥è®¾è®¡æé«˜äº†åˆ†å‰²å’Œé‡å»ºæ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFlemmeå¯ä½œä¸ºåˆ†æå·¥å…·è¯„ä¼°ä¸åŒç¼–ç å™¨çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è®¡ç®—æœºè§†è§‰çš„å¿«é€Ÿå‘å±•å’Œå¼ºå¤§çš„ç½‘ç»œbackboneåŠæ¶æ„çš„å‡ºç°ï¼Œæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨è¶Šæ¥è¶Šé‡è¦ã€‚</li>
<li>åŒ»å­¦å›¾åƒæ•°æ®é‡å°‘ã€æ¨¡æ€å¤šï¼Œè®­ç»ƒé€šç”¨æ¨¡å‹éš¾åº¦å¤§ã€‚</li>
<li>å®è·µä¸­ï¼Œæ‰‹åŠ¨åˆ›å»ºå’Œæµ‹è¯•ç»“åˆç‹¬ç«‹backboneå’Œæ¶æ„çš„æ¨¡å‹æ˜¯è€—æ—¶ä¸”ç¹ççš„è¿‡ç¨‹ã€‚</li>
<li>æå‡ºFlemmeå¹³å°ï¼Œå¯çµæ´»æ„å»ºå¤šç§æ¨¡å‹ï¼Œå°†ç¼–ç å™¨ä¸æ¨¡å‹æ¶æ„åˆ†ç¦»ã€‚</li>
<li>åˆ©ç”¨å·ç§¯ã€å˜å‹å™¨å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ç­‰æ„å»ºå—å¤„ç†2Då’Œ3Då›¾åƒã€‚</li>
<li>å®ç°åŸºç¡€æ¶æ„å¹¶è¡ç”Ÿå‡ºç”¨äºå›¾åƒåˆ†å‰²ã€é‡å»ºå’Œç”Ÿæˆç­‰ä»»åŠ¡çš„æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨é‡‘å­—å¡”æŸå¤±ä¼˜åŒ–å‚ç›´ç‰¹å¾èåˆï¼Œå®éªŒè¯æ˜æ€§èƒ½æå‡æ˜¾è‘—ã€‚Flemmeè¿˜æä¾›åˆ†æå·¥å…·è¯„ä¼°ç¼–ç å™¨æ•ˆèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.09369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73acd5f64b3aa81b8c4b5dc394ed7b82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40336f84495f0fdaf5460d4ebbe2054a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55386339702039a92516d306cab73d59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a71c3a570ced1f5e01857b5aa07db433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e634e6887f93e17fcea019f769ed675.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee4311b49144445ebf9e50f0ac0f4fc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18002f3abae5903bfb22d0a3decd4fbd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Distillation-Learning-Guided-by-Image-Reconstruction-for-One-Shot-Medical-Image-Segmentation"><a href="#Distillation-Learning-Guided-by-Image-Reconstruction-for-One-Shot-Medical-Image-Segmentation" class="headerlink" title="Distillation Learning Guided by Image Reconstruction for One-Shot   Medical Image Segmentation"></a>Distillation Learning Guided by Image Reconstruction for One-Shot   Medical Image Segmentation</h2><p><strong>Authors:Feng Zhou, Yanjie Zhou, Longjie Wang, Yun Peng, David E. Carlson, Liyun Tu</strong></p>
<p>Traditional one-shot medical image segmentation (MIS) methods use registration networks to propagate labels from a reference atlas or rely on comprehensive sampling strategies to generate synthetic labeled data for training. However, these methods often struggle with registration errors and low-quality synthetic images, leading to poor performance and generalization. To overcome this, we introduce a novel one-shot MIS framework based on knowledge distillation, which allows the network to directly â€˜seeâ€™ real images through a distillation process guided by image reconstruction. It focuses on anatomical structures in a single labeled image and a few unlabeled ones. A registration-based data augmentation network creates realistic, labeled samples, while a feature distillation module helps the student network learn segmentation from these samples, guided by the teacher network. During inference, the streamlined student network accurately segments new images. Evaluations on three public datasets (OASIS for T1 brain MRI, BCV for abdomen CT, and VerSe for vertebrae CT) show superior segmentation performance and generalization across different medical image datasets and modalities compared to leading methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/NoviceFodder/OS-MedSeg">https://github.com/NoviceFodder/OS-MedSeg</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„ä¸€æ¬¡æ€§åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆMISï¼‰æ–¹æ³•ä½¿ç”¨æ³¨å†Œç½‘ç»œæ¥ä»å‚è€ƒå›¾è°±ä¼ æ’­æ ‡ç­¾ï¼Œæˆ–è€…ä¾èµ–å…¨é¢çš„é‡‡æ ·ç­–ç•¥æ¥ç”Ÿæˆåˆæˆæ ‡è®°æ•°æ®ä»¥è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç»å¸¸é¢ä¸´æ³¨å†Œé”™è¯¯å’Œä½è´¨é‡åˆæˆå›¾åƒçš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³å’Œæ³›åŒ–èƒ½åŠ›å·®ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºçŸ¥è¯†è’¸é¦çš„æ–°å‹ä¸€æ¬¡æ€§MISæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸ç½‘ç»œé€šè¿‡ç”±å›¾åƒé‡å»ºå¼•å¯¼çš„è’¸é¦è¿‡ç¨‹ç›´æ¥â€œæŸ¥çœ‹â€çœŸå®å›¾åƒã€‚å®ƒä¸“æ³¨äºå•ä¸ªæ ‡è®°å›¾åƒå’Œå°‘é‡æœªæ ‡è®°å›¾åƒä¸­çš„è§£å‰–ç»“æ„ã€‚åŸºäºæ³¨å†Œçš„æ•°æ®å¢å¼ºç½‘ç»œåˆ›å»ºé€¼çœŸçš„æ ‡è®°æ ·æœ¬ï¼Œè€Œç‰¹å¾è’¸é¦æ¨¡å—å¸®åŠ©å­¦ç”Ÿç½‘ç»œä»è¿™äº›æ ·æœ¬ä¸­å­¦ä¹ åˆ†å‰²ï¼Œç”±æ•™å¸ˆç½‘ç»œè¿›è¡Œå¼•å¯¼ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç®€åŒ–çš„å­¦ç”Ÿç½‘ç»œèƒ½å¤Ÿå‡†ç¡®åœ°å¯¹æ–°å›¾åƒè¿›è¡Œåˆ†å‰²ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆç”¨äºT1è„‘MRIçš„OASISã€ç”¨äºè…¹éƒ¨CTçš„BCVå’Œç”¨äºæ¤ä½“CTçš„VerSeï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸é¢†å…ˆçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨ä¸åŒåŒ»å­¦å›¾åƒæ•°æ®é›†å’Œæ¨¡æ€çš„åˆ†å‰²æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NoviceFodder/OS-MedSeg%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NoviceFodder/OS-MedSegä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.03616v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºçŸ¥è¯†è’¸é¦çš„æ–°å‹å•é•œå¤´åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•é¢ä¸´çš„æ³¨å†Œè¯¯å·®å’Œä½è´¨é‡åˆæˆå›¾åƒé—®é¢˜ã€‚é€šè¿‡å›¾åƒé‡å»ºå¼•å¯¼è’¸é¦è¿‡ç¨‹ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿç›´æ¥â€œçœ‹åˆ°â€çœŸå®å›¾åƒã€‚è¯¥æ¡†æ¶ä¾§é‡äºå•ä¸ªæ ‡è®°å›¾åƒå’Œå°‘é‡æœªæ ‡è®°å›¾åƒä¸­çš„è§£å‰–ç»“æ„ã€‚é€šè¿‡æ³¨å†Œæ•°æ®å¢å¼ºç½‘ç»œåˆ›å»ºé€¼çœŸçš„æ ‡è®°æ ·æœ¬ï¼Œç‰¹å¾è’¸é¦æ¨¡å—å¸®åŠ©å­¦ç”Ÿç½‘ç»œä»è¿™äº›æ ·æœ¬ä¸­å­¦ä¹ åˆ†å‰²ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå‡†ç¡®åˆ†å‰²æ–°å›¾åƒã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶åœ¨ä¸åŒåŒ»å­¦å›¾åƒæ•°æ®é›†å’Œæ¨¡æ€ä¸Šçš„ä¼˜è¶Šåˆ†å‰²æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºçŸ¥è¯†è’¸é¦çš„æ–°å‹å•é•œå¤´åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•é¢ä¸´çš„æ³¨å†Œè¯¯å·®é—®é¢˜ã€‚</li>
<li>é€šè¿‡å›¾åƒé‡å»ºè¿‡ç¨‹ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿç›´æ¥â€œçœ‹åˆ°â€çœŸå®å›¾åƒï¼Œæé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶ä¾§é‡äºå•ä¸ªæ ‡è®°å›¾åƒå’Œå°‘é‡æœªæ ‡è®°å›¾åƒä¸­çš„è§£å‰–ç»“æ„ã€‚</li>
<li>æ³¨å†Œæ•°æ®å¢å¼ºç½‘ç»œåˆ›å»ºé€¼çœŸçš„æ ‡è®°æ ·æœ¬ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç‰¹å¾è’¸é¦æ¨¡å—å¸®åŠ©å­¦ç”Ÿç½‘ç»œä»æ ‡è®°æ ·æœ¬ä¸­å­¦ä¹ åˆ†å‰²ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†è¯¥æ¡†æ¶çš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.03616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bedcbd6ff72a456cafcb2ea4fdcbddbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6ecf495f6092bdc2f237e3aeb3bcdeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-565b0b3373b2c5dfbb8daa3f98455882.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcbdf5a5e816d1cb46cc34522cf43e96.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Interactive-3D-Medical-Image-Segmentation-with-SAM-2"><a href="#Interactive-3D-Medical-Image-Segmentation-with-SAM-2" class="headerlink" title="Interactive 3D Medical Image Segmentation with SAM 2"></a>Interactive 3D Medical Image Segmentation with SAM 2</h2><p><strong>Authors:Chuyun Shen, Wenhao Li, Yuhang Shi, Xiangfeng Wang</strong></p>
<p>Interactive medical image segmentation (IMIS) has shown significant potential in enhancing segmentation accuracy by integrating iterative feedback from medical professionals. However, the limited availability of enough 3D medical data restricts the generalization and robustness of most IMIS methods. The Segment Anything Model (SAM), though effective for 2D images, requires expensive semi-auto slice-by-slice annotations for 3D medical images. In this paper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta SAM model trained on videos, for 3D medical image segmentation. By treating sequential 2D slices of 3D images as video frames, SAM 2 can fully automatically propagate annotations from a single frame to the entire 3D volume. We propose a practical pipeline for using SAM 2 in 3D medical image segmentation and present key findings highlighting its efficiency and potential for further optimization. Concretely, numerical experiments on the BraTS2020 and the medical segmentation decathlon datasets demonstrate that SAM 2 still has a gap with supervised methods but can narrow the gap in specific settings and organ types, significantly reducing the annotation burden on medical professionals. Our code will be open-sourced and available at <a target="_blank" rel="noopener" href="https://github.com/Chuyun-Shen/SAM_2_Medical_3D">https://github.com/Chuyun-Shen/SAM_2_Medical_3D</a>. </p>
<blockquote>
<p>äº¤äº’å¼åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆIMISï¼‰é€šè¿‡æ•´åˆåŒ»å­¦ä¸“å®¶çš„è¿­ä»£åé¦ˆï¼Œåœ¨æå‡åˆ†å‰²ç²¾åº¦æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå……è¶³3DåŒ»å­¦æ•°æ®çš„æœ‰é™å¯ç”¨æ€§é™åˆ¶äº†å¤§å¤šæ•°IMISæ–¹æ³•çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚Segment Anything Modelï¼ˆSAMï¼‰å¯¹äº2Då›¾åƒéå¸¸æœ‰æ•ˆï¼Œä½†å¯¹äº3DåŒ»å­¦å›¾åƒéœ€è¦æ˜‚è´µçš„åŠè‡ªåŠ¨åˆ‡ç‰‡çº§æ ‡æ³¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åŸºäºè§†é¢‘çš„ä¸‹ä¸€ä»£Meta SAMæ¨¡å‹SAM 2åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚é€šè¿‡å°†3Då›¾åƒçš„è¿ç»­2Dåˆ‡ç‰‡è§†ä¸ºè§†é¢‘å¸§ï¼ŒSAM 2å¯ä»¥å®Œå…¨è‡ªåŠ¨åœ°å°†å•ä¸ªå¸§çš„æ ‡æ³¨ä¼ æ’­åˆ°æ•´ä¸ª3Dä½“ç§¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½¿ç”¨SAM 2è¿›è¡Œ3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„å®é™…æµç¨‹ï¼Œå¹¶ä»‹ç»äº†å…³é”®å‘ç°ï¼Œçªå‡ºå…¶æ•ˆç‡å’Œè¿›ä¸€æ­¥ä¼˜åŒ–çš„æ½œåŠ›ã€‚å…·ä½“åœ°è¯´ï¼Œåœ¨BraTS2020å’ŒåŒ»å­¦åˆ†å‰²åé¡¹å…¨èƒ½æ•°æ®é›†ä¸Šçš„æ•°å€¼å®éªŒè¡¨æ˜ï¼ŒSAM 2ä¸ç›‘ç£æ–¹æ³•ä¹‹é—´ä»å­˜åœ¨ä¸€å®šå·®è·ï¼Œä½†åœ¨ç‰¹å®šè®¾ç½®å’Œå™¨å®˜ç±»å‹ä¸­å¯ä»¥ç¼©å°å·®è·ï¼Œæ˜¾è‘—å‡å°‘åŒ»å­¦ä¸“ä¸šäººå£«çš„æ ‡æ³¨è´Ÿæ‹…ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Chuyun-Shen/SAM_2_Medical_3D%E5%BC%80%E6%BA%90%E5%B9%B6%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Chuyun-Shen/SAM_2_Medical_3Då¼€æºå¹¶æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02635v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²çš„é›¶æ ‡æ³¨èƒ½åŠ›ç ”ç©¶ï¼šSegment Anything Model 2ï¼ˆSAM 2ï¼‰æ¨¡å‹åŸºäºè§†é¢‘è®­ç»ƒçš„ä¸‹ä¸€ä»£æ¨¡å‹ï¼Œç”¨äºå…¨è‡ªåŠ¨åœ°å¤„ç†3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ ‡æ³¨é—®é¢˜ã€‚è¯¥æ¨¡å‹å¯å°†å•å¸§æ ‡æ³¨è‡ªåŠ¨ä¼ æ’­åˆ°æ•´ä¸ªä¸‰ç»´ä½“ç§¯ï¼Œä»è€Œæ˜¾è‘—å‡å°‘åŒ»å­¦ä¸“ä¸šäººå£«çš„æ ‡æ³¨è´Ÿæ‹…ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨SAM 2è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²çš„å®é™…æµç¨‹ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯å…¶åœ¨ç‰¹å®šåœºæ™¯å’Œå™¨å®˜ç±»å‹ä¸­çš„è¡¨ç°æ½œåŠ›ã€‚ä»£ç å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>IMISé€šè¿‡æ•´åˆä¸“ä¸šäººå‘˜çš„åé¦ˆæé«˜äº†åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>3DåŒ»å­¦æ•°æ®çš„æœ‰é™å¯ç”¨æ€§é™åˆ¶äº†å¤§å¤šæ•°IMISæ–¹æ³•çš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚</li>
<li>Segment Anything Modelï¼ˆSAMï¼‰å¯¹äº2Då›¾åƒæœ‰æ•ˆï¼Œä½†å¯¹3DåŒ»å­¦å›¾åƒéœ€è¦æ˜‚è´µçš„åŠè‡ªåŠ¨åˆ‡ç‰‡æ ‡æ³¨ã€‚</li>
<li>SAM 2æ¨¡å‹å…·å¤‡é›¶æ ‡æ³¨èƒ½åŠ›ï¼Œå¯å…¨è‡ªåŠ¨åœ°ä»å•ä¸ªå¸§ä¼ æ’­æ ‡æ³¨åˆ°æ•´ä¸ªä¸‰ç»´ä½“ç§¯ã€‚</li>
<li>SAM 2åœ¨BraTS2020å’ŒåŒ»å­¦åˆ†å‰²åé¡¹å…¨èƒ½æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°è¡¨æ˜å…¶åœ¨ç‰¹å®šè®¾ç½®å’Œå™¨å®˜ç±»å‹ä¸­æœ‰ç¼©å°ä¸ç›‘ç£æ–¹æ³•å·®è·çš„æ½œåŠ›ã€‚</li>
<li>SAM 2æ¨¡å‹çš„å¼€æºä»£ç å¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.02635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cae48beb8a2a278db559306d114d1e2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e8ac0129b6a6ff2629fe1003df12cd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5041caae0eded3684bf3ddde26fa2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-822d8260c3a1a3ac11cdab6cf9bb1568.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Restore-RWKV-Efficient-and-Effective-Medical-Image-Restoration-with-RWKV"><a href="#Restore-RWKV-Efficient-and-Effective-Medical-Image-Restoration-with-RWKV" class="headerlink" title="Restore-RWKV: Efficient and Effective Medical Image Restoration with   RWKV"></a>Restore-RWKV: Efficient and Effective Medical Image Restoration with   RWKV</h2><p><strong>Authors:Zhiwen Yang, Jiayin Li, Hui Zhang, Dan Zhao, Bingzheng Wei, Yan Xu</strong></p>
<p>Transformers have revolutionized medical image restoration, but the quadratic complexity still poses limitations for their application to high-resolution medical images. The recent advent of the Receptance Weighted Key Value (RWKV) model in the natural language processing field has attracted much attention due to its ability to process long sequences efficiently. To leverage its advanced design, we propose Restore-RWKV, the first RWKV-based model for medical image restoration. Since the original RWKV model is designed for 1D sequences, we make two necessary modifications for modeling spatial relations in 2D medical images. First, we present a recurrent WKV (Re-WKV) attention mechanism that captures global dependencies with linear computational complexity. Re-WKV incorporates bidirectional attention as basic for a global receptive field and recurrent attention to effectively model 2D dependencies from various scan directions. Second, we develop an omnidirectional token shift (Omni-Shift) layer that enhances local dependencies by shifting tokens from all directions and across a wide context range. These adaptations make the proposed Restore-RWKV an efficient and effective model for medical image restoration. Even a lightweight variant of Restore-RWKV, with only 1.16 million parameters, achieves comparable or even superior results compared to existing state-of-the-art (SOTA) methods. Extensive experiments demonstrate that the resulting Restore-RWKV achieves SOTA performance across a range of medical image restoration tasks, including PET image synthesis, CT image denoising, MRI image super-resolution, and all-in-one medical image restoration. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/Yaziwel/Restore-RWKV">https://github.com/Yaziwel/Restore-RWKV</a>. </p>
<blockquote>
<p>Transformerå·²ç»å½»åº•æ”¹å˜äº†åŒ»å­¦å›¾åƒä¿®å¤é¢†åŸŸï¼Œä½†æ˜¯å…¶äºŒæ¬¡å¤æ‚åº¦ä»ç„¶é™åˆ¶äº†å…¶åœ¨é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒä¸Šçš„åº”ç”¨ã€‚è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ€è¿‘å‡ºç°çš„æ¥çº³åŠ æƒé”®å€¼ï¼ˆReceptance Weighted Key Valueï¼Œç®€ç§°RWKVï¼‰æ¨¡å‹å› å…¶é«˜æ•ˆå¤„ç†é•¿åºåˆ—çš„èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ä¸ºäº†åˆ©ç”¨å®ƒçš„å…ˆè¿›è®¾è®¡ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºRWKVæ¨¡å‹çš„åŒ»å­¦å›¾åƒä¿®å¤æ¨¡å‹Restore-RWKVã€‚ç”±äºåŸå§‹çš„RWKVæ¨¡å‹æ˜¯ä¸º1Dåºåˆ—è®¾è®¡çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯¹å»ºæ¨¡äºŒç»´åŒ»å­¦å›¾åƒä¸­çš„ç©ºé—´å…³ç³»è¿›è¡Œäº†ä¸¤é¡¹å¿…è¦çš„ä¿®æ”¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¾ªç¯WKVï¼ˆRe-WKVï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒä»¥çº¿æ€§è®¡ç®—å¤æ‚åº¦æ•è·å…¨å±€ä¾èµ–å…³ç³»ã€‚Re-WKVå°†åŒå‘æ³¨æ„åŠ›ä½œä¸ºåŸºæœ¬å…¨å±€æ„Ÿå—é‡å’Œå¾ªç¯æ³¨æ„åŠ›ï¼Œä»¥æœ‰æ•ˆåœ°ä»å„ç§æ‰«ææ–¹å‘å¯¹äºŒç»´ä¾èµ–æ€§è¿›è¡Œå»ºæ¨¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨æ–¹å‘ä»¤ç‰Œç§»ä½ï¼ˆOmni-Shiftï¼‰å±‚ï¼Œå®ƒé€šè¿‡ä»å„ä¸ªæ–¹å‘ç§»åŠ¨ä»¤ç‰Œå¹¶åœ¨å¹¿æ³›çš„ä¸Šä¸‹æ–‡èŒƒå›´å†…è¿›è¡Œæ“ä½œæ¥å¢å¼ºå±€éƒ¨ä¾èµ–æ€§ã€‚è¿™äº›é€‚åº”æ€§è°ƒæ•´ä½¿å¾—æå‡ºçš„Restore-RWKVæˆä¸ºåŒ»å­¦å›¾åƒä¿®å¤çš„æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ¨¡å‹ã€‚å³ä½¿æ˜¯ä¸€ä¸ªåªæœ‰116ä¸‡ä¸ªå‚æ•°çš„Restore-RWKVçš„è½»é‡çº§å˜ä½“ï¼Œä¹Ÿèƒ½è¾¾åˆ°æˆ–è¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ–¹æ³•çš„ç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ¢å¤åçš„Restore-RWKVåœ¨å¤šç§åŒ»å­¦å›¾åƒä¿®å¤ä»»åŠ¡ä¸­å‡è¾¾åˆ°SOTAæ€§èƒ½ï¼ŒåŒ…æ‹¬PETå›¾åƒåˆæˆã€CTå›¾åƒå»å™ªã€MRIå›¾åƒè¶…åˆ†è¾¨ç‡ä»¥åŠå…¨æ–¹ä½çš„åŒ»å­¦å›¾åƒä¿®å¤ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Yaziwel/Restore-RWKV">https://github.com/Yaziwel/Restore-RWKV</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11087v3">PDF</a> This paper introduces the first RWKV-based model for image   restoration</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒæ¢å¤çš„RWKVæ¨¡å‹çš„åº”ç”¨ã€‚é’ˆå¯¹åŸå§‹RWKVæ¨¡å‹è®¾è®¡ç”¨äºä¸€ç»´åºåˆ—çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºé’ˆå¯¹äºŒç»´åŒ»å­¦å›¾åƒçš„æ”¹è¿›ç‰ˆæ¨¡å‹Restore-RWKVã€‚æ­¤æ¨¡å‹é‡‡ç”¨é€’å½’WKVæ³¨æ„åŠ›æœºåˆ¶å’Œå…¨æ–¹ä½tokenä½ç§»å±‚æ¥å¤„ç†äºŒç»´ä¾èµ–æ€§ã€‚ç»“æœè¯æ˜Restore-RWKVåœ¨å¤šç§åŒ»å­¦å›¾åƒæ¢å¤ä»»åŠ¡ä¸Šå‡è¾¾åˆ°æˆ–è¶…è¶Šç°æœ‰æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»‹ç»äº†Transformersåœ¨åŒ»å­¦å›¾åƒæ¢å¤ä¸­çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯äºŒæ¬¡å¤æ‚æ€§å¯¹äºé«˜åˆ†è¾¨ç‡å›¾åƒçš„æŒ‘æˆ˜ã€‚</li>
<li>RWKVæ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå—åˆ°å…³æ³¨ï¼Œç”±äºå…¶å¤„ç†é•¿åºåˆ—çš„æ•ˆç‡ã€‚</li>
<li>æå‡ºRestore-RWKVæ¨¡å‹ï¼Œæ˜¯é¦–ä¸ªåŸºäºRWKVçš„åŒ»å­¦å›¾åƒæ¢å¤æ¨¡å‹ã€‚</li>
<li>ä¸ºé€‚åº”äºŒç»´åŒ»å­¦å›¾åƒçš„ç©ºé—´å…³ç³»ï¼Œå¯¹åŸå§‹RWKVæ¨¡å‹è¿›è¡Œäº†ä¸¤é¡¹å¿…è¦ä¿®æ”¹ï¼šé€’å½’WKVæ³¨æ„åŠ›æœºåˆ¶å’Œå…¨æ–¹ä½tokenä½ç§»å±‚ã€‚</li>
<li>Restore-RWKVæ¨¡å‹åœ¨å¤šç§åŒ»å­¦å›¾åƒæ¢å¤ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬PETå›¾åƒåˆæˆã€CTå›¾åƒå»å™ªã€MRIå›¾åƒè¶…åˆ†è¾¨ç‡ç­‰ã€‚</li>
<li>æ¨¡å‹å…·æœ‰é«˜æ•ˆæ€§ï¼Œå³ä½¿æ˜¯è½»é‡çº§å˜ä½“ä¹Ÿèƒ½è¾¾åˆ°æˆ–è¶…è¶Šç°æœ‰æœ€ä½³æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-23ad565289242fb458ed9dc28dffe59c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-849665ef088240c2b6ac0b5b7be1f2e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-144b97e25d4b7acb505dc0fa221065d6.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-09/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-30ba2f38761f3646f04aa64dbf72bb15.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-09  MRJ-Agent An Effective Jailbreak Agent for Multi-Round Dialogue
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-09/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b85ce8e7cd121bc67c268889b6930d21.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-09  MC-VTON Minimal Control Virtual Try-On Diffusion Transformer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18863.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
