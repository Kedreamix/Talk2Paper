<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-09  Sa2VA Marrying SAM2 with LLaVA for Dense Grounded Understanding of   Images and Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-07080b1f8b29f074b0b50b1569e8df7b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-09-æ›´æ–°"><a href="#2025-01-09-æ›´æ–°" class="headerlink" title="2025-01-09 æ›´æ–°"></a>2025-01-09 æ›´æ–°</h1><h2 id="Sa2VA-Marrying-SAM2-with-LLaVA-for-Dense-Grounded-Understanding-of-Images-and-Videos"><a href="#Sa2VA-Marrying-SAM2-with-LLaVA-for-Dense-Grounded-Understanding-of-Images-and-Videos" class="headerlink" title="Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of   Images and Videos"></a>Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of   Images and Videos</h2><p><strong>Authors:Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, Ming-Hsuan Yang</strong></p>
<p>This work presents Sa2VA, the first unified model for dense grounded understanding of both images and videos. Unlike existing multi-modal large language models, which are often limited to specific modalities and tasks, Sa2VA supports a wide range of image and video tasks, including referring segmentation and conversation, with minimal one-shot instruction tuning. Sa2VA combines SAM-2, a foundation video segmentation model, with LLaVA, an advanced vision-language model, and unifies text, image, and video into a shared LLM token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2 in producing precise masks, enabling a grounded, multi-modal understanding of both static and dynamic visual content. Additionally, we introduce Ref-SAV, an auto-labeled dataset containing over 72k object expressions in complex video scenes, designed to boost model performance. We also manually validate 2k video objects in the Ref-SAV datasets to benchmark referring video object segmentation in complex environments. Experiments show that Sa2VA achieves state-of-the-art across multiple tasks, particularly in referring video object segmentation, highlighting its potential for complex real-world applications. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Sa2VAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ï¼Œå®ç°å¯¹å›¾åƒå’Œè§†é¢‘çš„å¯†é›†æ¥åœ°ç†è§£ã€‚ä¸ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸åŒï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä»…é™äºç‰¹å®šçš„æ¨¡æ€å’Œä»»åŠ¡ï¼Œè€ŒSa2VAæ”¯æŒå¹¿æ³›çš„å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¼•ç”¨åˆ†å‰²å’Œå¯¹è¯ï¼Œä»¥åŠæœ€å°‘çš„ä¸€æ¬¡æ€§æŒ‡ä»¤è°ƒæ•´ã€‚Sa2VAç»“åˆäº†SAM-2åŸºç¡€è§†é¢‘åˆ†å‰²æ¨¡å‹å’ŒLLaVAé«˜çº§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶å°†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç»Ÿä¸€åˆ°å…±äº«çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»¤ç‰Œç©ºé—´ä¸­ã€‚ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒSa2VAç”ŸæˆæŒ‡ä»¤ä»¤ç‰Œï¼ŒæŒ‡å¯¼SAM-2ç”Ÿæˆç²¾ç¡®è’™ç‰ˆï¼Œå®ç°å¯¹é™æ€å’ŒåŠ¨æ€è§†è§‰å†…å®¹çš„æ¥åœ°å¤šæ¨¡æ€ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Ref-SAVï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨æ ‡è®°çš„æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡7.2ä¸‡ä¸ªå¤æ‚è§†é¢‘åœºæ™¯ä¸­çš„å¯¹è±¡è¡¨è¾¾å¼ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ‰‹åŠ¨éªŒè¯äº†Ref-SAVæ•°æ®é›†ä¸­çš„2000ä¸ªè§†é¢‘å¯¹è±¡ï¼Œä»¥è¯„ä¼°å¤æ‚ç¯å¢ƒä¸­è§†é¢‘å¯¹è±¡åˆ†å‰²çš„å¼•ç”¨æƒ…å†µã€‚å®éªŒè¡¨æ˜ï¼ŒSa2VAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²æ–¹é¢ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨å¤æ‚ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04001v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://lxtgh.github.io/project/sa2va">https://lxtgh.github.io/project/sa2va</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Sa2VAæ¨¡å‹ï¼Œè¿™æ˜¯é¦–ä¸ªæ”¯æŒå›¾åƒå’Œè§†é¢‘å¯†é›†ç†è§£çš„ç»Ÿä¸€æ¨¡å‹ã€‚ç›¸è¾ƒäºç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒSa2VAæ”¯æŒå¹¿æ³›çš„å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¼•ç”¨åˆ†å‰²å’Œå¯¹è¯ç­‰ï¼Œåªéœ€ä¸€æ¬¡æŒ‡ä»¤è°ƒæ•´å³å¯å®Œæˆä»»åŠ¡ã€‚Sa2VAç»“åˆäº†SAM-2åŸºç¡€è§†é¢‘åˆ†å‰²æ¨¡å‹å’ŒLLaVAé«˜çº§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶å°†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç»Ÿä¸€åˆ°å…±äº«çš„LLMä»¤ç‰Œç©ºé—´ä¸­ã€‚ä½¿ç”¨LLMï¼ŒSa2VAç”ŸæˆæŒ‡å¯¼SAM-2äº§ç”Ÿç²¾ç¡®æ©ç çš„æŒ‡ä»¤ä»¤ç‰Œï¼Œå®ç°å¯¹é™æ€å’ŒåŠ¨æ€è§†è§‰å†…å®¹çš„åŸºäºåœ°é¢çš„å¤šæ¨¡æ€ç†è§£ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†ç”¨äºå¤æ‚è§†é¢‘åœºæ™¯çš„Ref-SAVæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡7ä¸‡å¤šä¸ªå¯¹è±¡è¡¨è¾¾å¼ï¼Œæ—¨åœ¨æå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒSa2VAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²æ–¹é¢ï¼Œçªæ˜¾å…¶åœ¨å¤æ‚ç°å®åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sa2VAæ˜¯é¦–ä¸ªæ”¯æŒå›¾åƒå’Œè§†é¢‘å¯†é›†ç†è§£çš„ç»Ÿä¸€æ¨¡å‹ã€‚</li>
<li>Sa2VAæ”¯æŒå¹¿æ³›çš„å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¼•ç”¨åˆ†å‰²å’Œå¯¹è¯ç­‰ã€‚</li>
<li>Sa2VAé€šè¿‡ç»“åˆSAM-2å’ŒLLaVAæ¨¡å‹ï¼Œå®ç°äº†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘çš„å…±äº«LLMä»¤ç‰Œç©ºé—´ã€‚</li>
<li>LLMç”Ÿæˆçš„æŒ‡ä»¤ä»¤ç‰ŒæŒ‡å¯¼SAM-2äº§ç”Ÿç²¾ç¡®æ©ç ï¼Œå®ç°åŸºäºåœ°é¢çš„å¤šæ¨¡æ€ç†è§£ã€‚</li>
<li>å¼•å…¥çš„Ref-SAVæ•°æ®é›†åŒ…å«å¤æ‚çš„è§†é¢‘åœºæ™¯ä¸­çš„å¤§é‡å¯¹è±¡è¡¨è¾¾å¼ï¼Œæ—¨åœ¨æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜Sa2VAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-37c1d24e3985fc4d0396a1cc4d563f80.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea99dbeca729ec6f4abfb0752a1ecb41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1c4fdd829bd1964e931938bed1d5671.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a12edd09f1b0fdc3efc235d297edc81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1674298d3fcaaf7f6774c209b9536dca.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Models-as-Values-Detectors"><a href="#Vision-Language-Models-as-Values-Detectors" class="headerlink" title="Vision Language Models as Values Detectors"></a>Vision Language Models as Values Detectors</h2><p><strong>Authors:Giulio Antonio Abbo, Tony Belpaeme</strong></p>
<p>Large Language Models integrating textual and visual inputs have introduced new possibilities for interpreting complex data. Despite their remarkable ability to generate coherent and contextually relevant text based on visual stimuli, the alignment of these models with human perception in identifying relevant elements in images requires further exploration. This paper investigates the alignment between state-of-the-art LLMs and human annotators in detecting elements of relevance within home environment scenarios. We created a set of twelve images depicting various domestic scenarios and enlisted fourteen annotators to identify the key element in each image. We then compared these human responses with outputs from five different LLMs, including GPT-4o and four LLaVA variants. Our findings reveal a varied degree of alignment, with LLaVA 34B showing the highest performance but still scoring low. However, an analysis of the results highlights the modelsâ€™ potential to detect value-laden elements in images, suggesting that with improved training and refined prompts, LLMs could enhance applications in social robotics, assistive technologies, and human-computer interaction by providing deeper insights and more contextually relevant responses. </p>
<blockquote>
<p>å°†æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ç»“åˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºè§£é‡Šå¤æ‚æ•°æ®å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨åŸºäºè§†è§‰åˆºæ¿€ç”Ÿæˆè¿è´¯ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„æ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¸äººç±»æ„ŸçŸ¥åœ¨è¯†åˆ«å›¾åƒä¸­ç›¸å…³å…ƒç´ æ–¹é¢çš„å¯¹é½ä»éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢ã€‚æœ¬æ–‡è°ƒæŸ¥äº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»æ³¨é‡Šå™¨åœ¨æ£€æµ‹å®¶åº­ç¯å¢ƒåœºæ™¯ä¸­ç›¸å…³å…ƒç´ æ–¹é¢çš„å¯¹é½æƒ…å†µã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ç»„åŒ…å«åäºŒå¼ å›¾åƒï¼Œæç»˜äº†å„ç§å®¶åº­åœºæ™¯ï¼Œå¹¶æ‹›å‹Ÿäº†åå››åæ³¨é‡Šå™¨æ¥è¯†åˆ«æ¯å¼ å›¾åƒä¸­çš„å…³é”®å…ƒç´ ã€‚ç„¶åæˆ‘ä»¬å°†è¿™äº›äººç±»ååº”ä¸æ¥è‡ªåŒ…æ‹¬GPT-4oå’Œå››ç§LLaVAå˜ä½“åœ¨å†…çš„äº”ç§ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹é½ç¨‹åº¦ä¸ä¸€ï¼ŒLLaVA 34Bè¡¨ç°æœ€å¥½ä½†å¾—åˆ†ä»ç„¶è¾ƒä½ã€‚ç„¶è€Œï¼Œå¯¹ç»“æœçš„åˆ†æçªæ˜¾äº†æ¨¡å‹æ£€æµ‹å›¾åƒä¸­ä»·å€¼ç›¸å…³å…ƒç´ çš„æ½œåŠ›ï¼Œè¿™è¡¨æ˜é€šè¿‡æ”¹è¿›è®­ç»ƒå’Œç²¾ç‚¼æç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥å¢å¼ºåœ¨ç¤¾ä¼šæœºå™¨äººã€è¾…åŠ©æŠ€æœ¯å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œæä¾›æ›´æ·±å…¥çš„è§è§£å’Œæ›´ä¸Šä¸‹æ–‡ç›¸å…³çš„ååº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03957v1">PDF</a> 13 pages, 2 figures</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹èåˆæ–‡æœ¬å’Œè§†è§‰è¾“å…¥ï¼Œä¸ºè§£è¯»å¤æ‚æ•°æ®å¸¦æ¥æ–°å¯èƒ½ã€‚å°½ç®¡è¿™äº›æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è§†è§‰åˆºæ¿€ç”Ÿæˆè¿è´¯ä¸”è¯­å¢ƒç›¸å…³çš„æ–‡æœ¬ï¼Œä½†å®ƒä»¬ä¸äººç±»æ„ŸçŸ¥åœ¨è¯†åˆ«å›¾åƒä¸­ç›¸å…³å…ƒç´ æ–¹é¢çš„å¯¹é½ä»éœ€æ¢ç´¢ã€‚æœ¬æ–‡è°ƒæŸ¥äº†æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ä¸äººç±»æ ‡æ³¨å™¨åœ¨è¯†åˆ«å®¶åº­ç¯å¢ƒåœºæ™¯ä¸­ç›¸å…³å…ƒç´ çš„å¯¹é½æƒ…å†µã€‚é€šè¿‡åˆ›å»ºåŒ…å«åäºŒå¼ æç»˜å„ç§å®¶åº­åœºæ™¯å›¾åƒçš„é›†åˆï¼Œå¹¶é‚€è¯·åå››åæ ‡æ³¨å™¨è¯†åˆ«æ¯å¼ å›¾åƒä¸­çš„å…³é”®å…ƒç´ ï¼Œå†å°†è¿™äº›äººç±»å“åº”ä¸GPT-4oç­‰äº”ç§ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºè¿›è¡Œæ¯”è¾ƒï¼ˆåŒ…æ‹¬LLaVAçš„å››ç§å˜ä½“ï¼‰ã€‚å‘ç°å­˜åœ¨ä¸€å®šç¨‹åº¦çš„å¯¹é½æ€§ï¼Œå…¶ä¸­LLaVA 34Bè¡¨ç°æœ€ä½³ä½†ä»å¾—åˆ†è¾ƒä½ã€‚ç„¶è€Œï¼Œå¯¹ç»“æœçš„åˆ†æçªæ˜¾äº†æ¨¡å‹æ£€æµ‹å›¾åƒä¸­ä»·å€¼ç›¸å…³å…ƒç´ çš„æ½œåŠ›ï¼Œè¡¨æ˜é€šè¿‡æ”¹è¿›è®­ç»ƒå’Œç»†åŒ–æç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯åœ¨ç¤¾äº¤æœºå™¨äººã€è¾…åŠ©æŠ€æœ¯å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸæä¾›æ›´æ·±å…¥è§è§£å’Œæ›´è¯­å¢ƒç›¸å…³çš„å“åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿèåˆæ–‡æœ¬å’Œè§†è§‰è¾“å…¥ï¼Œä¸ºæ•°æ®è§£è¯»å¸¦æ¥æ–°å¯èƒ½ã€‚</li>
<li>åœ¨è¯†åˆ«å›¾åƒä¸­çš„ç›¸å…³å…ƒç´ æ–¹é¢ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»æ„ŸçŸ¥çš„å¯¹é½ä»éœ€æ¢ç´¢ã€‚</li>
<li>é€šè¿‡åˆ›å»ºåŒ…å«å®¶åº­åœºæ™¯å›¾åƒçš„é›†åˆè¿›è¡Œå®éªŒï¼Œå‘ç°LLaVA 34Båœ¨è¯†åˆ«ç›¸å…³å…ƒç´ æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹å›¾åƒä¸­çš„ä»·å€¼ç›¸å…³å…ƒç´ æ–¹é¢å±•ç°æ½œåŠ›ã€‚</li>
<li>é€šè¿‡æ”¹è¿›è®­ç»ƒå’Œç»†åŒ–æç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾äº¤æœºå™¨äººã€è¾…åŠ©æŠ€æœ¯å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸçš„åº”ç”¨æœ‰æœ›å¢å¼ºã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å“è¶Šè¡¨ç°ä½“ç°åœ¨ç”Ÿæˆè¿è´¯ä¸”è¯­å¢ƒç›¸å…³çš„æ–‡æœ¬è¾“å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-83cad6315b43a305346205e30b6f1df6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f126c628cc32978812c2672376a482d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccd65fbe270f167867d87ae4c80279d8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Localizing-AI-Evaluating-Open-Weight-Language-Models-for-Languages-of-Baltic-States"><a href="#Localizing-AI-Evaluating-Open-Weight-Language-Models-for-Languages-of-Baltic-States" class="headerlink" title="Localizing AI: Evaluating Open-Weight Language Models for Languages of   Baltic States"></a>Localizing AI: Evaluating Open-Weight Language Models for Languages of   Baltic States</h2><p><strong>Authors:Jurgita KapoÄiÅ«tÄ—-DzikienÄ—, Toms Bergmanis, MÄrcis Pinnis</strong></p>
<p>Although large language models (LLMs) have transformed our expectations of modern language technologies, concerns over data privacy often restrict the use of commercially available LLMs hosted outside of EU jurisdictions. This limits their application in governmental, defence, and other data-sensitive sectors. In this work, we evaluate the extent to which locally deployable open-weight LLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian. We examine various size and precision variants of the top-performing multilingual open-weight models, Llama<del>3, Gemma</del>2, Phi, and NeMo, on machine translation, multiple-choice question answering, and free-form text generation. The results indicate that while certain models like Gemma~2 perform close to the top commercially available models, many LLMs struggle with these languages. Most surprisingly, however, we find that these models, while showing close to state-of-the-art translation performance, are still prone to lexical hallucinations with errors in at least 1 in 20 words for all open-weight multilingual LLMs. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ”¹å˜äº†æˆ‘ä»¬å¯¹ç°ä»£è¯­è¨€æŠ€æœ¯çš„æœŸæœ›ï¼Œä½†å¯¹æ•°æ®éšç§çš„æ‹…å¿§å¸¸å¸¸é™åˆ¶äº†åœ¨æ¬§ç›Ÿè¾–åŒºå¤–æ‰˜ç®¡çš„å¯å•†ç”¨LLMçš„ä½¿ç”¨ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨æ”¿åºœã€å›½é˜²å’Œå…¶ä»–å¯¹æ•°æ®æ•æ„Ÿçš„é¢†åŸŸçš„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¯åœ¨æœ¬åœ°éƒ¨ç½²çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹æ”¯æŒè¯¸å¦‚ç«‹é™¶å®›è¯­ã€æ‹‰è„±ç»´äºšè¯­å’Œçˆ±æ²™å°¼äºšè¯­ç­‰ä½¿ç”¨äººæ•°è¾ƒå°‘çš„è¯­è¨€çš„ç¨‹åº¦ã€‚æˆ‘ä»¬ç ”ç©¶äº†é¡¶çº§å¤šè¯­è¨€å¼€æºæ¨¡å‹çš„å„ç§å¤§å°å’Œç²¾åº¦å˜ä½“ï¼ŒåŒ…æ‹¬Llama 3ã€Gemma 2ã€Phiå’ŒNeMoï¼Œå¯¹æœºå™¨ç¿»è¯‘ã€å¤šé¡¹é€‰æ‹©é¢˜å›ç­”å’Œè‡ªç”±å½¢å¼æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶åƒGemma 2è¿™æ ·çš„æŸäº›æ¨¡å‹çš„è¡¨ç°æ¥è¿‘é¡¶çº§å¯å•†ç”¨æ¨¡å‹ï¼Œä½†è®¸å¤šå¤§å‹è¯­è¨€æ¨¡å‹å¯¹è¿™äº›è¯­è¨€çš„å¤„ç†ä»ç„¶æœ‰å›°éš¾ã€‚ç„¶è€Œï¼Œæœ€ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹è™½ç„¶åœ¨ç¿»è¯‘æ€§èƒ½ä¸Šæ¥è¿‘æœ€æ–°æ°´å¹³ï¼Œä½†ä»å®¹æ˜“å‡ºç°è¯æ±‡å¹»è§‰ï¼Œæ‰€æœ‰å¼€æºå¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è‡³å°‘æœ‰1&#x2F;20çš„è¯å­˜åœ¨é”™è¯¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03952v1">PDF</a> This paper is accepted to NoDaLiDa&#x2F;Baltic-HLT 2025</p>
<p><strong>Summary</strong></p>
<p>å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¯æŒç«‹é™¶å®›è¯­ã€æ‹‰è„±ç»´äºšè¯­å’Œçˆ±æ²™å°¼äºšè¯­ç­‰å°‘è¯­ç§çš„ç¨‹åº¦è¯„ä¼°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒæŸäº›æ¨¡å‹å¦‚Gemma 2è¡¨ç°æ¥è¿‘é¡¶çº§å•†ä¸šæ¨¡å‹ï¼Œä½†å¤šæ•°æ¨¡å‹å¯¹è¿™äº›è¯­è¨€çš„å¤„ç†èƒ½åŠ›ä»æœ‰å¾…æå‡ï¼Œä¸”å­˜åœ¨è¯æ±‡å¹»è§‰é—®é¢˜ï¼Œå³æ¯20ä¸ªè¯ä¸­è‡³å°‘æœ‰ä¸€ä¸ªè¯ä¼šå‡ºç°é”™è¯¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æ”¹å˜äº†æˆ‘ä»¬å¯¹ç°ä»£è¯­è¨€æŠ€æœ¯çš„æœŸå¾…ã€‚</li>
<li>åœ¨æ¬§ç›Ÿç®¡è¾–åŒºä»¥å¤–çš„å•†ä¸šå¯ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä½¿ç”¨å—åˆ°æ•°æ®éšç§é—®é¢˜çš„é™åˆ¶ã€‚</li>
<li>è¿™é™åˆ¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¿åºœéƒ¨é—¨ã€å†›äº‹å’Œå…¶ä»–æ•°æ®æ•æ„Ÿé¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å°‘è¯­ç§å¦‚ç«‹é™¶å®›è¯­ã€æ‹‰è„±ç»´äºšè¯­å’Œçˆ±æ²™å°¼äºšè¯­æ—¶ï¼Œæ€§èƒ½å„å¼‚ã€‚</li>
<li>éƒ¨åˆ†æ¨¡å‹å¦‚Gemma 2è¡¨ç°æ¥è¿‘é¡¶çº§å•†ä¸šæ¨¡å‹ã€‚</li>
<li>ä½†å¤§éƒ¨åˆ†æ¨¡å‹å¯¹è¿™äº›è¯­è¨€çš„å¤„ç†èƒ½åŠ›æœ‰å¾…æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fbf9bc265c7f0e9f8746e98e53ecf971.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45ce39bfd6cbb7590a724a5324eacfc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2120e3d31a1ca0c5f7f1bb012e164709.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f781c58ec564592144d1afbf4d29cd94.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Potential-of-Large-Language-Models-in-Public-Transportation-San-Antonio-Case-Study"><a href="#Exploring-the-Potential-of-Large-Language-Models-in-Public-Transportation-San-Antonio-Case-Study" class="headerlink" title="Exploring the Potential of Large Language Models in Public   Transportation: San Antonio Case Study"></a>Exploring the Potential of Large Language Models in Public   Transportation: San Antonio Case Study</h2><p><strong>Authors:Ramya Jonnala, Gongbo Liang, Jeong Yang, Izzat Alsmadi</strong></p>
<p>The integration of large language models (LLMs) into public transit systems presents a transformative opportunity to enhance urban mobility. This study explores the potential of LLMs to revolutionize public transportation management within the context of San Antonioâ€™s transit system. Leveraging the capabilities of LLMs in natural language processing and data analysis, we investigate their capabilities to optimize route planning, reduce wait times, and provide personalized travel assistance. By utilizing the General Transit Feed Specification (GTFS) and other relevant data, this research aims to demonstrate how LLMs can potentially improve resource allocation, elevate passenger satisfaction, and inform data-driven decision-making in transit operations. A comparative analysis of different ChatGPT models was conducted to assess their ability to understand transportation information, retrieve relevant data, and provide comprehensive responses. Findings from this study suggest that while LLMs hold immense promise for public transit, careful engineering and fine-tuning are essential to realizing their full potential. San Antonio serves as a case study to inform the development of LLM-powered transit systems in other urban environments. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°å…¬å…±äº¤é€šç³»ç»Ÿä¸­ï¼Œä¸ºæå‡åŸå¸‚æµåŠ¨æ€§å¸¦æ¥äº†å˜é©æ€§çš„æœºä¼šã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†LLMåœ¨åœ£å®‰ä¸œå°¼å¥¥å…¬å…±äº¤é€šç³»ç»ŸèƒŒæ™¯ä¸‹é©æ–°å…¬å…±äº¤é€šç®¡ç†çš„æ½œåŠ›ã€‚é€šè¿‡åˆ©ç”¨LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ•°æ®åˆ†ææ–¹é¢çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å®ƒä»¬ä¼˜åŒ–è·¯çº¿è§„åˆ’ã€å‡å°‘ç­‰å¾…æ—¶é—´å¹¶æä¾›ä¸ªæ€§åŒ–æ—…è¡Œå¸®åŠ©çš„èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨é€šç”¨äº¤é€šé¦ˆé€è§„èŒƒï¼ˆGTFSï¼‰å’Œå…¶ä»–ç›¸å…³æ•°æ®ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å±•ç¤ºLLMå¦‚ä½•å¯èƒ½åœ°æ”¹å–„èµ„æºåˆ†é…ã€æé«˜ä¹˜å®¢æ»¡æ„åº¦å¹¶ä¸ºåŸºäºæ•°æ®çš„äº¤é€šè¿è¥å†³ç­–æä¾›ä¿¡æ¯ã€‚å¯¹ä¸åŒçš„ChatGPTæ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”åˆ†æï¼Œä»¥è¯„ä¼°å®ƒä»¬ç†è§£äº¤é€šä¿¡æ¯ã€æ£€ç´¢ç›¸å…³æ•°æ®å¹¶æä¾›å…¨é¢å“åº”çš„èƒ½åŠ›ã€‚è¿™é¡¹ç ”ç©¶çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶LLMåœ¨å…¬å…±äº¤é€šæ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ï¼Œä½†è¦å®ç°å…¶å…¨éƒ¨æ½œåŠ›ï¼Œè¿˜éœ€è¦ç²¾ç»†çš„å·¥ç¨‹è®¾è®¡å’Œè°ƒæ•´ã€‚åœ£å®‰ä¸œå°¼å¥¥çš„æ¡ˆä¾‹ç ”ç©¶ä¸ºåœ¨å…¶ä»–åŸå¸‚ç¯å¢ƒä¸­å¼€å‘LLMé©±åŠ¨çš„äº¤é€šç³»ç»Ÿæä¾›äº†å‚è€ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03904v1">PDF</a> This work is accepted to AAAI 2025 Workshop on AI for Urban Planning.   arXiv admin note: substantial text overlap with arXiv:2407.11003</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èå…¥å…¬å…±äº¤é€šç³»ç»Ÿä¸ºæå‡åŸå¸‚å‡ºè¡Œæ•ˆç‡å¸¦æ¥é©å‘½æ€§æœºé‡ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†LLMåœ¨åœ£å®‰ä¸œå°¼å¥¥å…¬å…±äº¤é€šç³»ç»Ÿä¸­çš„æ½œåŠ›ï¼Œå¹¶å±•å¼€æ·±å…¥å¯¹æ¯”åˆ†æçš„ChatGPTæ¨¡å‹ä»¥è¯„ä¼°å…¶åœ¨è¿è¾“æ•°æ®å¤„ç†èƒ½åŠ›æ–¹é¢æ˜¯å¦å“è¶Šã€‚é€šè¿‡GTFSæ•°æ®å’Œå®é™…çŠ¶å†µï¼Œè¯¥ç ”ç©¶è¡¨æ˜LLMå¯ä»¥æ”¹å–„èµ„æºåˆ†é…ã€æé«˜ä¹˜å®¢æ»¡æ„åº¦å¹¶ä¸ºå†³ç­–åˆ¶å®šæä¾›æ•°æ®æ”¯æŒã€‚ç„¶è€Œï¼Œè¦å‘æŒ¥LLMçš„æœ€å¤§æ½œåŠ›ï¼Œéœ€è¦ç»†è‡´çš„å·¥ç¨‹è®¾è®¡å’Œç²¾ç»†è°ƒæ•´ã€‚è¿™ä¸€æ¡ˆä¾‹ç ”ç©¶å¯ä¸ºå…¶ä»–åŸå¸‚å¼€å‘LLMé©±åŠ¨çš„å…¬å…±äº¤é€šç³»ç»Ÿæä¾›å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsæœ‰æ½œåŠ›æ”¹å˜å…¬å…±äº¤é€šç³»ç»Ÿçš„ç®¡ç†æ–¹å¼ï¼Œå¢å¼ºåŸå¸‚æµåŠ¨æ€§ã€‚</li>
<li>åœ¨åœ£å®‰ä¸œå°¼å¥¥çš„å…¬å…±äº¤é€šç³»ç»Ÿä¸­ï¼ŒLLMsçš„åº”ç”¨å¯ä»¥è¿›è¡Œè·¯çº¿ä¼˜åŒ–ã€å‡å°‘ç­‰å¾…æ—¶é—´å¹¶æä¾›ä¸ªæ€§åŒ–æ—…è¡ŒæœåŠ¡ã€‚</li>
<li>é€šè¿‡GTFSæ•°æ®å’Œå…¶ä»–ç›¸å…³æ•°æ®ï¼ŒLLMså¯ä»¥æ”¹å–„èµ„æºåˆ†é…å’Œæé«˜ä¹˜å®¢æ»¡æ„åº¦ã€‚</li>
<li>LLMsåœ¨ç†è§£å’Œå¤„ç†è¿è¾“ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†éœ€è¦è¿›è¡Œç»†è‡´çš„å·¥ç¨‹è®¾è®¡å’Œç²¾ç»†è°ƒæ•´æ‰èƒ½å®ç°å…¶æœ€å¤§æ•ˆç”¨ã€‚</li>
<li>LLMsçš„åº”ç”¨å¯ä»¥ä¸ºæ•°æ®é©±åŠ¨çš„å†³ç­–æä¾›ä¿¡æ¯æ”¯æŒã€‚</li>
<li>é€šè¿‡åœ£å®‰ä¸œå°¼å¥¥çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå¯ä»¥ä¸ºå…¶ä»–åŸå¸‚å¼€å‘åŸºäºLLMçš„å…¬å…±äº¤é€šç³»ç»Ÿæä¾›å‚è€ƒå’Œå¯ç¤ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2bbd23868ab2d719720175e928bb734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d40d4f4b4257621e605e54111e5fbfd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcfb30459f0ee42640f068298a157057.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60632ff69dcb2fdf08a762ce10865b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ffce7def566a37b1ffd3d291c954c08.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LLaVA-Mini-Efficient-Image-and-Video-Large-Multimodal-Models-with-One-Vision-Token"><a href="#LLaVA-Mini-Efficient-Image-and-Video-Large-Multimodal-Models-with-One-Vision-Token" class="headerlink" title="LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One   Vision Token"></a>LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One   Vision Token</h2><p><strong>Authors:Shaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng</strong></p>
<p>The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory. </p>
<blockquote>
<p>å®æ—¶å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰å¦‚GPT-4oçš„å‡ºç°ï¼Œå¼•å‘äº†äººä»¬å¯¹é«˜æ•ˆLMMçš„æå¤§å…´è¶£ã€‚LMMæ¡†æ¶é€šå¸¸å°†è§†è§‰è¾“å…¥ç¼–ç ä¸ºè§†è§‰ä»¤ç‰Œï¼ˆè¿ç»­è¡¨ç¤ºï¼‰ï¼Œå¹¶å°†å®ƒä»¬å’Œæ–‡æœ¬æŒ‡ä»¤é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡ä¸­ã€‚å¤§è§„æ¨¡çš„å‚æ•°å’Œä¼—å¤šçš„ä¸Šä¸‹æ–‡ä»¤ç‰Œï¼ˆä¸»è¦æ˜¯è§†è§‰ä»¤ç‰Œï¼‰å¯¼è‡´äº†å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚å…ˆå‰å¯¹é«˜æ•ˆLMMçš„åŠªåŠ›æ€»æ˜¯é›†ä¸­åœ¨ç”¨è¾ƒå°çš„æ¨¡å‹æ›¿æ¢LLMä¸»å¹²ï¼Œè€Œå¿½è§†äº†ä»¤ç‰Œæ•°é‡è¿™ä¸€å…³é”®é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LLaVA-Miniï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰æœ€å°‘è§†è§‰ä»¤ç‰Œçš„é«˜æ•ˆLMMã€‚ä¸ºäº†å®ç°é«˜å‹ç¼©æ¯”çš„è§†è§‰ä»¤ç‰ŒåŒæ—¶ä¿ç•™è§†è§‰ä¿¡æ¯ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†æLMMå¦‚ä½•ç†è§£è§†è§‰ä»¤ç‰Œï¼Œå¹¶å‘ç°å¤§å¤šæ•°è§†è§‰ä»¤ç‰Œä»…åœ¨LLMä¸»å¹²çš„æ—©æœŸå±‚ä¸­å‘æŒ¥å…³é”®ä½œç”¨ï¼Œå®ƒä»¬ä¸»è¦æ˜¯å°†è§†è§‰ä¿¡æ¯èåˆåˆ°æ–‡æœ¬ä»¤ç‰Œä¸­ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼ŒLLaVA-Miniå¼•å…¥äº†æ¨¡æ€é¢„èåˆï¼Œé¢„å…ˆå°†è§†è§‰ä¿¡æ¯èåˆåˆ°æ–‡æœ¬ä»¤ç‰Œä¸­ï¼Œä»è€Œä¾¿äºå°†LLMä¸»å¹²æ‰€æ¥æ”¶çš„è§†è§‰ä»¤ç‰Œå‹ç¼©åˆ°ä¸€ä¸ªä»¤ç‰Œå†…ã€‚LLaVA-Miniæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¯ä»¥ä»¥é«˜æ•ˆçš„æ–¹å¼æ”¯æŒå›¾åƒã€é«˜åˆ†è¾¨ç‡å›¾åƒå’Œè§†é¢‘çš„ç†è§£ã€‚åœ¨11ä¸ªåŸºäºå›¾åƒå’Œ7ä¸ªåŸºäºè§†é¢‘çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLLaVA-Miniçš„æ€§èƒ½ä¼˜äºä½¿ç”¨576ä¸ªè§†è§‰ä»¤ç‰Œè€Œéä»…ä½¿ç”¨1ä¸ªä»¤ç‰Œçš„LLaVA-v1.5ã€‚æ•ˆç‡åˆ†ææ˜¾ç¤ºï¼ŒLLaVA-Miniå¯ä»¥å‡å°‘77%çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ˆFLOPsï¼‰ï¼Œåœ¨40æ¯«ç§’å†…å®ç°ä½å»¶è¿Ÿå“åº”ï¼Œå¹¶åœ¨å…·æœ‰24GBå†…å­˜çš„GPUç¡¬ä»¶ä¸Šå¤„ç†è¶…è¿‡1ä¸‡ä¸ªè§†é¢‘å¸§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03895v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/LLaVA-Mini">https://github.com/ictnlp/LLaVA-Mini</a>; Model:   <a target="_blank" rel="noopener" href="https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b">https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LLaVA-Miniè¿™ä¸€é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹ã€‚å®ƒé€šè¿‡é¢„å…ˆèåˆè§†è§‰ä¿¡æ¯ï¼Œå°†è§†è§‰ä»¤ç‰Œï¼ˆtokensï¼‰å‹ç¼©åˆ°ä¸€ä¸ªä»¤ç‰Œä¸­ï¼Œå¹¶ç®€åŒ–äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—é‡ã€‚å®éªŒç»“æœè¯æ˜äº†LLaVA-Miniç›¸è¾ƒäºå…ˆå‰çš„æ¨¡å‹æœ‰æ›´é«˜æ•ˆçš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ã€‚å…¶åœ¨å¤„ç†è§†é¢‘ä»»åŠ¡æ—¶å…·å¤‡é«˜åº¦ä¼˜åŒ–çš„èƒ½åŠ›ï¼Œåœ¨GPUç¡¬ä»¶ä¸Šå®ç°å¿«é€Ÿå“åº”å’Œå¤§é‡å¤„ç†èƒ½åŠ›ã€‚LLaVA-Minièƒ½åœ¨æä¾›æœ‰æ•ˆç»“æœçš„åŒæ—¶å¤§å¤§å‡å°‘è®¡ç®—èµ„æºå’Œæ—¶é—´çš„æ¶ˆè€—ã€‚å®ƒä¸ä»…å‡å°‘æµ®ç‚¹è¿ç®—é‡è¾¾77%ï¼Œå¹¶ä¸”æ”¯æŒåœ¨24GBå†…å­˜ç¯å¢ƒä¸‹å¤„ç†è¶…è¿‡ä¸€ä¸‡å¸§çš„è§†é¢‘æ•°æ®ï¼Œå¹¶ä¸”æä¾›å¿«é€Ÿçš„å“åº”æ—¶é—´ï¼ˆä½äº40æ¯«ç§’ï¼‰ã€‚æ€»çš„æ¥è¯´ï¼ŒLLaVA-Miniæ˜¯ä¸€ä¸ªé«˜æ•ˆã€å¿«é€Ÿçš„å¤šæ¨¡æ€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡çš„å…³é”®è§è§£è¦ç‚¹ï¼š</p>
<ol>
<li><p>LLaVA-Miniæ˜¯ä¸€ç§é«˜æ•ˆçš„å®æ—¶å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¼•å…¥é¢„èåˆç­–ç•¥ä»¥å‡å°‘è®¡ç®—é‡å’Œä»¤ç‰Œæ•°é‡ã€‚å®ƒèƒ½å¤Ÿç®€åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿ç®—å¤æ‚åº¦ï¼Œå¤§å¹…æé«˜æ¨¡å‹çš„æ‰§è¡Œæ•ˆç‡ã€‚æ­¤æ¨¡å‹ä½¿å¾—å¤æ‚çš„å›¾åƒç†è§£å’Œè§†é¢‘ç†è§£ä»»åŠ¡å˜å¾—æ›´ä¸ºé«˜æ•ˆã€‚</p>
</li>
<li><p>LLaVA-Minié€šè¿‡é¢„å…ˆèåˆè§†è§‰ä¿¡æ¯åˆ°æ–‡æœ¬ä»¤ç‰Œä¸­ï¼Œå¤§å¹…å‡å°‘äº†å¯¹è§†è§‰ä»¤ç‰Œçš„éœ€æ±‚ã€‚å¤§å¤šæ•°è§†è§‰ä»¤ç‰Œä»…åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ—©æœŸå±‚æ¬¡ä¸­å‘æŒ¥å…³é”®ä½œç”¨ï¼Œæ­¤å‘ç°æˆä¸ºè®¾è®¡LLaVA-Miniçš„åŸºç¡€åŸç†ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç”¨ä¸€ä¸ªä»¤ç‰Œæ›¿ä»£ä¼ ç»Ÿçš„æ•°ç™¾ä¸ªè§†è§‰ä»¤ç‰Œï¼Œæå¤§æå‡äº†æ•ˆç‡ã€‚</p>
</li>
<li><p>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLaVA-Miniåœ¨å›¾åƒå’Œè§†é¢‘çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒå¤„ç†çš„é€Ÿåº¦å’Œæ•ˆç‡ä¸Šç›¸è¾ƒäºå…¶ä»–ç‰ˆæœ¬æœ‰ç€æ˜¾è‘—æå‡ã€‚å®ƒé€šè¿‡ç®€æ´çš„è®¡ç®—é‡å’Œæ—¶é—´æ¶ˆè€—å±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å³ä½¿åœ¨å¤æ‚ä»»åŠ¡å¦‚å¤„ç†é«˜æ¸…æ™°åº¦å›¾åƒå’Œè§†é¢‘æ—¶ä¹Ÿèƒ½ç»´æŒé«˜æ•ˆè¡¨ç°ã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d8716729b2c68de4eed58d1b3c2ade76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c186918d43238d43abfe2e44ecf9372b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f447a80fb87a0ba522793e4b67d643a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b65d56aae258b5881132faaae688df86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-988e93e024773eca474d7ad9b7390523.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AlphaPO-â€“-Reward-shape-matters-for-LLM-alignment"><a href="#AlphaPO-â€“-Reward-shape-matters-for-LLM-alignment" class="headerlink" title="AlphaPO â€“ Reward shape matters for LLM alignment"></a>AlphaPO â€“ Reward shape matters for LLM alignment</h2><p><strong>Authors:Aman Gupta, Shao Tang, Qingquan Song, Sirou Zhu, Jiwoo Hong, Ankan Saha, Viral Gupta, Noah Lee, Eunki Kim, Jason Zhu, Natesh Pillai, S. Sathiya Keerthi</strong></p>
<p>Reinforcement Learning with Human Feedback (RLHF) and its variants have made huge strides toward the effective alignment of large language models (LLMs) to follow instructions and reflect human values. More recently, Direct Alignment Algorithms (DAAs) have emerged in which the reward modeling stage of RLHF is skipped by characterizing the reward directly as a function of the policy being learned. Examples include Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO). These methods often suffer from likelihood displacement, a phenomenon by which the probabilities of preferred responses are often reduced undesirably.   In this paper, we argue that, for DAAs the reward (function) shape matters. We introduce AlphaPO, a new DAA method that leverages an $\alpha$-parameter to help change the shape of the reward function beyond the standard log reward. AlphaPO helps maintain fine-grained control over likelihood displacement and over-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO leads to about 7% to 10% relative improvement in alignment performance for the instruct versions of Mistral-7B and Llama3-8B. The analysis and results presented highlight the importance of the reward shape, and how one can systematically change it to affect training dynamics, as well as improve alignment performance. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ç»“åˆäººç±»åé¦ˆï¼ˆRLHFï¼‰åŠå…¶å˜ä½“åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰æ•ˆå¯¹é½ä»¥éµå¾ªæŒ‡ä»¤å’Œåæ˜ äººç±»ä»·å€¼è§‚æ–¹é¢å–å¾—äº†å·¨å¤§è¿›æ­¥ã€‚æœ€è¿‘ï¼Œç›´æ¥å¯¹é½ç®—æ³•ï¼ˆDAAï¼‰åº”è¿è€Œç”Ÿï¼Œå…¶ä¸­çœç•¥äº†RLHFçš„å¥–åŠ±å»ºæ¨¡é˜¶æ®µï¼Œé€šè¿‡å°†å¥–åŠ±ç›´æ¥è¡¨å¾ä¸ºæ­£åœ¨å­¦ä¹ çš„ç­–ç•¥çš„å‡½æ•°æ¥å®ç°ã€‚ä¾‹å¦‚ï¼ŒåŒ…æ‹¬ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç®€å•åå¥½ä¼˜åŒ–ï¼ˆSimPOï¼‰ã€‚è¿™äº›æ–¹æ³•å¸¸å¸¸é­å—å¯èƒ½æ€§ä½ç§»ï¼ˆlikelihood displacementï¼‰çš„é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§ç°è±¡ï¼Œç”±æ­¤äº§ç”Ÿçš„ä¼˜é€‰å“åº”çš„æ¦‚ç‡å¾€å¾€è¢«ä¸å¿…è¦åœ°é™ä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸»å¼ å¯¹äºDAAsæ¥è¯´ï¼Œå¥–åŠ±ï¼ˆå‡½æ•°ï¼‰çš„å½¢çŠ¶å¾ˆé‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†AlphaPOï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„DAAæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ä¸€ä¸ªÎ±å‚æ•°æ¥å¸®åŠ©æ”¹å˜è¶…å‡ºæ ‡å‡†å¯¹æ•°å¥–åŠ±çš„å¥–åŠ±å‡½æ•°çš„å½¢çŠ¶ã€‚AlphaPOæœ‰åŠ©äºå¯¹å¯èƒ½æ€§ä½ç§»å’Œè¿‡åº¦ä¼˜åŒ–è¿›è¡Œç²¾ç»†æ§åˆ¶ã€‚ä¸è¡¨ç°æœ€ä½³çš„DAAä¹‹ä¸€SimPOç›¸æ¯”ï¼ŒAlphaPOåœ¨å¯¹Mistral-7Bå’ŒLlama3-8Bçš„æŒ‡ä»¤ç‰ˆæœ¬çš„å¯¹é½æ€§èƒ½ä¸Šå¸¦æ¥äº†çº¦7%åˆ°10%çš„ç›¸å¯¹æ”¹è¿›ã€‚æ‰€å‘ˆç°çš„åˆ†æå’Œç»“æœçªå‡ºäº†å¥–åŠ±å½¢çŠ¶çš„é‡è¦æ€§ï¼Œä»¥åŠå¦‚ä½•ç³»ç»Ÿåœ°æ”¹å˜å®ƒæ¥å½±å“è®­ç»ƒåŠ¨æ€å¹¶æé«˜å¯¹é½æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03884v1">PDF</a> Preprint. Work in progress</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰åŠå…¶å˜ä½“åœ¨å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆå¯¹é½ä»¥éµå¾ªæŒ‡ä»¤å’Œåæ˜ äººç±»ä»·å€¼è§‚æ–¹é¢å–å¾—äº†å·¨å¤§è¿›å±•ã€‚æœ€è¿‘ï¼Œç›´æ¥å¯¹é½ç®—æ³•ï¼ˆDAAï¼‰çš„å‡ºç°è·³è¿‡äº†RLHFçš„å¥–åŠ±å»ºæ¨¡é˜¶æ®µï¼Œç›´æ¥å°†å¥–åŠ±è¡¨å¾ä¸ºæ­£åœ¨å­¦ä¹ çš„ç­–ç•¥çš„å‡½æ•°ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸å—åˆ°å‡ ç‡ä½ç§»ç°è±¡çš„å½±å“ï¼Œä½¿å¾—é¦–é€‰å“åº”çš„æ¦‚ç‡ä¸æ°å½“åœ°é™ä½ã€‚æœ¬æ–‡æå‡ºAlphaPOï¼Œä¸€ç§æ–°å‹DAAæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨Î±å‚æ•°æ”¹å˜å¥–åŠ±å‡½æ•°çš„å½¢çŠ¶ï¼Œå¸®åŠ©ç²¾ç»†æ§åˆ¶å‡ ç‡ä½ç§»å’Œè¿‡åº¦ä¼˜åŒ–ã€‚ç›¸è¾ƒäºè¡¨ç°æœ€ä½³çš„DAAä¹‹ä¸€SimPOï¼ŒAlphaPOåœ¨å¯¹é½æ€§èƒ½ä¸Šç›¸å¯¹æå‡äº†çº¦7%åˆ°10%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç›´æ¥å¯¹é½ç®—æ³•ï¼ˆDAAï¼‰è·³è¿‡å¥–åŠ±å»ºæ¨¡é˜¶æ®µï¼Œç›´æ¥è¡¨å¾å¥–åŠ±ä¸ºæ­£åœ¨å­¦ä¹ çš„ç­–ç•¥çš„å‡½æ•°ã€‚</li>
<li>DAAæ–¹æ³•å¸¸é¢ä¸´å‡ ç‡ä½ç§»é—®é¢˜ï¼Œå³é¦–é€‰å“åº”çš„æ¦‚ç‡é™ä½ã€‚</li>
<li>AlphaPOæ˜¯ä¸€ç§æ–°å‹DAAæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨Î±å‚æ•°æ”¹å˜å¥–åŠ±å‡½æ•°å½¢çŠ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>AlphaPOç›¸è¾ƒäºSimPOåœ¨å¯¹é½æ€§èƒ½ä¸Šæœ‰ç›¸å¯¹æå‡ã€‚</li>
<li>å¥–åŠ±å‡½æ•°çš„å½¢çŠ¶å¯¹è®­ç»ƒåŠ¨æ€å’Œå¯¹é½æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9846fa8282d49f136829f6c4bb85926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a023f367d7e56a4047b6a06316ceb065.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CL3DOR-Contrastive-Learning-for-3D-Large-Multimodal-Models-via-Odds-Ratio-on-High-Resolution-Point-Clouds"><a href="#CL3DOR-Contrastive-Learning-for-3D-Large-Multimodal-Models-via-Odds-Ratio-on-High-Resolution-Point-Clouds" class="headerlink" title="CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds   Ratio on High-Resolution Point Clouds"></a>CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds   Ratio on High-Resolution Point Clouds</h2><p><strong>Authors:Keonwoo Kim, Yeongjae Cho, Taebaek Hwang, Minsoo Jo, Sangdo Han</strong></p>
<p>Recent research has demonstrated that Large Language Models (LLMs) are not limited to text-only tasks but can also function as multimodal models across various modalities, including audio, images, and videos. In particular, research on 3D Large Multimodal Models (3D LMMs) is making notable strides, driven by the potential of processing higher-dimensional data like point clouds. However, upon closer examination, we find that the visual and textual content within each sample of existing training datasets lacks both high informational granularity and clarity, which serve as a bottleneck for precise cross-modal understanding. To address these issues, we propose CL3DOR, Contrastive Learning for 3D large multimodal models via Odds ratio on high-Resolution point clouds, designed to ensure greater specificity and clarity in both visual and textual content. Specifically, we increase the density of point clouds per object and construct informative hard negative responses in the training dataset to penalize unwanted responses. To leverage hard negative responses, we incorporate the odds ratio as an auxiliary term for contrastive learning into the conventional language modeling loss. CL3DOR achieves state-of-the-art performance in 3D scene understanding and reasoning benchmarks. Additionally, we demonstrate the effectiveness of CL3DORâ€™s key components through extensive experiments. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ä»…é™äºæ–‡æœ¬ä»»åŠ¡ï¼Œè¿˜å¯ä»¥ä½œä¸ºè·¨å¤šç§æ¨¡æ€çš„å¤šåª’ä½“æ¨¡å‹ï¼ŒåŒ…æ‹¬éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘ã€‚ç‰¹åˆ«æ˜¯ï¼Œå…³äº3Då¤§å‹å¤šåª’ä½“æ¨¡å‹ï¼ˆ3D LMMï¼‰çš„ç ”ç©¶æ­£åœ¨å–å¾—æ˜¾è‘—è¿›å±•ï¼Œè¿™å¾—ç›Šäºå¤„ç†ç‚¹äº‘ç­‰é«˜ç»´æ•°æ®çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç»è¿‡ä»”ç»†è§‚å¯Ÿï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰è®­ç»ƒæ•°æ®é›†ä¸­çš„æ ·æœ¬åœ¨è§†è§‰å’Œæ–‡æœ¬å†…å®¹æ–¹é¢ç¼ºä¹é«˜åº¦ä¿¡æ¯ç²’åº¦å’Œæ¸…æ™°åº¦ï¼Œè¿™æˆä¸ºç²¾ç¡®è·¨æ¨¡æ€ç†è§£çš„ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CL3DORï¼Œå³åŸºäºé«˜åˆ†è¾¨ç‡ç‚¹äº‘çš„3Då¤§å‹å¤šåª’ä½“æ¨¡å‹çš„å¯¹æ¯”å­¦ä¹ æ³•ï¼ˆContrastive Learning for 3D large multimodal models via Odds ratioï¼‰ã€‚å®ƒæ—¨åœ¨ç¡®ä¿è§†è§‰å’Œæ–‡æœ¬å†…å®¹å…·æœ‰æ›´é«˜çš„ç‰¹å¼‚æ€§å’Œæ¸…æ™°åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¢åŠ äº†æ¯ä¸ªå¯¹è±¡çš„ç‚¹äº‘å¯†åº¦ï¼Œå¹¶åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ„å»ºæœ‰ä¿¡æ¯é‡çš„ç¡¬è´Ÿå“åº”æ¥æƒ©ç½šä¸æƒ³è¦çš„å“åº”ã€‚ä¸ºäº†åˆ©ç”¨ç¡¬è´Ÿå“åº”ï¼Œæˆ‘ä»¬å°†æ¯”å€¼ä½œä¸ºå¯¹æ¯”å­¦ä¹ çš„è¾…åŠ©æœ¯è¯­ï¼Œçº³å…¥ä¼ ç»Ÿè¯­è¨€å»ºæ¨¡æŸå¤±ä¸­ã€‚CL3DORåœ¨3Dåœºæ™¯ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†CL3DORå…³é”®ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03879v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ä»…èƒ½å®Œæˆæ–‡æœ¬ä»»åŠ¡ï¼Œè¿˜èƒ½ä½œä¸ºè·¨å¤šç§æ¨¡å¼çš„å¤šæ¨¡æ€æ¨¡å‹å¤„ç†éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘ã€‚ç‰¹åˆ«æ˜¯å…³äºä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆ3D LMMsï¼‰çš„ç ”ç©¶æ­£åœ¨å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå…¶å¤„ç†ç‚¹äº‘ç­‰æ›´é«˜ç»´åº¦æ•°æ®çš„èƒ½åŠ›å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰è®­ç»ƒæ•°æ®é›†æ ·æœ¬ä¸­çš„è§†è§‰å’Œæ–‡æœ¬å†…å®¹ç¼ºä¹é«˜ä¿¡æ¯ç²’åº¦å’Œæ¸…æ™°åº¦ï¼Œæˆä¸ºç²¾ç¡®è·¨æ¨¡æ€ç†è§£çš„ç“¶é¢ˆã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºCL3DORæ–¹æ³•ï¼Œé€šè¿‡é«˜åˆ†è¾¨ç‡ç‚¹äº‘çš„èµ”ç‡æ¯”ç‡è¿›è¡Œä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å¯¹æ¯”å­¦ä¹ ï¼Œç¡®ä¿è§†è§‰å’Œæ–‡æœ¬å†…å®¹æ›´å…·ç‰¹å¼‚æ€§å’Œæ¸…æ™°åº¦ã€‚CL3DORæ–¹æ³•å¢åŠ ç‚¹äº‘å¯¹è±¡çš„å¯†åº¦ï¼Œæ„å»ºè®­ç»ƒé›†ä¸­çš„ä¿¡æ¯æ€§ç¡¬è´Ÿå“åº”æ¥æƒ©ç½šä¸æƒ³è¦çš„å“åº”ã€‚å€ŸåŠ©ç¡¬è´Ÿå“åº”ï¼Œæˆ‘ä»¬å°†èµ”ç‡æ¯”ç‡ä½œä¸ºå¯¹æ¯”å­¦ä¹ çš„è¾…åŠ©æœ¯è¯­çº³å…¥å¸¸è§„è¯­è¨€å»ºæ¨¡æŸå¤±ä¸­ã€‚CL3DORåœ¨ä¸‰ç»´åœºæ™¯ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶é€šè¿‡å¤§é‡å®éªŒè¯æ˜äº†å…¶å…³é”®ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ä»…èƒ½å¤„ç†æ–‡æœ¬ä»»åŠ¡ï¼Œè¿˜èƒ½ä½œä¸ºå¤šæ¨¡æ€æ¨¡å‹å¤„ç†éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘ã€‚</li>
<li>ä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆ3D LMMsï¼‰ç ”ç©¶æ­£åœ¨å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶åœ¨å¤„ç†é«˜ç»´åº¦æ•°æ®å¦‚ç‚¹äº‘æ–¹é¢ã€‚</li>
<li>ç°æœ‰è®­ç»ƒæ•°æ®é›†å­˜åœ¨è§†è§‰å’Œæ–‡æœ¬å†…å®¹ç¼ºä¹é«˜ä¿¡æ¯ç²’åº¦å’Œæ¸…æ™°åº¦çš„é—®é¢˜ï¼Œè¿™æ˜¯ç²¾ç¡®è·¨æ¨¡æ€ç†è§£çš„ç“¶é¢ˆã€‚</li>
<li>CL3DORæ–¹æ³•é€šè¿‡å¢åŠ ç‚¹äº‘å¯¹è±¡çš„å¯†åº¦å’Œæ„å»ºä¿¡æ¯æ€§ç¡¬è´Ÿå“åº”æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>CL3DORå°†èµ”ç‡æ¯”ç‡çº³å…¥å¸¸è§„è¯­è¨€å»ºæ¨¡æŸå¤±ä¸­ï¼Œä»¥æé«˜æ¨¡å‹çš„ç‰¹å¼‚æ€§å’Œæ¸…æ™°åº¦ã€‚</li>
<li>CL3DORæ–¹æ³•åœ¨ä¸‰ç»´åœºæ™¯ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8881038bdf1eb6661224e6b635501da2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18c1601e6e4cc438e3d578658427d357.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e705c313c18dabadcf23fb518acf88f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b265c246d046d7b670ba3da6b186de3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6a6093800ba930987630534206ad92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b6a8722dcdfaf948f633f3d41e8095d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Progressive-Document-level-Text-Simplification-via-Large-Language-Models"><a href="#Progressive-Document-level-Text-Simplification-via-Large-Language-Models" class="headerlink" title="Progressive Document-level Text Simplification via Large Language Models"></a>Progressive Document-level Text Simplification via Large Language Models</h2><p><strong>Authors:Dengzhao Fang, Jipeng Qiang, Yi Zhu, Yunhao Yuan, Wei Li, Yan Liu</strong></p>
<p>Research on text simplification has primarily focused on lexical and sentence-level changes. Long document-level simplification (DS) is still relatively unexplored. Large Language Models (LLMs), like ChatGPT, have excelled in many natural language processing tasks. However, their performance on DS tasks is unsatisfactory, as they often treat DS as merely document summarization. For the DS task, the generated long sequences not only must maintain consistency with the original document throughout, but complete moderate simplification operations encompassing discourses, sentences, and word-level simplifications. Human editors employ a hierarchical complexity simplification strategy to simplify documents. This study delves into simulating this strategy through the utilization of a multi-stage collaboration using LLMs. We propose a progressive simplification method (ProgDS) by hierarchically decomposing the task, including the discourse-level, topic-level, and lexical-level simplification. Experimental results demonstrate that ProgDS significantly outperforms existing smaller models or direct prompting with LLMs, advancing the state-of-the-art in the document simplification task. </p>
<blockquote>
<p>å…³äºæ–‡æœ¬ç®€åŒ–çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è¯æ±‡å’Œå¥å­å±‚é¢çš„æ”¹å˜ä¸Šã€‚é•¿æ–‡æœ¬çº§åˆ«çš„ç®€åŒ–ï¼ˆDSï¼‰ä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚ChatGPTï¼Œåœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨DSä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸ä»¤äººæ»¡æ„ï¼Œå› ä¸ºå®ƒä»¬å¾€å¾€å°†DSä»…ä»…è§†ä¸ºæ–‡æ¡£æ‘˜è¦ã€‚å¯¹äºDSä»»åŠ¡ï¼Œç”Ÿæˆçš„é•¿åºåˆ—ä¸ä»…éœ€è¦ä¸åŸå§‹æ–‡æ¡£ä¿æŒä¸€è‡´ï¼Œè¿˜å¿…é¡»åŒ…å«æ¶µç›–æ®µè½ã€å¥å­å’Œè¯æ±‡çº§åˆ«çš„é€‚åº¦ç®€åŒ–æ“ä½œã€‚äººç±»ç¼–è¾‘é‡‡ç”¨åˆ†å±‚å¤æ‚ç®€åŒ–ç­–ç•¥æ¥ç®€åŒ–æ–‡æ¡£ã€‚æœ¬ç ”ç©¶è‡´åŠ›äºé€šè¿‡åˆ©ç”¨å¤šé˜¶æ®µåä½œæ¥æ¨¡æ‹Ÿè¿™ä¸€ç­–ç•¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚åˆ†è§£ä»»åŠ¡çš„æ¸è¿›ç®€åŒ–æ–¹æ³•ï¼ˆProgDSï¼‰ï¼ŒåŒ…æ‹¬ç¯‡ç« çº§åˆ«ã€ä¸»é¢˜çº§åˆ«å’Œè¯æ±‡çº§åˆ«çš„ç®€åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProgDSæ˜¾è‘—ä¼˜äºç°æœ‰çš„å°å‹æ¨¡å‹æˆ–ç›´æ¥ç”¨LLMè¿›è¡Œæç¤ºçš„æ–¹æ³•ï¼Œåœ¨æ–‡æ¡£ç®€åŒ–ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03857v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è¯æ±‡å’Œå¥å­å±‚é¢çš„ç®€åŒ–ï¼Œè€Œé•¿æ–‡æœ¬å±‚é¢çš„ç®€åŒ–ï¼ˆDSï¼‰ä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ChatGPTåœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨DSä»»åŠ¡ä¸Šçš„è¡¨ç°å´ä¸å°½äººæ„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¸è¿›å¼ç®€åŒ–æ–¹æ³•ï¼ˆProgDSï¼‰ï¼Œé€šè¿‡å±‚æ¬¡åˆ†è§£ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç¯‡ç« å±‚é¢ã€ä¸»é¢˜å±‚é¢å’Œè¯æ±‡å±‚é¢çš„ç®€åŒ–ï¼Œæ¥æ¨¡æ‹Ÿäººç±»ç¼–è¾‘çš„å±‚æ¬¡å¤æ‚æ€§ç®€åŒ–ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProgDSæ˜¾è‘—ä¼˜äºç°æœ‰çš„å°å‹æ¨¡å‹æˆ–ç›´æ¥åœ¨LLMsä¸­çš„æç¤ºï¼Œæ¨åŠ¨äº†æ–‡æ¡£ç®€åŒ–ä»»åŠ¡çš„æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è¡¨æ˜ï¼Œé•¿æ–‡æœ¬ç®€åŒ–ï¼ˆDSï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªç›¸å¯¹æœªè¢«æ¢ç´¢çš„é¢†åŸŸã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨DSä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸å°½äººæ„ï¼Œå› ä¸ºå®ƒä»¬å¾€å¾€å°†DSè§†ä¸ºç®€å•çš„æ–‡æ¡£æ‘˜è¦ã€‚</li>
<li>äººç±»ç¼–è¾‘é‡‡ç”¨å±‚æ¬¡å¤æ‚æ€§ç®€åŒ–ç­–ç•¥æ¥ç®€åŒ–æ–‡æ¡£ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¸è¿›å¼ç®€åŒ–æ–¹æ³•ï¼ˆProgDSï¼‰ï¼Œé€šè¿‡å±‚æ¬¡åˆ†è§£ä»»åŠ¡æ¥æ¨¡æ‹Ÿè¿™ç§ç­–ç•¥ã€‚</li>
<li>ProgDSåŒ…æ‹¬ç¯‡ç« å±‚é¢ã€ä¸»é¢˜å±‚é¢å’Œè¯æ±‡å±‚é¢çš„ç®€åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒProgDSåœ¨æ–‡æ¡£ç®€åŒ–ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f234ef38a279d21f0ddf4dcc874af400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b510225ad8a3834587c5059fad5f9d2b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="KAnoCLIP-Zero-Shot-Anomaly-Detection-through-Knowledge-Driven-Prompt-Learning-and-Enhanced-Cross-Modal-Integration"><a href="#KAnoCLIP-Zero-Shot-Anomaly-Detection-through-Knowledge-Driven-Prompt-Learning-and-Enhanced-Cross-Modal-Integration" class="headerlink" title="KAnoCLIP: Zero-Shot Anomaly Detection through Knowledge-Driven Prompt   Learning and Enhanced Cross-Modal Integration"></a>KAnoCLIP: Zero-Shot Anomaly Detection through Knowledge-Driven Prompt   Learning and Enhanced Cross-Modal Integration</h2><p><strong>Authors:Chengyuan Li, Suyang Zhou, Jieping Kong, Lei Qi, Hui Xue</strong></p>
<p>Zero-shot anomaly detection (ZSAD) identifies anomalies without needing training samples from the target dataset, essential for scenarios with privacy concerns or limited data. Vision-language models like CLIP show potential in ZSAD but have limitations: relying on manually crafted fixed textual descriptions or anomaly prompts is time-consuming and prone to semantic ambiguity, and CLIP struggles with pixel-level anomaly segmentation, focusing more on global semantics than local details. To address these limitations, We introduce KAnoCLIP, a novel ZSAD framework that leverages vision-language models. KAnoCLIP combines general knowledge from a Large Language Model (GPT-3.5) and fine-grained, image-specific knowledge from a Visual Question Answering system (Llama3) via Knowledge-Driven Prompt Learning (KnPL). KnPL uses a knowledge-driven (KD) loss function to create learnable anomaly prompts, removing the need for fixed text prompts and enhancing generalization. KAnoCLIP includes the CLIP visual encoder with V-V attention (CLIP-VV), Bi-Directional Cross-Attention for Multi-Level Cross-Modal Interaction (Bi-CMCI), and Conv-Adapter. These components preserve local visual semantics, improve local cross-modal fusion, and align global visual features with textual information, enhancing pixel-level anomaly detection. KAnoCLIP achieves state-of-the-art performance in ZSAD across 12 industrial and medical datasets, demonstrating superior generalization compared to existing methods. </p>
<blockquote>
<p>é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰èƒ½å¤Ÿåœ¨æ— éœ€ç›®æ ‡æ•°æ®é›†çš„è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹è¯†åˆ«å¼‚å¸¸å€¼ï¼Œå¯¹äºå­˜åœ¨éšç§æ‹…å¿§æˆ–æ•°æ®æœ‰é™çš„æƒ…å†µè‡³å…³é‡è¦ã€‚CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ZSADä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä¹Ÿå­˜åœ¨å±€é™æ€§ï¼šä¾èµ–æ‰‹åŠ¨åˆ¶ä½œçš„å›ºå®šæ–‡æœ¬æè¿°æˆ–å¼‚å¸¸æç¤ºæ—¢è€—æ—¶åˆå®¹æ˜“äº§ç”Ÿè¯­ä¹‰æ­§ä¹‰ï¼ŒCLIPåœ¨åƒç´ çº§å¼‚å¸¸åˆ†å‰²æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæ›´ä¾§é‡äºå…¨å±€è¯­ä¹‰è€Œéå±€éƒ¨ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†KAnoCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°å‹ZSADæ¡†æ¶ã€‚KAnoCLIPé€šè¿‡çŸ¥è¯†é©±åŠ¨æç¤ºå­¦ä¹ ï¼ˆKnPLï¼‰ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT-3.5ï¼‰çš„é€šç”¨çŸ¥è¯†å’Œæ¥è‡ªè§†è§‰é—®ç­”ç³»ç»Ÿï¼ˆLlama3ï¼‰çš„ç²¾ç»†å›¾åƒç‰¹å®šçŸ¥è¯†ã€‚KnPLä½¿ç”¨çŸ¥è¯†é©±åŠ¨ï¼ˆKDï¼‰æŸå¤±å‡½æ•°æ¥åˆ›å»ºå¯å­¦ä¹ çš„å¼‚å¸¸æç¤ºï¼Œæ¶ˆé™¤äº†å¯¹å›ºå®šæ–‡æœ¬æç¤ºçš„éœ€æ±‚ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚KAnoCLIPåŒ…æ‹¬å¸¦æœ‰V-Væ³¨æ„åŠ›ï¼ˆCLIP-VVï¼‰çš„CLIPè§†è§‰ç¼–ç å™¨ã€ç”¨äºå¤šçº§è·¨æ¨¡æ€äº¤äº’çš„åŒå‘è·¨æ³¨æ„åŠ›ï¼ˆBi-CMCIï¼‰å’ŒConv-Adapterã€‚è¿™äº›ç»„ä»¶ä¿ç•™äº†å±€éƒ¨è§†è§‰è¯­ä¹‰ï¼Œæ”¹è¿›äº†å±€éƒ¨è·¨æ¨¡æ€èåˆï¼Œå¹¶å°†å…¨å±€è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬ä¿¡æ¯å¯¹é½ï¼Œæé«˜äº†åƒç´ çº§å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚KAnoCLIPåœ¨12ä¸ªå·¥ä¸šå’ŒåŒ»ç–—æ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„å“è¶Šæ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03786v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong>ï¼šé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰æ— éœ€ç›®æ ‡æ•°æ®é›†çš„è®­ç»ƒæ ·æœ¬å³å¯è¯†åˆ«å¼‚å¸¸ã€‚è™½ç„¶CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ZSADä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¾èµ–æ‰‹åŠ¨æ„å»ºçš„æ–‡æœ¬æè¿°å’Œå¼‚å¸¸æç¤ºçš„é—®é¢˜ï¼Œä¸”éš¾ä»¥è¿›è¡Œåƒç´ çº§å¼‚å¸¸åˆ†å‰²ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†KAnoCLIPæ¡†æ¶ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT-3.5ï¼‰å’Œè§†è§‰é—®ç­”ç³»ç»Ÿï¼ˆLlama3ï¼‰çš„çŸ¥è¯†é©±åŠ¨æç¤ºå­¦ä¹ ï¼ˆKnPLï¼‰ï¼Œæ— éœ€å›ºå®šçš„æ–‡æœ¬æç¤ºå³å¯åˆ›å»ºå¯å­¦ä¹ çš„å¼‚å¸¸æç¤ºï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚KAnoCLIPåŒ…æ‹¬CLIPè§†è§‰ç¼–ç å™¨ã€åŒå‘è·¨æ¨¡æ€äº¤äº’çš„å¤šå±‚æ¬¡äº¤å‰æ³¨æ„åŠ›ç­‰æŠ€æœ¯ï¼Œå®ç°äº†åƒç´ çº§çš„å¼‚å¸¸æ£€æµ‹ï¼Œå¹¶åœ¨å¤šä¸ªå·¥ä¸šåŒ»ç–—æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰æ— éœ€ç›®æ ‡æ•°æ®é›†çš„è®­ç»ƒæ ·æœ¬å³å¯å®Œæˆå¼‚å¸¸è¯†åˆ«ã€‚</li>
<li>CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ZSADä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å­˜åœ¨å¯¹å›ºå®šæ–‡æœ¬æè¿°çš„ä¾èµ–å’Œè¯­ä¹‰æ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>KAnoCLIPæ¡†æ¶ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰é—®ç­”ç³»ç»Ÿçš„çŸ¥è¯†æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>çŸ¥è¯†é©±åŠ¨æç¤ºå­¦ä¹ ï¼ˆKnPLï¼‰èƒ½å¤Ÿåˆ›å»ºå¯å­¦ä¹ çš„å¼‚å¸¸æç¤ºï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>KAnoCLIPåŒ…å«å¤šé¡¹æŠ€æœ¯æ”¹è¿›ï¼ŒåŒ…æ‹¬æ”¹è¿›çš„CLIPè§†è§‰ç¼–ç å™¨ã€åŒå‘è·¨æ¨¡æ€äº¤äº’ç­‰ï¼Œå®ç°äº†åƒç´ çº§çš„å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>KAnoCLIPåœ¨å¤šä¸ªå·¥ä¸šåŒ»ç–—æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8fc72da61c407db50509eeb7d6445c43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-469b4b6cf81eeff928a58a049cb2ff86.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f19b490e61e026eeaa189d221f16e1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed92b9d7d53b3173ce4757d3893e62ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc684f493eba61edd1c78a0f4bf774b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e8cd5b525be91ad773344de7631183e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70d07c2fa34fdea64f182702f6671a55.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LLaVA-Steering-Visual-Instruction-Tuning-with-500x-Fewer-Parameters-through-Modality-Linear-Representation-Steering"><a href="#LLaVA-Steering-Visual-Instruction-Tuning-with-500x-Fewer-Parameters-through-Modality-Linear-Representation-Steering" class="headerlink" title="LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters   through Modality Linear Representation-Steering"></a>LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters   through Modality Linear Representation-Steering</h2><p><strong>Authors:Jinhe Bi, Yujun Wang, Haokun Chen, Xun Xiao, Artur Hecker, Volker Tresp, Yunpu Ma</strong></p>
<p>Multimodal Large Language Models (MLLMs) have significantly advanced visual tasks by integrating visual representations into large language models (LLMs). The textual modality, inherited from LLMs, equips MLLMs with abilities like instruction following and in-context learning. In contrast, the visual modality enhances performance in downstream tasks by leveraging rich semantic content, spatial information, and grounding capabilities. These intrinsic modalities work synergistically across various visual tasks. Our research initially reveals a persistent imbalance between these modalities, with text often dominating output generation during visual instruction tuning. This imbalance occurs when using both full fine-tuning and parameter-efficient fine-tuning (PEFT) methods. We then found that re-balancing these modalities can significantly reduce the number of trainable parameters required, inspiring a direction for further optimizing visual instruction tuning. We introduce Modality Linear Representation-Steering (MoReS) to achieve the goal. MoReS effectively re-balances the intrinsic modalities throughout the model, where the key idea is to steer visual representations through linear transformations in the visual subspace across each model layer. To validate our solution, we composed LLaVA Steering, a suite of models integrated with the proposed MoReS method. Evaluation results show that the composed LLaVA Steering models require, on average, 500 times fewer trainable parameters than LoRA needs while still achieving comparable performance across three visual benchmarks and eight visual question-answering tasks. Last, we present the LLaVA Steering Factory, an in-house developed platform that enables researchers to quickly customize various MLLMs with component-based architecture for seamlessly integrating state-of-the-art models, and evaluate their intrinsic modality imbalance. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡å°†è§†è§‰è¡¨ç¤ºé›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œæ˜¾è‘—åœ°æ¨è¿›äº†è§†è§‰ä»»åŠ¡çš„å‘å±•ã€‚æ–‡æœ¬æ¨¡æ€ç»§æ‰¿è‡ªLLMsï¼Œä½¿MLLMså…·å¤‡æŒ‡ä»¤éµå¾ªå’Œä¸Šä¸‹æ–‡å­¦ä¹ ç­‰åŠŸèƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè§†è§‰æ¨¡æ€é€šè¿‡åˆ©ç”¨ä¸°å¯Œçš„è¯­ä¹‰å†…å®¹ã€ç©ºé—´ä¿¡æ¯å’Œå®šä½èƒ½åŠ›ï¼Œæé«˜äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚è¿™äº›å†…åœ¨æ¨¡æ€åœ¨ä¸åŒçš„è§†è§‰ä»»åŠ¡ä¸­ååŒå·¥ä½œã€‚æˆ‘ä»¬çš„ç ”ç©¶æœ€åˆæ­ç¤ºäº†è¿™äº›æ¨¡æ€ä¹‹é—´æŒä¹…çš„å¤±è¡¡ï¼Œæ–‡æœ¬é€šå¸¸åœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ä¸­ä¸»å¯¼è¾“å‡ºç”Ÿæˆã€‚è¿™ç§ä¸å¹³è¡¡åœ¨ä½¿ç”¨å…¨å¾®è°ƒï¼ˆFull Fine-tuningï¼‰å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•æ—¶éƒ½ä¼šå‘ç”Ÿã€‚éšåï¼Œæˆ‘ä»¬å‘ç°é‡æ–°å¹³è¡¡è¿™äº›æ¨¡æ€å¯ä»¥å¤§å¤§å‡å°‘æ‰€éœ€çš„è®­ç»ƒå‚æ•°æ•°é‡ï¼Œä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–è§†è§‰æŒ‡ä»¤è°ƒæ•´æä¾›äº†æ–¹å‘ã€‚æˆ‘ä»¬å¼•å…¥æ¨¡æ€çº¿æ€§è¡¨ç¤ºè½¬å‘ï¼ˆMoReSï¼‰æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚MoReSæœ‰æ•ˆåœ°åœ¨æ•´ä¸ªæ¨¡å‹ä¸­é‡æ–°å¹³è¡¡äº†å†…åœ¨æ¨¡æ€ï¼Œå…¶å…³é”®æ€æƒ³æ˜¯é€šè¿‡æ¯ä¸ªæ¨¡å‹å±‚çš„è§†è§‰å­ç©ºé—´çš„çº¿æ€§è½¬æ¢æ¥å¼•å¯¼è§†è§‰è¡¨ç¤ºã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¼€å‘äº†LLaVAè½¬å‘å¥—ä»¶ï¼Œè¿™æ˜¯ä¸€å¥—é›†æˆäº†æ‰€æå‡ºçš„MoReSæ–¹æ³•çš„æ¨¡å‹ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒLLaVAè½¬å‘å¥—ä»¶ç»„æˆçš„æ¨¡å‹å¹³å‡éœ€è¦æ¯”LoRAå°‘500å€çš„è®­ç»ƒå‚æ•°ï¼ŒåŒæ—¶åœ¨ä¸‰ä¸ªè§†è§‰åŸºå‡†æµ‹è¯•å’Œå…«ä¸ªè§†è§‰é—®ç­”ä»»åŠ¡ä¸­ä»èƒ½ä¿æŒç›¸å½“çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†LLaVAè½¬å‘å·¥å‚â€”â€”ä¸€ä¸ªå†…éƒ¨å¼€å‘çš„å¹³å°ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿè¿…é€Ÿå®šåˆ¶å„ç§MLLMsï¼Œé€šè¿‡ç»„ä»¶å¼æ¶æ„æ— ç¼é›†æˆæœ€æ–°æ¨¡å‹ï¼Œå¹¶è¯„ä¼°å…¶å†…åœ¨æ¨¡æ€å¤±è¡¡æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12359v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é›†æˆè§†è§‰è¡¨å¾åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åå¯¹è§†è§‰ä»»åŠ¡çš„æ˜¾è‘—æ”¹è¿›ã€‚æ–‡æœ¬æ¨¡æ€èµ‹äºˆMLLMsæŒ‡ä»¤éµå¾ªå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›ï¼Œè€Œè§†è§‰æ¨¡æ€åˆ™é€šè¿‡ä¸°å¯Œçš„è¯­ä¹‰å†…å®¹ã€ç©ºé—´ä¿¡æ¯å’Œæ¥åœ°èƒ½åŠ›å¢å¼ºä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è§†è§‰æŒ‡ä»¤å¾®è°ƒä¸­å­˜åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¹‹é—´çš„æŒä¹…ä¸å¹³è¡¡ï¼Œæ–‡æœ¬é€šå¸¸ä¸»å¯¼è¾“å‡ºç”Ÿæˆã€‚é€šè¿‡é‡æ–°å¹³è¡¡è¿™äº›æ¨¡æ€ï¼Œå¯ä»¥å‡å°‘æ‰€éœ€çš„è®­ç»ƒå‚æ•°æ•°é‡ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†æ¨¡æ€çº¿æ€§è¡¨ç¤ºè½¬å‘ï¼ˆMoReSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡çº¿æ€§å˜æ¢å¼•å¯¼è§†è§‰è¡¨ç¤ºï¼Œæœ‰æ•ˆå¹³è¡¡äº†å†…åœ¨æ¨¡æ€ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨MoReSçš„LLaVAè½¬å‘æ¨¡å‹åœ¨å‡å°‘å¤§é‡è®­ç»ƒå‚æ•°çš„åŒæ—¶ï¼Œåœ¨ä¸‰ä¸ªè§†è§‰åŸºå‡†æµ‹è¯•å’Œå…«ä¸ªè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†ç›¸å½“çš„æ€§èƒ½ã€‚è¿˜ä»‹ç»äº†LLaVAè½¬å‘å·¥å‚ï¼Œè¿™æ˜¯ä¸€ä¸ªç»„ä»¶åŒ–æ¶æ„çš„å¹³å°ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿå¿«é€Ÿåœ°å®šåˆ¶å„ç§MLLMsï¼Œæ— ç¼é›†æˆæœ€æ–°æ¨¡å‹å¹¶è¯„ä¼°å…¶å†…åœ¨æ¨¡æ€ä¸å¹³è¡¡æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsé€šè¿‡é›†æˆè§†è§‰è¡¨å¾æ˜¾è‘—æ”¹è¿›äº†è§†è§‰ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>MLLMså…·å¤‡æ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡æ€ï¼Œåˆ†åˆ«èµ‹äºˆä¸åŒçš„èƒ½åŠ›ã€‚</li>
<li>åœ¨è§†è§‰æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œå­˜åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¹‹é—´çš„ä¸å¹³è¡¡ã€‚</li>
<li>é‡æ–°å¹³è¡¡æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€å¯ä»¥å‡å°‘æ‰€éœ€çš„è®­ç»ƒå‚æ•°æ•°é‡ã€‚</li>
<li>å¼•å…¥MoReSæ–¹æ³•ï¼Œé€šè¿‡çº¿æ€§å˜æ¢å¼•å¯¼è§†è§‰è¡¨ç¤ºï¼Œæœ‰æ•ˆå¹³è¡¡å†…åœ¨æ¨¡æ€ã€‚</li>
<li>LLaVAè½¬å‘æ¨¡å‹ä½¿ç”¨MoReSæ–¹æ³•ï¼Œåœ¨å‡å°‘å¤§é‡è®­ç»ƒå‚æ•°çš„åŒæ—¶å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12359">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0dda5c2405166ff95a8a2122f0cc118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-075864893a383bec9e4a3292438bfba6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-113ca071c69e490cb778f5af5c73008e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7854dbc881a567a1051b407344200167.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Information-Extraction-from-Clinical-Notes-Are-We-Ready-to-Switch-to-Large-Language-Models"><a href="#Information-Extraction-from-Clinical-Notes-Are-We-Ready-to-Switch-to-Large-Language-Models" class="headerlink" title="Information Extraction from Clinical Notes: Are We Ready to Switch to   Large Language Models?"></a>Information Extraction from Clinical Notes: Are We Ready to Switch to   Large Language Models?</h2><p><strong>Authors:Yan Hu, Xu Zuo, Yujia Zhou, Xueqing Peng, Jimin Huang, Vipina K. Keloth, Vincent J. Zhang, Ruey-Ling Weng, Qingyu Chen, Xiaoqian Jiang, Kirk E. Roberts, Hua Xu</strong></p>
<p>Backgrounds: Information extraction (IE) is critical in clinical natural language processing (NLP). While large language models (LLMs) excel on generative tasks, their performance on extractive tasks remains debated. Methods: We investigated Named Entity Recognition (NER) and Relation Extraction (RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples, MIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical entities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3 against BERT in terms of performance, generalizability, computational resources, and throughput to BERT. Results: LLaMA models outperformed BERT across datasets. With sufficient training data, LLaMA showed modest improvements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited training data. On unseen i2b2 data, LLaMA-3-70B outperformed BERT by 7% (F1) on NER and 4% on RE. However, LLaMA models required more computing resources and ran up to 28 times slower. We implemented â€œKiwi,â€ a clinical IE package featuring both models, available at <a target="_blank" rel="noopener" href="https://kiwi.clinicalnlp.org/">https://kiwi.clinicalnlp.org/</a>. Conclusion: This study is among the first to develop and evaluate a comprehensive clinical IE system using open-source LLMs. Results indicate that LLaMA models outperform BERT for clinical NER and RE but with higher computational costs and lower throughputs. These findings highlight that choosing between LLMs and traditional deep learning methods for clinical IE applications should remain task-specific, taking into account both performance metrics and practical considerations such as available computing resources and the intended use case scenarios. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šä¿¡æ¯æå–ï¼ˆIEï¼‰åœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è‡³å…³é‡è¦ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨æå–ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»å­˜åœ¨äº‰è®®ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬ç ”ç©¶äº†å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œå…³ç³»æå–ï¼ˆREï¼‰ï¼Œä½¿ç”¨äº†æ¥è‡ªå››ä¸ªæ¥æºçš„1588ä»½ä¸´åºŠç¬”è®°ï¼ˆUT Physiciansã€MTSampleã€MIMIC-IIIå’Œi2b2ï¼‰ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¶µç›–4ä¸ªä¸´åºŠå®ä½“å’Œ16ä¸ªä¿®é¥°è¯çš„æ³¨é‡Šè¯­æ–™åº“ï¼Œå¹¶æ¯”è¾ƒäº†é’ˆå¯¹æŒ‡ä»¤è°ƒæ•´çš„LLaMA-2å’ŒLLaMA-3ä¸BERTåœ¨æ€§èƒ½ã€é€šç”¨æ€§ã€è®¡ç®—èµ„æºå’Œååé‡æ–¹é¢çš„è¡¨ç°ã€‚</p>
<p>ç»“æœï¼šLLaMAæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºBERTã€‚åœ¨å……è¶³çš„è®­ç»ƒæ•°æ®ä¸‹ï¼ŒLLaMAçš„æ”¹è¿›å¹…åº¦è¾ƒå°ï¼ˆNERä¸Šæé«˜1%ï¼ŒREä¸Šæé«˜1.5-3.7%ï¼‰ï¼›è€Œåœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹ï¼Œæ”¹è¿›å¹…åº¦è¾ƒå¤§ã€‚åœ¨æœªè§è¿‡çš„i2b2æ•°æ®é›†ä¸Šï¼ŒLLaMA-3-70Båœ¨NERä¸Šçš„F1åˆ†æ•°é«˜å‡ºBERT 7%ï¼ŒREä¸Šé«˜å‡º4%ã€‚ç„¶è€Œï¼ŒLLaMAæ¨¡å‹éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºï¼Œè¿è¡Œé€Ÿåº¦æœ€æ…¢æ—¶é«˜è¾¾BERTçš„28å€ã€‚æˆ‘ä»¬å¼€å‘äº†åä¸ºâ€œKiwiâ€çš„ä¸´åºŠä¿¡æ¯æå–è½¯ä»¶åŒ…ï¼Œå®ƒåŒæ—¶åŒ…å«è¿™ä¸¤ç§æ¨¡å‹ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://kiwi.clinicalnlp.org/%E8%8E%B7%E5%8F%96%E3%80%82">https://kiwi.clinicalnlp.org/è·å–ã€‚</a></p>
<p>ç»“è®ºï¼šæœ¬ç ”ç©¶æ˜¯é¦–æ‰¹ä½¿ç”¨å¼€æºLLMè¿›è¡Œä¸´åºŠä¿¡æ¯æå–ç³»ç»Ÿçš„å¼€å‘å’Œè¯„ä¼°ä¹‹ä¸€ã€‚ç»“æœè¡¨æ˜ï¼ŒLLaMAæ¨¡å‹åœ¨ä¸´åºŠNERå’ŒREæ–¹é¢çš„è¡¨ç°ä¼˜äºBERTï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜ï¼Œååé‡æ›´ä½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåœ¨é€‰æ‹©ç”¨äºä¸´åºŠä¿¡æ¯æå–åº”ç”¨çš„LLMå’Œä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•æ—¶ï¼Œåº”è€ƒè™‘åˆ°æ€§èƒ½å’Œå®é™…å› ç´ ï¼Œå¦‚å¯ç”¨çš„è®¡ç®—èµ„æºä»¥åŠé¢„æœŸçš„ä½¿ç”¨åœºæ™¯ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10020v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸´åºŠä¿¡æ¯æå–ï¼ˆIEï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”LLMsæ¨¡å‹ï¼ˆLLaMA-2å’ŒLLaMA-3ï¼‰ä¸ä¼ ç»ŸBERTæ¨¡å‹ï¼Œåœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œå…³ç³»æŠ½å–ï¼ˆREï¼‰ä»»åŠ¡ä¸Šè¿›è¡Œæ€§èƒ½ã€æ³›åŒ–èƒ½åŠ›ã€è®¡ç®—èµ„æºå’Œå¤„ç†é€Ÿåº¦çš„è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLaMAæ¨¡å‹åœ¨è·¨æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºBERTï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™è®­ç»ƒæ•°æ®æƒ…å†µä¸‹æ”¹è¿›æ›´å¤§ã€‚ç„¶è€Œï¼ŒLLaMAæ¨¡å‹éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå¹¶ä¸”è¿è¡Œé€Ÿåº¦è¾ƒæ…¢ã€‚ç ”ç©¶è¿˜æ¨å‡ºäº†ä¸€æ¬¾èåˆä¸¤ç§æ¨¡å‹çš„ä¸´åºŠIEå·¥å…·â€œKiwiâ€ã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶æ˜¯é¦–æ‰¹åˆ©ç”¨å¼€æºLLMså¼€å‘å¹¶è¯„ä¼°ä¸´åºŠIEç³»ç»Ÿçš„ç ”ç©¶ä¹‹ä¸€ï¼Œå‘ç°LLaMAæ¨¡å‹åœ¨ä¸´åºŠNERå’ŒREä»»åŠ¡ä¸Šä¼˜äºBERTï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜ã€‚å®é™…åº”ç”¨ä¸­éœ€æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹ã€æ€§èƒ½æŒ‡æ ‡ã€è®¡ç®—èµ„æºç­‰å› ç´ é€‰æ‹©æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸´åºŠä¿¡æ¯æå–ï¼ˆIEï¼‰ä¸­çš„è¡¨ç°è¢«ç ”ç©¶ã€‚</li>
<li>å¯¹æ¯”äº†LLaMA-2å’ŒLLaMA-3ä¸BERTåœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œå…³ç³»æŠ½å–ï¼ˆREï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>LLaMAæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºBERTï¼Œå°¤å…¶åœ¨æœ‰é™è®­ç»ƒæ•°æ®æƒ…å†µä¸‹æ”¹è¿›æ›´å¤§ã€‚</li>
<li>LLaMAæ¨¡å‹éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºä¸”è¿è¡Œé€Ÿåº¦è¾ƒæ…¢ã€‚</li>
<li>ç ”ç©¶æ¨å‡ºäº†ä¸€æ¬¾èåˆä¸¤ç§æ¨¡å‹çš„ä¸´åºŠIEå·¥å…·â€œKiwiâ€ã€‚</li>
<li>æœ¬ç ”ç©¶æ˜¯é¦–æ‰¹åˆ©ç”¨å¼€æºLLMsè¿›è¡Œä¸´åºŠIEç ”ç©¶çš„å…¶ä¸­ä¹‹ä¸€ã€‚</li>
<li>ç»“æœè¡¨æ˜ï¼Œåœ¨é€‰æ‹©ä¸´åºŠIEåº”ç”¨çš„æ¨¡å‹æ—¶ï¼Œéœ€ç»¼åˆè€ƒè™‘ä»»åŠ¡ç‰¹ç‚¹ã€æ€§èƒ½æŒ‡æ ‡ã€è®¡ç®—èµ„æºç­‰å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9a354909673f7f94353811d40f3052ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-946c150a3f4a0c6f5c532188c9a6f918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c106e9e52b8e74ee7ac413770d86d0be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69c3d989ac03b5dab51f6a78ddf8edd3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Hallucination-Detox-Sensitivity-Dropout-SenD-for-Large-Language-Model-Training"><a href="#Hallucination-Detox-Sensitivity-Dropout-SenD-for-Large-Language-Model-Training" class="headerlink" title="Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model   Training"></a>Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model   Training</h2><p><strong>Authors:Shahrad Mohammadzadeh, Juan David Guerra, Marco Bonizzato, Reihaneh Rabbany, Golnoosh Farnadi</strong></p>
<p>As large language models (LLMs) are increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations - outputs that are factually inaccurate or irrelevant to user input - have grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M - 12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce Sensitivity Dropout (SenD), a novel training protocol designed to mitigate hallucinations by reducing variance during training. SenD achieves this by deterministically dropping embedding indices with significant variability, referred to as Sensitive Embedding Indices. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore at 2x speed. This efficient metric is integrated into our protocol, allowing SenD to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to Wikipedia, Medical, and LegalBench domains. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„è¡Œä¸šçš„éƒ¨ç½²æ—¥ç›Šå¢å¤šï¼Œå…³äºå…¶å¯é æ€§çš„æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯å› å¹»è§‰è¾“å‡ºï¼ˆå³äº‹å®ä¸Šä¸å‡†ç¡®æˆ–ä¸ç”¨æˆ·è¾“å…¥ä¸ç›¸å…³çš„è¾“å‡ºï¼‰è€Œäº§ç”Ÿçš„æ‹…å¿§ä¹Ÿåœ¨å¢é•¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨è°ƒæŸ¥è®­ç»ƒè¿‡ç¨‹ä¸å¹»è§‰å‡ºç°ä¹‹é—´çš„å…³ç³»ï¼Œä»¥å¡«è¡¥ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨äº‹åæ£€æµ‹å’Œç¼“è§£ç­–ç•¥çš„ä¸è¶³ã€‚æˆ‘ä»¬ä½¿ç”¨Pythiaå¥—ä»¶ï¼ˆä»7åƒä¸‡è‡³åäº¿å‚æ•°ï¼‰çš„æ¨¡å‹å’Œå‡ ç§å¹»è§‰æ£€æµ‹æŒ‡æ ‡ï¼Œåˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¹»è§‰è¶‹åŠ¿ï¼Œå¹¶æ¢ç´¢LLMçš„å†…éƒ¨åŠ¨æ€ã€‚æˆ‘ä»¬å¼•å…¥äº†æ•æ„Ÿæ€§ä¸¢å¼ƒï¼ˆSenDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è®­ç»ƒåè®®ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ–¹å·®æ¥ç¼“è§£å¹»è§‰ã€‚SenDé€šè¿‡ç¡®å®šæ€§åœ°ä¸¢å¼ƒå…·æœ‰é‡å¤§å¯å˜æ€§çš„åµŒå…¥ç´¢å¼•æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¿™äº›ç´¢å¼•è¢«ç§°ä¸ºæ•æ„ŸåµŒå…¥ç´¢å¼•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„å¹»è§‰æ£€æµ‹æŒ‡æ ‡â€”â€”é«˜æ•ˆç‰¹å¾å¾—åˆ†ï¼ˆEESï¼‰ï¼Œè¯¥æŒ‡æ ‡ä»¥ä¸¤å€çš„é€Ÿåº¦è¿‘ä¼¼ä¼ ç»Ÿç‰¹å¾å¾—åˆ†ã€‚æ­¤é«˜æ•ˆçš„æŒ‡æ ‡å·²é›†æˆåˆ°æˆ‘ä»¬çš„åè®®ä¸­ï¼Œä½¿å¾—SenDåœ¨è®¡ç®—ä¸Šæ—¢å¯æ‰©å±•ï¼Œåˆèƒ½æœ‰æ•ˆåœ°å‡å°‘å¹»è§‰ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸æ­£å¸¸è®­ç»ƒç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜æµ‹è¯•æ—¶LLMçš„å¯é æ€§æ–¹é¢æé«˜äº†é«˜è¾¾ç™¾åˆ†ä¹‹å››åï¼ŒåŒæ—¶æä¾›äº†ä¸€ç§åœ¨é€‚åº”LLMåˆ°Wikipediaã€åŒ»å­¦å’Œæ³•å¾‹åˆ¤ä¾‹ç­‰åŸŸæ—¶æé«˜äº‹å®å‡†ç¡®æ€§çš„é«˜æ•ˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15460v3">PDF</a> 23 pages, 15 figures, under review at ICLR, accepted to Safe   Generative AI Workshop @ NeurIPS 2024, resubmitting to change name to   appropriate name</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„å¯é æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å› å‡ºç°ä¸ç°å®ä¸ç¬¦æˆ–ä¸ç”¨æˆ·è¾“å…¥ä¸ç›¸å…³çš„è¾“å‡ºè€Œå¯¼è‡´çš„â€œå¹»è§‰â€ç°è±¡ã€‚ç ”ç©¶é€šè¿‡Pythiaå¥—ä»¶ä¸­çš„æ¨¡å‹ï¼ˆå‚æ•°èŒƒå›´ä»7åƒä¸‡è‡³12äº¿ï¼‰åŠä¸€ç³»åˆ—å¹»è§‰æ£€æµ‹æŒ‡æ ‡ï¼Œåˆ†æäº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¹»è§‰è¶‹åŠ¿ï¼Œå¹¶æ¢ç´¢äº†LLMçš„å†…éƒ¨åŠ¨æ€æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æ•æ„Ÿæ€§ä¸¢å¼ƒï¼ˆSenDï¼‰è¿™ä¸€æ–°å‹è®­ç»ƒåè®®ï¼Œé€šè¿‡å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜é‡æ¥å‡è½»å¹»è§‰ç°è±¡ã€‚åŒæ—¶ï¼Œå¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„å¹»è§‰æ£€æµ‹æŒ‡æ ‡â€”â€”é«˜æ•ˆç‰¹å¾å¾—åˆ†ï¼ˆEESï¼‰ï¼Œèƒ½åœ¨ä¿æŒæœ‰æ•ˆæ€§çš„åŒæ—¶ï¼ŒåŠ é€Ÿä¼ ç»Ÿçš„ç‰¹å¾å¾—åˆ†è¯„ä¼°ã€‚ç ”ç©¶æ˜¾ç¤ºï¼Œè¯¥è®­ç»ƒåè®®èƒ½å¤Ÿæå‡LLMçš„æµ‹è¯•å¯é æ€§è¾¾40%ï¼Œå¹¶ä¸ºåœ¨Wikipediaã€åŒ»ç–—å’Œæ³•å¾‹ç­‰é¢†åŸŸè‡ªé€‚åº”LLMæä¾›äº†æé«˜å…¶äº‹å®å‡†ç¡®æ€§çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯é æ€§é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è§‰ç°è±¡ã€‚</li>
<li>ç ”ç©¶é€šè¿‡Pythiaå¥—ä»¶ä¸­çš„ä¸åŒè§„æ¨¡æ¨¡å‹ï¼Œæ·±å…¥æ¢è®¨äº†å¹»è§‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¡¨ç°åŠä¸è®­ç»ƒè¿‡ç¨‹çš„å…³ç³»ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹è®­ç»ƒåè®®â€”â€”æ•æ„Ÿæ€§ä¸¢å¼ƒï¼ˆSenDï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜é‡æ¥å‡è½»å¹»è§‰ç°è±¡ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„å¹»è§‰æ£€æµ‹æŒ‡æ ‡â€”â€”é«˜æ•ˆç‰¹å¾å¾—åˆ†ï¼ˆEESï¼‰ï¼Œèƒ½å¤ŸåŠ é€Ÿä¼ ç»Ÿçš„ç‰¹å¾å¾—åˆ†è¯„ä¼°è¿‡ç¨‹ã€‚</li>
<li>SenDè®­ç»ƒåè®®èƒ½æ˜¾è‘—æå‡LLMçš„æµ‹è¯•å¯é æ€§ï¼Œç›¸æ¯”å¸¸è§„è®­ç»ƒæé«˜äº†æœ€å¤šè¾¾40%ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹è‡ªé€‚åº”LLMåˆ°ç‰¹å®šé¢†åŸŸå¦‚Wikipediaã€åŒ»ç–—å’Œæ³•å¾‹ç­‰é¢†åŸŸå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚è‡ªé€‚åº”æ–¹æ³•æœ‰åŠ©äºæå‡è¿™äº›é¢†åŸŸä¸­çš„äº‹å®å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3bee6b057ea5ffdcda6f872e8106fa00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4e8b9d922dee84d85fb497369847b09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4d1a40021de82b45c60bea88403a416.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GraphLoRA-Structure-Aware-Contrastive-Low-Rank-Adaptation-for-Cross-Graph-Transfer-Learning"><a href="#GraphLoRA-Structure-Aware-Contrastive-Low-Rank-Adaptation-for-Cross-Graph-Transfer-Learning" class="headerlink" title="GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for   Cross-Graph Transfer Learning"></a>GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for   Cross-Graph Transfer Learning</h2><p><strong>Authors:Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu</strong></p>
<p>Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in handling a range of graph analytical tasks across various domains, such as e-commerce and social networks. Despite their versatility, GNNs face significant challenges in transferability, limiting their utility in real-world applications. Existing research in GNN transfer learning overlooks discrepancies in distribution among various graph datasets, facing challenges when transferring across different distributions. How to effectively adopt a well-trained GNN to new graphs with varying feature and structural distributions remains an under-explored problem. Taking inspiration from the success of Low-Rank Adaptation (LoRA) in adapting large language models to various domains, we propose GraphLoRA, an effective and parameter-efficient method for transferring well-trained GNNs to diverse graph domains. Specifically, we first propose a Structure-aware Maximum Mean Discrepancy (SMMD) to align divergent node feature distributions across source and target graphs. Moreover, we introduce low-rank adaptation by injecting a small trainable GNN alongside the pre-trained one, effectively bridging structural distribution gaps while mitigating the catastrophic forgetting. Additionally, a structure-aware regularization objective is proposed to enhance the adaptability of the pre-trained GNN to target graph with scarce supervision labels. Extensive experiments on eight real-world datasets demonstrate the effectiveness of GraphLoRA against fourteen baselines by tuning only 20% of parameters, even across disparate graph domains. The code is available at <a target="_blank" rel="noopener" href="https://github.com/AllminerLab/GraphLoRA">https://github.com/AllminerLab/GraphLoRA</a>. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨ç”µå­å•†åŠ¡å’Œç¤¾ä¼šç½‘ç»œç­‰å¤šä¸ªé¢†åŸŸçš„å„ç§å›¾å½¢åˆ†æä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰é€šç”¨æ€§ï¼Œä½†åœ¨è¿ç§»æ€§æ–¹é¢é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚ç°æœ‰çš„å›¾ç¥ç»ç½‘ç»œè¿ç§»å­¦ä¹ ç ”ç©¶å¿½è§†äº†ä¸åŒå›¾æ•°æ®é›†ä¹‹é—´åˆ†å¸ƒçš„å·®å¼‚æ€§ï¼Œåœ¨è·¨ä¸åŒåˆ†å¸ƒè¿ç§»æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å¦‚ä½•æœ‰æ•ˆåœ°å°†è®­ç»ƒè‰¯å¥½çš„å›¾ç¥ç»ç½‘ç»œåº”ç”¨äºå…·æœ‰ä¸åŒç‰¹å¾å’Œç»“æ„åˆ†å¸ƒçš„æ–°å›¾ä¸Šä»ç„¶æ˜¯ä¸€ä¸ªå°šæœªæ·±å…¥ç ”ç©¶çš„é—®é¢˜ã€‚å—ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢†åŸŸé€‚åº”æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†GraphLoRAæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç»è¿‡è‰¯å¥½è®­ç»ƒçš„å›¾ç¥ç»ç½‘ç»œè½¬ç§»åˆ°å¤šæ ·åŒ–å›¾é¢†åŸŸçš„æœ‰æ•ˆä¸”å‚æ•°é«˜æ•ˆçš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ç»“æ„æ„ŸçŸ¥æœ€å¤§å¹³å‡å·®å¼‚æ³•ï¼ˆSMMDï¼‰ï¼Œä»¥å¯¹é½æºå›¾å’Œç›®æ ‡å›¾ä¸­å‘æ•£çš„èŠ‚ç‚¹ç‰¹å¾åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä½ç§©é€‚åº”æŠ€æœ¯ï¼Œå³åœ¨é¢„è®­ç»ƒæ¨¡å‹æ—è¾¹æ³¨å…¥ä¸€ä¸ªå°å‹å¯è®­ç»ƒçš„å›¾ç¥ç»ç½‘ç»œï¼Œæœ‰æ•ˆåœ°ç¼©å°äº†ç»“æ„åˆ†å¸ƒå·®è·å¹¶å‡è½»äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªç»“æ„æ„ŸçŸ¥çš„æ­£åˆ™åŒ–ç›®æ ‡ï¼Œä»¥å¢å¼ºé¢„è®­ç»ƒå›¾ç¥ç»ç½‘ç»œå¯¹ç›®æ ‡å›¾çš„é€‚åº”æ€§ï¼Œå°¤å…¶å½“ç›®æ ‡å›¾çš„ç›‘ç£æ ‡ç­¾ç¨€ç¼ºæ—¶ã€‚åœ¨å…«ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä»…é€šè¿‡è°ƒæ•´20%çš„å‚æ•°ï¼ŒGraphLoRAåœ¨è·¨ä¸åŒå›¾åŸŸçš„æƒ…å†µä¸‹ï¼Œç›¸å¯¹äºåå››ç§åŸºçº¿æ–¹æ³•è¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/AllminerLab/GraphLoRA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AllminerLab/GraphLoRAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16670v2">PDF</a> Accepted by KDD2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†GraphLoRAæ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ç§é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„æœ‰æ•ˆä¸”å‚æ•°é«˜æ•ˆçš„è·¨å›¾åŸŸè¿ç§»å­¦ä¹ æ–¹æ³•ã€‚å—ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢†åŸŸé€‚é…çš„æˆåŠŸå¯å‘ï¼ŒGraphLoRAé€šè¿‡ç»“æ„æ„ŸçŸ¥æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆSMMDï¼‰å¯¹é½ä¸åŒæºå’Œç›®æ ‡å›¾çš„èŠ‚ç‚¹ç‰¹å¾åˆ†å¸ƒï¼Œå¹¶é€šè¿‡å¼•å…¥ä½ç§©é€‚åº”å’Œç»“æ„åŒ–æ„ŸçŸ¥æ­£åˆ™åŒ–ç›®æ ‡æ¥å¢å¼ºé¢„è®­ç»ƒGNNå¯¹ç›®æ ‡å›¾çš„é€‚åº”æ€§ï¼Œå³ä½¿é¢å¯¹ç¨€ç–ç›‘ç£æ ‡ç­¾ä¹Ÿèƒ½æœ‰æ•ˆå¼¥ç»“æ„åˆ†å¸ƒå·®è·å¹¶é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚åœ¨å…«ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGraphLoRAåœ¨ä»…è°ƒæ•´20%å‚æ•°çš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºåå››ç§åŸºçº¿æ–¹æ³•ï¼Œå³ä½¿åœ¨ä¸åŒçš„å›¾åŸŸä¹‹é—´ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Graph Neural Networks (GNNs) åœ¨å¤„ç†å„ç§å›¾åˆ†æä»»åŠ¡æ—¶è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨è·¨ä¸åŒå›¾åŸŸçš„è¿ç§»å­¦ä¹ ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¿½è§†äº†ä¸åŒå›¾æ•°æ®é›†ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ï¼Œåœ¨è·¨ä¸åŒåˆ†å¸ƒè¿›è¡Œè¿ç§»æ—¶é¢ä¸´éš¾é¢˜ã€‚</li>
<li>GraphLoRAæ–¹æ³•è¢«æå‡ºï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°å°†é¢„è®­ç»ƒçš„GNNé€‚åº”åˆ°å…·æœ‰ä¸åŒç‰¹å¾å’Œç»“æ„åˆ†å¸ƒçš„æ–°å›¾ä¸Šã€‚</li>
<li>GraphLoRAå—åˆ°ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çš„å¯å‘ï¼Œå¹¶å¼•å…¥äº†ç»“æ„æ„ŸçŸ¥æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆSMMDï¼‰æ¥å¯¹é½æºå›¾å’Œç›®æ ‡å›¾çš„èŠ‚ç‚¹ç‰¹å¾åˆ†å¸ƒã€‚</li>
<li>é€šè¿‡å¼•å…¥ä½ç§©é€‚åº”å’Œç»“æ„åŒ–æ„ŸçŸ¥æ­£åˆ™åŒ–ç›®æ ‡ï¼ŒGraphLoRAèƒ½å¤Ÿå¼¥ç»“æ„åˆ†å¸ƒå·®è·å¹¶é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>åœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGraphLoRAåœ¨å‚æ•°æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä»…è°ƒæ•´ä¸€å°éƒ¨åˆ†å‚æ•°å°±èƒ½è¾¾åˆ°è‰¯å¥½çš„è¿ç§»å­¦ä¹ æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ab49f7d15244ca88bf800bc27aa0dd2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fab735d93196868e2f68d0a8695ee235.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57edc1b4b5ef2e8e76e6b28839ed3c2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a37199c337e9ef1223da635ab66fa2b5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Intelligent-Router-for-LLM-Workloads-Improving-Performance-Through-Workload-Aware-Load-Balancing"><a href="#Intelligent-Router-for-LLM-Workloads-Improving-Performance-Through-Workload-Aware-Load-Balancing" class="headerlink" title="Intelligent Router for LLM Workloads: Improving Performance Through   Workload-Aware Load Balancing"></a>Intelligent Router for LLM Workloads: Improving Performance Through   Workload-Aware Load Balancing</h2><p><strong>Authors:Kunal Jain, Anjaly Parayil, Ankur Mallick, Esha Choukse, Xiaoting Qin, Jue Zhang, ÃÃ±igo Goiri, Rujia Wang, Chetan Bansal, Victor RÃ¼hle, Anoop Kulkarni, Steve Kofsky, Saravan Rajmohan</strong></p>
<p>Large Language Model (LLM) workloads have distinct prefill and decode phases with different compute and memory requirements which should ideally be accounted for when scheduling input queries across different LLM instances in a cluster. However existing scheduling algorithms treat LLM workloads as monolithic jobs without considering the distinct characteristics of the two phases in each workload. This leads to sub-optimal scheduling and increased response latency. In this work, we start by characterizing factors affecting the response latency during LLM inference serving. We establish that better load balancing of inference requests across the available LLM instances can improve the end-to-end latency to a larger extent than merely focusing on optimizing the instance-level scheduler. Motivated by our findings, we propose a heuristic-guided reinforcement learning-based intelligent router for data-driven and workload-aware scheduling. Our router schedules queries across LLM instances by leveraging a trainable response-length predictor, and a novel formulation for estimating the impact of mixing different workloads and achieves over 11% lower end-to-end latency than existing approaches on a mix of public datasets and 7.8% lower end-to-end latency on real workload data with diverse input and output trends from Cloud Provider X. Additionally, the proposed framework can also serve as a standard for benchmarking different LLM inference schedulers since it provides the best latency for a given model, hardware, and instance-level scheduler combination. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å·¥ä½œè´Ÿè½½å…·æœ‰ä¸åŒçš„é¢„å¡«å……å’Œè§£ç é˜¶æ®µï¼Œè¿™ä¸¤ä¸ªé˜¶æ®µæœ‰ä¸åŒçš„è®¡ç®—å’Œå†…å­˜è¦æ±‚ã€‚åœ¨é›†ç¾¤ä¸­çš„ä¸åŒLLMå®ä¾‹ä¸Šè°ƒåº¦è¾“å…¥æŸ¥è¯¢æ—¶ï¼Œç†æƒ³æƒ…å†µä¸‹åº”è€ƒè™‘è¿™äº›è¦æ±‚ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è°ƒåº¦ç®—æ³•å°†LLMå·¥ä½œè´Ÿè½½è§†ä¸ºå•ä¸€ä½œä¸šï¼Œè€Œæ²¡æœ‰è€ƒè™‘åˆ°æ¯ä¸ªå·¥ä½œè´Ÿè½½ä¸­ä¸¤ä¸ªé˜¶æ®µçš„ç‹¬ç‰¹ç‰¹å¾ã€‚è¿™å¯¼è‡´äº†æ¬¡ä¼˜è°ƒåº¦å’Œå“åº”å»¶è¿Ÿå¢åŠ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13510v2">PDF</a> 16 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å·¥ä½œè´Ÿè½½ç‰¹æ€§ï¼ŒåŒ…æ‹¬é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„ä¸åŒè®¡ç®—å’Œå†…å­˜éœ€æ±‚ã€‚ç°æœ‰è°ƒåº¦ç®—æ³•æœªè€ƒè™‘è¿™äº›ç‰¹æ€§ï¼Œå¯¼è‡´è°ƒåº¦ä¸å¤Ÿä¼˜åŒ–å’Œå“åº”å»¶è¿Ÿå¢åŠ ã€‚ç ”ç©¶é€šè¿‡æ™ºèƒ½è·¯ç”±å™¨è¿›è¡Œæ•°æ®é©±åŠ¨å’Œè´Ÿè½½æ„ŸçŸ¥çš„è°ƒåº¦ï¼Œåˆ©ç”¨å¯è®­ç»ƒçš„å“åº”é•¿åº¦é¢„æµ‹å™¨å’Œä¼°è®¡æ··åˆä¸åŒå·¥ä½œè´Ÿè½½å½±å“çš„æ–°å…¬å¼ï¼Œå®ç°æ¯”ç°æœ‰æ–¹æ³•æ›´ä½çš„ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä½œä¸ºä¸åŒLLMæ¨ç†è°ƒåº¦å™¨çš„åŸºå‡†æµ‹è¯•æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå·¥ä½œè´Ÿè½½å…·æœ‰é¢„å¡«å……å’Œè§£ç é˜¶æ®µï¼Œå…·æœ‰ä¸åŒçš„è®¡ç®—å’Œå†…å­˜è¦æ±‚ã€‚</li>
<li>ç°æœ‰è°ƒåº¦ç®—æ³•æœªè€ƒè™‘LLMå·¥ä½œè´Ÿè½½çš„è¿™ä¸¤ä¸ªé˜¶æ®µçš„ç‰¹æ€§ï¼Œå¯¼è‡´è°ƒåº¦ä¸å¤Ÿä¼˜åŒ–ã€‚</li>
<li>è°ƒåº¦ç®—æ³•åº”è€ƒè™‘è´Ÿè½½å¹³è¡¡ä»¥æé«˜ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚</li>
<li>æå‡ºçš„æ™ºèƒ½è·¯ç”±å™¨é€šè¿‡æ•°æ®é©±åŠ¨å’Œè´Ÿè½½æ„ŸçŸ¥çš„è°ƒåº¦å®ç°äº†ä¼˜åŒ–çš„ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚</li>
<li>æ™ºèƒ½è·¯ç”±å™¨åˆ©ç”¨å“åº”é•¿åº¦é¢„æµ‹å™¨å’Œæ··åˆå·¥ä½œè´Ÿè½½å½±å“çš„æ–°å…¬å¼è¿›è¡Œè°ƒåº¦ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ™ºèƒ½è·¯ç”±å™¨é™ä½äº†è¶…è¿‡11%çš„ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01e115be673a4007a561126ac7150fa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07080b1f8b29f074b0b50b1569e8df7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-078af8f37122bdef1a839fa510ff8cb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7d07b6c7625b28ad72e3209fd06e162.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="IDEAL-Leveraging-Infinite-and-Dynamic-Characterizations-of-Large-Language-Models-for-Query-focused-Summarization"><a href="#IDEAL-Leveraging-Infinite-and-Dynamic-Characterizations-of-Large-Language-Models-for-Query-focused-Summarization" class="headerlink" title="IDEAL: Leveraging Infinite and Dynamic Characterizations of Large   Language Models for Query-focused Summarization"></a>IDEAL: Leveraging Infinite and Dynamic Characterizations of Large   Language Models for Query-focused Summarization</h2><p><strong>Authors:Jie Cao, Dian Jiao, Qiang Yan, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</strong></p>
<p>Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. With the advent of large language models (LLMs), shows their impressive capability of textual understanding through large-scale pretraining, which implies the great potential of extractive snippet generation. In this paper, we systematically investigated two indispensable characteristics that the LLMs-based QFS models should be harnessed, Lengthy Document Summarization and Efficiently Fine-grained Query-LLM Alignment, respectively. Correspondingly, we propose two modules called Query-aware HyperExpert and Query-focused Infini-attention to access the aforementioned characteristics. These innovations pave the way for broader application and accessibility in the field of QFS technology. Extensive experiments conducted on existing QFS benchmarks indicate the effectiveness and generalizability of the proposed approach. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/DCDmllm/IDEAL_Summary">https://github.com/DCDmllm/IDEAL_Summary</a>. </p>
<blockquote>
<p>æŸ¥è¯¢é‡ç‚¹æ‘˜è¦ï¼ˆQFSï¼‰æ—¨åœ¨ç”Ÿæˆå›ç­”ç‰¹å®šé—®é¢˜çš„æ‘˜è¦ï¼Œä»è€Œå®ç°æ›´å¤§çš„ç”¨æˆ·æ§åˆ¶å’Œä¸ªæ€§åŒ–ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œæ˜¾ç¤ºäº†é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå®ç°çš„ä»¤äººå°è±¡æ·±åˆ»çš„æ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œè¿™æš—ç¤ºäº†æå–ç‰‡æ®µç”Ÿæˆçš„å·¨å¤§æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†åŸºäºLLMçš„QFSæ¨¡å‹æ‰€å¿…éœ€çš„ä¸¤ä¸ªç‰¹å¾ï¼Œå³é•¿ç¯‡æ–‡æ¡£æ‘˜è¦å’Œç²¾ç»†æŸ¥è¯¢-LLMå¯¹é½ã€‚ç›¸åº”åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªæ¨¡å—ï¼Œå³æŸ¥è¯¢æ„ŸçŸ¥è¶…ä¸“å®¶å’ŒæŸ¥è¯¢é‡ç‚¹æ— é™æ³¨æ„åŠ›ï¼Œä»¥è®¿é—®ä¸Šè¿°ç‰¹æ€§ã€‚è¿™äº›åˆ›æ–°ä¸ºQFSæŠ€æœ¯çš„æ›´å¹¿æ³›åº”ç”¨å’Œå¯è®¿é—®æ€§é“ºå¹³äº†é“è·¯ã€‚åœ¨ç°æœ‰çš„QFSåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/DCDmllm/IDEAL_Summary%E3%80%82">https://github.com/DCDmllm/IDEAL_Summaryã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10486v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ‘˜è¦ï¼šè¯¥æ–‡æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŸ¥è¯¢èšç„¦æ‘˜è¦ï¼ˆQFSï¼‰æ–¹æ³•çš„ä¸¤ä¸ªå…³é”®ç‰¹æ€§ï¼Œå³é•¿æ–‡æ¡£æ‘˜è¦å’Œç²¾ç»†ç²’åº¦æŸ¥è¯¢å¯¹é½æ•ˆç‡ï¼Œå¹¶ç›¸åº”æå‡ºäº†Query-aware HyperExpertå’ŒQuery-focused Infini-attentionä¸¤ä¸ªæ¨¡å—ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•æœ‰æ•ˆä¸”é€šç”¨æ€§å¼ºã€‚ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong><br>å…³é”®è§è§£ï¼š</p>
<ol>
<li>æŸ¥è¯¢èšç„¦æ‘˜è¦ï¼ˆQFSï¼‰æ—¨åœ¨ç”Ÿæˆèƒ½å¤Ÿå›ç­”ç‰¹å®šé—®é¢˜çš„æ‘˜è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒè¿›è¡Œæ–‡æœ¬ç†è§£çš„æƒŠäººèƒ½åŠ›ã€‚</li>
<li>LLMåœ¨æŸ¥è¯¢èšç„¦æ‘˜è¦ä¸­æœ‰ä¸¤ä¸ªé‡è¦ç‰¹æ€§ï¼šé•¿æ–‡æ¡£æ‘˜è¦å’Œç²¾ç»†ç²’åº¦æŸ¥è¯¢å¯¹é½æ•ˆç‡ã€‚</li>
<li>ä¸ºå®ç°è¿™ä¸¤ä¸ªç‰¹æ€§ï¼Œæå‡ºäº†Query-aware HyperExpertå’ŒQuery-focused Infini-attentionä¸¤ä¸ªæ¨¡å—ã€‚</li>
<li>åœ¨ç°æœ‰QFSåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>ç›¸å…³ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šä¾›å…¬ä¼—ä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.10486">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57efb7c99db9459cde1dc14c61a60e58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d13c4a1180310bcfa1a25d13a3d64b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7bf6f5f6b3dfd09a6b808e46178e712f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd01ab9fd51b70054b504c05eb842d11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff0933430503c2a6b207b4637d4021d6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="T-FREE-Subword-Tokenizer-Free-Generative-LLMs-via-Sparse-Representations-for-Memory-Efficient-Embeddings"><a href="#T-FREE-Subword-Tokenizer-Free-Generative-LLMs-via-Sparse-Representations-for-Memory-Efficient-Embeddings" class="headerlink" title="T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse   Representations for Memory-Efficient Embeddings"></a>T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse   Representations for Memory-Efficient Embeddings</h2><p><strong>Authors:BjÃ¶rn Deiseroth, Manuel Brack, Patrick Schramowski, Kristian Kersting, Samuel Weinbach</strong></p>
<p>Tokenizers are crucial for encoding information in Large Language Models, but their development has recently stagnated, and they contain inherent weaknesses. Major limitations include computational overhead, ineffective vocabulary use, and unnecessarily large embedding and head layers. Additionally, their performance is biased towards a reference corpus, leading to reduced effectiveness for underrepresented languages.   To remedy these issues, we propose T-FREE, which directly embeds words through sparse activation patterns over character triplets, and does not require a reference corpus. T-FREE inherently exploits morphological similarities and allows for strong compression of embedding layers. In our exhaustive experimental evaluation, we achieve competitive downstream performance with a parameter reduction of more than 85% on these layers. Further, T-FREE shows significant improvements in cross-lingual transfer learning. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œåˆ†è¯å™¨åœ¨ä¿¡æ¯ç¼–ç ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå…¶å¼€å‘æœ€è¿‘åœæ»ä¸å‰ï¼Œå¹¶ä¸”å­˜åœ¨å›ºæœ‰çš„å¼±ç‚¹ã€‚ä¸»è¦å±€é™æ€§åŒ…æ‹¬è®¡ç®—å¼€é”€å¤§ã€è¯æ±‡ä½¿ç”¨æ•ˆç‡ä½ä¸‹ä»¥åŠåµŒå…¥å±‚å’Œå¤´éƒ¨å±‚è¿‡å¤§ä¸”ä¸å¿…è¦ã€‚æ­¤å¤–ï¼Œå…¶æ€§èƒ½åå‘äºå‚è€ƒè¯­æ–™åº“ï¼Œå¯¼è‡´å¯¹ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€çš„æ•ˆåŠ›é™ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†T-FREEã€‚å®ƒé€šè¿‡å­—ç¬¦ä¸‰å…ƒç»„çš„ç¨€ç–æ¿€æ´»æ¨¡å¼ç›´æ¥åµŒå…¥å•è¯ï¼Œæ— éœ€å‚è€ƒè¯­æ–™åº“ã€‚T-FREEèƒ½è‡ªç„¶åœ°åˆ©ç”¨å½¢æ€ç›¸ä¼¼æ€§ï¼Œå¹¶èƒ½å¯¹åµŒå…¥å±‚è¿›è¡Œå¼ºå¤§çš„å‹ç¼©ã€‚åœ¨æˆ‘ä»¬çš„è¯¦å°½å®éªŒè¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬åœ¨è¿™äº›å±‚ä¸Šå®ç°äº†è¶…è¿‡85%çš„å‚æ•°ç¼©å‡ï¼ŒåŒæ—¶ä¸‹æ¸¸æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒT-FREEåœ¨è·¨è¯­è¨€è¿ç§»å­¦ä¹ ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.19223v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ä»¤ç‰ŒåŒ–å™¨åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­ç¼–ç ä¿¡æ¯æ—¶è‡³å…³é‡è¦ï¼Œä½†å…¶å‘å±•å·²åœæ»ï¼Œå­˜åœ¨è®¡ç®—å¼€é”€å¤§ã€è¯æ±‡ä½¿ç”¨æ•ˆç‡ä½ä¸‹ã€åµŒå…¥å±‚å’Œå¤´éƒ¨å±‚è¿‡å¤§ç­‰å†…åœ¨ç¼ºé™·ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†T-FREEï¼Œå®ƒé€šè¿‡å­—ç¬¦ä¸‰é‡çš„ç¨€ç–æ¿€æ´»æ¨¡å¼ç›´æ¥åµŒå…¥å•è¯ï¼Œä¸éœ€è¦å‚è€ƒè¯­æ–™åº“ï¼Œå¹¶æ˜¾è‘—æé«˜äº†è·¨è¯­è¨€è¿ç§»å­¦ä¹ çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä»¤ç‰ŒåŒ–å™¨åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦æ€§åŠå…¶å­˜åœ¨çš„å±€é™æ€§ï¼Œå¦‚è®¡ç®—å¼€é”€å¤§ã€è¯æ±‡ä½¿ç”¨æ•ˆç‡ä½ä¸‹ç­‰ã€‚</li>
<li>T-FREEé€šè¿‡ç›´æ¥åµŒå…¥å•è¯æ¥è§£å†³ä»¤ç‰ŒåŒ–å™¨çš„é—®é¢˜ï¼Œä¸éœ€è¦å‚è€ƒè¯­æ–™åº“ã€‚</li>
<li>T-FREEåˆ©ç”¨å½¢æ€ç›¸ä¼¼æ€§çš„å†…åœ¨ç‰¹ç‚¹ï¼Œèƒ½å¤Ÿå®ç°åµŒå…¥å±‚çš„å¼ºåŠ›å‹ç¼©ã€‚</li>
<li>åœ¨å®éªŒè¯„ä¼°ä¸­ï¼ŒT-FREEåœ¨å‚æ•°å‡å°‘è¶…è¿‡85%çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ç«äº‰æ€§çš„ä¸‹æ¸¸æ€§èƒ½ã€‚</li>
<li>T-FREEåœ¨è·¨è¯­è¨€è¿ç§»å­¦ä¹ ä¸Šè¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>T-FREEå¯¹æ¬ ä»£è¡¨è¯­è¨€çš„æ•ˆç‡æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.19223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d77d82f9a075619ed733cd018985f09f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b102b856341acfaad5d0630db3730611.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83c740c80e2c2dca674d2f1a5a66f13f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ChatBug-A-Common-Vulnerability-of-Aligned-LLMs-Induced-by-Chat-Templates"><a href="#ChatBug-A-Common-Vulnerability-of-Aligned-LLMs-Induced-by-Chat-Templates" class="headerlink" title="ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat   Templates"></a>ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat   Templates</h2><p><strong>Authors:Fengqing Jiang, Zhangchen Xu, Luyao Niu, Bill Yuchen Lin, Radha Poovendran</strong></p>
<p>Large language models (LLMs) are expected to follow instructions from users and engage in conversations. Techniques to enhance LLMsâ€™ instruction-following capabilities typically fine-tune them using data structured according to a predefined chat template. Although chat templates are shown to be effective in optimizing LLM performance, their impact on safety alignment of LLMs has been less understood, which is crucial for deploying LLMs safely at scale.   In this paper, we investigate how chat templates affect safety alignment of LLMs. We identify a common vulnerability, named ChatBug, that is introduced by chat templates. Our key insight to identify ChatBug is that the chat templates provide a rigid format that need to be followed by LLMs, but not by users. Hence, a malicious user may not necessarily follow the chat template when prompting LLMs. Instead, malicious users could leverage their knowledge of the chat template and accordingly craft their prompts to bypass safety alignments of LLMs. We develop two attacks to exploit the ChatBug vulnerability. We demonstrate that a malicious user can exploit the ChatBug vulnerability of eight state-of-the-art (SOTA) LLMs and effectively elicit unintended responses from these models. Moreover, we show that ChatBug can be exploited by existing jailbreak attacks to enhance their attack success rates. We investigate potential countermeasures to ChatBug. Our results show that while adversarial training effectively mitigates the ChatBug vulnerability, the victim model incurs significant performance degradation. These results highlight the trade-off between safety alignment and helpfulness. Developing new methods for instruction tuning to balance this trade-off is an open and critical direction for future research </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦æŒ‰ç…§ç”¨æˆ·çš„æŒ‡ç¤ºè¿›è¡Œè¿ä½œå¹¶å‚ä¸å¯¹è¯ã€‚ä¸ºæå‡LLMçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œé€šå¸¸ä¼šä½¿ç”¨æ ¹æ®é¢„å®šä¹‰èŠå¤©æ¨¡æ¿ç»“æ„åŒ–çš„æ•°æ®è¿›è¡Œå¾®è°ƒã€‚è™½ç„¶èŠå¤©æ¨¡æ¿åœ¨ä¼˜åŒ–LLMæ€§èƒ½æ–¹é¢è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒä»¬å¯¹LLMå®‰å…¨å¯¹é½çš„å½±å“å´çŸ¥ä¹‹ç”šå°‘ï¼Œè¿™å¯¹äºå¤§è§„æ¨¡å®‰å…¨éƒ¨ç½²LLMè‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†èŠå¤©æ¨¡æ¿å¦‚ä½•å½±å“LLMçš„å®‰å…¨å¯¹é½ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸€ç§ç”±èŠå¤©æ¨¡æ¿å¼•å…¥çš„å¸¸è§æ¼æ´ï¼Œç§°ä¸ºChatBugã€‚æˆ‘ä»¬è¯†åˆ«ChatBugçš„å…³é”®è§è§£æ˜¯ï¼ŒèŠå¤©æ¨¡æ¿ä¸ºLLMæä¾›äº†ä¸€ç§éœ€è¦éµå¾ªçš„å›ºå®šæ ¼å¼ï¼Œè€Œä¸æ˜¯ä¸ºç”¨æˆ·ã€‚å› æ­¤ï¼Œæ¶æ„ç”¨æˆ·ä¸ä¸€å®šä¼šæŒ‰ç…§èŠå¤©æ¨¡æ¿çš„æŒ‡ç¤ºæ¥æç¤ºLLMã€‚ç›¸åï¼Œæ¶æ„ç”¨æˆ·å¯èƒ½ä¼šåˆ©ç”¨ä»–ä»¬å¯¹èŠå¤©æ¨¡æ¿çš„çŸ¥è¯†ï¼Œç›¸åº”åœ°è®¾è®¡æç¤ºæ¥ç»•è¿‡LLMçš„å®‰å…¨å¯¹é½ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸¤ç§æ”»å‡»æ¥åˆ©ç”¨ChatBugæ¼æ´ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ¶æ„ç”¨æˆ·å¯ä»¥åˆ©ç”¨å…«ä¸ªæœ€å…ˆè¿›çš„LLMçš„ChatBugæ¼æ´ï¼Œå¹¶æœ‰æ•ˆåœ°å¼•å‘è¿™äº›æ¨¡å‹çš„ä¸é¢„æœŸå“åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒChatBugå¯ä»¥è¢«ç°æœ‰çš„è¶Šç‹±æ”»å‡»æ‰€åˆ©ç”¨ï¼Œä»¥æé«˜å…¶æ”»å‡»æˆåŠŸç‡ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†é’ˆå¯¹ChatBugçš„æ½œåœ¨å¯¹ç­–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¯¹æŠ—æ€§è®­ç»ƒæœ‰æ•ˆåœ°å‡è½»äº†ChatBugæ¼æ´ï¼Œä½†å—å®³æ¨¡å‹ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å®‰å…¨å¯¹é½å’Œå®ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ã€‚å¼€å‘æ–°çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•æ¥å¹³è¡¡è¿™ä¸€æƒè¡¡æ˜¯æœªæ¥ç ”ç©¶çš„ä¸€ä¸ªå¼€æ”¾ä¸”å…³é”®çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12935v2">PDF</a> This paper is accepted to AAAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦éµå¾ªç”¨æˆ·æŒ‡ä»¤å¹¶è¿›è¡Œå¯¹è¯ã€‚å¢å¼ºLLMæŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„æŠ€æœ¯é€šå¸¸æ˜¯é€šè¿‡ä½¿ç”¨æ ¹æ®é¢„å®šèŠå¤©æ¨¡æ¿ç»“æ„åŒ–çš„æ•°æ®å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚å°½ç®¡èŠå¤©æ¨¡æ¿åœ¨ä¼˜åŒ–LLMæ€§èƒ½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¯¹LLMå®‰å…¨å¯¹é½çš„å½±å“å´è¢«äº†è§£å¾—è¾ƒå°‘ï¼Œè¿™å¯¹äºåœ¨å¤§è§„æ¨¡å®‰å…¨éƒ¨ç½²LLMè‡³å…³é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†èŠå¤©æ¨¡æ¿å¯¹LLMå®‰å…¨å¯¹é½çš„å½±å“ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸€ç§åä¸ºChatBugçš„å¸¸è§æ¼æ´ï¼Œè¯¥æ¼æ´ç”±èŠå¤©æ¨¡æ¿å¼•å…¥ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼ŒèŠå¤©æ¨¡æ¿ä¸ºLLMsæä¾›äº†ä¸€ç§éœ€è¦éµå¾ªçš„å›ºå®šæ ¼å¼ï¼Œä½†ç”¨æˆ·åˆ™ä¸å¿…éµå¾ªã€‚å› æ­¤ï¼Œæ¶æ„ç”¨æˆ·å¯èƒ½ä¼šä¸éµå¾ªèŠå¤©æ¨¡æ¿æ¥æç¤ºLLMsã€‚ç›¸åï¼Œæ¶æ„ç”¨æˆ·å¯ä»¥åˆ©ç”¨ä»–ä»¬å¯¹èŠå¤©æ¨¡æ¿çš„äº†è§£æ¥ç›¸åº”åœ°æ„å»ºæç¤ºï¼Œä»¥ç»•è¿‡LLMçš„å®‰å…¨å¯¹é½ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸¤ç§æ”»å‡»æ¥åˆ©ç”¨ChatBugæ¼æ´ã€‚æˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªæ¶æ„ç”¨æˆ·å¯ä»¥æˆåŠŸåˆ©ç”¨å…«ç§æœ€æ–°å‰æ²¿LLMsçš„ChatBugæ¼æ´ï¼Œå¹¶æœ‰æ•ˆåœ°å¼•å‘è¿™äº›æ¨¡å‹çš„æ„å¤–ååº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜å¯ä»¥é€šè¿‡ç°æœ‰çš„è¶Šç‹±æ”»å‡»æ¥åˆ©ç”¨ChatBugæ¼æ´ä»¥å¢å¼ºå…¶æ”»å‡»æˆåŠŸç‡ã€‚æˆ‘ä»¬å¯¹å¯èƒ½åº”å¯¹ChatBugçš„æ½œåœ¨æªæ–½è¿›è¡Œäº†è°ƒæŸ¥ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¯¹æŠ—æ€§è®­ç»ƒå¯ä»¥æœ‰æ•ˆåœ°ç¼“è§£ChatBugæ¼æ´ï¼Œä½†ç›®æ ‡æ¨¡å‹ä¼šé­å—æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†å®‰å…¨å¯¹é½ä¸æœ‰ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ã€‚å¼€å‘æ–°çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•æ¥å¹³è¡¡è¿™ä¸€æƒè¡¡æ˜¯æœªæ¥ç ”ç©¶çš„é‡è¦æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>èŠå¤©æ¨¡æ¿å¯¹LLMçš„å®‰å…¨å¯¹é½å…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>è¯†åˆ«äº†ä¸€ç§åä¸ºChatBugçš„æ¼æ´ï¼Œç”±èŠå¤©æ¨¡æ¿å¼•å…¥ã€‚</li>
<li>ChatBugæ¼æ´å…è®¸æ¶æ„ç”¨æˆ·åˆ©ç”¨èŠå¤©æ¨¡æ¿çš„çŸ¥è¯†æ¥æ„é€ æç¤ºï¼Œä»¥ç»•è¿‡LLMçš„å®‰å…¨æªæ–½ã€‚</li>
<li>ChatBugæ¼æ´å¯ä»¥åœ¨å…«ç§æœ€æ–°å‰æ²¿LLMsä¸­æˆåŠŸåˆ©ç”¨ï¼Œå¹¶å¼•å‘æ„å¤–ååº”ã€‚</li>
<li>ChatBugæ¼æ´å¯ä»¥è¢«ç°æœ‰çš„è¶Šç‹±æ”»å‡»åˆ©ç”¨ï¼Œä»¥æé«˜æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>å¯¹æŠ—è®­ç»ƒå¯ä»¥æœ‰æ•ˆç¼“è§£ChatBugæ¼æ´ï¼Œä½†ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f89baa2921048db6bb41eccdc0f3f1f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-907aaef47fbff21220e6f3800137b5e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a124a987637c8be798501764b2acb8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94c6a525c479db703941852017ed2ae7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90c5a997b4f872f4d69e7c06f4f49f9b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Finer-Investigating-and-Enhancing-Fine-Grained-Visual-Concept-Recognition-in-Large-Vision-Language-Models"><a href="#Finer-Investigating-and-Enhancing-Fine-Grained-Visual-Concept-Recognition-in-Large-Vision-Language-Models" class="headerlink" title="Finer: Investigating and Enhancing Fine-Grained Visual Concept   Recognition in Large Vision Language Models"></a>Finer: Investigating and Enhancing Fine-Grained Visual Concept   Recognition in Large Vision Language Models</h2><p><strong>Authors:Jeonghwan Kim, Heng Ji</strong></p>
<p>Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the LLMs. In an effort to further the communityâ€™s endeavor in this direction, we propose a multiple granularity attribute-centric evaluation benchmark, Finer, which aims to establish a ground to evaluate LVLMsâ€™ fine-grained visual comprehension ability and provide significantly improved explainability. </p>
<blockquote>
<p>è¿‘æœŸæŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥ä½¿å¾—è¿™äº›æ¨¡å‹èƒ½å¤Ÿè½»æ¾ç”Ÿæˆé«˜æ°´å¹³çš„ã€åŸºäºå›¾åƒçš„è§£é‡Šã€‚è™½ç„¶è¿™ç§èƒ½åŠ›ä¸»è¦å½’åŠŸäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­åŒ…å«çš„ä¸°å¯Œä¸–ç•ŒçŸ¥è¯†ï¼Œä½†æˆ‘ä»¬çš„å·¥ä½œæ­ç¤ºäº†å®ƒä»¬åœ¨å…­ä¸ªä¸åŒåŸºå‡†æµ‹è¯•ç¯å¢ƒä¸­çš„ç²¾ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰çš„çŸ­æ¿ã€‚æœ€è¿‘æœ€å…ˆè¿›çš„LVLMsï¼Œå¦‚LLaVa-1.5ã€InstructBLIPå’ŒGPT-4Vï¼Œä¸ä»…åœ¨åˆ†ç±»æ€§èƒ½ä¸Šä¸¥é‡ä¸‹é™ï¼Œä¾‹å¦‚LLaVA-1.5åœ¨Stanford Dogsä¸Šçš„å¹³å‡EMä¸‹é™65.58%ï¼Œè€Œä¸”åœ¨åŸºäºè¾“å…¥å›¾åƒä¸­å‡ºç°çš„æ¦‚å¿µç”Ÿæˆå‡†ç¡®ä¸”è¯¦ç»†çš„å±æ€§è§£é‡Šæ–¹é¢æ„Ÿåˆ°å›°éš¾ï¼Œå°½ç®¡å®ƒä»¬èƒ½å¤Ÿç”Ÿæˆæ•´ä½“å›¾åƒçº§åˆ«çš„æè¿°ã€‚æ·±å…¥åˆ†ææ˜¾ç¤ºï¼ŒæŒ‡ä»¤è°ƒæ•´åçš„LVLMsè¡¨ç°å‡ºæ¨¡æ€é—´éš™ï¼Œå½“ç»™å®šä¸åŒä¸€æ¦‚å¿µç›¸å¯¹åº”çš„æ–‡æœ¬å’Œè§†è§‰è¾“å…¥æ—¶ï¼Œæ˜¾ç¤ºå‡ºå…¥å·®å¼‚ï¼Œé˜»æ­¢å›¾åƒæ¨¡æ€åˆ©ç”¨LLMsä¸­çš„ä¸°å¯Œå‚æ•°çŸ¥è¯†ã€‚ä¸ºäº†æ¨åŠ¨ç¤¾åŒºåœ¨æ­¤æ–¹å‘ä¸Šçš„åŠªåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šç²’åº¦å±æ€§ä¸­å¿ƒè¯„ä¼°åŸºå‡†Finerï¼Œæ—¨åœ¨å»ºç«‹è¯„ä¼°LVLMsçš„ç²¾ç»†ç²’åº¦è§†è§‰ç†è§£èƒ½åŠ›å¹¶æä¾›æ˜¾è‘—æ”¹è¿›çš„è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.16315v4">PDF</a> EMNLP 2024; Main Conference</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„æœ€æ–°è¿›å±•èµ‹äºˆäº†æ¨¡å‹è½»æ¾ç”Ÿæˆé«˜çº§å›¾åƒè§£é‡Šçš„èƒ½åŠ›ã€‚è™½ç„¶è¿™ç§èƒ½åŠ›ä¸»è¦å½’åŠŸäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä½†æˆ‘ä»¬çš„å·¥ä½œæ­ç¤ºäº†åœ¨è·¨å…­ä¸ªä¸åŒåŸºå‡†è®¾ç½®çš„ç²¾ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰ä¸­ï¼ŒLVLMå­˜åœ¨ä¸è¶³ã€‚æœ€å…ˆè¿›çš„LVLMå¦‚LLaVa-1.5ã€InstructBLIPå’ŒGPT-4Vä¸ä»…åœ¨åˆ†ç±»æ€§èƒ½ä¸Šä¸¥é‡ä¸‹é™ï¼ˆä¾‹å¦‚ï¼Œæ–¯å¦ç¦çŠ¬ç±»æ•°æ®é›†ä¸Šçš„å¹³å‡EMå€¼ä¸‹é™65.58ï¼‰ï¼Œè€Œä¸”åœ¨ç”ŸæˆåŸºäºè¾“å…¥å›¾åƒå†…æ¦‚å¿µè¯¦ç»†å±æ€§çš„å‡†ç¡®è§£é‡Šæ–¹é¢ä¹Ÿé‡åˆ°å›°éš¾ï¼Œå°½ç®¡å®ƒä»¬èƒ½å¤Ÿç”Ÿæˆæ•´ä½“å›¾åƒçº§åˆ«çš„æè¿°ã€‚æ·±å…¥çš„åˆ†æè¡¨æ˜ï¼ŒæŒ‡ä»¤è°ƒä¼˜çš„LVLMä¼šå‡ºç°æ¨¡æ€é¸¿æ²Ÿï¼Œåœ¨ç»™å®šä¸åŒä¸€æ¦‚å¿µç›¸å¯¹åº”çš„æ–‡æœ¬å’Œè§†è§‰è¾“å…¥æ—¶è¡¨ç°å‡ºå·®å¼‚ï¼Œé˜»æ­¢å›¾åƒæ¨¡æ€åˆ©ç”¨LLMä¸­çš„ä¸°å¯Œå‚æ•°çŸ¥è¯†ã€‚ä¸ºäº†æ¨åŠ¨ç¤¾åŒºåœ¨è¿™æ–¹é¢çš„å‘å±•ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šç²’åº¦å±æ€§ä¸ºä¸­å¿ƒçš„è¯„ä¼°åŸºå‡†Finerï¼Œæ—¨åœ¨è¯„ä¼°LVLMçš„ç²¾ç»†ç²’åº¦è§†è§‰ç†è§£èƒ½åŠ›å¹¶æä¾›æ˜¾è‘—æ”¹è¿›çš„è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰èƒ½å¤Ÿè½»æ¾ç”Ÿæˆé«˜çº§å›¾åƒè§£é‡Šï¼Œè¿™ä¸»è¦å½’åŠŸäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä¸°å¯ŒçŸ¥è¯†ã€‚</li>
<li>åœ¨ç²¾ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰æ–¹é¢ï¼Œç°æœ‰LVLMå­˜åœ¨ä¸è¶³ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨åˆ†ç±»æ€§èƒ½å’Œç”Ÿæˆè¯¦ç»†å±æ€§è§£é‡Šæ–¹é¢è¡¨ç°ä¸ä½³ã€‚</li>
<li>æŒ‡ä»¤è°ƒä¼˜çš„LVLMä¼šå‡ºç°æ¨¡æ€é¸¿æ²Ÿï¼Œå³åœ¨å¤„ç†æ–‡æœ¬å’Œè§†è§‰è¾“å…¥æ—¶å­˜åœ¨å·®è·ã€‚</li>
<li>æ¨¡æ€é¸¿æ²Ÿå¯¼è‡´å›¾åƒæ¨¡æ€æ— æ³•å……åˆ†åˆ©ç”¨LLMä¸­çš„ä¸°å¯Œå‚æ•°çŸ¥è¯†ã€‚</li>
<li>ä¸ºäº†æ”¹è¿›LVLMçš„è¯„ä¼°ï¼Œæå‡ºäº†å¤šç²’åº¦å±æ€§ä¸ºä¸­å¿ƒçš„è¯„ä¼°åŸºå‡†Finerã€‚</li>
<li>FineråŸºå‡†æ—¨åœ¨è¯„ä¼°LVLMåœ¨ç²¾ç»†ç²’åº¦è§†è§‰ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.16315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-09\./crop_LLM/2402.16315v4/page_0_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5bdc369919e0a7dcc1c84b86b88efb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b979dae96d90c6746c31cd44cde50e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69f669e17819bb172cd36e379263c6ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2c6dbef61da02c702f9c7815df2e062.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="AllSpark-A-Multimodal-Spatio-Temporal-General-Intelligence-Model-with-Ten-Modalities-via-Language-as-a-Reference-Framework"><a href="#AllSpark-A-Multimodal-Spatio-Temporal-General-Intelligence-Model-with-Ten-Modalities-via-Language-as-a-Reference-Framework" class="headerlink" title="AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with   Ten Modalities via Language as a Reference Framework"></a>AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with   Ten Modalities via Language as a Reference Framework</h2><p><strong>Authors:Run Shao, Cheng Yang, Qiujun Li, Qing Zhu, Yongjun Zhang, YanSheng Li, Yu Liu, Yong Tang, Dapeng Liu, Shizhong Yang, Haifeng Li</strong></p>
<p>Leveraging multimodal data is an inherent requirement for comprehending geographic objects. However, due to the high heterogeneity in structure and semantics among various spatio-temporal modalities, the joint interpretation of multimodal spatio-temporal data has long been an extremely challenging problem. The primary challenge resides in striking a trade-off between the cohesion and autonomy of diverse modalities. This trade-off becomes progressively nonlinear as the number of modalities expands. Inspired by the human cognitive system and linguistic philosophy, where perceptual signals from the five senses converge into language, we introduce the Language as Reference Framework (LaRF), a fundamental principle for constructing a multimodal unified model. Building upon this, we propose AllSpark, a multimodal spatio-temporal general artificial intelligence model. Our model integrates ten different modalities into a unified framework. To achieve modal cohesion, AllSpark introduces a modal bridge and multimodal large language model (LLM) to map diverse modal features into the language feature space. To maintain modality autonomy, AllSpark uses modality-specific encoders to extract the tokens of various spatio-temporal modalities. Finally, observing a gap between the modelâ€™s interpretability and downstream tasks, we designed modality-specific prompts and task heads, enhancing the modelâ€™s generalization capability across specific tasks. Experiments indicate that the incorporation of language enables AllSpark to excel in few-shot classification tasks for RGB and point cloud modalities without additional training, surpassing baseline performance by up to 41.82%. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/GeoX-Lab/AllSpark">https://github.com/GeoX-Lab/AllSpark</a>. </p>
<blockquote>
<p>åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®æ˜¯ç†è§£åœ°ç†å¯¹è±¡çš„åŸºæœ¬è¦æ±‚ã€‚ç„¶è€Œï¼Œç”±äºå„ç§æ—¶ç©ºæ¨¡æ€åœ¨ç»“æ„å’Œè¯­ä¹‰ä¸Šçš„é«˜åº¦å¼‚è´¨æ€§ï¼Œå¤šæ¨¡æ€æ—¶ç©ºæ•°æ®çš„è”åˆè§£é‡Šä¸€ç›´æ˜¯ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨å¤šç§æ¨¡æ€çš„å‡èšåŠ›å’Œè‡ªä¸»æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚éšç€æ¨¡æ€æ•°é‡çš„å¢åŠ ï¼Œè¿™ç§å¹³è¡¡é€æ¸å˜å¾—éçº¿æ€§ã€‚å—äººç±»è®¤çŸ¥ç³»ç»Ÿå’Œè¯­è¨€å­¦å“²å­¦çš„å¯å‘ï¼Œå³äº”ç§æ„Ÿå®˜çš„æ„ŸçŸ¥ä¿¡å·æ±‡èšæˆè¯­è¨€ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­è¨€å‚è€ƒæ¡†æ¶ï¼ˆLaRFï¼‰ä½œä¸ºæ„å»ºå¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹çš„åŸºæœ¬åŸç†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€æ—¶ç©ºé€šç”¨äººå·¥æ™ºèƒ½æ¨¡å‹AllSparkã€‚æˆ‘ä»¬çš„æ¨¡å‹å°†åç§ä¸åŒçš„æ¨¡æ€é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚ä¸ºäº†å®ç°æ¨¡æ€å‡èšåŠ›ï¼ŒAllSparkå¼•å…¥äº†ä¸€ä¸ªæ¨¡æ€æ¡¥å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå°†å„ç§æ¨¡æ€ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€ç‰¹å¾ç©ºé—´ä¸­ã€‚ä¸ºäº†ä¿æŒæ¨¡æ€è‡ªä¸»æ€§ï¼ŒAllSparkä½¿ç”¨ç‰¹å®šæ¨¡æ€ç¼–ç å™¨æ¥æå–å„ç§æ—¶ç©ºæ¨¡æ€çš„æ ‡è®°ã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸ä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´å­˜åœ¨å·®è·ï¼Œå› æ­¤æˆ‘ä»¬è®¾è®¡äº†ç‰¹å®šäºæ¨¡æ€çš„æç¤ºå’Œä»»åŠ¡å¤´ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯­è¨€çš„èå…¥ä½¿AllSparkåœ¨RGBå’Œç‚¹äº‘æ¨¡æ€çš„å°‘é‡åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯è¶…è¶ŠåŸºçº¿æ€§èƒ½é«˜è¾¾41.82%ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GeoX-Lab/AllSpark">https://github.com/GeoX-Lab/AllSpark</a>ä¸­è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.00546v3">PDF</a> 19 pages, 19 tables, 3 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>èåˆå¤šæ¨¡æ€æ•°æ®æ˜¯ç†è§£åœ°ç†å¯¹è±¡çš„åŸºæœ¬éœ€æ±‚ã€‚ç„¶è€Œï¼Œç”±äºæ—¶ç©ºå¤šæ¨¡æ€æ•°æ®çš„ç»“æ„å’Œè¯­ä¹‰é«˜åº¦å¼‚æ„ï¼Œå¤šæ¨¡æ€æ—¶ç©ºæ•°æ®çš„è”åˆè§£é‡Šä¸€ç›´æ˜¯ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•åœ¨å¤šç§æ¨¡æ€çš„è¿è´¯æ€§å’Œè‡ªä¸»æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚éšç€æ¨¡æ€æ•°é‡çš„å¢åŠ ï¼Œè¿™ç§å¹³è¡¡å˜å¾—æ„ˆåŠ éçº¿æ€§ã€‚å—äººç±»è®¤çŸ¥ç³»ç»Ÿå’Œè¯­è¨€å“²å­¦çš„å¯å‘ï¼Œå³äº”ç§æ„ŸçŸ¥ä¿¡å·æ±‡èšæˆè¯­è¨€ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­è¨€å‚ç…§æ¡†æ¶ï¼ˆLaRFï¼‰ä½œä¸ºæ„å»ºå¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹çš„åŸºæœ¬åŸåˆ™ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€æ—¶ç©ºé€šç”¨äººå·¥æ™ºèƒ½æ¨¡å‹AllSparkã€‚è¯¥æ¨¡å‹æ•´åˆäº†åç§ä¸åŒæ¨¡æ€åˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚ä¸ºå®ç°æ¨¡æ€è¿è´¯æ€§ï¼ŒAllSparkå¼•å…¥æ¨¡æ€æ¡¥å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå°†å„ç§æ¨¡æ€ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€ç‰¹å¾ç©ºé—´ã€‚ä¸ºäº†ä¿æŒæ¨¡æ€è‡ªä¸»æ€§ï¼ŒAllSparkä½¿ç”¨ç‰¹å®šæ¨¡æ€ç¼–ç å™¨æå–å„ç§æ—¶ç©ºæ¨¡æ€çš„æ ‡è®°ã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸ä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´å­˜åœ¨å·®è·ï¼Œå› æ­¤è®¾è®¡äº†ç‰¹å®šæ¨¡æ€çš„æç¤ºå’Œä»»åŠ¡å¤´ï¼Œå¢å¼ºæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒåŠ å…¥è¯­è¨€åŠŸèƒ½ä½¿AllSparkåœ¨RGBå’Œç‚¹äº‘æ¨¡æ€çš„å°‘é‡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯è¶…è¶ŠåŸºçº¿æ€§èƒ½é«˜è¾¾41.82%ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GeoX-Lab/AllSpark%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/GeoX-Lab/AllSparkè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ•°æ®ç†è§£åœ°ç†å¯¹è±¡æ˜¯å†…åœ¨éœ€æ±‚ï¼Œä½†å¤šæ¨¡æ€æ—¶ç©ºæ•°æ®è”åˆè§£é‡Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨å¤šæ ·æ¨¡æ€çš„è¿è´¯æ€§å’Œè‡ªä¸»æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚</li>
<li>LaRFï¼ˆè¯­è¨€ä½œä¸ºå‚è€ƒæ¡†æ¶ï¼‰ä¸ºæ„å»ºå¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹æä¾›äº†åŸºæœ¬åŸåˆ™ã€‚</li>
<li>AllSparkæ¨¡å‹æ•´åˆäº†å¤šç§æ¨¡æ€ï¼Œå®ç°æ¨¡æ€è¿è´¯æ€§å¹¶ç»´æŒè‡ªä¸»æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ¨¡æ€æ¡¥å’Œå¤šæ¨¡æ€LLMï¼ŒAllSparkæˆåŠŸå°†ä¸åŒæ¨¡æ€ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€ç‰¹å¾ç©ºé—´ã€‚</li>
<li>é€šè¿‡ç‰¹å®šæ¨¡æ€çš„æç¤ºå’Œä»»åŠ¡å¤´ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œä»»åŠ¡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.00546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-910f6bb38ce785b60dd4b1f4d18018c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b5fb3a0a122dfd9a85e17252e39c6d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7857ba403395d3698a7e4561d0e36887.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92010c681613d7f5afe6ffa420c7f1b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0787e811b3dddb4fcd4815c7041fc1d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3b864e1052c8576afc40e2cdaba5ef52.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-09  LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-08/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8ad49455490d5e1050a9cc5faa8e9545.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-08  RDD4D 4D Attention-Guided Road Damage Detection And Classification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
