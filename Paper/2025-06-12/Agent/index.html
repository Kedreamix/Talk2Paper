<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-47047f50c44cb37898ed12da40830360.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-12-æ›´æ–°"><a href="#2025-06-12-æ›´æ–°" class="headerlink" title="2025-06-12 æ›´æ–°"></a>2025-06-12 æ›´æ–°</h1><h2 id="VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning"><a href="#VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning" class="headerlink" title="VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning"></a>VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning</h2><p><strong>Authors:Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</strong></p>
<p>Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems. </p>
<blockquote>
<p>åœ¨åŠ¨æ€ç¯å¢ƒä¸­åè°ƒå¤šä¸ªå®ä½“ä»£ç†ä»ç„¶æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè¿™éœ€è¦æ„ŸçŸ¥é©±åŠ¨çš„æ¨ç†å’Œå¯æ‰©å±•çš„åˆä½œç­–ç•¥ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶å·²ç»åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤šä»£ç†è§„åˆ’ï¼Œä½†å¾ˆå°‘æœ‰äººå¼€å§‹æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”¨äºè§†è§‰æ¨ç†ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºäºVLMçš„æ–¹æ³•åœ¨æ”¯æŒå¤šç§å®ä½“ç±»å‹æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“é—¨ä¸ºå®ä½“å¤šä»£ç†åˆä½œå®šåˆ¶çš„åˆ†å±‚åŸºå‡†â€”â€”VIKI-Benchï¼Œå®ƒåŒ…å«ä¸‰ä¸ªç»“æ„åŒ–çº§åˆ«ï¼šä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ã€‚VIKI-BenchåŒ…æ‹¬å¤šç§æœºå™¨äººå®ä½“ã€å¤šè§†è§’è§†è§‰è§‚å¯Ÿå’Œç»“æ„åŒ–çš„ç›‘ç£ä¿¡å·ï¼Œä»¥è¯„ä¼°åŸºäºè§†è§‰è¾“å…¥çš„æ¨ç†ã€‚ä¸ºäº†è¯æ˜VIKI-Benchçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†VIKI-Rï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ€ç»´é“¾æ³¨é‡Šæ¼”ç¤ºå¯¹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œç„¶ååœ¨å¤šå±‚æ¬¡å¥–åŠ±ä¿¡å·ä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVIKI-Råœ¨æ‰€æœ‰ä»»åŠ¡å±‚é¢éƒ½æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿä¿ƒè¿›å¼‚æ„ä»£ç†ä¹‹é—´ç»„åˆåˆä½œæ¨¡å¼çš„å‡ºç°ã€‚æ€»ä¹‹ï¼ŒVIKI-Benchå’ŒVIKI-Rä¸ºæ¨è¿›å®ä½“AIç³»ç»Ÿçš„å¤šä»£ç†ã€è§†è§‰é©±åŠ¨åˆä½œæä¾›äº†ç»Ÿä¸€çš„æµ‹è¯•å¹³å°å’Œæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09049v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://faceong.github.io/VIKI-R/">https://faceong.github.io/VIKI-R/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸä¸­ï¼Œåè°ƒå¤šä¸ªå®ä½“ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ„ŸçŸ¥é©±åŠ¨çš„æ¨ç†å’Œå¯æ‰©å±•çš„åˆä½œç­–ç•¥ã€‚è™½ç„¶å·²æœ‰å·¥ä½œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šä»£ç†è§„åˆ’ï¼Œä½†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†æ–¹æ³•ä»æœ‰é™æ”¯æŒå¤šç§å®ä½“ç±»å‹ã€‚æœ¬æ–‡å¼•å…¥VIKI-Benchï¼Œé¦–ä¸ªé’ˆå¯¹å®ä½“å¤šä»£ç†åˆä½œçš„åˆ†å±‚åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ä¸‰ä¸ªç»“æ„åŒ–çº§åˆ«ã€‚VIKI-BenchåŒ…å«å¤šç§æœºå™¨äººå®ä½“ã€å¤šè§†è§’è§†è§‰è§‚å¯Ÿå’Œç»“æ„åŒ–çš„ç›‘ç£ä¿¡å·ï¼Œä»¥è¯„ä¼°åŸºäºè§†è§‰è¾“å…¥çš„æ¨ç†ã€‚ä¸ºå±•ç¤ºVIKI-Benchçš„å®ç”¨æ€§ï¼Œæœ¬æ–‡æå‡ºVIKI-Rï¼Œä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨æ€ç»´é“¾æ³¨è§£æ¼”ç¤ºï¼Œå†é€šè¿‡å¤šçº§åˆ«å¥–åŠ±ä¿¡å·è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒVIKI-Råœ¨å„çº§ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¼ºåŒ–å­¦ä¹ ä¿ƒä½¿å¼‚è´¨ä»£ç†é—´å‡ºç°ç»„åˆåˆä½œæ¨¡å¼ã€‚æ€»ä¹‹ï¼ŒVIKI-Benchå’ŒVIKI-Rä¸ºæ¨è¿›å¤šä»£ç†ã€è§†è§‰é©±åŠ¨çš„åˆä½œå®ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ç»Ÿä¸€çš„æµ‹è¯•åºŠå’Œæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åè°ƒå¤šä¸ªå®ä½“ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œéœ€ç»“åˆæ„ŸçŸ¥é©±åŠ¨çš„æ¨ç†å’Œåˆä½œç­–ç•¥ã€‚</li>
<li>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹å·²ç”¨äºå¤šä»£ç†è§„åˆ’ï¼Œä½†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†æ–¹æ³•ä»éœ€æ¢ç´¢æ›´å¤šé¢†åŸŸã€‚</li>
<li>VIKI-Benchæ˜¯é¦–ä¸ªé’ˆå¯¹å®ä½“å¤šä»£ç†åˆä½œçš„åˆ†å±‚åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ç­‰ç»“æ„åŒ–çº§åˆ«ã€‚</li>
<li>VIKI-Benchæ³¨é‡å¤šæ ·æœºå™¨äººå®ä½“ã€å¤šè§†è§’è§†è§‰è§‚å¯Ÿå’Œç»“æ„åŒ–çš„ç›‘ç£ä¿¡å·çš„è¯„ä¼°ã€‚</li>
<li>VIKI-Ræ¡†æ¶ç»“åˆäº†é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„è°ƒæ•´ã€æ€ç»´é“¾æ³¨è§£æ¼”ç¤ºå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæé«˜äº†å¤šä»£ç†ä»»åŠ¡çš„è¡¨ç°ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æœ‰åŠ©äºå¼‚è´¨ä»£ç†é—´å‡ºç°ç»„åˆåˆä½œæ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f318d1c1f344134a0ccdcac656830d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b4ff7c2f9df7c67670ee1acf6207be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0a3569e6a044d60b77d21c9a1d0f4e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-546e911c893917079b05fe314b9dedc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-493cacde6faf7b9e2c94ba6ec86deaf5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Atomic-to-Compositional-Generalization-for-Mobile-Agents-with-A-New-Benchmark-and-Scheduling-System"><a href="#Atomic-to-Compositional-Generalization-for-Mobile-Agents-with-A-New-Benchmark-and-Scheduling-System" class="headerlink" title="Atomic-to-Compositional Generalization for Mobile Agents with A New   Benchmark and Scheduling System"></a>Atomic-to-Compositional Generalization for Mobile Agents with A New   Benchmark and Scheduling System</h2><p><strong>Authors:Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, Zhuosheng Zhang</strong></p>
<p>Autonomous agents powered by multimodal large language models have been developed to facilitate task execution on mobile devices. However, prior work has predominantly focused on atomic tasks â€“ such as shot-chain execution tasks and single-screen grounding tasks â€“ while overlooking the generalization to compositional tasks, which are indispensable for real-world applications. This work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile agents on three categories of compositional operations: Simple Concatenation, Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in 20 fully controllable local utility app environments, as well as 30 online Chinese and English service apps. It comprises 100 interactive task templates with an average optimal step count of 14.05. Experimental results across a range of mobile agents with agentic workflow or agent-as-a-model show that UI-NEXUS presents significant challenges. Specifically, existing agents generally struggle to balance performance and efficiency, exhibiting representative failure modes such as under-execution, over-execution, and attention drift, causing visible atomic-to-compositional generalization gap. Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient scheduling system to tackle compositional mobile tasks. AGENT-NEXUS extrapolates the abilities of existing mobile agents by dynamically decomposing long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS achieves 24% to 40% task success rate improvement for existing mobile agents on compositional operation tasks within the UI-NEXUS benchmark without significantly sacrificing inference overhead. The demo video, dataset, and code are available on the project page at <a target="_blank" rel="noopener" href="https://ui-nexus.github.io/">https://ui-nexus.github.io</a>. </p>
<blockquote>
<p>ç”±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è‡ªä¸»ä½“å·²è¢«å¼€å‘å‡ºæ¥ï¼Œä»¥åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šä¿ƒè¿›ä»»åŠ¡æ‰§è¡Œã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åŸå­ä»»åŠ¡ä¸Šï¼Œå¦‚å°„å‡»é“¾æ‰§è¡Œä»»åŠ¡å’Œå•å±å¹•æ¥åœ°ä»»åŠ¡ï¼Œè€Œå¿½ç•¥äº†å¯¹ç»„åˆä»»åŠ¡çš„æ¨å¹¿ï¼Œè¿™å¯¹äºå®é™…åº”ç”¨æ¥è¯´æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚æœ¬æ–‡ä»‹ç»äº†UI-NEXUSï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç§»åŠ¨ä½“åœ¨ä¸‰ç±»ç»„åˆæ“ä½œä¸Šçš„è¡¨ç°ï¼šç®€å•è¿æ¥ã€ä¸Šä¸‹æ–‡è½¬æ¢å’Œæ·±åº¦æ½œæ°´ã€‚UI-NEXUSæ”¯æŒåœ¨20ä¸ªå®Œå…¨å¯æ§çš„æœ¬åœ°å®ç”¨åº”ç”¨ç¨‹åºç¯å¢ƒä¸­çš„äº¤äº’å¼è¯„ä¼°ï¼Œä»¥åŠ30ä¸ªåœ¨çº¿ä¸­æ–‡å’Œè‹±æ–‡æœåŠ¡åº”ç”¨ç¨‹åºã€‚å®ƒåŒ…æ‹¬100ä¸ªäº¤äº’å¼ä»»åŠ¡æ¨¡æ¿ï¼Œå¹³å‡æœ€ä½³æ­¥éª¤è®¡æ•°ä¸º14.05ã€‚è·¨è¶Šä¸€ç³»åˆ—å…·æœ‰è‡ªä¸»å·¥ä½œæµç¨‹æˆ–ä»£ç†æ¨¡å‹çš„ç§»åŠ¨ä»£ç†çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUI-NEXUSå­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œç°æœ‰ä»£ç†é€šå¸¸åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´æŒ£æ‰ï¼Œè¡¨ç°å‡ºå…¸å‹çš„å¤±è´¥æ¨¡å¼ï¼Œå¦‚æ‰§è¡Œä¸è¶³ã€è¿‡åº¦æ‰§è¡Œå’Œæ³¨æ„åŠ›æ¼‚ç§»ï¼Œå¯¼è‡´æ˜æ˜¾çš„ä»åŸå­åˆ°ç»„åˆçš„ä¸€èˆ¬åŒ–å·®è·ã€‚æ ¹æ®è¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†AGENT-NEXUSï¼Œè¿™æ˜¯ä¸€ä¸ªè½»ä¾¿é«˜æ•ˆçš„è°ƒåº¦ç³»ç»Ÿï¼Œç”¨äºè§£å†³ç»„åˆç§»åŠ¨ä»»åŠ¡ã€‚AGENT-NEXUSé€šè¿‡åŠ¨æ€åœ°å°†é•¿å‘¨æœŸä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—ç‹¬ç«‹çš„åŸå­å­ä»»åŠ¡æ¥æ¨æ–­ç°æœ‰ç§»åŠ¨ä»£ç†çš„èƒ½åŠ›ã€‚åœ¨UI-NEXUSåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAGENT-NEXUSåœ¨ä¸æ˜¾è‘—å¢åŠ æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹ï¼Œä½¿ç°æœ‰ç§»åŠ¨ä»£ç†åœ¨ç»„åˆæ“ä½œä»»åŠ¡ä¸Šçš„ä»»åŠ¡æˆåŠŸç‡æé«˜äº†24%è‡³40%ã€‚æ¼”ç¤ºè§†é¢‘ã€æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://ui-nexus.github.ioé¡¹ç›®é¡µé¦–ä¸Šæ‰¾åˆ°./">https://ui-nexus.github.ioé¡¹ç›®é¡µé¢ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08972v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cccc140d6eb55396e85a7548cc388e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8ff7901492a4a68c1ad2895092234e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8434d8b9bd25410d7dfd7acb7e3fd8ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4b8f0edbd91b19b5774ebbebe761800.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="What-Limits-Virtual-Agent-Application-OmniBench-A-Scalable-Multi-Dimensional-Benchmark-for-Essential-Virtual-Agent-Capabilities"><a href="#What-Limits-Virtual-Agent-Application-OmniBench-A-Scalable-Multi-Dimensional-Benchmark-for-Essential-Virtual-Agent-Capabilities" class="headerlink" title="What Limits Virtual Agent Application? OmniBench: A Scalable   Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities"></a>What Limits Virtual Agent Application? OmniBench: A Scalable   Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities</h2><p><strong>Authors:Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation with limited scenarios, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, cross-platform, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91% human acceptance rate. Training on our graph-structured data shows that it can more efficiently guide agents compared to manually annotated data. We conduct multidimensional evaluations for various open-source and closed-source models, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at <a target="_blank" rel="noopener" href="https://omni-bench.github.io/">https://omni-bench.github.io/</a>. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¸æ–­å‘å±•ï¼ŒåŸºäºMLLMçš„è™šæ‹Ÿä»£ç†å·²ç»è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä»»åŠ¡å¤æ‚æ€§ä¸å¯æ§ã€åœºæ™¯æœ‰é™ä¸”éœ€è¦å¤§é‡æ‰‹åŠ¨æ ‡æ³¨ï¼Œä»¥åŠç¼ºä¹å¤šç»´è¯„ä¼°ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªæˆ‘ç”Ÿæˆã€è·¨å¹³å°ã€åŸºäºå›¾å½¢çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å­ä»»åŠ¡ç»„åˆï¼Œå…·æœ‰åˆæˆå¯æ§å¤æ‚åº¦ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ç®¡é“ã€‚ä¸ºäº†è¯„ä¼°è™šæ‹Ÿä»£ç†åœ¨å›¾ä¸Šçš„å„ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†OmniEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šç»´è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬å­ä»»åŠ¡çº§è¯„ä¼°ã€åŸºäºå›¾å½¢çš„æŒ‡æ ‡å’Œè·¨è¶Š10ç§èƒ½åŠ›çš„ç»¼åˆæµ‹è¯•ã€‚æˆ‘ä»¬åˆæˆçš„æ•°æ®é›†åŒ…å«20ä¸ªåœºæ™¯ä¸‹çš„3.6ä¸‡å›¾å½¢ç»“æ„ä»»åŠ¡ï¼Œè¾¾åˆ°äº†91%çš„äººç±»æ¥å—ç‡ã€‚åœ¨æˆ‘ä»¬çš„å›¾å½¢ç»“æ„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒè¡¨æ˜ï¼Œä¸æ‰‹åŠ¨æ³¨é‡Šçš„æ•°æ®ç›¸æ¯”ï¼Œå®ƒå¯ä»¥æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼ä»£ç†ã€‚æˆ‘ä»¬å¯¹å„ç§å¼€æºå’Œé—­æºæ¨¡å‹è¿›è¡Œäº†å¤šç»´è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å„ç§èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼Œä¸ºæœªæ¥çš„è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://omni-bench.github.io/%E8%AE%BF%E9%97%AE%E3%80%82">https://omni-bench.github.io/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08933v1">PDF</a> Accepted by ICML 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•ï¼ŒåŸºäºMLLMçš„è™šæ‹Ÿä»£ç†å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•é¢ä¸´è¯¸å¤šå±€é™æ€§ï¼ŒåŒ…æ‹¬ä»»åŠ¡å¤æ‚æ€§ä¸å¯æ§ã€åœºæ™¯æœ‰é™ä¸”éœ€å¤§é‡æ‰‹åŠ¨æ ‡æ³¨ï¼Œä»¥åŠç¼ºä¹å¤šç»´åº¦è¯„ä¼°ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºOmniBenchï¼Œä¸€ä¸ªè‡ªæˆ‘ç”Ÿæˆã€è·¨å¹³å°ã€åŸºäºå›¾è°±çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å­ä»»åŠ¡ç»„åˆæ¥åˆæˆå¯æ§å¤æ‚åº¦çš„ä»»åŠ¡ã€‚ä¸ºè¯„ä¼°è™šæ‹Ÿä»£ç†åœ¨å›¾è°±ä¸Šçš„å¤šå…ƒèƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºOmniEvalï¼Œä¸€ä¸ªåŒ…å«å­ä»»åŠ¡çº§åˆ«è¯„ä¼°ã€åŸºäºå›¾è°±çš„æŒ‡æ ‡å’Œå…¨é¢æµ‹è¯•10ç§èƒ½åŠ›çš„å¤šç»´åº¦è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬çš„åˆæˆæ•°æ®é›†åŒ…å«20ä¸ªåœºæ™¯ä¸‹çš„3.6ä¸‡å¼ å›¾è°±ä»»åŠ¡ï¼Œè¾¾åˆ°91%çš„äººç±»æ¥å—ç‡ã€‚ä½¿ç”¨æˆ‘ä»¬çš„å›¾è°±æ•°æ®è¿›è¡Œè®­ç»ƒè¡¨æ˜ï¼Œä¸æ‰‹åŠ¨æ³¨é‡Šæ•°æ®ç›¸æ¯”ï¼Œå®ƒå¯ä»¥æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼ä»£ç†ã€‚æˆ‘ä»¬å¯¹å„ç§å¼€æºå’Œé—­æºæ¨¡å‹è¿›è¡Œäº†å¤šç»´åº¦è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å„ç§èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œä¸ºæœªæ¥å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è™šæ‹Ÿä»£ç†è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨ä»»åŠ¡å¤æ‚æ€§ä¸å¯æ§ã€åœºæ™¯æœ‰é™å’Œæ‰‹åŠ¨æ ‡æ³¨é‡å¤§ç­‰å±€é™æ€§ã€‚</li>
<li>OmniBenchæ˜¯ä¸€ä¸ªè‡ªæˆ‘ç”Ÿæˆã€è·¨å¹³å°ã€åŸºäºå›¾è°±çš„åŸºå‡†æµ‹è¯•ï¼Œå¯åˆæˆå¯æ§å¤æ‚åº¦çš„ä»»åŠ¡ã€‚</li>
<li>OmniEvalæ˜¯ä¸€ä¸ªå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è™šæ‹Ÿä»£ç†åœ¨å›¾è°±ä¸Šçš„å¤šå…ƒèƒ½åŠ›ã€‚</li>
<li>åˆæˆæ•°æ®é›†åŒ…å«3.6ä¸‡å¼ å›¾è°±ä»»åŠ¡ï¼Œäººç±»æ¥å—ç‡é«˜è¾¾91%ã€‚</li>
<li>ä¸æ‰‹åŠ¨æ³¨é‡Šæ•°æ®ç›¸æ¯”ï¼Œä½¿ç”¨å›¾è°±æ•°æ®è¿›è¡Œè®­ç»ƒå¯ä»¥æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼è™šæ‹Ÿä»£ç†ã€‚</li>
<li>å¤šç»´åº¦è¯„ä¼°äº†å„ç§æ¨¡å‹åœ¨å¤šç§èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08933">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94fefb1b6723dc45026428f922a131a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ab7505d86fb36096182b108efafa352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f2cf80efd8feb950a53f38fd70daf5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e18f0ce0b4c569fb238440dd97fd3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bd8bf27b29d78336e7dd18a203af15c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35206aa3883c2327bd0e60b86a68b4cd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Improved-LLM-Agents-for-Financial-Document-Question-Answering"><a href="#Improved-LLM-Agents-for-Financial-Document-Question-Answering" class="headerlink" title="Improved LLM Agents for Financial Document Question Answering"></a>Improved LLM Agents for Financial Document Question Answering</h2><p><strong>Authors:Nelvin Tan, Zian Seng, Liang Zhang, Yu-Ching Shih, Dong Yang, Amol Salunkhe</strong></p>
<p>Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agentâ€™s performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMåœ¨å¤„ç†åŒ…å«è¡¨æ ¼å’Œæ–‡æœ¬æ•°æ®çš„é‡‘èæ–‡æ¡£çš„æ•°å€¼é—®ç­”æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿‘æœŸçš„ç ”ç©¶å·²ç»æ˜¾ç¤ºäº†æ‰¹è¯„ä»£ç†ï¼ˆå³è‡ªæˆ‘æ ¡æ­£ï¼‰åœ¨è¯¥ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œå‰ææ˜¯å¿…é¡»æœ‰æ­£ç¡®çš„æ ‡ç­¾ã€‚æœ¬æ–‡åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶äº†å½“æ²¡æœ‰æ­£ç¡®æ ‡ç­¾æ—¶ä¼ ç»Ÿæ‰¹è¯„ä»£ç†çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡å®éªŒè¡¨æ˜åœ¨è¿™ç§æƒ…å†µä¸‹æ‰¹è¯„ä»£ç†çš„æ€§èƒ½ä¼šä¸‹é™ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ‰¹è¯„ä»£ç†å’Œè®¡ç®—å™¨ä»£ç†ï¼Œè®¡ç®—å™¨ä»£ç†çš„æ€§èƒ½è¶…è¿‡äº†ä¹‹å‰çš„æœ€æ–°æ–¹æ³•ï¼ˆæ€ç»´ç¼–ç¨‹ï¼‰ï¼Œå¹¶ä¸”æ›´åŠ å®‰å…¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†è¿™ä¸¤ä¸ªä»£ç†ä¹‹é—´çš„äº¤äº’æ–¹å¼ä»¥åŠè¿™ç§äº¤äº’å¦‚ä½•å½±å“å®ƒä»¬çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08726v1">PDF</a> 12 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨é‡‘èæ–‡æ¡£çš„æ•°å€¼é—®ç­”ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹æ ‡å‡†æ ‡ç­¾çš„æƒ…å†µã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å½“æ²¡æœ‰æ ‡å‡†æ ‡ç­¾æ—¶ä¼ ç»Ÿæ‰¹è¯„ä»£ç†çš„è¡¨ç°ä¸‹é™çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ‰¹è¯„ä»£ç†å’Œè®¡ç®—å™¨ä»£ç†ã€‚è¯¥æ–°æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„ç¨‹åºæ€ç»´æ–¹æ³•çš„æ€§èƒ½å¹¶å…·æœ‰è¾ƒé«˜çš„å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æ¢è®¨äº†è¿™äº›ä»£ç†ä¹‹é—´çš„ç›¸äº’ä½œç”¨åŠå…¶å¯¹æ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨é‡‘èæ–‡æ¡£çš„æ•°å€¼é—®ç­”æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ‰¹è¯„ä»£ç†åœ¨ç¼ºä¹æ ‡å‡†æ ‡ç­¾æ—¶çš„æ€§èƒ½ä¼šä¸‹é™ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ‰¹è¯„ä»£ç†å’Œè®¡ç®—å™¨ä»£ç†ï¼Œè¯¥æ–¹æ³•åœ¨å®‰å…¨æ€§å’Œæ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„ç¨‹åºæ€ç»´æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†ä»£ç†ä¹‹é—´çš„ç›¸äº’ä½œç”¨åŠå…¶å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>æ”¹è¿›åçš„æ‰¹è¯„ä»£ç†å’Œè®¡ç®—å™¨ä»£ç†èƒ½æœ‰æ•ˆå¤„ç†é‡‘èæ–‡æ¡£ä¸­çš„è¡¨æ ¼å’Œæ–‡æœ¬æ•°æ®ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºé‡‘èæ–‡æ¡£çš„æ•°å€¼é—®ç­”ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c37e6766b1bcc22e0bd3470f39be91b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-363a8fffc27cbea629aa219d1698d557.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce05c720610589cf08383e82c2203467.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MasHost-Builds-It-All-Autonomous-Multi-Agent-System-Directed-by-Reinforcement-Learning"><a href="#MasHost-Builds-It-All-Autonomous-Multi-Agent-System-Directed-by-Reinforcement-Learning" class="headerlink" title="MasHost Builds It All: Autonomous Multi-Agent System Directed by   Reinforcement Learning"></a>MasHost Builds It All: Autonomous Multi-Agent System Directed by   Reinforcement Learning</h2><p><strong>Authors:Kuo Yang, Xingjie Yang, Linhui Yu, Qing Xu, Yan Fang, Xu Wang, Zhengyang Zhou, Yang Wang</strong></p>
<p>Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently emerged as a powerful paradigm for tackling complex real-world tasks. However, existing Mas construction methods typically rely on manually crafted interaction mechanisms or heuristic rules, introducing human biases and constraining the autonomous ability. Even with recent advances in adaptive Mas construction, existing systems largely remain within the paradigm of semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement Learning (RL)-based framework for autonomous and query-adaptive Mas design. By formulating Mas construction as a graph search problem, our proposed MasHost jointly samples agent roles and their interactions through a unified probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives pursued in prior works, we introduce component rationality as an additional and novel design principle in Mas. To achieve this multi-objective optimization, we propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy that collaboratively integrates group-relative advantages and action-wise rewards. To our knowledge, our proposed MasHost is the first RL-driven framework for autonomous Mas graph construction. Extensive experiments on six benchmarks demonstrate that MasHost consistently outperforms most competitive baselines, validating its effectiveness, efficiency, and structure rationality. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMasï¼‰æœ€è¿‘å‡ºç°ä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œç”¨äºå¤„ç†å¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„Masæ„å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨åˆ¶ä½œçš„äº¤äº’æœºåˆ¶æˆ–å¯å‘å¼è§„åˆ™ï¼Œè¿™å¼•å…¥äº†äººç±»åè§å¹¶é™åˆ¶äº†è‡ªä¸»æ€§èƒ½åŠ›ã€‚å°½ç®¡æœ€è¿‘åœ¨è‡ªé€‚åº”Masæ„å»ºæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰ç³»ç»Ÿå¤§å¤šä»ä¿æŒåœ¨åŠè‡ªä¸»æ¨¡å¼çš„èŒƒå¼å†…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MasHostï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªä¸»å’ŒæŸ¥è¯¢è‡ªé€‚åº”Masè®¾è®¡æ¡†æ¶ã€‚é€šè¿‡å°†Masæ„å»ºå…¬å¼åŒ–ä¸ºå›¾æœç´¢é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºçš„MasHosté€šè¿‡ç»Ÿä¸€çš„æ¦‚ç‡é‡‡æ ·æœºåˆ¶è”åˆé‡‡æ ·æ™ºèƒ½ä½“çš„è§’è‰²å’Œå®ƒä»¬çš„äº¤äº’ã€‚é™¤äº†å…ˆå‰å·¥ä½œä¸­è¿½æ±‚å‡†ç¡®æ€§å’Œæ•ˆç‡ç›®æ ‡ä¹‹å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»„ä»¶åˆç†æ€§ä½œä¸ºMasä¸­çš„é™„åŠ å’Œæ–°é¢–çš„è®¾è®¡åŸåˆ™ã€‚ä¸ºäº†å®ç°è¿™ç§å¤šç›®æ ‡ä¼˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆHRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„RLç­–ç•¥ï¼Œèƒ½å¤ŸååŒæ•´åˆç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿å’Œè¡ŒåŠ¨æ–¹é¢çš„å¥–åŠ±ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æå‡ºçš„MasHostæ˜¯ç¬¬ä¸€ä¸ªç”¨äºè‡ªä¸»Maså›¾æ„å»ºRLé©±åŠ¨çš„æ¡†æ¶ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMasHostå§‹ç»ˆä¼˜äºå¤§å¤šæ•°ç«äº‰åŸºçº¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œç»“æ„åˆç†æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08507v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è·¨ä¸»ä½“ç³»ç»Ÿé¢ä¸´äººå·¥äº¤äº’æœºåˆ¶å’Œå¯å‘å¼è§„åˆ™çš„å±€é™æ€§ï¼Œè¿™å¸¦æ¥äº†äººä¸ºåè§å¹¶é™åˆ¶äº†è‡ªä¸»æ€§ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†MasHostæ¡†æ¶ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯è‡ªä¸»è®¾è®¡å’Œæ„å»ºå¤šä¸»ä½“ç³»ç»Ÿï¼Œå¹¶å°†è¿™ä¸ªè¿‡ç¨‹å…¬å¼åŒ–ä¸ºå›¾å½¢æœç´¢é—®é¢˜ã€‚æ–°çš„ä¼˜åŒ–æ–¹æ³•å°†å‡†ç¡®æ€§ã€æ•ˆç‡å’Œç»„ä»¶åˆç†æ€§ç›¸ç»“åˆï¼Œé‡‡ç”¨åˆ†å±‚ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼ŒMasHoståœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œç»“æ„åˆç†æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šä¸»ä½“ç³»ç»Ÿï¼ˆMasï¼‰æ˜¯å¤„ç†å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡çš„æœ‰åŠ›å·¥å…·ã€‚</li>
<li>ä¼ ç»Ÿæ„å»ºæ–¹æ³•ä¾èµ–äººä¸ºè®¾è®¡çš„äº¤äº’æœºåˆ¶å’Œå¯å‘å¼è§„åˆ™ï¼Œå­˜åœ¨äººä¸ºåè§å’Œè‡ªä¸»æ€§çš„é™åˆ¶ã€‚</li>
<li>MasHostæ¡†æ¶é¦–æ¬¡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯è‡ªä¸»è®¾è®¡å’Œæ„å»ºå¤šä¸»ä½“ç³»ç»Ÿï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>MasHosté€šè¿‡å›¾å½¢æœç´¢å…¬å¼åŒ–è¿‡ç¨‹å®ç°è‡ªé€‚åº”çš„å¤šä¸»ä½“ç³»ç»Ÿè®¾è®¡ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ¦‚ç‡é‡‡æ ·æœºåˆ¶å¯¹ä¸»ä½“è§’è‰²å’Œäº’åŠ¨è¿›è¡Œé‡‡æ ·ã€‚</li>
<li>æå‡ºç»„ä»¶åˆç†æ€§ä½œä¸ºæ–°çš„è®¾è®¡åŸåˆ™ï¼Œä»¥å®ç°å¤šç›®æ ‡ä¼˜åŒ–ã€‚</li>
<li>é‡‡ç”¨åˆ†å±‚ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼ˆHRPOï¼‰ï¼Œæ•´åˆç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿å’Œè¡ŒåŠ¨å¥–åŠ±ã€‚</li>
<li>å®éªŒè¯æ˜MasHoståœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œç»“æ„åˆç†æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-06c723c2e4e13c67625d3ab1492d598a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6970965be6c1c999a66f43b64ceeb17e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CAF-I-A-Collaborative-Multi-Agent-Framework-for-Enhanced-Irony-Detection-with-Large-Language-Models"><a href="#CAF-I-A-Collaborative-Multi-Agent-Framework-for-Enhanced-Irony-Detection-with-Large-Language-Models" class="headerlink" title="CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony   Detection with Large Language Models"></a>CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony   Detection with Large Language Models</h2><p><strong>Authors:Ziqi. Liu, Ziyang. Zhou, Mingxuan. Hu</strong></p>
<p>Large language model (LLM) have become mainstream methods in the field of sarcasm detection. However, existing LLM methods face challenges in irony detection, including: 1. single-perspective limitations, 2. insufficient comprehensive understanding, and 3. lack of interpretability. This paper introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven multi-agent system designed to overcome these issues. CAF-I employs specialized agents for Context, Semantics, and Rhetoric, which perform multidimensional analysis and engage in interactive collaborative optimization. A Decision Agent then consolidates these perspectives, with a Refinement Evaluator Agent providing conditional feedback for optimization. Experiments on benchmark datasets establish CAF-Iâ€™s state-of-the-art zero-shot performance. Achieving SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of 76.31, a 4.98 absolute improvement over the strongest prior baseline. This success is attained by its effective simulation of human-like multi-perspective analysis, enhancing detection accuracy and interpretability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­å˜²è®½æ£€æµ‹çš„ä¸»æµæ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ–¹æ³•åœ¨æ£€æµ‹è®½åˆºæ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ï¼š1.å•ä¸€è§†è§’çš„å±€é™æ€§ï¼Œ2.ç»¼åˆç†è§£ä¸è¶³ï¼Œä»¥åŠ3.ç¼ºä¹å¯è§£é‡Šæ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ç”¨äºè®½åˆºçš„åä½œä»£ç†æ¡†æ¶ï¼ˆCAF-Iï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥LLMé©±åŠ¨çš„å¤šä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨å…‹æœè¿™äº›é—®é¢˜ã€‚CAF-Ié‡‡ç”¨ä¸“é—¨é’ˆå¯¹ä¸Šä¸‹æ–‡ã€è¯­ä¹‰å’Œä¿®è¾çš„ä»£ç†ï¼Œè¿›è¡Œå¤šç»´åˆ†æå¹¶å‚ä¸äº¤äº’å¼ååŒä¼˜åŒ–ã€‚ç„¶åï¼Œå†³ç­–ä»£ç†æ•´åˆè¿™äº›è§‚ç‚¹ï¼Œç»†åŒ–è¯„ä¼°ä»£ç†æä¾›æ¡ä»¶åé¦ˆä»¥å®ç°ä¼˜åŒ–ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†CAF-Iæœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚åœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒCAF-Içš„å¹³å‡å®è§‚F1åˆ†æ•°ä¸º76.31ï¼Œè¾ƒä¹‹å‰æœ€å¼ºçš„åŸºçº¿æ¨¡å‹æœ‰4.98çš„ç»å¯¹æ”¹è¿›ã€‚è¿™ä¸€æˆåŠŸæ˜¯é€šè¿‡å…¶æ¨¡æ‹Ÿäººç±»å¤šè§†è§’åˆ†æè€Œå®ç°çš„ï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦å’Œå¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08430v1">PDF</a> ICML 2025 Workshop on Collaborative and Federated Agentic Workflows</p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®½åˆºæ£€æµ‹é¢†åŸŸå·²æˆä¸ºä¸»æµæ–¹æ³•ï¼Œä½†åœ¨è®½åˆºæ£€æµ‹æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºLLMçš„å¤šä»£ç†ç³»ç»Ÿâ€”â€”åä½œä»£ç†æ¡†æ¶ï¼ˆCAF-Iï¼‰ï¼Œé€šè¿‡ä¸“ä¸šåŒ–çš„ä¸Šä¸‹æ–‡ã€è¯­ä¹‰å’Œä¿®è¾ä»£ç†è¿›è¡Œå¤šç»´åˆ†æï¼Œå¹¶é€šè¿‡å†³ç­–ä»£ç†è¿›è¡Œè§†è§’æ•´åˆï¼Œä¼˜åŒ–è¯„ä¼°ä»£ç†æä¾›æ¡ä»¶åé¦ˆä»¥å®ç°ä¼˜åŒ–ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†CAF-Içš„é›¶æ ·æœ¬æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œåœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡Macro-F1è¾¾åˆ°76.31ï¼Œç›¸è¾ƒäºæœ€å¼ºåŸºçº¿æœ‰4.98çš„ç»å¯¹æå‡ã€‚è¿™å¾—ç›Šäºå…¶æ¨¡æ‹Ÿäººç±»çš„å¤šè§’åº¦åˆ†æï¼Œæé«˜äº†æ£€æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®½åˆºæ£€æµ‹ä¸­æ˜¯ä¸»æµæ–¹æ³•ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰LLMæ–¹æ³•åœ¨è®½åˆºæ£€æµ‹ä¸­çš„æŒ‘æˆ˜åŒ…æ‹¬å•è§†è§’é™åˆ¶ã€ç¼ºä¹å…¨é¢ç†è§£å’Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚</li>
<li>CAF-Iæ˜¯ä¸€ç§åŸºäºLLMçš„å¤šä»£ç†ç³»ç»Ÿï¼Œé€šè¿‡ä¸“ä¸šåŒ–çš„ä¸Šä¸‹æ–‡ã€è¯­ä¹‰å’Œä¿®è¾ä»£ç†è¿›è¡Œå¤šç»´åˆ†æã€‚</li>
<li>CAF-Ié‡‡ç”¨å†³ç­–ä»£ç†æ¥æ•´åˆè§†è§’ï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–è¯„ä¼°ä»£ç†è¿›è¡Œæ¡ä»¶åé¦ˆä»¥å®ç°ä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¯æ˜CAF-Içš„é›¶æ ·æœ¬æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹³å‡Macro-F1è¾¾åˆ°76.31ã€‚</li>
<li>CAF-Iç›¸è¾ƒäºæœ€å¼ºåŸºçº¿æœ‰4.98çš„ç»å¯¹æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fc926ea3fa97bbb6ba9c92242df2aa21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d5cabe0303b8936abeabd0d989a660c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1c983b0a81bf8eb0737c945c92fb447.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f6242151ba12402a61da86da224b16d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1732c2cc7fa1e1988f0bb0003cf07c42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15454d3afb92be76c4201a9b99c89ba9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TACTIC-Translation-Agents-with-Cognitive-Theoretic-Interactive-Collaboration"><a href="#TACTIC-Translation-Agents-with-Cognitive-Theoretic-Interactive-Collaboration" class="headerlink" title="TACTIC: Translation Agents with Cognitive-Theoretic Interactive   Collaboration"></a>TACTIC: Translation Agents with Cognitive-Theoretic Interactive   Collaboration</h2><p><strong>Authors:Weiya Li, Junjie Chen, Bei Li, Boyang Liu, Zichen Wen, Nuanqiao Shan, Xiaoqian Liu, Anping Liu, Huajie Liu, Youyan Wang, Wujiuge Yin, Hu Song, Bing Huang, Zhiyuan Xia, Jialiang Chen, Linfeng Zhang</strong></p>
<p>Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at <a target="_blank" rel="noopener" href="https://github.com/weiyali126/TACTIC">https://github.com/weiyali126/TACTIC</a>. </p>
<blockquote>
<p>æœºå™¨ç¿»è¯‘é•¿æœŸä»¥æ¥éƒ½æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€é¡¹æ ¸å¿ƒä»»åŠ¡ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç¿»è¯‘è´¨é‡å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå®Œå…¨å®ç°LLMsçš„ç¿»è¯‘æ½œåŠ›ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚è¿‘æœŸçš„ç ”ç©¶æ¢ç´¢äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå°†å¤æ‚çš„ç¿»è¯‘ä»»åŠ¡åˆ†è§£æˆååŒçš„å­ä»»åŠ¡ï¼Œåˆæ­¥æ˜¾ç¤ºå‡ºé€šè¿‡æ™ºèƒ½ä½“åä½œå’Œä¸“ä¸šåŒ–å¢å¼ºç¿»è¯‘è´¨é‡çš„å¸Œæœ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ™ºèƒ½ä½“ç¿»è¯‘æ¡†æ¶å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†æ¥è‡ªè®¤çŸ¥ç¿»è¯‘ç ”ç©¶çš„è§è§£ã€‚è¿™äº›è§è§£å¼ºè°ƒäººç±»è¯‘è€…å¦‚ä½•é‡‡ç”¨ä¸åŒçš„è®¤çŸ¥ç­–ç•¥ï¼Œå¦‚å¹³è¡¡ç›´è¯‘å’Œæ„è¯‘ã€æ ¹æ®ä¸Šä¸‹æ–‡ä¼˜åŒ–è¡¨è¾¾ã€ä»¥åŠè¿­ä»£è¯„ä¼°è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå—è®¤çŸ¥å¯å‘çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåä¸ºTACTICï¼Œä»£è¡¨å…·æœ‰è®¤çŸ¥ç†è®ºäº¤äº’åä½œçš„ç¿»è¯‘æ™ºèƒ½ä½“ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å…­ä¸ªåŠŸèƒ½å„å¼‚çš„æ™ºèƒ½ä½“ï¼Œåæ˜ äººç±»ç¿»è¯‘è¡Œä¸ºä¸­è§‚å¯Ÿåˆ°çš„å…³é”®è®¤çŸ¥è¿‡ç¨‹ã€‚è¿™äº›æ™ºèƒ½ä½“åŒ…æ‹¬èµ·è‰ã€æ”¹è¿›ã€è¯„ä¼°ã€æ‰“åˆ†ã€ä¸Šä¸‹æ–‡æ¨ç†å’Œå¤–éƒ¨çŸ¥è¯†æ”¶é›†çš„æ™ºèƒ½ä½“ã€‚é€šè¿‡æ¨¡æ‹Ÿäº¤äº’å’ŒåŸºäºç†è®ºçš„ç¿»è¯‘å·¥ä½œæµç¨‹ï¼ŒTACTICæœ‰æ•ˆåœ°åˆ©ç”¨LLMsçš„å…¨éƒ¨èƒ½åŠ›å®ç°é«˜è´¨é‡ç¿»è¯‘ã€‚åœ¨FLORES-200å’ŒWMT24åŸºå‡†æµ‹è¯•ä¸Šçš„å¤šæ ·åŒ–è¯­è¨€å¯¹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸€ç›´è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»¥DeepSeek-V3ä¸ºåŸºç¡€æ¨¡å‹ï¼ŒTACTICå¹³å‡è¶…è¶ŠGPT-4.1ï¼ŒXCOMETæå‡+0.6ï¼ŒCOMETKIWI-23æå‡+1.18ã€‚ç›¸è¾ƒäºDeepSeek-R1ï¼Œè¿›ä¸€æ­¥åœ¨XCOMETä¸Šæå‡+0.84ï¼ŒCOMETKIWI-23æå‡+2.99ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/weiyali126/TACTIC%E3%80%82">https://github.com/weiyali126/TACTICã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08403v1">PDF</a> 20 pages, 4 figures, Under review. Code:   <a target="_blank" rel="noopener" href="https://github.com/weiyali126/TACTIC">https://github.com/weiyali126/TACTIC</a></p>
<p><strong>Summary</strong><br>     æœºå™¨ç¿»è¯‘æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç¿»è¯‘è´¨é‡å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†å®Œå…¨å®ç°LLMsçš„ç¿»è¯‘æ½œåŠ›ä»æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªè®¤çŸ¥å¯å‘ä¸‹çš„å¤šæ™ºèƒ½ä½“ç¿»è¯‘æ¡†æ¶TACTICï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬å…­ä¸ªåŠŸèƒ½å„å¼‚çš„æ™ºèƒ½ä½“ï¼Œæ¨¡æ‹Ÿäººç±»ç¿»è¯‘è¿‡ç¨‹ä¸­çš„å…³é”®è®¤çŸ¥è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTACTICæ¡†æ¶åœ¨å¤šç§è¯­è¨€å¯¹ä¸Šçš„ç¿»è¯‘æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¼˜äºGPT-4.1å’ŒDeepSeek-R1ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨ç¿»è¯‘æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„é‡è¦ä»»åŠ¡ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†ç¿»è¯‘è´¨é‡çš„æå‡ã€‚</li>
<li>å®Œå…¨å®ç°LLMsçš„ç¿»è¯‘æ½œåŠ›ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æ¢ç´¢æ–°çš„æ–¹æ³•å’ŒæŠ€æœ¯ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„åº”ç”¨å±•ç°å‡ºæå‡ç¿»è¯‘è´¨é‡çš„åˆæ­¥å¸Œæœ›ã€‚</li>
<li>ç°æœ‰å¤šæ™ºèƒ½ä½“ç¿»è¯‘æ¡†æ¶å¿½è§†äº†æ¥è‡ªè®¤çŸ¥ç¿»è¯‘ç ”ç©¶çš„è§è§£ã€‚</li>
<li>TACTICæ¡†æ¶æ˜¯ä¸€ä¸ªè®¤çŸ¥å¯å‘ä¸‹çš„å¤šæ™ºèƒ½ä½“ç¿»è¯‘æ¡†æ¶ï¼ŒåŒ…æ‹¬å…­ä¸ªåŠŸèƒ½å„å¼‚çš„æ™ºèƒ½ä½“ï¼Œæ¨¡æ‹Ÿäººç±»ç¿»è¯‘è¿‡ç¨‹ä¸­çš„å…³é”®è®¤çŸ¥è¡Œä¸ºã€‚</li>
<li>TACTICæ¡†æ¶åœ¨å¤šç§è¯­è¨€å¯¹ä¸Šçš„ç¿»è¯‘æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¼˜äºGPT-4.1å’ŒDeepSeek-R1ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-82e3164b8974baadaca80bc7e7cb4a6f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3bd90291c04e414fe8d4bb72f74b83de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7956cbdcea204e2eb142880833239a03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07c8fd77165b3d84f5497850e2d39a86.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ccb951654ecac392c907719c819465a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reinforce-LLM-Reasoning-through-Multi-Agent-Reflection"><a href="#Reinforce-LLM-Reasoning-through-Multi-Agent-Reflection" class="headerlink" title="Reinforce LLM Reasoning through Multi-Agent Reflection"></a>Reinforce LLM Reasoning through Multi-Agent Reflection</h2><p><strong>Authors:Yurun Yuan, Tengyang Xie</strong></p>
<p>Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization. </p>
<blockquote>
<p>åˆ©ç”¨æ›´å¤šçš„æµ‹è¯•æ—¶é—´è®¡ç®—å·²è¢«è¯æ˜æ˜¯æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚åœ¨å„ç§æ–¹æ³•ä¸­ï¼ŒéªŒè¯å’Œæ”¹è¿›èŒƒå¼è„±é¢–è€Œå‡ºï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿå®ç°åŠ¨æ€è§£å†³æ–¹æ¡ˆæ¢ç´¢å’Œåé¦ˆèåˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å—åˆ°åé¦ˆç©ºé—´é™åˆ¶å’Œå„æ–¹ç¼ºä¹åè°ƒè®­ç»ƒçš„å›°æ‰°ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†è¿™ç§å¤šè½®ä¼˜åŒ–è¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥DPSDPï¼ˆé€šè¿‡åŠ¨æ€è§„åˆ’è¿›è¡Œç›´æ¥ç­–ç•¥æœç´¢ï¼‰ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•è®­ç»ƒä¸€ä¸ªactor-critic LLMç³»ç»Ÿï¼Œé€šè¿‡ç›´æ¥åœ¨è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œåå¥½å­¦ä¹ æ¥è¿­ä»£ä¼˜åŒ–ç­”æ¡ˆã€‚ç†è®ºä¸Šï¼ŒDPSDPå¯ä»¥åœ¨è®­ç»ƒåˆ†å¸ƒå†…åŒ¹é…ä»»ä½•ç­–ç•¥çš„æ€§èƒ½ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬ç”¨å„ç§åŸºç¡€æ¨¡å‹å®ä¾‹åŒ–äº†DPSDPï¼Œå¹¶å±•ç¤ºäº†åœ¨å†…éƒ¨å’Œå¤–éƒ¨åŸºå‡†æµ‹è¯•ä¸­çš„æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œåœ¨MATH 500åŸºå‡†æµ‹è¯•ä¸­ï¼Œç»è¿‡äº”æ­¥ä¼˜åŒ–çš„å¤šæ•°æŠ•ç¥¨ç»“æœå°†åˆç­”å‡†ç¡®ç‡ä»58.2%æé«˜åˆ°äº†63.2%ï¼Œä½¿ç”¨Ministralä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚ä¸€é¡¹æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†å¤šæ™ºèƒ½ä½“åä½œå’Œè·¨åˆ†å¸ƒæ³›åŒ–çš„å¥½å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08379v1">PDF</a> International Conference on Machine Learning (ICML), 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»¥ç®€æ´çš„æ–¹å¼æè¿°äº†å¦‚ä½•åˆ©ç”¨æ›´å¤šçš„æµ‹è¯•æ—¶é—´è®¡ç®—æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å…¶ä¸­ï¼ŒéªŒè¯å’Œæ”¹è¿›èŒƒå¼ç‰¹åˆ«çªå‡ºï¼Œå®ƒå¯å®ç°åŠ¨æ€è§£å†³æ–¹æ¡ˆæ¢ç´¢å’Œåé¦ˆèåˆã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å­˜åœ¨çš„åé¦ˆç©ºé—´å—é™å’Œä¸åŒå‚ä¸æ–¹ç¼ºä¹ååŒè®­ç»ƒå¯¼è‡´æ€§èƒ½ä¸ä½³çš„é—®é¢˜ï¼Œæœ¬ç ”ç©¶é‡‡ç”¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ¨¡æ‹Ÿå¤šè½®ç»†åŒ–è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥DPSDPï¼ˆä¸€ç§åŸºäºåŠ¨æ€è§„åˆ’çš„ç›´æ¥æ”¿ç­–æœç´¢å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼‰ï¼Œè®­ç»ƒæ¼”å‘˜è¯„è®ºå®¶LLMç³»ç»Ÿé€šè¿‡ç›´æ¥åå¥½å­¦ä¹ åœ¨è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®ä¸Šè¿­ä»£ä¼˜åŒ–ç­”æ¡ˆã€‚ç†è®ºä¸Šï¼ŒDPSDPå¯åœ¨è®­ç»ƒåˆ†å¸ƒå†…åŒ¹é…ä»»ä½•ç­–ç•¥çš„æ€§èƒ½ã€‚å®è¯ç ”ç©¶è¯æ˜ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDPSDPå‡æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¦‚MATH 500åŸºå‡†æµ‹è¯•ä¸­ï¼Œç»è¿‡äº”æ­¥ä¼˜åŒ–åçš„å¤šæ•°æŠ•ç¥¨ç»“æœé¦–æ¬¡å‡†ç¡®ç‡ä»58.2%æå‡è‡³63.2%ã€‚åŒæ—¶ï¼Œé€šè¿‡æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å¤šæ™ºèƒ½ä½“åä½œå’Œè·¨åˆ†å¸ƒæ³›åŒ–çš„ä¼˜åŠ¿ã€‚è¿™æ˜¯ä¸€é¡¹è·¨è¯­è¨€å’Œå®é™…åº”ç”¨å‰æ™¯å¹¿æ³›çš„ç ”ç©¶æˆæœã€‚éšç€ç›¸å…³ç ”ç©¶å’Œåº”ç”¨çš„è¿›ä¸€æ­¥å‘å±•ï¼Œè¿™ä¸€é¢†åŸŸå°†ä¼šæœ‰æ›´å¤šçš„çªç ´å’Œè¿›å±•ã€‚ç®€è¨€ä¹‹ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ï¼Œå…·æœ‰è¾ƒå¼ºçš„ç†è®ºä¸å®è·µä»·å€¼ã€‚æ€»ä½“æ¥è¯´å¯¹å®é™…éƒ¨ç½²å…·æœ‰å¾ˆé«˜çš„å‚è€ƒæ„ä¹‰å’Œç ”ç©¶ä»·å€¼ã€‚æ€»ä¹‹åœ¨å®é™…åº”ç”¨ä¸­è¿˜éœ€éªŒè¯ä¸å®Œå–„æ•ˆæœå…·ä½“çš„æƒ…å†µå¯èƒ½éœ€è¦åç»­è¿›ä¸€æ­¥ç ”ç©¶è®¨è®ºè¯å®ç¡®è®¤æ¨å¹¿åº”ç”¨çš„å¯è¡Œæ€§å’Œå…·ä½“å®æ–½æ–¹æ¡ˆå¹¶ä¸”ä¸æ–­æ¢ç´¢è§£å†³å­˜åœ¨é—®é¢˜çš„èƒ½åŠ›ä¸ºæœªæ¥çš„å‘å±•æä¾›æ›´å¤šçš„åˆ›æ–°æƒ³æ³•ä»¥åŠå®é™…åº”ç”¨åœºæ™¯ç­‰ã€‚ç›®å‰è¯¥ç ”ç©¶ä»å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œæ½œåœ¨ä»·å€¼å€¼å¾—è¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶å’Œæ¢ç´¢ã€‚æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›åœ¨æœªæ¥è¯¥æŠ€æœ¯èƒ½å¤Ÿè½åœ°åº”ç”¨ä¸­å°†æ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•å¯¹äºæ¨è¿›ç›¸å…³é¢†åŸŸæŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ä¿ƒè¿›å¤šæ™ºèƒ½ä½“åä½œçš„è¿›ä¸€æ­¥å‘å±•å…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µä»·å€¼å¹¶å¯èƒ½äº§ç”Ÿæ·±è¿œçš„ç¤¾ä¼šå½±å“å’Œç»æµä»·å€¼æ¨åŠ¨äº§ä¸šçš„å‡çº§å’Œè½¬å‹æå‡å›½å®¶ç«äº‰åŠ›æ¨åŠ¨ç»æµå‘å±•å’Œç¤¾ä¼šè¿›æ­¥å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå‘å±•æ½œåŠ›ã€‚æ–‡ä¸­æå‡ºçš„ç®—æ³•å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œæ½œåœ¨ä»·å€¼æœªæ¥æœ‰æœ›å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ç›¸å…³åœºæ™¯å¦‚æ™ºèƒ½å®¢æœé—®ç­”ç³»ç»Ÿå¯¹è¯ç”Ÿæˆæœºå™¨äººç­‰é¢†åŸŸåœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰éå¸¸å¹¿é˜”çš„æ¨å¹¿å‰æ™¯ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å°†è¯¥æ–¹æ³•åº”ç”¨äºæ›´å¤šé¢†åŸŸåœºæ™¯ä»¥åŠä¼˜åŒ–ç®—æ³•æ€§èƒ½ç­‰æ–¹é¢ã€‚è¯¥ç®—æ³•ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥åˆ›æ–°çªç ´æœ‰æœ›æˆä¸ºæœªæ¥ç ”ç©¶çš„é‡è¦æ–¹å‘ä¹‹ä¸€å¯¹äºè¯¥ç®—æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å’Œè¡¨ç°æœ‰å¹¿æ³›æœŸå¾…å¹¶å¼•èµ·è¡Œä¸šçš„å…³æ³¨ä¸ç ”ç©¶ä»·å€¼è¡¨æ˜å®ƒåœ¨äººå·¥æ™ºèƒ½è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸçš„é‡è¦è´¡çŒ®åŒæ—¶æ„å‘³ç€äººç±»åœ¨åº”ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„é“è·¯ä¸Šåˆè¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚è¯¥ç®—æ³•å°†æå¤§åœ°æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ä¸ºäººä»¬çš„ç”Ÿæ´»å¸¦æ¥ä¾¿åˆ©å¹¶äº§ç”Ÿæ·±è¿œçš„å½±å“å’Œç»æµæ•ˆç›Šä¸ºç¤¾ä¼šè¿›æ­¥è´¡çŒ®åŠ›é‡ä¸ºäººç±»ç¤¾ä¼šçš„ç§‘æŠ€è¿›æ­¥æ³¨å…¥æ–°çš„æ´»åŠ›ã€‚<strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>åˆ©ç”¨æ›´å¤šçš„æµ‹è¯•æ—¶é—´è®¡ç®—å¯æœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éªŒè¯å’Œæ”¹è¿›èŒƒå¼å¯å®ç°åŠ¨æ€è§£å†³æ–¹æ¡ˆæ¢ç´¢å’Œåé¦ˆèåˆã€‚</li>
<li>DPSDPç®—æ³•é€šè¿‡ç›´æ¥åå¥½å­¦ä¹ åœ¨è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®ä¸Šè®­ç»ƒLLMç³»ç»Ÿä»¥è¿­ä»£ä¼˜åŒ–ç­”æ¡ˆã€‚</li>
<li>DPSDPç®—æ³•èƒ½åœ¨ç†è®ºä¸Šé™åŒ¹é…ä»»ä½•ç­–ç•¥çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯æ˜äº†å¤šæ™ºèƒ½ä½“åä½œå’Œè·¨åˆ†å¸ƒæ³›åŒ–çš„ä¼˜åŠ¿ã€‚</li>
<li>DPSDPç®—æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¤šä¸ªåœºæ™¯ï¼Œå¦‚æ™ºèƒ½å®¢æœã€é—®ç­”ç³»ç»Ÿå’Œå¯¹è¯ç”Ÿæˆæœºå™¨äººç­‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49c1760f8ba05cc11e039d522919f330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce6bbf75492be31ef7d94c58ed244be7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32dfa7be3e336f2faeb3b97b9c04df04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc111418c7306a020bf94a1b1ac69189.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HiBerNAC-Hierarchical-Brain-emulated-Robotic-Neural-Agent-Collective-for-Disentangling-Complex-Manipulation"><a href="#HiBerNAC-Hierarchical-Brain-emulated-Robotic-Neural-Agent-Collective-for-Disentangling-Complex-Manipulation" class="headerlink" title="HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective   for Disentangling Complex Manipulation"></a>HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective   for Disentangling Complex Manipulation</h2><p><strong>Authors:Hongjun Wu, Heng Zhang, Pengsong Zhang, Jin Wang, Cong Wang</strong></p>
<p>Recent advances in multimodal vision-language-action (VLA) models have revolutionized traditional robot learning, enabling systems to interpret vision, language, and action in unified frameworks for complex task planning. However, mastering complex manipulation tasks remains an open challenge, constrained by limitations in persistent contextual memory, multi-agent coordination under uncertainty, and dynamic long-horizon planning across variable sequences. To address this challenge, we propose \textbf{HiBerNAC}, a \textbf{Hi}erarchical \textbf{B}rain-\textbf{e}mulated \textbf{r}obotic \textbf{N}eural \textbf{A}gent \textbf{C}ollective, inspired by breakthroughs in neuroscience, particularly in neural circuit mechanisms and hierarchical decision-making. Our framework combines: (1) multimodal VLA planning and reasoning with (2) neuro-inspired reflection and multi-agent mechanisms, specifically designed for complex robotic manipulation tasks. By leveraging neuro-inspired functional modules with decentralized multi-agent collaboration, our approach enables robust and enhanced real-time execution of complex manipulation tasks. In addition, the agentic system exhibits scalable collective intelligence via dynamic agent specialization, adapting its coordination strategy to variable task horizons and complexity. Through extensive experiments on complex manipulation tasks compared with state-of-the-art VLA models, we demonstrate that \textbf{HiBerNAC} reduces average long-horizon task completion time by 23%, and achieves non-zero success rates (12\textendash 31%) on multi-path tasks where prior state-of-the-art VLA models consistently fail. These results provide indicative evidence for bridging biological cognition and robotic learning mechanisms. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„è¿›æ­¥å·²ç»å½»åº•æ”¹å˜äº†ä¼ ç»Ÿæœºå™¨äººå­¦ä¹ çš„æ–¹å¼ï¼Œä½¿ç³»ç»Ÿåœ¨ç»Ÿä¸€çš„æ¡†æ¶ä¸‹è§£é‡Šè§†è§‰ã€è¯­è¨€å’Œè¡Œä¸ºï¼Œä»è€Œè¿›è¡Œå¤æ‚çš„ä»»åŠ¡è§„åˆ’ã€‚ç„¶è€Œï¼ŒæŒæ¡å¤æ‚çš„æ“ä½œä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Œå—åˆ°æŒä¹…ä¸Šä¸‹æ–‡è®°å¿†é™åˆ¶ã€ä¸ç¡®å®šæ€§ä¸‹çš„å¤šæ™ºèƒ½ä½“åè°ƒå’Œå¯å˜åºåˆ—ä¸­çš„åŠ¨æ€é•¿æœŸè§„åˆ’é™åˆ¶çš„å½±å“ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{HiBerNAC}ï¼Œè¿™æ˜¯ä¸€ä¸ªå—ç¥ç»ç§‘å­¦çªç ´å¯å‘çš„åˆ†å±‚è„‘æ¨¡æ‹Ÿæœºå™¨äººç¥ç»ç½‘ç»œé›†ä½“ï¼ˆ\textbf{Hi}erarchical \textbf{B}rain-\textbf{e}mulated \textbf{r}obotic \textbf{N}eural \textbf{A}gent \textbf{C}ollectiveï¼‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†ï¼ˆ1ï¼‰å¤šæ¨¡æ€VLAè§„åˆ’å’Œæ¨ç†ä¸ï¼ˆ2ï¼‰ç¥ç»å¯å‘åæ€å’Œå¤šæ™ºèƒ½ä½“æœºåˆ¶ï¼Œä¸“ä¸ºå¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡è®¾è®¡ã€‚é€šè¿‡åˆ©ç”¨ç¥ç»å¯å‘çš„åŠŸèƒ½æ¨¡å—å’Œåˆ†æ•£çš„å¤šæ™ºèƒ½ä½“åä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç¨³å¥åœ°å¢å¼ºå¤æ‚æ“ä½œä»»åŠ¡çš„å®æ—¶æ‰§è¡Œã€‚æ­¤å¤–ï¼Œæ™ºèƒ½ç³»ç»Ÿé€šè¿‡åŠ¨æ€æ™ºèƒ½ä½“ä¸“ä¸šåŒ–å±•ç°å‡ºå¯æ‰©å±•çš„é›†ä½“æ™ºèƒ½ï¼Œä½¿å…¶åè°ƒç­–ç•¥é€‚åº”å¯å˜çš„ä»»åŠ¡è§†é‡å’Œå¤æ‚æ€§ã€‚é€šè¿‡ä¸æœ€æ–°çš„VLAæ¨¡å‹åœ¨å¤æ‚çš„æ“ä½œä»»åŠ¡ä¸Šè¿›è¡Œå¹¿æ³›çš„å®éªŒæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜\textbf{HiBerNAC}å¹³å‡é•¿æœŸä»»åŠ¡å®Œæˆæ—¶é—´å‡å°‘äº†23%ï¼Œå¹¶ä¸”åœ¨å¤šè·¯å¾„ä»»åŠ¡ä¸Šå®ç°äº†éé›¶æˆåŠŸç‡ï¼ˆ12%\textendash 31%ï¼‰ï¼Œè€Œå…ˆå‰çš„æœ€æ–°VLAæ¨¡å‹åˆ™ä¸€ç›´å¤±è´¥ã€‚è¿™äº›ç»“æœä¸ºè¿æ¥ç”Ÿç‰©è®¤çŸ¥å’Œæœºå™¨äººå­¦ä¹ æœºåˆ¶æä¾›äº†æŒ‡ç¤ºæ€§è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08296v1">PDF</a> 31 pages,5 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€æ–°å¤šæ¨¡æ€è§†è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„è¿›å±•å·²å¯¹ä¼ ç»Ÿæœºå™¨äººå­¦ä¹ å¸¦æ¥äº†é©å‘½æ€§å˜é©ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¡†æ¶ä¸­è§£é‡Šè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œï¼Œç”¨äºå¤æ‚ä»»åŠ¡è§„åˆ’ã€‚ç„¶è€Œï¼ŒæŒæ¡å¤æ‚æ“ä½œä»»åŠ¡ä»æ˜¯ä¸€é¡¹å¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Œå—é™äºæŒä¹…æ€§ä¸Šä¸‹æ–‡è®°å¿†ã€ä¸ç¡®å®šæ€§ä¸‹çš„å¤šæ™ºèƒ½ä½“åè°ƒå’Œå¯å˜åºåˆ—çš„åŠ¨æ€é•¿æœŸè§„åˆ’ç­‰æ–¹é¢çš„å±€é™ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†HiBerNACï¼Œä¸€ä¸ªå—ç¥ç»ç§‘å­¦çªç ´å¯å‘çš„åˆ†å±‚è„‘æ¨¡æ‹Ÿæœºå™¨äººç¥ç»ç½‘ç»œé›†ä½“ï¼ˆHierarchical Brain-emulated robotic Neural Agent Collectiveï¼‰ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šæ¨¡æ€VLAè§„åˆ’å’Œæ¨ç†ä¸ç¥ç»å¯å‘åæ€å’Œå¤šæ™ºèƒ½ä½“æœºåˆ¶ï¼Œä¸“ä¸ºå¤æ‚æœºå™¨äººæ“ä½œä»»åŠ¡è®¾è®¡ã€‚é€šè¿‡åˆ©ç”¨ç¥ç»å¯å‘åŠŸèƒ½æ¨¡å—ä¸åˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“åä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯å®ç°ç¨³å¥ä¸”å¢å¼ºçš„å®æ—¶æ‰§è¡Œå¤æ‚æ“ä½œä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ™ºèƒ½ä½“ç³»ç»Ÿé€šè¿‡åŠ¨æ€æ™ºèƒ½ä½“ä¸“ä¸šåŒ–å±•ç°å¯æ‰©å±•çš„é›†ä½“æ™ºèƒ½ï¼Œä½¿åè°ƒç­–ç•¥é€‚åº”å¯å˜çš„ä»»åŠ¡èŒƒå›´å’Œå¤æ‚æ€§ã€‚ä¸æœ€æ–°çš„VLAæ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨å¤æ‚çš„æ“ä½œä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜HiBerNACèƒ½å¤Ÿå‡å°‘å¹³å‡é•¿æœŸä»»åŠ¡å®Œæˆæ—¶é—´23%ï¼Œå¹¶åœ¨å¤šè·¯å¾„ä»»åŠ¡ä¸Šå®ç°éé›¶æˆåŠŸç‡ï¼ˆ12%~31%ï¼‰ï¼Œè€Œå…ˆå‰çš„æœ€æ–°VLAæ¨¡å‹åˆ™ä¸€ç›´æœªèƒ½æˆåŠŸã€‚è¿™äº›ç»“æœæä¾›äº†å°†ç”Ÿç‰©è®¤çŸ¥ä¸æœºå™¨äººå­¦ä¹ æœºåˆ¶ç›¸ç»“åˆçš„æŒ‡ç¤ºæ€§è¯æ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€è§†è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²ç»æ¨åŠ¨äº†æœºå™¨äººå­¦ä¹ é¢†åŸŸçš„å˜é©ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¡†æ¶å†…è§£é‡Šè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œã€‚</li>
<li>å¤æ‚æ“ä½œä»»åŠ¡å¯¹æœºå™¨äººå­¦ä¹ æ„æˆæŒ‘æˆ˜ï¼Œä¸»è¦åŒ…æ‹¬æŒä¹…ä¸Šä¸‹æ–‡è®°å¿†ã€å¤šæ™ºèƒ½ä½“åè°ƒå’ŒåŠ¨æ€é•¿æœŸè§„åˆ’çš„é—®é¢˜ã€‚</li>
<li>HiBerNACæ¡†æ¶å—ç¥ç»ç§‘å­¦å¯å‘ï¼Œç»“åˆå¤šæ¨¡æ€VLAè§„åˆ’å’Œæ¨ç†ä¸ç¥ç»å¯å‘åŠŸèƒ½æ¨¡å—ã€‚</li>
<li>é€šè¿‡ç¥ç»å¯å‘åŠŸèƒ½æ¨¡å—å’Œåˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“åä½œï¼ŒHiBerNACèƒ½å¤Ÿç¨³å¥åœ°æ‰§è¡Œå¤æ‚æ“ä½œä»»åŠ¡ï¼Œå¹¶å¢å¼ºå®æ—¶æ€§èƒ½ã€‚</li>
<li>HiBerNACå±•ç°å‡ºé€šè¿‡åŠ¨æ€æ™ºèƒ½ä½“ä¸“ä¸šåŒ–å®ç°çš„é›†ä½“æ™ºèƒ½çš„å¯æ‰©å±•æ€§ï¼Œé€‚åº”å¯å˜çš„ä»»åŠ¡èŒƒå›´å’Œå¤æ‚æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œä¸æœ€æ–°VLAæ¨¡å‹ç›¸æ¯”ï¼ŒHiBerNACåœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰æ‰€è¶…è¶Šï¼Œå‡å°‘äº†ä»»åŠ¡å®Œæˆæ—¶é—´ï¼Œå¹¶åœ¨å¤šè·¯å¾„ä»»åŠ¡ä¸Šå®ç°äº†éé›¶æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c44de91f17fe7b0455a3924f636c5dfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b50acf4dc0bca60d09e587feddb34082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-406e97c14d05c1cfc6259414ad91483e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3991cfbd9fa2aef9ad238d6a3b5fff2e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Thinking-vs-Doing-Agents-that-Reason-by-Scaling-Test-Time-Interaction"><a href="#Thinking-vs-Doing-Agents-that-Reason-by-Scaling-Test-Time-Interaction" class="headerlink" title="Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction"></a>Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction</h2><p><strong>Authors:Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, Aviral Kumar</strong></p>
<p>The current paradigm of test-time scaling relies on generating long reasoning traces (â€œthinkingâ€ more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agentâ€™s interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents. </p>
<blockquote>
<p>å½“å‰æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„èŒƒå¼ä¾èµ–äºåœ¨ç”Ÿæˆå“åº”ä¹‹å‰äº§ç”Ÿè¾ƒé•¿çš„æ¨ç†è½¨è¿¹ï¼ˆå³â€œæ€è€ƒâ€æ›´å¤šï¼‰ã€‚åœ¨éœ€è¦äº¤äº’çš„ä»£ç†é—®é¢˜ä¸­ï¼Œè¿™å¯ä»¥é€šè¿‡åœ¨ä¸–ç•Œä¸­é‡‡å–è¡ŒåŠ¨ä¹‹å‰ç”Ÿæˆæ€ç»´è½¨è¿¹æ¥å®Œæˆã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸å…è®¸ä»£ç†ä»ç¯å¢ƒä¸­è·å–æ–°ä¿¡æ¯ï¼Œä¹Ÿä¸èƒ½éšç€æ—¶é—´çš„æ¨ç§»æ”¹å˜å®ƒä»¬çš„è¡Œä¸ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æµ‹è¯•æ—¶é—´äº¤äº’çš„æ‰©å±•ï¼Œè¿™æ˜¯æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„ä¸€ä¸ªæœªè¢«å¼€å‘çš„ç»´åº¦ï¼Œå®ƒå¢åŠ äº†ä»£ç†çš„äº¤äº’èŒƒå›´ï¼Œä½¿ä»£ç†èƒ½å¤Ÿåœ¨å•ä¸ªè¿è¡Œä¸­æ‰§è¡Œä¸°å¯Œçš„è¡Œä¸ºï¼Œå¦‚æ¢ç´¢ã€å›æº¯å’ŒåŠ¨æ€é‡æ–°è§„åˆ’ã€‚ä¸ºäº†è¯æ˜è¿™ä¸€æ‰©å±•ç»´åº¦çš„æ½œåŠ›ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç½‘é¡µä»£ç†é¢†åŸŸã€‚æˆ‘ä»¬é¦–å…ˆè¡¨æ˜ï¼Œå³ä½¿åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒåŸºäºæç¤ºçš„äº¤äº’æ‰©å±•ä¹Ÿå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜ç½‘é¡µåŸºå‡†æµ‹è¯•çš„ä»»åŠ¡æˆåŠŸç‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†æµ‹è¯•æ—¶é—´äº¤äº’ï¼ˆTTIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¯¾ç¨‹çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ»šåŠ¨é•¿åº¦æ¥è®­ç»ƒä»£ç†ã€‚ä½¿ç”¨Gemma 3 12Bæ¨¡å‹ï¼ŒTTIåœ¨WebVoyagerå’ŒWebArenaåŸºå‡†æµ‹è¯•ä¸­äº§ç”Ÿäº†æœ€å…ˆè¿›çš„å¼€æºå¼€æ”¾æ•°æ®ç½‘é¡µä»£ç†ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å±•ç¤ºäº†TTIä½¿ä»£ç†èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†äº¤äº’ç¼©æ”¾ä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„ã€ä¸æ¯æ­¥è®¡ç®—ç¼©æ”¾ç›¸è¾…ç›¸æˆçš„ç»´åº¦ï¼Œä¸ºè®­ç»ƒè‡ªé€‚åº”ä»£ç†æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07976v2">PDF</a> Fixed typo in Figure 6 and Conclusion</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºæµ‹è¯•æ—¶äº¤äº’æ‰©å±•çš„æ¦‚å¿µï¼Œæ—¨åœ¨æé«˜æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­çš„äº¤äº’èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å•æ¬¡è¿è¡Œä¸­æ‰§è¡Œä¸°å¯Œçš„è¡Œä¸ºï¼Œå¦‚æ¢ç´¢ã€å›æº¯å’ŒåŠ¨æ€è§„åˆ’ã€‚è¯¥ç ”ç©¶é€šè¿‡åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹é‡‡ç”¨æç¤ºä¸ºåŸºç¡€çš„äº¤äº’æ‰©å±•æ–¹å¼ï¼Œå®ç°äº†åœ¨éåŸºå‡†æµ‹è¯•ä¸Šçš„ä»»åŠ¡æˆåŠŸç‡çš„æå‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æµ‹è¯•æ—¶äº¤äº’ï¼ˆTTIï¼‰æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ™ºèƒ½ä½“çš„è¿è¡Œæ—¶é•¿è¿›è¡Œè®­ç»ƒã€‚TTIä½¿ç”¨Gemma 3 12Bæ¨¡å‹åœ¨WebVoyagerå’ŒWebArenaåŸºå‡†æµ‹è¯•ä¸­äº§ç”Ÿäº†å…ˆè¿›çš„å¼€æºæ™ºèƒ½ä½“ã€‚ç»“æœè¡¨æ˜ï¼Œæµ‹è¯•æ—¶äº¤äº’æ‰©å±•æ˜¯ä¸€ç§å¼ºå¤§çš„ã€ä¸æ¯æ­¥è®¡ç®—æ‰©å±•ç›¸è¾…ç›¸æˆçš„è¡¥å……è½´ï¼Œä¸ºè®­ç»ƒè‡ªé€‚åº”æ™ºèƒ½ä½“æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æµ‹è¯•æ—¶é—´æ‰©å±•çš„èŒƒå¼ä¾èµ–äºç”Ÿæˆé•¿çš„æ¨ç†è½¨è¿¹å†åšå‡ºå›åº”ï¼Œä½†åœ¨éœ€è¦äº¤äº’çš„æ™ºèƒ½ä½“é—®é¢˜ä¸­ï¼Œè¿™ç§æ–¹æ³•æ— æ³•è®©æ™ºèƒ½ä½“ä»ç¯å¢ƒä¸­è·å–æ–°ä¿¡æ¯æˆ–éšæ—¶é—´æ”¹å˜è¡Œä¸ºã€‚</li>
<li>æµ‹è¯•æ—¶äº¤äº’æ‰©å±•æ˜¯æé«˜æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­çš„äº¤äº’èƒ½åŠ›çš„ä¸€ç§æ–°æ–¹æ³•ï¼Œèƒ½ä½¿å…¶æ‰§è¡Œæ¢ç´¢ã€å›æº¯å’ŒåŠ¨æ€è§„åˆ’ç­‰ä¸°å¯Œè¡Œä¸ºã€‚</li>
<li>é‡‡ç”¨æç¤ºä¸ºåŸºç¡€çš„äº¤äº’æ‰©å±•æ–¹å¼ï¼Œèƒ½åœ¨ä¸é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æå‡ä»»åŠ¡æˆåŠŸç‡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æµ‹è¯•æ—¶äº¤äº’ï¼ˆTTIï¼‰æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ™ºèƒ½ä½“çš„è¿è¡Œæ—¶é•¿è¿›è¡Œè®­ç»ƒã€‚</li>
<li>TTIä½¿ç”¨Gemma 3 12Bæ¨¡å‹åœ¨WebVoyagerå’ŒWebArenaåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æµ‹è¯•æ—¶äº¤äº’æ‰©å±•æ˜¯ä¸€ç§å¼ºå¤§çš„è¡¥å……è½´ï¼Œä¸æ¯æ­¥è®¡ç®—æ‰©å±•ç›¸è¾…ç›¸æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b20bf1b7b3980f2384e8a93eac899de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c5a90370d605ca6f1cbe84755e6eb36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3895e122cd82008845075175bef98d46.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SAFEFLOW-A-Principled-Protocol-for-Trustworthy-and-Transactional-Autonomous-Agent-Systems"><a href="#SAFEFLOW-A-Principled-Protocol-for-Trustworthy-and-Transactional-Autonomous-Agent-Systems" class="headerlink" title="SAFEFLOW: A Principled Protocol for Trustworthy and Transactional   Autonomous Agent Systems"></a>SAFEFLOW: A Principled Protocol for Trustworthy and Transactional   Autonomous Agent Systems</h2><p><strong>Authors:Peiran Li, Xinkai Zou, Zhuohang Wu, Ruifeng Li, Shuo Xing, Hanwen Zheng, Zhikai Hu, Yuping Wang, Haoxi Li, Qin Yuan, Yingmo Zhang, Zhengzhong Tu</strong></p>
<p>Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, todayâ€™s agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM&#x2F;VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¿›æ­¥ä½¿å¾—èƒ½å¤Ÿæ‰§è¡Œå¤æ‚æ¨ç†å’Œå¤šæ¨¡å¼å·¥å…·ä½¿ç”¨çš„å¼ºå¤§è‡ªä¸»ä»£ç†æˆä¸ºå¯èƒ½ã€‚å°½ç®¡å®ƒä»¬çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œä½†å½“å‰çš„ä»£ç†æ¡†æ¶ä»ç„¶è„†å¼±ï¼Œç¼ºä¹å®‰å…¨ä¿¡æ¯æµã€å¯é æ€§å’Œå¤šä»£ç†åè°ƒçš„åŸåˆ™æ€§æœºåˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†SAFEFLOWï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºå¯ä¿¡çš„LLM&#x2F;VLMåŸºäºä»£ç†çš„æ–°åè®®çº§æ¡†æ¶ã€‚SAFEFLOWå¼ºåˆ¶å®æ–½ç²¾ç»†çš„ä¿¡æ¯æµæ§åˆ¶ï¼ˆIFCï¼‰ï¼Œç²¾ç¡®è·Ÿè¸ªä»£ç†ã€å·¥å…·ã€ç”¨æˆ·å’Œç¯å¢ƒä¹‹é—´äº¤æ¢çš„æ‰€æœ‰æ•°æ®çš„æ¥æºã€å®Œæ•´æ€§å’Œæœºå¯†æ€§ã€‚é€šè¿‡é™åˆ¶LLMæ¨ç†ä»¥å°Šé‡è¿™äº›å®‰å…¨æ ‡ç­¾ï¼ŒSAFEFLOWé˜²æ­¢ä¸å—ä¿¡ä»»æˆ–å¯¹æ•Œè¾“å…¥æ±¡æŸ“é«˜å®Œæ•´æ€§å†³ç­–ã€‚ä¸ºäº†ç¡®ä¿åœ¨å¹¶å‘å¤šä»£ç†ç¯å¢ƒä¸­çš„ç¨³å¥æ€§ï¼ŒSAFEFLOWå¼•å…¥äº†äº‹åŠ¡æ‰§è¡Œã€å†²çªè§£å†³å’Œå…±äº«çŠ¶æ€çš„å®‰å…¨è°ƒåº¦ï¼Œä»¥ä¿ç•™å…¨å±€ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†åŒ…æ‹¬é¢„å†™æ—¥å¿—ã€å›æ»šå’Œå®‰å…¨ç¼“å­˜ç­‰æœºåˆ¶ï¼Œè¿›ä¸€æ­¥å¢å¼ºå¯¹è¿è¡Œæ—¶é”™è¯¯å’Œæ”¿ç­–è¿è§„çš„æŠµå¾¡èƒ½åŠ›ã€‚ä¸ºäº†éªŒè¯æ€§èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†SAFEFLOWBENCHï¼Œè¿™æ˜¯ä¸€å¥—ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ä»£ç†åœ¨æ•Œå¯¹ã€å˜ˆæ‚å’Œå¹¶å‘æ“ä½œæ¡ä»¶ä¸‹çš„å¯é æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨SAFEFLOWæ„å»ºçš„ä»£ç†å³ä½¿åœ¨æ¶åŠ£ç¯å¢ƒä¸­ä¹Ÿèƒ½ä¿æŒä»¤äººå°è±¡æ·±åˆ»çš„ä»»åŠ¡æ€§èƒ½å’Œå®‰å…¨ä¿è¯ï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚SAFEFLOWå’ŒSAFEFLOWBENCHä¸€èµ·å¥ å®šäº†åŸåˆ™æ€§ã€ç¨³å¥æ€§å’Œå®‰å…¨ä»£ç†ç”Ÿæ€ç³»ç»Ÿçš„åŸºçŸ³ï¼Œæ¨åŠ¨äº†å¯é è‡ªä¸»æ€§çš„å‰æ²¿å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07564v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æœ€æ–°è¿›å±•å·²ç»å‚¬ç”Ÿäº†èƒ½å¤Ÿè¿›è¡Œå¤æ‚æ¨ç†å’Œå¤šæ¨¡æ€å·¥å…·ä½¿ç”¨çš„å¼ºå¤§è‡ªä¸»ä»£ç†ã€‚ç„¶è€Œï¼Œå½“å‰çš„ä»£ç†æ¡†æ¶ä»ç„¶è„†å¼±ï¼Œç¼ºä¹å®‰å…¨ä¿¡æ¯æµã€å¯é æ€§å’Œå¤šä»£ç†åè°ƒçš„æœºåˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SAFEFLOWï¼Œä¸€ä¸ªä¸ºæ„å»ºå¯ä¿¡LLM&#x2F;VLMä»£ç†çš„æ–°åè®®çº§æ¡†æ¶ã€‚SAFEFLOWå¼ºåˆ¶æ‰§è¡Œç²¾ç»†çš„ä¿¡æ¯æµæ§åˆ¶ï¼ˆIFCï¼‰ï¼Œç²¾ç¡®è·Ÿè¸ªä»£ç†ã€å·¥å…·ã€ç”¨æˆ·å’Œç¯å¢ƒä¹‹é—´äº¤æ¢çš„æ‰€æœ‰æ•°æ®çš„æ¥æºã€å®Œæ•´æ€§å’Œæœºå¯†æ€§ã€‚é€šè¿‡çº¦æŸLLMæ¨ç†ä»¥å°Šé‡è¿™äº›å®‰å…¨æ ‡ç­¾ï¼ŒSAFEFLOWé˜²æ­¢ä¸å—ä¿¡ä»»æˆ–å¯¹æŠ—æ€§è¾“å…¥æ±¡æŸ“é«˜å®Œæ•´æ€§çš„å†³ç­–ã€‚ä¸ºç¡®ä¿å¹¶å‘å¤šä»£ç†è®¾ç½®ä¸­çš„ç¨³å¥æ€§ï¼ŒSAFEFLOWå¼•å…¥äº†äº‹åŠ¡æ‰§è¡Œã€å†²çªè§£å†³å’Œå…±äº«çŠ¶æ€çš„å®‰å…¨è°ƒåº¦ï¼Œä»¥ä¿ç•™å…¨å±€ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†åŒ…æ‹¬å†™å‰æ—¥å¿—ã€å›æ»šå’Œå®‰å…¨ç¼“å­˜ç­‰æœºåˆ¶ï¼Œä»¥å¢å¼ºå¯¹è¿è¡Œæ—¶é”™è¯¯å’Œæ”¿ç­–è¿è§„çš„æŠµå¾¡èƒ½åŠ›ã€‚ä¸ºäº†éªŒè¯æ€§èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†SAFEFLOWBENCHï¼Œä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæ—¨åœ¨è¯„ä¼°ä»£ç†åœ¨æ•Œå¯¹ã€å˜ˆæ‚å’Œå¹¶å‘æ“ä½œæ¡ä»¶ä¸‹çš„å¯é æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨SAFEFLOWæ„å»ºçš„ä»£ç†å³ä½¿åœ¨æ¶åŠ£ç¯å¢ƒä¸­ä¹Ÿèƒ½ä¿æŒä»¤äººå°è±¡æ·±åˆ»çš„ä»»åŠ¡æ€§èƒ½å’Œå®‰å…¨æ€§ä¿è¯ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚SAFEFLOWå’ŒSAFEFLOWBENCHå…±åŒä¸ºåŸç†åŒ–ã€ç¨³å¥å’Œå®‰å…¨çš„ä»£ç†ç”Ÿæ€ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼Œæ¨åŠ¨äº†å¯é è‡ªä¸»æ€§å‰æ²¿çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså’ŒVLMsçš„æœ€æ–°è¿›å±•ä½¿å¾—å¤æ‚æ¨ç†å’Œå¤šæ¨¡æ€å·¥å…·ä½¿ç”¨çš„å¼ºå¤§è‡ªä¸»ä»£ç†æˆä¸ºå¯èƒ½ã€‚</li>
<li>å½“å‰ä»£ç†æ¡†æ¶ç¼ºä¹å®‰å…¨ä¿¡æ¯æµã€å¯é æ€§å’Œå¤šä»£ç†åè°ƒçš„æœºåˆ¶ã€‚</li>
<li>SAFEFLOWæ˜¯ä¸€ä¸ªæ–°çš„åè®®çº§æ¡†æ¶ï¼Œä¸ºæ„å»ºå¯ä¿¡LLM&#x2F;VLMä»£ç†æä¾›æ–¹æ¡ˆã€‚</li>
<li>SAFEFLOWé€šè¿‡ç²¾ç»†çš„ä¿¡æ¯æµæ§åˆ¶ï¼ˆIFCï¼‰ç¡®ä¿æ•°æ®å®‰å…¨æ€§å’Œå®Œæ•´æ€§ã€‚</li>
<li>SAFEFLOWé˜²æ­¢ä¸å—ä¿¡ä»»æˆ–å¯¹æŠ—æ€§è¾“å…¥æ±¡æŸ“å†³ç­–ï¼Œå¹¶é€šè¿‡äº‹åŠ¡æ‰§è¡Œã€å†²çªè§£å†³å’Œå®‰å…¨è°ƒåº¦ç¡®ä¿å¹¶å‘ç¯å¢ƒä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>SAFEFLOWå¼•å…¥äº†å¤šç§æœºåˆ¶ä»¥å¢å¼ºå¯¹è¿è¡Œæ—¶é”™è¯¯å’Œæ”¿ç­–è¿è§„çš„æŠµå¾¡èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e775015c1c694a53a24ac85d591fe5a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8f1e40c16a973bee6c3946ea54b22b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d876d22601e61106212ce0886f8a1d6e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CrimeMind-Simulating-Urban-Crime-with-Multi-Modal-LLM-Agents"><a href="#CrimeMind-Simulating-Urban-Crime-with-Multi-Modal-LLM-Agents" class="headerlink" title="CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents"></a>CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents</h2><p><strong>Authors:Qingbin Zeng, Ruotong Zhao, Jinzhu Mao, Haoyang Li, Fengli Xu, Yong Li</strong></p>
<p>Modeling urban crime is an important yet challenging task that requires understanding the subtle visual, social, and cultural cues embedded in urban environments. Previous work has mainly focused on rule-based agent-based modeling (ABM) and deep learning methods. ABMs offer interpretability of internal mechanisms but exhibit limited predictive accuracy. In contrast, deep learning methods are often effective in prediction but are less interpretable and require extensive training data. Moreover, both lines of work lack the cognitive flexibility to adapt to changing environments. Leveraging the capabilities of large language models (LLMs), we propose CrimeMind, a novel LLM-driven ABM framework for simulating urban crime within a multi-modal urban context. A key innovation of our design is the integration of the Routine Activity Theory (RAT) into the agentic workflow of CrimeMind, enabling it to process rich multi-modal urban features and reason about criminal behavior. However, RAT requires LLM agents to infer subtle cues in evaluating environmental safety as part of assessing guardianship, which can be challenging for LLMs. To address this, we collect a small-scale human-annotated dataset and align CrimeMindâ€™s perception with human judgment via a training-free textual gradient method. Experiments across four major U.S. cities demonstrate that CrimeMind outperforms both traditional ABMs and deep learning baselines in crime hotspot prediction and spatial distribution accuracy, achieving up to a 24% improvement over the strongest baseline. Furthermore, we conduct counterfactual simulations of external incidents and policy interventions and it successfully captures the expected changes in crime patterns, demonstrating its ability to reflect counterfactual scenarios. Overall, CrimeMind enables fine-grained modeling of individual behaviors and facilitates evaluation of real-world interventions. </p>
<blockquote>
<p>å»ºæ¨¡åŸå¸‚çŠ¯ç½ªæ˜¯ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç†è§£åŸå¸‚ç¯å¢ƒä¸­å¾®å¦™çš„è§†è§‰ã€ç¤¾ä¼šå’Œæ–‡åŒ–çº¿ç´¢ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŸºäºè§„åˆ™çš„ä¸»ä½“å»ºæ¨¡ï¼ˆABMï¼‰å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ä¸Šã€‚ABMæä¾›äº†å†…éƒ¨æœºåˆ¶çš„è§£é‡Šæ€§ï¼Œä½†é¢„æµ‹ç²¾åº¦æœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨é¢„æµ‹æ–¹é¢é€šå¸¸å¾ˆæœ‰æ•ˆï¼Œä½†è§£é‡Šæ€§è¾ƒå·®ï¼Œä¸”éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ã€‚æ­¤å¤–ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½ç¼ºä¹é€‚åº”ç¯å¢ƒå˜åŒ–çš„è®¤çŸ¥çµæ´»æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ï¼Œæå‡ºäº†CrimeMindï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„LLMé©±åŠ¨ABMæ¡†æ¶ï¼Œå¯åœ¨å¤šæ¨¡å¼åŸå¸‚èƒŒæ™¯ä¸‹æ¨¡æ‹ŸåŸå¸‚çŠ¯ç½ªã€‚è®¾è®¡ä¸­çš„ä¸€ä¸ªå…³é”®åˆ›æ–°æ˜¯å°†æ—¥å¸¸æ´»åŠ¨ç†è®ºï¼ˆRATï¼‰é›†æˆåˆ°CrimeMindçš„ä¸»ä½“å·¥ä½œæµç¨‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†ä¸°å¯Œçš„å¤šæ¨¡å¼åŸå¸‚ç‰¹å¾å¹¶å¯¹çŠ¯ç½ªè¡Œä¸ºè¿›è¡Œæ¨ç†ã€‚ç„¶è€Œï¼ŒRATéœ€è¦LLMä¸»ä½“åœ¨è¯„ä¼°ç¯å¢ƒå®‰å…¨æ€§æ—¶æ¨æ–­å¾®å¦™çš„çº¿ç´¢ï¼Œä½œä¸ºè¯„ä¼°ç›‘æŠ¤æƒçš„ä¸€éƒ¨åˆ†ï¼Œè¿™å¯¹äºLLMæ¥è¯´å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå°è§„æ¨¡çš„äººå·¥æ³¨é‡Šæ•°æ®é›†ï¼Œå¹¶é€šè¿‡ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–‡æœ¬æ¢¯åº¦æ–¹æ³•ä¸CrimeMindçš„æ„ŸçŸ¥ä¸äººç±»åˆ¤æ–­å¯¹é½ã€‚åœ¨å››ä¸ªç¾å›½ä¸»è¦åŸå¸‚çš„å®éªŒè¡¨æ˜ï¼Œåœ¨çŠ¯ç½ªçƒ­ç‚¹é¢„æµ‹å’Œç©ºé—´åˆ†å¸ƒå‡†ç¡®æ€§æ–¹é¢ï¼ŒCrimeMindä¼˜äºä¼ ç»Ÿçš„ABMå’Œæ·±åº¦å­¦ä¹ åŸºçº¿ï¼Œæ¯”æœ€å¼ºåŸºçº¿æé«˜äº†é«˜è¾¾24%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¤–éƒ¨äº‹ä»¶å’Œæ”¿ç­–å¹²é¢„çš„åäº‹å®æ¨¡æ‹Ÿï¼ŒæˆåŠŸæ•æ‰äº†çŠ¯ç½ªæ¨¡å¼çš„é¢„æœŸå˜åŒ–ï¼Œè¯æ˜äº†å…¶åæ˜ åäº‹å®åœºæ™¯çš„èƒ½åŠ›ã€‚æ€»ä½“è€Œè¨€ï¼ŒCrimeMindèƒ½å¤Ÿå®ç°ä¸ªä½“è¡Œä¸ºçš„ç²¾ç»†å»ºæ¨¡ï¼Œå¹¶ä¾¿äºè¯„ä¼°ç°å®ä¸–ç•Œçš„å¹²é¢„æªæ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05981v2">PDF</a> Typos corrected</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ´»åŠ¨ç†è®ºï¼ˆRATï¼‰å’ŒåŸºäºä»£ç†çš„å»ºæ¨¡ï¼ˆABMï¼‰æ¡†æ¶ï¼Œæå‡ºäº†CrimeMindæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šæ¨¡æ€åŸå¸‚ç¯å¢ƒä¸­æ¨¡æ‹ŸåŸå¸‚çŠ¯ç½ªã€‚é€šè¿‡æ•´åˆRATï¼ŒCrimeMindèƒ½å¤Ÿå¤„ç†ä¸°å¯Œçš„å¤šæ¨¡æ€åŸå¸‚ç‰¹å¾ï¼Œå¹¶å¯¹çŠ¯ç½ªè¡Œä¸ºè¿›è¡Œæ¨ç†ã€‚ä¸ºè§£å†³LLMåœ¨è¯„ä¼°ç¯å¢ƒå®‰å…¨æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œé‡‡ç”¨æ— è®­ç»ƒæ–‡æœ¬æ¢¯åº¦æ–¹æ³•ä¸äººç±»åˆ¤æ–­å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒCrimeMindåœ¨çŠ¯ç½ªçƒ­ç‚¹é¢„æµ‹å’Œç©ºé—´åˆ†å¸ƒå‡†ç¡®æ€§æ–¹é¢ä¼˜äºä¼ ç»ŸABMå’Œæ·±åº¦å­¦ä¹ åŸºçº¿ï¼Œæ”¹è¿›ç‡æœ€é«˜è¾¾24%ã€‚æ­¤å¤–ï¼Œè¿˜èƒ½è¿›è¡Œå¤–éƒ¨äº‹ä»¶å’Œæ”¿ç­–å¹²é¢„çš„æ¨¡æ‹Ÿï¼ŒæˆåŠŸæ•æ‰çŠ¯ç½ªæ¨¡å¼çš„é¢„æœŸå˜åŒ–ã€‚æ€»ä½“è€Œè¨€ï¼ŒCrimeMindå¯å®ç°ç²¾ç»†çš„ä¸ªä½“è¡Œä¸ºå»ºæ¨¡ï¼Œå¹¶è¯„ä¼°ç°å®å¹²é¢„æªæ–½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸå¸‚çŠ¯ç½ªå»ºæ¨¡æ˜¯ä¸€é¡¹é‡è¦è€Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç†è§£åŸå¸‚ç¯å¢ƒä¸­çš„å¾®å¦™è§†è§‰ã€ç¤¾ä¼šå’Œæ–‡åŒ–çº¿ç´¢ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºåŸºäºè§„åˆ™çš„ä»£ç†å»ºæ¨¡ï¼ˆABMï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼Œä½†å„æœ‰å±€é™ï¼šABMé¢„æµ‹ç²¾åº¦æœ‰é™ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹è§£é‡Šæ€§è¾ƒå·®ä¸”éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ã€‚</li>
<li>æå‡ºäº†CrimeMindæ¨¡å‹ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€åŸå¸‚ç¯å¢ƒæ¨¡æ‹ŸåŸå¸‚çŠ¯ç½ªã€‚</li>
<li>CrimeMindæ•´åˆäº†æ´»åŠ¨ç†è®ºï¼ˆRATï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸°å¯Œçš„å¤šæ¨¡æ€åŸå¸‚ç‰¹å¾å¹¶æ¨ç†çŠ¯ç½ªè¡Œä¸ºã€‚</li>
<li>ä¸ºè§£å†³LLMåœ¨è¯„ä¼°ç¯å¢ƒå®‰å…¨æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œé‡‡ç”¨æ— è®­ç»ƒæ–‡æœ¬æ¢¯åº¦æ–¹æ³•å¯¹é½äººç±»åˆ¤æ–­ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCrimeMindåœ¨çŠ¯ç½ªçƒ­ç‚¹é¢„æµ‹å’Œç©ºé—´åˆ†å¸ƒå‡†ç¡®æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04f3749f28d181c2885842d811a1698e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41b204107070ede81fcf8712700dabbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-079f9ff7ac23494a67422f6964dd8b07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d76ab335e100f3a6dec1ac45ca47211.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TextAtari-100K-Frames-Game-Playing-with-Language-Agents"><a href="#TextAtari-100K-Frames-Game-Playing-with-Language-Agents" class="headerlink" title="TextAtari: 100K Frames Game Playing with Language Agents"></a>TextAtari: 100K Frames Game Playing with Language Agents</h2><p><strong>Authors:Wenhao Li, Wenwu Li, Chuyun Shen, Junjie Sheng, Zixiao Huang, Di Wu, Yun Hua, Wei Yin, Xiangfeng Wang, Hongyuan Zha, Bo Jin</strong></p>
<p>We present TextAtari, a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps. By translating the visual state representations of classic Atari games into rich textual descriptions, TextAtari creates a challenging test bed that bridges sequential decision-making with natural language processing. The benchmark includes nearly 100 distinct tasks with varying complexity, action spaces, and planning horizons, all rendered as text through an unsupervised representation learning framework (AtariARI). We evaluate three open-source large language models (Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks (zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how different forms of prior knowledge affect performance on these long-horizon challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and Reference-based-investigate the impact of semantic understanding, instruction comprehension, and expert demonstrations on agent decision-making. Our results reveal significant performance gaps between language agents and human players in extensive planning tasks, highlighting challenges in sequential reasoning, state tracking, and strategic planning across tens of thousands of steps. TextAtari provides standardized evaluation protocols, baseline implementations, and a framework for advancing research at the intersection of language models and planning. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Lww007/Text-Atari-Agents">https://github.com/Lww007/Text-Atari-Agents</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†TextAtariï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€æ™ºèƒ½ä½“åœ¨é•¿è¾¾10ä¸‡æ­¥çš„è¿œæœŸå†³ç­–ä»»åŠ¡ä¸Šçš„è¡¨ç°çš„åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡å°†ç»å…¸çš„Atariæ¸¸æˆçš„è§†è§‰çŠ¶æ€è¡¨ç¤ºè½¬åŒ–ä¸ºä¸°å¯Œçš„æ–‡æœ¬æè¿°ï¼ŒTextAtariåˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°å°†åºåˆ—å†³ç­–ä¸è‡ªç„¶è¯­è¨€å¤„ç†ç›¸ç»“åˆã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«è¿‘100ä¸ªä¸åŒä»»åŠ¡ï¼Œå…·æœ‰ä¸åŒçš„å¤æ‚åº¦ã€è¡ŒåŠ¨ç©ºé—´å’Œè®¡åˆ’æœŸé™ï¼Œæ‰€æœ‰å†…å®¹å‡é€šè¿‡æ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼ˆAtariARIï¼‰ä»¥æ–‡æœ¬å½¢å¼å‘ˆç°ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ä¸ªå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆQwen2.5-7Bã€Gemma-7Bå’ŒLlama3.1-8Bï¼‰åœ¨ä¸‰ç§æ™ºèƒ½ä½“æ¡†æ¶ï¼ˆé›¶æ ·æœ¬ã€å°‘é‡é“¾å¼æ€ç»´å’Œåæ€æ¨ç†ï¼‰ä¸‹çš„è¡¨ç°ï¼Œä»¥äº†è§£ä¸åŒå½¢å¼çš„å…ˆéªŒçŸ¥è¯†å¦‚ä½•å½±å“è¿™äº›é•¿æœŸå†³ç­–æŒ‘æˆ˜çš„æ€§èƒ½ã€‚å››ç§åœºæ™¯â€”â€”åŸºç¡€ã€é®è”½ã€æ‰‹åŠ¨å¢å¼ºå’Œå‚è€ƒåŸºç¡€â€”â€”æ¢è®¨äº†è¯­ä¹‰ç†è§£ã€æŒ‡ä»¤ç†è§£å’Œä¸“å®¶ç¤ºèŒƒå¯¹æ™ºèƒ½ä½“å†³ç­–çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œåœ¨è¯­è¨€æ™ºèƒ½ä½“å’Œäººç±»ç©å®¶åœ¨å¤æ‚è§„åˆ’ä»»åŠ¡ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œè¿™å‡¸æ˜¾äº†æ•°ä¸‡æ­¥çš„åºåˆ—æ¨ç†ã€çŠ¶æ€è·Ÿè¸ªå’Œæˆ˜ç•¥è§„åˆ’ä¸­çš„æŒ‘æˆ˜ã€‚TextAtariæä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ã€åŸºå‡†å®æ–½æ–¹æ¡ˆå’Œä¸€ä¸ªæ¡†æ¶ï¼Œä»¥ä¿ƒè¿›è¯­è¨€æ¨¡å‹å’Œè§„åˆ’äº¤å‰é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lww007/Text-Atari-Agents%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Lww007/Text-Atari-Agentsä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04098v2">PDF</a> 51 pages, 39 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ä¸ªåä¸ºTextAtariçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°æ—¨åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨é•¿è¾¾æ•°åä¸‡æ­¥çš„é•¿å‘¨æœŸå†³ç­–ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å°†ç»å…¸çš„Atariæ¸¸æˆçš„è§†è§‰çŠ¶æ€è¡¨ç¤ºè½¬åŒ–ä¸ºä¸°å¯Œçš„æ–‡æœ¬æè¿°ï¼ŒTextAtariä¸ºè¿æ¥åºåˆ—å†³ç­–ä¸è‡ªç„¶è¯­è¨€å¤„ç†æ­å»ºäº†ä¸€ä¸ªæŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°åŒ…å«è¿‘100ä¸ªä¸åŒä»»åŠ¡ï¼Œæ¶µç›–ä¸åŒçš„å¤æ‚åº¦ã€åŠ¨ä½œç©ºé—´å’Œè§„åˆ’å‘¨æœŸï¼Œæ‰€æœ‰ä»»åŠ¡å‡ä»¥æ–‡æœ¬å½¢å¼å‘ˆç°ã€‚æ–‡ç« è¯„ä¼°äº†ä¸‰ç§å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒä»£ç†æ¡†æ¶ä¸‹çš„è¡¨ç°ï¼Œå¹¶æ¢è®¨äº†ä¸åŒå…ˆéªŒçŸ¥è¯†å¯¹è¿™äº›é•¿å‘¨æœŸæŒ‘æˆ˜çš„å½±å“ã€‚TextAtariæä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ã€åŸºå‡†å®ç°å’Œä¸€ä¸ªæ¨åŠ¨è¯­è¨€æ¨¡å‹å’Œè§„åˆ’äº¤å‰ç ”ç©¶çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TextAtariæ˜¯ä¸€ä¸ªè¯„ä¼°è¯­è¨€æ¨¡å‹çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œä¸“æ³¨äºé•¿å‘¨æœŸå†³ç­–ä»»åŠ¡ï¼Œä»»åŠ¡è·¨åº¦é•¿è¾¾10ä¸‡æ­¥ã€‚</li>
<li>å¹³å°é€šè¿‡ç¿»è¯‘Atariæ¸¸æˆçš„è§†è§‰çŠ¶æ€è¡¨ç¤ºæˆæ–‡æœ¬æè¿°ï¼Œæ­å»ºäº†ä¸€ä¸ªè¿æ¥åºåˆ—å†³ç­–ä¸NLPçš„æ¡¥æ¢ã€‚</li>
<li>TextAtariåŒ…å«è¿‘100ä¸ªä¸åŒä»»åŠ¡ï¼Œæ¶µç›–ä¸åŒå¤æ‚åº¦ã€åŠ¨ä½œç©ºé—´å’Œè§„åˆ’å‘¨æœŸã€‚</li>
<li>è¯„ä¼°äº†ä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä»£ç†æ¡†æ¶ä¸‹çš„è¡¨ç°ã€‚</li>
<li>æ¢è®¨äº†ä¸åŒå½¢å¼çš„å…ˆéªŒçŸ¥è¯†å¯¹é•¿å‘¨æœŸæŒ‘æˆ˜ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚</li>
<li>é€šè¿‡å››ç§åœºæ™¯ç ”ç©¶ï¼Œæ¢è®¨äº†è¯­ä¹‰ç†è§£ã€æŒ‡ä»¤ç†è§£å’Œä¸“å®¶ç¤ºèŒƒå¯¹ä»£ç†å†³ç­–åˆ¶å®šçš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1eb9926189e9a7a3b31e0e0d1d8e6cbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d4ba4a8e070f6724fdbe7118858581a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1834b5297bab66416db4eb00e259b4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7ef136f2f50968c5fbf6f7587d4256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47047f50c44cb37898ed12da40830360.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DefenderBench-A-Toolkit-for-Evaluating-Language-Agents-in-Cybersecurity-Environments"><a href="#DefenderBench-A-Toolkit-for-Evaluating-Language-Agents-in-Cybersecurity-Environments" class="headerlink" title="DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments"></a>DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments</h2><p><strong>Authors:Chiyu Zhang, Marc-Alexandre Cote, Michael Albada, Anush Sankaran, Jack W. Stokes, Tong Wang, Amir Abdi, William Blum, Muhammad Abdul-Mageed</strong></p>
<p>Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBenchâ€™s modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench">https://github.com/microsoft/DefenderBench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨äººç±»è¯­è¨€ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç„¶è€Œå®ƒä»¬åœ¨ç½‘ç»œå®‰å…¨æ–¹é¢çš„æ½œåŠ›ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æ¨å‡ºäº†DefenderBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç”¨ã€å¼€æºçš„å·¥å…·åŒ…ï¼Œç”¨äºè¯„ä¼°è¯­è¨€ä»£ç†åœ¨è¿›æ”»ã€é˜²å¾¡å’Œç½‘ç»œå®‰å…¨çŸ¥è¯†ä»»åŠ¡æ–¹é¢çš„æ€§èƒ½ã€‚DefenderBenchåŒ…æ‹¬ç½‘ç»œå…¥ä¾µã€æ¶æ„å†…å®¹æ£€æµ‹ã€ä»£ç æ¼æ´åˆ†æå’Œç½‘ç»œå®‰å…¨çŸ¥è¯†è¯„ä¼°ç­‰ç¯å¢ƒã€‚å®ƒç‰¹æ„ä¸ºç ”ç©¶è€…è®¾è®¡ï¼Œç»æµå®æƒ ã€æ˜“äºè®¿é—®ï¼ŒåŒæ—¶æä¾›å…¬å¹³ä¸¥æ ¼çš„è¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†åŒ–çš„ä»£ç†æ¡†æ¶ï¼Œå¯¹è‹¥å¹²æœ€æ–°æŠ€æœ¯å’Œæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¼€æ”¾å’Œå°é—­æƒé‡æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒClaude-3.7-sonnetè¡¨ç°æœ€ä½³ï¼ŒDefenderBenchå¾—åˆ†ä¸º81.65ï¼Œå…¶æ¬¡æ˜¯Claude-3.7-sonnet-thinkï¼Œå¾—åˆ†ä¸º78.40ï¼Œè€Œæœ€å¥½çš„å¼€æ”¾æƒé‡æ¨¡å‹Llama 3.3 70Bç´§éšå…¶åï¼ŒDefenderBenchå¾—åˆ†ä¸º71.81ã€‚DefenderBenchçš„æ¨¡å—åŒ–è®¾è®¡å…è®¸æ— ç¼é›†æˆè‡ªå®šä¹‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä»»åŠ¡ï¼Œä¿ƒè¿›äº†å¯é‡å¤æ€§å’Œå…¬å¹³æ¯”è¾ƒã€‚DefenderBenchçš„åŒ¿åç‰ˆæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/microsoft/DefenderBenchä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00739v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œæ¨ç†äººç±»è¯­è¨€æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†DefenderBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨æ”»å‡»ã€é˜²å¾¡å’Œç½‘ç»œå®‰å…¨çŸ¥è¯†ä»»åŠ¡ä¸Šçš„å®ç”¨å¼€æºå·¥å…·åŒ…ã€‚å®ƒå¯¹ç ”ç©¶è€…å…·æœ‰ç»æµå®æƒ ã€æ˜“äºè®¿é—®çš„ç‰¹ç‚¹ï¼Œå¹¶èƒ½æä¾›å…¬å¹³å’Œä¸¥æ ¼çš„è¯„ä¼°ã€‚æˆ‘ä»¬å¯¹å‡ æ¬¾æœ€å…ˆè¿›å’Œæµè¡Œçš„LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜Claude-3.7-sonnetè¡¨ç°æœ€ä½³ï¼ŒDefenderBenchå¾—åˆ†ä¸º81.65ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>DefenderBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨æ–¹é¢çš„å®ç”¨å¼€æºå·¥å…·åŒ…ã€‚</li>
<li>DefenderBenchåŒ…æ‹¬ç½‘ç»œå…¥ä¾µã€æ¶æ„å†…å®¹æ£€æµ‹ã€ä»£ç æ¼æ´åˆ†æå’Œç½‘ç»œå®‰å…¨çŸ¥è¯†è¯„ä¼°ç­‰ç¯å¢ƒã€‚</li>
<li>DefenderBenchå…·æœ‰ç»æµå®æƒ ã€æ˜“äºè®¿é—®çš„ç‰¹ç‚¹ï¼Œä¸ºç ”ç©¶è€…æä¾›å…¬å¹³å’Œä¸¥æ ¼çš„è¯„ä¼°ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒClaude-3.7-sonnetè¡¨ç°æœ€ä½³ï¼ŒDefenderBenchå¾—åˆ†ä¸º81.65ã€‚</li>
<li>DefenderBenchçš„æ¨¡å—åŒ–è®¾è®¡å…è®¸æ— ç¼é›†æˆè‡ªå®šä¹‰çš„LLMå’Œä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a6bc86fc70229af47be50d4b82aba1e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bab7f4d11665b5e3feed5f17ce028c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ce477798b819c94cd5e4fe2f624ddee.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AnnaAgent-Dynamic-Evolution-Agent-System-with-Multi-Session-Memory-for-Realistic-Seeker-Simulation"><a href="#AnnaAgent-Dynamic-Evolution-Agent-System-with-Multi-Session-Memory-for-Realistic-Seeker-Simulation" class="headerlink" title="AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for   Realistic Seeker Simulation"></a>AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for   Realistic Seeker Simulation</h2><p><strong>Authors:Ming Wang, Peidong Wang, Lin Wu, Xiaocui Yang, Daling Wang, Shi Feng, Yuxin Chen, Bixuan Wang, Yifei Zhang</strong></p>
<p>Constrained by the cost and ethical concerns of involving real seekers in AI-driven mental health, researchers develop LLM-based conversational agents (CAs) with tailored configurations, such as profiles, symptoms, and scenarios, to simulate seekers. While these efforts advance AI in mental health, achieving more realistic seeker simulation remains hindered by two key challenges: dynamic evolution and multi-session memory. Seekersâ€™ mental states often fluctuate during counseling, which typically spans multiple sessions. To address this, we propose AnnaAgent, an emotional and cognitive dynamic agent system equipped with tertiary memory. AnnaAgent incorporates an emotion modulator and a complaint elicitor trained on real counseling dialogues, enabling dynamic control of the simulatorâ€™s configurations. Additionally, its tertiary memory mechanism effectively integrates short-term and long-term memory across sessions. Evaluation results, both automated and manual, demonstrate that AnnaAgent achieves more realistic seeker simulation in psychological counseling compared to existing baselines. The ethically reviewed and screened code can be found on <a target="_blank" rel="noopener" href="https://github.com/sci-m-wang/AnnaAgent">https://github.com/sci-m-wang/AnnaAgent</a>. </p>
<blockquote>
<p>å—é™äºäººå·¥æ™ºèƒ½å¿ƒç†å¥åº·é¢†åŸŸæ¶‰åŠçœŸå®æ±‚åŠ©è€…çš„æˆæœ¬å’Œä¼¦ç†é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹è¯ä»£ç†ï¼ˆCAï¼‰ï¼Œå¹¶å®šåˆ¶äº†é…ç½®ï¼Œå¦‚ä¸ªäººç®€ä»‹ã€ç—‡çŠ¶å’Œåœºæ™¯ï¼Œä»¥æ¨¡æ‹Ÿæ±‚åŠ©è€…ã€‚å°½ç®¡è¿™äº›åŠªåŠ›æ¨åŠ¨äº†äººå·¥æ™ºèƒ½åœ¨å¿ƒç†å¥åº·é¢†åŸŸçš„å‘å±•ï¼Œä½†ç”±äºä¸¤ä¸ªå…³é”®æŒ‘æˆ˜â€”â€”åŠ¨æ€æ¼”å˜å’Œå¤šä¼šè¯è®°å¿†ï¼Œå®ç°æ›´çœŸå®çš„æ±‚åŠ©è€…æ¨¡æ‹Ÿä»ç„¶å—åˆ°é™åˆ¶ã€‚åœ¨å’¨è¯¢è¿‡ç¨‹ä¸­ï¼Œæ±‚åŠ©è€…çš„å¿ƒç†çŠ¶æ€å¾€å¾€ä¼šæ³¢åŠ¨ï¼Œè¿™ä¸€è¿‡ç¨‹é€šå¸¸è·¨è¶Šå¤šä¸ªä¼šè¯ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AnnaAgentï¼Œä¸€ä¸ªé…å¤‡ä¸‰çº§è®°å¿†çš„æƒ…æ„Ÿå’Œè®¤çŸ¥åŠ¨æ€ä»£ç†ç³»ç»Ÿã€‚AnnaAgentèå…¥äº†ä¸€ä¸ªæƒ…æ„Ÿè°ƒèŠ‚å™¨å’ŒæŠ•è¯‰è¯±å‘å™¨ï¼Œç»è¿‡çœŸå®å’¨è¯¢å¯¹è¯çš„è®­ç»ƒï¼Œèƒ½å¤Ÿå®ç°æ¨¡æ‹Ÿå™¨é…ç½®çš„åŠ¨æ€æ§åˆ¶ã€‚æ­¤å¤–ï¼Œå…¶ä¸‰çº§è®°å¿†æœºåˆ¶æœ‰æ•ˆåœ°æ•´åˆäº†è·¨ä¼šè¯çš„çŸ­æœŸå’Œé•¿æœŸè®°å¿†ã€‚è‡ªåŠ¨åŒ–å’Œæ‰‹åŠ¨è¯„ä¼°ç»“æœå‡è¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿ï¼ŒAnnaAgentåœ¨å¿ƒç†å’¨è¯¢ä¸­å®ç°äº†æ›´çœŸå®çš„æ±‚åŠ©è€…æ¨¡æ‹Ÿã€‚ç»è¿‡ä¼¦ç†å®¡æŸ¥å’Œç­›é€‰çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sci-m-wang/AnnaAgent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sci-m-wang/AnnaAgentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00551v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºAIé©±åŠ¨çš„å¿ƒç†å¥åº·é¢†åŸŸä¸­çœŸå®å¯»æ±‚è€…å‚ä¸çš„æˆæœ¬å’Œä¼¦ç†é—®é¢˜ï¼Œç ”ç©¶è€…å¼€å‘äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹è¯ä»£ç†ï¼ˆCAï¼‰ï¼Œå¹¶å®šåˆ¶é…ç½®ä»¥æ¨¡æ‹Ÿå¯»æ±‚è€…ã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œä½†å®ç°æ›´çœŸå®çš„å¯»æ±‚è€…æ¨¡æ‹Ÿä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåŠ¨æ€æ¼”å˜å’Œå¤šä¼šè¯è®°å¿†ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†AnnaAgentç³»ç»Ÿï¼Œä¸€ä¸ªé…å¤‡ä¸‰çº§è®°å¿†çš„æƒ…æ„Ÿå’Œè®¤çŸ¥åŠ¨æ€ä»£ç†ç³»ç»Ÿã€‚AnnaAgentç»“åˆäº†åŸºäºçœŸå®å’¨è¯¢å¯¹è¯è®­ç»ƒçš„æƒ…æ„Ÿè°ƒèŠ‚å™¨å’ŒæŠ•è¯‰æ¿€å‘å™¨ï¼Œå®ç°äº†æ¨¡æ‹Ÿå™¨é…ç½®çš„åŠ¨æ€æ§åˆ¶ã€‚å…¶ä¸‰çº§è®°å¿†æœºåˆ¶æœ‰æ•ˆåœ°æ•´åˆäº†è·¨ä¼šè¯çš„çŸ­æœŸå’Œé•¿æœŸè®°å¿†ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿ï¼ŒAnnaAgentåœ¨å¿ƒç†å’¨è¯¢ä¸­çš„å¯»æ±‚è€…æ¨¡æ‹Ÿæ›´ä¸ºçœŸå®ã€‚ç›¸å…³ä¼¦ç†å®¡æŸ¥å’Œç­›é€‰çš„ä»£ç å¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/sci-m-wang/AnnaAgent">https://github.com/sci-m-wang/AnnaAgent</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶è€…ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼€å‘å¯¹è¯ä»£ç†ï¼ˆCAï¼‰ä»¥æ¨¡æ‹Ÿå¿ƒç†å¥åº·é¢†åŸŸçš„å¯»æ±‚è€…ã€‚</li>
<li>å®ç°æ›´çœŸå®çš„å¯»æ±‚è€…æ¨¡æ‹Ÿé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåŠ¨æ€æ¼”å˜å’Œå¤šä¼šè¯è®°å¿†ã€‚</li>
<li>AnnaAgentç³»ç»Ÿé€šè¿‡é…å¤‡ä¸‰çº§è®°å¿†æœºåˆ¶è§£å†³äº†è¿™ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>AnnaAgentç»“åˆäº†æƒ…æ„Ÿè°ƒèŠ‚å™¨å’ŒæŠ•è¯‰æ¿€å‘å™¨ï¼Œå®ç°æ¨¡æ‹Ÿå™¨é…ç½®çš„åŠ¨æ€æ§åˆ¶ã€‚</li>
<li>AnnaAgentçš„æ¨¡æ‹Ÿæ•ˆæœç»è¿‡è‡ªåŠ¨åŒ–å’Œæ‰‹åŠ¨è¯„ä¼°ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ›´ä¸ºçœŸå®ã€‚</li>
<li>AnnaAgentç³»ç»Ÿç»è¿‡ä¼¦ç†å®¡æŸ¥å’Œç­›é€‰ï¼Œç›¸å…³ä»£ç å¯å…¬å¼€è®¿é—®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-978d53947412af624b4f6a5adccc0666.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9590124cce9c5ebd348dd3b62219cfb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eca133825acf6b1e28b46158337b23b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d23e5c575d20de0de5f3f08ea92f27e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Understanding-Bias-Reinforcement-in-LLM-Agents-Debate"><a href="#Understanding-Bias-Reinforcement-in-LLM-Agents-Debate" class="headerlink" title="Understanding Bias Reinforcement in LLM Agents Debate"></a>Understanding Bias Reinforcement in LLM Agents Debate</h2><p><strong>Authors:Jihwan Oh, Minchan Jeong, Jongwoo Ko, Se-Young Yun</strong></p>
<p>Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MADâ€™s limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse $\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLMâ€™s strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¦‚æç¤ºå·¥ç¨‹å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ¥è§£å†³å¤æ‚é—®é¢˜ï¼Œä½†ç¡®ä¿æ¨ç†çš„æ­£ç¡®æ€§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶è‡ªæˆ‘æ ¡æ­£æ–¹æ³•ï¼ˆå¦‚è‡ªæˆ‘ä¸€è‡´æ€§å’Œè‡ªæˆ‘æ”¹è¿›ï¼‰æ—¨åœ¨æé«˜å¯é æ€§ï¼Œä½†ç”±äºç¼ºä¹æœ‰æ•ˆçš„åé¦ˆæœºåˆ¶ï¼Œå®ƒä»¬å¾€å¾€ä¼šå¼ºåŒ–åè§ã€‚å¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆMADï¼‰ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ³•å·²ç»å‡ºç°ï¼Œä½†æˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªå…³é”®å±€é™ï¼šåè§å¼ºåŒ–ï¼Œå³è¾©è®ºæ”¾å¤§æ¨¡å‹åè§è€Œä¸æ˜¯çº æ­£å®ƒä»¬ï¼›ä»¥åŠç¼ºä¹è§‚ç‚¹å¤šæ ·æ€§ï¼Œå› ä¸ºæ‰€æœ‰æ™ºèƒ½ä½“éƒ½ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å’Œæ¨ç†æ¨¡å¼ï¼Œé™åˆ¶äº†çœŸæ­£è¾©è®ºçš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<em>MetaNIM Arena</em>ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨å¯¹æŠ—æ€§æˆ˜ç•¥å†³ç­–ä¸­çš„èƒ½åŠ›ï¼ŒåŠ¨æ€äº¤äº’ä¼šå½±å“æœ€ä¼˜å†³ç­–ã€‚ä¸ºäº†å…‹æœMADçš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DReaMADï¼ˆé€šè¿‡ä¿®æ”¹æç¤ºä¿ƒè¿›å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„å¤šæ ·åŒ–æ¨ç†ï¼‰ï¼Œä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒï¼ˆ1ï¼‰ç²¾ç‚¼LLMçš„æˆ˜ç•¥å…ˆéªŒçŸ¥è¯†ä»¥æé«˜æ¨ç†è´¨é‡ï¼Œï¼ˆ2ï¼‰é€šè¿‡ç³»ç»Ÿåœ°ä¿®æ”¹æç¤ºä¿ƒè¿›å•ä¸€æ¨¡å‹å†…çš„ä¸åŒè§‚ç‚¹ï¼Œä»è€Œå‡å°‘åè§ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒDReaMADåœ¨å¤šä¸ªæˆ˜ç•¥ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†å†³ç­–å‡†ç¡®æ€§ã€æ¨ç†å¤šæ ·æ€§å’Œåè§ç¼“è§£ï¼Œç¡®ç«‹å…¶ä¸ºLLMå†³ç­–åˆ¶å®šä¸­æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16814v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ— éœ€è®­ç»ƒçš„æ–¹æ³•å¦‚æç¤ºå·¥ç¨‹å’Œè‡ªç„¶è¯­å¢ƒå­¦ä¹ æ¥è§£å†³å¤æ‚é—®é¢˜ï¼Œä½†ç¡®ä¿æ¨ç†çš„æ­£ç¡®æ€§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°½ç®¡è‡ªæˆ‘æ ¡æ­£æ–¹æ³•å¦‚è‡ªæˆ‘ä¸€è‡´æ€§å’Œè‡ªæˆ‘æ”¹è¿›æ—¨åœ¨æé«˜å¯é æ€§ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå› ç¼ºä¹æœ‰æ•ˆçš„åé¦ˆæœºåˆ¶è€Œå¼ºåŒ–åè§ã€‚å¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆMADï¼‰ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ³•åº”è¿è€Œç”Ÿï¼Œä½†å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™ï¼šåè§å¼ºåŒ–å’Œè§†è§’ç¼ºä¹å¤šæ ·æ€§ã€‚ä¸ºç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MetaNIM ArenaåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨å¯¹æŠ—æ€§æˆ˜ç•¥å†³ç­–åˆ¶å®šä¸­çš„èƒ½åŠ›ï¼Œå…¶ä¸­åŠ¨æ€äº¤äº’å½±å“æœ€ä¼˜å†³ç­–ã€‚ä¸ºäº†å…‹æœMADçš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DReaMADæ¡†æ¶ï¼ˆé€šè¿‡å¤šå…ƒæ™ºèƒ½ä½“è¾©è®ºå’Œç²¾ç‚¼æç¤ºå®ç°å¤šæ ·åŒ–çš„æ¨ç†ï¼‰ï¼Œè¯¥æ¡†æ¶ï¼ˆ1ï¼‰å®Œå–„LLMçš„æˆ˜ç•¥å…ˆéªŒçŸ¥è¯†ä»¥æé«˜æ¨ç†è´¨é‡ï¼Œï¼ˆ2ï¼‰é€šè¿‡ç³»ç»Ÿåœ°ä¿®æ”¹æç¤ºï¼Œåœ¨å•ä¸ªæ¨¡å‹å†…ä¿ƒè¿›ä¸åŒè§‚ç‚¹çš„å½¢æˆï¼Œä»è€Œå‡å°‘åè§ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒDReaMADåœ¨å¤šä¸ªæˆ˜ç•¥ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†å†³ç­–å‡†ç¡®æ€§ã€æ¨ç†å¤šæ ·æ€§å’Œåè§ç¼“è§£ï¼Œç¡®ç«‹å…¶ä½œä¸ºLLMå†³ç­–åˆ¶å®šçš„æ›´æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶é¢ä¸´æ¨ç†æ­£ç¡®æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>è‡ªæˆ‘æ ¡æ­£æ–¹æ³•è™½ç„¶æé«˜äº†å¯é æ€§ï¼Œä½†å¯èƒ½å¼ºåŒ–æ¨¡å‹åè§ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆMADï¼‰ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ³•å­˜åœ¨åè§å¼ºåŒ–å’Œè§†è§’ç¼ºä¹å¤šæ ·æ€§ä¸¤ä¸ªé—®é¢˜ã€‚</li>
<li>MetaNIM ArenaåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°LLMsåœ¨æˆ˜ç•¥å†³ç­–ä¸­çš„èƒ½åŠ›ã€‚</li>
<li>DReaMADæ¡†æ¶é€šè¿‡å®Œå–„æˆ˜ç•¥å…ˆéªŒçŸ¥è¯†å’Œä¿ƒè¿›ä¸åŒè§‚ç‚¹çš„å½¢æˆæ¥æé«˜æ¨ç†è´¨é‡å’Œå‡å°‘åè§ã€‚</li>
<li>DReaMADæ¡†æ¶åœ¨å¤šä¸ªæˆ˜ç•¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„å†³ç­–å‡†ç¡®æ€§ã€æ¨ç†å¤šæ ·æ€§å’Œåè§ç¼“è§£æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-069f28dc8d1f96d10118d3c25605c027.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbd6932c0fc5c6ed38783530a76fe110.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9341ba464e8d7147fde2df82cc4cd2fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c7e272cddf71634cdae5e650030df16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4644be4aabd665414c438b41f5a4ae0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67fb524f23eddf29f91799c63e276c8e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FlickerFusion-Intra-trajectory-Domain-Generalizing-Multi-Agent-RL"><a href="#FlickerFusion-Intra-trajectory-Domain-Generalizing-Multi-Agent-RL" class="headerlink" title="FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL"></a>FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL</h2><p><strong>Authors:Woosung Koh, Wonbeen Oh, Siyeol Kim, Suhin Shin, Hyeongjin Kim, Jaein Jang, Junghyun Lee, Se-Young Yun</strong></p>
<p>Multi-agent reinforcement learning has demonstrated significant potential in addressing complex cooperative tasks across various real-world applications. However, existing MARL approaches often rely on the restrictive assumption that the number of entities (e.g., agents, obstacles) remains constant between training and inference. This overlooks scenarios where entities are dynamically removed or added during the inference trajectory â€“ a common occurrence in real-world environments like search and rescue missions and dynamic combat situations. In this paper, we tackle the challenge of intra-trajectory dynamic entity composition under zero-shot out-of-domain (OOD) generalization, where such dynamic changes cannot be anticipated beforehand. Our empirical studies reveal that existing MARL methods suffer significant performance degradation and increased uncertainty in these scenarios. In response, we propose FlickerFusion, a novel OOD generalization method that acts as a universally applicable augmentation technique for MARL backbone methods. FlickerFusion stochastically drops out parts of the observation space, emulating being in-domain when inferenced OOD. The results show that FlickerFusion not only achieves superior inference rewards but also uniquely reduces uncertainty vis-`a-vis the backbone, compared to existing methods. Benchmarks, implementations, and model weights are organized and open-sourced at flickerfusion305.github.io, accompanied by ample demo video renderings. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åœ¨å„ç§ç°å®ä¸–ç•Œåº”ç”¨ä¸­è§£å†³å¤æ‚åˆä½œä»»åŠ¡æ–¹é¢å·²æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMulti-Agent Reinforcement Learningï¼Œç®€ç§°MARLï¼‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸€ä¸ªé™åˆ¶æ€§å‡è®¾ï¼Œå³åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å®ä½“çš„æ•°é‡ï¼ˆä¾‹å¦‚æ™ºèƒ½ä½“ã€éšœç¢ç‰©ï¼‰ä¿æŒä¸å˜ã€‚è¿™å¿½ç•¥äº†åœ¨æ¨ç†è½¨è¿¹è¿‡ç¨‹ä¸­å®ä½“è¢«åŠ¨æ€ç§»é™¤æˆ–æ·»åŠ çš„åœºæ™¯â€”â€”è¿™åœ¨æœç´¢å’Œæ•‘æ´ä»»åŠ¡ä»¥åŠåŠ¨æ€æˆ˜æ–—æƒ…å†µç­‰ç°å®ç¯å¢ƒä¸­æ˜¯å¸¸è§çš„æƒ…å†µã€‚æœ¬æ–‡è§£å†³æ— é¢„è®¾ç¯å¢ƒä¸‹è½¨è¿¹å†…åŠ¨æ€å®ä½“ç»„æˆæŒ‘æˆ˜ï¼Œå³åœ¨æ— æ³•é¢„æµ‹çš„æƒ…å†µä¸‹ä¼šå‘ç”Ÿè¿™æ ·çš„åŠ¨æ€å˜åŒ–ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶å‘ç°åœ¨è¿™äº›åœºæ™¯ä¸­ç°æœ‰MARLæ–¹æ³•æ€§èƒ½æ˜¾è‘—ä¸‹é™ä¸”ä¸ç¡®å®šæ€§å¢åŠ ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬æå‡ºäº†FlickerFusionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç¦»åŸŸæ³›åŒ–æ–¹æ³•ï¼Œä½œä¸ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸»è¦æ–¹æ³•çš„ä¸€ç§æ™®éé€‚ç”¨çš„å¢å¼ºæŠ€æœ¯ã€‚FlickerFusionä¼šéšæœºä¸¢å¤±è§‚æµ‹ç©ºé—´çš„éƒ¨åˆ†ä¿¡æ¯ï¼Œæ¨¡æ‹Ÿåœ¨åŸŸå†…çš„æƒ…å†µè¿›è¡Œæ¨ç†ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFlickerFusionä¸ä»…å®ç°äº†æ›´é«˜çš„æ¨ç†å¥–åŠ±ï¼Œè€Œä¸”ç‹¬ç‰¹åœ°é™ä½äº†ç›¸å¯¹äºä¸»è¦æ–¹æ³•çš„ä¸ç¡®å®šæ€§ã€‚åŸºå‡†æµ‹è¯•ã€å®æ–½æ–¹æ³•å’Œæ¨¡å‹æƒé‡éƒ½åœ¨å…¬å¼€ç½‘ç«™ä¸Šæ•´ç†å¹¶å¼€æºï¼ŒåŒæ—¶é…æœ‰ä¸°å¯Œçš„æ¼”ç¤ºè§†é¢‘æ¸²æŸ“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15876v4">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åœ¨è§£å†³å„ç§ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¤æ‚åˆä½œä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å‡è®¾å®ä½“æ•°é‡åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒä¸å˜ï¼Œå¿½ç•¥äº†ç°å®ä¸–ç•Œä¸­å®ä½“åŠ¨æ€å˜åŒ–çš„æƒ…å†µã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFlickerFusionçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡éšæœºä¸¢å¼ƒè§‚æµ‹ç©ºé—´çš„éƒ¨åˆ†ä¿¡æ¯ï¼Œæ¨¡æ‹Ÿåœ¨ä¸åŒåˆ†å¸ƒä¸‹æ¨ç†æ‰€é¢å¯¹çš„å›°éš¾ï¼Œä»¥æ¨¡æ‹Ÿin-domainçš„ç¯å¢ƒï¼Œæå‡äº†å¼ºåŒ–å­¦ä¹ æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œä¸ç¡®å®šæ€§çš„é²æ£’æ€§ã€‚å…¶ç»“æœè¡¨æ˜ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼ŒFlickerFusionèƒ½å¤Ÿæ˜¾è‘—åœ°æå‡æ¨ç†å¥–åŠ±å¹¶å‡å°‘ä¸ç¡®å®šæ€§ã€‚å…¶æ¨¡å‹èµ„æºå·²å…¬å¼€äºæŒ‡å®šç½‘ç«™ä¾›å¤§ä¼—è®¿é—®å’Œå‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åœ¨å¤„ç†å¤æ‚åˆä½œä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>ç°æœ‰å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å®ä½“æ•°é‡å˜åŒ–æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>FlickerFusionä½œä¸ºä¸€ç§æ–°çš„æ³›åŒ–æ–¹æ³•ï¼Œè§£å†³äº†å®ä½“åŠ¨æ€å˜åŒ–çš„é—®é¢˜ã€‚</li>
<li>FlickerFusioné€šè¿‡éšæœºä¸¢å¼ƒè§‚æµ‹ç©ºé—´ä¿¡æ¯æ¨¡æ‹ŸåŠ¨æ€å˜åŒ–çš„å®ä½“åœºæ™¯ï¼Œå®ç°äº†æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚</li>
<li>FlickerFusionç›¸è¾ƒäºç°æœ‰æ–¹æ³•èƒ½å¤Ÿåœ¨æé«˜æ¨ç†å¥–åŠ±çš„åŒæ—¶é™ä½ä¸ç¡®å®šæ€§ã€‚</li>
<li>FlickerFusionæ–¹æ³•å…·æœ‰æ™®éé€‚ç”¨æ€§ï¼Œå¯åº”ç”¨äºå¤šç§å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ èƒŒæ™¯æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ade91bd02c09b5b2594084f291cc728e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bdad938375bc3e55aa9e4f6607b7775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d69d3692d0aceab77e6cddacdc80a1de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73f6ed0502af5989c9bc03ca67d8bc05.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-12/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-12/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-12/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-01bd51cf355ba2bd9554a5a5c451e0f6.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-12  AraReasoner Evaluating Reasoning-Based LLMs for Arabic NLP
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8d1a08e8c7dbb9ea8fc7bc567907c814.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
