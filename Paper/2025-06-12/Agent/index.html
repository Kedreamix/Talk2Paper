<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-47047f50c44cb37898ed12da40830360.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    75 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-12-更新"><a href="#2025-06-12-更新" class="headerlink" title="2025-06-12 更新"></a>2025-06-12 更新</h1><h2 id="VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning"><a href="#VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning" class="headerlink" title="VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning"></a>VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning</h2><p><strong>Authors:Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</strong></p>
<p>Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems. </p>
<blockquote>
<p>在动态环境中协调多个实体代理仍然是人工智能的核心挑战，这需要感知驱动的推理和可扩展的合作策略。虽然最近的研究已经利用大型语言模型（LLM）进行多代理规划，但很少有人开始探索视觉语言模型（VLM）用于视觉推理。然而，这些基于VLM的方法在支持多种实体类型方面仍存在局限性。在这项工作中，我们引入了专门为实体多代理合作定制的分层基准——VIKI-Bench，它包含三个结构化级别：代理激活、任务规划和轨迹感知。VIKI-Bench包括多种机器人实体、多视角视觉观察和结构化的监督信号，以评估基于视觉输入的推理。为了证明VIKI-Bench的实用性，我们提出了VIKI-R，这是一个两阶段框架，它通过思维链注释演示对预训练的视觉语言模型（VLM）进行微调，然后在多层次奖励信号下进行强化学习。我们的广泛实验表明，VIKI-R在所有任务层面都显著优于基准方法。此外，我们还表明，强化学习能够促进异构代理之间组合合作模式的出现。总之，VIKI-Bench和VIKI-R为推进实体AI系统的多代理、视觉驱动合作提供了统一的测试平台和方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09049v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://faceong.github.io/VIKI-R/">https://faceong.github.io/VIKI-R/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了在人工智能领域中，协调多个实体代理在动态环境中的挑战，需要感知驱动的推理和可扩展的合作策略。虽然已有工作利用大型语言模型进行多代理规划，但基于视觉语言模型的视觉推理方法仍有限支持多种实体类型。本文引入VIKI-Bench，首个针对实体多代理合作的分层基准测试，包括代理激活、任务规划和轨迹感知三个结构化级别。VIKI-Bench包含多种机器人实体、多视角视觉观察和结构化的监督信号，以评估基于视觉输入的推理。为展示VIKI-Bench的实用性，本文提出VIKI-R，一个两阶段框架，通过精细调整预训练的视觉语言模型，利用思维链注解演示，再通过多级别奖励信号进行强化学习。实验表明，VIKI-R在各级任务上显著优于基准方法，强化学习促使异质代理间出现组合合作模式。总之，VIKI-Bench和VIKI-R为推进多代理、视觉驱动的合作实体人工智能系统提供了统一的测试床和方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>协调多个实体代理在动态环境中是人工智能的核心挑战，需结合感知驱动的推理和合作策略。</li>
<li>虽然大型语言模型已用于多代理规划，但基于视觉语言模型的视觉推理方法仍需探索更多领域。</li>
<li>VIKI-Bench是首个针对实体多代理合作的分层基准测试，包含代理激活、任务规划和轨迹感知等结构化级别。</li>
<li>VIKI-Bench注重多样机器人实体、多视角视觉观察和结构化的监督信号的评估。</li>
<li>VIKI-R框架结合了预训练视觉语言模型的调整、思维链注解演示和强化学习，提高了多代理任务的表现。</li>
<li>强化学习有助于异质代理间出现组合合作模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09049">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f318d1c1f344134a0ccdcac656830d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b4ff7c2f9df7c67670ee1acf6207be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0a3569e6a044d60b77d21c9a1d0f4e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-546e911c893917079b05fe314b9dedc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-493cacde6faf7b9e2c94ba6ec86deaf5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Atomic-to-Compositional-Generalization-for-Mobile-Agents-with-A-New-Benchmark-and-Scheduling-System"><a href="#Atomic-to-Compositional-Generalization-for-Mobile-Agents-with-A-New-Benchmark-and-Scheduling-System" class="headerlink" title="Atomic-to-Compositional Generalization for Mobile Agents with A New   Benchmark and Scheduling System"></a>Atomic-to-Compositional Generalization for Mobile Agents with A New   Benchmark and Scheduling System</h2><p><strong>Authors:Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, Zhuosheng Zhang</strong></p>
<p>Autonomous agents powered by multimodal large language models have been developed to facilitate task execution on mobile devices. However, prior work has predominantly focused on atomic tasks – such as shot-chain execution tasks and single-screen grounding tasks – while overlooking the generalization to compositional tasks, which are indispensable for real-world applications. This work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile agents on three categories of compositional operations: Simple Concatenation, Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in 20 fully controllable local utility app environments, as well as 30 online Chinese and English service apps. It comprises 100 interactive task templates with an average optimal step count of 14.05. Experimental results across a range of mobile agents with agentic workflow or agent-as-a-model show that UI-NEXUS presents significant challenges. Specifically, existing agents generally struggle to balance performance and efficiency, exhibiting representative failure modes such as under-execution, over-execution, and attention drift, causing visible atomic-to-compositional generalization gap. Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient scheduling system to tackle compositional mobile tasks. AGENT-NEXUS extrapolates the abilities of existing mobile agents by dynamically decomposing long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS achieves 24% to 40% task success rate improvement for existing mobile agents on compositional operation tasks within the UI-NEXUS benchmark without significantly sacrificing inference overhead. The demo video, dataset, and code are available on the project page at <a target="_blank" rel="noopener" href="https://ui-nexus.github.io/">https://ui-nexus.github.io</a>. </p>
<blockquote>
<p>由多模态大型语言模型驱动的自主体已被开发出来，以在移动设备上促进任务执行。然而，先前的工作主要集中在原子任务上，如射击链执行任务和单屏幕接地任务，而忽略了对组合任务的推广，这对于实际应用来说是不可或缺的。本文介绍了UI-NEXUS，这是一个全面的基准测试，旨在评估移动体在三类组合操作上的表现：简单连接、上下文转换和深度潜水。UI-NEXUS支持在20个完全可控的本地实用应用程序环境中的交互式评估，以及30个在线中文和英文服务应用程序。它包括100个交互式任务模板，平均最佳步骤计数为14.05。跨越一系列具有自主工作流程或代理模型的移动代理的实验结果表明，UI-NEXUS存在重大挑战。具体来说，现有代理通常在性能和效率之间挣扎，表现出典型的失败模式，如执行不足、过度执行和注意力漂移，导致明显的从原子到组合的一般化差距。根据这些发现，我们提出了AGENT-NEXUS，这是一个轻便高效的调度系统，用于解决组合移动任务。AGENT-NEXUS通过动态地将长周期任务分解为一系列独立的原子子任务来推断现有移动代理的能力。在UI-NEXUS基准测试中，AGENT-NEXUS在不显著增加推理开销的情况下，使现有移动代理在组合操作任务上的任务成功率提高了24%至40%。演示视频、数据集和代码可在<a target="_blank" rel="noopener" href="https://ui-nexus.github.io项目页首上找到./">https://ui-nexus.github.io项目页面上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08972v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08972">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7cccc140d6eb55396e85a7548cc388e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8ff7901492a4a68c1ad2895092234e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8434d8b9bd25410d7dfd7acb7e3fd8ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4b8f0edbd91b19b5774ebbebe761800.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="What-Limits-Virtual-Agent-Application-OmniBench-A-Scalable-Multi-Dimensional-Benchmark-for-Essential-Virtual-Agent-Capabilities"><a href="#What-Limits-Virtual-Agent-Application-OmniBench-A-Scalable-Multi-Dimensional-Benchmark-for-Essential-Virtual-Agent-Capabilities" class="headerlink" title="What Limits Virtual Agent Application? OmniBench: A Scalable   Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities"></a>What Limits Virtual Agent Application? OmniBench: A Scalable   Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities</h2><p><strong>Authors:Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation with limited scenarios, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, cross-platform, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91% human acceptance rate. Training on our graph-structured data shows that it can more efficiently guide agents compared to manually annotated data. We conduct multidimensional evaluations for various open-source and closed-source models, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at <a target="_blank" rel="noopener" href="https://omni-bench.github.io/">https://omni-bench.github.io/</a>. </p>
<blockquote>
<p>随着多模态大型语言模型（MLLMs）的不断发展，基于MLLM的虚拟代理已经表现出了显著的性能。然而，现有的基准测试面临着重大挑战，包括任务复杂性不可控、场景有限且需要大量手动标注，以及缺乏多维评估。为了应对这些挑战，我们引入了OmniBench，这是一个自我生成、跨平台、基于图形的基准测试，通过子任务组合，具有合成可控复杂度任务的自动化管道。为了评估虚拟代理在图上的各种能力，我们还推出了OmniEval，这是一个多维评估框架，包括子任务级评估、基于图形的指标和跨越10种能力的综合测试。我们合成的数据集包含20个场景下的3.6万图形结构任务，达到了91%的人类接受率。在我们的图形结构数据上进行训练表明，与手动注释的数据相比，它可以更有效地指导代理。我们对各种开源和闭源模型进行了多维评估，揭示了它们在各种能力方面的表现，为未来的进步铺平了道路。我们的项目可在<a target="_blank" rel="noopener" href="https://omni-bench.github.io/%E8%AE%BF%E9%97%AE%E3%80%82">https://omni-bench.github.io/访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08933v1">PDF</a> Accepted by ICML 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>随着多模态大型语言模型（MLLMs）的发展，基于MLLM的虚拟代理展现出卓越的性能。然而，现有基准测试面临诸多局限性，包括任务复杂性不可控、场景有限且需大量手动标注，以及缺乏多维度评估。为应对这些挑战，我们推出OmniBench，一个自我生成、跨平台、基于图谱的基准测试，通过子任务组合来合成可控复杂度的任务。为评估虚拟代理在图谱上的多元能力，我们还推出OmniEval，一个包含子任务级别评估、基于图谱的指标和全面测试10种能力的多维度评估框架。我们的合成数据集包含20个场景下的3.6万张图谱任务，达到91%的人类接受率。使用我们的图谱数据进行训练表明，与手动注释数据相比，它可以更有效地指导代理。我们对各种开源和闭源模型进行了多维度评估，揭示了它们在各种能力上的表现，为未来发展铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）的虚拟代理表现出卓越性能。</li>
<li>现有基准测试存在任务复杂性不可控、场景有限和手动标注量大等局限性。</li>
<li>OmniBench是一个自我生成、跨平台、基于图谱的基准测试，可合成可控复杂度的任务。</li>
<li>OmniEval是一个多维度评估框架，用于评估虚拟代理在图谱上的多元能力。</li>
<li>合成数据集包含3.6万张图谱任务，人类接受率高达91%。</li>
<li>与手动注释数据相比，使用图谱数据进行训练可以更有效地指导虚拟代理。</li>
<li>多维度评估了各种模型在多种能力上的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08933">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-94fefb1b6723dc45026428f922a131a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ab7505d86fb36096182b108efafa352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f2cf80efd8feb950a53f38fd70daf5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e18f0ce0b4c569fb238440dd97fd3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bd8bf27b29d78336e7dd18a203af15c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35206aa3883c2327bd0e60b86a68b4cd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Improved-LLM-Agents-for-Financial-Document-Question-Answering"><a href="#Improved-LLM-Agents-for-Financial-Document-Question-Answering" class="headerlink" title="Improved LLM Agents for Financial Document Question Answering"></a>Improved LLM Agents for Financial Document Question Answering</h2><p><strong>Authors:Nelvin Tan, Zian Seng, Liang Zhang, Yu-Ching Shih, Dong Yang, Amol Salunkhe</strong></p>
<p>Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent’s performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance. </p>
<blockquote>
<p>大型语言模型（LLM）在自然语言处理任务中表现出了令人印象深刻的能力。然而，LLM在处理包含表格和文本数据的金融文档的数值问答方面仍然面临挑战。近期的研究已经显示了批评代理（即自我校正）在该任务上的有效性，前提是必须有正确的标签。本文在此基础上，研究了当没有正确标签时传统批评代理的有效性，并通过实验表明在这种情况下批评代理的性能会下降。鉴于此，我们提出了一种改进的批评代理和计算器代理，计算器代理的性能超过了之前的最新方法（思维编程），并且更加安全。此外，我们还研究了这两个代理之间的交互方式以及这种交互如何影响它们的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08726v1">PDF</a> 12 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在自然语言处理任务中表现出强大的能力，但在金融文档的数值问答任务中面临挑战，特别是缺乏标准标签的情况。本研究探讨了当没有标准标签时传统批评代理的表现下降的问题，并提出了一种改进的批评代理和计算器代理。该新方法超越了现有的程序思维方法的性能并具有较高的安全性。此外，本研究还探讨了这些代理之间的相互作用及其对性能的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在自然语言处理任务中表现出强大的能力，但在金融文档的数值问答方面存在挑战。</li>
<li>传统批评代理在缺乏标准标签时的性能会下降。</li>
<li>研究提出了一种改进的批评代理和计算器代理，该方法在安全性和性能上超越了现有的程序思维方法。</li>
<li>研究探讨了代理之间的相互作用及其对性能的影响。</li>
<li>改进后的批评代理和计算器代理能有效处理金融文档中的表格和文本数据。</li>
<li>该研究为金融文档的数值问答任务提供了一种新的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08726">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c37e6766b1bcc22e0bd3470f39be91b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-363a8fffc27cbea629aa219d1698d557.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce05c720610589cf08383e82c2203467.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MasHost-Builds-It-All-Autonomous-Multi-Agent-System-Directed-by-Reinforcement-Learning"><a href="#MasHost-Builds-It-All-Autonomous-Multi-Agent-System-Directed-by-Reinforcement-Learning" class="headerlink" title="MasHost Builds It All: Autonomous Multi-Agent System Directed by   Reinforcement Learning"></a>MasHost Builds It All: Autonomous Multi-Agent System Directed by   Reinforcement Learning</h2><p><strong>Authors:Kuo Yang, Xingjie Yang, Linhui Yu, Qing Xu, Yan Fang, Xu Wang, Zhengyang Zhou, Yang Wang</strong></p>
<p>Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently emerged as a powerful paradigm for tackling complex real-world tasks. However, existing Mas construction methods typically rely on manually crafted interaction mechanisms or heuristic rules, introducing human biases and constraining the autonomous ability. Even with recent advances in adaptive Mas construction, existing systems largely remain within the paradigm of semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement Learning (RL)-based framework for autonomous and query-adaptive Mas design. By formulating Mas construction as a graph search problem, our proposed MasHost jointly samples agent roles and their interactions through a unified probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives pursued in prior works, we introduce component rationality as an additional and novel design principle in Mas. To achieve this multi-objective optimization, we propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy that collaboratively integrates group-relative advantages and action-wise rewards. To our knowledge, our proposed MasHost is the first RL-driven framework for autonomous Mas graph construction. Extensive experiments on six benchmarks demonstrate that MasHost consistently outperforms most competitive baselines, validating its effectiveness, efficiency, and structure rationality. </p>
<blockquote>
<p>基于大型语言模型（LLM）的多智能体系统（Mas）最近出现为一种强大的范式，用于处理复杂的现实世界任务。然而，现有的Mas构建方法通常依赖于手动制作的交互机制或启发式规则，这引入了人类偏见并限制了自主性能力。尽管最近在自适应Mas构建方面取得了进展，但现有系统大多仍保持在半自主模式的范式内。在这项工作中，我们提出了MasHost，这是一个基于强化学习（RL）的自主和查询自适应Mas设计框架。通过将Mas构建公式化为图搜索问题，我们提出的MasHost通过统一的概率采样机制联合采样智能体的角色和它们的交互。除了先前工作中追求准确性和效率目标之外，我们引入了组件合理性作为Mas中的附加和新颖的设计原则。为了实现这种多目标优化，我们提出了分层相对策略优化（HRPO），这是一种新型的RL策略，能够协同整合群体相对优势和行动方面的奖励。据我们所知，我们提出的MasHost是第一个用于自主Mas图构建RL驱动的框架。在六个基准测试上的广泛实验表明，MasHost始终优于大多数竞争基线，验证了其有效性、效率和结构合理性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08507v1">PDF</a> </p>
<p><strong>Summary</strong><br>大语言模型驱动的跨主体系统面临人工交互机制和启发式规则的局限性，这带来了人为偏见并限制了自主性。研究团队提出了MasHost框架，使用强化学习技术自主设计和构建多主体系统，并将这个过程公式化为图形搜索问题。新的优化方法将准确性、效率和组件合理性相结合，采用分层相对策略优化方法。实验证明，MasHost在多个基准测试中表现优异，验证了其有效性、效率和结构合理性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大语言模型驱动的多主体系统（Mas）是处理复杂现实世界任务的有力工具。</li>
<li>传统构建方法依赖人为设计的交互机制和启发式规则，存在人为偏见和自主性的限制。</li>
<li>MasHost框架首次使用强化学习技术自主设计和构建多主体系统，解决了上述问题。</li>
<li>MasHost通过图形搜索公式化过程实现自适应的多主体系统设计，使用统一的概率采样机制对主体角色和互动进行采样。</li>
<li>提出组件合理性作为新的设计原则，以实现多目标优化。</li>
<li>采用分层相对策略优化方法（HRPO），整合群体相对优势和行动奖励。</li>
<li>实验证明MasHost在多个基准测试中表现优异，验证其有效性、效率和结构合理性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08507">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-06c723c2e4e13c67625d3ab1492d598a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6970965be6c1c999a66f43b64ceeb17e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CAF-I-A-Collaborative-Multi-Agent-Framework-for-Enhanced-Irony-Detection-with-Large-Language-Models"><a href="#CAF-I-A-Collaborative-Multi-Agent-Framework-for-Enhanced-Irony-Detection-with-Large-Language-Models" class="headerlink" title="CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony   Detection with Large Language Models"></a>CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony   Detection with Large Language Models</h2><p><strong>Authors:Ziqi. Liu, Ziyang. Zhou, Mingxuan. Hu</strong></p>
<p>Large language model (LLM) have become mainstream methods in the field of sarcasm detection. However, existing LLM methods face challenges in irony detection, including: 1. single-perspective limitations, 2. insufficient comprehensive understanding, and 3. lack of interpretability. This paper introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven multi-agent system designed to overcome these issues. CAF-I employs specialized agents for Context, Semantics, and Rhetoric, which perform multidimensional analysis and engage in interactive collaborative optimization. A Decision Agent then consolidates these perspectives, with a Refinement Evaluator Agent providing conditional feedback for optimization. Experiments on benchmark datasets establish CAF-I’s state-of-the-art zero-shot performance. Achieving SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of 76.31, a 4.98 absolute improvement over the strongest prior baseline. This success is attained by its effective simulation of human-like multi-perspective analysis, enhancing detection accuracy and interpretability. </p>
<blockquote>
<p>大型语言模型（LLM）已成为自然语言处理领域中嘲讽检测的主流方法。然而，现有的LLM方法在检测讽刺时面临挑战，包括：1.单一视角的局限性，2.综合理解不足，以及3.缺乏可解释性。本文介绍了用于讽刺的协作代理框架（CAF-I），这是一个以LLM驱动的多代理系统，旨在克服这些问题。CAF-I采用专门针对上下文、语义和修辞的代理，进行多维分析并参与交互式协同优化。然后，决策代理整合这些观点，细化评估代理提供条件反馈以实现优化。在基准数据集上的实验证明了CAF-I最先进的零样本性能。在大多数指标上达到最新水平，CAF-I的平均宏观F1分数为76.31，较之前最强的基线模型有4.98的绝对改进。这一成功是通过其模拟人类多视角分析而实现的，提高了检测精度和可解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08430v1">PDF</a> ICML 2025 Workshop on Collaborative and Federated Agentic Workflows</p>
<p><strong>Summary</strong>：<br>大型语言模型（LLM）在讽刺检测领域已成为主流方法，但在讽刺检测方面仍面临挑战。本文介绍了一种基于LLM的多代理系统——协作代理框架（CAF-I），通过专业化的上下文、语义和修辞代理进行多维分析，并通过决策代理进行视角整合，优化评估代理提供条件反馈以实现优化。在基准数据集上的实验证明了CAF-I的零样本性能达到最新水平，在大多数指标上均表现优异，平均Macro-F1达到76.31，相较于最强基线有4.98的绝对提升。这得益于其模拟人类的多角度分析，提高了检测准确性和可解释性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）在讽刺检测中是主流方法，但仍面临挑战。</li>
<li>现有LLM方法在讽刺检测中的挑战包括单视角限制、缺乏全面理解和缺乏可解释性。</li>
<li>CAF-I是一种基于LLM的多代理系统，通过专业化的上下文、语义和修辞代理进行多维分析。</li>
<li>CAF-I采用决策代理来整合视角，并使用优化评估代理进行条件反馈以实现优化。</li>
<li>实验证明CAF-I的零样本性能达到最新水平，平均Macro-F1达到76.31。</li>
<li>CAF-I相较于最强基线有4.98的绝对提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08430">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fc926ea3fa97bbb6ba9c92242df2aa21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d5cabe0303b8936abeabd0d989a660c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1c983b0a81bf8eb0737c945c92fb447.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f6242151ba12402a61da86da224b16d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1732c2cc7fa1e1988f0bb0003cf07c42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15454d3afb92be76c4201a9b99c89ba9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TACTIC-Translation-Agents-with-Cognitive-Theoretic-Interactive-Collaboration"><a href="#TACTIC-Translation-Agents-with-Cognitive-Theoretic-Interactive-Collaboration" class="headerlink" title="TACTIC: Translation Agents with Cognitive-Theoretic Interactive   Collaboration"></a>TACTIC: Translation Agents with Cognitive-Theoretic Interactive   Collaboration</h2><p><strong>Authors:Weiya Li, Junjie Chen, Bei Li, Boyang Liu, Zichen Wen, Nuanqiao Shan, Xiaoqian Liu, Anping Liu, Huajie Liu, Youyan Wang, Wujiuge Yin, Hu Song, Bing Huang, Zhiyuan Xia, Jialiang Chen, Linfeng Zhang</strong></p>
<p>Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at <a target="_blank" rel="noopener" href="https://github.com/weiyali126/TACTIC">https://github.com/weiyali126/TACTIC</a>. </p>
<blockquote>
<p>机器翻译长期以来都是自然语言处理中的一项核心任务。随着大型语言模型（LLMs）的快速发展，翻译质量取得了显著的进步。然而，完全实现LLMs的翻译潜力仍然是一个开放性的挑战。近期的研究探索了多智能体系统将复杂的翻译任务分解成协同的子任务，初步显示出通过智能体协作和专业化增强翻译质量的希望。然而，现有的多智能体翻译框架很大程度上忽视了来自认知翻译研究的见解。这些见解强调人类译者如何采用不同的认知策略，如平衡直译和意译、根据上下文优化表达、以及迭代评估输出。为了解决这个问题，我们提出了一个受认知启发的多智能体框架，名为TACTIC，代表具有认知理论交互协作的翻译智能体。该框架包括六个功能各异的智能体，反映人类翻译行为中观察到的关键认知过程。这些智能体包括起草、改进、评估、打分、上下文推理和外部知识收集的智能体。通过模拟交互和基于理论的翻译工作流程，TACTIC有效地利用LLMs的全部能力实现高质量翻译。在FLORES-200和WMT24基准测试上的多样化语言对的实验结果表明，我们的方法一直达到最新技术水平。以DeepSeek-V3为基础模型，TACTIC平均超越GPT-4.1，XCOMET提升+0.6，COMETKIWI-23提升+1.18。相较于DeepSeek-R1，进一步在XCOMET上提升+0.84，COMETKIWI-23提升+2.99。代码可用在<a target="_blank" rel="noopener" href="https://github.com/weiyali126/TACTIC%E3%80%82">https://github.com/weiyali126/TACTIC。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08403v1">PDF</a> 20 pages, 4 figures, Under review. Code:   <a target="_blank" rel="noopener" href="https://github.com/weiyali126/TACTIC">https://github.com/weiyali126/TACTIC</a></p>
<p><strong>Summary</strong><br>     机器翻译是自然语言处理中的核心任务之一。随着大型语言模型（LLMs）的快速发展，翻译质量取得了显著进步，但完全实现LLMs的翻译潜力仍是一个开放性的挑战。本文提出一个认知启发下的多智能体翻译框架TACTIC，该框架包括六个功能各异的智能体，模拟人类翻译过程中的关键认知行为。实验结果表明，TACTIC框架在多种语言对上的翻译性能达到最新水平，优于GPT-4.1和DeepSeek-R1。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器翻译是自然语言处理的重要任务，大型语言模型（LLMs）的进步推动了翻译质量的提升。</li>
<li>完全实现LLMs的翻译潜力仍存在挑战，需要探索新的方法和技术。</li>
<li>多智能体系统在翻译任务中的应用展现出提升翻译质量的初步希望。</li>
<li>现有多智能体翻译框架忽视了来自认知翻译研究的见解。</li>
<li>TACTIC框架是一个认知启发下的多智能体翻译框架，包括六个功能各异的智能体，模拟人类翻译过程中的关键认知行为。</li>
<li>TACTIC框架在多种语言对上的翻译性能达到最新水平，优于GPT-4.1和DeepSeek-R1。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08403">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-82e3164b8974baadaca80bc7e7cb4a6f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3bd90291c04e414fe8d4bb72f74b83de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7956cbdcea204e2eb142880833239a03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07c8fd77165b3d84f5497850e2d39a86.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ccb951654ecac392c907719c819465a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reinforce-LLM-Reasoning-through-Multi-Agent-Reflection"><a href="#Reinforce-LLM-Reasoning-through-Multi-Agent-Reflection" class="headerlink" title="Reinforce LLM Reasoning through Multi-Agent Reflection"></a>Reinforce LLM Reasoning through Multi-Agent Reflection</h2><p><strong>Authors:Yurun Yuan, Tengyang Xie</strong></p>
<p>Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization. </p>
<blockquote>
<p>利用更多的测试时间计算已被证明是提高大型语言模型（LLM）推理能力的一种有效方法。在各种方法中，验证和改进范式脱颖而出，因为它能够实现动态解决方案探索和反馈融合。然而，现有方法常常受到反馈空间限制和各方缺乏协调训练的困扰，导致性能不佳。为解决这一问题，我们将这种多轮优化过程建模为马尔可夫决策过程，并引入DPSDP（通过动态规划进行直接策略搜索）一种强化学习算法，该算法训练一个actor-critic LLM系统，通过直接在自我生成的数据上进行偏好学习来迭代优化答案。理论上，DPSDP可以在训练分布内匹配任何策略的性能。实际上，我们用各种基础模型实例化了DPSDP，并展示了在内部和外部基准测试中的改进。例如，在MATH 500基准测试中，经过五步优化的多数投票结果将初答准确率从58.2%提高到了63.2%，使用Ministral作为基础模型。一项消融研究进一步证实了多智能体协作和跨分布泛化的好处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08379v1">PDF</a> International Conference on Machine Learning (ICML), 2025</p>
<p><strong>Summary</strong></p>
<p>本摘要以简洁的方式描述了如何利用更多的测试时间计算来提升大型语言模型的推理能力。其中，验证和改进范式特别突出，它可实现动态解决方案探索和反馈融合。针对现有方法存在的反馈空间受限和不同参与方缺乏协同训练导致性能不佳的问题，本研究采用马尔可夫决策过程模拟多轮细化过程，并引入DPSDP（一种基于动态规划的直接政策搜索强化学习算法），训练演员评论家LLM系统通过直接偏好学习在自我生成的数据上迭代优化答案。理论上，DPSDP可在训练分布内匹配任何策略的性能。实证研究证明，在各种基准测试中，DPSDP均有显著改善，如MATH 500基准测试中，经过五步优化后的多数投票结果首次准确率从58.2%提升至63.2%。同时，通过消融研究进一步验证了多智能体协作和跨分布泛化的优势。这是一项跨语言和实际应用前景广泛的研究成果。随着相关研究和应用的进一步发展，这一领域将会有更多的突破和进展。简言之，本文提出一种利用强化学习算法提升大型语言模型推理能力的新方法，具有较强的理论与实践价值。总体来说对实际部署具有很高的参考意义和研究价值。总之在实际应用中还需验证与完善效果具体的情况可能需要后续进一步研究讨论证实确认推广应用的可行性和具体实施方案并且不断探索解决存在问题的能力为未来的发展提供更多的创新想法以及实际应用场景等。目前该研究仍具有广阔的应用前景和潜在价值值得进一步深入研究和探索。文中提出了一种新的强化学习算法来提升大型语言模型的推理能力在未来该技术能够落地应用中将推动自然语言处理领域的进一步发展对于推进相关领域技术的发展与应用具有重要意义促进多智能体协作的进一步发展具有重要的理论和实践价值并可能产生深远的社会影响和经济价值推动产业的升级和转型提升国家竞争力推动经济发展和社会进步具有广泛的应用前景和发展潜力。文中提出的算法具有广阔的应用前景和潜在价值未来有望广泛应用于自然语言处理领域的相关场景如智能客服问答系统对话生成机器人等领域在实际应用中具有非常广阔的推广前景。未来可以进一步探索将该方法应用于更多领域场景以及优化算法性能等方面。该算法为自然语言处理领域带来创新突破有望成为未来研究的重要方向之一对于该算法在实际应用中的性能和表现有广泛期待并引起行业的关注与研究价值表明它在人工智能自然语言处理等领域的重要贡献同时意味着人类在应用人工智能技术的道路上又迈出了重要的一步。该算法将极大地推动人工智能技术的发展和应用为人们的生活带来便利并产生深远的影响和经济效益为社会进步贡献力量为人类社会的科技进步注入新的活力。<strong>Key Takeaways</strong>：</p>
<ul>
<li>利用更多的测试时间计算可有效提升大型语言模型的推理能力。</li>
<li>验证和改进范式可实现动态解决方案探索和反馈融合。</li>
<li>DPSDP算法通过直接偏好学习在自我生成的数据上训练LLM系统以迭代优化答案。</li>
<li>DPSDP算法能在理论上限匹配任何策略的性能，并在多个基准测试中实现显著改进。</li>
<li>消融研究证明了多智能体协作和跨分布泛化的优势。</li>
<li>DPSDP算法具有广泛的应用前景，可应用于自然语言处理领域的多个场景，如智能客服、问答系统和对话生成机器人等。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08379">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49c1760f8ba05cc11e039d522919f330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce6bbf75492be31ef7d94c58ed244be7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32dfa7be3e336f2faeb3b97b9c04df04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc111418c7306a020bf94a1b1ac69189.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HiBerNAC-Hierarchical-Brain-emulated-Robotic-Neural-Agent-Collective-for-Disentangling-Complex-Manipulation"><a href="#HiBerNAC-Hierarchical-Brain-emulated-Robotic-Neural-Agent-Collective-for-Disentangling-Complex-Manipulation" class="headerlink" title="HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective   for Disentangling Complex Manipulation"></a>HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective   for Disentangling Complex Manipulation</h2><p><strong>Authors:Hongjun Wu, Heng Zhang, Pengsong Zhang, Jin Wang, Cong Wang</strong></p>
<p>Recent advances in multimodal vision-language-action (VLA) models have revolutionized traditional robot learning, enabling systems to interpret vision, language, and action in unified frameworks for complex task planning. However, mastering complex manipulation tasks remains an open challenge, constrained by limitations in persistent contextual memory, multi-agent coordination under uncertainty, and dynamic long-horizon planning across variable sequences. To address this challenge, we propose \textbf{HiBerNAC}, a \textbf{Hi}erarchical \textbf{B}rain-\textbf{e}mulated \textbf{r}obotic \textbf{N}eural \textbf{A}gent \textbf{C}ollective, inspired by breakthroughs in neuroscience, particularly in neural circuit mechanisms and hierarchical decision-making. Our framework combines: (1) multimodal VLA planning and reasoning with (2) neuro-inspired reflection and multi-agent mechanisms, specifically designed for complex robotic manipulation tasks. By leveraging neuro-inspired functional modules with decentralized multi-agent collaboration, our approach enables robust and enhanced real-time execution of complex manipulation tasks. In addition, the agentic system exhibits scalable collective intelligence via dynamic agent specialization, adapting its coordination strategy to variable task horizons and complexity. Through extensive experiments on complex manipulation tasks compared with state-of-the-art VLA models, we demonstrate that \textbf{HiBerNAC} reduces average long-horizon task completion time by 23%, and achieves non-zero success rates (12\textendash 31%) on multi-path tasks where prior state-of-the-art VLA models consistently fail. These results provide indicative evidence for bridging biological cognition and robotic learning mechanisms. </p>
<blockquote>
<p>近期多模态视觉语言动作（VLA）模型的进步已经彻底改变了传统机器人学习的方式，使系统在统一的框架下解释视觉、语言和行为，从而进行复杂的任务规划。然而，掌握复杂的操作任务仍然是一个开放性的挑战，受到持久上下文记忆限制、不确定性下的多智能体协调和可变序列中的动态长期规划限制的影响。为了应对这一挑战，我们提出了\textbf{HiBerNAC}，这是一个受神经科学突破启发的分层脑模拟机器人神经网络集体（\textbf{Hi}erarchical \textbf{B}rain-\textbf{e}mulated \textbf{r}obotic \textbf{N}eural \textbf{A}gent \textbf{C}ollective）。我们的框架结合了（1）多模态VLA规划和推理与（2）神经启发反思和多智能体机制，专为复杂的机器人操作任务设计。通过利用神经启发的功能模块和分散的多智能体协作，我们的方法能够稳健地增强复杂操作任务的实时执行。此外，智能系统通过动态智能体专业化展现出可扩展的集体智能，使其协调策略适应可变的任务视野和复杂性。通过与最新的VLA模型在复杂的操作任务上进行广泛的实验比较，我们证明\textbf{HiBerNAC}平均长期任务完成时间减少了23%，并且在多路径任务上实现了非零成功率（12%\textendash 31%），而先前的最新VLA模型则一直失败。这些结果为连接生物认知和机器人学习机制提供了指示性证据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08296v1">PDF</a> 31 pages,5 figures</p>
<p><strong>摘要</strong></p>
<p>最新多模态视语言动作（VLA）模型的进展已对传统机器人学习带来了革命性变革，使系统能够在统一框架中解释视觉、语言和动作，用于复杂任务规划。然而，掌握复杂操作任务仍是一项开放性的挑战，受限于持久性上下文记忆、不确定性下的多智能体协调和可变序列的动态长期规划等方面的局限。为解决此挑战，我们提出了HiBerNAC，一个受神经科学突破启发的分层脑模拟机器人神经网络集体（Hierarchical Brain-emulated robotic Neural Agent Collective）。该框架结合了多模态VLA规划和推理与神经启发反思和多智能体机制，专为复杂机器人操作任务设计。通过利用神经启发功能模块与分布式多智能体协作，我们的方法可实现稳健且增强的实时执行复杂操作任务。此外，该智能体系统通过动态智能体专业化展现可扩展的集体智能，使协调策略适应可变的任务范围和复杂性。与最新的VLA模型相比，我们在复杂的操作任务上进行了广泛的实验，证明HiBerNAC能够减少平均长期任务完成时间23%，并在多路径任务上实现非零成功率（12%~31%），而先前的最新VLA模型则一直未能成功。这些结果提供了将生物认知与机器人学习机制相结合的指示性证据。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>多模态视语言动作（VLA）模型的最新进展已经推动了机器人学习领域的变革，使机器人能够在统一框架内解释视觉、语言和动作。</li>
<li>复杂操作任务对机器人学习构成挑战，主要包括持久上下文记忆、多智能体协调和动态长期规划的问题。</li>
<li>HiBerNAC框架受神经科学启发，结合多模态VLA规划和推理与神经启发功能模块。</li>
<li>通过神经启发功能模块和分布式多智能体协作，HiBerNAC能够稳健地执行复杂操作任务，并增强实时性能。</li>
<li>HiBerNAC展现出通过动态智能体专业化实现的集体智能的可扩展性，适应可变的任务范围和复杂性。</li>
<li>实验证明，与最新VLA模型相比，HiBerNAC在复杂操作任务上的表现有所超越，减少了任务完成时间，并在多路径任务上实现了非零成功率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08296">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c44de91f17fe7b0455a3924f636c5dfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b50acf4dc0bca60d09e587feddb34082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-406e97c14d05c1cfc6259414ad91483e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3991cfbd9fa2aef9ad238d6a3b5fff2e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Thinking-vs-Doing-Agents-that-Reason-by-Scaling-Test-Time-Interaction"><a href="#Thinking-vs-Doing-Agents-that-Reason-by-Scaling-Test-Time-Interaction" class="headerlink" title="Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction"></a>Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction</h2><p><strong>Authors:Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, Aviral Kumar</strong></p>
<p>The current paradigm of test-time scaling relies on generating long reasoning traces (“thinking” more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent’s interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents. </p>
<blockquote>
<p>当前测试时间缩放的范式依赖于在生成响应之前产生较长的推理轨迹（即“思考”更多）。在需要交互的代理问题中，这可以通过在世界中采取行动之前生成思维轨迹来完成。然而，这个过程不允许代理从环境中获取新信息，也不能随着时间的推移改变它们的行为。在这项工作中，我们提出了测试时间交互的扩展，这是测试时间缩放的一个未被开发的维度，它增加了代理的交互范围，使代理能够在单个运行中执行丰富的行为，如探索、回溯和动态重新规划。为了证明这一扩展维度的潜力，我们研究了网页代理领域。我们首先表明，即使在没有任何训练的情况下，基于提示的交互扩展也可以在一定程度上提高网页基准测试的任务成功率。在此基础上，我们引入了测试时间交互（TTI），这是一种基于课程的在线强化学习（RL）方法，通过自适应调整滚动长度来训练代理。使用Gemma 3 12B模型，TTI在WebVoyager和WebArena基准测试中产生了最先进的开源开放数据网页代理。我们还进一步展示了TTI使代理能够自适应地平衡探索和利用。我们的结果确立了交互缩放作为一个强大的、与每步计算缩放相辅相成的维度，为训练自适应代理提供了新的途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07976v2">PDF</a> Fixed typo in Figure 6 and Conclusion</p>
<p><strong>Summary</strong></p>
<p>本文提出测试时交互扩展的概念，旨在提高智能体在环境中的交互能力，使其能够在单次运行中执行丰富的行为，如探索、回溯和动态规划。该研究通过在无需额外训练的情况下采用提示为基础的交互扩展方式，实现了在非基准测试上的任务成功率的提升。此外，该研究还引入了一种基于在线强化学习的测试时交互（TTI）方法，通过自适应调整智能体的运行时长进行训练。TTI使用Gemma 3 12B模型在WebVoyager和WebArena基准测试中产生了先进的开源智能体。结果表明，测试时交互扩展是一种强大的、与每步计算扩展相辅相成的补充轴，为训练自适应智能体提供了新的途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前测试时间扩展的范式依赖于生成长的推理轨迹再做出回应，但在需要交互的智能体问题中，这种方法无法让智能体从环境中获取新信息或随时间改变行为。</li>
<li>测试时交互扩展是提高智能体在环境中的交互能力的一种新方法，能使其执行探索、回溯和动态规划等丰富行为。</li>
<li>采用提示为基础的交互扩展方式，能在不额外训练的情况下提升任务成功率。</li>
<li>引入了一种基于在线强化学习的测试时交互（TTI）方法，通过自适应调整智能体的运行时长进行训练。</li>
<li>TTI使用Gemma 3 12B模型在WebVoyager和WebArena基准测试中表现出色。</li>
<li>测试时交互扩展是一种强大的补充轴，与每步计算扩展相辅相成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07976">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6b20bf1b7b3980f2384e8a93eac899de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c5a90370d605ca6f1cbe84755e6eb36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3895e122cd82008845075175bef98d46.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SAFEFLOW-A-Principled-Protocol-for-Trustworthy-and-Transactional-Autonomous-Agent-Systems"><a href="#SAFEFLOW-A-Principled-Protocol-for-Trustworthy-and-Transactional-Autonomous-Agent-Systems" class="headerlink" title="SAFEFLOW: A Principled Protocol for Trustworthy and Transactional   Autonomous Agent Systems"></a>SAFEFLOW: A Principled Protocol for Trustworthy and Transactional   Autonomous Agent Systems</h2><p><strong>Authors:Peiran Li, Xinkai Zou, Zhuohang Wu, Ruifeng Li, Shuo Xing, Hanwen Zheng, Zhikai Hu, Yuping Wang, Haoxi Li, Qin Yuan, Yingmo Zhang, Zhengzhong Tu</strong></p>
<p>Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today’s agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM&#x2F;VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy. </p>
<blockquote>
<p>最近的大型语言模型（LLM）和视觉语言模型（VLM）的进步使得能够执行复杂推理和多模式工具使用的强大自主代理成为可能。尽管它们的能力日益增强，但当前的代理框架仍然脆弱，缺乏安全信息流、可靠性和多代理协调的原则性机制。在这项工作中，我们引入了SAFEFLOW，这是一个用于构建可信的LLM&#x2F;VLM基于代理的新协议级框架。SAFEFLOW强制实施精细的信息流控制（IFC），精确跟踪代理、工具、用户和环境之间交换的所有数据的来源、完整性和机密性。通过限制LLM推理以尊重这些安全标签，SAFEFLOW防止不受信任或对敌输入污染高完整性决策。为了确保在并发多代理环境中的稳健性，SAFEFLOW引入了事务执行、冲突解决和共享状态的安全调度，以保留全局一致性。我们还引入了包括预写日志、回滚和安全缓存等机制，进一步增强对运行时错误和政策违规的抵御能力。为了验证性能，我们构建了SAFEFLOWBENCH，这是一套综合基准测试，旨在评估代理在敌对、嘈杂和并发操作条件下的可靠性。大量实验表明，使用SAFEFLOW构建的代理即使在恶劣环境中也能保持令人印象深刻的任务性能和安全保证，显著优于最新技术。SAFEFLOW和SAFEFLOWBENCH一起奠定了原则性、稳健性和安全代理生态系统的基石，推动了可靠自主性的前沿发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07564v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）和视觉语言模型（VLM）的最新进展已经催生了能够进行复杂推理和多模态工具使用的强大自主代理。然而，当前的代理框架仍然脆弱，缺乏安全信息流、可靠性和多代理协调的机制。为此，我们引入了SAFEFLOW，一个为构建可信LLM&#x2F;VLM代理的新协议级框架。SAFEFLOW强制执行精细的信息流控制（IFC），精确跟踪代理、工具、用户和环境之间交换的所有数据的来源、完整性和机密性。通过约束LLM推理以尊重这些安全标签，SAFEFLOW防止不受信任或对抗性输入污染高完整性的决策。为确保并发多代理设置中的稳健性，SAFEFLOW引入了事务执行、冲突解决和共享状态的安全调度，以保留全局一致性。我们进一步引入了包括写前日志、回滚和安全缓存等机制，以增强对运行时错误和政策违规的抵御能力。为了验证性能，我们构建了SAFEFLOWBENCH，一个综合基准测试套件，旨在评估代理在敌对、嘈杂和并发操作条件下的可靠性。实验表明，使用SAFEFLOW构建的代理即使在恶劣环境中也能保持令人印象深刻的任务性能和安全性保证，显著优于现有技术。SAFEFLOW和SAFEFLOWBENCH共同为原理化、稳健和安全的代理生态系统奠定了基础，推动了可靠自主性前沿的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs和VLMs的最新进展使得复杂推理和多模态工具使用的强大自主代理成为可能。</li>
<li>当前代理框架缺乏安全信息流、可靠性和多代理协调的机制。</li>
<li>SAFEFLOW是一个新的协议级框架，为构建可信LLM&#x2F;VLM代理提供方案。</li>
<li>SAFEFLOW通过精细的信息流控制（IFC）确保数据安全性和完整性。</li>
<li>SAFEFLOW防止不受信任或对抗性输入污染决策，并通过事务执行、冲突解决和安全调度确保并发环境中的稳健性。</li>
<li>SAFEFLOW引入了多种机制以增强对运行时错误和政策违规的抵御能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07564">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e775015c1c694a53a24ac85d591fe5a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8f1e40c16a973bee6c3946ea54b22b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d876d22601e61106212ce0886f8a1d6e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CrimeMind-Simulating-Urban-Crime-with-Multi-Modal-LLM-Agents"><a href="#CrimeMind-Simulating-Urban-Crime-with-Multi-Modal-LLM-Agents" class="headerlink" title="CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents"></a>CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents</h2><p><strong>Authors:Qingbin Zeng, Ruotong Zhao, Jinzhu Mao, Haoyang Li, Fengli Xu, Yong Li</strong></p>
<p>Modeling urban crime is an important yet challenging task that requires understanding the subtle visual, social, and cultural cues embedded in urban environments. Previous work has mainly focused on rule-based agent-based modeling (ABM) and deep learning methods. ABMs offer interpretability of internal mechanisms but exhibit limited predictive accuracy. In contrast, deep learning methods are often effective in prediction but are less interpretable and require extensive training data. Moreover, both lines of work lack the cognitive flexibility to adapt to changing environments. Leveraging the capabilities of large language models (LLMs), we propose CrimeMind, a novel LLM-driven ABM framework for simulating urban crime within a multi-modal urban context. A key innovation of our design is the integration of the Routine Activity Theory (RAT) into the agentic workflow of CrimeMind, enabling it to process rich multi-modal urban features and reason about criminal behavior. However, RAT requires LLM agents to infer subtle cues in evaluating environmental safety as part of assessing guardianship, which can be challenging for LLMs. To address this, we collect a small-scale human-annotated dataset and align CrimeMind’s perception with human judgment via a training-free textual gradient method. Experiments across four major U.S. cities demonstrate that CrimeMind outperforms both traditional ABMs and deep learning baselines in crime hotspot prediction and spatial distribution accuracy, achieving up to a 24% improvement over the strongest baseline. Furthermore, we conduct counterfactual simulations of external incidents and policy interventions and it successfully captures the expected changes in crime patterns, demonstrating its ability to reflect counterfactual scenarios. Overall, CrimeMind enables fine-grained modeling of individual behaviors and facilitates evaluation of real-world interventions. </p>
<blockquote>
<p>建模城市犯罪是一项重要且具有挑战性的任务，需要理解城市环境中微妙的视觉、社会和文化线索。以往的研究主要集中在基于规则的主体建模（ABM）和深度学习方法上。ABM提供了内部机制的解释性，但预测精度有限。相比之下，深度学习方法在预测方面通常很有效，但解释性较差，且需要大量训练数据。此外，这两种方法都缺乏适应环境变化的认知灵活性。我们利用大型语言模型（LLM）的能力，提出了CrimeMind，这是一种新型的LLM驱动ABM框架，可在多模式城市背景下模拟城市犯罪。设计中的一个关键创新是将日常活动理论（RAT）集成到CrimeMind的主体工作流程中，使其能够处理丰富的多模式城市特征并对犯罪行为进行推理。然而，RAT需要LLM主体在评估环境安全性时推断微妙的线索，作为评估监护权的一部分，这对于LLM来说可能具有挑战性。为了解决这一问题，我们收集了一个小规模的人工注释数据集，并通过一种无需训练的文本梯度方法与CrimeMind的感知与人类判断对齐。在四个美国主要城市的实验表明，在犯罪热点预测和空间分布准确性方面，CrimeMind优于传统的ABM和深度学习基线，比最强基线提高了高达24%。此外，我们进行了外部事件和政策干预的反事实模拟，成功捕捉了犯罪模式的预期变化，证明了其反映反事实场景的能力。总体而言，CrimeMind能够实现个体行为的精细建模，并便于评估现实世界的干预措施。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05981v2">PDF</a> Typos corrected</p>
<p><strong>摘要</strong></p>
<p>利用大型语言模型（LLM）驱动的活动理论（RAT）和基于代理的建模（ABM）框架，提出了CrimeMind模型，该模型能够在多模态城市环境中模拟城市犯罪。通过整合RAT，CrimeMind能够处理丰富的多模态城市特征，并对犯罪行为进行推理。为解决LLM在评估环境安全性方面的挑战，采用无训练文本梯度方法与人类判断对齐。实验表明，CrimeMind在犯罪热点预测和空间分布准确性方面优于传统ABM和深度学习基线，改进率最高达24%。此外，还能进行外部事件和政策干预的模拟，成功捕捉犯罪模式的预期变化。总体而言，CrimeMind可实现精细的个体行为建模，并评估现实干预措施。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>城市犯罪建模是一项重要而具有挑战性的任务，需要理解城市环境中的微妙视觉、社会和文化线索。</li>
<li>现有方法主要集中于基于规则的代理建模（ABM）和深度学习，但各有局限：ABM预测精度有限，深度学习模型解释性较差且需要大量训练数据。</li>
<li>提出了CrimeMind模型，结合大型语言模型（LLM）和多模态城市环境模拟城市犯罪。</li>
<li>CrimeMind整合了活动理论（RAT），使模型能够处理丰富的多模态城市特征并推理犯罪行为。</li>
<li>为解决LLM在评估环境安全性方面的挑战，采用无训练文本梯度方法对齐人类判断。</li>
<li>实验表明，CrimeMind在犯罪热点预测和空间分布准确性方面优于传统方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05981">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-04f3749f28d181c2885842d811a1698e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41b204107070ede81fcf8712700dabbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-079f9ff7ac23494a67422f6964dd8b07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d76ab335e100f3a6dec1ac45ca47211.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TextAtari-100K-Frames-Game-Playing-with-Language-Agents"><a href="#TextAtari-100K-Frames-Game-Playing-with-Language-Agents" class="headerlink" title="TextAtari: 100K Frames Game Playing with Language Agents"></a>TextAtari: 100K Frames Game Playing with Language Agents</h2><p><strong>Authors:Wenhao Li, Wenwu Li, Chuyun Shen, Junjie Sheng, Zixiao Huang, Di Wu, Yun Hua, Wei Yin, Xiangfeng Wang, Hongyuan Zha, Bo Jin</strong></p>
<p>We present TextAtari, a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps. By translating the visual state representations of classic Atari games into rich textual descriptions, TextAtari creates a challenging test bed that bridges sequential decision-making with natural language processing. The benchmark includes nearly 100 distinct tasks with varying complexity, action spaces, and planning horizons, all rendered as text through an unsupervised representation learning framework (AtariARI). We evaluate three open-source large language models (Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks (zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how different forms of prior knowledge affect performance on these long-horizon challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and Reference-based-investigate the impact of semantic understanding, instruction comprehension, and expert demonstrations on agent decision-making. Our results reveal significant performance gaps between language agents and human players in extensive planning tasks, highlighting challenges in sequential reasoning, state tracking, and strategic planning across tens of thousands of steps. TextAtari provides standardized evaluation protocols, baseline implementations, and a framework for advancing research at the intersection of language models and planning. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Lww007/Text-Atari-Agents">https://github.com/Lww007/Text-Atari-Agents</a>. </p>
<blockquote>
<p>我们推出了TextAtari，这是一个用于评估语言智能体在长达10万步的远期决策任务上的表现的基准测试。通过将经典的Atari游戏的视觉状态表示转化为丰富的文本描述，TextAtari创建了一个具有挑战性的测试平台，该平台将序列决策与自然语言处理相结合。该基准测试包含近100个不同任务，具有不同的复杂度、行动空间和计划期限，所有内容均通过无监督表示学习框架（AtariARI）以文本形式呈现。我们评估了三个开源大型语言模型（Qwen2.5-7B、Gemma-7B和Llama3.1-8B）在三种智能体框架（零样本、少量链式思维和反思推理）下的表现，以了解不同形式的先验知识如何影响这些长期决策挑战的性能。四种场景——基础、遮蔽、手动增强和参考基础——探讨了语义理解、指令理解和专家示范对智能体决策的影响。我们的研究结果显示，在语言智能体和人类玩家在复杂规划任务之间存在显著的性能差距，这凸显了数万步的序列推理、状态跟踪和战略规划中的挑战。TextAtari提供了标准化的评估协议、基准实施方案和一个框架，以促进语言模型和规划交叉领域的研究进展。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Lww007/Text-Atari-Agents%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Lww007/Text-Atari-Agents上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04098v2">PDF</a> 51 pages, 39 figures</p>
<p><strong>Summary</strong></p>
<p>文本介绍了一个名为TextAtari的基准测试平台，该平台旨在评估语言模型在长达数十万步的长周期决策任务中的表现。通过将经典的Atari游戏的视觉状态表示转化为丰富的文本描述，TextAtari为连接序列决策与自然语言处理搭建了一个挑战性的测试平台。该平台包含近100个不同任务，涵盖不同的复杂度、动作空间和规划周期，所有任务均以文本形式呈现。文章评估了三种开源大型语言模型在不同代理框架下的表现，并探讨了不同先验知识对这些长周期挑战的影响。TextAtari提供了标准化的评估协议、基准实现和一个推动语言模型和规划交叉研究的框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TextAtari是一个评估语言模型的基准测试平台，专注于长周期决策任务，任务跨度长达10万步。</li>
<li>平台通过翻译Atari游戏的视觉状态表示成文本描述，搭建了一个连接序列决策与NLP的桥梁。</li>
<li>TextAtari包含近100个不同任务，涵盖不同复杂度、动作空间和规划周期。</li>
<li>评估了三种大型语言模型在多种代理框架下的表现。</li>
<li>探讨了不同形式的先验知识对长周期挑战任务性能的影响。</li>
<li>通过四种场景研究，探讨了语义理解、指令理解和专家示范对代理决策制定的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04098">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1eb9926189e9a7a3b31e0e0d1d8e6cbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d4ba4a8e070f6724fdbe7118858581a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1834b5297bab66416db4eb00e259b4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7ef136f2f50968c5fbf6f7587d4256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47047f50c44cb37898ed12da40830360.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DefenderBench-A-Toolkit-for-Evaluating-Language-Agents-in-Cybersecurity-Environments"><a href="#DefenderBench-A-Toolkit-for-Evaluating-Language-Agents-in-Cybersecurity-Environments" class="headerlink" title="DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments"></a>DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments</h2><p><strong>Authors:Chiyu Zhang, Marc-Alexandre Cote, Michael Albada, Anush Sankaran, Jack W. Stokes, Tong Wang, Amir Abdi, William Blum, Muhammad Abdul-Mageed</strong></p>
<p>Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBench’s modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench">https://github.com/microsoft/DefenderBench</a>. </p>
<blockquote>
<p>大型语言模型（LLM）代理在人类语言理解和推理方面表现出了令人印象深刻的能力，然而它们在网络安全方面的潜力仍未被充分探索。我们推出了DefenderBench，这是一个实用、开源的工具包，用于评估语言代理在进攻、防御和网络安全知识任务方面的性能。DefenderBench包括网络入侵、恶意内容检测、代码漏洞分析和网络安全知识评估等环境。它特意为研究者设计，经济实惠、易于访问，同时提供公平严格的评估。我们使用标准化的代理框架，对若干最新技术和流行的大型语言模型进行了基准测试，包括开放和封闭权重模型。我们的结果表明，Claude-3.7-sonnet表现最佳，DefenderBench得分为81.65，其次是Claude-3.7-sonnet-think，得分为78.40，而最好的开放权重模型Llama 3.3 70B紧随其后，DefenderBench得分为71.81。DefenderBench的模块化设计允许无缝集成自定义的大型语言模型和任务，促进了可重复性和公平比较。DefenderBench的匿名版本可在<a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/microsoft/DefenderBench上获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00739v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在理解和推理人类语言方面表现出强大的能力，但在网络安全领域的应用潜力尚未得到充分探索。本文介绍了DefenderBench，这是一个用于评估语言模型在攻击、防御和网络安全知识任务上的实用开源工具包。它对研究者具有经济实惠、易于访问的特点，并能提供公平和严格的评估。我们对几款最先进和流行的LLM进行了基准测试，结果表明Claude-3.7-sonnet表现最佳，DefenderBench得分为81.65。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在网络安全领域的应用潜力尚未充分探索。</li>
<li>DefenderBench是一个用于评估语言模型在网络安全方面的实用开源工具包。</li>
<li>DefenderBench包括网络入侵、恶意内容检测、代码漏洞分析和网络安全知识评估等环境。</li>
<li>DefenderBench具有经济实惠、易于访问的特点，为研究者提供公平和严格的评估。</li>
<li>在基准测试中，Claude-3.7-sonnet表现最佳，DefenderBench得分为81.65。</li>
<li>DefenderBench的模块化设计允许无缝集成自定义的LLM和任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00739">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a6bc86fc70229af47be50d4b82aba1e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bab7f4d11665b5e3feed5f17ce028c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ce477798b819c94cd5e4fe2f624ddee.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AnnaAgent-Dynamic-Evolution-Agent-System-with-Multi-Session-Memory-for-Realistic-Seeker-Simulation"><a href="#AnnaAgent-Dynamic-Evolution-Agent-System-with-Multi-Session-Memory-for-Realistic-Seeker-Simulation" class="headerlink" title="AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for   Realistic Seeker Simulation"></a>AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for   Realistic Seeker Simulation</h2><p><strong>Authors:Ming Wang, Peidong Wang, Lin Wu, Xiaocui Yang, Daling Wang, Shi Feng, Yuxin Chen, Bixuan Wang, Yifei Zhang</strong></p>
<p>Constrained by the cost and ethical concerns of involving real seekers in AI-driven mental health, researchers develop LLM-based conversational agents (CAs) with tailored configurations, such as profiles, symptoms, and scenarios, to simulate seekers. While these efforts advance AI in mental health, achieving more realistic seeker simulation remains hindered by two key challenges: dynamic evolution and multi-session memory. Seekers’ mental states often fluctuate during counseling, which typically spans multiple sessions. To address this, we propose AnnaAgent, an emotional and cognitive dynamic agent system equipped with tertiary memory. AnnaAgent incorporates an emotion modulator and a complaint elicitor trained on real counseling dialogues, enabling dynamic control of the simulator’s configurations. Additionally, its tertiary memory mechanism effectively integrates short-term and long-term memory across sessions. Evaluation results, both automated and manual, demonstrate that AnnaAgent achieves more realistic seeker simulation in psychological counseling compared to existing baselines. The ethically reviewed and screened code can be found on <a target="_blank" rel="noopener" href="https://github.com/sci-m-wang/AnnaAgent">https://github.com/sci-m-wang/AnnaAgent</a>. </p>
<blockquote>
<p>受限于人工智能心理健康领域涉及真实求助者的成本和伦理问题，研究人员开发了基于大型语言模型（LLM）的对话代理（CA），并定制了配置，如个人简介、症状和场景，以模拟求助者。尽管这些努力推动了人工智能在心理健康领域的发展，但由于两个关键挑战——动态演变和多会话记忆，实现更真实的求助者模拟仍然受到限制。在咨询过程中，求助者的心理状态往往会波动，这一过程通常跨越多个会话。针对这一问题，我们提出了AnnaAgent，一个配备三级记忆的情感和认知动态代理系统。AnnaAgent融入了一个情感调节器和投诉诱发器，经过真实咨询对话的训练，能够实现模拟器配置的动态控制。此外，其三级记忆机制有效地整合了跨会话的短期和长期记忆。自动化和手动评估结果均表明，相较于现有基线，AnnaAgent在心理咨询中实现了更真实的求助者模拟。经过伦理审查和筛选的代码可在<a target="_blank" rel="noopener" href="https://github.com/sci-m-wang/AnnaAgent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sci-m-wang/AnnaAgent找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00551v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于AI驱动的心理健康领域中真实寻求者参与的成本和伦理问题，研究者开发了基于大型语言模型（LLM）的对话代理（CA），并定制配置以模拟寻求者。尽管有所进展，但实现更真实的寻求者模拟仍面临两大挑战：动态演变和多会话记忆。为此，研究者提出了AnnaAgent系统，一个配备三级记忆的情感和认知动态代理系统。AnnaAgent结合了基于真实咨询对话训练的情感调节器和投诉激发器，实现了模拟器配置的动态控制。其三级记忆机制有效地整合了跨会话的短期和长期记忆。评估结果表明，相较于现有基线，AnnaAgent在心理咨询中的寻求者模拟更为真实。相关伦理审查和筛选的代码可在此处找到：<a target="_blank" rel="noopener" href="https://github.com/sci-m-wang/AnnaAgent">https://github.com/sci-m-wang/AnnaAgent</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究者使用大型语言模型（LLM）开发对话代理（CA）以模拟心理健康领域的寻求者。</li>
<li>实现更真实的寻求者模拟面临两大挑战：动态演变和多会话记忆。</li>
<li>AnnaAgent系统通过配备三级记忆机制解决了这两大挑战。</li>
<li>AnnaAgent结合了情感调节器和投诉激发器，实现模拟器配置的动态控制。</li>
<li>AnnaAgent的模拟效果经过自动化和手动评估，相较于现有基线更为真实。</li>
<li>AnnaAgent系统经过伦理审查和筛选，相关代码可公开访问。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00551">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-978d53947412af624b4f6a5adccc0666.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9590124cce9c5ebd348dd3b62219cfb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eca133825acf6b1e28b46158337b23b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d23e5c575d20de0de5f3f08ea92f27e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Understanding-Bias-Reinforcement-in-LLM-Agents-Debate"><a href="#Understanding-Bias-Reinforcement-in-LLM-Agents-Debate" class="headerlink" title="Understanding Bias Reinforcement in LLM Agents Debate"></a>Understanding Bias Reinforcement in LLM Agents Debate</h2><p><strong>Authors:Jihwan Oh, Minchan Jeong, Jongwoo Ko, Se-Young Yun</strong></p>
<p>Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD’s limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse $\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM’s strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making. </p>
<blockquote>
<p>大型语言模型（LLMs）通过无需训练的方法，如提示工程和上下文学习来解决复杂问题，但确保推理的正确性仍然具有挑战性。虽然自我校正方法（如自我一致性和自我改进）旨在提高可靠性，但由于缺乏有效的反馈机制，它们往往会强化偏见。多智能体辩论（MAD）作为一种替代方法已经出现，但我们发现了两个关键局限：偏见强化，即辩论放大模型偏见而不是纠正它们；以及缺乏观点多样性，因为所有智能体都使用相同的模型和推理模式，限制了真正辩论的有效性。为了系统地评估这些问题，我们引入了<em>MetaNIM Arena</em>，这是一个基准测试，旨在评估LLMs在对抗性战略决策中的能力，动态交互会影响最优决策。为了克服MAD的局限性，我们提出了DReaMAD（通过修改提示促进多智能体辩论中的多样化推理），一个新型框架，它（1）精炼LLM的战略先验知识以提高推理质量，（2）通过系统地修改提示促进单一模型内的不同观点，从而减少偏见。实证结果表明，DReaMAD在多个战略任务中显著提高了决策准确性、推理多样性和偏见缓解，确立其为LLM决策制定中更有效的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16814v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）通过无需训练的方法如提示工程和自然语境学习来解决复杂问题，但确保推理的正确性仍然具有挑战性。尽管自我校正方法如自我一致性和自我改进旨在提高可靠性，但它们往往会因缺乏有效的反馈机制而强化偏见。多智能体辩论（MAD）作为一种替代方法应运而生，但存在两个关键局限：偏见强化和视角缺乏多样性。为系统地评估这些问题，我们推出了MetaNIM Arena基准测试，用于评估LLMs在对抗性战略决策制定中的能力，其中动态交互影响最优决策。为了克服MAD的局限性，我们提出了DReaMAD框架（通过多元智能体辩论和精炼提示实现多样化的推理），该框架（1）完善LLM的战略先验知识以提高推理质量，（2）通过系统地修改提示，在单个模型内促进不同观点的形成，从而减少偏见。实证结果表明，DReaMAD在多个战略任务中显著提高了决策准确性、推理多样性和偏见缓解，确立其作为LLM决策制定的更有效方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在解决复杂问题时面临推理正确性的挑战。</li>
<li>自我校正方法虽然提高了可靠性，但可能强化模型偏见。</li>
<li>多智能体辩论（MAD）作为一种替代方法存在偏见强化和视角缺乏多样性两个问题。</li>
<li>MetaNIM Arena基准测试用于评估LLMs在战略决策中的能力。</li>
<li>DReaMAD框架通过完善战略先验知识和促进不同观点的形成来提高推理质量和减少偏见。</li>
<li>DReaMAD框架在多个战略任务中表现出显著的决策准确性、推理多样性和偏见缓解效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16814">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-069f28dc8d1f96d10118d3c25605c027.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbd6932c0fc5c6ed38783530a76fe110.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9341ba464e8d7147fde2df82cc4cd2fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c7e272cddf71634cdae5e650030df16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4644be4aabd665414c438b41f5a4ae0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67fb524f23eddf29f91799c63e276c8e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FlickerFusion-Intra-trajectory-Domain-Generalizing-Multi-Agent-RL"><a href="#FlickerFusion-Intra-trajectory-Domain-Generalizing-Multi-Agent-RL" class="headerlink" title="FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL"></a>FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL</h2><p><strong>Authors:Woosung Koh, Wonbeen Oh, Siyeol Kim, Suhin Shin, Hyeongjin Kim, Jaein Jang, Junghyun Lee, Se-Young Yun</strong></p>
<p>Multi-agent reinforcement learning has demonstrated significant potential in addressing complex cooperative tasks across various real-world applications. However, existing MARL approaches often rely on the restrictive assumption that the number of entities (e.g., agents, obstacles) remains constant between training and inference. This overlooks scenarios where entities are dynamically removed or added during the inference trajectory – a common occurrence in real-world environments like search and rescue missions and dynamic combat situations. In this paper, we tackle the challenge of intra-trajectory dynamic entity composition under zero-shot out-of-domain (OOD) generalization, where such dynamic changes cannot be anticipated beforehand. Our empirical studies reveal that existing MARL methods suffer significant performance degradation and increased uncertainty in these scenarios. In response, we propose FlickerFusion, a novel OOD generalization method that acts as a universally applicable augmentation technique for MARL backbone methods. FlickerFusion stochastically drops out parts of the observation space, emulating being in-domain when inferenced OOD. The results show that FlickerFusion not only achieves superior inference rewards but also uniquely reduces uncertainty vis-`a-vis the backbone, compared to existing methods. Benchmarks, implementations, and model weights are organized and open-sourced at flickerfusion305.github.io, accompanied by ample demo video renderings. </p>
<blockquote>
<p>多智能体强化学习在各种现实世界应用中解决复杂合作任务方面已显示出巨大潜力。然而，现有的多智能体强化学习（Multi-Agent Reinforcement Learning，简称MARL）方法通常依赖于一个限制性假设，即在训练和推理过程中实体的数量（例如智能体、障碍物）保持不变。这忽略了在推理轨迹过程中实体被动态移除或添加的场景——这在搜索和救援任务以及动态战斗情况等现实环境中是常见的情况。本文解决无预设环境下轨迹内动态实体组成挑战，即在无法预测的情况下会发生这样的动态变化。我们的实证研究发现在这些场景中现有MARL方法性能显著下降且不确定性增加。作为回应，我们提出了FlickerFusion，这是一种新型的离域泛化方法，作为多智能体强化学习主要方法的一种普遍适用的增强技术。FlickerFusion会随机丢失观测空间的部分信息，模拟在域内的情况进行推理。结果显示，与现有方法相比，FlickerFusion不仅实现了更高的推理奖励，而且独特地降低了相对于主要方法的不确定性。基准测试、实施方法和模型权重都在公开网站上整理并开源，同时配有丰富的演示视频渲染。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15876v4">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>多智能体强化学习在解决各种现实世界应用中的复杂合作任务方面显示出巨大潜力。然而，现有方法常假设实体数量在训练和推理过程中保持不变，忽略了现实世界中实体动态变化的情况。为解决此问题，本文提出了一种名为FlickerFusion的新方法，通过随机丢弃观测空间的部分信息，模拟在不同分布下推理所面对的困难，以模拟in-domain的环境，提升了强化学习模型泛化能力和不确定性的鲁棒性。其结果表明，相比现有方法，FlickerFusion能够显著地提升推理奖励并减少不确定性。其模型资源已公开于指定网站供大众访问和参考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体强化学习在处理复杂合作任务时具有显著潜力。</li>
<li>现有多智能体强化学习方法在实体数量变化时存在局限性。</li>
<li>FlickerFusion作为一种新的泛化方法，解决了实体动态变化的问题。</li>
<li>FlickerFusion通过随机丢弃观测空间信息模拟动态变化的实体场景，实现了模型性能的提升。</li>
<li>FlickerFusion相较于现有方法能够在提高推理奖励的同时降低不确定性。</li>
<li>FlickerFusion方法具有普遍适用性，可应用于多种多智能体强化学习背景方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15876">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ade91bd02c09b5b2594084f291cc728e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bdad938375bc3e55aa9e4f6607b7775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d69d3692d0aceab77e6cddacdc80a1de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73f6ed0502af5989c9bc03ca67d8bc05.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-12/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-12/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-12/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-01bd51cf355ba2bd9554a5a5c451e0f6.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-06-12  AraReasoner Evaluating Reasoning-Based LLMs for Arabic NLP
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8d1a08e8c7dbb9ea8fc7bc567907c814.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
