<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-a1b4e28a76543540f761275a06b6abda.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-12-æ›´æ–°"><a href="#2025-06-12-æ›´æ–°" class="headerlink" title="2025-06-12 æ›´æ–°"></a>2025-06-12 æ›´æ–°</h1><h2 id="VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning"><a href="#VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning" class="headerlink" title="VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning"></a>VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning</h2><p><strong>Authors:Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</strong></p>
<p>Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems. </p>
<blockquote>
<p>åœ¨åŠ¨æ€ç¯å¢ƒä¸­åè°ƒå¤šä¸ªå®ä½“ä»£ç†ä»ç„¶æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè¿™éœ€è¦æ„ŸçŸ¥é©±åŠ¨çš„æ¨ç†å’Œå¯æ‰©å±•çš„åˆä½œç­–ç•¥ã€‚è™½ç„¶è¿‘æœŸçš„å·¥ä½œå·²ç»åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¤šä»£ç†è§„åˆ’ï¼Œä½†å¾ˆå°‘æœ‰äººå¼€å§‹æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”¨äºè§†è§‰æ¨ç†ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºäºVLMçš„æ–¹æ³•åœ¨æ”¯æŒå¤šç§å®ä½“ç±»å‹æ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹å®ä½“å¤šä»£ç†åˆä½œçš„å®šåˆ¶åˆ†å±‚åŸºå‡†VIKI-Benchï¼Œå®ƒåŒ…å«ä¸‰ä¸ªç»“æ„åŒ–çº§åˆ«ï¼šä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ã€‚VIKI-BenchåŒ…æ‹¬å„ç§æœºå™¨äººå®ä½“ã€å¤šè§†å›¾è§†è§‰è§‚å¯Ÿä»¥åŠç»“æ„åŒ–ç›‘ç£ä¿¡å·ï¼Œä»¥è¯„ä¼°åŸºäºè§†è§‰è¾“å…¥çš„æ¨ç†ã€‚ä¸ºäº†è¯æ˜VIKI-Benchçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†VIKI-Rï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å¸¦æœ‰æ€ç»´é“¾æ³¨é‡Šçš„æ¼”ç¤ºå¯¹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œç„¶ååœ¨å¤šå±‚æ¬¡å¥–åŠ±ä¿¡å·ä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVIKI-Råœ¨æ‰€æœ‰ä»»åŠ¡çº§åˆ«ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿä½¿å¼‚è´¨ä»£ç†ä¹‹é—´å‡ºç°ç»„åˆåˆä½œæ¨¡å¼ã€‚æ€»ä¹‹ï¼ŒVIKI-Benchå’ŒVIKI-Rä¸ºæ¨è¿›å®ä½“AIç³»ç»Ÿä¸­çš„å¤šä»£ç†è§†è§‰é©±åŠ¨åˆä½œæä¾›äº†ç»Ÿä¸€çš„æµ‹è¯•å¹³å°å’Œæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09049v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://faceong.github.io/VIKI-R/">https://faceong.github.io/VIKI-R/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŠ¨æ€ç¯å¢ƒä¸­åè°ƒå¤šä¸ªå®ä½“ä»£ç†çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œéœ€è¦æ„ŸçŸ¥é©±åŠ¨æ¨ç†å’Œå¯æ‰©å±•çš„åˆä½œç­–ç•¥ã€‚å°½ç®¡è¿‘æœŸåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šä»£ç†è§„åˆ’çš„ç ”ç©¶å–å¾—äº†è¿›å±•ï¼Œä½†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•åœ¨æ”¯æŒå¤šç§å®ä½“ç±»å‹æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†VIKI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å®ä½“å¤šä»£ç†åˆä½œçš„åˆ†å±‚åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ä¸‰ä¸ªç»“æ„åŒ–çº§åˆ«ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†VIKI-Ræ¡†æ¶ï¼Œé€šè¿‡æ€ç»´é“¾æ³¨é‡Šæ¼”ç¤ºå¯¹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç„¶ååœ¨å¤šå±‚æ¬¡å¥–åŠ±ä¿¡å·ä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒVIKI-Råœ¨æ‰€æœ‰ä»»åŠ¡çº§åˆ«ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶ä¸”å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿä¿ƒè¿›å¼‚æ„ä»£ç†ä¹‹é—´çš„ç»„åˆåˆä½œæ¨¡å¼çš„å‡ºç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå®ä½“ä»£ç†åè°ƒåœ¨åŠ¨æ€ç¯å¢ƒä¸­æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŒ‘æˆ˜ä¹‹ä¸€ï¼Œéœ€è¦æ„ŸçŸ¥é©±åŠ¨æ¨ç†å’Œåˆä½œç­–ç•¥ã€‚</li>
<li>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä»£ç†è§„åˆ’æ–¹é¢æœ‰æ‰€åº”ç”¨ï¼Œä½†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•åœ¨æ”¯æŒå¤šç§å®ä½“ç±»å‹æ–¹é¢ä»æœ‰å±€é™æ€§ã€‚</li>
<li>VIKI-Benchæ˜¯é¦–ä¸ªé’ˆå¯¹å®ä½“å¤šä»£ç†åˆä½œçš„åˆ†å±‚åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ä¸‰ä¸ªç»“æ„åŒ–çº§åˆ«ã€‚</li>
<li>VIKI-Benchå…·æœ‰å¤šæ ·åŒ–çš„æœºå™¨äººå®ä½“ã€å¤šè§†è§’è§†è§‰è§‚å¯Ÿå’Œç»“æ„åŒ–ç›‘ç£ä¿¡å·ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¾“å…¥åŸºç¡€ä¸Šçš„æ¨ç†ã€‚</li>
<li>VIKI-Ræ¡†æ¶é€šè¿‡æ€ç»´é“¾æ³¨é‡Šæ¼”ç¤ºå¯¹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒVIKI-Råœ¨æ‰€æœ‰ä»»åŠ¡çº§åˆ«ä¸Šå‡ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9f318d1c1f344134a0ccdcac656830d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b4ff7c2f9df7c67670ee1acf6207be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0a3569e6a044d60b77d21c9a1d0f4e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-546e911c893917079b05fe314b9dedc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-493cacde6faf7b9e2c94ba6ec86deaf5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning"><a href="#Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning" class="headerlink" title="Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via   Reinforcement Learning"></a>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via   Reinforcement Learning</h2><p><strong>Authors:Haozhen Zhang, Tao Feng, Jiaxuan You</strong></p>
<p>The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave â€œthinkâ€ actions (internal deliberation) with â€œrouteâ€ actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1">https://github.com/ulab-uiuc/Router-R1</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿…é€Ÿæ¶Œç°æ¨åŠ¨äº†LLMè·¯ç”±å™¨çš„å¼€å‘ï¼Œè¿™äº›è·¯ç”±å™¨èƒ½å¤Ÿå°†ç”¨æˆ·æŸ¥è¯¢åˆ†é…ç»™æœ€åˆé€‚çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMè·¯ç”±å™¨é€šå¸¸é‡‡ç”¨å•ä¸€è½®æ¬¡ã€ä¸€å¯¹ä¸€çš„æ˜ å°„æ–¹å¼ï¼ˆå³æ¯æ¬¡æŸ¥è¯¢åªåˆ†é…ç»™ä¸€ä¸ªæ¨¡å‹ï¼‰ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè€Œè¿™äº›ä»»åŠ¡éœ€è¦å¤šä¸ªLLMæ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶Router-R1ï¼Œå®ƒå°†å¤šLLMè·¯ç”±å’Œèšåˆä½œä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹è¿›è¡Œè¡¨è¿°ã€‚Router-R1å®ä¾‹åŒ–è·¯ç”±å™¨æœ¬èº«ä½œä¸ºä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„LLMæ¨¡å‹ï¼Œåˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›å°†â€œæ€è€ƒâ€åŠ¨ä½œï¼ˆå†…éƒ¨æ€è€ƒï¼‰ä¸â€œè·¯ç”±â€åŠ¨ä½œï¼ˆåŠ¨æ€æ¨¡å‹è°ƒç”¨ï¼‰äº¤ç»‡åœ¨ä¸€èµ·ï¼Œå¹¶å°†æ¯ä¸ªå“åº”é›†æˆåˆ°ä¸æ–­å‘å±•çš„ä¸Šä¸‹æ–‡ä¸­ã€‚ä¸ºäº†æŒ‡å¯¼å­¦ä¹ ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è½»é‡çº§çš„åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼ŒåŒ…æ‹¬æ ¼å¼å¥–åŠ±ã€æœ€ç»ˆæˆæœå¥–åŠ±ä»¥åŠä¸€ç§æ–°å‹çš„æˆæœ¬å¥–åŠ±ï¼Œç”¨äºæ€§èƒ½ä¸æˆæœ¬æƒè¡¡ä¼˜åŒ–ï¼Œä»è€Œä¸ºé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°æ€§èƒ½æˆæœ¬ä¼˜åŒ–çš„è·¯å¾„å¼€è¾Ÿäº†é“è·¯ã€‚Router-R1ä»…æ ¹æ®ç®€å•çš„æ¨¡å‹æè¿°ç¬¦ï¼ˆå¦‚ä»·æ ¼ã€å»¶è¿Ÿå’Œç¤ºä¾‹æ€§èƒ½ï¼‰è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå®ç°å¯¹æœªè§æ¨¡å‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªé€šç”¨å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRouter-R1åœ¨å¤šä¸ªå¼ºåŸºçº¿æµ‹è¯•ä¸­å…·æœ‰å‡ºè‰²çš„è¡¨ç°ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥çš„æ³›åŒ–å’Œæˆæœ¬ç®¡ç†ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R">https://github.com/ulab-uiuc/Router-R</a> ç»“è¿›è¡Œè®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09033v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1">https://github.com/ulab-uiuc/Router-R1</a></p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è·¯ç”±å™¨ï¼ˆLLM Routerï¼‰çš„å‘å±•ä¸ºåˆ†é…ç”¨æˆ·æŸ¥è¯¢è‡³æœ€åˆé€‚çš„æ¨¡å‹æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰LLMè·¯ç”±å™¨é€šå¸¸é‡‡ç”¨å•ä¸€è½®æ¬¡ä¸€å¯¹ä¸€æ˜ å°„ï¼Œé™åˆ¶äº†å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„Router-R1æ¡†æ¶ï¼Œå°†å¤šLLMè·¯ç”±å’Œèšåˆè§†ä¸ºè¿ç»­å†³ç­–è¿‡ç¨‹ã€‚Router-R1å®ä¾‹åŒ–è·¯ç”±å™¨æœ¬èº«ä¸ºåŠŸèƒ½å¼ºå¤§çš„LLMï¼Œåˆ©ç”¨æ¨ç†èƒ½åŠ›äº¤æ›¿è¿›è¡Œâ€œæ€è€ƒâ€å’Œâ€œè·¯ç”±â€åŠ¨ä½œï¼Œå¹¶æ•´åˆå“åº”è‡³ä¸æ–­æ¼”åŒ–çš„è¯­å¢ƒä¸­ã€‚é€šè¿‡æ ¼å¼å¥–åŠ±ã€æœ€ç»ˆæˆæœå¥–åŠ±å’Œæˆæœ¬å¥–åŠ±ç»„æˆçš„è½»é‡çº§è§„åˆ™åŸºç¡€å¥–åŠ±æ¥æŒ‡å¯¼å­¦ä¹ ï¼Œä¼˜åŒ–äº†æ€§èƒ½å’Œæˆæœ¬çš„æƒè¡¡ã€‚Router-R1ä»…ä¾èµ–äºç®€å•çš„æ¨¡å‹æè¿°ç¬¦ï¼Œå¦‚ä»·æ ¼ã€å»¶è¿Ÿå’Œç¤ºä¾‹æ€§èƒ½ï¼Œå®ç°å¯¹æœªè§æ¨¡å‹çš„å¼ºå¤§æ³›åŒ–ã€‚åœ¨ä¸ƒä¸ªé€šç”¨å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šï¼ŒRouter-R1è¡¨ç°ä¼˜äºå¤šä¸ªå¼ºå¤§åŸºçº¿ï¼Œå®ç°äº†å“è¶Šæ€§èƒ½ã€ç¨³å¥æ³›åŒ–å’Œæˆæœ¬ç®¡ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè·¯ç”±å™¨çš„ç°æœ‰æŒ‘æˆ˜ï¼šä¸€å¯¹ä¸€æ˜ å°„é™åˆ¶äº†å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>Router-R1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šå°†å¤šLLMè·¯ç”±å’Œèšåˆè§†ä¸ºè¿ç»­å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>Router-R1åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›ï¼šé€šè¿‡äº¤æ›¿â€œæ€è€ƒâ€å’Œâ€œè·¯ç”±â€åŠ¨ä½œè¿›è¡Œä¼˜åŒ–å†³ç­–ã€‚</li>
<li>Router-R1é€šè¿‡è§„åˆ™åŸºç¡€å¥–åŠ±æŒ‡å¯¼å­¦ä¹ ï¼šåŒ…æ‹¬æ ¼å¼å¥–åŠ±ã€æœ€ç»ˆæˆæœå¥–åŠ±å’Œæˆæœ¬å¥–åŠ±ã€‚</li>
<li>Router-R1å®ç°æ€§èƒ½ä¸æˆæœ¬çš„ä¼˜åŒ–æƒè¡¡ï¼šé€šè¿‡RLæ¥å¹³è¡¡äºŒè€…ã€‚</li>
<li>Router-R1å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼šä»…ä¾èµ–ç®€å•çš„æ¨¡å‹æè¿°ç¬¦ï¼Œå¦‚ä»·æ ¼ã€å»¶è¿Ÿå’Œç¤ºä¾‹æ€§èƒ½ã€‚</li>
<li>Router-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼šåœ¨ä¸ƒä¸ªé€šç”¨å’Œå¤šè·³é—®ç­”åŸºå‡†ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f820e13d181d02e5200ed723cc1cb4cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d1a08e8c7dbb9ea8fc7bc567907c814.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522d6789bcf433b474749bd875ba174e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="e3-Learning-to-Explore-Enables-Extrapolation-of-Test-Time-Compute-for-LLMs"><a href="#e3-Learning-to-Explore-Enables-Extrapolation-of-Test-Time-Compute-for-LLMs" class="headerlink" title="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for   LLMs"></a>e3: Learning to Explore Enables Extrapolation of Test-Time Compute for   LLMs</h2><p><strong>Authors:Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar</strong></p>
<p>Test-time scaling offers a promising path to improve LLM reasoning by utilizing more compute at inference time; however, the true promise of this paradigm lies in extrapolation (i.e., improvement in performance on hard problems as LLMs keep â€œthinkingâ€ for longer, beyond the maximum token budget they were trained on). Surprisingly, we find that most existing reasoning models do not extrapolate well. We show that one way to enable extrapolation is by training the LLM to perform in-context exploration: training the LLM to effectively spend its test time budget by chaining operations (such as generation, verification, refinement, etc.), or testing multiple hypotheses before it commits to an answer. To enable in-context exploration, we identify three key ingredients as part of our recipe e3: (1) chaining skills that the base LLM has asymmetric competence in, e.g., chaining verification (easy) with generation (hard), as a way to implement in-context search; (2) leveraging â€œnegativeâ€ gradients from incorrect traces to amplify exploration during RL, resulting in longer search traces that chains additional asymmetries; and (3) coupling task difficulty with training token budget during training via a specifically-designed curriculum to structure in-context exploration. Our recipe e3 produces the best known 1.7B model according to AIMEâ€™25 and HMMTâ€™25 scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not only attains high pass@1 scores, but also improves pass@k over the base model. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯é€šè¿‡åœ¨æ¨ç†æ—¶é—´åˆ©ç”¨æ›´å¤šçš„è®¡ç®—èƒ½åŠ›ä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€æ¡æœ‰å‰æ™¯çš„é“è·¯ã€‚ç„¶è€Œï¼Œè¯¥èŒƒå¼çš„çœŸæ­£æ½œåŠ›åœ¨äºå¤–æ¨ï¼ˆå³ï¼Œéšç€LLMâ€œæ€è€ƒâ€çš„æ—¶é—´è¶…è¿‡å…¶è®­ç»ƒæ—¶çš„æœ€å¤§ä»¤ç‰Œé¢„ç®—ï¼Œå…¶åœ¨å›°éš¾é—®é¢˜ä¸Šçš„æ€§èƒ½å¾—åˆ°æ”¹è¿›ï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°ç°æœ‰çš„æ¨ç†æ¨¡å‹çš„å¤–æ¨èƒ½åŠ›å¹¶ä¸å¼ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå®ç°å¤–æ¨çš„ä¸€ç§æ–¹æ³•æ˜¯è®­ç»ƒLLMè¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢ï¼šè®­ç»ƒLLMé€šè¿‡é“¾æ¥æ“ä½œï¼ˆå¦‚ç”Ÿæˆã€éªŒè¯ã€ç²¾ç‚¼ç­‰ï¼‰æœ‰æ•ˆåœ°åˆ©ç”¨å…¶æµ‹è¯•æ—¶é—´é¢„ç®—ï¼Œæˆ–åœ¨æäº¤ç­”æ¡ˆä¹‹å‰æµ‹è¯•å¤šä¸ªå‡è®¾ã€‚ä¸ºäº†å®ç°ä¸Šä¸‹æ–‡æ¢ç´¢ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä½œä¸ºæˆ‘ä»¬é…æ–¹e3ä¸€éƒ¨åˆ†çš„ä¸‰ä¸ªå…³é”®è¦ç´ ï¼šï¼ˆ1ï¼‰é“¾æ¥åŸºæœ¬æŠ€èƒ½ï¼Œå¦‚LLMåœ¨éªŒè¯ï¼ˆç®€å•ï¼‰å’Œç”Ÿæˆï¼ˆå›°éš¾ï¼‰æ–¹é¢å…·æœ‰ä¸å¯¹ç§°ç«äº‰åŠ›ï¼Œä»¥æ­¤å®ç°ä¸Šä¸‹æ–‡æœç´¢ï¼›ï¼ˆ2ï¼‰åˆ©ç”¨é”™è¯¯è½¨è¿¹çš„â€œè´Ÿé¢â€æ¢¯åº¦æ¥æ”¾å¤§å¼ºåŒ–å­¦ä¹ æœŸé—´çš„æ¢ç´¢ï¼Œä»è€Œäº§ç”Ÿæ›´é•¿çš„æœç´¢è½¨è¿¹ï¼Œé“¾æ¥é¢å¤–çš„ä¸å¯¹ç§°æ€§ï¼›ï¼ˆ3ï¼‰é€šè¿‡ä¸“é—¨è®¾è®¡çš„è¯¾ç¨‹å°†ä»»åŠ¡éš¾åº¦ä¸è®­ç»ƒæ—¶çš„ä»¤ç‰Œé¢„ç®—ç›¸ç»“åˆï¼Œåœ¨è®­ç»ƒæœŸé—´æ„å»ºä¸Šä¸‹æ–‡æ¢ç´¢çš„ç»“æ„ã€‚æˆ‘ä»¬çš„e3é…æ–¹äº§ç”Ÿçš„æ¨¡å‹æ˜¯å·²çŸ¥çš„æœ€ä½³1.7Bæ¨¡å‹ï¼Œæ ¹æ®AIMEâ€™25å’ŒHMMTâ€™25çš„åˆ†æ•°ï¼Œå¹¶ä¸”å¤–æ¨åˆ°è®­ç»ƒä»¤ç‰Œé¢„ç®—çš„2å€ã€‚æˆ‘ä»¬çš„e3-1.7Bæ¨¡å‹ä¸ä»…è·å¾—äº†é«˜pass@1åˆ†æ•°ï¼Œè€Œä¸”ç›¸å¯¹äºåŸºç¡€æ¨¡å‹è¿˜æé«˜äº†pass@kã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09026v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºæ¥æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹åœ¨æµ‹è¯•æ—¶è¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢ï¼Œå®ç°é•¿æ—¶é—´çš„æ¨ç†ï¼Œè¿›è€Œæé«˜åœ¨éš¾é¢˜ä¸Šçš„è¡¨ç°ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºe3çš„æ–¹æ³•ï¼Œé€šè¿‡æŠ€èƒ½é“¾ã€åˆ©ç”¨é”™è¯¯æ¢¯åº¦å¼ºåŒ–å­¦ä¹ å’Œä»»åŠ¡éš¾åº¦ä¸è®­ç»ƒtokené¢„ç®—çš„ç»“åˆï¼Œå®ç°äº†LLMçš„ä¸Šä¸‹æ–‡æ¢ç´¢è®­ç»ƒã€‚è¿™ç§æ–¹æ³•èƒ½æé«˜AIåœ¨è¯„ä¼°ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶å®ç°æ¨ç†æ—¶é•¿çš„å»¶é•¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æµ‹è¯•æ—¶æ‰©å±•åˆ©ç”¨æ›´å¤šè®¡ç®—èµ„æºæ˜¯æé«˜LLMæ¨ç†èƒ½åŠ›çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>ä¸Šä¸‹æ–‡æ¢ç´¢æ˜¯å®ç°é•¿æ—¶é—´æ¨ç†çš„å…³é”®ï¼Œå¯ä»¥é€šè¿‡è®­ç»ƒLLMè¿›è¡ŒæŠ€èƒ½é“¾æ“ä½œæ¥å®ç°ã€‚</li>
<li>æŠ€èƒ½é“¾åŒ…æ‹¬ç”Ÿæˆã€éªŒè¯ã€ç»†åŒ–ç­‰æ“ä½œï¼Œé€šè¿‡æµ‹è¯•å¤šä¸ªå‡è®¾å†ç»™å‡ºç­”æ¡ˆã€‚</li>
<li>å®ç°ä¸Šä¸‹æ–‡æ¢ç´¢çš„ä¸‰ç§å…³é”®æ–¹æ³•åŒ…æ‹¬æŠ€èƒ½é“¾ã€åˆ©ç”¨é”™è¯¯æ¢¯åº¦å¼ºåŒ–å­¦ä¹ å’Œä»»åŠ¡éš¾åº¦ä¸è®­ç»ƒtokené¢„ç®—çš„ç»“åˆã€‚</li>
<li>e3æ–¹æ³•æé«˜äº†AIåœ¨è¯„ä¼°ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶èƒ½å®ç°æ¨ç†æ—¶é•¿çš„å»¶é•¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79063f526e6338653b27ef1a207f06ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40fc0dbe113ec3791fb56f3b892fa654.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e92c894c318ce797534c1976e6e1b1fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba1a7daded12a6b6c15d2d36209431d3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-Across-Parallel-Samples-for-LLM-Reasoning"><a href="#Learning-to-Reason-Across-Parallel-Samples-for-LLM-Reasoning" class="headerlink" title="Learning to Reason Across Parallel Samples for LLM Reasoning"></a>Learning to Reason Across Parallel Samples for LLM Reasoning</h2><p><strong>Authors:Jianing Qi, Xi Ye, Hao Tang, Zhigang Zhu, Eunsol Choi</strong></p>
<p>Scaling test-time compute brings substantial performance gains for large language models (LLMs). By sampling multiple answers and heuristically aggregate their answers (e.g., either through majority voting or using verifiers to rank the answers), one can achieve consistent performance gains in math domains. In this paper, we propose a new way to leverage such multiple sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that takes a concatenated sequence of multiple samples and output the final answer, optimizing it for the answer accuracy with reinforcement learning. Experiments on multiple reasoning datasets show that SSA outperforms other test-time scaling methods such as reward model-based re-ranking. Our approach also shows a promising generalization ability, across sample set sizes, base model families and scales, and tasks. By separating LLMs to generate answers and LLMs to analyze and aggregate sampled answers, our approach can work with the outputs from premier black box models easily and efficiently. </p>
<blockquote>
<p>æ‰©å¤§æµ‹è¯•æ—¶é—´è®¡ç®—ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¦æ¥äº†å·¨å¤§çš„æ€§èƒ½æå‡ã€‚é€šè¿‡é‡‡æ ·å¤šä¸ªç­”æ¡ˆå¹¶å¯å‘å¼åœ°èšåˆå®ƒä»¬çš„ç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡æŠ•ç¥¨å¤šæ•°æˆ–åˆ©ç”¨éªŒè¯å™¨å¯¹ç­”æ¡ˆè¿›è¡Œæ’åï¼‰ï¼Œå¯ä»¥åœ¨æ•°å­¦é¢†åŸŸå®ç°æŒç»­çš„æ€§èƒ½æå‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è¿™ç§å¤šæ ·æ ·æœ¬é›†çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§ç´§å‡‘çš„LLMï¼Œç§°ä¸ºæ ·æœ¬é›†èšåˆå™¨ï¼ˆSSAï¼‰ï¼Œå®ƒæ¥å—å¤šä¸ªæ ·æœ¬çš„è¿ç»­åºåˆ—ï¼Œè¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSSAä¼˜äºå…¶ä»–æµ‹è¯•æ—¶é—´è§„æ¨¡çš„æ–¹æ³•ï¼Œå¦‚åŸºäºå¥–åŠ±æ¨¡å‹çš„é‡æ–°æ’åã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æ˜¾ç¤ºå‡ºå¾ˆå¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯åº”ç”¨äºä¸åŒçš„æ ·æœ¬é›†å¤§å°ã€åŸºç¡€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä»¥åŠä»»åŠ¡ã€‚é€šè¿‡å°†LLMåˆ†ç¦»ä»¥ç”Ÿæˆç­”æ¡ˆå’ŒLLMæ¥åˆ†æå¹¶èšåˆé‡‡æ ·ç­”æ¡ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè½»æ¾é«˜æ•ˆåœ°å¤„ç†é¡¶çº§é»‘ç›’æ¨¡å‹çš„è¾“å‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09014v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹è¯•æ—¶é€šè¿‡è®¡ç®—æ‰©å±•èƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡é‡‡æ ·å¤šä¸ªç­”æ¡ˆå¹¶å¯å‘å¼åœ°èšåˆå®ƒä»¬ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡å¤šæ•°æŠ•ç¥¨æˆ–ä½¿ç”¨éªŒè¯å™¨å¯¹ç­”æ¡ˆè¿›è¡Œæ’åï¼‰ï¼Œå¯ä»¥åœ¨æ•°å­¦é¢†åŸŸå®ç°æŒç»­çš„æ€§èƒ½æå‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ©ç”¨è¿™æ ·çš„å¤šé‡æ ·æœ¬é›†çš„æ–¹æ³•ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§ç´§å‡‘çš„LLMï¼Œç§°ä¸ºæ ·æœ¬é›†èšåˆå™¨ï¼ˆSSAï¼‰ï¼Œå®ƒæ¥å—å¤šä¸ªæ ·æœ¬çš„è¿ç»­åºåˆ—å¹¶è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSSAä¼˜äºå…¶ä»–æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œå¦‚åŸºäºå¥–åŠ±æ¨¡å‹çš„æ’åã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºä¸åŒçš„æ ·æœ¬é›†å¤§å°ã€åŸºç¡€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä»¥åŠä»»åŠ¡ã€‚é€šè¿‡å°†LLMåˆ†ç¦»ä¸ºç”Ÿæˆç­”æ¡ˆçš„LLMå’Œåˆ†æå¹¶èšåˆé‡‡æ ·ç­”æ¡ˆçš„LLMï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè½»æ¾é«˜æ•ˆåœ°ä½¿ç”¨ä¸€æµçš„é»‘ç›’æ¨¡å‹çš„è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶çš„è®¡ç®—æ‰©å±•å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½æå‡è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡é‡‡æ ·å¤šä¸ªç­”æ¡ˆå¹¶å¯å‘å¼åœ°èšåˆå®ƒä»¬ï¼Œå¯ä»¥åœ¨æ•°å­¦é¢†åŸŸå®ç°LLMçš„æŒç»­æ€§èƒ½æå‡ã€‚</li>
<li>æ ·æœ¬é›†èšåˆå™¨ï¼ˆSSAï¼‰æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥è®­ç»ƒLLMä»¥ä¼˜åŒ–ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚</li>
<li>SSAé€šè¿‡æ¥å—å¤šä¸ªæ ·æœ¬çš„è¿ç»­åºåˆ—å¹¶è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œåœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>SSAä¼˜äºå…¶ä»–æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œå¦‚åŸºäºå¥–åŠ±æ¨¡å‹çš„æ’åã€‚</li>
<li>SSAå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºä¸åŒçš„æ ·æœ¬é›†å¤§å°ã€åŸºç¡€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä»¥åŠä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43cffa28f5ed9dfe35ed5b4af90b12fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2625138392f1f388a1278f420018a790.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Can-A-Gamer-Train-A-Mathematical-Reasoning-Model"><a href="#Can-A-Gamer-Train-A-Mathematical-Reasoning-Model" class="headerlink" title="Can A Gamer Train A Mathematical Reasoning Model?"></a>Can A Gamer Train A Mathematical Reasoning Model?</h2><p><strong>Authors:Andrew Shin</strong></p>
<p>While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. <a target="_blank" rel="noopener" href="https://github.com/shinandrew/YouronMath">https://github.com/shinandrew/YouronMath</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œä½†å®ƒä»¬çš„å¼€å‘é€šå¸¸éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚å°½ç®¡æœ€è¿‘çš„è¿›æ­¥é™ä½äº†è®­ç»ƒæœ‰èƒ½åŠ›æ¨¡å‹çš„æˆæœ¬ï¼Œä½†è¿™äº›æ–¹æ³•ä»ç„¶ä¾èµ–äºé«˜ç«¯ç¡¬ä»¶é›†ç¾¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œå±•ç¤ºäº†ä½¿ç”¨å•ä¸ªæ™®é€šæ¸¸æˆGPUå°±å¯ä»¥è®­ç»ƒå‡ºç¨³å¥çš„æ•°å­¦æ¨ç†æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å†…å­˜ä¸º16GBçš„RTX 3080 Tiä¸Šè®­ç»ƒäº†ä¸€ä¸ª1.5äº¿å‚æ•°çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œè¯¥æ¨¡å‹åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¸æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¥½ã€‚æˆ‘ä»¬çš„ç»“æœæŒ‘æˆ˜äº†æœ€å…ˆè¿›æ•°å­¦æ¨ç†éœ€è¦å¤§è§„æ¨¡åŸºç¡€è®¾æ–½çš„ç°çŠ¶ï¼Œä¸ºé«˜æ€§èƒ½äººå·¥æ™ºèƒ½ç ”ç©¶çš„æ™®åŠåŒ–æä¾›äº†å¯èƒ½ã€‚å¯ä»¥é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/shinandrew/YouronMath%E8%AE%BF%E9%97%AE%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/shinandrew/YouronMathè®¿é—®ç›¸å…³èµ„æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08935v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶å¼€å‘éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚æœ€æ–°ç ”ç©¶è¯æ˜ï¼Œä½¿ç”¨å•ä¸ªæ™®é€šæ¸¸æˆGPUç»“åˆå¼ºåŒ–å­¦ä¹ ä¸å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯ä»¥è®­ç»ƒå‡ºæ€§èƒ½å‡ºè‰²çš„æ•°å­¦æ¨ç†æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ä»…æœ‰16GBå†…å­˜çš„RTX 3080 Tiä¸Šè®­ç»ƒäº†ä¸€ä¸ª1.5äº¿å‚æ•°çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œå…¶åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸æ›´å¤§æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜ï¼Œæ˜¾è‘—é™ä½äº†èµ„æºæ¶ˆè€—ã€‚è¯¥ç ”ç©¶æ‰“ç ´äº†é«˜æ€§èƒ½æ•°å­¦æ¨ç†å¿…é¡»ä¾èµ–å¤§è§„æ¨¡è®¾æ–½çš„å›ºæœ‰è§‚å¿µï¼Œæ¨åŠ¨äº†AIç ”ç©¶çš„æ°‘ä¸»åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨å•ä¸ªæ™®é€šæ¸¸æˆGPUå¯ä»¥è®­ç»ƒå‡ºé«˜æ€§èƒ½çš„æ•°å­¦æ¨ç†æ¨¡å‹ã€‚</li>
<li>ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å®ç°è‰¯å¥½çš„æ•°å­¦æ¨ç†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è®­ç»ƒçš„æ•°å­¦æ¨ç†æ¨¡å‹åœ¨RTX 3080 Tiä¸Šå®ç°äº†ä¸æ›´å¤§æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶æŒ‘æˆ˜äº†é«˜æ€§èƒ½æ•°å­¦æ¨ç†å¿…é¡»ä¾èµ–å¤§è§„æ¨¡è®¾æ–½çš„ç°æœ‰è§‚å¿µã€‚</li>
<li>è¿™ç§æ–¹æ³•çš„å¯è¡Œæ€§ä¸ºæ°‘ä¸»åŒ–è®¿é—®é«˜æ€§èƒ½AIç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</li>
<li>æ­¤ç ”ç©¶å±•ç¤ºäº†GitHubä¸Šçš„å¼€æºé¡¹ç›®ï¼Œå³ï¼šå¦‚ä½•æ›´å¹¿æ³›åœ°åº”ç”¨è¯¥æ–¹æ³•åŠæ”¹è¿›ç ”ç©¶è·¯å¾„å˜å¾—å°¤ä¸ºå…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-341a77519510c65e6805cf79349133e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccaacf4bf77e8ebfc70ecde6c7ffe85b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="What-Limits-Virtual-Agent-Application-OmniBench-A-Scalable-Multi-Dimensional-Benchmark-for-Essential-Virtual-Agent-Capabilities"><a href="#What-Limits-Virtual-Agent-Application-OmniBench-A-Scalable-Multi-Dimensional-Benchmark-for-Essential-Virtual-Agent-Capabilities" class="headerlink" title="What Limits Virtual Agent Application? OmniBench: A Scalable   Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities"></a>What Limits Virtual Agent Application? OmniBench: A Scalable   Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities</h2><p><strong>Authors:Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation with limited scenarios, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, cross-platform, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91% human acceptance rate. Training on our graph-structured data shows that it can more efficiently guide agents compared to manually annotated data. We conduct multidimensional evaluations for various open-source and closed-source models, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at <a target="_blank" rel="noopener" href="https://omni-bench.github.io/">https://omni-bench.github.io/</a>. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¸æ–­å‘å±•ï¼ŒåŸºäºMLLMçš„è™šæ‹Ÿä»£ç†äººåœ¨æ€§èƒ½ä¸Šå±•ç°å‡ºäº†æ˜¾è‘—çš„æˆæœã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä»»åŠ¡å¤æ‚æ€§ä¸å¯æ§ã€åœºæ™¯æœ‰é™éœ€è¦å¹¿æ³›çš„æ‰‹åŠ¨æ³¨é‡Šä»¥åŠç¼ºä¹å¤šç»´è¯„ä¼°ç­‰é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªæˆ‘ç”Ÿæˆã€è·¨å¹³å°ã€åŸºäºå›¾çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å­ä»»åŠ¡ç»„åˆï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºåˆæˆå¯æ§å¤æ‚åº¦çš„ä»»åŠ¡ã€‚ä¸ºäº†è¯„ä¼°è™šæ‹Ÿä»£ç†äººåœ¨å›¾ä¸Šçš„å„ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†OmniEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šç»´è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬å­ä»»åŠ¡çº§è¯„ä¼°ã€åŸºäºå›¾çš„æŒ‡æ ‡ä»¥åŠè·¨è¶Š10ç§èƒ½åŠ›çš„ç»¼åˆæµ‹è¯•ã€‚æˆ‘ä»¬çš„åˆæˆæ•°æ®é›†åŒ…å«20ä¸ªåœºæ™¯ä¸‹çš„3.6ä¸‡ä¸ªå›¾ç»“æ„ä»»åŠ¡ï¼Œè¾¾åˆ°äº†91%çš„äººç±»æ¥å—ç‡ã€‚åœ¨æˆ‘ä»¬å›¾ç»“æ„æ•°æ®ä¸Šçš„è®­ç»ƒç»“æœè¡¨æ˜ï¼Œä¸æ‰‹åŠ¨æ³¨é‡Šæ•°æ®ç›¸æ¯”ï¼Œå®ƒæ›´èƒ½æœ‰æ•ˆåœ°æŒ‡å¯¼è™šæ‹Ÿä»£ç†äººã€‚æˆ‘ä»¬å¯¹å„ç§å¼€æºå’Œä¸“æœ‰æ¨¡å‹è¿›è¡Œäº†å¤šç»´è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å„ç§èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼Œä¸ºæœªæ¥çš„è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„é¡¹ç›®åœ¨<a target="_blank" rel="noopener" href="https://omni-bench.github.io/%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://omni-bench.github.io/ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08933v1">PDF</a> Accepted by ICML 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•ï¼ŒåŸºäºMLLMçš„è™šæ‹Ÿä»£ç†äººåœ¨å¤šä¸ªåœºæ™¯å±•ç°å‡ºè‰²æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•é¢ä¸´å¯æ§æ€§ä»»åŠ¡å¤æ‚ã€æ‰‹åŠ¨æ ‡æ³¨ç¹çä¸”åœºæ™¯æœ‰é™ã€å¤šç»´åº¦è¯„ä¼°ç¼ºå¤±ç­‰æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†OmniBenchï¼Œä¸€ä¸ªè‡ªæˆ‘ç”Ÿæˆã€è·¨å¹³å°ã€åŸºäºå›¾å½¢çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å­ä»»åŠ¡ç»„åˆæ¥åˆæˆå¯æ§å¤æ‚åº¦çš„ä»»åŠ¡ã€‚åŒæ—¶ï¼Œä¸ºè¯„ä¼°è™šæ‹Ÿä»£ç†äººåœ¨å›¾å½¢ä¸Šçš„å¤šæ ·åŒ–èƒ½åŠ›ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†OmniEvalï¼Œä¸€ä¸ªåŒ…æ‹¬å­ä»»åŠ¡çº§åˆ«è¯„ä¼°ã€å›¾å½¢æŒ‡æ ‡å’Œå…¨é¢æµ‹è¯•10ç§èƒ½åŠ›çš„å¤šç»´åº¦è¯„ä¼°æ¡†æ¶ã€‚åˆæˆçš„æ•°æ®é›†åŒ…å«36,000ä¸ªå›¾å½¢ç»“æ„ä»»åŠ¡ï¼Œè¦†ç›–20ä¸ªåœºæ™¯ï¼Œè¾¾åˆ°91%çš„äººç±»æ¥å—ç‡ã€‚ä½¿ç”¨æˆ‘ä»¬çš„å›¾å½¢ç»“æ„æ•°æ®è¿›è¡Œè®­ç»ƒè¡¨æ˜ï¼Œå®ƒå¯ä»¥æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼ä»£ç†ï¼Œç›¸æ¯”æ‰‹åŠ¨æ³¨é‡Šçš„æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è™šæ‹Ÿä»£ç†äººè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>OmniBenchæ˜¯ä¸€ä¸ªè‡ªæˆ‘ç”Ÿæˆã€è·¨å¹³å°çš„å›¾å½¢åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å­ä»»åŠ¡ç»„åˆå®ç°å¯æ§ä»»åŠ¡å¤æ‚åº¦ã€‚</li>
<li>OmniEvalæ˜¯ä¸€ä¸ªå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è™šæ‹Ÿä»£ç†äººåœ¨å›¾å½¢ä¸Šçš„å¤šæ ·åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬å­ä»»åŠ¡çº§åˆ«è¯„ä¼°ã€å›¾å½¢æŒ‡æ ‡å’Œå…¨é¢æµ‹è¯•10ç§èƒ½åŠ›ã€‚</li>
<li>åˆæˆçš„æ•°æ®é›†åŒ…å«å¤§é‡å›¾å½¢ç»“æ„ä»»åŠ¡ï¼Œè¦†ç›–å¤šç§åœºæ™¯ï¼Œä¸”è·å¾—é«˜äººç±»æ¥å—ç‡ã€‚</li>
<li>ä½¿ç”¨å›¾å½¢ç»“æ„æ•°æ®è¿›è¡Œè®­ç»ƒå¯æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼è™šæ‹Ÿä»£ç†äººã€‚</li>
<li>OmniBenché¡¹ç›®æä¾›äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°å¹³å°ï¼Œæœ‰åŠ©äºæœªæ¥æŠ€æœ¯è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08933">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94fefb1b6723dc45026428f922a131a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ab7505d86fb36096182b108efafa352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f2cf80efd8feb950a53f38fd70daf5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e18f0ce0b4c569fb238440dd97fd3ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bd8bf27b29d78336e7dd18a203af15c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35206aa3883c2327bd0e60b86a68b4cd.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Intention-Conditioned-Flow-Occupancy-Models"><a href="#Intention-Conditioned-Flow-Occupancy-Models" class="headerlink" title="Intention-Conditioned Flow Occupancy Models"></a>Intention-Conditioned Flow Occupancy Models</h2><p><strong>Authors:Chongyi Zheng, Seohong Park, Sergey Levine, Benjamin Eysenbach</strong></p>
<p>Large-scale pre-training has fundamentally changed how machine learning research is done today: large foundation models are trained once, and then can be used by anyone in the community (including those without data or compute resources to train a model from scratch) to adapt and fine-tune to specific tasks. Applying this same framework to reinforcement learning (RL) is appealing because it offers compelling avenues for addressing core challenges in RL, including sample efficiency and robustness. However, there remains a fundamental challenge to pre-train large models in the context of RL: actions have long-term dependencies, so training a foundation model that reasons across time is important. Recent advances in generative AI have provided new tools for modeling highly complex distributions. In this paper, we build a probabilistic model to predict which states an agent will visit in the temporally distant future (i.e., an occupancy measure) using flow matching. As large datasets are often constructed by many distinct users performing distinct tasks, we include in our model a latent variable capturing the user intention. This intention increases the expressivity of our model, and enables adaptation with generalized policy improvement. We call our proposed method intention-conditioned flow occupancy models (InFOM). Comparing with alternative methods for pre-training, our experiments on $36$ state-based and $4$ image-based benchmark tasks demonstrate that the proposed method achieves $1.8 \times$ median improvement in returns and increases success rates by $36%$. Website: <a target="_blank" rel="noopener" href="https://chongyi-zheng.github.io/infom">https://chongyi-zheng.github.io/infom</a> Code: <a target="_blank" rel="noopener" href="https://github.com/chongyi-zheng/infom">https://github.com/chongyi-zheng/infom</a> </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒå·²ç»ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†å½“å‰æœºå™¨å­¦ä¹ ç ”ç©¶çš„æ–¹å¼ï¼šå¤§å‹åŸºç¡€æ¨¡å‹åªéœ€è®­ç»ƒä¸€æ¬¡ï¼Œç¤¾åŒºä¸­çš„ä»»ä½•äººï¼ˆåŒ…æ‹¬é‚£äº›æ²¡æœ‰æ•°æ®æˆ–è®¡ç®—èµ„æºä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„äººï¼‰éƒ½å¯ä»¥ç”¨æ¥é€‚åº”å’Œå¾®è°ƒç‰¹å®šä»»åŠ¡ã€‚å°†è¿™ä¸€æ¡†æ¶åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯å¾ˆæœ‰å¸å¼•åŠ›çš„ï¼Œå› ä¸ºå®ƒä¸ºè§£å†³RLä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜æä¾›äº†æœ‰å¸å¼•åŠ›çš„é€”å¾„ï¼ŒåŒ…æ‹¬æ ·æœ¬æ•ˆç‡å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œåœ¨å¼ºåŒ–å­¦ä¹ çš„èƒŒæ™¯ä¸‹å¯¹å¤§å‹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒä»å­˜åœ¨åŸºæœ¬æŒ‘æˆ˜ï¼šè¡ŒåŠ¨å…·æœ‰é•¿æœŸä¾èµ–æ€§ï¼Œå› æ­¤è®­ç»ƒèƒ½å¤Ÿè·¨æ—¶é—´æ¨ç†çš„åŸºç¡€æ¨¡å‹å¾ˆé‡è¦ã€‚ç”ŸæˆAIçš„æœ€æ–°è¿›å±•ä¸ºå»ºç«‹é«˜åº¦å¤æ‚çš„åˆ†å¸ƒæä¾›äº†æ–°çš„å·¥å…·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œä½¿ç”¨æµåŒ¹é…æ¥é¢„æµ‹ä»£ç†åœ¨é¥è¿œæœªæ¥è®¿é—®çš„çŠ¶æ€ï¼ˆå³å ç”¨åº¦é‡ï¼‰ã€‚ç”±äºå¤§å‹æ•°æ®é›†é€šå¸¸æ˜¯ç”±æ‰§è¡Œä¸åŒä»»åŠ¡çš„è®¸å¤šä¸åŒç”¨æˆ·æ„å»ºçš„ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ¨¡å‹ä¸­åŒ…å«ä¸€ä¸ªæ•æ‰ç”¨æˆ·æ„å›¾çš„æ½œåœ¨å˜é‡ã€‚è¿™ç§æ„å›¾å¢åŠ äº†æˆ‘ä»¬æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶é€šè¿‡é€šç”¨ç­–ç•¥æ”¹è¿›å®ç°äº†é€‚åº”ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ç§°ä¸ºæ„å›¾è°ƒèŠ‚æµå ç”¨æ¨¡å‹ï¼ˆInFOMï¼‰ã€‚ä¸å…¶ä»–é¢„è®­ç»ƒæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬åœ¨36ä¸ªåŸºäºçŠ¶æ€å’Œ4ä¸ªåŸºäºå›¾åƒçš„æ ‡å‡†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ”¶ç›Šä¸­ä½æ•°çš„1.8å€å¢é•¿ï¼ŒæˆåŠŸç‡æé«˜äº†36%ã€‚ç½‘ç«™åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://chongyi-zheng.github.io/infom%E4%BB%A3%E7%A0%81%E5%9C%B0%E5%9D%80%E4%B8%BA%EF%BC%9Ahttps://github.com/chongyi-zheng/infom%E3%80%82">https://chongyi-zheng.github.io/infomä»£ç åœ°å€ä¸ºï¼šhttps://github.com/chongyi-zheng/infomã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08902v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–°æ–¹æ³•ã€‚ç”±äºåŠ¨ä½œå­˜åœ¨é•¿æœŸä¾èµ–å…³ç³»ï¼Œå› æ­¤éœ€è¦å»ºç«‹èƒ½å¤Ÿæ¨ç†è·¨æ—¶é—´çš„åŸºç¡€æ¨¡å‹ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ¦‚ç‡æ¨¡å‹â€”â€”æ„å›¾æ¡ä»¶æµå ç”¨æ¨¡å‹ï¼ˆInFOMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹ä»£ç†åœ¨è¿œæœŸè®¿é—®çš„çŠ¶æ€ï¼Œå¹¶ä½¿ç”¨æµåŒ¹é…æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çŠ¶æ€åŸºå‡†å’Œå›¾åƒåŸºå‡†ä»»åŠ¡ä¸Šå®ç°äº†ä¼˜äºå…¶ä»–é¢„è®­ç»ƒæ–¹æ³•çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œæœ‰åŠ©äºè§£å†³æ ·æœ¬æ•ˆç‡å’Œç¨³å¥æ€§ç­‰æ–¹é¢çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>åŠ¨ä½œå­˜åœ¨é•¿æœŸä¾èµ–å…³ç³»ï¼Œå› æ­¤éœ€è¦å»ºç«‹èƒ½å¤Ÿæ¨ç†è·¨æ—¶é—´çš„åŸºç¡€æ¨¡å‹ã€‚</li>
<li>ä½œè€…æå‡ºäº†æ„å›¾æ¡ä»¶æµå ç”¨æ¨¡å‹ï¼ˆInFOMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¦‚ç‡æ¨¡å‹ï¼Œå¯ä»¥é¢„æµ‹ä»£ç†åœ¨è¿œæœŸè®¿é—®çš„çŠ¶æ€ã€‚</li>
<li>InFOMæ¨¡å‹é€šè¿‡ä½¿ç”¨æµåŒ¹é…æŠ€æœ¯å®ç°é¢„æµ‹ï¼Œå¹¶åŒ…æ‹¬ä¸€ä¸ªæ•æ‰ç”¨æˆ·æ„å›¾çš„æ½œåœ¨å˜é‡ï¼Œè¿™æé«˜äº†æ¨¡å‹çš„è¡¨è¾¾åŠ›å¹¶å®ç°äº†é€‚åº”æ€§æ”¹è¿›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒInFOMæ–¹æ³•åœ¨çŠ¶æ€åŸºå‡†å’Œå›¾åƒåŸºå‡†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–é¢„è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®ç°ä¸Šä½¿ç”¨äº†å…ˆè¿›çš„ç”ŸæˆAIå·¥å…·ï¼Œå¦‚æµåŒ¹é…æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de131a7ed57085485693b30cfaef362f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SeerAttention-R-Sparse-Attention-Adaptation-for-Long-Reasoning"><a href="#SeerAttention-R-Sparse-Attention-Adaptation-for-Long-Reasoning" class="headerlink" title="SeerAttention-R: Sparse Attention Adaptation for Long Reasoning"></a>SeerAttention-R: Sparse Attention Adaptation for Long Reasoning</h2><p><strong>Authors:Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, Mao Yang</strong></p>
<p>We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64&#x2F;128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/microsoft/SeerAttention">https://github.com/microsoft/SeerAttention</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SeerAttention-Rï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ¨ç†æ¨¡å‹çš„é•¿è§£ç è®¾è®¡çš„ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶ã€‚SeerAttention-Ræ˜¯å¯¹SeerAttentionçš„æ‰©å±•ï¼Œå®ƒä¿ç•™äº†é€šè¿‡è‡ªè’¸é¦é—¨æ§æœºåˆ¶å­¦ä¹ æ³¨æ„åŠ›ç¨€ç–æ€§çš„è®¾è®¡ï¼ŒåŒæ—¶ç§»é™¤äº†æŸ¥è¯¢æ± åŒ–ä»¥é€‚åº”è‡ªå›å½’è§£ç ã€‚é€šè¿‡ä½¿ç”¨è½»é‡çº§æ’ä»¶é—¨æ§ï¼ŒSeerAttention-Réå¸¸çµæ´»ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°é›†æˆåˆ°ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œè€Œæ— éœ€ä¿®æ”¹åŸå§‹å‚æ•°ã€‚æˆ‘ä»¬åœ¨AIMEåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†SeerAttention-Rï¼Œä»…åœ¨0.4Bä»¤ç‰Œä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¤§å‹ç¨€ç–æ³¨æ„åŠ›å—å¤§å°ï¼ˆ64&#x2F;128ï¼‰ä¸‹ï¼Œå…·æœ‰è¿‘æ— æŸçš„æ¨ç†å‡†ç¡®æ€§ï¼Œä»¤ç‰Œé¢„ç®—ä¸º4Kã€‚ä½¿ç”¨TileLangï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé«˜åº¦ä¼˜åŒ–çš„ç¨€ç–è§£ç å†…æ ¸ï¼Œåœ¨H100 GPUä¸Šç›¸å¯¹äºFlashAttention-3å®ç°äº†è¿‘9å€çš„ç†è®ºåŠ é€Ÿï¼Œåœ¨90%çš„ç¨€ç–æ€§ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/microsoft/SeerAttention">https://github.com/microsoft/SeerAttention</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08889v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SeerAttention-Ræ˜¯ä¸€ç§é’ˆå¯¹é•¿æ–‡æœ¬æ¨ç†æ¨¡å‹çš„ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶ã€‚å®ƒé€šè¿‡è‡ªæˆ‘è’¸é¦çš„é—¨æ§æœºåˆ¶å­¦ä¹ æ³¨æ„åŠ›ç¨€ç–æ€§ï¼Œé€‚ç”¨äºè‡ªå›å½’è§£ç ã€‚è¯¥æ¡†æ¶æ˜“äºé›†æˆåˆ°ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œæ— éœ€ä¿®æ”¹åŸå§‹å‚æ•°ã€‚åœ¨AIMEåŸºå‡†æµ‹è¯•ä¸­ï¼Œå³ä½¿åœ¨å¤§å‹ç¨€ç–æ³¨æ„åŠ›å—å¤§å°ä¸‹ï¼ŒSeerAttention-Råœ¨4Kä»¤ç‰Œé¢„ç®—å†…ä¹Ÿèƒ½ä¿æŒè¿‘æ— æŸçš„æ¨ç†å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡TileLangå¼€å‘çš„ä¼˜åŒ–ç¨€ç–è§£ç å†…æ ¸ï¼Œå¯åœ¨H100 GPUä¸Šå®ç°æ¥è¿‘ç†è®ºé€Ÿåº¦çš„åŠ é€Ÿï¼Œåœ¨90%çš„ç¨€ç–æ€§ä¸‹ï¼Œé€Ÿåº¦æå‡å¯è¾¾è¿‘9å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SeerAttention-Ræ˜¯ä¸€ä¸ªä¸“ä¸ºé•¿æ–‡æœ¬è§£ç è®¾è®¡çš„ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶ã€‚</li>
<li>å®ƒé€šè¿‡è‡ªæˆ‘è’¸é¦çš„é—¨æ§æœºåˆ¶å­¦ä¹ æ³¨æ„åŠ›ç¨€ç–æ€§ï¼Œé€‚ç”¨äºè‡ªå›å½’è§£ç ã€‚</li>
<li>SeerAttention-Ræ˜“äºé›†æˆåˆ°ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œä¸”æ— éœ€ä¿®æ”¹åŸå§‹å‚æ•°ã€‚</li>
<li>åœ¨AIMEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSeerAttention-Råœ¨å¤§å‹ç¨€ç–æ³¨æ„åŠ›å—å¤§å°ä¸‹èƒ½ä¿æŒè¿‘æ— æŸçš„æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨è®­ç»ƒæ—¶ä»…ä½¿ç”¨äº†0.4Bä»¤ç‰Œï¼Œè¡¨ç°å‡ºé«˜æ•ˆçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨TileLangå¼€å‘çš„ä¼˜åŒ–ç¨€ç–è§£ç å†…æ ¸ï¼Œå¯åœ¨H100 GPUä¸Šå®ç°æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d12da991be3ed8a32e8753e8bc6d260c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-917074cf2c2b9e433ef46dc1ee42a736.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2db47c22841cff17fb9e87ae0418b760.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74c88a5b0043a2b1c95868b56f9efada.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54db2b361d0dc94c87f1508596d4b0dd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AraReasoner-Evaluating-Reasoning-Based-LLMs-for-Arabic-NLP"><a href="#AraReasoner-Evaluating-Reasoning-Based-LLMs-for-Arabic-NLP" class="headerlink" title="AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP"></a>AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP</h2><p><strong>Authors:Ahmed Hasanaath, Aisha Alansari, Ahmed Ashraf, Chafik Salmane, Hamzah Luqman, Saad Ezzini</strong></p>
<p>Large language models (LLMs) have shown remarkable progress in reasoning abilities and general natural language processing (NLP) tasks, yet their performance on Arabic data, characterized by rich morphology, diverse dialects, and complex script, remains underexplored. This paper presents a comprehensive benchmarking study of multiple reasoning-focused LLMs, with a special emphasis on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP tasks. We experiment with various strategies, including zero-shot, few-shot, and fine-tuning. This allows us to systematically evaluate performance on datasets covering a range of applications to examine their capacity for linguistic reasoning under different levels of complexity. Our experiments reveal several key findings. First, carefully selecting just three in-context examples delivers an average uplift of over 13 F1 points on classification tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures outperform a strong GPT o4-mini baseline by an average of 12 F1 points on complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning yields up to an additional 8 points in F1 and BLEU compared to equivalent increases in model scale. The code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AraReasoner41299">https://anonymous.4open.science/r/AraReasoner41299</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›å’Œé€šç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†å…·æœ‰ä¸°å¯Œå½¢æ€ã€å¤šæ ·æ–¹è¨€å’Œå¤æ‚è„šæœ¬çš„é˜¿æ‹‰ä¼¯è¯­æ•°æ®æ–¹é¢çš„è¡¨ç°ä»ç„¶ç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢ã€‚æœ¬æ–‡å…¨é¢è¯„ä¼°äº†å¤šä¸ªä»¥æ¨ç†ä¸ºé‡ç‚¹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«å…³æ³¨æ–°æ¨å‡ºçš„DeepSeekæ¨¡å‹ï¼Œåœ¨åäº”é¡¹é˜¿æ‹‰ä¼¯è¯­NLPä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å°è¯•äº†å„ç§ç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç³»ç»Ÿåœ°è¯„ä¼°åœ¨å„ç§æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œæ¶µç›–å¤šç§åº”ç”¨ï¼Œä»¥æ£€éªŒå®ƒä»¬åœ¨ä¸åŒå¤æ‚ç¨‹åº¦ä¸‹çš„è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†å‡ ä¸ªå…³é”®å‘ç°ã€‚é¦–å…ˆï¼Œç²¾å¿ƒé€‰æ‹©ä»…ä¸‰ä¸ªä¸Šä¸‹æ–‡å®ä¾‹å¯ä»¥åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šå¹³å‡æé«˜è¶…è¿‡13ä¸ªF1ç‚¹ï¼Œæƒ…æ„Ÿåˆ†æä»35.3%æé«˜åˆ°87.5%ï¼Œè€Œé‡Šä¹‰æ£€æµ‹ä»56.1%æé«˜åˆ°87.0%ã€‚å…¶æ¬¡ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼Œä»¥æ¨ç†ä¸ºé‡ç‚¹çš„DeepSeekæ¶æ„åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå¹³å‡ä¼˜äºå¼ºå¤§çš„GPT o4-miniåŸºçº¿12ä¸ªF1ç‚¹ã€‚ç¬¬ä¸‰ï¼Œä¸æ¨¡å‹è§„æ¨¡çš„ç­‰æ•ˆå¢é•¿ç›¸æ¯”ï¼ŒåŸºäºLoRAçš„å¾®è°ƒåœ¨F1å’ŒBLEUå¾—åˆ†ä¸Šæœ€å¤šå¯æé«˜8ä¸ªç‚¹ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AraReasoner41299%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/AraReasoner41299è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08768v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†é˜¿æ‹‰ä¼¯æ•°æ®æ–¹é¢çš„æ€§èƒ½ä»æœ‰å¾…æ¢ç´¢ã€‚æœ¬æ–‡å…¨é¢è¯„ä¼°äº†å¤šä¸ªä»¥æ¨ç†ä¸ºé‡ç‚¹çš„LLMsï¼Œç‰¹åˆ«æ˜¯æ–°æ¨å‡ºçš„DeepSeekæ¨¡å‹ï¼Œåœ¨åäº”é¡¹é˜¿æ‹‰ä¼¯NLPä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç²¾å¿ƒé€‰æ‹©çš„ä¸‰ä¾‹ä¸Šä¸‹æ–‡ç¤ºä¾‹å¹³å‡æé«˜äº†è¶…è¿‡13ä¸ªF1ç‚¹çš„åˆ†ç±»ä»»åŠ¡æ€§èƒ½ï¼›DeepSeekæ¶æ„åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å¹³å‡è¡¨ç°ä¼˜äºGPT o4-miniåŸºå‡†æµ‹è¯•12ä¸ªF1ç‚¹ï¼›åŸºäºLoRAçš„å¾®è°ƒä¸æ¨¡å‹è§„æ¨¡çš„ç­‰æ•ˆå¢åŠ ç›¸æ¯”ï¼Œå¯é¢å¤–æé«˜F1å’ŒBLEUå¾—åˆ†é«˜è¾¾8åˆ†ã€‚æœ¬æ–‡çš„æˆæœå°†æœ‰åŠ©äºè¿›ä¸€æ­¥äº†è§£LLMsåœ¨å¤„ç†å¤æ‚é˜¿æ‹‰ä¼¯è¯­æ•°æ®æ—¶çš„æ€§èƒ½å’Œä¼˜åŒ–ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é˜¿æ‹‰ä¼¯æ•°æ®å¤„ç†ä¸Šçš„è¡¨ç°ä»éœ€æ·±å…¥ç ”ç©¶ã€‚</li>
<li>ç²¾å¿ƒé€‰æ‹©çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹å¯ä»¥æ˜¾è‘—æé«˜LLMsåœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ¨ç†ä¸ºé‡ç‚¹çš„DeepSeekæ¨¡å‹åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>DeepSeekæ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºGPT o4-miniåŸºå‡†æµ‹è¯•ã€‚</li>
<li>LoRA-basedå¾®è°ƒå¯ä»¥è¿›ä¸€æ­¥æé«˜LLMsçš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ï¼Œæœ‰åŠ©äºä¼˜åŒ–LLMsåœ¨å¤„ç†å¤æ‚é˜¿æ‹‰ä¼¯è¯­æ•°æ®æ—¶çš„æ€§èƒ½å’Œç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-421341e02bd06b158e6d7a4040f483ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1b4e28a76543540f761275a06b6abda.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7b71d304e3eb728dd55864cbf2e976d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Consistent-Paths-Lead-to-Truth-Self-Rewarding-Reinforcement-Learning-for-LLM-Reasoning"><a href="#Consistent-Paths-Lead-to-Truth-Self-Rewarding-Reinforcement-Learning-for-LLM-Reasoning" class="headerlink" title="Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning   for LLM Reasoning"></a>Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning   for LLM Reasoning</h2><p><strong>Authors:Kongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, Jieping Ye, Mingli Song, Dacheng Tao</strong></p>
<p>Recent advances of Reinforcement Learning (RL) have highlighted its potential in complex reasoning tasks, yet effective training often relies on external supervision, which limits the broader applicability. In this work, we propose a novel self-rewarding reinforcement learning framework to enhance Large Language Model (LLM) reasoning by leveraging the consistency of intermediate reasoning states across different reasoning trajectories. Our key insight is that correct responses often exhibit consistent trajectory patterns in terms of model likelihood: their intermediate reasoning states tend to converge toward their own final answers (high consistency) with minimal deviation toward other candidates (low volatility). Inspired by this observation, we introduce CoVo, an intrinsic reward mechanism that integrates Consistency and Volatility via a robust vector-space aggregation strategy, complemented by a curiosity bonus to promote diverse exploration. CoVo enables LLMs to perform RL in a self-rewarding manner, offering a scalable pathway for learning to reason without external supervision. Extensive experiments on diverse reasoning benchmarks show that CoVo achieves performance comparable to or even surpassing supervised RL. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sastpg/CoVo">https://github.com/sastpg/CoVo</a>. </p>
<blockquote>
<p>æœ€è¿‘å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›å±•å‡¸æ˜¾äº†å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œç„¶è€Œæœ‰æ•ˆçš„è®­ç»ƒé€šå¸¸ä¾èµ–äºå¤–éƒ¨ç›‘ç£ï¼Œè¿™é™åˆ¶äº†å…¶æ›´å¹¿æ³›çš„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªå¥–åŠ±å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ä¸åŒæ¨ç†è½¨è¿¹ä¸­çš„ä¸­é—´çŠ¶æ€ä¸€è‡´æ€§æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œæ­£ç¡®çš„å›ç­”å¾€å¾€è¡¨ç°å‡ºä¸€è‡´çš„è½¨è¿¹æ¨¡å¼ï¼Œä»æ¨¡å‹çš„å¯èƒ½æ€§æ¥çœ‹ï¼šå®ƒä»¬çš„ä¸­é—´æ¨ç†çŠ¶æ€å€¾å‘äºæ”¶æ•›åˆ°å®ƒä»¬è‡ªå·±çš„æœ€ç»ˆç­”æ¡ˆï¼ˆé«˜ä¸€è‡´æ€§ï¼‰ï¼Œå¹¶ä¸”å¯¹å…¶ä»–å€™é€‰ç­”æ¡ˆçš„åå·®æœ€å°ï¼ˆä½æ³¢åŠ¨æ€§ï¼‰ã€‚å—è¿™ä¸€è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoVoï¼Œè¿™æ˜¯ä¸€ç§å†…åœ¨å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡é²æ£’çš„å‘é‡ç©ºé—´èšåˆç­–ç•¥èåˆäº†ä¸€è‡´æ€§å’Œæ³¢åŠ¨æ€§ï¼Œè¾…ä»¥å¥½å¥‡å¿ƒå¥–åŠ±æ¥ä¿ƒè¿›å¤šæ ·åŒ–çš„æ¢ç´¢ã€‚CoVoä½¿LLMèƒ½å¤Ÿä»¥è‡ªå¥–åŠ±çš„æ–¹å¼è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä¸ºæ— éœ€å¤–éƒ¨ç›‘ç£çš„å­¦ä¹ æ¨ç†æä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„è·¯å¾„ã€‚åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCoVoçš„æ€§èƒ½å¯ä¸æœ‰ç›‘ç£çš„å¼ºåŒ–å­¦ä¹ ç›¸åª²ç¾ï¼Œç”šè‡³æ›´èƒœä¸€ç­¹ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sastpg/CoVo%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sastpg/CoVoæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08745v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†æœ‰æ•ˆçš„è®­ç»ƒé€šå¸¸ä¾èµ–äºå¤–éƒ¨ç›‘ç£ï¼Œè¿™é™åˆ¶äº†å…¶æ›´å¹¿æ³›çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªå¥–åŠ±å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ä¸åŒæ¨ç†è½¨è¿¹ä¸­ä¸­é—´æ¨ç†çŠ¶æ€çš„ä¸€è‡´æ€§ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶çš„å…³é”®è§è§£æ˜¯ï¼Œæ­£ç¡®çš„å›ç­”é€šå¸¸å±•ç°å‡ºä¸€è‡´çš„è½¨è¿¹æ¨¡å¼ï¼Œåœ¨æ¨¡å‹å¯èƒ½æ€§æ–¹é¢ï¼Œå®ƒä»¬çš„ä¸­é—´æ¨ç†çŠ¶æ€å¾€å¾€æ”¶æ•›äºå®ƒä»¬è‡ªå·±çš„æœ€ç»ˆç­”æ¡ˆï¼ˆé«˜ä¸€è‡´æ€§ï¼‰å¹¶å°½é‡å‡å°‘å¯¹å…¶ä»–å€™é€‰ç­”æ¡ˆçš„åç¦»ï¼ˆä½æ³¢åŠ¨æ€§ï¼‰ã€‚å—æ­¤è§‚å¯Ÿå¯å‘ï¼Œç ”ç©¶å¼•å…¥äº†CoVoï¼Œä¸€ç§é€šè¿‡ç¨³å¥çš„å‘é‡ç©ºé—´èšåˆç­–ç•¥ç»“åˆä¸€è‡´æ€§å’Œæ³¢åŠ¨æ€§çš„å†…åœ¨å¥–åŠ±æœºåˆ¶ï¼Œè¾…ä»¥å¥½å¥‡å¿ƒå¥–é‡‘ä»¥ä¿ƒè¿›å¤šæ ·åŒ–çš„æ¢ç´¢ã€‚CoVoä½¿LLMèƒ½å¤Ÿä»¥è‡ªå¥–åŠ±çš„æ–¹å¼è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä¸ºæ— éœ€å¤–éƒ¨ç›‘ç£çš„å­¦ä¹ æ¨ç†æä¾›äº†å¯æ‰©å±•çš„é€”å¾„ã€‚åœ¨å¤šæ ·åŒ–çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCoVoçš„æ€§èƒ½å¯ä¸ç›‘ç£å¼ºåŒ–å­¦ä¹ ç›¸æ¯”ï¼Œç”šè‡³å®ç°è¶…è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>æœ‰æ•ˆçš„è®­ç»ƒé€šå¸¸ä¾èµ–äºå¤–éƒ¨ç›‘ç£ï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è‡ªå¥–åŠ±å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ä¸­é—´æ¨ç†çŠ¶æ€çš„ä¸€è‡´æ€§å¢å¼ºLLMæ¨ç†ã€‚</li>
<li>æ­£ç¡®å›ç­”å±•ç°å‡ºä¸€è‡´çš„è½¨è¿¹æ¨¡å¼ï¼Œç»“åˆé«˜ä¸€è‡´æ€§å’Œä½æ³¢åŠ¨æ€§ã€‚</li>
<li>å¼•å…¥CoVoï¼Œç»“åˆä¸€è‡´æ€§å’Œæ³¢åŠ¨æ€§çš„å†…åœ¨å¥–åŠ±æœºåˆ¶ã€‚</li>
<li>CoVoä½¿LLMä»¥è‡ªå¥–åŠ±æ–¹å¼è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå®ç°æ— éœ€å¤–éƒ¨ç›‘ç£çš„å­¦ä¹ æ¨ç†ã€‚</li>
<li>åœ¨å¤šæ ·åŒ–æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒCoVoè¡¨ç°å‡ºä¸ç›‘ç£å¼ºåŒ–å­¦ä¹ ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-642f892f48c841a22abbf1f2fe5a36e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fd37b5cd4d4627a1b975f4f13f9c932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-275e5e78492967afc7541efafa277486.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ClimateViz-A-Benchmark-for-Statistical-Reasoning-and-Fact-Verification-on-Scientific-Charts"><a href="#ClimateViz-A-Benchmark-for-Statistical-Reasoning-and-Fact-Verification-on-Scientific-Charts" class="headerlink" title="ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification   on Scientific Charts"></a>ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification   on Scientific Charts</h2><p><strong>Authors:Ruiran Su, Jiasheng Si, Zhijiang Guo, Janet B. Pierrehumbert</strong></p>
<p>Scientific fact-checking has mostly focused on text and tables, overlooking scientific charts, which are key for presenting quantitative evidence and statistical reasoning. We introduce ClimateViz, the first large-scale benchmark for scientific fact-checking using expert-curated scientific charts. ClimateViz contains 49,862 claims linked to 2,896 visualizations, each labeled as support, refute, or not enough information. To improve interpretability, each example includes structured knowledge graph explanations covering trends, comparisons, and causal relations. We evaluate state-of-the-art multimodal language models, including both proprietary and open-source systems, in zero-shot and few-shot settings. Results show that current models struggle with chart-based reasoning: even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to 77.8 percent accuracy in label-only settings, far below human performance (89.3 and 92.7 percent). Explanation-augmented outputs improve performance in some models. We released our dataset and code alongside the paper. </p>
<blockquote>
<p>ç§‘å­¦äº‹å®æ ¸æŸ¥ä¸»è¦é›†ä¸­äºæ–‡æœ¬å’Œè¡¨æ ¼ï¼Œå¿½è§†äº†ç§‘å­¦å›¾è¡¨ï¼Œè¿™äº›å›¾è¡¨å¯¹äºå‘ˆç°å®šé‡è¯æ®å’Œè¿›è¡Œç»Ÿè®¡åˆ†æè‡³å…³é‡è¦ã€‚æˆ‘ä»¬æ¨å‡ºäº†ClimateVizï¼Œè¿™æ˜¯ä½¿ç”¨ä¸“å®¶åˆ¶ä½œçš„ç§‘å­¦å›¾è¡¨è¿›è¡Œç§‘å­¦äº‹å®æ ¸æŸ¥çš„ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚ClimateVizåŒ…å«ä¸2,896ä¸ªå¯è§†åŒ–å†…å®¹ç›¸å…³çš„49,862ä¸ªå£°æ˜ï¼Œæ¯ä¸ªå£°æ˜éƒ½è¢«æ ‡è®°ä¸ºæ”¯æŒã€åé©³æˆ–ä¿¡æ¯ä¸è¶³ã€‚ä¸ºäº†æé«˜å¯è§£é‡Šæ€§ï¼Œæ¯ä¸ªç¤ºä¾‹éƒ½åŒ…æ‹¬ç»“æ„åŒ–çŸ¥è¯†å›¾è°±è§£é‡Šï¼Œæ¶µç›–è¶‹åŠ¿ã€æ¯”è¾ƒå’Œå› æœå…³ç³»ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤šæ¨¡å¼è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸“æœ‰å’Œå¼€æºç³»ç»Ÿï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨åŸºäºå›¾è¡¨çš„æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼šå³ä½¿æ˜¯æœ€å¥½çš„ç³»ç»Ÿï¼Œå¦‚Gemini 2.5å’ŒInternVL 2.5ï¼Œåœ¨ä»…æ ‡ç­¾è®¾ç½®ä¸‹çš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰76.2%è‡³77.8%ï¼Œè¿œä½äºäººç±»è¡¨ç°ï¼ˆ89.3%å’Œ92.7%ï¼‰ã€‚è§£é‡Šå¢å¼ºè¾“å‡ºå¯æé«˜æŸäº›æ¨¡å‹çš„è¡¨ç°ã€‚æˆ‘ä»¬éšè®ºæ–‡ä¸€èµ·å‘å¸ƒäº†æ•°æ®é›†å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08700v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç§‘å­¦äº‹å®æ ¸æŸ¥é€šå¸¸å¿½ç•¥ç§‘å­¦å›¾è¡¨çš„é‡è¦æ€§ï¼Œè€Œä¸“æ³¨äºæ–‡æœ¬å’Œè¡¨æ ¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« å¼•å…¥äº†ClimateVizï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ç§‘å­¦äº‹å®æ ¸æŸ¥åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸“å®¶ç­–åˆ’çš„ç§‘å­¦å›¾è¡¨ä¸­çš„49ï¼Œ862ä¸ªå£°æ˜ã€‚ClimateVizä¸­çš„æ¯ä¸ªä¾‹å­éƒ½åŒ…æ‹¬ç»“æ„åŒ–çŸ¥è¯†å›¾è°±çš„è§£é‡Šï¼Œæ¶µç›–è¶‹åŠ¿ã€æ¯”è¾ƒå’Œå› æœå…³ç³»ï¼Œä»¥æé«˜å¯è§£é‡Šæ€§ã€‚æ–‡ç« è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤šåª’ä½“è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨å›¾è¡¨æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå³ä½¿æœ€å¥½çš„ç³»ç»Ÿå¦‚Gemini 2.5å’ŒInternVL 2.5åœ¨ä»…æ ‡ç­¾è®¾ç½®ä¸‹çš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰76.2è‡³77.8%ï¼Œè¿œä½äºäººç±»çš„è¡¨ç°ï¼ˆ89.3%å’Œ92.7%ï¼‰ã€‚å¢åŠ è§£é‡Šåçš„è¾“å‡ºå¯æé«˜æŸäº›æ¨¡å‹çš„æ€§èƒ½ã€‚æ–‡ç« åŒæ—¶å…¬å¼€äº†æ•°æ®é›†å’Œä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§‘å­¦äº‹å®æ ¸æŸ¥é€šå¸¸å¿½è§†ç§‘å­¦å›¾è¡¨çš„é‡è¦æ€§ï¼Œè€Œä¸“æ³¨äºæ–‡æœ¬å’Œè¡¨æ ¼ã€‚</li>
<li>ClimateVizæ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„ç§‘å­¦äº‹å®æ ¸æŸ¥åŸºå‡†æµ‹è¯•ï¼Œå¼ºè°ƒå›¾è¡¨åœ¨è¯æ®å‘ˆç°ä¸­çš„ä½œç”¨ã€‚</li>
<li>ClimateVizåŒ…å«ç»“æ„åŒ–çŸ¥è¯†å›¾è°±çš„è§£é‡Šä»¥å¢å¼ºå¯è§£é‡Šæ€§ã€‚</li>
<li>å½“å‰å¤šåª’ä½“è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå³ä½¿æ˜¯æœ€å¥½çš„ç³»ç»Ÿä¹Ÿè¿œä½äºäººç±»è¡¨ç°ã€‚</li>
<li>å¢åŠ è§£é‡Šåçš„è¾“å‡ºå¯ä»¥æé«˜æŸäº›æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ–‡ç« å…¬å¼€äº†æ•°æ®é›†å’Œä»£ç ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91d390e349fedf85f76fc4587c4381d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6368c28cf322b83e3b278bd330873e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14b370b22dae9dedaae604369dd5f8e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ff5ddf4df5725cb1dbb52e8812f57b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da32eeba9056854a8b1b600142fc4a1d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VReST-Enhancing-Reasoning-in-Large-Vision-Language-Models-through-Tree-Search-and-Self-Reward-Mechanism"><a href="#VReST-Enhancing-Reasoning-in-Large-Vision-Language-Models-through-Tree-Search-and-Self-Reward-Mechanism" class="headerlink" title="VReST: Enhancing Reasoning in Large Vision-Language Models through Tree   Search and Self-Reward Mechanism"></a>VReST: Enhancing Reasoning in Large Vision-Language Models through Tree   Search and Self-Reward Mechanism</h2><p><strong>Authors:Congzhi Zhang, Jiawei Peng, Zhenglin Wang, Yilong Lai, Haowen Sun, Heng Chang, Fei Ma, Weijiang Yu</strong></p>
<p>Large Vision-Language Models (LVLMs) have shown exceptional performance in multimodal tasks, but their effectiveness in complex visual reasoning is still constrained, especially when employing Chain-of-Thought prompting techniques. In this paper, we propose VReST, a novel training-free approach that enhances Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms. VReST meticulously traverses the reasoning landscape by establishing a search tree, where each node encapsulates a reasoning step, and each path delineates a comprehensive reasoning sequence. Our innovative multimodal Self-Reward mechanism assesses the quality of reasoning steps by integrating the utility of sub-questions, answer correctness, and the relevance of vision-language clues, all without the need for additional models. VReST surpasses current prompting methods and secures state-of-the-art performance across three multimodal mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy of test-time scaling laws in multimodal tasks, offering a promising direction for future research. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚è§†è§‰æ¨ç†æ–¹é¢çš„æ•ˆæœä»ç„¶å—é™ï¼Œå°¤å…¶æ˜¯åœ¨é‡‡ç”¨æ€ç»´é“¾æç¤ºæŠ€æœ¯æ—¶ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å…è®­ç»ƒæ–¹æ³•VReSTï¼Œé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢å’Œè‡ªæˆ‘å¥–åŠ±æœºåˆ¶å¢å¼ºLVLMsçš„æ¨ç†èƒ½åŠ›ã€‚VReSTé€šè¿‡æ„å»ºæœç´¢æ ‘ç²¾å¿ƒéå†æ¨ç†é¢†åŸŸï¼Œæ¯ä¸ªèŠ‚ç‚¹å°è£…ä¸€ä¸ªæ¨ç†æ­¥éª¤ï¼Œæ¯æ¡è·¯å¾„æè¿°ä¸€ä¸ªå…¨é¢çš„æ¨ç†åºåˆ—ã€‚æˆ‘ä»¬åˆ›æ–°çš„å¤šæ¨¡æ€è‡ªæˆ‘å¥–åŠ±æœºåˆ¶é€šè¿‡æ•´åˆå­é—®é¢˜çš„å®ç”¨æ€§ã€ç­”æ¡ˆçš„æ­£ç¡®æ€§ä»¥åŠè§†è§‰è¯­è¨€çº¿ç´¢çš„ç›¸å…³æ€§æ¥è¯„ä¼°æ¨ç†æ­¥éª¤çš„è´¨é‡ï¼Œæ— éœ€ä½¿ç”¨é¢å¤–çš„æ¨¡å‹ã€‚VReSTè¶…è¶Šäº†å½“å‰çš„æç¤ºæ–¹æ³•ï¼Œåœ¨ä¸‰ä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒè¯å®äº†å¤šæ¨¡æ€ä»»åŠ¡ä¸­æµ‹è¯•æ—¶ç¼©æ”¾å®šå¾‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08691v1">PDF</a> Accepted by ACL 2025 main</p>
<p><strong>Summary</strong><br>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨å¤æ‚è§†è§‰æ¨ç†æ–¹é¢çš„æ•ˆæœä»ç„¶å—é™ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‡ç”¨é“¾å¼æ€ç»´æç¤ºæŠ€æœ¯æ—¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹è®­ç»ƒå¤–æ–¹æ³•VReSTï¼Œå®ƒé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢å’Œè‡ªæˆ‘å¥–åŠ±æœºåˆ¶æé«˜äº†LVLMsçš„æ¨ç†èƒ½åŠ›ã€‚VReSTç²¾å¿ƒéå†æ¨ç†é¢†åŸŸï¼Œå»ºç«‹æœç´¢æ ‘ï¼Œæ¯ä¸ªèŠ‚ç‚¹åŒ…å«ä¸€ä¸ªæ¨ç†æ­¥éª¤ï¼Œæ¯æ¡è·¯å¾„æè¿°äº†ä¸€ä¸ªå…¨é¢çš„æ¨ç†åºåˆ—ã€‚æˆ‘ä»¬çš„åˆ›æ–°å¤šæ¨¡æ€è‡ªæˆ‘å¥–åŠ±æœºåˆ¶é€šè¿‡æ•´åˆå­é—®é¢˜çš„å®ç”¨æ€§ã€ç­”æ¡ˆçš„æ­£ç¡®æ€§ä»¥åŠè§†è§‰è¯­è¨€çº¿ç´¢çš„ç›¸å…³æ€§æ¥è¯„ä¼°æ¨ç†æ­¥éª¤çš„è´¨é‡ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å‹ã€‚VReSTè¶…è¶Šäº†å½“å‰çš„æç¤ºæ–¹æ³•ï¼Œåœ¨ä¸‰ä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒè¯å®äº†æµ‹è¯•æ—¶ç¼©æ”¾å®šå¾‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VReSTæ˜¯ä¸€ç§æ–°å‹çš„ã€è®­ç»ƒå¤–çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤æ‚è§†è§‰æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>VReSTé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢è¿›è¡Œè‡ªæˆ‘éå†ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªæ¨ç†æ­¥éª¤ï¼Œæ•´ä¸ªè·¯å¾„åˆ™ä»£è¡¨å®Œæ•´çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>VReSTå¼•å…¥äº†å¤šæ¨¡æ€è‡ªæˆ‘å¥–åŠ±æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½è¯„ä¼°æ¨ç†æ­¥éª¤çš„è´¨é‡ï¼Œç»¼åˆè€ƒè™‘å­é—®é¢˜çš„å®ç”¨æ€§ã€ç­”æ¡ˆçš„æ­£ç¡®æ€§ä»¥åŠè§†è§‰è¯­è¨€çº¿ç´¢çš„ç›¸å…³æ€§ã€‚</li>
<li>VReSTåœ¨ä¸‰ä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ä¸ç°æœ‰çš„æç¤ºæ–¹æ³•ç›¸æ¯”ï¼ŒVReSTå±•ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚</li>
<li>VReSTå®éªŒè¯å®äº†æµ‹è¯•æ—¶ç¼©æ”¾å®šå¾‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08691">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-29691517c322d9284f5cd32b385f02a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b181178a593f69289ff5b4c549e3ed6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8567f549a01a466b91b84470ffa82952.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RuleReasoner-Reinforced-Rule-based-Reasoning-via-Domain-aware-Dynamic-Sampling"><a href="#RuleReasoner-Reinforced-Rule-based-Reasoning-via-Domain-aware-Dynamic-Sampling" class="headerlink" title="RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic   Sampling"></a>RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic   Sampling</h2><p><strong>Authors:Yang Liu, Jiaqi Li, Zilong Zheng</strong></p>
<p>Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1% average points on eight ID tasks and $\Delta$10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL. </p>
<blockquote>
<p>åŸºäºè§„åˆ™çš„æ¨ç†å·²è¢«è®¤ä¸ºæ˜¯æ¨ç†é¢†åŸŸçš„ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼Œè€Œåœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œè§„åˆ™æ ¼å¼ã€ç±»å‹å’Œå¤æ‚æ€§çš„åå·®å¸¦æ¥äº†ä¸¥é‡çš„æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å…·æœ‰å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”å…¶æ€§èƒ½é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ï¼Œå³å°å‹æ¨ç†æ¨¡å‹ï¼ˆSRMsï¼‰æ˜¯å¦èƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œé¢†åŸŸä¸­æœ‰æ•ˆåœ°å­¦ä¹ åŸºäºè§„åˆ™çš„æ¨ç†å¹¶å…·æœ‰ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¼ºåŒ–è§„åˆ™æ¨ç†ï¼ˆReinforced Rule-based Reasoningï¼‰ï¼Œä¹Ÿç§°ä¸ºRuleReasonerã€‚è¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡å¤§é‡ç²¾é€‰çš„ä»»åŠ¡å’Œä¸€ç§æ–°çš„é¢†åŸŸæ„ŸçŸ¥åŠ¨æ€é‡‡æ ·æ–¹æ³•è¿›è¡ŒåŸºäºè§„åˆ™çš„æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒRuleReasoneré€šè¿‡æ ¹æ®å†å²å¥–åŠ±æ›´æ–°ä¸åŒé¢†åŸŸçš„é‡‡æ ·æƒé‡æ¥é‡æ–°é‡‡æ ·æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ã€‚è¿™ä¿ƒè¿›äº†é¢†åŸŸæ‰©å……å’Œå¼ºåŒ–å­¦ä¹ çš„çµæ´»åœ¨çº¿å­¦ä¹ æ—¶é—´è¡¨ï¼Œé¿å…äº†ç°æœ‰æ–¹æ³•ä¸­ä½¿ç”¨çš„é¢„å…ˆè®¾è®¡å¥½çš„æ··åˆè®­ç»ƒé…æ–¹ã€‚åœ¨å†…éƒ¨æ•°æ®åˆ†å¸ƒï¼ˆIDï¼‰å’Œå¤–éƒ¨æ•°æ®åˆ†å¸ƒï¼ˆOODï¼‰åŸºå‡†ä¸Šçš„ç»éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒRuleReasoneræ˜¾è‘—è¶…è¶Šäº†å‰æ²¿çš„LRMsï¼ˆåœ¨å…«ä¸ªIDä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†Î”4.1%ï¼Œåœ¨ä¸‰ä¸ªOODä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†Î”10.4%ï¼Œè¶…è¿‡äº†OpenAI-o1ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºå…ˆå‰çš„å¼ºåŒ–å­¦ä¹ åŠ¨æ€é‡‡æ ·æ–¹æ³•è¿˜è¡¨ç°å‡ºäº†æ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08672v1">PDF</a> 22 pages, 10 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹è§„åˆ™æ¨ç†é¢†åŸŸçš„ä¸€é¡¹æ–°æŠ€æœ¯â€”â€”Reinforced Rule-based Reasoningï¼ˆRuleReasonerï¼‰ã€‚é’ˆå¯¹å°æ¨ç†æ¨¡å‹åœ¨å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œé¢†åŸŸä¸­æ˜¯å¦èƒ½æœ‰æ•ˆåœ°å­¦ä¹ è§„åˆ™æ¨ç†çš„é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥æ‰§è¡Œè§„åˆ™æ¨ç†ã€‚å®ƒé€šè¿‡å¹¿æ³›çš„æ”¶é›†ç²¾é€‰ä»»åŠ¡å’Œä¸€ç§æ–°çš„é¢†åŸŸæ„ŸçŸ¥åŠ¨æ€é‡‡æ ·æ–¹æ³•æ¥å®ç°ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®å†å²å¥–åŠ±åŠ¨æ€è°ƒæ•´ä¸åŒé¢†åŸŸçš„é‡‡æ ·æƒé‡ï¼Œç®€åŒ–äº†RLçš„é¢„è®­ç»ƒæ··åˆè®­ç»ƒé…æ–¹éœ€æ±‚ã€‚åœ¨å†…éƒ¨å’Œå¤–éƒ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRuleReasoneræ˜¾è‘—ä¼˜äºå‰æ²¿çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼Œå¹¶è¡¨ç°å‡ºæ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–è§„åˆ™æ¨ç†ï¼ˆReinforced Rule-based Reasoningï¼‰æ˜¯ä¸€ç§è§£å†³å°æ¨ç†æ¨¡å‹ï¼ˆSRMsï¼‰åœ¨å¤šæ ·åŒ–ä»»åŠ¡å’Œé¢†åŸŸä¸­å­¦ä¹ è§„åˆ™æ¨ç†çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>RuleReasoneré€šè¿‡å¹¿æ³›çš„æ”¶é›†ç²¾é€‰ä»»åŠ¡å’Œé¢†åŸŸæ„ŸçŸ¥åŠ¨æ€é‡‡æ ·æ–¹æ³•å®ç°è§„åˆ™æ¨ç†ã€‚</li>
<li>åŠ¨æ€é‡‡æ ·æ–¹æ³•èƒ½å¤Ÿæ ¹æ®å†å²å¥–åŠ±è°ƒæ•´é‡‡æ ·æƒé‡ï¼Œç®€åŒ–äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>RuleReasoneråœ¨å†…éƒ¨å’Œå¤–éƒ¨åŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ã€‚</li>
<li>RuleReasonerå¯¹OpenAI-o1çš„å…«ä¸ªå†…éƒ¨ä»»åŠ¡å¹³å‡æé«˜äº†4.1%çš„å¾—åˆ†ï¼Œå¯¹ä¸‰ä¸ªå¤–éƒ¨ä»»åŠ¡å¹³å‡æé«˜äº†10.4%çš„å¾—åˆ†ã€‚</li>
<li>ä¸ç°æœ‰çš„åŠ¨æ€é‡‡æ ·æ–¹æ³•ç›¸æ¯”ï¼ŒRuleReasonerå±•ç°å‡ºæ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-55cf19956af6c21a20753679c359405a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09f964c4738a760da4766bf44be09cf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d7c08fd8ea51610f98d6584ed8abf5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca1914f431b593786b2e21ca913c96e6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enhancing-Reasoning-Capabilities-of-Small-Language-Models-with-Blueprints-and-Prompt-Template-Search"><a href="#Enhancing-Reasoning-Capabilities-of-Small-Language-Models-with-Blueprints-and-Prompt-Template-Search" class="headerlink" title="Enhancing Reasoning Capabilities of Small Language Models with   Blueprints and Prompt Template Search"></a>Enhancing Reasoning Capabilities of Small Language Models with   Blueprints and Prompt Template Search</h2><p><strong>Authors:Dongge Han, Menglin Xia, Daniel Madrigal Diaz, Samuel Kessler, Ankur Mallick, Xuchao Zhang, Mirian Del Carmen Hipolito Garcia, Jin Xu, Victor RÃ¼hle, Saravan Rajmohan</strong></p>
<p>Small language models (SLMs) offer promising and efficient alternatives to large language models (LLMs). However, SLMsâ€™ limited capacity restricts their reasoning capabilities and makes them sensitive to prompt variations. To address these challenges, we propose a novel framework that enhances SLM reasoning capabilities through LLM generated blueprints. The blueprints provide structured, high-level reasoning guides that help SLMs systematically tackle related problems. Furthermore, our framework integrates a prompt template search mechanism to mitigate the SLMsâ€™ sensitivity to prompt variations. Our framework demonstrates improved SLM performance across various tasks, including math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves the reasoning capabilities of SLMs without increasing model size or requiring additional training, offering a lightweight and deployment-friendly solution for on-device or resource-constrained environments. </p>
<blockquote>
<p>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›äº†æœ‰å‰æ™¯å’Œé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒSLMçš„èƒ½åŠ›æœ‰é™ï¼Œé™åˆ¶äº†å…¶æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¯¹æç¤ºå˜åŒ–æ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡LLMç”Ÿæˆçš„è“å›¾å¢å¼ºSLMçš„æ¨ç†èƒ½åŠ›ã€‚è¿™äº›è“å›¾æä¾›ç»“æ„åŒ–ã€é«˜çº§åˆ«çš„æ¨ç†æŒ‡å—ï¼Œå¸®åŠ©SLMç³»ç»Ÿåœ°è§£å†³ç›¸å…³é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†ä¸€ç§æç¤ºæ¨¡æ¿æœç´¢æœºåˆ¶ï¼Œä»¥å‡è½»SLMå¯¹æç¤ºå˜åŒ–çš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç¤ºäº†æ”¹è¿›çš„SLMæ€§èƒ½ï¼ŒåŒ…æ‹¬æ•°å­¦ï¼ˆGSM8Kï¼‰ã€ç¼–ç ï¼ˆMBPPï¼‰å’Œé€»è¾‘æ¨ç†ï¼ˆBBHï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†SLMçš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€å¢åŠ æ¨¡å‹å¤§å°æˆ–è¿›è¡Œé¢å¤–çš„è®­ç»ƒï¼Œä¸ºè®¾å¤‡ç«¯æˆ–èµ„æºå—é™çš„ç¯å¢ƒæä¾›äº†è½»ä¾¿ä¸”æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08669v1">PDF</a> TTODLer-FM Workshop@ICMLâ€™25 (Tiny Titans: The next wave of On-Device   Learning for Foundational Models)</p>
<p><strong>Summary</strong></p>
<p>å°è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä½œä¸ºä¸€ç§å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå±•ç°å‡ºå…¶æ½œåŠ›ä¸é«˜æ•ˆæ€§ã€‚ç„¶è€Œï¼ŒSLMså—é™äºå…¶èƒ½åŠ›ï¼Œå­˜åœ¨æ¨ç†èƒ½åŠ›ä¸è¶³å’Œå¯¹æç¤ºå˜åŒ–æ•æ„Ÿçš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨LLMç”Ÿæˆçš„è“å›¾æ¥æå‡SLMçš„æ¨ç†èƒ½åŠ›ã€‚è¿™äº›è“å›¾ä¸ºSLMsæä¾›ç»“æ„åŒ–ã€é«˜çº§åˆ«çš„æ¨ç†æŒ‡å—ï¼Œå¸®åŠ©å®ƒä»¬ç³»ç»Ÿåœ°è§£å†³ç›¸å…³é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¿˜æ•´åˆäº†æç¤ºæ¨¡æ¿æœç´¢æœºåˆ¶ï¼Œä»¥é™ä½SLMså¯¹æç¤ºå˜åŒ–çš„æ•æ„Ÿæ€§ã€‚åœ¨å„é¡¹ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å±•ç°å‡ºå¯¹SLMæ€§èƒ½çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬æ•°å­¦ï¼ˆGSM8Kï¼‰ã€ç¼–ç¨‹ï¼ˆMBPPï¼‰å’Œé€»è¾‘æ¨ç†ï¼ˆBBHï¼‰ç­‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸å¢åŠ æ¨¡å‹è§„æ¨¡æˆ–ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æå‡äº†SLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä¸ºè®¾å¤‡ç«¯æˆ–èµ„æºå—é™ç¯å¢ƒæä¾›äº†è½»ä¾¿ä¸”æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å±•ç°å‡ºå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ›¿ä»£æ½œåŠ›ï¼Œå¹¶å…·å¤‡é«˜æ•ˆæ€§ã€‚</li>
<li>SLMå—é™äºå…¶æ¨ç†èƒ½åŠ›ä»¥åŠå¯¹æç¤ºå˜åŒ–çš„æ•æ„Ÿæ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨LLMç”Ÿæˆçš„è“å›¾å¢å¼ºSLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è“å›¾ä¸ºSLMsæä¾›ç»“æ„åŒ–ã€é«˜çº§åˆ«çš„æ¨ç†æŒ‡å—ï¼Œå¸®åŠ©è§£å†³ç›¸å…³é—®é¢˜ã€‚</li>
<li>æ•´åˆæç¤ºæ¨¡æ¿æœç´¢æœºåˆ¶ä»¥é™ä½SLMså¯¹æç¤ºå˜åŒ–çš„æ•æ„Ÿæ€§ã€‚</li>
<li>åœ¨æ•°å­¦ã€ç¼–ç¨‹å’Œé€»è¾‘æ¨ç†ç­‰ä»»åŠ¡ä¸­ï¼Œæ–°æ¡†æ¶æå‡äº†SLMçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-97431bd4312e91478c2752646a0dc2a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2973fa04c8826c8dc0a3e6caa416b90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-742f34c5e4445b73e6193416739a55c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-332db9fbe1cf38dd54f3d55ed66fbcf2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e59bcf97c6f75c78d41237c79d6235ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90bfc13eb75fc1a44cdeecd522acacd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7634da8fdde5ce9e58353d030511e940.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RAISE-Enhancing-Scientific-Reasoning-in-LLMs-via-Step-by-Step-Retrieval"><a href="#RAISE-Enhancing-Scientific-Reasoning-in-LLMs-via-Step-by-Step-Retrieval" class="headerlink" title="RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval"></a>RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval</h2><p><strong>Authors:Minhae Oh, Jeonghye Kim, Nakyung Lee, Donggeon Seo, Taeuk Kim, Jungwoo Lee</strong></p>
<p>Scientific reasoning requires not only long-chain reasoning processes, but also knowledge of domain-specific terminologies and adaptation to updated findings. To deal with these challenges for scientific reasoning, we introduce RAISE, a step-by-step retrieval-augmented framework which retrieves logically relevant documents from in-the-wild corpus. RAISE is divided into three steps: problem decomposition, logical query generation, and logical retrieval. We observe that RAISE consistently outperforms other baselines on scientific reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves documents that are not only similar in terms of the domain knowledge, but also documents logically more relevant. </p>
<blockquote>
<p>ç§‘å­¦æ¨ç†ä¸ä»…éœ€è¦é•¿é“¾æ¨ç†è¿‡ç¨‹ï¼Œè¿˜éœ€è¦å¯¹ç‰¹å®šé¢†åŸŸçš„æœ¯è¯­æœ‰äº†è§£ï¼Œå¹¶é€‚åº”æœ€æ–°çš„ç ”ç©¶æˆæœã€‚ä¸ºäº†åº”å¯¹ç§‘å­¦æ¨ç†çš„è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RAISEï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†æ­¥éª¤çš„æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œå¯ä»¥ä»é‡ç”Ÿè¯­æ–™åº“ä¸­æ£€ç´¢å‡ºé€»è¾‘ç›¸å…³çš„æ–‡æ¡£ã€‚RAISEåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šé—®é¢˜åˆ†è§£ã€é€»è¾‘æŸ¥è¯¢ç”Ÿæˆå’Œé€»è¾‘æ£€ç´¢ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ç§‘å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAISEçš„æ€§èƒ½å§‹ç»ˆä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬åˆ†æè®¤ä¸ºï¼Œä¸å…¶ä»–åŸºçº¿æ–¹æ³•ä¸åŒï¼ŒRAISEæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ä»…å…·æœ‰ç›¸ä¼¼çš„é¢†åŸŸçŸ¥è¯†ï¼Œè€Œä¸”åœ¨é€»è¾‘ä¸Šæ›´åŠ ç›¸å…³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08625v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RAISEè¿™ä¸€é€æ­¥å¢å¼ºçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³ç§‘å­¦æ¨ç†ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬é—®é¢˜åˆ†è§£ã€é€»è¾‘æŸ¥è¯¢ç”Ÿæˆå’Œé€»è¾‘æ£€ç´¢ä¸‰ä¸ªæ­¥éª¤ã€‚RAISEèƒ½å¤Ÿä»é‡å¤–è¯­æ–™åº“ä¸­æ£€ç´¢å‡ºä¸ç§‘å­¦æ¨ç†ç›¸å…³çš„é€»è¾‘æ–‡æ¡£ï¼Œå¹¶ä¸”åœ¨ç§‘å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼ŒRAISEä¸ä»…èƒ½å¤Ÿæ£€ç´¢åˆ°é¢†åŸŸçŸ¥è¯†ç›¸ä¼¼çš„æ–‡æ¡£ï¼Œè¿˜èƒ½æ£€ç´¢åˆ°é€»è¾‘ä¸Šæ›´ä¸ºç›¸å…³çš„æ–‡æ¡£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAISEæ˜¯ä¸€ä¸ªç”¨äºè§£å†³ç§‘å­¦æ¨ç†æŒ‘æˆ˜çš„é€æ­¥å¢å¼ºæ¡†æ¶ã€‚</li>
<li>RAISEåŒ…æ‹¬é—®é¢˜åˆ†è§£ã€é€»è¾‘æŸ¥è¯¢ç”Ÿæˆå’Œé€»è¾‘æ£€ç´¢ä¸‰ä¸ªä¸»è¦æ­¥éª¤ã€‚</li>
<li>RAISEèƒ½å¤Ÿä»é‡å¤–è¯­æ–™åº“ä¸­æ£€ç´¢å‡ºä¸ç§‘å­¦æ¨ç†ç›¸å…³çš„é€»è¾‘æ–‡æ¡£ã€‚</li>
<li>RAISEåœ¨ç§‘å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸å…¶ä»–åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒRAISEä¸ä»…èƒ½æ£€ç´¢åˆ°é¢†åŸŸçŸ¥è¯†ç›¸ä¼¼çš„æ–‡æ¡£ï¼Œè¿˜èƒ½æ‰¾åˆ°é€»è¾‘ä¸Šæ›´ç›¸å…³çš„æ–‡æ¡£ã€‚</li>
<li>ç§‘å­¦æ¨ç†ä¸ä»…éœ€è¦é•¿é“¾æ¨ç†è¿‡ç¨‹ï¼Œè¿˜éœ€è¦å¯¹ç‰¹å®šé¢†åŸŸçš„æœ¯è¯­çŸ¥è¯†çš„äº†è§£ä»¥åŠé€‚åº”æœ€æ–°çš„å‘ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cfdd7e08d2ba9866f1fe648a7f301565.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23b728989b065a68657899cf0b15678c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0009f90c2fc59ce1f33794b5a72058f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-098db76624996e59f4b6c455359ef89e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a688c1cdd3a64e53a65ecb8061c8c04.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RE-oriented-Model-Development-with-LLM-Support-and-Deduction-based-Verification"><a href="#RE-oriented-Model-Development-with-LLM-Support-and-Deduction-based-Verification" class="headerlink" title="RE-oriented Model Development with LLM Support and Deduction-based   Verification"></a>RE-oriented Model Development with LLM Support and Deduction-based   Verification</h2><p><strong>Authors:Radoslaw Klimek</strong></p>
<p>The requirements engineering (RE) phase is pivotal in developing high-quality software. Integrating advanced modelling techniques with large language models (LLMs) and formal verification in a logical style can significantly enhance this process. We propose a comprehensive framework that focuses on specific Unified Modelling Language (UML) diagrams for preliminary system development. This framework offers visualisations at various modelling stages and seamlessly integrates large language models and logical reasoning engines. The behavioural models generated with the assistance of LLMs are automatically translated into formal logical specifications. Deductive formal verification ensures that logical requirements and interrelations between software artefacts are thoroughly addressed. Ultimately, the framework facilitates the automatic generation of program skeletons, streamlining the transition from design to implementation. </p>
<blockquote>
<p>éœ€æ±‚å·¥ç¨‹ï¼ˆREï¼‰é˜¶æ®µåœ¨å¼€å‘é«˜è´¨é‡è½¯ä»¶è¿‡ç¨‹ä¸­è‡³å…³é‡è¦ã€‚é€šè¿‡é›†æˆå…ˆè¿›çš„å»ºæ¨¡æŠ€æœ¯ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œé€»è¾‘é£æ ¼çš„æ­£å¼éªŒè¯ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºæ­¤è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œä¸“æ³¨äºåˆæ­¥ç³»ç»Ÿå¼€å‘çš„ç‰¹å®šç»Ÿä¸€å»ºæ¨¡è¯­è¨€ï¼ˆUMLï¼‰å›¾è¡¨ã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸åŒå»ºæ¨¡é˜¶æ®µçš„å¯è§†åŒ–ï¼Œæ— ç¼é›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œé€»è¾‘æ¨ç†å¼•æ“ã€‚å€ŸåŠ©LLMç”Ÿæˆçš„è¡Œä¸ºæ¨¡å‹ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºæ­£å¼çš„é€»è¾‘è§„èŒƒã€‚æ¼”ç»å¼æ­£å¼éªŒè¯ç¡®ä¿é€»è¾‘éœ€æ±‚å’Œè½¯ä»¶å·¥ä»¶ä¹‹é—´çš„ç›¸äº’ä½œç”¨å¾—åˆ°å…¨é¢è§£å†³ã€‚æœ€ç»ˆï¼Œè¯¥æ¡†æ¶ä¿ƒè¿›äº†ç¨‹åºéª¨æ¶çš„è‡ªåŠ¨ç”Ÿæˆï¼Œç®€åŒ–äº†ä»è®¾è®¡åˆ°å®æ–½çš„è¿‡æ¸¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08606v1">PDF</a> The paper has been peer-reviewed and accepted for publication to the   1st International Workshop on Artificial Intelligence for Integrated   Development Environments (AI-IDE) of the 33rd ACM Symposium on the   Foundations of Software Engineering (FSE â€˜25), June 23â€“27, 2025, Trondheim,   Norway</p>
<p><strong>Summary</strong></p>
<p>åœ¨è½¯ä»¶å¼€å‘ä¸­ï¼Œéœ€æ±‚å·¥ç¨‹ï¼ˆREï¼‰é˜¶æ®µè‡³å…³é‡è¦ã€‚é€šè¿‡é›†æˆå…ˆè¿›çš„å»ºæ¨¡æŠ€æœ¯ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œé€»è¾‘é£æ ¼çš„æ­£å¼éªŒè¯ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºæ­¤è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œä¾§é‡äºåˆæ­¥ç³»ç»Ÿå¼€å‘çš„ç‰¹å®šç»Ÿä¸€å»ºæ¨¡è¯­è¨€ï¼ˆUMLï¼‰å›¾ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªå»ºæ¨¡é˜¶æ®µæä¾›å¯è§†åŒ–ï¼Œæ— ç¼é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œé€»è¾‘æ¨ç†å¼•æ“ã€‚å€ŸåŠ©LLMç”Ÿæˆçš„è¡Œä¸ºæ¨¡å‹è‡ªåŠ¨ç¿»è¯‘ä¸ºæ­£å¼çš„é€»è¾‘è§„èŒƒã€‚æ¼”ç»å¼æ­£å¼éªŒè¯ç¡®ä¿å…¨é¢è§£å†³é€»è¾‘è¦æ±‚å’Œè½¯ä»¶å·¥ä»¶ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚æœ€ç»ˆï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆç¨‹åºéª¨æ¶ï¼Œä½¿è®¾è®¡åˆ°å®æ–½çš„è¿‡æ¸¡æ›´åŠ é¡ºç•…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éœ€æ±‚å·¥ç¨‹ï¼ˆREï¼‰åœ¨è½¯ä»¶å¼€å‘ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>å…ˆè¿›å»ºæ¨¡æŠ€æœ¯ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œé€»è¾‘éªŒè¯çš„é›†æˆèƒ½æ˜¾è‘—å¢å¼ºREè¿‡ç¨‹ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶åˆ©ç”¨UMLå›¾è¿›è¡Œåˆæ­¥ç³»ç»Ÿå¼€å‘ï¼Œå¹¶åœ¨ä¸åŒå»ºæ¨¡é˜¶æ®µæä¾›å¯è§†åŒ–ã€‚</li>
<li>æ¡†æ¶æ— ç¼é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œé€»è¾‘æ¨ç†å¼•æ“ã€‚</li>
<li>è¡Œä¸ºæ¨¡å‹å¯è‡ªåŠ¨è½¬æ¢ä¸ºæ­£å¼çš„é€»è¾‘è§„èŒƒã€‚</li>
<li>æ¼”ç»å¼æ­£å¼éªŒè¯ç¡®ä¿é€»è¾‘éœ€æ±‚å’Œè½¯ä»¶å·¥ä»¶é—´çš„ç›¸äº’ä½œç”¨å¾—åˆ°å……åˆ†è§£å†³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5c6468cdbebafa52338981b849758ec3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71e1851dc214946ef758af51ce82561e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SLEEPYLAND-trust-begins-with-fair-evaluation-of-automatic-sleep-staging-models"><a href="#SLEEPYLAND-trust-begins-with-fair-evaluation-of-automatic-sleep-staging-models" class="headerlink" title="SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging   models"></a>SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging   models</h2><p><strong>Authors:Alvise Dei Rossi, Matteo Metaldi, Michal Bechny, Irina Filchenko, Julia van der Meer, Markus H. Schmidt, Claudio L. A. Bassetti, Athina Tzovara, Francesca D. Faraci, Luigi Fiorillo</strong></p>
<p>Despite advances in deep learning for automatic sleep staging, clinical adoption remains limited due to challenges in fair model evaluation, generalization across diverse datasets, model bias, and variability in human annotations. We present SLEEPYLAND, an open-source sleep staging evaluation framework designed to address these barriers. It includes more than 22â€™0000 hours in-domain (ID) sleep recordings, and more than 84â€™000 hours out-of-domain (OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders, and hardware setups. We release pre-trained models based on high-performing SoA architectures and evaluate them under standardized conditions across single- and multi-channel EEG&#x2F;EOG configurations. We introduce SOMNUS, an ensemble combining models across architectures and channel setups via soft voting. SOMNUS achieves robust performance across twenty-four different datasets, with macro-F1 scores between 68.7% and 87.2%, outperforming individual models in 94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including cases where compared models were trained ID while SOMNUS treated the same data as OOD. Using a subset of the BSWR (N&#x3D;6â€™633), we quantify model biases linked to age, gender, AHI, and PLMI, showing that while ensemble improves robustness, no model architecture consistently minimizes bias in performance and clinical markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H, DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus than any individual expert (k &#x3D; 0.89&#x2F;0.85 and ACS &#x3D; 0.95&#x2F;0.94 for healthy&#x2F;OSA cohorts). Finally, we introduce ensemble disagreement metrics - entropy and inter-model divergence based - predicting regions of scorer disagreement with ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty. </p>
<blockquote>
<p>å°½ç®¡æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨ç¡çœ åˆ†æœŸæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç”±äºæ¨¡å‹å…¬å¹³è¯„ä¼°ã€è·¨ä¸åŒæ•°æ®é›†æ¨å¹¿ã€æ¨¡å‹åè§å’Œäººä¸ºæ³¨é‡Šå·®å¼‚ç­‰æŒ‘æˆ˜ï¼Œå…¶åœ¨ä¸´åºŠé‡‡çº³æ–¹é¢ä»ç„¶å—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†SLEEPYLANDï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³è¿™äº›éšœç¢çš„å¼€æºç¡çœ åˆ†æœŸè¯„ä¼°æ¡†æ¶ã€‚å®ƒåŒ…å«è¶…è¿‡22ä¸‡å°æ—¶é¢†åŸŸå†…ï¼ˆIDï¼‰ç¡çœ è®°å½•ï¼Œä»¥åŠè¶…è¿‡8.4ä¸‡å°æ—¶é¢†åŸŸå¤–ï¼ˆOODï¼‰ç¡çœ è®°å½•ï¼Œæ¶µç›–å¹¿æ³›å¹´é¾„æ®µã€ç¡çœ éšœç¢å’Œç¡¬ä»¶è®¾ç½®ã€‚æˆ‘ä»¬åŸºäºé«˜æ€§èƒ½æœ€æ–°æ¶æ„å‘å¸ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨å•ä¸€å’Œå¤šé€šé“è„‘ç”µå›¾&#x2F;çœ¼ç”µå›¾é…ç½®çš„æ ‡å‡†æ¡ä»¶ä¸‹å¯¹å®ƒä»¬è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬å¼•å…¥äº†SOMNUSï¼Œä¸€ç§é€šè¿‡è½¯æŠ•ç¥¨ç»“åˆä¸åŒæ¶æ„å’Œé¢‘é“è®¾ç½®çš„æ¨¡å‹é›†åˆã€‚SOMNUSåœ¨äºŒåå››ç§ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°ç¨³å¥ï¼Œå®è§‚F1åˆ†æ•°ä»‹äº68.7%å’Œ87.2%ä¹‹é—´ï¼Œåœ¨94.9%çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜äºå•ä¸ªæ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨å°†å¯¹æ¯”æ¨¡å‹è§†ä¸ºé¢†åŸŸå†…æ•°æ®è€ŒSOMNUSå°†ç›¸åŒæ•°æ®è§†ä¸ºé¢†åŸŸå¤–æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒSOMNUSä¹Ÿè¶…è¿‡äº†ä¹‹å‰çš„æœ€æ–°æ–¹æ³•ã€‚ä½¿ç”¨BSWRå­é›†ï¼ˆN&#x3D;6633ï¼‰ï¼Œæˆ‘ä»¬é‡åŒ–äº†ä¸å¹´é¾„ã€æ€§åˆ«ã€AHIå’ŒPLMIç›¸å…³çš„æ¨¡å‹åè§ï¼Œç»“æœè¡¨æ˜ï¼Œå°½ç®¡é›†åˆæé«˜äº†ç¨³å¥æ€§ï¼Œä½†æ²¡æœ‰æ¨¡å‹æ¶æ„å§‹ç»ˆå¦‚ä¸€åœ°å‡å°‘åè§å’Œä¸´åºŠæ ‡è®°ä¼°è®¡çš„åå·®ã€‚åœ¨é¢†åŸŸå¤–å¤šæ³¨é‡Šæ•°æ®é›†ï¼ˆDOD-Hï¼ŒDOD-Oï¼‰çš„è¯„ä¼°ä¸­ï¼ŒSOMNUSçš„è¡¨ç°è¶…è¿‡äº†æœ€ä½³äººç±»è¯„åˆ†è€…ï¼ˆå³åœ¨DOD-Hä¸Šçš„MF1ä¸º85.2%å¯¹æ¯”80.8%ï¼Œä»¥åŠåœ¨DOD-Oä¸Šçš„MF1ä¸º80.2%å¯¹æ¯”75.9%ï¼‰ï¼Œæ›´å‡†ç¡®åœ°å¤åˆ¶äº†è¯„åˆ†å…±è¯†ç›¸æ¯”ä»»ä½•ä¸ªäººä¸“å®¶ï¼ˆå¥åº·ç»„&#x2F;OSAç»„çš„kå€¼ä¸º0.89&#x2F;0.85å’ŒACSå€¼ä¸º0.95&#x2F;0.94ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºé›†åˆä¸ä¸€è‡´æ€§åº¦é‡çš„é¢„æµ‹åŒºåŸŸè¯„åˆ†è€…åˆ†æ­§çš„æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ç†µå’Œæ¨¡å‹é—´å‘æ•£åº¦ç­‰æŒ‡æ ‡ï¼ŒROC AUCé«˜è¾¾0.828ï¼Œä¸ºäººä¸ºä¸ç¡®å®šæ€§æä¾›äº†æ•°æ®é©±åŠ¨ä»£ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08574v1">PDF</a> 41 pages, 4 Figures, 7 Tables</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†SLEEPYLANDè¿™ä¸€ç¡çœ åˆ†æœŸè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æŒ‘æˆ˜ã€‚æ¡†æ¶åŒ…å«å¤§é‡ç¡çœ è®°å½•æ•°æ®ï¼Œå¹¶åŸºäºé«˜æ€§èƒ½æ¶æ„å‘å¸ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†SOMNUSæ¨¡å‹ç»„åˆæŠ€æœ¯ï¼Œé€šè¿‡è½¯æŠ•ç¥¨æ–¹å¼å®ç°ç¨³å¥æ€§èƒ½ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒSOMNUSåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå•ä¸€æ¨¡å‹åŠå…ˆå‰æœ€ä½³æ–¹æ³•ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†æ¨¡å‹ä¸ä¸´åºŠæŒ‡æ ‡ä¼°è®¡çš„åè§é—®é¢˜ï¼Œå¹¶å¼•å…¥æ¨¡å‹ç»„åˆåˆ†æ­§åº¦é‡æŒ‡æ ‡é¢„æµ‹è¯„åˆ†è€…åˆ†æ­§åŒºåŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLEEPYLANDæ˜¯ä¸€ä¸ªå¼€æºçš„ç¡çœ åˆ†æœŸè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å…¬å¹³æ¨¡å‹è¯„ä¼°ã€è·¨ä¸åŒæ•°æ®é›†æ³›åŒ–ç­‰ã€‚</li>
<li>æ¡†æ¶åŒ…å«å¤§é‡ç¡çœ è®°å½•æ•°æ®ï¼Œæ—¢åŒ…æ‹¬in-domainï¼ˆIDï¼‰æ•°æ®ä¹ŸåŒ…æ‹¬out-of-domainï¼ˆOODï¼‰æ•°æ®ï¼Œå¹¶åŸºäºé«˜æ€§èƒ½æ¶æ„å‘å¸ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>å¼•å…¥SOMNUSæ¨¡å‹ç»„åˆæŠ€æœ¯ï¼Œé€šè¿‡è½¯æŠ•ç¥¨æ–¹å¼ç»“åˆä¸åŒæ¶æ„å’Œé€šé“è®¾ç½®çš„æ¨¡å‹ï¼Œå®ç°ç¨³å¥æ€§èƒ½ã€‚</li>
<li>SOMNUSåœ¨å¤šä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå•ä¸€æ¨¡å‹åŠå…ˆå‰æœ€ä½³æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤„ç†è®­ç»ƒæ•°æ®ä½œä¸ºOODçš„æƒ…å†µã€‚</li>
<li>ç ”ç©¶å‘ç°æ¨¡å‹ä¸å¹´é¾„ã€æ€§åˆ«ã€AHIå’ŒPLMIç­‰ä¸´åºŠæŒ‡æ ‡ä¼°è®¡å­˜åœ¨åè§ï¼Œè€Œæ¨¡å‹ç»„åˆèƒ½æé«˜ç¨³å¥æ€§ï¼Œä½†æ— æ³•ä¸€è‡´å‡å°‘åè§ã€‚</li>
<li>åœ¨å¤šæ³¨è§£æ•°æ®é›†ä¸Šçš„è¯„ä»·æ˜¾ç¤ºï¼ŒSOMNUSçš„è¡¨ç°ä¼˜äºæœ€ä½³äººç±»è¯„åˆ†è€…ï¼Œæ›´èƒ½åæ˜ è¯„åˆ†è€…å…±è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6297971ca0bd5d400bb1608fbb6828d8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DeepForm-Reasoning-Large-Language-Model-for-Communication-System-Formulation"><a href="#DeepForm-Reasoning-Large-Language-Model-for-Communication-System-Formulation" class="headerlink" title="DeepForm: Reasoning Large Language Model for Communication System   Formulation"></a>DeepForm: Reasoning Large Language Model for Communication System   Formulation</h2><p><strong>Authors:Panlong Wu, Ting Wang, Yifei Zhong, Haoqi Zhang, Zitong Wang, Fangxin Wang</strong></p>
<p>Communication system formulation is critical for advancing 6G and future wireless technologies, yet it remains a complex, expertise-intensive task. While Large Language Models (LLMs) offer potential, existing general-purpose models often lack the specialized domain knowledge, nuanced reasoning capabilities, and access to high-quality, domain-specific training data required for adapting a general LLM into an LLM specially for communication system formulation. To bridge this gap, we introduce DeepForm, the first reasoning LLM specially for automated communication system formulation. We propose the world-first large-scale, open-source dataset meticulously curated for this domain called Communication System Formulation Reasoning Corpus (CSFRC). Our framework employs a two-stage training strategy: first, Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge; second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated reasoning patterns like self-correction and verification. Extensive experiments demonstrate that our model achieves state-of-the-art performance, significantly outperforming larger proprietary LLMs on diverse senerios. We will release related resources to foster further research in this area after the paper is accepted. </p>
<blockquote>
<p>é€šä¿¡ç³»ç»Ÿæ„å»ºå¯¹äºæ¨åŠ¨6Gå’Œæœªæ¥æ— çº¿æŠ€æœ¯çš„å‘å±•è‡³å…³é‡è¦ï¼Œç„¶è€Œå®ƒä»ç„¶æ˜¯ä¸€é¡¹å¤æ‚ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„ä»»åŠ¡ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†æ½œåŠ›ï¼Œä½†ç°æœ‰çš„é€šç”¨æ¨¡å‹é€šå¸¸ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€å¾®å¦™çš„æ¨ç†èƒ½åŠ›ä»¥åŠè®¿é—®é«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸçš„è®­ç»ƒæ•°æ®ï¼Œè¿™äº›å‡æ˜¯é€‚åº”é€šç”¨LLMä¸ºä¸“é—¨ç”¨äºé€šä¿¡ç³»ç»Ÿæ„å»ºçš„LLMæ‰€å¿…éœ€çš„è¦æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeepFormï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè‡ªåŠ¨åŒ–é€šä¿¡ç³»ç»Ÿæ„å»ºçš„ç‰¹æ®Šæ¨ç†LLMã€‚æˆ‘ä»¬æå‡ºäº†ä¸–ç•Œä¸Šç¬¬ä¸€ä¸ªé’ˆå¯¹è¯¥é¢†åŸŸç²¾å¿ƒç­–åˆ’çš„å¤§è§„æ¨¡å¼€æºæ•°æ®é›†ï¼Œç§°ä¸ºé€šä¿¡ç³»ç»Ÿæ„å»ºæ¨ç†è¯­æ–™åº“ï¼ˆCSFRCï¼‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆï¼Œä½¿ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä»¥æç‚¼é¢†åŸŸçŸ¥è¯†ï¼›å…¶æ¬¡ï¼ŒåŸºäºReMaxçš„æ–°å‹åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•C-ReMaxï¼Œä»¥åŸ¹å…»é«˜çº§å»ºæ¨¡èƒ½åŠ›å¹¶æ¿€å‘è‡ªæˆ‘ä¿®æ­£å’ŒéªŒè¯ç­‰å¤æ‚æ¨ç†æ¨¡å¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å¤šç§åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºæ›´å¤§çš„ä¸“æœ‰LLMã€‚è®ºæ–‡è¢«æ¥å—åï¼Œæˆ‘ä»¬å°†å‘å¸ƒç›¸å…³èµ„æºä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08551v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é€šä¿¡ç³»ç»Ÿçš„æ„å»ºå¯¹äºæ¨åŠ¨6Gå’Œæœªæ¥æ— çº¿æŠ€æœ¯çš„å‘å±•è‡³å…³é‡è¦ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šä¿¡ç³»ç»Ÿè®¾è®¡é¢†åŸŸçš„ä¸è¶³ï¼Œæå‡ºäº†DeepFormï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè‡ªåŠ¨åŒ–é€šä¿¡ç³»ç»Ÿè®¾è®¡çš„æ¨ç†LLMã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†é¦–ä¸ªå¤§è§„æ¨¡ã€å¼€æºçš„é€šä¿¡ç³»ç»Ÿè®¾è®¡æ¨ç†è¯­æ–™åº“CSFRCã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µæ˜¯é‡‡ç”¨å¸¦æœ‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥æç‚¼é¢†åŸŸçŸ¥è¯†ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯ä¸€ç§åŸºäºReMaxçš„æ–°å‹è§„åˆ™å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•C-ReMaxï¼Œä»¥åŸ¹å…»é«˜çº§å»ºæ¨¡èƒ½åŠ›å’Œæ¿€å‘å¤æ‚çš„æ¨ç†æ¨¡å¼ï¼Œå¦‚è‡ªæˆ‘ä¿®æ­£å’ŒéªŒè¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹å‡è¾¾åˆ°ä¸šç•Œæœ€ä½³æ€§èƒ½ï¼Œæ˜æ˜¾ä¼˜äºæ›´å¤§çš„ä¸“æœ‰LLMã€‚è®ºæ–‡æ¥å—åå°†å…¬å¼€ç›¸å…³èµ„æºä»¥ä¿ƒè¿›è¯¥é¢†åŸŸè¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šä¿¡ç³»ç»Ÿçš„æ„å»ºæ˜¯æ¨åŠ¨æœªæ¥æ— çº¿æŠ€æœ¯å’Œ6Gå‘å±•çš„å…³é”®ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šä¿¡ç³»ç»Ÿè®¾è®¡é¢†åŸŸå­˜åœ¨ä¸è¶³ï¼Œéœ€è¦ä¸“é—¨çš„æ¨¡å‹æ¥å¤„ç†å¤æ‚ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„LLMæ¨¡å‹â€”â€”DeepFormï¼Œä¸“é—¨ç”¨äºè‡ªåŠ¨åŒ–é€šä¿¡ç³»ç»Ÿæ„å»ºã€‚</li>
<li>å¼•å…¥äº†é¦–ä¸ªå¤§è§„æ¨¡ã€å¼€æºçš„é€šä¿¡ç³»ç»Ÿè®¾è®¡æ¨ç†è¯­æ–™åº“CSFRCã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥è®­ç»ƒDeepFormæ¨¡å‹ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20baaacfb246f2faac4584ab5ddfdadb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54a7a598f79451e0d2527daebd18c931.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc07cd428912766da80b3235fa737a6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-209fe704ab1a6bd5cd81ea3c45ab3ba3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61a04ebf85bcef9131d586d42838e055.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Safe-and-Economical-UAV-Trajectory-Planning-in-Low-Altitude-Airspace-A-Hybrid-DRL-LLM-Approach-with-Compliance-Awareness"><a href="#Safe-and-Economical-UAV-Trajectory-Planning-in-Low-Altitude-Airspace-A-Hybrid-DRL-LLM-Approach-with-Compliance-Awareness" class="headerlink" title="Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A   Hybrid DRL-LLM Approach with Compliance Awareness"></a>Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A   Hybrid DRL-LLM Approach with Compliance Awareness</h2><p><strong>Authors:Yanwei Gong, Xiaolin Chang</strong></p>
<p>The rapid growth of the low-altitude economy has driven the widespread adoption of unmanned aerial vehicles (UAVs). This growing deployment presents new challenges for UAV trajectory planning in complex urban environments. However, existing studies often overlook key factors, such as urban airspace constraints and economic efficiency, which are essential in low-altitude economy contexts. Deep reinforcement learning (DRL) is regarded as a promising solution to these issues, while its practical adoption remains limited by low learning efficiency. To overcome this limitation, we propose a novel UAV trajectory planning framework that combines DRL with large language model (LLM) reasoning to enable safe, compliant, and economically viable path planning. Experimental results demonstrate that our method significantly outperforms existing baselines across multiple metrics, including data collection rate, collision avoidance, successful landing, regulatory compliance, and energy efficiency. These results validate the effectiveness of our approach in addressing UAV trajectory planning key challenges under constraints of the low-altitude economy networking. </p>
<blockquote>
<p>ä½ç©ºç»æµçš„å¿«é€Ÿå¢é•¿æ¨åŠ¨äº†æ— äººæœºï¼ˆUAVsï¼‰çš„å¹¿æ³›åº”ç”¨ã€‚è¿™ç§ä¸æ–­å¢é•¿çš„éƒ¨ç½²ä¸ºå¤æ‚åŸå¸‚ç¯å¢ƒä¸­çš„æ— äººæœºè½¨è¿¹è§„åˆ’å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶å¾€å¾€å¿½è§†äº†åŸå¸‚ç©ºåŸŸçº¦æŸå’Œç»æµæ•ˆç‡ç­‰å…³é”®å› ç´ ï¼Œè¿™åœ¨ä½ç©ºç»æµèƒŒæ™¯ä¸‹è‡³å…³é‡è¦ã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰è¢«è®¤ä¸ºæ˜¯è§£å†³è¿™äº›é—®é¢˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å…¶å®é™…åº”ç”¨å—åˆ°å­¦ä¹ æ•ˆç‡ä½çš„é™åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— äººæœºè½¨è¿¹è§„åˆ’æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ï¼Œä»¥å®ç°å®‰å…¨ã€åˆè§„ä¸”ç»æµå¯è¡Œçš„è·¯å¾„è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬æ•°æ®é‡‡é›†ç‡ã€é¿éšœã€æˆåŠŸç€é™†ã€æ³•è§„åˆè§„å’Œèƒ½æ•ˆã€‚è¿™äº›ç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨è§£å†³ä½ç©ºç»æµç½‘ç»œçº¦æŸä¸‹æ— äººæœºè½¨è¿¹è§„åˆ’çš„å…³é”®æŒ‘æˆ˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08532v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ— äººæœºæŠ€æœ¯æ—¥ç›Šæ™®åŠå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚åŸå¸‚ç¯å¢ƒä¸­çš„æ— äººæœºè½¨è¿¹è§„åˆ’ã€‚ä¸ºæé«˜æ•ˆç‡å’Œæ»¡è¶³åŸå¸‚ç»æµéœ€æ±‚ï¼Œç»“åˆæ·±åº¦å¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹æå‡ºæ–°çš„æ— äººæœºè½¨è¿¹è§„åˆ’æ¡†æ¶ï¼Œæœ‰æ•ˆåº”å¯¹æ•°æ®æ”¶é›†ã€ç¢°æ’é¿å…ã€æˆåŠŸç€é™†ã€æ³•è§„éµå®ˆå’Œèƒ½æºæ•ˆç‡ç­‰å¤šæ–¹é¢çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½ç©ºç»æµçš„å¿«é€Ÿå¢é•¿ä¿ƒè¿›äº†æ— äººæœºçš„å¹¿æ³›åº”ç”¨ã€‚</li>
<li>æ— äººæœºè½¨è¿¹è§„åˆ’é¢ä¸´å¤æ‚åŸå¸‚ç¯å¢ƒä¸­çš„æ–°æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¿½ç•¥äº†åŸå¸‚ç©ºåŸŸçº¦æŸå’Œç»æµæ•ˆç‡ç­‰å…³é”®å› ç´ ã€‚</li>
<li>æ·±åº¦å¼ºåŒ–å­¦ä¹ è¢«è®¤ä¸ºæ˜¯è§£å†³è¿™äº›é—®é¢˜çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆï¼Œæé«˜äº†æ— äººæœºè½¨è¿¹è§„åˆ’çš„æ•ˆç‡ã€‚</li>
<li>æå‡ºçš„æ–°æ–¹æ³•åœ¨å¤šæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ç‡ã€ç¢°æ’é¿å…ã€æˆåŠŸç€é™†ã€æ³•è§„éµå®ˆå’Œèƒ½æºæ•ˆç‡ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a6f7eccca263d1d26a8ea08527415565.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d814e9e4b962936996ea9e871b3f473.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f79c1ac1b070bf6fcdb02914a89be3c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MasHost-Builds-It-All-Autonomous-Multi-Agent-System-Directed-by-Reinforcement-Learning"><a href="#MasHost-Builds-It-All-Autonomous-Multi-Agent-System-Directed-by-Reinforcement-Learning" class="headerlink" title="MasHost Builds It All: Autonomous Multi-Agent System Directed by   Reinforcement Learning"></a>MasHost Builds It All: Autonomous Multi-Agent System Directed by   Reinforcement Learning</h2><p><strong>Authors:Kuo Yang, Xingjie Yang, Linhui Yu, Qing Xu, Yan Fang, Xu Wang, Zhengyang Zhou, Yang Wang</strong></p>
<p>Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently emerged as a powerful paradigm for tackling complex real-world tasks. However, existing Mas construction methods typically rely on manually crafted interaction mechanisms or heuristic rules, introducing human biases and constraining the autonomous ability. Even with recent advances in adaptive Mas construction, existing systems largely remain within the paradigm of semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement Learning (RL)-based framework for autonomous and query-adaptive Mas design. By formulating Mas construction as a graph search problem, our proposed MasHost jointly samples agent roles and their interactions through a unified probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives pursued in prior works, we introduce component rationality as an additional and novel design principle in Mas. To achieve this multi-objective optimization, we propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy that collaboratively integrates group-relative advantages and action-wise rewards. To our knowledge, our proposed MasHost is the first RL-driven framework for autonomous Mas graph construction. Extensive experiments on six benchmarks demonstrate that MasHost consistently outperforms most competitive baselines, validating its effectiveness, efficiency, and structure rationality. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMasï¼‰æœ€è¿‘ä½œä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼æ¥è§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡è€Œå‡ºç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„Masæ„å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„äº¤äº’æœºåˆ¶æˆ–å¯å‘å¼è§„åˆ™ï¼Œè¿™å¼•å…¥äº†äººç±»åè§å¹¶é™åˆ¶äº†è‡ªä¸»æ€§èƒ½åŠ›ã€‚å°½ç®¡æœ€è¿‘è‡ªé€‚åº”Masæ„å»ºæœ‰æ‰€è¿›å±•ï¼Œä½†ç°æœ‰ç³»ç»Ÿå¤§å¤šä»åœç•™åœ¨åŠè‡ªä¸»æ¨¡å¼çš„èŒƒå¼å†…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MasHostï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªä¸»å’ŒæŸ¥è¯¢è‡ªé€‚åº”Masè®¾è®¡æ¡†æ¶ã€‚é€šè¿‡å°†Masæ„å»ºå…¬å¼åŒ–ä¸ºå›¾æœç´¢é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºçš„MasHosté€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„æ¦‚ç‡é‡‡æ ·æœºåˆ¶è”åˆé‡‡æ ·æ™ºèƒ½ä½“è§’è‰²åŠå…¶äº¤äº’ã€‚é™¤äº†å…ˆå‰å·¥ä½œä¸­è¿½æ±‚çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ç›®æ ‡å¤–ï¼Œæˆ‘ä»¬è¿˜å°†ç»„ä»¶åˆç†æ€§ä½œä¸ºMasä¸­çš„é™„åŠ å’Œæ–°é¢–è®¾è®¡åŸåˆ™ã€‚ä¸ºäº†å®ç°è¿™ç§å¤šç›®æ ‡ä¼˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆHRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„RLç­–ç•¥ï¼Œå¯ä»¥ååŒæ•´åˆç»„ç›¸å¯¹ä¼˜åŠ¿å’Œè¡ŒåŠ¨æ–¹é¢çš„å¥–åŠ±ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æå‡ºçš„MasHostæ˜¯ç¬¬ä¸€ä¸ªç”¨äºè‡ªä¸»Maså›¾æ„å»ºçš„RLé©±åŠ¨æ¡†æ¶ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMasHostå§‹ç»ˆè¶…è¶Šå¤§å¤šæ•°ç«äº‰åŸºçº¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œç»“æ„åˆç†æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08507v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆLLM-driven Masï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ„å»ºäº†ä¸€ç§è‡ªä¸»ã€æŸ¥è¯¢é€‚åº”çš„æ¡†æ¶ï¼Œå³MasHostã€‚ä¸ä¼ ç»Ÿæ‰‹åŠ¨æ„å»ºçš„äº¤äº’æœºåˆ¶ä¸åŒï¼ŒMasHosté‡‡ç”¨å›¾å½¢æœç´¢æ–¹æ³•ï¼Œé€šè¿‡ç»Ÿä¸€æ¦‚ç‡é‡‡æ ·æœºåˆ¶è‡ªä¸»é‡‡æ ·æ™ºèƒ½ä½“è§’è‰²åŠå…¶äº¤äº’ã€‚åœ¨å®ç°å‡†ç¡®æ€§ã€æ•ˆç‡çš„åŒæ—¶ï¼Œå¼•å…¥äº†ç»„ä»¶åˆç†æ€§ä½œä¸ºæ–°çš„è®¾è®¡åŸåˆ™ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆHRPOï¼‰å®ç°å¤šç›®æ ‡ä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒMasHoståœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œç»“æ„åˆç†æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬ä¸­çš„å…³é”®è¦ç‚¹æ€»ç»“ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆLLM-driven Masï¼‰å·²æˆä¸ºè§£å†³å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡çš„æœ‰åŠ›å·¥å…·ã€‚</li>
<li>ä¼ ç»Ÿå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹åŠ¨æ„å»ºçš„äº¤äº’æœºåˆ¶å’Œå¯å‘å¼è§„åˆ™ï¼Œå¼•å…¥äººç±»åè§å¹¶é™åˆ¶è‡ªä¸»æ€§ã€‚</li>
<li>MasHostæ˜¯åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡æ¡†æ¶ï¼Œé€šè¿‡å›¾å½¢æœç´¢æ–¹æ³•è‡ªä¸»é‡‡æ ·æ™ºèƒ½ä½“è§’è‰²åŠå…¶äº¤äº’ã€‚</li>
<li>MasHostå¼•å…¥ç»„ä»¶åˆç†æ€§ä½œä¸ºæ–°çš„è®¾è®¡åŸåˆ™ï¼Œæ—¨åœ¨å®ç°å¤šç›®æ ‡ä¼˜åŒ–ã€‚</li>
<li>åˆ†å±‚ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆHRPOï¼‰æ˜¯MasHostæå‡ºçš„ä¸€ç§æ–°å‹RLç­–ç•¥ï¼Œç»“åˆäº†ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿å’Œè¡ŒåŠ¨å¥–åŠ±ã€‚</li>
<li>MasHostæ˜¯é¦–ä¸ªé‡‡ç”¨RLé©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå›¾å½¢æ„å»ºæ¡†æ¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-06c723c2e4e13c67625d3ab1492d598a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6970965be6c1c999a66f43b64ceeb17e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-12/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-12/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8d1a08e8c7dbb9ea8fc7bc567907c814.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7e1736a05646dc51af3cafc4f74d0c3a.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Efficient Long-duration Talking Video Synthesis with Linear Diffusion   Transformer under Multimodal Guidance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
