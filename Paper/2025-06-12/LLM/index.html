<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8d1a08e8c7dbb9ea8fc7bc567907c814.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    22.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    90 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-12-更新"><a href="#2025-06-12-更新" class="headerlink" title="2025-06-12 更新"></a>2025-06-12 更新</h1><h2 id="VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning"><a href="#VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning" class="headerlink" title="VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning"></a>VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning</h2><p><strong>Authors:Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</strong></p>
<p>Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems. </p>
<blockquote>
<p>在动态环境中协调多个实体代理仍然是人工智能的核心挑战，这需要感知驱动的推理和可扩展的合作策略。虽然最近的研究已经利用大型语言模型（LLM）进行多代理规划，但很少有人开始探索视觉语言模型（VLM）用于视觉推理。然而，这些基于VLM的方法在支持多种实体类型方面仍然存在局限性。在这项工作中，我们引入了VIKI-Bench，这是针对实体多代理合作定制的首个分层基准，包含三个结构化级别：代理激活、任务规划和轨迹感知。VIKI-Bench包括各种机器人实体、多视图视觉观察和结构化的监督信号，以评估基于视觉输入的推理。为了证明VIKI-Bench的实用性，我们提出了VIKI-R，这是一个两阶段框架，使用Chain-of-Thought注释的演示对预训练的视觉语言模型（VLM）进行微调，随后在多级奖励信号下进行强化学习。我们的广泛实验表明，VIKI-R在各级任务上的表现均显著优于基准方法。此外，我们还表明，强化学习能够促进异构代理之间组合合作模式的出现。总之，VIKI-Bench和VIKI-R为推进实体AI系统的多代理、视觉驱动合作提供了统一的测试平台和方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09049v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://faceong.github.io/VIKI-R/">https://faceong.github.io/VIKI-R/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了在动态环境中协调多个实体代理的核心挑战，需要感知驱动的推理和可扩展的合作策略。虽然最近的工作已经利用大型语言模型（LLM）进行多代理规划，但很少有人开始探索视觉语言模型（VLM）用于视觉推理。针对这一问题，本文引入了VIKI-Bench，这是一个专为实体多代理合作设计的分层基准测试，包括三个结构化级别：代理激活、任务规划和轨迹感知。此外，本文提出了VIKI-R框架，通过利用Chain-of-Thought注释演示对预训练的视觉语言模型进行微调，然后在多层次奖励信号下进行强化学习。实验表明，VIKI-R在各级任务上均显著优于基准方法，并且强化学习能够促进异构代理之间出现组合合作模式。总的来说，VIKI-Bench和VIKI-R为推进实体AI系统的多代理视觉驱动合作提供了一个统一的测试平台和解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多实体代理在动态环境中的协调是人工智能的核心挑战之一，需要感知驱动的推理和合作策略。</li>
<li>虽然大型语言模型（LLM）已用于多代理规划，但视觉语言模型（VLM）在视觉推理方面的应用仍有限。</li>
<li>VIKI-Bench是首个针对实体多代理合作的分层基准测试，包括代理激活、任务规划和轨迹感知三个结构化级别。</li>
<li>VIKI-Bench包含多种机器人实体、多视角视觉观察和结构化监督信号，以评估视觉输入基础上的推理能力。</li>
<li>VIKI-R框架通过利用带有Chain-of-Thought注释的演示对预训练视觉语言模型进行微调，并在多层次奖励信号下进行强化学习。</li>
<li>实验表明，VIKI-R在各级任务上均优于基准方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09049">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9f318d1c1f344134a0ccdcac656830d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b4ff7c2f9df7c67670ee1acf6207be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0a3569e6a044d60b77d21c9a1d0f4e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-546e911c893917079b05fe314b9dedc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-493cacde6faf7b9e2c94ba6ec86deaf5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FZOO-Fast-Zeroth-Order-Optimizer-for-Fine-Tuning-Large-Language-Models-towards-Adam-Scale-Speed"><a href="#FZOO-Fast-Zeroth-Order-Optimizer-for-Fine-Tuning-Large-Language-Models-towards-Adam-Scale-Speed" class="headerlink" title="FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models   towards Adam-Scale Speed"></a>FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models   towards Adam-Scale Speed</h2><p><strong>Authors:Sizhe Dang, Yangyang Guo, Yanjun Zhao, Haishan Ye, Xiaodong Zheng, Guang Dai, Ivor Tsang</strong></p>
<p>Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually require many more steps to converge. Can this trade-off between speed and memory in ZO be fundamentally improved? Normalized-SGD demonstrates strong empirical performance with greater memory efficiency than Adam. In light of this, we introduce FZOO, a Fast Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward passes needed for convergence by employing batched one-sided estimates that adapt step sizes based on the standard deviation of batch losses. It also accelerates per-batch computation through the use of Rademacher random vector perturbations coupled with CUDA’s parallel processing. Extensive experiments on diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3, across 11 tasks validate FZOO’s effectiveness. On average, FZOO outperforms MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy and an 18 times reduction in forward passes compared to MeZO, achieving convergence speeds comparable to Adam. We also provide theoretical analysis proving FZOO’s formal equivalence to a normalized-SGD update rule and its convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling even larger memory savings. Overall, our results make single-GPU, high-speed, full-parameter fine-tuning practical and point toward future work on memory-efficient pre-training. </p>
<blockquote>
<p>微调大型语言模型（LLM）常常面临GPU内存瓶颈问题：像Adam这样的一阶优化器的反向传播会将内存使用量增加到推理水平的10倍以上（例如，OPT-30B需要633GB）。零阶（ZO）优化器通过仅从前向传递中估计梯度来避免这一成本，但现有方法如MeZO通常需要更多的步骤才能达到收敛。ZO在速度和内存之间的权衡能否得到根本改进？Normalized-SGD在内存效率上表现出强大的实证性能，优于Adam。鉴于此，我们引入了FZOO，一个面向Adam级速度的快速零阶优化器。FZOO通过采用批处理单边估计来减少达到收敛所需的总前向传递次数，该估计根据批损失的标准差自适应调整步长。它还通过结合Rademacher随机向量扰动和CUDA的并行处理来加速每批计算。在RoBERTa-large、OPT（350M-66B）、Phi-2和Llama3等多种模型上进行的11项任务的大量实验验证了FZOO的有效性。平均而言，FZOO在准确性方面优于MeZO 3%，同时所需的前向传递次数减少了三倍。对于RoBERTa-large，FZOO在准确性方面实现了平均5.6%的改进，与前向传递相比减少了18倍，实现了与Adam相当的收敛速度。我们还提供了理论上的分析，证明FZOO与normalized-SGD更新规则的形式等效性及其收敛保证。FZOO可以平稳地集成到PEFT技术中，从而实现更大的内存节省。总的来说，我们的研究结果使单GPU、高速、全参数微调变得实用，并为未来的内存高效预训练工作指明了方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09034v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）的微调常常面临GPU内存瓶颈问题。一阶优化器如Adam的反向传递会增加内存使用至推理阶段的十倍以上。零阶（ZO）优化器通过仅从前向传递中估算梯度来避免这一成本，但现有方法如MeZO通常需要更多的步骤来达到收敛。是否存在一种可以根本改善ZO在速度和内存之间权衡的方法？Normalized-SGD在内存效率方面表现出强大的实证性能，超越了Adam。基于此，我们引入了面向Adam规模速度的快速零阶优化器FZOO。FZOO通过采用批处理单侧估计，根据批损失的标准差调整步长，减少了达到收敛所需的前向传递次数。它还通过Rademacher随机向量扰动结合CUDA的并行处理来加速每批次的计算。在包括RoBERTa-large、OPT（350M-66B）、Phi-2和Llama3等多种模型上的广泛实验，以及在11个任务上的验证，证明了FZOO的有效性。平均而言，FZOO在准确性方面优于MeZO的3%，同时所需的前向传递次数减少了三倍。对于RoBERTa-large，FZOO在准确性方面实现了平均提高5.6%，并且与前向传递次数相比，实现了与MeZO相比的18倍减少，收敛速度可与Adam相当。我们还提供了理论上的分析，证明了FZOO与normalized-SGD更新规则的正式等价性以及其收敛性的保证。FZOO可以顺利集成到PEFT技术中，从而实现更大的内存节省。总体而言，我们的研究结果表明单GPU高速全参数微调是实用的，并为未来的内存高效预训练工作指明了方向。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型微调面临GPU内存瓶颈问题，一阶优化器如Adam内存使用量大。</li>
<li>零阶优化器通过仅从前向传递估算梯度来避免这个问题，但现有方法如MeZO收敛速度慢。</li>
<li>FZOO作为一种快速零阶优化器被引入，通过采用批处理单侧估计和Rademacher随机向量扰动等技术，提高了内存效率和收敛速度。</li>
<li>FZOO在多种模型和任务上的实验结果表明其有效性，相比MeZO在准确性上有所提升，同时减少了所需的前向传递次数。</li>
<li>FZOO与normalized-SGD的理论分析证明了其正式等价性和收敛性的保证。</li>
<li>FZOO可以顺利集成到PEFT技术中，实现更大的内存节省。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09034">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e938896dc06515572a07a775cc453a93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2c2ac489a1559df4d1e8a469cf92730.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning"><a href="#Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning" class="headerlink" title="Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via   Reinforcement Learning"></a>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via   Reinforcement Learning</h2><p><strong>Authors:Haozhen Zhang, Tao Feng, Jiaxuan You</strong></p>
<p>The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave “think” actions (internal deliberation) with “route” actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1">https://github.com/ulab-uiuc/Router-R1</a>. </p>
<blockquote>
<p>多样的大型语言模型（LLM）的迅速涌现，推动了LLM路由器的发展，这些路由器将用户查询分配给最合适的模型。然而，现有的LLM路由器通常执行单轮一对一映射（即，将每个查询孤立地分配给一个单一模型），这限制了它们处理复杂任务的能力，这些复杂任务需要多个LLM的互补优势。在本文中，我们提出了基于强化学习（RL）的框架<strong>Router-R1</strong>，将多LLM路由和聚合制定为序列决策过程。Router-R1将路由器本身实例化为一个功能强大的LLM，利用其推理能力将“思考”行动（内部思考）与“路由”行动（动态模型调用）交织在一起，并将每个响应集成到不断发展的上下文中。为了指导学习，我们采用了一种轻量级的基于规则的奖励机制，包括格式奖励、最终成果奖励以及一种新型的成本奖励，用于性能和成本权衡优化，从而为通过强化学习优化性能成本权衡开辟了道路。Router-R1仅依赖于简单的模型描述符，如定价、延迟和示例性能，能够很好地推广到未见过的模型选择。在七个通用和多跳问答基准测试上的实验表明，Router-R1优于几个强大的基线，在保持稳健的泛化和成本管理的同时实现了卓越的性能。代码可用在<a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1%E3%80%82">https://github.com/ulab-uiuc/Router-R1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09033v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1">https://github.com/ulab-uiuc/Router-R1</a></p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）的快速发展推动了LLM路由器的出现，用于将用户查询分配给最合适的模型。然而，现有LLM路由器通常执行单一回合一对一映射，即孤立地将每个查询分配给一个模型，这限制了它们处理复杂任务的能力，这些任务需要多个LLM的互补优势。本文提出了基于强化学习（RL）的Router-R1框架，将多LLM路由和聚合公式化为一个序列决策过程。Router-R1实例化路由器本身作为一个强大的LLM，利用其推理能力交替进行“思考”行动（内部思考）和“路由”行动（动态模型调用），并将每个响应集成到不断发展的上下文中。为了指导学习，我们采用了一种轻量级的基于规则奖励机制，包括格式奖励、最终成果奖励和一种新型成本奖励，用于性能和成本权衡优化，从而为通过强化学习优化性能成本权衡开辟了道路。Router-R1仅依赖于简单的模型描述符（如定价、延迟和示例性能），可实现对未见模型的强大泛化。在七个通用和多跳问答基准测试上的实验表明，Router-R1优于几个强大的基线，在保持卓越性能的同时实现稳健的泛化和成本管理。代码可在<a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ulab-uiuc/Router-R1找到。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM路由器的出现是为了根据用户查询分配最合适的模型。</li>
<li>现有LLM路由器通常采用单一回合一对一映射策略，限制了处理复杂任务的能力。</li>
<li>Router-R1框架利用强化学习进行多LLM路由和聚合，公式化为一个序列决策过程。</li>
<li>Router-R1实例化路由器为一个具备推理能力的LLM，可以交替进行思考和路由行动。</li>
<li>采用轻量级基于规则奖励机制指导学习，包括格式奖励、最终成果奖励和成本奖励。</li>
<li>Router-R1可实现性能和成本的优化权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09033">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f820e13d181d02e5200ed723cc1cb4cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d1a08e8c7dbb9ea8fc7bc567907c814.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522d6789bcf433b474749bd875ba174e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="e3-Learning-to-Explore-Enables-Extrapolation-of-Test-Time-Compute-for-LLMs"><a href="#e3-Learning-to-Explore-Enables-Extrapolation-of-Test-Time-Compute-for-LLMs" class="headerlink" title="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for   LLMs"></a>e3: Learning to Explore Enables Extrapolation of Test-Time Compute for   LLMs</h2><p><strong>Authors:Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar</strong></p>
<p>Test-time scaling offers a promising path to improve LLM reasoning by utilizing more compute at inference time; however, the true promise of this paradigm lies in extrapolation (i.e., improvement in performance on hard problems as LLMs keep “thinking” for longer, beyond the maximum token budget they were trained on). Surprisingly, we find that most existing reasoning models do not extrapolate well. We show that one way to enable extrapolation is by training the LLM to perform in-context exploration: training the LLM to effectively spend its test time budget by chaining operations (such as generation, verification, refinement, etc.), or testing multiple hypotheses before it commits to an answer. To enable in-context exploration, we identify three key ingredients as part of our recipe e3: (1) chaining skills that the base LLM has asymmetric competence in, e.g., chaining verification (easy) with generation (hard), as a way to implement in-context search; (2) leveraging “negative” gradients from incorrect traces to amplify exploration during RL, resulting in longer search traces that chains additional asymmetries; and (3) coupling task difficulty with training token budget during training via a specifically-designed curriculum to structure in-context exploration. Our recipe e3 produces the best known 1.7B model according to AIME’25 and HMMT’25 scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not only attains high pass@1 scores, but also improves pass@k over the base model. </p>
<blockquote>
<p>测试时缩放提供了一种利用推理时间更多计算资源来改善大型语言模型（LLM）推理的有前途的途径。然而，该模式的真正潜力在于外推（即，随着LLM“思考”的时间超过其训练时的最大令牌预算，它们在难题上的性能得到改善）。令人惊讶的是，我们发现大多数现有的推理模型外推效果并不好。我们展示了一种实现外推的方法，即训练LLM进行上下文探索：训练LLM通过操作链（如生成、验证、细化等）有效地利用其测试时间预算，或在提交答案之前测试多个假设。</p>
</blockquote>
<p>为了实现上下文探索，我们确定了作为我们配方e3的一部分的三个关键要素：（1）链接基础LLM具有不对称能力技能，例如将验证（容易）与生成（困难）链接起来，作为实现上下文搜索的一种方式；（2）利用错误轨迹的“负面”梯度来放大强化学习过程中的探索，从而产生更长的搜索轨迹，链接额外的不对称性；（3）通过专门设计的课程在训练过程中将任务难度与训练令牌预算相结合，以构建上下文探索的结构。我们的e3配方生产的最佳已知1.7B模型根据AIME’25和HMMT’25分数，外推到训练令牌预算的2倍。我们的e3-1.7B模型不仅达到了高pass@1分数，而且相对于基础模型还改善了pass@k分数。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09026v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在测试时间尺度上，利用更多的计算资源进行推理是一种改进大型语言模型（LLM）的有前途的路径。真正的潜力在于外推能力，即在更长时间的推理过程中提高在困难问题上的性能，但这取决于如何分配LLM的长时间计算能力，从而增加它的泛化性能。然而，大多数现有的推理模型的外推性能并不理想。本文提出了一种实现外推的方法，即训练LLM进行上下文探索，包括链接操作（如生成、验证、精炼等）或测试多个假设再给出答案。为了支持上下文探索，本文提出了三个关键要素：链接不对称技能的技巧；利用反向传播修正方法的优化能力进行反向搜索探索；训练时按照任务难度设置训练标记预算的特定课程计划结构。基于这些要素的训练策略实现了最好的已知性能，并在AIME’25和HMMT’25测试中得到了验证，且在训练令牌预算的两倍上进行外推测试也表现出优异的效果。我们的模型不仅获得较高的准确得分，还能在更广泛的范围内改善性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时间尺度是改进LLM推理的一种有前途的方法。通过利用更多的计算资源在推理时间进行精细化操作，可以提高模型性能。</li>
<li>外推能力对于解决复杂问题至关重要。通过训练LLM进行上下文探索，可以实现更好的外推性能。</li>
<li>实现上下文探索的关键要素包括：链接不对称技能的技巧、利用反向搜索探索的优化能力以及在训练过程中考虑任务难度的特定课程计划结构。 </li>
<li>链接不对称技能的技巧是指针对某些操作的能力进行针对性强化，使得模型的技能更匹配需求并有助于提高模型的上下文搜索效率。  </li>
<li>训练有素的有效技能设置增强了LLM通过修改训练中的算法响应误报的能力，从而提高模型的搜索效率和准确性。 </li>
<li>通过特定的课程计划结构训练模型，使其适应不同难度的任务并分配相应的计算资源，有助于改善模型的泛化能力。  </li>
<li>此策略训练的模型具有良好的表现性、高效性以及对更大规模的上下文语境有良好的适应能力，提高了在各种环境下的通用性效果及用户体验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09026">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-79063f526e6338653b27ef1a207f06ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40fc0dbe113ec3791fb56f3b892fa654.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e92c894c318ce797534c1976e6e1b1fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba1a7daded12a6b6c15d2d36209431d3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-Across-Parallel-Samples-for-LLM-Reasoning"><a href="#Learning-to-Reason-Across-Parallel-Samples-for-LLM-Reasoning" class="headerlink" title="Learning to Reason Across Parallel Samples for LLM Reasoning"></a>Learning to Reason Across Parallel Samples for LLM Reasoning</h2><p><strong>Authors:Jianing Qi, Xi Ye, Hao Tang, Zhigang Zhu, Eunsol Choi</strong></p>
<p>Scaling test-time compute brings substantial performance gains for large language models (LLMs). By sampling multiple answers and heuristically aggregate their answers (e.g., either through majority voting or using verifiers to rank the answers), one can achieve consistent performance gains in math domains. In this paper, we propose a new way to leverage such multiple sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that takes a concatenated sequence of multiple samples and output the final answer, optimizing it for the answer accuracy with reinforcement learning. Experiments on multiple reasoning datasets show that SSA outperforms other test-time scaling methods such as reward model-based re-ranking. Our approach also shows a promising generalization ability, across sample set sizes, base model families and scales, and tasks. By separating LLMs to generate answers and LLMs to analyze and aggregate sampled answers, our approach can work with the outputs from premier black box models easily and efficiently. </p>
<blockquote>
<p>扩大测试时的计算能力为大语言模型（LLM）带来了显著的性能提升。通过采样多个答案并启发式地聚合它们的答案（例如，通过多数投票或使用验证器对答案进行排名），可以在数学领域实现持续的性能提升。在本文中，我们提出了一种利用这种多样样本集的新方法。我们训练了一个紧凑的LLM，称为“样本集聚合器（SSA）”，它接受多个样本的串联序列并输出最终答案，通过强化学习优化答案的准确性。在多个推理数据集上的实验表明，SSA优于其他测试时扩展方法，如基于奖励模型的重新排名。我们的方法还显示出很有希望的泛化能力，涵盖样本集大小、基础模型家族和规模以及任务。通过将LLM分离以生成答案和LLM以分析和聚合采样答案，我们的方法能够轻松高效地处理一流黑盒模型的输出。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09014v1">PDF</a> </p>
<p><strong>总结</strong></p>
<p>大规模语言模型（LLM）在测试时间计算扩展时具有显著性能提升。本文通过采样多个答案并启发式地聚合这些答案（例如通过多数投票或使用验证器对答案进行排名），在数学领域实现了持续的性能提升。本文提出了一种新的利用这种多样样本集的方法。我们训练了一个紧凑的大型语言模型，称为样本集聚合器（SSA），它接受多个样本的连续序列，并输出最终答案，通过强化学习对答案的准确性进行优化。在多个推理数据集上的实验表明，SSA优于其他测试时间扩展方法，如基于奖励模型的重新排名。我们的方法还表现出良好的泛化能力，适用于不同的样本集大小、基础模型家族和规模以及任务。通过将大型语言模型分离为生成答案和分析聚合采样答案的大型语言模型，我们的方法可以轻松高效地使用一流的黑盒模型的输出。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>测试时间计算扩展为大型语言模型（LLM）带来了显著的性能提升。</li>
<li>通过采样多个答案并启发式地聚合它们，可以在数学领域实现持续的性能改进。</li>
<li>提出了一种新的利用多样样本集的方法，即样本集聚合器（SSA）。</li>
<li>SSA通过优化答案的准确性，可以生成最终的聚合答案。</li>
<li>在多个推理数据集上，SSA的表现优于其他测试时间扩展方法。</li>
<li>SSA具有良好的泛化能力，适用于不同的样本集大小、基础模型家族和规模以及任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09014">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-43cffa28f5ed9dfe35ed5b4af90b12fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2625138392f1f388a1278f420018a790.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AdaDec-Uncertainty-Guided-Adaptive-Decoding-for-LLM-based-Code-Generation"><a href="#AdaDec-Uncertainty-Guided-Adaptive-Decoding-for-LLM-based-Code-Generation" class="headerlink" title="AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code   Generation"></a>AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code   Generation</h2><p><strong>Authors:Kaifeng He, Mingwei Liu, Chong Wang, Zike Li, Yanlin Wang, Xin Peng, Zibin Zheng</strong></p>
<p>Code generation with large language models (LLMs) is highly sensitive to token selection during decoding, particularly at uncertain decision points that influence program logic. While standard strategies like greedy and beam search treat all tokens uniformly, they overlook code-specific uncertainty patterns, leading to suboptimal performance. This paper presents an empirical study revealing that many generation errors stem from ranking mistakes at high-uncertainty steps, where the correct token is present but not top-ranked.   Motivated by these findings, we propose AdaDec, an uncertainty-guided adaptive decoding framework that integrates a token-level pause-then-rerank mechanism driven by token uncertainty (Shannon entropy). AdaDec learns model-specific uncertainty thresholds and applies a lookahead-based reranking strategy when uncertainty is high. Experiments on HumanEval and MBPP benchmarks show that AdaDec improves Pass@1 accuracy by up to 15.5% over greedy decoding, outperforms or matches beam search, and reduces computational cost and latency through efficient, selective pausing. Our results highlight the promise of uncertainty-aware adaptive decoding for improving the reliability and efficiency of LLM-based code generation. </p>
<blockquote>
<p>使用大型语言模型（LLM）的代码生成对解码过程中的令牌选择非常敏感，特别是在影响程序逻辑的不确定决策点。尽管贪婪搜索和束搜索等标准策略对所有令牌一视同仁，但它们忽略了代码特定的不确定性模式，导致性能不佳。本文进行了一项实证研究，发现许多生成错误源于高不确定性步骤的排名错误，其中正确的令牌虽然存在，但并没有排在首位。受这些发现的启发，我们提出了AdaDec，这是一个由不确定性引导的自适应解码框架，它集成了由令牌不确定性（香农熵）驱动的令牌级暂停-然后重新排名机制。AdaDec学习特定于模型的不确定性阈值，并在不确定性很高时应用基于前瞻的重新排名策略。在人类评估（HumanEval）和MBPP基准测试上的实验表明，与贪婪解码相比，AdaDec的Pass@1准确率提高了高达15.5%，优于或相当于束搜索，并通过高效的选择性暂停降低了计算成本和延迟。我们的结果突显了不确定性感知自适应解码在提高基于LLM的代码生成可靠性和效率方面的前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08980v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的代码生成对解码过程中的令牌选择非常敏感，特别是在影响程序逻辑的不确定决策点。标准策略如贪婪搜索和集束搜索均匀对待所有令牌，忽略了代码特定的不确定性模式，导致性能不佳。本文提出AdaDec，一个不确定性引导的自适应解码框架，该框架集成了一种基于令牌不确定性的暂停然后重新排名机制。AdaDec学习模型特定的不确定性阈值，并在不确定性高时应用基于前瞻的重新排名策略。实验表明，AdaDec在HumanEval和MBPP基准测试上的准确率比贪婪解码提高高达15.5%，优于或等于集束搜索，并通过高效的选择性暂停降低了计算成本和延迟。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码生成中，大型语言模型（LLM）对解码过程中的令牌选择非常敏感。</li>
<li>标准解码策略（如贪婪搜索和集束搜索）在不确定决策点上存在性能不足。</li>
<li>解码过程中的错误很多源于高不确定性步骤的排名错误。</li>
<li>AdaDec是一个不确定性引导的自适应解码框架，基于令牌不确定性进行暂停然后重新排名。</li>
<li>AdaDec能提高代码生成的准确率，相较于贪婪解码最高可提高15.5%。</li>
<li>AdaDec在性能上优于或至少与集束搜索相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08980">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-315da90946fb5b223fe68536a7398ae1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91fa1da8f0e4855c541abfa74c50be43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0403a133965f3c06f33ff76d63a95b60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecc480f25dd77d1d798fbcf901621413.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Atomic-to-Compositional-Generalization-for-Mobile-Agents-with-A-New-Benchmark-and-Scheduling-System"><a href="#Atomic-to-Compositional-Generalization-for-Mobile-Agents-with-A-New-Benchmark-and-Scheduling-System" class="headerlink" title="Atomic-to-Compositional Generalization for Mobile Agents with A New   Benchmark and Scheduling System"></a>Atomic-to-Compositional Generalization for Mobile Agents with A New   Benchmark and Scheduling System</h2><p><strong>Authors:Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, Zhuosheng Zhang</strong></p>
<p>Autonomous agents powered by multimodal large language models have been developed to facilitate task execution on mobile devices. However, prior work has predominantly focused on atomic tasks – such as shot-chain execution tasks and single-screen grounding tasks – while overlooking the generalization to compositional tasks, which are indispensable for real-world applications. This work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile agents on three categories of compositional operations: Simple Concatenation, Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in 20 fully controllable local utility app environments, as well as 30 online Chinese and English service apps. It comprises 100 interactive task templates with an average optimal step count of 14.05. Experimental results across a range of mobile agents with agentic workflow or agent-as-a-model show that UI-NEXUS presents significant challenges. Specifically, existing agents generally struggle to balance performance and efficiency, exhibiting representative failure modes such as under-execution, over-execution, and attention drift, causing visible atomic-to-compositional generalization gap. Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient scheduling system to tackle compositional mobile tasks. AGENT-NEXUS extrapolates the abilities of existing mobile agents by dynamically decomposing long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS achieves 24% to 40% task success rate improvement for existing mobile agents on compositional operation tasks within the UI-NEXUS benchmark without significantly sacrificing inference overhead. The demo video, dataset, and code are available on the project page at <a target="_blank" rel="noopener" href="https://ui-nexus.github.io/">https://ui-nexus.github.io</a>. </p>
<blockquote>
<p>由多模态大型语言模型驱动的自主体已被开发出来，以在移动设备上执行任务的执行。然而，先前的工作主要集中在原子任务上，如短链执行任务和单屏幕定位任务，而忽略了对组合任务的推广，这对于实际应用来说是不可或缺的。本文介绍了UI-NEXUS，这是一个旨在评估移动代理在三类组合操作上的综合基准测试：简单连接、上下文转换和深入探索。UI-NEXUS支持在20个可完全控制的本地实用应用程序环境中的交互式评估，以及30个在线中文和英文服务应用程序。它包含100个交互式任务模板，平均最佳步骤数为14.05。一系列具有自主工作流程或代理模型的移动代理的实验结果表明，UI-NEXUS存在重大挑战。具体来说，现有代理通常在性能和效率之间难以平衡，表现出典型的失败模式，如执行不足、过度执行和注意力漂移，导致明显的从原子到组合的一般化差距。从这些发现中汲取灵感，我们提出了AGENT-NEXUS，这是一个轻量级、高效的调度系统，用于解决组合移动任务。AGENT-NEXUS通过动态地将长期任务分解为一系列独立的原子子任务来推断现有移动代理的能力。AGENT-NEXUS在不显著增加推理开销的情况下，在UI-NEXUS基准测试中实现了对现有移动代理在组合操作任务上任务成功率提高24%至40%。演示视频、数据集和代码可在项目页面<a target="_blank" rel="noopener" href="https://ui-nexus.github.io上找到./">https://ui-nexus.github.io上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08972v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>多模态大型语言模型驱动的自主代理已用于促进移动设备上的任务执行。然而，先前的工作主要集中在原子任务上，如连续拍摄执行任务和单屏定位任务，忽略了对于现实世界应用至关重要的组合任务的推广。本工作介绍了UI-NEXUS，这是一个全面的基准测试，旨在评估移动代理在三类组合操作上的表现：简单连接、上下文转换和深入探索。UI-NEXUS支持在20个完全可控的本地实用应用程序环境和30个在线中英文服务应用程序中的交互式评估。它包括100个交互式任务模板，平均最佳步骤计数为14.05。实验结果显示，UI-NEXUS对移动代理构成了重大挑战。特别是现有代理在平衡性能和效率方面普遍存在困难，表现出典型的失败模式，如执行不足、过度执行和注意力漂移，导致明显的从原子到组合推广的差距。结合这些发现，我们提出了AGENT-NEXUS，这是一个轻量级、高效的调度系统，用于解决组合移动任务。AGENT-NEXUS通过动态地将长周期任务分解成一系列独立的原子子任务，从而提升现有移动代理的能力。在UI-NEXUS基准测试中，AGENT-NEXUS在不显著增加推理开销的情况下，提高了现有移动代理在组合操作任务上的成功率达24%~40%。相关演示视频、数据集和代码可在项目页面<a target="_blank" rel="noopener" href="https://ui-nexus.github.io上找到./">https://ui-nexus.github.io上找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UI-NEXUS是一个用于评估移动代理在组合操作上的全面基准测试，涵盖简单连接、上下文转换和深入探索三类操作。</li>
<li>UI-NEXUS支持在多种移动设备和应用程序环境中的交互式评估，提供100个交互式任务模板。</li>
<li>移动代理在UI-NEXUS基准测试中面临显著挑战，尤其是在平衡性能和效率方面。</li>
<li>现有代理在组合任务中表现出多种失败模式，如执行不足、过度执行和注意力漂移。</li>
<li>AGENT-NEXUS是一个轻量级、高效的调度系统，通过动态分解任务提升移动代理的能力。</li>
<li>AGENT-NEXUS在不显著增加推理开销的情况下，提高了现有移动代理在组合操作任务上的成功率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08972">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7cccc140d6eb55396e85a7548cc388e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8ff7901492a4a68c1ad2895092234e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8434d8b9bd25410d7dfd7acb7e3fd8ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4b8f0edbd91b19b5774ebbebe761800.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FaithfulRAG-Fact-Level-Conflict-Modeling-for-Context-Faithful-Retrieval-Augmented-Generation"><a href="#FaithfulRAG-Fact-Level-Conflict-Modeling-for-Context-Faithful-Retrieval-Augmented-Generation" class="headerlink" title="FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful   Retrieval-Augmented Generation"></a>FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful   Retrieval-Augmented Generation</h2><p><strong>Authors:Qinggang Zhang, Zhishang Xiang, Yilin Xiao, Le Wang, Junhui Li, Xinrun Wang, Jinsong Su</strong></p>
<p>Large language models (LLMs) augmented with retrieval systems have demonstrated significant potential in handling knowledge-intensive tasks. However, these models often struggle with unfaithfulness issues, generating outputs that either ignore the retrieved context or inconsistently blend it with the LLM<code>s parametric knowledge. This issue is particularly severe in cases of knowledge conflict, where the retrieved context conflicts with the model</code>s parametric knowledge. While existing faithful RAG approaches enforce strict context adherence through well-designed prompts or modified decoding strategies, our analysis reveals a critical limitation: they achieve faithfulness by forcibly suppressing the model<code>s parametric knowledge, which undermines the model</code>s internal knowledge structure and increases the risk of misinterpreting the context. To this end, this paper proposes FaithfulRAG, a novel framework that resolves knowledge conflicts by explicitly modeling discrepancies between the model&#96;s parametric knowledge and retrieved context. Specifically, FaithfulRAG identifies conflicting knowledge at the fact level and designs a self-thinking process, allowing LLMs to reason about and integrate conflicting facts before generating responses. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. The code is available at https:&#x2F;&#x2F; github.com&#x2F;DeepLearnXMU&#x2F;Faithful-RAG </p>
<blockquote>
<p>通过融入检索系统，大型语言模型（LLM）在处理知识密集型任务时显示出巨大潜力。然而，这些模型经常面临不忠实的问题，生成输出时要么忽略检索的上下文，要么将其与LLM的参数知识不一致地混合。在知识冲突的情况下，这个问题尤为严重，此时检索的上下文与模型的参数知识相冲突。虽然现有的忠实RAG方法通过精心设计的提示或修改解码策略来强制实施严格的上下文遵循，但我们的分析揭示了一个关键局限：它们通过强制抑制模型的参数知识来实现忠实性，这破坏了模型的内部知识结构，并增加了误解上下文的风险。为此，本文提出了FaithfulRAG这一新型框架，它通过显式建模模型参数知识与检索上下文之间的差异来解决知识冲突。具体来说，FaithfulRAG在事实层面识别冲突知识，并设计一个自我思考过程，允许LLM在生成响应之前对冲突事实进行推理和整合。大量实验表明，我们的方法优于现有最先进的方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/Faithful-RAG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DeepLearnXMU/Faithful-RAG找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08938v1">PDF</a> Qinggang Zhang and Zhishang Xiang contributed equally to this work.   Corresponding author: Jinsong Su</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）结合检索系统在处理知识密集型任务时展现出巨大潜力，但面临知识不忠实问题，即在检索到的知识与模型参数知识冲突时，模型输出往往忽略检索到的语境或与模型参数知识混合。现有忠实RAG方法通过精心设计提示或修改解码策略来强制语境遵循，但存在压制模型参数知识的局限，这破坏了模型内部知识结构并增加了误解语境的风险。本文提出的FaithfulRAG框架通过显式建模模型参数知识与检索语境之间的差异来解决知识冲突问题。它能识别事实层面的冲突知识，并设计自我思考过程，使LLM在生成响应前能够推理和整合冲突事实。实验证明，该方法优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM结合检索系统在处理知识密集型任务时表现出显著潜力。</li>
<li>LLM在处理知识时存在不忠实问题，特别是在知识冲突情况下。</li>
<li>现有忠实RAG方法通过强制语境遵循来达成忠实性，但存在压制模型参数知识的局限。</li>
<li>FaithfulRAG框架通过显式建模模型参数知识与检索语境之间的差异来解决知识冲突。</li>
<li>FaithfulRAG能识别事实层面的冲突知识，并设计自我思考过程。</li>
<li>FaithfulRAG框架在实验中表现出优于现有方法的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08938">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-97c607b0ff8155510f07133505e99c15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-425ab6f95200b90d7fd06206ceb59353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b957631fe71c77fb4b0cac0d2055580.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03e75dbe0361d1ba8ef6e41549bf8767.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-A-Gamer-Train-A-Mathematical-Reasoning-Model"><a href="#Can-A-Gamer-Train-A-Mathematical-Reasoning-Model" class="headerlink" title="Can A Gamer Train A Mathematical Reasoning Model?"></a>Can A Gamer Train A Mathematical Reasoning Model?</h2><p><strong>Authors:Andrew Shin</strong></p>
<p>While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. <a target="_blank" rel="noopener" href="https://github.com/shinandrew/YouronMath">https://github.com/shinandrew/YouronMath</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在数学推理等任务中取得了显著的成绩，但其开发通常需要大量的计算资源。虽然最近的进展降低了训练有能力模型的成本，但这些方法仍然依赖于高端硬件集群。在本文中，我们通过结合强化学习和内存优化技术，展示了使用单个平均游戏GPU就可以训练出坚实的数学推理模型。具体来说，我们在RTX 3080 Ti的16GB内存上训练了一个1.5B参数的数学推理模型，在资源受限的环境中，其在数学推理基准测试上的性能与更大规模的模型相当或更好。我们的结果挑战了最先进数学推理必须依赖大规模基础设施的现状，为高性能人工智能研究的普及化提供了可能。可以通过<a target="_blank" rel="noopener" href="https://github.com/shinandrew/YouronMath%E8%AE%BF%E9%97%AE%E6%88%91%E4%BB%AC%E7%9A%84%E7%A0%94%E7%A9%B6%E6%88%90%E6%9E%9C%E3%80%82">https://github.com/shinandrew/YouronMath访问我们的研究成果。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08935v1">PDF</a> </p>
<p><strong>Summary</strong>：大型语言模型（LLM）在数学推理等任务上表现出卓越性能，但其开发需要巨大的计算资源。最近的研究降低了训练这些模型的成本，但仍依赖高端硬件集群。本研究通过结合强化学习和内存优化技术，在单个普通游戏GPU上训练了一个出色的数学推理模型。该模型在RTX 3080 Ti 16GB内存上训练了一个1.5亿参数的数学推理模型，在资源受限的环境中实现了与更大模型相当或更好的性能。这挑战了最先进的数学推理需要大规模基础设施的观点，为高性能人工智能研究的普及化提供了可能性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）在数学推理方面表现出卓越性能，但需要巨大的计算资源。</li>
<li>最近的研究已经降低了训练这些模型的硬件成本。</li>
<li>通过结合强化学习和内存优化技术，可以在单个普通游戏GPU上训练出色的数学推理模型。</li>
<li>本研究在RTX 3080 Ti 16GB内存上训练了一个参数规模为1.5亿的模型。</li>
<li>该模型在资源受限的环境中实现了与更大模型相当或更好的性能。</li>
<li>研究结果挑战了最先进的数学推理需要大规模基础设施的观点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08935">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-341a77519510c65e6805cf79349133e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ccaacf4bf77e8ebfc70ecde6c7ffe85b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="What-Limits-Virtual-Agent-Application-OmniBench-A-Scalable-Multi-Dimensional-Benchmark-for-Essential-Virtual-Agent-Capabilities"><a href="#What-Limits-Virtual-Agent-Application-OmniBench-A-Scalable-Multi-Dimensional-Benchmark-for-Essential-Virtual-Agent-Capabilities" class="headerlink" title="What Limits Virtual Agent Application? OmniBench: A Scalable   Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities"></a>What Limits Virtual Agent Application? OmniBench: A Scalable   Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities</h2><p><strong>Authors:Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation with limited scenarios, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, cross-platform, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91% human acceptance rate. Training on our graph-structured data shows that it can more efficiently guide agents compared to manually annotated data. We conduct multidimensional evaluations for various open-source and closed-source models, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at <a target="_blank" rel="noopener" href="https://omni-bench.github.io/">https://omni-bench.github.io/</a>. </p>
<blockquote>
<p>随着多模态大型语言模型（MLLM）的发展，基于MLLM的虚拟代理表现出了出色的性能。然而，现有的基准测试面临重大挑战，包括不可控的任务复杂性、场景有限的手动标注广泛以及缺乏多维评估。为了应对这些挑战，我们引入了OmniBench，这是一个自我生成、跨平台、基于图的基准测试，具有通过子任务组合合成可控复杂性任务的自动化管道。为了评估虚拟代理在图上的各种能力，我们还推出了OmniEval，这是一个多维评估框架，包括子任务级评估、基于图的指标和跨越10种能力的综合测试。我们合成的数据集包含20个场景下的3.6万图结构任务，达到了91%的人类接受率。在我们的图结构数据上进行训练表明，与手动注释数据相比，它可以更有效地指导代理。我们对各种开源和闭源模型进行了多维评估，揭示了它们在各种能力方面的表现，为未来的进步铺平了道路。我们的项目可在<a target="_blank" rel="noopener" href="https://omni-bench.github.io/%E8%AE%BF%E9%97%AE%E3%80%82">https://omni-bench.github.io/访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08933v1">PDF</a> Accepted by ICML 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>随着多模态大型语言模型（MLLMs）的发展，基于MLLM的虚拟代理表现出卓越的性能。然而，现有基准测试面临诸多局限性，包括任务复杂性不可控、场景有限的手动标注和缺乏多维评估等问题。为应对这些挑战，我们推出OmniBench，一个自我生成、跨平台、基于图形的基准测试，通过子任务组合合成可控复杂度的任务。同时，我们提出OmniEval，一个多维评估框架，包括子任务级评估、基于图形的指标以及跨10项能力的全面测试。我们的合成数据集包含36k图形结构化任务，覆盖20个场景，达到了91%的人类接受率。在图形结构化数据上的训练显示，它能更有效地指导代理相比手动注释的数据。我们对各种开源和闭源模型进行了多维评估，揭示了它们在各种能力方面的表现，为未来的进步铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）的进步使得虚拟代理表现卓越。</li>
<li>现有基准测试存在局限性，包括任务复杂性不可控、手动标注的局限性以及缺乏多维评估等问题。</li>
<li>OmniBench是一个自我生成、跨平台、基于图形的基准测试，可以合成可控复杂度的任务。</li>
<li>OmniEval是一个多维评估框架，用于评估虚拟代理在各种能力上的表现。</li>
<li>合成数据集包含36k图形结构化任务，覆盖多个场景，人类接受率高达91%。</li>
<li>在图形结构化数据上训练可以更有效地指导虚拟代理。</li>
<li>对各种模型的多维评估揭示了它们在多种能力上的表现差异，为未来的研究指明了方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08933">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-94fefb1b6723dc45026428f922a131a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ab7505d86fb36096182b108efafa352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f2cf80efd8feb950a53f38fd70daf5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e18f0ce0b4c569fb238440dd97fd3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bd8bf27b29d78336e7dd18a203af15c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35206aa3883c2327bd0e60b86a68b4cd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PropMEND-Hypernetworks-for-Knowledge-Propagation-in-LLMs"><a href="#PropMEND-Hypernetworks-for-Knowledge-Propagation-in-LLMs" class="headerlink" title="PropMEND: Hypernetworks for Knowledge Propagation in LLMs"></a>PropMEND: Hypernetworks for Knowledge Propagation in LLMs</h2><p><strong>Authors:Zeyu Leo Liu, Greg Durrett, Eunsol Choi</strong></p>
<p>Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations. </p>
<blockquote>
<p>针对大型语言模型（LLM）的知识编辑技术可以注入知识并在以后进行逐字复制，但它们在传播知识方面存在不足：模型无法回答需要利用注入知识进行推理的问题。我们提出了一种基于超网络的知识传播方法，名为PropMEND。在该方法中，我们元学习如何修改语言建模损失的梯度，以鼓励注入信息进行传播。我们的方法扩展了MEND [29]的元目标，使知识上的梯度更新能够转化为能够回答涉及该知识的多跳问题。我们在RippleEdit数据集上展示了改进的性能，在具有挑战性的多跳问题上，其答案在注入的事实中并未明确给出，准确率几乎提高了一倍。为了进一步评估我们的超网络的泛化能力，我们还引入了一个新的数据集Controlled RippleEdit，以测试超网络训练期间未见过的关系和实体的知识传播情况。PropMEND在未见过的实体-关系对上仍然优于现有方法，但性能差距大幅缩小，这表明未来在知识传播到各种关系上的工作仍有很大空间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08920v1">PDF</a> Under review</p>
<p><strong>总结</strong></p>
<p>大型语言模型（LLM）的知识编辑技术可以注入知识并可以原封不动地再现，但在传播知识方面存在不足：模型无法回答需要利用注入知识进行推理的问题。本文提出了一种基于超网络的知识传播方法，名为PropMEND，该方法通过元学习学会如何修改语言建模损失的梯度，以鼓励注入的信息得以传播。我们的方法扩展了MEND的元目标，使知识上的梯度更新能够转化为回答涉及该知识的多跳问题。在RippleEdit数据集上的实验表明，对于答案未在注入事实中明确给出的具有挑战性的多跳问题，其准确率几乎提高了一倍。此外，为了评估超网络在未见实体关系上的知识传播能力，我们还引入了新的数据集Controlled RippleEdit。PropMEND在未见实体关系对上仍优于现有方法，但性能差距大幅缩小，这表明未来在将知识传播到广泛的关系上仍需进一步努力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM的知识编辑技术能注入并原样再现知识，但在知识传播方面存在局限。</li>
<li>提出了一种基于超网络的知识传播方法——PropMEND。</li>
<li>PropMEND通过元学习学会修改语言建模损失的梯度，促进注入知识的传播。</li>
<li>在RippleEdit数据集上，PropMEND对于复杂问题的准确率显著提高。</li>
<li>引入新的数据集Controlled RippleEdit，以评估超网络在未见实体关系上的知识传播能力。</li>
<li>PropMEND在未见实体关系对上表现出超越现有方法的能力，但性能差距有所缩小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08920">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-04cd5e41a2a25027a3cc83644549262a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed6a216231e06f3093c597ed4c71a573.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d60dbaf4f31ec0857f86c5740ad9fd73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ccb3390d053867380e3a257cc2b8764.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Your-Brain-on-ChatGPT-Accumulation-of-Cognitive-Debt-when-Using-an-AI-Assistant-for-Essay-Writing-Task"><a href="#Your-Brain-on-ChatGPT-Accumulation-of-Cognitive-Debt-when-Using-an-AI-Assistant-for-Essay-Writing-Task" class="headerlink" title="Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI   Assistant for Essay Writing Task"></a>Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI   Assistant for Essay Writing Task</h2><p><strong>Authors:Nataliya Kosmyna, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ, Xian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, Pattie Maes</strong></p>
<p>This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI’s role in learning. </p>
<blockquote>
<p>本研究探讨了LLM辅助写作对神经和行为方面的影响。参与者被分为三组：LLM组、搜索引擎组和仅大脑组（无工具）。三组成员均在相同条件下完成三场测试。在第四场测试中，LLM组的用户被重新分配到仅大脑组（LLM到大脑），而仅大脑组的用户被分配到LLM条件组（大脑到LLM）。前三次测试共有54名参与者，其中18人完成了第四次测试。我们使用脑电图（EEG）来评估写作过程中的认知负荷，并使用NLP对作文进行分析，同时还请人类教师和AI评分员对作文进行评分。各组的命名实体识别、n元模式以及主题本体论显示了组内的一致性。脑电图显示大脑连接存在显著差异：仅大脑组的参与者表现出最强、最分布广泛的网络；搜索引擎用户表现出中等程度的参与度；而LLM用户显示出最弱的连接。与使用外部工具相关的认知活动规模有所减小。在第四次测试中，从LLM到大脑的参与者显示出降低的α和β连接性，表明参与不足。而从大脑到LLM的参与者则表现出更高的记忆回忆和枕骨-顶叶及前额叶区域的激活，这与搜索引擎用户相似。关于作文的自我所有权报告，LLM组的最低，仅大脑组最高。LLM用户在准确引用自己的作品时遇到困难。虽然LLM提供了即时的便利，但我们的研究结果突出了其潜在认知成本。在四个月的时间里，LLM用户在神经、语言和行为层面一直表现较差。这些结果引发了人们对长期依赖LLM的教育影响的担忧，并强调了深入探究AI在学习中的作用必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08872v1">PDF</a> 206 pages, 92 figures, 4 tables and appendix</p>
<p><strong>Summary</strong></p>
<p>本研究探讨了LLM辅助写作对大脑和行为的影响。研究将参与者分为三组：LLM组、搜索引擎组和仅大脑组（无工具）。每组参与者在不同条件下完成三次写作会话。第四次会话时，对LLM用户重新分配到仅大脑组，而对仅大脑用户分配到LLM条件组。共有54名参与者完成了前三次会话，其中18人完成了第四次会话。研究使用脑电图评估写作过程中的认知负荷，并用自然语言处理和人类教师和AI评估文章质量。各组内文章的语言特征具有一致性，但大脑连接性在组间存在显著差异。仅大脑组的网络连接最强且分布最广；搜索引擎组表现为中等参与；而LLM组则显示出最弱的连通性。与使用外部工具相关的认知活动有所减少。在第四次会话中，从LLM转到仅大脑的参与者显示出连通性降低，而大脑到LLM的参与者则表现出更高的记忆召回和激活的脑区与搜索引擎组相似。LLM组的文章自我所有权报告最低，且难以准确引用自己的作品。虽然LLM提供了即时便利，但研究结果指出了其潜在认知成本。长期而言，在神经、语言和行为层面，LLM用户表现较差。这引发了关于依赖LLM的长期教育影响的担忧，并强调需要更深入地研究AI在学习中的角色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM辅助写作对大脑和行为有影响，参与者分为三组进行研究。</li>
<li>通过脑电图评估写作过程中的认知负荷。</li>
<li>文章质量通过自然语言处理、人类教师和AI评估。</li>
<li>各组内文章语言特征一致，但大脑连接性存在显著差异。</li>
<li>LLM用户显示出最弱的连通性，而仅大脑组表现出最强的网络连接。</li>
<li>LLM组的自我所有权报告最低，且存在引用困难。</li>
<li>长期依赖LLM可能在神经、语言和行为层面产生负面影响，引发对长期教育影响的担忧。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08872">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1b9d0a85c8334cd2d9e85dda38db6ffc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-465e116ad4bc088f5067f0fe8edf4a0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dead912d30482dbbd9bf296954142351.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18843faa84f1884c1edea41f63f5d492.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLaVA-c-Continual-Improved-Visual-Instruction-Tuning"><a href="#LLaVA-c-Continual-Improved-Visual-Instruction-Tuning" class="headerlink" title="LLaVA-c: Continual Improved Visual Instruction Tuning"></a>LLaVA-c: Continual Improved Visual Instruction Tuning</h2><p><strong>Authors:Wenzhuo Liu, Fei Zhu, Haiyang Guo, Longhui Wei, Cheng-Lin Liu</strong></p>
<p>Multimodal models like LLaVA-1.5 achieve state-of-the-art visual understanding through visual instruction tuning on multitask datasets, enabling strong instruction-following and multimodal performance. However, multitask learning faces challenges such as task balancing, requiring careful adjustment of data proportions, and expansion costs, where new tasks risk catastrophic forgetting and need costly retraining. Continual learning provides a promising alternative to acquiring new knowledge incrementally while preserving existing capabilities. However, current methods prioritize task-specific performance, neglecting base model degradation from overfitting to specific instructions, which undermines general capabilities. In this work, we propose a simple but effective method with two modifications on LLaVA-1.5: spectral-aware consolidation for improved task balance and unsupervised inquiry regularization to prevent base model degradation. We evaluate both general and task-specific performance across continual pretraining and fine-tuning. Experiments demonstrate that LLaVA-c consistently enhances standard benchmark performance and preserves general capabilities. For the first time, we show that task-by-task continual learning can achieve results that match or surpass multitask joint learning. The code will be publicly released. </p>
<blockquote>
<p>LLaVA-1.5等多模态模型通过多任务数据集上的视觉指令调整，实现了最先进的视觉理解，从而实现了强大的指令遵循和多模态性能。然而，多任务学习面临任务平衡等挑战，需要仔细调整数据比例和扩展成本，新增任务存在灾难性遗忘的风险并需要昂贵的重新训练。持续学习提供了一种很有前途的替代方案，可以增量获取新知识，同时保留现有能力。然而，当前的方法优先考虑特定任务的性能，忽视了由于过度拟合特定指令导致的基准模型性能下降问题，从而损害了通用能力。在这项工作中，我们对LLaVA-1.5进行了两项修改，提出了一种简单有效的方法：光谱感知巩固，以改进任务平衡和无监督查询正则化，以防止基础模型性能下降。我们评估了持续预训练和微调过程中的通用和特定任务性能。实验表明，LLaVA-c持续提高了标准基准测试性能并保留了通用能力。我们首次证明，按任务顺序的持续学习可以达到或多任务联合学习的效果。代码将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08666v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于LLaVA-1.5的多模态模型通过视觉指令调整在多任务数据集上实现了先进的视觉理解，表现出强大的指令遵循和多模态性能。然而，多任务学习面临任务平衡和扩展成本的挑战。本文提出一种简单有效的方法对LLaVA-1.5进行两项改进：光谱感知巩固以改善任务平衡和无监督查询正则化以防止基础模型过度拟合特定指令导致的性能下降。实验表明，LLaVA-c在标准基准测试中表现优异，并保持了通用能力。本研究首次证明了任务逐个的增量学习可以达到或多于多任务联合学习的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态模型如LLaVA-1.5通过视觉指令调整和多任务数据集实现了先进视觉理解。</li>
<li>多任务学习面临任务平衡和扩展成本的挑战。</li>
<li>提出一种简单有效的方法对LLaVA-1.5进行改进，包括光谱感知巩固和无监督查询正则化。</li>
<li>方法旨在改善任务平衡并防止基础模型过度拟合特定指令导致的性能下降。</li>
<li>LLaVA-c在标准基准测试中表现优异，并保持了通用能力。</li>
<li>任务逐个的增量学习可以达到或多于多任务联合学习的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08666">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2c493b3d0a83643c4fae13fc5f3cd234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66f125ec5a9bd761b4a6960f8179e867.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa2e0d1034cfbdc9489c707222e1cdc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c2a78e6382a441a72eb41128b924703.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-915cb02b371f411b4c79c1da79fd1ab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1893772bcaeff96415435098df0e75dd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="JoFormer-Journey-based-Transformer-Theory-and-Empirical-Analysis-on-the-Tiny-Shakespeare-Dataset"><a href="#JoFormer-Journey-based-Transformer-Theory-and-Empirical-Analysis-on-the-Tiny-Shakespeare-Dataset" class="headerlink" title="JoFormer (Journey-based Transformer): Theory and Empirical Analysis on   the Tiny Shakespeare Dataset"></a>JoFormer (Journey-based Transformer): Theory and Empirical Analysis on   the Tiny Shakespeare Dataset</h2><p><strong>Authors:Mahesh Godavarti</strong></p>
<p>Transformers have demonstrated remarkable success in sequence modeling, yet effectively incorporating positional information remains a challenging and active area of research. In this paper, we introduce JoFormer, a journey-based Transformer architecture grounded in a recently proposed non-commutative algebra for composing transformations across positions. JoFormer represents relative positions through learnable directional transforms that are sequentially composed along the input, thereby extending and generalizing existing approaches based on relative position representations. We derive the JoFormer attention mechanism from first principles and show that it subsumes standard methods such as rotary transformations as special cases. To evaluate its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny Shakespeare character-level language modeling task. Our results demonstrate that   JoFormer consistently achieves lower perplexity and faster convergence, highlighting the advantages of its more expressive, journey-based treatment of position. Notably, the per-token JoFormer is still a primitive, conceptual variant with layer-independent angles, yet it already demonstrates strong performance-underscoring its promise as a proof of concept for more expressive architectures. We conclude by discussing how JoFormer offers a principled approach to integrating positional structure into Transformer architectures. The code used in this work is available at <a target="_blank" rel="noopener" href="https://github.com/mahesh-godavarti/joformer">https://github.com/mahesh-godavarti/joformer</a>. </p>
<blockquote>
<p>Transformer在序列建模方面取得了显著的成就，但如何有效地融入位置信息仍然是一个具有挑战性和活跃的研究领域。本文介绍了JoFormer，这是一种基于旅程的Transformer架构，它基于最近提出的非交换代数，用于在位置之间组合转换。JoFormer通过可学习的方向转换来表示相对位置，这些转换沿着输入顺序组合，从而扩展和概括了基于相对位置表示的现有方法。我们从基本原理推导出JoFormer的注意力机制，并证明它涵盖了标准方法，如旋转变换等特殊情况。为了评估其有效性，我们在Tiny Shakespeare字符级语言建模任务上将JoFormer与RoFormer基线进行了比较。结果表明，JoFormer持续实现更低的困惑度和更快的收敛速度，凸显了其基于旅程的位置处理的更强大的表达能力。值得注意的是，每个令牌的JoFormer仍然是一个具有层独立角度的原始概念变体，但它已经显示出强大的性能，强调了其作为更具表现力架构的概念证明的前景。最后，我们讨论了JoFormer如何将位置结构融入Transformer架构的原则方法。本工作中使用的代码可在<a target="_blank" rel="noopener" href="https://github.com/mahesh-godavarti/joformer">https://github.com/mahesh-godavarti/joformer</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08652v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于旅程的Transformer架构JoFormer，它通过非交换代数来组合不同位置之间的转换，从而有效地融入位置信息。JoFormer通过可学习的方向转换来表示相对位置，并沿着输入进行顺序组合，从而扩展并推广了基于相对位置表示的方法。实验结果表明，JoFormer在Tiny Shakespeare字符级语言建模任务上较RoFormer基线模型取得了更低的困惑度和更快的收敛速度。尽管JoFormer作为一个基于层独立的视角的原始概念变体仍然具有前景，但其表现已展现出强大的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JoFormer是一种基于旅程的Transformer架构，旨在更有效地融入位置信息。</li>
<li>它通过非交换代数来组合不同位置间的转换，从而表示相对位置。</li>
<li>JoFormer引入了可学习的方向转换，这些转换沿着输入进行顺序组合。</li>
<li>JoFormer在Tiny Shakespeare字符级语言建模任务上表现出较低困惑度和更快收敛速度。</li>
<li>JoFormer作为一个概念验证模型，已经展现出强大的潜力。</li>
<li>JoFormer提供了一个将位置结构整合到Transformer架构中的原则性方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08652">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3c8e3d5e94684a91f07aa84ff8cf044c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ab9a8b3cc6d6c5ed57160876a784edf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adec4e9a38bf4115834e4dffc1440d79.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TableDreamer-Progressive-and-Weakness-guided-Data-Synthesis-from-Scratch-for-Table-Instruction-Tuning"><a href="#TableDreamer-Progressive-and-Weakness-guided-Data-Synthesis-from-Scratch-for-Table-Instruction-Tuning" class="headerlink" title="TableDreamer: Progressive and Weakness-guided Data Synthesis from   Scratch for Table Instruction Tuning"></a>TableDreamer: Progressive and Weakness-guided Data Synthesis from   Scratch for Table Instruction Tuning</h2><p><strong>Authors:Mingyu Zheng, Zhifan Feng, Jia Wang, Lanrui Wang, Zheng Lin, Yang Hao, Weiping Wang</strong></p>
<p>Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data. The code and data is available at <a target="_blank" rel="noopener" href="https://github.com/SpursGoZmy/TableDreamer">https://github.com/SpursGoZmy/TableDreamer</a> </p>
<blockquote>
<p>尽管最近基于大型语言模型（LLM）的数据合成方法取得了值得称赞的进展，但在生成表格指令调整数据方面仍面临两个局限。首先，它们无法全面探索表格理解任务的广阔输入空间，导致数据多样性有限。其次，它们忽略了目标LLM在表格理解方面的弱点，盲目追求数据量的增加，导致数据效率不佳。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08646v1">PDF</a> 27 pages, 19 figures, Findings of ACL 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对表格指令调整的数据合成框架TableDreamer，旨在解决现有LLM在表格理解任务上的数据多样性不足及数据效率不高的问题。通过合成多样化的表格和相关指令作为种子数据，并在新识别的弱点数据指导下进行输入空间的迭代探索，最终生成用于微调目标LLM的训练数据。实验证明，该框架在多个表格基准测试上表现出卓越性能，使用2.7万条GPT-4o合成数据将Llama3.1-8B-instruct的平均准确率提升11.62%，并优于使用更多训练数据的其他数据合成基线方法。代码和数据集可在GitHub上获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在生成表格指令调整数据时面临输入空间探索不全面和忽略模型弱点的问题。</li>
<li>TableDreamer框架旨在解决这些问题，通过合成多样化的表格和指令作为种子数据。</li>
<li>在新识别的弱点数据指导下进行输入空间的迭代探索，提高数据效率。</li>
<li>实验证明TableDreamer在多个表格基准测试上表现优越。</li>
<li>使用较少的合成数据（2.7万条GPT-4o数据）显著提升Llama3.1-8B-instruct模型的平均准确率。</li>
<li>与其他使用更多训练数据的方法相比，TableDreamer表现出更高的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08646">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a7db72c96fe18b5ded803f3c46a0a6db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-368cc45fa8bf82ae13ed5b756251351e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcd0aaac2cbfeac24c0f167471263488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9df469feefed2d0bf2ffd73bcf22257.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Fairness-is-Not-Silence-Unmasking-Vacuous-Neutrality-in-Small-Language-Models"><a href="#Fairness-is-Not-Silence-Unmasking-Vacuous-Neutrality-in-Small-Language-Models" class="headerlink" title="Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language   Models"></a>Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language   Models</h2><p><strong>Authors:Sumanth Manduru, Carlotta Domeniconi</strong></p>
<p>The rapid adoption of Small Language Models (SLMs) for on-device and resource-constrained deployments has outpaced our understanding of their ethical risks. To the best of our knowledge, we present the first large-scale audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an overlooked “middle tier” between BERT-class encoders and flagship LLMs. Our evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma 3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we analyze both utility and fairness across ambiguous and disambiguated contexts. This evaluation reveals three key insights. First, competence and fairness need not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while exhibiting minimal bias, showing that efficient and ethical NLP is attainable. Second, social bias varies significantly by architecture: Qwen 2.5 models may appear fair, but this often reflects vacuous neutrality, random guessing, or evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2 models exhibit stronger stereotypical bias, suggesting overconfidence rather than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but increases disability-related bias in Phi-4-Mini by over 7 percentage points. These insights provide practical guidance for the responsible deployment of SLMs in applications demanding fairness and efficiency, particularly benefiting small enterprises and resource-constrained environments. </p>
<blockquote>
<p>小型语言模型（SLM）在设备和资源受限的部署中的快速采纳已经超出了我们对它们伦理风险的认知。据我们所知，我们对跨越0.5至5亿参数的指令调整SLM进行了首次大规模审计，这是一个被忽视的介于BERT类编码器和旗舰大型语言模型（LLM）之间的“中层”。我们的评估包括来自Qwen 2.5、LLaMA 3.2、Gemma 3和Phi家族的九个开源模型。我们使用BBQ基准测试在无提示的情况下分析模糊和明确语境中的实用性和公平性。这次评估揭示了三个关键见解。首先，能力和公平不必相互对立：Phi模型在F1得分上超过了90%，同时表现出极低的偏见，表明高效和伦理自然语言处理是可行的。其次，社会偏见因架构而异：Qwen 2.5模型可能看起来是公平的，但这往往反映出空洞的中立、随机猜测或回避行为，而不是真正的道德契合。相比之下，LLaMA 3.2模型表现出更强的刻板偏见，这表明过于自信而不是中立。第三，压缩带来了微妙的权衡：对于LLaMA 3.2-3B来说，使用AWQ 4位量化提高了模糊设置中的F1得分，但增加了Phi-4-Mini中与残疾相关的偏见超过7个百分点。这些见解为在需要公平和效率的应用中负责任地部署SLM提供了实际指导，尤其有益于中小企业和资源受限的环境。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08487v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文对跨越0.5至5亿参数的中等规模指令调整的小型语言模型进行了大规模的审计评估，这是介于BERT类编码器和旗舰大型语言模型之间被忽视的中间层。通过开放源代码的九个模型（包括Qwen 2.5、LLaMA 3.2、Gemma 3和Phi系列模型）的评估，利用BBQ基准测试下的零样本提示法，本文分析了模糊和明确语境下的实用性和公平性。关键发现包括：能力和公平并非对立关系；不同架构的社会偏见差异显著；压缩带来微妙的权衡等。这些见解为小型企业和资源受限环境中的语言模型负责任部署提供了实际指导。</p>
<p><strong>关键见解</strong></p>
<p>一、能力和公平并非对立关系。Phi模型在F1得分上超过90%的同时展现出微小的偏见，证明了高效且道德的NLP是可行的。</p>
<p>二、不同架构的语言模型存在显著的社会偏见差异。Qwen 2.5模型可能表现出公平，但这往往反映出空洞的中立性、随机猜测或回避行为而非真正的道德对齐。相比之下，LLaMA 3.2模型表现出更强的刻板偏见，暗示其过于自信而非中立。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08487">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3f09700b9a57629e4cfaaa8f88559620.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ffdc140c943a78ccd61f1880a72d136.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a826c65a7041cf46ca4a73f9b49293f1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Instruction-Tuned-Video-Audio-Models-Elucidate-Functional-Specialization-in-the-Brain"><a href="#Instruction-Tuned-Video-Audio-Models-Elucidate-Functional-Specialization-in-the-Brain" class="headerlink" title="Instruction-Tuned Video-Audio Models Elucidate Functional Specialization   in the Brain"></a>Instruction-Tuned Video-Audio Models Elucidate Functional Specialization   in the Brain</h2><p><strong>Authors:Subba Reddy Oota, Khushbu Pahwa, Prachi Jindal, Satya Sai Srinath Namburi, Maneesh Singh, Tanmoy Chakraborty, Bapi S. Raju, Manish Gupta</strong></p>
<p>Recent voxel-wise multimodal brain encoding studies have shown that multimodal large language models (MLLMs) exhibit a higher degree of brain alignment compared to unimodal models in both unimodal and multimodal stimulus settings. More recently, instruction-tuned multimodal models have shown to generate task-specific representations that align strongly with brain activity. However, prior work evaluating the brain alignment of MLLMs has primarily focused on unimodal settings or relied on non-instruction-tuned multimodal models for multimodal stimuli. To address this gap, we investigated brain alignment, that is, measuring the degree of predictivity of neural activity recorded while participants were watching naturalistic movies (video along with audio) with representations derived from MLLMs. We utilized instruction-specific embeddings from six video and two audio instruction-tuned MLLMs. Experiments with 13 video task-specific instructions show that instruction-tuned video MLLMs significantly outperform non-instruction-tuned multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for both video and audio tasks using language-guided instructions shows clear disentanglement in task-specific representations from MLLMs, leading to precise differentiation of multimodal functional processing in the brain. We also find that MLLM layers align hierarchically with the brain, with early sensory areas showing strong alignment with early layers, while higher-level visual and language regions align more with middle to late layers. These findings provide clear evidence for the role of task-specific instructions in improving the alignment between brain activity and MLLMs, and open new avenues for mapping joint information processing in both the systems. We make the code publicly available [<a target="_blank" rel="noopener" href="https://github.com/subbareddy248/mllm_videos]">https://github.com/subbareddy248/mllm_videos]</a>. </p>
<blockquote>
<p>近期的逐体素多模态大脑编码研究表明，与单模态模型相比，多模态大语言模型（MLLMs）在单模态和多模态刺激环境中都表现出更高的大脑对齐程度。最近，经过指令调整的多模态模型显示出能够生成与大脑活动紧密对齐的任务特定表示。然而，先前评估MLLM的大脑对齐的工作主要集中在单模态环境，或者依赖于非指令调整的多模态模型进行多模态刺激。为了弥补这一空白，我们研究了大脑对齐，即测量参与者在观看自然电影（视频和音频）时的神经活动预测度，该预测度来源于MLLMs的表示。我们使用了六个视频和两个音频指令调整过的MLLM的指令特定嵌入。有13个视频任务特定指令的实验显示，指令调整过的视频MLLMs显著优于非指令调整过的多模态模型（高出15%）和单模态模型（高出20%）。我们对视频和音频任务使用语言指导指令的MLLM评估显示，任务特定表示在MLLM中明确分离，导致大脑中的多模态功能处理精确区分。我们还发现，MLLM层与大脑层次对齐，早期感官区域与早期层对齐强烈，而高级视觉和语言区域与中层至后期层对齐更多。这些发现清楚地证明了任务特定指令在提高大脑活动与MLLM对齐方面的作用，并为映射两个系统中的联合信息处理开辟了新途径。我们公开提供了代码：[<a target="_blank" rel="noopener" href="https://github.com/subbareddy248/mllm_videos]%E3%80%82">https://github.com/subbareddy248/mllm_videos]。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08277v1">PDF</a> 39 pages, 22 figures</p>
<p><strong>摘要</strong></p>
<p>最近的多模态大脑编码研究表明，与单模态模型相比，多模态大语言模型（MLLMs）在单模态和多模态刺激环境下与大脑的契合度更高。特别是经过指令调整的多模态模型能够生成与大脑活动高度契合的任务特定表征。然而，先前对MLLMs的大脑契合度的评估主要集中在单模态环境，或在多模态刺激下依赖于非指令调整的多模态模型。本研究旨在解决这一空白，通过测量参与者在观看自然电影（视频和音频）时的神经活动与MLLMs表征的预测度来探究大脑契合度。我们使用来自六个视频和两个音频指令调整过的MLLMs的指令特定嵌入。实验显示，针对视频任务的指令调整过的MLLMs比非指令调整的多模态模型高出15%，比单模态模型高出20%。我们对视频和音频任务使用语言指导指令的MLLMs的评估显示，任务特定表征的清晰分离导致大脑中的多模态功能处理的精确区分。我们还发现，MLLM层与大脑的层次结构相吻合，早期感官区域与早期层表现出强烈的契合度，而高级视觉和语言区域则与中层至后期层更加契合。这些发现证明了任务特定指令在提高大脑活动与MLLMs契合度方面的作用，并为映射两个系统的联合信息处理开辟了新途径。我们公开了相关代码：[<a target="_blank" rel="noopener" href="https://github.com/subbareddy248/mllm_videos]%E3%80%82">https://github.com/subbareddy248/mllm_videos]。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>多模态大语言模型（MLLMs）在单模态和多模态刺激环境下与大脑的契合度较高。</li>
<li>指令调整过的多模态模型在生成任务特定表征方面表现出色，与大脑活动高度契合。</li>
<li>与非指令调整的多模态模型和单模态模型相比，指令调整过的视频MLLMs性能显著提高。</li>
<li>MLLM的任务特定表征能够精确区分大脑中的多模态功能处理。</li>
<li>MLLM层与大脑层次结构相吻合，早期和高级感官区域的契合度表现不同。</li>
<li>任务特定指令有助于提高大脑活动与MLLMs的契合度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08277">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fb05c3ca48c75e77d663d09270b2690f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a259d1920b9309f95c804e685d6745d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a683f4c1a37643ef4860af64d473396f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd819f9d6e491f4e3b28fa756d56722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-307b46993b2f27eaf5fbc7e7205ab656.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="GTR-CoT-Graph-Traversal-as-Visual-Chain-of-Thought-for-Molecular-Structure-Recognition"><a href="#GTR-CoT-Graph-Traversal-as-Visual-Chain-of-Thought-for-Molecular-Structure-Recognition" class="headerlink" title="GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular   Structure Recognition"></a>GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular   Structure Recognition</h2><p><strong>Authors:Jingchao Wang, Haote Yang, Jiang Wu, Yifan He, Xingjian Wei, Yinfan Wang, Chengjin Liu, Lingli Ge, Lijun Wu, Bin Wang, Dahua Lin, Conghui He</strong></p>
<p>Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What You’ve Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at <a target="_blank" rel="noopener" href="https://github.com/opendatalab/GTR-CoT">https://github.com/opendatalab/GTR-CoT</a>. </p>
<blockquote>
<p>光学化学结构识别（OCSR）是将化学知识数字化的关键环节，它通过转化分子图像为机器可读的格式来实现。尽管最近的视觉语言模型（VLM）在此任务中显示出潜力，但它们在处理复杂的分子结构和不一致的注释时经常遇到困难。为了克服这些挑战，我们引入了GTR-Mol-VLM这一新型框架，它具有两个关键创新点：（一）通过序列化的原子键预测逐步解析分子图的思维链机制，模拟人类的推理过程；（二）忠实识别你所看到的的数据中心原则，解决图像中的缩略结构与扩展注释之间的不匹配问题。为了支持模型开发，我们构建了大规模的指令调整数据集GTR-CoT-1.3M，其中包含精心修正的注释，并推出了MolRec-Bench这一首个针对OCSR中图形解析准确性的精细评估设计的基准测试。综合实验表明，GTR-Mol-VLM相较于专业模型、化学领域的VLM以及商业通用VLM取得了优越的结果。值得注意的是，在处理带有官能团缩略的分子图像场景中，GTR-Mol-VLM在SMILES和图形度量指标上的表现均优于第二名基准测试约14个百分点。我们希望这项工作能推动OCSR技术更有效地满足现实世界的需要，从而促进化学信息学和人工智能科学领域的发展。我们将在<a target="_blank" rel="noopener" href="https://github.com/opendatalab/GTR-CoT%E4%B8%8A%E5%8F%91%E5%B8%83GTR-CoT%E3%80%82">https://github.com/opendatalab/GTR-CoT上发布GTR-CoT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07553v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>光学化学结构识别（OCSR）是数字化化学知识的重要技术，能将分子图像转化为机器可读的格式。针对现有视觉语言模型在处理复杂分子结构和不一致注释时的挑战，本文提出了GTR-Mol-VLM框架，包含两项关键创新：一是图遍历作为视觉思维链机制，通过连续的原子键预测逐步解析分子图，模拟人类推理；二是忠实识别你所见到的数据为中心的原则，解决图像中的简化结构与扩展注释之间的不匹配问题。为支持模型开发，本文构建了GTR-CoT-1.3M大规模指令调整数据集，并引入了MolRec-Bench基准测试，用于精细评估OCSR中图解析的准确性。实验表明，GTR-Mol-VLM相较于专业模型、化学领域视觉语言模型和商业通用视觉语言模型取得了显著优势，特别是在处理带有功能组缩略语的分子图像时，GTR-Mol-VLM在SMILES和图形指标上的表现均优于第二名基准测试约14个百分点。本文工作将推动OCSR技术更好地满足现实需求，推动化学信息学和人工智能科学领域的发展。GTR-CoT将在<a target="_blank" rel="noopener" href="https://github.com/opendatalab/gtr-cot%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/opendatalab/GTR-CoT发布。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>光学化学结构识别（OCSR）是数字化化学知识的关键技术。</li>
<li>现有视觉语言模型在处理复杂分子结构和不一致注释时面临挑战。</li>
<li>GTR-Mol-VLM框架通过模拟人类推理和解决图像与注释不匹配问题来克服这些挑战。</li>
<li>GTR-CoT-1.3M数据集用于支持模型开发，并提供精细的OCSR图解析评估。</li>
<li>GTR-Mol-VLM在处理和解析带有功能组缩略语的分子图像时表现优异。</li>
<li>GTR-Mol-VLM在SMILES和图形指标上的表现优于其他模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07553">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f6254963c6362b9410cecc7d470ae6b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e66ff9a7989f7206f8faf4a9bcbc1b83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2356decec6238f37994b975776cad5e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04f3054bcb2af3a39565c202059e7abb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c47d29fef216a4706ab4f75e44828a4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DefenderBench-A-Toolkit-for-Evaluating-Language-Agents-in-Cybersecurity-Environments"><a href="#DefenderBench-A-Toolkit-for-Evaluating-Language-Agents-in-Cybersecurity-Environments" class="headerlink" title="DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments"></a>DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments</h2><p><strong>Authors:Chiyu Zhang, Marc-Alexandre Cote, Michael Albada, Anush Sankaran, Jack W. Stokes, Tong Wang, Amir Abdi, William Blum, Muhammad Abdul-Mageed</strong></p>
<p>Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBench’s modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench">https://github.com/microsoft/DefenderBench</a>. </p>
<blockquote>
<p>大型语言模型（LLM）代理在理解和推理人类语言方面表现出令人印象深刻的性能，但它们在网络安全方面的潜力仍未得到充分探索。我们推出了DefenderBench，这是一个实用的开源工具包，旨在评估代理在攻击、防御和基于网络安全知识的任务中的表现。DefenderBench包括网络入侵环境、恶意内容检测环境、代码漏洞分析环境和网络安全知识评估环境。它专门设计用于研究人员，经济实惠且易于访问，同时提供公平严格的评估。我们使用标准化的代理框架对多个最新和流行的大型语言模型进行评估，包括开源和闭源模型。我们的结果显示，Claude-3.7-sonnet表现最佳，在DefenderBench上的得分为81.65，其次是Claude-3.7-sonnet-think，得分为78.40，而表现最好的开源模型Llama 3.3 70B紧随其后，得分为71.81。DefenderBench的模块化设计允许无缝集成自定义的大型语言模型和任务，促进了可重复性和公平比较。DefenderBench的匿名版本可在<a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/microsoft/DefenderBench找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00739v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>大型语言模型（LLM）在理解人类语言和推理方面展现出惊人的能力，但在网络安全领域的潜力尚未得到充分探索。介绍了一款名为DefenderBench的实用开源工具包，用于评估语言模型在攻击、防御和基于网络安全知识任务方面的表现。该工具包包括网络入侵、恶意内容检测、代码漏洞分析和网络安全知识评估等环境。它旨在让研究人员以负担得起且易于访问的方式，进行公平和严格的语言模型评估。对现有顶尖的大型语言模型进行了基准测试，包括开放式和封闭式模型。结果显示，Claude-3.7-sonnet表现最佳，得分为81.65，其次是Claude-3.7-sonnet-think，得分为78.40，而领先的开放式模型Llama 3.3 70B紧随其后，得分为71.81。DefenderBench的模块化设计允许无缝集成自定义的大型语言模型和任务，促进研究的可重复性和公平比较。匿名的DefenderBench版本可在<a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/microsoft/DefenderBench获取。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）在网络安全领域的潜力尚未得到充分探索。</li>
<li>DefenderBench是一个用于评估语言模型在网络安全领域的实用开源工具包。</li>
<li>DefenderBench包括网络入侵、恶意内容检测、代码漏洞分析和网络安全知识评估等环境。</li>
<li>DefenderBench旨在让研究人员以负担得起且易于访问的方式进行公平和严格的语言模型评估。</li>
<li>基准测试结果显示，Claude-3.7-sonnet在DefenderBench测试中表现最佳。</li>
<li>DefenderBench的模块化设计允许无缝集成自定义的大型语言模型和任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00739">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a6bc86fc70229af47be50d4b82aba1e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bab7f4d11665b5e3feed5f17ce028c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ce477798b819c94cd5e4fe2f624ddee.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling"><a href="#Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling" class="headerlink" title="Curse of High Dimensionality Issue in Transformer for Long-context   Modeling"></a>Curse of High Dimensionality Issue in Transformer for Long-context   Modeling</h2><p><strong>Authors:Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan</strong></p>
<p>Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at <a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention">https://github.com/bolixinyu/DynamicGroupAttention</a>. </p>
<blockquote>
<p>基于Transformer的大型语言模型（LLM）通过自注意力机制捕捉长程依赖关系，在自然语言处理任务上表现出色。然而，由于冗余的注意力计算，长上下文建模面临着巨大的计算效率低下的问题：虽然注意力权重通常是稀疏的，但所有标记都消耗着平等的计算资源。在本文中，我们将传统的概率序列建模重新表述为“监督学习任务”，这能够区分相关和无关的标记，并更清楚地了解冗余情况。基于这种重新表述，我们从理论上分析了注意力稀疏性，发现只有少数标记对预测有重大贡献。在此基础上，我们将注意力优化表述为线性编码问题，并提出“分组编码策略”，理论上显示出其提高对抗随机噪声的稳健性和提高学习效率的能力。受此启发，我们提出了“动态分组注意力”（DGA），它利用分组编码来通过聚合不太重要的标记来明确减少冗余的注意力计算。经验结果表明，我们的DGA在显著降低计算成本的同时，保持了竞争力。代码可在<a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bolixinyu/DynamicGroupAttention找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22107v3">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于Transformer的大型语言模型（LLM）在自然语言处理任务中的计算效率问题。针对长文本建模中由于冗余注意力计算导致的计算效率低下问题，文章提出了一种新的方法——动态组注意力（DGA）。该方法通过分离重要和不重要的令牌，理论上分析了注意力稀疏性，并通过线性编码问题提出了分组编码策略，从而减少冗余并提高计算效率。经验结果表明，DGA在保持竞争力的同时显著降低了计算成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-based LLMs 擅长捕捉长距离依赖关系。</li>
<li>传统概率序列建模被重新表述为一个监督学习任务。</li>
<li>注意力稀疏性表明只有少数令牌对预测有重要贡献。</li>
<li>提出了理论上的注意力优化作为线性编码问题。</li>
<li>提出了分组编码策略和动态组注意力（DGA）方法。</li>
<li>DGA 通过减少不重要令牌的注意力计算来明确减少冗余。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22107">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fbd85c7abb481e216fc601671865643d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eca6ec960b889a2e81e3ec88cc965ea2.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-12/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-47047f50c44cb37898ed12da40830360.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-12/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-a1b4e28a76543540f761275a06b6abda.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
