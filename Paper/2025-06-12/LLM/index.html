<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8d1a08e8c7dbb9ea8fc7bc567907c814.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-12-æ›´æ–°"><a href="#2025-06-12-æ›´æ–°" class="headerlink" title="2025-06-12 æ›´æ–°"></a>2025-06-12 æ›´æ–°</h1><h2 id="VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning"><a href="#VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning" class="headerlink" title="VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning"></a>VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning</h2><p><strong>Authors:Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</strong></p>
<p>Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems. </p>
<blockquote>
<p>åœ¨åŠ¨æ€ç¯å¢ƒä¸­åè°ƒå¤šä¸ªå®ä½“ä»£ç†ä»ç„¶æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè¿™éœ€è¦æ„ŸçŸ¥é©±åŠ¨çš„æ¨ç†å’Œå¯æ‰©å±•çš„åˆä½œç­–ç•¥ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶å·²ç»åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤šä»£ç†è§„åˆ’ï¼Œä½†å¾ˆå°‘æœ‰äººå¼€å§‹æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”¨äºè§†è§‰æ¨ç†ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºäºVLMçš„æ–¹æ³•åœ¨æ”¯æŒå¤šç§å®ä½“ç±»å‹æ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VIKI-Benchï¼Œè¿™æ˜¯é’ˆå¯¹å®ä½“å¤šä»£ç†åˆä½œå®šåˆ¶çš„é¦–ä¸ªåˆ†å±‚åŸºå‡†ï¼ŒåŒ…å«ä¸‰ä¸ªç»“æ„åŒ–çº§åˆ«ï¼šä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ã€‚VIKI-BenchåŒ…æ‹¬å„ç§æœºå™¨äººå®ä½“ã€å¤šè§†å›¾è§†è§‰è§‚å¯Ÿå’Œç»“æ„åŒ–çš„ç›‘ç£ä¿¡å·ï¼Œä»¥è¯„ä¼°åŸºäºè§†è§‰è¾“å…¥çš„æ¨ç†ã€‚ä¸ºäº†è¯æ˜VIKI-Benchçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†VIKI-Rï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œä½¿ç”¨Chain-of-Thoughtæ³¨é‡Šçš„æ¼”ç¤ºå¯¹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œéšååœ¨å¤šçº§å¥–åŠ±ä¿¡å·ä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVIKI-Råœ¨å„çº§ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿä¿ƒè¿›å¼‚æ„ä»£ç†ä¹‹é—´ç»„åˆåˆä½œæ¨¡å¼çš„å‡ºç°ã€‚æ€»ä¹‹ï¼ŒVIKI-Benchå’ŒVIKI-Rä¸ºæ¨è¿›å®ä½“AIç³»ç»Ÿçš„å¤šä»£ç†ã€è§†è§‰é©±åŠ¨åˆä½œæä¾›äº†ç»Ÿä¸€çš„æµ‹è¯•å¹³å°å’Œæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09049v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://faceong.github.io/VIKI-R/">https://faceong.github.io/VIKI-R/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŠ¨æ€ç¯å¢ƒä¸­åè°ƒå¤šä¸ªå®ä½“ä»£ç†çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œéœ€è¦æ„ŸçŸ¥é©±åŠ¨çš„æ¨ç†å’Œå¯æ‰©å±•çš„åˆä½œç­–ç•¥ã€‚è™½ç„¶æœ€è¿‘çš„å·¥ä½œå·²ç»åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤šä»£ç†è§„åˆ’ï¼Œä½†å¾ˆå°‘æœ‰äººå¼€å§‹æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”¨äºè§†è§‰æ¨ç†ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†VIKI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå®ä½“å¤šä»£ç†åˆä½œè®¾è®¡çš„åˆ†å±‚åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªç»“æ„åŒ–çº§åˆ«ï¼šä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†VIKI-Ræ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨Chain-of-Thoughtæ³¨é‡Šæ¼”ç¤ºå¯¹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç„¶ååœ¨å¤šå±‚æ¬¡å¥–åŠ±ä¿¡å·ä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒVIKI-Råœ¨å„çº§ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶ä¸”å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿä¿ƒè¿›å¼‚æ„ä»£ç†ä¹‹é—´å‡ºç°ç»„åˆåˆä½œæ¨¡å¼ã€‚æ€»çš„æ¥è¯´ï¼ŒVIKI-Benchå’ŒVIKI-Rä¸ºæ¨è¿›å®ä½“AIç³»ç»Ÿçš„å¤šä»£ç†è§†è§‰é©±åŠ¨åˆä½œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æµ‹è¯•å¹³å°å’Œè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå®ä½“ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åè°ƒæ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŒ‘æˆ˜ä¹‹ä¸€ï¼Œéœ€è¦æ„ŸçŸ¥é©±åŠ¨çš„æ¨ç†å’Œåˆä½œç­–ç•¥ã€‚</li>
<li>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç”¨äºå¤šä»£ç†è§„åˆ’ï¼Œä½†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è§†è§‰æ¨ç†æ–¹é¢çš„åº”ç”¨ä»æœ‰é™ã€‚</li>
<li>VIKI-Benchæ˜¯é¦–ä¸ªé’ˆå¯¹å®ä½“å¤šä»£ç†åˆä½œçš„åˆ†å±‚åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä»£ç†æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ä¸‰ä¸ªç»“æ„åŒ–çº§åˆ«ã€‚</li>
<li>VIKI-BenchåŒ…å«å¤šç§æœºå™¨äººå®ä½“ã€å¤šè§†è§’è§†è§‰è§‚å¯Ÿå’Œç»“æ„åŒ–ç›‘ç£ä¿¡å·ï¼Œä»¥è¯„ä¼°è§†è§‰è¾“å…¥åŸºç¡€ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>VIKI-Ræ¡†æ¶é€šè¿‡åˆ©ç”¨å¸¦æœ‰Chain-of-Thoughtæ³¨é‡Šçš„æ¼”ç¤ºå¯¹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨å¤šå±‚æ¬¡å¥–åŠ±ä¿¡å·ä¸‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒVIKI-Råœ¨å„çº§ä»»åŠ¡ä¸Šå‡ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9f318d1c1f344134a0ccdcac656830d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b4ff7c2f9df7c67670ee1acf6207be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0a3569e6a044d60b77d21c9a1d0f4e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-546e911c893917079b05fe314b9dedc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-493cacde6faf7b9e2c94ba6ec86deaf5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FZOO-Fast-Zeroth-Order-Optimizer-for-Fine-Tuning-Large-Language-Models-towards-Adam-Scale-Speed"><a href="#FZOO-Fast-Zeroth-Order-Optimizer-for-Fine-Tuning-Large-Language-Models-towards-Adam-Scale-Speed" class="headerlink" title="FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models   towards Adam-Scale Speed"></a>FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models   towards Adam-Scale Speed</h2><p><strong>Authors:Sizhe Dang, Yangyang Guo, Yanjun Zhao, Haishan Ye, Xiaodong Zheng, Guang Dai, Ivor Tsang</strong></p>
<p>Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually require many more steps to converge. Can this trade-off between speed and memory in ZO be fundamentally improved? Normalized-SGD demonstrates strong empirical performance with greater memory efficiency than Adam. In light of this, we introduce FZOO, a Fast Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward passes needed for convergence by employing batched one-sided estimates that adapt step sizes based on the standard deviation of batch losses. It also accelerates per-batch computation through the use of Rademacher random vector perturbations coupled with CUDAâ€™s parallel processing. Extensive experiments on diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3, across 11 tasks validate FZOOâ€™s effectiveness. On average, FZOO outperforms MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy and an 18 times reduction in forward passes compared to MeZO, achieving convergence speeds comparable to Adam. We also provide theoretical analysis proving FZOOâ€™s formal equivalence to a normalized-SGD update rule and its convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling even larger memory savings. Overall, our results make single-GPU, high-speed, full-parameter fine-tuning practical and point toward future work on memory-efficient pre-training. </p>
<blockquote>
<p>å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸å¸¸é¢ä¸´GPUå†…å­˜ç“¶é¢ˆé—®é¢˜ï¼šåƒAdamè¿™æ ·çš„ä¸€é˜¶ä¼˜åŒ–å™¨çš„åå‘ä¼ æ’­ä¼šå°†å†…å­˜ä½¿ç”¨é‡å¢åŠ åˆ°æ¨ç†æ°´å¹³çš„10å€ä»¥ä¸Šï¼ˆä¾‹å¦‚ï¼ŒOPT-30Béœ€è¦633GBï¼‰ã€‚é›¶é˜¶ï¼ˆZOï¼‰ä¼˜åŒ–å™¨é€šè¿‡ä»…ä»å‰å‘ä¼ é€’ä¸­ä¼°è®¡æ¢¯åº¦æ¥é¿å…è¿™ä¸€æˆæœ¬ï¼Œä½†ç°æœ‰æ–¹æ³•å¦‚MeZOé€šå¸¸éœ€è¦æ›´å¤šçš„æ­¥éª¤æ‰èƒ½è¾¾åˆ°æ”¶æ•›ã€‚ZOåœ¨é€Ÿåº¦å’Œå†…å­˜ä¹‹é—´çš„æƒè¡¡èƒ½å¦å¾—åˆ°æ ¹æœ¬æ”¹è¿›ï¼ŸNormalized-SGDåœ¨å†…å­˜æ•ˆç‡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„å®è¯æ€§èƒ½ï¼Œä¼˜äºAdamã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†FZOOï¼Œä¸€ä¸ªé¢å‘Adamçº§é€Ÿåº¦çš„å¿«é€Ÿé›¶é˜¶ä¼˜åŒ–å™¨ã€‚FZOOé€šè¿‡é‡‡ç”¨æ‰¹å¤„ç†å•è¾¹ä¼°è®¡æ¥å‡å°‘è¾¾åˆ°æ”¶æ•›æ‰€éœ€çš„æ€»å‰å‘ä¼ é€’æ¬¡æ•°ï¼Œè¯¥ä¼°è®¡æ ¹æ®æ‰¹æŸå¤±çš„æ ‡å‡†å·®è‡ªé€‚åº”è°ƒæ•´æ­¥é•¿ã€‚å®ƒè¿˜é€šè¿‡ç»“åˆRademacheréšæœºå‘é‡æ‰°åŠ¨å’ŒCUDAçš„å¹¶è¡Œå¤„ç†æ¥åŠ é€Ÿæ¯æ‰¹è®¡ç®—ã€‚åœ¨RoBERTa-largeã€OPTï¼ˆ350M-66Bï¼‰ã€Phi-2å’ŒLlama3ç­‰å¤šç§æ¨¡å‹ä¸Šè¿›è¡Œçš„11é¡¹ä»»åŠ¡çš„å¤§é‡å®éªŒéªŒè¯äº†FZOOçš„æœ‰æ•ˆæ€§ã€‚å¹³å‡è€Œè¨€ï¼ŒFZOOåœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºMeZO 3%ï¼ŒåŒæ—¶æ‰€éœ€çš„å‰å‘ä¼ é€’æ¬¡æ•°å‡å°‘äº†ä¸‰å€ã€‚å¯¹äºRoBERTa-largeï¼ŒFZOOåœ¨å‡†ç¡®æ€§æ–¹é¢å®ç°äº†å¹³å‡5.6%çš„æ”¹è¿›ï¼Œä¸å‰å‘ä¼ é€’ç›¸æ¯”å‡å°‘äº†18å€ï¼Œå®ç°äº†ä¸Adamç›¸å½“çš„æ”¶æ•›é€Ÿåº¦ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ç†è®ºä¸Šçš„åˆ†æï¼Œè¯æ˜FZOOä¸normalized-SGDæ›´æ–°è§„åˆ™çš„å½¢å¼ç­‰æ•ˆæ€§åŠå…¶æ”¶æ•›ä¿è¯ã€‚FZOOå¯ä»¥å¹³ç¨³åœ°é›†æˆåˆ°PEFTæŠ€æœ¯ä¸­ï¼Œä»è€Œå®ç°æ›´å¤§çš„å†…å­˜èŠ‚çœã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœä½¿å•GPUã€é«˜é€Ÿã€å…¨å‚æ•°å¾®è°ƒå˜å¾—å®ç”¨ï¼Œå¹¶ä¸ºæœªæ¥çš„å†…å­˜é«˜æ•ˆé¢„è®­ç»ƒå·¥ä½œæŒ‡æ˜äº†æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09034v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒå¸¸å¸¸é¢ä¸´GPUå†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚ä¸€é˜¶ä¼˜åŒ–å™¨å¦‚Adamçš„åå‘ä¼ é€’ä¼šå¢åŠ å†…å­˜ä½¿ç”¨è‡³æ¨ç†é˜¶æ®µçš„åå€ä»¥ä¸Šã€‚é›¶é˜¶ï¼ˆZOï¼‰ä¼˜åŒ–å™¨é€šè¿‡ä»…ä»å‰å‘ä¼ é€’ä¸­ä¼°ç®—æ¢¯åº¦æ¥é¿å…è¿™ä¸€æˆæœ¬ï¼Œä½†ç°æœ‰æ–¹æ³•å¦‚MeZOé€šå¸¸éœ€è¦æ›´å¤šçš„æ­¥éª¤æ¥è¾¾åˆ°æ”¶æ•›ã€‚æ˜¯å¦å­˜åœ¨ä¸€ç§å¯ä»¥æ ¹æœ¬æ”¹å–„ZOåœ¨é€Ÿåº¦å’Œå†…å­˜ä¹‹é—´æƒè¡¡çš„æ–¹æ³•ï¼ŸNormalized-SGDåœ¨å†…å­˜æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„å®è¯æ€§èƒ½ï¼Œè¶…è¶Šäº†Adamã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å‘Adamè§„æ¨¡é€Ÿåº¦çš„å¿«é€Ÿé›¶é˜¶ä¼˜åŒ–å™¨FZOOã€‚FZOOé€šè¿‡é‡‡ç”¨æ‰¹å¤„ç†å•ä¾§ä¼°è®¡ï¼Œæ ¹æ®æ‰¹æŸå¤±çš„æ ‡å‡†å·®è°ƒæ•´æ­¥é•¿ï¼Œå‡å°‘äº†è¾¾åˆ°æ”¶æ•›æ‰€éœ€çš„å‰å‘ä¼ é€’æ¬¡æ•°ã€‚å®ƒè¿˜é€šè¿‡Rademacheréšæœºå‘é‡æ‰°åŠ¨ç»“åˆCUDAçš„å¹¶è¡Œå¤„ç†æ¥åŠ é€Ÿæ¯æ‰¹æ¬¡çš„è®¡ç®—ã€‚åœ¨åŒ…æ‹¬RoBERTa-largeã€OPTï¼ˆ350M-66Bï¼‰ã€Phi-2å’ŒLlama3ç­‰å¤šç§æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒï¼Œä»¥åŠåœ¨11ä¸ªä»»åŠ¡ä¸Šçš„éªŒè¯ï¼Œè¯æ˜äº†FZOOçš„æœ‰æ•ˆæ€§ã€‚å¹³å‡è€Œè¨€ï¼ŒFZOOåœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºMeZOçš„3%ï¼ŒåŒæ—¶æ‰€éœ€çš„å‰å‘ä¼ é€’æ¬¡æ•°å‡å°‘äº†ä¸‰å€ã€‚å¯¹äºRoBERTa-largeï¼ŒFZOOåœ¨å‡†ç¡®æ€§æ–¹é¢å®ç°äº†å¹³å‡æé«˜5.6%ï¼Œå¹¶ä¸”ä¸å‰å‘ä¼ é€’æ¬¡æ•°ç›¸æ¯”ï¼Œå®ç°äº†ä¸MeZOç›¸æ¯”çš„18å€å‡å°‘ï¼Œæ”¶æ•›é€Ÿåº¦å¯ä¸Adamç›¸å½“ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ç†è®ºä¸Šçš„åˆ†æï¼Œè¯æ˜äº†FZOOä¸normalized-SGDæ›´æ–°è§„åˆ™çš„æ­£å¼ç­‰ä»·æ€§ä»¥åŠå…¶æ”¶æ•›æ€§çš„ä¿è¯ã€‚FZOOå¯ä»¥é¡ºåˆ©é›†æˆåˆ°PEFTæŠ€æœ¯ä¸­ï¼Œä»è€Œå®ç°æ›´å¤§çš„å†…å­˜èŠ‚çœã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜å•GPUé«˜é€Ÿå…¨å‚æ•°å¾®è°ƒæ˜¯å®ç”¨çš„ï¼Œå¹¶ä¸ºæœªæ¥çš„å†…å­˜é«˜æ•ˆé¢„è®­ç»ƒå·¥ä½œæŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒé¢ä¸´GPUå†…å­˜ç“¶é¢ˆé—®é¢˜ï¼Œä¸€é˜¶ä¼˜åŒ–å™¨å¦‚Adamå†…å­˜ä½¿ç”¨é‡å¤§ã€‚</li>
<li>é›¶é˜¶ä¼˜åŒ–å™¨é€šè¿‡ä»…ä»å‰å‘ä¼ é€’ä¼°ç®—æ¢¯åº¦æ¥é¿å…è¿™ä¸ªé—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•å¦‚MeZOæ”¶æ•›é€Ÿåº¦æ…¢ã€‚</li>
<li>FZOOä½œä¸ºä¸€ç§å¿«é€Ÿé›¶é˜¶ä¼˜åŒ–å™¨è¢«å¼•å…¥ï¼Œé€šè¿‡é‡‡ç”¨æ‰¹å¤„ç†å•ä¾§ä¼°è®¡å’ŒRademacheréšæœºå‘é‡æ‰°åŠ¨ç­‰æŠ€æœ¯ï¼Œæé«˜äº†å†…å­˜æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>FZOOåœ¨å¤šç§æ¨¡å‹å’Œä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æœ‰æ•ˆæ€§ï¼Œç›¸æ¯”MeZOåœ¨å‡†ç¡®æ€§ä¸Šæœ‰æ‰€æå‡ï¼ŒåŒæ—¶å‡å°‘äº†æ‰€éœ€çš„å‰å‘ä¼ é€’æ¬¡æ•°ã€‚</li>
<li>FZOOä¸normalized-SGDçš„ç†è®ºåˆ†æè¯æ˜äº†å…¶æ­£å¼ç­‰ä»·æ€§å’Œæ”¶æ•›æ€§çš„ä¿è¯ã€‚</li>
<li>FZOOå¯ä»¥é¡ºåˆ©é›†æˆåˆ°PEFTæŠ€æœ¯ä¸­ï¼Œå®ç°æ›´å¤§çš„å†…å­˜èŠ‚çœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e938896dc06515572a07a775cc453a93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2c2ac489a1559df4d1e8a469cf92730.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning"><a href="#Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning" class="headerlink" title="Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via   Reinforcement Learning"></a>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via   Reinforcement Learning</h2><p><strong>Authors:Haozhen Zhang, Tao Feng, Jiaxuan You</strong></p>
<p>The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave â€œthinkâ€ actions (internal deliberation) with â€œrouteâ€ actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1">https://github.com/ulab-uiuc/Router-R1</a>. </p>
<blockquote>
<p>å¤šæ ·çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿…é€Ÿæ¶Œç°ï¼Œæ¨åŠ¨äº†LLMè·¯ç”±å™¨çš„å‘å±•ï¼Œè¿™äº›è·¯ç”±å™¨å°†ç”¨æˆ·æŸ¥è¯¢åˆ†é…ç»™æœ€åˆé€‚çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMè·¯ç”±å™¨é€šå¸¸æ‰§è¡Œå•è½®ä¸€å¯¹ä¸€æ˜ å°„ï¼ˆå³ï¼Œå°†æ¯ä¸ªæŸ¥è¯¢å­¤ç«‹åœ°åˆ†é…ç»™ä¸€ä¸ªå•ä¸€æ¨¡å‹ï¼‰ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè¿™äº›å¤æ‚ä»»åŠ¡éœ€è¦å¤šä¸ªLLMçš„äº’è¡¥ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶<strong>Router-R1</strong>ï¼Œå°†å¤šLLMè·¯ç”±å’Œèšåˆåˆ¶å®šä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ã€‚Router-R1å°†è·¯ç”±å™¨æœ¬èº«å®ä¾‹åŒ–ä¸ºä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„LLMï¼Œåˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›å°†â€œæ€è€ƒâ€è¡ŒåŠ¨ï¼ˆå†…éƒ¨æ€è€ƒï¼‰ä¸â€œè·¯ç”±â€è¡ŒåŠ¨ï¼ˆåŠ¨æ€æ¨¡å‹è°ƒç”¨ï¼‰äº¤ç»‡åœ¨ä¸€èµ·ï¼Œå¹¶å°†æ¯ä¸ªå“åº”é›†æˆåˆ°ä¸æ–­å‘å±•çš„ä¸Šä¸‹æ–‡ä¸­ã€‚ä¸ºäº†æŒ‡å¯¼å­¦ä¹ ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è½»é‡çº§çš„åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼ŒåŒ…æ‹¬æ ¼å¼å¥–åŠ±ã€æœ€ç»ˆæˆæœå¥–åŠ±ä»¥åŠä¸€ç§æ–°å‹çš„æˆæœ¬å¥–åŠ±ï¼Œç”¨äºæ€§èƒ½å’Œæˆæœ¬æƒè¡¡ä¼˜åŒ–ï¼Œä»è€Œä¸ºé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ€§èƒ½æˆæœ¬æƒè¡¡å¼€è¾Ÿäº†é“è·¯ã€‚Router-R1ä»…ä¾èµ–äºç®€å•çš„æ¨¡å‹æè¿°ç¬¦ï¼Œå¦‚å®šä»·ã€å»¶è¿Ÿå’Œç¤ºä¾‹æ€§èƒ½ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„æ¨¡å‹é€‰æ‹©ã€‚åœ¨ä¸ƒä¸ªé€šç”¨å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRouter-R1ä¼˜äºå‡ ä¸ªå¼ºå¤§çš„åŸºçº¿ï¼Œåœ¨ä¿æŒç¨³å¥çš„æ³›åŒ–å’Œæˆæœ¬ç®¡ç†çš„åŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1%E3%80%82">https://github.com/ulab-uiuc/Router-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09033v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1">https://github.com/ulab-uiuc/Router-R1</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†LLMè·¯ç”±å™¨çš„å‡ºç°ï¼Œç”¨äºå°†ç”¨æˆ·æŸ¥è¯¢åˆ†é…ç»™æœ€åˆé€‚çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰LLMè·¯ç”±å™¨é€šå¸¸æ‰§è¡Œå•ä¸€å›åˆä¸€å¯¹ä¸€æ˜ å°„ï¼Œå³å­¤ç«‹åœ°å°†æ¯ä¸ªæŸ¥è¯¢åˆ†é…ç»™ä¸€ä¸ªæ¨¡å‹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å¤šä¸ªLLMçš„äº’è¡¥ä¼˜åŠ¿ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„Router-R1æ¡†æ¶ï¼Œå°†å¤šLLMè·¯ç”±å’Œèšåˆå…¬å¼åŒ–ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–è¿‡ç¨‹ã€‚Router-R1å®ä¾‹åŒ–è·¯ç”±å™¨æœ¬èº«ä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„LLMï¼Œåˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›äº¤æ›¿è¿›è¡Œâ€œæ€è€ƒâ€è¡ŒåŠ¨ï¼ˆå†…éƒ¨æ€è€ƒï¼‰å’Œâ€œè·¯ç”±â€è¡ŒåŠ¨ï¼ˆåŠ¨æ€æ¨¡å‹è°ƒç”¨ï¼‰ï¼Œå¹¶å°†æ¯ä¸ªå“åº”é›†æˆåˆ°ä¸æ–­å‘å±•çš„ä¸Šä¸‹æ–‡ä¸­ã€‚ä¸ºäº†æŒ‡å¯¼å­¦ä¹ ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è½»é‡çº§çš„åŸºäºè§„åˆ™å¥–åŠ±æœºåˆ¶ï¼ŒåŒ…æ‹¬æ ¼å¼å¥–åŠ±ã€æœ€ç»ˆæˆæœå¥–åŠ±å’Œä¸€ç§æ–°å‹æˆæœ¬å¥–åŠ±ï¼Œç”¨äºæ€§èƒ½å’Œæˆæœ¬æƒè¡¡ä¼˜åŒ–ï¼Œä»è€Œä¸ºé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ€§èƒ½æˆæœ¬æƒè¡¡å¼€è¾Ÿäº†é“è·¯ã€‚Router-R1ä»…ä¾èµ–äºç®€å•çš„æ¨¡å‹æè¿°ç¬¦ï¼ˆå¦‚å®šä»·ã€å»¶è¿Ÿå’Œç¤ºä¾‹æ€§èƒ½ï¼‰ï¼Œå¯å®ç°å¯¹æœªè§æ¨¡å‹çš„å¼ºå¤§æ³›åŒ–ã€‚åœ¨ä¸ƒä¸ªé€šç”¨å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRouter-R1ä¼˜äºå‡ ä¸ªå¼ºå¤§çš„åŸºçº¿ï¼Œåœ¨ä¿æŒå“è¶Šæ€§èƒ½çš„åŒæ—¶å®ç°ç¨³å¥çš„æ³›åŒ–å’Œæˆæœ¬ç®¡ç†ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ulab-uiuc/Router-R1æ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMè·¯ç”±å™¨çš„å‡ºç°æ˜¯ä¸ºäº†æ ¹æ®ç”¨æˆ·æŸ¥è¯¢åˆ†é…æœ€åˆé€‚çš„æ¨¡å‹ã€‚</li>
<li>ç°æœ‰LLMè·¯ç”±å™¨é€šå¸¸é‡‡ç”¨å•ä¸€å›åˆä¸€å¯¹ä¸€æ˜ å°„ç­–ç•¥ï¼Œé™åˆ¶äº†å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>Router-R1æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¤šLLMè·¯ç”±å’Œèšåˆï¼Œå…¬å¼åŒ–ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>Router-R1å®ä¾‹åŒ–è·¯ç”±å™¨ä¸ºä¸€ä¸ªå…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMï¼Œå¯ä»¥äº¤æ›¿è¿›è¡Œæ€è€ƒå’Œè·¯ç”±è¡ŒåŠ¨ã€‚</li>
<li>é‡‡ç”¨è½»é‡çº§åŸºäºè§„åˆ™å¥–åŠ±æœºåˆ¶æŒ‡å¯¼å­¦ä¹ ï¼ŒåŒ…æ‹¬æ ¼å¼å¥–åŠ±ã€æœ€ç»ˆæˆæœå¥–åŠ±å’Œæˆæœ¬å¥–åŠ±ã€‚</li>
<li>Router-R1å¯å®ç°æ€§èƒ½å’Œæˆæœ¬çš„ä¼˜åŒ–æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f820e13d181d02e5200ed723cc1cb4cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d1a08e8c7dbb9ea8fc7bc567907c814.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522d6789bcf433b474749bd875ba174e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="e3-Learning-to-Explore-Enables-Extrapolation-of-Test-Time-Compute-for-LLMs"><a href="#e3-Learning-to-Explore-Enables-Extrapolation-of-Test-Time-Compute-for-LLMs" class="headerlink" title="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for   LLMs"></a>e3: Learning to Explore Enables Extrapolation of Test-Time Compute for   LLMs</h2><p><strong>Authors:Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar</strong></p>
<p>Test-time scaling offers a promising path to improve LLM reasoning by utilizing more compute at inference time; however, the true promise of this paradigm lies in extrapolation (i.e., improvement in performance on hard problems as LLMs keep â€œthinkingâ€ for longer, beyond the maximum token budget they were trained on). Surprisingly, we find that most existing reasoning models do not extrapolate well. We show that one way to enable extrapolation is by training the LLM to perform in-context exploration: training the LLM to effectively spend its test time budget by chaining operations (such as generation, verification, refinement, etc.), or testing multiple hypotheses before it commits to an answer. To enable in-context exploration, we identify three key ingredients as part of our recipe e3: (1) chaining skills that the base LLM has asymmetric competence in, e.g., chaining verification (easy) with generation (hard), as a way to implement in-context search; (2) leveraging â€œnegativeâ€ gradients from incorrect traces to amplify exploration during RL, resulting in longer search traces that chains additional asymmetries; and (3) coupling task difficulty with training token budget during training via a specifically-designed curriculum to structure in-context exploration. Our recipe e3 produces the best known 1.7B model according to AIMEâ€™25 and HMMTâ€™25 scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not only attains high pass@1 scores, but also improves pass@k over the base model. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾æä¾›äº†ä¸€ç§åˆ©ç”¨æ¨ç†æ—¶é—´æ›´å¤šè®¡ç®—èµ„æºæ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„æœ‰å‰é€”çš„é€”å¾„ã€‚ç„¶è€Œï¼Œè¯¥æ¨¡å¼çš„çœŸæ­£æ½œåŠ›åœ¨äºå¤–æ¨ï¼ˆå³ï¼Œéšç€LLMâ€œæ€è€ƒâ€çš„æ—¶é—´è¶…è¿‡å…¶è®­ç»ƒæ—¶çš„æœ€å¤§ä»¤ç‰Œé¢„ç®—ï¼Œå®ƒä»¬åœ¨éš¾é¢˜ä¸Šçš„æ€§èƒ½å¾—åˆ°æ”¹å–„ï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°ç°æœ‰çš„æ¨ç†æ¨¡å‹å¤–æ¨æ•ˆæœå¹¶ä¸å¥½ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§å®ç°å¤–æ¨çš„æ–¹æ³•ï¼Œå³è®­ç»ƒLLMè¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢ï¼šè®­ç»ƒLLMé€šè¿‡æ“ä½œé“¾ï¼ˆå¦‚ç”Ÿæˆã€éªŒè¯ã€ç»†åŒ–ç­‰ï¼‰æœ‰æ•ˆåœ°åˆ©ç”¨å…¶æµ‹è¯•æ—¶é—´é¢„ç®—ï¼Œæˆ–åœ¨æäº¤ç­”æ¡ˆä¹‹å‰æµ‹è¯•å¤šä¸ªå‡è®¾ã€‚</p>
</blockquote>
<p>ä¸ºäº†å®ç°ä¸Šä¸‹æ–‡æ¢ç´¢ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä½œä¸ºæˆ‘ä»¬é…æ–¹e3çš„ä¸€éƒ¨åˆ†çš„ä¸‰ä¸ªå…³é”®è¦ç´ ï¼šï¼ˆ1ï¼‰é“¾æ¥åŸºç¡€LLMå…·æœ‰ä¸å¯¹ç§°èƒ½åŠ›æŠ€èƒ½ï¼Œä¾‹å¦‚å°†éªŒè¯ï¼ˆå®¹æ˜“ï¼‰ä¸ç”Ÿæˆï¼ˆå›°éš¾ï¼‰é“¾æ¥èµ·æ¥ï¼Œä½œä¸ºå®ç°ä¸Šä¸‹æ–‡æœç´¢çš„ä¸€ç§æ–¹å¼ï¼›ï¼ˆ2ï¼‰åˆ©ç”¨é”™è¯¯è½¨è¿¹çš„â€œè´Ÿé¢â€æ¢¯åº¦æ¥æ”¾å¤§å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ï¼Œä»è€Œäº§ç”Ÿæ›´é•¿çš„æœç´¢è½¨è¿¹ï¼Œé“¾æ¥é¢å¤–çš„ä¸å¯¹ç§°æ€§ï¼›ï¼ˆ3ï¼‰é€šè¿‡ä¸“é—¨è®¾è®¡çš„è¯¾ç¨‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†ä»»åŠ¡éš¾åº¦ä¸è®­ç»ƒä»¤ç‰Œé¢„ç®—ç›¸ç»“åˆï¼Œä»¥æ„å»ºä¸Šä¸‹æ–‡æ¢ç´¢çš„ç»“æ„ã€‚æˆ‘ä»¬çš„e3é…æ–¹ç”Ÿäº§çš„æœ€ä½³å·²çŸ¥1.7Bæ¨¡å‹æ ¹æ®AIMEâ€™25å’ŒHMMTâ€™25åˆ†æ•°ï¼Œå¤–æ¨åˆ°è®­ç»ƒä»¤ç‰Œé¢„ç®—çš„2å€ã€‚æˆ‘ä»¬çš„e3-1.7Bæ¨¡å‹ä¸ä»…è¾¾åˆ°äº†é«˜pass@1åˆ†æ•°ï¼Œè€Œä¸”ç›¸å¯¹äºåŸºç¡€æ¨¡å‹è¿˜æ”¹å–„äº†pass@kåˆ†æ•°ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09026v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨æµ‹è¯•æ—¶é—´å°ºåº¦ä¸Šï¼Œåˆ©ç”¨æ›´å¤šçš„è®¡ç®—èµ„æºè¿›è¡Œæ¨ç†æ˜¯ä¸€ç§æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰å‰é€”çš„è·¯å¾„ã€‚çœŸæ­£çš„æ½œåŠ›åœ¨äºå¤–æ¨èƒ½åŠ›ï¼Œå³åœ¨æ›´é•¿æ—¶é—´çš„æ¨ç†è¿‡ç¨‹ä¸­æé«˜åœ¨å›°éš¾é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼Œä½†è¿™å–å†³äºå¦‚ä½•åˆ†é…LLMçš„é•¿æ—¶é—´è®¡ç®—èƒ½åŠ›ï¼Œä»è€Œå¢åŠ å®ƒçš„æ³›åŒ–æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„æ¨ç†æ¨¡å‹çš„å¤–æ¨æ€§èƒ½å¹¶ä¸ç†æƒ³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å®ç°å¤–æ¨çš„æ–¹æ³•ï¼Œå³è®­ç»ƒLLMè¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢ï¼ŒåŒ…æ‹¬é“¾æ¥æ“ä½œï¼ˆå¦‚ç”Ÿæˆã€éªŒè¯ã€ç²¾ç‚¼ç­‰ï¼‰æˆ–æµ‹è¯•å¤šä¸ªå‡è®¾å†ç»™å‡ºç­”æ¡ˆã€‚ä¸ºäº†æ”¯æŒä¸Šä¸‹æ–‡æ¢ç´¢ï¼Œæœ¬æ–‡æå‡ºäº†ä¸‰ä¸ªå…³é”®è¦ç´ ï¼šé“¾æ¥ä¸å¯¹ç§°æŠ€èƒ½çš„æŠ€å·§ï¼›åˆ©ç”¨åå‘ä¼ æ’­ä¿®æ­£æ–¹æ³•çš„ä¼˜åŒ–èƒ½åŠ›è¿›è¡Œåå‘æœç´¢æ¢ç´¢ï¼›è®­ç»ƒæ—¶æŒ‰ç…§ä»»åŠ¡éš¾åº¦è®¾ç½®è®­ç»ƒæ ‡è®°é¢„ç®—çš„ç‰¹å®šè¯¾ç¨‹è®¡åˆ’ç»“æ„ã€‚åŸºäºè¿™äº›è¦ç´ çš„è®­ç»ƒç­–ç•¥å®ç°äº†æœ€å¥½çš„å·²çŸ¥æ€§èƒ½ï¼Œå¹¶åœ¨AIMEâ€™25å’ŒHMMTâ€™25æµ‹è¯•ä¸­å¾—åˆ°äº†éªŒè¯ï¼Œä¸”åœ¨è®­ç»ƒä»¤ç‰Œé¢„ç®—çš„ä¸¤å€ä¸Šè¿›è¡Œå¤–æ¨æµ‹è¯•ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ•ˆæœã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…è·å¾—è¾ƒé«˜çš„å‡†ç¡®å¾—åˆ†ï¼Œè¿˜èƒ½åœ¨æ›´å¹¿æ³›çš„èŒƒå›´å†…æ”¹å–„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´å°ºåº¦æ˜¯æ”¹è¿›LLMæ¨ç†çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨æ›´å¤šçš„è®¡ç®—èµ„æºåœ¨æ¨ç†æ—¶é—´è¿›è¡Œç²¾ç»†åŒ–æ“ä½œï¼Œå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¤–æ¨èƒ½åŠ›å¯¹äºè§£å†³å¤æ‚é—®é¢˜è‡³å…³é‡è¦ã€‚é€šè¿‡è®­ç»ƒLLMè¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢ï¼Œå¯ä»¥å®ç°æ›´å¥½çš„å¤–æ¨æ€§èƒ½ã€‚</li>
<li>å®ç°ä¸Šä¸‹æ–‡æ¢ç´¢çš„å…³é”®è¦ç´ åŒ…æ‹¬ï¼šé“¾æ¥ä¸å¯¹ç§°æŠ€èƒ½çš„æŠ€å·§ã€åˆ©ç”¨åå‘æœç´¢æ¢ç´¢çš„ä¼˜åŒ–èƒ½åŠ›ä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è€ƒè™‘ä»»åŠ¡éš¾åº¦çš„ç‰¹å®šè¯¾ç¨‹è®¡åˆ’ç»“æ„ã€‚ </li>
<li>é“¾æ¥ä¸å¯¹ç§°æŠ€èƒ½çš„æŠ€å·§æ˜¯æŒ‡é’ˆå¯¹æŸäº›æ“ä½œçš„èƒ½åŠ›è¿›è¡Œé’ˆå¯¹æ€§å¼ºåŒ–ï¼Œä½¿å¾—æ¨¡å‹çš„æŠ€èƒ½æ›´åŒ¹é…éœ€æ±‚å¹¶æœ‰åŠ©äºæé«˜æ¨¡å‹çš„ä¸Šä¸‹æ–‡æœç´¢æ•ˆç‡ã€‚  </li>
<li>è®­ç»ƒæœ‰ç´ çš„æœ‰æ•ˆæŠ€èƒ½è®¾ç½®å¢å¼ºäº†LLMé€šè¿‡ä¿®æ”¹è®­ç»ƒä¸­çš„ç®—æ³•å“åº”è¯¯æŠ¥çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æœç´¢æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ </li>
<li>é€šè¿‡ç‰¹å®šçš„è¯¾ç¨‹è®¡åˆ’ç»“æ„è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶é€‚åº”ä¸åŒéš¾åº¦çš„ä»»åŠ¡å¹¶åˆ†é…ç›¸åº”çš„è®¡ç®—èµ„æºï¼Œæœ‰åŠ©äºæ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚  </li>
<li>æ­¤ç­–ç•¥è®­ç»ƒçš„æ¨¡å‹å…·æœ‰è‰¯å¥½çš„è¡¨ç°æ€§ã€é«˜æ•ˆæ€§ä»¥åŠå¯¹æ›´å¤§è§„æ¨¡çš„ä¸Šä¸‹æ–‡è¯­å¢ƒæœ‰è‰¯å¥½çš„é€‚åº”èƒ½åŠ›ï¼Œæé«˜äº†åœ¨å„ç§ç¯å¢ƒä¸‹çš„é€šç”¨æ€§æ•ˆæœåŠç”¨æˆ·ä½“éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-79063f526e6338653b27ef1a207f06ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40fc0dbe113ec3791fb56f3b892fa654.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e92c894c318ce797534c1976e6e1b1fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba1a7daded12a6b6c15d2d36209431d3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-Across-Parallel-Samples-for-LLM-Reasoning"><a href="#Learning-to-Reason-Across-Parallel-Samples-for-LLM-Reasoning" class="headerlink" title="Learning to Reason Across Parallel Samples for LLM Reasoning"></a>Learning to Reason Across Parallel Samples for LLM Reasoning</h2><p><strong>Authors:Jianing Qi, Xi Ye, Hao Tang, Zhigang Zhu, Eunsol Choi</strong></p>
<p>Scaling test-time compute brings substantial performance gains for large language models (LLMs). By sampling multiple answers and heuristically aggregate their answers (e.g., either through majority voting or using verifiers to rank the answers), one can achieve consistent performance gains in math domains. In this paper, we propose a new way to leverage such multiple sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that takes a concatenated sequence of multiple samples and output the final answer, optimizing it for the answer accuracy with reinforcement learning. Experiments on multiple reasoning datasets show that SSA outperforms other test-time scaling methods such as reward model-based re-ranking. Our approach also shows a promising generalization ability, across sample set sizes, base model families and scales, and tasks. By separating LLMs to generate answers and LLMs to analyze and aggregate sampled answers, our approach can work with the outputs from premier black box models easily and efficiently. </p>
<blockquote>
<p>æ‰©å¤§æµ‹è¯•æ—¶çš„è®¡ç®—èƒ½åŠ›ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡é‡‡æ ·å¤šä¸ªç­”æ¡ˆå¹¶å¯å‘å¼åœ°èšåˆå®ƒä»¬çš„ç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡å¤šæ•°æŠ•ç¥¨æˆ–ä½¿ç”¨éªŒè¯å™¨å¯¹ç­”æ¡ˆè¿›è¡Œæ’åï¼‰ï¼Œå¯ä»¥åœ¨æ•°å­¦é¢†åŸŸå®ç°æŒç»­çš„æ€§èƒ½æå‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è¿™ç§å¤šæ ·æ ·æœ¬é›†çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç´§å‡‘çš„LLMï¼Œç§°ä¸ºâ€œæ ·æœ¬é›†èšåˆå™¨ï¼ˆSSAï¼‰â€ï¼Œå®ƒæ¥å—å¤šä¸ªæ ·æœ¬çš„ä¸²è”åºåˆ—å¹¶è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSSAä¼˜äºå…¶ä»–æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ï¼Œå¦‚åŸºäºå¥–åŠ±æ¨¡å‹çš„é‡æ–°æ’åã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æ˜¾ç¤ºå‡ºå¾ˆæœ‰å¸Œæœ›çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ¶µç›–æ ·æœ¬é›†å¤§å°ã€åŸºç¡€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä»¥åŠä»»åŠ¡ã€‚é€šè¿‡å°†LLMåˆ†ç¦»ä»¥ç”Ÿæˆç­”æ¡ˆå’ŒLLMä»¥åˆ†æå’Œèšåˆé‡‡æ ·ç­”æ¡ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè½»æ¾é«˜æ•ˆåœ°å¤„ç†ä¸€æµé»‘ç›’æ¨¡å‹çš„è¾“å‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09014v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹è¯•æ—¶é—´è®¡ç®—æ‰©å±•æ—¶å…·æœ‰æ˜¾è‘—æ€§èƒ½æå‡ã€‚æœ¬æ–‡é€šè¿‡é‡‡æ ·å¤šä¸ªç­”æ¡ˆå¹¶å¯å‘å¼åœ°èšåˆè¿™äº›ç­”æ¡ˆï¼ˆä¾‹å¦‚é€šè¿‡å¤šæ•°æŠ•ç¥¨æˆ–ä½¿ç”¨éªŒè¯å™¨å¯¹ç­”æ¡ˆè¿›è¡Œæ’åï¼‰ï¼Œåœ¨æ•°å­¦é¢†åŸŸå®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ©ç”¨è¿™ç§å¤šæ ·æ ·æœ¬é›†çš„æ–¹æ³•ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç´§å‡‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç§°ä¸ºæ ·æœ¬é›†èšåˆå™¨ï¼ˆSSAï¼‰ï¼Œå®ƒæ¥å—å¤šä¸ªæ ·æœ¬çš„è¿ç»­åºåˆ—ï¼Œå¹¶è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹ç­”æ¡ˆçš„å‡†ç¡®æ€§è¿›è¡Œä¼˜åŒ–ã€‚åœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSSAä¼˜äºå…¶ä»–æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œå¦‚åŸºäºå¥–åŠ±æ¨¡å‹çš„é‡æ–°æ’åã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºä¸åŒçš„æ ·æœ¬é›†å¤§å°ã€åŸºç¡€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä»¥åŠä»»åŠ¡ã€‚é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹åˆ†ç¦»ä¸ºç”Ÿæˆç­”æ¡ˆå’Œåˆ†æèšåˆé‡‡æ ·ç­”æ¡ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è½»æ¾é«˜æ•ˆåœ°ä½¿ç”¨ä¸€æµçš„é»‘ç›’æ¨¡å‹çš„è¾“å‡ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´è®¡ç®—æ‰©å±•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>é€šè¿‡é‡‡æ ·å¤šä¸ªç­”æ¡ˆå¹¶å¯å‘å¼åœ°èšåˆå®ƒä»¬ï¼Œå¯ä»¥åœ¨æ•°å­¦é¢†åŸŸå®ç°æŒç»­çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åˆ©ç”¨å¤šæ ·æ ·æœ¬é›†çš„æ–¹æ³•ï¼Œå³æ ·æœ¬é›†èšåˆå™¨ï¼ˆSSAï¼‰ã€‚</li>
<li>SSAé€šè¿‡ä¼˜åŒ–ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå¯ä»¥ç”Ÿæˆæœ€ç»ˆçš„èšåˆç­”æ¡ˆã€‚</li>
<li>åœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šï¼ŒSSAçš„è¡¨ç°ä¼˜äºå…¶ä»–æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ã€‚</li>
<li>SSAå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºä¸åŒçš„æ ·æœ¬é›†å¤§å°ã€åŸºç¡€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä»¥åŠä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43cffa28f5ed9dfe35ed5b4af90b12fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2625138392f1f388a1278f420018a790.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AdaDec-Uncertainty-Guided-Adaptive-Decoding-for-LLM-based-Code-Generation"><a href="#AdaDec-Uncertainty-Guided-Adaptive-Decoding-for-LLM-based-Code-Generation" class="headerlink" title="AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code   Generation"></a>AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code   Generation</h2><p><strong>Authors:Kaifeng He, Mingwei Liu, Chong Wang, Zike Li, Yanlin Wang, Xin Peng, Zibin Zheng</strong></p>
<p>Code generation with large language models (LLMs) is highly sensitive to token selection during decoding, particularly at uncertain decision points that influence program logic. While standard strategies like greedy and beam search treat all tokens uniformly, they overlook code-specific uncertainty patterns, leading to suboptimal performance. This paper presents an empirical study revealing that many generation errors stem from ranking mistakes at high-uncertainty steps, where the correct token is present but not top-ranked.   Motivated by these findings, we propose AdaDec, an uncertainty-guided adaptive decoding framework that integrates a token-level pause-then-rerank mechanism driven by token uncertainty (Shannon entropy). AdaDec learns model-specific uncertainty thresholds and applies a lookahead-based reranking strategy when uncertainty is high. Experiments on HumanEval and MBPP benchmarks show that AdaDec improves Pass@1 accuracy by up to 15.5% over greedy decoding, outperforms or matches beam search, and reduces computational cost and latency through efficient, selective pausing. Our results highlight the promise of uncertainty-aware adaptive decoding for improving the reliability and efficiency of LLM-based code generation. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆå¯¹è§£ç è¿‡ç¨‹ä¸­çš„ä»¤ç‰Œé€‰æ‹©éå¸¸æ•æ„Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨å½±å“ç¨‹åºé€»è¾‘çš„ä¸ç¡®å®šå†³ç­–ç‚¹ã€‚å°½ç®¡è´ªå©ªæœç´¢å’ŒæŸæœç´¢ç­‰æ ‡å‡†ç­–ç•¥å¯¹æ‰€æœ‰ä»¤ç‰Œä¸€è§†åŒä»ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†ä»£ç ç‰¹å®šçš„ä¸ç¡®å®šæ€§æ¨¡å¼ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡è¿›è¡Œäº†ä¸€é¡¹å®è¯ç ”ç©¶ï¼Œå‘ç°è®¸å¤šç”Ÿæˆé”™è¯¯æºäºé«˜ä¸ç¡®å®šæ€§æ­¥éª¤çš„æ’åé”™è¯¯ï¼Œå…¶ä¸­æ­£ç¡®çš„ä»¤ç‰Œè™½ç„¶å­˜åœ¨ï¼Œä½†å¹¶æ²¡æœ‰æ’åœ¨é¦–ä½ã€‚å—è¿™äº›å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†AdaDecï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ä¸ç¡®å®šæ€§å¼•å¯¼çš„è‡ªé€‚åº”è§£ç æ¡†æ¶ï¼Œå®ƒé›†æˆäº†ç”±ä»¤ç‰Œä¸ç¡®å®šæ€§ï¼ˆé¦™å†œç†µï¼‰é©±åŠ¨çš„ä»¤ç‰Œçº§æš‚åœ-ç„¶åé‡æ–°æ’åæœºåˆ¶ã€‚AdaDecå­¦ä¹ ç‰¹å®šäºæ¨¡å‹çš„ä¸ç¡®å®šæ€§é˜ˆå€¼ï¼Œå¹¶åœ¨ä¸ç¡®å®šæ€§å¾ˆé«˜æ—¶åº”ç”¨åŸºäºå‰ç»çš„é‡æ–°æ’åç­–ç•¥ã€‚åœ¨äººç±»è¯„ä¼°ï¼ˆHumanEvalï¼‰å’ŒMBPPåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸è´ªå©ªè§£ç ç›¸æ¯”ï¼ŒAdaDecçš„Pass@1å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾15.5%ï¼Œä¼˜äºæˆ–ç›¸å½“äºæŸæœç´¢ï¼Œå¹¶é€šè¿‡é«˜æ•ˆçš„é€‰æ‹©æ€§æš‚åœé™ä½äº†è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªé€‚åº”è§£ç åœ¨æé«˜åŸºäºLLMçš„ä»£ç ç”Ÿæˆå¯é æ€§å’Œæ•ˆç‡æ–¹é¢çš„å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08980v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆå¯¹è§£ç è¿‡ç¨‹ä¸­çš„ä»¤ç‰Œé€‰æ‹©éå¸¸æ•æ„Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨å½±å“ç¨‹åºé€»è¾‘çš„ä¸ç¡®å®šå†³ç­–ç‚¹ã€‚æ ‡å‡†ç­–ç•¥å¦‚è´ªå©ªæœç´¢å’Œé›†æŸæœç´¢å‡åŒ€å¯¹å¾…æ‰€æœ‰ä»¤ç‰Œï¼Œå¿½ç•¥äº†ä»£ç ç‰¹å®šçš„ä¸ç¡®å®šæ€§æ¨¡å¼ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºAdaDecï¼Œä¸€ä¸ªä¸ç¡®å®šæ€§å¼•å¯¼çš„è‡ªé€‚åº”è§£ç æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†ä¸€ç§åŸºäºä»¤ç‰Œä¸ç¡®å®šæ€§çš„æš‚åœç„¶åé‡æ–°æ’åæœºåˆ¶ã€‚AdaDecå­¦ä¹ æ¨¡å‹ç‰¹å®šçš„ä¸ç¡®å®šæ€§é˜ˆå€¼ï¼Œå¹¶åœ¨ä¸ç¡®å®šæ€§é«˜æ—¶åº”ç”¨åŸºäºå‰ç»çš„é‡æ–°æ’åç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒAdaDecåœ¨HumanEvalå’ŒMBPPåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æ¯”è´ªå©ªè§£ç æé«˜é«˜è¾¾15.5%ï¼Œä¼˜äºæˆ–ç­‰äºé›†æŸæœç´¢ï¼Œå¹¶é€šè¿‡é«˜æ•ˆçš„é€‰æ‹©æ€§æš‚åœé™ä½äº†è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç ç”Ÿæˆä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹è§£ç è¿‡ç¨‹ä¸­çš„ä»¤ç‰Œé€‰æ‹©éå¸¸æ•æ„Ÿã€‚</li>
<li>æ ‡å‡†è§£ç ç­–ç•¥ï¼ˆå¦‚è´ªå©ªæœç´¢å’Œé›†æŸæœç´¢ï¼‰åœ¨ä¸ç¡®å®šå†³ç­–ç‚¹ä¸Šå­˜åœ¨æ€§èƒ½ä¸è¶³ã€‚</li>
<li>è§£ç è¿‡ç¨‹ä¸­çš„é”™è¯¯å¾ˆå¤šæºäºé«˜ä¸ç¡®å®šæ€§æ­¥éª¤çš„æ’åé”™è¯¯ã€‚</li>
<li>AdaDecæ˜¯ä¸€ä¸ªä¸ç¡®å®šæ€§å¼•å¯¼çš„è‡ªé€‚åº”è§£ç æ¡†æ¶ï¼ŒåŸºäºä»¤ç‰Œä¸ç¡®å®šæ€§è¿›è¡Œæš‚åœç„¶åé‡æ–°æ’åã€‚</li>
<li>AdaDecèƒ½æé«˜ä»£ç ç”Ÿæˆçš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºè´ªå©ªè§£ç æœ€é«˜å¯æé«˜15.5%ã€‚</li>
<li>AdaDecåœ¨æ€§èƒ½ä¸Šä¼˜äºæˆ–è‡³å°‘ä¸é›†æŸæœç´¢ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-315da90946fb5b223fe68536a7398ae1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91fa1da8f0e4855c541abfa74c50be43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0403a133965f3c06f33ff76d63a95b60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecc480f25dd77d1d798fbcf901621413.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Atomic-to-Compositional-Generalization-for-Mobile-Agents-with-A-New-Benchmark-and-Scheduling-System"><a href="#Atomic-to-Compositional-Generalization-for-Mobile-Agents-with-A-New-Benchmark-and-Scheduling-System" class="headerlink" title="Atomic-to-Compositional Generalization for Mobile Agents with A New   Benchmark and Scheduling System"></a>Atomic-to-Compositional Generalization for Mobile Agents with A New   Benchmark and Scheduling System</h2><p><strong>Authors:Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, Zhuosheng Zhang</strong></p>
<p>Autonomous agents powered by multimodal large language models have been developed to facilitate task execution on mobile devices. However, prior work has predominantly focused on atomic tasks â€“ such as shot-chain execution tasks and single-screen grounding tasks â€“ while overlooking the generalization to compositional tasks, which are indispensable for real-world applications. This work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile agents on three categories of compositional operations: Simple Concatenation, Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in 20 fully controllable local utility app environments, as well as 30 online Chinese and English service apps. It comprises 100 interactive task templates with an average optimal step count of 14.05. Experimental results across a range of mobile agents with agentic workflow or agent-as-a-model show that UI-NEXUS presents significant challenges. Specifically, existing agents generally struggle to balance performance and efficiency, exhibiting representative failure modes such as under-execution, over-execution, and attention drift, causing visible atomic-to-compositional generalization gap. Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient scheduling system to tackle compositional mobile tasks. AGENT-NEXUS extrapolates the abilities of existing mobile agents by dynamically decomposing long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS achieves 24% to 40% task success rate improvement for existing mobile agents on compositional operation tasks within the UI-NEXUS benchmark without significantly sacrificing inference overhead. The demo video, dataset, and code are available on the project page at <a target="_blank" rel="noopener" href="https://ui-nexus.github.io/">https://ui-nexus.github.io</a>. </p>
<blockquote>
<p>ç”±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è‡ªä¸»ä½“å·²è¢«å¼€å‘å‡ºæ¥ï¼Œä»¥åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šæ‰§è¡Œä»»åŠ¡çš„æ‰§è¡Œã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åŸå­ä»»åŠ¡ä¸Šï¼Œå¦‚çŸ­é“¾æ‰§è¡Œä»»åŠ¡å’Œå•å±å¹•å®šä½ä»»åŠ¡ï¼Œè€Œå¿½ç•¥äº†å¯¹ç»„åˆä»»åŠ¡çš„æ¨å¹¿ï¼Œè¿™å¯¹äºå®é™…åº”ç”¨æ¥è¯´æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚æœ¬æ–‡ä»‹ç»äº†UI-NEXUSï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ç§»åŠ¨ä»£ç†åœ¨ä¸‰ç±»ç»„åˆæ“ä½œä¸Šçš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼šç®€å•è¿æ¥ã€ä¸Šä¸‹æ–‡è½¬æ¢å’Œæ·±å…¥æ¢ç´¢ã€‚UI-NEXUSæ”¯æŒåœ¨20ä¸ªå¯å®Œå…¨æ§åˆ¶çš„æœ¬åœ°å®ç”¨åº”ç”¨ç¨‹åºç¯å¢ƒä¸­çš„äº¤äº’å¼è¯„ä¼°ï¼Œä»¥åŠ30ä¸ªåœ¨çº¿ä¸­æ–‡å’Œè‹±æ–‡æœåŠ¡åº”ç”¨ç¨‹åºã€‚å®ƒåŒ…å«100ä¸ªäº¤äº’å¼ä»»åŠ¡æ¨¡æ¿ï¼Œå¹³å‡æœ€ä½³æ­¥éª¤æ•°ä¸º14.05ã€‚ä¸€ç³»åˆ—å…·æœ‰è‡ªä¸»å·¥ä½œæµç¨‹æˆ–ä»£ç†æ¨¡å‹çš„ç§»åŠ¨ä»£ç†çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUI-NEXUSå­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œç°æœ‰ä»£ç†é€šå¸¸åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´éš¾ä»¥å¹³è¡¡ï¼Œè¡¨ç°å‡ºå…¸å‹çš„å¤±è´¥æ¨¡å¼ï¼Œå¦‚æ‰§è¡Œä¸è¶³ã€è¿‡åº¦æ‰§è¡Œå’Œæ³¨æ„åŠ›æ¼‚ç§»ï¼Œå¯¼è‡´æ˜æ˜¾çš„ä»åŸå­åˆ°ç»„åˆçš„ä¸€èˆ¬åŒ–å·®è·ã€‚ä»è¿™äº›å‘ç°ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†AGENT-NEXUSï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€é«˜æ•ˆçš„è°ƒåº¦ç³»ç»Ÿï¼Œç”¨äºè§£å†³ç»„åˆç§»åŠ¨ä»»åŠ¡ã€‚AGENT-NEXUSé€šè¿‡åŠ¨æ€åœ°å°†é•¿æœŸä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—ç‹¬ç«‹çš„åŸå­å­ä»»åŠ¡æ¥æ¨æ–­ç°æœ‰ç§»åŠ¨ä»£ç†çš„èƒ½åŠ›ã€‚AGENT-NEXUSåœ¨ä¸æ˜¾è‘—å¢åŠ æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹ï¼Œåœ¨UI-NEXUSåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¯¹ç°æœ‰ç§»åŠ¨ä»£ç†åœ¨ç»„åˆæ“ä½œä»»åŠ¡ä¸Šä»»åŠ¡æˆåŠŸç‡æé«˜24%è‡³40%ã€‚æ¼”ç¤ºè§†é¢‘ã€æ•°æ®é›†å’Œä»£ç å¯åœ¨é¡¹ç›®é¡µé¢<a target="_blank" rel="noopener" href="https://ui-nexus.github.ioä¸Šæ‰¾åˆ°./">https://ui-nexus.github.ioä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08972v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è‡ªä¸»ä»£ç†å·²ç”¨äºä¿ƒè¿›ç§»åŠ¨è®¾å¤‡ä¸Šçš„ä»»åŠ¡æ‰§è¡Œã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åŸå­ä»»åŠ¡ä¸Šï¼Œå¦‚è¿ç»­æ‹æ‘„æ‰§è¡Œä»»åŠ¡å’Œå•å±å®šä½ä»»åŠ¡ï¼Œå¿½ç•¥äº†å¯¹äºç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦çš„ç»„åˆä»»åŠ¡çš„æ¨å¹¿ã€‚æœ¬å·¥ä½œä»‹ç»äº†UI-NEXUSï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç§»åŠ¨ä»£ç†åœ¨ä¸‰ç±»ç»„åˆæ“ä½œä¸Šçš„è¡¨ç°ï¼šç®€å•è¿æ¥ã€ä¸Šä¸‹æ–‡è½¬æ¢å’Œæ·±å…¥æ¢ç´¢ã€‚UI-NEXUSæ”¯æŒåœ¨20ä¸ªå®Œå…¨å¯æ§çš„æœ¬åœ°å®ç”¨åº”ç”¨ç¨‹åºç¯å¢ƒå’Œ30ä¸ªåœ¨çº¿ä¸­è‹±æ–‡æœåŠ¡åº”ç”¨ç¨‹åºä¸­çš„äº¤äº’å¼è¯„ä¼°ã€‚å®ƒåŒ…æ‹¬100ä¸ªäº¤äº’å¼ä»»åŠ¡æ¨¡æ¿ï¼Œå¹³å‡æœ€ä½³æ­¥éª¤è®¡æ•°ä¸º14.05ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUI-NEXUSå¯¹ç§»åŠ¨ä»£ç†æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯ç°æœ‰ä»£ç†åœ¨å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢æ™®éå­˜åœ¨å›°éš¾ï¼Œè¡¨ç°å‡ºå…¸å‹çš„å¤±è´¥æ¨¡å¼ï¼Œå¦‚æ‰§è¡Œä¸è¶³ã€è¿‡åº¦æ‰§è¡Œå’Œæ³¨æ„åŠ›æ¼‚ç§»ï¼Œå¯¼è‡´æ˜æ˜¾çš„ä»åŸå­åˆ°ç»„åˆæ¨å¹¿çš„å·®è·ã€‚ç»“åˆè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†AGENT-NEXUSï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€é«˜æ•ˆçš„è°ƒåº¦ç³»ç»Ÿï¼Œç”¨äºè§£å†³ç»„åˆç§»åŠ¨ä»»åŠ¡ã€‚AGENT-NEXUSé€šè¿‡åŠ¨æ€åœ°å°†é•¿å‘¨æœŸä»»åŠ¡åˆ†è§£æˆä¸€ç³»åˆ—ç‹¬ç«‹çš„åŸå­å­ä»»åŠ¡ï¼Œä»è€Œæå‡ç°æœ‰ç§»åŠ¨ä»£ç†çš„èƒ½åŠ›ã€‚åœ¨UI-NEXUSåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAGENT-NEXUSåœ¨ä¸æ˜¾è‘—å¢åŠ æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†ç°æœ‰ç§»åŠ¨ä»£ç†åœ¨ç»„åˆæ“ä½œä»»åŠ¡ä¸Šçš„æˆåŠŸç‡è¾¾24%~40%ã€‚ç›¸å…³æ¼”ç¤ºè§†é¢‘ã€æ•°æ®é›†å’Œä»£ç å¯åœ¨é¡¹ç›®é¡µé¢<a target="_blank" rel="noopener" href="https://ui-nexus.github.ioä¸Šæ‰¾åˆ°./">https://ui-nexus.github.ioä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UI-NEXUSæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ç§»åŠ¨ä»£ç†åœ¨ç»„åˆæ“ä½œä¸Šçš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ç®€å•è¿æ¥ã€ä¸Šä¸‹æ–‡è½¬æ¢å’Œæ·±å…¥æ¢ç´¢ä¸‰ç±»æ“ä½œã€‚</li>
<li>UI-NEXUSæ”¯æŒåœ¨å¤šç§ç§»åŠ¨è®¾å¤‡å’Œåº”ç”¨ç¨‹åºç¯å¢ƒä¸­çš„äº¤äº’å¼è¯„ä¼°ï¼Œæä¾›100ä¸ªäº¤äº’å¼ä»»åŠ¡æ¨¡æ¿ã€‚</li>
<li>ç§»åŠ¨ä»£ç†åœ¨UI-NEXUSåŸºå‡†æµ‹è¯•ä¸­é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢ã€‚</li>
<li>ç°æœ‰ä»£ç†åœ¨ç»„åˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå¤šç§å¤±è´¥æ¨¡å¼ï¼Œå¦‚æ‰§è¡Œä¸è¶³ã€è¿‡åº¦æ‰§è¡Œå’Œæ³¨æ„åŠ›æ¼‚ç§»ã€‚</li>
<li>AGENT-NEXUSæ˜¯ä¸€ä¸ªè½»é‡çº§ã€é«˜æ•ˆçš„è°ƒåº¦ç³»ç»Ÿï¼Œé€šè¿‡åŠ¨æ€åˆ†è§£ä»»åŠ¡æå‡ç§»åŠ¨ä»£ç†çš„èƒ½åŠ›ã€‚</li>
<li>AGENT-NEXUSåœ¨ä¸æ˜¾è‘—å¢åŠ æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†ç°æœ‰ç§»åŠ¨ä»£ç†åœ¨ç»„åˆæ“ä½œä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7cccc140d6eb55396e85a7548cc388e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8ff7901492a4a68c1ad2895092234e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8434d8b9bd25410d7dfd7acb7e3fd8ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4b8f0edbd91b19b5774ebbebe761800.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FaithfulRAG-Fact-Level-Conflict-Modeling-for-Context-Faithful-Retrieval-Augmented-Generation"><a href="#FaithfulRAG-Fact-Level-Conflict-Modeling-for-Context-Faithful-Retrieval-Augmented-Generation" class="headerlink" title="FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful   Retrieval-Augmented Generation"></a>FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful   Retrieval-Augmented Generation</h2><p><strong>Authors:Qinggang Zhang, Zhishang Xiang, Yilin Xiao, Le Wang, Junhui Li, Xinrun Wang, Jinsong Su</strong></p>
<p>Large language models (LLMs) augmented with retrieval systems have demonstrated significant potential in handling knowledge-intensive tasks. However, these models often struggle with unfaithfulness issues, generating outputs that either ignore the retrieved context or inconsistently blend it with the LLM<code>s parametric knowledge. This issue is particularly severe in cases of knowledge conflict, where the retrieved context conflicts with the model</code>s parametric knowledge. While existing faithful RAG approaches enforce strict context adherence through well-designed prompts or modified decoding strategies, our analysis reveals a critical limitation: they achieve faithfulness by forcibly suppressing the model<code>s parametric knowledge, which undermines the model</code>s internal knowledge structure and increases the risk of misinterpreting the context. To this end, this paper proposes FaithfulRAG, a novel framework that resolves knowledge conflicts by explicitly modeling discrepancies between the model&#96;s parametric knowledge and retrieved context. Specifically, FaithfulRAG identifies conflicting knowledge at the fact level and designs a self-thinking process, allowing LLMs to reason about and integrate conflicting facts before generating responses. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. The code is available at https:&#x2F;&#x2F; github.com&#x2F;DeepLearnXMU&#x2F;Faithful-RAG </p>
<blockquote>
<p>é€šè¿‡èå…¥æ£€ç´¢ç³»ç»Ÿï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ç»å¸¸é¢ä¸´ä¸å¿ å®çš„é—®é¢˜ï¼Œç”Ÿæˆè¾“å‡ºæ—¶è¦ä¹ˆå¿½ç•¥æ£€ç´¢çš„ä¸Šä¸‹æ–‡ï¼Œè¦ä¹ˆå°†å…¶ä¸LLMçš„å‚æ•°çŸ¥è¯†ä¸ä¸€è‡´åœ°æ··åˆã€‚åœ¨çŸ¥è¯†å†²çªçš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªé—®é¢˜å°¤ä¸ºä¸¥é‡ï¼Œæ­¤æ—¶æ£€ç´¢çš„ä¸Šä¸‹æ–‡ä¸æ¨¡å‹çš„å‚æ•°çŸ¥è¯†ç›¸å†²çªã€‚è™½ç„¶ç°æœ‰çš„å¿ å®RAGæ–¹æ³•é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºæˆ–ä¿®æ”¹è§£ç ç­–ç•¥æ¥å¼ºåˆ¶å®æ–½ä¸¥æ ¼çš„ä¸Šä¸‹æ–‡éµå¾ªï¼Œä½†æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå…³é”®å±€é™ï¼šå®ƒä»¬é€šè¿‡å¼ºåˆ¶æŠ‘åˆ¶æ¨¡å‹çš„å‚æ•°çŸ¥è¯†æ¥å®ç°å¿ å®æ€§ï¼Œè¿™ç ´åäº†æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ç»“æ„ï¼Œå¹¶å¢åŠ äº†è¯¯è§£ä¸Šä¸‹æ–‡çš„é£é™©ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FaithfulRAGè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ˜¾å¼å»ºæ¨¡æ¨¡å‹å‚æ•°çŸ¥è¯†ä¸æ£€ç´¢ä¸Šä¸‹æ–‡ä¹‹é—´çš„å·®å¼‚æ¥è§£å†³çŸ¥è¯†å†²çªã€‚å…·ä½“æ¥è¯´ï¼ŒFaithfulRAGåœ¨äº‹å®å±‚é¢è¯†åˆ«å†²çªçŸ¥è¯†ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªè‡ªæˆ‘æ€è€ƒè¿‡ç¨‹ï¼Œå…è®¸LLMåœ¨ç”Ÿæˆå“åº”ä¹‹å‰å¯¹å†²çªäº‹å®è¿›è¡Œæ¨ç†å’Œæ•´åˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DeepLearnXMU/Faithful-RAG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DeepLearnXMU/Faithful-RAGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08938v1">PDF</a> Qinggang Zhang and Zhishang Xiang contributed equally to this work.   Corresponding author: Jinsong Su</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆæ£€ç´¢ç³»ç»Ÿåœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†é¢ä¸´çŸ¥è¯†ä¸å¿ å®é—®é¢˜ï¼Œå³åœ¨æ£€ç´¢åˆ°çš„çŸ¥è¯†ä¸æ¨¡å‹å‚æ•°çŸ¥è¯†å†²çªæ—¶ï¼Œæ¨¡å‹è¾“å‡ºå¾€å¾€å¿½ç•¥æ£€ç´¢åˆ°çš„è¯­å¢ƒæˆ–ä¸æ¨¡å‹å‚æ•°çŸ¥è¯†æ··åˆã€‚ç°æœ‰å¿ å®RAGæ–¹æ³•é€šè¿‡ç²¾å¿ƒè®¾è®¡æç¤ºæˆ–ä¿®æ”¹è§£ç ç­–ç•¥æ¥å¼ºåˆ¶è¯­å¢ƒéµå¾ªï¼Œä½†å­˜åœ¨å‹åˆ¶æ¨¡å‹å‚æ•°çŸ¥è¯†çš„å±€é™ï¼Œè¿™ç ´åäº†æ¨¡å‹å†…éƒ¨çŸ¥è¯†ç»“æ„å¹¶å¢åŠ äº†è¯¯è§£è¯­å¢ƒçš„é£é™©ã€‚æœ¬æ–‡æå‡ºçš„FaithfulRAGæ¡†æ¶é€šè¿‡æ˜¾å¼å»ºæ¨¡æ¨¡å‹å‚æ•°çŸ¥è¯†ä¸æ£€ç´¢è¯­å¢ƒä¹‹é—´çš„å·®å¼‚æ¥è§£å†³çŸ¥è¯†å†²çªé—®é¢˜ã€‚å®ƒèƒ½è¯†åˆ«äº‹å®å±‚é¢çš„å†²çªçŸ¥è¯†ï¼Œå¹¶è®¾è®¡è‡ªæˆ‘æ€è€ƒè¿‡ç¨‹ï¼Œä½¿LLMåœ¨ç”Ÿæˆå“åº”å‰èƒ½å¤Ÿæ¨ç†å’Œæ•´åˆå†²çªäº‹å®ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMç»“åˆæ£€ç´¢ç³»ç»Ÿåœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>LLMåœ¨å¤„ç†çŸ¥è¯†æ—¶å­˜åœ¨ä¸å¿ å®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ¥è¯†å†²çªæƒ…å†µä¸‹ã€‚</li>
<li>ç°æœ‰å¿ å®RAGæ–¹æ³•é€šè¿‡å¼ºåˆ¶è¯­å¢ƒéµå¾ªæ¥è¾¾æˆå¿ å®æ€§ï¼Œä½†å­˜åœ¨å‹åˆ¶æ¨¡å‹å‚æ•°çŸ¥è¯†çš„å±€é™ã€‚</li>
<li>FaithfulRAGæ¡†æ¶é€šè¿‡æ˜¾å¼å»ºæ¨¡æ¨¡å‹å‚æ•°çŸ¥è¯†ä¸æ£€ç´¢è¯­å¢ƒä¹‹é—´çš„å·®å¼‚æ¥è§£å†³çŸ¥è¯†å†²çªã€‚</li>
<li>FaithfulRAGèƒ½è¯†åˆ«äº‹å®å±‚é¢çš„å†²çªçŸ¥è¯†ï¼Œå¹¶è®¾è®¡è‡ªæˆ‘æ€è€ƒè¿‡ç¨‹ã€‚</li>
<li>FaithfulRAGæ¡†æ¶åœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-97c607b0ff8155510f07133505e99c15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-425ab6f95200b90d7fd06206ceb59353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b957631fe71c77fb4b0cac0d2055580.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03e75dbe0361d1ba8ef6e41549bf8767.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-A-Gamer-Train-A-Mathematical-Reasoning-Model"><a href="#Can-A-Gamer-Train-A-Mathematical-Reasoning-Model" class="headerlink" title="Can A Gamer Train A Mathematical Reasoning Model?"></a>Can A Gamer Train A Mathematical Reasoning Model?</h2><p><strong>Authors:Andrew Shin</strong></p>
<p>While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. <a target="_blank" rel="noopener" href="https://github.com/shinandrew/YouronMath">https://github.com/shinandrew/YouronMath</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œä½†å…¶å¼€å‘é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚è™½ç„¶æœ€è¿‘çš„è¿›å±•é™ä½äº†è®­ç»ƒæœ‰èƒ½åŠ›æ¨¡å‹çš„æˆæœ¬ï¼Œä½†è¿™äº›æ–¹æ³•ä»ç„¶ä¾èµ–äºé«˜ç«¯ç¡¬ä»¶é›†ç¾¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œå±•ç¤ºäº†ä½¿ç”¨å•ä¸ªå¹³å‡æ¸¸æˆGPUå°±å¯ä»¥è®­ç»ƒå‡ºåšå®çš„æ•°å­¦æ¨ç†æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨RTX 3080 Tiçš„16GBå†…å­˜ä¸Šè®­ç»ƒäº†ä¸€ä¸ª1.5Bå‚æ•°çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå…¶åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¸æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ã€‚æˆ‘ä»¬çš„ç»“æœæŒ‘æˆ˜äº†æœ€å…ˆè¿›æ•°å­¦æ¨ç†å¿…é¡»ä¾èµ–å¤§è§„æ¨¡åŸºç¡€è®¾æ–½çš„ç°çŠ¶ï¼Œä¸ºé«˜æ€§èƒ½äººå·¥æ™ºèƒ½ç ”ç©¶çš„æ™®åŠåŒ–æä¾›äº†å¯èƒ½ã€‚å¯ä»¥é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/shinandrew/YouronMath%E8%AE%BF%E9%97%AE%E6%88%91%E4%BB%AC%E7%9A%84%E7%A0%94%E7%A9%B6%E6%88%90%E6%9E%9C%E3%80%82">https://github.com/shinandrew/YouronMathè®¿é—®æˆ‘ä»¬çš„ç ”ç©¶æˆæœã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08935v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶å¼€å‘éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚æœ€è¿‘çš„ç ”ç©¶é™ä½äº†è®­ç»ƒè¿™äº›æ¨¡å‹çš„æˆæœ¬ï¼Œä½†ä»ä¾èµ–é«˜ç«¯ç¡¬ä»¶é›†ç¾¤ã€‚æœ¬ç ”ç©¶é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œåœ¨å•ä¸ªæ™®é€šæ¸¸æˆGPUä¸Šè®­ç»ƒäº†ä¸€ä¸ªå‡ºè‰²çš„æ•°å­¦æ¨ç†æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨RTX 3080 Ti 16GBå†…å­˜ä¸Šè®­ç»ƒäº†ä¸€ä¸ª1.5äº¿å‚æ•°çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°äº†ä¸æ›´å¤§æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚è¿™æŒ‘æˆ˜äº†æœ€å…ˆè¿›çš„æ•°å­¦æ¨ç†éœ€è¦å¤§è§„æ¨¡åŸºç¡€è®¾æ–½çš„è§‚ç‚¹ï¼Œä¸ºé«˜æ€§èƒ½äººå·¥æ™ºèƒ½ç ”ç©¶çš„æ™®åŠåŒ–æä¾›äº†å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶å·²ç»é™ä½äº†è®­ç»ƒè¿™äº›æ¨¡å‹çš„ç¡¬ä»¶æˆæœ¬ã€‚</li>
<li>é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯ä»¥åœ¨å•ä¸ªæ™®é€šæ¸¸æˆGPUä¸Šè®­ç»ƒå‡ºè‰²çš„æ•°å­¦æ¨ç†æ¨¡å‹ã€‚</li>
<li>æœ¬ç ”ç©¶åœ¨RTX 3080 Ti 16GBå†…å­˜ä¸Šè®­ç»ƒäº†ä¸€ä¸ªå‚æ•°è§„æ¨¡ä¸º1.5äº¿çš„æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°äº†ä¸æ›´å¤§æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†æœ€å…ˆè¿›çš„æ•°å­¦æ¨ç†éœ€è¦å¤§è§„æ¨¡åŸºç¡€è®¾æ–½çš„è§‚ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-341a77519510c65e6805cf79349133e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ccaacf4bf77e8ebfc70ecde6c7ffe85b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="What-Limits-Virtual-Agent-Application-OmniBench-A-Scalable-Multi-Dimensional-Benchmark-for-Essential-Virtual-Agent-Capabilities"><a href="#What-Limits-Virtual-Agent-Application-OmniBench-A-Scalable-Multi-Dimensional-Benchmark-for-Essential-Virtual-Agent-Capabilities" class="headerlink" title="What Limits Virtual Agent Application? OmniBench: A Scalable   Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities"></a>What Limits Virtual Agent Application? OmniBench: A Scalable   Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities</h2><p><strong>Authors:Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation with limited scenarios, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, cross-platform, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91% human acceptance rate. Training on our graph-structured data shows that it can more efficiently guide agents compared to manually annotated data. We conduct multidimensional evaluations for various open-source and closed-source models, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at <a target="_blank" rel="noopener" href="https://omni-bench.github.io/">https://omni-bench.github.io/</a>. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å‘å±•ï¼ŒåŸºäºMLLMçš„è™šæ‹Ÿä»£ç†è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸å¯æ§çš„ä»»åŠ¡å¤æ‚æ€§ã€åœºæ™¯æœ‰é™çš„æ‰‹åŠ¨æ ‡æ³¨å¹¿æ³›ä»¥åŠç¼ºä¹å¤šç»´è¯„ä¼°ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªæˆ‘ç”Ÿæˆã€è·¨å¹³å°ã€åŸºäºå›¾çš„åŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰é€šè¿‡å­ä»»åŠ¡ç»„åˆåˆæˆå¯æ§å¤æ‚æ€§ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ç®¡é“ã€‚ä¸ºäº†è¯„ä¼°è™šæ‹Ÿä»£ç†åœ¨å›¾ä¸Šçš„å„ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†OmniEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šç»´è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬å­ä»»åŠ¡çº§è¯„ä¼°ã€åŸºäºå›¾çš„æŒ‡æ ‡å’Œè·¨è¶Š10ç§èƒ½åŠ›çš„ç»¼åˆæµ‹è¯•ã€‚æˆ‘ä»¬åˆæˆçš„æ•°æ®é›†åŒ…å«20ä¸ªåœºæ™¯ä¸‹çš„3.6ä¸‡å›¾ç»“æ„ä»»åŠ¡ï¼Œè¾¾åˆ°äº†91%çš„äººç±»æ¥å—ç‡ã€‚åœ¨æˆ‘ä»¬çš„å›¾ç»“æ„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒè¡¨æ˜ï¼Œä¸æ‰‹åŠ¨æ³¨é‡Šæ•°æ®ç›¸æ¯”ï¼Œå®ƒå¯ä»¥æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼ä»£ç†ã€‚æˆ‘ä»¬å¯¹å„ç§å¼€æºå’Œé—­æºæ¨¡å‹è¿›è¡Œäº†å¤šç»´è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å„ç§èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼Œä¸ºæœªæ¥çš„è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://omni-bench.github.io/%E8%AE%BF%E9%97%AE%E3%80%82">https://omni-bench.github.io/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08933v1">PDF</a> Accepted by ICML 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•ï¼ŒåŸºäºMLLMçš„è™šæ‹Ÿä»£ç†è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•é¢ä¸´è¯¸å¤šå±€é™æ€§ï¼ŒåŒ…æ‹¬ä»»åŠ¡å¤æ‚æ€§ä¸å¯æ§ã€åœºæ™¯æœ‰é™çš„æ‰‹åŠ¨æ ‡æ³¨å’Œç¼ºä¹å¤šç»´è¯„ä¼°ç­‰é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºOmniBenchï¼Œä¸€ä¸ªè‡ªæˆ‘ç”Ÿæˆã€è·¨å¹³å°ã€åŸºäºå›¾å½¢çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å­ä»»åŠ¡ç»„åˆåˆæˆå¯æ§å¤æ‚åº¦çš„ä»»åŠ¡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºOmniEvalï¼Œä¸€ä¸ªå¤šç»´è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬å­ä»»åŠ¡çº§è¯„ä¼°ã€åŸºäºå›¾å½¢çš„æŒ‡æ ‡ä»¥åŠè·¨10é¡¹èƒ½åŠ›çš„å…¨é¢æµ‹è¯•ã€‚æˆ‘ä»¬çš„åˆæˆæ•°æ®é›†åŒ…å«36kå›¾å½¢ç»“æ„åŒ–ä»»åŠ¡ï¼Œè¦†ç›–20ä¸ªåœºæ™¯ï¼Œè¾¾åˆ°äº†91%çš„äººç±»æ¥å—ç‡ã€‚åœ¨å›¾å½¢ç»“æ„åŒ–æ•°æ®ä¸Šçš„è®­ç»ƒæ˜¾ç¤ºï¼Œå®ƒèƒ½æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼ä»£ç†ç›¸æ¯”æ‰‹åŠ¨æ³¨é‡Šçš„æ•°æ®ã€‚æˆ‘ä»¬å¯¹å„ç§å¼€æºå’Œé—­æºæ¨¡å‹è¿›è¡Œäº†å¤šç»´è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å„ç§èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼Œä¸ºæœªæ¥çš„è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—è™šæ‹Ÿä»£ç†è¡¨ç°å“è¶Šã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼ŒåŒ…æ‹¬ä»»åŠ¡å¤æ‚æ€§ä¸å¯æ§ã€æ‰‹åŠ¨æ ‡æ³¨çš„å±€é™æ€§ä»¥åŠç¼ºä¹å¤šç»´è¯„ä¼°ç­‰é—®é¢˜ã€‚</li>
<li>OmniBenchæ˜¯ä¸€ä¸ªè‡ªæˆ‘ç”Ÿæˆã€è·¨å¹³å°ã€åŸºäºå›¾å½¢çš„åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥åˆæˆå¯æ§å¤æ‚åº¦çš„ä»»åŠ¡ã€‚</li>
<li>OmniEvalæ˜¯ä¸€ä¸ªå¤šç»´è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è™šæ‹Ÿä»£ç†åœ¨å„ç§èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚</li>
<li>åˆæˆæ•°æ®é›†åŒ…å«36kå›¾å½¢ç»“æ„åŒ–ä»»åŠ¡ï¼Œè¦†ç›–å¤šä¸ªåœºæ™¯ï¼Œäººç±»æ¥å—ç‡é«˜è¾¾91%ã€‚</li>
<li>åœ¨å›¾å½¢ç»“æ„åŒ–æ•°æ®ä¸Šè®­ç»ƒå¯ä»¥æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼è™šæ‹Ÿä»£ç†ã€‚</li>
<li>å¯¹å„ç§æ¨¡å‹çš„å¤šç»´è¯„ä¼°æ­ç¤ºäº†å®ƒä»¬åœ¨å¤šç§èƒ½åŠ›ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08933">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94fefb1b6723dc45026428f922a131a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ab7505d86fb36096182b108efafa352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f2cf80efd8feb950a53f38fd70daf5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e18f0ce0b4c569fb238440dd97fd3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bd8bf27b29d78336e7dd18a203af15c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35206aa3883c2327bd0e60b86a68b4cd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PropMEND-Hypernetworks-for-Knowledge-Propagation-in-LLMs"><a href="#PropMEND-Hypernetworks-for-Knowledge-Propagation-in-LLMs" class="headerlink" title="PropMEND: Hypernetworks for Knowledge Propagation in LLMs"></a>PropMEND: Hypernetworks for Knowledge Propagation in LLMs</h2><p><strong>Authors:Zeyu Leo Liu, Greg Durrett, Eunsol Choi</strong></p>
<p>Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations. </p>
<blockquote>
<p>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯å¯ä»¥æ³¨å…¥çŸ¥è¯†å¹¶åœ¨ä»¥åè¿›è¡Œé€å­—å¤åˆ¶ï¼Œä½†å®ƒä»¬åœ¨ä¼ æ’­çŸ¥è¯†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼šæ¨¡å‹æ— æ³•å›ç­”éœ€è¦åˆ©ç”¨æ³¨å…¥çŸ¥è¯†è¿›è¡Œæ¨ç†çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¶…ç½‘ç»œçš„çŸ¥è¯†ä¼ æ’­æ–¹æ³•ï¼Œåä¸ºPropMENDã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å…ƒå­¦ä¹ å¦‚ä½•ä¿®æ”¹è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ¢¯åº¦ï¼Œä»¥é¼“åŠ±æ³¨å…¥ä¿¡æ¯è¿›è¡Œä¼ æ’­ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•äº†MEND [29]çš„å…ƒç›®æ ‡ï¼Œä½¿çŸ¥è¯†ä¸Šçš„æ¢¯åº¦æ›´æ–°èƒ½å¤Ÿè½¬åŒ–ä¸ºèƒ½å¤Ÿå›ç­”æ¶‰åŠè¯¥çŸ¥è¯†çš„å¤šè·³é—®é¢˜ã€‚æˆ‘ä»¬åœ¨RippleEditæ•°æ®é›†ä¸Šå±•ç¤ºäº†æ”¹è¿›çš„æ€§èƒ½ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè·³é—®é¢˜ä¸Šï¼Œå…¶ç­”æ¡ˆåœ¨æ³¨å…¥çš„äº‹å®ä¸­å¹¶æœªæ˜ç¡®ç»™å‡ºï¼Œå‡†ç¡®ç‡å‡ ä¹æé«˜äº†ä¸€å€ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°æˆ‘ä»¬çš„è¶…ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†Controlled RippleEditï¼Œä»¥æµ‹è¯•è¶…ç½‘ç»œè®­ç»ƒæœŸé—´æœªè§è¿‡çš„å…³ç³»å’Œå®ä½“çš„çŸ¥è¯†ä¼ æ’­æƒ…å†µã€‚PropMENDåœ¨æœªè§è¿‡çš„å®ä½“-å…³ç³»å¯¹ä¸Šä»ç„¶ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä½†æ€§èƒ½å·®è·å¤§å¹…ç¼©å°ï¼Œè¿™è¡¨æ˜æœªæ¥åœ¨çŸ¥è¯†ä¼ æ’­åˆ°å„ç§å…³ç³»ä¸Šçš„å·¥ä½œä»æœ‰å¾ˆå¤§ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08920v1">PDF</a> Under review</p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯å¯ä»¥æ³¨å…¥çŸ¥è¯†å¹¶å¯ä»¥åŸå°ä¸åŠ¨åœ°å†ç°ï¼Œä½†åœ¨ä¼ æ’­çŸ¥è¯†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼šæ¨¡å‹æ— æ³•å›ç­”éœ€è¦åˆ©ç”¨æ³¨å…¥çŸ¥è¯†è¿›è¡Œæ¨ç†çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¶…ç½‘ç»œçš„çŸ¥è¯†ä¼ æ’­æ–¹æ³•ï¼Œåä¸ºPropMENDï¼Œè¯¥æ–¹æ³•é€šè¿‡å…ƒå­¦ä¹ å­¦ä¼šå¦‚ä½•ä¿®æ”¹è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ¢¯åº¦ï¼Œä»¥é¼“åŠ±æ³¨å…¥çš„ä¿¡æ¯å¾—ä»¥ä¼ æ’­ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•äº†MENDçš„å…ƒç›®æ ‡ï¼Œä½¿çŸ¥è¯†ä¸Šçš„æ¢¯åº¦æ›´æ–°èƒ½å¤Ÿè½¬åŒ–ä¸ºå›ç­”æ¶‰åŠè¯¥çŸ¥è¯†çš„å¤šè·³é—®é¢˜ã€‚åœ¨RippleEditæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¯¹äºç­”æ¡ˆæœªåœ¨æ³¨å…¥äº‹å®ä¸­æ˜ç¡®ç»™å‡ºçš„å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè·³é—®é¢˜ï¼Œå…¶å‡†ç¡®ç‡å‡ ä¹æé«˜äº†ä¸€å€ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°è¶…ç½‘ç»œåœ¨æœªè§å®ä½“å…³ç³»ä¸Šçš„çŸ¥è¯†ä¼ æ’­èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†æ–°çš„æ•°æ®é›†Controlled RippleEditã€‚PropMENDåœ¨æœªè§å®ä½“å…³ç³»å¯¹ä¸Šä»ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä½†æ€§èƒ½å·®è·å¤§å¹…ç¼©å°ï¼Œè¿™è¡¨æ˜æœªæ¥åœ¨å°†çŸ¥è¯†ä¼ æ’­åˆ°å¹¿æ³›çš„å…³ç³»ä¸Šä»éœ€è¿›ä¸€æ­¥åŠªåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMçš„çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯èƒ½æ³¨å…¥å¹¶åŸæ ·å†ç°çŸ¥è¯†ï¼Œä½†åœ¨çŸ¥è¯†ä¼ æ’­æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè¶…ç½‘ç»œçš„çŸ¥è¯†ä¼ æ’­æ–¹æ³•â€”â€”PropMENDã€‚</li>
<li>PropMENDé€šè¿‡å…ƒå­¦ä¹ å­¦ä¼šä¿®æ”¹è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ¢¯åº¦ï¼Œä¿ƒè¿›æ³¨å…¥çŸ¥è¯†çš„ä¼ æ’­ã€‚</li>
<li>åœ¨RippleEditæ•°æ®é›†ä¸Šï¼ŒPropMENDå¯¹äºå¤æ‚é—®é¢˜çš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>å¼•å…¥æ–°çš„æ•°æ®é›†Controlled RippleEditï¼Œä»¥è¯„ä¼°è¶…ç½‘ç»œåœ¨æœªè§å®ä½“å…³ç³»ä¸Šçš„çŸ¥è¯†ä¼ æ’­èƒ½åŠ›ã€‚</li>
<li>PropMENDåœ¨æœªè§å®ä½“å…³ç³»å¯¹ä¸Šè¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„èƒ½åŠ›ï¼Œä½†æ€§èƒ½å·®è·æœ‰æ‰€ç¼©å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-04cd5e41a2a25027a3cc83644549262a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed6a216231e06f3093c597ed4c71a573.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d60dbaf4f31ec0857f86c5740ad9fd73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ccb3390d053867380e3a257cc2b8764.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Your-Brain-on-ChatGPT-Accumulation-of-Cognitive-Debt-when-Using-an-AI-Assistant-for-Essay-Writing-Task"><a href="#Your-Brain-on-ChatGPT-Accumulation-of-Cognitive-Debt-when-Using-an-AI-Assistant-for-Essay-Writing-Task" class="headerlink" title="Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI   Assistant for Essay Writing Task"></a>Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI   Assistant for Essay Writing Task</h2><p><strong>Authors:Nataliya Kosmyna, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ, Xian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, Pattie Maes</strong></p>
<p>This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AIâ€™s role in learning. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†LLMè¾…åŠ©å†™ä½œå¯¹ç¥ç»å’Œè¡Œä¸ºæ–¹é¢çš„å½±å“ã€‚å‚ä¸è€…è¢«åˆ†ä¸ºä¸‰ç»„ï¼šLLMç»„ã€æœç´¢å¼•æ“ç»„å’Œä»…å¤§è„‘ç»„ï¼ˆæ— å·¥å…·ï¼‰ã€‚ä¸‰ç»„æˆå‘˜å‡åœ¨ç›¸åŒæ¡ä»¶ä¸‹å®Œæˆä¸‰åœºæµ‹è¯•ã€‚åœ¨ç¬¬å››åœºæµ‹è¯•ä¸­ï¼ŒLLMç»„çš„ç”¨æˆ·è¢«é‡æ–°åˆ†é…åˆ°ä»…å¤§è„‘ç»„ï¼ˆLLMåˆ°å¤§è„‘ï¼‰ï¼Œè€Œä»…å¤§è„‘ç»„çš„ç”¨æˆ·è¢«åˆ†é…åˆ°LLMæ¡ä»¶ç»„ï¼ˆå¤§è„‘åˆ°LLMï¼‰ã€‚å‰ä¸‰æ¬¡æµ‹è¯•å…±æœ‰54åå‚ä¸è€…ï¼Œå…¶ä¸­18äººå®Œæˆäº†ç¬¬å››æ¬¡æµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰æ¥è¯„ä¼°å†™ä½œè¿‡ç¨‹ä¸­çš„è®¤çŸ¥è´Ÿè·ï¼Œå¹¶ä½¿ç”¨NLPå¯¹ä½œæ–‡è¿›è¡Œåˆ†æï¼ŒåŒæ—¶è¿˜è¯·äººç±»æ•™å¸ˆå’ŒAIè¯„åˆ†å‘˜å¯¹ä½œæ–‡è¿›è¡Œè¯„åˆ†ã€‚å„ç»„çš„å‘½åå®ä½“è¯†åˆ«ã€nå…ƒæ¨¡å¼ä»¥åŠä¸»é¢˜æœ¬ä½“è®ºæ˜¾ç¤ºäº†ç»„å†…çš„ä¸€è‡´æ€§ã€‚è„‘ç”µå›¾æ˜¾ç¤ºå¤§è„‘è¿æ¥å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼šä»…å¤§è„‘ç»„çš„å‚ä¸è€…è¡¨ç°å‡ºæœ€å¼ºã€æœ€åˆ†å¸ƒå¹¿æ³›çš„ç½‘ç»œï¼›æœç´¢å¼•æ“ç”¨æˆ·è¡¨ç°å‡ºä¸­ç­‰ç¨‹åº¦çš„å‚ä¸åº¦ï¼›è€ŒLLMç”¨æˆ·æ˜¾ç¤ºå‡ºæœ€å¼±çš„è¿æ¥ã€‚ä¸ä½¿ç”¨å¤–éƒ¨å·¥å…·ç›¸å…³çš„è®¤çŸ¥æ´»åŠ¨è§„æ¨¡æœ‰æ‰€å‡å°ã€‚åœ¨ç¬¬å››æ¬¡æµ‹è¯•ä¸­ï¼Œä»LLMåˆ°å¤§è„‘çš„å‚ä¸è€…æ˜¾ç¤ºå‡ºé™ä½çš„Î±å’ŒÎ²è¿æ¥æ€§ï¼Œè¡¨æ˜å‚ä¸ä¸è¶³ã€‚è€Œä»å¤§è„‘åˆ°LLMçš„å‚ä¸è€…åˆ™è¡¨ç°å‡ºæ›´é«˜çš„è®°å¿†å›å¿†å’Œæ•éª¨-é¡¶å¶åŠå‰é¢å¶åŒºåŸŸçš„æ¿€æ´»ï¼Œè¿™ä¸æœç´¢å¼•æ“ç”¨æˆ·ç›¸ä¼¼ã€‚å…³äºä½œæ–‡çš„è‡ªæˆ‘æ‰€æœ‰æƒæŠ¥å‘Šï¼ŒLLMç»„çš„æœ€ä½ï¼Œä»…å¤§è„‘ç»„æœ€é«˜ã€‚LLMç”¨æˆ·åœ¨å‡†ç¡®å¼•ç”¨è‡ªå·±çš„ä½œå“æ—¶é‡åˆ°å›°éš¾ã€‚è™½ç„¶LLMæä¾›äº†å³æ—¶çš„ä¾¿åˆ©ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†å…¶æ½œåœ¨è®¤çŸ¥æˆæœ¬ã€‚åœ¨å››ä¸ªæœˆçš„æ—¶é—´é‡Œï¼ŒLLMç”¨æˆ·åœ¨ç¥ç»ã€è¯­è¨€å’Œè¡Œä¸ºå±‚é¢ä¸€ç›´è¡¨ç°è¾ƒå·®ã€‚è¿™äº›ç»“æœå¼•å‘äº†äººä»¬å¯¹é•¿æœŸä¾èµ–LLMçš„æ•™è‚²å½±å“çš„æ‹…å¿§ï¼Œå¹¶å¼ºè°ƒäº†æ·±å…¥æ¢ç©¶AIåœ¨å­¦ä¹ ä¸­çš„ä½œç”¨å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08872v1">PDF</a> 206 pages, 92 figures, 4 tables and appendix</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†LLMè¾…åŠ©å†™ä½œå¯¹å¤§è„‘å’Œè¡Œä¸ºçš„å½±å“ã€‚ç ”ç©¶å°†å‚ä¸è€…åˆ†ä¸ºä¸‰ç»„ï¼šLLMç»„ã€æœç´¢å¼•æ“ç»„å’Œä»…å¤§è„‘ç»„ï¼ˆæ— å·¥å…·ï¼‰ã€‚æ¯ç»„å‚ä¸è€…åœ¨ä¸åŒæ¡ä»¶ä¸‹å®Œæˆä¸‰æ¬¡å†™ä½œä¼šè¯ã€‚ç¬¬å››æ¬¡ä¼šè¯æ—¶ï¼Œå¯¹LLMç”¨æˆ·é‡æ–°åˆ†é…åˆ°ä»…å¤§è„‘ç»„ï¼Œè€Œå¯¹ä»…å¤§è„‘ç”¨æˆ·åˆ†é…åˆ°LLMæ¡ä»¶ç»„ã€‚å…±æœ‰54åå‚ä¸è€…å®Œæˆäº†å‰ä¸‰æ¬¡ä¼šè¯ï¼Œå…¶ä¸­18äººå®Œæˆäº†ç¬¬å››æ¬¡ä¼šè¯ã€‚ç ”ç©¶ä½¿ç”¨è„‘ç”µå›¾è¯„ä¼°å†™ä½œè¿‡ç¨‹ä¸­çš„è®¤çŸ¥è´Ÿè·ï¼Œå¹¶ç”¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººç±»æ•™å¸ˆå’ŒAIè¯„ä¼°æ–‡ç« è´¨é‡ã€‚å„ç»„å†…æ–‡ç« çš„è¯­è¨€ç‰¹å¾å…·æœ‰ä¸€è‡´æ€§ï¼Œä½†å¤§è„‘è¿æ¥æ€§åœ¨ç»„é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä»…å¤§è„‘ç»„çš„ç½‘ç»œè¿æ¥æœ€å¼ºä¸”åˆ†å¸ƒæœ€å¹¿ï¼›æœç´¢å¼•æ“ç»„è¡¨ç°ä¸ºä¸­ç­‰å‚ä¸ï¼›è€ŒLLMç»„åˆ™æ˜¾ç¤ºå‡ºæœ€å¼±çš„è¿é€šæ€§ã€‚ä¸ä½¿ç”¨å¤–éƒ¨å·¥å…·ç›¸å…³çš„è®¤çŸ¥æ´»åŠ¨æœ‰æ‰€å‡å°‘ã€‚åœ¨ç¬¬å››æ¬¡ä¼šè¯ä¸­ï¼Œä»LLMè½¬åˆ°ä»…å¤§è„‘çš„å‚ä¸è€…æ˜¾ç¤ºå‡ºè¿é€šæ€§é™ä½ï¼Œè€Œå¤§è„‘åˆ°LLMçš„å‚ä¸è€…åˆ™è¡¨ç°å‡ºæ›´é«˜çš„è®°å¿†å¬å›å’Œæ¿€æ´»çš„è„‘åŒºä¸æœç´¢å¼•æ“ç»„ç›¸ä¼¼ã€‚LLMç»„çš„æ–‡ç« è‡ªæˆ‘æ‰€æœ‰æƒæŠ¥å‘Šæœ€ä½ï¼Œä¸”éš¾ä»¥å‡†ç¡®å¼•ç”¨è‡ªå·±çš„ä½œå“ã€‚è™½ç„¶LLMæä¾›äº†å³æ—¶ä¾¿åˆ©ï¼Œä½†ç ”ç©¶ç»“æœæŒ‡å‡ºäº†å…¶æ½œåœ¨è®¤çŸ¥æˆæœ¬ã€‚é•¿æœŸè€Œè¨€ï¼Œåœ¨ç¥ç»ã€è¯­è¨€å’Œè¡Œä¸ºå±‚é¢ï¼ŒLLMç”¨æˆ·è¡¨ç°è¾ƒå·®ã€‚è¿™å¼•å‘äº†å…³äºä¾èµ–LLMçš„é•¿æœŸæ•™è‚²å½±å“çš„æ‹…å¿§ï¼Œå¹¶å¼ºè°ƒéœ€è¦æ›´æ·±å…¥åœ°ç ”ç©¶AIåœ¨å­¦ä¹ ä¸­çš„è§’è‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè¾…åŠ©å†™ä½œå¯¹å¤§è„‘å’Œè¡Œä¸ºæœ‰å½±å“ï¼Œå‚ä¸è€…åˆ†ä¸ºä¸‰ç»„è¿›è¡Œç ”ç©¶ã€‚</li>
<li>é€šè¿‡è„‘ç”µå›¾è¯„ä¼°å†™ä½œè¿‡ç¨‹ä¸­çš„è®¤çŸ¥è´Ÿè·ã€‚</li>
<li>æ–‡ç« è´¨é‡é€šè¿‡è‡ªç„¶è¯­è¨€å¤„ç†ã€äººç±»æ•™å¸ˆå’ŒAIè¯„ä¼°ã€‚</li>
<li>å„ç»„å†…æ–‡ç« è¯­è¨€ç‰¹å¾ä¸€è‡´ï¼Œä½†å¤§è„‘è¿æ¥æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>LLMç”¨æˆ·æ˜¾ç¤ºå‡ºæœ€å¼±çš„è¿é€šæ€§ï¼Œè€Œä»…å¤§è„‘ç»„è¡¨ç°å‡ºæœ€å¼ºçš„ç½‘ç»œè¿æ¥ã€‚</li>
<li>LLMç»„çš„è‡ªæˆ‘æ‰€æœ‰æƒæŠ¥å‘Šæœ€ä½ï¼Œä¸”å­˜åœ¨å¼•ç”¨å›°éš¾ã€‚</li>
<li>é•¿æœŸä¾èµ–LLMå¯èƒ½åœ¨ç¥ç»ã€è¯­è¨€å’Œè¡Œä¸ºå±‚é¢äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå¼•å‘å¯¹é•¿æœŸæ•™è‚²å½±å“çš„æ‹…å¿§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1b9d0a85c8334cd2d9e85dda38db6ffc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-465e116ad4bc088f5067f0fe8edf4a0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dead912d30482dbbd9bf296954142351.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18843faa84f1884c1edea41f63f5d492.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLaVA-c-Continual-Improved-Visual-Instruction-Tuning"><a href="#LLaVA-c-Continual-Improved-Visual-Instruction-Tuning" class="headerlink" title="LLaVA-c: Continual Improved Visual Instruction Tuning"></a>LLaVA-c: Continual Improved Visual Instruction Tuning</h2><p><strong>Authors:Wenzhuo Liu, Fei Zhu, Haiyang Guo, Longhui Wei, Cheng-Lin Liu</strong></p>
<p>Multimodal models like LLaVA-1.5 achieve state-of-the-art visual understanding through visual instruction tuning on multitask datasets, enabling strong instruction-following and multimodal performance. However, multitask learning faces challenges such as task balancing, requiring careful adjustment of data proportions, and expansion costs, where new tasks risk catastrophic forgetting and need costly retraining. Continual learning provides a promising alternative to acquiring new knowledge incrementally while preserving existing capabilities. However, current methods prioritize task-specific performance, neglecting base model degradation from overfitting to specific instructions, which undermines general capabilities. In this work, we propose a simple but effective method with two modifications on LLaVA-1.5: spectral-aware consolidation for improved task balance and unsupervised inquiry regularization to prevent base model degradation. We evaluate both general and task-specific performance across continual pretraining and fine-tuning. Experiments demonstrate that LLaVA-c consistently enhances standard benchmark performance and preserves general capabilities. For the first time, we show that task-by-task continual learning can achieve results that match or surpass multitask joint learning. The code will be publicly released. </p>
<blockquote>
<p>LLaVA-1.5ç­‰å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡å¤šä»»åŠ¡æ•°æ®é›†ä¸Šçš„è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„è§†è§‰ç†è§£ï¼Œä»è€Œå®ç°äº†å¼ºå¤§çš„æŒ‡ä»¤éµå¾ªå’Œå¤šæ¨¡æ€æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤šä»»åŠ¡å­¦ä¹ é¢ä¸´ä»»åŠ¡å¹³è¡¡ç­‰æŒ‘æˆ˜ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´æ•°æ®æ¯”ä¾‹å’Œæ‰©å±•æˆæœ¬ï¼Œæ–°å¢ä»»åŠ¡å­˜åœ¨ç¾éš¾æ€§é—å¿˜çš„é£é™©å¹¶éœ€è¦æ˜‚è´µçš„é‡æ–°è®­ç»ƒã€‚æŒç»­å­¦ä¹ æä¾›äº†ä¸€ç§å¾ˆæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¯ä»¥å¢é‡è·å–æ–°çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™ç°æœ‰èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¼˜å…ˆè€ƒè™‘ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼Œå¿½è§†äº†ç”±äºè¿‡åº¦æ‹Ÿåˆç‰¹å®šæŒ‡ä»¤å¯¼è‡´çš„åŸºå‡†æ¨¡å‹æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œä»è€ŒæŸå®³äº†é€šç”¨èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹LLaVA-1.5è¿›è¡Œäº†ä¸¤é¡¹ä¿®æ”¹ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼šå…‰è°±æ„ŸçŸ¥å·©å›ºï¼Œä»¥æ”¹è¿›ä»»åŠ¡å¹³è¡¡å’Œæ— ç›‘ç£æŸ¥è¯¢æ­£åˆ™åŒ–ï¼Œä»¥é˜²æ­¢åŸºç¡€æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬è¯„ä¼°äº†æŒç»­é¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­çš„é€šç”¨å’Œç‰¹å®šä»»åŠ¡æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒLLaVA-cæŒç»­æé«˜äº†æ ‡å‡†åŸºå‡†æµ‹è¯•æ€§èƒ½å¹¶ä¿ç•™äº†é€šç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–æ¬¡è¯æ˜ï¼ŒæŒ‰ä»»åŠ¡é¡ºåºçš„æŒç»­å­¦ä¹ å¯ä»¥è¾¾åˆ°æˆ–å¤šä»»åŠ¡è”åˆå­¦ä¹ çš„æ•ˆæœã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08666v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºLLaVA-1.5çš„å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´åœ¨å¤šä»»åŠ¡æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„è§†è§‰ç†è§£ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æŒ‡ä»¤éµå¾ªå’Œå¤šæ¨¡æ€æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤šä»»åŠ¡å­¦ä¹ é¢ä¸´ä»»åŠ¡å¹³è¡¡å’Œæ‰©å±•æˆæœ¬çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•å¯¹LLaVA-1.5è¿›è¡Œä¸¤é¡¹æ”¹è¿›ï¼šå…‰è°±æ„ŸçŸ¥å·©å›ºä»¥æ”¹å–„ä»»åŠ¡å¹³è¡¡å’Œæ— ç›‘ç£æŸ¥è¯¢æ­£åˆ™åŒ–ä»¥é˜²æ­¢åŸºç¡€æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç‰¹å®šæŒ‡ä»¤å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚å®éªŒè¡¨æ˜ï¼ŒLLaVA-cåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¿æŒäº†é€šç”¨èƒ½åŠ›ã€‚æœ¬ç ”ç©¶é¦–æ¬¡è¯æ˜äº†ä»»åŠ¡é€ä¸ªçš„å¢é‡å­¦ä¹ å¯ä»¥è¾¾åˆ°æˆ–å¤šäºå¤šä»»åŠ¡è”åˆå­¦ä¹ çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ¨¡å‹å¦‚LLaVA-1.5é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´å’Œå¤šä»»åŠ¡æ•°æ®é›†å®ç°äº†å…ˆè¿›è§†è§‰ç†è§£ã€‚</li>
<li>å¤šä»»åŠ¡å­¦ä¹ é¢ä¸´ä»»åŠ¡å¹³è¡¡å’Œæ‰©å±•æˆæœ¬çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•å¯¹LLaVA-1.5è¿›è¡Œæ”¹è¿›ï¼ŒåŒ…æ‹¬å…‰è°±æ„ŸçŸ¥å·©å›ºå’Œæ— ç›‘ç£æŸ¥è¯¢æ­£åˆ™åŒ–ã€‚</li>
<li>æ–¹æ³•æ—¨åœ¨æ”¹å–„ä»»åŠ¡å¹³è¡¡å¹¶é˜²æ­¢åŸºç¡€æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç‰¹å®šæŒ‡ä»¤å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>LLaVA-cåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¿æŒäº†é€šç”¨èƒ½åŠ›ã€‚</li>
<li>ä»»åŠ¡é€ä¸ªçš„å¢é‡å­¦ä¹ å¯ä»¥è¾¾åˆ°æˆ–å¤šäºå¤šä»»åŠ¡è”åˆå­¦ä¹ çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c493b3d0a83643c4fae13fc5f3cd234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66f125ec5a9bd761b4a6960f8179e867.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa2e0d1034cfbdc9489c707222e1cdc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c2a78e6382a441a72eb41128b924703.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-915cb02b371f411b4c79c1da79fd1ab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1893772bcaeff96415435098df0e75dd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="JoFormer-Journey-based-Transformer-Theory-and-Empirical-Analysis-on-the-Tiny-Shakespeare-Dataset"><a href="#JoFormer-Journey-based-Transformer-Theory-and-Empirical-Analysis-on-the-Tiny-Shakespeare-Dataset" class="headerlink" title="JoFormer (Journey-based Transformer): Theory and Empirical Analysis on   the Tiny Shakespeare Dataset"></a>JoFormer (Journey-based Transformer): Theory and Empirical Analysis on   the Tiny Shakespeare Dataset</h2><p><strong>Authors:Mahesh Godavarti</strong></p>
<p>Transformers have demonstrated remarkable success in sequence modeling, yet effectively incorporating positional information remains a challenging and active area of research. In this paper, we introduce JoFormer, a journey-based Transformer architecture grounded in a recently proposed non-commutative algebra for composing transformations across positions. JoFormer represents relative positions through learnable directional transforms that are sequentially composed along the input, thereby extending and generalizing existing approaches based on relative position representations. We derive the JoFormer attention mechanism from first principles and show that it subsumes standard methods such as rotary transformations as special cases. To evaluate its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny Shakespeare character-level language modeling task. Our results demonstrate that   JoFormer consistently achieves lower perplexity and faster convergence, highlighting the advantages of its more expressive, journey-based treatment of position. Notably, the per-token JoFormer is still a primitive, conceptual variant with layer-independent angles, yet it already demonstrates strong performance-underscoring its promise as a proof of concept for more expressive architectures. We conclude by discussing how JoFormer offers a principled approach to integrating positional structure into Transformer architectures. The code used in this work is available at <a target="_blank" rel="noopener" href="https://github.com/mahesh-godavarti/joformer">https://github.com/mahesh-godavarti/joformer</a>. </p>
<blockquote>
<p>Transformeråœ¨åºåˆ—å»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œä½†å¦‚ä½•æœ‰æ•ˆåœ°èå…¥ä½ç½®ä¿¡æ¯ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡ä»‹ç»äº†JoFormerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ—…ç¨‹çš„Transformeræ¶æ„ï¼Œå®ƒåŸºäºæœ€è¿‘æå‡ºçš„éäº¤æ¢ä»£æ•°ï¼Œç”¨äºåœ¨ä½ç½®ä¹‹é—´ç»„åˆè½¬æ¢ã€‚JoFormeré€šè¿‡å¯å­¦ä¹ çš„æ–¹å‘è½¬æ¢æ¥è¡¨ç¤ºç›¸å¯¹ä½ç½®ï¼Œè¿™äº›è½¬æ¢æ²¿ç€è¾“å…¥é¡ºåºç»„åˆï¼Œä»è€Œæ‰©å±•å’Œæ¦‚æ‹¬äº†åŸºäºç›¸å¯¹ä½ç½®è¡¨ç¤ºçš„ç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬ä»åŸºæœ¬åŸç†æ¨å¯¼å‡ºJoFormerçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶è¯æ˜å®ƒæ¶µç›–äº†æ ‡å‡†æ–¹æ³•ï¼Œå¦‚æ—‹è½¬å˜æ¢ç­‰ç‰¹æ®Šæƒ…å†µã€‚ä¸ºäº†è¯„ä¼°å…¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨Tiny Shakespeareå­—ç¬¦çº§è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šå°†JoFormerä¸RoFormeråŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒJoFormeræŒç»­å®ç°æ›´ä½çš„å›°æƒ‘åº¦å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œå‡¸æ˜¾äº†å…¶åŸºäºæ—…ç¨‹çš„ä½ç½®å¤„ç†çš„æ›´å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¯ä¸ªä»¤ç‰Œçš„JoFormerä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰å±‚ç‹¬ç«‹è§’åº¦çš„åŸå§‹æ¦‚å¿µå˜ä½“ï¼Œä½†å®ƒå·²ç»æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¼ºè°ƒäº†å…¶ä½œä¸ºæ›´å…·è¡¨ç°åŠ›æ¶æ„çš„æ¦‚å¿µè¯æ˜çš„å‰æ™¯ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†JoFormerå¦‚ä½•å°†ä½ç½®ç»“æ„èå…¥Transformeræ¶æ„çš„åŸåˆ™æ–¹æ³•ã€‚æœ¬å·¥ä½œä¸­ä½¿ç”¨çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mahesh-godavarti/joformer">https://github.com/mahesh-godavarti/joformer</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08652v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ—…ç¨‹çš„Transformeræ¶æ„JoFormerï¼Œå®ƒé€šè¿‡éäº¤æ¢ä»£æ•°æ¥ç»„åˆä¸åŒä½ç½®ä¹‹é—´çš„è½¬æ¢ï¼Œä»è€Œæœ‰æ•ˆåœ°èå…¥ä½ç½®ä¿¡æ¯ã€‚JoFormeré€šè¿‡å¯å­¦ä¹ çš„æ–¹å‘è½¬æ¢æ¥è¡¨ç¤ºç›¸å¯¹ä½ç½®ï¼Œå¹¶æ²¿ç€è¾“å…¥è¿›è¡Œé¡ºåºç»„åˆï¼Œä»è€Œæ‰©å±•å¹¶æ¨å¹¿äº†åŸºäºç›¸å¯¹ä½ç½®è¡¨ç¤ºçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJoFormeråœ¨Tiny Shakespeareå­—ç¬¦çº§è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šè¾ƒRoFormeråŸºçº¿æ¨¡å‹å–å¾—äº†æ›´ä½çš„å›°æƒ‘åº¦å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚å°½ç®¡JoFormerä½œä¸ºä¸€ä¸ªåŸºäºå±‚ç‹¬ç«‹çš„è§†è§’çš„åŸå§‹æ¦‚å¿µå˜ä½“ä»ç„¶å…·æœ‰å‰æ™¯ï¼Œä½†å…¶è¡¨ç°å·²å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JoFormeræ˜¯ä¸€ç§åŸºäºæ—…ç¨‹çš„Transformeræ¶æ„ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°èå…¥ä½ç½®ä¿¡æ¯ã€‚</li>
<li>å®ƒé€šè¿‡éäº¤æ¢ä»£æ•°æ¥ç»„åˆä¸åŒä½ç½®é—´çš„è½¬æ¢ï¼Œä»è€Œè¡¨ç¤ºç›¸å¯¹ä½ç½®ã€‚</li>
<li>JoFormerå¼•å…¥äº†å¯å­¦ä¹ çš„æ–¹å‘è½¬æ¢ï¼Œè¿™äº›è½¬æ¢æ²¿ç€è¾“å…¥è¿›è¡Œé¡ºåºç»„åˆã€‚</li>
<li>JoFormeråœ¨Tiny Shakespeareå­—ç¬¦çº§è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¾ƒä½å›°æƒ‘åº¦å’Œæ›´å¿«æ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>JoFormerä½œä¸ºä¸€ä¸ªæ¦‚å¿µéªŒè¯æ¨¡å‹ï¼Œå·²ç»å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚</li>
<li>JoFormeræä¾›äº†ä¸€ä¸ªå°†ä½ç½®ç»“æ„æ•´åˆåˆ°Transformeræ¶æ„ä¸­çš„åŸåˆ™æ€§æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c8e3d5e94684a91f07aa84ff8cf044c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ab9a8b3cc6d6c5ed57160876a784edf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adec4e9a38bf4115834e4dffc1440d79.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TableDreamer-Progressive-and-Weakness-guided-Data-Synthesis-from-Scratch-for-Table-Instruction-Tuning"><a href="#TableDreamer-Progressive-and-Weakness-guided-Data-Synthesis-from-Scratch-for-Table-Instruction-Tuning" class="headerlink" title="TableDreamer: Progressive and Weakness-guided Data Synthesis from   Scratch for Table Instruction Tuning"></a>TableDreamer: Progressive and Weakness-guided Data Synthesis from   Scratch for Table Instruction Tuning</h2><p><strong>Authors:Mingyu Zheng, Zhifan Feng, Jia Wang, Lanrui Wang, Zheng Lin, Yang Hao, Weiping Wang</strong></p>
<p>Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data. The code and data is available at <a target="_blank" rel="noopener" href="https://github.com/SpursGoZmy/TableDreamer">https://github.com/SpursGoZmy/TableDreamer</a> </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°æ®åˆæˆæ–¹æ³•å–å¾—äº†å€¼å¾—ç§°èµçš„è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆè¡¨æ ¼æŒ‡ä»¤è°ƒæ•´æ•°æ®æ–¹é¢ä»é¢ä¸´ä¸¤ä¸ªå±€é™ã€‚é¦–å…ˆï¼Œå®ƒä»¬æ— æ³•å…¨é¢æ¢ç´¢è¡¨æ ¼ç†è§£ä»»åŠ¡çš„å¹¿é˜”è¾“å…¥ç©ºé—´ï¼Œå¯¼è‡´æ•°æ®å¤šæ ·æ€§æœ‰é™ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬å¿½ç•¥äº†ç›®æ ‡LLMåœ¨è¡¨æ ¼ç†è§£æ–¹é¢çš„å¼±ç‚¹ï¼Œç›²ç›®è¿½æ±‚æ•°æ®é‡çš„å¢åŠ ï¼Œå¯¼è‡´æ•°æ®æ•ˆç‡ä¸ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08646v1">PDF</a> 27 pages, 19 figures, Findings of ACL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¡¨æ ¼æŒ‡ä»¤è°ƒæ•´çš„æ•°æ®åˆæˆæ¡†æ¶TableDreamerï¼Œæ—¨åœ¨è§£å†³ç°æœ‰LLMåœ¨è¡¨æ ¼ç†è§£ä»»åŠ¡ä¸Šçš„æ•°æ®å¤šæ ·æ€§ä¸è¶³åŠæ•°æ®æ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚é€šè¿‡åˆæˆå¤šæ ·åŒ–çš„è¡¨æ ¼å’Œç›¸å…³æŒ‡ä»¤ä½œä¸ºç§å­æ•°æ®ï¼Œå¹¶åœ¨æ–°è¯†åˆ«çš„å¼±ç‚¹æ•°æ®æŒ‡å¯¼ä¸‹è¿›è¡Œè¾“å…¥ç©ºé—´çš„è¿­ä»£æ¢ç´¢ï¼Œæœ€ç»ˆç”Ÿæˆç”¨äºå¾®è°ƒç›®æ ‡LLMçš„è®­ç»ƒæ•°æ®ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªè¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½¿ç”¨2.7ä¸‡æ¡GPT-4oåˆæˆæ•°æ®å°†Llama3.1-8B-instructçš„å¹³å‡å‡†ç¡®ç‡æå‡11.62%ï¼Œå¹¶ä¼˜äºä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®çš„å…¶ä»–æ•°æ®åˆæˆåŸºçº¿æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨GitHubä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç”Ÿæˆè¡¨æ ¼æŒ‡ä»¤è°ƒæ•´æ•°æ®æ—¶é¢ä¸´è¾“å…¥ç©ºé—´æ¢ç´¢ä¸å…¨é¢å’Œå¿½ç•¥æ¨¡å‹å¼±ç‚¹çš„é—®é¢˜ã€‚</li>
<li>TableDreameræ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡åˆæˆå¤šæ ·åŒ–çš„è¡¨æ ¼å’ŒæŒ‡ä»¤ä½œä¸ºç§å­æ•°æ®ã€‚</li>
<li>åœ¨æ–°è¯†åˆ«çš„å¼±ç‚¹æ•°æ®æŒ‡å¯¼ä¸‹è¿›è¡Œè¾“å…¥ç©ºé—´çš„è¿­ä»£æ¢ç´¢ï¼Œæé«˜æ•°æ®æ•ˆç‡ã€‚</li>
<li>å®éªŒè¯æ˜TableDreameråœ¨å¤šä¸ªè¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ä½¿ç”¨è¾ƒå°‘çš„åˆæˆæ•°æ®ï¼ˆ2.7ä¸‡æ¡GPT-4oæ•°æ®ï¼‰æ˜¾è‘—æå‡Llama3.1-8B-instructæ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡ã€‚</li>
<li>ä¸å…¶ä»–ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®çš„æ–¹æ³•ç›¸æ¯”ï¼ŒTableDreamerè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7db72c96fe18b5ded803f3c46a0a6db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-368cc45fa8bf82ae13ed5b756251351e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcd0aaac2cbfeac24c0f167471263488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9df469feefed2d0bf2ffd73bcf22257.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Fairness-is-Not-Silence-Unmasking-Vacuous-Neutrality-in-Small-Language-Models"><a href="#Fairness-is-Not-Silence-Unmasking-Vacuous-Neutrality-in-Small-Language-Models" class="headerlink" title="Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language   Models"></a>Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language   Models</h2><p><strong>Authors:Sumanth Manduru, Carlotta Domeniconi</strong></p>
<p>The rapid adoption of Small Language Models (SLMs) for on-device and resource-constrained deployments has outpaced our understanding of their ethical risks. To the best of our knowledge, we present the first large-scale audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an overlooked â€œmiddle tierâ€ between BERT-class encoders and flagship LLMs. Our evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma 3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we analyze both utility and fairness across ambiguous and disambiguated contexts. This evaluation reveals three key insights. First, competence and fairness need not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while exhibiting minimal bias, showing that efficient and ethical NLP is attainable. Second, social bias varies significantly by architecture: Qwen 2.5 models may appear fair, but this often reflects vacuous neutrality, random guessing, or evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2 models exhibit stronger stereotypical bias, suggesting overconfidence rather than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but increases disability-related bias in Phi-4-Mini by over 7 percentage points. These insights provide practical guidance for the responsible deployment of SLMs in applications demanding fairness and efficiency, particularly benefiting small enterprises and resource-constrained environments. </p>
<blockquote>
<p>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨è®¾å¤‡å’Œèµ„æºå—é™çš„éƒ¨ç½²ä¸­çš„å¿«é€Ÿé‡‡çº³å·²ç»è¶…å‡ºäº†æˆ‘ä»¬å¯¹å®ƒä»¬ä¼¦ç†é£é™©çš„è®¤çŸ¥ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬å¯¹è·¨è¶Š0.5è‡³5äº¿å‚æ•°çš„æŒ‡ä»¤è°ƒæ•´SLMè¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡å®¡è®¡ï¼Œè¿™æ˜¯ä¸€ä¸ªè¢«å¿½è§†çš„ä»‹äºBERTç±»ç¼–ç å™¨å’Œæ——èˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„â€œä¸­å±‚â€ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬æ¥è‡ªQwen 2.5ã€LLaMA 3.2ã€Gemma 3å’ŒPhiå®¶æ—çš„ä¹ä¸ªå¼€æºæ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨BBQåŸºå‡†æµ‹è¯•åœ¨æ— æç¤ºçš„æƒ…å†µä¸‹åˆ†ææ¨¡ç³Šå’Œæ˜ç¡®è¯­å¢ƒä¸­çš„å®ç”¨æ€§å’Œå…¬å¹³æ€§ã€‚è¿™æ¬¡è¯„ä¼°æ­ç¤ºäº†ä¸‰ä¸ªå…³é”®è§è§£ã€‚é¦–å…ˆï¼Œèƒ½åŠ›å’Œå…¬å¹³ä¸å¿…ç›¸äº’å¯¹ç«‹ï¼šPhiæ¨¡å‹åœ¨F1å¾—åˆ†ä¸Šè¶…è¿‡äº†90%ï¼ŒåŒæ—¶è¡¨ç°å‡ºæä½çš„åè§ï¼Œè¡¨æ˜é«˜æ•ˆå’Œä¼¦ç†è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯å¯è¡Œçš„ã€‚å…¶æ¬¡ï¼Œç¤¾ä¼šåè§å› æ¶æ„è€Œå¼‚ï¼šQwen 2.5æ¨¡å‹å¯èƒ½çœ‹èµ·æ¥æ˜¯å…¬å¹³çš„ï¼Œä½†è¿™å¾€å¾€åæ˜ å‡ºç©ºæ´çš„ä¸­ç«‹ã€éšæœºçŒœæµ‹æˆ–å›é¿è¡Œä¸ºï¼Œè€Œä¸æ˜¯çœŸæ­£çš„é“å¾·å¥‘åˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLLaMA 3.2æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„åˆ»æ¿åè§ï¼Œè¿™è¡¨æ˜è¿‡äºè‡ªä¿¡è€Œä¸æ˜¯ä¸­ç«‹ã€‚ç¬¬ä¸‰ï¼Œå‹ç¼©å¸¦æ¥äº†å¾®å¦™çš„æƒè¡¡ï¼šå¯¹äºLLaMA 3.2-3Bæ¥è¯´ï¼Œä½¿ç”¨AWQ 4ä½é‡åŒ–æé«˜äº†æ¨¡ç³Šè®¾ç½®ä¸­çš„F1å¾—åˆ†ï¼Œä½†å¢åŠ äº†Phi-4-Miniä¸­ä¸æ®‹ç–¾ç›¸å…³çš„åè§è¶…è¿‡7ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›è§è§£ä¸ºåœ¨éœ€è¦å…¬å¹³å’Œæ•ˆç‡çš„åº”ç”¨ä¸­è´Ÿè´£ä»»åœ°éƒ¨ç½²SLMæä¾›äº†å®é™…æŒ‡å¯¼ï¼Œå°¤å…¶æœ‰ç›Šäºä¸­å°ä¼ä¸šå’Œèµ„æºå—é™çš„ç¯å¢ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08487v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å¯¹è·¨è¶Š0.5è‡³5äº¿å‚æ•°çš„ä¸­ç­‰è§„æ¨¡æŒ‡ä»¤è°ƒæ•´çš„å°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡çš„å®¡è®¡è¯„ä¼°ï¼Œè¿™æ˜¯ä»‹äºBERTç±»ç¼–ç å™¨å’Œæ——èˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´è¢«å¿½è§†çš„ä¸­é—´å±‚ã€‚é€šè¿‡å¼€æ”¾æºä»£ç çš„ä¹ä¸ªæ¨¡å‹ï¼ˆåŒ…æ‹¬Qwen 2.5ã€LLaMA 3.2ã€Gemma 3å’ŒPhiç³»åˆ—æ¨¡å‹ï¼‰çš„è¯„ä¼°ï¼Œåˆ©ç”¨BBQåŸºå‡†æµ‹è¯•ä¸‹çš„é›¶æ ·æœ¬æç¤ºæ³•ï¼Œæœ¬æ–‡åˆ†æäº†æ¨¡ç³Šå’Œæ˜ç¡®è¯­å¢ƒä¸‹çš„å®ç”¨æ€§å’Œå…¬å¹³æ€§ã€‚å…³é”®å‘ç°åŒ…æ‹¬ï¼šèƒ½åŠ›å’Œå…¬å¹³å¹¶éå¯¹ç«‹å…³ç³»ï¼›ä¸åŒæ¶æ„çš„ç¤¾ä¼šåè§å·®å¼‚æ˜¾è‘—ï¼›å‹ç¼©å¸¦æ¥å¾®å¦™çš„æƒè¡¡ç­‰ã€‚è¿™äº›è§è§£ä¸ºå°å‹ä¼ä¸šå’Œèµ„æºå—é™ç¯å¢ƒä¸­çš„è¯­è¨€æ¨¡å‹è´Ÿè´£ä»»éƒ¨ç½²æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€èƒ½åŠ›å’Œå…¬å¹³å¹¶éå¯¹ç«‹å…³ç³»ã€‚Phiæ¨¡å‹åœ¨F1å¾—åˆ†ä¸Šè¶…è¿‡90%çš„åŒæ—¶å±•ç°å‡ºå¾®å°çš„åè§ï¼Œè¯æ˜äº†é«˜æ•ˆä¸”é“å¾·çš„NLPæ˜¯å¯è¡Œçš„ã€‚</p>
<p>äºŒã€ä¸åŒæ¶æ„çš„è¯­è¨€æ¨¡å‹å­˜åœ¨æ˜¾è‘—çš„ç¤¾ä¼šåè§å·®å¼‚ã€‚Qwen 2.5æ¨¡å‹å¯èƒ½è¡¨ç°å‡ºå…¬å¹³ï¼Œä½†è¿™å¾€å¾€åæ˜ å‡ºç©ºæ´çš„ä¸­ç«‹æ€§ã€éšæœºçŒœæµ‹æˆ–å›é¿è¡Œä¸ºè€ŒéçœŸæ­£çš„é“å¾·å¯¹é½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLLaMA 3.2æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„åˆ»æ¿åè§ï¼Œæš—ç¤ºå…¶è¿‡äºè‡ªä¿¡è€Œéä¸­ç«‹ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3f09700b9a57629e4cfaaa8f88559620.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ffdc140c943a78ccd61f1880a72d136.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a826c65a7041cf46ca4a73f9b49293f1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Instruction-Tuned-Video-Audio-Models-Elucidate-Functional-Specialization-in-the-Brain"><a href="#Instruction-Tuned-Video-Audio-Models-Elucidate-Functional-Specialization-in-the-Brain" class="headerlink" title="Instruction-Tuned Video-Audio Models Elucidate Functional Specialization   in the Brain"></a>Instruction-Tuned Video-Audio Models Elucidate Functional Specialization   in the Brain</h2><p><strong>Authors:Subba Reddy Oota, Khushbu Pahwa, Prachi Jindal, Satya Sai Srinath Namburi, Maneesh Singh, Tanmoy Chakraborty, Bapi S. Raju, Manish Gupta</strong></p>
<p>Recent voxel-wise multimodal brain encoding studies have shown that multimodal large language models (MLLMs) exhibit a higher degree of brain alignment compared to unimodal models in both unimodal and multimodal stimulus settings. More recently, instruction-tuned multimodal models have shown to generate task-specific representations that align strongly with brain activity. However, prior work evaluating the brain alignment of MLLMs has primarily focused on unimodal settings or relied on non-instruction-tuned multimodal models for multimodal stimuli. To address this gap, we investigated brain alignment, that is, measuring the degree of predictivity of neural activity recorded while participants were watching naturalistic movies (video along with audio) with representations derived from MLLMs. We utilized instruction-specific embeddings from six video and two audio instruction-tuned MLLMs. Experiments with 13 video task-specific instructions show that instruction-tuned video MLLMs significantly outperform non-instruction-tuned multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for both video and audio tasks using language-guided instructions shows clear disentanglement in task-specific representations from MLLMs, leading to precise differentiation of multimodal functional processing in the brain. We also find that MLLM layers align hierarchically with the brain, with early sensory areas showing strong alignment with early layers, while higher-level visual and language regions align more with middle to late layers. These findings provide clear evidence for the role of task-specific instructions in improving the alignment between brain activity and MLLMs, and open new avenues for mapping joint information processing in both the systems. We make the code publicly available [<a target="_blank" rel="noopener" href="https://github.com/subbareddy248/mllm_videos]">https://github.com/subbareddy248/mllm_videos]</a>. </p>
<blockquote>
<p>è¿‘æœŸçš„é€ä½“ç´ å¤šæ¨¡æ€å¤§è„‘ç¼–ç ç ”ç©¶è¡¨æ˜ï¼Œä¸å•æ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€åˆºæ¿€ç¯å¢ƒä¸­éƒ½è¡¨ç°å‡ºæ›´é«˜çš„å¤§è„‘å¯¹é½ç¨‹åº¦ã€‚æœ€è¿‘ï¼Œç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡æ€æ¨¡å‹æ˜¾ç¤ºå‡ºèƒ½å¤Ÿç”Ÿæˆä¸å¤§è„‘æ´»åŠ¨ç´§å¯†å¯¹é½çš„ä»»åŠ¡ç‰¹å®šè¡¨ç¤ºã€‚ç„¶è€Œï¼Œå…ˆå‰è¯„ä¼°MLLMçš„å¤§è„‘å¯¹é½çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨å•æ¨¡æ€ç¯å¢ƒï¼Œæˆ–è€…ä¾èµ–äºéæŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€åˆºæ¿€ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤§è„‘å¯¹é½ï¼Œå³æµ‹é‡å‚ä¸è€…åœ¨è§‚çœ‹è‡ªç„¶ç”µå½±ï¼ˆè§†é¢‘å’ŒéŸ³é¢‘ï¼‰æ—¶çš„ç¥ç»æ´»åŠ¨é¢„æµ‹åº¦ï¼Œè¯¥é¢„æµ‹åº¦æ¥æºäºMLLMsçš„è¡¨ç¤ºã€‚æˆ‘ä»¬ä½¿ç”¨äº†å…­ä¸ªè§†é¢‘å’Œä¸¤ä¸ªéŸ³é¢‘æŒ‡ä»¤è°ƒæ•´è¿‡çš„MLLMçš„æŒ‡ä»¤ç‰¹å®šåµŒå…¥ã€‚æœ‰13ä¸ªè§†é¢‘ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤çš„å®éªŒæ˜¾ç¤ºï¼ŒæŒ‡ä»¤è°ƒæ•´è¿‡çš„è§†é¢‘MLLMsæ˜¾è‘—ä¼˜äºéæŒ‡ä»¤è°ƒæ•´è¿‡çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆé«˜å‡º15%ï¼‰å’Œå•æ¨¡æ€æ¨¡å‹ï¼ˆé«˜å‡º20%ï¼‰ã€‚æˆ‘ä»¬å¯¹è§†é¢‘å’ŒéŸ³é¢‘ä»»åŠ¡ä½¿ç”¨è¯­è¨€æŒ‡å¯¼æŒ‡ä»¤çš„MLLMè¯„ä¼°æ˜¾ç¤ºï¼Œä»»åŠ¡ç‰¹å®šè¡¨ç¤ºåœ¨MLLMä¸­æ˜ç¡®åˆ†ç¦»ï¼Œå¯¼è‡´å¤§è„‘ä¸­çš„å¤šæ¨¡æ€åŠŸèƒ½å¤„ç†ç²¾ç¡®åŒºåˆ†ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼ŒMLLMå±‚ä¸å¤§è„‘å±‚æ¬¡å¯¹é½ï¼Œæ—©æœŸæ„Ÿå®˜åŒºåŸŸä¸æ—©æœŸå±‚å¯¹é½å¼ºçƒˆï¼Œè€Œé«˜çº§è§†è§‰å’Œè¯­è¨€åŒºåŸŸä¸ä¸­å±‚è‡³åæœŸå±‚å¯¹é½æ›´å¤šã€‚è¿™äº›å‘ç°æ¸…æ¥šåœ°è¯æ˜äº†ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤åœ¨æé«˜å¤§è„‘æ´»åŠ¨ä¸MLLMå¯¹é½æ–¹é¢çš„ä½œç”¨ï¼Œå¹¶ä¸ºæ˜ å°„ä¸¤ä¸ªç³»ç»Ÿä¸­çš„è”åˆä¿¡æ¯å¤„ç†å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚æˆ‘ä»¬å…¬å¼€æä¾›äº†ä»£ç ï¼š[<a target="_blank" rel="noopener" href="https://github.com/subbareddy248/mllm_videos]%E3%80%82">https://github.com/subbareddy248/mllm_videos]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08277v1">PDF</a> 39 pages, 22 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§è„‘ç¼–ç ç ”ç©¶è¡¨æ˜ï¼Œä¸å•æ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€åˆºæ¿€ç¯å¢ƒä¸‹ä¸å¤§è„‘çš„å¥‘åˆåº¦æ›´é«˜ã€‚ç‰¹åˆ«æ˜¯ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸å¤§è„‘æ´»åŠ¨é«˜åº¦å¥‘åˆçš„ä»»åŠ¡ç‰¹å®šè¡¨å¾ã€‚ç„¶è€Œï¼Œå…ˆå‰å¯¹MLLMsçš„å¤§è„‘å¥‘åˆåº¦çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨å•æ¨¡æ€ç¯å¢ƒï¼Œæˆ–åœ¨å¤šæ¨¡æ€åˆºæ¿€ä¸‹ä¾èµ–äºéæŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€ç©ºç™½ï¼Œé€šè¿‡æµ‹é‡å‚ä¸è€…åœ¨è§‚çœ‹è‡ªç„¶ç”µå½±ï¼ˆè§†é¢‘å’ŒéŸ³é¢‘ï¼‰æ—¶çš„ç¥ç»æ´»åŠ¨ä¸MLLMsè¡¨å¾çš„é¢„æµ‹åº¦æ¥æ¢ç©¶å¤§è„‘å¥‘åˆåº¦ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªå…­ä¸ªè§†é¢‘å’Œä¸¤ä¸ªéŸ³é¢‘æŒ‡ä»¤è°ƒæ•´è¿‡çš„MLLMsçš„æŒ‡ä»¤ç‰¹å®šåµŒå…¥ã€‚å®éªŒæ˜¾ç¤ºï¼Œé’ˆå¯¹è§†é¢‘ä»»åŠ¡çš„æŒ‡ä»¤è°ƒæ•´è¿‡çš„MLLMsæ¯”éæŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡æ€æ¨¡å‹é«˜å‡º15%ï¼Œæ¯”å•æ¨¡æ€æ¨¡å‹é«˜å‡º20%ã€‚æˆ‘ä»¬å¯¹è§†é¢‘å’ŒéŸ³é¢‘ä»»åŠ¡ä½¿ç”¨è¯­è¨€æŒ‡å¯¼æŒ‡ä»¤çš„MLLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä»»åŠ¡ç‰¹å®šè¡¨å¾çš„æ¸…æ™°åˆ†ç¦»å¯¼è‡´å¤§è„‘ä¸­çš„å¤šæ¨¡æ€åŠŸèƒ½å¤„ç†çš„ç²¾ç¡®åŒºåˆ†ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼ŒMLLMå±‚ä¸å¤§è„‘çš„å±‚æ¬¡ç»“æ„ç›¸å»åˆï¼Œæ—©æœŸæ„Ÿå®˜åŒºåŸŸä¸æ—©æœŸå±‚è¡¨ç°å‡ºå¼ºçƒˆçš„å¥‘åˆåº¦ï¼Œè€Œé«˜çº§è§†è§‰å’Œè¯­è¨€åŒºåŸŸåˆ™ä¸ä¸­å±‚è‡³åæœŸå±‚æ›´åŠ å¥‘åˆã€‚è¿™äº›å‘ç°è¯æ˜äº†ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤åœ¨æé«˜å¤§è„‘æ´»åŠ¨ä¸MLLMså¥‘åˆåº¦æ–¹é¢çš„ä½œç”¨ï¼Œå¹¶ä¸ºæ˜ å°„ä¸¤ä¸ªç³»ç»Ÿçš„è”åˆä¿¡æ¯å¤„ç†å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚æˆ‘ä»¬å…¬å¼€äº†ç›¸å…³ä»£ç ï¼š[<a target="_blank" rel="noopener" href="https://github.com/subbareddy248/mllm_videos]%E3%80%82">https://github.com/subbareddy248/mllm_videos]ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€åˆºæ¿€ç¯å¢ƒä¸‹ä¸å¤§è„‘çš„å¥‘åˆåº¦è¾ƒé«˜ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´è¿‡çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ç‰¹å®šè¡¨å¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸å¤§è„‘æ´»åŠ¨é«˜åº¦å¥‘åˆã€‚</li>
<li>ä¸éæŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡æ€æ¨¡å‹å’Œå•æ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼ŒæŒ‡ä»¤è°ƒæ•´è¿‡çš„è§†é¢‘MLLMsæ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
<li>MLLMçš„ä»»åŠ¡ç‰¹å®šè¡¨å¾èƒ½å¤Ÿç²¾ç¡®åŒºåˆ†å¤§è„‘ä¸­çš„å¤šæ¨¡æ€åŠŸèƒ½å¤„ç†ã€‚</li>
<li>MLLMå±‚ä¸å¤§è„‘å±‚æ¬¡ç»“æ„ç›¸å»åˆï¼Œæ—©æœŸå’Œé«˜çº§æ„Ÿå®˜åŒºåŸŸçš„å¥‘åˆåº¦è¡¨ç°ä¸åŒã€‚</li>
<li>ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤æœ‰åŠ©äºæé«˜å¤§è„‘æ´»åŠ¨ä¸MLLMsçš„å¥‘åˆåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb05c3ca48c75e77d663d09270b2690f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a259d1920b9309f95c804e685d6745d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a683f4c1a37643ef4860af64d473396f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd819f9d6e491f4e3b28fa756d56722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-307b46993b2f27eaf5fbc7e7205ab656.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="GTR-CoT-Graph-Traversal-as-Visual-Chain-of-Thought-for-Molecular-Structure-Recognition"><a href="#GTR-CoT-Graph-Traversal-as-Visual-Chain-of-Thought-for-Molecular-Structure-Recognition" class="headerlink" title="GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular   Structure Recognition"></a>GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular   Structure Recognition</h2><p><strong>Authors:Jingchao Wang, Haote Yang, Jiang Wu, Yifan He, Xingjian Wei, Yinfan Wang, Chengjin Liu, Lingli Ge, Lijun Wu, Bin Wang, Dahua Lin, Conghui He</strong></p>
<p>Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What Youâ€™ve Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at <a target="_blank" rel="noopener" href="https://github.com/opendatalab/GTR-CoT">https://github.com/opendatalab/GTR-CoT</a>. </p>
<blockquote>
<p>å…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼ˆOCSRï¼‰æ˜¯å°†åŒ–å­¦çŸ¥è¯†æ•°å­—åŒ–çš„å…³é”®ç¯èŠ‚ï¼Œå®ƒé€šè¿‡è½¬åŒ–åˆ†å­å›¾åƒä¸ºæœºå™¨å¯è¯»çš„æ ¼å¼æ¥å®ç°ã€‚å°½ç®¡æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„åˆ†å­ç»“æ„å’Œä¸ä¸€è‡´çš„æ³¨é‡Šæ—¶ç»å¸¸é‡åˆ°å›°éš¾ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GTR-Mol-VLMè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå…·æœ‰ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆä¸€ï¼‰é€šè¿‡åºåˆ—åŒ–çš„åŸå­é”®é¢„æµ‹é€æ­¥è§£æåˆ†å­å›¾çš„æ€ç»´é“¾æœºåˆ¶ï¼Œæ¨¡æ‹Ÿäººç±»çš„æ¨ç†è¿‡ç¨‹ï¼›ï¼ˆäºŒï¼‰å¿ å®è¯†åˆ«ä½ æ‰€çœ‹åˆ°çš„çš„æ•°æ®ä¸­å¿ƒåŸåˆ™ï¼Œè§£å†³å›¾åƒä¸­çš„ç¼©ç•¥ç»“æ„ä¸æ‰©å±•æ³¨é‡Šä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹å¼€å‘ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†GTR-CoT-1.3Mï¼Œå…¶ä¸­åŒ…å«ç²¾å¿ƒä¿®æ­£çš„æ³¨é‡Šï¼Œå¹¶æ¨å‡ºäº†MolRec-Benchè¿™ä¸€é¦–ä¸ªé’ˆå¯¹OCSRä¸­å›¾å½¢è§£æå‡†ç¡®æ€§çš„ç²¾ç»†è¯„ä¼°è®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒGTR-Mol-VLMç›¸è¾ƒäºä¸“ä¸šæ¨¡å‹ã€åŒ–å­¦é¢†åŸŸçš„VLMä»¥åŠå•†ä¸šé€šç”¨VLMå–å¾—äº†ä¼˜è¶Šçš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å¤„ç†å¸¦æœ‰å®˜èƒ½å›¢ç¼©ç•¥çš„åˆ†å­å›¾åƒåœºæ™¯ä¸­ï¼ŒGTR-Mol-VLMåœ¨SMILESå’Œå›¾å½¢åº¦é‡æŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç¬¬äºŒååŸºå‡†æµ‹è¯•çº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¨åŠ¨OCSRæŠ€æœ¯æ›´æœ‰æ•ˆåœ°æ»¡è¶³ç°å®ä¸–ç•Œçš„éœ€è¦ï¼Œä»è€Œä¿ƒè¿›åŒ–å­¦ä¿¡æ¯å­¦å’Œäººå·¥æ™ºèƒ½ç§‘å­¦é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/opendatalab/GTR-CoT%E4%B8%8A%E5%8F%91%E5%B8%83GTR-CoT%E3%80%82">https://github.com/opendatalab/GTR-CoTä¸Šå‘å¸ƒGTR-CoTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07553v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼ˆOCSRï¼‰æ˜¯æ•°å­—åŒ–åŒ–å­¦çŸ¥è¯†çš„é‡è¦æŠ€æœ¯ï¼Œèƒ½å°†åˆ†å­å›¾åƒè½¬åŒ–ä¸ºæœºå™¨å¯è¯»çš„æ ¼å¼ã€‚é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚åˆ†å­ç»“æ„å’Œä¸ä¸€è‡´æ³¨é‡Šæ—¶çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†GTR-Mol-VLMæ¡†æ¶ï¼ŒåŒ…å«ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯å›¾éå†ä½œä¸ºè§†è§‰æ€ç»´é“¾æœºåˆ¶ï¼Œé€šè¿‡è¿ç»­çš„åŸå­é”®é¢„æµ‹é€æ­¥è§£æåˆ†å­å›¾ï¼Œæ¨¡æ‹Ÿäººç±»æ¨ç†ï¼›äºŒæ˜¯å¿ å®è¯†åˆ«ä½ æ‰€è§åˆ°çš„æ•°æ®ä¸ºä¸­å¿ƒçš„åŸåˆ™ï¼Œè§£å†³å›¾åƒä¸­çš„ç®€åŒ–ç»“æ„ä¸æ‰©å±•æ³¨é‡Šä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºæ”¯æŒæ¨¡å‹å¼€å‘ï¼Œæœ¬æ–‡æ„å»ºäº†GTR-CoT-1.3Må¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†MolRec-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºç²¾ç»†è¯„ä¼°OCSRä¸­å›¾è§£æçš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒGTR-Mol-VLMç›¸è¾ƒäºä¸“ä¸šæ¨¡å‹ã€åŒ–å­¦é¢†åŸŸè§†è§‰è¯­è¨€æ¨¡å‹å’Œå•†ä¸šé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¸¦æœ‰åŠŸèƒ½ç»„ç¼©ç•¥è¯­çš„åˆ†å­å›¾åƒæ—¶ï¼ŒGTR-Mol-VLMåœ¨SMILESå’Œå›¾å½¢æŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç¬¬äºŒååŸºå‡†æµ‹è¯•çº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚æœ¬æ–‡å·¥ä½œå°†æ¨åŠ¨OCSRæŠ€æœ¯æ›´å¥½åœ°æ»¡è¶³ç°å®éœ€æ±‚ï¼Œæ¨åŠ¨åŒ–å­¦ä¿¡æ¯å­¦å’Œäººå·¥æ™ºèƒ½ç§‘å­¦é¢†åŸŸçš„å‘å±•ã€‚GTR-CoTå°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/opendatalab/gtr-cot%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/opendatalab/GTR-CoTå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼ˆOCSRï¼‰æ˜¯æ•°å­—åŒ–åŒ–å­¦çŸ¥è¯†çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚åˆ†å­ç»“æ„å’Œä¸ä¸€è‡´æ³¨é‡Šæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>GTR-Mol-VLMæ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿäººç±»æ¨ç†å’Œè§£å†³å›¾åƒä¸æ³¨é‡Šä¸åŒ¹é…é—®é¢˜æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>GTR-CoT-1.3Mæ•°æ®é›†ç”¨äºæ”¯æŒæ¨¡å‹å¼€å‘ï¼Œå¹¶æä¾›ç²¾ç»†çš„OCSRå›¾è§£æè¯„ä¼°ã€‚</li>
<li>GTR-Mol-VLMåœ¨å¤„ç†å’Œè§£æå¸¦æœ‰åŠŸèƒ½ç»„ç¼©ç•¥è¯­çš„åˆ†å­å›¾åƒæ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>GTR-Mol-VLMåœ¨SMILESå’Œå›¾å½¢æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f6254963c6362b9410cecc7d470ae6b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e66ff9a7989f7206f8faf4a9bcbc1b83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2356decec6238f37994b975776cad5e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04f3054bcb2af3a39565c202059e7abb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c47d29fef216a4706ab4f75e44828a4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DefenderBench-A-Toolkit-for-Evaluating-Language-Agents-in-Cybersecurity-Environments"><a href="#DefenderBench-A-Toolkit-for-Evaluating-Language-Agents-in-Cybersecurity-Environments" class="headerlink" title="DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments"></a>DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments</h2><p><strong>Authors:Chiyu Zhang, Marc-Alexandre Cote, Michael Albada, Anush Sankaran, Jack W. Stokes, Tong Wang, Amir Abdi, William Blum, Muhammad Abdul-Mageed</strong></p>
<p>Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBenchâ€™s modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench">https://github.com/microsoft/DefenderBench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨ç†è§£å’Œæ¨ç†äººç±»è¯­è¨€æ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨ç½‘ç»œå®‰å…¨æ–¹é¢çš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æ¨å‡ºäº†DefenderBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç”¨çš„å¼€æºå·¥å…·åŒ…ï¼Œæ—¨åœ¨è¯„ä¼°ä»£ç†åœ¨æ”»å‡»ã€é˜²å¾¡å’ŒåŸºäºç½‘ç»œå®‰å…¨çŸ¥è¯†çš„ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚DefenderBenchåŒ…æ‹¬ç½‘ç»œå…¥ä¾µç¯å¢ƒã€æ¶æ„å†…å®¹æ£€æµ‹ç¯å¢ƒã€ä»£ç æ¼æ´åˆ†æç¯å¢ƒå’Œç½‘ç»œå®‰å…¨çŸ¥è¯†è¯„ä¼°ç¯å¢ƒã€‚å®ƒä¸“é—¨è®¾è®¡ç”¨äºç ”ç©¶äººå‘˜ï¼Œç»æµå®æƒ ä¸”æ˜“äºè®¿é—®ï¼ŒåŒæ—¶æä¾›å…¬å¹³ä¸¥æ ¼çš„è¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†åŒ–çš„ä»£ç†æ¡†æ¶å¯¹å¤šä¸ªæœ€æ–°å’Œæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬å¼€æºå’Œé—­æºæ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼ŒClaude-3.7-sonnetè¡¨ç°æœ€ä½³ï¼Œåœ¨DefenderBenchä¸Šçš„å¾—åˆ†ä¸º81.65ï¼Œå…¶æ¬¡æ˜¯Claude-3.7-sonnet-thinkï¼Œå¾—åˆ†ä¸º78.40ï¼Œè€Œè¡¨ç°æœ€å¥½çš„å¼€æºæ¨¡å‹Llama 3.3 70Bç´§éšå…¶åï¼Œå¾—åˆ†ä¸º71.81ã€‚DefenderBenchçš„æ¨¡å—åŒ–è®¾è®¡å…è®¸æ— ç¼é›†æˆè‡ªå®šä¹‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä»»åŠ¡ï¼Œä¿ƒè¿›äº†å¯é‡å¤æ€§å’Œå…¬å¹³æ¯”è¾ƒã€‚DefenderBenchçš„åŒ¿åç‰ˆæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/microsoft/DefenderBenchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00739v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£äººç±»è¯­è¨€å’Œæ¨ç†æ–¹é¢å±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ï¼Œä½†åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä»‹ç»äº†ä¸€æ¬¾åä¸ºDefenderBenchçš„å®ç”¨å¼€æºå·¥å…·åŒ…ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨æ”»å‡»ã€é˜²å¾¡å’ŒåŸºäºç½‘ç»œå®‰å…¨çŸ¥è¯†ä»»åŠ¡æ–¹é¢çš„è¡¨ç°ã€‚è¯¥å·¥å…·åŒ…åŒ…æ‹¬ç½‘ç»œå…¥ä¾µã€æ¶æ„å†…å®¹æ£€æµ‹ã€ä»£ç æ¼æ´åˆ†æå’Œç½‘ç»œå®‰å…¨çŸ¥è¯†è¯„ä¼°ç­‰ç¯å¢ƒã€‚å®ƒæ—¨åœ¨è®©ç ”ç©¶äººå‘˜ä»¥è´Ÿæ‹…å¾—èµ·ä¸”æ˜“äºè®¿é—®çš„æ–¹å¼ï¼Œè¿›è¡Œå…¬å¹³å’Œä¸¥æ ¼çš„è¯­è¨€æ¨¡å‹è¯„ä¼°ã€‚å¯¹ç°æœ‰é¡¶å°–çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¼€æ”¾å¼å’Œå°é—­å¼æ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼ŒClaude-3.7-sonnetè¡¨ç°æœ€ä½³ï¼Œå¾—åˆ†ä¸º81.65ï¼Œå…¶æ¬¡æ˜¯Claude-3.7-sonnet-thinkï¼Œå¾—åˆ†ä¸º78.40ï¼Œè€Œé¢†å…ˆçš„å¼€æ”¾å¼æ¨¡å‹Llama 3.3 70Bç´§éšå…¶åï¼Œå¾—åˆ†ä¸º71.81ã€‚DefenderBenchçš„æ¨¡å—åŒ–è®¾è®¡å…è®¸æ— ç¼é›†æˆè‡ªå®šä¹‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä»»åŠ¡ï¼Œä¿ƒè¿›ç ”ç©¶çš„å¯é‡å¤æ€§å’Œå…¬å¹³æ¯”è¾ƒã€‚åŒ¿åçš„DefenderBenchç‰ˆæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/microsoft/DefenderBenchè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>DefenderBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„å®ç”¨å¼€æºå·¥å…·åŒ…ã€‚</li>
<li>DefenderBenchåŒ…æ‹¬ç½‘ç»œå…¥ä¾µã€æ¶æ„å†…å®¹æ£€æµ‹ã€ä»£ç æ¼æ´åˆ†æå’Œç½‘ç»œå®‰å…¨çŸ¥è¯†è¯„ä¼°ç­‰ç¯å¢ƒã€‚</li>
<li>DefenderBenchæ—¨åœ¨è®©ç ”ç©¶äººå‘˜ä»¥è´Ÿæ‹…å¾—èµ·ä¸”æ˜“äºè®¿é—®çš„æ–¹å¼è¿›è¡Œå…¬å¹³å’Œä¸¥æ ¼çš„è¯­è¨€æ¨¡å‹è¯„ä¼°ã€‚</li>
<li>åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒClaude-3.7-sonnetåœ¨DefenderBenchæµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>DefenderBenchçš„æ¨¡å—åŒ–è®¾è®¡å…è®¸æ— ç¼é›†æˆè‡ªå®šä¹‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a6bc86fc70229af47be50d4b82aba1e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bab7f4d11665b5e3feed5f17ce028c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ce477798b819c94cd5e4fe2f624ddee.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling"><a href="#Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling" class="headerlink" title="Curse of High Dimensionality Issue in Transformer for Long-context   Modeling"></a>Curse of High Dimensionality Issue in Transformer for Long-context   Modeling</h2><p><strong>Authors:Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan</strong></p>
<p>Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at <a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention">https://github.com/bolixinyu/DynamicGroupAttention</a>. </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”±äºå†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡é¢ä¸´ç€å·¨å¤§çš„è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼šè™½ç„¶æ³¨æ„åŠ›æƒé‡é€šå¸¸æ˜¯ç¨€ç–çš„ï¼Œä½†æ‰€æœ‰æ ‡è®°éƒ½æ¶ˆè€—ç€å¹³ç­‰çš„è®¡ç®—èµ„æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä¼ ç»Ÿçš„æ¦‚ç‡åºåˆ—å»ºæ¨¡é‡æ–°è¡¨è¿°ä¸ºâ€œç›‘ç£å­¦ä¹ ä»»åŠ¡â€ï¼Œè¿™èƒ½å¤ŸåŒºåˆ†ç›¸å…³å’Œæ— å…³çš„æ ‡è®°ï¼Œå¹¶æ›´æ¸…æ¥šåœ°äº†è§£å†—ä½™æƒ…å†µã€‚åŸºäºè¿™ç§é‡æ–°è¡¨è¿°ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šåˆ†æäº†æ³¨æ„åŠ›ç¨€ç–æ€§ï¼Œå‘ç°åªæœ‰å°‘æ•°æ ‡è®°å¯¹é¢„æµ‹æœ‰é‡å¤§è´¡çŒ®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›ä¼˜åŒ–è¡¨è¿°ä¸ºçº¿æ€§ç¼–ç é—®é¢˜ï¼Œå¹¶æå‡ºâ€œåˆ†ç»„ç¼–ç ç­–ç•¥â€ï¼Œç†è®ºä¸Šæ˜¾ç¤ºå‡ºå…¶æé«˜å¯¹æŠ—éšæœºå™ªå£°çš„ç¨³å¥æ€§å’Œæé«˜å­¦ä¹ æ•ˆç‡çš„èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†â€œåŠ¨æ€åˆ†ç»„æ³¨æ„åŠ›â€ï¼ˆDGAï¼‰ï¼Œå®ƒåˆ©ç”¨åˆ†ç»„ç¼–ç æ¥é€šè¿‡èšåˆä¸å¤ªé‡è¦çš„æ ‡è®°æ¥æ˜ç¡®å‡å°‘å†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DGAåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¿æŒäº†ç«äº‰åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bolixinyu/DynamicGroupAttentionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22107v3">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚é’ˆå¯¹é•¿æ–‡æœ¬å»ºæ¨¡ä¸­ç”±äºå†—ä½™æ³¨æ„åŠ›è®¡ç®—å¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”åŠ¨æ€ç»„æ³¨æ„åŠ›ï¼ˆDGAï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†ç¦»é‡è¦å’Œä¸é‡è¦çš„ä»¤ç‰Œï¼Œç†è®ºä¸Šåˆ†æäº†æ³¨æ„åŠ›ç¨€ç–æ€§ï¼Œå¹¶é€šè¿‡çº¿æ€§ç¼–ç é—®é¢˜æå‡ºäº†åˆ†ç»„ç¼–ç ç­–ç•¥ï¼Œä»è€Œå‡å°‘å†—ä½™å¹¶æé«˜è®¡ç®—æ•ˆç‡ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒDGAåœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-based LLMs æ“…é•¿æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚</li>
<li>ä¼ ç»Ÿæ¦‚ç‡åºåˆ—å»ºæ¨¡è¢«é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªç›‘ç£å­¦ä¹ ä»»åŠ¡ã€‚</li>
<li>æ³¨æ„åŠ›ç¨€ç–æ€§è¡¨æ˜åªæœ‰å°‘æ•°ä»¤ç‰Œå¯¹é¢„æµ‹æœ‰é‡è¦è´¡çŒ®ã€‚</li>
<li>æå‡ºäº†ç†è®ºä¸Šçš„æ³¨æ„åŠ›ä¼˜åŒ–ä½œä¸ºçº¿æ€§ç¼–ç é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åˆ†ç»„ç¼–ç ç­–ç•¥å’ŒåŠ¨æ€ç»„æ³¨æ„åŠ›ï¼ˆDGAï¼‰æ–¹æ³•ã€‚</li>
<li>DGA é€šè¿‡å‡å°‘ä¸é‡è¦ä»¤ç‰Œçš„æ³¨æ„åŠ›è®¡ç®—æ¥æ˜ç¡®å‡å°‘å†—ä½™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fbd85c7abb481e216fc601671865643d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eca6ec960b889a2e81e3ec88cc965ea2.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-12/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-47047f50c44cb37898ed12da40830360.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-12/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-a1b4e28a76543540f761275a06b6abda.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-12  VIKI-R Coordinating Embodied Multi-Agent Cooperation via Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
