<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Crossing Borders A Multimodal Challenge for Indian Poetry Translation and Image Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8debf26c5852befb184a55e778aa7a61')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    92 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="Crossing-Borders-A-Multimodal-Challenge-for-Indian-Poetry-Translation-and-Image-Generation"><a href="#Crossing-Borders-A-Multimodal-Challenge-for-Indian-Poetry-Translation-and-Image-Generation" class="headerlink" title="Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation"></a>Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation</h2><p><strong>Authors:Sofia Jamil, Kotla Sai Charan, Sriparna Saha, Koustava Goswami, Joseph K J</strong></p>
<p>Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the readerâ€™s experience.</p>
<blockquote>
<p>å°åº¦è¯—æ­Œä»¥å…¶è¯­è¨€å¤æ‚æ€§å’Œæ·±åšçš„æ–‡åŒ–å…±é¸£è€Œé—»åï¼Œæ‹¥æœ‰è·¨è¶Šæ•°åƒå¹´çš„ä¸°å¯Œå¤šæ ·çš„ä¼ ç»Ÿã€‚ç„¶è€Œï¼Œå…¶å±‚æ¬¡ä¸°å¯Œçš„å«ä¹‰ã€æ–‡åŒ–å…¸æ•…å’Œå¤æ‚çš„è¯­æ³•ç»“æ„å¸¸å¸¸æ„æˆç†è§£ä¸Šçš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹äºéæ¯è¯­è€…æˆ–å¯¹å…¶è¯­å¢ƒå’Œè¯­è¨€ä¸ç†Ÿæ‚‰çš„è¯»è€…ã€‚å°½ç®¡å…¶åœ¨æ–‡åŒ–ä¸Šå…·æœ‰é‡è¦æ€§ï¼Œä½†ç°æœ‰å…³äºè¯—æ­Œçš„ä½œå“å¤§å¤šå¿½è§†äº†å°åº¦è¯­è¨€çš„è¯—æ­Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¿»è¯‘ä¸å›¾åƒç”Ÿæˆï¼ˆTAIï¼‰æ¡†æ¶ï¼Œå®ƒå€ŸåŠ©é€‚å½“æç¤ºè°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„åŠ›é‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒè”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡ä¸­çš„ä¼˜è´¨æ•™è‚²ï¼ˆSDG 4ï¼‰å’Œå‡å°‘ä¸å¹³ç­‰ï¼ˆSDG 10ï¼‰ï¼Œé€šè¿‡æé«˜å¯Œæœ‰æ–‡åŒ–çš„å°åº¦è¯­è¨€è¯—æ­Œçš„æ˜“è®¿é—®æ€§ï¼Œé¢å‘å…¨çƒå—ä¼—ã€‚å®ƒåŒ…æ‹¬ï¼ˆ1ï¼‰ä¸€ä¸ªç¿»è¯‘æ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨å‡ ç‡æ¯”ç‡åå¥½å¯¹é½ç®—æ³•ï¼Œå°†å½¢æ€ä¸°å¯Œçš„è¯—æ­Œå‡†ç¡®ç¿»è¯‘æˆè‹±è¯­ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªå›¾åƒç”Ÿæˆæ¨¡å—ï¼Œå®ƒåˆ©ç”¨è¯­ä¹‰å›¾æ¥æ•è·æ ‡è®°ã€ä¾å­˜å…³ç³»å’Œéšå–»åŠå…¶æ„ä¹‰ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä»¥åˆ›å»ºå°åº¦è¯—æ­Œçš„è§†è§‰æœ‰æ„ä¹‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å…¨é¢å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬äººç±»å’Œå®šé‡è¯„ä¼°ï¼Œè¯æ˜äº†TAIæ‰©æ•£åœ¨è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³å°åº¦è¯­è¨€è¯—æ­Œèµ„æºç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å½¢æ€ä¸°å¯Œå°åº¦è¯­è¨€è¯—æ­ŒMorphoVerseæ•°æ®é›†ï¼ŒåŒ…å«1570é¦–è·¨è¶Š21ç§ä½èµ„æºå°åº¦è¯­è¨€çš„è¯—æ­Œã€‚é€šè¿‡è§£å†³è¯—æ­Œç¿»è¯‘å’Œè§†è§‰ç†è§£ä¹‹é—´çš„å·®è·ï¼Œè¿™é¡¹å·¥ä½œæ—¨åœ¨æé«˜å¯åŠæ€§å¹¶ä¸°å¯Œè¯»è€…çš„ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13689v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºç¿»è¯‘ä¸å›¾åƒç”Ÿæˆæ¡†æ¶ï¼ˆTAIï¼‰ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡é€‚å½“çš„æç¤ºè°ƒæ•´ï¼Œæ”¯æŒå°åº¦è¯—æ­Œçš„ç¿»è¯‘å’Œå›¾åƒç”Ÿæˆã€‚æ¡†æ¶åŒ…æ‹¬ç¿»è¯‘æ¨¡å—å’Œå›¾åƒç”Ÿæˆæ¨¡å—ï¼Œåˆ†åˆ«é‡‡ç”¨æ¯”ç‡åå¥½å¯¹é½ç®—æ³•å’Œè¯­ä¹‰å›¾æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜å°åº¦è¯—æ­Œçš„å…¨çƒæ€§å¯è®¿é—®æ€§å’Œè¯»è€…ä½“éªŒã€‚åŒæ—¶ï¼Œå¼•å…¥å°åº¦è¯­è¨€è¯—æ­Œæ•°æ®é›†MorphoVerseï¼ŒåŒ…å«å¤šç§ä½èµ„æºå°åº¦è¯­è¨€çš„è¯—æ­Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°åº¦è¯—æ­Œå…·æœ‰è¯­è¨€å¤æ‚ã€æ–‡åŒ–å…±é¸£æ·±åšçš„ç‰¹ç‚¹ï¼Œä½†å…¶ä¸°å¯Œçš„æ–‡åŒ–å†…æ¶µå’Œå¤æ‚çš„è¯­æ³•ç»“æ„å¯¹éæ¯è¯­è€…æˆ–ä¸äº†è§£å…¶æ–‡åŒ–å’Œè¯­è¨€èƒŒæ™¯çš„è¯»è€…æ¥è¯´ï¼Œç†è§£èµ·æ¥å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰å¯¹è¯—æ­Œçš„ç ”ç©¶å¤§å¤šå¿½è§†äº†å°åº¦è¯­è¨€çš„è¯—æ­Œã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„Translation and Image Generation (TAI)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹æé«˜å°åº¦è¯—æ­Œçš„å¯è®¿é—®æ€§ï¼Œä½¿å…¶å¯¹å…¨çƒè§‚ä¼—æ›´å…·å¸å¼•åŠ›ã€‚</li>
<li>TAIæ¡†æ¶åŒ…æ‹¬ç¿»è¯‘æ¨¡å—å’Œå›¾åƒç”Ÿæˆæ¨¡å—ã€‚ç¿»è¯‘æ¨¡å—é‡‡ç”¨æ¯”ç‡åå¥½å¯¹é½ç®—æ³•å‡†ç¡®ç¿»è¯‘å½¢æ€ä¸°å¯Œçš„è¯—æ­Œï¼›å›¾åƒç”Ÿæˆæ¨¡å—åˆ©ç”¨è¯­ä¹‰å›¾æŠ€æœ¯ï¼Œåˆ›å»ºå°åº¦è¯—æ­Œçš„è§†è§‰æ„ä¹‰è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡å¯¹è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡çš„å…¨é¢å®éªŒè¯„ä¼°ï¼Œè¯æ˜äº†TAIæ¡†æ¶çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ä¸ºè§£å†³å°åº¦è¯­è¨€è¯—æ­Œèµ„æºç¨€ç¼ºçš„é—®é¢˜ï¼Œå¼•å…¥äº†MorphoVerseæ•°æ®é›†ï¼ŒåŒ…å«1,570é¦–è·¨è¶Š21ç§ä½èµ„æºå°åº¦è¯­è¨€çš„è¯—æ­Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe8f56b640106e849238894ae71f84f3" align="middle">
<img src="https://picx.zhimg.com/v2-fcf6490258e970f26ba6a174333559a7" align="middle">
<img src="https://picx.zhimg.com/v2-b7b13598440fc3707843a91a6a8c965d" align="middle">
<img src="https://picx.zhimg.com/v2-2f02d6dcf4b57a639f1aa397320af375" align="middle">
<img src="https://picx.zhimg.com/v2-9d5028d10aa948a9debf3fbabe5f23d7" align="middle">
<img src="https://picx.zhimg.com/v2-2569534174f2934f2d2a780b02a6d8ef" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CacheFlow-Compressive-Streaming-Memory-for-Efficient-Long-Form-Video-Understanding"><a href="#CacheFlow-Compressive-Streaming-Memory-for-Efficient-Long-Form-Video-Understanding" class="headerlink" title="CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding"></a>CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding</h2><p><strong>Authors:Shrenik Patel, Daivik Patel</strong></p>
<p>Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each oneâ€™s keys are summarized by a tiny recurrent encoder to form a retrieval index, while the blockâ€™s full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.</p>
<blockquote>
<p>é•¿è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰å¯¹å½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ„æˆäº†æŒ‘æˆ˜ï¼Œå› ä¸ºéšç€è¿è¡Œæ—¶çš„å¢é•¿ï¼Œæ³¨æ„åŠ›æœºåˆ¶å’Œé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ä¼šå—åˆ°å½±å“ï¼Œè¿™è¿«ä½¿æ¨¡å‹é€‰æ‹©æ˜‚è´µçš„æ¨ç†è¿‡ç¨‹æˆ–ä½¿ç”¨ç›®å…‰çŸ­æµ…çš„æ»‘åŠ¨çª—å£ã€‚æˆ‘ä»¬å¼•å…¥äº†CacheFlowï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ç®¡é“ï¼Œå®ƒç»“åˆäº†åŠ¨æ€ä»¤ç‰Œä¸¢å¼ƒï¼ˆDTDï¼‰å’Œå‹ç¼©é•¿æœŸå†…å­˜ã€‚DTDé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦æ¥åœ¨çº¿ä¸¢å¼ƒä¸å‰å¸§ç›¸ä¼¼çš„æ¯å—ä»¤ç‰Œï¼Œå¹¶å°†å‰©ä½™çš„ä»¤ç‰Œæ‰“åŒ…æˆå›ºå®šå¤§å°çš„å—ã€‚è¿™ç§åœ¨çº¿çš„é€å¸§å¤„ç†ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•éå¸¸é€‚åˆäºå®æ—¶æµåª’ä½“VQAã€‚åœ¨å¤„ç†å—æ—¶ï¼Œæ¯ä¸ªå—çš„é”®ä¼šè¢«ä¸€ä¸ªå¾®å°çš„å¾ªç¯ç¼–ç å™¨æ‰€æ€»ç»“ï¼Œå½¢æˆä¸€ä¸ªæ£€ç´¢ç´¢å¼•ï¼Œè€Œè¯¥å—çš„å®Œæ•´KVå¯¹ä¼šè¢«å¸è½½å¹¶åœ¨ç¨åè¿›è¡Œé‡æ–°æ°´åŒ–ä»¥ç”¨äºç”Ÿæˆç­”æ¡ˆï¼Œä¿æŒç­”æ¡ˆçš„ä¿çœŸåº¦ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒåŸºäºå…±è¯†çš„æ£€ç´¢æœºåˆ¶åªæ£€ç´¢å‰Kä¸ªæœ€ç›¸å…³çš„å—ï¼Œå¹¶åœ¨æ£€ç´¢åˆ°çš„å†…å®¹å’Œæœ¬åœ°ä¸Šä¸‹æ–‡ä¸Šè¿›è¡Œå…³æ³¨ï¼Œä»¥å®ç°ç²¾ç¡®çš„é•¿ç¨‹æ¨ç†ã€‚CacheFlowå³æ’å³ç”¨ï¼Œä¸å—æ¶æ„é™åˆ¶ï¼Œæ— éœ€å¾®è°ƒã€‚åœ¨ç¦»çº¿è§†é¢‘é—®ç­”å’Œæµåª’ä½“è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCacheFlowä¼˜äºå½“å‰çš„å¼ºå¤§åŸºçº¿ï¼ŒåŒæ—¶å¤„ç†çš„ä»¤ç‰Œå‡å°‘äº†é«˜è¾¾87%ã€‚æˆ‘ä»¬çš„åŒé‡æ–¹æ³•ä½¿VLMèƒ½å¤ŸåŒæ—¶å®ç°é«˜æ•ˆå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œä¸ºå®é™…çš„é•¿è§†é¢‘ç†è§£é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13644v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CacheFlowï¼Œä¸€ç§é’ˆå¯¹é•¿å½¢å¼è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰çš„è®­ç»ƒå…è´¹ç®¡é“ï¼Œé€šè¿‡åŠ¨æ€ä»¤ç‰Œä¸¢å¼ƒï¼ˆDTDï¼‰å’Œå‹ç¼©é•¿æœŸè®°å¿†æ¥åº”å¯¹å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚DTDåœ¨çº¿å¯¹æ¯å¸§çš„ä»¤ç‰Œè¿›è¡Œä¿®å‰ªï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦æ¥å†³å®šå“ªäº›ä»¤ç‰Œä¿ç•™ä¸‹æ¥å¹¶å½¢æˆå›ºå®šå¤§å°çš„å—ã€‚è¿™ç§åœ¨çº¿ã€æŒ‰å¸§å¤„ç†çš„æ–¹å¼ä½¿CacheFlowéå¸¸é€‚åˆå®æ—¶æµåª’ä½“VQAã€‚éšç€å—çš„å¤„ç†ï¼Œå…¶å…³é”®ä¼šè¢«ç®€æ´çš„å¾ªç¯ç¼–ç å™¨æ€»ç»“æˆæ£€ç´¢ç´¢å¼•ï¼Œè€Œå®Œæ•´çš„KVå¯¹åˆ™è¢«å¸è½½å¹¶åœ¨ç¨åè¿›è¡Œé‡æ–°å¡«å……ä»¥ç”Ÿæˆç­”æ¡ˆã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒåŸºäºå…±è¯†çš„æ£€ç´¢æœºåˆ¶åªæ£€ç´¢æœ€ç›¸å…³çš„å‰Kä¸ªå—ï¼ŒåŒæ—¶å…³æ³¨æ£€ç´¢å’Œæœ¬åœ°ä¸Šä¸‹æ–‡ä»¥è¿›è¡Œç²¾ç¡®çš„é•¿ç¨‹æ¨ç†ã€‚CacheFlowæ˜¯å³æ’å³ç”¨ã€æ¶æ„æ— å…³ä¸”æ— éœ€å¾®è°ƒçš„æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç¦»çº¿åŠæµåª’ä½“VQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCacheFlowè¡¨ç°ä¼˜äºå½“å‰å¼ºå¤§çš„åŸºçº¿ï¼ŒåŒæ—¶å¤„ç†ä»¤ç‰Œçš„æ•ˆç‡æé«˜äº†é«˜è¾¾87%ã€‚è¿™ä¸€åŒé‡æ–¹æ³•ä½¿VLMsæ—¢é«˜æ•ˆåˆå…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºå®é™…çš„é•¿å½¢å¼è§†é¢‘ç†è§£é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CacheFlowè§£å†³äº†é•¿å½¢å¼è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰ä¸­è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡åŠ¨æ€ä»¤ç‰Œä¸¢å¼ƒï¼ˆDTDï¼‰å’Œå‹ç¼©é•¿æœŸè®°å¿†æ¥ä¼˜åŒ–å¤„ç†è¿‡ç¨‹ã€‚</li>
<li>DTDåœ¨çº¿æŒ‰å¸§å¤„ç†ä»¤ç‰Œï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œä¿®å‰ªã€‚</li>
<li>CacheFlowé€‚ç”¨äºå®æ—¶æµåª’ä½“VQAï¼Œå…·å¤‡é«˜æ•ˆçš„åœ¨çº¿å¤„ç†èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨ç®€æ´çš„å¾ªç¯ç¼–ç å™¨æ€»ç»“å—çš„å…³é”®ä¿¡æ¯å¹¶å½¢æˆæ£€ç´¢ç´¢å¼•ã€‚</li>
<li>CacheFlowæé«˜äº†å¤„ç†ä»¤ç‰Œçš„æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†ç­”æ¡ˆçš„ç²¾ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5572afcb0a61d906445a075cfea77add" align="middle">
<img src="https://picx.zhimg.com/v2-b419b39489c9d2b055bcbf98693a43d7" align="middle">
<img src="https://picx.zhimg.com/v2-d44affe2f4d7912619e1d94afd005f01" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning"><a href="#P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning" class="headerlink" title="P1: Mastering Physics Olympiads with Reinforcement Learning"></a>P1: Mastering Physics Olympiads with Reinforcement Learning</h2><p><strong>Authors:Jiacheng Chen, Qianjia Cheng, Fangchen Yu, Haiyuan Wan, Yuchen Zhang, Shenghe Zheng, Junchi Yao, Qingyang Zhang, Haonan He, Yun Luo, Yufeng Zhao, Futing Wang, Li Sheng, Chengxing Xie, Yuxin Zuo, Yizhuo Li, Wenxauan Zeng, Yulun Wu, Rui Huang, Dongzhan Zhou, Kai Chen, Yu Qiao, Lei Bai, Yu Cheng, Ning Ding, Bowen Zhou, Peng Ye, Ganqu Cui</strong></p>
<p>Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international&#x2F;regional physics competitions in 2024&#x2F;2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.</p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ¨åŠ¨äº†å‰æ²¿é¢†åŸŸä»è§£è°œå‘ç§‘å­¦çº§æ¨ç†çš„è½¬å˜ã€‚è¿™ç§æ¨ç†æ–¹å¼æ˜¯ä¸ºäº†è§£å†³é‚£äº›ç­”æ¡ˆå¿…é¡»ç»å¾—èµ·è‡ªç„¶è€ƒéªŒï¼Œè€Œä¸ä»…ä»…æ˜¯ç¬¦åˆæŸäº›è§„å®šçš„é—®é¢˜ã€‚ç‰©ç†å­¦æ˜¯è¿™ä¸€è½¬å˜çš„æœ€ä¸¥æ ¼è€ƒéªŒï¼Œå®ƒä»¥æ ¹æœ¬çš„æ–¹å¼å°†ç¬¦å·ä¸ç°å®ç›¸ç»“åˆï¼Œæˆä¸ºå¤§å¤šæ•°ç°ä»£æŠ€æœ¯çš„åŸºçŸ³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æˆåŠŸå¼€å‘äº†å…·æœ‰å‡ºè‰²ç‰©ç†æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°¤å…¶æ“…é•¿è§£å†³å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ç‰©ç†é—®é¢˜ï¼Œä»è€Œæ¨åŠ¨äº†ç‰©ç†å­¦ç ”ç©¶ã€‚æˆ‘ä»¬ä»‹ç»äº†P1ç³»åˆ—ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å®Œå…¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè€Œæˆçš„å¼€æºç‰©ç†æ¨ç†æ¨¡å‹ã€‚å…¶ä¸­ï¼ŒP1-235B-A22Bæ˜¯æœ€æ–°å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›ï¼ˆIPhO 2025ï¼‰ä¸­è·å¾—é‡‘ç‰Œçš„é¦–ä¸ªå¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨2024&#x2F;2025å¹´çš„13åœºå›½é™…&#x2F;åŒºåŸŸç‰©ç†ç«èµ›ä¸­èµ¢å¾—äº†12æšé‡‘ç‰Œã€‚P1-30B-A3Båœ¨IPhO 2025ä¸Šä¹Ÿå‡ ä¹è¶…è¶Šäº†å…¶ä»–æ‰€æœ‰å¼€æºæ¨¡å‹ï¼Œè·å¾—äº†é“¶ç‰Œã€‚è¿›ä¸€æ­¥é…å¤‡ä»£ç†æ¡†æ¶PhysicsMinionsçš„P1-235B-A22B+PhysicsMinionsåœ¨IPhO 2025ä¸Šè·å¾—æ•´ä½“ç¬¬ä¸€åï¼Œå¹¶åœ¨13åœºç‰©ç†ç«èµ›ä¸­è·å¾—æœ€é«˜å¹³å‡åˆ†ã€‚é™¤äº†ç‰©ç†å­¦ï¼ŒP1æ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç ç­‰å…¶ä»–æ¨ç†ä»»åŠ¡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†P1ç³»åˆ—çš„å·¨å¤§é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13612v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å·²ä»è§£è°œè½¬å‘ç§‘å­¦çº§çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯è§£å†³éœ€è¦æ ¹æ®è‡ªç„¶æ¥éªŒè¯ç­”æ¡ˆçš„é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç³»åˆ—é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒå‡ºæ¥çš„å…·å¤‡å“è¶Šç‰©ç†æ¨ç†èƒ½åŠ›çš„å¼€æ”¾æºä»£ç ç‰©ç†æ¨¡å‹P1å®¶æ—ï¼Œèƒ½å¤Ÿåœ¨å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›ä¸­è·å¾—ä¼˜å¼‚æˆç»©ï¼Œè¯æ˜äº†å…¶è§£å†³ç‰©ç†å­¦éš¾é¢˜çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹åœ¨å…¶ä»–é¢†åŸŸå¦‚æ•°å­¦å’Œç¼–ç¨‹ä¹Ÿå±•ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ã€‚è¿™ä½“ç°äº†å…¶åœ¨å„é¢†åŸŸçš„æ³›ç”¨æ€§å’Œä¼˜ç§€æ€§èƒ½ã€‚è¿™ä¸€ç ”ç©¶æ¨åŠ¨äº†å¯¹è‡ªç„¶è¯­è¨€ç†è§£ä¸æ¨ç†çš„ç ”ç©¶å‘å±•ï¼Œç‰¹åˆ«æ˜¯è·¨å­¦ç§‘çš„æ•´åˆç ”ç©¶ï¼Œå¹¶ä¸ºåç»­ç›¸å…³æ¨¡å‹çš„è®­ç»ƒæä¾›äº†æ–°çš„è§†è§’å’Œå¯èƒ½ã€‚æ­¤å¤–ï¼Œç‰©ç†æ¨¡å‹çš„æ¨å¹¿ä¸åº”ç”¨æœ‰æœ›ä¿ƒè¿›ç§‘æŠ€é¢†åŸŸçš„è¿›æ­¥ä¸å‘å±•ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ä»è§£è°œè½¬å‘ç§‘å­¦çº§çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³éœ€è¦æ ¹æ®è‡ªç„¶éªŒè¯ç­”æ¡ˆçš„é—®é¢˜ä¸Šæœ‰æ‰€çªç ´ã€‚</li>
<li>P1ç³»åˆ—æ¨¡å‹æ˜¯å…·å¤‡å“è¶Šç‰©ç†æ¨ç†èƒ½åŠ›çš„å¼€æ”¾æºä»£ç ç‰©ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›ä¸­è·å¾—ä¼˜å¼‚æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c48f48424afaa95443441b75a8deab2" align="middle">
<img src="https://picx.zhimg.com/v2-8563479d6d46005336ee0fda1c8c46a2" align="middle">
<img src="https://picx.zhimg.com/v2-2730ddf1809072c92db7ab95c4467940" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adaptive-Multi-Scale-Integration-Unlocks-Robust-Cell-Annotation-in-Histopathology-Images"><a href="#Adaptive-Multi-Scale-Integration-Unlocks-Robust-Cell-Annotation-in-Histopathology-Images" class="headerlink" title="Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images"></a>Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images</h2><p><strong>Authors:Yinuo Xu, Yan Cui, Mingyao Li, Zhi Huang</strong></p>
<p>Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cellâ€™s function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.   To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.   To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.</p>
<blockquote>
<p>ä»å¸¸è§„ç—…ç†å›¾åƒä¸­è¯†åˆ«ç»†èƒç±»å‹å’Œäºšå‹å¯¹äºæé«˜äººç±»ç–¾ç—…çš„è®¡ç®—ç†è§£è‡³å…³é‡è¦ã€‚ç°æœ‰çš„åŸºäºç“¦ç‰‡çš„æ¨¡å‹å¯ä»¥æ•æ‰è¯¦ç»†çš„æ ¸å½¢æ€ï¼Œä½†å¾€å¾€æ— æ³•èå…¥æ›´å¹¿æ³›çš„ç»„ç»‡èƒŒæ™¯ï¼Œè¿™ä¼šå½±å“ç»†èƒçš„åŠŸèƒ½å’Œèº«ä»½ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„äººç±»æ³¨é‡Šé€šå¸¸ç²’åº¦è¾ƒç²—ï¼Œä¸”åœ¨ä¸åŒç ”ç©¶ä¸­çš„åˆ†å¸ƒä¸å‡ï¼Œå¯¼è‡´éš¾ä»¥è·å¾—ç²¾ç»†çš„äºšå‹çº§åˆ«çš„ç›‘ç£ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†NuClassï¼Œè¿™æ˜¯ä¸€ä¸ªå—ç—…ç†å­¦å®¶å·¥ä½œæµç¨‹å¯å‘çš„æ¡†æ¶ï¼Œç”¨äºç»†èƒçº§çš„æ ¸å½¢æ€å’Œå¾®ç¯å¢ƒèƒŒæ™¯çš„å¤šå…ƒèåˆã€‚NuClassä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç»„ä»¶ï¼šPath localï¼Œä¸“æ³¨äº224åƒç´ ä¹˜224åƒç´ çš„è£å‰ªåŒºåŸŸçš„æ ¸å½¢æ€; Path globalï¼Œåˆ™å»ºæ¨¡å‘¨å›´1024åƒç´ ä¹˜1024åƒç´ çš„é‚»åŸŸã€‚ä¸€ä¸ªå¯å­¦ä¹ çš„é—¨æ§æ¨¡å—å¯ä»¥è‡ªé€‚åº”åœ°å¹³è¡¡å±€éƒ¨ç»†èŠ‚å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚ä¸ºäº†é¼“åŠ±äº’è¡¥å­¦ä¹ ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸ç¡®å®šæ€§å¯¼å‘çš„ç›®æ ‡ï¼Œå¼•å¯¼å…¨å±€è·¯å¾„ä¼˜å…ˆå¤„ç†å±€éƒ¨è·¯å¾„ä¸ç¡®å®šçš„åŒºåŸŸã€‚æˆ‘ä»¬è¿˜æä¾›äº†æ ¡å‡†çš„ä¿¡å¿ƒä¼°è®¡å’ŒGrad-CAMå¯è§†åŒ–ï¼Œä»¥å¢å¼ºå¯è§£é‡Šæ€§ã€‚ä¸ºäº†å…‹æœé«˜è´¨é‡æ³¨é‡Šçš„ç¼ºä¹ï¼Œæˆ‘ä»¬ä»Xeniumç©ºé—´è½¬å½•ç»„æµ‹å®šä¸­æ„å»ºäº†ä¸€ä¸ªæ ‡è®°å¼•å¯¼çš„æ•°æ®é›†ï¼Œä¸ºè·¨è¶Šå…«ä¸ªå™¨å®˜çš„è¶…è¿‡ä¸¤ç™¾ä¸‡ä¸ªç»†èƒæä¾›äº†16ç±»çš„å•ç»†èƒåˆ†è¾¨ç‡æ ‡ç­¾ã€‚åœ¨ä¸‰ä¸ªå®Œå…¨ç‹¬ç«‹çš„ç ”ç©¶é˜Ÿåˆ—ä¸­è¿›è¡Œè¯„ä¼°ï¼ŒNuClassçš„æœ€ä½³è¡¨ç°ç±»åˆ«è¾¾åˆ°äº†96%çš„F1åˆ†æ•°ï¼Œè¶…è¿‡äº†å¼ºå¤§çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤šå°ºåº¦ã€æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„èåˆå¯ä»¥å¼¥è¡¥ç—…ç†åŸºç¡€æ¨¡å‹å’Œå¯é ç»†èƒæ°´å¹³è¡¨å‹é¢„æµ‹ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13586v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹ä»å¸¸è§„ç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­è¯†åˆ«ç»†èƒç±»å‹å’Œäºšå‹å¯¹æ”¹å–„å¯¹äººç±»ç–¾ç—…è®¡ç®—ç†è§£çš„é‡è¦æ€§ï¼Œç°æœ‰åŸºäºç“¦ç‰‡çš„æ¨¡å‹è™½ç„¶èƒ½å¤Ÿæ•æ‰è¯¦ç»†çš„æ ¸å½¢æ€ï¼Œä½†å¿½ç•¥äº†å½±å“ç»†èƒåŠŸèƒ½å’Œèº«ä»½çš„æ›´å¤§èŒƒå›´çš„ç»„ç»‡ä¸Šä¸‹æ–‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†NuClassï¼Œè¿™æ˜¯ä¸€ç§å—ç—…ç†å­¦å®¶å·¥ä½œæµç¨‹å¯å‘çš„æ¡†æ¶ï¼Œç”¨äºç»†èƒçº§åˆ«çš„å¤šå°ºåº¦æ•´åˆæ ¸å½¢æ€å’Œå¾®ç¯å¢ƒä¸Šä¸‹æ–‡ã€‚NuClassåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šå±€éƒ¨è·¯å¾„ï¼Œä¸“æ³¨äº224åƒç´ è§æ–¹çš„æ ¸å½¢æ€ï¼›å…¨å±€è·¯å¾„ï¼Œå¯¹å‘¨å›´1024åƒç´ è§æ–¹çš„åŒºåŸŸè¿›è¡Œå»ºæ¨¡ã€‚å¯å­¦ä¹ çš„é—¨æ¨¡å—è‡ªé€‚åº”åœ°å¹³è¡¡å±€éƒ¨ç»†èŠ‚å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚ä¸ºäº†ä¿ƒè¿›äº’è¡¥å­¦ä¹ ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸ç¡®å®šæ€§å¼•å¯¼çš„ç›®æ ‡ï¼Œå¼•å¯¼å…¨å±€è·¯å¾„ä¼˜å…ˆå¤„ç†å±€éƒ¨è·¯å¾„ä¸ç¡®å®šçš„åŒºåŸŸã€‚æˆ‘ä»¬è¿˜æä¾›äº†æ ¡å‡†çš„ä¿¡å¿ƒä¼°è®¡å’ŒGrad-CAMå¯è§†åŒ–ä»¥å¢å¼ºå¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³é«˜è´¨é‡æ³¨é‡Šçš„ç¼ºä¹ï¼Œæˆ‘ä»¬ä»Xeniumç©ºé—´è½¬å½•ç»„åˆ†æä¸­æ„å»ºäº†æ ‡è®°å¼•å¯¼æ•°æ®é›†ï¼Œä¸ºè¶…è¿‡ä¸¤ç™¾ä¸‡ç»†èƒæä¾›å•ç»†èƒåˆ†è¾¨ç‡æ ‡ç­¾ï¼Œæ¶µç›–å…«ä¸ªå™¨å®˜å’Œåå…­ç±»ã€‚åœ¨ä¸‰ä¸ªå®Œå…¨ç‹¬ç«‹çš„æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒNuClassçš„æœ€ä½³è¡¨ç°ç±»åˆ«è¾¾åˆ°äº†é«˜è¾¾96%çš„F1åˆ†æ•°ï¼Œä¼˜äºå¼ºå¤§çš„åŸºçº¿ã€‚ç»“æœè¡¨æ˜ï¼Œå¤šå°ºåº¦ã€å…·æœ‰æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„èåˆå¯ä»¥åœ¨å¹»ç¯ç‰‡çº§åˆ«çš„ç—…ç†åŸºç¡€æ¨¡å‹å’Œå¯é ã€ç»†èƒçº§åˆ«çš„è¡¨å‹é¢„æµ‹ä¹‹é—´å»ºç«‹æ¡¥æ¢ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¸¸è§„ç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­çš„ç»†èƒç±»å‹å’Œäºšå‹è¯†åˆ«å¯¹äºç†è§£äººç±»ç–¾ç—…è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨æ•æ‰æ ¸å½¢æ€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¿½ç•¥äº†ç»„ç»‡ä¸Šä¸‹æ–‡çš„å½±å“ã€‚</li>
<li>NuClassæ¡†æ¶ç»“åˆäº†ç»†èƒçº§åˆ«çš„å¤šå°ºåº¦æ ¸å½¢æ€å’Œå¾®ç¯å¢ƒä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>NuClassåŒ…æ‹¬å±€éƒ¨å’Œå…¨å±€è·¯å¾„ï¼Œé€šè¿‡å¯å­¦ä¹ çš„é—¨æ¨¡å—å¹³è¡¡å±€éƒ¨ç»†èŠ‚å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨äº†ä¸ç¡®å®šæ€§å¼•å¯¼çš„ç›®æ ‡æ¥å¢å¼ºæ¨¡å‹åœ¨å±€éƒ¨ä¸ç¡®å®šåŒºåŸŸçš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æä¾›æ ¡å‡†çš„ä¿¡å¿ƒä¼°è®¡å’Œå¯è§†åŒ–å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13586">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5433417870096b2ff7c6be9586432de" align="middle">
<img src="https://picx.zhimg.com/v2-22a35d143b5a997024f2a7c233dbaa17" align="middle">
<img src="https://picx.zhimg.com/v2-48b4c1f49031ae90ea96c399bd25890e" align="middle">
<img src="https://picx.zhimg.com/v2-8cc1604f2b75c39478ee6bff24e6bbcb" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RAC-DMVC-Reliability-Aware-Contrastive-Deep-Multi-View-Clustering-under-Multi-Source-Noise"><a href="#RAC-DMVC-Reliability-Aware-Contrastive-Deep-Multi-View-Clustering-under-Multi-Source-Noise" class="headerlink" title="RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise"></a>RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise</h2><p><strong>Authors:Shihao Dong, Yue Liu, Xiaotong Zhou, Yuhui Zheng, Huiying Xu, Xinzhong Zhu</strong></p>
<p>Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.</p>
<blockquote>
<p>å¤šè§†è§’èšç±»ï¼ˆMVCï¼‰æ—¨åœ¨ä»¥æ— ç›‘ç£çš„æ–¹å¼å°†å¤šè§†è§’æ•°æ®åˆ†æˆä¸åŒçš„ç°‡ï¼Œè¿™æ˜¯ä¸€é¡¹åŸºæœ¬ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä¸ºäº†å¢å¼ºå…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æ€§ï¼Œæœ¬æ–‡è§£å†³äº†ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼šåœ¨åŒ…æ‹¬ç¼ºå¤±å™ªå£°å’Œè§‚æµ‹å™ªå£°çš„å¤šæºå™ªå£°ä¸‹çš„MVCã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå³å¯é å¯¹æ¯”æ·±åº¦å¤šè§†è§’èšç±»ï¼ˆRAC-DMVCï¼‰ï¼Œè¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªå¯é æ€§å›¾ï¼Œä»¥åœ¨å™ªå£°ç¯å¢ƒä¸­æŒ‡å¯¼ç¨³å¥è¡¨ç¤ºå­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†è§£å†³è§‚æµ‹å™ªå£°é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨è§†å›¾é‡å»ºï¼Œä»¥å¢å¼ºæ•°æ®çº§åˆ«çš„ç¨³å¥æ€§ï¼Œå¹¶å¼•å…¥å¯é å¯¹æ¯”å­¦ä¹ ä»¥å‡è½»ç”±å™ªå£°è¡¨ç¤ºå¼•èµ·çš„æ­£è´Ÿæ ·æœ¬é€‰æ‹©ä¸­çš„åè§ã€‚ä¸ºäº†å¤„ç†ç¼ºå¤±å™ªå£°ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŒæ³¨æ„åŠ›å¡«è¡¥ç­–ç•¥ï¼Œä»¥æ•è·è·¨è§†å›¾çš„å…±äº«ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™è§†å›¾ç‰¹å®šç‰¹å¾ã€‚æ­¤å¤–ï¼Œè‡ªç›‘ç£èšç±»è’¸é¦æ¨¡å—è¿›ä¸€æ­¥ç²¾ç‚¼äº†å­¦åˆ°çš„è¡¨ç¤ºå¹¶æé«˜äº†èšç±»æ€§èƒ½ã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒRAC-DMVCåœ¨å¤šä¸ªè¯„ä»·æŒ‡æ ‡ä¸Šä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå¹¶åœ¨ä¸åŒæ¯”ä¾‹çš„å™ªå£°ä¸‹ä¿æŒå“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13561v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¤šè§†è§’èšç±»æ–¹æ³•ï¼Œåä¸ºå¯é æ€§æ„ŸçŸ¥å¯¹æ¯”æ·±åº¦å¤šè§†è§’èšç±»ï¼ˆRAC-DMVCï¼‰ã€‚è¯¥æ–¹æ³•æ„å»ºå¯é æ€§å›¾ï¼Œä»¥æŒ‡å¯¼åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„ç¨³å¥è¡¨ç¤ºå­¦ä¹ ã€‚RAC-DMVCé€šè¿‡è·¨è§†è§’é‡å»ºå¢å¼ºæ•°æ®å±‚é¢çš„ç¨³å¥æ€§ï¼Œå¹¶å¼•å…¥å¯é æ€§æ„ŸçŸ¥å™ªå£°å¯¹æ¯”å­¦ä¹ æ¥å‡è½»ç”±å™ªå£°è¡¨ç¤ºå¼•èµ·çš„æ­£è´Ÿæ ·æœ¬é€‰æ‹©ä¸­çš„åè§ï¼Œä»¥åº”å¯¹è§‚æµ‹å™ªå£°ã€‚ä¸ºå¤„ç†ç¼ºå¤±å™ªå£°ï¼Œè®¾è®¡äº†ä¸€ç§åŒå…³æ³¨å¡«è¡¥æœºåˆ¶ï¼Œæ—¨åœ¨æ•æ‰è·¨è§†è§’çš„å…±äº«ä¿¡æ¯åŒæ—¶ä¿ç•™ç‰¹å®šè§†è§’çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œé€šè¿‡è‡ªæˆ‘ç›‘ç£çš„é›†ç¾¤è’¸é¦æ¨¡å—è¿›ä¸€æ­¥ä¼˜åŒ–äº†å­¦ä¹ åˆ°çš„è¡¨ç¤ºå¹¶æé«˜äº†èšç±»æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒRAC-DMVCåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•ï¼Œå¹¶åœ¨ä¸åŒå™ªå£°æ¯”ç‡ä¸‹ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAC-DMVCæ˜¯ä¸€ç§å¤šè§†è§’èšç±»æ–¹æ³•ï¼Œæ—¨åœ¨å°†å¤šè§†è§’æ•°æ®åœ¨æ— éœ€ç›‘ç£çš„æƒ…å†µä¸‹åˆ†æˆä¸åŒçš„é›†ç¾¤ã€‚</li>
<li>è¯¥æ–¹æ³•æ„å»ºå¯é æ€§å›¾ä»¥å¤„ç†ç°å®ä¸–ç•Œä¸­å¯èƒ½å­˜åœ¨çš„å„ç§å™ªå£°ã€‚</li>
<li>ä¸ºäº†åº”å¯¹è§‚æµ‹å™ªå£°ï¼ŒRAC-DMVCé‡‡ç”¨è·¨è§†è§’é‡å»ºå’Œå¯é æ€§æ„ŸçŸ¥å™ªå£°å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>ç¼ºå¤±å™ªå£°çš„å¤„ç†é€šè¿‡åŒå…³æ³¨å¡«è¡¥æœºåˆ¶å®ç°ï¼Œè¯¥æœºåˆ¶èƒ½æ•æ‰è·¨è§†è§’çš„å…±äº«ä¿¡æ¯å¹¶ä¿ç•™ç‰¹å®šè§†è§’çš„ç‰¹å¾ã€‚</li>
<li>RAC-DMVCè¿˜é‡‡ç”¨è‡ªæˆ‘ç›‘ç£çš„é›†ç¾¤è’¸é¦æ¨¡å—æ¥ä¼˜åŒ–è¡¨ç¤ºå¹¶æé«˜èšç±»æ€§èƒ½ã€‚</li>
<li>åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRAC-DMVCåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”èƒ½åœ¨ä¸åŒçš„å™ªå£°æ¯”ç‡ä¸‹ç»´æŒè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>RAC-DMVCä¸ºå¤„ç†å¤šæºå™ªå£°ä¸‹çš„å¤šè§†è§’èšç±»æä¾›äº†ä¸€ä¸ªæ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eeea42ec1f6b268457a5c5ae91a62519" align="middle">
<img src="https://picx.zhimg.com/v2-79a36491d02b4409b7900403ac316be2" align="middle">
<img src="https://picx.zhimg.com/v2-7ed462999b8d2b663a51d1d97e68b322" align="middle">
<img src="https://picx.zhimg.com/v2-999169ed3687b10fe77d8b9462c9a439" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TSE-Net-Semi-supervised-Monocular-Height-Estimation-from-Single-Remote-Sensing-Images"><a href="#TSE-Net-Semi-supervised-Monocular-Height-Estimation-from-Single-Remote-Sensing-Images" class="headerlink" title="TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images"></a>TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images</h2><p><strong>Authors:Sining Chen, Xiao Xiang Zhu</strong></p>
<p>Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/zhu-xlab/tse-net">https://github.com/zhu-xlab/tse-net</a>.</p>
<blockquote>
<p>å•çœ¼é«˜åº¦ä¼°è®¡åœ¨é¥æ„Ÿ3Dæ„ŸçŸ¥ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå®ƒä¸ºå¤šè§†è§’æˆ–åŸºäºæ¿€å…‰é›·è¾¾çš„æ–¹æ³•æä¾›äº†ç»æµé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚è™½ç„¶æ·±åº¦å­¦ä¹ å·²ç»å¤§å¤§æé«˜äº†å•çœ¼é«˜åº¦ä¼°è®¡çš„èƒ½åŠ›ï¼Œä½†è¿™äº›æ–¹æ³•ä»æ ¹æœ¬ä¸Šä»ç„¶å—åˆ°æ ‡è®°æ•°æ®å¯ç”¨æ€§çš„é™åˆ¶ï¼Œå¤§è§„æ¨¡è·å–è¿™äº›æ ‡è®°æ•°æ®æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚é«˜è´¨é‡æ³¨é‡Šçš„ç¨€ç¼ºæ€§é˜»ç¢äº†ç°æœ‰æ¨¡å‹çš„æ³›åŒ–å’Œæ€§èƒ½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡åŠç›‘ç£å­¦ä¹ æ¡†æ¶åˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»æœªæ ‡è®°æ ·æœ¬ä¸­æå–ä¿¡æ¯çº¿ç´¢ï¼Œæé«˜å…¶é¢„æµ‹æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TSE-Netï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåŠç›‘ç£å•çœ¼é«˜åº¦ä¼°è®¡çš„è‡ªæˆ‘è®­ç»ƒç®¡é“ã€‚è¯¥ç®¡é“æ•´åˆäº†æ•™å¸ˆã€å­¦ç”Ÿå’Œè€ƒè¯•ç½‘ç»œã€‚å­¦ç”Ÿç½‘ç»œä½¿ç”¨æ•™å¸ˆç½‘ç»œç”Ÿæˆçš„ä¼ªæ ‡ç­¾åœ¨æœªç»æ ‡è®°çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè€Œè€ƒè¯•ç½‘ç»œåˆ™å……å½“å­¦ç”Ÿç½‘ç»œçš„ä¸´æ—¶ç»„åˆä»¥ç¨³å®šæ€§èƒ½ã€‚æ•™å¸ˆç½‘ç»œè¢«åˆ¶å®šä¸ºè”åˆå›å½’å’Œåˆ†ç±»æ¨¡å‹ï¼šå›å½’åˆ†æ”¯é¢„æµ‹é«˜åº¦å€¼ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œåˆ†ç±»åˆ†æ”¯é¢„æµ‹é«˜åº¦å€¼ç±»åˆ«ä»¥åŠç±»åˆ«æ¦‚ç‡ï¼Œç”¨äºè¿‡æ»¤ä¼ªæ ‡ç­¾ã€‚é«˜åº¦å€¼ç±»åˆ«æ˜¯é€šè¿‡å±‚æ¬¡äºŒåˆ†ç­–ç•¥å®šä¹‰çš„ï¼Œä»¥è§£å†³é«˜åº¦å›ºæœ‰çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œé¢„æµ‹çš„ç±»åˆ«æ¦‚ç‡é€šè¿‡Plackett-Luceæ¨¡å‹è¿›è¡Œæ ¡å‡†ï¼Œä»¥åæ˜ ä¼ªæ ‡ç­¾çš„é¢„æœŸå‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒåˆ†è¾¨ç‡å’Œæˆåƒæ¨¡å¼çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„ç®¡é“ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhu-xlab/tse-net%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zhu-xlab/tse-netè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13552v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å•ç›®é«˜åº¦ä¼°è®¡åœ¨é¥æ„Ÿ3Dæ„ŸçŸ¥ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å•ç›®é«˜åº¦ä¼°è®¡ä¸­çš„åº”ç”¨å—åˆ°æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚ä¸ºå…‹æœæ­¤é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†åˆ©ç”¨å¤§é‡æ— æ ‡ç­¾æ•°æ®çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶TSE-Netã€‚è¯¥æ¡†æ¶é€šè¿‡æ•™å¸ˆç½‘ç»œç”Ÿæˆä¼ªæ ‡ç­¾æ¥è®­ç»ƒå­¦ç”Ÿç½‘ç»œï¼Œå¹¶å¼•å…¥è€ƒè¯•ç½‘ç»œä»¥ç¨³å®šæ€§èƒ½ã€‚æ•™å¸ˆç½‘ç»œè”åˆå›å½’å’Œåˆ†ç±»æ¨¡å‹ï¼Œå›å½’åˆ†æ”¯ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œåˆ†ç±»åˆ†æ”¯é¢„æµ‹é«˜åº¦å€¼ç±»åˆ«åŠç±»åˆ«æ¦‚ç‡ä»¥è¿‡æ»¤ä¼ªæ ‡ç­¾ã€‚é‡‡ç”¨å±‚æ¬¡äºŒåˆ†ç­–ç•¥å®šä¹‰é«˜åº¦å€¼ç±»åˆ«ï¼Œå¹¶ç”¨Plackett-Luceæ¨¡å‹æ ¡å‡†é¢„æµ‹ç±»åˆ«æ¦‚ç‡ä»¥åæ˜ ä¼ªæ ‡ç­¾çš„é¢„æœŸå‡†ç¡®æ€§ã€‚åœ¨ä¸‰ä¸ªä¸åŒåˆ†è¾¨ç‡å’Œæˆåƒæ¨¡æ€çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•ç›®é«˜åº¦ä¼°è®¡åœ¨é¥æ„Ÿ3Dæ„ŸçŸ¥ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½œä¸ºå¤šè§†è§’æˆ–æ¿€å…‰é›·è¾¾æ–¹æ³•çš„æˆæœ¬æ•ˆç›Šæ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å•ç›®é«˜åº¦ä¼°è®¡ä¸­çš„åº”ç”¨å—åˆ°æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚</li>
<li>ä¸ºè§£å†³æ ‡æ³¨æ•°æ®é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨å¤§é‡æ— æ ‡ç­¾æ•°æ®çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶TSE-Netã€‚</li>
<li>TSE-Neté€šè¿‡æ•™å¸ˆç½‘ç»œç”Ÿæˆä¼ªæ ‡ç­¾æ¥è®­ç»ƒå­¦ç”Ÿç½‘ç»œï¼Œå¼•å…¥è€ƒè¯•ç½‘ç»œä»¥æ”¹å–„æ€§èƒ½ç¨³å®šæ€§ã€‚</li>
<li>æ•™å¸ˆç½‘ç»œç»“åˆå›å½’å’Œåˆ†ç±»æ¨¡å‹ï¼Œä»¥ç”Ÿæˆæ›´å‡†ç¡®çš„ä¼ªæ ‡ç­¾ã€‚</li>
<li>é‡‡ç”¨å±‚æ¬¡äºŒåˆ†ç­–ç•¥å®šä¹‰é«˜åº¦å€¼ç±»åˆ«ï¼Œä»¥é€‚åº”é«˜åº¦å€¼çš„å›ºæœ‰é•¿å°¾åˆ†å¸ƒã€‚</li>
<li>ä½¿ç”¨Plackett-Luceæ¨¡å‹æ ¡å‡†é¢„æµ‹ç±»åˆ«æ¦‚ç‡ï¼Œä»¥åæ˜ ä¼ªæ ‡ç­¾çš„é¢„æœŸå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b20b7e58e13ee0e90c4a39dbca1d24df" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ForgeDAN-An-Evolutionary-Framework-for-Jailbreaking-Aligned-Large-Language-Models"><a href="#ForgeDAN-An-Evolutionary-Framework-for-Jailbreaking-Aligned-Large-Language-Models" class="headerlink" title="ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models"></a>ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models</h2><p><strong>Authors:Siyang Cheng, Gaotian Liu, Rui Mei, Yilin Wang, Kejia Zhang, Kaishuo Wei, Yuqi Yu, Weiping Wen, Xiaojie Wu, Junhua Liu</strong></p>
<p>The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿…é€Ÿé‡‡çº³å¸¦æ¥äº†å˜é©æ€§çš„åº”ç”¨å’Œæ–°å®‰å…¨é£é™©ï¼ŒåŒ…æ‹¬ç»•è¿‡å¯¹é½ä¿éšœæªæ–½ä»¥å¼•å‘æœ‰å®³è¾“å‡ºçš„è¶Šç‹±æ”»å‡»ã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–è¶Šç‹±ç”Ÿæˆæ–¹æ³•ï¼ˆä¾‹å¦‚AutoDANï¼‰å­˜åœ¨å˜å¼‚å¤šæ ·æ€§æœ‰é™ã€é€‚åº”åº¦è¯„ä¼°è‚¤æµ…ä»¥åŠåŸºäºå…³é”®è¯çš„æ£€æµ‹è„†å¼±ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ForgeDANï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¯¹é½LLMç”Ÿæˆè¯­ä¹‰è¿è´¯ä¸”é«˜åº¦æœ‰æ•ˆçš„å¯¹æŠ—æ€§æç¤ºçš„æ–°å‹è¿›åŒ–æ¡†æ¶ã€‚é¦–å…ˆï¼ŒForgeDANå¼•å…¥è·¨å­—ç¬¦ã€å•è¯å’Œå¥å­çº§åˆ«çš„å¤šç­–ç•¥æ–‡æœ¬æ‰°åŠ¨ï¼Œä»¥å¢å¼ºæ”»å‡»å¤šæ ·æ€§ï¼›ç„¶åï¼Œæˆ‘ä»¬åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§æ¨¡å‹é‡‡ç”¨å¯è§£é‡Šçš„è¯­ä¹‰é€‚åº”åº¦è¯„ä¼°æ¥æŒ‡å¯¼è¿›åŒ–è¿‡ç¨‹ï¼Œä»¥äº§ç”Ÿè¯­ä¹‰ç›¸å…³ä¸”æœ‰å®³çš„è¾“å‡ºï¼›æœ€åï¼ŒForgeDANæ•´åˆäº†åŒé‡ç»´åº¦çš„è¶Šç‹±åˆ¤æ–­ï¼Œåˆ©ç”¨åŸºäºLLMçš„åˆ†ç±»å™¨è”åˆè¯„ä¼°æ¨¡å‹åˆè§„æ€§å’Œè¾“å‡ºå±å®³æ€§ï¼Œä»è€Œé™ä½è¯¯æŠ¥å¹¶æé«˜æ£€æµ‹æ•ˆç‡ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒForgeDANåœ¨ä¿æŒè‡ªç„¶æ€§å’Œéšç§˜æ€§çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¶Šç‹±æˆåŠŸç‡ï¼Œä¼˜äºç°æœ‰çš„æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13548v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿…é€Ÿé‡‡çº³å¸¦æ¥äº†å˜é©æ€§çš„åº”ç”¨å’Œæ–°å®‰å…¨å¨èƒï¼ŒåŒ…æ‹¬ç»•è¿‡å¯¹é½ä¿éšœçš„ç›‘ç‹±çªç ´æ”»å‡»ï¼Œä»¥äº§ç”Ÿæœ‰å®³è¾“å‡ºã€‚ç°æœ‰è‡ªåŠ¨ç›‘ç‹±çªç ´ç”Ÿæˆæ–¹æ³•ï¼Œå¦‚AutoDANï¼Œå­˜åœ¨å˜å¼‚å¤šæ ·æ€§æœ‰é™ã€é€‚åº”åº¦è¯„ä¼°è‚¤æµ…ã€ä»¥åŠåŸºäºå…³é”®è¯çš„æ£€æµ‹è„†å¼±ç­‰å±€é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºForgeDANï¼Œä¸€ç§æ–°å‹çš„è¿›åŒ–æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé’ˆå¯¹å¯¹é½LLMsçš„è¯­ä¹‰è¿è´¯ä¸”é«˜æ•ˆçš„å¯¹æŠ—æ€§æç¤ºã€‚ForgeDANå¼•å…¥è·¨å­—ç¬¦ã€å•è¯å’Œå¥å­çº§åˆ«çš„å¤šç­–ç•¥æ–‡æœ¬æ‰°åŠ¨ï¼Œä»¥æé«˜æ”»å‡»å¤šæ ·æ€§ï¼›ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§çš„å¯è§£é‡Šè¯­ä¹‰é€‚åº”åº¦è¯„ä¼°æ¥æŒ‡å¯¼è¿›åŒ–è¿‡ç¨‹ï¼Œä»¥äº§ç”Ÿè¯­ä¹‰ç›¸å…³ä¸”æœ‰å®³çš„è¾“å‡ºï¼›æœ€åï¼ŒForgeDANç»“åˆåŒé‡ç»´åº¦çš„ç›‘ç‹±çªç ´åˆ¤æ–­ï¼Œåˆ©ç”¨LLMåˆ†ç±»å™¨è”åˆè¯„ä¼°æ¨¡å‹åˆè§„æ€§å’Œè¾“å‡ºå±å®³æ€§ï¼Œä»è€Œé™ä½è¯¯æŠ¥å¹¶æé«˜æ£€æµ‹æ•ˆç‡ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒForgeDANåœ¨ä¿æŒè‡ªç„¶æ€§å’Œéšç§˜æ€§çš„åŒæ—¶ï¼Œå®ç°äº†é«˜çªç ´æˆåŠŸç‡ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ™®åŠå¸¦æ¥äº†æ–°å®‰å…¨å¨èƒï¼ŒåŒ…æ‹¬ç›‘ç‹±çªç ´æ”»å‡»ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨ç›‘ç‹±çªç ´ç”Ÿæˆæ–¹æ³•å­˜åœ¨å˜å¼‚å¤šæ ·æ€§æœ‰é™ã€é€‚åº”åº¦è¯„ä¼°è‚¤æµ…ç­‰é—®é¢˜ã€‚</li>
<li>ForgeDANæ¡†æ¶é€šè¿‡å¼•å…¥å¤šç­–ç•¥æ–‡æœ¬æ‰°åŠ¨å¢å¼ºæ”»å‡»å¤šæ ·æ€§ã€‚</li>
<li>ForgeDANåˆ©ç”¨åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§çš„è¯­ä¹‰é€‚åº”åº¦è¯„ä¼°æŒ‡å¯¼è¿›åŒ–è¿‡ç¨‹ã€‚</li>
<li>ForgeDANç»“åˆåŒé‡ç»´åº¦çš„ç›‘ç‹±çªç ´åˆ¤æ–­ï¼Œé™ä½è¯¯æŠ¥å¹¶æé«˜æ£€æµ‹æ•ˆç‡ã€‚</li>
<li>ForgeDANå®ç°äº†é«˜çªç ´æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒè‡ªç„¶æ€§å’Œéšç§˜æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1392b86c3f1cd91ee42c6519b2c67ae4" align="middle">
<img src="https://picx.zhimg.com/v2-fd47d14a83094b9c8dcface9316459a0" align="middle">
<img src="https://picx.zhimg.com/v2-bf0625dfb62a4e243a388d7e148e3691" align="middle">
<img src="https://picx.zhimg.com/v2-9c553a521a0ac08f31cd04b990f8b945" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Interpretable-Ransomware-Detection-Using-Hybrid-Large-Language-Models-A-Comparative-Analysis-of-BERT-RoBERTa-and-DeBERTa-Through-LIME-and-SHAP"><a href="#Interpretable-Ransomware-Detection-Using-Hybrid-Large-Language-Models-A-Comparative-Analysis-of-BERT-RoBERTa-and-DeBERTa-Through-LIME-and-SHAP" class="headerlink" title="Interpretable Ransomware Detection Using Hybrid Large Language Models: A Comparative Analysis of BERT, RoBERTa, and DeBERTa Through LIME and SHAP"></a>Interpretable Ransomware Detection Using Hybrid Large Language Models: A Comparative Analysis of BERT, RoBERTa, and DeBERTa Through LIME and SHAP</h2><p><strong>Authors:Elodie Mutombo Ngoie, Mike Nkongolo Wa Nkongolo, Peace Azugo, Mahmut Tokmak</strong></p>
<p>Ransomware continues to evolve in complexity, making early and explainable detection a critical requirement for modern cybersecurity systems. This study presents a comparative analysis of three Transformer-based Large Language Models (LLMs) (BERT, RoBERTa, and DeBERTa) for ransomware detection using two structured datasets: UGRansome and Process Memory (PM). Since LLMs are primarily designed for natural language processing (NLP), numerical and categorical ransomware features were transformed into textual sequences using KBinsDiscretizer and token-based encoding. This enabled the models to learn behavioural patterns from system activity and network traffic through contextual embeddings. The models were fine-tuned on approximately 2,500 labelled samples and evaluated using accuracy, F1 score, and ROC-AUC. To ensure transparent decision-making in this high-stakes domain, two explainable AI techniques (LIME and SHAP) were applied to interpret feature contributions. The results show that the models learn distinct ransomware-related cues: BERT relies heavily on dominant file-operation features, RoBERTa demonstrates balanced reliance on network and financial signals, while DeBERTa exhibits strong sensitivity to financial and network-traffic indicators. Visualisation of embeddings further reveals structural differences in token representation, with RoBERTa producing more isotropic embeddings and DeBERTa capturing highly directional, disentangled patterns. In general, RoBERTa achieved the strongest F1-score, while BERT yielded the highest ROC-AUC performance. The integration of LLMs with XAI provides a transparent framework capable of identifying feature-level evidence behind ransomware predictions.</p>
<blockquote>
<p>å‹’ç´¢è½¯ä»¶ä¸æ–­åœ¨å¤æ‚æ€§ä¸Šè¿›åŒ–ï¼Œä½¿å¾—æ—©æœŸå’Œå¯è§£é‡Šçš„æ£€æµ‹æˆä¸ºç°ä»£ç½‘ç»œå®‰å…¨ç³»ç»Ÿçš„å…³é”®è¦æ±‚ã€‚æœ¬ç ”ç©¶å¯¹ä¸‰ç§åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆBERTã€RoBERTaå’ŒDeBERTaï¼‰è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œç”¨äºä½¿ç”¨ä¸¤ç§ç»“æ„åŒ–æ•°æ®é›†UGRansomeå’ŒProcess Memoryï¼ˆPMï¼‰è¿›è¡Œå‹’ç´¢è½¯ä»¶æ£€æµ‹ã€‚ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦è®¾è®¡ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼Œå› æ­¤ä½¿ç”¨KBinsDiscretizerå’ŒåŸºäºä»¤ç‰Œçš„ç¼–ç å°†æ•°å€¼å’Œåˆ†ç±»å‹’ç´¢è½¯ä»¶ç‰¹å¾è½¬æ¢ä¸ºæ–‡æœ¬åºåˆ—ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡åµŒå…¥ä»ç³»ç»Ÿæ´»åŠ¨å’Œç½‘ç»œæµé‡ä¸­å­¦ä¹ è¡Œä¸ºæ¨¡å¼ã€‚è¿™äº›æ¨¡å‹åœ¨å¤§çº¦2500ä¸ªæ ‡è®°æ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨å‡†ç¡®åº¦ã€F1åˆ†æ•°å’ŒROC-AUCè¿›è¡Œè¯„ä¼°ã€‚ä¸ºäº†ç¡®ä¿é«˜é£é™©é¢†åŸŸå†³ç­–é€æ˜åŒ–ï¼Œé‡‡ç”¨äº†ä¸¤ç§å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼ˆLIMEå’ŒSHAPï¼‰æ¥è§£é‡Šç‰¹å¾è´¡çŒ®ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹å­¦ä¼šäº†ä¸åŒçš„å‹’ç´¢è½¯ä»¶ç›¸å…³çº¿ç´¢ï¼šBERTä¸¥é‡ä¾èµ–äºä¸»è¦çš„æ–‡ä»¶æ“ä½œç‰¹å¾ï¼ŒRoBERTaå¯¹ç½‘ç»œå’Œé‡‘èä¿¡å·çš„ä¾èµ–è¡¨ç°å‡ºå¹³è¡¡ï¼Œè€ŒDeBERTaå¯¹é‡‘èå’Œç½‘ç»œæµé‡æŒ‡æ ‡è¡¨ç°å‡ºå¼ºçƒˆçš„æ•æ„Ÿæ€§ã€‚åµŒå…¥çš„å¯è§†åŒ–è¿›ä¸€æ­¥æ­ç¤ºäº†ä»¤ç‰Œè¡¨ç¤ºçš„ç»“æ„æ€§å·®å¼‚ï¼ŒRoBERTaäº§ç”Ÿæ›´ç­‰è·çš„åµŒå…¥ï¼Œè€ŒDeBERTaæ•æ‰é«˜åº¦å®šå‘ã€è§£è€¦çš„æ¨¡å¼ã€‚æ€»çš„æ¥è¯´ï¼ŒRoBERTaè·å¾—äº†æœ€é«˜çš„F1åˆ†æ•°ï¼Œè€ŒBERTè·å¾—äº†æœ€é«˜çš„ROC-AUCæ€§èƒ½ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¯è§£é‡Šæ€§äººå·¥æ™ºèƒ½çš„ç»“åˆæä¾›äº†ä¸€ä¸ªé€æ˜çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿè¯†åˆ«å‹’ç´¢è½¯ä»¶é¢„æµ‹èƒŒåçš„ç‰¹å¾çº§åˆ«è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13517v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶å¯¹æ¯”åˆ†æäº†BERTã€RoBERTaå’ŒDeBERTaä¸‰ç§åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ©ç”¨UGRansomeå’ŒProcess Memoryï¼ˆPMï¼‰ä¸¤ä¸ªç»“æ„åŒ–æ•°æ®é›†è¿›è¡Œå‹’ç´¢è½¯ä»¶æ£€æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚é€šè¿‡KBinsDiscretizerå’ŒåŸºäºtokençš„ç¼–ç æŠ€æœ¯å°†æ•°å€¼å’Œåˆ†ç±»å‹’ç´¢è½¯ä»¶ç‰¹å¾è½¬æ¢ä¸ºæ–‡æœ¬åºåˆ—ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡åµŒå…¥å­¦ä¹ ç³»ç»Ÿæ´»åŠ¨å’Œç½‘ç»œæµé‡çš„è¡Œä¸ºæ¨¡å¼ã€‚æ¨¡å‹åœ¨çº¦2500ä¸ªæ ‡è®°æ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨å‡†ç¡®æ€§ã€F1åˆ†æ•°å’ŒROC-AUCè¿›è¡Œè¯„ä¼°ã€‚åº”ç”¨LIMEå’ŒSHAPä¸¤ç§å¯è§£é‡Šçš„AIæŠ€æœ¯æ¥è§£é‡Šç‰¹å¾è´¡çŒ®ï¼Œä»¥ç¡®ä¿å†³ç­–é€æ˜åŒ–ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹å­¦ä¹ åˆ°çš„å‹’ç´¢è½¯ä»¶ç›¸å…³çº¿ç´¢å„ä¸ç›¸åŒï¼šBERTä¾èµ–æ–‡ä»¶æ“ä½œç‰¹å¾ï¼ŒRoBERTaå¹³è¡¡ä¾èµ–ç½‘ç»œå’Œé‡‘èä¿¡å·ï¼Œè€ŒDeBERTaå¯¹é‡‘èå’Œç½‘ç»œæµé‡æŒ‡æ ‡æ•æ„Ÿæ€§å¼ºã€‚å¯è§†åŒ–åµŒå…¥è¿›ä¸€æ­¥æ­ç¤ºäº†tokenè¡¨ç¤ºçš„ç»“æ„å·®å¼‚ã€‚æ€»ä½“è€Œè¨€ï¼ŒRoBERTaçš„F1åˆ†æ•°æœ€é«˜ï¼Œè€ŒBERTçš„ROC-AUCæ€§èƒ½æœ€ä½³ã€‚å°†LLMsä¸XAIç»“åˆï¼Œæä¾›äº†ä¸€ä¸ªèƒ½å¤Ÿè¯†åˆ«å‹’ç´¢è½¯ä»¶é¢„æµ‹èƒŒåç‰¹å¾çº§åˆ«è¯æ®çš„é€æ˜æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å‹’ç´¢è½¯ä»¶ä¸æ–­è¿›åŒ–ï¼Œæ—©æœŸå’Œå¯è§£é‡Šçš„æ£€æµ‹å¯¹ç°ä»£ç½‘ç»œå®‰å…¨ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>å¯¹æ¯”åˆ†æä¸‰ç§åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆBERTã€RoBERTaã€DeBERTaï¼‰ç”¨äºå‹’ç´¢è½¯ä»¶æ£€æµ‹ã€‚</li>
<li>é€šè¿‡KBinsDiscretizerå’ŒåŸºäºtokençš„ç¼–ç ï¼Œå°†æ•°å€¼å’Œåˆ†ç±»ç‰¹å¾è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è¡Œä¸ºæ¨¡å¼ã€‚</li>
<li>æ¨¡å‹åœ¨ç»“æ„åŒ–æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå¹¶è¯„ä¼°ï¼Œä½¿ç”¨å‡†ç¡®æ€§ã€F1åˆ†æ•°å’ŒROC-AUCè¡¡é‡æ€§èƒ½ã€‚</li>
<li>LIMEå’ŒSHAPæŠ€æœ¯ç”¨äºè§£é‡Šæ¨¡å‹å†³ç­–ï¼Œå¢å¼ºé€æ˜åº¦ã€‚</li>
<li>ä¸åŒæ¨¡å‹å¯¹å‹’ç´¢è½¯ä»¶ç›¸å…³çº¿ç´¢çš„å­¦ä¹ å„æœ‰ä¾§é‡ï¼šBERTä¾èµ–æ–‡ä»¶æ“ä½œï¼ŒRoBERTaå¹³è¡¡è€ƒè™‘ç½‘ç»œå’Œé‡‘èä¿¡å·ï¼ŒDeBERTaå¼ºè°ƒé‡‘èå’Œç½‘ç»œæµé‡ã€‚</li>
<li>ç»“åˆLLMsä¸å¯è§£é‡Šçš„AIæŠ€æœ¯æä¾›é€æ˜æ¡†æ¶ï¼Œæœ‰åŠ©äºè¯†åˆ«å‹’ç´¢è½¯ä»¶é¢„æµ‹çš„ç‰¹å¾çº§åˆ«è¯æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13517">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffd3cb14eb3ee6ad0b9f2d3e18feeff1" align="middle">
<img src="https://picx.zhimg.com/v2-de0c59345cceadf38da7b0aa19aa68e3" align="middle">
<img src="https://picx.zhimg.com/v2-9f63cd8fbc0e86d05ce6595957685c46" align="middle">
<img src="https://picx.zhimg.com/v2-ef7cc91301f8d2e7d2f4771298620ce5" align="middle">
<img src="https://picx.zhimg.com/v2-aad27894985988cc4e41bec37a043de8" align="middle">
<img src="https://picx.zhimg.com/v2-ea3954c8bdc1c478c78a790ef85f08dc" align="middle">
<img src="https://picx.zhimg.com/v2-a53b3acc44d744f6f108a304e86f5b68" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Multimodal-Large-Language-Model-Framework-for-Automated-Interpretation-of-Fuel-Efficiency-Analytics-in-Public-Transportation"><a href="#Multi-Agent-Multimodal-Large-Language-Model-Framework-for-Automated-Interpretation-of-Fuel-Efficiency-Analytics-in-Public-Transportation" class="headerlink" title="Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation"></a>Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation</h2><p><strong>Authors:Zhipeng Ma, Ali Rida Bahja, Andreas Burgdorf, AndrÃ© Pomp, Tobias Meisen, Bo NÃ¸rregaard JÃ¸rgensen, Zheng Grace Ma</strong></p>
<p>Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.</p>
<blockquote>
<p>æå‡å…¬å…±äº¤é€šçš„ç‡ƒæ²¹æ•ˆç‡éœ€è¦æ•´åˆå¤æ‚çš„å¤šæ¨¡å¼æ•°æ®ä¸ºå¯è§£è¯»çš„ã€ä¸å†³ç­–ç›¸å…³çš„è§è§£ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åˆ†ææ–¹æ³•å’Œå¯è§†åŒ–æ–¹æ³•é€šå¸¸äº§ç”Ÿç¢ç‰‡åŒ–çš„è¾“å‡ºï¼Œéœ€è¦å¤§é‡çš„äººå·¥è§£è¯»ï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§å’Œä¸€è‡´æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è‡ªåŠ¨è¿›è¡Œæ•°æ®å™è¿°å’Œèƒ½æºæ´å¯Ÿç”Ÿæˆã€‚è¯¥æ¡†æ¶åè°ƒä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ•°æ®å™è¿°æ™ºèƒ½ä½“ã€ä¸€ä¸ªä½œä¸ºæ³•å®˜çš„LLMæ™ºèƒ½ä½“ï¼Œä»¥åŠä¸€ä¸ªå¯é€‰çš„äººæœºå¾ªç¯è¯„ä¼°æ™ºèƒ½ä½“ï¼Œä»¥è¿­ä»£çš„æ–¹å¼å°†åˆ†æäº§ç‰©è½¬åŒ–ä¸ºè¿è´¯çš„ã€ä»¥åˆ©ç›Šç›¸å…³è€…ä¸ºå¯¼å‘çš„æŠ¥å‘Šã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸¹éº¦åŒ—æ—¥å¾·å…°åœ°åŒºå…¬äº¤è½¦å…¬å…±äº¤é€šè¿è¾“çš„çœŸå®æ¡ˆä¾‹ç ”ç©¶è¿›è¡ŒéªŒè¯ï¼Œå…¶ä¸­4006æ¬¡è¡Œç¨‹çš„ç‡ƒæ²¹æ•ˆç‡æ•°æ®ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹èšç±»è¿›è¡Œåˆ†æã€‚è·¨è¶Šäº”ç§æœ€æ–°LLMå’Œä¸‰ç§æç¤ºèŒƒå¼çš„æ¯”è¾ƒå®éªŒç¡®å®šäº†GPT-4.1 miniä¸Chain-of-Thoughtæç¤ºä¸ºæœ€ä½³é…ç½®ï¼Œè¾¾åˆ°äº†97.3%çš„å™è¿°å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¹³è¡¡äº†å¯è§£é‡Šæ€§å’Œè®¡ç®—æˆæœ¬ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“çš„ååŒå·¥ä½œæ˜¾è‘—æé«˜äº†åŸºäºLLMçš„æŠ¥å‘Šçš„äº‹å®ç²¾ç¡®æ€§ã€è¿è´¯æ€§å’Œå¯æ‰©å±•æ€§ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ºAIé©±åŠ¨çš„å™äº‹ç”Ÿæˆå’Œèƒ½æºä¿¡æ¯å†³ç­–æ”¯æŒå»ºç«‹äº†å¯å¤åˆ¶å’Œé€‚åº”é¢†åŸŸçš„çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13476v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨åŒ–æ•°æ®å™äº‹å’Œèƒ½æºæ´å¯Ÿç”Ÿæˆï¼Œæé«˜å…¬å…±äº¤é€šçš„ç‡ƒæ–™æ•ˆç‡ã€‚æ¡†æ¶åŒ…æ‹¬æ•°æ®å™äº‹æ™ºèƒ½ä½“ã€LLMä½œä¸ºæ³•å®˜çš„æ™ºèƒ½ä½“ï¼Œä»¥åŠå¯é€‰çš„äººæœºå¾ªç¯è¯„ä¼°å™¨ï¼Œå°†åˆ†æäº§ç‰©è½¬åŒ–ä¸ºè¿è´¯ã€é¢å‘åˆ©ç›Šç›¸å…³è€…çš„æŠ¥å‘Šã€‚é€šè¿‡ä¸¹éº¦åŒ—æ—¥å¾·å…°å…¬å…±å·´å£«è¿è¾“çš„å®ä¾‹ç ”ç©¶éªŒè¯äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œå¯¹æ¯”å®éªŒè¡¨æ˜GPT-4.1 minié…åˆChain-of-Thoughtæç¤ºæ³•ä¸ºæœ€ä½³é…ç½®ï¼Œå®ç°äº†97.3%çš„å™äº‹å‡†ç¡®æ€§ï¼Œå¹³è¡¡äº†å¯è§£é‡Šæ€§å’Œè®¡ç®—æˆæœ¬ã€‚è¯¥ç ”ç©¶å»ºç«‹äº†å¯å¤åˆ¶ã€é€‚åº”é¢†åŸŸçš„AIé©±åŠ¨å™äº‹ç”Ÿæˆå’Œèƒ½æºä¿¡æ¯åŒ–å†³ç­–æ”¯æŒæ–¹æ³•è®ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé›†æˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä»¥æé«˜å…¬å…±äº¤é€šç‡ƒæ–™æ•ˆç‡çš„æ•°æ®åˆ†æå’Œè§£é‡Šæ•ˆç‡ã€‚</li>
<li>é€šè¿‡å®é™…æ¡ˆä¾‹ç ”ç©¶ï¼ŒéªŒè¯äº†æ¡†æ¶åœ¨å…¬å…±å·´å£«è¿è¾“ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¯¹æ¯”å®éªŒè¡¨æ˜GPT-4.1 minié…åˆChain-of-Thoughtæç¤ºæ³•ä¸ºæœ€ä½³é…ç½®ï¼Œå…·æœ‰è¾ƒé«˜çš„å™äº‹å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¹³è¡¡äº†æ•°æ®å™äº‹çš„å¯è§£é‡Šæ€§å’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†å¤šæ™ºèƒ½ä½“ååŒå·¥ä½œåœ¨æé«˜LLMæŠ¥å‘Šçš„äº‹å®ç²¾ç¡®æ€§ã€è¿è´¯æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„ä½œç”¨ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºAIé©±åŠ¨çš„å™äº‹ç”Ÿæˆå’Œèƒ½æºä¿¡æ¯åŒ–å†³ç­–æ”¯æŒæä¾›äº†å¯å¤åˆ¶å’Œé€‚åº”é¢†åŸŸçš„æ–¹æ³•è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25747f40ab66994b1251671576656ab2" align="middle">
<img src="https://picx.zhimg.com/v2-a2a673e5b9cff4daf61bf82ba8e02983" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MedDCR-Learning-to-Design-Agentic-Workflows-for-Medical-Coding"><a href="#MedDCR-Learning-to-Design-Agentic-Workflows-for-Medical-Coding" class="headerlink" title="MedDCR: Learning to Design Agentic Workflows for Medical Coding"></a>MedDCR: Learning to Design Agentic Workflows for Medical Coding</h2><p><strong>Authors:Jiyang Zheng, Islam Nassar, Thanh Vu, Xu Zhong, Yang Lin, Tongliang Liu, Long Duong, Yuan-Fang Li</strong></p>
<p>Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.</p>
<blockquote>
<p>åŒ»ç–—ç¼–ç å°†è‡ªç”±æ–‡æœ¬çš„ä¸´åºŠç¬”è®°è½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„è¯Šæ–­å’Œç¨‹åºä»£ç ï¼Œå¯¹äºè®¡è´¹ã€åŒ»é™¢è¿è¥å’ŒåŒ»å­¦ç ”ç©¶è‡³å…³é‡è¦ã€‚ä¸åŒäºæ™®é€šçš„æ–‡æœ¬åˆ†ç±»ï¼Œå®ƒéœ€è¦è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ï¼šæå–è¯Šæ–­æ¦‚å¿µã€åº”ç”¨æŒ‡å—çº¦æŸã€æ˜ å°„åˆ°åˆ†å±‚ç¼–ç ç°¿ï¼Œå¹¶ç¡®ä¿è·¨æ–‡æ¡£çš„ä¸€è‡´æ€§ã€‚æœ€è¿‘çš„è¿›å±•åˆ©ç”¨äº†ä»£ç†å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å¤§å¤šæ•°ä¾èµ–äºåƒµåŒ–ã€æ‰‹å·¥åˆ¶ä½œçš„å·¥ä½œæµç¨‹ï¼Œæ— æ³•æ•æ‰çœŸå®ä¸–ç•Œæ–‡æ¡£çš„ç»†å¾®å·®åˆ«å’Œå¯å˜æ€§ï¼Œç•™ä¸‹äº†å¦‚ä½•ç³»ç»Ÿåœ°å­¦ä¹ æœ‰æ•ˆå·¥ä½œæµç¨‹çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†MedDCRï¼Œè¿™æ˜¯ä¸€ä¸ªé—­ç¯æ¡†æ¶ï¼Œå°†å·¥ä½œæµç¨‹è®¾è®¡è§†ä¸ºå­¦ä¹ é—®é¢˜ã€‚è®¾è®¡è€…æå‡ºå·¥ä½œæµç¨‹ï¼Œç¼–ç è€…æ‰§è¡Œå®ƒä»¬ï¼Œåæ€è€…å¯¹é¢„æµ‹è¿›è¡Œè¯„ä¼°å¹¶æä¾›å»ºè®¾æ€§åé¦ˆï¼ŒåŒæ—¶è®°å¿†å­˜æ¡£ä¿ç•™å…ˆå‰è®¾è®¡ä»¥ä¾›é‡ç”¨å’Œè¿­ä»£ä¼˜åŒ–ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒMedDCRä¼˜äºæœ€æ–°åŸºçº¿ï¼Œäº§ç”Ÿå¯è§£é‡Šã€å¯é€‚åº”çš„å·¥ä½œæµç¨‹ï¼Œæ›´å¥½åœ°åæ˜ å®é™…ç¼–ç å®è·µï¼Œæé«˜è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13361v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»ç–—ç¼–ç çš„é‡è¦æ€§åŠå…¶ä¸æ™®é€šæ–‡æœ¬åˆ†ç±»çš„åŒºåˆ«ï¼Œå®ƒéœ€è¦å°†ä¸´åºŠç¬”è®°ä¸­çš„è‡ªç”±æ–‡æœ¬è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„è¯Šæ–­å’Œç¨‹åºä»£ç ï¼Œç”¨äºè®¡è´¹ã€åŒ»é™¢è¿è¥å’ŒåŒ»å­¦ç ”ç©¶ã€‚ä¸ºäº†è§£å†³ç°æœ‰åŒ»ç–—ç¼–ç ç³»ç»Ÿéš¾ä»¥æ•æ‰ç°å®ä¸–ç•Œæ–‡æ¡£ä¸­çš„ç»†å¾®å·®åˆ«å’Œå˜åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMedDCRçš„é—­ç¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å·¥ä½œæµç¨‹è®¾è®¡ä½œä¸ºå­¦ä¹ é—®é¢˜æ¥å¤„ç†ï¼ŒåŒ…æ‹¬è®¾è®¡å¸ˆæå‡ºå·¥ä½œæµç¨‹ã€ç¼–ç å™¨æ‰§è¡Œå·¥ä½œæµç¨‹ã€åå°„å™¨è¯„ä¼°é¢„æµ‹å¹¶æä¾›å»ºè®¾æ€§åé¦ˆï¼Œä»¥åŠè®°å¿†å­˜æ¡£ä¿ç•™å…ˆå‰è®¾è®¡ä»¥ä¾›é‡ç”¨å’Œè¿­ä»£ä¼˜åŒ–ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒMedDCRä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯åŸºçº¿ï¼Œå¹¶äº§ç”Ÿå¯è§£é‡Šã€å¯é€‚åº”çš„å·¥ä½œæµç¨‹ï¼Œæ›´å¥½åœ°åæ˜ å®é™…ç¼–ç å®è·µï¼Œæé«˜è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—ç¼–ç æ˜¯å°†ä¸´åºŠç¬”è®°ä¸­çš„è‡ªç”±æ–‡æœ¬è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„è¯Šæ–­å’Œç¨‹åºä»£ç çš„è¿‡ç¨‹ï¼Œå¯¹è®¡è´¹ã€åŒ»é™¢è¿è¥å’ŒåŒ»å­¦ç ”ç©¶è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰çš„åŒ»ç–—ç¼–ç ç³»ç»Ÿå¤§å¤šä¾èµ–äºæ‰‹åŠ¨åˆ¶ä½œçš„å·¥ä½œæµç¨‹ï¼Œéš¾ä»¥é€‚åº”ç°å®ä¸–ç•Œä¸­æ–‡æ¡£çš„ç»†å¾®å·®åˆ«å’Œå˜åŒ–ã€‚</li>
<li>MedDCRæ¡†æ¶é€šè¿‡è®¾è®¡å¸ˆã€ç¼–ç å™¨å’Œåå°„å™¨çš„ååŒå·¥ä½œï¼Œå®ç°äº†åŒ»ç–—ç¼–ç å·¥ä½œæµç¨‹çš„è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–ã€‚</li>
<li>MedDCRæ¡†æ¶å…·æœ‰å¯è§£é‡Šæ€§å’Œé€‚åº”æ€§ï¼Œèƒ½å¤Ÿåæ˜ å®é™…çš„ç¼–ç å®è·µã€‚</li>
<li>MedDCRæ¡†æ¶é€šè¿‡è®°å¿†å­˜æ¡£åŠŸèƒ½ï¼Œå®ç°äº†å…ˆå‰è®¾è®¡çš„é‡ç”¨å’Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>MedDCRåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›æŠ€æœ¯åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1218510dfa67defa23141c1b584ec9f4" align="middle">
<img src="https://picx.zhimg.com/v2-8debf26c5852befb184a55e778aa7a61" align="middle">
<img src="https://picx.zhimg.com/v2-a0afee7cace89393548a3f0a49a333c6" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO"><a href="#Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO" class="headerlink" title="Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO"></a>Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO</h2><p><strong>Authors:Haoyang Hong, Jiajun Yin, Yuan Wang, Jingnan Liu, Zhe Chen, Ailing Yu, Ji Li, Zhiling Ye, Hansong Xiao, Yefei Chen, Hualei Zhou, Yun Yue, Minghui Yang, Chunxiao Guo, Junwei Liu, Peng Wei, Jinjie Gu</strong></p>
<p>Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.</p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é€šç”¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ä¸“ä¸šé¢†åŸŸçš„è®­ç»ƒï¼Œå®ƒä»¬å‡†ç¡®æ€§å—åˆ°å½±å“ã€‚å½“å‰è®­ç»ƒæ–¹æ³•é›†ä¸­ä¸ºç³»ç»Ÿå†…çš„æ‰€æœ‰æ™ºèƒ½ä½“è®­ç»ƒä¸€ä¸ªç»Ÿä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ä½†ç”±äºä¸åŒæ™ºèƒ½ä½“èƒŒåçš„åˆ†å¸ƒä¸åŒï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶æ€§èƒ½ã€‚å› æ­¤ï¼Œä½¿ç”¨ä¸åŒLLMè®­ç»ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåº”æ˜¯ä¸‹ä¸€æ­¥çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¸¦æ¥äº†ä¼˜åŒ–æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œæ™ºèƒ½ä½“è¿è¡Œé¢‘ç‡ä¸åŒï¼Œæ“ä½œæ¶‰åŠå„ç§å­æ™ºèƒ½ä½“è°ƒç”¨ï¼Œå¹¶ä¸”æ™ºèƒ½ä½“é€šå¸¸éƒ¨ç½²åœ¨ä¸åŒçš„æœåŠ¡å™¨ä¸Šï¼Œç ´åç«¯åˆ°ç«¯æ¢¯åº¦æµã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºM-GRPOï¼Œå®ƒæ˜¯Group Relative Policy Optimizationçš„åˆ†å±‚æ‰©å±•ï¼Œä¸“ä¸ºå…·æœ‰ä¸»æ™ºèƒ½ä½“ï¼ˆè§„åˆ’å™¨ï¼‰å’Œå¤šä¸ªå­æ™ºèƒ½ä½“ï¼ˆå¤šè½®å·¥å…·æ‰§è¡Œå™¨ï¼‰çš„å‚ç›´å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè€Œè®¾è®¡ã€‚M-GRPOè®¡ç®—ä¸»æ™ºèƒ½ä½“å’Œå­æ™ºèƒ½ä½“çš„ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ï¼Œä¿æŒåˆ†å±‚ä¿¡ç”¨åˆ†é…ã€‚å®ƒè¿˜å¼•å…¥äº†ä¸€ç§è½¨è¿¹å¯¹é½æ–¹æ¡ˆï¼Œå³ä½¿åœ¨å¯å˜å­æ™ºèƒ½ä½“è°ƒç”¨çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ç”Ÿæˆå›ºå®šå¤§å°çš„æ‰¹æ¬¡ã€‚æˆ‘ä»¬éƒ¨ç½²äº†ä¸€ä¸ªè§£è€¦çš„è®­ç»ƒç®¡é“ï¼Œæ™ºèƒ½ä½“åœ¨å•ç‹¬çš„æœåŠ¡å™¨ä¸Šè¿è¡Œå¹¶é€šè¿‡å…±äº«å­˜å‚¨äº¤æ¢æœ€å°ç»Ÿè®¡ä¿¡æ¯ã€‚è¿™èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè·¨æœåŠ¡å™¨åå‘ä¼ æ’­çš„æƒ…å†µä¸‹å®ç°å¯æ‰©å±•çš„è®­ç»ƒã€‚åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚GAIAã€XBench-DeepSearchå’ŒWebWalkerQAï¼‰çš„å®éªŒä¸­ï¼ŒM-GRPOå§‹ç»ˆä¼˜äºå•æ™ºèƒ½ä½“GRPOå’Œå¤šæ™ºèƒ½ä½“GRPOï¼ˆå¸¦æœ‰å†»ç»“å­æ™ºèƒ½ä½“ï¼‰ï¼Œæ˜¾ç¤ºå‡ºæé«˜çš„ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯¹é½å¼‚æ„è½¨è¿¹å¹¶åœ¨ä¸“ä¸šæ™ºèƒ½ä½“ä¹‹é—´è§£è€¦ä¼˜åŒ–æœ‰åŠ©äºå¢å¼ºå·¥å…·è¾…åŠ©æ¨ç†ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13288v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦è®¨è®ºäº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é€šç”¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å½“å‰è®­ç»ƒæ–¹å¼å­˜åœ¨çš„å±€é™æ€§ä½¿å¾—é’ˆå¯¹ä¸åŒç±»å‹çš„æ™ºèƒ½ä½“è®­ç»ƒç»Ÿä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½æ€§èƒ½å—é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä½¿ç”¨ä¸åŒçš„è¯­è¨€æ¨¡å‹æ¥è®­ç»ƒæ¯ä¸ªæ™ºèƒ½ä½“çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™å¸¦æ¥äº†ä¼˜åŒ–æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†M-GRPOæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºå‚ç›´å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„åˆ†å±‚æ‰©å±•ã€‚M-GRPOèƒ½å¤Ÿè®¡ç®—ä¸»æ™ºèƒ½ä½“å’Œå­æ™ºèƒ½ä½“çš„ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¿æŒåˆ†å±‚ä¿¡ç”¨åˆ†é…ï¼Œå¹¶å¼•å…¥è½¨è¿¹å¯¹é½æ–¹æ¡ˆä»¥ç”Ÿæˆå›ºå®šå¤§å°çš„æ‰¹æ¬¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM-GRPOåœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå•æ™ºèƒ½ä½“GRPOå’Œå¤šæ™ºèƒ½ä½“GRPOï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚è¿™è¡¨æ˜ï¼Œå¯¹é½å¼‚æ„è½¨è¿¹å¹¶åœ¨ä¸“ä¸šæ™ºèƒ½ä½“ä¸Šè§£è€¦ä¼˜åŒ–æœ‰åŠ©äºæé«˜å·¥å…·è¾…åŠ©æ¨ç†ä»»åŠ¡çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é€šç”¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸç¼ºä¹è®­ç»ƒä¼šå½±å“å…¶å‡†ç¡®æ€§ã€‚</li>
<li>å½“å‰è®­ç»ƒæ–¹æ³•æ˜¯ç»Ÿä¸€è®­ç»ƒæ‰€æœ‰æ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™å¯èƒ½é™åˆ¶äº†æ€§èƒ½ã€‚</li>
<li>é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæå‡ºä½¿ç”¨ä¸åŒçš„è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒä»¥åº”å¯¹ä¸åŒç±»å‹çš„æ™ºèƒ½ä½“ã€‚</li>
<li>è®­ç»ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ—¶é¢ä¸´ä¼˜åŒ–æŒ‘æˆ˜ï¼Œå¦‚æ“ä½œé¢‘ç‡ä¸åŒã€å­æ™ºèƒ½ä½“è°ƒç”¨ä¸åŒç­‰ã€‚</li>
<li>æå‡ºM-GRPOæ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ƒæ˜¯ä¸€ç§ä¸ºå‚ç›´å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„åˆ†å±‚æ‰©å±•ã€‚</li>
<li>M-GRPOèƒ½å¤Ÿè®¡ç®—ä¸»æ™ºèƒ½ä½“å’Œå­æ™ºèƒ½ä½“çš„ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ï¼Œå¹¶ä¿æŒä¿¡ç”¨åˆ†é…çš„å±‚æ¬¡ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-025ccf26e259c60a13108aff633b8b39" align="middle">
<img src="https://picx.zhimg.com/v2-f404ff05a47d9d14aaf1c0a4a474a8f1" align="middle">
<img src="https://picx.zhimg.com/v2-658f493b7f08405a48f19f9125a583e0" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MMD-Thinker-Adaptive-Multi-Dimensional-Thinking-for-Multimodal-Misinformation-Detection"><a href="#MMD-Thinker-Adaptive-Multi-Dimensional-Thinking-for-Multimodal-Misinformation-Detection" class="headerlink" title="MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection"></a>MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection</h2><p><strong>Authors:Junjie Wu, Guohong Fu</strong></p>
<p>Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.</p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„æ—¶ä»£ï¼Œå„ç§ç¤¾äº¤åª’ä½“ä¸Šçš„å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ³›æ»¥æˆç¾ï¼Œå¹¶ç»§ç»­æ¼”å˜ã€‚å‡ºç°é”™è¯¯ä¿¡æ¯çš„æˆæœ¬ä½ã€æ¬ºéª—æ€§é«˜ï¼Œå¯¹ç¤¾ä¼šé€ æˆäº†ä¸¥é‡å¨èƒã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€ä¿¡æ¯æ£€æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å®ƒä»¬é¢ä¸´ä¸¤ä¸ªå…³é”®çš„å±€é™æ€§ï¼šä¸€æ˜¯æ¨ç†ä¸è¶³ï¼Œé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸é‡‡ç”¨ç»Ÿä¸€çš„æ¨ç†æ¨¡å¼ï¼Œä½†ç”±äºç¼ºä¹å¤šæ¨¡æ€ä¿¡æ¯æ£€æµ‹ä»»åŠ¡çš„ç‰¹å®šçŸ¥è¯†ï¼Œå®ƒä»¬ä¼šç”Ÿæˆä¸å‡†ç¡®çš„è§£é‡Šå’Œåˆ¤æ–­ã€‚äºŒæ˜¯æ¨ç†åè§é—®é¢˜ï¼Œå•ä¸€æ€ç»´æ¨¡å¼çš„æ£€æµ‹å™¨åœ¨è¿›è¡Œåˆ¤æ–­æ—¶èµ°çš„æ˜¯ä¸€æ¡æ¬¡ä¼˜è·¯å¾„ï¼Œéš¾ä»¥è·Ÿä¸Šæ—¥ç›Šå¢é•¿çš„å¤æ‚å¤šå˜çš„å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹æ¡†æ¶MMD-Thinkerï¼Œé€šè¿‡è‡ªé€‚åº”çš„å¤šç»´åº¦æ€è€ƒè¿›è¡Œå¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é’ˆå¯¹å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹è®¾è®¡äº†ä¸“é—¨çš„æ€ç»´æ¨¡å¼ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é‡‡ç”¨ä»»åŠ¡ç‰¹å®šçš„æŒ‡ä»¤å¾®è°ƒï¼Œå°†ç‰¹å®šçš„æ€ç»´æ¨¡å¼æ³¨å…¥é€šç”¨çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚å†æ¬¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¹¶è®¾ç½®æ··åˆä¼˜åŠ¿å‡½æ•°ï¼Œåœ¨è·¯å¾„è½¨è¿¹ä¸­æ¿€åŠ±æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ¨ç†ï¼ˆMMRï¼‰æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡8Kçš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œä»¥åŠæ¨ç†è¿‡ç¨‹å’Œåˆ†ç±»æ ‡ç­¾ç­‰è¯¦ç»†ä¿¡æ¯ç­‰ä»¥å¸®åŠ©è¿›æ­¥çš„å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯çš„æ£€æµ‹é¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æå‡ºçš„MMD-Thinkeræ— è®ºæ˜¯åœ¨å†…éƒ¨é¢†åŸŸè¿˜æ˜¯å¤–éƒ¨é¢†åŸŸçš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½åŒæ—¶ä¿æŒäº†çµæ´»çš„æ¨ç†å’Œä»¤ç‰Œä½¿ç”¨ã€‚ä»£ç å°†åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13242v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹AIç”Ÿæˆå†…å®¹æ—¶ä»£å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ³›æ»¥çš„é—®é¢˜ï¼Œç°æœ‰é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹æ—¶å­˜åœ¨æ¨ç†ä¸è¶³å’Œæ¨ç†åè§ä¸¤ä¸ªå…³é”®å±€é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºMMD-Thinkeræ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”çš„å¤šç»´åº¦æ€è€ƒè¿›è¡Œå¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸ºå¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹é‡èº«å®šåˆ¶çš„æ¨ç†æ¨¡å¼ã€é€šè¿‡ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´å°†æ¨ç†æ¨¡å¼æ³¨å…¥é€šç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¿€åŠ±æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ¨ç†æ•°æ®é›†MMRï¼ŒåŒ…å«è¶…è¿‡8Kçš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMD-Thinkeråœ¨åŸŸå†…å’Œè·¨åŸŸåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒçµæ´»çš„æ¨ç†å’Œä»¤ç‰Œä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯åœ¨ç¤¾ä¼šä¸­æ„æˆé‡å¤§å¨èƒï¼Œå› å…¶ä½æˆæœ¬åˆ›å»ºå’Œé«˜æ¬ºéª—æ€§ã€‚</li>
<li>ç°æœ‰é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹ä¸­å­˜åœ¨æ¨ç†ä¸è¶³å’Œåè§é—®é¢˜ã€‚</li>
<li>MMD-Thinkeræ¡†æ¶é€šè¿‡è‡ªé€‚åº”çš„å¤šç»´åº¦æ€è€ƒæ¥è§£å†³è¿™äº›é—®é¢˜ï¼ŒåŒ…æ‹¬é‡èº«å®šåˆ¶çš„æ¨ç†æ¨¡å¼ã€ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„åº”ç”¨ã€‚</li>
<li>MMRæ•°æ®é›†çš„æ„å»ºä¸ºé¢†åŸŸå‘å±•æä¾›äº†æ”¯æŒï¼ŒåŒ…å«å›¾åƒæ–‡æœ¬å¯¹åŠå¯¹åº”çš„æ¨ç†è¿‡ç¨‹å’Œåˆ†ç±»æ ‡ç­¾ã€‚</li>
<li>MMD-Thinkeræ¡†æ¶åœ¨å®éªŒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ä»…é€‚ç”¨äºåŸŸå†…æ•°æ®ï¼Œä¹Ÿé€‚ç”¨äºè·¨åŸŸæ•°æ®ã€‚</li>
<li>MMD-Thinkeræ¡†æ¶å…·æœ‰çµæ´»çš„æ¨ç†å’Œä»¤ç‰Œä½¿ç”¨ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3561294a8b58744ac74bb0850d2bbb32" align="middle">
<img src="https://picx.zhimg.com/v2-c70ab4ece9005502db6df65919703a2b" align="middle">
<img src="https://picx.zhimg.com/v2-ea90c10990dbaba7342bef57db3fa42e" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TokenSqueeze-Performance-Preserving-Compression-for-Reasoning-LLMs"><a href="#TokenSqueeze-Performance-Preserving-Compression-for-Reasoning-LLMs" class="headerlink" title="TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs"></a>TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs</h2><p><strong>Authors:Yuxiang Zhang, Zhengxu Yu, Weihang Pan, Zhongming Jin, Qiang Fu, Deng Cai, Binbin Lin, Jieping Ye</strong></p>
<p>Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the modelâ€™s self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zhangyx1122/TokenSqueeze">https://github.com/zhangyx1122/TokenSqueeze</a>.</p>
<blockquote>
<p>æ–°å…´æ¨ç†LLMï¼ˆå¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼‰å¦‚OpenAI-o1å’ŒDeepSeek-R1é€šè¿‡ç”Ÿæˆé•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è½¨è¿¹ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†å¼ºåŠ²çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œè¿™äº›é•¿çš„æ€ç»´é“¾å¯¼è‡´ä»¤ç‰Œä½¿ç”¨é‡å¢åŠ ï¼Œè¿›è€Œå¯¼è‡´æ¨ç†å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—å¢åŠ ã€‚å› æ­¤ï¼Œåœ¨å®é™…éƒ¨ç½²æ¨ç†LLMæ—¶ï¼Œå¹³è¡¡å‡†ç¡®æ€§å’Œæ¨ç†æ•ˆç‡å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰çš„é•¿è½¬çŸ­ï¼ˆLong2Shortï¼‰æ–¹æ³•æ—¨åœ¨å‡å°‘æ¨ç†é•¿åº¦ï¼Œä½†å¾€å¾€ç‰ºç‰²äº†å‡†ç¡®æ€§ï¼Œè¿™å‡¸æ˜¾äº†ä¸€ç§æ—¢é™ä½ä»¤ç‰Œæˆæœ¬åˆèƒ½ç»´æŒæ€§èƒ½çš„æ–¹æ³•çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ•ˆç‡ä¸å‡†ç¡®åº¦çš„æƒè¡¡é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TokenSqueezeï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é•¿è½¬çŸ­æ–¹æ³•ï¼Œèƒ½åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‹ç¼©æ¨ç†è·¯å¾„ï¼Œå¹¶ä¸”ä»…ä¾èµ–è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®ã€‚é¦–å…ˆï¼Œä¸ºäº†é˜²æ­¢è¿‡åº¦å‹ç¼©æ¨ç†æ·±åº¦å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼Œæˆ‘ä»¬æå‡ºé€‰æ‹©è‡ªæˆ‘ç”Ÿæˆçš„æ ·æœ¬ï¼Œå…¶æ¨ç†æ·±åº¦ä¸é—®é¢˜çš„å¤æ‚æ€§ç›¸é€‚åº”ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–è¯­è¨€è¡¨è¾¾è€Œä¸æ”¹å˜åŸºæœ¬çš„æ¨ç†è·¯å¾„ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å¸ƒå¯¹é½çš„è¯­è¨€ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒæé«˜äº†æ¨ç†è·¯å¾„çš„æ¸…æ™°åº¦å’Œç®€æ´æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å…¶é€»è¾‘å®Œæ•´æ€§ã€‚å…¨é¢çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTokenSqueezeåœ¨å‡å°‘ä»¤ç‰Œä½¿ç”¨çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆåœ°ä¿æŒå‡†ç¡®æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¯¹DeepSeek-R1-Distill-Qwen-7Bè¿›è¡Œå¾®è°ƒåï¼Œå…¶åœ¨MATH500åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¹³å‡ä»¤ç‰Œå‡å°‘50%çš„åŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ã€‚TokenSqueezeä»…åˆ©ç”¨æ¨¡å‹è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®ï¼Œèƒ½åœ¨ä¸åŒåº”ç”¨ä¸­å®ç°é«˜æ•ˆå’Œé«˜ä¿çœŸçš„æ¨ç†ï¼Œæ— éœ€ä¾èµ–äººå·¥ç¼–åˆ¶çš„ç®€çŸ­ç­”æ¡ˆæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhangyx1122/TokenSqueeze%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/zhangyx1122/TokenSqueezeè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13223v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong><br>å¤§æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†ç”Ÿæˆçš„é•¿æ¨ç†é“¾ï¼ˆCoTï¼‰ä¼šå¯¼è‡´æ›´é«˜çš„æ¨ç†å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—ã€‚å› æ­¤ï¼Œåœ¨éƒ¨ç½²æ¨ç†å¤§æ¨¡å‹æ—¶ï¼Œéœ€è¦åœ¨å‡†ç¡®æ€§å’Œæ¨ç†æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç°æœ‰çš„é•¿çŸ­æ–¹æ³•æ—¨åœ¨å‡å°‘æ¨ç†é•¿åº¦ä½†ç‰ºç‰²äº†å‡†ç¡®æ€§ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½åœ¨é™ä½ä»¤ç‰Œæˆæœ¬çš„åŒæ—¶ä¿æŒæ€§èƒ½çš„æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TokenSqueezeï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é•¿çŸ­æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‹ç¼©æ¨ç†è·¯å¾„ï¼Œå¹¶ä¸”åªä¾èµ–äºè‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®ã€‚é€šè¿‡é€‰æ‹©è‡ªé€‚åº”åŒ¹é…é—®é¢˜å¤æ‚åº¦çš„è‡ªæˆ‘ç”Ÿæˆæ ·æœ¬ï¼Œé˜²æ­¢äº†è¿‡åº¦å‹ç¼©å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸åˆ†å¸ƒå¯¹é½çš„è¯­è¨€ç²¾ç‚¼æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–è¯­è¨€è¡¨è¾¾è€Œä¸æ”¹å˜åŸºæœ¬çš„æ¨ç†è·¯å¾„ï¼Œå¢å¼ºæ¨ç†è·¯å¾„çš„æ¸…æ™°åº¦å’Œç®€æ´æ€§ï¼ŒåŒæ—¶ä¿æŒå…¶é€»è¾‘å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTokenSqueezeåœ¨å‡å°‘ä»¤ç‰Œä½¿ç”¨çš„åŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•åœ¨DeepSeek-R1-Distill-Qwen-7Bä¸Šè¿›è¡Œå¾®è°ƒåï¼Œå®ç°äº†MATH500åŸºå‡†æµ‹è¯•ä¸Šä»¤ç‰Œå‡å°‘50%çš„å¹³å‡æˆç»©åŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚TokenSqueezeä»…åˆ©ç”¨æ¨¡å‹è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®ï¼Œèƒ½åœ¨å¤šç§åº”ç”¨ä¸Šå®ç°é«˜æ•ˆä¸”é«˜ä¿çœŸåº¦çš„æ¨ç†ï¼Œæ— éœ€ä¾èµ–æ‰‹åŠ¨ç¼–çº‚çš„çŸ­ç­”æ¡ˆæ•°æ®é›†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰çš„æ¨ç†å¤§æ¨¡å‹é¢ä¸´å‡†ç¡®æ€§å’Œæ¨ç†æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>TokenSqueezeæ˜¯ä¸€ç§æ–°çš„é•¿çŸ­æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€æƒè¡¡é—®é¢˜ï¼Œé€šè¿‡å‹ç¼©æ¨ç†è·¯å¾„æ¥é™ä½ä»¤ç‰Œä½¿ç”¨æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚</li>
<li>TokenSqueezeé€šè¿‡é€‰æ‹©è‡ªé€‚åº”åŒ¹é…é—®é¢˜å¤æ‚åº¦çš„è‡ªæˆ‘ç”Ÿæˆæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä»¥é˜²æ­¢æ€§èƒ½ä¸‹é™ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ä¸åˆ†å¸ƒå¯¹é½çš„è¯­è¨€ç²¾ç‚¼æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–è¯­è¨€è¡¨è¾¾åŒæ—¶ä¿æŒåŸºæœ¬çš„æ¨ç†è·¯å¾„å’Œé€»è¾‘å®Œæ•´æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºTokenSqueezeèƒ½æœ‰æ•ˆå‡å°‘ä»¤ç‰Œä½¿ç”¨åŒæ—¶ç»´æŒå‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨TokenSqueezeæ–¹æ³•çš„DeepSeek-R1-Distill-Qwen-7Bæ¨¡å‹åœ¨MATH500åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„ä»¤ç‰Œå‡å°‘æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3dd9e7b0b9eaa6c53e569244777e5951" align="middle">
<img src="https://picx.zhimg.com/v2-0b1cf145cc57efeaa190f658e92b12aa" align="middle">
<img src="https://picx.zhimg.com/v2-6bb267c5d1b718efd2a61f6f31150ef2" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PIGEON-VLM-Driven-Object-Navigation-via-Points-of-Interest-Selection"><a href="#PIGEON-VLM-Driven-Object-Navigation-via-Points-of-Interest-Selection" class="headerlink" title="PIGEON: VLM-Driven Object Navigation via Points of Interest Selection"></a>PIGEON: VLM-Driven Object Navigation via Points of Interest Selection</h2><p><strong>Authors:Cheng Peng, Zhenzhe Zhang, Cheng Chi, Xiaobao Wei, Yanhao Zhang, Heng Wang, Pengwei Wang, Zhongyuan Wang, Jing Liu, Shanghang Zhang</strong></p>
<p>Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the modelâ€™s semantic guidance capabilities, enabling deep reasoning during real-time navigation.</p>
<blockquote>
<p>åœ¨æœªçŸ¥ç¯å¢ƒä¸­å¯¼èˆªåˆ°æŒ‡å®šå¯¹è±¡æ˜¯å…·æœ‰èº«ä½“æ™ºèƒ½çš„åŸºæœ¬ä½†å…·æœ‰æŒ‘æˆ˜çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨å†³ç­–é¢‘ç‡ä¸æ™ºèƒ½ä¹‹é—´éš¾ä»¥å–å¾—å¹³è¡¡ï¼Œå¯¼è‡´ç¼ºä¹è¿œè§æˆ–è¡ŒåŠ¨ä¸è¿è´¯çš„å†³ç­–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºPIGEONï¼šä»¥å…´è¶£ç‚¹ä¸ºå¯¼å‘çš„VLMå¯¹è±¡å¯¼èˆªæ¢ç´¢ï¼Œåœ¨æ¢ç´¢è¿‡ç¨‹ä¸­ä¿æŒè½»é‡çº§ä¸”è¯­ä¹‰å¯¹é½çš„å¿«ç…§å†…å­˜ä½œä¸ºæ¢ç´¢ç­–ç•¥çš„è¯­ä¹‰è¾“å…¥ã€‚æˆ‘ä»¬ä½¿ç”¨åä¸ºPIGEON-VLçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¥é€‰æ‹©æ¢ç´¢è¿‡ç¨‹ä¸­å½¢æˆçš„å…´è¶£ç‚¹ï¼ˆPoIï¼‰ï¼Œç„¶åé‡‡ç”¨è¾ƒä½å±‚æ¬¡çš„è§„åˆ’å™¨è¿›è¡ŒåŠ¨ä½œè¾“å‡ºï¼Œä»¥æé«˜å†³ç­–é¢‘ç‡ã€‚æ­¤å¤–ï¼Œè¿™ç§åŸºäºå…´è¶£ç‚¹çš„å†³ç­–ç”Ÿæˆé€‚åˆæ¨¡æ‹Ÿå™¨çš„å¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ•°æ®ã€‚åœ¨ç»å…¸å¯¹è±¡å¯¼èˆªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬è¿ç§»æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€ŒRLVRè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„è¯­ä¹‰å¼•å¯¼èƒ½åŠ›ï¼Œå®ç°äº†å®æ—¶å¯¼èˆªè¿‡ç¨‹ä¸­çš„æ·±åº¦æ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13207v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†PIGEONç³»ç»Ÿï¼Œåˆ©ç”¨ç‚¹å…´è¶£å¼•å¯¼æ¢ç´¢å®ç°ç›®æ ‡å¯¼èˆªã€‚é€šè¿‡ç»´æŠ¤è½»é‡çº§è¯­ä¹‰å¯¹é½çš„å¿«ç…§è®°å¿†ï¼ŒPIGEONé‡‡ç”¨å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œç‚¹å…´è¶£ï¼ˆPoIï¼‰é€‰æ‹©ï¼Œæé«˜å†³ç­–é¢‘ç‡ã€‚æ­¤å¤–ï¼ŒåŸºäºPoIçš„å†³ç­–åˆ¶å®šèƒ½å¤Ÿç”Ÿæˆé€‚ç”¨äºæ¨¡æ‹Ÿå™¨çš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œé›¶æ ·æœ¬è¿ç§»æ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„ç›®æ ‡å¯¼èˆªæ€§èƒ½ï¼ŒRLVRå¢å¼ºäº†æ¨¡å‹çš„è¯­ä¹‰å¼•å¯¼èƒ½åŠ›ï¼Œå®ç°å®æ—¶å¯¼èˆªçš„æ·±åº¦æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PIGEONç³»ç»Ÿä½¿ç”¨ç‚¹å…´è¶£ï¼ˆPoIï¼‰å¼•å¯¼æ¢ç´¢ï¼Œå®ç°åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„ç›®æ ‡å¯¼èˆªã€‚</li>
<li>é€šè¿‡ç»´æŠ¤è½»é‡çº§è¯­ä¹‰å¯¹é½çš„å¿«ç…§è®°å¿†ï¼Œæé«˜å†³ç­–é¢‘ç‡å’Œæ™ºèƒ½å¹³è¡¡ã€‚</li>
<li>åˆ©ç”¨å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡ŒPoIé€‰æ‹©ã€‚</li>
<li>åŸºäºPoIçš„å†³ç­–åˆ¶å®šå¯ä»¥ç”Ÿæˆé€‚ç”¨äºæ¨¡æ‹Ÿå™¨çš„å¼ºåŒ–å­¦ä¹ æ•°æ®ã€‚</li>
<li>é›¶æ ·æœ¬è¿ç§»æ–¹æ³•å–å¾—æœ€å…ˆè¿›çš„ç›®æ ‡å¯¼èˆªæ€§èƒ½ã€‚</li>
<li>RLVRæŠ€æœ¯å¢å¼ºäº†æ¨¡å‹çš„è¯­ä¹‰å¼•å¯¼èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-461015cc30759ff67aeebbf30cf31ccb" align="middle">
<img src="https://picx.zhimg.com/v2-b3d7c1895be02f00ec004d241cd667fd" align="middle">
<img src="https://picx.zhimg.com/v2-a56e1a58692a2514fce9b0793b34d4c6" align="middle">
<img src="https://picx.zhimg.com/v2-ffe073e8f4c8b81225292f9055d0dcbe" align="middle">
<img src="https://picx.zhimg.com/v2-c86b687fc8e3d659eff5658ad5330839" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Video-Spatial-Reasoning-with-Object-Centric-3D-Rollout"><a href="#Video-Spatial-Reasoning-with-Object-Centric-3D-Rollout" class="headerlink" title="Video Spatial Reasoning with Object-Centric 3D Rollout"></a>Video Spatial Reasoning with Object-Centric 3D Rollout</h2><p><strong>Authors:Haoran Tang, Meng Cao, Ruyang Liu, Xiaoxi Liang, Linglong Li, Ge Li, Xiaodan Liang</strong></p>
<p>Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCRâ€™s superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).</p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ç°ç¨³å¥çš„è§†é¢‘ç©ºé—´æ¨ç†â€”â€”åœ¨åŠ¨æ€3Dåœºæ™¯ä¸­ç†è§£ç‰©ä½“ä½ç½®ã€æ–¹å‘å’Œç‰©ä½“é—´å…³ç³»çš„èƒ½åŠ›ï¼Œä»ç„¶æ˜¯å…³é”®æœªè§£å†³çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç©ºé—´åŸºç¡€ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ï¼Œä½†æˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹é€šå¸¸è¡¨ç°å‡ºæŸ¥è¯¢é”å®šæ¨ç†ï¼Œåªå…³æ³¨æç¤ºä¸­æ˜ç¡®æåˆ°çš„ç‰©ä½“ï¼Œè€Œå¿½è§†å…³é”®ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Object-Centric 3D Rolloutï¼ˆOCRï¼‰è¿™ä¸€æ–°ç­–ç•¥ï¼Œå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é€‰å®šç‰©ä½“çš„3Då‡ ä½•ç»“æ„è¿›è¡Œç»“æ„åŒ–æ‰°åŠ¨ã€‚é€šè¿‡é€€åŒ–ç‰¹å®šç‰©ä½“çš„è§†è§‰çº¿ç´¢ï¼Œå¹¶å°†æ”¹å˜çš„å‡ ä½•ç»“æ„æŠ•å½±åˆ°äºŒç»´ç©ºé—´ï¼ŒOCRè¿«ä½¿æ¨¡å‹åœ¨æ•´ä¸ªåœºæ™¯ä¸­è¿›è¡Œæ•´ä½“æ¨ç†ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§åŸºäºæ»šåŠ¨è®­ç»ƒçš„ç®¡é“ï¼Œè”åˆä½¿ç”¨æ™®é€šå’ŒåŒºåŸŸå™ªå£°è§†é¢‘æ¥ä¼˜åŒ–ç©ºé—´æ¨ç†è½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½å¤„äºé¢†å…ˆæ°´å¹³ï¼šæˆ‘ä»¬çš„å‚æ•°è§„æ¨¡ä¸º3Bçš„æ¨¡å‹åœ¨VSI-Benchä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†47.5%ï¼Œè¶…è¿‡äº†å¤šä¸ªå‚æ•°è§„æ¨¡ä¸º7Bçš„åŸºçº¿æ¨¡å‹ã€‚å¯¹æ¯”å®éªŒè¯å®äº†OCRåœ¨ä¹‹å‰çš„æ»šåŠ¨ç­–ç•¥ï¼ˆå¦‚T-GRPOã€NoisyRolloutï¼‰ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13190v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚ç„¶è€Œï¼Œå®ç°ç¨³å¥çš„è§†é¢‘ç©ºé—´æ¨ç†ä»æ˜¯å…³é”®æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç©ºé—´ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ã€‚ä¸ºè§£å†³æ­¤å±€é™ï¼Œæœ¬æ–‡æå‡ºObject-Centric 3D Rolloutï¼ˆOCRï¼‰ç­–ç•¥ï¼Œé€šè¿‡ç»“æ„åŒ–æ‰°åŠ¨è®­ç»ƒæ—¶é€‰å®šå¯¹è±¡çš„3Då‡ ä½•å½¢çŠ¶ï¼Œæé«˜æ¨¡å‹çš„å…¨å±€åœºæ™¯æ¨ç†èƒ½åŠ›ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨VSI-Benchä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†47.5%ï¼Œä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMsåœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è§†é¢‘ç©ºé—´æ¨ç†ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç©ºé—´ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ï¼Œä½†å­˜åœ¨æŸ¥è¯¢é”å®šæ¨ç†é—®é¢˜ã€‚</li>
<li>OCRç­–ç•¥é€šè¿‡ç»“æ„åŒ–æ‰°åŠ¨é€‰å®šå¯¹è±¡çš„3Då‡ ä½•å½¢çŠ¶ï¼Œæé«˜æ¨¡å‹çš„å…¨å±€åœºæ™¯æ¨ç†èƒ½åŠ›ã€‚</li>
<li>OCRé€šè¿‡é™è§£ç‰¹å®šå¯¹è±¡çš„è§†è§‰çº¿ç´¢å¹¶å°†æ›´æ”¹åçš„å‡ ä½•å½¢çŠ¶æŠ•å½±åˆ°2Dç©ºé—´æ¥å·¥ä½œã€‚</li>
<li>è®¾è®¡äº†åŸºäºrolloutçš„è®­ç»ƒç®¡é“ï¼Œåˆ©ç”¨æ™®é€šå’ŒåŒºåŸŸå™ªå£°è§†é¢‘ä¼˜åŒ–ç©ºé—´æ¨ç†è½¨è¿¹ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒOCRæ–¹æ³•åœ¨VSI-Benchä¸Šè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º47.5%ï¼Œä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-002fa669033bdcdee240e5db83c21f39" align="middle">
<img src="https://picx.zhimg.com/v2-62d53ce08ab8a6852c2e9d0cdba59901" align="middle">
<img src="https://picx.zhimg.com/v2-021e5e51271c7193e6a34f2b3946b77f" align="middle">
<img src="https://picx.zhimg.com/v2-62b8c2b68b297c9a6acff38a32fa6f08" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TCM-5CEval-Extended-Deep-Evaluation-Benchmark-for-LLMâ€™s-Comprehensive-Clinical-Research-Competence-in-Traditional-Chinese-Medicine"><a href="#TCM-5CEval-Extended-Deep-Evaluation-Benchmark-for-LLMâ€™s-Comprehensive-Clinical-Research-Competence-in-Traditional-Chinese-Medicine" class="headerlink" title="TCM-5CEval: Extended Deep Evaluation Benchmark for LLMâ€™s Comprehensive Clinical Research Competence in Traditional Chinese Medicine"></a>TCM-5CEval: Extended Deep Evaluation Benchmark for LLMâ€™s Comprehensive Clinical Research Competence in Traditional Chinese Medicine</h2><p><strong>Authors:Tianai Huang, Jiayuan Chen, Lu Lu, Pengcheng Chen, Tianbin Li, Bing Han, Wenchao Tang, Jie Xu, Ming Li</strong></p>
<p>Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek_r1 and gemini_2_5_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the â€œIn-depth Challenge for Comprehensive TCM Abilitiesâ€ special track.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨é¢†åŸŸè¡¨ç°å‡ºäº†å‡ºè‰²çš„èƒ½åŠ›ï¼Œç„¶è€Œï¼Œå®ƒä»¬åœ¨ä¼ ç»Ÿä¸­åŒ»ï¼ˆTCMï¼‰ç­‰é«˜åº¦ä¸“ä¸šåŒ–å’Œæ–‡åŒ–ä¸°å¯Œçš„é¢†åŸŸçš„åº”ç”¨ï¼Œéœ€è¦è¿›è¡Œä¸¥æ ¼å’Œç»†è‡´çš„è¯„ä»·ã€‚åŸºäºå…ˆå‰çš„åŸºç¡€å·¥ä½œï¼Œå¦‚TCM-3CEvalï¼Œå®ƒå¼ºè°ƒäº†ç³»ç»ŸçŸ¥è¯†å·®è·å’Œæ–‡åŒ–èƒŒæ™¯å¯¹é½çš„é‡è¦æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†TCM-5CEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´ç²¾ç»†å’Œå…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚TCM-5CEvalæ—¨åœ¨è¯„ä¼°LLMåœ¨äº”ä¸ªå…³é”®é¢†åŸŸçš„èƒ½åŠ›ï¼šï¼ˆ1ï¼‰æ ¸å¿ƒçŸ¥è¯†ï¼ˆTCM-Examï¼‰ã€ï¼ˆ2ï¼‰å¤å…¸æ–‡å­¦ç´ å…»ï¼ˆTCM-LitQAï¼‰ã€ï¼ˆ3ï¼‰ä¸´åºŠå†³ç­–ï¼ˆTCM-MRCDï¼‰ã€ï¼ˆ4ï¼‰ä¸­è¯çŸ¥è¯†ï¼ˆTCM-CMMï¼‰ï¼Œä»¥åŠï¼ˆ5ï¼‰ä¸´åºŠéè¯ç‰©æ²»ç–—ï¼ˆTCM-ClinNPTï¼‰ã€‚æˆ‘ä»¬å¯¹åäº”ä¸ªçªå‡ºçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°äº†æ€§èƒ½ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œå¹¶å‘ç°äº†è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå¦‚deepseek_r1å’Œgemini_2_5_proã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶æ¨¡å‹åœ¨å›å¿†åŸºç¡€çŸ¥è¯†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§£é‡Šå¤å…¸æ–‡æœ¬çš„å¤æ‚æ€§æ–¹é¢å´æ„Ÿåˆ°å›°éš¾ã€‚å…³é”®çš„æ˜¯ï¼ŒåŸºäºæ’åˆ—ç»„åˆçš„çš„ä¸€è‡´æ€§æµ‹è¯•æ­ç¤ºäº†æ¨¡å‹æ¨ç†çš„å¹¿æ³›è„†å¼±æ€§ã€‚æ‰€æœ‰ç»è¿‡è¯„ä¼°çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬å¾—åˆ†æœ€é«˜çš„æ¨¡å‹ï¼Œåœ¨é¢å¯¹ä¸åŒçš„æé—®é€‰é¡¹é¡ºåºæ—¶ï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œè¿™è¡¨æ˜å®ƒä»¬å¯¹ä½ç½®åå·®æ•æ„Ÿï¼Œç¼ºä¹ç¨³å¥çš„ç†è§£ã€‚TCM-5CEvalä¸ä»…ä¸ºLLMåœ¨TCMé¢†åŸŸçš„èƒ½åŠ›æä¾›äº†æ›´è¯¦ç»†çš„è¯Šæ–­å·¥å…·ï¼Œè¿˜æš´éœ²äº†å…¶æ¨ç†ç¨³å®šæ€§æ–¹é¢çš„æ ¹æœ¬å¼±ç‚¹ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ ‡å‡†åŒ–æ¯”è¾ƒï¼ŒTCM-5CEvalå·²ä¸Šä¼ åˆ°Medbenchå¹³å°ï¼ŒåŠ å…¥å…¶å‰èº«â€œä¸­åŒ»ç»¼åˆèƒ½åŠ›æ·±åº¦æŒ‘æˆ˜â€ä¸“é¡¹èµ›é“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13169v1">PDF</a> 17 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨é¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨ä¼ ç»Ÿä¸­åŒ»ï¼ˆTCMï¼‰ç­‰é«˜åº¦ä¸“ä¸šåŒ–å’Œæ–‡åŒ–ä¸°å¯Œçš„é¢†åŸŸçš„åº”ç”¨ï¼Œéœ€è¦è¿›è¡Œä¸¥æ ¼å’Œç»†è‡´çš„è¯„ä»·ã€‚åŸºäºTCM-3CEvalç­‰å‰æœŸåŸºç¡€å·¥ä½œï¼Œæˆ‘ä»¬æ¨å‡ºäº†TCM-5CEvalï¼Œä¸€ä¸ªæ›´ç²¾ç»†å’Œå…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚TCM-5CEvalæ—¨åœ¨è¯„ä¼°LLMåœ¨äº”ä¸ªå…³é”®é¢†åŸŸçš„èƒ½åŠ›ï¼šï¼ˆ1ï¼‰æ ¸å¿ƒçŸ¥è¯†ï¼ˆTCM-Examï¼‰ã€ï¼ˆ2ï¼‰å¤å…¸æ–‡å­¦ï¼ˆTCM-LitQAï¼‰ã€ï¼ˆ3ï¼‰ä¸´åºŠå†³ç­–ï¼ˆTCM-MRCDï¼‰ã€ï¼ˆ4ï¼‰ä¸­è¯ææ–™ï¼ˆTCM-CMMï¼‰å’Œï¼ˆ5ï¼‰ä¸´åºŠéè¯ç‰©æ²»ç–—ï¼ˆTCM-ClinNPTï¼‰ã€‚æˆ‘ä»¬å¯¹åäº”ä¸ªçªå‡ºçš„LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œå¹¶ç¡®å®šäº†è¡¨ç°æœ€ä½³æ¨¡å‹å¦‚deepseek_r1å’Œgemini_2_5_proã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶æ¨¡å‹åœ¨å›å¿†åŸºç¡€çŸ¥è¯†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§£é‡Šå¤å…¸æ–‡æœ¬çš„å¤æ‚æ€§æ–¹é¢å´é‡åˆ°å›°éš¾ã€‚é‡è¦çš„æ˜¯ï¼ŒåŸºäºæ’åˆ—ç»„åˆçš„çš„ä¸€è‡´æ€§æµ‹è¯•æ­ç¤ºäº†æ¨¡å‹æ¨ç†çš„æ™®éè„†å¼±æ€§ã€‚æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬å¾—åˆ†æœ€é«˜çš„æ¨¡å‹ï¼Œåœ¨é¢å¯¹ä¸åŒçš„æé—®é€‰é¡¹é¡ºåºæ—¶ï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œè¿™è¡¨æ˜å®ƒä»¬å¯¹ä½ç½®åå·®æ•æ„Ÿï¼Œç¼ºä¹ç¨³å¥çš„ç†è§£èƒ½åŠ›ã€‚TCM-5CEvalä¸ä»…ä¸ºLLMåœ¨TCMä¸­çš„èƒ½åŠ›æä¾›äº†æ›´è¯¦ç»†çš„è¯Šæ–­å·¥å…·ï¼Œè¿˜æš´éœ²äº†å…¶æ¨ç†ç¨³å®šæ€§çš„æ ¹æœ¬å¼±ç‚¹ã€‚ä¸ºä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ ‡å‡†åŒ–æ¯”è¾ƒï¼ŒTCM-5CEvalå·²ä¸Šä¼ åˆ°Medbenchå¹³å°ï¼ŒåŠ å…¥å…¶â€œæ·±åº¦æŒ‘æˆ˜ä¸­åŒ»ç»¼åˆèƒ½åŠ›â€ä¸“é¡¹èµ›é“çš„å‰æœŸåŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸­åŒ»é¢†åŸŸçš„ä¸“ä¸šèƒ½åŠ›è¯„ä¼°éœ€è¦æ›´ç²¾ç»†å’Œå…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>TCM-5CEvalè¦†ç›–äº†LLMåœ¨äº”ä¸ªå…³é”®ä¸­åŒ»é¢†åŸŸçš„èƒ½åŠ›è¯„ä¼°ï¼ŒåŒ…æ‹¬æ ¸å¿ƒçŸ¥è¯†ã€å¤å…¸æ–‡å­¦ã€ä¸´åºŠå†³ç­–ã€ä¸­è¯ææ–™ä»¥åŠä¸´åºŠéè¯ç‰©æ²»ç–—ã€‚</li>
<li>é€šè¿‡å¯¹åäº”ä¸ªçªå‡ºçš„LLMè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œéƒ¨åˆ†æ¨¡å‹å¦‚deepseek_r1å’Œgemini_2_5_proè¡¨ç°æœ€ä½³ã€‚</li>
<li>æ¨¡å‹åœ¨å›å¿†åŸºç¡€çŸ¥è¯†æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è§£é‡Šå¤å…¸æ–‡æœ¬çš„å¤æ‚æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æ¨¡å‹æ¨ç†å­˜åœ¨æ™®éè„†å¼±æ€§ï¼Œå¯¹ä½ç½®åå·®æ•æ„Ÿï¼Œç¼ºä¹ç¨³å¥çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>TCM-5CEvalæä¾›äº†ä¸€ä¸ªè¯¦ç»†çš„è¯Šæ–­å·¥å…·æ¥è¯„ä¼°LLMåœ¨ä¸­åŒ»é¢†åŸŸçš„èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºäº†å…¶æ¨ç†ç¨³å®šæ€§çš„å¼±ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8c87a949f09ff62ecbaffd4d814f6d0" align="middle">
<img src="https://picx.zhimg.com/v2-fc8d0726ca4eb9273cfc0dea726e4209" align="middle">
<img src="https://picx.zhimg.com/v2-188955e90b00df7bcd7f3bd26cc73ddd" align="middle">
<img src="https://picx.zhimg.com/v2-ec6e5ec43239388d95f1b7348f06a0b5" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Conditional-Diffusion-Model-for-Multi-Agent-Dynamic-Task-Decomposition"><a href="#Conditional-Diffusion-Model-for-Multi-Agent-Dynamic-Task-Decomposition" class="headerlink" title="Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition"></a>Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition</h2><p><strong>Authors:Yanda Zhu, Yuanyang Zhu, Daoyi Dong, Caihua Chen, Chunlin Chen</strong></p>
<p>Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.</p>
<blockquote>
<p>ä»»åŠ¡åˆ†è§£åœ¨å¤æ‚çš„åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½¿åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­çš„é•¿æœŸä»»åŠ¡èƒ½å¤Ÿè¿›è¡Œé«˜æ•ˆåˆ†å±‚å­¦ä¹ ã€‚ç„¶è€Œï¼Œä»å¤´å¼€å§‹å­¦ä¹ åŠ¨æ€ä»»åŠ¡åˆ†è§£é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ ·æœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨éƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹æ¢ç´¢å·¨å¤§çš„è”åˆåŠ¨ä½œç©ºé—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€ä»»åŠ¡åˆ†è§£çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆC$\text{D}^\text{3}$Tï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤çº§åˆ†å±‚MARLæ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨æ¨æ–­å­ä»»åŠ¡å’Œåè°ƒæ¨¡å¼ã€‚é«˜çº§ç­–ç•¥å­¦ä¹ å­ä»»åŠ¡è¡¨ç¤ºï¼ŒåŸºäºå­ä»»åŠ¡æ•ˆæœç”Ÿæˆå­ä»»åŠ¡é€‰æ‹©ç­–ç•¥ã€‚ä¸ºäº†æ•æ‰å­ä»»åŠ¡å¯¹ç¯å¢ƒçš„å½±å“ï¼ŒC$\text{D}^\text{3}$Tä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè§‚å¯Ÿå’Œå¥–åŠ±ã€‚åœ¨ä½çº§ï¼Œæ™ºèƒ½ä½“åœ¨å…¶åˆ†é…çš„å­ä»»åŠ¡å†…åä½œå­¦ä¹ å¹¶å…±äº«ä¸“ä¸šæŠ€èƒ½ã€‚æ­¤å¤–ï¼Œå­¦åˆ°çš„å­ä»»åŠ¡è¡¨ç¤ºè¿˜ç”¨ä½œå¤šå¤´æ³¨æ„åŠ›æ··åˆç½‘ç»œä¸­çš„é™„åŠ è¯­ä¹‰ä¿¡æ¯ï¼Œä»¥å¢å¼ºä»·å€¼åˆ†è§£ï¼Œå¹¶åœ¨ä¸ªä½“å’Œè”åˆä»·å€¼å‡½æ•°ä¹‹é—´æä¾›æœ‰æ•ˆçš„æ¨ç†æ¡¥æ¢ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒC$\text{D}^\text{3}$Tçš„æ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13137v1">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤æ‚åˆä½œçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä»»åŠ¡ä¸­ï¼Œä»»åŠ¡åˆ†è§£æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ï¼Œèƒ½å¤Ÿå®ç°å¯¹é•¿å‘¨æœŸä»»åŠ¡çš„åŠ¨æ€åˆ†å±‚å­¦ä¹ ï¼Œå¹¶é€‚ç”¨äºåŠ¨æ€å’Œä¸ç¡®å®šçš„ç¯å¢ƒã€‚æœ¬æ–‡æå‡ºäº†åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åŠ¨æ€ä»»åŠ¡åˆ†è§£ï¼ˆC$\text{D}^\text{3}$Tï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä¸¤çº§åˆ†å±‚MARLæ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨æ¨æ–­å­ä»»åŠ¡å’Œåè°ƒæ¨¡å¼ã€‚é«˜çº§ç­–ç•¥å­¦ä¹ å­ä»»åŠ¡è¡¨ç¤ºï¼ŒåŸºäºå­ä»»åŠ¡æ•ˆæœç”Ÿæˆå­ä»»åŠ¡é€‰æ‹©ç­–ç•¥ã€‚C$\text{D}^\text{3}$Té€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªè§‚æµ‹å€¼å’Œå¥–åŠ±å€¼æ¥æ•è·å­ä»»åŠ¡å¯¹ç¯å¢ƒçš„å½±å“ï¼Œè€Œä½çº§æ™ºèƒ½ä½“åˆ™åœ¨åˆ†é…çš„å­ä»»åŠ¡å†…å­¦ä¹ å’Œå…±äº«ä¸“ä¸šæŠ€èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒC$\text{D}^\text{3}$Tåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»»åŠ¡åˆ†è§£åœ¨å¤æ‚åˆä½œçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½å‰æ™¯ï¼Œèƒ½æå‡å¯¹é•¿å‘¨æœŸä»»åŠ¡çš„åˆ†å±‚å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>C$\text{D}^\text{3}$Tæ˜¯ä¸€ç§æ–°é¢–çš„ä¸¤å±‚åˆ†çº§MARLæ¡†æ¶ï¼Œå¯è‡ªåŠ¨æ¨æ–­å­ä»»åŠ¡å’Œåè°ƒæ¨¡å¼ã€‚</li>
<li>é«˜çº§ç­–ç•¥é€šè¿‡åŸºäºå­ä»»åŠ¡æ•ˆæœçš„è¡¨ç¤ºå­¦ä¹ ç”Ÿæˆå­ä»»åŠ¡é€‰æ‹©ç­–ç•¥ã€‚</li>
<li>C$\text{D}^\text{3}$Té€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªè§‚æµ‹å€¼å’Œå¥–åŠ±å€¼æ¥æ•è·å­ä»»åŠ¡å¯¹ç¯å¢ƒçš„å½±å“ã€‚</li>
<li>åœ¨ä½çº§å±‚é¢ï¼Œæ™ºèƒ½ä½“åœ¨åˆ†é…çš„å­ä»»åŠ¡å†…å­¦ä¹ å’Œå…±äº«ä¸“ä¸šæŠ€èƒ½ã€‚</li>
<li>C$\text{D}^\text{3}$Tå°†ä¹ å¾—çš„å­ä»»åŠ¡è¡¨ç¤ºç”¨äºå¤šå¤´éƒ¨æ³¨æ„åŠ›æ··åˆç½‘ç»œä¸­çš„é¢å¤–è¯­ä¹‰ä¿¡æ¯ï¼Œå¢å¼ºä»·å€¼åˆ†è§£å¹¶æä¾›ä¸ªä½“ä¸è”åˆä»·å€¼å‡½æ•°ä¹‹é—´çš„æœ‰æ•ˆæ¨ç†æ¡¥æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cd0b37280f097b8359e9cb92bf94dd7" align="middle">
<img src="https://picx.zhimg.com/v2-4f7f263c4be05f60ecf7900fe3cf1e40" align="middle">
<img src="https://picx.zhimg.com/v2-cae5787bc6363fd532a3e7cc30406b8b" align="middle">
<img src="https://picx.zhimg.com/v2-36d01a203c91f7ef747001f473a49b3f" align="middle">
<img src="https://picx.zhimg.com/v2-9eeefbe1c380e3653089f4f255aff19f" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="BeDiscovER-The-Benchmark-of-Discourse-Understanding-in-the-Era-of-Reasoning-Language-Models"><a href="#BeDiscovER-The-Benchmark-of-Discourse-Understanding-in-the-Era-of-Reasoning-Language-Models" class="headerlink" title="BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models"></a>BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models</h2><p><strong>Authors:Chuyuan Li, Giuseppe Carenini</strong></p>
<p>We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., &#96;&#96;justâ€™â€™), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.</p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†BeDiscovERï¼ˆæ—¶ä»£æ¨ç†è¯­è¨€æ¨¡å‹ä¸­çš„è¯è¯­ç†è§£åŸºå‡†æµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€å¥—æœ€æ–°ä¸”å…¨é¢çš„è¯„ä¼°ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯è¯­å±‚é¢çŸ¥è¯†çš„å·¥å…·ã€‚BeDiscovERæ±‡é›†äº†5ä¸ªå…¬å¼€å¯ç”¨çš„è¯è¯­ä»»åŠ¡ï¼Œæ¶µç›–è¯è¯­è¯æ±‡ã€ï¼ˆå¤šï¼‰å¥å­å’Œæ–‡æ¡£å±‚é¢ï¼Œæ€»è®¡52ä¸ªå•ç‹¬çš„æ•°æ®é›†ã€‚å®ƒæ¶µç›–äº†å¹¿æ³›ç ”ç©¶çš„è¯è¯­ä»»åŠ¡ï¼Œå¦‚è¯è¯­è§£æå’Œæ—¶é—´å…³ç³»æå–ï¼Œä»¥åŠä¸€äº›æ–°é¢–çš„æŒ‘æˆ˜ï¼Œå¦‚è¯è¯­ç²’å­æ¶ˆæ­§ï¼ˆä¾‹å¦‚ï¼Œâ€œåªæ˜¯â€ï¼‰ï¼Œå¹¶æ±‡æ€»äº†å…³äºå¤šè¯­è¨€å’Œå¤šæ¡†æ¶è¯è¯­å…³ç³»åˆ†ç±»çš„Discourse Relation Parsingå’ŒTreebankingå…±äº«ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨BeDiscovERä¸Šè¯„ä¼°äº†å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼šQwen3ç³»åˆ—ã€DeepSeek-R1ä»¥åŠå‰æ²¿æ¨¡å‹å¦‚GPT-5-miniï¼Œå‘ç°æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ—¶åºæ¨ç†çš„ç®—æœ¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å…¨æ–‡æ¨ç†å’Œä¸€äº›å¾®å¦™çš„è¯­ä¹‰å’Œè¯è¯­ç°è±¡ï¼ˆå¦‚ä¿®è¾å…³ç³»è¯†åˆ«ï¼‰æ–¹é¢é‡åˆ°å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13095v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬ä»‹ç»äº†BeDiscovERï¼ˆåŸºäºç°ä»£æ¨ç†è¯­è¨€æ¨¡å‹çš„ç¯‡ç« ç†è§£åŸºå‡†æµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€æ–°çš„ã€å…¨é¢çš„è¯„ä¼°å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¯‡ç« çº§åˆ«çš„çŸ¥è¯†ã€‚BeDiscovERæ±‡é›†äº†5ä¸ªå…¬å¼€çš„ç¯‡ç« ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç¯‡ç« è¯æ±‡ã€ï¼ˆå¤šï¼‰å¥å­å’Œæ–‡æ¡£çº§åˆ«ï¼Œæ€»å…±æœ‰52ä¸ªå•ç‹¬çš„æ•°æ®é›†ã€‚å®ƒæ¶µç›–äº†å¹¿æ³›ç ”ç©¶çš„ä»»åŠ¡ï¼Œå¦‚ç¯‡ç« è§£æå’Œæ—¶é—´å…³ç³»æŠ½å–ï¼Œä»¥åŠä¸€äº›æ–°çš„æŒ‘æˆ˜ï¼Œå¦‚ç¯‡ç« å°å“è¯ï¼ˆä¾‹å¦‚ï¼Œâ€œjustâ€ï¼‰çš„æ­§ä¹‰æ€§ï¼Œå¹¶æ±‡æ€»äº†å…³äºå¤šè¯­è¨€å’Œå¤šæ¡†æ¶çš„ç¯‡ç« å…³ç³»è§£æå’Œæ ‘åº“çš„å…±äº«ä»»åŠ¡ã€‚æˆ‘ä»¬å¯¹å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Qwen3ç³»åˆ—ã€DeepSeek-R1å’Œå‰æ²¿æ¨¡å‹GPT-5-miniï¼‰è¿›è¡Œäº†BeDiscovERè¯„ä¼°ï¼Œå‘ç°æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ—¶æ€æ¨ç†çš„ç®—æœ¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å…¨æ–‡æ¨ç†å’Œä¸€äº›å¾®å¦™çš„è¯­ä¹‰å’Œç¯‡ç« ç°è±¡ï¼ˆå¦‚ä¿®è¾å…³ç³»è¯†åˆ«ï¼‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>BeDiscovERæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¯‡ç« çº§åˆ«çš„çŸ¥è¯†çš„ç»¼åˆå¥—ä»¶ã€‚</li>
<li>å®ƒæ¶µç›–äº†å¤šç§ç¯‡ç« ä»»åŠ¡å’Œæ•°æ®é›†ï¼ŒåŒ…æ‹¬ç¯‡ç« è¯æ±‡ã€ï¼ˆå¤šï¼‰å¥å­å’Œæ–‡æ¡£çº§åˆ«ã€‚</li>
<li>BeDiscovERåŒ…å«äº†å¹¿æ³›ç ”ç©¶çš„ä»»åŠ¡ï¼ˆå¦‚ç¯‡ç« è§£æå’Œæ—¶é—´å…³ç³»æŠ½å–ï¼‰å’Œæ–°æŒ‘æˆ˜ï¼ˆå¦‚ç¯‡ç« å°å“è¯çš„æ­§ä¹‰æ€§ï¼‰ã€‚</li>
<li>è¯„ä¼°å‘ç°ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ—¶æ€æ¨ç†çš„ç®—æœ¯æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨å…¨æ–‡æ¨ç†å’Œä¸€äº›å¾®å¦™çš„è¯­ä¹‰å’Œç¯‡ç« ç°è±¡ï¼ˆå¦‚ä¿®è¾å…³ç³»è¯†åˆ«ï¼‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>BeDiscovERè¿˜åŒ…æ‹¬ä¸€ä¸ªå…³äºå¤šè¯­è¨€å’Œå¤šæ¡†æ¶çš„ç¯‡ç« å…³ç³»è§£æå’Œæ ‘åº“çš„å…±äº«ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8c26d739f86f0b879734d8e5e42bd0d" align="middle">
<img src="https://picx.zhimg.com/v2-e2a2c22b0dcaafc816086ba899d12dde" align="middle">
<img src="https://picx.zhimg.com/v2-7ea06e0f0344e24a6aff1bd3d3a70904" align="middle">
<img src="https://picx.zhimg.com/v2-00f54dd5c39cadfc162a242afaff4eb1" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ViSS-R1-Self-Supervised-Reinforcement-Video-Reasoning"><a href="#ViSS-R1-Self-Supervised-Reinforcement-Video-Reasoning" class="headerlink" title="ViSS-R1: Self-Supervised Reinforcement Video Reasoning"></a>ViSS-R1: Self-Supervised Reinforcement Video Reasoning</h2><p><strong>Authors:Bo Fang, Yuxin Song, Qiangqiang Wu, Haoyuan Sun, Wenhao Wu, Antoni B. Chan</strong></p>
<p>Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLMâ€™s R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.</p>
<blockquote>
<p>å¤æ‚è§†é¢‘æ¨ç†å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå½“å‰åŸºäºR1çš„æ–¹æ³•å¾€å¾€ä¼˜å…ˆé‡‡ç”¨æ–‡æœ¬ä¸­å¿ƒçš„æ¨ç†ï¼Œè¿™äº›æ¨ç†æ˜¯ä»æ–‡æœ¬å’Œå›¾åƒå‘å±•ä¸­è¡ç”Ÿå‡ºæ¥çš„ã€‚åœ¨è§†é¢‘ä»»åŠ¡ä¸­ï¼Œè¿™äº›ç­–ç•¥å¸¸å¸¸æœªèƒ½å……åˆ†åˆ©ç”¨ä¸°å¯Œçš„è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´æ½œåœ¨çš„æ·å¾„å­¦ä¹ å’Œæ›´å®¹æ˜“äº§ç”Ÿå¹»è§‰ã€‚ä¸ºäº†ä¿ƒè¿›æ›´ç¨³å¥çš„è§†è§‰ä¸­å¿ƒè§†é¢‘ç†è§£ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†ä¸€ç§æ–°å‹çš„è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ GRPOç®—æ³•ï¼ˆPretext-GRPOï¼‰ï¼Œè¯¥ç®—æ³•è¢«å¼•å…¥æ ‡å‡†çš„R1ç®¡é“ä¸­ã€‚åœ¨Pretext-GRPOä¸­ï¼Œé€šè¿‡ä¸ºæ­£ç¡®è§£å†³å˜æ¢è§†è§‰è¾“å…¥è€Œæ­£ç¡®è§£å†³çš„é¢„è®¾ä»»åŠ¡åˆ†é…æ­£å‘å¥–åŠ±ï¼Œä½¿æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯è¿›è¡Œéå¹³å‡¡å¤„ç†ã€‚åŸºäºPretext-GRPOçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ViSS-R1æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç®€åŒ–äº†é¢„æµ‹è¯•ä»»åŠ¡åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„ç†å¿µå¹¶å°†å…¶ç›´æ¥æ•´åˆåˆ°MLLMçš„R1è®­ç»ƒåçš„èŒƒå¼ä¸­ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ä»…ä¾èµ–äºç¨€ç–çš„è§†è§‰çº¿ç´¢ï¼Œè€Œä¸”è¿˜è¿«ä½¿æ¨¡å‹é€šè¿‡åŒæ—¶å¤„ç†é¢„è®¾é—®é¢˜å’ŒçœŸå®çš„ç”¨æˆ·æŸ¥è¯¢æ¥å¯¹å˜æ¢åçš„è§†è§‰è¾“å…¥è¿›è¡Œæ¨ç†ã€‚è¿™éœ€è¦è¯†åˆ«å‡ºåº”ç”¨çš„å˜æ¢å¹¶é‡å»ºåŸå§‹è§†é¢‘ä»¥å½¢æˆå‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚åœ¨å…­ä¸ªå¹¿æ³›ä½¿ç”¨çš„è§†é¢‘æ¨ç†å’Œç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†æˆ‘ä»¬çš„Pretext-GRPOå’ŒViSS-R1åœ¨å¤æ‚è§†é¢‘æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13054v1">PDF</a> Our paper was initially titled â€œVideo-SSR1: Self-Supervised Reinforcement Video Reasoning.â€ Upon noticing its close resemblance to the title of a recently released paper, we have decided to rename our work as â€œViSS-R1.â€</p>
<p><strong>Summary</strong><br>è§†é¢‘æ¨ç†å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰R1æ–¹æ³•é€šå¸¸ä¾§é‡æ–‡æœ¬ä¸ºä¸­å¿ƒçš„æ¨ç†ï¼Œå¿½è§†è§†é¢‘ä¸­çš„ä¸°å¯Œè§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹æ˜“èµ°æ·å¾„å’Œäº§ç”Ÿå¹»è§‰ã€‚ä¸ºæå‡æ¨¡å‹å¯¹è§†é¢‘çš„è§†è§‰ä¸ºä¸­å¿ƒçš„ç†è§£èƒ½åŠ›ï¼Œç ”ç©¶å¼•å…¥äº†æ–°å‹è‡ªæˆ‘ç›‘ç£å¼ºåŒ–å­¦ä¹ GRPOç®—æ³•ï¼ˆPretext-GRPOï¼‰ï¼Œå¹¶å°†å…¶çº³å…¥æ ‡å‡†R<br>è§†é¢‘ä»»åŠ¡ä¸­å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è‡ªæˆ‘ç›‘ç£å¼ºåŒ–å­¦ä¹ GRPOç®—æ³•ï¼ˆPretext-GRPOï¼‰ï¼Œé€šè¿‡è§£å†³å˜æ¢è§†è§‰è¾“å…¥çš„ä¼ªè£…ä»»åŠ¡ç»™äºˆæ­£å‘å¥–åŠ±ï¼Œä½¿æ¨¡å‹æ›´æ·±å…¥åœ°å¤„ç†è§†è§‰ä¿¡æ¯ã€‚åŸºäºPretext-GRPOçš„æœ‰æ•ˆæ€§ï¼Œè¿›ä¸€æ­¥æå‡ºäº†ViSS-R1æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç®€åŒ–äº†ä¼ªè£…ä»»åŠ¡ä¸ºåŸºç¡€çš„è‡ªç›‘ç£å­¦ä¹ å¹¶å°†å…¶ç›´æ¥èå…¥MLLMçš„R1è®­ç»ƒåèŒƒå¼ä¸­ã€‚å®ƒä¿ƒä½¿æ¨¡å‹åŒæ—¶å¤„ç†ä¼ªè£…é—®é¢˜å’ŒçœŸå®ç”¨æˆ·æŸ¥è¯¢ï¼Œè€Œä¸ä»…ä»…ä¾èµ–äºç¨€ç–çš„è§†è§‰çº¿ç´¢ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒPretext-GRPOå’ŒViSS-R1åœ¨è§†é¢‘æ¨ç†å’Œç†è§£æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ•ˆæœã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€ä¾›ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†è§†é¢‘æ¨ç†æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>R1æ–¹æ³•å¸¸å¸¸è¿‡äºä¾èµ–æ–‡æœ¬ä¸­å¿ƒæ¨ç†è€Œå¿½è§†è§†é¢‘çš„è§†è§‰ä¿¡æ¯ã€‚</li>
<li>æå‡ºçš„æ–°å‹è‡ªæˆ‘ç›‘ç£å¼ºåŒ–å­¦ä¹ GRPOç®—æ³•ï¼ˆPretext-GRPOï¼‰æœ‰åŠ©äºå¢å¼ºæ¨¡å‹å¯¹è§†é¢‘è§†è§‰ä¿¡æ¯çš„å¤„ç†èƒ½åŠ›ã€‚</li>
<li>Pretext-GRPOç®—æ³•é€šè¿‡è§£å†³å˜æ¢è§†è§‰è¾“å…¥çš„ä¼ªè£…ä»»åŠ¡æ¥èµ‹äºˆæ­£å‘å¥–åŠ±ã€‚</li>
<li>ViSS-R1æ¡†æ¶ç®€åŒ–äº†ä¼ªè£…ä»»åŠ¡ä¸ºåŸºç¡€çš„è‡ªç›‘ç£å­¦ä¹ å¹¶å°†å…¶èå…¥MLLMçš„R1è®­ç»ƒåèŒƒå¼ä¸­ã€‚</li>
<li>ViSS-R1æ¡†æ¶è¦æ±‚æ¨¡å‹åŒæ—¶å¤„ç†ä¼ªè£…é—®é¢˜å’ŒçœŸå®ç”¨æˆ·æŸ¥è¯¢ï¼Œè€Œéä»…ä¾èµ–ç¨€ç–çš„è§†è§‰çº¿ç´¢ã€‚</li>
<li>ç»¼åˆè¯„ä¼°æ˜¾ç¤ºPretext-GRPOå’ŒViSS-R1åœ¨è§†é¢‘æ¨ç†å’Œç†è§£æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9ff3dfac2817434590c8fd0797d8839" align="middle">
<img src="https://picx.zhimg.com/v2-5f5408943d3c1d8a877c4cf80d4ece6a" align="middle">
<img src="https://picx.zhimg.com/v2-cc64abfd88e71043114327b116c204f8" align="middle">
<img src="https://picx.zhimg.com/v2-3d7454212221bf7a496c79d4b229b9f6" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Spark-Prover-X1-Formal-Theorem-Proving-Through-Diverse-Data-Training"><a href="#Spark-Prover-X1-Formal-Theorem-Proving-Through-Diverse-Data-Training" class="headerlink" title="Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training"></a>Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training</h2><p><strong>Authors:Xinyuan Zhou, Yi Lei, Xiaoyu Zhou, Jingyi Sun, Yu Zhu, Zhongyi Ye, Weitai Zhang, Quan Liu, Si Wei, Cong Liu</strong></p>
<p>Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a â€œCoT-augmented state predictionâ€ task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the proverâ€™s capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:<a target="_blank" rel="noopener" href="https://www.modelscope.cn/organization/iflytek">https://www.modelscope.cn/organization/iflytek</a>, <a target="_blank" rel="noopener" href="https://gitcode.com/ifly_opensource">https://gitcode.com/ifly_opensource</a>.</p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç„¶è€Œï¼Œè¿›å±•å¾€å¾€å—åˆ°å¤šæ ·åŒ–å’Œé«˜è´¨é‡è‡ªç„¶è¯­è¨€æ•°æ®çš„ç¨€ç¼ºæ€§çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Spark-Prover-X1ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ä¸‰é˜¶æ®µæ¡†æ¶è®­ç»ƒçš„7Bå‚æ•°æ¨¡å‹ï¼Œæ—¨åœ¨è§£é”å¯è®¿é—®å’Œä¸­ç­‰è§„æ¨¡LLMsçš„æ¨ç†æ½œåŠ›ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åœ¨ä¸€ç³»åˆ—æ–°é¢–çš„æ•°æ®ä»»åŠ¡çš„æ”¯æŒä¸‹ï¼Œåœ¨å¹¿æ³›çš„æ•°å­¦è¯­æ–™åº“ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œä»¥èå…¥æ·±åº¦çŸ¥è¯†ã€‚å…³é”®çš„åˆ›æ–°ä¹‹å¤„åœ¨äºâ€œè®¤çŸ¥çŠ¶æ€é¢„æµ‹å¢å¼ºâ€ä»»åŠ¡ï¼Œä»¥å®ç°ç²¾ç»†æ¨ç†ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨ä¸“ä¸šè¿­ä»£å¾ªç¯ä¸­å¯¹Spark-Prover-X1-7Bå’ŒSpark-Formalizer-X1-7Bæ¨¡å‹è¿›è¡Œä¸“ä¸šåŒ–å¤„ç†ã€‚æœ€åï¼Œåº”ç”¨ä¸€è½®æœ‰é’ˆå¯¹æ€§çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥æé«˜è¯æ˜å™¨åœ¨æœ€å…·æŒ‘æˆ˜æ€§é—®é¢˜ä¸Šçš„èƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›ç¨³å¥çš„è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•Œè€ƒè¯•ä¸­çš„é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ExamFormal-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŒ…å«402ä¸ªå½¢å¼é—®é¢˜çš„åŸºå‡†æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpark-Prover-X1-7Båœ¨åŒç±»å¼€æºæ¨¡å‹ä¸­è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå¹³å‡é€šè¿‡ç‡ï¼ˆpass@32ï¼‰è¾¾åˆ°37.0%ã€‚å®ƒåœ¨å›°éš¾çš„ç«èµ›åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨PutnamBenchä¸Šè§£å†³äº†27ä¸ªé—®é¢˜ï¼ˆpass@32ï¼‰ï¼Œå¹¶åœ¨CombiBenchä¸Šè¾¾åˆ°24.0%ï¼ˆpass@32ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œéªŒè¯äº†å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®å’Œé€æ­¥ä¼˜åŒ–çš„è®­ç»ƒç®¡é“ä¸ºå¢å¼ºè½»é‡çº§LLMsçš„å½¢å¼æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€æ¡æœ‰æ•ˆçš„é€”å¾„ã€‚Spark-Prover-X1-7Bã€Spark-Formalizer-X1-7Bä»¥åŠExamFormal-Benchæ•°æ®é›†å‡å·²åœ¨<a target="_blank" rel="noopener" href="https://www.modelscope.cn/organization/iflytek%E5%92%8Chttps://gitcode.com/ifly_opensource%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://www.modelscope.cn/organization/iflytekå’Œhttps://gitcode.com/ifly_opensourceä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13043v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†è¿›æ­¥å—é™äºå¤šæ ·ä¸”é«˜è´¨é‡çš„å½¢å¼è¯­è¨€æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºSpark-Prover-X1ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ä¸‰é˜¶æ®µæ¡†æ¶è®­ç»ƒçš„7Bå‚æ•°æ¨¡å‹ï¼Œæ—¨åœ¨è§£é”æ›´æ˜“äºè·å–å’Œä¸­ç­‰è§„æ¨¡çš„è¯­è¨€æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡è¿ç»­é¢„è®­ç»ƒå¹¿æ³›æ•°å­¦è¯­æ–™åº“å’Œä¸€ç³»åˆ—æ–°å‹æ•°æ®ä»»åŠ¡æ¥æ³¨å…¥æ·±åº¦çŸ¥è¯†ï¼Œå…¶ä¸­â€œCoTå¢å¼ºçŠ¶æ€é¢„æµ‹â€ä»»åŠ¡æ˜¯å®ç°ç²¾ç»†æ¨ç†çš„å…³é”®ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç»“åˆä¸“å®¶è¿­ä»£å¾ªç¯ï¼Œå¯¹Spark-Prover-X1-7Bå’ŒSpark-Formalizer-X1-7Bæ¨¡å‹è¿›è¡Œä¸“ä¸šåŒ–ã€‚æœ€åï¼Œåº”ç”¨æœ‰é’ˆå¯¹æ€§çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥æé«˜è§£å†³æœ€éš¾é¢˜ç›®çš„èƒ½åŠ›ã€‚ä¸ºè¿›è¡Œç¨³å¥è¯„ä¼°ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹ç°å®è€ƒè¯•ä¸­çš„é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ExamFormal-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«402ä¸ªå½¢å¼é—®é¢˜çš„æ–°åŸºå‡†æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpark-Prover-X1-7Båœ¨åŒç±»å¼€æºæ¨¡å‹ä¸­è¡¨ç°å“è¶Šï¼Œå¹³å‡é€šè¿‡ç‡ä¸º37.0%ï¼ˆpass@32ï¼‰ã€‚åœ¨å›°éš¾çš„ç«èµ›åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¦‚PutnamBenchå’ŒCombiBenchï¼Œå®ƒè¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„å·¥ä½œéªŒè¯äº†å¤šæ ·åŒ–è®­ç»ƒæ•°æ®å’Œé€æ­¥ä¼˜åŒ–çš„è®­ç»ƒç®¡é“æ˜¯å¢å¼ºè½»é‡çº§è¯­è¨€æ¨¡å‹å½¢å¼æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆé€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥Spark-Prover-X1æ¨¡å‹ï¼Œé€šè¿‡ä¸‰é˜¶æ®µæ¡†æ¶è®­ç»ƒï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡é¢„è®­ç»ƒå’Œæ–°å‹æ•°æ®ä»»åŠ¡æ³¨å…¥æ·±åº¦çŸ¥è¯†ï¼ŒåŒ…æ‹¬â€œCoT-augmented state predictionâ€ä»»åŠ¡ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç»“åˆä¸“å®¶è¿­ä»£å¾ªç¯ï¼Œå¯¹æ¨¡å‹è¿›è¡Œä¸“ä¸šåŒ–ã€‚</li>
<li>åº”ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä»¥æé«˜è§£å†³æœ€éš¾é¢˜ç›®çš„èƒ½åŠ›ã€‚</li>
<li>æ¨å‡ºæ–°çš„åŸºå‡†æ•°æ®é›†ExamFormal-Benchï¼ŒåŒ…å«402ä¸ªå½¢å¼é—®é¢˜ï¼Œç”¨äºç¨³å¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Spark-Prover-X1-7Båœ¨åŒç±»æ¨¡å‹ä¸­è¡¨ç°å“è¶Šï¼Œå¹³å‡é€šè¿‡ç‡ä¸º37.0%ã€‚</li>
<li>æ¨¡å‹åœ¨å›°éš¾ç«èµ›åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œå¦‚PutnamBenchå’ŒCombiBenchã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef2c0a1a99b8a60d7dd0db6d32a54a39" align="middle">
<img src="https://picx.zhimg.com/v2-86c09d1c14aa55bda4b1dee4fa5ebf93" align="middle">
<img src="https://picx.zhimg.com/v2-ff564ed4276ce1a8527b3ad21190cb2f" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bf0625dfb62a4e243a388d7e148e3691" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Crossing Borders A Multimodal Challenge for Indian Poetry Translation and Image Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-18/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cdf2e42fd52e983b65c27dce783896b9" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
