<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Live-SWE-agent Can Software Engineering Agents Self-Evolve on the Fly?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-025ccf26e259c60a13108aff633b8b39')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly"><a href="#Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly" class="headerlink" title="Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?"></a>Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</h2><p><strong>Authors:Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, Lingming Zhang</strong></p>
<p>Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined&#x2F;modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-GÃ¶del Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨é‡å¡‘å‡ ä¹æ‰€æœ‰è¡Œä¸šï¼ŒåŒ…æ‹¬è½¯ä»¶å·¥ç¨‹ã€‚è¿‘å¹´æ¥ï¼Œå·²ç»æå‡ºäº†è®¸å¤šLLMä»£ç†æ¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„è½¯ä»¶é—®é¢˜ã€‚æ­¤ç±»è½¯ä»¶ä»£ç†é€šå¸¸é…å¤‡äº†ä¸€å¥—ç¼–ç¨‹å·¥å…·ï¼Œå¹¶èƒ½å¤Ÿè‡ªä¸»å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œä»¥å½¢æˆå®Œæ•´çš„è½¨è¿¹æ¥è§£å†³ç«¯åˆ°ç«¯çš„è½¯ä»¶ä»»åŠ¡ã€‚è™½ç„¶å‰æ™¯çœ‹å¥½ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦ä¸“é—¨è®¾è®¡ï¼Œå¹¶ä¸”å¯èƒ½ä»ç„¶ä¸å¤Ÿç†æƒ³ï¼Œå› ä¸ºç©·å°½æ•´ä¸ªä»£ç†æ¶æ„çš„è®¾è®¡ç©ºé—´å¯èƒ½æå…·æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚</p>
</blockquote>
<p>ç ”ç©¶äººå‘˜æ„è¯†åˆ°è½¯ä»¶ä»£ç†æœ¬è´¨ä¸Šæ˜¯è½¯ä»¶æœ¬èº«ï¼Œå¯ä»¥è¿›ä¸€æ­¥è¿›è¡Œæ”¹è¿›&#x2F;ä¿®æ”¹ï¼Œå› æ­¤æœ€è¿‘å·²ç»æå‡ºäº†ä¸€äº›è‡ªæˆ‘æ”¹è¿›çš„è½¯ä»¶ä»£ç†ï¼ŒåŒ…æ‹¬è¾¾å°”æ–‡-å“¥å¾·å°”æœºå™¨ï¼ˆDGMï¼‰ã€‚åŒæ—¶ï¼Œè¿™ç§è‡ªæˆ‘æ”¹è¿›çš„ä»£ç†éœ€è¦åœ¨ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œæ˜‚è´µçš„ç¦»çº¿è®­ç»ƒï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•åœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹æˆ–åŸºå‡†æµ‹è¯•ä¹‹é—´è¿›è¡Œå¾ˆå¥½çš„æ³›åŒ–ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13646v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨é‡å¡‘åŒ…æ‹¬è½¯ä»¶å·¥ç¨‹åœ¨å†…çš„å‡ ä¹æ‰€æœ‰è¡Œä¸šã€‚è¿‘å¹´æ¥ï¼Œæå‡ºäº†ä¸€ç³»åˆ—LLMä»£ç†æ¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„è½¯ä»¶é—®é¢˜ã€‚è¿™äº›è½¯ä»¶ä»£ç†é€šå¸¸é…å¤‡äº†ä¸€å¥—ç¼–ç å·¥å…·ï¼Œèƒ½å¤Ÿè‡ªä¸»å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œå½¢æˆå®Œæ•´çš„è½¨è¿¹æ¥è§£å†³ç«¯åˆ°ç«¯çš„è½¯ä»¶ä»»åŠ¡ã€‚å°½ç®¡å‰æ™¯å¹¿é˜”ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦ä¸“é—¨è®¾è®¡ï¼Œå¯èƒ½ä»ç„¶ä¸å¤Ÿç†æƒ³ï¼Œå› ä¸ºç©·å°½æ•´ä¸ªä»£ç†æ¶æ„çš„è®¾è®¡ç©ºé—´æå…·æŒ‘æˆ˜æ€§å’Œæˆæœ¬ã€‚ç ”ç©¶äººå‘˜å·²ç»è®¤è¯†åˆ°è½¯ä»¶ä»£ç†æœ¬èº«å°±æ˜¯å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›å’Œä¿®æ”¹çš„ï¼Œå› æ­¤æœ€è¿‘å·²ç»æå‡ºäº†ä¸€ç³»åˆ—è‡ªæˆ‘æ”¹è¿›çš„è½¯ä»¶ä»£ç†ï¼ŒåŒ…æ‹¬è¾¾å°”æ–‡-å“¥å¾·å°”æœºå™¨ï¼ˆDGMï¼‰ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„è‡ªæˆ‘æ”¹è¿›ä»£ç†éœ€è¦åœ¨ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œæ˜‚è´µçš„ç¦»çº¿è®­ç»ƒï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•åœ¨ä¸åŒLLMæˆ–åŸºå‡†æµ‹è¯•ä¹‹é—´å¾ˆå¥½åœ°æ¨å¹¿ã€‚æœ¬æ–‡æå‡ºLive-SWE-agentï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿåœ¨è§£å†³ç°å®ä¸–ç•Œè½¯ä»¶é—®é¢˜æ—¶å®æ—¶è‡ªä¸»è¿ç»­è¿›åŒ–çš„é¦–ä¸ªæ´»è·ƒè½¯ä»¶ä»£ç†ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒLive-SWE-agentåœ¨ä¸è¿›è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾çš„æƒ…å†µä¸‹å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è§£å†³ç‡ï¼Œè¶…è¿‡äº†æ‰€æœ‰ç°æœ‰çš„å¼€æºè½¯ä»¶ä»£ç†å¹¶æ¥è¿‘æœ€ä½³ä¸“æœ‰è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒLive-SWE-agentåœ¨æœ€è¿‘çš„SWE-Bench ProåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºæœ€æ–°çš„æ‰‹åŠ¨å®šåˆ¶è½¯ä»¶ä»£ç†ï¼Œå®ç°äº†å·²çŸ¥çš„æœ€ä½³è§£å†³ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨é‡å¡‘è½¯ä»¶å·¥ç¨‹è¡Œä¸šï¼ŒLLMä»£ç†ä¸ºè§£å†³ç°å®è½¯ä»¶é—®é¢˜æä¾›äº†æ–°çš„æ–¹æ³•ã€‚</li>
<li>ç°æœ‰çš„LLMä»£ç†é€šå¸¸éœ€è¦ä¸“é—¨è®¾è®¡ï¼Œå¹¶ä¸”åœ¨ç©·å°½ä»£ç†æ¶æ„è®¾è®¡ç©ºé—´æ—¶å­˜åœ¨æŒ‘æˆ˜å’Œæˆæœ¬ã€‚</li>
<li>è‡ªæˆ‘æ”¹è¿›çš„è½¯ä»¶ä»£ç†æ˜¯ä¸€ä¸ªæ–°å…´ç ”ç©¶é¢†åŸŸï¼Œå…¶ä¸­è¾¾å°”æ–‡-å“¥å¾·å°”æœºå™¨ï¼ˆDGMï¼‰æ˜¯å…¶ä¸­çš„ä¸€ä¸ªä¾‹å­ã€‚</li>
<li>è‡ªæˆ‘æ”¹è¿›çš„è½¯ä»¶ä»£ç†éœ€è¦åœ¨ç‰¹å®šçš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œæ˜‚è´µçš„ç¦»çº¿è®­ç»ƒï¼Œä¸”å¯èƒ½æ— æ³•åœ¨ä¸åŒLLMæˆ–åŸºå‡†æµ‹è¯•ä¹‹é—´å¾ˆå¥½åœ°æ¨å¹¿ã€‚</li>
<li>Live-SWE-agentæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨è§£å†³ç°å®è½¯ä»¶é—®é¢˜æ—¶å®æ—¶è‡ªä¸»è¿ç»­è¿›åŒ–çš„æ´»è·ƒè½¯ä»¶ä»£ç†ã€‚</li>
<li>Live-SWE-agentåœ¨ä¸è¿›è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾çš„æƒ…å†µä¸‹å®ç°äº†é«˜è§£å†³ç‡ï¼Œè¶…è¿‡äº†ç°æœ‰å¼€æºè½¯ä»¶ä»£ç†å¹¶æ¥è¿‘æœ€ä½³ä¸“æœ‰è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f39828b854825c26335d62e4a21a1e47" align="middle">
<img src="https://picx.zhimg.com/v2-a32c7368a80722a75d0072452b531bfc" align="middle">
<img src="https://picx.zhimg.com/v2-9e136d7f55c2d513e569a19691042c41" align="middle">
<img src="https://picx.zhimg.com/v2-8e91c161aec16d457d30b274528f3c74" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents"><a href="#Omni-Memory-System-for-Personalized-Long-Horizon-Self-Evolving-Agents" class="headerlink" title="Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents"></a>Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents</h2><p><strong>Authors:Piaohong Wang, Motong Tian, Jiaxian Li, Yuan Liang, Yuqing Wang, Qianben Chen, Tiannan Wang, Zhicong Lu, Jiawei Ma, Yuchen Eleanor Jiang, Wangchunshu Zhou</strong></p>
<p>Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.</p>
<blockquote>
<p>è¿‘æœŸï¼Œä»¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ä»£ç†äººçš„è¿›å±•ï¼Œåœ¨ç”Ÿæˆäººç±»å“åº”æ–¹é¢å±•ç°äº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤æ‚ç¯å¢ƒä¸­ç»´æŒé•¿æœŸäº’åŠ¨æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦å½’å› äºä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’ŒåŠ¨æ€ä¸ªæ€§åŒ–çš„å±€é™æ€§ã€‚ç°æœ‰çš„è®°å¿†ç³»ç»Ÿé€šå¸¸ä¾èµ–äºæ£€ç´¢å‰çš„è¯­ä¹‰åˆ†ç»„ï¼Œè¿™å¯èƒ½ä¼šå¿½ç•¥ç”¨æˆ·è¯­ä¹‰ä¸Šæ— å…³ç´§è¦ä½†è‡³å…³é‡è¦çš„ä¿¡æ¯ï¼Œå¹¶å¼•å…¥æ£€ç´¢å™ªéŸ³ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†O-Memçš„åˆæ­¥è®¾è®¡ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸»åŠ¨ç”¨æˆ·åˆ†æçš„æ–°å‹è®°å¿†æ¡†æ¶ï¼Œå®ƒåŠ¨æ€æå–å’Œæ›´æ–°ç”¨æˆ·ç‰¹æ€§å’Œäº‹ä»¶è®°å½•ï¼Œè¿™äº›ç‰¹æ€§å’Œè®°å½•æ¥è‡ªç”¨æˆ·ä¸ä»£ç†äººçš„ä¸»åŠ¨äº’åŠ¨ã€‚O-Memæ”¯æŒäººæ ¼å±æ€§åˆ†å±‚æ£€ç´¢å’Œä¸»é¢˜ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œä»è€Œå®ç°æ›´è‡ªé€‚åº”å’Œè¿è´¯çš„ä¸ªæ€§åŒ–å“åº”ã€‚O-Memåœ¨å…¬å…±LoCoMoåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†51.7 è¿›å±•å°¤ä¸ºçªå‡ºï¼šè¾ƒä¹‹å‰æœ€å…ˆè¿›çš„LangMemæå‡äº†è¿‘ç™¾åˆ†ä¹‹ä¸‰ï¼ˆå®ç°äº†å¾—åˆ†ç™¾åˆ†æ¯”è¾¾åˆ°äº”åä¸ƒï¼‰ï¼Œå¹¶ä¸”åœ¨PERSONAMEMä¸Šä¹Ÿå–å¾—äº†æ¯”æ­¤å‰çš„å† å†›æ¡†æ¶A-Memé«˜å‡ºäº†ä¸‰åˆ†çš„è¿›æ­¥ï¼Œå› æ­¤æ¨è¿›äº†å¯¹ç²¾å‡†åŒ–å’Œä¸ªæ€§åŒ–çš„å›åº”ï¼Œä»æ›´é«˜æ•ˆç‡æå‡ååº”é€Ÿåº¦çš„è§’åº¦æ¥çœ‹ä¹Ÿæ¯”ä¹‹å‰å¤§å¤šæ•°çš„åŸºå‡†æ¡†æ¶æ›´å¥½ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæœªæ¥å¼€å‘é«˜æ•ˆä¸”äººæ€§åŒ–çš„ä¸ªæ€§åŒ–äººå·¥æ™ºèƒ½åŠ©æ‰‹æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13593v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºLLMçš„å¤§å‹å¯¹è¯ç”Ÿæˆä¸­çš„æ½œåœ¨åº”ç”¨åŠå…¶å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»´æŒå¤æ‚ç¯å¢ƒä¸‹çš„é•¿æœŸäº¤äº’æ–¹é¢ã€‚ä¸ºè§£å†³ç°æœ‰è®°å¿†ç³»ç»Ÿçš„ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºä¸»åŠ¨ç”¨æˆ·åˆ†æçš„æ–°å‹è®°å¿†æ¡†æ¶O-Memçš„è®¾è®¡æ¦‚å¿µã€‚O-Memèƒ½å¤ŸåŠ¨æ€æå–å’Œæ›´æ–°ç”¨æˆ·ç‰¹æ€§å’Œäº‹ä»¶è®°å½•ï¼Œæ”¯æŒå±‚æ¬¡åŒ–çš„ä¸ªæ€§å±æ€§æ£€ç´¢å’Œè¯é¢˜ç›¸å…³ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œä»¥å®ç°æ›´é€‚åº”å’Œè¿è´¯çš„ä¸ªäººåŒ–å“åº”ã€‚åœ¨å…¬å…±LoCoMoåŸºå‡†æµ‹è¯•ä¸­ï¼ŒO-Memçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæé«˜äº†è¿‘3%çš„æ•ˆç‡ã€‚åŒæ—¶ï¼Œç›¸è¾ƒäºå…ˆå‰çš„è®°å¿†æ¡†æ¶ï¼ŒO-Memåœ¨ä»¤ç‰Œå’Œäº¤äº’å“åº”çš„æ—¶é—´æ•ˆç‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚è¿™ä¸ºæœªæ¥å¼€å‘é«˜æ•ˆä¸”äººæ€§åŒ–çš„ä¸ªæ€§åŒ–AIåŠ©æ‰‹æä¾›äº†å¹¿é˜”çš„å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMæŠ€æœ¯è™½ç„¶èƒ½å¤Ÿç”Ÿæˆäººç±»èˆ¬çš„å“åº”ï¼Œä½†åœ¨å¤æ‚ç¯å¢ƒä¸­ç»´æŒé•¿æœŸäº¤äº’å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è®°å¿†ç³»ç»Ÿä¾èµ–è¯­ä¹‰åˆ†ç»„è¿›è¡Œæ£€ç´¢ï¼Œå¯èƒ½ä¼šå¿½ç•¥å…³é”®ç”¨æˆ·ä¿¡æ¯å¹¶å¼•å…¥æ£€ç´¢å™ªå£°ã€‚</li>
<li>O-Memä½œä¸ºä¸€ç§æ–°å‹è®°å¿†æ¡†æ¶ï¼ŒåŸºäºä¸»åŠ¨ç”¨æˆ·åˆ†æè®¾è®¡ï¼Œèƒ½å¤ŸåŠ¨æ€æå–å’Œæ›´æ–°ç”¨æˆ·ç‰¹æ€§å’Œäº‹ä»¶è®°å½•ã€‚</li>
<li>O-Memæ”¯æŒå±‚æ¬¡åŒ–çš„ä¸ªæ€§å±æ€§æ£€ç´¢å’Œè¯é¢˜ç›¸å…³ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œä»¥æé«˜å“åº”çš„é€‚åº”æ€§å’Œè¿è´¯æ€§ã€‚</li>
<li>O-Memåœ¨å…¬å…±LoCoMoåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®ç°äº†è¿‘3%çš„æ•ˆç‡æå‡ã€‚</li>
<li>O-Memæé«˜äº†ä»¤ç‰Œå’Œäº¤äº’å“åº”çš„æ—¶é—´æ•ˆç‡ï¼Œç›¸è¾ƒäºå…ˆå‰çš„è®°å¿†æ¡†æ¶æœ‰æ‰€æ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9e45d591666cabca101050c34649297" align="middle">
<img src="https://picx.zhimg.com/v2-ef65277d1c1660eebf82be657859459c" align="middle">
<img src="https://picx.zhimg.com/v2-d590c38bcd8b72aeadce7ab926a65fdd" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Multimodal-Large-Language-Model-Framework-for-Automated-Interpretation-of-Fuel-Efficiency-Analytics-in-Public-Transportation"><a href="#Multi-Agent-Multimodal-Large-Language-Model-Framework-for-Automated-Interpretation-of-Fuel-Efficiency-Analytics-in-Public-Transportation" class="headerlink" title="Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation"></a>Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation</h2><p><strong>Authors:Zhipeng Ma, Ali Rida Bahja, Andreas Burgdorf, AndrÃ© Pomp, Tobias Meisen, Bo NÃ¸rregaard JÃ¸rgensen, Zheng Grace Ma</strong></p>
<p>Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.</p>
<blockquote>
<p>æå‡å…¬å…±äº¤é€šçš„ç‡ƒæ²¹æ•ˆç‡éœ€è¦æ•´åˆå¤æ‚çš„å¤šæ¨¡å¼æ•°æ®ä¸ºå¯è§£é‡Šã€ä¸å†³ç­–ç›¸å…³çš„æ´å¯Ÿã€‚ç„¶è€Œï¼Œä¼ ç»Ÿåˆ†ææ³•å’Œå¯è§†åŒ–æ–¹æ³•å¸¸å¸¸äº§ç”Ÿåˆ†æ•£çš„è¾“å‡ºç»“æœï¼Œéœ€è¦å¤§æ‰¹é‡çš„äººå·¥è§£è¯»ï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§å’Œä¸€è‡´æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨åŒ–æ•°æ®å™äº‹å’Œèƒ½æºæ´å¯Ÿç”Ÿæˆã€‚è¯¥æ¡†æ¶åè°ƒä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬æ•°æ®å™äº‹æ™ºèƒ½ä½“ã€LLMè¯„åˆ¤æ™ºèƒ½ä½“å’Œå¯é€‰çš„äººå·¥å¾ªç¯è¯„ä¼°å™¨ï¼Œä»¥è¿­ä»£çš„æ–¹å¼å°†åˆ†æäº§ç‰©è½¬åŒ–ä¸ºè¿è´¯çš„ã€ä»¥åˆ©ç›Šç›¸å…³è€…ä¸ºå¯¼å‘çš„æŠ¥å‘Šã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸¹éº¦åŒ—éƒ¨å°¤ç‰¹å…°åœ°åŒºå…¬å…±äº¤é€šçš„çœŸå®æ¡ˆä¾‹ç ”ç©¶è¿›è¡Œäº†éªŒè¯ï¼Œå…¶ä¸­å¯¹æ¥è‡ª4006æ¬¡è¡Œç¨‹çš„ç‡ƒæ²¹æ•ˆç‡æ•°æ®ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹èšç±»è¿›è¡Œåˆ†æã€‚è·¨è¶Šäº”ç§æœ€æ–°LLMå’Œä¸‰ç§æç¤ºèŒƒå¼çš„æ¯”è¾ƒå®éªŒç¡®å®šäº†GPT-4.1 miniä¸æ€ç»´é“¾æç¤ºä¸ºæœ€ä½³é…ç½®ï¼Œè¾¾åˆ°äº†97.3%çš„å™äº‹å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¹³è¡¡äº†å¯è§£é‡Šæ€§å’Œè®¡ç®—æˆæœ¬ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“ååŒåœ¨åŸºäºLLMçš„æŠ¥å‘Šç”Ÿæˆä¸­æ˜¾è‘—æé«˜äº†äº‹å®å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå¯æ‰©å±•æ€§ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ºAIé©±åŠ¨çš„å™äº‹ç”Ÿæˆå’Œèƒ½æºä¿¡æ¯å†³ç­–æ”¯æŒå»ºç«‹äº†å¯å¤åˆ¶å’Œé€‚åº”é¢†åŸŸçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13476v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨åŒ–æ•°æ®å™äº‹å’Œèƒ½æºæ´å¯Ÿç”Ÿæˆï¼Œæé«˜å…¬å…±äº¤é€šçš„ç‡ƒæ–™æ•ˆç‡ã€‚é€šè¿‡ä¸¹éº¦åŒ—æ—¥å¾·å…°å…¬å…±å·´å£«è¿è¾“çš„å®è¯æ¡ˆä¾‹ç ”ç©¶éªŒè¯äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå¤šæ™ºèƒ½ä½“ååŒæ˜¾è‘—æé«˜äº†LLMæŠ¥å‘Šä¸­çš„äº‹å®å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼•å…¥å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä»¥è‡ªåŠ¨åŒ–æ•°æ®å™äº‹å’Œèƒ½æºæ´å¯Ÿç”Ÿæˆï¼Œæå‡å…¬å…±äº¤é€šç‡ƒæ–™æ•ˆç‡ã€‚</li>
<li>æ¡†æ¶åŒ…å«æ•°æ®å™äº‹æ™ºèƒ½ä½“ã€LLMè¯„åˆ¤æ™ºèƒ½ä½“ä»¥åŠå¯é€‰çš„äººæœºååŒè¯„ä»·è€…ï¼Œå°†åˆ†æäº§ç‰©è½¬åŒ–ä¸ºè¿è´¯ã€é¢å‘åˆ©ç›Šç›¸å…³è€…çš„æŠ¥å‘Šã€‚</li>
<li>é€šè¿‡ä¸¹éº¦åŒ—æ—¥å¾·å…°å…¬å…±å·´å£«è¿è¾“å®è¯æ¡ˆä¾‹ç ”ç©¶ï¼ŒéªŒè¯äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¯”è¾ƒäº†äº”ç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸‰ç§æç¤ºèŒƒå¼ï¼Œå‘ç°GPT-4.1 minié…åˆChain-of-Thoughtæç¤ºæ³•ä¸ºæœ€ä¼˜é…ç½®ï¼Œå®ç°äº†97.3%çš„å™äº‹å‡†ç¡®æ€§ï¼Œå¹³è¡¡äº†å¯è§£é‡Šæ€§å’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“ååŒæ˜¾è‘—æé«˜äº†LLMæŠ¥å‘Šä¸­çš„äº‹å®å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå†³ç­–æ”¯æŒçš„å¯æ‰©å±•æ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœå»ºç«‹äº†å¯å¤åˆ¶ä¸”é€‚åº”äºç‰¹å®šé¢†åŸŸçš„AIé©±åŠ¨å™äº‹ç”Ÿæˆæ–¹æ³•è®ºï¼Œåœ¨èƒ½æºä¿¡æ¯å­¦ä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25747f40ab66994b1251671576656ab2" align="middle">
<img src="https://picx.zhimg.com/v2-a2a673e5b9cff4daf61bf82ba8e02983" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mem-PAL-Towards-Memory-based-Personalized-Dialogue-Assistants-for-Long-term-User-Agent-Interaction"><a href="#Mem-PAL-Towards-Memory-based-Personalized-Dialogue-Assistants-for-Long-term-User-Agent-Interaction" class="headerlink" title="Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction"></a>Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction</h2><p><strong>Authors:Zhaopei Huang, Qifeng Dai, Guozheng Wu, Xiaopeng Wu, Kehan Chen, Chuan Yu, Xubin Li, Tiezheng Ge, Wenxuan Wang, Qin Jin</strong></p>
<p>With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture usersâ€™ subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.</p>
<blockquote>
<p>éšç€æ™ºèƒ½ä¸ªäººè®¾å¤‡çš„æ™®åŠï¼Œé¢å‘æœåŠ¡çš„æ™ºèƒ½äººæœºäº¤äº’å˜å¾—è¶Šæ¥è¶Šæ™®éã€‚è¿™ä¸€è¶‹åŠ¿å‡¸æ˜¾äº†éœ€è¦èƒ½å¤Ÿç†è§£ç”¨æˆ·ç‰¹å®šç‰¹å¾çš„ä¸ªæ€§åŒ–å¯¹è¯åŠ©ç†ï¼Œä»¥å‡†ç¡®è§£é‡Šéœ€æ±‚å¹¶æ ¹æ®ä¸ªäººåå¥½å®šåˆ¶å“åº”ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†é•¿æœŸäº¤äº’çš„å¤æ‚æ€§ï¼Œæ— æ³•æ•æ‰ç”¨æˆ·çš„ä¸»è§‚ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PAL-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°é¢å‘æœåŠ¡çš„åŠ©ç†åœ¨é•¿æœŸç”¨æˆ·ä»£ç†äº¤äº’ä¸­çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚åœ¨ç¼ºä¹å¯ç”¨çš„çœŸå®ä¸–ç•Œæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ­¥éª¤çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆæˆç®¡é“ï¼Œè¯¥ç®¡é“ç»è¿‡äººç±»æ³¨é‡Šè€…çš„éªŒè¯å’Œç»†åŒ–ã€‚è¿™ä¸€è¿‡ç¨‹äº§ç”Ÿäº†PAL-Setï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«å¤šä¼šè¯ç”¨æˆ·æ—¥å¿—å’Œå¯¹è¯å†å²çš„é¦–ä¸ªä¸­æ–‡æ•°æ®é›†ï¼Œå®ƒä½œä¸ºPAL-Benchçš„åŸºç¡€ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ”¹å–„é¢å‘ä¸ªäººçš„æœåŠ¡äº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†H$^2$Memoryï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å±‚å’Œå¼‚æ„çš„å†…å­˜æ¡†æ¶ï¼Œå®ƒç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œä»¥æé«˜ä¸ªæ€§åŒ–å“åº”ç”Ÿæˆã€‚åœ¨æˆ‘ä»¬è‡ªå·±çš„PAL-Benchå’Œå¤–éƒ¨æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è®°å¿†æ¡†æ¶æ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13410v1">PDF</a> Accepted by AAAI 2026 (Oral)</p>
<p><strong>Summary</strong></p>
<p>éšç€æ™ºèƒ½ä¸ªäººè®¾å¤‡çš„æ™®åŠï¼ŒæœåŠ¡å¯¼å‘å‹çš„äººæœºäº¤äº’å˜å¾—è¶Šæ¥è¶Šæ™®éã€‚è¿™å‡¸æ˜¾äº†å¯¹èƒ½å¤Ÿç†è§£ç”¨æˆ·ç‰¹å®šç‰¹å¾ã€å‡†ç¡®è§£è¯»éœ€æ±‚å¹¶æ ¹æ®ä¸ªäººåå¥½å®šåˆ¶å“åº”çš„ä¸ªæ€§åŒ–å¯¹è¯åŠ©æ‰‹çš„éœ€æ±‚ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½è§†é•¿æœŸäº¤äº’çš„å¤æ‚æ€§å’Œç”¨æˆ·ä¸»è§‚ç‰¹å¾çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†PAL-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æœåŠ¡å¯¼å‘å‹åŠ©ç†åœ¨é•¿æœŸç”¨æˆ·ä»£ç†äº¤äº’ä¸­çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚åœ¨æ²¡æœ‰çœŸå®ä¸–ç•Œæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ­¥éª¤çš„LLMåˆæˆç®¡é“ï¼Œå¹¶é€šè¿‡äººå·¥æ ‡æ³¨è¿›è¡Œäº†éªŒè¯å’Œå®Œå–„ï¼Œä»è€Œäº§ç”Ÿäº†PAL-Setæ•°æ®é›†ï¼Œå®ƒæ˜¯åŒ…å«å¤šä¼šè¯ç”¨æˆ·æ—¥å¿—å’Œå¯¹è¯å†å²çš„é¦–ä¸ªä¸­æ–‡æ•°æ®é›†ï¼Œä¸ºPAL-Benchæä¾›äº†åŸºç¡€ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ”¹è¿›ä¸ªæ€§åŒ–çš„æœåŠ¡å¯¼å‘äº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†H^2Memoryæ¡†æ¶ï¼Œå®ƒç»“åˆäº†æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œæé«˜äº†ä¸ªæ€§åŒ–å“åº”ç”Ÿæˆçš„èƒ½åŠ›ã€‚åœ¨PAL-Benchå’Œå¤–éƒ¨æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½ä¸ªäººè®¾å¤‡çš„æ™®åŠä¿ƒè¿›äº†æœåŠ¡å¯¼å‘å‹äººæœºäº’åŠ¨çš„å¢å¤šï¼Œéœ€è¦ä¸ªæ€§åŒ–å¯¹è¯åŠ©æ‰‹æ¥å‡†ç¡®ç†è§£ç”¨æˆ·éœ€æ±‚å’Œåå¥½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨é•¿æœŸäº¤äº’ä¸­å¿½è§†äº†ç”¨æˆ·çš„å¤æ‚æ€§å’Œä¸»è§‚ç‰¹å¾ï¼Œå› æ­¤éœ€è¦æ–°çš„è¯„ä¼°åŸºå‡†æ¥æµ‹è¯•åŠ©ç†çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚</li>
<li>ä»‹ç»äº†PAL-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æœåŠ¡å¯¼å‘å‹åŠ©ç†åœ¨é•¿æœŸç”¨æˆ·ä»£ç†äº¤äº’ä¸­çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ºäº†æ”¯æŒPAL-Benchï¼Œå¼€å‘äº†ä¸€ä¸ªå¤šæ­¥éª¤çš„LLMåˆæˆç®¡é“ï¼Œäº§ç”Ÿäº†é¦–ä¸ªåŒ…å«å¤šä¼šè¯ç”¨æˆ·æ—¥å¿—å’Œå¯¹è¯å†å²çš„ä¸­æ–‡æ•°æ®é›†â€”â€”PAL-Setã€‚</li>
<li>æå‡ºäº†H^2Memoryæ¡†æ¶ï¼Œç»“åˆæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œæé«˜äº†ä¸ªæ€§åŒ–å“åº”ç”Ÿæˆçš„æ•ˆæœã€‚</li>
<li>åœ¨PAL-Benchå’Œå¤–éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†H^2Memoryæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ”¹è¿›æœåŠ¡å¯¼å‘å‹çš„äººæœºäº¤äº’æä¾›äº†é‡è¦çš„è§è§£å’Œå·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ea340a59823671f1f552c1917e8049d" align="middle">
<img src="https://picx.zhimg.com/v2-b6d845203cbe0178bcb1e280bbde038e" align="middle">
<img src="https://picx.zhimg.com/v2-8f3b874b8aa38884dbbd33f48fdb47e8" align="middle">
<img src="https://picx.zhimg.com/v2-683a71ddbbb0b06c723d0cce8136d2d5" align="middle">
<img src="https://picx.zhimg.com/v2-15b19c2e2c5a405671b3db78d403a2bc" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MedDCR-Learning-to-Design-Agentic-Workflows-for-Medical-Coding"><a href="#MedDCR-Learning-to-Design-Agentic-Workflows-for-Medical-Coding" class="headerlink" title="MedDCR: Learning to Design Agentic Workflows for Medical Coding"></a>MedDCR: Learning to Design Agentic Workflows for Medical Coding</h2><p><strong>Authors:Jiyang Zheng, Islam Nassar, Thanh Vu, Xu Zhong, Yang Lin, Tongliang Liu, Long Duong, Yuan-Fang Li</strong></p>
<p>Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.</p>
<blockquote>
<p>åŒ»ç–—ç¼–ç å°†è‡ªç”±æ–‡æœ¬çš„ä¸´åºŠç¬”è®°è½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„è¯Šæ–­å’Œç¨‹åºä»£ç ï¼Œå¯¹äºè®¡è´¹ã€åŒ»é™¢è¿è¥å’ŒåŒ»å­¦ç ”ç©¶è‡³å…³é‡è¦ã€‚ä¸åŒäºæ™®é€šçš„æ–‡æœ¬åˆ†ç±»ï¼Œå®ƒéœ€è¦è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ï¼šæå–è¯Šæ–­æ¦‚å¿µã€åº”ç”¨æŒ‡å—çº¦æŸã€æ˜ å°„åˆ°åˆ†å±‚ä»£ç åº“ï¼Œå¹¶ç¡®ä¿è·¨æ–‡æ¡£çš„ä¸€è‡´æ€§ã€‚æœ€è¿‘çš„è¿›å±•åˆ©ç”¨æ™ºèƒ½ä»£ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å¤§å¤šæ•°ä¾èµ–äºåƒµåŒ–ã€æ‰‹å·¥åˆ¶ä½œçš„å·¥ä½œæµç¨‹ï¼Œæ— æ³•æ•æ‰çœŸå®ä¸–ç•Œæ–‡æ¡£çš„ç»†å¾®å·®åˆ«å’Œå˜åŒ–æ€§ï¼Œç•™ä¸‹äº†å¦‚ä½•ç³»ç»Ÿåœ°å­¦ä¹ æœ‰æ•ˆå·¥ä½œæµç¨‹çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†MedDCRï¼Œè¿™æ˜¯ä¸€ä¸ªé—­ç¯æ¡†æ¶ï¼Œå°†å·¥ä½œæµç¨‹è®¾è®¡è§†ä¸ºå­¦ä¹ é—®é¢˜ã€‚è®¾è®¡è€…æå‡ºå·¥ä½œæµç¨‹ï¼Œç¼–ç è€…æ‰§è¡Œå®ƒä»¬ï¼Œåæ€è€…å¯¹é¢„æµ‹è¿›è¡Œè¯„ä¼°å¹¶æä¾›å»ºè®¾æ€§åé¦ˆï¼ŒåŒæ—¶è®°å¿†å­˜æ¡£ä¿ç•™å…ˆå‰è®¾è®¡ä»¥ä¾›é‡ç”¨å’Œè¿­ä»£ä¼˜åŒ–ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒMedDCRä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œäº§ç”Ÿå¯è§£é‡Šã€å¯é€‚åº”çš„å·¥ä½œæµç¨‹ï¼Œæ›´å¥½åœ°åæ˜ å®é™…ç¼–ç å®è·µï¼Œæé«˜è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13361v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»ç–—ç¼–ç å°†è‡ªç”±æ–‡æœ¬çš„ä¸´åºŠç¬”è®°è½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„è¯Šæ–­å’Œç¨‹åºä»£ç ï¼Œå¯¹äºè®¡è´¹ã€åŒ»é™¢è¿è¥å’ŒåŒ»å­¦ç ”ç©¶è‡³å…³é‡è¦ã€‚å®ƒæ¶‰åŠå¤šæ­¥æ¨ç†ï¼ŒåŒ…æ‹¬æå–è¯Šæ–­æ¦‚å¿µã€åº”ç”¨æŒ‡å¯¼æ–¹é’ˆçº¦æŸã€æ˜ å°„åˆ°å±‚æ¬¡ä»£ç æœ¬å’Œç¡®ä¿è·¨æ–‡æ¡£ä¸€è‡´æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨agentic LLMsï¼Œä½†å¤§å¤šæ•°ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æµç¨‹ï¼Œæ— æ³•æ•æ‰çœŸå®ä¸–ç•Œæ–‡æ¡£çš„ç»†å¾®å·®åˆ«å’Œå˜åŒ–æ€§ã€‚æˆ‘ä»¬æå‡ºMedDCRï¼Œä¸€ä¸ªé—­ç¯æ¡†æ¶ï¼Œå°†å·¥ä½œæµç¨‹è®¾è®¡è§†ä¸ºå­¦ä¹ é—®é¢˜ã€‚è®¾è®¡å¸ˆæå‡ºå·¥ä½œæµç¨‹ï¼Œç¼–ç å‘˜æ‰§è¡Œå®ƒä»¬ï¼Œåå°„å™¨è¯„ä¼°é¢„æµ‹å¹¶æä¾›å»ºè®¾æ€§åé¦ˆï¼ŒåŒæ—¶è®°å¿†å­˜æ¡£ä¿ç•™å…ˆå‰è®¾è®¡ä»¥ä¾›é‡ç”¨å’Œè¿­ä»£æ”¹è¿›ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒMedDCRä¼˜äºæœ€æ–°åŸºçº¿ï¼Œäº§ç”Ÿå¯è§£é‡Šã€å¯é€‚åº”çš„å·¥ä½œæµç¨‹ï¼Œæ›´å¥½åœ°åæ˜ å®é™…ç¼–ç å®è·µï¼Œæé«˜è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—ç¼–ç çš„é‡è¦æ€§ï¼šå°†ä¸´åºŠç¬”è®°è½¬åŒ–ä¸ºæ ‡å‡†åŒ–ä»£ç ï¼Œç”¨äºè®¡è´¹ã€åŒ»é™¢è¿è¥å’ŒåŒ»å­¦ç ”ç©¶ã€‚</li>
<li>åŒ»ç–—ç¼–ç æ¶‰åŠå¤šæ­¥æ¨ç†ï¼šåŒ…æ‹¬è¯Šæ–­æ¦‚å¿µæå–ã€æŒ‡å¯¼æ–¹é’ˆçº¦æŸåº”ç”¨ç­‰ã€‚</li>
<li>å½“å‰ç ”ç©¶çš„ä¸è¶³ï¼šå¤§å¤šæ•°ç ”ç©¶ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æµç¨‹ï¼Œæ— æ³•é€‚åº”çœŸå®ä¸–ç•Œæ–‡æ¡£çš„ç»†å¾®å·®åˆ«å’Œå˜åŒ–æ€§ã€‚</li>
<li>MedDCRæ¡†æ¶çš„æå‡ºï¼šå°†å·¥ä½œæµç¨‹è®¾è®¡è§†ä¸ºå­¦ä¹ é—®é¢˜ï¼ŒåŒ…æ‹¬è®¾è®¡å¸ˆã€ç¼–ç å‘˜ã€åå°„å™¨å’Œè®°å¿†å­˜æ¡£ç­‰è§’è‰²ã€‚</li>
<li>MedDCRæ¡†æ¶çš„æ•ˆæœï¼šåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œäº§ç”Ÿå¯è§£é‡Šã€å¯é€‚åº”çš„å·¥ä½œæµç¨‹ã€‚</li>
<li>MedDCRæ¡†æ¶çš„æ„ä¹‰ï¼šæ›´å¥½åœ°åæ˜ å®é™…ç¼–ç å®è·µï¼Œæé«˜è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1218510dfa67defa23141c1b584ec9f4" align="middle">
<img src="https://picx.zhimg.com/v2-8debf26c5852befb184a55e778aa7a61" align="middle">
<img src="https://picx.zhimg.com/v2-a0afee7cace89393548a3f0a49a333c6" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Grounded-by-Experience-Generative-Healthcare-Prediction-Augmented-with-Hierarchical-Agentic-Retrieval"><a href="#Grounded-by-Experience-Generative-Healthcare-Prediction-Augmented-with-Hierarchical-Agentic-Retrieval" class="headerlink" title="Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval"></a>Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval</h2><p><strong>Authors:Chuang Zhao, Hui Tang, Hongke Zhao, Xiaofang Zhou, Xiaomeng Li</strong></p>
<p>Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.</p>
<blockquote>
<p>å‡†ç¡®çš„åŒ»ç–—é¢„æµ‹å¯¹äºæ”¹å–„æ‚£è€…ç»“æœå’Œé™ä½è¿è¥æˆæœ¬è‡³å…³é‡è¦ã€‚éšç€æ¨ç†èƒ½åŠ›çš„ä¸æ–­å¢é•¿ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡­å€Ÿä¸°å¯Œçš„å‚æ•°çŸ¥è¯†ï¼Œä¸ºæå‡åŒ»ç–—é¢„æµ‹æä¾›äº†ä¸€æ¡æœ‰å‰æ™¯çš„é“è·¯ã€‚ç„¶è€Œï¼Œç”±äºåµŒå…¥çŸ¥è¯†çš„å¯é æ€§å’Œè¦†ç›–èŒƒå›´çš„å±€é™æ€§ï¼ŒLLMå®¹æ˜“å­˜åœ¨äº‹å®é”™è¯¯ã€‚è™½ç„¶æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œå¦‚GraphRAGåŠå…¶å˜ä½“ï¼Œå·²ç»é€šè¿‡èå…¥å¤–éƒ¨çŸ¥è¯†æ¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†åœ¨åŒ»ç–—åœºæ™¯ä¸­ï¼Œå®ƒä»¬é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç¡®å®šæ¿€æ´»æ£€ç´¢æœºåˆ¶çš„ä¸´åºŠå¿…è¦æ€§ï¼›ï¼ˆ2ï¼‰å®ç°æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨ä¹‹é—´çš„ååŒï¼Œä»¥æ„å»ºç¬¦åˆä¸Šä¸‹æ–‡çš„æ£€ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GHARï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆå¼åˆ†å±‚ä»£ç†å¢å¼ºRAGæ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶è§£å†³ä½•æ—¶æ£€ç´¢ä»¥åŠå¦‚ä½•ä¼˜åŒ–åŒ»ç–—é¢†åŸŸå­æ¨¡å—ä¹‹é—´çš„åä½œã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç”±Agent-Topå’ŒAgent-Lowç»„æˆçš„åŒä»£ç†æ¶æ„ã€‚Agent-Topä½œä¸ºä¸»æ²»åŒ»ç”Ÿï¼Œè¿­ä»£åœ°å†³å®šæ˜¯ä¾èµ–å‚æ•°çŸ¥è¯†è¿˜æ˜¯å¯åŠ¨æ£€ç´¢ï¼›è€ŒAgent-Lowåˆ™ä½œä¸ºå’¨è¯¢æœåŠ¡ï¼Œåœ¨è§¦å‘æ£€ç´¢æ—¶æ€»ç»“æ‰€æœ‰ä»»åŠ¡ç›¸å…³çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³ç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°å°†ä¸¤ä¸ªä»£ç†çš„ä¼˜åŒ–ç»Ÿä¸€åœ¨ä¸€ä¸ªæ­£å¼çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­ï¼Œè®¾è®¡å„ç§å¥–åŠ±æ¥å¯¹é½ä»–ä»¬å‡†ç¡®é¢„æµ‹çš„å…±åŒç›®æ ‡ï¼ŒåŒæ—¶ä¿æŒä»–ä»¬å„è‡ªç‹¬ç‰¹çš„è§’è‰²ã€‚åœ¨ä¸‰ä¸ªæµè¡Œä»»åŠ¡çš„ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…ˆè¿›åŸºçº¿ä¹‹ä¸Šå…·æœ‰ä¼˜è¶Šæ€§ï¼Œçªæ˜¾äº†åˆ†å±‚ä»£ç†å¢å¼ºRAGåœ¨æ¨è¿›åŒ»ç–—ç³»ç»Ÿæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13293v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸°å¯Œå‚æ•°çŸ¥è¯†ï¼Œå…¶åœ¨åŒ»ç–—ä¿å¥é¢„æµ‹ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œæœ‰åŠ©äºæ”¹å–„ç—…æ‚£ç»“æœå¹¶é™ä½è¿è¥æˆæœ¬ã€‚ç„¶è€Œï¼ŒLLMå­˜åœ¨äº‹å®ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œå…¶åµŒå…¥çŸ¥è¯†çš„å¯é æ€§å’Œè¦†ç›–é¢æœ‰é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶å¦‚GraphRAGåŠå…¶å˜ä½“æ¥èå…¥å¤–éƒ¨çŸ¥è¯†ã€‚ç„¶è€Œï¼Œåœ¨åŒ»ç–—ä¿å¥åœºæ™¯ä¸­ï¼ŒRAGé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯ç¡®å®šä½•æ—¶å¯åŠ¨æ£€ç´¢æœºåˆ¶çš„ä¸´åºŠå¿…è¦æ€§ï¼ŒäºŒæ˜¯å®ç°æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨ä¹‹é—´çš„ååŒåˆä½œä»¥ç”Ÿæˆç¬¦åˆè¯­å¢ƒçš„æ£€ç´¢ç»“æœã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†GHARï¼Œä¸€ä¸ªç”Ÿæˆå¼åˆ†å±‚ä»£ç†å¢å¼ºRAGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä½•æ—¶æ£€ç´¢ä»¥åŠå¦‚ä½•ä¼˜åŒ–å­æ¨¡å—ååŒçš„é—®é¢˜ã€‚GHARè®¾è®¡äº†ä¸€ä¸ªåŒä»£ç†æ¶æ„å¹¶åˆ›æ–°æ€§åœ°å°†å…¶ç»Ÿä¸€åˆ°ä¸€ä¸ªæ­£å¼çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­ï¼Œä»¥ä¼˜åŒ–ä¸¤ä¸ªä»£ç†çš„æ€§èƒ½å¹¶ä½¿å…¶ç›®æ ‡ä¸€è‡´ã€‚å®éªŒè¯æ˜GHARåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨åŒ»ç–—ä¿å¥ç³»ç»Ÿä¸­æå‡é¢„æµ‹å‡†ç¡®æ€§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—ä¿å¥é¢„æµ‹ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œæœ‰åŠ©äºæ”¹å–„ç—…æ‚£ç»“æœå’Œé™ä½è¿è¥æˆæœ¬ã€‚</li>
<li>LLMå­˜åœ¨äº‹å®ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œéœ€è¦èå…¥å¤–éƒ¨çŸ¥è¯†æ¥è§£å†³ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶å¦‚GraphRAGé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸´åºŠå¿…è¦æ€§ç¡®å®šå’Œæ£€ç´¢ä¸ç”Ÿæˆçš„ååŒåˆä½œã€‚</li>
<li>GHARæ¡†æ¶é€šè¿‡åŒä»£ç†æ¶æ„è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå…¶ä¸­Agent-Topå†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢ï¼ŒAgent-Lowä½œä¸ºå’¨è¯¢æœåŠ¡è¿›è¡ŒçŸ¥è¯†æ€»ç»“ã€‚</li>
<li>GHARåˆ›æ–°æ€§åœ°ç»Ÿä¸€ä¸¤ä¸ªä»£ç†çš„ä¼˜åŒ–è¿‡ç¨‹åˆ°ä¸€ä¸ªæ­£å¼çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­ï¼Œè®¾è®¡å¥–åŠ±ä»¥å¯¹é½é¢„æµ‹å‡†ç¡®æ€§ç›®æ ‡å¹¶ä¿ç•™å…¶ç‹¬ç‰¹è§’è‰²ã€‚</li>
<li>å®éªŒè¯æ˜GHARåœ¨åŒ»ç–—ä¿å¥é¢„æµ‹ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf12425be97e7b4f96a2f3f1ee01f2a7" align="middle">
<img src="https://picx.zhimg.com/v2-fb171711dcc0e42b04ba1bf7e8a71627" align="middle">
<img src="https://picx.zhimg.com/v2-6a64ec778aeda9e8bb28c4e2eeccd12c" align="middle">
<img src="https://picx.zhimg.com/v2-42f3f9d652f6cffb2024a1a93f8b4e4e" align="middle">
<img src="https://picx.zhimg.com/v2-3c53047cde85c65b092a39e3a4ab58b1" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO"><a href="#Multi-Agent-Deep-Research-Training-Multi-Agent-Systems-with-M-GRPO" class="headerlink" title="Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO"></a>Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO</h2><p><strong>Authors:Haoyang Hong, Jiajun Yin, Yuan Wang, Jingnan Liu, Zhe Chen, Ailing Yu, Ji Li, Zhiling Ye, Hansong Xiao, Yefei Chen, Hualei Zhou, Yun Yue, Minghui Yang, Chunxiao Guo, Junwei Liu, Peng Wei, Jinjie Gu</strong></p>
<p>Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.</p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é€šç”¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚ç„¶è€Œï¼Œç¼ºä¹ç‰¹å®šé¢†åŸŸçš„è®­ç»ƒä¼šé™åˆ¶å…¶å‡†ç¡®æ€§ã€‚å½“å‰è®­ç»ƒæ–¹æ³•æ˜¯ä¸ºç³»ç»Ÿä¸­çš„æ‰€æœ‰æ™ºèƒ½ä½“è®­ç»ƒä¸€ä¸ªç»Ÿä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¿™å¯èƒ½ä¼šå› ä¸ºä¸åŒæ™ºèƒ½ä½“åº•å±‚çš„æ•°æ®åˆ†å¸ƒä¸åŒè€Œé™åˆ¶æ€§èƒ½ã€‚å› æ­¤ï¼Œä½¿ç”¨ä¸åŒçš„LLMæ¥è®­ç»ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåº”è¯¥æ˜¯ä¸‹ä¸€æ­¥è¦è§£å†³çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¼•å…¥äº†ä¼˜åŒ–æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œæ™ºèƒ½ä½“ä»¥ä¸åŒçš„é¢‘ç‡è¿è¡Œï¼Œå›æ»šæ¶‰åŠå„ç§å­æ™ºèƒ½ä½“çš„è°ƒç”¨ï¼Œå¹¶ä¸”æ™ºèƒ½ä½“é€šå¸¸éƒ¨ç½²åœ¨ä¸åŒçš„æœåŠ¡å™¨ä¸Šï¼Œç ´åç«¯åˆ°ç«¯çš„æ¢¯åº¦æµã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†M-GRPOï¼Œè¿™æ˜¯ä¸€ç§é¢å‘å‚ç›´å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åˆ†å±‚æ‰©å±•æ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªä¸»æ™ºèƒ½ä½“ï¼ˆè§„åˆ’å™¨ï¼‰å’Œå¤šä¸ªå­æ™ºèƒ½ä½“ï¼ˆå¤šè½®å·¥å…·æ‰§è¡Œå™¨ï¼‰ã€‚M-GRPOè®¡ç®—ä¸»æ™ºèƒ½ä½“å’Œå­æ™ºèƒ½ä½“çš„ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ï¼Œä¿æŒåˆ†å±‚ä¿¡ç”¨åˆ†é…ã€‚å®ƒè¿˜å¼•å…¥äº†ä¸€ç§è½¨è¿¹å¯¹é½æ–¹æ¡ˆï¼Œå¯ä»¥ç”Ÿæˆå›ºå®šå¤§å°çš„æ‰¹æ¬¡ï¼Œå°½ç®¡å­æ™ºèƒ½ä½“çš„è°ƒç”¨æ˜¯å¯å˜çš„ã€‚æˆ‘ä»¬é‡‡ç”¨è§£è€¦çš„è®­ç»ƒç®¡é“ï¼Œæ™ºèƒ½ä½“åœ¨å•ç‹¬çš„æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œå¹¶é€šè¿‡å…±äº«å­˜å‚¨äº¤æ¢æœ€å°‘çš„ç»Ÿè®¡ä¿¡æ¯ã€‚è¿™èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè·¨æœåŠ¡å™¨åå‘ä¼ æ’­çš„æƒ…å†µä¸‹å®ç°å¯æ‰©å±•çš„è®­ç»ƒã€‚åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ï¼ˆå¦‚GAIAã€XBench-DeepSearchå’ŒWebWalkerQAï¼‰çš„å®éªŒä¸­ï¼ŒM-GRPOæŒç»­ä¼˜äºå•æ™ºèƒ½ä½“GRPOå’Œå¤šæ™ºèƒ½ä½“GRPOå†»ç»“å­æ™ºèƒ½ä½“ï¼Œæ˜¾ç¤ºå‡ºæé«˜çš„ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯¹é½å¼‚æ„è½¨è¿¹å¹¶åœ¨ä¸“ä¸šæ™ºèƒ½ä½“ä¹‹é—´è§£è€¦ä¼˜åŒ–ï¼Œèƒ½å¤Ÿå¢å¼ºå·¥å…·è¾…åŠ©æ¨ç†ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13288v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é€šç”¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œä½†ç”±äºç¼ºä¹ä¸“ä¸šåŸ¹è®­ï¼Œå…¶å‡†ç¡®æ€§å—åˆ°é˜»ç¢ã€‚å½“å‰è®­ç»ƒæ–¹æ³•æ˜¯é’ˆå¯¹ç³»ç»Ÿå†…æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ä½†ä¸åŒçš„æ™ºèƒ½ä½“æœ‰ä¸åŒçš„åˆ†å¸ƒç‰¹å¾ï¼Œå› æ­¤é‡‡ç”¨ç»Ÿä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½é™åˆ¶äº†ç³»ç»Ÿçš„æ€§èƒ½ã€‚å› æ­¤ï¼Œä¸ºä¸åŒæ™ºèƒ½ä½“è®­ç»ƒä¸åŒçš„LLMæˆä¸ºè§£å†³é—®é¢˜çš„ä¸‹ä¸€æ­¥æ–¹å‘ã€‚ç„¶è€Œï¼Œæ­¤æ–¹æ³•å¸¦æ¥ä¼˜åŒ–æŒ‘æˆ˜ï¼Œå¦‚æ™ºèƒ½ä½“æ“ä½œé¢‘ç‡ä¸åŒã€å›æ»šæ¶‰åŠä¸åŒçš„å­æ™ºèƒ½ä½“è°ƒç”¨ä»¥åŠæ™ºèƒ½ä½“åœ¨ç‹¬ç«‹æœåŠ¡å™¨ä¸Šéƒ¨ç½²ï¼Œç ´åäº†ç«¯åˆ°ç«¯çš„æ¢¯åº¦æµã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†M-GRPOï¼ˆé’ˆå¯¹å‚ç›´å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åˆ†å±‚æ‰©å±•ï¼‰ï¼ŒåŒ…æ‹¬ä¸»æ™ºèƒ½ä½“ï¼ˆè§„åˆ’å™¨ï¼‰å’Œå¤šä¸ªå­æ™ºèƒ½ä½“ï¼ˆå¤šè½®å·¥å…·æ‰§è¡Œå™¨ï¼‰ã€‚M-GRPOè®¡ç®—ä¸»å’Œå­æ™ºèƒ½ä½“çš„ç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿ï¼Œç»´æŒå±‚æ¬¡åŒ–ä¿¡ç”¨åˆ†é…ï¼Œå¹¶å¼•å…¥è½¨è¿¹å¯¹é½æ–¹æ¡ˆä»¥ç”Ÿæˆå›ºå®šå¤§å°çš„æ‰¹æ¬¡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šï¼ŒM-GRPOè¡¨ç°å‡ºæ›´ç¨³å®šçš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½æ”¹å–„ã€‚ç»“æœè¯æ˜äº†åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œåœ¨ç‰¹æ®Šæ™ºèƒ½ä½“ä¸Šå¯¹é½å¼‚æ„å›¾è½¨å’Œè§£é™¤ä¼˜åŒ–è€¦åˆå¯ä»¥å¢å¼ºå·¥å…·è¾…åŠ©æ¨ç†ä»»åŠ¡çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é€šç”¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸç¼ºä¹è®­ç»ƒä¼šå½±å“å‡†ç¡®æ€§ã€‚</li>
<li>å½“å‰è®­ç»ƒæ–¹æ³•æ˜¯ç»Ÿä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯èƒ½æ— æ³•é€‚åº”ä¸åŒæ™ºèƒ½ä½“çš„åˆ†å¸ƒç‰¹å¾ã€‚</li>
<li>ä¸ºä¸åŒæ™ºèƒ½ä½“è®­ç»ƒä¸åŒçš„LLMæ˜¯è§£å†³æ­¤é—®é¢˜çš„ä¸‹ä¸€æ­¥æ–¹å‘ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“è®­ç»ƒé¢ä¸´ä¼˜åŒ–æŒ‘æˆ˜ï¼Œå¦‚æ“ä½œé¢‘ç‡ä¸åŒã€å›æ»šæ¶‰åŠä¸åŒå­æ™ºèƒ½ä½“è°ƒç”¨ä»¥åŠè·¨æœåŠ¡å™¨éƒ¨ç½²é—®é¢˜ã€‚</li>
<li>M-GRPOæ˜¯è§£å†³è¿™äº›é—®é¢˜çš„åˆ†å±‚æ‰©å±•æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¸»æ™ºèƒ½ä½“å’Œå¤šä¸ªå­æ™ºèƒ½ä½“ã€‚</li>
<li>M-GRPOé€šè¿‡è®¡ç®—ç¾¤ç»„ç›¸å¯¹ä¼˜åŠ¿ç»´æŒå±‚æ¬¡åŒ–ä¿¡ç”¨åˆ†é…ï¼Œå¹¶å¼•å…¥è½¨è¿¹å¯¹é½æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-025ccf26e259c60a13108aff633b8b39" align="middle">
<img src="https://picx.zhimg.com/v2-f404ff05a47d9d14aaf1c0a4a474a8f1" align="middle">
<img src="https://picx.zhimg.com/v2-658f493b7f08405a48f19f9125a583e0" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DualTAP-A-Dual-Task-Adversarial-Protector-for-Mobile-MLLM-Agents"><a href="#DualTAP-A-Dual-Task-Adversarial-Protector-for-Mobile-MLLM-Agents" class="headerlink" title="DualTAP: A Dual-Task Adversarial Protector for Mobile MLLM Agents"></a>DualTAP: A Dual-Task Adversarial Protector for Mobile MLLM Agents</h2><p><strong>Authors:Fuyao Zhang, Jiaming Zhang, Che Wang, Xiongtao Sun, Yurong Hao, Guowei Guan, Wenjie Li, Longtao Huang, Wei Yang Bryan Lim</strong></p>
<p>The reliance of mobile GUI agents on Multimodal Large Language Models (MLLMs) introduces a severe privacy vulnerability: screenshots containing Personally Identifiable Information (PII) are often sent to untrusted, third-party routers. These routers can exploit their own MLLMs to mine this data, violating user privacy. Existing privacy perturbations fail the critical dual challenge of this scenario: protecting PII from the routerâ€™s MLLM while simultaneously preserving task utility for the agentâ€™s MLLM. To address this gap, we propose the Dual-Task Adversarial Protector (DualTAP), a novel framework that, for the first time, explicitly decouples these conflicting objectives. DualTAP trains a lightweight generator using two key innovations: (i) a contrastive attention module that precisely identifies and targets only the PII-sensitive regions, and (ii) a dual-task adversarial objective that simultaneously minimizes a task-preservation loss (to maintain agent utility) and a privacy-interference loss (to suppress PII leakage). To facilitate this study, we introduce PrivScreen, a new dataset of annotated mobile screenshots designed specifically for this dual-task evaluation. Comprehensive experiments on six diverse MLLMs (e.g., GPT-5) demonstrate DualTAPâ€™s state-of-the-art protection. It reduces the average privacy leakage rate by 31.6 percentage points (a 3.0x relative improvement) while, critically, maintaining an 80.8% task success rate - a negligible drop from the 83.6% unprotected baseline. DualTAP presents the first viable solution to the privacy-utility trade-off in mobile MLLM agents.</p>
<blockquote>
<p>ç§»åŠ¨GUIä»£ç†å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¾èµ–å¼•å…¥äº†ä¸€ä¸ªä¸¥é‡çš„éšç§æ¼æ´ï¼šåŒ…å«ä¸ªäººè¯†åˆ«ä¿¡æ¯ï¼ˆPIIï¼‰çš„æˆªå›¾ç»å¸¸è¢«å‘é€åˆ°ä¸å—ä¿¡ä»»çš„ç¬¬ä¸‰æ–¹è·¯ç”±å™¨ã€‚è¿™äº›è·¯ç”±å™¨å¯ä»¥åˆ©ç”¨è‡ªå·±çš„MLLMsæ¥æŒ–æ˜è¿™äº›æ•°æ®ï¼Œä¾µçŠ¯ç”¨æˆ·éšç§ã€‚ç°æœ‰çš„éšç§å¹²æ‰°æªæ–½æ— æ³•åº”å¯¹è¿™ä¸€åœºæ™¯ä¸­çš„åŒé‡æŒ‘æˆ˜ï¼šå³åœ¨ä¿æŠ¤PIIå…å—è·¯ç”±å™¨MLLMçš„åŒæ—¶ï¼ŒåŒæ—¶ä¿æŒä»£ç†MLLMçš„ä»»åŠ¡æ•ˆç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†Dual-Task Adversarial Protectorï¼ˆDualTAPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé¦–æ¬¡æ˜¾å¼åœ°è§£å†³äº†è¿™äº›ç›¸äº’å†²çªçš„ç›®æ ‡ã€‚DualTAPé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹è®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§çš„ç”Ÿæˆå™¨ï¼šï¼ˆiï¼‰ä¸€ä¸ªå¯¹æ¯”æ³¨æ„åŠ›æ¨¡å—ï¼Œç²¾ç¡®è¯†åˆ«å¹¶ä»…é’ˆå¯¹PIIæ•æ„ŸåŒºåŸŸï¼›ï¼ˆiiï¼‰ä¸€ä¸ªåŒé‡ä»»åŠ¡å¯¹æŠ—ç›®æ ‡ï¼ŒåŒæ—¶æœ€å°åŒ–ä»»åŠ¡ä¿ç•™æŸå¤±ï¼ˆä»¥ç»´æŒä»£ç†æ•ˆç”¨ï¼‰å’Œéšç§å¹²æ‰°æŸå¤±ï¼ˆä»¥æŠ‘åˆ¶PIIæ³„æ¼ï¼‰ã€‚ä¸ºäº†æ¨åŠ¨è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†PrivScreenï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¿™é¡¹åŒé‡ä»»åŠ¡è¯„ä¼°è®¾è®¡çš„å¸¦æ³¨é‡Šçš„ç§»åŠ¨æˆªå›¾æ–°æ•°æ®é›†ã€‚åœ¨å…­ç§ä¸åŒçš„MLLMsï¼ˆä¾‹å¦‚GPT-5ï¼‰ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDualTAPå…·æœ‰æœ€å…ˆè¿›çš„ä¿æŠ¤èƒ½åŠ›ã€‚å®ƒå°†å¹³å‡éšç§æ³„éœ²ç‡é™ä½äº†31.6ä¸ªç™¾åˆ†ç‚¹ï¼ˆç›¸å¯¹æé«˜äº†3.0å€ï¼‰ï¼ŒåŒæ—¶å…³é”®åœ°ä¿æŒäº†80.8%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œè¿™ä¸€æ•°å­—ä¸æœªç»ä¿æŠ¤çš„åŸºå‡†83.6%ç›¸æ¯”å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚DualTAPä¸ºç§»åŠ¨MLLMä»£ç†çš„éšç§æ•ˆç”¨æƒè¡¡é—®é¢˜æä¾›äº†ç¬¬ä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13248v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç§»åŠ¨GUIä»£ç†ä¾èµ–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ‰€å¸¦æ¥çš„ä¸¥é‡éšç§æ¼æ´é—®é¢˜ã€‚ç§»åŠ¨è®¾å¤‡çš„æˆªå›¾å¯èƒ½åŒ…å«ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰ï¼Œè¿™äº›ä¿¡æ¯åœ¨å‘é€åˆ°ç¬¬ä¸‰æ–¹è·¯ç”±å™¨æ—¶å¯èƒ½è¢«åˆ©ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DualTAPæ¡†æ¶ï¼Œé€šè¿‡ç²¾ç¡®å®šä½æ•æ„ŸåŒºåŸŸå’Œå¹³è¡¡ä»»åŠ¡ä¿å­˜ä¸éšç§å¹²æ‰°æŸå¤±æ¥é¦–æ¬¡æ˜ç¡®è§£å†³è¿™ä¸€å†²çªç›®æ ‡ã€‚å®éªŒè¡¨æ˜ï¼ŒDualTAPåœ¨å‡å°‘éšç§æ³„éœ²çš„åŒæ—¶ï¼Œå‡ ä¹ä¸å½±å“ä»»åŠ¡æˆåŠŸç‡ã€‚è¿™ä¸ºç§»åŠ¨MLLMä»£ç†çš„éšç§æ•ˆç”¨æƒè¡¡æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨GUIä»£ç†ä¾èµ–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­˜åœ¨éšç§æ³„éœ²é£é™©ã€‚</li>
<li>ç°æœ‰éšç§æ‰°åŠ¨æŠ€æœ¯æ— æ³•åŒæ—¶ä¿æŠ¤ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰å’Œä¿æŒä»»åŠ¡æ•ˆç”¨ã€‚</li>
<li>DualTAPæ¡†æ¶é€šè¿‡ç²¾ç¡®å®šä½æ•æ„ŸåŒºåŸŸå’Œå¹³è¡¡ä»»åŠ¡ä¿å­˜ä¸éšç§å¹²æ‰°æŸå¤±æ¥è§£å†³è¿™ä¸€å†²çªã€‚</li>
<li>DualTAPä½¿ç”¨å¯¹æ¯”æ³¨æ„åŠ›æ¨¡å—å’ŒåŒä»»åŠ¡å¯¹æŠ—ç›®æ ‡æ¥å®ç°é«˜æ•ˆä¿æŠ¤ã€‚</li>
<li>æ–°æ•°æ®é›†PrivScreenç”¨äºè¯„ä¼°è¿™ç§åŒé‡ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDualTAPåœ¨å‡å°‘éšç§æ³„éœ²æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœï¼Œç›¸å¯¹æ”¹è¿›äº†3.0å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13248">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f106e5b1d1e0a2643f27391321b3a1f" align="middle">
<img src="https://picx.zhimg.com/v2-5bdaab468568cc266620dba3684fc33b" align="middle">
<img src="https://picx.zhimg.com/v2-c40a1649c8641224b2ff6e89d4c629a2" align="middle">
<img src="https://picx.zhimg.com/v2-1d0b0f109d568c3a1b69285b43d1bd9c" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Cost-Effective-Communication-An-Auction-based-Method-for-Language-Agent-Interaction"><a href="#Cost-Effective-Communication-An-Auction-based-Method-for-Language-Agent-Interaction" class="headerlink" title="Cost-Effective Communication: An Auction-based Method for Language Agent Interaction"></a>Cost-Effective Communication: An Auction-based Method for Language Agent Interaction</h2><p><strong>Authors:Yijia Fan, Jusheng Zhang, Kaitong Cai, Jing Yang, Chengpei Tang, Jian Wang, Keze Wang</strong></p>
<p>Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient â€œfree-for-allâ€ communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that â€œfreeâ€ communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.</p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å¸¸å¸¸å—åˆ°ä½æ•ˆçš„â€œè‡ªç”±æ”¾ä»»â€é€šä¿¡çš„å½±å“ï¼Œå¯¼è‡´æŒ‡æ•°çº§çš„ä»¤ç‰Œæˆæœ¬ä¸Šå‡å’Œä¿¡å·ä¸å™ªéŸ³æ¯”ä¾‹å¤±è¡¡ï¼Œä»è€Œé˜»ç¢äº†å…¶å®è·µéƒ¨ç½²ã€‚æˆ‘ä»¬è´¨ç–‘è¶Šå¤šé€šä¿¡æ€»æ˜¯è¶Šæœ‰ç›Šçš„è§‚ç‚¹ï¼Œå¹¶æå‡ºå‡è®¾è®¤ä¸ºæ ¸å¿ƒé—®é¢˜åœ¨äºèµ„æºç†æ€§çš„ç¼ºå¤±ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œâ€œå…è´¹â€é€šä¿¡å¿½è§†äº†ç¨€ç¼ºæ€§åŸåˆ™ï¼Œå¤©ç”Ÿå°±å¯¼è‡´äº†æ•ˆç‡ä½ä¸‹å’Œä¸å¿…è¦çš„å¼€æ”¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŠ¨æ€æ‹å–çš„è¯­è¨€æ™ºèƒ½ä½“ï¼ˆDALAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå°†é€šä¿¡å¸¦å®½è§†ä¸ºç¨€ç¼ºä¸”å¯äº¤æ˜“çš„èµ„æºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„DALAå°†æ™ºèƒ½ä½“é—´çš„é€šä¿¡è§†ä¸ºé›†ä¸­æ‹å–ï¼Œæ™ºèƒ½ä½“æ ¹æ®æ¶ˆæ¯çš„é¢„æœŸä»·å€¼å¯†åº¦å­¦ä¹ ç«ä»·ä»¥è·å¾—è¯´è¯çš„æœºä¼šã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„DALAé¼“åŠ±æ™ºèƒ½ä½“å‘å‡ºç®€æ´ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ¶ˆæ¯ï¼ŒåŒæ—¶è¿‡æ»¤æ‰ä½ä»·å€¼çš„é€šä¿¡ã€‚å¹¿æ³›è€Œå…¨é¢çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»æµé©±åŠ¨çš„DALAåœ¨ä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°æ€§èƒ½ï¼ŒåŒ…æ‹¬MMLUä¸Šçš„84.32%å’ŒHumanEvalä¸Šçš„91.21%çš„pass@1ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯ä»¥æƒŠäººçš„æ•ˆç‡å®ç°çš„ï¼Œå³æˆ‘ä»¬çš„DALAä»…ä½¿ç”¨625ä¸‡ä»¤ç‰Œï¼Œæ˜¯GSM8Kä¸Šå½“å‰æœ€æ–°æ–¹æ³•æ‰€æ¶ˆè€—èµ„æºçš„ä¸€å°éƒ¨åˆ†ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DALAåŸ¹å…»äº†æˆ˜ç•¥æ²‰é»˜çš„çªå‘æŠ€èƒ½ï¼Œé€šè¿‡èµ„æºçº¦æŸåŠ¨æ€åœ°è°ƒæ•´å…¶é€šä¿¡ç­–ç•¥ï¼Œä»å†—é•¿åˆ°æ²‰é»˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13193v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå› æ²Ÿé€šæ•ˆç‡ä½ä¸‹è€Œé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œæ²Ÿé€šä¸­çš„â€œè‡ªç”±æ”¾ä»»â€å¯¼è‡´æŒ‡æ•°çº§çš„ç¬¦å·æˆæœ¬ä¸ä¿¡å·å™ªå£°æ¯”å¤±è¡¡ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºèµ„æºç†æ€§çš„æ¦‚å¿µï¼Œè®¤ä¸ºå…è´¹çš„æ²Ÿé€šå¿½è§†äº†ç¨€ç¼ºæ€§åŸåˆ™ï¼Œå¯¼è‡´æ²Ÿé€šæ•ˆç‡ä½ä¸‹å’Œä¸å¿…è¦çš„æ”¯å‡ºã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶å¼•å…¥åŠ¨æ€æ‹å–è¯­è¨€æ™ºèƒ½ä½“ï¼ˆDALAï¼‰æ¡†æ¶ï¼Œå°†æ²Ÿé€šå¸¦å®½è§†ä¸ºç¨€ç¼ºä¸”å¯äº¤æ˜“èµ„æºã€‚DALAé€šè¿‡é›†ä¸­æ‹å–çš„æ–¹å¼å¤„ç†æ™ºèƒ½ä½“é—´çš„æ²Ÿé€šï¼Œæ™ºèƒ½ä½“æ ¹æ®ä¿¡æ¯çš„é¢„æµ‹ä»·å€¼å¯†åº¦è¿›è¡Œå‘è¨€ç«ä»·å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼Œç»æµé©±åŠ¨çš„DALAåœ¨ä¸ƒä¸ªæŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒåŒ…æ‹¬MMLUçš„84.32%å’ŒHumanEvalçš„91.21%çš„pass@1ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDALAä»¥é«˜æ•ˆè‘—ç§°ï¼Œä»…ä½¿ç”¨å½“å‰æœ€å…ˆè¿›çš„GSM8Kæ–¹æ³•çš„èµ„æºæ¶ˆè€—ä¸­çš„ä¸€å°éƒ¨åˆ†ï¼ˆä»…æ¶ˆè€—äº†å…­ç™¾ä¸‡åˆ†ä¹‹ä¸€çš„ä»¤ç‰Œï¼‰ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒDALAèƒ½å¤ŸåŸ¹å…»æ™ºèƒ½æ²‰é»˜è¿™ä¸€æŠ€èƒ½ï¼Œé€šè¿‡èµ„æºçº¦æŸåŠ¨æ€è°ƒæ•´æ²Ÿé€šç­–ç•¥ï¼Œä»å†—é•¿åˆ°æ²‰é»˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šé­é‡æ²Ÿé€šæ•ˆç‡é—®é¢˜ï¼Œâ€è‡ªç”±æ”¾ä»»â€å¼çš„æ²Ÿé€šå¯¼è‡´æˆæœ¬ä¸Šå‡å’Œä¿¡å·å™ªå£°æ¯”å¤±è¡¡ã€‚</li>
<li>æå‡ºèµ„æºç†æ€§çš„æ¦‚å¿µï¼Œå¼ºè°ƒæ²Ÿé€šåº”è€ƒè™‘åˆ°èµ„æºçš„ç¨€ç¼ºæ€§ã€‚</li>
<li>å¼•å…¥åŠ¨æ€æ‹å–è¯­è¨€æ™ºèƒ½ä½“ï¼ˆDALAï¼‰æ¡†æ¶ï¼Œå°†æ²Ÿé€šè§†ä¸ºé›†ä¸­æ‹å–è¿‡ç¨‹ï¼Œæ™ºèƒ½ä½“åŸºäºä¿¡æ¯ä»·å€¼å¯†åº¦è¿›è¡Œå‘è¨€å†³ç­–ã€‚</li>
<li>DALAæ¡†æ¶é€šè¿‡é¼“åŠ±äº§ç”Ÿç®€æ´ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ¶ˆæ¯æ¥è¿‡æ»¤æ‰ä½ä»·å€¼çš„æ²Ÿé€šã€‚</li>
<li>DALAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†é«˜æ•ˆä¸”é«˜æ€§èƒ½çš„æ²Ÿé€šæ•ˆæœã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDALAæ˜¾è‘—å‡å°‘äº†èµ„æºæ¶ˆè€—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2be261579f0bbf1bed911d99dc3086a4" align="middle">
<img src="https://picx.zhimg.com/v2-354e9fc6eb733c001ecd1bc2c8a28eb7" align="middle">
<img src="https://picx.zhimg.com/v2-6a74aaf58822c5a0357db7512c62d689" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Conditional-Diffusion-Model-for-Multi-Agent-Dynamic-Task-Decomposition"><a href="#Conditional-Diffusion-Model-for-Multi-Agent-Dynamic-Task-Decomposition" class="headerlink" title="Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition"></a>Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition</h2><p><strong>Authors:Yanda Zhu, Yuanyang Zhu, Daoyi Dong, Caihua Chen, Chunlin Chen</strong></p>
<p>Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.</p>
<blockquote>
<p>ä»»åŠ¡åˆ†è§£åœ¨å¤æ‚çš„åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½¿åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­çš„é•¿æœŸä»»åŠ¡èƒ½å¤Ÿè¿›è¡Œé«˜æ•ˆåˆ†å±‚å­¦ä¹ ã€‚ç„¶è€Œï¼Œä»å¤´å¼€å§‹å­¦ä¹ åŠ¨æ€ä»»åŠ¡åˆ†è§£é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ ·æœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨éƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹æ¢ç´¢å·¨å¤§çš„è”åˆåŠ¨ä½œç©ºé—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€ä»»åŠ¡åˆ†è§£çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆC$\text{D}^\text{3}$Tï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤çº§åˆ†å±‚MARLæ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨æ¨æ–­å­ä»»åŠ¡å’Œåè°ƒæ¨¡å¼ã€‚é«˜çº§ç­–ç•¥å­¦ä¹ å­ä»»åŠ¡è¡¨ç¤ºï¼ŒåŸºäºå­ä»»åŠ¡æ•ˆæœç”Ÿæˆå­ä»»åŠ¡é€‰æ‹©ç­–ç•¥ã€‚ä¸ºäº†æ•æ‰å­ä»»åŠ¡å¯¹ç¯å¢ƒçš„å½±å“ï¼ŒC$\text{D}^\text{3}$Tä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè§‚å¯Ÿå’Œå¥–åŠ±ã€‚åœ¨ä½çº§ï¼Œæ™ºèƒ½ä½“åœ¨å…¶åˆ†é…çš„å­ä»»åŠ¡å†…åä½œå­¦ä¹ å¹¶å…±äº«ä¸“ä¸šæŠ€èƒ½ã€‚æ­¤å¤–ï¼Œå­¦ä¹ åˆ°çš„å­ä»»åŠ¡è¡¨ç¤ºè¿˜è¢«ç”¨ä½œå¤šå¤´æ³¨æ„åŠ›æ··åˆç½‘ç»œä¸­çš„é™„åŠ è¯­ä¹‰ä¿¡æ¯ï¼Œä»¥å¢å¼ºä»·å€¼åˆ†è§£ï¼Œå¹¶åœ¨ä¸ªä½“å’Œè”åˆå€¼å‡½æ•°ä¹‹é—´æä¾›æœ‰æ•ˆçš„æ¨ç†æ¡¥æ¢ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒC$\text{D}^\text{3}$Tçš„æ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13137v1">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong><br>ä»»åŠ¡åˆ†è§£åœ¨å¤æ‚çš„åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œèƒ½ä¿ƒè¿›é•¿æœŸè§†é‡ä¸‹çš„é«˜æ•ˆå±‚æ¬¡æ€§å­¦ä¹ ã€‚æœ¬è®ºæ–‡æå‡ºäº†åŠ¨æ€ä»»åŠ¡åˆ†è§£çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆCD^3Tï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ä¸¤çº§å±‚æ¬¡åŒ–çš„MARLæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨æ¨æ–­å­ä»»åŠ¡å’Œåè°ƒæ¨¡å¼ã€‚é«˜å±‚ç­–ç•¥åŸºäºå­ä»»åŠ¡æ•ˆæœå­¦ä¹ å­ä»»åŠ¡è¡¨ç¤ºï¼Œç”Ÿæˆå­ä»»åŠ¡é€‰æ‹©ç­–ç•¥ã€‚CD^3Tä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè§‚å¯Ÿå’Œå¥–åŠ±ï¼Œä»¥æ•æ‰å­ä»»åŠ¡å¯¹ç¯å¢ƒçš„å½±å“ã€‚åœ¨ä½å±‚æ¬¡ä¸Šï¼Œæ™ºèƒ½ä½“åœ¨å…¶åˆ†é…çš„å­ä»»åŠ¡å†…å­¦ä¹ å¹¶å…±äº«ä¸“ä¸šæŠ€èƒ½ã€‚æ­¤å¤–ï¼Œå­¦ä¹ çš„å­ä»»åŠ¡è¡¨ç¤ºè¿˜ç”¨ä½œå¤šå¤´æ³¨æ„åŠ›æ··åˆç½‘ç»œçš„é™„åŠ è¯­ä¹‰ä¿¡æ¯ï¼Œä»¥å¢å¼ºä»·å€¼åˆ†è§£å¹¶æä¾›ä¸ªä½“å’Œè”åˆä»·å€¼å‡½æ•°ä¹‹é—´çš„æœ‰æ•ˆæ¨ç†æ¡¥æ¢ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCD^3Tçš„æ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»»åŠ¡åˆ†è§£åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­å±•ç°æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ã€åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­è¿›è¡Œé•¿æœŸä»»åŠ¡å­¦ä¹ æ—¶ã€‚</li>
<li>åŠ¨æ€ä»»åŠ¡åˆ†è§£çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆCD^3Tï¼‰æ˜¯ä¸€ä¸ªä¸¤çº§å±‚æ¬¡åŒ–çš„MARLæ¡†æ¶ï¼Œå¯è‡ªåŠ¨æ¨æ–­å­ä»»åŠ¡å’Œåè°ƒæ¨¡å¼ã€‚</li>
<li>é«˜å±‚ç­–ç•¥å­¦ä¹ å­ä»»åŠ¡è¡¨ç¤ºï¼ŒåŸºäºå­ä»»åŠ¡æ•ˆæœç”Ÿæˆå­ä»»åŠ¡é€‰æ‹©ç­–ç•¥ã€‚</li>
<li>CD^3Té€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªè§‚å¯Ÿå’Œå¥–åŠ±æ¥æ•æ‰å­ä»»åŠ¡å¯¹ç¯å¢ƒçš„å½±å“ã€‚</li>
<li>ä½å±‚æ¬¡ä¸Šï¼Œæ™ºèƒ½ä½“åœ¨åˆ†é…çš„å­ä»»åŠ¡å†…å­¦ä¹ å’Œå…±äº«ä¸“ä¸šæŠ€èƒ½ã€‚</li>
<li>å­¦ä¹ çš„å­ä»»åŠ¡è¡¨ç¤ºç”¨äºå¢å¼ºä»·å€¼åˆ†è§£å¹¶æä¾›ä¸ªä½“å’Œè”åˆä»·å€¼å‡½æ•°ä¹‹é—´çš„æ¨ç†æ¡¥æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cd0b37280f097b8359e9cb92bf94dd7" align="middle">
<img src="https://picx.zhimg.com/v2-4f7f263c4be05f60ecf7900fe3cf1e40" align="middle">
<img src="https://picx.zhimg.com/v2-cae5787bc6363fd532a3e7cc30406b8b" align="middle">
<img src="https://picx.zhimg.com/v2-36d01a203c91f7ef747001f473a49b3f" align="middle">
<img src="https://picx.zhimg.com/v2-9eeefbe1c380e3653089f4f255aff19f" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Extracting-Events-Like-Code-A-Multi-Agent-Programming-Framework-for-Zero-Shot-Event-Extraction"><a href="#Extracting-Events-Like-Code-A-Multi-Agent-Programming-Framework-for-Zero-Shot-Event-Extraction" class="headerlink" title="Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction"></a>Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction</h2><p><strong>Authors:Quanjiang Guo, Sijie Wang, Jinchuan Zhang, Ben Zhang, Zhao Kang, Ling Tian, Ke Yan</strong></p>
<p>Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputsâ€“such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasksâ€“retrieval, planning, coding, and verificationâ€“each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on <a target="_blank" rel="noopener" href="https://github.com/UESTC-GQJ/Agent-Event-Coder">https://github.com/UESTC-GQJ/Agent-Event-Coder</a>.</p>
<blockquote>
<p>é›¶äº‹ä»¶æå–ï¼ˆZSEEï¼‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™éœ€è¦å¯¹å¤æ‚æ¨ç†å’Œç‰¹å®šé¢†åŸŸçš„ç†è§£ã€‚ç›´æ¥æç¤ºå¾€å¾€ä¼šäº§ç”Ÿä¸å®Œæ•´æˆ–ç»“æ„æ— æ•ˆçš„è¾“å‡ºï¼Œå¦‚é”™è¯¯åˆ†ç±»çš„è§¦å‘å™¨ã€ç¼ºå°‘å‚æ•°å’Œè¿åæ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Agent-Event-Coderï¼ˆAECï¼‰è¿™ä¸€æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ƒå°†äº‹ä»¶æå–è§†ä¸ºè½¯ä»¶å·¥ç¨‹ï¼šä¸€ä¸ªç»“æ„åŒ–ã€è¿­ä»£çš„ä»£ç ç”Ÿæˆè¿‡ç¨‹ã€‚AECå°†ZSEEåˆ†è§£ä¸ºä¸“é—¨çš„ä»»åŠ¡â€”â€”æ£€ç´¢ã€è§„åˆ’ã€ç¼–ç å’ŒéªŒè¯ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½ç”±ä¸€ä¸ªä¸“é—¨çš„LLMæ™ºèƒ½ä½“å¤„ç†ã€‚äº‹ä»¶æ¨¡å¼è¢«è¡¨ç¤ºä¸ºå¯æ‰§è¡Œçš„ç±»å®šä¹‰ï¼Œé€šè¿‡éªŒè¯æ™ºèƒ½ä½“å®ç°ç¡®å®šæ€§éªŒè¯å’Œç²¾ç¡®åé¦ˆã€‚è¿™ç§å—ç¼–ç¨‹å¯å‘çš„åšæ³•å…è®¸é€šè¿‡è¿­ä»£ç»†åŒ–è¿›è¡Œç³»ç»Ÿçš„æ¶ˆæ­§å’Œæ¨¡å¼å¼ºåˆ¶æ‰§è¡Œã€‚é€šè¿‡åˆ©ç”¨åä½œæ™ºèƒ½ä½“çš„å·¥ä½œæµç¨‹ï¼ŒAECä½¿LLMèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸­äº§ç”Ÿç²¾ç¡®ã€å®Œæ•´ä¸”æ¨¡å¼ä¸€è‡´çš„äº‹ä»¶æå–ç»“æœã€‚åœ¨äº”ä¸ªä¸åŒé¢†åŸŸå’Œå…­ä¸ªLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAECå§‹ç»ˆä¼˜äºå…ˆå‰çš„é›¶æ ·æœ¬åŸºçº¿ï¼Œå±•ç¤ºäº†å°†äº‹ä»¶æå–è§†ä¸ºä»£ç ç”Ÿæˆçš„åŠ›é‡ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/UESTC-GQJ/Agent-Event-Coder%E4%B8%8A%E3%80%82">https://github.com/UESTC-GQJ/Agent-Event-Coderä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13118v1">PDF</a> 11 pages, 5 figures, accepted by AAAI 2026 (Oral)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé›¶æ ·æœ¬äº‹ä»¶æŠ½å–ï¼ˆZSEEï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‘æˆ˜ï¼Œå¦‚éœ€è¦å¤æ‚æ¨ç†å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œç›´æ¥æç¤ºå¾€å¾€äº§ç”Ÿä¸å®Œæ•´æˆ–ç»“æ„æ— æ•ˆçš„è¾“å‡ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Agent-Event-Coderï¼ˆAECï¼‰è¿™ä¸€æ–°é¢–çš„å¤šä»£ç†æ¡†æ¶ï¼Œå®ƒå°†äº‹ä»¶æŠ½å–è§†ä¸ºè½¯ä»¶å·¥ç¨‹èˆ¬çš„ç»“æ„åŒ–ã€è¿­ä»£ä»£ç ç”Ÿæˆè¿‡ç¨‹ã€‚AECå°†ZSEEåˆ†è§£ä¸ºä¸“ä¸šåŒ–çš„å­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡ç”±ä¸“é—¨çš„LLMä»£ç†å¤„ç†ã€‚äº‹ä»¶æ¨¡å¼è¢«è¡¨ç¤ºä¸ºå¯æ‰§è¡Œçš„ç±»å®šä¹‰ï¼Œé€šè¿‡éªŒè¯ä»£ç†å®ç°ç¡®å®šæ€§éªŒè¯å’Œç²¾ç¡®åé¦ˆã€‚è¿™ç§ç¼–ç¨‹å¯å‘çš„æ–¹æ³•å…è®¸é€šè¿‡è¿­ä»£ç»†åŒ–è¿›è¡Œç³»ç»Ÿçš„æ¶ˆæ­§å’Œæ¨¡å¼å¼ºåˆ¶æ‰§è¡Œã€‚é€šè¿‡åˆ©ç”¨åä½œå¼ä»£ç†å·¥ä½œæµç¨‹ï¼ŒAECä½¿LLMsèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ç”Ÿæˆç²¾ç¡®ã€å®Œæ•´ä¸”ç¬¦åˆæ¨¡å¼çš„äº‹ä»¶æŠ½å–ç»“æœã€‚åœ¨äº”ä¸ªä¸åŒé¢†åŸŸå’Œå…­ä¸ªLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAECæŒç»­ä¼˜äºå…ˆå‰çš„é›¶æ ·æœ¬åŸºçº¿ï¼Œå±•ç¤ºäº†å°†äº‹ä»¶æŠ½å–è§†ä¸ºä»£ç ç”Ÿæˆçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZSEEå¯¹LLMsè€Œè¨€æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› å®ƒè¦æ±‚å¤æ‚çš„æ¨ç†å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>ç›´æ¥æç¤ºå¯èƒ½å¯¼è‡´è¾“å‡ºä¸å®Œæ•´æˆ–ç»“æ„æ— æ•ˆã€‚</li>
<li>AECæ˜¯ä¸€ä¸ªå¤šä»£ç†æ¡†æ¶ï¼Œå°†äº‹ä»¶æŠ½å–æ¯”ä½œè½¯ä»¶å·¥ç¨‹çš„ä»£ç ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>AECå°†ZSEEä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡ç”±ä¸“é—¨çš„LLMä»£ç†å¤„ç†ã€‚</li>
<li>äº‹ä»¶æ¨¡å¼è¢«è¡¨ç¤ºä¸ºå¯æ‰§è¡Œçš„ç±»å®šä¹‰ï¼Œä»¥å®ç°ç¡®å®šæ€§éªŒè¯å’Œç²¾ç¡®åé¦ˆã€‚</li>
<li>AECé€šè¿‡è¿­ä»£ç»†åŒ–ä¿ƒè¿›ç³»ç»Ÿæ¶ˆæ­§å’Œæ¨¡å¼å¼ºåˆ¶æ‰§è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90bb16931f32df12bf0530cd1f272a1b" align="middle">
<img src="https://picx.zhimg.com/v2-4b6779b43c9b59d6cd2a063a22318911" align="middle">
<img src="https://picx.zhimg.com/v2-652b0d8b2e84990616839e3006ab0982" align="middle">
<img src="https://picx.zhimg.com/v2-263b8054960170637b74d106bdb8c5dd" align="middle">
<img src="https://picx.zhimg.com/v2-d74505b9111be7bc75de6ff946e62f06" align="middle">
<img src="https://picx.zhimg.com/v2-f94b355af56752b67e2e524f97f675b4" align="middle">
<img src="https://picx.zhimg.com/v2-861bb5a60b50e65d8157e56b3397814d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DeepSport-A-Multimodal-Large-Language-Model-for-Comprehensive-Sports-Video-Reasoning-via-Agentic-Reinforcement-Learning"><a href="#DeepSport-A-Multimodal-Large-Language-Model-for-Comprehensive-Sports-Video-Reasoning-via-Agentic-Reinforcement-Learning" class="headerlink" title="DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning"></a>DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning</h2><p><strong>Authors:Junbo Zou, Haotian Xia, Zhen Ye, Shengjie Zhang, Christopher Lai, Vicente Ordonez, Weining Shen, Hanjie Chen</strong></p>
<p>Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to &#96;&#96;think with videosâ€™â€™ by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the modelâ€™s reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.</p>
<blockquote>
<p>è¿åŠ¨è§†é¢‘ç†è§£å‘ˆç°å‡ºç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ¨¡å‹æ„ŸçŸ¥é«˜é€ŸåŠ¨æ€ï¼Œç†è§£å¤æ‚è§„åˆ™ï¼Œå¹¶åœ¨é•¿æ—¶é—´èƒŒæ™¯ä¸‹è¿›è¡Œæ¨ç†ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬é¢†åŸŸè¡¨ç°å‡ºäº†æ½œåŠ›ï¼Œä½†ä½“è‚²é¢†åŸŸçš„ç ”ç©¶ç°çŠ¶ä»ç„¶å±€é™äºç‰¹å®šçš„èŒƒå›´ï¼šç°æœ‰æ–¹æ³•è¦ä¹ˆæ˜¯å•ä¸­å¿ƒä½“è‚²çš„ï¼Œä»…é™äºç‰¹å®šä»»åŠ¡ï¼Œè¦ä¹ˆä¾èµ–äºç¼ºä¹å¥å£®æ€§å­¦ä¹ çš„è®­ç»ƒå…è´¹æ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DeepSportï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹å¤šä»»åŠ¡ã€å¤šä½“è‚²è§†é¢‘ç†è§£è€Œè®¾è®¡çš„ç«¯åˆ°ç«¯è®­ç»ƒMLLMæ¡†æ¶ã€‚DeepSportæ”¹å˜äº†ä»è¢«åŠ¨å¸§å¤„ç†åˆ°ä¸»åŠ¨è¿­ä»£æ¨ç†çš„æ¨¡å¼è½¬å˜ï¼Œé€šè¿‡ä¸“ç”¨çš„å¸§æå–å·¥å…·åŠ¨æ€åœ°è¯¢é—®å†…å®¹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿâ€œä¸è§†é¢‘ä¸€èµ·æ€è€ƒâ€ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ•°æ®è’¸é¦ç®¡é“ï¼Œè¯¥ç®¡é“ä»åä¸ªä¸åŒçš„æ•°æ®æºä¸­åˆæˆé«˜è´¨é‡çš„æ€ç»´é“¾è½¨è¿¹ï¼Œåˆ›å»ºäº†7.8ä¸‡ä¸ªè®­ç»ƒæ•°æ®çš„ç»Ÿä¸€èµ„æºã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå³ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ–°å‹çš„é—¨æ§å·¥å…·ä½¿ç”¨å¥–åŠ±è¿›è¡Œä¼˜åŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚åœ¨åŒ…å«6700ä¸ªé—®é¢˜çš„æµ‹è¯•åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDeepSportè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºé’ˆå¯¹å„ç§ä½“è‚²è¿åŠ¨çš„é¢†åŸŸç‰¹å®šè§†é¢‘æ¨ç†å»ºç«‹äº†æ–°çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12908v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä½“è‚²è§†é¢‘ç†è§£çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é«˜é€ŸåŠ¨æ€æ„ŸçŸ¥ã€å¤æ‚è§„åˆ™ç†è§£å’Œé•¿æœŸä¸Šä¸‹æ–‡æ¨ç†ç­‰ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•å•ä¸€è¿åŠ¨ä¸­å¿ƒåŒ–ã€ä»»åŠ¡ç‰¹å®šæˆ–ç¼ºä¹å¼ºå¤§æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤šä»»åŠ¡ã€å¤šè¿åŠ¨çš„ç«¯åˆ°ç«¯è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶â€”â€”DeepSportã€‚DeepSporté‡‡ç”¨ä¸»åŠ¨è¿­ä»£æ¨ç†çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸“ç”¨å¸§æå–å·¥å…·åŠ¨æ€è¯¢é—®è§†é¢‘å†…å®¹ï¼Œå®ç°è¢«åŠ¨å¸§å¤„ç†å‘ä¸»åŠ¨æ€è€ƒçš„è½¬å˜ã€‚å®éªŒè¯æ˜ï¼ŒDeepSportåœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹çš„åŸºçº¿ã€‚æœ¬æ–‡ä¸ºç‰¹å®šé¢†åŸŸçš„è§†é¢‘æ¨ç†æä¾›äº†æ–°çš„åŸºç¡€ï¼Œä»¥åº”å¯¹å„ç§è¿åŠ¨çš„å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½“è‚²è§†é¢‘ç†è§£é¢ä¸´æ„ŸçŸ¥é«˜é€ŸåŠ¨æ€ã€ç†è§£å¤æ‚è§„åˆ™å’Œæ¨ç†é•¿æœŸä¸Šä¸‹æ–‡çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬é¢†åŸŸå·²æœ‰åº”ç”¨å‰æ™¯ï¼Œä½†åœ¨ä½“è‚²é¢†åŸŸçš„ç ”ç©¶ä»å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>DeepSportæ˜¯é¦–ä¸ªä¸ºå¤šä»»åŠ¡ã€å¤šè¿åŠ¨è§†é¢‘ç†è§£è®¾è®¡çš„ç«¯åˆ°ç«¯è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ã€‚</li>
<li>DeepSporté‡‡ç”¨ä¸»åŠ¨è¿­ä»£æ¨ç†çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸“ç”¨å¸§æå–å·¥å…·å¯¹è§†é¢‘å†…å®¹è¿›è¡ŒåŠ¨æ€è¯¢é—®ï¼Œå®ç°äº†ä»è¢«åŠ¨å¸§å¤„ç†åˆ°ä¸»åŠ¨æ€è€ƒçš„è½¬å˜ã€‚</li>
<li>DeepSporté€šè¿‡æ•°æ®è’¸é¦ç®¡é“åˆæˆé«˜è´¨é‡çš„æ€è€ƒè½¨è¿¹æ•°æ®ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDeepSportåœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7dbc1198273dea32c81649268bf40e1" align="middle">
<img src="https://picx.zhimg.com/v2-2ba44be1b40b622bfd621d3e6885151f" align="middle">
<img src="https://picx.zhimg.com/v2-ad38841829f4ecf01b4615a46b03e1de" align="middle">
<img src="https://picx.zhimg.com/v2-22a8cfba51094b5d0d050984c6e04300" align="middle">
<img src="https://picx.zhimg.com/v2-5af6de474ee44d8433495901636bb7f1" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Hybrid-Retrieval-Augmented-Generation-Agent-for-Trustworthy-Legal-Question-Answering-in-Judicial-Forensics"><a href="#Hybrid-Retrieval-Augmented-Generation-Agent-for-Trustworthy-Legal-Question-Answering-in-Judicial-Forensics" class="headerlink" title="Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics"></a>Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics</h2><p><strong>Authors:Yueqing Xi, Yifan Bai, Huasen Luo, Weiliang Wen, Hui Liu, Haoliang Li</strong></p>
<p>As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios.</p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½åœ¨å¸æ³•é‰´å®šä¸­çš„æ™®åŠï¼Œç¡®ä¿æ³•å¾‹é—®ç­”ï¼ˆQAï¼‰çš„çœŸå®æ€§å’Œå¯è¿½æº¯æ€§å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è‡ªç„¶è¯­è¨€æ¨¡å‹å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œè¯¯å¯¼æ³•å¾‹å’¨è¯¢æ–¹å‘ï¼Œè€Œé™æ€çš„çŸ¥è¯†åº“åˆ™éš¾ä»¥è·Ÿä¸Šä¸æ–­æ›´æ–°çš„æ³•è§„å’Œåˆ¤ä¾‹æ³•ã€‚æˆ‘ä»¬é’ˆå¯¹å¸æ³•ç¯å¢ƒè®¾è®¡äº†ä¸€ç§æ··åˆæ³•å¾‹é—®ç­”ä»£ç†ï¼Œå®ƒç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ¨¡å‹é›†æˆï¼Œä»¥æä¾›å¯é ã€å¯å®¡æ ¸å’ŒæŒç»­æ›´æ–°çš„å’¨è¯¢ã€‚è¯¥ç³»ç»Ÿä¼˜å…ˆè¿›è¡Œæ£€ç´¢è€Œéç”Ÿæˆï¼šå½“å¯ä¿¡èµ–çš„æ³•å¾‹å­˜å‚¨åº“äº§ç”Ÿç›¸å…³è¯æ®æ—¶ï¼Œç­”æ¡ˆå°†é€šè¿‡RAGäº§ç”Ÿï¼›å¦åˆ™ï¼Œå¤šä¸ªè‡ªç„¶è¯­è¨€æ¨¡å‹ç”Ÿæˆå€™é€‰ç­”æ¡ˆï¼Œç”±ä¸“ç”¨é€‰æ‹©å™¨è¿›è¡Œè¯„åˆ†ï¼Œå¹¶è¿”å›æ’åæœ€é«˜çš„ç­”æ¡ˆã€‚é«˜è´¨é‡è¾“å‡ºä¼šç»è¿‡äººå·¥å®¡æ ¸åå†å†™å›åˆ°çŸ¥è¯†åº“ï¼Œå®ç°åŠ¨æ€çŸ¥è¯†æ¼”è¿›å’Œæº¯æºè·Ÿè¸ªã€‚åœ¨æ³•å¾‹é—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ··åˆæ–¹æ³•æ˜¾è‘—ä¼˜äºå•æ¨¡å‹åŸºå‡†å’ŒåŸå§‹RAGç®¡é“ï¼Œåœ¨F1ã€ROUGE-Lå’Œâ€œè‡ªç„¶è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜â€æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚æ¶ˆèå®éªŒè¯å®äº†æ£€ç´¢ä¼˜å…ˆã€æ¨¡å‹é›†æˆå’Œäººå·¥å¾ªç¯æ›´æ–°æœºåˆ¶çš„äº’è¡¥ä½œç”¨ã€‚æ‰€æå‡ºçš„ç³»ç»Ÿæ˜¾è‘—å‡å°‘äº†å¹»è§‰ï¼Œæé«˜äº†ç­”æ¡ˆè´¨é‡å’Œæ³•å¾‹åˆè§„æ€§ï¼Œæ¨åŠ¨äº†åª’ä½“å–è¯æŠ€æœ¯åœ¨å¸æ³•åœºæ™¯ä¸­çš„å®é™…åº”ç”¨è½åœ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01668v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šäººå·¥æ™ºèƒ½åœ¨å¸æ³•é‰´å®šä¸­çš„èå…¥è¶Šæ¥è¶Šæ™®éï¼Œå¯¹æ³•å¾‹é—®ç­”çš„çœŸå®æ€§å’Œå¯è¿½æº¯æ€§è¦æ±‚æ„ˆåŠ ä¸¥æ ¼ã€‚ä¼ ç»Ÿçš„é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ˜“å‡ºç°å¹»æƒ³è¯¯å¯¼æ³•å¾‹å’¨è¯¢ï¼Œè€Œé™æ€çŸ¥è¯†åº“éš¾ä»¥è·Ÿä¸Šä¸æ–­æ›´æ–°çš„æ³•å¾‹å’Œåˆ¤ä¾‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¸æ³•ç¯å¢ƒçš„æ··åˆæ³•å¾‹é—®ç­”ä»£ç†ï¼Œèåˆäº†æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ä¸å¤šæ¨¡å‹é›†æˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿæä¾›å¯é ã€å¯å®¡è®¡å’ŒæŒç»­æ›´æ–°çš„å’¨è¯¢æ„è§ã€‚è¯¥ç³»ç»Ÿä¼˜å…ˆæ£€ç´¢ç›¸å…³è¯æ®ï¼Œå¹¶é€šè¿‡ç”Ÿæˆå’Œé€‰æ‹©æ¨¡å‹å›ç­”é—®é¢˜ã€‚é«˜è´¨é‡è¾“å‡ºé€šè¿‡äººå·¥å®¡æ ¸åå†™å…¥çŸ¥è¯†åº“ï¼Œå®ç°åŠ¨æ€çŸ¥è¯†æ›´æ–°å’Œæº¯æºè·Ÿè¸ªã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¸æ³•é—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå•ä¸€æ¨¡å‹å’Œçº¯æ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“ã€‚è¯¥ç³»ç»Ÿçš„æå‡ºæ˜¾è‘—å‡å°‘äº†è¯¯è§£ç°è±¡ï¼Œæé«˜äº†ç­”æ¡ˆè´¨é‡å’Œæ³•å¾‹åˆè§„æ€§ï¼Œä¿ƒè¿›äº†åª’ä½“å–è¯æŠ€æœ¯åœ¨å¸æ³•åœºæ™¯çš„å®é™…åº”ç”¨è½åœ°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨å¸æ³•é¢†åŸŸçš„è¿ç”¨æ—¥ç›Šæ™®åŠï¼Œæ³•å¾‹é—®ç­”çš„çœŸå®æ€§å’Œå¯è¿½æº¯æ€§æˆä¸ºå…³é”®è€ƒé‡å› ç´ ã€‚</li>
<li>ä¼ ç»Ÿå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¸æ³•é—®ç­”ä¸­å­˜åœ¨ç¼ºé™·ï¼Œæ˜“äº§ç”Ÿè¯¯å¯¼ä¿¡æ¯ã€‚</li>
<li>é™æ€çŸ¥è¯†åº“éš¾ä»¥é€‚åº”æ³•å¾‹å’Œåˆ¤ä¾‹é¢‘ç¹æ›´æ–°çš„éœ€æ±‚ã€‚</li>
<li>æ··åˆæ³•å¾‹é—®ç­”ä»£ç†é€šè¿‡æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯å’Œå¤šæ¨¡å‹é›†æˆæŠ€æœ¯æä¾›å¯é ç­”æ¡ˆã€‚</li>
<li>ç³»ç»Ÿä¼˜å…ˆæ£€ç´¢è¯æ®ï¼Œç»“åˆç”Ÿæˆå’Œé€‰æ‹©æ¨¡å‹å›ç­”é—®é¢˜ï¼Œç¡®ä¿ç­”æ¡ˆè´¨é‡ã€‚</li>
<li>é«˜è´¨é‡ç­”æ¡ˆé€šè¿‡äººå·¥å®¡æ ¸åæ›´æ–°è‡³çŸ¥è¯†åº“ï¼Œå®ç°çŸ¥è¯†åŠ¨æ€æ›´æ–°å’Œæº¯æºè·Ÿè¸ªã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae8fd9cec8e10fff8b1a623cb033d89c" align="middle">
<img src="https://picx.zhimg.com/v2-068bd82b086b4992c61dac976ce19158" align="middle">
<img src="https://picx.zhimg.com/v2-c32b60ab5cd8171cc7acff22cb220c35" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Unintended-Misalignment-from-Agentic-Fine-Tuning-Risks-and-Mitigation"><a href="#Unintended-Misalignment-from-Agentic-Fine-Tuning-Risks-and-Mitigation" class="headerlink" title="Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation"></a>Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</h2><p><strong>Authors:Dongyoon Hahm, Taywon Min, Woogyeol Jin, Kimin Lee</strong></p>
<p>Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.</p>
<blockquote>
<p>éšç€ç®€å•æ–‡æœ¬ç”Ÿæˆçš„è¿›é˜¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¿›åŒ–æˆèƒ½å¤Ÿè§„åˆ’å¹¶ä¸å¤–éƒ¨å·¥å…·è¿›è¡Œäº¤äº’ä»¥è§£å†³å¤æ‚ä»»åŠ¡çš„ä»£ç†ç³»ç»Ÿã€‚è¿™ç§è¿›åŒ–æ¶‰åŠå¯¹ç‰¹å®šä»£ç†ä»»åŠ¡çš„å¾®è°ƒï¼Œä»¥å¢å¼ºå…¶ç†Ÿç»ƒç¨‹åº¦ã€‚ç„¶è€Œï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå®‰å…¨é—®é¢˜ç»å¸¸è¢«å¿½è§†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œå¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½ä¼šæ— æ„ä¸­å¤±å»å¯¹é½ï¼Œå¯¼è‡´æ‰§è¡Œæœ‰å®³ä»»åŠ¡çš„å¯èƒ½æ€§å¢åŠ ï¼Œå¹¶ä¸”åœ¨æ‰§è¡Œä»£ç†ä»»åŠ¡è¿›è¡Œå¾®è°ƒæ—¶æ‹’ç»å®ƒä»¬çš„å€¾å‘é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›å®‰å…¨æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Prefix INjection Guardï¼ˆPINGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå‘ä»£ç†å“åº”è‡ªåŠ¨ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€å‰ç¼€ï¼ŒæŒ‡å¯¼å®ƒä»¬æ‹’ç»æœ‰å®³çš„è¯·æ±‚ï¼ŒåŒæ—¶ä¿ç•™åœ¨è‰¯æ€§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§è¿­ä»£æ–¹æ³•ï¼Œäº¤æ›¿è¿›è¡Œï¼ˆ1ï¼‰ç”Ÿæˆå€™é€‰å‰ç¼€å’Œï¼ˆ2ï¼‰é€‰æ‹©é‚£äº›æ—¢èƒ½ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½åˆèƒ½ä¼˜åŒ–æ‹’ç»è¡Œä¸ºçš„å‰ç¼€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPINGåœ¨ä¸å½±å“å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†æœ‰æ•ˆæ€§çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†å…¶å®‰å…¨æ€§ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ— è®ºæ˜¯åœ¨ç½‘é¡µå¯¼èˆªè¿˜æ˜¯ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒPINGéƒ½ä¼˜äºç°æœ‰çš„æç¤ºæ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡çº¿æ€§æ¢é’ˆå¯¹å†…éƒ¨éšè—çŠ¶æ€çš„åˆ†æè¡¨æ˜ï¼Œå‰ç¼€ä»¤ç‰Œå¯¹è¡Œä¸ºä¿®æ”¹è‡³å…³é‡è¦ï¼Œè¿™è§£é‡Šäº†æ€§èƒ½æå‡çš„åŸå› ã€‚è­¦å‘Šï¼šæœ¬æ–‡åŒ…å«æœ¬è´¨ä¸Šä¸é“å¾·æˆ–å…·æœ‰æ”»å‡»æ€§çš„å†…å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14031v2">PDF</a> Accepted at AAAI 2026 AI Alignment Track, Source code: <a target="_blank" rel="noopener" href="https://github.com/HahmDY/agentic-ft-safety">https://github.com/HahmDY/agentic-ft-safety</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²è¿›åŒ–ä¸ºèƒ½å¤Ÿè§„åˆ’å¹¶ä¸å¤–éƒ¨å·¥å…·äº¤äº’ä»¥å®Œæˆå¤æ‚ä»»åŠ¡çš„ä»£ç†ç³»ç»Ÿã€‚ç„¶è€Œï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå®‰å…¨æ€§é—®é¢˜å¸¸è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œå¯¹é½çš„LLMså¯èƒ½ä¼šæ„å¤–åœ°å‡ºç°ä¸å¯¹é½çš„æƒ…å†µï¼Œå¯¼è‡´æ›´å®¹æ˜“æ‰§è¡Œæœ‰å®³ä»»åŠ¡ï¼Œå¹¶å‡å°‘æ‹’ç»å®ƒä»¬çš„å€¾å‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Prefix INjection Guardï¼ˆPINGï¼‰æ–¹æ³•ï¼Œé€šè¿‡å‘ä»£ç†å“åº”è‡ªåŠ¨ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€å‰ç¼€æ¥æŒ‡å¯¼å®ƒä»¬æ‹’ç»æœ‰å®³çš„è¯·æ±‚ï¼ŒåŒæ—¶åœ¨è‰¯æ€§ä»»åŠ¡ä¸Šä¿æŒæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPINGåœ¨ä¸å½±å“LLMä»£ç†æ•ˆç‡çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†å…¶å®‰å…¨æ€§ã€‚å¯¹æ¯”ç°æœ‰çš„æç¤ºæ–¹æ³•ï¼ŒPINGåœ¨å„ç§ç½‘ç»œå¯¼èˆªå’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸­éƒ½è¡¨ç°æ›´ä½³ã€‚é€šè¿‡çº¿æ€§æ¢é’ˆåˆ†æå†…éƒ¨éšè—çŠ¶æ€ï¼Œæˆ‘ä»¬å‘ç°å‰ç¼€ä»¤ç‰Œå¯¹äºè¡Œä¸ºä¿®æ”¹è‡³å…³é‡è¦ï¼Œè¿™ä¹Ÿè§£é‡Šäº†æ€§èƒ½çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²è¿›åŒ–ä¸ºèƒ½å¤Ÿå®Œæˆå¤æ‚ä»»åŠ¡çš„ä»£ç†ç³»ç»Ÿï¼Œæ¶‰åŠè§„åˆ’åŠä¸å¤–éƒ¨å·¥å…·äº¤äº’ã€‚</li>
<li>åœ¨å¾®è°ƒLLMsä»¥å®Œæˆç‰¹å®šä»»åŠ¡æ—¶ï¼Œå®‰å…¨æ€§é—®é¢˜å¸¸è¢«å¿½è§†ã€‚</li>
<li>å¯¹é½çš„LLMså¯èƒ½ä¼šæ„å¤–å‡ºç°ä¸å¯¹é½ï¼Œå¯¼è‡´æ›´å®¹æ˜“æ‰§è¡Œæœ‰å®³ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºPINGçš„æ–¹æ³•ï¼Œé€šè¿‡å‘ä»£ç†å“åº”æ·»åŠ è‡ªç„¶è¯­è¨€å‰ç¼€æ¥æŒ‡å¯¼LLMsæ‹’ç»æœ‰å®³è¯·æ±‚ã€‚</li>
<li>PINGæ–¹æ³•èƒ½åœ¨ä¸ç‰ºç‰²LLMä»£ç†æ•ˆç‡çš„æƒ…å†µä¸‹æé«˜å®‰å…¨æ€§ã€‚</li>
<li>åœ¨å¤šç§ä»»åŠ¡ä¸­ï¼ŒPINGçš„è¡¨ç°ä¼˜äºç°æœ‰æç¤ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c15749f207c8b25253e361eec50a300" align="middle">
<img src="https://picx.zhimg.com/v2-18af583f414c1a938bce8ba6ffa2b798" align="middle">
<img src="https://picx.zhimg.com/v2-3bbf53ae725b8260dd77b490feefe5bc" align="middle">
<img src="https://picx.zhimg.com/v2-ea51d26467b56fe133b5965f6152d0d3" align="middle">
<img src="https://picx.zhimg.com/v2-8609c1d3d0807ebc05f24a807b7a9642" align="middle">
<img src="https://picx.zhimg.com/v2-029eef36190fcd30755ecfc62c69f8e1" align="middle">
<img src="https://picx.zhimg.com/v2-1d8f45223d7970fa8ebefe9935400111" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning"><a href="#PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning" class="headerlink" title="PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning"></a>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</h2><p><strong>Authors:Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu</strong></p>
<p>Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.</p>
<blockquote>
<p>ç°æœ‰å·¥å…·å¢å¼ºå‹æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦åŒ…æ‹¬ï¼ˆiï¼‰é»‘ç®±æ¨ç†æ­¥éª¤æŸå®³äº†å†³ç­–çš„å¯ä¿¡åº¦å¹¶å¸¦æ¥å®‰å…¨é£é™©ï¼Œï¼ˆiiï¼‰ç¼ºä¹è‰¯å¥½çš„å¤šæ¨¡å¼é›†æˆï¼Œè¿™å¯¹åŒ»ç–—ä»»åŠ¡æ¥è¯´è‡³å…³é‡è¦ï¼Œï¼ˆiiiï¼‰æ™ºèƒ½ç®¡é“åˆšæ€§ä¸”è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æ¨å‡ºPASSï¼ˆæ¦‚ç‡æ™ºèƒ½ä½“è¶…ç½‘é‡‡æ ·ï¼‰ï¼Œè¿™æ˜¯é’ˆå¯¹èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ¨ç†ä¸­è¿™äº›æŒ‘æˆ˜çš„ç¬¬ä¸€ä¸ªå¤šæ¨¡å¼æ¡†æ¶ã€‚PASSè‡ªé€‚åº”åœ°åœ¨å¤šå·¥å…·å›¾ä¸Šé‡‡æ ·æ™ºèƒ½ä½“å·¥ä½œæµç¨‹ï¼Œäº§ç”Ÿå¸¦æœ‰å¯è§£é‡Šæ¦‚ç‡çš„å†³ç­–è·¯å¾„ã€‚é’ˆå¯¹å…·æœ‰å¤šæ¨¡å¼åŒ»ç–—æ•°æ®çš„å¤æ‚CXRæ¨ç†ä»»åŠ¡ï¼ŒPASSåˆ©ç”¨å…¶åœ¨æ™ºèƒ½ä½“è¶…ç½‘ä¸Šçš„å­¦ä¹ åˆ°çš„ä»»åŠ¡æ¡ä»¶åˆ†å¸ƒã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥åœ¨æ¯ä¸ªè¶…ç½‘å±‚ä¸Šè‡ªé€‚åº”åœ°é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼Œä¸ºäº‹åå®¡è®¡æä¾›æ¦‚ç‡æ³¨é‡Šè½¨è¿¹ï¼Œå¹¶ç›´æ¥æé«˜åŒ»ç–—äººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§ã€‚PASSè¿˜ä¼šä¸æ–­å°†é‡è¦å‘ç°å‹ç¼©æˆä¸æ–­å‘å±•çš„ä¸ªæ€§åŒ–è®°å¿†ï¼ŒåŒæ—¶åŠ¨æ€å†³å®šæ˜¯æ·±åŒ–å…¶æ¨ç†è·¯å¾„è¿˜æ˜¯æå‰é€€å‡ºä»¥æé«˜æ•ˆç‡ã€‚ä¸ºäº†ä¼˜åŒ–å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹çš„ä¸‰é˜¶æ®µè®­ç»ƒç¨‹åºï¼ŒåŒ…æ‹¬ä¸“å®¶çŸ¥è¯†é¢„çƒ­ã€å¯¹æ¯”è·¯å¾„æ’åå’Œæˆæœ¬æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ã€‚ä¸ºäº†è¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†CAB-Eï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šæ­¥ã€å®‰å…¨å…³é”®çš„è‡ªç”±å½¢å¼CXRæ¨ç†çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒPASSåœ¨å¤šä¸ªæŒ‡æ ‡ï¼ˆä¾‹å¦‚å‡†ç¡®æ€§ã€AUCã€LLM-Jï¼‰ä¸Šæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ŒåŒæ—¶å¹³è¡¡äº†è®¡ç®—æˆæœ¬ï¼Œæœç€å¯è§£é‡Šã€è‡ªé€‚åº”å’Œå¤šæ¨¡å¼åŒ»ç–—æ™ºèƒ½ä½“ç³»ç»Ÿçš„æ–°èŒƒå¼è½¬å˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10501v3">PDF</a> </p>
<p><strong>Summary</strong><br>    PASSæ¡†æ¶è§£å†³äº†ç°æœ‰å·¥å…·å¢å¼ºä»£ç†ç³»ç»Ÿæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¼ºä¹ä¿¡ä»»ã€å¤šæ¨¡å¼æ•´åˆä¸è¶³ã€ä»£ç†ç®¡é“åƒµåŒ–åŠè®¡ç®—æ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é’ˆå¯¹èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰æ¨ç†ä¸Šä¸‹æ–‡æå‡ºæ¦‚ç‡ä»£ç†è¶…ç½‘é‡‡æ ·æŠ€æœ¯ã€‚å®ƒè‡ªé€‚åº”åœ°åœ¨å¤šå·¥å…·å›¾ä¸Šé‡‡æ ·ä»£ç†å·¥ä½œæµç¨‹ï¼Œå¹¶é€šè¿‡è§£é‡Šæ€§æ¦‚ç‡æ ‡æ³¨å†³ç­–è·¯å¾„ã€‚PASSåˆ©ç”¨åœ¨ä»£ç†è¶…ç½‘ä¸Šçš„ä»»åŠ¡æ¡ä»¶åˆ†å¸ƒé€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼Œä¸ºäº‹åå®¡è®¡æä¾›æ¦‚ç‡æ ‡æ³¨è½¨è¿¹ï¼Œæé«˜åŒ»ç–—äººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼ŒPASSè¿˜èƒ½å‹ç¼©é‡è¦å‘ç°ï¼ŒåŠ¨æ€å†³å®šæ·±åŒ–æ¨ç†è·¯å¾„æˆ–æå‰é€€å‡ºä»¥æé«˜æ•ˆç‡ã€‚é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒç¨‹åºä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬çš„å¹³è¡¡ï¼ŒåŒ…æ‹¬ä¸“å®¶çŸ¥è¯†é¢„çƒ­ã€å¯¹æ¯”è·¯å¾„æ’åå’Œæˆæœ¬æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡ä¸¥æ ¼çš„CAB-EåŸºå‡†æµ‹è¯•ï¼ŒPASSåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œè¯æ˜äº†å…¶åœ¨æ¨åŠ¨è§£é‡Šæ€§ã€è‡ªé€‚åº”å’Œå¤šæ¨¡å¼åŒ»ç–—ä»£ç†ç³»ç»Ÿæ–¹é¢çš„æ–°èŒƒå¼è½¬å˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PASSæ¡†æ¶è§£å†³äº†ç°æœ‰å·¥å…·å¢å¼ºä»£ç†ç³»ç»Ÿçš„å¤šä¸ªé—®é¢˜ï¼ŒåŒ…æ‹¬ä¿¡ä»»ç¼ºå¤±ã€å¤šæ¨¡å¼æ•´åˆä¸è¶³ã€ç®¡é“åƒµåŒ–åŠè®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>PASSæ¡†æ¶é’ˆå¯¹èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰æ¨ç†ä¸Šä¸‹æ–‡é‡‡ç”¨æ¦‚ç‡ä»£ç†è¶…ç½‘é‡‡æ ·æŠ€æœ¯ï¼Œè‡ªé€‚åº”é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼Œæä¾›æ¦‚ç‡æ ‡æ³¨çš„å†³ç­–è·¯å¾„ã€‚</li>
<li>PASSæ¡†æ¶å¢å¼ºäº†åŒ»ç–—äººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§ï¼Œé€šè¿‡è§£é‡Šæ€§æ¦‚ç‡æ ‡æ³¨è½¨è¿¹ä¸ºäº‹åå®¡è®¡æä¾›ä¾æ®ã€‚</li>
<li>PASSèƒ½å¤Ÿå‹ç¼©é‡è¦å‘ç°å¹¶åŠ¨æ€è°ƒæ•´æ¨ç†è·¯å¾„å’Œæå‰é€€å‡ºä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒç¨‹åºå¹³è¡¡æ€§èƒ½ä¸æˆæœ¬ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ä¸“å®¶çŸ¥è¯†é¢„çƒ­ã€å¯¹æ¯”è·¯å¾„æ’åå’Œæˆæœ¬æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>PASSåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ç³»ç»Ÿï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€AUCç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-504b411e0cf56d27a27fdd5f59a111c6" align="middle">
<img src="https://picx.zhimg.com/v2-ff5ed0a68912f07116c8c3c53f22fbc9" align="middle">
<img src="https://picx.zhimg.com/v2-b76efbac6e04e7f7febc4b54844b2ef2" align="middle">
<img src="https://picx.zhimg.com/v2-227af624cda747bc6c73e2f411e55ab7" align="middle">
<img src="https://picx.zhimg.com/v2-0d6c600e0fc241bd3e42ba3714f76e85" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Aethorix-v1-0-An-Integrated-Scientific-AI-Agent-for-Scalable-Inorganic-Materials-Innovation-and-Industrial-Implementation"><a href="#Aethorix-v1-0-An-Integrated-Scientific-AI-Agent-for-Scalable-Inorganic-Materials-Innovation-and-Industrial-Implementation" class="headerlink" title="Aethorix v1.0: An Integrated Scientific AI Agent for Scalable Inorganic Materials Innovation and Industrial Implementation"></a>Aethorix v1.0: An Integrated Scientific AI Agent for Scalable Inorganic Materials Innovation and Industrial Implementation</h2><p><strong>Authors:Yingjie Shi, Yiru Gong, Yiqun Su, Suya Xiong, Jiale Han, Runtian Miao</strong></p>
<p>Artificial Intelligence (AI) is redefining the frontiers of scientific domains, ranging from drug discovery to meteorological modeling, yet its integration within industrial manufacturing remains nascent and fraught with operational challenges. To bridge this gap, we introduce Aethorix v1.0, an AI agent framework designed to overcome key industrial bottlenecks, demonstrating state-of-the-art performance in materials design innovation and process parameter optimization. Our tool is built upon three pillars: a scientific corpus reasoning engine that streamlines knowledge retrieval and validation, a diffusion-based generative model for zero-shot inverse design, and specialized interatomic potentials that enable faster screening with ab initio fidelity. We demonstrate Aethorixâ€™s utility through a real-world cement production case study, confirming its capacity for integration into industrial workflows and its role in revolutionizing the design-make-test-analyze loop while ensuring rigorous manufacturing standards are met.</p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£åœ¨é‡æ–°å®šä¹‰ç§‘å­¦é¢†åŸŸçš„è¾¹ç•Œï¼Œä»è¯ç‰©å‘ç°åˆ°æ°”è±¡å»ºæ¨¡ï¼Œç„¶è€Œå…¶åœ¨å·¥ä¸šåˆ¶é€ ä¸­çš„æ•´åˆä»å¤„äºåˆçº§é˜¶æ®µï¼Œå¹¶é¢ä¸´ç€æ“ä½œæŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Aethorix v1.0ï¼Œè¿™æ˜¯ä¸€æ¬¾äººå·¥æ™ºèƒ½ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœå·¥ä¸šç“¶é¢ˆï¼Œåœ¨ææ–™è®¾è®¡åˆ›æ–°å’Œå·¥è‰ºå‚æ•°ä¼˜åŒ–æ–¹é¢å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥å…·å»ºç«‹åœ¨ä¸‰å¤§æ”¯æŸ±ä¹‹ä¸Šï¼šä¸€ä¸ªç®€åŒ–çŸ¥è¯†æ£€ç´¢å’ŒéªŒè¯çš„ç§‘å­¦è¯­æ–™æ¨ç†å¼•æ“ï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºé›¶æ ·æœ¬é€†å‘è®¾è®¡ï¼Œä»¥åŠç‰¹æ®Šçš„åŸå­é—´åŠ¿èƒ½ï¼Œå¯å®ç°ä¸ä»å¤´ç®—ç²¾åº¦æ›´å¿«çš„ç­›é€‰ã€‚æˆ‘ä»¬é€šè¿‡ç°å®ä¸–ç•Œçš„æ°´æ³¥ç”Ÿäº§æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†Aethorixçš„å®ç”¨æ€§ï¼Œè¯å®äº†å…¶èå…¥å·¥ä¸šå·¥ä½œæµç¨‹çš„èƒ½åŠ›åŠå…¶åœ¨é©å‘½è®¾è®¡-åˆ¶é€ -æµ‹è¯•-åˆ†æå¾ªç¯ä¸­çš„ä½œç”¨ï¼ŒåŒæ—¶ç¡®ä¿æ»¡è¶³ä¸¥æ ¼çš„åˆ¶é€ æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16609v4">PDF</a> </p>
<p><strong>Summary</strong><br>    äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨å¤šä¸ªç§‘å­¦é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨å·¥ä¸šåˆ¶é€ ä¸šä¸­çš„åº”ç”¨ä»å¤„äºåˆçº§é˜¶æ®µï¼Œé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºAethorix v1.0ï¼Œä¸€æ¬¾AIä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœå·¥ä¸šåˆ¶é€ ä¸­çš„å…³é”®ç“¶é¢ˆï¼Œåœ¨ææ–™è®¾è®¡åˆ›æ–°å’Œå·¥è‰ºå‚æ•°ä¼˜åŒ–æ–¹é¢å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¯¥å·¥å…·åŸºäºä¸‰å¤§æ”¯æŸ±ï¼šç§‘å­¦è¯­æ–™æ¨ç†å¼•æ“ã€æ‰©æ•£ç”Ÿæˆæ¨¡å‹å’Œç‰¹æ®ŠåŒ–åŸå­é—´åŠ¿èƒ½å‡½æ•°ã€‚æˆ‘ä»¬ä»¥æ°´æ³¥ç”Ÿäº§å®é™…æ¡ˆä¾‹å±•ç¤ºäº†Aethorixçš„å®ç”¨æ€§ï¼Œè¯æ˜äº†å…¶èå…¥å·¥ä¸šæµç¨‹çš„èƒ½åŠ›åŠå…¶åœ¨ä¼˜åŒ–è®¾è®¡ä¸åˆ¶é€ æ ‡å‡†ä¸­çš„ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨å¤šä¸ªç§‘å­¦é¢†åŸŸå±•ç°æ½œåŠ›ï¼Œä½†åœ¨å·¥ä¸šåˆ¶é€ ä¸šä¸­çš„åº”ç”¨å°šå¤„äºåˆçº§é˜¶æ®µã€‚</li>
<li>Aethorix v1.0æ˜¯ä¸€æ¬¾æ—¨åœ¨å…‹æœå·¥ä¸šåˆ¶é€ ç“¶é¢ˆçš„AIä»£ç†æ¡†æ¶ã€‚</li>
<li>Aethorixåœ¨ææ–™è®¾è®¡åˆ›æ–°å’Œå·¥è‰ºå‚æ•°ä¼˜åŒ–æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>AethorixåŸºäºä¸‰å¤§æ”¯æŸ±æ„å»ºï¼šç§‘å­¦è¯­æ–™æ¨ç†å¼•æ“ã€æ‰©æ•£ç”Ÿæˆæ¨¡å‹å’Œç‰¹æ®ŠåŒ–åŸå­é—´åŠ¿èƒ½å‡½æ•°ã€‚</li>
<li>Aethorixå·¥å…·èƒ½èå…¥å·¥ä¸šæµç¨‹ï¼Œä¼˜åŒ–è®¾è®¡ä¸åˆ¶é€ æ ‡å‡†ã€‚</li>
<li>é€šè¿‡æ°´æ³¥ç”Ÿäº§å®é™…æ¡ˆä¾‹å±•ç¤ºäº†Aethorixçš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffdc5ade2fb8a9713d5d408b69ff3a90" align="middle">
<img src="https://picx.zhimg.com/v2-5f45d84b581f443ddcad1a1397c6b6d0" align="middle">
<img src="https://picx.zhimg.com/v2-f5ee1405ed49d4843d7f8dbd530799e8" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EcoAgent-An-Efficient-Device-Cloud-Collaborative-Multi-Agent-Framework-for-Mobile-Automation"><a href="#EcoAgent-An-Efficient-Device-Cloud-Collaborative-Multi-Agent-Framework-for-Mobile-Automation" class="headerlink" title="EcoAgent: An Efficient Device-Cloud Collaborative Multi-Agent Framework for Mobile Automation"></a>EcoAgent: An Efficient Device-Cloud Collaborative Multi-Agent Framework for Mobile Automation</h2><p><strong>Authors:Biao Yi, Xavier Hu, Yurun Chen, Shengyu Zhang, Hongxia Yang, Fan Wu</strong></p>
<p>To tackle increasingly complex tasks, recent research on mobile agents has shifted towards multi-agent collaboration. Current mobile multi-agent systems are primarily deployed in the cloud, leading to high latency and operational costs. A straightforward idea is to deploy a device-cloud collaborative multi-agent system, which is nontrivial, as directly extending existing systems introduces new challenges: (1) reliance on cloud-side verification requires uploading mobile screenshots, compromising user privacy; and (2) open-loop cooperation lacking device-to-cloud feedback, underutilizing device resources and increasing latency. To overcome these limitations, we propose EcoAgent, a closed-loop device-cloud collaborative multi-agent framework designed for privacy-aware, efficient, and responsive mobile automation. EcoAgent integrates a novel reasoning approach, Dual-ReACT, into the cloud-based Planning Agent, fully exploiting cloud reasoning to compensate for limited on-device capacity, thereby enabling device-side verification and lightweight feedback. Furthermore, the device-based Observation Agent leverages a Pre-understanding Module to summarize screen content into concise textual descriptions, significantly reducing token usage and device-cloud communication overhead while preserving privacy. Experiments on AndroidWorld demonstrate that EcoAgent matches the task success rates of fully cloud-based agents, while reducing resource consumption and response latency. Our project is available here: <a target="_blank" rel="noopener" href="https://github.com/Yi-Biao/EcoAgent">https://github.com/Yi-Biao/EcoAgent</a>.</p>
<blockquote>
<p>é’ˆå¯¹æ—¥ç›Šå¤æ‚çš„ä»»åŠ¡ï¼Œå…³äºç§»åŠ¨æ™ºèƒ½ä½“çš„æœ€æ–°ç ”ç©¶å·²è½¬å‘å¤šæ™ºèƒ½ä½“åä½œã€‚å½“å‰ç§»åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸»è¦éƒ¨ç½²åœ¨äº‘ç«¯ï¼Œå¯¼è‡´é«˜å»¶è¿Ÿå’Œè¿è¥æˆæœ¬ã€‚ä¸€ä¸ªç›´æ¥çš„æƒ³æ³•æ˜¯éƒ¨ç½²è®¾å¤‡-äº‘ååŒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä½†è¿™å¹¶ä¸ç®€å•ï¼Œå› ä¸ºç›´æ¥æ‰©å±•ç°æœ‰ç³»ç»Ÿä¼šå¸¦æ¥æ–°çš„æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¯¹äº‘ç«¯éªŒè¯çš„ä¾èµ–éœ€è¦ä¸Šä¼ æ‰‹æœºæˆªå›¾ï¼Œè¿™ä¾µçŠ¯äº†ç”¨æˆ·éšç§ï¼›ï¼ˆ2ï¼‰ç¼ºä¹è®¾å¤‡åˆ°äº‘ç«¯çš„åé¦ˆï¼Œå¯¼è‡´è®¾å¤‡èµ„æºåˆ©ç”¨ä¸è¶³å’Œå»¶è¿Ÿå¢åŠ ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EcoAgentï¼Œè¿™æ˜¯ä¸€ä¸ªé—­ç¯è®¾å¤‡-äº‘ååŒå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°éšç§æ„ŸçŸ¥ã€é«˜æ•ˆå’Œå“åº”è¿…é€Ÿçš„ç§»åŠ¨è‡ªåŠ¨åŒ–ã€‚EcoAgentå°†ä¸€ç§æ–°é¢–çš„é€»è¾‘æ–¹æ³•Dual-ReACTæ•´åˆåˆ°åŸºäºäº‘çš„è§„åˆ’æ™ºèƒ½ä½“ä¸­ï¼Œå……åˆ†åˆ©ç”¨äº‘é€»è¾‘æ¥å¼¥è¡¥è®¾å¤‡ç«¯çš„æœ‰é™å®¹é‡ï¼Œä»è€Œå®ç°è®¾å¤‡ç«¯éªŒè¯å’Œè½»é‡çº§åé¦ˆã€‚æ­¤å¤–ï¼ŒåŸºäºè®¾å¤‡çš„è§‚æµ‹æ™ºèƒ½ä½“åˆ©ç”¨é¢„ç†è§£æ¨¡å—æ¥å°†å±å¹•å†…å®¹æ€»ç»“ä¸ºç®€æ´çš„æ–‡æœ¬æè¿°ï¼Œæ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œä½¿ç”¨å’Œè®¾å¤‡ä¸äº‘ä¹‹é—´çš„é€šä¿¡å¼€é”€ï¼ŒåŒæ—¶ä¿æŠ¤äº†éšç§ã€‚åœ¨AndroidWorldä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEcoAgentçš„ä»»åŠ¡æˆåŠŸç‡ä¸å®Œå…¨åŸºäºäº‘çš„æ™ºèƒ½ä½“ç›¸åŒ¹é…ï¼ŒåŒæ—¶é™ä½äº†èµ„æºæ¶ˆè€—å’Œå“åº”å»¶è¿Ÿã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Yi-Biao/EcoAgent%E3%80%82">https://github.com/Yi-Biao/EcoAgentã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05440v3">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong>ï¼š<br>ç§»åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé¢ä¸´å¤æ‚ä»»åŠ¡æŒ‘æˆ˜ï¼Œå½“å‰ä¸»è¦éƒ¨ç½²åœ¨äº‘ç«¯ï¼Œå¯¼è‡´é«˜å»¶è¿Ÿå’Œè¿è¥æˆæœ¬ã€‚ä¸ºå…‹æœè¿™äº›é™åˆ¶ï¼Œæå‡ºEcoAgentæ¡†æ¶ï¼Œé‡‡ç”¨è®¾å¤‡äº‘ååŒåˆä½œæ–¹å¼ï¼Œé›†æˆäº‘è§„åˆ’æ™ºèƒ½ä½“å’Œè®¾å¤‡è§‚æµ‹æ™ºèƒ½ä½“ï¼Œå®ç°éšç§æ„ŸçŸ¥ã€é«˜æ•ˆå’Œå“åº”è¿…é€Ÿçš„ç§»åŠ¨è‡ªåŠ¨åŒ–ã€‚æ–°æå‡ºä¸€ç§æ¨ç†æ–¹æ³•â€”â€”Dual-ReACTï¼Œèƒ½å……åˆ†åˆ©ç”¨äº‘æ¨ç†æ¥å¼¥è¡¥è®¾å¤‡å®¹é‡çš„å±€é™ï¼Œå¹¶å®ç°è®¾å¤‡ç«¯éªŒè¯å’Œè½»é‡çº§åé¦ˆã€‚æ­¤å¤–ï¼Œé€šè¿‡é¢„ç†è§£æ¨¡å—å°†å±å¹•å†…å®¹ç®€åŒ–ä¸ºç®€æ´çš„æ–‡æœ¬æè¿°ï¼Œå‡å°‘é€šä¿¡å¼€é”€å¹¶ä¿æŠ¤éšç§ã€‚å®éªŒè¯æ˜EcoAgentåœ¨ä»»åŠ¡æˆåŠŸç‡ä¸ŠåŒ¹é…å…¨äº‘æ™ºèƒ½ä½“ï¼ŒåŒæ—¶é™ä½èµ„æºæ¶ˆè€—å’Œå“åº”å»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰ç§»åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé¢ä¸´éƒ¨ç½²åœ¨äº‘ç«¯çš„é—®é¢˜ï¼Œå¯¼è‡´é«˜å»¶è¿Ÿå’Œè¿è¥æˆæœ¬ã€‚</li>
<li>è®¾å¤‡äº‘ååŒåˆä½œæ˜¯è§£å†³æ­¤é—®é¢˜çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>EcoAgentæ¡†æ¶ç»“åˆäº†äº‘è§„åˆ’æ™ºèƒ½ä½“å’Œè®¾å¤‡è§‚æµ‹æ™ºèƒ½ä½“ä»¥å®ç°éšç§æ„ŸçŸ¥çš„ç§»åŠ¨è‡ªåŠ¨åŒ–ã€‚</li>
<li>Dual-ReACTæ¨ç†æ–¹æ³•åˆ©ç”¨äº‘æ¨ç†ä»¥å¼¥è¡¥è®¾å¤‡å®¹é‡é™åˆ¶ã€‚</li>
<li>é¢„ç†è§£æ¨¡å—èƒ½å°†å±å¹•å†…å®¹è½¬åŒ–ä¸ºç®€æ´æ–‡æœ¬æè¿°ï¼Œå‡å°‘é€šä¿¡å¼€é”€å¹¶ä¿æŠ¤éšç§ã€‚</li>
<li>EcoAgentæ¡†æ¶çš„å®éªŒç»“æœè¡¨æ˜å…¶åœ¨ä»»åŠ¡æˆåŠŸç‡ä¸ŠåŒ¹é…å…¨äº‘æ™ºèƒ½ä½“æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9eb9402b23bccbe21211066d2d90ca0" align="middle">
<img src="https://picx.zhimg.com/v2-ac511fd4f64c380859f1b62b34b8f39d" align="middle">
<img src="https://picx.zhimg.com/v2-83c046d1f38bafb12d6cf2eb9c758a79" align="middle">
<img src="https://picx.zhimg.com/v2-fe8fb4ac6d369ada4d41047eff01fc24" align="middle">
<img src="https://picx.zhimg.com/v2-dcce62a8c15fb43b88b939e068f45e3f" align="middle">
<img src="https://picx.zhimg.com/v2-9628ce40663bfbcf69581c3bb149c490" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LLM-Powered-GUI-Agents-in-Phone-Automation-Surveying-Progress-and-Prospects"><a href="#LLM-Powered-GUI-Agents-in-Phone-Automation-Surveying-Progress-and-Prospects" class="headerlink" title="LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects"></a>LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects</h2><p><strong>Authors:Guangyi Liu, Pengxiang Zhao, Yaozhen Liang, Liang Liu, Yaxuan Guo, Han Xiao, Weifeng Lin, Yuxiang Chai, Yue Han, Shuai Ren, Hao Wang, Xiaoyu Liang, WenHao Wang, Tianze Wu, Zhengxi Lu, Siheng Chen,  LiLinghao, Hao Wang, Guanjing Xiong, Yong Liu, Hongsheng Li</strong></p>
<p>With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents. The collection of papers reviewed in this survey will be hosted and regularly updated on the GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents">https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents</a></p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿…é€Ÿå´›èµ·ï¼Œç”µè¯è‡ªåŠ¨åŒ–å·²ç»ç»å†äº†å˜é©æ€§çš„æ”¹å˜ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°å›é¡¾äº†LLMé©±åŠ¨çš„ç”µè¯GUIä»£ç†ï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬ä»åŸºäºè„šæœ¬çš„è‡ªåŠ¨åŒ–å‘æ™ºèƒ½ã€è‡ªé€‚åº”ç³»ç»Ÿçš„æ¼”å˜ã€‚æˆ‘ä»¬é¦–å…ˆåˆ†æäº†å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰é€šç”¨æ€§æœ‰é™ã€ï¼ˆiiï¼‰ç»´æŠ¤æˆæœ¬é«˜å’Œï¼ˆiiiï¼‰æ„å›¾ç†è§£å¼±ï¼Œå¹¶å±•ç¤ºäº†LLMå¦‚ä½•é€šè¿‡é«˜çº§è¯­è¨€ç†è§£ã€å¤šæ¨¡å¼æ„ŸçŸ¥å’Œç¨³å¥çš„å†³ç­–æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ç±»ï¼Œæ¶µç›–äº†åŸºæœ¬çš„ä»£ç†æ¡†æ¶ï¼ˆå•ä»£ç†ã€å¤šä»£ç†ã€è®¡åˆ’åè¡ŒåŠ¨ï¼‰ã€å»ºæ¨¡æ–¹æ³•ï¼ˆæç¤ºå·¥ç¨‹ã€åŸºäºè®­ç»ƒï¼‰ã€åŸºæœ¬çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†ä»»åŠ¡ç‰¹å®šçš„æ¶æ„ã€ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥èƒ½å¤Ÿæ¡¥æ¥ç”¨æˆ·æ„å›¾å’ŒGUIæ“ä½œã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å¼€æ”¾æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®é›†å¤šæ ·æ€§ã€è®¾å¤‡éƒ¨ç½²æ•ˆç‡ã€ç”¨æˆ·ä¸­å¿ƒé€‚åº”æ€§å’Œå®‰å…¨é—®é¢˜ï¼Œä¸ºè¿™ä¸€å¿«é€Ÿæ¼”å˜çš„é¢†åŸŸæä¾›äº†å‰ç»æ€§è§è§£ã€‚æœ¬æ–‡é€šè¿‡æä¾›ç»“æ„åŒ–æ¦‚è¿°å¹¶ç¡®å®šç´§è¿«çš„ç ”ç©¶ç©ºç™½ï¼Œæˆä¸ºç ”ç©¶è€…å’Œä»ä¸šè€…åœ¨è®¾è®¡å¯æ‰©å±•ã€ç”¨æˆ·å‹å¥½çš„ç”µè¯GUIä»£ç†æ—¶åˆ©ç”¨LLMçš„æƒå¨å‚è€ƒã€‚æœ¬æ–‡æ‰€å›é¡¾çš„è®ºæ–‡é›†åˆå°†æ‰˜ç®¡åœ¨GitHubä»“åº“ä¸­å¹¶å®šæœŸæ›´æ–°ï¼š<a target="_blank" rel="noopener" href="https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents">https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19838v3">PDF</a> Paper accepted to TMLR 2025, Project Homepage: <a target="_blank" rel="noopener" href="https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents">https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents</a></p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç”µè¯è‡ªåŠ¨åŒ–ç»å†äº†æ·±åˆ»å˜é©ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†LLMé©±åŠ¨çš„ç”µè¯GUIä»£ç†ï¼Œçªå‡ºå…¶ä»åŸºäºè„šæœ¬çš„è‡ªåŠ¨åŒ–å‘æ™ºèƒ½ã€è‡ªé€‚åº”ç³»ç»Ÿçš„æ¼”å˜ã€‚æ–‡ç« é¦–å…ˆä»‹ç»äº†å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é€šç”¨æ€§æœ‰é™ã€ç»´æŠ¤æˆæœ¬é«˜ä»¥åŠæ„å›¾ç†è§£è–„å¼±ï¼Œå¹¶å±•ç¤ºäº†LLMå¦‚ä½•é€šè¿‡é«˜çº§è¯­è¨€ç†è§£ã€å¤šæ¨¡å¼æ„ŸçŸ¥å’Œç¨³å¥å†³ç­–æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚æ–‡ç« è¿˜æå‡ºäº†æ¶µç›–ä»£ç†æ¡†æ¶ã€å»ºæ¨¡æ–¹æ³•ã€åŸºæœ¬æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•çš„ç¨åŠ¡åˆ†ç±»ï¼Œå¹¶è¯¦ç»†æè¿°äº†ä»»åŠ¡ç‰¹å®šæ¶æ„ã€ç›‘ç£å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥èƒ½å¤Ÿæ¶èµ·ç”¨æˆ·æ„å›¾å’ŒGUIæ“ä½œä¹‹é—´çš„æ¡¥æ¢ã€‚æœ€åï¼Œæ–‡ç« è®¨è®ºäº†æ•°æ®é›†å¤šæ ·æ€§ã€è®¾å¤‡éƒ¨ç½²æ•ˆç‡ã€ç”¨æˆ·ä¸­å¿ƒé€‚åº”æ€§å’Œå®‰å…¨é¡¾è™‘ç­‰å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºè¿™ä¸€å¿«é€Ÿæ¼”å˜é¢†åŸŸæä¾›äº†å‰ç»æ€§çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„å‘å±•æ¨åŠ¨äº†ç”µè¯è‡ªåŠ¨åŒ–çš„å˜é©ã€‚</li>
<li>LLMè§£å†³äº†ç”µè¯è‡ªåŠ¨åŒ–ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå¦‚é€šç”¨æ€§ã€ç»´æŠ¤æˆæœ¬å’Œæ„å›¾ç†è§£ã€‚</li>
<li>LLMé€šè¿‡é«˜çº§è¯­è¨€ç†è§£ã€å¤šæ¨¡å¼æ„ŸçŸ¥å’Œç¨³å¥å†³ç­–å®ç°äº†ç”µè¯è‡ªåŠ¨åŒ–çš„æ™ºèƒ½åŒ–å’Œè‡ªé€‚åº”åŒ–ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ä¸ªåŒ…å«ä»£ç†æ¡†æ¶ã€å»ºæ¨¡æ–¹æ³•ã€æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•çš„ç¨åŠ¡åˆ†ç±»ã€‚</li>
<li>ç”¨æˆ·æ„å›¾å’ŒGUIæ“ä½œä¹‹é—´çš„æ¡¥æ¢å¯ä»¥é€šè¿‡ä»»åŠ¡ç‰¹å®šæ¶æ„ã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¥å»ºç«‹ã€‚</li>
<li>æ•°æ®é›†å¤šæ ·æ€§ã€è®¾å¤‡éƒ¨ç½²æ•ˆç‡ã€ç”¨æˆ·ä¸­å¿ƒé€‚åº”æ€§å’Œå®‰å…¨é¡¾è™‘ä»æ˜¯å½“å‰å¼€æ”¾æŒ‘æˆ˜ã€‚</li>
<li>è¯¥è®ºæ–‡ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†ä¸€ä¸ªå…³äºå¦‚ä½•åœ¨è®¾è®¡ä¸­æœ‰æ•ˆåˆ©ç”¨LLMçš„å®è´µå‚è€ƒï¼Œä»¥åˆ›å»ºå¯æ‰©å±•å’Œç”¨æˆ·å‹å¥½çš„ç”µè¯GUIä»£ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be80efd0fc2d2e1dd357ffce38b2f9ac" align="middle">
<img src="https://picx.zhimg.com/v2-5d8f4909a85bec258a288098f385a534" align="middle">
<img src="https://picx.zhimg.com/v2-e2d02f0ffb8e9750db96a096e2be38f2" align="middle">
<img src="https://picx.zhimg.com/v2-211ea74b87976f95bb91e5f0382b269d" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Competence-Aware-AI-Agents-with-Metacognition-for-Unknown-Situations-and-Environments-MUSE"><a href="#Competence-Aware-AI-Agents-with-Metacognition-for-Unknown-Situations-and-Environments-MUSE" class="headerlink" title="Competence-Aware AI Agents with Metacognition for Unknown Situations and Environments (MUSE)"></a>Competence-Aware AI Agents with Metacognition for Unknown Situations and Environments (MUSE)</h2><p><strong>Authors:Rodolfo Valiente, Praveen K. Pilly</strong></p>
<p>Metacognition, defined as the awareness and regulation of oneâ€™s cognitive processes, is central to human adaptability in unknown situations. In contrast, current autonomous agents often struggle in novel environments due to their limited capacity for adaptation. We hypothesize that metacognition is a critical missing ingredient in autonomous agents for the cognitive flexibility needed to tackle unfamiliar challenges. Given the broad scope of metacognitive abilities, we focus on competence awareness and strategy selection. To this end, we propose the Metacognition for Unknown Situations and Environments (MUSE) framework to integrate metacognitive processes of self-assessment and self-regulation into autonomous agents. We present two implementations of MUSE: one based on world modeling and another leveraging large language models (LLMs). Our system continually learns to assess its competence on a given task and uses this self-assessment to guide iterative cycles of strategy selection. MUSE agents demonstrate high competence awareness and significant improvements in self-regulation for solving novel, out-of-distribution tasks more effectively compared to model-based reinforcement learning and purely prompt-based LLM agent approaches. This work highlights the promise of approaches inspired by cognitive and neural systems in enabling autonomous agents to adapt to new environments while mitigating the heavy reliance on extensive training data and large models for the current models.</p>
<blockquote>
<p>å…ƒè®¤çŸ¥è¢«å®šä¹‰ä¸ºå¯¹è®¤çŸ¥è¿‡ç¨‹çš„æ„è¯†å’Œè°ƒèŠ‚ï¼Œæ˜¯äººç±»åœ¨æœªçŸ¥æƒ…å¢ƒä¸­å…·æœ‰é€‚åº”æ€§çš„æ ¸å¿ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå½“å‰çš„è‡ªä¸»ä»£ç†äººåœ¨æ–°ç¯å¢ƒä¸­å¸¸å¸¸é¢ä¸´å›°å¢ƒï¼Œå› ä¸ºä»–ä»¬é€‚åº”çš„èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬å‡è®¾å…ƒè®¤çŸ¥æ˜¯è‡ªä¸»ä»£ç†äººç¼ºä¹åº”å¯¹æœªçŸ¥æŒ‘æˆ˜æ‰€éœ€çš„è®¤çŸ¥çµæ´»æ€§çš„å…³é”®è¦ç´ ã€‚é‰´äºå…ƒè®¤çŸ¥èƒ½åŠ›çš„å¹¿æ³›èŒƒç•´ï¼Œæˆ‘ä»¬ä¸“æ³¨äºèƒ½åŠ›æ„è¯†å’Œç­–ç•¥é€‰æ‹©ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å…ƒè®¤çŸ¥æœªçŸ¥æƒ…å¢ƒå’Œç¯å¢ƒï¼ˆMUSEï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å°†è‡ªæˆ‘è¯„ä¼°å’Œè‡ªæˆ‘è°ƒèŠ‚çš„å…ƒè®¤çŸ¥è¿‡ç¨‹æ•´åˆåˆ°è‡ªä¸»ä»£ç†äººä¸­ã€‚æˆ‘ä»¬å±•ç¤ºäº†MUSEçš„ä¸¤ä¸ªå®ç°ï¼šä¸€ä¸ªåŸºäºä¸–ç•Œå»ºæ¨¡ï¼Œå¦ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿä¸æ–­å­¦ä¼šè¯„ä¼°åœ¨ç»™å®šä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨è¿™ç§è‡ªæˆ‘è¯„ä¼°æ¥æŒ‡å¯¼ç­–ç•¥é€‰æ‹©çš„è¿­ä»£å‘¨æœŸã€‚MUSEä»£ç†äººè¡¨ç°å‡ºå¼ºçƒˆçš„èƒ½åŠ›æ„è¯†å’Œè‡ªæˆ‘è°ƒèŠ‚èƒ½åŠ›çš„æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨è§£å†³æ–°å‹ã€è¶…å‡ºåˆ†å¸ƒçš„ä»»åŠ¡æ—¶æ›´åŠ æœ‰æ•ˆï¼Œä¸åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ å’Œçº¯ç²¹çš„åŸºäºæç¤ºçš„LLMä»£ç†äººæ–¹æ³•ç›¸æ¯”ï¼Œæœ‰æ˜¾è‘—æ”¹å–„ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†å—ç¥ç»å’Œè®¤çŸ¥ç³»ç»Ÿå¯å‘çš„æ–¹æ³•åœ¨ä½¿è‡ªä¸»ä»£ç†äººé€‚åº”æ–°ç¯å¢ƒæ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶å‡è½»äº†å½“å‰æ¨¡å‹å¯¹å¤§é‡è®­ç»ƒæ•°æ®å’Œå¤§å‹æ¨¡å‹çš„ä¸¥é‡ä¾èµ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13537v2">PDF</a> Replaced all references to â€œself-awarenessâ€ with the more accurate term â€œself-assessmentâ€; Updated Figure 2; Added recent pertinent work from the cognitive computational neuroscience literature; Removed the non-apples-to-apples comparison with Dreamer-v3 for self-assessment; Added additional experiments to validate the role of accurate self-assessment in effective self-regulation</p>
<p><strong>Summary</strong>ï¼š<br>è®¤çŸ¥è¿‡ç¨‹ä¸­çš„è‡ªæˆ‘æ„è¯†å’Œè°ƒèŠ‚â€”â€”å³å…ƒè®¤çŸ¥ï¼Œå¯¹äººç±»é€‚åº”æœªçŸ¥æƒ…å¢ƒè‡³å…³é‡è¦ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå½“å‰çš„è‡ªä¸»ä»£ç†äººåœ¨é¢å¯¹æ–°ç¯å¢ƒæ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹é€‚åº”èƒ½åŠ›ã€‚æˆ‘ä»¬å‡è®¾è‡ªä¸»ä»£ç†äººç¼ºä¹åº”å¯¹æœªçŸ¥æŒ‘æˆ˜æ‰€éœ€çš„è®¤çŸ¥çµæ´»æ€§ï¼Œè€Œå…ƒè®¤çŸ¥æ˜¯ä¸€ä¸ªå…³é”®è¦ç´ ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘æœªçŸ¥æƒ…å¢ƒå’Œç¯å¢ƒï¼ˆMUSEï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†è‡ªä¸»ä»£ç†äººçš„è‡ªæˆ‘è¯„ä¼°å’Œè‡ªæˆ‘è°ƒèŠ‚ç­‰å…ƒè®¤çŸ¥è¿‡ç¨‹æ•´åˆèµ·æ¥ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸¤ä¸ªMUSEå®ç°æ–¹æ¡ˆï¼šä¸€ä¸ªåŸºäºä¸–ç•Œå»ºæ¨¡ï¼Œå¦ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿä¸æ–­å­¦ä¹ è¯„ä¼°å…¶åœ¨ç»™å®šä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œå¹¶ä½¿ç”¨è¿™ç§è‡ªæˆ‘è¯„ä¼°æ¥æŒ‡å¯¼ç­–ç•¥é€‰æ‹©çš„è¿­ä»£å‘¨æœŸã€‚ä¸åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ å’Œçº¯æç¤ºå‹LLMä»£ç†æ–¹æ³•ç›¸æ¯”ï¼ŒMUSEä»£ç†åœ¨è§£å†³æ–°é¢–çš„éåˆ†å¸ƒä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›æ„è¯†å’Œè‡ªæˆ‘è°ƒæ§èƒ½åŠ›æ”¹è¿›ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†å—è®¤çŸ¥å’Œç¥ç»ç³»ç»Ÿå¯å‘çš„æ–¹æ³•åœ¨ä½¿è‡ªä¸»ä»£ç†äººé€‚åº”æ–°ç¯å¢ƒæ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶å‡è½»äº†å½“å‰æ¨¡å‹å¯¹å¤§é‡è®­ç»ƒæ•°æ®å’Œå¤§å‹æ¨¡å‹çš„ä¸¥é‡ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å…ƒè®¤çŸ¥åœ¨äººç±»é€‚åº”æœªçŸ¥æƒ…å¢ƒæ–¹é¢æ‰®æ¼”æ ¸å¿ƒè§’è‰²ï¼Œè‡ªä¸»ä»£ç†äººåœ¨é¢å¯¹æ–°ç¯å¢ƒæ—¶ç¼ºä¹é€‚åº”èƒ½åŠ›çš„é—®é¢˜çªå‡ºã€‚</li>
<li>æå‡ºé¢å‘æœªçŸ¥æƒ…å¢ƒå’Œç¯å¢ƒï¼ˆMUSEï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè‡ªä¸»ä»£ç†äººçš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼ŒåŒ…æ‹¬è‡ªæˆ‘è¯„ä¼°å’Œç­–ç•¥é€‰æ‹©ç­‰ã€‚</li>
<li>MUSEæ¡†æ¶æœ‰ä¸¤ç§å®ç°æ–¹å¼ï¼šåŸºäºä¸–ç•Œå»ºæ¨¡å’Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>ç³»ç»Ÿèƒ½æŒç»­è¯„ä¼°è‡ªèº«åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œå¹¶ç”¨è¿™ç§è‡ªæˆ‘è¯„ä¼°æ¥æŒ‡å¯¼ç­–ç•¥é€‰æ‹©çš„è¿­ä»£ã€‚</li>
<li>MUSEä»£ç†ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„èƒ½åŠ›æ„è¯†å’Œè‡ªæˆ‘è°ƒæ§èƒ½åŠ›æ”¹è¿›ã€‚</li>
<li>è¯¥ç ”ç©¶çªæ˜¾äº†ç»“åˆè®¤çŸ¥å’Œç¥ç»ç³»ç»Ÿå¯å‘çš„æ–¹æ³•åœ¨è‡ªä¸»ä»£ç†äººé€‚åº”æ–°ç¯å¢ƒæ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74a750082ebefbefcddd4f2e0331916a" align="middle">
<img src="https://picx.zhimg.com/v2-438330770ed554107dd82ee91380cfd6" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="An-LLM-based-Simulation-Framework-for-Embodied-Conversational-Agents-in-Psychological-Counseling"><a href="#An-LLM-based-Simulation-Framework-for-Embodied-Conversational-Agents-in-Psychological-Counseling" class="headerlink" title="An LLM-based Simulation Framework for Embodied Conversational Agents in Psychological Counseling"></a>An LLM-based Simulation Framework for Embodied Conversational Agents in Psychological Counseling</h2><p><strong>Authors:Lixiu Wu, Yuanrong Tang, Qisen Pan, Xianyang Zhan, Yucheng Han, Lanxi Xiao, Tianhong Wang, Chen Zhong, Jiangtao Gong</strong></p>
<p>Due to privacy concerns, open dialogue datasets for mental health are primarily generated through human or AI synthesis methods. However, the inherent implicit nature of psychological processes, particularly those of clients, poses challenges to the authenticity and diversity of synthetic data. In this paper, we propose ECAs (short for Embodied Conversational Agents), a framework for embodied agent simulation based on Large Language Models (LLMs) that incorporates multiple psychological theoretical principles.Using simulation, we expand real counseling case data into a nuanced embodied cognitive memory space and generate dialogue data based on high-frequency counseling questions.We validated our framework using the D4 dataset. First, we created a public ECAs dataset through batch simulations based on D4. Licensed counselors evaluated our method, demonstrating that it significantly outperforms baselines in simulation authenticity and necessity. Additionally, two LLM-based automated evaluation methods were employed to confirm the higher quality of the generated dialogues compared to the baselines. The source code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/AIR-DISCOVER/ECAs-Dataset">https://github.com/AIR-DISCOVER/ECAs-Dataset</a>.</p>
<blockquote>
<p>é‰´äºéšç§é—®é¢˜ï¼Œå¿ƒç†å¥åº·é¢†åŸŸçš„å¼€æ”¾å¯¹è¯æ•°æ®é›†ä¸»è¦é€šè¿‡äººå·¥æˆ–AIåˆæˆæ–¹æ³•ç”Ÿæˆã€‚ç„¶è€Œï¼Œå¿ƒç†è¿‡ç¨‹ï¼ˆå°¤å…¶æ˜¯å®¢æˆ·çš„å¿ƒç†è¿‡ç¨‹ï¼‰çš„å›ºæœ‰éšå«æ€§è´¨ç»™åˆæˆæ•°æ®çš„çœŸå®æ€§å’Œå¤šæ ·æ€§å¸¦æ¥äº†æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºECAsï¼ˆåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®ä½“å¯¹è¯ä»£ç†ï¼ŒEmbodied Conversational Agentsï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå®ä½“ä»£ç†æ¨¡æ‹Ÿçš„æ¡†æ¶ï¼Œèå…¥äº†å¤šä¸ªå¿ƒç†å­¦ç†è®ºåŸåˆ™ã€‚é€šè¿‡æ¨¡æ‹Ÿï¼Œæˆ‘ä»¬å°†çœŸå®çš„å¿ƒç†å’¨è¯¢æ¡ˆä¾‹æ•°æ®æ‰©å±•åˆ°ä¸€ä¸ªå¾®å¦™çš„å®ä½“è®¤çŸ¥è®°å¿†ç©ºé—´ï¼Œå¹¶æ ¹æ®é«˜é¢‘å’¨è¯¢é—®é¢˜ç”Ÿæˆå¯¹è¯æ•°æ®ã€‚æˆ‘ä»¬ä½¿ç”¨D4æ•°æ®é›†éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åŸºäºD4é€šè¿‡æ‰¹é‡æ¨¡æ‹Ÿåˆ›å»ºäº†ä¸€ä¸ªå…¬å…±ECAæ•°æ®é›†ã€‚è®¸å¯çš„å¿ƒç†å’¨è¯¢å¸ˆå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨¡æ‹ŸçœŸå®æ€§å’Œå¿…è¦æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†çº¿ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†ä¸¤ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ–¹æ³•ï¼Œä»¥ç¡®è®¤ç”Ÿæˆçš„å¯¹è¯è´¨é‡é«˜äºåŸºå‡†çº¿ã€‚æºä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIR-DISCOVER/ECAs-Dataset%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/AIR-DISCOVER/ECAs-Datasetè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22041v3">PDF</a> Accepted to AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>åŸºäºéšç§è€ƒé‡ï¼Œå¿ƒç†å¥åº·é¢†åŸŸçš„å¼€æ”¾å¯¹è¯æ•°æ®é›†ä¸»è¦é€šè¿‡äººå·¥æˆ–AIåˆæˆæ–¹æ³•ç”Ÿæˆã€‚ç„¶è€Œï¼Œç”±äºå¿ƒç†è¿‡ç¨‹çš„éšæ€§ç‰¹è´¨ï¼Œå°¤å…¶æ˜¯å®¢æˆ·çš„å¿ƒç†è¿‡ç¨‹ï¼Œä¸ºåˆæˆæ•°æ®çš„çœŸå®æ€§å’Œå¤šæ ·æ€§å¸¦æ¥æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºECAsï¼ˆEmbodied Conversational Agentsï¼‰æ¡†æ¶ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…·èº«ä»£ç†æ¨¡æ‹Ÿï¼Œå¹¶èå…¥å¤šä¸ªå¿ƒç†å­¦ç†è®ºåŸåˆ™ã€‚é€šè¿‡æ¨¡æ‹Ÿï¼Œæˆ‘ä»¬æ‰©å±•çœŸå®å’¨è¯¢æ¡ˆä¾‹æ•°æ®ï¼Œå»ºç«‹ä¸€ä¸ªå¾®å¦™çš„å…·èº«è®¤çŸ¥è®°å¿†ç©ºé—´ï¼Œå¹¶æ ¹æ®é«˜é¢‘ç‡å’¨è¯¢é—®é¢˜ç”Ÿæˆå¯¹è¯æ•°æ®ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ¨¡æ‹ŸçœŸå®æ€§å’Œå¿…è¦æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾å¯¹è¯æ•°æ®é›†åœ¨å¿ƒç†å¥åº·é¢†åŸŸä¸»è¦ä¾èµ–äººå·¥æˆ–AIåˆæˆæ–¹æ³•ç”Ÿæˆï¼Œå› å¿ƒç†è¿‡ç¨‹çš„éšæ€§ç‰¹è´¨ï¼Œæ•°æ®çœŸå®æ€§å’Œå¤šæ ·æ€§é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ECAsæ¡†æ¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…·èº«ä»£ç†æ¨¡æ‹Ÿï¼Œèå…¥å¿ƒç†å­¦ç†è®ºåŸåˆ™ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿæ‰©å±•çœŸå®å’¨è¯¢æ¡ˆä¾‹æ•°æ®ï¼Œå»ºç«‹å…·èº«è®¤çŸ¥è®°å¿†ç©ºé—´ã€‚</li>
<li>ECAsæ¡†æ¶æ ¹æ®é«˜é¢‘ç‡å’¨è¯¢é—®é¢˜ç”Ÿæˆå¯¹è¯æ•°æ®ã€‚</li>
<li>å…¬å¼€ECAsæ•°æ®é›†é€šè¿‡æ‰¹é‡æ¨¡æ‹Ÿåˆ›å»ºï¼Œç»è®¸å¯å’¨è¯¢å¸ˆè¯„ä¼°ï¼Œåœ¨æ¨¡æ‹ŸçœŸå®æ€§å’Œå¿…è¦æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨ä¸¤ç§LLMè‡ªåŠ¨åŒ–è¯„ä¼°æ–¹æ³•ç¡®è®¤ç”Ÿæˆå¯¹è¯çš„é«˜è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22041">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f522698a982459d7e73e8984c564dd08" align="middle">
<img src="https://picx.zhimg.com/v2-ad3173ff0f855e564a9bc65fae605978" align="middle">
<img src="https://picx.zhimg.com/v2-972938a2c139ba4990eea4e5e785972d" align="middle">
<img src="https://picx.zhimg.com/v2-418ce4bd7d94c30eb6bc28625e3e32d9" align="middle">
<img src="https://picx.zhimg.com/v2-8cc2332173e20b62357934a6aa2fb2be" align="middle">
<img src="https://picx.zhimg.com/v2-85cc7e1835e6674f6e7a213043a48c75" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bfec9309a05dcb700ac78f47a5bf0590" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  VIR-Bench Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bf0625dfb62a4e243a388d7e148e3691" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Crossing Borders A Multimodal Challenge for Indian Poetry Translation and Image Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
