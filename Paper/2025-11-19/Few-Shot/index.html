<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Part-X-MLLM Part-aware 3D Multimodal Large Language Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1aced29c745c9aac9f1b328c35ca5b23')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model"><a href="#Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model" class="headerlink" title="Part-X-MLLM: Part-aware 3D Multimodal Large Language Model"></a>Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</h2><p><strong>Authors:Chunshi Wang, Junliang Ye, Yunhan Yang, Yang Li, Zizhuo Lin, Jun Zhu, Zhuo Chen, Yawei Luo, Chunchao Guo</strong></p>
<p>We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q&amp;A, compositional generation, and localized editing through one unified interface. Project page: <a target="_blank" rel="noopener" href="https://chunshi.wang/Part-X-MLLM/">https://chunshi.wang/Part-X-MLLM/</a></p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Part-X-MLLMï¼Œè¿™æ˜¯ä¸€ç§åŸç”Ÿ3Då¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡åˆ¶å®šç»“æ„åŒ–ã€å¯æ‰§è¡Œçš„è¯­æ³•å°†å„ç§3Dä»»åŠ¡ç»Ÿä¸€èµ·æ¥ã€‚ç»™å®šRGBç‚¹äº‘å’Œè‡ªç„¶è¯­è¨€æç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥è‡ªå›å½’åœ°ç”Ÿæˆä¸€ä¸ªå•ä¸€ã€è¿è´¯çš„ä»¤ç‰Œåºåˆ—ï¼Œè¯¥åºåˆ—ç¼–ç éƒ¨åˆ†çº§è¾¹ç•Œæ¡†ã€è¯­ä¹‰æè¿°å’Œç¼–è¾‘å‘½ä»¤ã€‚è¿™ç§ç»“æ„åŒ–è¾“å‡ºä½œä¸ºé€šç”¨æ¥å£ï¼Œä¸ºåŸºäºé›¶ä»¶ç”Ÿæˆå’Œç¼–è¾‘çš„ä¸‹æ¸¸å‡ ä½•æ„ŸçŸ¥æ¨¡å—æä¾›åŠ¨åŠ›ã€‚é€šè¿‡å°†ç¬¦å·è§„åˆ’ä¸å‡ ä½•ç»¼åˆç›¸åˆ†ç¦»ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸ä»»ä½•å…¼å®¹çš„å‡ ä½•å¼•æ“é€šè¿‡å•ä¸€ã€æœ¬åœ°åŒ–çš„å‰ç«¯è¿›è¡Œæ§åˆ¶ã€‚æˆ‘ä»¬é¢„è®­ç»ƒäº†ä¸€ä¸ªåŒç¼–ç å™¨æ¶æ„æ¥åˆ†ç¦»ç»“æ„ä¸è¯­ä¹‰ï¼Œå¹¶åœ¨å¤§è§„æ¨¡ã€ä»¥éƒ¨åˆ†ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ä¸Šè°ƒæ•´æ¨¡å‹æŒ‡ä»¤ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡ã€ç»“æ„åŒ–è®¡åˆ’æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé€šè¿‡ç»Ÿä¸€æ¥å£å®ç°äº†é¢†å…ˆçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åŸºäºåœ°é¢çš„é—®ç­”ã€ç»„åˆç”Ÿæˆå’Œå±€éƒ¨ç¼–è¾‘ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://chunshi.wang/Part-X-MLLM/%E3%80%82">https://chunshi.wang/Part-X-MLLM/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13647v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Part-X-MLLMæ˜¯ä¸€æ¬¾ç»“åˆ3Då¤šä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯å°†å„ç§3Dä»»åŠ¡è½¬åŒ–ä¸ºç»“æ„åŒ–ã€å¯æ‰§è¡Œçš„è¯­æ³•ç¨‹åºã€‚è¯¥æ¨¡å‹æ ¹æ®RGBç‚¹äº‘å’Œè‡ªç„¶è¯­è¨€æç¤ºï¼Œè‡ªåŠ¨ç”Ÿæˆç¼–ç éƒ¨åˆ†çº§åˆ«è¾¹ç•Œæ¡†ã€è¯­ä¹‰æè¿°å’Œç¼–è¾‘å‘½ä»¤çš„å•ä¸€ã€è¿è´¯çš„ä»¤ç‰Œåºåˆ—ã€‚è¿™ä¸€ç»“æ„åŒ–çš„è¾“å‡ºä¸ºåŸºäºé›¶ä»¶çš„ç”Ÿæˆå’Œç¼–è¾‘æä¾›äº†ä¸‹æ¸¸å‡ ä½•æ„ŸçŸ¥æ¨¡å—çš„é€šç”¨æ¥å£ã€‚é€šè¿‡ç¬¦å·è§„åˆ’ä¸å‡ ä½•ç»¼åˆçš„è§£è€¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸ä»»ä½•å…¼å®¹çš„å‡ ä½•å¼•æ“é€šè¿‡å•ä¸€ã€è¯­è¨€åŸç”Ÿå‰ç«¯è¿›è¡Œæ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Part-X-MLLMæ˜¯ä¸€ä¸ª3Då¤šä»»åŠ¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½ç»Ÿä¸€å„ç§3Dä»»åŠ¡ã€‚</li>
<li>è¯¥æ¨¡å‹å°†3Dä»»åŠ¡è½¬åŒ–ä¸ºç»“æ„åŒ–ã€å¯æ‰§è¡Œçš„è¯­æ³•ç¨‹åºã€‚</li>
<li>æ¨¡å‹æ ¹æ®RGBç‚¹äº‘å’Œè‡ªç„¶è¯­è¨€æç¤ºè‡ªåŠ¨ç”Ÿæˆç¼–ç éƒ¨åˆ†çº§åˆ«è¾¹ç•Œæ¡†ã€è¯­ä¹‰æè¿°å’Œç¼–è¾‘å‘½ä»¤çš„è¿è´¯åºåˆ—ã€‚</li>
<li>ç»“æ„åŒ–çš„è¾“å‡ºä¸ºåŸºäºé›¶ä»¶çš„ç”Ÿæˆå’Œç¼–è¾‘æä¾›äº†é€šç”¨æ¥å£ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç¬¦å·è§„åˆ’ä¸å‡ ä½•ç»¼åˆçš„è§£è€¦ï¼Œå…è®¸ä½¿ç”¨ä»»ä½•å…¼å®¹çš„å‡ ä½•å¼•æ“ã€‚</li>
<li>æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒçš„åŒç¼–ç å™¨æ¶æ„æ¥åˆ†ç¦»ç»“æ„å’Œè¯­ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb988bde1804e3431a26c8c5c922103b" align="middle">
<img src="https://picx.zhimg.com/v2-05c59a5780c191632b454266a7a51e8a" align="middle">
<img src="https://picx.zhimg.com/v2-f89e8d26c6a3a3a20e2eba074f3edb85" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CreBench-Human-Aligned-Creativity-Evaluation-from-Idea-to-Process-to-Product"><a href="#CreBench-Human-Aligned-Creativity-Evaluation-from-Idea-to-Process-to-Product" class="headerlink" title="CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product"></a>CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product</h2><p><strong>Authors:Kaiwen Xue, Chenglong Li, Zhonghong Ou, Guoxin Zhang, Kaoyan Lu, Shuai Lyu, Yifan Zhu, Ping Zong Junpeng Ding, Xinyu Liu, Qunlin Chen, Weiwei Qin, Yiran Shen, Jiayi Cen</strong></p>
<p>Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.</p>
<blockquote>
<p>äººç±»å®šä¹‰çš„åˆ›é€ åŠ›å…·æœ‰é«˜åº¦æŠ½è±¡æ€§ï¼Œå¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ï¼Œç†è§£å’Œè¯„ä¼°ä¸äººç±»åˆ¤æ–­ç›¸ç¬¦çš„åˆ›é€ åŠ›æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•çš„ç¼ºå¤±è¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸€å›°å¢ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CreBenchï¼Œå®ƒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š1ï¼‰ä¸€ä¸ªè¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–ä»åˆ›æ„æƒ³æ³•åˆ°è¿‡ç¨‹å†åˆ°äº§å“çš„å¤šä¸ªç»´åº¦ï¼›2ï¼‰CreMITï¼ˆåˆ›é€ åŠ›å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åˆ›é€ åŠ›è¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…å«2.2Kæ¥è‡ªä¸åŒæ¥æºçš„å¤šæ¨¡æ€æ•°æ®ã€79.2Kçš„äººç±»åé¦ˆå’Œ470ä¸‡æ¡å¤šç±»å‹æŒ‡ä»¤ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†ç¡®ä¿MLLMsèƒ½å¤Ÿå¤„ç†å„ç§ä¸åˆ›é€ åŠ›ç›¸å…³çš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬æç¤ºGPTå¯¹è¿™äº›äººç±»åé¦ˆè¿›è¡Œæ”¹è¿›ï¼Œä»¥æ¿€æ´»æ›´å¼ºçš„åˆ›é€ åŠ›è¯„ä¼°èƒ½åŠ›ã€‚CreBenchä½œä¸ºæ„å»ºç†è§£äººç±»åˆ›é€ åŠ›MLLMsçš„åŸºç¡€ã€‚åŸºäºCreBenchï¼Œæˆ‘ä»¬å¯¹å¼€æºçš„é€šç”¨MLLMsè¿›è¡Œäº†å¾®è°ƒï¼Œä»è€Œå¾—åˆ°äº†CreExpertï¼Œä¸€ä¸ªå¤šæ¨¡æ€åˆ›é€ åŠ›è¯„ä¼°ä¸“å®¶æ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°çš„MLLMsç›¸æ¯”ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„GPT-4Vå’ŒGemini-Pro-Visionï¼Œæ‰€æå‡ºçš„CreExpertæ¨¡å‹åœ¨ä¸äººç±»åˆ›é€ åŠ›è¯„ä»·çš„å¥‘åˆåº¦ä¸Šå–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13626v1">PDF</a> 13 pages, 3 figures,The 40th Annual AAAI Conference on Artificial Intelligence(AAAI 2026),Paper has been accepted for a poster presentation</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹äººç±»å®šä¹‰ä¸‹çš„åˆ›é€ åŠ›çš„è¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCreBenchçš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬è¯„ä»·åˆ›æ„æƒ³æ³•ã€è¿‡ç¨‹å’Œäº§å“çš„å¤šä¸ªç»´åº¦ã€‚åŒæ—¶ï¼Œä¸ºäº†è®­ç»ƒèƒ½å¤Ÿç†è§£äººç±»åˆ›é€ åŠ›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œæå‡ºäº†CreMITæ•°æ®é›†ã€‚é€šè¿‡å¯¹GPTè¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿äº†MLLMsèƒ½å¤Ÿå¤„ç†å„ç§åˆ›é€ åŠ›ç›¸å…³çš„æŸ¥è¯¢ï¼Œå¹¶å¼€å‘å‡ºCreExpertæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¸äººç±»åˆ›é€ åŠ›è¯„ä»·å¯¹é½æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»å®šä¹‰çš„åˆ›é€ åŠ›å…·æœ‰é«˜åº¦æŠ½è±¡æ€§ï¼Œå¯¹äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç†è§£å’Œè¯„ä¼°åˆ›é€ åŠ›å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹ç°æœ‰çš„åŸºå‡†æµ‹è¯•åŠ å‰§äº†è¿™ä¸€éš¾é¢˜ã€‚</li>
<li>æå‡ºCreBenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä»åˆ›æ„æƒ³æ³•ã€è¿‡ç¨‹åˆ°äº§å“çš„å¤šä¸ªç»´åº¦è¯„ä»·ã€‚</li>
<li>å¼•å…¥CreMITæ•°æ®é›†ï¼Œç”¨äºå¤šæ¨¡æ€åˆ›é€ åŠ›è¯„ä»·ï¼ŒåŒ…å«2.2Kå¤šæºå¤šæ¨¡æ€æ•°æ®ã€79.2Käººç±»åé¦ˆå’Œ470ä¸‡æ¡å¤šç±»å‹æŒ‡ä»¤ã€‚</li>
<li>é€šè¿‡GPTå¾®è°ƒï¼Œç¡®ä¿MLLMsèƒ½å¤„ç†å„ç§åˆ›é€ åŠ›ç›¸å…³æŸ¥è¯¢ã€‚</li>
<li>å¼€å‘å‡ºåŸºäºCreBenchçš„CreExpertæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¸äººç±»åˆ›é€ åŠ›è¯„ä»·å¯¹é½æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fee230f633780b3cf7f93a55a6ac6f4" align="middle">
<img src="https://picx.zhimg.com/v2-d6054adeedf4205340c741b831275759" align="middle">
<img src="https://picx.zhimg.com/v2-4df601a459c96ba4d298a09c9725dcef" align="middle">
<img src="https://picx.zhimg.com/v2-c4a828821545e1501a0b947d224e9edc" align="middle">
<img src="https://picx.zhimg.com/v2-746bb424eea7a7062764299d388395ad" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ForgeDAN-An-Evolutionary-Framework-for-Jailbreaking-Aligned-Large-Language-Models"><a href="#ForgeDAN-An-Evolutionary-Framework-for-Jailbreaking-Aligned-Large-Language-Models" class="headerlink" title="ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models"></a>ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models</h2><p><strong>Authors:Siyang Cheng, Gaotian Liu, Rui Mei, Yilin Wang, Kejia Zhang, Kaishuo Wei, Yuqi Yu, Weiping Wen, Xiaojie Wu, Junhua Liu</strong></p>
<p>The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿé‡‡çº³å¸¦æ¥äº†å˜é©æ€§çš„åº”ç”¨å’Œæ–°å®‰å…¨é£é™©ï¼ŒåŒ…æ‹¬ç»•è¿‡å¯¹é½ä¿éšœæ¥è§¦å‘æœ‰å®³è¾“å‡ºçš„è¶Šç‹±æ”»å‡»ã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–è¶Šç‹±ç”Ÿæˆæ–¹æ³•ï¼ˆä¾‹å¦‚AutoDANï¼‰å­˜åœ¨å˜å¼‚å¤šæ ·æ€§æœ‰é™ã€é€‚åº”åº¦è¯„ä¼°è‚¤æµ…ä»¥åŠåŸºäºå…³é”®è¯çš„æ£€æµ‹è„†å¼±ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ForgeDANï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¯¹é½LLMç”Ÿæˆè¯­ä¹‰è¿è´¯ä¸”é«˜åº¦æœ‰æ•ˆçš„å¯¹æŠ—æ€§æç¤ºçš„æ–°å‹è¿›åŒ–æ¡†æ¶ã€‚é¦–å…ˆï¼ŒForgeDANå¼•å…¥äº†è·¨å­—ç¬¦ã€å•è¯å’Œå¥å­çº§åˆ«çš„å¤šç­–ç•¥æ–‡æœ¬æ‰°åŠ¨æ“ä½œï¼Œä»¥å¢å¼ºæ”»å‡»å¤šæ ·æ€§ï¼›ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§æ¨¡å‹çš„å¯è§£é‡Šè¯­ä¹‰é€‚åº”åº¦è¯„ä¼°æ¥æŒ‡å¯¼è¿›åŒ–è¿‡ç¨‹ï¼Œä»¥äº§ç”Ÿè¯­ä¹‰ç›¸å…³ä¸”æœ‰å®³çš„è¾“å‡ºï¼›æœ€åï¼ŒForgeDANæ•´åˆäº†åŒé‡ç»´åº¦çš„è¶Šç‹±åˆ¤æ–­ï¼Œåˆ©ç”¨åŸºäºLLMçš„åˆ†ç±»å™¨è”åˆè¯„ä¼°æ¨¡å‹åˆè§„æ€§å’Œè¾“å‡ºå±å®³æ€§ï¼Œä»è€Œé™ä½è¯¯æŠ¥å¹¶æé«˜æ£€æµ‹æ•ˆç‡ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒForgeDANåœ¨ä¿æŒè‡ªç„¶æ€§å’Œéšè”½æ€§çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¶Šç‹±æˆåŠŸç‡ï¼Œä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13548v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿…é€Ÿé‡‡ç”¨å¸¦æ¥çš„å˜é©æ€§åº”ç”¨å’Œæ–°å®‰å…¨é£é™©çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç»•è¿‡å¯¹é½ä¿éšœä»¥äº§ç”Ÿæœ‰å®³è¾“å‡ºçš„è¶Šç‹±æ”»å‡»ã€‚é’ˆå¯¹ç°æœ‰è‡ªåŠ¨åŒ–è¶Šç‹±ç”Ÿæˆæ–¹æ³•ï¼ˆå¦‚AutoDANï¼‰åœ¨çªå˜å¤šæ ·æ€§ã€é€‚åº”åº¦è¯„ä¼°å’Œå…³é”®è¯æ£€æµ‹æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ForgeDANè¿™ä¸€æ–°å‹è¿›åŒ–æ¡†æ¶ã€‚ForgeDANå¼•å…¥è·¨å­—ç¬¦ã€å•è¯å’Œå¥å­çº§åˆ«çš„å¤šç­–ç•¥æ–‡æœ¬æ‰°åŠ¨ï¼Œä»¥å¢å¼ºæ”»å‡»å¤šæ ·æ€§ï¼›åŒæ—¶é‡‡ç”¨åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§æ¨¡å‹çš„è§£é‡Šæ€§è¯­ä¹‰é€‚åº”åº¦è¯„ä¼°æ¥å¼•å¯¼è¿›åŒ–è¿‡ç¨‹ç”Ÿæˆè¯­ä¹‰ç›¸å…³ä¸”æœ‰å®³çš„è¾“å‡ºï¼›æœ€åï¼Œå®ƒé›†æˆäº†åŸºäºLLMçš„åˆ†ç±»å™¨æ¥è¿›è¡ŒåŒé‡ç»´åº¦è¶Šç‹±åˆ¤æ–­ï¼Œè”åˆè¯„ä¼°æ¨¡å‹åˆè§„æ€§å’Œè¾“å‡ºå±å®³æ€§ï¼Œä»¥é™ä½è¯¯æŠ¥å¹¶å¢å¼ºæ£€æµ‹æ•ˆæœã€‚è¯„ä¼°è¡¨æ˜ï¼ŒForgeDANåœ¨é«˜è¶Šç‹±æˆåŠŸç‡çš„åŒæ—¶ä¿æŒäº†è‡ªç„¶æ€§å’Œéšè”½æ€§ï¼Œä¼˜äºç°æœ‰æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ™®åŠå¸¦æ¥äº†å®‰å…¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯è¶Šç‹±æ”»å‡»é—®é¢˜ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆå¦‚AutoDANå­˜åœ¨çªå˜å¤šæ ·æ€§ä¸è¶³ã€é€‚åº”åº¦è¯„ä¼°ä¸å…¨é¢å’ŒåŸºäºå…³é”®è¯çš„æ£€æµ‹æ–¹å¼è„†å¼±ç­‰é—®é¢˜ã€‚</li>
<li>ForgeDANé€šè¿‡å¼•å…¥å¤šç­–ç•¥æ–‡æœ¬æ‰°åŠ¨å¢å¼ºæ”»å‡»å¤šæ ·æ€§ã€‚</li>
<li>ForgeDANé‡‡ç”¨åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§æ¨¡å‹çš„è§£é‡Šæ€§è¯­ä¹‰é€‚åº”åº¦è¯„ä¼°ï¼Œä½¿è¿›åŒ–è¿‡ç¨‹èƒ½ç”Ÿæˆè¯­ä¹‰ç›¸å…³ä¸”æœ‰å®³çš„è¾“å‡ºã€‚</li>
<li>ForgeDANé›†æˆäº†åŸºäºLLMçš„åˆ†ç±»å™¨è¿›è¡ŒåŒé‡ç»´åº¦è¶Šç‹±åˆ¤æ–­ï¼Œæé«˜æ£€æµ‹æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ForgeDANåœ¨ä¿æŒè‡ªç„¶æ€§å’Œéšè”½æ€§çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¶Šç‹±æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1392b86c3f1cd91ee42c6519b2c67ae4" align="middle">
<img src="https://picx.zhimg.com/v2-fd47d14a83094b9c8dcface9316459a0" align="middle">
<img src="https://picx.zhimg.com/v2-bf0625dfb62a4e243a388d7e148e3691" align="middle">
<img src="https://picx.zhimg.com/v2-9c553a521a0ac08f31cd04b990f8b945" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Multimodal-Large-Language-Model-Framework-for-Automated-Interpretation-of-Fuel-Efficiency-Analytics-in-Public-Transportation"><a href="#Multi-Agent-Multimodal-Large-Language-Model-Framework-for-Automated-Interpretation-of-Fuel-Efficiency-Analytics-in-Public-Transportation" class="headerlink" title="Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation"></a>Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation</h2><p><strong>Authors:Zhipeng Ma, Ali Rida Bahja, Andreas Burgdorf, AndrÃ© Pomp, Tobias Meisen, Bo NÃ¸rregaard JÃ¸rgensen, Zheng Grace Ma</strong></p>
<p>Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.</p>
<blockquote>
<p>æé«˜å…¬å…±äº¤é€šçš„ç‡ƒæ²¹æ•ˆç‡éœ€è¦æ•´åˆå¤æ‚çš„å¤šæ¨¡å¼æ•°æ®ï¼Œä»¥å½¢æˆå¯è§£é‡Šçš„ã€ä¸å†³ç­–ç›¸å…³çš„è§è§£ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åˆ†ææ–¹æ³•å’Œå¯è§†åŒ–æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿç¢ç‰‡åŒ–çš„è¾“å‡ºï¼Œéœ€è¦å¤§é‡çš„äººå·¥è§£è¯»ï¼Œä»è€Œé™åˆ¶äº†å¯æ‰©å±•æ€§å’Œä¸€è‡´æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è¿›è¡Œæ•°æ®å™äº‹å’Œèƒ½æºæ´å¯Ÿç”Ÿæˆã€‚è¯¥æ¡†æ¶åè°ƒäº†ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬æ•°æ®å™äº‹æ™ºèƒ½ä½“ã€LLMåˆ¤æ–­æ™ºèƒ½ä½“å’Œå¯é€‰çš„äººæœºå¾ªç¯è¯„ä¼°æ™ºèƒ½ä½“ï¼Œä»¥å°†åˆ†æäº§ç‰©è¿­ä»£åœ°è½¬åŒ–ä¸ºè¿è´¯çš„ã€ä»¥åˆ©ç›Šç›¸å…³è€…ä¸ºå¯¼å‘çš„æŠ¥å‘Šã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸¹éº¦åŒ—éƒ¨å°¤ç‰¹å…°åœ°åŒºå…¬å…±å·´å£«è¿è¾“çš„å®é™…æƒ…å†µç ”ç©¶å¾—åˆ°äº†éªŒè¯ï¼Œå…¶ä¸­4006æ¬¡å‡ºè¡Œçš„ç‡ƒæ²¹æ•ˆç‡æ•°æ®ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹èšç±»è¿›è¡Œåˆ†æã€‚è·¨è¶Šäº”ç§æœ€æ–°LLMå’Œä¸‰ç§æç¤ºèŒƒå¼çš„æ¯”è¾ƒå®éªŒç¡®å®šäº†GPT-4.1 minié…åˆChain-of-Thoughtæç¤ºä¸ºæœ€ä½³é…ç½®ï¼Œå®ç°äº†97.3%çš„å™è¿°å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¹³è¡¡äº†å¯è§£é‡Šæ€§å’Œè®¡ç®—æˆæœ¬ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“ååŒæ˜¾è‘—æé«˜äº†åŸºäºLLMçš„æŠ¥å‘Šçš„äº‹å®ç²¾ç¡®æ€§ã€è¿è´¯æ€§å’Œå¯æ‰©å±•æ€§ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ºèƒ½æºä¿¡æ¯å­¦ä¸­çš„AIé©±åŠ¨å™äº‹ç”Ÿæˆå’Œå†³ç­–æ”¯æŒå»ºç«‹äº†å¯å¤åˆ¶å’Œé€‚åº”é¢†åŸŸçš„çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13476v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–è¿›è¡Œæ•°æ®çš„å™äº‹å’Œèƒ½æºæ´å¯Ÿçš„ç”Ÿæˆã€‚æ­¤æ¡†æ¶åŒ…æ‹¬æ•°æ®å™äº‹æ™ºèƒ½ä½“ã€å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜çš„æ™ºèƒ½ä½“ä»¥åŠå¯é€‰çš„äººæœºäº¤äº’è¯„ä»·è€…ï¼Œå°†åˆ†æäº§ç‰©è½¬åŒ–ä¸ºè¿è´¯çš„ã€é¢å‘åˆ©ç›Šç›¸å…³è€…çš„æŠ¥å‘Šã€‚åœ¨ä¸¹éº¦åŒ—éƒ¨å…¬äº¤è¿è¾“çš„å®é™…æ¡ˆä¾‹ç ”ç©¶ä¸­éªŒè¯äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡é«˜æ–¯æ··åˆæ¨¡å‹èšç±»åˆ†ææ¥è‡ª4006æ¬¡çš„è¡Œç¨‹çš„ç‡ƒæ–™æ•ˆç‡æ•°æ®ã€‚å¯¹æ¯”å®éªŒè¡¨æ˜GPT-4.1 minié…åˆChain-of-Thoughtæç¤ºæ³•æ˜¯æœ€ä¼˜é…ç½®ï¼Œèƒ½åœ¨ä¿æŒè§£é‡Šæ€§å’Œè®¡ç®—æˆæœ¬å¹³è¡¡çš„åŒæ—¶ï¼Œè¾¾åˆ°97.3%çš„å™äº‹å‡†ç¡®æ€§ã€‚æ­¤ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„å™äº‹ç”Ÿæˆå’Œèƒ½æºä¿¡æ¯å­¦å†³ç­–æ”¯æŒæä¾›äº†å¯å¤åˆ¶å’Œé€‚åº”é¢†åŸŸçš„æ–¹æ³•è®ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŒ…å«æ•°æ®å™äº‹æ™ºèƒ½ä½“ã€å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜çš„æ™ºèƒ½ä½“ä»¥åŠäººæœºäº’åŠ¨è¯„ä»·è€…çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–å¤„ç†å¤šæ¨¡æ€æ•°æ®å¹¶ç”Ÿæˆèƒ½æºæ´å¯Ÿã€‚</li>
<li>é€šè¿‡å®é™…æ¡ˆä¾‹ç ”ç©¶éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å…¬äº¤è¿è¾“é¢†åŸŸåˆ†æç‡ƒæ–™æ•ˆç‡æ•°æ®ã€‚</li>
<li>å¯¹æ¯”å®éªŒè¡¨æ˜GPT-4.1 minié…åˆChain-of-Thoughtæç¤ºæ³•æ˜¯æœ€ä¼˜é…ç½®ï¼Œå…·æœ‰è¾ƒé«˜çš„å™äº‹å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶æé«˜äº†äº‹å®å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå¯æ‰©å±•æ€§ï¼Œåœ¨èƒ½æºä¿¡æ¯å­¦é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºAIé©±åŠ¨çš„å™äº‹ç”Ÿæˆå’Œå†³ç­–æ”¯æŒæä¾›äº†å¯å¤åˆ¶çš„æ–¹æ³•è®ºï¼Œé€‚ç”¨äºä¸åŒé¢†åŸŸçš„ä¿¡æ¯å¤„ç†å’Œåˆ†æã€‚</li>
<li>è¯¥æ¡†æ¶å¼ºè°ƒäº†åœ¨å¤æ‚æ•°æ®å¤„ç†å’Œåˆ†æè¿‡ç¨‹ä¸­äººæœºåä½œçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25747f40ab66994b1251671576656ab2" align="middle">
<img src="https://picx.zhimg.com/v2-a2a673e5b9cff4daf61bf82ba8e02983" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Referring-Camouflaged-Object-Detection-With-Multi-Context-Overlapped-Windows-Cross-Attention"><a href="#Referring-Camouflaged-Object-Detection-With-Multi-Context-Overlapped-Windows-Cross-Attention" class="headerlink" title="Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention"></a>Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention</h2><p><strong>Authors:Yu Wen, Shuyong Gao, Shuping Zhang, Miao Huang, Lili Tao, Han Yang, Haozhe Xing, Lihe Zhang, Boxue Hou</strong></p>
<p>Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.</p>
<blockquote>
<p>æŒ‡ä»£ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆRef-CODï¼‰æ—¨åœ¨é€šè¿‡èå…¥å›¾åƒå’Œæ–‡æœ¬æè¿°ç­‰å‚è€ƒä¿¡æ¯æ¥è¯†åˆ«éšè—çš„ç›®æ ‡ã€‚ä»¥å¾€çš„ç ”ç©¶å·²å°†å…·æœ‰æ˜¾è‘—ç›®æ ‡çš„å‚è€ƒå›¾åƒè½¬æ¢ä¸ºä¸€ç»´æç¤ºï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚æˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡èåˆä¸°å¯Œæ˜¾è‘—çš„å›¾åƒç‰¹å¾å’Œä¼ªè£…ç›®æ ‡ç‰¹å¾çš„å¤šä¸Šä¸‹æ–‡æ¥å¢å¼ºæ€§èƒ½çš„æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RFMNetï¼Œå®ƒåˆ©ç”¨å‚è€ƒæ˜¾è‘—å›¾åƒå¤šä¸ªç¼–ç é˜¶æ®µçš„ç‰¹å¾ï¼Œå¹¶åœ¨ç›¸åº”çš„ç¼–ç é˜¶æ®µä¸ä¼ªè£…ç‰¹å¾è¿›è¡Œäº¤äº’èåˆã€‚é‰´äºæ˜¾è‘—ç›®æ ‡å›¾åƒä¸­çš„ç‰¹å¾åŒ…å«å¤§é‡çš„ç›®æ ‡ç›¸å…³è¯¦ç»†ä¿¡æ¯ï¼Œåœ¨å±€éƒ¨åŒºåŸŸè¿›è¡Œç‰¹å¾èåˆæ›´æœ‰åˆ©äºæ£€æµ‹ä¼ªè£…ç›®æ ‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡å çª—å£äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºå‚è€ƒç‰¹å¾æ›´åŠ å…³æ³¨å±€éƒ¨ä¿¡æ¯åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æŒ‡ä»£ç‰¹å¾èšåˆï¼ˆRFAï¼‰æ¨¡å—ï¼Œä»¥é€æ­¥è§£ç å’Œåˆ†å‰²ä¼ªè£…ç›®æ ‡ã€‚åœ¨Ref-CODåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13249v1">PDF</a> 12 pages, 7figures, This work is supported by National Nature Science Foundation of China (Grant No. 62203291)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•ç»“åˆå‚è€ƒä¿¡æ¯ï¼ˆå¦‚å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼‰æ¥è¯†åˆ«éšè—ç‰©ä½“ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”RFMNetã€‚è¯¥æ–¹æ³•åˆ©ç”¨å‚è€ƒæ˜¾è‘—å›¾åƒçš„å¤šé˜¶æ®µç¼–ç ç‰¹å¾ï¼Œä¸éšè”½ç‰©ä½“ç‰¹å¾è¿›è¡Œäº¤äº’å¼èåˆã€‚é€šè¿‡å±€éƒ¨åŒºåŸŸç‰¹å¾èåˆå’Œé‡å çª—å£äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜éšè”½ç‰©ä½“æ£€æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœåœ¨Ref-CODåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RFMNetåˆ©ç”¨å‚è€ƒæ˜¾è‘—å›¾åƒçš„å¤šé˜¶æ®µç¼–ç ç‰¹å¾ï¼Œä¸éšè”½ç‰©ä½“ç‰¹å¾è¿›è¡Œäº¤äº’å¼èåˆï¼Œä»¥æé«˜è¯†åˆ«éšè—ç‰©ä½“çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å±€éƒ¨åŒºåŸŸç‰¹å¾èåˆï¼Œå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨æ˜¾è‘—å›¾åƒä¸­çš„ä¸°å¯Œå¯¹è±¡ç›¸å…³è¯¦ç»†ä¿¡æ¯ï¼Œæœ‰åŠ©äºæé«˜éšè”½ç‰©ä½“æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é‡å çª—å£äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºå‚è€ƒç‰¹å¾æ›´åŠ å…³æ³¨å±€éƒ¨ä¿¡æ¯åŒ¹é…ã€‚</li>
<li>RFMNetä¸­çš„å‚ç…§ç‰¹å¾èšåˆï¼ˆRFAï¼‰æ¨¡å—å¯ä»¥é€æ­¥è§£ç å’Œåˆ†å‰²éšè”½ç‰©ä½“ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRFMNetåœ¨Ref-CODåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>RFMNeté€šè¿‡èå…¥å¤šä¸Šä¸‹æ–‡ä¿¡æ¯æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¤æ‚çš„çœŸå®åœºæ™¯ä¸­å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8839d67a8693c8f4b7ff511d03c6e3e5" align="middle">
<img src="https://picx.zhimg.com/v2-60e77b55de521e8a6dc150a7937abc22" align="middle">
<img src="https://picx.zhimg.com/v2-36e8afa508b37d5ae96dc1d0a25e45b0" align="middle">
<img src="https://picx.zhimg.com/v2-5b934e377c5c82e9c589437a32bfcbf1" align="middle">
<img src="https://picx.zhimg.com/v2-ddd7ae75350976f98f3c7d6fac3e9e45" align="middle">
<img src="https://picx.zhimg.com/v2-48b70d4f2b24a7d928285174ff5ab48f" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Video-Spatial-Reasoning-with-Object-Centric-3D-Rollout"><a href="#Video-Spatial-Reasoning-with-Object-Centric-3D-Rollout" class="headerlink" title="Video Spatial Reasoning with Object-Centric 3D Rollout"></a>Video Spatial Reasoning with Object-Centric 3D Rollout</h2><p><strong>Authors:Haoran Tang, Meng Cao, Ruyang Liu, Xiaoxi Liang, Linglong Li, Ge Li, Xiaodan Liang</strong></p>
<p>Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCRâ€™s superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).</p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å±•ç¤ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ç°ç¨³å¥çš„è§†é¢‘ç©ºé—´æ¨ç†â€”â€”åœ¨åŠ¨æ€çš„3Dåœºæ™¯ä¸­ç†è§£å¯¹è±¡çš„ä½ç½®ã€æ–¹å‘å’Œå¯¹è±¡é—´å…³ç³»çš„èƒ½åŠ›ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå°šæœªè§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç©ºé—´æ¥åœ°ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ï¼Œä½†æˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹å¾€å¾€è¡¨ç°å‡ºæŸ¥è¯¢é”å®šæ¨ç†ï¼Œåªå…³æ³¨æç¤ºä¸­æ˜ç¡®æåˆ°çš„å¯¹è±¡ï¼Œè€Œå¿½è§†å…³é”®çš„ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Object-Centric 3D Rolloutï¼ˆOCRï¼‰è¿™ä¸€æ–°ç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é€‰å®šå¯¹è±¡çš„3Då‡ ä½•ç»“æ„è¿›è¡Œç»“æ„åŒ–æ‰°åŠ¨ã€‚é€šè¿‡é™ä½ç‰¹å®šå¯¹è±¡çš„è§†è§‰çº¿ç´¢å¹¶å°†æ”¹å˜çš„å‡ ä½•ç»“æ„æŠ•å½±åˆ°äºŒç»´ç©ºé—´ï¼ŒOCRè¿«ä½¿æ¨¡å‹å¯¹æ•´ä¸ªåœºæ™¯è¿›è¡Œæ•´ä½“æ¨ç†ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§åŸºäºrolloutçš„è®­ç»ƒç®¡é“ï¼Œè”åˆä½¿ç”¨æ™®é€šå’ŒåŒºåŸŸå™ªå£°è§†é¢‘æ¥ä¼˜åŒ–ç©ºé—´æ¨ç†è½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½å¤„äºé¢†å…ˆæ°´å¹³ï¼šæˆ‘ä»¬çš„è§„æ¨¡ä¸º3Bå‚æ•°çš„æ¨¡å‹åœ¨VSI-Benchä¸Šè¾¾åˆ°äº†47.5%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºå¤šä¸ªè§„æ¨¡ä¸º7Bçš„åŸºçº¿æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¯å®äº†OCRç›¸è¾ƒäºå…ˆå‰çš„rolloutç­–ç•¥ï¼ˆå¦‚T-GRPOã€NoisyRolloutï¼‰çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13190v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œå¹¶æŒ‡å‡ºè§†é¢‘ç©ºé—´æ¨ç†æ˜¯ä¸€é¡¹å°šæœªè§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç©ºé—´å®šä½çš„ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ï¼Œä½†å­˜åœ¨æŸ¥è¯¢é”å®šæ¨ç†çš„é—®é¢˜ï¼Œå¿½ç•¥å…³é”®ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥â€”â€”å¯¹è±¡ä¸­å¿ƒä¸‰ç»´æ»šåŠ¨ï¼ˆOCRï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘é€‰å®šå¯¹è±¡çš„3Då‡ ä½•ç»“æ„å¼•å…¥ç»“æ„åŒ–æ‰°åŠ¨ã€‚é€šè¿‡é™ä½å¯¹è±¡ç‰¹å®šçš„è§†è§‰çº¿ç´¢å¹¶å°†æ›´æ”¹åçš„å‡ ä½•ç»“æ„æŠ•å½±åˆ°äºŒç»´ç©ºé—´ï¼ŒOCRè¿«ä½¿æ¨¡å‹åœ¨æ•´ä¸ªåœºæ™¯ä¸­è¿›è¡Œå…¨é¢æ¨ç†ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºæ»šåŠ¨æ¡çš„åŸ¹è®­ç®¡é“ï¼Œè¯¥ç®¡é“ç»“åˆäº†æ™®é€šå’ŒåŒºåŸŸå™ªå£°è§†é¢‘ï¼Œä»¥ä¼˜åŒ–ç©ºé—´æ¨ç†è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨VSI-Benchä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°47.5%ï¼Œä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è§†é¢‘ç©ºé—´æ¨ç†ä»æ˜¯æœªè§£å†³çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ è¿›è¡Œç©ºé—´å®šä½ï¼Œä½†å­˜åœ¨æŸ¥è¯¢é”å®šæ¨ç†é—®é¢˜ã€‚</li>
<li>æå‡ºäº†å¯¹è±¡ä¸­å¿ƒä¸‰ç»´æ»šåŠ¨ï¼ˆOCRï¼‰ç­–ç•¥ï¼Œé€šè¿‡ç»“æ„åŒ–æ‰°åŠ¨è®­ç»ƒæ—¶å¯¹è±¡çš„3Då‡ ä½•ç»“æ„æ¥æ”¹è¿›æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>OCRç­–ç•¥è¿«ä½¿æ¨¡å‹åœ¨æ•´ä¸ªåœºæ™¯ä¸­å…¨é¢æ¨ç†ï¼Œé€šè¿‡é™ä½å¯¹è±¡ç‰¹å®šçš„è§†è§‰çº¿ç´¢å¹¶å°†æ›´æ”¹çš„å‡ ä½•ç»“æ„æŠ•å½±åˆ°äºŒç»´ç©ºé—´ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºæ»šåŠ¨æ¡çš„åŸ¹è®­ç®¡é“ï¼Œç»“åˆäº†æ™®é€šå’ŒåŒºåŸŸå™ªå£°è§†é¢‘ï¼Œä¼˜åŒ–ç©ºé—´æ¨ç†è½¨è¿¹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨VSI-Benchæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°47.5%ï¼Œä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-002fa669033bdcdee240e5db83c21f39" align="middle">
<img src="https://picx.zhimg.com/v2-62d53ce08ab8a6852c2e9d0cdba59901" align="middle">
<img src="https://picx.zhimg.com/v2-021e5e51271c7193e6a34f2b3946b77f" align="middle">
<img src="https://picx.zhimg.com/v2-62b8c2b68b297c9a6acff38a32fa6f08" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Distinguishing-Repetition-Disfluency-from-Morphological-Reduplication-in-Bangla-ASR-Transcripts-A-Novel-Corpus-and-Benchmarking-Analysis"><a href="#Distinguishing-Repetition-Disfluency-from-Morphological-Reduplication-in-Bangla-ASR-Transcripts-A-Novel-Corpus-and-Benchmarking-Analysis" class="headerlink" title="Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis"></a>Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis</h2><p><strong>Authors:Zaara Zabeen Arpa, Sadnam Sakib Apurbo, Nazia Karim Khan Oishee, Ajwad Abrar</strong></p>
<p>Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error&#x2F;hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.</p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ï¼Œç‰¹åˆ«æ˜¯åœ¨å­ŸåŠ æ‹‰è¯­ç­‰ä½èµ„æºè¯­è¨€ä¸­ï¼Œå­˜åœ¨å…³é”®çš„ä¸ç¡®å®šæ€§ï¼šå•è¯é‡å¤å¯èƒ½æ˜¯é‡å¤å¤±è¯¯ï¼ˆæ— æ„çš„ASRé”™è¯¯&#x2F;çŠ¹è±«ï¼‰æˆ–å½¢æ€å¤ç°ï¼ˆæ•…æ„çš„è¯­æ³•ç»“æ„ï¼‰ã€‚æ ‡å‡†çš„å¤±æ€ä¿®æ­£ä¼šé”™è¯¯åœ°åˆ é™¤æœ‰æ•ˆçš„è¯­è¨€ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„ã€æ‰‹åŠ¨æ³¨é‡Šçš„å­ŸåŠ æ‹‰è¯­è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“åŒ…å«2ä¸‡è¡Œæ•°æ®ï¼Œæ—¨åœ¨æ˜ç¡®åŒºåˆ†è¿™ä¸¤ç§ç°è±¡åœ¨å˜ˆæ‚çš„ASRè½¬å½•ä¸­çš„åŒºåˆ«ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ç§èŒƒå¼æ¥è¯„ä¼°è¿™ä¸€æ–°èµ„æºï¼šæœ€å…ˆè¿›çš„å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç¼–ç å™¨æ¨¡å‹å¾®è°ƒã€‚LLMé€šè¿‡å°‘é‡æç¤ºå®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ˆæœ€é«˜è¾¾82.68%çš„å‡†ç¡®ç‡ï¼‰ã€‚ç„¶è€Œï¼Œå¾®è°ƒè¯æ˜æ›´ä¸ºä¼˜è¶Šï¼Œç‰¹å®šçš„å­ŸåŠ æ‹‰è¯­BERTæ¨¡å‹è¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡84.78%ï¼ŒF1åˆ†æ•°ä¸º0.677ã€‚è¿™ä¸ºå‘å±•å­ŸåŠ æ‹‰è¯­çš„å¤æ‚ã€è¯­ä¹‰ä¿ç•™æ–‡æœ¬è§„èŒƒåŒ–ç³»ç»Ÿå»ºç«‹äº†åšå®çš„è¯­è¨€åŸºç¡€ï¼Œå¹¶ä¸ºå¼€å‘æ­¤ç±»ç³»ç»Ÿæä¾›äº†å¿…è¦çš„æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13159v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡ç« çš„æ‘˜è¦ä¸ºï¼šé’ˆå¯¹å­ŸåŠ æ‹‰è¯­ç­‰ä½èµ„æºè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•æ–‡æœ¬ä¸­çš„é‡å¤ç°è±¡ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åŒºåˆ†é‡å¤ç°è±¡ä¸é‡å¤å¤±è¯­çš„æ–°æ–¹æ³•ã€‚é€šè¿‡æ„å»ºé¦–ä¸ªå…¬å¼€å¯ç”¨çš„å­ŸåŠ æ‹‰è¯­è¯­æ–™åº“ï¼Œå¯¹ASRè½¬å½•ä¸­çš„é‡å¤ç°è±¡è¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨ï¼Œå¹¶åˆ†åˆ«é‡‡ç”¨å½“å‰æœ€å…ˆè¿›çš„å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹å’Œç‰¹å®šä»»åŠ¡å¾®è°ƒç¼–ç å™¨æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œå¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å°‘é‡æ ·æœ¬æç¤ºå°±å®ç°äº†ç«äº‰æ€§è¡¨ç°ï¼›è€Œé€šè¿‡å¾®è°ƒè¯­è¨€ç‰¹å®šçš„å­ŸåŠ æ‹‰è¯­BERTæ¨¡å‹è·å¾—äº†æœ€ä½³æ€§èƒ½å’ŒF1åˆ†æ•°ã€‚è¿™ä¸ºå‘å±•å¤æ‚çš„è¯­ä¹‰ä¿ç•™æ–‡æœ¬è§„èŒƒåŒ–ç³»ç»Ÿæä¾›äº†é‡è¦æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼š</p>
<ol>
<li>ä½èµ„æºè¯­è¨€çš„ASRè½¬å½•æ–‡æœ¬ä¸­å­˜åœ¨ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œå³éœ€è¦åŒºåˆ†é‡å¤çš„æ„å›¾æ˜¯å¦ä¸ºå¤±è¯­æˆ–æœ‰æ„ä¸ºä¹‹çš„è¯­æ³•æ„é€ ã€‚ </li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„ã€å«æœ‰äºŒåä¸‡ä¸ªæ‰‹åŠ¨æ ‡æ³¨æ•°æ®çš„å­ŸåŠ æ‹‰è¯­è¯­æ–™åº“ã€‚</li>
<li>æ–‡ç« åˆ©ç”¨ä¸¤ç§ä¸åŒçš„æ–¹æ³•æ¥æµ‹è¯•è¿™ä¸€æ–°èµ„æºï¼šå½“å‰æœ€å…ˆè¿›çš„å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹å’Œç‰¹å®šä»»åŠ¡å¾®è°ƒç¼–ç å™¨æ¨¡å‹ã€‚ </li>
<li>å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºäº†ç«äº‰åŠ›ï¼Œå‡†ç¡®æ€§è¾¾åˆ°82.68%ã€‚ </li>
<li>ä»»åŠ¡ç‰¹å®šå¾®è°ƒç¼–ç å™¨æ¨¡å‹è¡¨ç°æ›´ä½³ï¼Œå…¶ä¸­å­ŸåŠ æ‹‰è¯­BERTæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«ä¸º84.78%å’Œ0.677ã€‚ </li>
<li>æ­¤ç ”ç©¶æä¾›äº†ç”¨äºå­ŸåŠ æ‹‰è¯­çš„æ›´ä¸ºå¤æ‚çš„è¯­ä¹‰ä¿ç•™æ–‡æœ¬è§„èŒƒåŒ–ç³»ç»Ÿçš„é‡è¦æ•°æ®æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f05e27973a12c4809011c2d3697f7355" align="middle">
<img src="https://picx.zhimg.com/v2-249c99f5253d4ee7055be1797731e772" align="middle">
<img src="https://picx.zhimg.com/v2-ba62b6f715374202c816b0bc54650953" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Soft-Conflict-Resolution-Decision-Transformer-for-Offline-Multi-Task-Reinforcement-Learning"><a href="#Soft-Conflict-Resolution-Decision-Transformer-for-Offline-Multi-Task-Reinforcement-Learning" class="headerlink" title="Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning"></a>Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning</h2><p><strong>Authors:Shudong Wang, Xinfei Wang, Chenhao Zhang, Shanchen Pang, Haiyuan Gui, Wenhao Ji, Xiaojian Liao</strong></p>
<p>Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the modelâ€™s generalization and learning efficiency.   To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.</p>
<blockquote>
<p>å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼ˆMTRLï¼‰æ—¨åœ¨å­¦ä¹ å¤šç§ä»»åŠ¡çš„ç»Ÿä¸€ç­–ç•¥ï¼Œä½†é€šå¸¸é¢ä¸´è·¨ä»»åŠ¡çš„æ¢¯åº¦å†²çªé—®é¢˜ã€‚ç°æœ‰çš„åŸºäºæ©ç çš„æ–¹æ³•è¯•å›¾é€šè¿‡åˆ†é…ä»»åŠ¡ç‰¹å®šçš„å‚æ•°æ©ç æ¥ç¼“è§£è¿™ç§å†²çªã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç»éªŒç ”ç©¶è¡¨æ˜ï¼Œç²—ç²’åº¦çš„äºŒè¿›åˆ¶æ©ç ä¼šè¿‡åº¦æŠ‘åˆ¶å…³é”®å†²çªå‚æ•°ï¼Œé˜»ç¢ä»»åŠ¡ä¹‹é—´çš„çŸ¥è¯†å…±äº«ã€‚æ­¤å¤–ï¼Œä¸åŒä»»åŠ¡è¡¨ç°å‡ºä¸åŒçº§åˆ«çš„å†²çªï¼Œè€Œç°æœ‰æ–¹æ³•é‡‡ç”¨ä¸€åˆ€åˆ‡å¼çš„å›ºå®šç¨€ç–åº¦ç­–ç•¥æ¥ç»´æŒè®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ï¼Œè¿™è¯æ˜æ˜¯ä¸è¶³çš„ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†æ¨¡å‹çš„æ¨å¹¿å’Œå­¦ä¹ æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13133v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼ˆMTRLï¼‰ä¸­æ¢¯åº¦å†²çªçš„é—®é¢˜ã€‚ç°æœ‰åŸºäºæ©ç çš„æ–¹æ³•è¯•å›¾é€šè¿‡åˆ†é…ä»»åŠ¡ç‰¹å®šå‚æ•°æ©ç æ¥ç¼“è§£è¿™ç§å†²çªï¼Œä½†å­˜åœ¨ç²—ç²’åº¦çš„äºŒè¿›åˆ¶æ©ç ä¼šè¿‡åº¦æŠ‘åˆ¶å…³é”®å†²çªå‚æ•°çš„é—®é¢˜ï¼Œé˜»ç¢ä»»åŠ¡é—´çš„çŸ¥è¯†å…±äº«ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºå‚æ•°é‡è¦æ€§çš„è½¯å†²çªè§£å†³æ–¹æ³•çš„SoCo-DTæ–¹æ¡ˆï¼Œåˆ©ç”¨Fisherä¿¡æ¯åŠ¨æ€è°ƒæ•´æ©ç å€¼ä»¥ä¿ç•™é‡è¦å‚æ•°å¹¶æŠ‘åˆ¶å†²çªå‚æ•°ã€‚åŒæ—¶å¼•å…¥åŸºäºIQRçš„åŠ¨æ€ç¨€ç–è°ƒæ•´ç­–ç•¥ï¼Œå¹¶é€šè¿‡ä¸å¯¹ç§°ä½™å¼¦é€€ç«æ—¶é—´è¡¨å®ç°è‡ªé€‚åº”ç¨€ç–æ¼”å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoCo-DTåœ¨Meta-WorldåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç¼“è§£æ¢¯åº¦å†²çªå’Œæé«˜æ•´ä½“å¤šä»»åŠ¡æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼ˆMTRLï¼‰é¢ä¸´æ¢¯åº¦å†²çªçš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰åŸºäºæ©ç çš„æ–¹æ³•è¯•å›¾ç¼“è§£æ¢¯åº¦å†²çªï¼Œä½†å­˜åœ¨ç²—ç²’åº¦çš„äºŒè¿›åˆ¶æ©ç ä¼šè¿‡åº¦æŠ‘åˆ¶å…³é”®å‚æ•°çš„é—®é¢˜ã€‚</li>
<li>SoCo-DTæ–¹æ¡ˆé€šè¿‡å‚æ•°é‡è¦æ€§çš„è½¯å†²çªè§£å†³æ–¹æ³•ï¼Œåˆ©ç”¨Fisherä¿¡æ¯åŠ¨æ€è°ƒæ•´æ©ç å€¼ã€‚</li>
<li>å¼•å…¥åŸºäºIQRçš„åŠ¨æ€ç¨€ç–è°ƒæ•´ç­–ç•¥ï¼Œæ ¹æ®ä»»åŠ¡çš„å†²çªå’Œå’Œè°åˆ†æ•°åˆ†å¸ƒæ„å»ºä»»åŠ¡ç‰¹å®šé˜ˆå€¼æ–¹æ¡ˆã€‚</li>
<li>é‡‡ç”¨ä¸å¯¹ç§°ä½™å¼¦é€€ç«æ—¶é—´è¡¨ï¼Œä½¿ç¨€ç–é˜ˆå€¼èƒ½å¤Ÿè‡ªé€‚åº”æ¼”å˜ã€‚</li>
<li>åœ¨Meta-WorldåŸºå‡†æµ‹è¯•ä¸Šï¼ŒSoCo-DTè¡¨ç°ä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶ç¼“è§£æ¢¯åº¦å†²çªå’Œæé«˜å¤šä»»åŠ¡æ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c789917722590bb784ea7a79c6371887" align="middle">
<img src="https://picx.zhimg.com/v2-da971ae0c0cbc93d715292b41cb4a48e" align="middle">
<img src="https://picx.zhimg.com/v2-d62a1cec0a7edb6e1f15bb8ad930270d" align="middle">
<img src="https://picx.zhimg.com/v2-a6b33478c9d8128f13a5ec6128c63922" align="middle">
<img src="https://picx.zhimg.com/v2-5e0a3a10cbf40a424589b192359d9c18" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MergeSlide-Continual-Model-Merging-and-Task-to-Class-Prompt-Aligned-Inference-for-Lifelong-Learning-on-Whole-Slide-Images"><a href="#MergeSlide-Continual-Model-Merging-and-Task-to-Class-Prompt-Aligned-Inference-for-Lifelong-Learning-on-Whole-Slide-Images" class="headerlink" title="MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images"></a>MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images</h2><p><strong>Authors:Doanh C. Bui, Ba Hung Ngo, Hoai Luan Pham, Khang Nguyen, MaÃ¯ K. Nguyen, Yasuhiko Nakashima</strong></p>
<p>Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/caodoanh2001/MergeSlide">https://github.com/caodoanh2001/MergeSlide</a>.</p>
<blockquote>
<p>ç»ˆèº«å­¦ä¹ åœ¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰ä¸Šçš„åº”ç”¨æ—¨åœ¨åœ¨ä¸€ç³»åˆ—ç™Œç—‡ç›¸å…³ä»»åŠ¡ä¸Šé¡ºåºåœ°è®­ç»ƒæˆ–å¾®è°ƒç»Ÿä¸€æ¨¡å‹ï¼Œä»è€Œå‡å°‘æ•°æ®è¿ç§»å’Œå¤„ç†çš„èµ„æºæ¶ˆè€—å’ŒåŠªåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è€ƒè™‘åˆ°WSIçš„åƒå…†å­—èŠ‚è§„æ¨¡å¤§å°çš„æƒ…å†µä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MergeSlideï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œå®ƒå°†ç»ˆèº«å­¦ä¹ è§†ä¸ºæ¨¡å‹åˆå¹¶é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€ç—…ç†å­¦åŸºç¡€æ¨¡å‹æ¥è§£å†³ã€‚å½“æœ‰æ–°çš„ä»»åŠ¡åˆ°æ¥æ—¶ï¼Œå®ƒæ˜¯ï¼š1ï¼‰é€šè¿‡ç±»æ„ŸçŸ¥æç¤ºè¿›è¡Œå®šä¹‰ï¼Œ2ï¼‰ä½¿ç”¨æ— å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„éª¨å¹²è¿›è¡Œå‡ ä¸ªå‘¨æœŸçš„å¾®è°ƒï¼Œ3ï¼‰ä½¿ç”¨æ­£äº¤æŒç»­åˆå¹¶ç­–ç•¥å°†å…¶åˆå¹¶åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œè¿™ä¿ç•™äº†æ€§èƒ½å¹¶å‡è½»äº†ç¾éš¾æ€§é—å¿˜ã€‚åœ¨ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆCLASS-ILï¼‰è®¾ç½®ä¸‹è¿›è¡Œæ¨ç†æ—¶ï¼Œä»»åŠ¡èº«ä»½æ˜¯æœªçŸ¥çš„ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†ä»»åŠ¡åˆ°ç±»åˆ«æç¤ºå¯¹é½ï¼ˆTCPï¼‰æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒTCPé¦–å…ˆä½¿ç”¨ä»»åŠ¡çº§æç¤ºè¯†åˆ«æœ€ç›¸å…³çš„ä»»åŠ¡ï¼Œç„¶ååº”ç”¨ç›¸åº”çš„ç±»æ„ŸçŸ¥æç¤ºæ¥ç”Ÿæˆé¢„æµ‹ã€‚ä¸ºäº†è¯„ä¼°MergeSlideï¼Œæˆ‘ä»¬åœ¨å…­ä¸ªTCGAæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒMergeSlideä¼˜äºåŸºäºå¤æ¼”çš„æŒç»­å­¦ä¹ å’Œè§†è§‰è¯­è¨€é›¶æ ·æœ¬åŸºçº¿ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/caodoanh2001/MergeSlide%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/caodoanh2001/MergeSlideä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13099v1">PDF</a> WACV2026 Accepted</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰çš„ç»ˆèº«å­¦ä¹ æ—¨åœ¨é€šè¿‡ä¸€ç³»åˆ—ç™Œç—‡ç›¸å…³ä»»åŠ¡æ¥è®­ç»ƒæˆ–å¾®è°ƒç»Ÿä¸€æ¨¡å‹ï¼Œä»è€Œå‡å°‘å¯¹æ•°æ®ä¼ è¾“å’Œå¤„ç†çš„èµ„æºéœ€æ±‚ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶â€”â€”MergeSlideï¼Œå®ƒå°†ç»ˆèº«å­¦ä¹ è§†ä¸ºæ¨¡å‹åˆå¹¶é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€ç—…ç†å­¦åŸºç¡€æ¨¡å‹æ¥è§£å†³ã€‚å½“æœ‰æ–°çš„ä»»åŠ¡æ—¶ï¼Œå®ƒé€šè¿‡ç±»æ„ŸçŸ¥æç¤ºè¿›è¡Œå®šä¹‰ï¼Œä½¿ç”¨æ— å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„éª¨æ¶è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨æ­£äº¤æŒç»­åˆå¹¶ç­–ç•¥å°†å…¶åˆå¹¶åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œä»è€Œä¿ç•™æ€§èƒ½å¹¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚åœ¨ä»»åŠ¡èº«ä»½æœªçŸ¥çš„ç±»å¢é‡å­¦ä¹ ï¼ˆCLASS-ILï¼‰è®¾ç½®ä¸‹ï¼Œå¼•å…¥ä»»åŠ¡åˆ°ç±»æç¤ºå¯¹é½ï¼ˆTCPï¼‰æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMergeSlideåœ¨å…­ä¸ªTCGAæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºäºå¤ä¹ çš„è¿ç»­å­¦ä¹ å’Œè§†è§‰è¯­è¨€é›¶æ ·æœ¬åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MergeSlideåˆ©ç”¨è§†è§‰è¯­è¨€ç—…ç†å­¦åŸºç¡€æ¨¡å‹ï¼Œå°†ç»ˆèº«å­¦ä¹ è§†ä¸ºæ¨¡å‹åˆå¹¶é—®é¢˜æ¥è§£å†³ã€‚</li>
<li>æ–°ä»»åŠ¡é€šè¿‡ç±»æ„ŸçŸ¥æç¤ºè¿›è¡Œå®šä¹‰ï¼Œå¹¶ä½¿ç”¨æ— å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„éª¨æ¶è¿›è¡Œå¾®è°ƒã€‚</li>
<li>ä½¿ç”¨æ­£äº¤æŒç»­åˆå¹¶ç­–ç•¥ï¼Œå°†æ–°ä»»åŠ¡åˆå¹¶åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œä¿ç•™æ€§èƒ½å¹¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>å¼•å…¥ä»»åŠ¡åˆ°ç±»æç¤ºå¯¹é½ï¼ˆTCPï¼‰æ¨ç†ï¼Œç”¨äºç±»å¢é‡å­¦ä¹ è®¾ç½®ä¸‹çš„æ¨æ–­ã€‚</li>
<li>TCPé¦–å…ˆé€šè¿‡ä»»åŠ¡çº§åˆ«æç¤ºè¯†åˆ«æœ€ç›¸å…³çš„ä»»åŠ¡ï¼Œç„¶ååº”ç”¨ç›¸åº”çš„ç±»æ„ŸçŸ¥æç¤ºæ¥ç”Ÿæˆé¢„æµ‹ã€‚</li>
<li>MergeSlideåœ¨å…­ä¸ªTCGAæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå¤ä¹ å¼è¿ç»­å­¦ä¹ å’Œè§†è§‰è¯­è¨€é›¶æ ·æœ¬åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-049c5743b6429a5556599de446dfb39f" align="middle">
<img src="https://picx.zhimg.com/v2-f7a0eb038c23ea26ee797ffa6be62044" align="middle">
<img src="https://picx.zhimg.com/v2-66ef1da2d2bbba4e69dd0485c4611263" align="middle">
<img src="https://picx.zhimg.com/v2-8e17af0bcaa5d46928e2436770c71779" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GEM-Generative-Entropy-Guided-Preference-Modeling-for-Few-shot-Alignment-of-LLMs"><a href="#GEM-Generative-Entropy-Guided-Preference-Modeling-for-Few-shot-Alignment-of-LLMs" class="headerlink" title="GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs"></a>GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs</h2><p><strong>Authors:Yiyang Zhao, Huiyu Bai, Xuejiao Zhao</strong></p>
<p>Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½é€šå¸¸ä¾èµ–äºç›‘ç£å¥–åŠ±æ¨¡å‹æˆ–å¤–éƒ¨è¯„åˆ¤è€…ï¼Œè¿™éœ€è¦å¤§é‡æ ‡æ³¨ã€‚ç„¶è€Œï¼Œåœ¨ä¾èµ–ä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸï¼Œå¦‚åŒ»å­¦å’Œæ³•å¾‹ï¼Œè¿™ç§å¤§è§„æ¨¡åå¥½æ ‡ç­¾å¾€å¾€éš¾ä»¥å®ç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºLLMåœ¨èµ„æºç¨€ç¼ºå’Œç‰¹å®šé¢†åŸŸåœºæ™¯ä¸­å¯¹é½çš„ç”Ÿæˆç†µå¼•å¯¼åå¥½å»ºæ¨¡æ–¹æ³•ï¼Œåä¸ºGEMã€‚æˆ‘ä»¬ä¸åœ¨åå¥½æ•°æ®ä¸Šè®­ç»ƒåˆ¤åˆ«å¥–åŠ±æ¨¡å‹ï¼Œè€Œæ˜¯ç›´æ¥è®­ç»ƒLLMå†…åŒ–é—­ç¯ä¼˜åŒ–æ¶æ„ï¼Œèƒ½å¤Ÿæå–å’Œåˆ©ç”¨äººç±»åå¥½ä¸­éšå«çš„å¤šç»´åº¦ã€ç²¾ç»†çš„è®¤çŸ¥ä¿¡å·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„è®¤çŸ¥è¿‡æ»¤æ¨¡å—åŸºäºå†³ç­–ä¸­çš„ç†µç†è®ºï¼Œé¦–å…ˆåˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºç”Ÿæˆæ¥è‡ªåå¥½æ•°æ®çš„å¤šæ ·åŒ–å€™é€‰æ¨ç†é“¾ï¼ˆCoTsï¼‰ã€‚éšåï¼Œå®ƒå¼•å…¥äº†ä¸€ç§ä»¤ç‰Œè¯„åˆ†æœºåˆ¶æ¥å¯¹é‡‡æ ·åˆ°çš„CoTsè¿›è¡Œæ’åå’ŒåŠ æƒï¼Œæé«˜é«˜ç½®ä¿¡åº¦ç­”æ¡ˆå’Œæˆ˜ç•¥é«˜ç†µä»¤ç‰Œçš„é‡è¦æ€§ã€‚åŸºäºè¿™äº›è¿‡æ»¤åçš„åå¥½ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–°å‹è‡ªæˆ‘è¯„ä»·ç¾¤ä½“ä¼˜åŠ¿ç®—æ³•SEGAå¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œè¯¥ç®—æ³•å¯æœ‰æ•ˆèšåˆç¾¤ä½“å±‚é¢çš„è®¤çŸ¥ä¿¡å·ï¼Œå°†ç†µå¾—åˆ†è½¬åŒ–ä¸ºç­–ç•¥ä¼˜åŒ–çš„éšæ€§å¥–åŠ±ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒGEMä½¿LLMèƒ½å¤Ÿä¾èµ–å…¶è‡ªå·±çš„åˆ¤æ–­å¹¶å»ºç«‹ç†µå¼•å¯¼çš„é—­ç¯è®¤çŸ¥ä¼˜åŒ–æ¡†æ¶ï¼Œå®ç°LLMçš„é«˜æ•ˆå°‘æ ·æœ¬å¯¹é½ã€‚åœ¨é€šç”¨åŸºå‡†æµ‹è¯•å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ï¼ˆå¦‚æ•°å­¦æ¨ç†å’ŒåŒ»ç–—å¯¹è¯ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GEMåœ¨å°‘é‡åå¥½æ•°æ®çš„æƒ…å†µä¸‹å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13007v1">PDF</a> This paper has been accepted by AAAI 2026-AIA and designated as an oral presentation paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆç†µå¼•å¯¼åå¥½å»ºæ¨¡çš„æ–¹æ³•ï¼ˆGEMï¼‰ï¼Œç”¨äºåœ¨ä½èµ„æºå’Œç‰¹å®šé¢†åŸŸçš„åœºæ™¯ä¸‹å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½ã€‚è¯¥æ–¹æ³•æ— éœ€ä¾èµ–å¤§é‡çš„åå¥½æ ‡ç­¾æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡ä¼˜åŒ–å†…éƒ¨åŒ–é—­ç¯æ¶æ„ï¼Œæå–å’Œåˆ©ç”¨äººç±»åå¥½ä¸­çš„å¤šç»´ç²¾ç»†è®¤çŸ¥ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨é€šç”¨åŸºå‡†æµ‹è¯•å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸Šï¼ŒGEMæ–¹æ³•åˆ©ç”¨å°‘é‡çš„åå¥½æ•°æ®å³å¯å®ç°æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•GEMï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºå’Œç‰¹å®šé¢†åŸŸåœºæ™¯ã€‚</li>
<li>GEMæ–¹æ³•ä¸ä¾èµ–å¤§é‡çš„åå¥½æ ‡ç­¾æ•°æ®ï¼Œè€Œæ˜¯ç›´æ¥è®­ç»ƒLLMå†…éƒ¨åŒ–é—­ç¯ä¼˜åŒ–æ¶æ„ã€‚</li>
<li>GEMé€šè¿‡ç†µç†è®ºå†³ç­–ä¸­çš„è®¤çŸ¥è¿‡æ»¤æ¨¡å—ï¼Œåˆ©ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºç”Ÿæˆå¤šæ ·çš„å€™é€‰æ¨ç†é“¾ï¼Œå¹¶å¼•å…¥æ ‡è®°æœºåˆ¶å¯¹é‡‡æ ·ç»“æœè¿›è¡Œæ’åå’ŒåŠ æƒã€‚</li>
<li>SEGAç®—æ³•ç”¨äºåŸºäºè¿‡æ»¤åçš„åå¥½å¾®è°ƒLLMï¼Œæœ‰æ•ˆèšåˆç¾¤ä½“è®¤çŸ¥ä¿¡å·ï¼Œå¹¶å°†ç†µå¾—åˆ†è½¬åŒ–ä¸ºéšå¼å¥–åŠ±ï¼Œç”¨äºç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>GEMæ–¹æ³•ä½¿LLMèƒ½å¤Ÿä¾é è‡ªèº«åˆ¤æ–­ï¼Œå»ºç«‹äº†ä¸€ä¸ªåŸºäºç†µå¼•å¯¼çš„é—­ç¯è®¤çŸ¥ä¼˜åŒ–æ¡†æ¶ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é€šç”¨åŸºå‡†æµ‹è¯•å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸Šï¼ŒGEMæ–¹æ³•åˆ©ç”¨å°‘é‡çš„åå¥½æ•°æ®å³å¯å®ç°æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†å’ŒåŒ»ç–—å¯¹è¯ç­‰ä»»åŠ¡ä¸Šã€‚</li>
<li>GEMæ–¹æ³•ä¸ºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½çš„é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„å¯èƒ½é€”å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™å’Œé¢†åŸŸç‰¹å®šçš„åœºæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebfd2d17d728ace7c801962d91060439" align="middle">
<img src="https://picx.zhimg.com/v2-a6fff22cb17c436f0047814caa685047" align="middle">
<img src="https://picx.zhimg.com/v2-1aced29c745c9aac9f1b328c35ca5b23" align="middle">
<img src="https://picx.zhimg.com/v2-e5fe3477047755660cf50e68a606928a" align="middle">
<img src="https://picx.zhimg.com/v2-163d8e717b89076fdafb28dee5e374c3" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SAGE-Spuriousness-Aware-Guided-Prompt-Exploration-for-Mitigating-Multimodal-Bias"><a href="#SAGE-Spuriousness-Aware-Guided-Prompt-Exploration-for-Mitigating-Multimodal-Bias" class="headerlink" title="SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias"></a>SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias</h2><p><strong>Authors:Wenqian Ye, Di Wang, Guangtao Zheng, Bohan Liu, Aidong Zhang</strong></p>
<p>Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the objectâ€™s core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.</p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¦‚CLIPï¼Œé€šè¿‡åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­å¯¹é½å›¾åƒå’Œæ–‡æœ¬ï¼Œå±•ç°å‡ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚ç„¶è€Œï¼ŒCLIPæ¨¡å‹ç»å¸¸ä¼šäº§ç”Ÿå¤šæ¨¡æ€çš„è™šå‡åè§ï¼Œå³ä¸ç†æƒ³çš„ä¾èµ–è™šå‡ç‰¹å¾çš„å€¾å‘ã€‚ä¾‹å¦‚ï¼ŒCLIPå¯èƒ½ä¼šæ ¹æ®ç»å¸¸å…±åŒå‡ºç°çš„èƒŒæ™¯è€Œä¸æ˜¯å¯¹è±¡çš„æ ¸å¿ƒç‰¹å¾æ¥æ¨æ–­å›¾åƒä¸­çš„å¯¹è±¡ç±»å‹ã€‚è¿™ç§åè§ä¼šæ˜¾è‘—æŸå®³é¢„è®­ç»ƒCLIPæ¨¡å‹åœ¨éåˆ†å¸ƒæ•°æ®ä¸Šçš„ç¨³å¥æ€§ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè·¨æ¨¡æ€å…³è”ä¸å†æœ‰æ•ˆã€‚ç°æœ‰çš„å‡è½»å¤šæ¨¡æ€è™šå‡åè§çš„æ–¹æ³•é€šå¸¸éœ€è¦åœ¨ä¸‹æ¸¸æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒæˆ–é¢„å…ˆäº†è§£åè§ï¼Œè¿™ç ´åäº†CLIPçš„å¼€ç®±å³ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»ç†è®ºä¸Šåˆ†æäº†å¤šæ¨¡æ€è™šå‡åè§å¯¹é›¶æ ·æœ¬åˆ†ç±»çš„å½±å“ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†Spuriousness-Aware Guided Explorationï¼ˆSAGEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼•å¯¼æç¤ºé€‰æ‹©æ¥å‡è½»è™šå‡åè§ã€‚SAGEæ— éœ€è®­ç»ƒã€å¾®è°ƒæˆ–å¤–éƒ¨æ³¨é‡Šã€‚å®ƒæ¢ç´¢æç¤ºæ¨¡æ¿çš„ç©ºé—´ï¼Œé€‰æ‹©é‚£äº›åœ¨ç±»ä¹‹é—´äº§ç”Ÿæœ€å¤§è¯­ä¹‰åˆ†ç¦»çš„æç¤ºï¼Œä»è€Œæé«˜æœ€å·®ç»„çš„ç¨³å¥æ€§ã€‚åœ¨å››ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†å’Œäº”ä¸ªæµè¡Œéª¨å¹²æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSAGEæŒç»­æé«˜äº†é›¶æ ·æœ¬æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¶Šäº†ä»¥å‰ä¸éœ€è¦å¤–éƒ¨çŸ¥è¯†æˆ–æ¨¡å‹æ›´æ–°çš„é›¶æ ·æœ¬æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13005v1">PDF</a> Accepted at AAAI 2026</p>
<p><strong>Summary</strong>ï¼šåŸºäºCLIPçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬åˆ†ç±»ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å­˜åœ¨å¤šæ¨¡æ€å¶ç„¶åè§é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSAGEçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼æç¤ºé€‰æ‹©æ¥å‡è½»è™šå‡åè§ã€‚SAGEæ— éœ€è®­ç»ƒã€å¾®è°ƒæˆ–å¤–éƒ¨æ³¨é‡Šï¼Œé€šè¿‡æ¢ç´¢æç¤ºæ¨¡æ¿å¹¶é€‰æ‹©èƒ½å¤Ÿæœ€å¤§ç¨‹åº¦åœ°åœ¨ç±»ä¹‹é—´äº§ç”Ÿè¯­ä¹‰åˆ†éš”çš„æç¤ºï¼Œä»è€Œæé«˜æœ€å·®ç»„çš„ç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼ŒSAGEåœ¨é›¶æ ·æœ¬åˆ†ç±»å’Œæ³›åŒ–æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CLIPç­‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬åˆ†ç±»ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å­˜åœ¨å¤šæ¨¡æ€å¶ç„¶åè§é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦ä¸‹æ¸¸æ•°æ®çš„å¾®è°ƒæˆ–é¢„å…ˆäº†è§£åè§ï¼Œè¿™å½±å“äº†CLIPçš„å³æ’å³ç”¨æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Spuriousness-Aware Guided Explorationï¼ˆSAGEï¼‰æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>SAGEé€šè¿‡æ¢ç´¢æç¤ºæ¨¡æ¿å¹¶é€‰æ‹©èƒ½å¤Ÿæœ€å¤§ç¨‹åº¦åœ°åœ¨ç±»ä¹‹é—´äº§ç”Ÿè¯­ä¹‰åˆ†éš”çš„æç¤ºï¼Œä»¥å‡è½»åè§é—®é¢˜ã€‚</li>
<li>SAGEä¸éœ€è¦è®­ç»ƒã€å¾®è°ƒæˆ–å¤–éƒ¨æ³¨é‡Šï¼Œå…·æœ‰ç®€å•æœ‰æ•ˆçš„ç‰¹ç‚¹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSAGEåœ¨å››ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†å’Œäº”ä¸ªæµè¡Œéª¨å¹²æ¨¡å‹ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…ˆå‰çš„é›¶æ ·æœ¬æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-228ba730afbaaedfdcc1e984500358d5" align="middle">
<img src="https://picx.zhimg.com/v2-dd51879f894837e1f2eb9ff930eae1f5" align="middle">
<img src="https://picx.zhimg.com/v2-71956217baae8d4161076756843f7723" align="middle">
<img src="https://picx.zhimg.com/v2-9a454acd16d0a233591231e883afc2c4" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Infinite-Story-A-Training-Free-Consistent-Text-to-Image-Generation"><a href="#Infinite-Story-A-Training-Free-Consistent-Text-to-Image-Generation" class="headerlink" title="Infinite-Story: A Training-Free Consistent Text-to-Image Generation"></a>Infinite-Story: A Training-Free Consistent Text-to-Image Generation</h2><p><strong>Authors:Jihun Park, Kyoungmin Lee, Jongmin Gim, Hyeonseo Jo, Minseok Oh, Wonhyeok Choi, Kyumin Hwang, Jaeyeul Kim, Minwoo Choi, Sunghoon Im</strong></p>
<p>We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¡†æ¶ï¼Œåä¸ºInfinite-Storyï¼Œå®ƒé€‚ç”¨äºå¤šæç¤ºå™äº‹åœºæ™¯ã€‚åŸºäºè§„æ¨¡åŒ–çš„è‡ªå›å½’æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šèº«ä»½ä¸ä¸€è‡´å’Œé£æ ¼ä¸ä¸€è‡´ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ç§äº’è¡¥æŠ€æœ¯ï¼šèº«ä»½æç¤ºæ›¿æ¢ï¼Œå®ƒå‡è½»äº†æ–‡æœ¬ç¼–ç å™¨ä¸­ä¸Šä¸‹æ–‡åè§çš„å½±å“ï¼Œä½¿ä¸åŒæç¤ºä¸­çš„èº«ä»½å±æ€§ä¿æŒä¸€è‡´ï¼›ä»¥åŠåŒ…å«è‡ªé€‚åº”é£æ ¼æ³¨å…¥å’ŒåŒæ­¥æŒ‡å¯¼é€‚åº”çš„ç»Ÿä¸€æ³¨æ„åŠ›å¼•å¯¼æœºåˆ¶ï¼Œå®ƒä»¬å…±åŒç¡®ä¿äº†å…¨å±€é£æ ¼å’Œèº«ä»½å¤–è§‚çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒäº†æç¤ºä¿çœŸåº¦ã€‚ä¸åŒäºåŸºäºæ‰©æ•£çš„æ–¹æ³•éœ€è¦å¾®è°ƒæˆ–åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¾ƒæ…¢çš„é—®é¢˜ï¼ŒInfinite-Storyå®Œå…¨åœ¨æµ‹è¯•é˜¶æ®µè¿è¡Œï¼Œç¡®ä¿åœ¨ä¸åŒæç¤ºä¸‹çš„èº«ä»½å’Œé£æ ¼ä¸€è‡´æ€§éå¸¸é«˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆæ€§èƒ½ï¼Œå¹¶ä¸”ç›¸æ¯”ç°æœ‰æœ€å¿«çš„æ–‡æœ¬ä¸€è‡´T2Iæ¨¡å‹æ¨ç†é€Ÿåº¦æé«˜äº†6å€ä»¥ä¸Šï¼ˆæ¯å¼ å›¾åƒåªéœ€1.72ç§’ï¼‰ï¼Œè¿™å‡¸æ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„å™äº‹æ•…äº‹ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13002v1">PDF</a> 18pages, 13 figures, AAAI 2026 Oral</p>
<p><strong>Summary</strong></p>
<p>æ— é™æ•…äº‹ï¼šæ— éœ€è®­ç»ƒå³å¯å®ç°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ–¹æ³•ã€‚è§£å†³äº†è·¨æç¤ºèº«ä»½ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¼•å…¥äº†èº«ä»½æç¤ºæ›¿æ¢ç­‰æŠ€æœ¯ã€‚å…·æœ‰å¿«é€Ÿæ¨ç†å’Œå®ç”¨æ€§å¼ºçš„ç‰¹ç‚¹ã€‚é‡‡ç”¨è‡ªé€‚åº”é£æ ¼æ³¨å…¥ç­‰æŠ€æœ¯å®ç°å…¨å±€é£æ ¼å’Œèº«ä»½çš„ä¸€è‡´æ€§ï¼Œå¯ç”¨äºç°å®ä¸–ç•Œä¸­çš„è§†è§‰æ•…äº‹åˆ›ä½œã€‚å…¶åœ¨å¤šç§åœºæ™¯ä¸‹å…·æœ‰ä¼˜å¼‚æ€§èƒ½ã€‚æ— é™æ•…äº‹é¢å‘å¤šç§åœºåˆåº”ç”¨è®¾è®¡çš„ç‰¹ç‚¹è§£å†³äº†ä¸€è‡´æ€§T2Iç”Ÿæˆçš„éš¾é¢˜ã€‚åœ¨ä¿è¯ç»˜å›¾é€Ÿåº¦å’Œç”»è´¨çš„æƒ…å†µä¸‹ä»ç„¶æ˜¾è‘—è¶…è¶Šå½“å‰å¿«é€Ÿæ¨¡å‹æ°´å¹³çš„æŠ€æœ¯éå¸¸å…ˆè¿›ä¸”å®ç”¨ã€‚å…¨çƒä¸€æµçš„åˆ›æ–°æŠ€æœ¯å’Œç§‘å­¦è§£å†³æ–¹æ¡ˆæ˜¾è‘—æ¨åŠ¨äº†æ•°å­—å™äº‹äº§ä¸šçš„æŒç»­å‘å±•ä¸åˆ›æ–°æ–¹å‘çš„å®ç°æ–¹å‘çš„å¯å®è·µæ€§å’Œå‘å±•æ½œèƒ½å¹¿æ³›é€‚ç”¨äºå¤šé¢†åŸŸåº”ç”¨åœºæ™¯ï¼Œå¹¶å¯å®ç°æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯åˆ›æ–°ä¸å‘å±•æ–¹å‘çš„å¯æ‰©å±•æ€§æ˜¾è‘—æé«˜äº†è§†è§‰å™äº‹çš„è´¨é‡ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œæœªæ¥æ•°å­—å™äº‹é¢†åŸŸçš„å‘å±•å‰æ™¯å°†æ›´åŠ å¹¿é˜”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Infinite-Storyæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¤šæç¤ºæ•…äº‹å™è¿°åœºæ™¯ã€‚</li>
<li>è§£å†³äº†èº«ä»½ä¸ä¸€è‡´å’Œé£æ ¼ä¸ä¸€è‡´çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥èº«ä»½æç¤ºæ›¿æ¢æŠ€æœ¯æ¥å‡è½»æ–‡æœ¬ç¼–ç å™¨çš„ä¸Šä¸‹æ–‡åè§ï¼Œä»¥å®ç°è·¨æç¤ºçš„èº«ä»½å±æ€§å¯¹é½ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”é£æ ¼æ³¨å…¥å’ŒåŒæ­¥æŒ‡å¯¼é€‚åº”çš„ç»Ÿä¸€æ³¨æ„åŠ›æŒ‡å¯¼æœºåˆ¶æ¥ä¿æŒå…¨å±€é£æ ¼å’Œèº«ä»½çš„ä¸€è‡´æ€§ã€‚æ­¤æ–¹æ³•èƒ½å¤Ÿä¿ç•™æç¤ºä¿çœŸåº¦å¹¶ç»´æŠ¤ä¸€è‡´çš„é£æ ¼ä¸èº«ä»½å¤–è§‚ã€‚ç›¸è¾ƒäºä¾èµ–ç²¾ç»†è°ƒæ•´æˆ–æ¨ç†ç¼“æ…¢çš„æ‰©æ•£æ¨¡å‹ï¼ŒInfinite-Storyèƒ½å¤Ÿåœ¨æµ‹è¯•é˜¶æ®µç‹¬ç«‹è¿è¡Œã€‚æ— é™æ•…äº‹çš„ç‰¹ç‚¹åŒ…æ‹¬å…¶åˆ›æ–°æ€§å’ŒæŠ€æœ¯ä¼˜è¶Šæ€§èƒ½å¤Ÿå¤§å¤§æé«˜å·¥ä½œæ•ˆç‡å’Œè´¨é‡å¯¹å™äº‹çš„ç†è§£å’Œå‘å±•èƒ½åŠ›ä½¿å…¶åœ¨å®è·µä¸­å…·æœ‰å¾ˆé«˜çš„åº”ç”¨ä»·å€¼å’Œçµæ´»æ€§æ˜¯ç ”ç©¶äººå‘˜é’ˆå¯¹å®é™…é—®é¢˜éœ€æ±‚è€Œè¿›è¡Œåˆ›æ–°æ€§ç ”ç©¶å’Œè®¾è®¡çš„ä»£è¡¨è§£å†³æ–¹æ¡ˆä½¿å…¶ä¸å…¶ä»–å·¥ä½œè¿›è¡Œäº†æ˜æ˜¾åŒºåˆ†èƒ½å¤Ÿå®ç°è¾ƒå¿«çš„é€Ÿåº¦åŠ å¿«å’Œä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒå¹¶åŠ å¼ºçµæ´»æ€§å¹¶èƒ½å¤Ÿæ‰©å¤§åœ¨å¤šæ¨¡æ€ä»¥åŠéç‰¹å®šå¯¹è±¡ç»“æ„ç­‰åœºæ™¯çš„åº”ç”¨åœºæ™¯å’Œåˆ›æ–°æ–¹å‘å®ç°æ›´å¿«ã€æ›´å‡†ç¡®çš„æ¨ç†é€Ÿåº¦å¹¶å…·æœ‰é«˜åº¦çš„å¯æ¨å¹¿æ€§å’Œé€‚ç”¨æ€§å¯ä»¥é€‚åº”ä¸åŒé¢†åŸŸçš„å¤æ‚åœºæ™¯å’Œé—®é¢˜å¹¶ä¸”å…·å¤‡è‰¯å¥½çš„å¼€å‘ç¯å¢ƒå’Œå¯æŒç»­æ€§æå¤§åœ°æ‹“å±•äº†ä¼ ç»Ÿçš„å·¥ä½œè¾¹ç•Œæ‰©å¤§äº†ç ”ç©¶å’Œåˆ›æ–°çš„æ½œåŠ›å¤§å¤§å¢å¼ºäº†æ¡†æ¶çš„ç¨³å®šæ€§å¹¶ä½¿å…³é”®æŠ€æœ¯åº”ç”¨æˆä¸ºå¯èƒ½å¹¶ä¸”å¯ä»¥å…‹æœé«˜éš¾åº¦çš„è®¡ç®—éš¾é¢˜æ¥æ‰©å±•å®é™…åœºæ™¯çš„é€šç”¨æ€§è¶…è¶Šäº†ä»¥å‰çš„æŠ€æœ¯æ°´å¹³ä¸ºè¡Œä¸šå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„å˜é©å’Œåˆ›æ–°å‘å±•æé«˜äº†å®é™…åº”ç”¨åœºæ™¯ä¸­çš„çµæ´»æ€§å’Œæ•ˆç‡æ€§åŒæ—¶æå¤§åœ°æé«˜äº†ç”¨æˆ·ä½“éªŒå¹¶ä¿ƒè¿›äº†è¡Œä¸šçš„æŠ€æœ¯è¿›æ­¥å’Œåˆ›æ–°å‘å±•è¿›ä¸€æ­¥æ¨åŠ¨äº†è¡Œä¸šçš„æŠ€æœ¯è¿­ä»£å’Œè¡Œä¸šéœ€æ±‚çš„å¯æŒç»­æ€§å‘å±•çš„è¡Œä¸šä½¿ç”¨å’Œæ¨å¹¿å…·æœ‰ç§¯æçš„ä»·å€¼å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯å……åˆ†å±•ç°äº†äººå·¥æ™ºèƒ½çš„å¼ºå¤§é­…åŠ›å’Œå®ç”¨ä»·å€¼å¹¶è¿›ä¸€æ­¥æé«˜äº†äººç±»çš„ç”Ÿæ´»å’Œå·¥ä½œè´¨é‡åŠ å¿«æŠ€æœ¯è¿›æ­¥ä»¥é€‚åº”æ•°å­—åŒ–æ—¶ä»£çš„å‘å±•éœ€æ±‚å’Œå®é™…å·¥ä½œçš„ç°å®è¦æ±‚å’Œåˆ›æ–°å‘å±•çš„æ½œåœ¨è¶‹åŠ¿æœ‰åŠ©äºè¿›ä¸€æ­¥æ¨åŠ¨æ•°å­—åŒ–æ—¶ä»£çš„å‘å±•æ¨åŠ¨æŠ€æœ¯åˆ›æ–°ä¸æ–­æ»¡è¶³è¡Œä¸šçš„å¤šå…ƒåŒ–éœ€æ±‚ä»è€ŒåŠ å¿«æ™ºèƒ½åŒ–ç¤¾ä¼šçš„å‘å±•è¿›ç¨‹ä»¥åŠæ•´ä¸ªç¤¾ä¼šçš„ç§‘æŠ€è¿›æ­¥æ°´å¹³å¹¶ä¸ºæœªæ¥çš„å‘å±•å¸¦æ¥æ›´å¤§çš„å½±å“åŠ›å’Œè´¡çŒ®åŠ›æ¨åŠ¨äº†ç§‘æŠ€çš„è¿›æ­¥å’Œåˆ›æ–°ä¸ºè¡Œä¸šçš„æœªæ¥å‘å±•æä¾›äº†å¼ºå¤§çš„æŠ€æœ¯æ”¯æŒå¹¶å¸¦æ¥äº†é‡è¦çš„ä»·å€¼å’Œåº”ç”¨å‰æ™¯å’Œå¹¿é˜”çš„æœªæ¥å‘å±•è¶‹åŠ¿å¹¶å±•ç¤ºäº†æ— é™æ•…äº‹æ¡†æ¶åœ¨æ•°å­—åŒ–æ—¶ä»£çš„é‡è¦æ€§å’Œå½±å“åŠ›ä»¥åŠå…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œåº”ç”¨ä»·å€¼åœ¨ç§‘æŠ€é¢†åŸŸçš„å¹¿æ³›åº”ç”¨å‰æ™¯å’Œæœªæ¥çš„å‘å±•è¶‹åŠ¿ä»¥åŠå…¶å¯¹äºæ•°å­—åŒ–æ—¶ä»£çš„é‡è¦è´¡çŒ®å’Œå½±å“åŠ›å’Œæœªæ¥çš„å¹¿é˜”å‘å±•å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a7de20c428568bb44a5244e10b0cd8e" align="middle">
<img src="https://picx.zhimg.com/v2-34495cf12d4a27c4f74a97fb7ef405a3" align="middle">
<img src="https://picx.zhimg.com/v2-c2c715f4700c0b2a3bcb4aa616e04295" align="middle">
<img src="https://picx.zhimg.com/v2-649edf44e85588c504c9f95f07d1ad33" align="middle">
<img src="https://picx.zhimg.com/v2-e2e5d193b335db3b11fe753b621eb75c" align="middle">
<img src="https://picx.zhimg.com/v2-a52332db0c01dfff94577547803cfee4" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation"><a href="#Medal-S-Spatio-Textual-Prompt-Model-for-Medical-Segmentation" class="headerlink" title="Medal S: Spatio-Textual Prompt Model for Medical Segmentation"></a>Medal S: Spatio-Textual Prompt Model for Medical Segmentation</h2><p><strong>Authors:Pengcheng Shi, Jiawei Chen, Jiaqi Liu, Xinglin Zhang, Tao Chen, Lei Li</strong></p>
<p>We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/yinghemedical/Medal-S">https://github.com/yinghemedical/Medal-S</a>.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Medal Sï¼Œè¿™æ˜¯ä¸€ç§åŒ»å­¦åˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œå®ƒåœ¨ä¸€ä¸ªç«¯åˆ°ç«¯å¯è®­ç»ƒæ¡†æ¶å†…æ”¯æŒåŸç”Ÿåˆ†è¾¨ç‡çš„ç©ºé—´å’Œæ–‡æœ¬æç¤ºã€‚ä¸ç¼ºä¹ç©ºé—´æ„è¯†çš„çº¯æ–‡æœ¬æ–¹æ³•ä¸åŒï¼ŒMedal Så®ç°äº†ä½“ç§¯æç¤ºå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„é€šé“å¯¹é½ï¼Œç¼“è§£äº†åˆ†è¾¨ç‡ä¸åŒ¹é…å¸¦æ¥çš„ä¸å‡†ç¡®é—®é¢˜ã€‚é€šè¿‡ä¿ç•™å®Œæ•´çš„3Dä¸Šä¸‹æ–‡ï¼Œå®ƒèƒ½å¤Ÿå¹¶è¡Œå¤„ç†å¤šä¸ªåŸç”Ÿåˆ†è¾¨ç‡çš„æ©è†œï¼Œæé«˜å¤šç±»åˆ†å‰²æ€§èƒ½ã€‚ä¸€ä¸ªè½»é‡çº§çš„3Då·ç§¯æ¨¡å—èƒ½å¤Ÿå®ç°ç”±ä¸¤ç§æç¤ºç±»å‹å¼•å¯¼çš„ç²¾ç¡®ä½“ç´ ç©ºé—´ç»†åŒ–ï¼Œæ”¯æŒBiomedSegFMæ•°æ®é›†ä¸­çš„CTã€MRIã€PETã€è¶…å£°å’Œæ˜¾å¾®é•œæ¨¡å¼çš„æœ€å¤š243ç±»ã€‚Medal Sæä¾›ä¸¤ç§æç¤ºæ¨¡å¼ï¼šçº¯æ–‡æœ¬æ¨¡å¼ï¼Œå…¶ä¸­æ¨¡å‹é¢„æµ‹ä½œä¸ºç©ºé—´æç¤ºè¿›è¡Œè‡ªæˆ‘ç»†åŒ–è€Œæ— éœ€äººå·¥è¾“å…¥ï¼›ä»¥åŠæ··åˆæ¨¡å¼ï¼Œç»“åˆæ‰‹åŠ¨æ³¨é‡Šä»¥å¢å¼ºçµæ´»æ€§ã€‚å¯¹äº24ç±»åˆ†å‰²ï¼Œå¹¶è¡Œç©ºé—´æç¤ºä½¿æ¨ç†æ—¶é—´æ¯”é¡ºåºæç¤ºå‡å°‘äº†90%ä»¥ä¸Šã€‚ä¸ºäº†è§£å†³ç›®æ ‡è¡¥ä¸æ¯”ä¾‹å¤±è¡¡çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€é‡é‡‡æ ·ï¼Œæ‰©å±•äº†SATå’ŒnnU-Netç”¨äºæ•°æ®å¢å¼ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¼˜åŒ–çš„æ–‡æœ¬é¢„å¤„ç†ã€ä¸¤é˜¶æ®µæ¨ç†ç­–ç•¥å’Œåå¤„ç†æŠ€å·§ï¼Œä»¥æé«˜å†…å­˜æ•ˆç‡ã€ç²¾åº¦å’Œæ¨ç†é€Ÿåº¦ã€‚åœ¨éªŒè¯é›†ä¸Šçš„äº”ç§æ¨¡å¼å¹³å‡è¡¨ç°ä¸­ï¼ŒMedal Såœ¨DSCã€NSDã€F1å’ŒDSC TPç­‰å„é¡¹æŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºSATã€‚å…·ä½“è€Œè¨€ï¼ŒDSCä¸º75.44ï¼ˆå¯¹æ¯”SATä¸º69.83ï¼‰ï¼ŒNSDä¸º77.34ï¼ˆå¯¹æ¯”SATä¸º71.06ï¼‰ï¼ŒF1ä¸º38.24ï¼ˆå¯¹æ¯”SATä¸º24.88ï¼‰ï¼ŒDSC TPä¸º65.46ï¼ˆå¯¹æ¯”SATä¸º46.97ï¼‰ã€‚Medal Sé€šè¿‡ç©ºé—´ç²¾åº¦ä¸è¯­ä¹‰æ–‡æœ¬æŒ‡å¯¼çš„å’Œè°èåˆï¼Œå®ç°äº†åœ¨å¤šç±»åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºäºé¡ºåºæç¤ºçš„æ–¹æ³•çš„ä¼˜è¶Šæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚Medal Så°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/yinghemedical/Medal-S%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/yinghemedical/Medal-Så…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13001v1">PDF</a> Accepted by CVPR 2025 Workshop MedSegFM</p>
<p><strong>Summary</strong><br>     åŒ»ç–—åˆ†å‰²æ¨¡å‹Medal Sæ”¯æŒç«¯åˆ°ç«¯çš„è®­ç»ƒæ¡†æ¶å†…çš„åŸç”Ÿåˆ†è¾¨ç‡ç©ºé—´æ–‡æœ¬æç¤ºã€‚ä¸å…¶ä»–ä»…ä½¿ç”¨æ–‡æœ¬çš„æ–¹æ³•ä¸åŒï¼ŒMedal Så®ç°äº†ä½“ç§¯æç¤ºä¸æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„é€šé“å¯¹é½ï¼Œå‡å°‘äº†åˆ†è¾¨ç‡ä¸åŒ¹é…å¯¼è‡´çš„è¯¯å·®ã€‚å®ƒé€šè¿‡ä¿ç•™å®Œæ•´çš„3Dä¸Šä¸‹æ–‡ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†å¤šä¸ªåŸç”Ÿåˆ†è¾¨ç‡æ©ç ï¼Œæé«˜å¤šç±»åˆ«åˆ†å‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒMedal Sæä¾›ä¸¤ç§æç¤ºæ¨¡å¼ï¼ŒåŒ…æ‹¬æ— éœ€äººå·¥è¾“å…¥çš„æ–‡æœ¬æ¨¡å¼ä»¥åŠç»“åˆæ‰‹åŠ¨æ³¨é‡Šçš„æ··åˆæ¨¡å¼ã€‚åœ¨äº”ä¸ªæ¨¡æ€çš„å¹³å‡éªŒè¯é›†ä¸Šï¼ŒMedal Sä¼˜äºSATçš„åˆ†å‰²ç»“æœã€‚å¥–ç‰ŒSå…¬å¼€ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/yinghemedical/Medal-S">é“¾æ¥åœ°å€</a>ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒMedal Sç»“åˆç©ºé—´ç²¾ç¡®æ€§å’Œè¯­ä¹‰æ–‡æœ¬æŒ‡å¯¼å®ç°åŒ»ç–—å›¾åƒåˆ†å‰²çš„é«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Medal Sæ˜¯ä¸€ä¸ªæ”¯æŒåŸç”Ÿåˆ†è¾¨ç‡ç©ºé—´æ–‡æœ¬æç¤ºçš„åŒ»ç–—åˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å®ç°ä½“ç§¯æç¤ºä¸æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„é€šé“å¯¹é½ï¼Œæé«˜äº†å¤šç±»åˆ«åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿå¹¶è¡Œå¤„ç†å¤šä¸ªåŸç”Ÿåˆ†è¾¨ç‡æ©ç ï¼Œä¿ç•™å®Œæ•´çš„3Dä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>Medal Sæä¾›ä¸¤ç§æç¤ºæ¨¡å¼ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ¨¡å¼å’Œæ··åˆæ¨¡å¼ï¼Œä»¥é€‚åº”ä¸åŒéœ€æ±‚ã€‚</li>
<li>åœ¨äº”ä¸ªæ¨¡æ€çš„å¹³å‡éªŒè¯é›†ä¸Šï¼ŒMedal Såœ¨åˆ†å‰²æ€§èƒ½ä¸Šä¼˜äºSATã€‚</li>
<li>åŠ¨æ€é‡é‡‡æ ·æŠ€æœ¯è§£å†³äº†ç›®æ ‡æ–‘å—æ¯”ä¾‹å¤±è¡¡çš„é—®é¢˜ï¼Œæé«˜äº†æ•°æ®å¢å¼ºçš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-091efef46acdb7b5a9093751fd422f37" align="middle">
<img src="https://picx.zhimg.com/v2-87d0c2e8741263fe14388f14b3995808" align="middle">
<img src="https://picx.zhimg.com/v2-3c3d70c3cc4662fe134db9a7ed6fafac" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ArtiWorld-LLM-Driven-Articulation-of-3D-Objects-in-Scenes"><a href="#ArtiWorld-LLM-Driven-Articulation-of-3D-Objects-in-Scenes" class="headerlink" title="ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes"></a>ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes</h2><p><strong>Authors:Yixuan Yang, Luyang Xie, Zhen Luo, Zixiang Zhao, Mingqi Gao, Feng Zheng</strong></p>
<p>Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.</p>
<blockquote>
<p>æ„å»ºäº¤äº’å¼æ¨¡æ‹Ÿå™¨å’Œå¯æ‰©å±•çš„æœºå™¨äººå­¦ä¹ ç¯å¢ƒéœ€è¦å¤§é‡çš„å…³èŠ‚å¼èµ„äº§ã€‚ç„¶è€Œï¼Œæ¨¡æ‹Ÿä¸­ç°æœ‰çš„å¤§å¤šæ•°3Dèµ„äº§éƒ½æ˜¯åˆšæ€§çš„ï¼Œæ‰‹åŠ¨å°†å®ƒä»¬è½¬æ¢ä¸ºå…³èŠ‚å¼ç‰©ä½“éå¸¸è€—è´¹åŠ³åŠ¨åŠ›å’Œæˆæœ¬ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦è‡ªåŠ¨è¯†åˆ«åœºæ™¯ä¸­çš„å¯åŠ¨å…³èŠ‚ç‰©ä½“å¹¶å°†å…¶ç›´æ¥è½¬æ¢ä¸ºå…³èŠ‚å¼èµ„äº§ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ArtiWorldï¼Œè¿™æ˜¯ä¸€ä¸ªåœºæ™¯æ„ŸçŸ¥çš„ç®¡é“ï¼Œå¯ä»¥ä»æ–‡æœ¬åœºæ™¯æè¿°ä¸­å®šä½å€™é€‰çš„å¯åŠ¨å…³èŠ‚ç‰©ä½“ï¼Œå¹¶é‡å»ºå¯æ‰§è¡Œçš„URDFæ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å‡ ä½•å½¢çŠ¶ã€‚è¯¥ç®¡é“çš„æ ¸å¿ƒæ˜¯Arti4URDFï¼Œå®ƒåˆ©ç”¨3Dç‚¹äº‘ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åŠé¢å‘URDFçš„æç¤ºè®¾è®¡ï¼Œå¯ä»¥å¿«é€Ÿå°†åˆšä½“è½¬æ¢ä¸ºåŸºäºURDFçš„äº¤äº’å¼å¯åŠ¨å…³èŠ‚ç‰©ä½“ï¼ŒåŒæ—¶ä¿æŒå…¶3Då½¢çŠ¶ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå±‚æ¬¡ä¸Šè¯„ä¼°ArtiWorldï¼š3Dæ¨¡æ‹Ÿç‰©ä½“ã€å®Œæ•´çš„3Dæ¨¡æ‹Ÿåœºæ™¯å’ŒçœŸå®ä¸–ç•Œæ‰«æåœºæ™¯ã€‚åœ¨æ‰€æœ‰è¿™ä¸‰ä¸ªè®¾ç½®ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶å¹¶æ­£ç¡®æ•æ‰äº†ç‰©ä½“çš„äº¤äº’æ€§ï¼Œä»è€Œäº§ç”Ÿå¯ç”¨çš„åŸºäºURDFçš„å¯åŠ¨å…³èŠ‚æ¨¡å‹ã€‚è¿™ä¸ºç›´æ¥ä»ç°æœ‰3Dèµ„äº§æ„å»ºäº¤äº’å¼ã€é€‚ç”¨äºæœºå™¨äººçš„æ¨¡æ‹Ÿç¯å¢ƒæä¾›äº†åˆ‡å®å¯è¡Œçš„é€”å¾„ã€‚ä»£ç å’Œæ•°æ®å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12977v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºArtiWorldçš„åœºæ™¯æ„ŸçŸ¥ç®¡é“ï¼Œå¯ä»æ–‡æœ¬åœºæ™¯æè¿°ä¸­å®šä½å¯åŠ¨å¯¹è±¡å¹¶é‡å»ºå¯æ‰§è¡ŒURDFæ¨¡å‹ï¼Œå®ç°äº¤äº’å¼æ¨¡æ‹Ÿå™¨çš„å»ºè®¾ã€‚æ ¸å¿ƒç»„ä»¶Arti4URDFå¯ç»“åˆ3Dç‚¹äº‘ã€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’ŒURDFå¯¼å‘æç¤ºè®¾è®¡ï¼Œå°†åˆšä½“å¿«é€Ÿè½¬æ¢ä¸ºäº¤äº’å¼URDFåŸºå…³èŠ‚å¯¹è±¡ï¼ŒåŒæ—¶ä¿æŒå…¶ä¸‰ç»´å½¢çŠ¶ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå¯¹è±¡å’Œåœºæ™¯ä»¥åŠçœŸå®ä¸–ç•Œæ‰«æåœºæ™¯ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•å¹¶è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚è¿™ä¸ºç›´æ¥ä»ç°æœ‰ä¸‰ç»´èµ„äº§æ„å»ºäº¤äº’å¼æœºå™¨äººæ¨¡æ‹Ÿç¯å¢ƒæä¾›äº†å®ç”¨é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ArtiWorldèƒ½ä»æ–‡æœ¬åœºæ™¯æè¿°ä¸­è‡ªåŠ¨è¯†åˆ«å’Œè½¬æ¢å¯åŠ¨å¯¹è±¡ï¼Œæ„å»ºäº¤äº’å¼æ¨¡æ‹Ÿç¯å¢ƒã€‚</li>
<li>Arti4URDFæ˜¯è¯¥ç®¡é“çš„æ ¸å¿ƒç»„ä»¶ï¼Œèƒ½ç»“åˆå¤šç§æŠ€æœ¯å°†åˆšä½“è½¬æ¢ä¸ºäº¤äº’å¼URDFåŸºå…³èŠ‚å¯¹è±¡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒå¯¹è±¡å‡ ä½•å½¢çŠ¶çš„åŒæ—¶ï¼Œæ­£ç¡®æ•æ‰å¯¹è±¡äº¤äº’æ€§ï¼Œç”Ÿæˆå¯ç”¨çš„URDFåŸºå…³èŠ‚æ¨¡å‹ã€‚</li>
<li>ArtiWorldåœ¨æ¨¡æ‹Ÿå¯¹è±¡å’Œåœºæ™¯ä»¥åŠçœŸå®ä¸–ç•Œæ‰«æåœºæ™¯ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæœºå™¨äººæ¨¡æ‹Ÿç¯å¢ƒçš„æ„å»ºæä¾›äº†å®ç”¨é€”å¾„ï¼Œç‰¹åˆ«æ˜¯ä»ç°æœ‰çš„ä¸‰ç»´èµ„äº§å‡ºå‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90fd830ecd9ce744c8908bc509815f87" align="middle">
<img src="https://picx.zhimg.com/v2-726f303d7c3e8df2a16c74c7d83cf8a7" align="middle">
<img src="https://picx.zhimg.com/v2-dab68817bb8c38ad6194a0ae79946814" align="middle">
<img src="https://picx.zhimg.com/v2-ed5daa386342cbd2bd6af79dfad8a1b8" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PFAvatar-Pose-Fusion-3D-Personalized-Avatar-Reconstruction-from-Real-World-Outfit-of-the-Day-Photos"><a href="#PFAvatar-Pose-Fusion-3D-Personalized-Avatar-Reconstruction-from-Real-World-Outfit-of-the-Day-Photos" class="headerlink" title="PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos"></a>PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos</h2><p><strong>Authors:Dianbing Xi, Guoyuan An, Jingsen Zhu, Zhijian Liu, Yuan Liu, Ruiyuan Zhang, Jiayuan Lu, Rui Wang, Yuchi Huo</strong></p>
<p>We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from &#96;&#96;Outfit of the Dayâ€™â€™ (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions&#x2F;truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†PFAvatarï¼ˆå§¿æ€èåˆåŒ–èº«ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥ä»â€œæ—¥å¸¸ç©¿æ­â€ï¼ˆOOTDï¼‰ç…§ç‰‡é‡å»ºé«˜è´¨é‡3DåŒ–èº«ã€‚è¿™äº›ç…§ç‰‡å±•ç°äº†å¤šç§å§¿æ€ã€é®æŒ¡å’Œå¤æ‚èƒŒæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰ç”¨å°‘é‡OOTDç¤ºä¾‹å¯¹å§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›ï¼ˆ2ï¼‰é€šè¿‡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¡¨ç¤º3DåŒ–èº«å¹¶è¿›è¡Œæç‚¼ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä¸ä»¥å¾€å°†å›¾åƒåˆ†å‰²æˆèµ„äº§ï¼ˆå¦‚æœè£…ã€é…é¥°ï¼‰è¿›è¡Œ3Dç»„è£…çš„æ–¹æ³•ä¸åŒï¼Œè¿™ç§æ–¹æ³•å®¹æ˜“å¯¼è‡´ä¸ä¸€è‡´æ€§ã€‚æˆ‘ä»¬é¿å…åˆ†è§£ï¼Œç›´æ¥å¯¹å…¨èº«å¤–è§‚è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡é›†æˆé¢„å…ˆè®­ç»ƒçš„ControlNetè¿›è¡Œå§¿æ€ä¼°è®¡å’Œæ–°é¢–çš„æ¡ä»¶å…ˆéªŒä¿ç•™æŸå¤±ï¼ˆCPPLï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç«¯åˆ°ç«¯å­¦ä¹ ä¸­ç²¾ç»†ç»†èŠ‚ï¼ŒåŒæ—¶å‡è½»å°‘é‡è®­ç»ƒä¸­çš„è¯­è¨€æ¼‚ç§»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»…5åˆ†é’Ÿå†…å®Œæˆä¸ªæ€§åŒ–è®¾ç½®ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”å®ç°äº†48å€çš„é€Ÿåº¦æå‡ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºNeRFçš„åŒ–èº«è¡¨ç¤ºï¼Œé€šè¿‡è§„èŒƒçš„SMPL-Xç©ºé—´é‡‡æ ·å’Œå¤šåˆ†è¾¨ç‡3D-SDSè¿›è¡Œä¼˜åŒ–ã€‚ä¸åŸºäºç½‘æ ¼çš„è¡¨ç¤ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è¿ç»­è¾å°„åœºå¯ä»¥ä¿ç•™é«˜é¢‘çº¹ç†ï¼ˆä¾‹å¦‚å¤´å‘ï¼‰ï¼Œå¹¶é€šè¿‡é€å°„ç‡æ­£ç¡®åœ°å¤„ç†é®æŒ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒPFAvataråœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™ä»¥åŠé®æŒ¡&#x2F;æˆªæ–­é²æ£’æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œæ¨åŠ¨äº†ä»ç°å®ä¸–ç•ŒOOTDç›¸å†Œç”Ÿæˆå®ç”¨3DåŒ–èº«çš„å‘å±•ã€‚æ­¤å¤–ï¼Œé‡å»ºçš„3DåŒ–èº«æ”¯æŒä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚è™šæ‹Ÿè¯•ç©¿ã€åŠ¨ç”»å’Œäººç±»è§†é¢‘é‡æ–°æ¼”ç»ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„é€šç”¨æ€§å’Œå®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12935v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPFAvatarçš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å¤šæ ·åŒ–çš„å§¿åŠ¿ã€é®æŒ¡å’Œå¤æ‚èƒŒæ™¯çš„â€œæ¯æ—¥ç©¿æ­â€ï¼ˆOOTDï¼‰ç…§ç‰‡ä¸­é‡å»ºé«˜è´¨é‡çš„ä¸‰ç»´è™šæ‹Ÿè§’è‰²ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šä¸€æ˜¯å¾®è°ƒåŸºäºå°‘é‡OOTDä¾‹å­çš„å§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼ŒäºŒæ˜¯è’¸é¦ä»¥ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¡¨ç¤ºçš„ä¸‰ç»´è™šæ‹Ÿè§’è‰²ã€‚æ–°æ–¹æ³•é¿å…äº†ä»¥å¾€æ–¹æ³•çš„åˆ†è§£ç¼ºç‚¹ï¼Œç›´æ¥å¯¹å…¨èº«å¤–è§‚è¿›è¡Œå»ºæ¨¡ï¼Œèƒ½å¤Ÿç«¯åˆ°ç«¯åœ°å­¦ä¹ ç²¾ç»†ç»†èŠ‚å¹¶ç¼“è§£å°‘é‡è®­ç»ƒæ—¶çš„è¯­è¨€æ¼‚ç§»é—®é¢˜ã€‚åŒæ—¶å¼•å…¥NeRFåŸºäºçš„è™šæ‹Ÿè§’è‰²è¡¨ç¤ºæ³•ï¼Œé€šè¿‡æ ‡å‡†SMPL-Xç©ºé—´é‡‡æ ·å’Œå¤šåˆ†è¾¨ç‡3D-SDSè¿›è¡Œä¼˜åŒ–ï¼Œèƒ½å¤Ÿä¿ç•™é«˜é¢‘çº¹ç†å¹¶æ­£ç¡®å¤„ç†é®æŒ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒPFAvataråœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™ä»¥åŠé®æŒ¡&#x2F;æˆªæ–­ç¨³å¥æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæ¨åŠ¨äº†å®é™…åº”ç”¨çš„OOTDä¸“è¾‘çš„ä¸‰ç»´è™šæ‹Ÿè§’è‰²ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PFAvatarèƒ½å¤Ÿä»OOTDç…§ç‰‡ä¸­é‡å»ºé«˜è´¨é‡çš„ä¸‰ç»´è™šæ‹Ÿè§’è‰²ï¼Œå¤„ç†å¤šæ ·åŒ–å§¿åŠ¿ã€é®æŒ¡å’Œå¤æ‚èƒŒæ™¯ã€‚</li>
<li>æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¾®è°ƒå§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹å’Œè’¸é¦ä»¥NeRFè¡¨ç¤ºçš„ä¸‰ç»´è™šæ‹Ÿè§’è‰²ã€‚</li>
<li>é¿å…åˆ†è§£å»ºæ¨¡ï¼Œç›´æ¥å¯¹å…¨èº«å¤–è§‚è¿›è¡Œå»ºæ¨¡ï¼Œèƒ½å¤Ÿç«¯åˆ°ç«¯åœ°å­¦ä¹ ç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥Condition Prior Preservation Lossï¼ˆCPPLï¼‰ç¼“è§£è¯­è¨€æ¼‚ç§»é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨NeRFåŸºè™šæ‹Ÿè§’è‰²è¡¨ç¤ºæ³•ï¼Œèƒ½å¤Ÿä¿ç•™é«˜é¢‘çº¹ç†å¹¶æ­£ç¡®å¤„ç†é®æŒ¡ã€‚</li>
<li>PFAvataråœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™å’Œé®æŒ¡ç¨³å¥æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7babe9aa8f4e76891007ba85aec4735" align="middle">
<img src="https://picx.zhimg.com/v2-f69f246eef6a79116dd8a1c3c0437772" align="middle">
<img src="https://picx.zhimg.com/v2-dc4cce51cb10c3588fd127745a8e1a41" align="middle">
<img src="https://picx.zhimg.com/v2-5c8c04429049eeef36d71747b460ea83" align="middle">
<img src="https://picx.zhimg.com/v2-79f18b03f7c81325d79e755ba0fe6749" align="middle">
<img src="https://picx.zhimg.com/v2-87c8e6052af98dbaef6dec80e7c738af" align="middle">
<img src="https://picx.zhimg.com/v2-a8d8f82dc104053190c27ed1c2c01c14" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Prompt-Driven-Domain-Adaptation-for-End-to-End-Autonomous-Driving-via-In-Context-RL"><a href="#Prompt-Driven-Domain-Adaptation-for-End-to-End-Autonomous-Driving-via-In-Context-RL" class="headerlink" title="Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL"></a>Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL</h2><p><strong>Authors:Aleesha Khurram, Amir Moeini, Shangtong Zhang, Rohan Chandra</strong></p>
<p>Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.</p>
<blockquote>
<p>å°½ç®¡è‡ªåŠ¨é©¾é©¶é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•å’Œçªç ´ï¼Œä½†è®¸å¤šç«¯åˆ°ç«¯ç³»ç»Ÿä»ç„¶é¢ä¸´åŸŸé€‚åº”ï¼ˆDAï¼‰çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚å°†æ¸…æ™°å¤©æ°”ä¸‹è®­ç»ƒçš„ç­–ç•¥è½¬ç§»åˆ°æ¶åŠ£å¤©æ°”æ¡ä»¶ã€‚æ–‡çŒ®ä¸­çš„å…¸å‹DAç­–ç•¥åŒ…æ‹¬åœ¨ç›®æ ‡åŸŸä¸­æ”¶é›†é¢å¤–æ•°æ®ã€é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–ä¸¤è€…éƒ½è¿›è¡Œã€‚éšç€é©¾é©¶è§„æ¨¡å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œè¿™äº›ç­–ç•¥å¾ˆå¿«å°±ä¼šå˜å¾—ä¸åˆ‡å®é™…ã€‚è¿™äº›å±€é™æ€§ä¿ƒä½¿äººä»¬å¼€å§‹åœ¨æ¨ç†æ—¶é—´æ¢ç´¢å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬æç¤ºé©±åŠ¨DAçš„ç ”ç©¶ï¼Œæ¶‰åŠLLMså’ŒVLMsã€‚è¿™äº›æ–¹æ³•é€šè¿‡æ¨ç†è¿‡ç¨‹ä¸­æ·»åŠ å°‘æ•°çŠ¶æ€åŠ¨ä½œè½¨è¿¹åˆ°æç¤ºä¸­ï¼ˆç±»ä¼¼äºä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰æ¥å·¥ä½œã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æœ‰ä¸¤ä¸ªå±€é™æ€§ï¼šä¸€æ˜¯æç¤ºé©±åŠ¨DAæ–¹æ³•ç›®å‰ä»…é™äºæ„ŸçŸ¥ä»»åŠ¡ï¼Œå¦‚æ£€æµ‹å’Œåˆ†å‰²ï¼›äºŒæ˜¯å®ƒä»¬éœ€è¦ä¸“å®¶å°‘æ ·æœ¬æ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼ˆICRLï¼‰åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹è¿›è¡Œé—­ç¯è‡ªåŠ¨é©¾é©¶çš„æ¨ç†æ—¶é—´å°‘æ ·æœ¬æç¤ºé©±åŠ¨DAã€‚ä¸å…¶ä»–æç¤ºé©±åŠ¨DAæ–¹æ³•ç±»ä¼¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä¹Ÿä¸éœ€è¦åœ¨å¯¹æŠ—å¤©æ°”æƒ…å†µä¸‹æ”¶é›†é¢å¤–æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ‰©å±•åˆ°ä½¿ç”¨æ¨ç†è¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„é€šç”¨è½¨è¿¹æ¥é©¾é©¶ï¼Œä»è€Œæé«˜äº†æç¤ºé©±åŠ¨DAçš„æœ€æ–°æ°´å¹³ã€‚åœ¨CARLAæ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°çš„æç¤ºé©±åŠ¨DAåŸºçº¿ç›¸æ¯”ï¼ŒICRLåœ¨ç›®æ ‡é¢†åŸŸå¯¼è‡´äº†æ›´å®‰å…¨ã€æ›´é«˜æ•ˆã€æ›´èˆ’é€‚çš„é©¾é©¶ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12755v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå°½ç®¡æœ‰æ˜¾è‘—çš„è¿›æ­¥å’Œçªç ´ï¼Œä½†ç«¯åˆ°ç«¯ç³»ç»Ÿåœ¨åŸŸé€‚åº”ï¼ˆDAï¼‰æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå¼ºåŒ–å­¦ä¹ å’Œè½¨è¿¹æ•°æ®çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå³æ¨ç†æ—¶é—´å°‘æç¤ºé©±åŠ¨çš„åŸŸé€‚åº”æ–¹æ³•ï¼ˆICRLï¼‰ã€‚è¿™ç§æ–¹æ³•æ—¢ä¸éœ€è¦æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä¹Ÿä¸éœ€è¦æ”¶é›†é¢å¤–çš„æ•°æ®æ¥åº”å¯¹æ¶åŠ£å¤©æ°”æ¡ä»¶ï¼Œä½¿é©¾é©¶æ›´å®‰å…¨ã€é«˜æ•ˆå’Œèˆ’é€‚ã€‚ç›¸å…³å®éªŒä¹Ÿè¯å®äº†è¿™ä¸€æ–¹æ³•çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„å…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>è‡ªåŠ¨é©¾é©¶åœ¨åŸŸé€‚åº”ï¼ˆDAï¼‰æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„é©¾é©¶ç­–ç•¥è½¬ç§»ã€‚</li>
<li>å½“å‰å…¸å‹çš„åŸŸé€‚åº”ç­–ç•¥åŒ…æ‹¬åœ¨ç›®æ ‡åŸŸæ”¶é›†é¢å¤–æ•°æ®æˆ–é‡æ–°è®­ç»ƒæ¨¡å‹ç­‰ï¼Œä½†éšç€é©¾é©¶è§„æ¨¡å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œè¿™äº›æ–¹æ³•å˜å¾—ä¸åˆ‡å®é™…ã€‚</li>
<li>åŸºäºå°‘æç¤ºé©±åŠ¨çš„åŸŸé€‚åº”æ–¹æ³•ï¼ˆICRLï¼‰åœ¨æ¨ç†æ—¶é—´å…·æœ‰æ½œåŠ›ï¼Œé€šè¿‡æ·»åŠ å°‘é‡çŠ¶æ€åŠ¨ä½œè½¨è¿¹åˆ°æç¤ºä¸­è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä»…é™äºæ„ŸçŸ¥ä»»åŠ¡ï¼Œå¹¶éœ€è¦ä¸“å®¶æ•°æ®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-057f8898e3bf45dbb2e74a99769c2a8a" align="middle">
<img src="https://picx.zhimg.com/v2-920e0052e9a6b8ae4e8514185949dec8" align="middle">
<img src="https://picx.zhimg.com/v2-8c99d4687d95b9ae2caa7162ad135c7d" align="middle">
<img src="https://picx.zhimg.com/v2-b314fef86b52cc397928159bb640b7e3" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Self-Supervised-Visual-Prompting-for-Cross-Domain-Road-Damage-Detection"><a href="#Self-Supervised-Visual-Prompting-for-Cross-Domain-Road-Damage-Detection" class="headerlink" title="Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection"></a>Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection</h2><p><strong>Authors:Xi Xiao, Zhuxuanzi Wang, Mingqiao Mo, Chen Liu, Chenrui Ma, Yanshu Li, Smita Krishnaswamy, Xiao Wang, Tianyang Wang</strong></p>
<p>The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: <a target="_blank" rel="noopener" href="https://github.com/xixiaouab/PROBE/tree/main">https://github.com/xixiaouab/PROBE/tree/main</a></p>
<blockquote>
<p>è‡ªåŠ¨åŒ–è·¯é¢ç¼ºé™·æ£€æµ‹ç³»ç»Ÿçš„éƒ¨ç½²ç»å¸¸å—åˆ°è·¨åŸŸæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é˜»ç¢ã€‚ç›‘ç£æ£€æµ‹å™¨åœ¨ç‰¹å®šé¢†åŸŸå†…éƒ¨ç½²æ—¶èƒ½å–å¾—å‡†ç¡®çš„æ£€æµ‹æ•ˆæœï¼Œä½†åœ¨æ–°çš„ç¯å¢ƒä¸‹éƒ¨ç½²éœ€è¦æ˜‚è´µçš„é‡æ–°æ ‡æ³¨è¿‡ç¨‹ã€‚æ ‡å‡†çš„è‡ªæˆ‘ç›‘ç£æ–¹æ³•è™½ç„¶èƒ½æ•æ‰åˆ°é€šç”¨ç‰¹å¾ï¼Œä½†ä»å®¹æ˜“å—åˆ°é¢†åŸŸå˜åŒ–çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªæˆ‘ç›‘ç£æ¡†æ¶â€”â€”oursï¼Œå®ƒæ— éœ€æ ‡ç­¾å°±èƒ½å¯¹ç›®æ ‡é¢†åŸŸè¿›è¡Œè§†è§‰æ¢æµ‹ã€‚ourså¼•å…¥äº†ä¸€ä¸ªè‡ªæˆ‘ç›‘ç£æç¤ºå¢å¼ºæ¨¡å—ï¼ˆSPEMï¼‰ï¼Œè¯¥æ¨¡å—ä»ç›®æ ‡æ•°æ®çš„æ— æ ‡ç­¾æ•°æ®ä¸­è¡ç”Ÿå‡ºç¼ºé™·æ„ŸçŸ¥æç¤ºï¼Œä»¥æŒ‡å¯¼å†»ç»“çš„ViTä¸»å¹²ç½‘ç»œï¼Œä»¥åŠä¸€ä¸ªé¢†åŸŸæ„ŸçŸ¥æç¤ºå¯¹é½ï¼ˆDAPAï¼‰ç›®æ ‡ï¼Œè¯¥ç›®æ ‡å°†æºæ•°æ®å’Œç›®æ ‡æ•°æ®çš„æç¤ºæ¡ä»¶è¡¨ç¤ºå¯¹é½ã€‚åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒoursæŒç»­ä¼˜äºå¼ºå¤§çš„ç›‘ç£ã€è‡ªæˆ‘ç›‘ç£å’Œé€‚åº”åŸºå‡†æµ‹è¯•ï¼Œå®ç°äº†ç¨³å¥çš„é›¶æ ·æœ¬è¿ç§»ã€å¯¹é¢†åŸŸå˜åŒ–çš„å¢å¼ºæŠµæŠ—åŠ›å’Œé«˜æ•ˆçš„æ•°æ®ä½¿ç”¨æ•ˆç‡ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†è‡ªæˆ‘ç›‘ç£æç¤ºä½œä¸ºä¸€ä¸ªå®ç”¨æ–¹å‘ï¼Œå¯ç”¨äºæ„å»ºå¯æ‰©å±•å’Œé€‚åº”æ€§å¼ºçš„è§†è§‰æ£€æµ‹ç³»ç»Ÿã€‚æºä»£ç å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/xixiaouab/PROBE/tree/main">https://github.com/xixiaouab/PROBE/tree/main</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12410v1">PDF</a> Accepted by WACV 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§è‡ªç›‘ç£çš„æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰æ¢æµ‹ç›®æ ‡åŸŸï¼Œæ— éœ€æ ‡ç­¾å³å¯å®ç°è·¯é¢ç¼ºé™·æ£€æµ‹çš„è‡ªåŠ¨åŒ–éƒ¨ç½²ã€‚è¯¥æ¡†æ¶å¼•å…¥è‡ªç›‘ç£æç¤ºå¢å¼ºæ¨¡å—ï¼ˆSPEMï¼‰ï¼Œä»æ— æ ‡ç­¾çš„ç›®æ ‡æ•°æ®ä¸­æ¨å¯¼å‡ºç¼ºé™·æ„ŸçŸ¥æç¤ºï¼Œå¼•å¯¼å†»ç»“çš„ViTä¸»å¹²ç½‘ç»œï¼›åŒæ—¶é‡‡ç”¨é¢†åŸŸæ„ŸçŸ¥æç¤ºå¯¹é½ï¼ˆDAPAï¼‰ç›®æ ‡ï¼Œå¯¹é½æç¤ºæ¡ä»¶ä¸‹çš„æºå’Œç›®æ ‡è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰ç¨³å¥çš„é›¶æ ·æœ¬è¿ç§»ã€è‰¯å¥½çš„é¢†åŸŸå˜åŒ–é€‚åº”æ€§å’Œé«˜æ•°æ®æ•ˆç‡çš„å°‘æ ·æœ¬é€‚åº”æ€§ã€‚è¿™è¯æ˜äº†è‡ªç›‘ç£æç¤ºæ–¹å‘å¯¹äºæ„å»ºå¯æ‰©å±•å’Œé€‚åº”æ€§å¼ºçš„è§†è§‰æ£€æµ‹ç³»ç»Ÿå…·æœ‰å®ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§è‡ªç›‘ç£æ¡†æ¶ç”¨äºè·¯é¢ç¼ºé™·æ£€æµ‹ï¼Œè¯¥æ¡†æ¶å¯åœ¨æ— éœ€æ ‡ç­¾çš„æƒ…å†µä¸‹æ¢æµ‹ç›®æ ‡åŸŸã€‚</li>
<li>å¼•å…¥è‡ªç›‘ç£æç¤ºå¢å¼ºæ¨¡å—ï¼ˆSPEMï¼‰ï¼Œä»æ— æ ‡ç­¾çš„ç›®æ ‡æ•°æ®ä¸­æ¨å¯¼å‡ºç¼ºé™·æ„ŸçŸ¥æç¤ºã€‚</li>
<li>é‡‡ç”¨é¢†åŸŸæ„ŸçŸ¥æç¤ºå¯¹é½ï¼ˆDAPAï¼‰ç›®æ ‡ï¼Œæé«˜æºå’Œç›®æ ‡åŸŸä¹‹é—´çš„æ•°æ®å¯¹é½ç¨‹åº¦ã€‚</li>
<li>åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬å¼ºç›‘ç£ã€è‡ªç›‘ç£å’Œè‡ªé€‚åº”æ–¹æ³•ã€‚</li>
<li>å®ç°ç¨³å¥çš„é›¶æ ·æœ¬è¿ç§»ã€è‰¯å¥½çš„é¢†åŸŸå˜åŒ–é€‚åº”æ€§å’Œé«˜æ•°æ®æ•ˆç‡çš„å°‘æ ·æœ¬é€‚åº”æ€§ã€‚</li>
<li>è¯æ˜äº†è‡ªç›‘ç£æç¤ºæ–¹å‘å¯¹äºæ„å»ºå¯æ‰©å±•å’Œé€‚åº”æ€§å¼ºçš„è§†è§‰æ£€æµ‹ç³»ç»Ÿå…·æœ‰å®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f023db5295ed0bf911f009d80171d91e" align="middle">
<img src="https://picx.zhimg.com/v2-efe79a5f16862c44d4c5981ca63a6f80" align="middle">
<img src="https://picx.zhimg.com/v2-ca26776796383b54608f5019e9c97571" align="middle">
<img src="https://picx.zhimg.com/v2-fbbb36348d80f0828fb36fa631fba4c0" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Bridging-Granularity-Gaps-Hierarchical-Semantic-Learning-for-Cross-domain-Few-shot-Segmentation"><a href="#Bridging-Granularity-Gaps-Hierarchical-Semantic-Learning-for-Cross-domain-Few-shot-Segmentation" class="headerlink" title="Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation"></a>Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation</h2><p><strong>Authors:Sujun Sun, Haowen Gu, Cheng Xie, Yanxu Ren, Mingwu Ren, Haofeng Zhang</strong></p>
<p>Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the modelâ€™s ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.</p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰çš„ç›®æ ‡æ˜¯å¯¹ç›®æ ‡åŸŸä¸­çš„æ–°ç±»åˆ«è¿›è¡Œåˆ†å‰²ï¼Œè¿™äº›ç±»åˆ«åœ¨è®­ç»ƒæ—¶æ²¡æœ‰æ¶‰åŠï¼Œå¹¶ä¸”ä¸æºåŸŸçš„æ•°æ®åˆ†å¸ƒæœ‰å¾ˆå¤§çš„ä¸åŒï¼Œä»…ä½¿ç”¨å°‘é‡çš„æ ‡æ³¨æ ·æœ¬ã€‚è¿‘å¹´æ¥ï¼Œæ­¤ä»»åŠ¡å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„CD-FSSæ–¹æ³•ä¸»è¦å…³æ³¨æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„é£æ ¼å·®å¼‚ï¼Œè€Œå¿½ç•¥äº†åˆ†å‰²ç²’åº¦å·®å¼‚ï¼Œå¯¼è‡´ç›®æ ‡åŸŸä¸­æ–°ç±»åˆ«çš„è¯­ä¹‰è¾¨åˆ«èƒ½åŠ›ä¸è¶³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚è¯­ä¹‰å­¦ä¹ ï¼ˆHSLï¼‰æ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒé£æ ¼éšæœºåŒ–ï¼ˆDSRï¼‰æ¨¡å—å’Œåˆ†å±‚è¯­ä¹‰æŒ–æ˜ï¼ˆHSMï¼‰æ¨¡å—æ¥å­¦ä¹ åˆ†å±‚è¯­ä¹‰ç‰¹å¾ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨ä¸åŒç²’åº¦ä¸Šè¯†åˆ«è¯­ä¹‰çš„èƒ½åŠ›ã€‚DSRé€šè¿‡å‰æ™¯å’Œå…¨å±€é£æ ¼éšæœºåŒ–ï¼Œåˆ†åˆ«æ¨¡æ‹Ÿå…·æœ‰ä¸åŒå‰æ™¯èƒŒæ™¯é£æ ¼å·®å¼‚å’Œæ•´ä½“é£æ ¼å˜åŒ–çš„ç›®æ ‡åŸŸæ•°æ®ã€‚HSMåˆ™åˆ©ç”¨å¤šå°ºåº¦è¶…åƒç´ æ¥æŒ‡å¯¼æ¨¡å‹åœ¨ä¸åŒç²’åº¦ä¸ŠæŒ–æ˜ç±»å†…ä¸€è‡´æ€§å’Œç±»é—´åŒºåˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åŸå‹ç½®ä¿¡åº¦è°ƒåˆ¶é˜ˆå€¼ï¼ˆPCMTï¼‰æ¨¡å—ï¼Œä»¥ç¼“è§£å‰æ™¯å’ŒèƒŒæ™¯è¿‡äºç›¸ä¼¼æ—¶çš„åˆ†å‰²æ¨¡ç³Šé—®é¢˜ã€‚åœ¨å››ä¸ªæµè¡Œçš„ç›®æ ‡åŸŸæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12200v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è·¨åŸŸå°æ ·åˆ†å‰²ï¼ˆCD-FSSï¼‰ä»»åŠ¡çš„ç›®æ ‡å’ŒæŒ‘æˆ˜ï¼Œå³åœ¨æ–°ç±»åˆ«ç›®æ ‡åŸŸä¸­å¯¹ä»…æœ‰å°‘é‡æ ‡æ³¨æ ·æœ¬è¿›è¡Œåˆ†å‰²ï¼Œä¸”ç›®æ ‡åŸŸä¸æºåŸŸçš„æ•°æ®åˆ†å¸ƒæ˜¾è‘—ä¸åŒã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é£æ ¼å·®è·è€Œå¿½è§†åˆ†å‰²ç²’åº¦å·®è·çš„é—®é¢˜ï¼Œæå‡ºäº†å±‚æ¬¡è¯­ä¹‰å­¦ä¹ ï¼ˆHSLï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬åŒé£æ ¼éšæœºåŒ–ï¼ˆDSRï¼‰æ¨¡å—ã€å±‚æ¬¡è¯­ä¹‰æŒ–æ˜ï¼ˆHSMï¼‰æ¨¡å—å’ŒåŸå‹ç½®ä¿¡è°ƒåˆ¶é˜ˆå€¼åŒ–ï¼ˆPCMTï¼‰æ¨¡å—æ¥è§£å†³æ­¤é—®é¢˜ã€‚è¯¥æ¡†æ¶å¯æé«˜æ¨¡å‹è¯†åˆ«ä¸åŒç²’åº¦å±‚æ¬¡è¯­ä¹‰çš„èƒ½åŠ›ï¼Œé€šè¿‡æ¨¡æ‹Ÿç›®æ ‡åŸŸæ•°æ®çš„å¤šæ ·é£æ ¼å·®å¼‚å’Œå‰æ™¯èƒŒæ™¯å·®å¼‚ï¼ŒæŒ–æ˜ç±»å†…ä¸€è‡´æ€§å’Œç±»é—´åŒºåˆ«ã€‚åœ¨å››ä¸ªæµè¡Œçš„ç›®æ ‡åŸŸæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSSä»»åŠ¡æ—¨åœ¨åˆ†å‰²æ–°ç±»åˆ«ï¼Œè¿™äº›ç±»åˆ«åœ¨ç›®æ ‡åŸŸä¸­æœªå‚ä¸è®­ç»ƒï¼Œä¸”ä¸æºåŸŸçš„æ•°æ®åˆ†å¸ƒæ˜¾è‘—ä¸åŒã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é£æ ¼å·®è·ï¼Œå¿½ç•¥äº†åˆ†å‰²ç²’åº¦å·®è·ï¼Œå¯¼è‡´å¯¹ç›®æ ‡åŸŸä¸­æ–°ç±»åˆ«çš„è¯­ä¹‰åˆ¤åˆ«ä¸è¶³ã€‚</li>
<li>æå‡ºçš„HSLæ¡†æ¶åŒ…æ‹¬DSRã€HSMå’ŒPCMTä¸‰ä¸ªæ¨¡å—æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>DSRæ¨¡å—æ¨¡æ‹Ÿç›®æ ‡åŸŸæ•°æ®çš„å¤šæ ·é£æ ¼å·®å¼‚å’Œå‰æ™¯èƒŒæ™¯å·®å¼‚ã€‚</li>
<li>HSMæ¨¡å—åˆ©ç”¨å¤šå°ºåº¦è¶…åƒç´ æ¥æŒ–æ˜ä¸åŒç²’åº¦å±‚æ¬¡çš„ç±»å†…ä¸€è‡´æ€§å’Œç±»é—´åŒºåˆ«ã€‚</li>
<li>PCMTæ¨¡å—ç”¨äºç¼“è§£å‰æ™¯å’ŒèƒŒæ™¯è¿‡äºç›¸ä¼¼æ—¶çš„åˆ†å‰²æ¨¡ç³Šé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85ea189222aa9594fedb4cbcc1a5529b" align="middle">
<img src="https://picx.zhimg.com/v2-813dc43a445c7940bd5747ad0aa1e43b" align="middle">
<img src="https://picx.zhimg.com/v2-de757559a00b4f847eec3eedb250a397" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="OAD-Promoter-Enhancing-Zero-shot-VQA-using-Large-Language-Models-with-Object-Attribute-Description"><a href="#OAD-Promoter-Enhancing-Zero-shot-VQA-using-Large-Language-Models-with-Object-Attribute-Description" class="headerlink" title="OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description"></a>OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description</h2><p><strong>Authors:Quanxing Xu, Ling Zhou, Feifei Zhang, Jinyu Tian, Rubing Huang</strong></p>
<p>Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆä¸ºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­çš„å…³é”®å·¥å…·ï¼Œåœ¨å¤„ç†å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬åœºæ™¯ä¸­çš„çŸ¥è¯†å¯†é›†å‹é—®é¢˜æ—¶å°¤ä¸ºå¦‚æ­¤ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹å¤§é‡è®­ç»ƒæ•°æ®é›†çš„ä¾èµ–å¾€å¾€å¯¼è‡´åœ¨è·å–çŸ¥è¯†æ—¶ç»§æ‰¿è¯­è¨€åè§ã€‚è¿™ä¸€å±€é™æ€§å¯¹ç°æœ‰æ–¹æ³•æå‡ºäº†ä¸¤ä¸ªå…³é”®çº¦æŸï¼šï¼ˆ1ï¼‰ç”±äºåè§åˆ©ç”¨ï¼ŒLLMçš„é¢„æµ‹å˜å¾—ä¸é‚£ä¹ˆå¯é ï¼›ï¼ˆ2ï¼‰å°½ç®¡å…·æœ‰å¼ºå¤§çš„çŸ¥è¯†æ¨ç†èƒ½åŠ›ï¼ŒLLMåœ¨å¤„ç†è¶…å‡ºåˆ†å¸ƒèŒƒå›´ï¼ˆOODï¼‰çš„æ³›åŒ–é—®é¢˜æ—¶ä»ç„¶æ„Ÿåˆ°å›°éš¾é‡é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Object Attribute Description Promoterï¼ˆOAD-Promoterï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç¼“è§£è¯­è¨€åè§å¹¶å¢å¼ºè·¨é¢†åŸŸç¨³å¥æ€§æ¥å¢å¼ºåŸºäºLLMçš„VQAæ€§èƒ½çš„æ–°æ–¹æ³•ã€‚OAD-PromoteråŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šObject-concentrated Example Generationï¼ˆOEGï¼‰æ¨¡å—ã€Memory Knowledge Assistanceï¼ˆMKAï¼‰æ¨¡å—å’ŒOADæç¤ºã€‚OEGæ¨¡å—ç”Ÿæˆå…¨å±€æ ‡é¢˜å’Œå¯¹è±¡é›†ä¸­æ ·æœ¬ï¼Œå…±åŒå¢å¼ºå¯¹LLMçš„è§†è§‰ä¿¡æ¯è¾“å…¥ï¼Œå¹¶é€šè¿‡äº’è¡¥çš„å…¨å±€å’ŒåŒºåŸŸæ€§è§†è§‰çº¿ç´¢ç¼“è§£åè§ã€‚MKAæ¨¡å—é€šè¿‡ä»å­˜å‚¨çš„ç¤ºä¾‹ä¸­æ£€ç´¢ä¸æœªè§é¢†åŸŸçš„é—®é¢˜ç›¸å…³çš„çŸ¥è¯†ï¼ŒååŠ©LLMå¤„ç†OODæ ·æœ¬ã€‚æœ€åï¼ŒOADæç¤ºæ•´åˆäº†å‰é¢æ¨¡å—çš„è¾“å‡ºä»¥ä¼˜åŒ–LLMæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒOAD-Promoteråœ¨åŸºäºLLMçš„VQAæ–¹æ³•ä¸­æ˜¾è‘—æé«˜äº†å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬è®¾ç½®ä¸­çš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†æœ€æ–°çš„æœ€ä½³ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12131v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­å¯¹äºå¤„ç†çŸ¥è¯†å¯†é›†å‹é—®é¢˜ååˆ†é‡è¦ï¼Œä½†åœ¨å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬åœºæ™¯ä¸‹ï¼Œå…¶ä¾èµ–å¤§é‡è®­ç»ƒæ•°æ®é›†å¯¼è‡´çš„è¯­è¨€åè§é—®é¢˜é™åˆ¶äº†å…¶å¯é æ€§åŠæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºObject Attribute Description Promoterï¼ˆOAD-Promoterï¼‰æ–°æ–¹æ³•ï¼Œé€šè¿‡å‡è½»è¯­è¨€åè§å’Œæé«˜é¢†åŸŸåç§»ç¨³å¥æ€§ï¼Œå¢å¼ºLLMåœ¨VQAä¸­çš„æ€§èƒ½ã€‚OAD-PromoteråŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šObject-concentrated Example Generationï¼ˆOEGï¼‰ã€Memory Knowledge Assistanceï¼ˆMKAï¼‰å’ŒOAD Promptã€‚å®ƒä»¬åˆ†åˆ«é€šè¿‡ç”Ÿæˆå…¨å±€å­—å¹•å’Œå¯¹è±¡é›†ä¸­æ ·æœ¬ã€ä»å­˜å‚¨çš„ç¤ºä¾‹ä¸­æ£€ç´¢ç›¸å…³çŸ¥è¯†ç‚¹ä»¥æ”¯æŒæœªè§é¢†åŸŸçš„é—®é¢˜å¤„ç†ã€ä»¥åŠä¼˜åŒ–LLMæ¨ç†ï¼Œå…±åŒæå‡VQAçš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨VQAä¸­å¤„ç†çŸ¥è¯†å¯†é›†å‹é—®é¢˜æ—¶ï¼Œä¾èµ–å¤§é‡è®­ç»ƒæ•°æ®é›†å¯èƒ½å¯¼è‡´è¯­è¨€åè§ã€‚</li>
<li>è¯­è¨€åè§ä¼šå½±å“LLMé¢„æµ‹çš„å¯é æ€§å¹¶é™åˆ¶å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>OAD-Promoteræ˜¯ä¸€ç§å¢å¼ºLLMåœ¨VQAä¸­çš„æ€§èƒ½çš„æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬OEGã€MKAå’ŒOAD Promptä¸‰ä¸ªæ¨¡å—ã€‚</li>
<li>OEGæ¨¡å—é€šè¿‡ç”Ÿæˆå…¨å±€å’Œå¯¹è±¡é›†ä¸­çš„æ ·æœ¬ï¼Œæé«˜è§†è§‰ä¿¡æ¯è¾“å…¥ï¼Œå¹¶å‡è½»è¯­è¨€åè§ã€‚</li>
<li>MKAæ¨¡å—é€šè¿‡ä»å­˜å‚¨çš„ç¤ºä¾‹ä¸­æ£€ç´¢ç›¸å…³çŸ¥è¯†ç‚¹ï¼Œå¸®åŠ©LLMå¤„ç†æ¥è‡ªæœªè§é¢†åŸŸçš„é—®é¢˜ã€‚</li>
<li>OAD Promptæ¨¡å—ä¼˜åŒ–äº†LLMçš„æ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33cc172254ea50df72c6f4ae0fcbc5c5" align="middle">
<img src="https://picx.zhimg.com/v2-c9662e4d7adceffd1f7e344648d9203a" align="middle">
<img src="https://picx.zhimg.com/v2-a00140384200cc1195a48c810a313db4" align="middle">
<img src="https://picx.zhimg.com/v2-1452563b67c0229577612097043d3c22" align="middle">
<img src="https://picx.zhimg.com/v2-9568f3398ef6f893f742f5a96c05e4c6" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Towards-Mitigating-Systematics-in-Large-Scale-Surveys-via-Few-Shot-Optimal-Transport-Based-Feature-Alignment"><a href="#Towards-Mitigating-Systematics-in-Large-Scale-Surveys-via-Few-Shot-Optimal-Transport-Based-Feature-Alignment" class="headerlink" title="Towards Mitigating Systematics in Large-Scale Surveys via Few-Shot Optimal Transport-Based Feature Alignment"></a>Towards Mitigating Systematics in Large-Scale Surveys via Few-Shot Optimal Transport-Based Feature Alignment</h2><p><strong>Authors:Sultan Hassan, Sambatra Andrianomena, Benjamin D. Wandelt</strong></p>
<p>Systematics contaminate observables, leading to distribution shifts relative to theoretically simulated signals-posing a major challenge for using pre-trained models to label such observables. Since systematics are often poorly understood and difficult to model, removing them directly and entirely may not be feasible. To address this challenge, we propose a novel method that aligns learned features between in-distribution (ID) and out-of-distribution (OOD) samples by optimizing a feature-alignment loss on the representations extracted from a pre-trained ID model. We first experimentally validate the method on the MNIST dataset using possible alignment losses, including mean squared error and optimal transport, and subsequently apply it to large-scale maps of neutral hydrogen. Our results show that optimal transport is particularly effective at aligning OOD features when parity between ID and OOD samples is unknown, even with limited data-mimicking real-world conditions in extracting information from large-scale surveys. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sultan-hassan/feature-alignment-for-OOD-generalization">https://github.com/sultan-hassan/feature-alignment-for-OOD-generalization</a>.</p>
<blockquote>
<p>ç³»ç»Ÿè¯¯å·®æ±¡æŸ“äº†è§‚æµ‹ç»“æœï¼Œå¯¼è‡´ç›¸å¯¹äºç†è®ºæ¨¡æ‹Ÿä¿¡å·çš„åˆ†å¸ƒåç§»ï¼Œä¸ºä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯¹è¿™ç±»è§‚æµ‹ç»“æœè¿›è¡Œæ ‡æ³¨å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ç”±äºç³»ç»Ÿè¯¯å·®é€šå¸¸éš¾ä»¥ç†è§£å’Œå»ºæ¨¡ï¼Œç›´æ¥ä¸”å®Œå…¨æ¶ˆé™¤å®ƒä»¬å¯èƒ½ä¸å¯è¡Œã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹ä»è®­ç»ƒå†…æ ·æœ¬ï¼ˆIn-Distributionï¼ŒIDï¼‰ä¸­æå–ç‰¹å¾çš„å¯¹é½æŸå¤±æ¥å¯¹é½è®­ç»ƒå†…å’Œè®­ç»ƒå¤–æ ·æœ¬ï¼ˆOut-of-Distributionï¼ŒOODï¼‰çš„å·²å­¦ä¹ ç‰¹å¾ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¯èƒ½çš„å¯¹é½æŸå¤±åœ¨MNISTæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬å‡æ–¹è¯¯å·®å’Œæœ€ä¼˜ä¼ è¾“ï¼Œç„¶åå°†å…¶åº”ç”¨äºå¤§è§„æ¨¡ä¸­æ€§æ°¢åœ°å›¾ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ä¸çŸ¥é“IDå’ŒOODæ ·æœ¬ä¹‹é—´å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨æœ‰é™æ•°æ®æ¨¡ä»¿ç°å®ä¸–ç•Œçš„æå–ä¿¡æ¯æ¡ä»¶è¿›è¡Œå¤§è§„æ¨¡è°ƒæŸ¥æ—¶ï¼Œæœ€ä¼˜ä¼ è¾“åœ¨å¯¹é½OODç‰¹å¾æ–¹é¢å°¤å…¶æœ‰æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/sultan-hassan/feature-%E5%AF%B9%E9%BD%BF%E4%BB%A5-%E4%BA%9Bgeneralization">https://github.com/sultan-hassan/feature-alignment-for-OOD-generalization</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11787v1">PDF</a> 5 pages, 3 figures, accepted to NeurIPS Workshop on Unifying Representations in Neural Models (UniReps 2025)</p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹ç³»ç»Ÿè¯¯å·®å¯¹è§‚æµ‹é‡çš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç‰¹å¾å¯¹é½æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹æå–çš„ç‰¹å¾è¡¨ç¤ºä¸Šçš„ç‰¹å¾å¯¹é½æŸå¤±ï¼Œå®ç°å¯¹å†…åˆ†å¸ƒï¼ˆIDï¼‰å’Œå¼‚å¸¸åˆ†å¸ƒï¼ˆOODï¼‰æ ·æœ¬ä¹‹é—´çš„ç‰¹å¾å¯¹é½ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MNISTæ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ï¼Œé€‚ç”¨äºä¸­æ€§æ°¢çš„å¤§å°ºåº¦åœ°å›¾ã€‚ç»“æœè¡¨æ˜ï¼Œå½“IDå’ŒOODæ ·æœ¬ä¹‹é—´çš„å¹³è¡¡æœªçŸ¥æ—¶ï¼Œæœ€ä¼˜ä¼ è¾“åœ¨ç‰¹å¾å¯¹é½æ–¹é¢å°¤ä¸ºæœ‰æ•ˆï¼Œå³ä½¿åœ¨æ¨¡ä»¿çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹ä»å¤§è§„æ¨¡è°ƒæŸ¥ä¸­æå–ä¿¡æ¯æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä»£ç å·²å…¬å¼€åˆ†äº«ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>ç³»ç»Ÿè¯¯å·®ä¼šå½±å“è§‚æµ‹é‡çš„åˆ†å¸ƒï¼Œå¯¼è‡´ç†è®ºæ¨¡æ‹Ÿä¿¡å·ä¸å®é™…åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚è¿™æ„æˆäº†ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥æ ‡æ³¨æ­¤ç±»è§‚æµ‹é‡ã€‚</li>
<li>ç›´æ¥å»é™¤ç³»ç»Ÿè¯¯å·®å¯èƒ½ä¸å¯è¡Œï¼Œå› ä¸ºå¯¹å®ƒä»¬çš„ç†è§£é€šå¸¸æœ‰é™ä¸”éš¾ä»¥å»ºæ¨¡ã€‚å› æ­¤ï¼Œæå‡ºäº†ä¸€ç§ç‰¹å¾å¯¹é½çš„æ–°æ–¹æ³•æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>ç‰¹å¾å¯¹é½æ–¹æ³•é€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹æå–çš„ç‰¹å¾è¡¨ç¤ºä¸Šçš„ç‰¹å¾å¯¹é½æŸå¤±æ¥å®ç°ã€‚æ­¤æ–¹æ³•ä¸ä»…é€‚ç”¨äºIDæ ·æœ¬ï¼Œä¹Ÿé€‚ç”¨äºOODæ ·æœ¬ã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨MNISTæ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æœ€ä¼˜ä¼ è¾“è¿›è¡Œç‰¹å¾å¯¹é½æ—¶æ•ˆæœæ›´ä½³ã€‚å³ä½¿åœ¨æ²¡æœ‰è¶³å¤Ÿæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿèƒ½å¾ˆå¥½åœ°æ¨¡ä»¿çœŸå®ä¸–ç•Œçš„æ¡ä»¶ã€‚è¿™å¯¹äºä»å¤§è§„æ¨¡è°ƒæŸ¥ä¸­æå–ä¿¡æ¯éå¸¸æœ‰ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be63737520e856823831ba1deeb314b2" align="middle">
<img src="https://picx.zhimg.com/v2-c1e5263f7235b7eaf5b3adef2032f968" align="middle">
<img src="https://picx.zhimg.com/v2-6d7f851c0af8fbee8b1d71cb18874533" align="middle">
<img src="https://picx.zhimg.com/v2-038f761b36daac6603b816fd4cc9f283" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-516fb79028978983af85fdb61db0164c" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Free-Form Scene Editor Enabling Multi-Round Object Manipulation like in a 3D Engine
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bfec9309a05dcb700ac78f47a5bf0590" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  VIR-Bench Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
