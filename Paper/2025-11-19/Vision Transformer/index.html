<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  MergeSlide Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e07c52a6ebb5464c9cea2c1ebe0160e9')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    33 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="MergeSlide-Continual-Model-Merging-and-Task-to-Class-Prompt-Aligned-Inference-for-Lifelong-Learning-on-Whole-Slide-Images"><a href="#MergeSlide-Continual-Model-Merging-and-Task-to-Class-Prompt-Aligned-Inference-for-Lifelong-Learning-on-Whole-Slide-Images" class="headerlink" title="MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images"></a>MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images</h2><p><strong>Authors:Doanh C. Bui, Ba Hung Ngo, Hoai Luan Pham, Khang Nguyen, MaÃ¯ K. Nguyen, Yasuhiko Nakashima</strong></p>
<p>Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/caodoanh2001/MergeSlide">https://github.com/caodoanh2001/MergeSlide</a>.</p>
<blockquote>
<p>åœ¨å…¨æ™¯å›¾åƒï¼ˆWhole Slide Imagesï¼Œç®€ç§°WSIï¼‰ä¸Šçš„ç»ˆèº«å­¦ä¹ æ—¨åœ¨åœ¨ä¸€ç³»åˆ—ç™Œç—‡ç›¸å…³ä»»åŠ¡ä¸Šé¡ºåºåœ°è®­ç»ƒæˆ–å¾®è°ƒç»Ÿä¸€æ¨¡å‹ï¼Œä»è€Œå‡å°‘æ•°æ®è¿ç§»å’Œå¤„ç†çš„èµ„æºå’ŒåŠªåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è€ƒè™‘åˆ°å…¨æ™¯å›¾åƒå‰å­—èŠ‚è§„æ¨¡å¤§å°çš„æƒ…å†µä¸‹æ›´æ˜¯å¦‚æ­¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MergeSlideï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œå®ƒå°†ç»ˆèº«å­¦ä¹ è§†ä¸ºæ¨¡å‹åˆå¹¶é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€ç—…ç†å­¦åŸºç¡€æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚å½“æœ‰æ–°çš„ä»»åŠ¡åˆ°æ¥æ—¶ï¼Œå®ƒæ˜¯ï¼š1ï¼‰é€šè¿‡ç±»æ„ŸçŸ¥æç¤ºè¿›è¡Œå®šä¹‰ï¼Œ2ï¼‰ä½¿ç”¨ä¸€ä¸ªæ— å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„éª¨å¹²è¿›è¡Œå‡ ä¸ªå‘¨æœŸçš„å¾®è°ƒï¼Œä»¥åŠ3ï¼‰ä½¿ç”¨æ­£äº¤æŒç»­åˆå¹¶ç­–ç•¥å°†å…¶åˆå¹¶åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œè¯¥ç­–ç•¥æ—¢ä¿ç•™äº†æ€§èƒ½åˆå‡è½»äº†ç¾éš¾æ€§é—å¿˜ã€‚åœ¨ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆCLASS-ILï¼‰è®¾ç½®ä¸‹è¿›è¡Œæ¨ç†æ—¶ï¼Œä»»åŠ¡èº«ä»½æ˜¯æœªçŸ¥çš„ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†ä»»åŠ¡åˆ°ç±»åˆ«æç¤ºå¯¹é½ï¼ˆTCPï¼‰æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒTCPé¦–å…ˆä½¿ç”¨ä»»åŠ¡çº§æç¤ºè¯†åˆ«æœ€ç›¸å…³çš„ä»»åŠ¡ï¼Œç„¶ååº”ç”¨ç›¸åº”çš„ç±»æ„ŸçŸ¥æç¤ºæ¥ç”Ÿæˆé¢„æµ‹ã€‚ä¸ºäº†è¯„ä¼°MergeSlideï¼Œæˆ‘ä»¬åœ¨å…­ä¸ªTCGAæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒMergeSlideçš„è¡¨ç°ä¼˜äºåŸºäºå¤ä¹ çš„è¿ç»­å­¦ä¹ å’Œè§†è§‰è¯­è¨€é›¶æ ·æœ¬åŸºçº¿ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/caodoanh2001/MergeSlide%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/caodoanh2001/MergeSlideæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13099v1">PDF</a> WACV2026 Accepted</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MergeSlideæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç»ˆèº«å­¦ä¹ è§†ä¸ºæ¨¡å‹åˆå¹¶é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€ç—…ç†å­¦åŸºç¡€æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚æ–°ä»»åŠ¡é€šè¿‡ç±»æ„ŸçŸ¥æç¤ºè¿›è¡Œå®šä¹‰ï¼Œä½¿ç”¨æ— MLPéª¨æ¶è¿›è¡Œå¾®è°ƒï¼Œå¹¶é‡‡ç”¨æ­£äº¤æŒç»­åˆå¹¶ç­–ç•¥åˆå¹¶åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œä»è€Œä¿ç•™æ€§èƒ½å¹¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚åœ¨ç±»å¢é‡å­¦ä¹ è®¾ç½®ä¸‹è¿›è¡Œæ¨ç†æ—¶ï¼Œå¼•å…¥ä»»åŠ¡åˆ°ç±»æç¤ºå¯¹é½ï¼ˆTCPï¼‰æ¨ç†ï¼Œé€šè¿‡ä»»åŠ¡çº§åˆ«æç¤ºè¯†åˆ«æœ€ç›¸å…³ä»»åŠ¡ï¼Œç„¶ååº”ç”¨ç›¸åº”çš„ç±»æ„ŸçŸ¥æç¤ºç”Ÿæˆé¢„æµ‹ã€‚åœ¨TCGAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMergeSlideä¼˜äºåŸºäºå¤ä¹ çš„æŒç»­å­¦ä¹ å’Œè§†è§‰è¯­è¨€é›¶æ ·æœ¬åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MergeSlideæ¡†æ¶å°†ç»ˆèº«å­¦ä¹ è§†ä¸ºæ¨¡å‹åˆå¹¶é—®é¢˜ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€ç—…ç†å­¦åŸºç¡€æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚</li>
<li>æ–°ä»»åŠ¡é€šè¿‡ç±»æ„ŸçŸ¥æç¤ºè¿›è¡Œå®šä¹‰ï¼Œå¹¶é‡‡ç”¨æ— MLPéª¨æ¶è¿›è¡Œå¾®è°ƒã€‚</li>
<li>é‡‡ç”¨æ­£äº¤æŒç»­åˆå¹¶ç­–ç•¥ï¼Œå°†æ–°ä»»åŠ¡åˆå¹¶åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œä»¥ä¿ç•™æ€§èƒ½å¹¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>å¼•å…¥ä»»åŠ¡åˆ°ç±»æç¤ºå¯¹é½ï¼ˆTCPï¼‰æ¨ç†ï¼Œé€‚åº”ç±»å¢é‡å­¦ä¹ è®¾ç½®ã€‚</li>
<li>TCPæ¨ç†é€šè¿‡ä»»åŠ¡çº§åˆ«æç¤ºè¯†åˆ«æœ€ç›¸å…³ä»»åŠ¡ï¼Œç„¶ååº”ç”¨ç±»æ„ŸçŸ¥æç¤ºç”Ÿæˆé¢„æµ‹ã€‚</li>
<li>åœ¨TCGAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMergeSlideæ¡†æ¶çš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-049c5743b6429a5556599de446dfb39f" align="middle">
<img src="https://picx.zhimg.com/v2-f7a0eb038c23ea26ee797ffa6be62044" align="middle">
<img src="https://picx.zhimg.com/v2-66ef1da2d2bbba4e69dd0485c4611263" align="middle">
<img src="https://picx.zhimg.com/v2-8e17af0bcaa5d46928e2436770c71779" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Denoising-Vision-Transformer-Autoencoder-with-Spectral-Self-Regularization"><a href="#Denoising-Vision-Transformer-Autoencoder-with-Spectral-Self-Regularization" class="headerlink" title="Denoising Vision Transformer Autoencoder with Spectral Self-Regularization"></a>Denoising Vision Transformer Autoencoder with Spectral Self-Regularization</h2><p><strong>Authors:Xunzhi Xiang, Xingye Tian, Guiyu Zhang, Yabo Chen, Shaofeng Zhang, Xuebo Wang, Xin Tao, Qi Fan</strong></p>
<p>Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID &#x3D; 0.28, PSNR &#x3D; 27.26) and competitive generation performance (gFID &#x3D; 1.82) on the ImageNet 256$\times$256 benchmark.</p>
<blockquote>
<p>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰é€šå¸¸å°†å›¾åƒç¼–ç æˆç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½†åŒæ—¶ä¹Ÿå¼•å…¥äº†ä¼˜åŒ–å›°å¢ƒï¼šé«˜ç»´æ½œåœ¨ç©ºé—´æé«˜äº†é‡å»ºä¿çœŸåº¦ï¼Œä½†å¾€å¾€ä¼šå¦¨ç¢ç”Ÿæˆæ€§èƒ½ã€‚æœ€è¿‘çš„æ–¹æ³•å°è¯•é€šè¿‡ä½¿ç”¨å¤–éƒ¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å¯¹é«˜ç»´æ½œåœ¨ç©ºé—´è¿›è¡Œæ­£åˆ™åŒ–æ¥è§£å†³è¿™ä¸€å›°å¢ƒã€‚ç„¶è€Œï¼Œé«˜ç»´VAEæ½œåœ¨ç©ºé—´å¦‚ä½•å½±å“ç”Ÿæˆæ¨¡å‹çš„ä¼˜åŒ–ä»ä¸æ¸…æ¥šã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„åˆ†æé¦–æ¬¡æ­ç¤ºäº†é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­çš„å†—ä½™é«˜é¢‘æˆåˆ†ä¼šé˜»ç¢æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ”¶æ•›ï¼Œä»è€Œé™ä½äº†ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…‰è°±è‡ªæ­£åˆ™åŒ–ç­–ç•¥ï¼Œä»¥æŠ‘åˆ¶å†—ä½™çš„é«˜é¢‘å™ªå£°ï¼ŒåŒæ—¶ä¿æŒé‡å»ºè´¨é‡ã€‚ç”±æ­¤äº§ç”Ÿçš„å»å™ª-VAEæ˜¯ä¸€ç§åŸºäºViTçš„è‡ªç¼–ç å™¨ï¼Œå®ƒä¸ä¾èµ–äºVFMsï¼Œäº§ç”Ÿæ›´å¹²å‡€ã€å™ªå£°æ›´ä½çš„æ½œåœ¨ç‰¹å¾ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆè´¨é‡å’Œæ›´å¿«çš„ä¼˜åŒ–æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–åŸºäºå»å™ª-VAEçš„ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å…‰è°±å¯¹é½ç­–ç•¥ã€‚æˆ‘ä»¬çš„å®Œæ•´æ–¹æ³•ä½¿æ‰©æ•£æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å¤§çº¦æ¯”SD-VAEå¿«2å€ï¼ŒåŒæ—¶åœ¨ImageNet 256x256åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼ˆrFID &#x3D; 0.28ï¼ŒPSNR &#x3D; 27.26ï¼‰å’Œå…·æœ‰ç«äº‰åŠ›çš„ç”Ÿæˆæ€§èƒ½ï¼ˆgFID &#x3D; 1.82ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12633v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åœ¨é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­å¯¹å›¾åƒç¼–ç æ—¶é¢ä¸´çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œé«˜ç»´æ½œåœ¨ç©ºé—´ä¸­çš„å†—ä½™é«˜é¢‘æˆåˆ†ä¼šé˜»ç¢æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ”¶æ•›ï¼Œå½±å“ç”Ÿæˆè´¨é‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†è°±è‡ªæ­£åˆ™åŒ–ç­–ç•¥ï¼Œèƒ½å¤ŸæŠ‘åˆ¶å†—ä½™é«˜é¢‘å™ªå£°ï¼ŒåŒæ—¶ä¿æŒé‡å»ºè´¨é‡ã€‚åŸºäºæ­¤ç­–ç•¥çš„Denoising-VAEæ¨¡å‹ï¼Œåœ¨ä¸ä¾èµ–å¤–éƒ¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆäº†æ›´æ¸…æ´ã€ä½å™ªå£°çš„æ½œåœ¨ç©ºé—´ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡å’Œä¼˜åŒ–æ”¶æ•›é€Ÿåº¦ã€‚åŒæ—¶ï¼Œè¿˜å¼•å…¥äº†è°±å¯¹é½ç­–ç•¥ï¼Œæœ‰åŠ©äºä¼˜åŒ–åŸºäºDenoising-VAEçš„ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ–¹æ³•ä½¿æ‰©æ•£æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å¤§çº¦æé«˜äº†ä¸¤å€ï¼ŒåŒæ—¶åœ¨ImageNet 256Ã—256åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼ˆrFID &#x3D; 0.28ï¼ŒPSNR &#x3D; 27.26ï¼‰å’Œå…·æœ‰ç«äº‰åŠ›çš„ç”Ÿæˆæ€§èƒ½ï¼ˆgFID &#x3D; 1.82ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åœ¨é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­é¢ä¸´ä¼˜åŒ–éš¾é¢˜ï¼Œå†—ä½™é«˜é¢‘æˆåˆ†å½±å“ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ”¶æ•›ã€‚</li>
<li>è°±è‡ªæ­£åˆ™åŒ–ç­–ç•¥èƒ½å¤ŸæŠ‘åˆ¶é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­çš„å†—ä½™é«˜é¢‘å™ªå£°ï¼Œæé«˜é‡å»ºè´¨é‡ã€‚</li>
<li>Denoising-VAEæ¨¡å‹åœ¨ä¸ä¾èµ–å¤–éƒ¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆäº†æ›´æ¸…æ´ã€ä½å™ªå£°çš„æ½œåœ¨ç©ºé—´ã€‚</li>
<li>Denoising-VAEæé«˜äº†ç”Ÿæˆè´¨é‡å’Œä¼˜åŒ–æ”¶æ•›é€Ÿåº¦ï¼ŒåŒæ—¶å¼•å…¥äº†è°±å¯¹é½ç­–ç•¥æ¥ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿æ‰©æ•£æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å¤§çº¦æé«˜äº†ä¸¤å€ã€‚</li>
<li>åœ¨ImageNet 256Ã—256åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDenoising-VAEå®ç°äº†å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œå…·æœ‰ç«äº‰åŠ›çš„ç”Ÿæˆæ€§èƒ½ã€‚<br>7.è¯¥ç ”ç©¶å¯¹äºè§£å†³å˜åˆ†è‡ªç¼–ç å™¨åœ¨é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­çš„ä¼˜åŒ–é—®é¢˜å…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bfc7a6da35a9dbe4ef7ccca3ad7dc2f9" align="middle">
<img src="https://picx.zhimg.com/v2-596e63e7f7f58b0dca986d84917b5992" align="middle">
<img src="https://picx.zhimg.com/v2-bfcefb1106bf066fbac5d12436390ff4" align="middle">
<img src="https://picx.zhimg.com/v2-31a35bc098559733b16de6b316c6895d" align="middle">
<img src="https://picx.zhimg.com/v2-dfe9e781a8e96824f288a605e83673b7" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LoRA-Enhanced-Vision-Transformer-for-Single-Image-based-Morphing-Attack-Detection-via-Knowledge-Distillation-from-EfficientNet"><a href="#LoRA-Enhanced-Vision-Transformer-for-Single-Image-based-Morphing-Attack-Detection-via-Knowledge-Distillation-from-EfficientNet" class="headerlink" title="LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet"></a>LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet</h2><p><strong>Authors:Ria Shekhawat, Sushrut Patwardhan, Raghavendra Ramachandra, Praveen Kumar Chandaliya, Kishor P. Upla</strong></p>
<p>Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.</p>
<blockquote>
<p>äººè„¸è¯†åˆ«ç³»ç»Ÿï¼ˆFRSï¼‰å¯¹å®‰å…¨è‡³å…³é‡è¦ï¼Œä½†ä»å®¹æ˜“å—åˆ°æ¢è„¸æ”»å‡»çš„å½±å“ï¼Œå³åˆæˆå›¾åƒèåˆäº†å¤šä¸ªä¸ªä½“çš„ç”Ÿç‰©è¯†åˆ«ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶çš„å•å›¾åƒæ¢è„¸æ”»å‡»æ£€æµ‹ï¼ˆS-MADï¼‰æ–¹æ³•ï¼Œå…¶ä¸­åŸºäºCNNçš„æ•™å¸ˆæ¨¡å‹å¯¹åŸºäºViTçš„å­¦ç”Ÿæ¨¡å‹è¿›è¡Œç²¾ç»†åŒ–è°ƒæ•´ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬é›†æˆäº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒï¼Œåœ¨ä¿æŒé«˜æ£€æµ‹ç²¾åº¦çš„åŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬åœ¨ç”±ä¸‰ä¸ªå…¬å¼€å¯ç”¨çš„äººè„¸æ•°æ®é›†æ„å»ºçš„æ¢è„¸æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“åˆäº†åç§ä¸åŒçš„æ¢è„¸ç”Ÿæˆç®—æ³•æ¥è¯„ä¼°ç¨³å¥æ€§ã€‚è¯¥æ–¹æ³•ä¸å…­ç§æœ€å…ˆè¿›çš„S-MADæŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ£€æµ‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12602v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºæ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å•å›¾åƒå˜å½¢æ”»å‡»æ£€æµ‹ï¼ˆS-MADï¼‰æ–¹æ³•ï¼Œä½¿ç”¨CNNæ•™å¸ˆæ¨¡å‹ä¼˜åŒ–ViTå­¦ç”Ÿæ¨¡å‹ã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬æ•´åˆäº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒï¼Œé™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒé«˜æ£€æµ‹ç²¾åº¦ã€‚åœ¨ç”±ä¸‰ä¸ªå…¬å¼€äººè„¸æ•°æ®é›†æ„å»ºçš„å˜å½¢æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œè¯„ä¼°äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚ä¸å…­ç§æœ€å…ˆè¿›çš„S-MADæŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ£€æµ‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„Single-Image Morphing Attack Detection (S-MAD)æ–¹æ³•ï¼Œåˆ©ç”¨æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶è¿›è¡Œäººè„¸è¯†åˆ«ç³»ç»Ÿçš„å®‰å…¨æ€§å¢å¼ºã€‚</li>
<li>é‡‡ç”¨CNNæ•™å¸ˆæ¨¡å‹å¯¹ViTå­¦ç”Ÿæ¨¡å‹è¿›è¡Œç²¾ç»†åŒ–è®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡é›†æˆLow-Rank Adaptation (LoRA)è¿›è¡Œå¾®è°ƒï¼Œå¢å¼ºäº†æ¨¡å‹çš„è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å¤šç§ç®—æ³•ç”Ÿæˆçš„åˆæˆäººè„¸å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒéªŒè¯ã€‚</li>
<li>å¯¹æ¯”å…­ç§å½“å‰æœ€å‰æ²¿çš„S-MADæŠ€æœ¯ï¼Œå±•ç¤ºäº†è‡ªå·±æå‡ºæ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>æ­¤æ–¹æ³•å¯ä»¥æœ‰æ•ˆæ£€æµ‹å‡ºåˆ©ç”¨å¤šä¸ªäººçš„ç”Ÿç‰©ç‰¹å¾åˆæˆçš„äººè„¸å›¾åƒï¼Œæœ‰åŠ©äºäººè„¸è¯†åˆ«ç³»ç»Ÿçš„å®‰å…¨æ€§æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e297aa1f3cddca4484037c0aa5e46029" align="middle">
<img src="https://picx.zhimg.com/v2-42361b8ebfbccdb12912e7a3570c24ac" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Image-POSER-Reflective-RL-for-Multi-Expert-Image-Generation-and-Editing"><a href="#Image-POSER-Reflective-RL-for-Multi-Expert-Image-Generation-and-Editing" class="headerlink" title="Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing"></a>Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing</h2><p><strong>Authors:Hossein Mohebbi, Mohammed Abdulrahman, Yanting Miao, Pascal Poupart, Suraj Kothawade</strong></p>
<p>Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.</p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æœ€æ–°è¿›å±•å·²ç»äº§ç”Ÿäº†å¼ºå¤§çš„å•é•œå¤´æ¨¡å‹ï¼Œç„¶è€Œï¼Œæ²¡æœ‰å•ä¸ªç³»ç»Ÿèƒ½å¤Ÿå¯é åœ°æ‰§è¡Œåˆ›é€ æ€§å·¥ä½œæµç¨‹ä¸­æ‰€å…¸å‹çš„é•¿æœŸç»„åˆæç¤ºã€‚æˆ‘ä»¬å¼•å…¥äº†Image-POSERï¼Œè¿™æ˜¯ä¸€ç§åå°„æ€§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒï¼ˆiï¼‰åè°ƒäº†å¤šæ ·åŒ–çš„é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°å›¾åƒçš„ä¸“å®¶æ³¨å†Œï¼Œï¼ˆiiï¼‰é€šè¿‡åŠ¨æ€ä»»åŠ¡åˆ†è§£ç«¯åˆ°ç«¯åœ°å¤„ç†é•¿å½¢å¼æç¤ºï¼Œä»¥åŠï¼ˆiiiï¼‰é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹æ‰¹è¯„è€…çš„ç»“æ„åŒ–åé¦ˆæ¥ç›‘ç£æ¯ä¸€æ­¥çš„å¯¹é½ã€‚é€šè¿‡å°†å›¾åƒåˆæˆå’Œç¼–è¾‘ä½œä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæˆ‘ä»¬å­¦ä¹ äº†éå¹³å‡¡çš„ä¸“å®¶ç®¡é“ï¼Œè¿™äº›ç®¡é“èƒ½å¤Ÿè‡ªé€‚åº”åœ°ç»“åˆå„æ¨¡å‹çš„ä¼˜ç‚¹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ ‡å‡†è¡Œä¸šåŸºå‡†å’Œè‡ªå®šä¹‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒImage-POSERåœ¨å¯¹é½ã€ä¿çœŸåº¦å’Œç¾å­¦æ–¹é¢çš„è¡¨ç°å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨äººç±»è¯„ä¼°ä¸­å§‹ç»ˆå—åˆ°é’çã€‚è¿™äº›ç»“æœçªæ˜¾äº†å¼ºåŒ–å­¦ä¹ å¯ä»¥ä½¿AIç³»ç»Ÿå…·å¤‡è‡ªä¸»åˆ†è§£ã€é‡æ–°æ’åºå’Œç»„åˆè§†è§‰æ¨¡å‹çš„èƒ½åŠ›ï¼Œæœç€é€šç”¨è§†è§‰åŠ©ç†çš„æ–¹å‘å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11780v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Image-POSERï¼Œä¸€ä¸ªåå°„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåè°ƒä¸åŒçš„æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°å›¾åƒçš„é¢„è®­ç»ƒä¸“å®¶æ¨¡å‹ï¼Œå¤„ç†é•¿å½¢å¼çš„æç¤ºå¹¶è¿›è¡Œç«¯åˆ°ç«¯çš„åŠ¨æ€ä»»åŠ¡åˆ†è§£ï¼Œå¹¶é€šè¿‡æ¥è‡ªè§†è§‰è¯­è¨€æ¨¡å‹æ‰¹è¯„è€…çš„ç»“æ„åŒ–åé¦ˆæ¥ç›‘ç£æ¯ä¸€æ­¥çš„å¯¹é½ã€‚é€šè¿‡é‡‡ç”¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å¯¹å›¾åƒåˆæˆå’Œç¼–è¾‘è¿›è¡Œå­¦ä¹ ï¼ŒImage-POSERå¯ä»¥è‡ªé€‚åº”åœ°ç»“åˆå„ä¸ªæ¨¡å‹çš„ä¼˜ç‚¹ã€‚å®éªŒè¡¨æ˜ï¼ŒImage-POSERåœ¨å¯¹é½ã€ä¿çœŸåº¦å’Œç¾å­¦æ–¹é¢ä¼˜äºåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å‰æ²¿æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨äººç±»è¯„ä¼°ä¸­è¡¨ç°ä¸€è‡´ã€‚è¿™ä¸€ç»“æœçªæ˜¾äº†å¼ºåŒ–å­¦ä¹ å¯ä»¥ä½¿AIç³»ç»Ÿå…·å¤‡è‡ªä¸»åˆ†è§£ã€é‡æ–°æ’åºå’Œç»„åˆè§†è§‰æ¨¡å‹çš„èƒ½åŠ›ï¼Œæœç€é€šç”¨è§†è§‰åŠ©ç†çš„æ–¹å‘å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Image-POSERæ˜¯ä¸€ä¸ªåå°„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåè°ƒå¤šç§æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°å›¾åƒçš„é¢„è®­ç»ƒä¸“å®¶æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†é•¿å½¢å¼çš„æç¤ºå¹¶è¿›è¡Œç«¯åˆ°ç«¯çš„åŠ¨æ€ä»»åŠ¡åˆ†è§£ã€‚</li>
<li>Image-POSERé€šè¿‡ç»“æ„åŒ–åé¦ˆç›‘ç£æ¯ä¸€æ­¥çš„å¯¹é½ï¼Œæ¥è‡ªè§†è§‰è¯­è¨€æ¨¡å‹æ‰¹è¯„è€…ã€‚</li>
<li>é‡‡ç”¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å¯¹å›¾åƒåˆæˆå’Œç¼–è¾‘è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>Image-POSERå¯ä»¥è‡ªé€‚åº”åœ°ç»“åˆå„ä¸ªæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œå½¢æˆå¤æ‚çš„è§†è§‰æ¨¡å‹æµæ°´çº¿ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒImage-POSERåœ¨å¯¹é½ã€ä¿çœŸåº¦å’Œç¾å­¦æ–¹é¢ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒ…æ‹¬å‰æ²¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e07c52a6ebb5464c9cea2c1ebe0160e9" align="middle">
<img src="https://picx.zhimg.com/v2-e2601d0fbec69d3603ec30fae17e747b" align="middle">
<img src="https://picx.zhimg.com/v2-cddff02ab4ee87cb0dc71feb9e6480fb" align="middle">
<img src="https://picx.zhimg.com/v2-f8ddfdf5bc21dde7d19a34de85630e34" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LeakyCLIP-Extracting-Training-Data-from-CLIP"><a href="#LeakyCLIP-Extracting-Training-Data-from-CLIP" class="headerlink" title="LeakyCLIP: Extracting Training Data from CLIP"></a>LeakyCLIP: Extracting Training Data from CLIP</h2><p><strong>Authors:Yunhao Chen, Shujie Wang, Xin Wang, Xingjun Ma</strong></p>
<p>Understanding the memorization and privacy leakage risks in Contrastive Languageâ€“Image Pretraining (CLIP) is critical for ensuring the security of multimodal models. Recent studies have demonstrated the feasibility of extracting sensitive training examples from diffusion models, with conditional diffusion models exhibiting a stronger tendency to memorize and leak information. In this work, we investigate data memorization and extraction risks in CLIP through the lens of CLIP inversion, a process that aims to reconstruct training images from text prompts. To this end, we introduce \textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality, semantically accurate image reconstruction from CLIP embeddings. We identify three key challenges in CLIP inversion: 1) non-robust features, 2) limited visual semantics in text embeddings, and 3) low reconstruction fidelity. To address these challenges, LeakyCLIP employs 1) adversarial fine-tuning to enhance optimization smoothness, 2) linear transformation-based embedding alignment, and 3) Stable Diffusion-based refinement to improve fidelity. Empirical results demonstrate the superiority of LeakyCLIP, achieving over 258% improvement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared to baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive leakage risk, showing that training data membership can even be successfully inferred from the metrics of low-fidelity reconstructions. Our work introduces a practical method for CLIP inversion while offering novel insights into the nature and scope of privacy risks in multimodal models.</p>
<blockquote>
<p>ç†è§£å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ä¸­çš„è®°å¿†å’Œéšç§æ³„éœ²é£é™©å¯¹äºç¡®ä¿å¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»è¯æ˜äº†ä»æ‰©æ•£æ¨¡å‹ä¸­æå–æ•æ„Ÿè®­ç»ƒå®ä¾‹çš„å¯è¡Œæ€§ï¼Œå…¶ä¸­æ¡ä»¶æ‰©æ•£æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„è®°å¿†å’Œæ³„éœ²ä¿¡æ¯çš„å€¾å‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡CLIPåè½¬çš„è§†è§’æ¥ç ”ç©¶CLIPä¸­çš„æ•°æ®è®°å¿†å’Œæå–é£é™©ã€‚CLIPåè½¬æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»æ–‡æœ¬æç¤ºé‡å»ºè®­ç»ƒå›¾åƒçš„è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºâ€œLeakyCLIPâ€çš„æ–°å‹æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡CLIPåµŒå…¥å®ç°é«˜è´¨é‡ã€è¯­ä¹‰å‡†ç¡®çš„å›¾åƒé‡å»ºã€‚æˆ‘ä»¬å‘ç°CLIPåè½¬é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰ç‰¹å¾ä¸ç¨³å¥ï¼Œ2ï¼‰æ–‡æœ¬åµŒå…¥ä¸­çš„è§†è§‰è¯­ä¹‰æœ‰é™ï¼Œä»¥åŠ3ï¼‰é‡å»ºä¿çœŸåº¦ä½ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒLeakyCLIPé‡‡ç”¨1ï¼‰å¯¹æŠ—æ€§å¾®è°ƒä»¥å¢å¼ºä¼˜åŒ–çš„å¹³ç¨³æ€§ï¼Œ2ï¼‰åŸºäºçº¿æ€§å˜æ¢çš„åµŒå…¥å¯¹é½ï¼Œä»¥åŠ3ï¼‰åŸºäºç¨³å®šæ‰©æ•£çš„ç²¾ç‚¼ä»¥æé«˜ä¿çœŸåº¦ã€‚ç»éªŒç»“æœè¡¨æ˜LeakyCLIPçš„ä¼˜è¶Šæ€§ï¼Œåœ¨LAION-2Bå­é›†ä¸Šä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒViT-B-16çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°æµ‹é‡ï¼ˆSSIMï¼‰æé«˜äº†è¶…è¿‡258ï¼…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†æ™®éçš„æ³„æ¼é£é™©ï¼Œå¹¶å±•ç¤ºå³ä½¿æ˜¯ä»ä½ä¿çœŸé‡å»ºçš„æŒ‡æ ‡ä¸­ä¹Ÿå¯ä»¥æˆåŠŸæ¨æ–­å‡ºè®­ç»ƒæ•°æ®æˆå‘˜èº«ä»½ã€‚æˆ‘ä»¬çš„å·¥ä½œä»‹ç»äº†ä¸€ç§å®ç”¨çš„CLIPåè½¬æ–¹æ³•ï¼ŒåŒæ—¶æä¾›äº†å¯¹å¤šæ¨¡æ€æ¨¡å‹ä¸­éšç§é£é™©æœ¬è´¨å’ŒèŒƒå›´çš„å…¨æ–°è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00756v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Contrastive Languageâ€“Image Pretrainingï¼ˆCLIPï¼‰æ¨¡å‹ä¸­çš„è®°å¿†å’Œéšç§æ³„éœ²é£é™©é—®é¢˜ã€‚é€šè¿‡CLIPåæ¼”çš„è§’åº¦è¿›è¡Œç ”ç©¶ï¼Œå¼•å…¥äº†LeakyCLIPæ”»å‡»æ¡†æ¶ï¼Œå®ç°äº†é«˜è´¨é‡ã€è¯­ä¹‰å‡†ç¡®çš„å›¾åƒé‡å»ºã€‚é’ˆå¯¹CLIPåæ¼”ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼ŒLeakyCLIPé‡‡å–äº†ç›¸åº”çš„æªæ–½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLeakyCLIPåœ¨ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶æ­ç¤ºäº†è®­ç»ƒæ•°æ®æˆå‘˜èº«ä»½å¯ä»¥ä»ä½è´¨é‡é‡å»ºçš„å›¾åƒä¸­æ¨æ–­å‡ºæ¥ï¼Œè¿™è¡¨æ˜å­˜åœ¨æ™®éçš„æ³„éœ²é£é™©ã€‚æœ¬æ–‡æä¾›äº†å…³äºå¤šæ¨¡æ€æ¨¡å‹ä¸­éšç§é£é™©çš„å®ç”¨æ–¹æ³•å’Œæ–°è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨CLIPæ¨¡å‹çš„è®°å¿†å’Œéšç§æ³„éœ²é£é™©é—®é¢˜ï¼Œå¼ºè°ƒäº†å…¶å®‰å…¨æ€§å’Œéšç§ä¿æŠ¤çš„é‡è¦æ€§ã€‚</li>
<li>é€šè¿‡CLIPåæ¼”çš„è§’åº¦è¿›è¡Œç ”ç©¶ï¼Œå¼•å…¥LeakyCLIPæ”»å‡»æ¡†æ¶è¿›è¡Œå›¾åƒé‡å»ºã€‚</li>
<li>LeakyCLIPè§£å†³äº†CLIPåæ¼”ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç‰¹å¾ä¸ç¨³å¥ã€æ–‡æœ¬åµŒå…¥ä¸­çš„è§†è§‰è¯­ä¹‰æœ‰é™ä»¥åŠé‡å»ºä¿çœŸåº¦ä½ã€‚</li>
<li>LeakyCLIPé€šè¿‡å¯¹æŠ—æ€§å¾®è°ƒã€åŸºäºçº¿æ€§å˜æ¢çš„åµŒå…¥å¯¹é½å’ŒåŸºäºStable Diffusionçš„ç»†åŒ–æ¥æ”¹è¿›æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜LeakyCLIPåœ¨SSIMä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¶…è¿‡äº†åŸºçº¿æ–¹æ³•ã€‚</li>
<li>è®­ç»ƒæ•°æ®æˆå‘˜èº«ä»½å¯ä»¥ä»ä½è´¨é‡é‡å»ºçš„å›¾åƒä¸­æ¨æ–­å‡ºæ¥ï¼Œå­˜åœ¨æ™®éçš„æ³„éœ²é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-745fa33cfdd4c3f2480d35a285b925dd" align="middle">
<img src="https://picx.zhimg.com/v2-b835d5fd0c6ffd33d2d515f89def09a2" align="middle">
<img src="https://picx.zhimg.com/v2-5af933ba5bebc22d1387fbfbe5c377be" align="middle">
<img src="https://picx.zhimg.com/v2-d0708497d092cbdfb749a5730e52936c" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FaceShield-Explainable-Face-Anti-Spoofing-with-Multimodal-Large-Language-Models"><a href="#FaceShield-Explainable-Face-Anti-Spoofing-with-Multimodal-Large-Language-Models" class="headerlink" title="FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models"></a>FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models</h2><p><strong>Authors:Hongyang Wang, Yichen Shi, Zhuofu Tao, Yuhao Gao, Liepiao Zhang, Xun Lin, Jun Feng, Xiaochen Yuan, Zitong Yu, Xiaochun Cao</strong></p>
<p>Face anti-spoofing (FAS) is crucial for protecting facial recognition systems from presentation attacks. Previous methods approached this task as a classification problem, lacking interpretability and reasoning behind the predicted results. Recently, multimodal large language models (MLLMs) have shown strong capabilities in perception, reasoning, and decision-making in visual tasks. However, there is currently no universal and comprehensive MLLM and dataset specifically designed for FAS task. To address this gap, we propose FaceShield, a MLLM for FAS, along with the corresponding pre-training and supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K. FaceShield is capable of determining the authenticity of faces, identifying types of spoofing attacks, providing reasoning for its judgments, and detecting attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that incorporates both the original image and auxiliary information based on prior knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to random mask vision tokens, thereby improving the modelâ€™s generalization ability. We conducted extensive experiments on three benchmark datasets, demonstrating that FaceShield significantly outperforms previous deep learning models and general MLLMs on four FAS tasks, i.e., coarse-grained classification, fine-grained classification, reasoning, and attack localization. Our instruction datasets, protocols, and codes will be released at <a target="_blank" rel="noopener" href="https://github.com/Why0912/FaceShield">https://github.com/Why0912/FaceShield</a>. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«é˜²æ¬ºéª—ï¼ˆFace Anti-Spoofing, FASï¼‰å¯¹äºä¿æŠ¤äººè„¸è¯†åˆ«ç³»ç»Ÿå…å—ä¼ªé€ æ”»å‡»è‡³å…³é‡è¦ã€‚ä¹‹å‰çš„æ–¹æ³•å°†è¿™é¡¹ä»»åŠ¡è§†ä¸ºä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œç¼ºä¹é¢„æµ‹ç»“æœçš„å¯è§£é‡Šæ€§å’Œåˆç†æ€§ã€‚æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Models, MLLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡çš„æ„ŸçŸ¥ã€æ¨ç†å’Œå†³ç­–æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰å°šæœªå‡ºç°ä¸“é—¨é’ˆå¯¹FASä»»åŠ¡çš„é€šç”¨ä¸”å…¨é¢çš„MLLMå’Œä¸“é—¨è®¾è®¡çš„æ•°æ®é›†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†FaceShieldï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºFASçš„MLLMï¼Œä»¥åŠç›¸åº”çš„é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-tuning, SFTï¼‰æ•°æ®é›†FaceShield-pre10Kå’ŒFaceShield-sft45Kã€‚FaceShieldèƒ½å¤Ÿç¡®å®šäººè„¸çš„çœŸå®æ€§ã€è¯†åˆ«æ¬ºéª—æ”»å‡»çš„ç±»å‹ã€ä¸ºåˆ¤æ–­æä¾›ç†ç”±ï¼Œå¹¶æ£€æµ‹æ”»å‡»åŒºåŸŸã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºå…ˆéªŒçŸ¥è¯†çš„æ¬ºéª—æ„ŸçŸ¥è§†è§‰æ„ŸçŸ¥ï¼ˆSpoof-Aware Vision Perception, SAVPï¼‰ï¼Œå®ƒç»“åˆäº†åŸå§‹å›¾åƒå’Œè¾…åŠ©ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æç¤ºå¼•å¯¼è§†è§‰ä»¤ç‰Œå±è”½ï¼ˆPrompt-Guided Vision Token Masking, PVTMï¼‰ç­–ç•¥æ¥éšæœºå±è”½è§†è§‰ä»¤ç‰Œï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜FaceShieldåœ¨å››é¡¹FASä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡æ¨¡å‹å’Œé€šç”¨MLLMï¼ŒåŒ…æ‹¬ç²—ç²’åº¦åˆ†ç±»ã€ç»†ç²’åº¦åˆ†ç±»ã€æ¨ç†å’Œæ”»å‡»å®šä½ã€‚æˆ‘ä»¬çš„æŒ‡ä»¤æ•°æ®é›†ã€åè®®å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Why0912/FaceShield%E4%B8%8A%E5%8F%91%E8%A1%A8%E3%80%82">https://github.com/Why0912/FaceShieldä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09415v2">PDF</a> Accepted by AAAI 2025. Hongyang Wang and Yichen Shi contribute equally. Corresponding author: Zitong Yu</p>
<p><strong>Summary</strong><br>     é¢éƒ¨åæ¬ºè¯ˆæŠ€æœ¯å¯¹äºä¿æŠ¤äººè„¸è¯†åˆ«ç³»ç»Ÿå…å—ä¼ªè£…æ”»å‡»è‡³å…³é‡è¦ã€‚ä¸ºå¼¥è¡¥ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™ä¸€ä»»åŠ¡æ—¶å­˜åœ¨çš„åˆ†ç±»é—®é¢˜å’Œè§£é‡Šæ€§ä¸è¶³ï¼Œæœ¬ç ”ç©¶æå‡ºFaceShieldï¼Œä¸€ç§é€‚ç”¨äºé¢éƒ¨åæ¬ºè¯ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶è¾…ä»¥é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®é›†FaceShield-pre10Kå’ŒFaceShield-sft45Kã€‚FaceShieldå¯åˆ¤æ–­é¢éƒ¨çœŸå®æ€§ã€è¯†åˆ«æ¬ºè¯ˆæ”»å‡»ç±»å‹ã€æä¾›åˆ¤æ–­ä¾æ®å¹¶æ£€æµ‹æ”»å‡»åŒºåŸŸã€‚é€šè¿‡é‡‡ç”¨ç»“åˆåŸå§‹å›¾åƒå’Œå…ˆéªŒçŸ¥è¯†çš„è¾…åŠ©ä¿¡æ¯çš„æ¬ºéª—æ„ŸçŸ¥è§†è§‰æ„ŸçŸ¥æŠ€æœ¯ï¼Œä»¥åŠå¼•å¯¼å¼è§†è§‰ä»¤ç‰Œæ©ç ç­–ç•¥ï¼Œè¯¥æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šæœ‰æ‰€æå‡ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFaceShieldåœ¨å››ä¸ªé¢éƒ¨åæ¬ºè¯ˆä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºä»¥å¾€çš„æ·±åº¦å­¦ä¹ å’Œä¸€èˆ¬çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç›¸å…³èµ„æºå°†å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/Why0912/FaceShield%E3%80%82">https://github.com/Why0912/FaceShieldã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Face anti-spoofing (FAS)æ˜¯ä¿æŠ¤é¢éƒ¨è¯†åˆ«ç³»ç»Ÿå…å—ä¼ªè£…æ”»å‡»çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å¤„ç†FASä»»åŠ¡æ—¶ç¼ºä¹è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºFaceShieldï¼Œä¸€ç§é€‚ç”¨äºé¢éƒ¨åæ¬ºè¯ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯åˆ¤æ–­é¢éƒ¨çœŸå®æ€§ã€è¯†åˆ«æ”»å‡»ç±»å‹ã€æä¾›åˆ¤æ–­ä¾æ®åŠæ£€æµ‹æ”»å‡»åŒºåŸŸã€‚</li>
<li>FaceShieldé‡‡ç”¨æ¬ºéª—æ„ŸçŸ¥è§†è§‰æ„ŸçŸ¥æŠ€æœ¯å’Œå¼•å¯¼å¼è§†è§‰ä»¤ç‰Œæ©ç ç­–ç•¥ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>FaceShieldåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºä»¥å¾€çš„æ·±åº¦å­¦ä¹ å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>FaceShieldåŒ…å«é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®é›†FaceShield-pre10Kå’ŒFaceShield-sft45Kã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0c541bdf572a344bffdab811ee8349b" align="middle">
<img src="https://picx.zhimg.com/v2-f1635ef4ffa38e0fddde6ae326dceff5" align="middle">
<img src="https://picx.zhimg.com/v2-a988a949725e6af94aea8de46279a037" align="middle">
<img src="https://picx.zhimg.com/v2-bfbcbb2b629ff630d62e53dfab3c077a" align="middle">
<img src="https://picx.zhimg.com/v2-5f4b699e68908caa2504a5c81811488d" align="middle">
<img src="https://picx.zhimg.com/v2-d7b3ae5bc696aae221a892b135d698e7" align="middle">
<img src="https://picx.zhimg.com/v2-705b19e94686299e09d824ae12573986" align="middle">
<img src="https://picx.zhimg.com/v2-f723a9cbdef88f1dc17f3512f5214cd1" align="middle">
<img src="https://picx.zhimg.com/v2-2a708ab8c2b2277bec9ded4e4dd69d37" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RAC3-Retrieval-Augmented-Corner-Case-Comprehension-for-Autonomous-Driving-with-Vision-Language-Models"><a href="#RAC3-Retrieval-Augmented-Corner-Case-Comprehension-for-Autonomous-Driving-with-Vision-Language-Models" class="headerlink" title="RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models"></a>RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models</h2><p><strong>Authors:Yujin Wang, Quanfeng Liu, Jiaqi Fan, Jinlong Hong, Hongqing Chu, Mengjian Tian, Bingzhao Gao, Hong Chen</strong></p>
<p>Understanding and addressing corner cases is essential for ensuring the safety and reliability of autonomous driving systems. Vision-language models (VLMs) play a crucial role in enhancing scenario comprehension, yet they face significant challenges, such as hallucination and insufficient real-world grounding, which compromise their performance in critical driving scenarios. In this work, RAC3, a novel framework designed to enhance the performance of VLMs in corner case comprehension, is proposed. RAC3 integrates a frequency-spatial fusion (FSF) image encoder, a cross-modal alignment training method for embedding models with hard and semi-hard negative mining, and a fast querying and retrieval pipeline based on K-Means clustering and hierarchical navigable small world (HNSW) indexing. A multimodal chain-of-thought (CoT) prompting strategy to guide analogical reasoning and reduce hallucinations during inference is introduced. Moreover, an update mechanism is integrated into RAC3 to ensure continual learning within the framework. Extensive experiments on the CODA and nuScenes datasets demonstrate that RAC3 significantly improves corner case comprehension across multiple downstream tasks. Compared to prior state-of-the-art methods, RAC3 achieves the highest final score of 74.46 on the CODA-LM benchmark and shows consistent performance gains when integrated with end-to-end frameworks like DriveLM. These results demonstrate the effectiveness of retrieval-augmented strategies and cross-modal alignment for safer and more interpretable autonomous driving.</p>
<blockquote>
<p>ç†è§£å¹¶å¤„ç†è¾¹ç•Œæƒ…å†µå¯¹äºç¡®ä¿è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æé«˜åœºæ™¯ç†è§£æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å®ƒä»¬é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¦‚å¹»è§‰å’Œç°å®ä¸–ç•Œå®šä½ä¸è¶³ï¼Œè¿™ä¼šå½±å“å®ƒä»¬åœ¨å…³é”®é©¾é©¶åœºæ™¯ä¸­çš„è¡¨ç°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæå‡ºäº†RAC3è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜VLMsåœ¨è¾¹ç•Œæ¡ˆä¾‹ç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚RAC3é›†æˆäº†é¢‘ç‡ç©ºé—´èåˆï¼ˆFSFï¼‰å›¾åƒç¼–ç å™¨ã€ç”¨äºåµŒå…¥æ¨¡å‹çš„è·¨æ¨¡æ€å¯¹é½è®­ç»ƒæ–¹æ³•å’Œç¡¬è´Ÿæ ·æœ¬åŠåŠç¡¬è´Ÿæ ·æœ¬æŒ–æ˜ã€åŸºäºK-Meansèšç±»å’Œåˆ†å±‚å¯å¯¼èˆªå°å‹ä¸–ç•Œï¼ˆHNSWï¼‰ç´¢å¼•çš„å¿«é€ŸæŸ¥è¯¢å’Œæ£€ç´¢ç®¡é“ã€‚è¿˜å¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºç­–ç•¥ï¼Œä»¥å¼•å¯¼ç±»æ¯”æ¨ç†å¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡å°‘å¹»è§‰ã€‚æ­¤å¤–ï¼ŒRAC3è¿˜é›†æˆäº†æ›´æ–°æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ¡†æ¶å†…çš„æŒç»­å­¦ä¹ ã€‚åœ¨CODAå’ŒnuScenesæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRAC3åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†è¾¹ç•Œæ¡ˆä¾‹çš„ç†è§£èƒ½åŠ›ã€‚ä¸ä¹‹å‰çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒRAC3åœ¨CODA-LMåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€é«˜åˆ†74.46åˆ†ï¼Œåœ¨ä¸ç«¯åˆ°ç«¯æ¡†æ¶ï¼ˆå¦‚DriveLMï¼‰é›†æˆæ—¶ï¼Œè¡¨ç°å‡ºä¸€è´¯çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœè¯æ˜äº†æ£€ç´¢å¢å¼ºç­–ç•¥å’Œè·¨æ¨¡æ€å¯¹é½åœ¨æ›´å®‰å…¨ã€æ›´å¯è§£é‡Šçš„è‡ªåŠ¨é©¾é©¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11050v4">PDF</a> Accepted by IEEE Transactions on Multimedia</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒç†è§£å’Œå¤„ç†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è¾¹ç¼˜æ¡ˆä¾‹çš„é‡è¦æ€§ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æé«˜åœºæ™¯ç†è§£æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œä½†å®ƒä»¬é¢ä¸´ç€è¯¸å¦‚å¹»è§‰å’Œç¼ºä¹ç°å®ä¸–ç•ŒåŸºç¡€ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†RAC3æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜VLMsåœ¨è¾¹ç¼˜æ¡ˆä¾‹ç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚RAC3èåˆäº†é¢‘ç‡ç©ºé—´èåˆå›¾åƒç¼–ç å™¨ã€ç”¨äºåµŒå…¥æ¨¡å‹çš„è·¨æ¨¡æ€å¯¹é½è®­ç»ƒæ–¹æ³•å’ŒåŸºäºK-Meansèšç±»å’Œåˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œç´¢å¼•çš„å¿«é€ŸæŸ¥è¯¢å’Œæ£€ç´¢ç®¡é“ã€‚è¿˜å¼•å…¥äº†å¤šæ¨¡æ€æ€ç»´é“¾æç¤ºç­–ç•¥ï¼Œä»¥å¼•å¯¼ç±»æ¯”æ¨ç†å¹¶å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å¹»è§‰ã€‚RAC3è¿˜é›†æˆäº†æ›´æ–°æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ¡†æ¶å†…çš„æŒç»­å­¦ä¹ ã€‚åœ¨CODAå’ŒnuScenesæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRAC3æ˜¾è‘—æé«˜äº†è¾¹ç¼˜æ¡ˆä¾‹ç†è§£çš„æ€§èƒ½ï¼Œå¹¶åœ¨CODA-LMåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€é«˜åˆ†74.46åˆ†ã€‚ä¸å…ˆå‰çš„å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒRAC3åœ¨ä¸ç«¯åˆ°ç«¯æ¡†æ¶ï¼ˆå¦‚DriveLMï¼‰é›†æˆæ—¶è¡¨ç°å‡ºæŒç»­çš„æ€§èƒ½æå‡ã€‚ç»“æœè¡¨æ˜ï¼Œæ£€ç´¢å¢å¼ºç­–ç•¥å’Œè·¨æ¨¡æ€å¯¹é½å¯¹äºæ›´å®‰å…¨ã€æ›´å¯è§£é‡Šçš„è‡ªåŠ¨é©¾é©¶æ›´ä¸ºæœ‰æ•ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç†è§£å’Œå¤„ç†è¾¹ç¼˜æ¡ˆä¾‹å¯¹äºç¡®ä¿è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æé«˜è‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£æ–¹é¢å‘æŒ¥å…³é”®ä½œç”¨ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>RAC3æ¡†æ¶é€šè¿‡èåˆå¤šç§æŠ€æœ¯æé«˜äº†VLMsåœ¨è¾¹ç¼˜æ¡ˆä¾‹ç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>RAC3é‡‡ç”¨äº†é¢‘ç‡ç©ºé—´èåˆå›¾åƒç¼–ç å™¨ã€è·¨æ¨¡æ€å¯¹é½è®­ç»ƒæ–¹æ³•å’Œå¿«é€ŸæŸ¥è¯¢æ£€ç´¢ç®¡é“ã€‚</li>
<li>å¤šæ¨¡æ€æ€ç»´é“¾æç¤ºç­–ç•¥å¼•å¯¼ç±»æ¯”æ¨ç†ï¼Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å¹»è§‰ã€‚</li>
<li>RAC3çš„æ›´æ–°æœºåˆ¶ç¡®ä¿äº†æ¡†æ¶å†…çš„æŒç»­å­¦ä¹ ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAC3æ˜¾è‘—æé«˜äº†è¾¹ç¼˜æ¡ˆä¾‹ç†è§£çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0326b4e03322248c4484d9cf3ec98b46" align="middle">
<img src="https://picx.zhimg.com/v2-bca4cc0a97c92983d222cb697209bf0f" align="middle">
<img src="https://picx.zhimg.com/v2-53aa166790c71ae628d6b15c93072754" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ComFe-An-Interpretable-Head-for-Vision-Transformers"><a href="#ComFe-An-Interpretable-Head-for-Vision-Transformers" class="headerlink" title="ComFe: An Interpretable Head for Vision Transformers"></a>ComFe: An Interpretable Head for Vision Transformers</h2><p><strong>Authors:Evelyn J. Mannix, Liam Hodgkinson, Howard Bondell</strong></p>
<p>Interpretable computer vision models explain their classifications through comparing the distances between the local embeddings of an image and a set of prototypes that represent the training data. However, these approaches introduce additional hyper-parameters that need to be tuned to apply to new datasets, scale poorly, and are more computationally intensive to train in comparison to black-box approaches. In this work, we introduce Component Features (ComFe), a highly scalable interpretable-by-design image classification head for pretrained Vision Transformers (ViTs) that can obtain competitive performance in comparison to comparable non-interpretable methods. To our knowledge, ComFe is the first interpretable head and unlike other interpretable approaches can be readily applied to large-scale datasets such as ImageNet-1K. Additionally, ComFe provides improved robustness and outperforms previous interpretable approaches on key benchmark datasets while using a consistent set of hyperparameters and without finetuning the pretrained ViT backbone. With only global image labels and no segmentation or part annotations, ComFe can identify consistent component features within an image and determine which of these features are informative in making a prediction. Code is available at github.com&#x2F;emannix&#x2F;comfe-component-features.</p>
<blockquote>
<p>è§£é‡Šæ€§è®¡ç®—æœºè§†è§‰æ¨¡å‹é€šè¿‡æ¯”è¾ƒå›¾åƒå±€éƒ¨åµŒå…¥ä¸ä»£è¡¨è®­ç»ƒæ•°æ®çš„ä¸€ç»„åŸå‹ä¹‹é—´çš„è·ç¦»æ¥è§£é‡Šå…¶åˆ†ç±»ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¼•å…¥äº†é¢å¤–çš„è¶…å‚æ•°ï¼Œéœ€è¦é’ˆå¯¹æ–°æ•°æ®é›†è¿›è¡Œè°ƒæ•´ï¼Œæ‰©å±•æ€§å·®ï¼Œä¸é»‘ç›’æ–¹æ³•ç›¸æ¯”è®¡ç®—è®­ç»ƒæ›´ä¸ºå¯†é›†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»„ä»¶ç‰¹å¾ï¼ˆComFeï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºé¢„è®­ç»ƒçš„è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰è®¾è®¡çš„å¯è§£é‡Šæ€§å›¾åƒåˆ†ç±»å¤´ï¼Œå…·æœ‰é«˜åº¦çš„å¯æ‰©å±•æ€§ï¼Œåœ¨ä¸å…¶ä»–ç±»ä¼¼çš„éè§£é‡Šæ€§æ–¹æ³•ç›¸æ¯”æ—¶å¯ä»¥è·å¾—ç«äº‰æ€§çš„æ€§èƒ½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒComFeæ˜¯ç¬¬ä¸€ä¸ªå¯è§£é‡Šçš„å¤´éƒ¨ï¼Œä¸åŒäºå…¶ä»–å¯è§£é‡Šçš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥è½»æ¾åº”ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¦‚ImageNet-1Kã€‚æ­¤å¤–ï¼ŒComFeæä¾›äº†æ”¹è¿›çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨å…³é”®åŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºä¹‹å‰çš„å¯è§£é‡Šæ–¹æ³•ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€ç»„ä¸€è‡´çš„è¶…å‚æ•°ä¸”æ— éœ€å¾®è°ƒé¢„è®­ç»ƒçš„ViTä¸»å¹²ã€‚ä»…ä½¿ç”¨å…¨å±€å›¾åƒæ ‡ç­¾ï¼Œæ— éœ€åˆ†å‰²æˆ–éƒ¨åˆ†æ³¨é‡Šï¼ŒComFeå¯ä»¥è¯†åˆ«å›¾åƒå†…çš„ä¸€è‡´ç»„ä»¶ç‰¹å¾ï¼Œå¹¶ç¡®å®šå“ªäº›ç‰¹å¾åœ¨åšå‡ºé¢„æµ‹æ—¶æ˜¯æœ‰ç”¨çš„ã€‚ä»£ç å¯åœ¨github.com&#x2F;emannix&#x2F;comfe-component-featuresæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.04125v6">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºå±€éƒ¨åµŒå…¥è·ç¦»æ¯”è¾ƒçš„åˆ†ç±»è§£é‡Šæ¨¡å‹é€šè¿‡æ¯”è¾ƒå›¾åƒå±€éƒ¨åµŒå…¥ä¸è®­ç»ƒæ•°æ®é›†çš„åŸå‹é›†ä¹‹é—´çš„è·ç¦»æ¥è§£é‡Šå…¶åˆ†ç±»ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¼•å…¥é¢å¤–çš„è¶…å‚æ•°ï¼Œéœ€è¦é’ˆå¯¹æ–°æ•°æ®é›†è¿›è¡Œè°ƒæ•´ï¼Œæ‰©å±•æ€§å·®ï¼Œå¹¶ä¸”ç›¸è¾ƒäºé»‘ç›’æ–¹æ³•è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»„ä»¶ç‰¹å¾ï¼ˆComFeï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºé¢„è®­ç»ƒçš„è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰è®¾è®¡çš„å¯è§„æ¨¡åŒ–ä¸”è®¾è®¡ä¸ºå¯è§£é‡Šçš„å›¾åƒåˆ†ç±»å¤´éƒ¨ï¼Œä¸ç›¸åº”çš„éè§£é‡Šæ€§æ–¹æ³•ç›¸æ¯”ï¼Œå¯ä»¥è·å¾—æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ã€‚æ®äº†è§£ï¼ŒComFeæ˜¯é¦–ä¸ªå¯è§£é‡Šçš„å¤´éƒ¨ï¼Œä¸åŒäºå…¶ä»–å¯è§£é‡Šæ–¹æ³•çš„æ˜¯å®ƒå¯ä»¥è½»æ¾åº”ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†å¦‚ImageNet-1Kã€‚æ­¤å¤–ï¼ŒComFeæä¾›äº†æ›´é«˜çš„ç¨³å¥æ€§ï¼Œåœ¨å…³é”®åŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºä¹‹å‰çš„è§£é‡Šæ–¹æ³•ï¼Œå¹¶ä¸”ä½¿ç”¨äº†ä¸€ç»„å›ºå®šçš„è¶…å‚æ•°ä¸”æœªå¯¹é¢„è®­ç»ƒçš„ViTä¸»å¹²è¿›è¡Œå¾®è°ƒã€‚ä»…å‡­å…¨å±€å›¾åƒæ ‡ç­¾ä¸”æ— éœ€åˆ†æ®µæˆ–éƒ¨åˆ†æ³¨é‡Šï¼ŒComFeå°±å¯ä»¥åœ¨å›¾åƒä¸­è¯†åˆ«å‡ºä¸€è‡´ç»„ä»¶ç‰¹å¾å¹¶ç¡®å®šå“ªäº›ç‰¹å¾å¯¹äºåšå‡ºé¢„æµ‹æ˜¯æœ‰æ„ä¹‰çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£é‡Šæ€§è®¡ç®—æœºè§†è§‰æ¨¡å‹é€šè¿‡æ¯”è¾ƒå›¾åƒå±€éƒ¨åµŒå…¥ä¸è®­ç»ƒæ•°æ®åŸå‹ä¹‹é—´çš„è·ç¦»æ¥è§£é‡Šåˆ†ç±»ç»“æœã€‚</li>
<li>å½“å‰æ–¹æ³•å¼•å…¥è¶…å‚æ•°å¹¶éœ€è¦é’ˆå¯¹æ–°æ•°æ®é›†è¿›è¡Œè°ƒæ•´ï¼Œå­˜åœ¨æ‰©å±•æ€§å·®å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚</li>
<li>ComFeä½œä¸ºä¸€ç§ä¸ºé¢„è®­ç»ƒè§†è§‰å˜å‹å™¨è®¾è®¡çš„å¯è§£é‡Šå›¾åƒåˆ†ç±»å¤´éƒ¨ï¼Œå…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>ComFeæ˜¯é¦–ä¸ªå¯è§£é‡Šçš„å¤´éƒ¨ï¼Œå¯è½»æ¾åº”ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†å¦‚ImageNet-1Kã€‚</li>
<li>ComFeæé«˜äº†ç¨³å¥æ€§ï¼Œå¹¶åœ¨å…³é”®åŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–è§£é‡Šæ–¹æ³•ã€‚</li>
<li>ComFeä½¿ç”¨å›ºå®šè¶…å‚æ•°ï¼Œæ— éœ€å¾®è°ƒé¢„è®­ç»ƒè§†è§‰å˜å‹å™¨ä¸»å¹²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.04125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-733c09ea239296051fb8d96a75a61119" align="middle">
<img src="https://picx.zhimg.com/v2-c69a519735b36c009d8d2b08e392b982" align="middle">
<img src="https://picx.zhimg.com/v2-46a1bd80daeeca4010b42eff02da10bd" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-36e8afa508b37d5ae96dc1d0a25e45b0" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-676b1004d755069f28ec465bf0bbf6de" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  CacheFlow Compressive Streaming Memory for Efficient Long-Form Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
