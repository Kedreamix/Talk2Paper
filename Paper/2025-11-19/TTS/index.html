<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  FoleyBench A Benchmark For Video-to-Audio Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4fbf6eae5c80051fc5966dffb97d4896')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="FoleyBench-A-Benchmark-For-Video-to-Audio-Models"><a href="#FoleyBench-A-Benchmark-For-Video-to-Audio-Models" class="headerlink" title="FoleyBench: A Benchmark For Video-to-Audio Models"></a>FoleyBench: A Benchmark For Video-to-Audio Models</h2><p><strong>Authors:Satvik Dixit, Koichi Saito, Zhi Zhong, Yuki Mitsufuji, Chris Donahue</strong></p>
<p>Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR&#x2F;VR, and sound design, particularly for the creation of Foley sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music, domains that lie outside the use case for Foley. To address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS&#x2F;AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark several state-of-the-art V2A models, evaluating them on audio quality, audio-video alignment, temporal synchronization, and audio-text consistency. Samples are available at: <a target="_blank" rel="noopener" href="https://gclef-cmu.org/foleybench">https://gclef-cmu.org/foleybench</a></p>
<blockquote>
<p>è§†é¢‘è½¬éŸ³é¢‘ç”Ÿæˆï¼ˆV2Aï¼‰åœ¨ç”µå½±åæœŸåˆ¶ä½œã€AR&#x2F;VRå’Œå£°éŸ³è®¾è®¡ç­‰é¢†åŸŸä¸­çš„é‡è¦æ€§æ—¥ç›Šå¢åŠ ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ›å»ºä¸å±å¹•åŠ¨ä½œåŒæ­¥çš„ç¦è±éŸ³æ•ˆæ•ˆæœæ–¹é¢ã€‚ç¦è±éŸ³æ•ˆéœ€è¦ç”Ÿæˆä¸å¯è§äº‹ä»¶è¯­ä¹‰ä¸Šå¯¹é½å¹¶ä¸”ä¸å…¶æ—¶é—´å¯¹é½çš„éŸ³é¢‘ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é’ˆå¯¹ç¦è±é£æ ¼åœºæ™¯çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°ä¸ä¸‹æ¸¸åº”ç”¨ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ã€‚æˆ‘ä»¬å‘ç°è¿‡å»è¯„ä¼°æ•°æ®é›†ä¸­çš„74%çš„è§†é¢‘å­˜åœ¨éŸ³è§†é¢‘å¯¹åº”ä¸è‰¯çš„æƒ…å†µã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¸»è¦ä»¥è¯­éŸ³å’ŒéŸ³ä¹ä¸ºä¸»ï¼Œè¿™äº›é¢†åŸŸå¹¶ä¸é€‚ç”¨äºç¦è±éŸ³æ•ˆçš„ä½¿ç”¨æ¡ˆä¾‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FoleyBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºç¦è±é£æ ¼V2Aè¯„ä¼°è®¾è®¡çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚FoleyBenchåŒ…å«5000ä¸ªï¼ˆè§†é¢‘ã€åœ°é¢çœŸå®éŸ³é¢‘ã€æ–‡æœ¬å­—å¹•ï¼‰ä¸‰å…ƒç»„ï¼Œæ¯ä¸ªä¸‰å…ƒç»„éƒ½å¸¦æœ‰ä¸å±å¹•äº‹ä»¶æœ‰å› æœå…³ç³»çš„å¯è§å£°éŸ³æºéŸ³é¢‘ã€‚è¯¥æ•°æ®é›†æ˜¯ä½¿ç”¨åº”ç”¨äºYouTubeå’ŒVimeoç­‰æ¥æºçš„é‡å¤–äº’è”ç½‘è§†é¢‘çš„è‡ªåŠ¨åŒ–å¯æ‰©å±•ç®¡é“æ„å»ºçš„ã€‚ä¸è¿‡å»çš„æ•°æ®é›†ç›¸æ¯”ï¼Œæˆ‘ä»¬è¯æ˜äº†FoleyBenchçš„è§†é¢‘å¯¹ç¦è±éŸ³æ•ˆä¸“é—¨è®¾è®¡çš„åˆ†ç±»æ³•ä¸­çš„å£°éŸ³ç±»åˆ«å…·æœ‰æ›´å¼ºçš„è¦†ç›–æ€§ã€‚æ¯ä¸ªç‰‡æ®µéƒ½è¿›ä¸€æ­¥ç”¨å…ƒæ•°æ®æ ‡è®°ï¼Œæ•æ‰æºå¤æ‚æ€§ã€UCS&#x2F;AudioSetç±»åˆ«å’Œè§†é¢‘é•¿åº¦ï¼Œå®ç°å¯¹æ¨¡å‹æ€§èƒ½å’Œå¤±è´¥æ¨¡å¼çš„ç²¾ç»†åˆ†æã€‚æˆ‘ä»¬å¯¹ä¸€äº›æœ€å…ˆè¿›çš„V2Aæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å®ƒä»¬çš„éŸ³è´¨ã€éŸ³è§†é¢‘å¯¹é½ã€æ—¶é—´åŒæ­¥å’ŒéŸ³é¢‘æ–‡æœ¬ä¸€è‡´æ€§ã€‚æ ·æœ¬å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://gclef-cmu.org/foleybench%E6%89%BE%E5%88%B0%E3%80%82">https://gclef-cmu.org/foleybenchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13219v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>V2Aï¼ˆè§†é¢‘è½¬éŸ³é¢‘ç”Ÿæˆï¼‰åœ¨å½±è§†åæœŸåˆ¶ä½œã€AR&#x2F;VRå’Œå£°éŸ³è®¾è®¡ç­‰é¢†åŸŸä¸­è¶Šæ¥è¶Šé‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ›å»ºä¸å±å¹•åŠ¨ä½œåŒæ­¥çš„éŸ³æ•ˆï¼ˆå¦‚FoleyéŸ³æ•ˆï¼‰æ–¹é¢ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é’ˆå¯¹Foleyåœºæ™¯çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å’Œä¸‹æ¸¸åº”ç”¨ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ã€‚æˆ‘ä»¬å‘ç°è¿‡å»è¯„ä¼°æ•°æ®é›†ä¸­æœ‰74%çš„è§†é¢‘å­˜åœ¨éŸ³è§†é¢‘å¯¹åº”ä¸è‰¯çš„é—®é¢˜ï¼Œä¸”å®ƒä»¬ä¸»è¦é›†ä¸­åœ¨è¯­éŸ³å’ŒéŸ³ä¹é¢†åŸŸï¼Œå¹¶ä¸é€‚ç”¨äºFoleyã€‚ä¸ºè§£å†³æ­¤ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºFoleyé£æ ¼V2Aè¯„ä¼°è®¾è®¡çš„é¦–ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•â€”â€”FoleyBenchã€‚FoleyBenchåŒ…å«5000ä¸ªï¼ˆè§†é¢‘ã€åœ°é¢çœŸå®éŸ³é¢‘ã€æ–‡æœ¬å­—å¹•ï¼‰ä¸‰å…ƒç»„ï¼Œæ¯ä¸ªä¸‰å…ƒç»„éƒ½å…·æœ‰ä¸å±å¹•äº‹ä»¶å› æœç›¸å…³çš„å¯è§éŸ³æºã€‚æ•°æ®é›†æ˜¯ä½¿ç”¨åº”ç”¨äºYouTubeå’ŒVimeoç­‰ç½‘ç»œè§†é¢‘çš„è‡ªåŠ¨åŒ–å¯æ‰©å±•ç®¡é“æ„å»ºçš„ã€‚ä¸è¿‡å»çš„æ•°æ®é›†ç›¸æ¯”ï¼ŒFoleyBenchçš„è§†é¢‘å…·æœ‰æ›´å¹¿æ³›çš„ä»ä¸“ä¸ºFoleyè®¾è®¡çš„åˆ†ç±»æ³•ä¸­çš„å£°éŸ³ç±»åˆ«è¦†ç›–ã€‚æ¯ä¸ªå‰ªè¾‘è¿˜å¸¦æœ‰å…ƒæ•°æ®æ ‡ç­¾ï¼ŒåŒ…æ‹¬æ¥æºå¤æ‚æ€§ã€UCS&#x2F;AudioSetç±»åˆ«å’Œè§†é¢‘é•¿åº¦ç­‰ï¼Œä»è€Œå®ç°æ¨¡å‹æ€§èƒ½å’Œå¤±è´¥æ¨¡å¼çš„ç²¾ç»†åˆ†æã€‚æˆ‘ä»¬å¯¹å‡ ä¸ªæœ€å…ˆè¿›çš„V2Aæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å®ƒä»¬åœ¨éŸ³é¢‘è´¨é‡ã€éŸ³è§†é¢‘å¯¹é½ã€æ—¶é—´åŒæ­¥å’ŒéŸ³é¢‘æ–‡æœ¬ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚æ ·å“å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://gclef-cmu.org/foleybench%E8%8E%B7%E5%8F%96%E3%80%82">https://gclef-cmu.org/foleybenchè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘è½¬éŸ³é¢‘ç”Ÿæˆï¼ˆV2Aï¼‰åœ¨å¤šä¸ªé¢†åŸŸçš„é‡è¦æ€§æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ›å»ºä¸å±å¹•åŠ¨ä½œåŒæ­¥çš„éŸ³æ•ˆæ–¹é¢ã€‚</li>
<li>å½“å‰è¯„ä¼°æ•°æ®é›†å­˜åœ¨éŸ³è§†é¢‘å¯¹åº”ä¸è‰¯çš„é—®é¢˜ï¼Œä¸”å¤šæ•°è§†é¢‘é›†ä¸­åœ¨éFoleyé¢†åŸŸï¼ˆå¦‚è¯­éŸ³å’ŒéŸ³ä¹ï¼‰ã€‚</li>
<li>å¼•å…¥FoleyBenchï¼šé¦–ä¸ªä¸“ä¸ºFoleyé£æ ¼V2Aè¯„ä¼°è®¾è®¡çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ã€‚</li>
<li>FoleyBenchåŒ…å«å¤§é‡è§†é¢‘ã€åœ°é¢çœŸå®éŸ³é¢‘å’Œæ–‡æœ¬å­—å¹•ä¸‰å…ƒç»„ï¼Œå¼ºè°ƒéŸ³è§†é¢‘å¯¹åº”å’Œæ–‡æœ¬ä¸€è‡´æ€§ã€‚</li>
<li>æ•°æ®é›†ä½¿ç”¨è‡ªåŠ¨åŒ–å¯æ‰©å±•ç®¡é“æ„å»ºï¼Œé€‚ç”¨äºç½‘ç»œè§†é¢‘ã€‚</li>
<li>FoleyBenchè§†é¢‘è¦†ç›–å¹¿æ³›çš„FoleyéŸ³æ•ˆåˆ†ç±»ï¼Œå¹¶å¸¦æœ‰å…ƒæ•°æ®æ ‡ç­¾è¿›è¡Œç²¾ç»†åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d289dd568c43b56beb36896dbec6c16" align="middle">
<img src="https://picx.zhimg.com/v2-835340d96be1176f9e92ecb935065765" align="middle">
<img src="https://picx.zhimg.com/v2-e77b1d56fcd70a47731b6e67216388a7" align="middle">
<img src="https://picx.zhimg.com/v2-6e544bc5d4c507cae0936bb3cbd755e8" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Distinguishing-Repetition-Disfluency-from-Morphological-Reduplication-in-Bangla-ASR-Transcripts-A-Novel-Corpus-and-Benchmarking-Analysis"><a href="#Distinguishing-Repetition-Disfluency-from-Morphological-Reduplication-in-Bangla-ASR-Transcripts-A-Novel-Corpus-and-Benchmarking-Analysis" class="headerlink" title="Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis"></a>Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis</h2><p><strong>Authors:Zaara Zabeen Arpa, Sadnam Sakib Apurbo, Nazia Karim Khan Oishee, Ajwad Abrar</strong></p>
<p>Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error&#x2F;hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.</p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ï¼Œç‰¹åˆ«æ˜¯åœ¨å­ŸåŠ æ‹‰è¯­ç­‰ä½èµ„æºè¯­è¨€ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªå…³é”®çš„æ¨¡ç³Šæ€§ï¼šå•è¯é‡å¤å¯èƒ½æ˜¯é‡å¤ä¸æµç•…ï¼ˆæ— æ„çš„ASRé”™è¯¯&#x2F;çŠ¹è±«ï¼‰æˆ–å½¢æ€å¤ç°ï¼ˆæ•…æ„çš„è¯­æ³•ç»“æ„ï¼‰ã€‚æ ‡å‡†çš„æµç•…æ€§ä¿®æ­£ä¼šé”™è¯¯åœ°åˆ é™¤æœ‰æ•ˆçš„è¯­è¨€ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„ã€åŒ…å«2ä¸‡è¡Œçš„å­ŸåŠ æ‹‰è¯­è¯­æ–™åº“ï¼Œé€šè¿‡æ‰‹åŠ¨æ³¨é‡Šæ¥æ˜ç¡®åŒºåˆ†è¿™ä¸¤ç§ç°è±¡åœ¨å˜ˆæ‚çš„ASRè½¬å½•ä¸­çš„åŒºåˆ«ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ç§èŒƒå¼æ¥è¯„ä¼°è¿™ä¸€æ–°é¢–èµ„æºï¼šæœ€å…ˆè¿›çš„è·¨è¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç¼–ç å™¨æ¨¡å‹å¾®è°ƒã€‚LLMé€šè¿‡å°‘é‡æç¤ºå³å¯å®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ˆæœ€é«˜è¾¾82.68%çš„å‡†ç¡®ç‡ï¼‰ã€‚ç„¶è€Œï¼Œå¾®è°ƒè¯æ˜æ›´ä¸ºä¼˜è¶Šï¼Œé’ˆå¯¹å­ŸåŠ æ‹‰è¯­çš„ç‰¹å®šè¯­è¨€æ¨¡å‹BanglaBERTè¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡84.78%ï¼ŒF1åˆ†æ•°ä¸º0.677ã€‚è¿™ä¸ºå­ŸåŠ æ‹‰è¯­çš„å¤æ‚ã€è¯­ä¹‰ä¿ç•™æ–‡æœ¬å½’ä¸€åŒ–ç³»ç»Ÿçš„å‘å±•å»ºç«‹äº†å¼ºå¤§ä¸”è¯­è¨€ä¿¡æ¯ä¸°å¯Œçš„åŸºçº¿ï¼Œå¹¶ä¸ºå¼€å‘æ­¤ç±»ç³»ç»Ÿæä¾›äº†å¿…è¦çš„æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13159v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹å­ŸåŠ æ‹‰è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ä¸­çš„é‡å¤è¯ç°è±¡ï¼ŒåŒºåˆ†é‡å¤è¯æ˜¯é‡å¤å¤±è¯¯è¿˜æ˜¯è¯­æ³•ç»“æ„ä¸­çš„å½¢æ€é‡å¤çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„ã€æ‰‹åŠ¨æ ‡æ³¨çš„å­ŸåŠ æ‹‰è¯­è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“åŒ…å«äºŒåä¸‡è¡Œæ•°æ®ã€‚é€šè¿‡é‡‡ç”¨æœ€æ–°å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹å’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç¼–ç å™¨æ¨¡å‹å¾®è°ƒï¼Œè¯¥è¯­æ–™åº“çš„åŸºå‡†æµ‹è¯•è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚å…¶ä¸­ï¼Œè¯­è¨€ç‰¹å®šçš„å­ŸåŠ æ‹‰BERTæ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œå‡†ç¡®ç‡è¾¾åˆ°ç™¾åˆ†ä¹‹å…«åå››ç‚¹ä¸ƒåå…«ï¼ŒF1åˆ†æ•°è¾¾åˆ°é›¶ç‚¹å…­ä¸ƒä¸ƒã€‚è¿™ä¸ºå‘å±•å¤æ‚ä¸”è¯­ä¹‰ä¿ç•™çš„å­ŸåŠ æ‹‰è¯­æ–‡æœ¬è§„èŒƒåŒ–ç³»ç»Ÿæä¾›äº†é‡è¦æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•åœ¨å­ŸåŠ æ‹‰è¯­ç­‰ä½èµ„æºè¯­è¨€ä¸­å­˜åœ¨å…³é”®æ¨¡ç³Šæ€§ï¼šè¯è¯é‡å¤å¯èƒ½æ˜¯é‡å¤å¤±è¯¯ï¼ˆASRè¯¯å·®&#x2F;çŠ¹è±«ï¼‰æˆ–å½¢æ€é‡å¤ï¼ˆæœ‰æ„ä¸ºä¹‹çš„è¯­æ³•ç»“æ„ï¼‰ã€‚</li>
<li>æ ‡å‡†æµç•…æ€§æ ¡æ­£æ–¹æ³•ä¼šé”™è¯¯åœ°åˆ é™¤æœ‰æ•ˆçš„è¯­è¨€ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„ã€é’ˆå¯¹ASRè½¬å½•ä¸­ä¸¤ç§ç°è±¡è¿›è¡ŒåŒºåˆ†çš„å­ŸåŠ æ‹‰è¯­è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“åŒ…å«äºŒåä¸‡è¡Œæ•°æ®å¹¶è¿›è¡Œäº†æ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨æœ€æ–°å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹å’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç¼–ç å™¨æ¨¡å‹å¾®è°ƒï¼Œå¯¹è¯¥è¯­æ–™åº“è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ–¹æ³•è¡¨ç°æ›´ä½³ã€‚å…¶ä¸­ï¼Œå­ŸåŠ æ‹‰BERTæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°å‡è¾ƒé«˜ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¼€å‘å¤æ‚ä¸”è¯­ä¹‰ä¿ç•™çš„å­ŸåŠ æ‹‰è¯­æ–‡æœ¬è§„èŒƒåŒ–ç³»ç»Ÿæä¾›äº†é‡è¦æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f05e27973a12c4809011c2d3697f7355" align="middle">
<img src="https://picx.zhimg.com/v2-249c99f5253d4ee7055be1797731e772" align="middle">
<img src="https://picx.zhimg.com/v2-ba62b6f715374202c816b0bc54650953" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Direct-Persian-English-Speech-to-Speech-Translation-with-Discrete-Units-and-Synthetic-Parallel-Data"><a href="#Improving-Direct-Persian-English-Speech-to-Speech-Translation-with-Discrete-Units-and-Synthetic-Parallel-Data" class="headerlink" title="Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data"></a>Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data</h2><p><strong>Authors:Sina Rashidi, Hossein Sameti</strong></p>
<p>Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English</p>
<blockquote>
<p>ç›´æ¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆS2STï¼‰ï¼Œæ‰€æœ‰ç»„ä»¶éƒ½è¿›è¡Œè”åˆè®­ç»ƒï¼Œä½œä¸ºçº§è”ç³»ç»Ÿçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› å…¶æ›´ç®€å•çš„ç®¡é“å’Œæ›´ä½çš„æ¨ç†å»¶è¿Ÿè€Œå…·æœ‰å¸å¼•åŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥S2STæ¨¡å‹éœ€è¦å¤§é‡æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„å¹¶è¡Œè¯­éŸ³æ•°æ®ï¼Œå¯¹äºæ³¢æ–¯è¯­ç­‰ä½èµ„æºè¯­è¨€æ¥è¯´ï¼Œè¿™ç§æ•°æ®å¾ˆå°‘å¯ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†æ³¢æ–¯è¯­è¯­éŸ³ç¿»è¯‘æˆè‹±è¯­è¯­éŸ³çš„ç›´æ¥S2STç³»ç»Ÿï¼Œä»¥åŠä¸€ä¸ªç”¨äºç”Ÿæˆæ³¢æ–¯è¯­-è‹±è¯­å¹¶è¡Œè¯­éŸ³çš„åˆæˆç®¡é“ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šï¼ˆ1ï¼‰åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„ç¼–ç å™¨ï¼Œé€šè¿‡è‡ªç›‘ç£é¢„è®­ç»ƒè¿›è¡Œåˆå§‹åŒ–ï¼Œå°†æºè¯­éŸ³æ˜ å°„åˆ°é«˜çº§å£°å­¦è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰å¸¦æœ‰ç›¸å¯¹ä½ç½®å¤šå¤´æ³¨æ„åŠ›çš„å› æœå˜å‹å™¨è§£ç å™¨ï¼Œå°†è¿™äº›è¡¨ç¤ºç¿»è¯‘æˆç¦»æ•£çš„ç›®æ ‡è¯­éŸ³å•å…ƒï¼›ï¼ˆ3ï¼‰åŸºäºå•å…ƒçš„ç¥ç»vocoderæ ¹æ®é¢„æµ‹çš„ç¦»æ•£å•å…ƒç”Ÿæˆæ³¢å½¢ã€‚ä¸ºäº†ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ³¢æ–¯è¯­-è‹±è¯­å¹¶è¡Œè¯­éŸ³è¯­æ–™åº“ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å°†æ³¢æ–¯è¯­è¯­éŸ³è½¬å½•ç¿»è¯‘æˆè‹±è¯­ï¼Œç„¶åä½¿ç”¨æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿåˆæˆç›¸åº”çš„è‹±è¯­è¯­éŸ³ã€‚ç”±æ­¤äº§ç”Ÿçš„è¯­æ–™åº“ä½¿å¯ç”¨çš„å¹¶è¡Œè¯­éŸ³æ•°é‡å¢åŠ äº†å¤§çº¦å…­å€ã€‚åœ¨CVSSè¯­æ–™åº“çš„æ³¢æ–¯è¯­-è‹±è¯­éƒ¨åˆ†ä¸Šï¼Œä¸ç›´æ¥åŸºçº¿ç›¸æ¯”ï¼Œä½¿ç”¨åˆæˆæ•°æ®åï¼Œæ‰€æå‡ºæ¨¡å‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«BLEUå¾—åˆ†æé«˜äº†4.6ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç»“åˆè‡ªç›‘ç£é¢„è®­ç»ƒã€ç¦»æ•£è¯­éŸ³å•å…ƒå’Œåˆæˆå¹¶è¡Œæ•°æ®å¯¹äºæ”¹è¿›ä½èµ„æºè¯­è¨€å¯¹ï¼ˆå¦‚æ³¢æ–¯è¯­-è‹±è¯­ï¼‰çš„ç›´æ¥S2STéå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12690v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§æ³¢æ–¯è¯­åˆ°è‹±è¯­çš„ç›´æ¥è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ç³»ç»Ÿï¼ˆS2STï¼‰ï¼Œå¹¶æ„å»ºåˆæˆå¹³è¡Œè¯­æ–™åº“ä»¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚é€šè¿‡è‡ªç›‘ç£é¢„è®­ç»ƒã€ç¦»æ•£è¯­éŸ³å•å…ƒå’Œåˆæˆå¹³è¡Œæ•°æ®ç›¸ç»“åˆï¼Œæé«˜ä½èµ„æºè¯­è¨€å¯¹ï¼ˆå¦‚æ³¢æ–¯è¯­-è‹±è¯­ï¼‰çš„ç›´æ¥S2STæ€§èƒ½ï¼Œåœ¨CVSSè¯­æ–™åº“çš„æ³¢æ–¯è¯­-è‹±è¯­éƒ¨åˆ†ï¼Œä½¿ç”¨åˆæˆæ•°æ®è¾ƒç›´æ¥åŸºå‡†æé«˜äº†4.6 ASR BLEUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›´æ¥è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼ˆS2STï¼‰ä¸ºçº§è”ç³»ç»Ÿæä¾›ç®€æ´æ›¿ä»£æ–¹æ¡ˆï¼Œå‡å°‘æ¨ç†å»¶è¿Ÿã€‚</li>
<li>S2STæ¨¡å‹éœ€è¦å¤§é‡å¹³è¡Œè¯­éŸ³æ•°æ®ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€å¦‚æ³¢æ–¯è¯­ä¸­éš¾ä»¥è·å–ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºæ³¢æ–¯è¯­åˆ°è‹±è¯­çš„ç›´æ¥S2STç³»ç»Ÿã€‚</li>
<li>ç³»ç»ŸåŒ…æ‹¬åŸºäºconformerçš„ç¼–ç å™¨ã€å› æœè½¬æ¢å™¨è§£ç å™¨å’ŒåŸºäºå•å…ƒçš„ç¥ç»vocoderã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†æ³¢æ–¯è¯­è¯­éŸ³è½¬å½•ç¿»è¯‘æˆè‹±è¯­ï¼Œå¹¶ç”¨å…ˆè¿›çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿåˆæˆå¯¹åº”è‹±è¯­è¯­éŸ³ï¼Œæ„å»ºæ–°çš„æ³¢æ–¯è¯­-è‹±è¯­å¹³è¡Œè¯­éŸ³è¯­æ–™åº“ã€‚</li>
<li>åˆæˆè¯­æ–™åº“å¢åŠ äº†çº¦å…­å€çš„å¯ç”¨å¹³è¡Œè¯­éŸ³æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62240c21a7518c82d9fa85d2b3f2a4aa" align="middle">
<img src="https://picx.zhimg.com/v2-2d67808ee8a49f90bcbd8ed1ddaf18a9" align="middle">
<img src="https://picx.zhimg.com/v2-b76571a0934c490c3f09cf8e81e16b91" align="middle">
<img src="https://picx.zhimg.com/v2-fa88cbe490ffdadfbe3e974157089a3e" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data"><a href="#Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data" class="headerlink" title="Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data"></a>Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</h2><p><strong>Authors:Yunxin Li, Xinyu Chen, Shenyuan Jiang, Haoyuan Shi, Zhenyu Liu, Xuanyu Zhang, Nanhao Deng, Zhenran Xu, Yicheng Ma, Meishan Zhang, Baotian Hu, Min Zhang</strong></p>
<p>We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lycheeâ€™s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Lycheeç³»åˆ—çš„Uni-MoE 2.0ã€‚ä½œä¸ºä¸€æ¬¾å…¨å¼€æºçš„è·¨æ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆOLMï¼‰ï¼Œå®ƒæ˜¾è‘—åœ°æ¨è¿›äº†ä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„è·¨æ¨¡æ€ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢çš„Lycheeçš„Uni-MoEç³»åˆ—ã€‚åŸºäºQwen2.5-7Bå¯†é›†æ¶æ„ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒè´¡çŒ®ä»å¤´æ„å»ºäº†Uni-MoE-2.0-Omniï¼šåŠ¨æ€å®¹é‡ä¸“å®¶æ··åˆï¼ˆMoEï¼‰è®¾è®¡ã€é‡‡ç”¨è¿­ä»£å¢å¼ºç­–ç•¥çš„æ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œä»¥åŠç²¾å¿ƒç­–åˆ’çš„è·¨æ¨¡æ€æ•°æ®åŒ¹é…æŠ€æœ¯ã€‚å®ƒèƒ½å¤Ÿè¿›è¡Œè·¨æ¨¡æ€ç†è§£ï¼Œå¹¶ç”Ÿæˆå›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ã€‚ä»æ¶æ„ä¸Šçœ‹ï¼Œæˆ‘ä»¬æ–°çš„MoEæ¡†æ¶åœ¨åˆ©ç”¨å…±äº«ã€è·¯ç”±å’Œç©ºä¸“å®¶å¤„ç†10ä¸ªè·¨æ¨¡æ€è¾“å…¥æ—¶ï¼Œå®ç°äº†è®¡ç®—æ•ˆç‡å’Œèƒ½åŠ›ä¹‹é—´çš„å¹³è¡¡ï¼Œè€Œæˆ‘ä»¬çš„Omni-Modality 3D RoPEåˆ™ç¡®ä¿äº†è‡ªæ³¨æ„åŠ›å±‚ä¸­çš„æ—¶ç©ºè·¨æ¨¡æ€å¯¹é½ã€‚åœ¨è®­ç»ƒæ–¹é¢ï¼Œæˆ‘ä»¬é‡‡ç”¨è·¨æ¨¡æ€é¢„è®­ç»ƒä¹‹åçš„æ¸è¿›å¼ç›‘ç£å¾®è°ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿæ¿€æ´»ç‰¹å®šæ¨¡æ€çš„ä¸“å®¶ï¼Œå¹¶é€šè¿‡å¹³è¡¡æ•°æ®ç»„åˆå’Œè¿­ä»£GSPO-DPOæ–¹æ³•æ¥ç¨³å®šå¼ºåŒ–å­¦ä¹ è®­ç»ƒå’Œæå‡æ¨ç†èƒ½åŠ›ã€‚ä»æ•°æ®è§’åº¦çœ‹ï¼ŒåŸºç¡€æ¨¡å‹åœ¨å¤§çº¦75Bä¸ªå¼€æºè·¨æ¨¡æ€æ•°æ®æ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé…å¤‡äº†ç‰¹æ®Šçš„è¯­éŸ³å’Œå›¾åƒç”Ÿæˆæ ‡è®°ï¼Œé€šè¿‡ä»¥è¯­è¨€çº¿ç´¢ä¸ºæ¡ä»¶æ¥ç”Ÿæˆè¿™äº›ç”Ÿæˆä»»åŠ¡ã€‚åœ¨85ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é¢†å…ˆçš„OLMä¸­è¾¾åˆ°äº†SOTAæˆ–æå…·ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼Œåœ¨è¶…è¿‡ä¸€åŠçš„åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ä½¿ç”¨1.2ä¸‡äº¿æ ‡è®°è®­ç»ƒçš„Qwen2.5-Omniï¼ˆåœ¨76ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼‰ã€‚ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬è§†é¢‘ç†è§£ï¼ˆ+å¹³å‡æå‡7%ï¼Œå…±è¯„ä¼°8é¡¹ï¼‰ã€è·¨æ¨¡æ€ç†è§£ï¼ˆ+å¹³å‡æå‡7%ï¼Œå…±è¯„ä¼°4é¡¹ï¼‰å’Œè§†å¬æ¨ç†ï¼ˆ+å¹³å‡æå‡4%ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æ¨è¿›äº†é•¿è¯­éŸ³å¤„ç†ï¼ˆé™ä½WER 4.2%ï¼‰ï¼Œå¹¶åœ¨ä½çº§åˆ«å›¾åƒå¤„ç†å’Œå¯æ§ç”Ÿæˆæ–¹é¢é¢†å…ˆäº†äº”ä¸ªæŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12609v1">PDF</a> 47 pages,10 Figures, Project Website: <a target="_blank" rel="noopener" href="https://idealistxy.github.io/Uni-MoE-v2.github.io/">https://idealistxy.github.io/Uni-MoE-v2.github.io/</a>; Codes: <a target="_blank" rel="noopener" href="https://github.com/HITsz-TMG/Uni-MoE">https://github.com/HITsz-TMG/Uni-MoE</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºQwen2.5-7Bå¯†é›†æ¶æ„ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å…¨æ–°çš„å¼€æ”¾å¼å¤šæ¨¡æ€å¤§å‹æ¨¡å‹Uni-MoE 2.0ã€‚å®ƒé‡‡ç”¨åŠ¨æ€å®¹é‡çš„Mixture-of-Expertsï¼ˆMoEï¼‰è®¾è®¡ï¼Œé€šè¿‡æ¸è¿›è®­ç»ƒç­–ç•¥å’Œè¿­ä»£å¼ºåŒ–ç­–ç•¥ï¼Œä»¥åŠç²¾å¿ƒç­–åˆ’çš„å¤šæ¨¡æ€æ•°æ®åŒ¹é…æŠ€æœ¯ï¼Œå®ç°äº†è¯­è¨€ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›çš„æ˜¾è‘—æå‡ã€‚è¯¥æ¨¡å‹å…·å¤‡è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆå›¾åƒã€æ–‡æœ¬ã€è¯­éŸ³çš„èƒ½åŠ›ã€‚å…¶ç‹¬ç‰¹çš„MoEæ¡†æ¶åœ¨ä¿è¯è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå¤„ç†å¤šè¾¾åç§è·¨æ¨¡æ€è¾“å…¥ã€‚ç»è¿‡è·¨æ¨¡æ€é¢„è®­ç»ƒåï¼Œæˆ‘ä»¬ä½¿ç”¨æ¸è¿›çš„ç›‘ç£å’Œå¾®è°ƒç­–ç•¥æ¿€æ´»æ¨¡æ€ç‰¹å®šçš„ä¸“å®¶ï¼Œå¹¶é€šè¿‡å¹³è¡¡æ•°æ®ç»„åˆå’Œè¿­ä»£GSPO-DPOæ–¹æ³•æ¥ç¨³å®šå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¹¶æé«˜æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ç»è¿‡çº¦75Bä»¤ç‰Œå¼€æºå¤šæ¨¡æ€æ•°æ®è®­ç»ƒï¼Œå¹¶é…å¤‡äº†ç”¨äºè¯­éŸ³å’Œå›¾åƒç”Ÿæˆçš„ç‰¹æ®Šä»¤ç‰Œã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¶Šæœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå°¤å…¶åœ¨è§†é¢‘ç†è§£ã€å¤šæ¨¡æ€ç†è§£å’Œè§†å¬æ¨ç†æ–¹é¢è¡¨ç°çªå‡ºã€‚å¹¶åœ¨é•¿è¯­éŸ³å¤„ç†ã€ä½çº§åˆ«å›¾åƒå¤„ç†å’Œå¯æ§ç”Ÿæˆæ–¹é¢é¢†å…ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Uni-MoE 2.0æ˜¯åŸºäºQwen2.5-7Bæ¶æ„çš„å¼€æ”¾å¼å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ã€‚</li>
<li>é€šè¿‡åŠ¨æ€å®¹é‡çš„Mixture-of-Expertsè®¾è®¡å’Œæ¸è¿›è®­ç»ƒç­–ç•¥ç­‰æŠ€æœ¯æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å…·å¤‡è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼ŒåŒ…æ‹¬å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ã€‚</li>
<li>MoEæ¡†æ¶åœ¨ä¿è¯è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œèƒ½å¤„ç†å¤šç§è·¨æ¨¡æ€è¾“å…¥ã€‚</li>
<li>æ¨¡å‹ç»è¿‡å¤§é‡æ•°æ®è®­ç»ƒï¼ŒåŒ…æ‹¬çº¦75Bä»¤ç‰Œçš„å¼€æºå¤šæ¨¡æ€æ•°æ®ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒUni-MoE 2.0åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨è§†é¢‘ç†è§£ã€å¤šæ¨¡æ€ç†è§£å’Œè§†å¬æ¨ç†æ–¹é¢é¢†å…ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fbf6eae5c80051fc5966dffb97d4896" align="middle">
<img src="https://picx.zhimg.com/v2-2b72f469dd496a4ebcc1cd796a1db34b" align="middle">
<img src="https://picx.zhimg.com/v2-a964386fcf1c47706e06fde04609f299" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VoiceCraft-X-Unifying-Multilingual-Voice-Cloning-Speech-Synthesis-and-Speech-Editing"><a href="#VoiceCraft-X-Unifying-Multilingual-Voice-Cloning-Speech-Synthesis-and-Speech-Editing" class="headerlink" title="VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing"></a>VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing</h2><p><strong>Authors:Zhisheng Zheng, Puyuan Peng, Anuj Diwan, Cong Phuoc Huynh, Xiaohang Sun, Zhu Liu, Vimal Bhat, David Harwath</strong></p>
<p>We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at <a target="_blank" rel="noopener" href="https://zhishengzheng.com/voicecraft-x/">https://zhishengzheng.com/voicecraft-x/</a>.</p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºVoiceCraft-Xï¼Œè¿™æ˜¯ä¸€æ¬¾è‡ªåŠ¨å›å½’ç¥ç»ç¼–ç è¯­è¨€æ¨¡å‹ï¼Œå®ƒç»Ÿä¸€äº†è·¨11ç§è¯­è¨€çš„å¤šè¯­ç§è¯­éŸ³ç¼–è¾‘å’Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆï¼Œè¿™11ç§è¯­è¨€åŒ…æ‹¬è‹±è¯­ã€æ™®é€šè¯ã€éŸ©è¯­ã€æ—¥è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ã€å¾·è¯­ã€è·å…°è¯­ã€æ„å¤§åˆ©è¯­ã€è‘¡è„ç‰™è¯­å’Œæ³¢å…°è¯­ã€‚VoiceCraft-Xåˆ©ç”¨Qwen3å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ— éŸ³ç´ è·¨è¯­è¨€æ–‡æœ¬å¤„ç†ï¼Œå¹¶é‡‡ç”¨ä¸€ç§æ–°çš„ä»¤ç‰Œé‡æ’æœºåˆ¶ï¼Œé€šè¿‡æ–‡æœ¬å’Œè¯­éŸ³ä»¤ç‰Œçš„æ—¶åºå¯¹é½ï¼Œå°†ä¸¤ä¸ªä»»åŠ¡ä½œä¸ºå•ä¸ªåºåˆ—ç”Ÿæˆé—®é¢˜æ¥å¤„ç†ã€‚è¯¥æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ã€è‡ªç„¶è¯­éŸ³çš„è¯­éŸ³ï¼Œå¯ä»¥åœ¨ä¸€ä¸ªæ¡†æ¶å†…æ— ç¼åœ°åˆ›å»ºæ–°çš„éŸ³é¢‘æˆ–ç¼–è¾‘ç°æœ‰çš„å½•éŸ³ã€‚VoiceCraft-Xåœ¨ä¸åŒçš„è¯­è¨€ç¯å¢ƒä¸­è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨æ¯è¯­è¨€æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè¿™çªæ˜¾å‡ºç»Ÿä¸€è‡ªåŠ¨å›å½’æ–¹æ³•åœ¨æ¨åŠ¨å¤æ‚ã€ç°å®çš„å¤šè¯­ç§è¯­éŸ³åº”ç”¨æ–¹é¢çš„åŠ›é‡ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://zhishengzheng.com/voicecraft-x%E6%89%BE%E5%88%B0%E3%80%82">https://zhishengzheng.com/voicecraft-x/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12347v1">PDF</a> EMNLP 2025. Demo and code are available at <a target="_blank" rel="noopener" href="https://zhishengzheng.com/voicecraft-x/">https://zhishengzheng.com/voicecraft-x/</a></p>
<p><strong>Summary</strong><br>è¯­éŸ³Craft-Xæ˜¯ä¸€ä¸ªèåˆäº†å¤šç§è¯­è¨€è¯­éŸ³ç¼–è¾‘å’Œé›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆçš„è‡ªå›å½’ç¥ç»ç¼–ç è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒ11ç§è¯­è¨€ã€‚å®ƒé‡‡ç”¨Qwen3å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè·¨è¯­è¨€æ–‡æœ¬å¤„ç†ï¼Œå¹¶é‡‡ç”¨ä¸€ç§æ–°çš„ä»¤ç‰Œé‡æ’æœºåˆ¶æ¥å¤„ç†å•ä¸€åºåˆ—ç”Ÿæˆé—®é¢˜ã€‚è¯­éŸ³Craft-Xå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„è‡ªç„¶è¯­éŸ³ï¼Œæ— éœ€å¤æ‚çš„æ“ä½œå³å¯åœ¨æ–°çš„æˆ–ç°æœ‰çš„å½•éŸ³ä¸­åˆ›å»ºæˆ–ç¼–è¾‘éŸ³é¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VoiceCraft-Xæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šè¯­è¨€è¯­éŸ³ç¼–è¾‘å’Œé›¶æ ·æœ¬TTSåˆæˆçš„è‡ªå›å½’ç¥ç»ç¼–ç è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ”¯æŒåŒ…æ‹¬è‹±è¯­ã€æ™®é€šè¯ã€éŸ©è¯­ã€æ—¥è¯­ç­‰åœ¨å†…çš„11ç§è¯­è¨€ã€‚</li>
<li>ä½¿ç”¨Qwen3å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè·¨è¯­è¨€æ–‡æœ¬å¤„ç†ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„ä»¤ç‰Œé‡æ’æœºåˆ¶æ¥å¤„ç†å•ä¸€åºåˆ—ç”Ÿæˆé—®é¢˜ã€‚</li>
<li>å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„è‡ªç„¶è¯­éŸ³ï¼Œå®ç°æ–°çš„éŸ³é¢‘åˆ›å»ºæˆ–ç°æœ‰å½•éŸ³çš„ç¼–è¾‘ã€‚</li>
<li>åœ¨ä¸åŒçš„è¯­è¨€ç¯å¢ƒä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨æ¯ç§è¯­è¨€çš„æœ‰é™æ•°æ®ä¸‹ä¹Ÿèƒ½ç¨³å¥è¿è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12347">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-277147b07c70f22f124af5dd0d68640c" align="middle">
<img src="https://picx.zhimg.com/v2-9bcabef88ddddce9aff3ab2c2e4381c6" align="middle">
<img src="https://picx.zhimg.com/v2-5a95a4e8d84c1e8744684757e27467c5" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Metric-Preference-Alignment-for-Generative-Speech-Restoration"><a href="#Multi-Metric-Preference-Alignment-for-Generative-Speech-Restoration" class="headerlink" title="Multi-Metric Preference Alignment for Generative Speech Restoration"></a>Multi-Metric Preference Alignment for Generative Speech Restoration</h2><p><strong>Authors:Junan Zhang, Xueyao Zhang, Jing Yang, Yuancheng Wang, Fan Fan, Zhizheng Wu</strong></p>
<p>Recent generative models have significantly advanced speech restoration tasks, yet their training objectives often misalign with human perceptual preferences, resulting in suboptimal quality. While post-training alignment has proven effective in other generative domains like text and image generation, its application to generative speech restoration remains largely under-explored. This work investigates the challenges of applying preference-based post-training to this task, focusing on how to define a robust preference signal and curate high-quality data to avoid reward hacking. To address these challenges, we propose a multi-metric preference alignment strategy. We construct a new dataset, GenSR-Pref, comprising 80K preference pairs, where each chosen sample is unanimously favored by a complementary suite of metrics covering perceptual quality, signal fidelity, content consistency, and timbre preservation. This principled approach ensures a holistic preference signal. Applying Direct Preference Optimization (DPO) with our dataset, we observe consistent and significant performance gains across three diverse generative paradigms: autoregressive models (AR), masked generative models (MGM), and flow-matching models (FM) on various restoration benchmarks, in both objective and subjective evaluations. Ablation studies confirm the superiority of our multi-metric strategy over single-metric approaches in mitigating reward hacking. Furthermore, we demonstrate that our aligned models can serve as powerful â€˜â€™data annotatorsâ€™â€™, generating high-quality pseudo-labels to serve as a supervision signal for traditional discriminative models in data-scarce scenarios like singing voice restoration. Demo Page:<a target="_blank" rel="noopener" href="https://gensr-pref.github.io/">https://gensr-pref.github.io</a></p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹åœ¨è¯­éŸ³ä¿®å¤ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬çš„è®­ç»ƒç›®æ ‡å¾€å¾€ä¸äººç±»æ„ŸçŸ¥åå¥½ä¸ä¸€è‡´ï¼Œå¯¼è‡´è´¨é‡ä¸ä½³ã€‚è™½ç„¶åŸºäºåå¥½çš„åè®­ç»ƒå¯¹é½åœ¨æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆç­‰å…¶ä»–ç”Ÿæˆé¢†åŸŸå·²ç»è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å…¶åœ¨ç”Ÿæˆè¯­éŸ³ä¿®å¤ä¸­çš„åº”ç”¨ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å°†åŸºäºåå¥½çš„åè®­ç»ƒåº”ç”¨äºè¿™ä¸€ä»»åŠ¡æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œé‡ç‚¹å…³æ³¨å¦‚ä½•å®šä¹‰ç¨³å¥çš„åå¥½ä¿¡å·ä»¥åŠç­›é€‰é«˜è´¨é‡æ•°æ®ä»¥é¿å…å¥–åŠ±æ“çºµã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæŒ‡æ ‡åå¥½å¯¹é½ç­–ç•¥ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†GenSR-Prefï¼ŒåŒ…å«80ä¸‡å¯¹åå¥½æ ·æœ¬ï¼Œæ¯ä¸ªè¢«é€‰ä¸­çš„æ ·æœ¬éƒ½ç”±ä¸€å¥—æ¶µç›–æ„ŸçŸ¥è´¨é‡ã€ä¿¡å·ä¿çœŸåº¦ã€å†…å®¹ä¸€è‡´æ€§å’ŒéŸ³è‰²ä¿æŒçš„äº’è¡¥æŒ‡æ ‡ä¸€è‡´è®¤å¯ã€‚è¿™ç§æœ‰åŸåˆ™çš„æ–¹æ³•ç¡®ä¿äº†å…¨é¢çš„åå¥½ä¿¡å·ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç§ä¸åŒçš„ç”ŸæˆèŒƒå¼ä¸­è§‚å¯Ÿåˆ°äº†ä¸€è‡´ä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼šåŒ…æ‹¬è‡ªå›å½’æ¨¡å‹ï¼ˆARï¼‰ã€é®ç½©ç”Ÿæˆæ¨¡å‹ï¼ˆMGMï¼‰å’ŒæµåŒ¹é…æ¨¡å‹ï¼ˆFMï¼‰åœ¨å„ç§ä¿®å¤åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡æœ‰æå‡ã€‚æ¶ˆèç ”ç©¶è¯å®ï¼Œæˆ‘ä»¬çš„å¤šæŒ‡æ ‡ç­–ç•¥åœ¨ç¼“è§£å¥–åŠ±æ“çºµæ–¹é¢ä¼˜äºå•æŒ‡æ ‡æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„å¯¹é½æ¨¡å‹å¯ä»¥ä½œä¸ºå¼ºå¤§çš„â€œæ•°æ®æ³¨é‡Šå™¨â€ï¼Œç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼Œä½œä¸ºæ•°æ®ç¨€ç¼ºåœºæ™¯ï¼ˆå¦‚æ­Œå£°ä¿®å¤ï¼‰ä¸­ä¼ ç»Ÿåˆ¤åˆ«æ¨¡å‹çš„ç›‘ç£ä¿¡å·ã€‚Demo Pageï¼š<a target="_blank" rel="noopener" href="https://gensr-pref.github.io/">https://gensr-pref.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17229v2">PDF</a> Accepted by AAAI 2026. Demopage: <a target="_blank" rel="noopener" href="https://gensr-pref.github.io/">https://gensr-pref.github.io</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹åœ¨è¯­éŸ³ä¿®å¤ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶è®­ç»ƒç›®æ ‡å¸¸ä¸äººç±»æ„ŸçŸ¥åå¥½ä¸ä¸€è‡´ï¼Œå¯¼è‡´è´¨é‡ä¸ä½³ã€‚æœ¬æ–‡æ¢è®¨äº†å°†åŸºäºåå¥½çš„åè®­ç»ƒå¯¹é½åº”ç”¨äºæ­¤ä»»åŠ¡æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶èšç„¦å¦‚ä½•å®šä¹‰ç¨³å¥çš„åå¥½ä¿¡å·å’Œç­›é€‰é«˜è´¨é‡æ•°æ®ä»¥é¿å…å¥–åŠ±æ“çºµã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¤šæŒ‡æ ‡åå¥½å¯¹é½ç­–ç•¥ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†GenSR-Prefï¼ŒåŒ…å«80Kåå¥½å¯¹ï¼Œå…¶ä¸­æ¯ä¸ªæ ·æœ¬éƒ½è¢«ä¸€ç³»åˆ—äº’è¡¥æŒ‡æ ‡ä¸€è‡´é’çï¼Œæ¶µç›–æ„ŸçŸ¥è´¨é‡ã€ä¿¡å·ä¿çœŸåº¦ã€å†…å®¹ä¸€è‡´æ€§å’ŒéŸ³è‰²ä¿æŒã€‚åº”ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸æ•°æ®é›†ï¼Œè§‚å¯Ÿåˆ°ä¸‰ç§ä¸åŒç”ŸæˆèŒƒå¼ä¸‹æ€§èƒ½å¾—åˆ°æŒç»­å’Œæ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡åºŸé™¤ç ”ç©¶ç¡®è®¤å¤šæŒ‡æ ‡ç­–ç•¥ä¼˜äºå•æŒ‡æ ‡æ–¹æ³•ï¼Œåœ¨ç¼“è§£å¥–åŠ±æ“çºµæ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚æœ€åï¼Œè¯æ˜å¯¹é½æ¨¡å‹å¯ä½œä¸ºå¼ºå¤§çš„â€œæ•°æ®æ ‡æ³¨å™¨â€ï¼Œç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼Œä½œä¸ºæ•°æ®ç¨€ç¼ºåœºæ™¯ï¼ˆå¦‚æ­Œå£°ä¿®å¤ï¼‰ä¸­ä¼ ç»Ÿåˆ¤åˆ«æ¨¡å‹çš„ç›‘ç£ä¿¡å·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¿‘æœŸç”Ÿæˆæ¨¡å‹åœ¨è¯­éŸ³ä¿®å¤ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨è®­ç»ƒç›®æ ‡ä¸äººç±»æ„ŸçŸ¥åå¥½ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>åŸºäºåå¥½çš„åè®­ç»ƒå¯¹é½åœ¨è¯­éŸ³ä¿®å¤ä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæŒ‡æ ‡åå¥½å¯¹é½ç­–ç•¥ï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†GenSR-Prefï¼ŒåŒ…å«80Kåå¥½å¯¹ï¼Œä»¥å®šä¹‰ç¨³å¥çš„åå¥½ä¿¡å·å¹¶é¿å…å¥–åŠ±æ“çºµã€‚</li>
<li>åº”ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åï¼Œä¸‰ç§ä¸åŒçš„ç”ŸæˆèŒƒå¼æ€§èƒ½å¾—åˆ°æ˜¾è‘—æå‡ã€‚</li>
<li>å¤šæŒ‡æ ‡ç­–ç•¥ä¼˜äºå•æŒ‡æ ‡æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆç¼“è§£å¥–åŠ±æ“çºµé—®é¢˜ã€‚</li>
<li>å¯¹é½æ¨¡å‹å¯ä»¥ä½œä¸ºå¼ºå¤§çš„â€œæ•°æ®æ ‡æ³¨å™¨â€ï¼Œç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ç”¨äºç›‘ç£ä¼ ç»Ÿåˆ¤åˆ«æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ï¼ˆå¦‚æ­Œå£°ä¿®å¤ï¼‰ä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8fabb93c689cfc53f11801c7fc429fb" align="middle">
<img src="https://picx.zhimg.com/v2-069d7d11b61b47fa9c002b986fbee072" align="middle">
<img src="https://picx.zhimg.com/v2-6bb614072a21415b5891b6a5f078d34b" align="middle">
<img src="https://picx.zhimg.com/v2-42d1a26a551bf01b52e3176d954a5385" align="middle">
<img src="https://picx.zhimg.com/v2-7738e2216b6769727c182db5da506463" align="middle">
<img src="https://picx.zhimg.com/v2-251289290d47a81eaf64d6cc5dfec6f9" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DualSpeechLM-Towards-Unified-Speech-Understanding-and-Generation-via-Dual-Speech-Token-Modeling-with-Large-Language-Models"><a href="#DualSpeechLM-Towards-Unified-Speech-Understanding-and-Generation-via-Dual-Speech-Token-Modeling-with-Large-Language-Models" class="headerlink" title="DualSpeechLM: Towards Unified Speech Understanding and Generation via Dual Speech Token Modeling with Large Language Models"></a>DualSpeechLM: Towards Unified Speech Understanding and Generation via Dual Speech Token Modeling with Large Language Models</h2><p><strong>Authors:Yuanyuan Wang, Dongchao Yang, Yiwen Shao, Hangting Chen, Jiankun Zhao, Zhiyong Wu, Helen Meng, Xixin Wu</strong></p>
<p>Extending pre-trained text Large Language Models (LLMs)â€™s speech understanding or generation abilities by introducing various effective speech tokens has attracted great attention in the speech community. However, building a unified speech understanding and generation model still faces the following challenges: (1) Due to the huge modality gap between speech and text tokens, extending text LLMs to unified speech LLMs relies on large-scale paired data for fine-tuning, and (2) Generation and understanding tasks prefer information at different levels, e.g., generation benefits from detailed acoustic features, while understanding favors high-level semantics. This divergence leads to difficult performance optimization in one unified model. To solve these challenges, in this paper, we present two key insights in speech tokenization and speech language modeling. Specifically, we first propose an Understanding-driven Speech Tokenizer (USTokenizer), which extracts high-level semantic information essential for accomplishing understanding tasks using text LLMs. In this way, USToken enjoys better modality commonality with text, which reduces the difficulty of modality alignment in adapting text LLMs to speech LLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that concurrently models USToken as input and acoustic token as output within a unified, end-to-end framework, seamlessly integrating speech understanding and generation capabilities. Furthermore, we propose a novel semantic supervision loss and a Chain-of-Condition (CoC) strategy to stabilize model training and enhance speech generation performance. Experimental results demonstrate that our proposed approach effectively fosters a complementary relationship between understanding and generation tasks, highlighting the promising strategy of mutually enhancing both tasks in one unified model.</p>
<blockquote>
<p>é€šè¿‡å¼•å…¥å„ç§æœ‰æ•ˆçš„è¯­éŸ³ä»¤ç‰Œæ¥æ‰©å±•é¢„è®­ç»ƒçš„æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³ç†è§£æˆ–ç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨è¯­éŸ³é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œæ„å»ºç»Ÿä¸€çš„è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ¨¡å‹ä»ç„¶é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç”±äºè¯­éŸ³å’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´å­˜åœ¨å·¨å¤§çš„æ¨¡æ€å·®è·ï¼Œå°†æ–‡æœ¬LLMæ‰©å±•åˆ°ç»Ÿä¸€çš„è¯­éŸ³LLMä¾èµ–äºå¤§è§„æ¨¡é…å¯¹æ•°æ®è¿›è¡Œå¾®è°ƒï¼›ï¼ˆ2ï¼‰ç”Ÿæˆå’Œç†è§£ä»»åŠ¡åå¥½ä¸åŒå±‚çº§çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼Œç”Ÿæˆå—ç›Šäºè¯¦ç»†çš„å£°å­¦ç‰¹å¾ï¼Œè€Œç†è§£åˆ™åå‘äºé«˜çº§è¯­ä¹‰ã€‚è¿™ç§åˆ†æ­§å¯¼è‡´åœ¨ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­éš¾ä»¥è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08961v3">PDF</a> Accepted by AAAI 2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©å±•é¢„è®­ç»ƒæ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³ç†è§£æˆ–ç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡å¼•å…¥å„ç§æœ‰æ•ˆçš„è¯­éŸ³ä»¤ç‰Œå·²ç»å¼•èµ·äº†è¯­éŸ³ç¤¾åŒºçš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œæ„å»ºç»Ÿä¸€çš„è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ¨¡å‹ä»ç„¶é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼šä¸€æ˜¯ç”±äºè¯­éŸ³å’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´å­˜åœ¨å·¨å¤§çš„æ¨¡æ€å·®è·ï¼Œå°†æ–‡æœ¬LLMæ‰©å±•åˆ°ç»Ÿä¸€è¯­éŸ³LLMä¾èµ–äºå¤§è§„æ¨¡é…å¯¹æ•°æ®è¿›è¡Œå¾®è°ƒï¼›äºŒæ˜¯ç”Ÿæˆå’Œç†è§£ä»»åŠ¡åå¥½ä¸åŒå±‚çº§çš„ä¿¡æ¯ï¼Œå¦‚ç”Ÿæˆå—ç›Šäºè¯¦ç»†çš„å£°å­¦ç‰¹å¾ï¼Œè€Œç†è§£åˆ™åå¥½é«˜çº§è¯­ä¹‰ã€‚è¿™å¯¼è‡´åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ä¼˜åŒ–æ€§èƒ½å˜å¾—å›°éš¾ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªå…³äºè¯­éŸ³ä»¤ç‰ŒåŒ–å’Œè¯­éŸ³è¯­è¨€å»ºæ¨¡çš„å…³é”®è§è§£ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç†è§£é©±åŠ¨çš„è¯­éŸ³åˆ†è¯å™¨ï¼ˆUSTokenizerï¼‰ï¼Œå®ƒä½¿ç”¨æ–‡æœ¬LLMæå–å¯¹å®Œæˆç†è§£ä»»åŠ¡è‡³å…³é‡è¦çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒUSTokenä¸æ–‡æœ¬çš„æ¨¡æ€å…±æ€§æ›´å¥½ï¼Œé™ä½äº†å°†æ–‡æœ¬LLMé€‚åº”åˆ°è¯­éŸ³LLMæ—¶æ¨¡æ€å¯¹é½çš„éš¾åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†DualSpeechLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŒä»¤ç‰Œå»ºæ¨¡æ¡†æ¶ï¼ŒåŒæ—¶ä»¥USTokenä½œä¸ºè¾“å…¥å’ŒéŸ³é¢‘ä»¤ç‰Œä½œä¸ºè¾“å‡ºè¿›è¡Œå»ºæ¨¡ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€ã€ç«¯åˆ°ç«¯çš„æ¡†æ¶ä¸­æ— ç¼é›†æˆè¯­éŸ³ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è¯­ä¹‰ç›‘ç£æŸå¤±å’Œé“¾å¼æ¡ä»¶ï¼ˆCoCï¼‰ç­–ç•¥ï¼Œä»¥ç¨³å®šæ¨¡å‹è®­ç»ƒå¹¶å¢å¼ºè¯­éŸ³ç”Ÿæˆæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æœ‰æ•ˆåœ°ä¿ƒè¿›äº†ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„äº’è¡¥å…³ç³»ï¼Œçªæ˜¾å‡ºåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ç›¸äº’å¢å¼ºè¿™ä¸¤ä¸ªä»»åŠ¡çš„ç­–ç•¥å‰æ™¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³å’Œæ–‡æœ¬ä¹‹é—´å­˜åœ¨å·¨å¤§çš„æ¨¡æ€å·®è·ï¼Œæ‰©å±•æ–‡æœ¬LLMåˆ°ç»Ÿä¸€çš„è¯­éŸ³LLMéœ€è¦å¤§è§„æ¨¡é…å¯¹æ•°æ®è¿›è¡Œå¾®è°ƒã€‚</li>
<li>ç”Ÿæˆå’Œç†è§£ä»»åŠ¡åå¥½ä¸åŒå±‚çº§çš„ä¿¡æ¯ï¼Œè¿™å¯¼è‡´åœ¨ä¼˜åŒ–ç»Ÿä¸€æ¨¡å‹æ—¶é¢ä¸´å›°éš¾ã€‚</li>
<li>æå‡ºç†è§£é©±åŠ¨çš„è¯­éŸ³åˆ†è¯å™¨ï¼ˆUSTokenizerï¼‰ï¼Œæå–é«˜çº§è¯­ä¹‰ä¿¡æ¯ä»¥ä¿ƒè¿›è¯­éŸ³ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>USTokenä¸æ–‡æœ¬å…·æœ‰æ›´å¥½çš„æ¨¡æ€å…±æ€§ï¼Œé™ä½æ¨¡æ€å¯¹é½éš¾åº¦ã€‚</li>
<li>ä»‹ç»DualSpeechLMæ¡†æ¶ï¼Œç»Ÿä¸€å»ºæ¨¡è¯­éŸ³ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹è¯­ä¹‰ç›‘ç£æŸå¤±æ¥ç¨³å®šæ¨¡å‹è®­ç»ƒå¹¶å¢å¼ºè¯­éŸ³ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„æ–¹æ³•æœ‰æ•ˆä¿ƒè¿›ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„äº’è¡¥å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2206b1455c63155755579852387ab85a" align="middle">
<img src="https://picx.zhimg.com/v2-a44d17111f663b828e788cde15e32827" align="middle">
<img src="https://picx.zhimg.com/v2-463362a4c36c7d814fbf850a76648737" align="middle">
<img src="https://picx.zhimg.com/v2-0727f96a0c8fd4a3a910cee56a9f51a0" align="middle">
<img src="https://picx.zhimg.com/v2-8f8fabd84dc1f35c5df7ddcec36c0b77" align="middle">
<img src="https://picx.zhimg.com/v2-587af9e8dec45d69ba126a400bf9f3e1" align="middle">
<img src="https://picx.zhimg.com/v2-7ae9ffca755b7d9b9dfa01ad198c7a06" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SceneJailEval-A-Scenario-Adaptive-Multi-Dimensional-Framework-for-Jailbreak-Evaluation"><a href="#SceneJailEval-A-Scenario-Adaptive-Multi-Dimensional-Framework-for-Jailbreak-Evaluation" class="headerlink" title="SceneJailEval: A Scenario-Adaptive Multi-Dimensional Framework for Jailbreak Evaluation"></a>SceneJailEval: A Scenario-Adaptive Multi-Dimensional Framework for Jailbreak Evaluation</h2><p><strong>Authors:Lai Jiang, Yuekang Li, Xiaohan Zhang, Youtao Ding, Li Pan</strong></p>
<p>Accurate jailbreak evaluation is critical for LLM red team testing and jailbreak research. Mainstream methods rely on binary classification (string matching, toxic text classifiers, and LLM-based methods), outputting only â€œyes&#x2F;noâ€ labels without quantifying harm severity. Emerged multi-dimensional frameworks (e.g., Security Violation, Relative Truthfulness and Informativeness) use unified evaluation standards across scenarios, leading to scenario-specific mismatches (e.g., â€œRelative Truthfulnessâ€ is irrelevant to â€œhate speechâ€), undermining evaluation accuracy. To address these, we propose SceneJailEval, with key contributions: (1) A pioneering scenario-adaptive multi-dimensional framework for jailbreak evaluation, overcoming the critical â€œone-size-fits-allâ€ limitation of existing multi-dimensional methods, and boasting robust extensibility to seamlessly adapt to customized or emerging scenarios. (2) A novel 14-scenario dataset featuring rich jailbreak variants and regional cases, addressing the long-standing gap in high-quality, comprehensive benchmarks for scenario-adaptive evaluation. (3) SceneJailEval delivers state-of-the-art performance with an F1 score of 0.917 on our full-scenario dataset (+6% over SOTA) and 0.995 on JBB (+3% over SOTA), breaking through the accuracy bottleneck of existing evaluation methods in heterogeneous scenarios and solidifying its superiority.</p>
<blockquote>
<p>ç²¾ç¡®ç‰¢ä¸å¯ç ´çš„è¯„ä¼°å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çº¢é˜Ÿæµ‹è¯•å’Œç‰¢ä¸å¯ç ´ç ”ç©¶è‡³å…³é‡è¦ã€‚ä¸»æµæ–¹æ³•ä¾èµ–äºäºŒåˆ†ç±»ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ã€æœ‰æ¯’æ–‡æœ¬åˆ†ç±»å™¨å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼‰ï¼Œåªè¾“å‡ºâ€œæ˜¯&#x2F;å¦â€æ ‡ç­¾ï¼Œæ²¡æœ‰é‡åŒ–ä¼¤å®³ä¸¥é‡æ€§ã€‚æ–°å…´çš„å¤šç»´åº¦æ¡†æ¶ï¼ˆä¾‹å¦‚å®‰å…¨è¿è§„ã€ç›¸å¯¹çœŸå®æ€§å’Œä¿¡æ¯é‡ï¼‰åœ¨åœºæ™¯ä¸­ç»Ÿä¸€è¯„ä¼°æ ‡å‡†ï¼Œå¯¼è‡´åœºæ™¯ç‰¹å®šä¸åŒ¹é…ï¼ˆä¾‹å¦‚ï¼Œâ€œç›¸å¯¹çœŸå®æ€§â€ä¸â€œä»‡æ¨è¨€è®ºâ€æ— å…³ï¼‰ï¼Œä»è€Œé™ä½äº†è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SceneJailEvalï¼Œå…¶ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰å¼€åˆ›æ€§çš„åœºæ™¯è‡ªé€‚åº”å¤šç»´åº¦ç‰¢ä¸å¯ç ´è¯„ä¼°æ¡†æ¶ï¼Œå…‹æœäº†ç°æœ‰å¤šç»´åº¦æ–¹æ³•çš„â€œä¸€åˆ€åˆ‡â€å±€é™æ€§ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„å¯æ‰©å±•æ€§ï¼Œå¯æ— ç¼é€‚åº”è‡ªå®šä¹‰æˆ–æ–°å…´åœºæ™¯ã€‚ï¼ˆ2ï¼‰åŒ…å«ä¸°å¯Œç‰¢ä¸å¯ç ´å˜ç§å’ŒåŒºåŸŸæ¡ˆä¾‹çš„14ç§åœºæ™¯æ•°æ®é›†ï¼Œè§£å†³äº†é•¿æœŸå­˜åœ¨çš„é«˜è´¨é‡ã€å…¨é¢åŸºå‡†æµ‹è¯•åœºæ™¯è‡ªé€‚åº”è¯„ä¼°çš„ç©ºç™½ã€‚ï¼ˆ3ï¼‰SceneJailEvalåœ¨æˆ‘ä»¬çš„å…¨åœºæ™¯æ•°æ®é›†ä¸Šè¾¾åˆ°äº†0.917çš„F1åˆ†æ•°ï¼ˆè¾ƒæœ€æ–°æŠ€æœ¯é«˜å‡º6%ï¼‰ï¼Œåœ¨JBBä¸Šè¾¾åˆ°äº†0.995ï¼ˆè¾ƒæœ€æ–°æŠ€æœ¯é«˜å‡º3%ï¼‰ï¼Œçªç ´äº†ç°æœ‰è¯„ä¼°æ–¹æ³•åœ¨å¼‚è´¨åœºæ™¯ä¸­çš„å‡†ç¡®æ€§ç“¶é¢ˆï¼Œå¹¶å·©å›ºäº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06194v2">PDF</a> This paper has been accepted by AAAI 2026 as a poster</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æå‡ºé’ˆå¯¹LLMçº¢é˜Ÿæµ‹è¯•å’Œè¶Šç‹±ç ”ç©¶çš„ç²¾ç¡®è¶Šç‹±è¯„ä¼°è‡³å…³é‡è¦ã€‚ç°æœ‰ä¸»æµæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œä»…é‡‡ç”¨äºŒå…ƒåˆ†ç±»æ–¹å¼è¾“å‡ºç®€å•çš„â€œæ˜¯&#x2F;å¦â€æ ‡ç­¾ï¼Œç¼ºä¹é‡åŒ–ä¼¤å®³ä¸¥é‡ç¨‹åº¦çš„è¯„ä¼°ã€‚æ–°å…´çš„å¤šç»´åº¦æ¡†æ¶è™½è¯•å›¾ç»Ÿä¸€è¯„ä»·æ ‡å‡†ï¼Œä½†ç”±äºåœºæ™¯ç‰¹å®šä¸åŒ¹é…é—®é¢˜ï¼Œå¦‚â€œç›¸å¯¹çœŸå®æ€§â€ä¸â€œä»‡æ¨è¨€è®ºâ€æ— å…³ï¼Œå¯¼è‡´è¯„ä¼°å‡†ç¡®æ€§ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæ–‡æœ¬æå‡ºäº†SceneJailEvalï¼Œå…¶å…³é”®è´¡çŒ®åŒ…æ‹¬ï¼šåˆ›æ–°çš„åœºæ™¯è‡ªé€‚åº”å¤šç»´åº¦æ¡†æ¶è¿›è¡Œè¶Šç‹±è¯„ä¼°ï¼Œå…‹æœç°æœ‰å¤šç»´åº¦æ–¹æ³•çš„â€œä¸€åˆ€åˆ‡â€å±€é™ï¼Œå¹¶å…·å¤‡è½»æ¾é€‚åº”è‡ªå®šä¹‰æˆ–æ–°å…´åœºæ™¯çš„é²æ£’å¯æ‰©å±•æ€§ï¼›åŒ…å«ä¸°å¯Œè¶Šç‹±å˜ç§å’ŒåŒºåŸŸæ¡ˆä¾‹çš„14åœºæ™¯æ•°æ®é›†ï¼Œè§£å†³äº†é«˜è´¨é‡ã€å…¨é¢åŸºå‡†æµ‹è¯•çš„åœºæ™¯è‡ªé€‚åº”è¯„ä¼°é•¿æœŸç¼ºå£é—®é¢˜ï¼›SceneJailEvalåœ¨å…¨é¢åœºæ™¯æ•°æ®é›†ä¸Šçš„F1åˆ†æ•°è¾¾åˆ°0.917ï¼ˆè¾ƒSOTAé«˜å‡º6%ï¼‰ï¼Œåœ¨JBBä¸Šè¾¾åˆ°0.995ï¼ˆè¾ƒSOTAé«˜å‡º3%ï¼‰ï¼Œçªç ´ç°æœ‰è¯„ä¼°æ–¹æ³•åœ¨å¼‚æ„åœºæ™¯ä¸­çš„å‡†ç¡®æ€§ç“¶é¢ˆï¼Œå¹¶è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®è¿›è¡Œè¶Šç‹±è¯„ä¼°å¯¹LLMçº¢é˜Ÿæµ‹è¯•å’Œè¶Šç‹±ç ”ç©¶è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºŒå…ƒåˆ†ç±»ï¼Œç¼ºä¹é‡åŒ–ä¼¤å®³ä¸¥é‡ç¨‹åº¦çš„è¯„ä¼°ã€‚</li>
<li>å¤šç»´åº¦æ¡†æ¶è¯•å›¾ç»Ÿä¸€è¯„ä»·æ ‡å‡†ï¼Œä½†å­˜åœ¨åœºæ™¯ç‰¹å®šä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>SceneJailEvalæå‡ºåˆ›æ–°çš„åœºæ™¯è‡ªé€‚åº”å¤šç»´åº¦æ¡†æ¶ï¼Œå…‹æœç°æœ‰æ–¹æ³•çš„å±€é™ã€‚</li>
<li>SceneJailEvalåŒ…å«14åœºæ™¯æ•°æ®é›†ï¼Œè¦†ç›–ä¸°å¯Œçš„è¶Šç‹±å˜ç§å’ŒåŒºåŸŸæ¡ˆä¾‹ã€‚</li>
<li>SceneJailEvalåœ¨å…¨é¢åœºæ™¯æ•°æ®é›†å’ŒJBBä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒSOTAæœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d32a81e03736257a6dbbae84b136622" align="middle">
<img src="https://picx.zhimg.com/v2-dc3828f700a74d10232027234b03f4a0" align="middle">
<img src="https://picx.zhimg.com/v2-d96dd67276f6e57cbb364efb19446998" align="middle">
<img src="https://picx.zhimg.com/v2-d816fca2b1806cb9565ccc42b30a42ce" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="READ-Real-time-and-Efficient-Asynchronous-Diffusion-for-Audio-driven-Talking-Head-Generation"><a href="#READ-Real-time-and-Efficient-Asynchronous-Diffusion-for-Audio-driven-Talking-Head-Generation" class="headerlink" title="READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation"></a>READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</h2><p><strong>Authors:Haotian Wang, Yuzhe Weng, Jun Du, Haoran Xu, Xiaoyan Wu, Shan He, Bing Yin, Cong Liu, Jianqing Gao, Qingfeng Liu</strong></p>
<p>The introduction of diffusion models has brought significant advances to the field of audio-driven talking head generation. However, the extremely slow inference speed severely limits the practical implementation of diffusion-based talking head generation models. In this study, we propose READ, a real-time diffusion-transformer-based talking head generation framework. Our approach first learns a spatiotemporal highly compressed video latent space via a temporal VAE, significantly reducing the token count to accelerate generation. To achieve better audio-visual alignment within this compressed latent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to generate temporally compressed speech latent codes corresponding to the video latent space. These latent representations are then modeled by a carefully designed Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient talking head synthesis. Furthermore, to ensure temporal consistency and accelerated inference in extended generation, we propose a novel asynchronous noise scheduler (ANS) for both the training and inference processes of our framework. The ANS leverages asynchronous add-noise and asynchronous motion-guided generation in the latent space, ensuring consistency in generated video clips. Experimental results demonstrate that READ outperforms state-of-the-art methods by generating competitive talking head videos with significantly reduced runtime, achieving an optimal balance between quality and speed while maintaining robust metric stability in long-time generation.</p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å¼•å…¥ä¸ºéŸ³é¢‘é©±åŠ¨çš„æœ‰å£°è§†é¢‘å¤´éƒ¨ç”Ÿæˆé¢†åŸŸå¸¦æ¥äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºå…¶ææ…¢çš„æ¨ç†é€Ÿåº¦ï¼ŒåŸºäºæ‰©æ•£çš„æœ‰å£°è§†é¢‘å¤´éƒ¨ç”Ÿæˆæ¨¡å‹çš„å®é™…åº”ç”¨å—åˆ°äº†ä¸¥é‡é™åˆ¶ã€‚æœ¬ç ”ç©¶æå‡ºäº†READï¼Œä¸€ä¸ªåŸºäºå®æ—¶æ‰©æ•£å˜å‹å™¨çš„æœ‰å£°è§†é¢‘å¤´éƒ¨ç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡æ—¶é—´VAEå­¦ä¹ ä¸€ä¸ªæ—¶ç©ºé«˜åº¦å‹ç¼©çš„è§†é¢‘æ½œåœ¨ç©ºé—´ï¼Œå¤§å¤§å‡å°‘ä»¤ç‰Œè®¡æ•°ä»¥åŠ é€Ÿç”Ÿæˆã€‚ä¸ºäº†åœ¨è¿™ä¸ªå‹ç¼©çš„æ½œåœ¨ç©ºé—´å†…å®ç°æ›´å¥½çš„éŸ³è§†é¢‘å¯¹é½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¢„è®­ç»ƒçš„è‡ªç¼–ç å™¨è¯­éŸ³è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSpeechAEï¼‰æ¥ç”Ÿæˆä¸è§†é¢‘æ½œåœ¨ç©ºé—´ç›¸å¯¹åº”çš„æ—¶é—´å‹ç¼©è¯­éŸ³æ½œåœ¨ä»£ç ã€‚è¿™äº›æ½œåœ¨è¡¨ç¤ºç„¶åé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„éŸ³é¢‘åˆ°è§†é¢‘æ‰©æ•£å˜å‹å™¨ï¼ˆA2V-DiTï¼‰ä¸»å¹²è¿›è¡Œå»ºæ¨¡ï¼Œä»¥å®ç°é«˜æ•ˆçš„æœ‰å£°è§†é¢‘å¤´éƒ¨åˆæˆã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿æ‰©å±•ç”Ÿæˆçš„æ—¶åºä¸€è‡´æ€§å’ŒåŠ é€Ÿæ¨ç†ï¼Œæˆ‘ä»¬ä¸ºæ¡†æ¶çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹æå‡ºäº†æ–°å‹å¼‚æ­¥å™ªå£°è°ƒåº¦å™¨ï¼ˆANSï¼‰ã€‚ANSåˆ©ç”¨æ½œä¼ç©ºé—´ä¸­çš„å¼‚æ­¥æ·»åŠ å™ªå£°å’Œå¼‚æ­¥è¿åŠ¨å¼•å¯¼ç”Ÿæˆï¼Œç¡®ä¿ç”Ÿæˆçš„è§†é¢‘ç‰‡æ®µçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒREADåœ¨ç”Ÿæˆå…·æœ‰ç«äº‰åŠ›çš„æœ‰å£°è§†é¢‘å¤´éƒ¨è§†é¢‘æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨ä¿æŒé•¿æ—¶é—´ç”Ÿæˆç¨³å¥çš„åº¦é‡ç¨³å®šæ€§çš„åŒæ—¶å®ç°äº†è´¨é‡å’Œé€Ÿåº¦çš„æœ€ä¼˜å¹³è¡¡ï¼Œæ˜¾è‘—å‡å°‘äº†è¿è¡Œæ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03457v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://readportrait.github.io/READ/">https://readportrait.github.io/READ/</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘é©±åŠ¨å¯¹è¯å¤´ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ææ…¢çš„æ¨ç†é€Ÿåº¦é™åˆ¶äº†å®é™…åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†READæ¡†æ¶ï¼ŒåŸºäºå®æ—¶æ‰©æ•£å˜å‹å™¨è¿›è¡Œå¯¹è¯å¤´ç”Ÿæˆã€‚é€šè¿‡æ—¶é—´å˜åˆ†è‡ªç¼–ç å™¨å­¦ä¹ é«˜åº¦å‹ç¼©çš„è§†é¢‘æ½œåœ¨ç©ºé—´ï¼Œæ˜¾è‘—é™ä½ä»¤ç‰Œè®¡æ•°ä»¥åŠ é€Ÿç”Ÿæˆã€‚ä¸ºåœ¨æ­¤å‹ç¼©æ½œåœ¨ç©ºé—´å†…å®ç°æ›´å¥½çš„è§†å¬å¯¹é½ï¼Œæå‡ºäº†é¢„è®­ç»ƒçš„è¯­éŸ³è‡ªç¼–ç å™¨ï¼ˆSpeechAEï¼‰ç”Ÿæˆä¸æ—¶é—´å‹ç¼©çš„è¯­éŸ³æ½œåœ¨ä»£ç ç›¸å¯¹åº”çš„è§†é¢‘æ½œåœ¨ç©ºé—´ã€‚è¿™äº›æ½œåœ¨è¡¨ç¤ºç”±ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„éŸ³é¢‘åˆ°è§†é¢‘æ‰©æ•£å˜å‹å™¨ï¼ˆA2V-DiTï¼‰ä¸»å¹²è¿›è¡Œå»ºæ¨¡ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¯¹è¯å¤´åˆæˆã€‚æ­¤å¤–ï¼Œä¸ºç¡®ä¿æ—¶é—´ä¸€è‡´æ€§å’ŒåŠ é€Ÿæ‰©å±•ç”Ÿæˆçš„æ¨ç†ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ç”¨äºè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹çš„æ–°å‹å¼‚æ­¥å™ªå£°è°ƒåº¦å™¨ï¼ˆANSï¼‰ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒREADæ¡†æ¶åœ¨ç”Ÿæˆå…·æœ‰ç«äº‰åŠ›çš„å¯¹è¯å¤´è§†é¢‘æ—¶ï¼Œå®ç°äº†è´¨é‡ä¸é€Ÿåº¦çš„å¹³è¡¡ï¼ŒåŒæ—¶åœ¨é•¿æ—¶é—´ç”Ÿæˆä¸­ä¿æŒäº†ç¨³å¥çš„åº¦é‡ç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘é©±åŠ¨å¯¹è¯å¤´ç”Ÿæˆä¸­æœ‰æ˜¾è‘—è¿›å±•ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>READæ¡†æ¶åˆ©ç”¨å®æ—¶æ‰©æ•£å˜å‹å™¨è¿›è¡Œå¯¹è¯å¤´ç”Ÿæˆã€‚</li>
<li>é€šè¿‡æ—¶é—´å˜åˆ†è‡ªç¼–ç å™¨å­¦ä¹ é«˜åº¦å‹ç¼©çš„è§†é¢‘æ½œåœ¨ç©ºé—´ä»¥åŠ é€Ÿç”Ÿæˆã€‚</li>
<li>é¢„è®­ç»ƒçš„è¯­éŸ³è‡ªç¼–ç å™¨ï¼ˆSpeechAEï¼‰ç”¨äºç”Ÿæˆä¸æ—¶é—´å‹ç¼©çš„è¯­éŸ³æ½œåœ¨ä»£ç ç›¸å¯¹åº”çš„è§†é¢‘æ½œåœ¨ç©ºé—´ã€‚</li>
<li>å¼‚æ­¥å™ªå£°è°ƒåº¦å™¨ï¼ˆANSï¼‰ç”¨äºè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œç¡®ä¿æ—¶é—´ä¸€è‡´æ€§å’ŒåŠ é€Ÿæ‰©å±•ç”Ÿæˆçš„æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a532b8468daf339f42803efdd68b7c0" align="middle">
<img src="https://picx.zhimg.com/v2-731d5dbc10c5ac9d73a7726d8ed3ebf0" align="middle">
<img src="https://picx.zhimg.com/v2-7eae0d8a1445ea860f62593723a3237a" align="middle">
<img src="https://picx.zhimg.com/v2-79207cfd1edc5c217efa7f43aa7abdc9" align="middle">
<img src="https://picx.zhimg.com/v2-ecdb31a262bf872fb8ee3cfb127a4685" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MetaTT-A-Global-Tensor-Train-Adapter-for-Parameter-Efficient-Fine-Tuning"><a href="#MetaTT-A-Global-Tensor-Train-Adapter-for-Parameter-Efficient-Fine-Tuning" class="headerlink" title="MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning"></a>MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning</h2><p><strong>Authors:Javier Lopez-Piqueres, Pranav Deshpande, Archan Ray, Mattia J. Villani, Marco Pistoia, Niraj Kumar</strong></p>
<p>We present MetaTT, a Tensor Train (TT) adapter framework for fine-tuning of pre-trained transformers. MetaTT enables flexible and parameter-efficient model adaptation by using a single shared TT to factorize transformer sub-modules. This factorization indexes key structural dimensions, including layer and matrix type, and can optionally incorporate heads and tasks. This design allows MetaTTâ€™s parameter count to scale with the sum, rather than the product, of the modes, resulting in a substantially more compact adapter. Our benchmarks compare MetaTT with LoRA along with recent state-of-the-art matrix and tensor decomposition based fine-tuning methods. We observe that when tested on single-task standard language modeling benchmarks, MetaTT achieves competitive parameter efficiency to accuracy tradeoff. We further demonstrate that MetaTT performs competitively when compared to state-of-the-art methods on multi-task learning. Finally, we leverage the TT-ansatz to design a rank adaptive optimizer inspired by the DMRG method from many-body physics. Our results demonstrate that integrating this approach with AdamW enhances optimization performance for a specified target rank.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†MetaTTï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé¢„è®­ç»ƒå˜å‹å™¨ç²¾ç»†è°ƒæ•´çš„Tensor Trainï¼ˆTTï¼‰é€‚é…å™¨æ¡†æ¶ã€‚MetaTTé€šè¿‡ä½¿ç”¨å•ä¸ªå…±äº«TTå¯¹å˜å‹å™¨å­æ¨¡å—è¿›è¡Œåˆ†è§£ï¼Œä»è€Œå®ç°äº†çµæ´»ä¸”å‚æ•°é«˜æ•ˆçš„æ¨¡å‹é€‚åº”ã€‚è¿™ç§åˆ†è§£ç´¢å¼•äº†å…³é”®çš„ç»“æ„ç»´åº¦ï¼ŒåŒ…æ‹¬å±‚å’ŒçŸ©é˜µç±»å‹ï¼Œå¹¶å¯ä»¥é€‰æ‹©æ€§åœ°çº³å…¥å¤´éƒ¨å’Œä»»åŠ¡ã€‚è¿™ç§è®¾è®¡ä½¿å¾—MetaTTçš„å‚æ•°è®¡æ•°ä¸æ¨¡å¼ä¹‹å’Œè€Œä¸æ˜¯ä¹˜ç§¯æˆæ¯”ä¾‹ï¼Œä»è€Œäº§ç”Ÿäº†æ›´ç´§å‡‘çš„é€‚é…å™¨ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å°†MetaTTä¸LoRAä»¥åŠæœ€æ–°çš„æœ€å…ˆè¿›çŸ©é˜µå’ŒåŸºäºå¼ é‡åˆ†è§£çš„å¾®è°ƒæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨å•ä»»åŠ¡æ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaTTåœ¨å‚æ•°æ•ˆç‡ä¸å‡†ç¡®åº¦ä¹‹é—´çš„æƒè¡¡æ–¹é¢å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œä¸å¤šä»»åŠ¡å­¦ä¹ çš„æœ€æ–°å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼ŒMetaTTè¡¨ç°è‰¯å¥½ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨TTæ–¹æ³•è®¾è®¡äº†ä¸€ç§å—å¤šä½“ç‰©ç†å­¦ä¸­çš„DMRGæ–¹æ³•å¯å‘çš„ç§©è‡ªé€‚åº”ä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°†è¿™ç§æ–¹æ³•ä¸AdamWç›¸ç»“åˆï¼Œå¯ä»¥æé«˜å¯¹æŒ‡å®šç›®æ ‡ç§©çš„ä¼˜åŒ–æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09105v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MetaTTæ˜¯ä¸€ä¸ªåŸºäºTensor Trainï¼ˆTTï¼‰çš„é€‚é…å™¨æ¡†æ¶ï¼Œç”¨äºé¢„è®­ç»ƒå˜å‹å™¨çš„å¾®è°ƒã€‚å®ƒé€šè¿‡å•ä¸ªå…±äº«çš„TTå¯¹å˜å‹å™¨å­æ¨¡å—è¿›è¡Œå› å­åˆ†è§£ï¼Œå®ç°çµæ´»ä¸”å‚æ•°é«˜æ•ˆçš„æ¨¡å‹é€‚åº”ã€‚è¯¥è®¾è®¡ä½¿MetaTTçš„å‚æ•°è®¡æ•°éšæ¨¡å¼ä¹‹å’Œè€Œä¸æ˜¯ä¹˜ç§¯è€Œæ‰©å±•ï¼Œä»è€Œäº§ç”Ÿæ›´ç´§å‡‘çš„é€‚é…å™¨ã€‚åœ¨æ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaTTåœ¨å‚æ•°æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´è¾¾åˆ°ç«äº‰æ°´å¹³ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸Šä¸æœ€æ–°æŠ€æœ¯æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æœ€åï¼Œåˆ©ç”¨TT-ansatzè®¾è®¡äº†ä¸€ç§å—å¤šä½“ç‰©ç†å­¦DMRGæ–¹æ³•å¯å‘çš„ç§©è‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼Œä¸AdamWé›†æˆåï¼Œå¯æé«˜é’ˆå¯¹æŒ‡å®šç›®æ ‡ç§©çš„ä¼˜åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaTTæ˜¯ä¸€ä¸ªåŸºäºTensor Trainï¼ˆTTï¼‰çš„é€‚é…å™¨æ¡†æ¶ï¼Œç”¨äºé¢„è®­ç»ƒå˜å‹å™¨çš„çµæ´»å¾®è°ƒã€‚</li>
<li>MetaTTé€šè¿‡å› å­åˆ†è§£å®ç°å‚æ•°é«˜æ•ˆçš„æ¨¡å‹é€‚åº”ï¼Œä½¿ç”¨å•ä¸ªå…±äº«çš„TTå¯¹å˜å‹å™¨å­æ¨¡å—è¿›è¡Œç´¢å¼•ã€‚</li>
<li>MetaTTçš„å‚æ•°è®¾è®¡ä½¿å…¶æ›´ç´§å‡‘ï¼Œå‚æ•°è®¡æ•°éšæ¨¡å¼ä¹‹å’Œæ‰©å±•ï¼Œè€Œéä¹˜ç§¯ã€‚</li>
<li>åœ¨æ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaTTåœ¨å‚æ•°æ•ˆç‡ä¸å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>MetaTTåœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸Šä¸æœ€æ–°æŠ€æœ¯æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>åˆ©ç”¨TT-ansatzè®¾è®¡çš„ç§©è‡ªé€‚åº”ä¼˜åŒ–å™¨å—å¤šä½“ç‰©ç†å­¦DMRGæ–¹æ³•å¯å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-415338dd87bbe6770e6f33e7adef8bde" align="middle">
<img src="https://picx.zhimg.com/v2-3b5132ebf79e0ec0fa65f88f2af26196" align="middle">
<img src="https://picx.zhimg.com/v2-d3c8e74cf25d960020e8a40028287bc9" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lina-Speech-Gated-Linear-Attention-and-Initial-State-Tuning-for-Multi-Sample-Prompting-Text-To-Speech-Synthesis"><a href="#Lina-Speech-Gated-Linear-Attention-and-Initial-State-Tuning-for-Multi-Sample-Prompting-Text-To-Speech-Synthesis" class="headerlink" title="Lina-Speech: Gated Linear Attention and Initial-State Tuning for Multi-Sample Prompting Text-To-Speech Synthesis"></a>Lina-Speech: Gated Linear Attention and Initial-State Tuning for Multi-Sample Prompting Text-To-Speech Synthesis</h2><p><strong>Authors:ThÃ©odor Lemerle, TÃ©o Guichoux, Axel Roebel, Nicolas Obin</strong></p>
<p>Neural codec language models, built on transformer architecture, have revolutionized text-to-speech (TTS) synthesis, excelling in voice cloning by treating it as a prefix continuation task. However, their limited context length hinders their effectiveness to short speech samples. As a result, the voice cloning ability is restricted to a limited coverage and diversity of the speakerâ€™s prosody and style. Besides, adapting prosody, accent, or appropriate emotion from a short prefix remains a challenging task. Finally, the quadratic complexity of self-attention limits inference throughput. In this work, we introduce Lina-Speech, a TTS model with Gated Linear Attention (GLA) to replace standard self-attention as a principled backbone, improving inference throughput while matching state-of-the-art performance. Leveraging the stateful property of recurrent architecture, we introduce an Initial-State Tuning (IST) strategy that unlocks the possibility of multiple speech sample conditioning of arbitrary numbers and lengths and provides a comprehensive and efficient strategy for voice cloning and out-of-domain speaking style and emotion adaptation. We demonstrate the effectiveness of this approach for controlling fine-grained characteristics such as prosody and emotion. Code, checkpoints, and demo are freely available: <a target="_blank" rel="noopener" href="https://github.com/theodorblackbird/lina-speech">https://github.com/theodorblackbird/lina-speech</a></p>
<blockquote>
<p>åŸºäºç¥ç»ç½‘ç»œç¼–ç å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œå»ºç«‹åœ¨Transformeræ¶æ„ä¹‹ä¸Šï¼Œå·²ç»å½»åº•æ”¹å˜äº†æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„åˆæˆæ–¹å¼ï¼Œé€šè¿‡å°†è¯­éŸ³å…‹éš†è§†ä¸ºå‰ç¼€å»¶ç»­ä»»åŠ¡ï¼Œå…¶åœ¨è¯­éŸ³å…‹éš†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶æœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶äº†å®ƒä»¬åœ¨çŸ­è¯­éŸ³æ ·æœ¬ä¸­çš„æœ‰æ•ˆæ€§ã€‚å› æ­¤ï¼Œè¯­éŸ³å…‹éš†èƒ½åŠ›ä»…é™äºå¯¹è¯´è¯äººçš„è¯­è°ƒå’Œé£æ ¼çš„æœ‰é™è¦†ç›–å’Œå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œä»ç®€çŸ­çš„å‰ç¼€ä¸­é€‚åº”è¯­è°ƒã€å£éŸ³æˆ–é€‚å½“çš„æƒ…ç»ªä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ€åï¼Œè‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚æ€§é™åˆ¶äº†æ¨ç†ååé‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Lina-Speechï¼Œè¿™æ˜¯ä¸€ç§å¸¦æœ‰é—¨æ§çº¿æ€§æ³¨æ„åŠ›ï¼ˆGLAï¼‰çš„TTSæ¨¡å‹ï¼Œå®ƒæ›¿ä»£äº†æ ‡å‡†çš„è‡ªæ³¨æ„åŠ›ä½œä¸ºæ ¸å¿ƒæ¶æ„ï¼Œæé«˜äº†æ¨ç†ååé‡ï¼ŒåŒæ—¶åŒ¹é…äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨é€’å½’æ¶æ„çš„statefulå±æ€§ï¼Œå¼•å…¥äº†ä¸€ç§åˆå§‹çŠ¶æ€è°ƒæ•´ï¼ˆISTï¼‰ç­–ç•¥ï¼Œè¿™ç§ç­–ç•¥å¼€å¯äº†ä»»æ„æ•°é‡å’Œé•¿åº¦çš„å¤šä¸ªè¯­éŸ³æ ·æœ¬æ¡ä»¶çš„å¯èƒ½æ€§ï¼Œå¹¶ä¸ºè¯­éŸ³å…‹éš†ä»¥åŠåŸŸå¤–è¯´è¯é£æ ¼å’Œæƒ…æ„Ÿé€‚åº”æä¾›äº†å…¨é¢è€Œæœ‰æ•ˆçš„ç­–ç•¥ã€‚æˆ‘ä»¬è¯æ˜äº†è¿™ç§æ–¹æ³•åœ¨æ§åˆ¶ç»†å¾®ç‰¹å¾ï¼ˆå¦‚è¯­è°ƒå’Œæƒ…æ„Ÿï¼‰æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç ã€æ£€æŸ¥ç‚¹å’Œæ¼”ç¤ºå¯å…è´¹è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/theodorblackbird/lina-speech">https://github.com/theodorblackbird/lina-speech</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23320v2">PDF</a> Audio-AAAI Workshop, 2026</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œç¼–ç è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆé¢†åŸŸå®ç°äº†é©å‘½æ€§çš„çªç ´ï¼Œå°¤å…¶åœ¨è¯­éŸ³å…‹éš†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶æœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶äº†çŸ­è¯­éŸ³æ ·æœ¬çš„æœ‰æ•ˆæ€§ï¼Œå¯¼è‡´è¯­éŸ³å…‹éš†èƒ½åŠ›åœ¨è¯´è¯è€…çš„è¯­è°ƒå’Œé£æ ¼æ–¹é¢çš„è¦†ç›–èŒƒå›´å’Œå¤šæ ·æ€§æœ‰é™ã€‚æ­¤å¤–ï¼Œä»çŸ­å‰ç¼€ä¸­é€‚åº”è¯­è°ƒã€å£éŸ³æˆ–é€‚å½“çš„æƒ…ç»ªä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLina-Speechçš„TTSæ¨¡å‹ï¼Œé‡‡ç”¨é—¨æ§çº¿æ€§æ³¨æ„åŠ›ï¼ˆGLAï¼‰æ›¿ä»£æ ‡å‡†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦å¹¶ä¿æŒäº†æœ€æ–°æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆå§‹çŠ¶æ€è°ƒæ•´ï¼ˆISTï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨é€’å½’æ¶æ„çš„çŠ¶æ€ç‰¹æ€§ï¼Œå®ç°äº†ä»»æ„æ•°é‡å’Œé•¿åº¦çš„å¤šä¸ªè¯­éŸ³æ ·æœ¬æ¡ä»¶åŒ–ï¼Œä¸ºè¯­éŸ³å…‹éš†å’Œè·¨åŸŸè¯­éŸ³é£æ ¼å’Œæƒ…æ„Ÿé€‚åº”æä¾›äº†å…¨é¢æœ‰æ•ˆçš„ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç¥ç»ç½‘ç»œç¼–ç è¯­è¨€æ¨¡å‹åœ¨TTSé¢†åŸŸçš„è¯­éŸ³å…‹éš†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶äº†çŸ­è¯­éŸ³æ ·æœ¬çš„æœ‰æ•ˆæ€§ï¼Œå½±å“è¯­éŸ³å…‹éš†çš„è¦†ç›–å’Œå¤šæ ·æ€§ã€‚</li>
<li>é€‚åº”è¯­è°ƒã€å£éŸ³å’Œæƒ…ç»ªä»çŸ­å‰ç¼€ä¸­ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>Lina-Speechæ¨¡å‹é‡‡ç”¨é—¨æ§çº¿æ€§æ³¨æ„åŠ›ï¼ˆGLAï¼‰æé«˜æ¨ç†é€Ÿåº¦å¹¶ä¿æŒæœ€æ–°æ€§èƒ½ã€‚</li>
<li>åˆå§‹çŠ¶æ€è°ƒæ•´ï¼ˆISTï¼‰ç­–ç•¥åˆ©ç”¨é€’å½’æ¶æ„çš„çŠ¶æ€ç‰¹æ€§ï¼Œå®ç°ä»»æ„æ•°é‡å’Œé•¿åº¦çš„è¯­éŸ³æ ·æœ¬æ¡ä»¶åŒ–ã€‚</li>
<li>Lina-Speechä¸ºè¯­éŸ³å…‹éš†å’Œè·¨åŸŸè¯­éŸ³é£æ ¼åŠæƒ…æ„Ÿé€‚åº”æä¾›äº†å…¨é¢æœ‰æ•ˆçš„ç­–ç•¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.23320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a44497310260886c518450d34063a4a" align="middle">
<img src="https://picx.zhimg.com/v2-dde9388401ea574aea9c6e1165a29122" align="middle">
<img src="https://picx.zhimg.com/v2-82adf92651164e91abfa3e2a2646ae75" align="middle">
<img src="https://picx.zhimg.com/v2-88f2e24151d614bce3dbb2e2f437c6f1" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-32ae26372073d422b8e441f96267fe94" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Mem-PAL Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d998ab0a97627d7ea7a195593cb6cf60" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Free-Form Scene Editor Enabling Multi-Round Object Manipulation like in a 3D Engine
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
