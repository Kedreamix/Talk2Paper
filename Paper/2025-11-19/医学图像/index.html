<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Free-Form Scene Editor Enabling Multi-Round Object Manipulation like in a 3D Engine">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d998ab0a97627d7ea7a195593cb6cf60')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="Free-Form-Scene-Editor-Enabling-Multi-Round-Object-Manipulation-like-in-a-3D-Engine"><a href="#Free-Form-Scene-Editor-Enabling-Multi-Round-Object-Manipulation-like-in-a-3D-Engine" class="headerlink" title="Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine"></a>Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine</h2><p><strong>Authors:Xincheng Shuai, Zhenyuan Qin, Henghui Ding, Dacheng Tao</strong></p>
<p>Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.</p>
<blockquote>
<p>æœ€è¿‘æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•æ˜¾è‘—åœ°æé«˜äº†è¯­ä¹‰å›¾åƒç¼–è¾‘èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•åœ¨æ‰§è¡Œ3Dæ„ŸçŸ¥å¯¹è±¡æ“ä½œæ—¶ä»ç„¶ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FFSEï¼Œè¿™æ˜¯ä¸€ä¸ª3Dæ„ŸçŸ¥è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°åœ¨ç°å®ä¸–ç•Œå›¾åƒä¸Šç›´æ¥è¿›è¡Œç›´è§‚ã€ç‰©ç†ä¸€è‡´çš„å¯¹è±¡ç¼–è¾‘ã€‚ä¸åŒäºä»¥å‰çš„æ–¹æ³•ï¼Œå®ƒä»¬è¦ä¹ˆåœ¨å›¾åƒç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œè¦ä¹ˆéœ€è¦ç¼“æ…¢ä¸”å®¹æ˜“å‡ºé”™çš„3Dé‡å»ºï¼ŒFFSEå°†ç¼–è¾‘å»ºæ¨¡ä¸ºä¸€ç³»åˆ—å­¦ä¹ çš„3Dè½¬æ¢ï¼Œå…è®¸ç”¨æˆ·æ‰§è¡Œä»»æ„æ“ä½œï¼Œå¦‚å¹³ç§»ã€ç¼©æ”¾å’Œæ—‹è½¬ï¼ŒåŒæ—¶ä¿ç•™é€¼çœŸçš„èƒŒæ™¯æ•ˆæœï¼ˆä¾‹å¦‚é˜´å½±ã€åå°„ï¼‰å¹¶ä¿æŒå…¨å±€åœºæ™¯åœ¨å¤šè½®ç¼–è¾‘ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†æ”¯æŒå¤šè½®3Dæ„ŸçŸ¥å¯¹è±¡æ“ä½œçš„å­¦ä¹ ï¼Œæˆ‘ä»¬å¼•å…¥äº†3DObjectEditorï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆæ•°æ®é›†ï¼Œç”±å„ç§å¯¹è±¡å’Œåœºæ™¯ä¸­çš„æ¨¡æ‹Ÿç¼–è¾‘åºåˆ—æ„æˆï¼Œèƒ½å¤Ÿåœ¨å¤šè½®å’ŒåŠ¨æ€æ¡ä»¶ä¸‹è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„FFSEåœ¨å•è½®å’Œå¤šè½®3Dæ„ŸçŸ¥ç¼–è¾‘åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13713v1">PDF</a> AAAI 2026, Project Page: <a target="_blank" rel="noopener" href="https://henghuiding.com/FFSE/">https://henghuiding.com/FFSE/</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æå¤§åœ°æ¨åŠ¨äº†è¯­ä¹‰å›¾åƒç¼–è¾‘çš„å‘å±•ï¼Œä½†åœ¨3Dæ„ŸçŸ¥ç‰©ä½“æ“ä½œæ–¹é¢ï¼Œå¤§å¤šæ•°æ–¹æ³•ä»æ˜¾ä¸è¶³ã€‚æœ¬ç ”ç©¶æå‡ºFFSEï¼Œä¸€ä¸ª3Dæ„ŸçŸ¥çš„è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¯¹çœŸå®ä¸–ç•Œå›¾åƒçš„ç›´è§‚ã€ç‰©ç†ä¸€è‡´çš„ç‰©ä½“ç¼–è¾‘ã€‚ä¸åŒäºå…¶ä»–æ–¹æ³•åœ¨å›¾åƒç©ºé—´æ“ä½œæˆ–éœ€è¦ç¼“æ…¢ä¸”æ˜“å‡ºé”™çš„3Dé‡å»ºï¼ŒFFSEå°†ç¼–è¾‘å»ºæ¨¡ä¸ºä¸€ç³»åˆ—å­¦ä¹ çš„3Dè½¬æ¢ï¼Œå…è®¸ç”¨æˆ·è¿›è¡Œä»»æ„æ“ä½œï¼Œå¦‚å¹³ç§»ã€ç¼©æ”¾å’Œæ—‹è½¬ï¼ŒåŒæ—¶ä¿æŒèƒŒæ™¯æ•ˆæœï¼ˆå¦‚é˜´å½±ã€åå°„ï¼‰çš„çœŸå®æ€§ï¼Œå¹¶åœ¨å¤šæ¬¡ç¼–è¾‘ä¸­ä¿æŒå…¨å±€åœºæ™¯çš„ä¸€è‡´æ€§ã€‚ä¸ºæ”¯æŒå¤šè½®3Dæ„ŸçŸ¥ç‰©ä½“æ“ä½œçš„å­¦ä¹ ï¼Œæˆ‘ä»¬å¼•å…¥äº†3DObjectEditorï¼Œä¸€ä¸ªç”±æ¨¡æ‹Ÿç¼–è¾‘åºåˆ—æ„å»ºçš„æ··åˆæ•°æ®é›†ï¼Œæ¶µç›–å„ç§ç‰©ä½“å’Œåœºæ™¯ï¼Œä»¥æ”¯æŒå¤šè½®å’ŒåŠ¨æ€æ¡ä»¶ä¸‹çš„æœ‰æ•ˆè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒFFSEåœ¨å•è½®å’Œå¤šè½®3Dæ„ŸçŸ¥ç¼–è¾‘åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²æ¨åŠ¨è¯­ä¹‰å›¾åƒç¼–è¾‘çš„æ˜¾è‘—æå‡ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨3Dæ„ŸçŸ¥ç‰©ä½“æ“ä½œæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>FFSEæ˜¯ä¸€ä¸ª3Dæ„ŸçŸ¥è‡ªå›å½’æ¡†æ¶ï¼Œæ”¯æŒçœŸå®å›¾åƒçš„ç›´æ¥ç¼–è¾‘ï¼ŒåŒ…æ‹¬å¹³ç§»ã€ç¼©æ”¾å’Œæ—‹è½¬ç­‰ã€‚</li>
<li>FFSEèƒ½ç»´æŒèƒŒæ™¯æ•ˆæœçš„çœŸå®æ€§ï¼Œå¹¶åœ¨å¤šæ¬¡ç¼–è¾‘ä¸­ä¿æŒå…¨å±€åœºæ™¯çš„ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥çš„3DObjectEditoræ•°æ®é›†ç”¨äºæ”¯æŒå¤šè½®3Dæ„ŸçŸ¥ç‰©ä½“æ“ä½œçš„å­¦ä¹ ã€‚</li>
<li>FFSEåœ¨å•è½®å’Œå¤šè½®3Dæ„ŸçŸ¥ç¼–è¾‘åœºæ™¯ä¸­æ€§èƒ½ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e486ec630255a9e088d4dcef0df4d2b" align="middle">
<img src="https://picx.zhimg.com/v2-e2057da5dc3605910014efe68f3c499c" align="middle">
<img src="https://picx.zhimg.com/v2-30ce8ae4a7b0023ac92f38006a856676" align="middle">
<img src="https://picx.zhimg.com/v2-4649d3cb51767b6901c20a70be5cf876" align="middle">
<img src="https://picx.zhimg.com/v2-047708640f62851445a559dc7e453b84" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Training-Free-Multi-View-Extension-of-IC-Light-for-Textual-Position-Aware-Scene-Relighting"><a href="#Training-Free-Multi-View-Extension-of-IC-Light-for-Textual-Position-Aware-Scene-Relighting" class="headerlink" title="Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting"></a>Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting</h2><p><strong>Authors:Jiangnan Ye, Jiedong Zhuang, Lianrui Mu, Wenjie Zheng, Jiaqi Hu, Xingze Zou, Jing Wang, Haoji Hu</strong></p>
<p>We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†GS-Lightï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ã€å¯¹æ–‡æœ¬ä½ç½®æœ‰æ„è¯†çš„ç®¡é“ï¼Œç”¨äºå¯¹é€šè¿‡é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºçš„3Dåœºæ™¯è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„é‡ç…§æ˜ã€‚GS-Lightå®ç°äº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å•è¾“å…¥æ‰©æ•£æ¨¡å‹çš„å¤šè§†å›¾è¾“å…¥æ‰©å±•ã€‚ç»™å®šç”¨æˆ·æç¤ºï¼Œå¯èƒ½æŒ‡å®šç…§æ˜æ–¹å‘ã€é¢œè‰²ã€å¼ºåº¦æˆ–å‚è€ƒå¯¹è±¡ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å°†æç¤ºè§£æä¸ºç…§æ˜å…ˆéªŒã€‚æˆ‘ä»¬ä½¿ç”¨ç°æˆçš„å‡ ä½•å’Œè¯­ä¹‰ä¼°è®¡å™¨ï¼ˆæ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ï¼Œå°†è¿™äº›ç…§æ˜å…ˆéªŒä¸è§†å›¾å‡ ä½•çº¦æŸèåˆï¼Œä»¥è®¡ç®—ç…§æ˜åœ°å›¾å¹¶ä¸ºæ¯ä¸ªè§†å›¾ç”Ÿæˆåˆå§‹æ½œåœ¨ä»£ç ã€‚è¿™äº›ç²¾å¿ƒå¾—å‡ºçš„åˆå§‹æ½œåœ¨ä»£ç æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®åœ°åæ˜ ç”¨æˆ·æœŸæœ›çš„ç…§æ˜è¾“å‡ºï¼Œå°¤å…¶æ˜¯åœ¨ç…§æ˜æ–¹å‘æ–¹é¢ã€‚é€šè¿‡å°†ä»¥å¤šè§†å›¾å‘ˆç°çš„å›¾åƒå’Œåˆå§‹æ½œåœ¨ä»£ç è¾“å…¥åˆ°æˆ‘ä»¬çš„å¤šè§†å›¾ç…§æ˜æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é«˜ä¿çœŸã€è‰ºæœ¯åŒ–çš„ç…§æ˜å›¾åƒã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨é‡ç…§æ˜çš„å¤–è§‚å¯¹3DGSåœºæ™¯è¿›è¡Œå¾®è°ƒï¼Œä»¥è·å¾—å®Œå…¨é‡ç…§æ˜çš„3Dåœºæ™¯ã€‚æˆ‘ä»¬åœ¨å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸Šè¯„ä¼°äº†GS-Lightï¼Œå°†å…¶ä¸æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼ˆåŒ…æ‹¬å•è§†å›¾é‡ç…§æ˜ã€è§†é¢‘é‡ç…§æ˜å’Œåœºæ™¯ç¼–è¾‘æ–¹æ³•ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚ä½¿ç”¨å®šé‡æŒ‡æ ‡ï¼ˆå¤šè§†å›¾ä¸€è‡´æ€§ã€æˆåƒè´¨é‡ã€ç¾å­¦åˆ†æ•°ã€è¯­ä¹‰ç›¸ä¼¼æ€§ç­‰ï¼‰å’Œå®šæ€§è¯„ä¼°ï¼ˆç”¨æˆ·ç ”ç©¶ï¼‰ï¼ŒGS-Lightåœ¨åŸºçº¿æ–¹æ³•ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„ä¼˜åŠ¿ã€‚ä»£ç å’Œèµ„æºå°†åœ¨å‘å¸ƒæ—¶æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13684v1">PDF</a> Submitting for Neurocomputing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GS-Lightï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆã€å¯¹æ–‡æœ¬ä½ç½®æ•æ„Ÿçš„ç®¡é“ï¼Œç”¨äºå¯¹é€šè¿‡é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºçš„3Dåœºæ™¯è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„é‡ç…§æ˜ã€‚GS-Lightå®ç°äº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å•è¾“å…¥æ‰©æ•£æ¨¡å‹çš„å¤šè§†å›¾è¾“å…¥æ‰©å±•ã€‚é€šè¿‡ç”¨æˆ·æç¤ºï¼ˆå¯èƒ½æŒ‡å®šç…§æ˜æ–¹å‘ã€é¢œè‰²ã€å¼ºåº¦æˆ–å‚è€ƒå¯¹è±¡ï¼‰ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å°†æç¤ºè§£æä¸ºç…§æ˜å…ˆéªŒã€‚ç»“åˆç°æˆçš„å‡ ä½•å’Œè¯­ä¹‰ä¼°è®¡å™¨ï¼ˆæ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ï¼Œæˆ‘ä»¬å°†è¿™äº›ç…§æ˜å…ˆéªŒä¸è§†å›¾å‡ ä½•çº¦æŸèåˆï¼Œè®¡ç®—ç…§æ˜å›¾å¹¶ä¸ºæ¯ä¸ªè§†å›¾ç”Ÿæˆåˆå§‹æ½œåœ¨ä»£ç ã€‚è¿™äº›ç²¾å¿ƒå¾—å‡ºçš„åˆå§‹æ½œåœ¨ä»£ç æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®åœ°åæ˜ ç”¨æˆ·æœŸæœ›çš„ç…§æ˜è¾“å‡ºï¼Œå°¤å…¶æ˜¯åœ¨ç…§æ˜æ–¹å‘æ–¹é¢ã€‚é€šè¿‡å‘å¤šè§†å›¾é‡ç…§æ˜æ¨¡å‹è¾“å…¥å¤šè§†å›¾æ¸²æŸ“å›¾åƒä»¥åŠåˆå§‹æ½œåœ¨ä»£ç ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é«˜ä¿çœŸã€è‰ºæœ¯æ€§é‡ç…§æ˜çš„å›¾åƒã€‚æœ€åï¼Œä½¿ç”¨é‡ç…§æ˜çš„å¤–è§‚å¯¹3DGSåœºæ™¯è¿›è¡Œå¾®è°ƒï¼Œä»¥è·å¾—å®Œå…¨é‡ç…§æ˜çš„3Dåœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GS-Lightæ˜¯ä¸€ç§ç”¨äº3Dåœºæ™¯æ–‡æœ¬å¼•å¯¼é‡ç…§æ˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å®ƒé€šè¿‡ç»“åˆç”¨æˆ·æç¤ºã€ç…§æ˜å…ˆéªŒã€è§†å›¾å‡ ä½•çº¦æŸæ¥ç”Ÿæˆç…§æ˜å›¾ã€‚</li>
<li>GS-Lighté‡‡ç”¨å¤šè§†å›¾æ¸²æŸ“å›¾åƒå’Œåˆå§‹æ½œåœ¨ä»£ç æ¥ç”Ÿæˆé«˜ä¿çœŸã€è‰ºæœ¯æ€§é‡ç…§æ˜å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒ3Dåœºæ™¯ä»¥åŒ¹é…é‡ç…§æ˜çš„å¤–è§‚ï¼Œå®ç°å…¨åœºæ™¯çš„ç…§æ˜ã€‚</li>
<li>GS-Lightåœ¨å®¤å†…å¤–åœºæ™¯ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</li>
<li>é€šè¿‡å®šé‡æŒ‡æ ‡ï¼ˆå¦‚å¤šè§†å›¾ä¸€è‡´æ€§ã€æˆåƒè´¨é‡ã€ç¾å­¦è¯„åˆ†ã€è¯­ä¹‰ç›¸ä¼¼æ€§ï¼‰å’Œå®šæ€§è¯„ä¼°ï¼ˆç”¨æˆ·ç ”ç©¶ï¼‰ï¼ŒéªŒè¯äº†GS-Lightçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5de1aded6bd287d2278b1b06d3ab4d1e" align="middle">
<img src="https://picx.zhimg.com/v2-cf50de6bd2a2b021c48557a52d59e1ee" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Delineate-Anything-Flow-Fast-Country-Level-Field-Boundary-Detection-from-Any-Source"><a href="#Delineate-Anything-Flow-Fast-Country-Level-Field-Boundary-Detection-from-Any-Source" class="headerlink" title="Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source"></a>Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source</h2><p><strong>Authors:Mykola Lavreniuk, Nataliia Kussul, Andrii Shelestov, Yevhenii Salii, Volodymyr Kuzin, Sergii Skakun, Zoltan Szantoi</strong></p>
<p>Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at <a target="_blank" rel="noopener" href="https://lavreniuk.github.io/Delineate-Anything/">https://lavreniuk.github.io/Delineate-Anything/</a>.</p>
<blockquote>
<p>ä»å«æ˜Ÿå›¾åƒå‡†ç¡®æç»˜å†œä¸šç”°ç•Œå¯¹åœŸåœ°ç®¡ç†å’Œä½œç‰©ç›‘æµ‹è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€äº§ç”Ÿä¸å®Œæ•´çš„è¾¹ç•Œï¼Œåˆå¹¶ç›¸é‚»ç”°åœ°ï¼Œå¹¶ä¸”éš¾ä»¥æ‰©å±•ã€‚æˆ‘ä»¬æå‡ºäº†Delineate Anything Flowï¼ˆDelAnyFlowï¼‰æ–¹æ³•è®ºï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤§è§„æ¨¡ç”°ç•Œæ˜ å°„çš„åˆ†è¾¨ç‡æ— å…³æ–¹æ³•ã€‚DelAnyFlowç»“åˆäº†åŸºäºYOLOv11ä¸»å¹²ç½‘çš„DelAnyå®ä¾‹åˆ†å‰²æ¨¡å‹ï¼Œä»¥åŠåœ¨å¤§è§„æ¨¡Field Boundary Instance Segmentation-22Mï¼ˆFBIS 22Mï¼‰æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œé‡‡ç”¨ç»“æ„åŒ–åå¤„ç†ã€åˆå¹¶å’ŒçŸ¢é‡åŒ–åºåˆ—ï¼Œä»¥ç”Ÿæˆæ‹“æ‰‘ä¸€è‡´çš„çŸ¢é‡è¾¹ç•Œã€‚FBIS 22Mæ˜¯åŒç±»ä¸­æœ€å¤§çš„æ•°æ®é›†ï¼ŒåŒ…å«672909ä¸ªå¤šåˆ†è¾¨ç‡å›¾åƒè¡¥ä¸ï¼ˆ0.25-10mï¼‰å’Œ2290ä¸‡ä¸ªéªŒè¯çš„å­—æ®µå®ä¾‹ã€‚DelAnyæ¨¡å‹ä»¥è¶…è¿‡SAM2çš„100ï¼…ä»¥ä¸Šçš„mAPå’Œ400å€æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚DelAnyè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ”¯æŒå›½å®¶çº§åº”ç”¨ï¼šä½¿ç”¨Sentinel 2æ•°æ®è‡³2024å¹´ï¼ŒDelAnyFlowåœ¨å•ä¸ªå·¥ä½œç«™ä¸åˆ°å…­å°æ—¶å†…ä¸ºä¹Œå…‹å…°ï¼ˆ603000å¹³æ–¹å…¬é‡Œï¼‰ç”Ÿæˆäº†å®Œæ•´çš„ç”°ç•Œå±‚ã€‚ç›¸å¯¹äºSinergise Solutionså’ŒNASA Harvestçš„æ“ä½œäº§å“ï¼ŒDelAnyFlowçš„è¾“å‡ºåœ¨è¾¹ç•Œå®Œæ•´æ€§æ–¹é¢æœ‰äº†æ˜¾è‘—æ”¹å–„ï¼Œç‰¹åˆ«æ˜¯åœ¨å°å†œæˆ·å’Œç¢ç‰‡åŒ–ç³»ç»Ÿï¼ˆ0.25-1å…¬é¡·ï¼‰ä¸­æ›´ä¸ºæ˜æ˜¾ã€‚å¯¹äºä¹Œå…‹å…°ï¼ŒDelAnyFlowåœ¨5må¤„åˆ’åˆ†äº†375ä¸‡ä¸ªå­—æ®µï¼Œåœ¨2.5må¤„åˆ’åˆ†äº†515ä¸‡ä¸ªå­—æ®µï¼Œè€ŒSinergise Solutionsæ£€æµ‹åˆ°266ä¸‡ä¸ªå­—æ®µï¼ŒNASA Harvestæ£€æµ‹åˆ°169ä¸‡ä¸ªå­—æ®µã€‚è¿™é¡¹å·¥ä½œä¸ºç¼ºä¹æ•°å­—åœ°ç±æ•°æ®çš„åœ°åŒºæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„ç”°åœ°åˆ’åˆ†æ–¹æ³•ã€‚æœ‰å…³æ¨¡å‹æƒé‡ã€ä»£ç ã€å›½å®¶çº§çŸ¢é‡è¾“å‡ºå’Œæ•°æ®é›†çš„é“¾æ¥å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://lavreniuk.github.io/Delineate-Anything/%E4%B8%8A%E6%89%BE%E5%88%B0%E9%A1%B9%E7%9B%AE%E7%9D%80%E9%99%86%E9%A1%B5%E9%9D%A2%E3%80%82">https://lavreniuk.github.io/Delineate-Anything/ä¸Šæ‰¾åˆ°é¡¹ç›®ç€é™†é¡µé¢ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13417v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå¤§è§„æ¨¡å†œç”°è¾¹ç•Œæ˜ å°„çš„Delineate Anything Flowï¼ˆDelAnyFlowï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆDelAnyå®ä¾‹åˆ†å‰²æ¨¡å‹å’Œç»“æ„åŒ–åå¤„ç†ã€åˆå¹¶å’ŒçŸ¢é‡åŒ–åºåˆ—ï¼Œç”Ÿæˆæ‹“æ‰‘ä¸€è‡´çš„çŸ¢é‡è¾¹ç•Œï¼Œå®ç°äº†å†œä¸šç”°é—´ç•Œé™çš„å‡†ç¡®æç»˜ã€‚å…¶ä¸­ï¼Œæ‰€ç”¨çš„FBIS 22Mæ•°æ®é›†æ˜¯ç›®å‰æœ€å¤§è§„æ¨¡çš„æ•°æ®é›†ä¹‹ä¸€ï¼ŒåŒ…å«672,909ä¸ªå¤šåˆ†è¾¨ç‡å›¾åƒæ–‘å—å’Œ2290ä¸‡ä¸ªéªŒè¯çš„ç”°é—´å®ä¾‹ã€‚DelAnyæ¨¡å‹å…·æœ‰è¶…é«˜çš„å‡†ç¡®åº¦å’Œæ¨ç†é€Ÿåº¦ï¼Œæ”¯æŒå¤§è§„æ¨¡åº”ç”¨ã€‚åœ¨ä¹Œå…‹å…°çš„æµ‹è¯•ä¸­ï¼ŒDelAnyFlowåœ¨å•ä¸ªå·¥ä½œç«™å†…ä¸åˆ°å…­å°æ—¶å†…å®Œæˆäº†æ•´ä¸ªå›½å®¶çš„å†œç”°è¾¹ç•Œå±‚ç”Ÿæˆã€‚æ­¤æ–¹æ³•æé«˜äº†è¾¹ç•Œå®Œæ•´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°å‹å’Œåˆ†æ•£çš„ç³»ç»Ÿä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Delineate Anything Flow (DelAnyFlow)æ–¹æ³•ç»“åˆäº†DelAnyå®ä¾‹åˆ†å‰²æ¨¡å‹å’Œç»“æ„åŒ–åå¤„ç†åºåˆ—ï¼Œç”¨äºå¤§è§„æ¨¡å†œç”°è¾¹ç•Œæ˜ å°„ã€‚</li>
<li>DelAnyFlowä½¿ç”¨çš„FBIS 22Mæ•°æ®é›†æ˜¯ç›®å‰æœ€å¤§çš„åŒç±»æ•°æ®é›†ä¹‹ä¸€ï¼ŒåŒ…å«å¤§é‡å¤šåˆ†è¾¨ç‡å›¾åƒæ–‘å—å’ŒéªŒè¯çš„ç”°é—´å®ä¾‹ã€‚</li>
<li>DelAnyæ¨¡å‹å…·æœ‰è¶…é«˜çš„å‡†ç¡®åº¦å’Œæ¨ç†é€Ÿåº¦ï¼Œç›¸è¾ƒäºSAM2æ¨¡å‹ï¼Œå…¶mAPè¶…è¿‡100%ï¼Œæ¨ç†é€Ÿåº¦æå‡400å€ã€‚</li>
<li>DelAnyFlowå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ”¯æŒå›½å®¶è§„æ¨¡çš„åº”ç”¨ã€‚</li>
<li>åœ¨ä¹Œå…‹å…°çš„æµ‹è¯•ä¸­ï¼ŒDelAnyFlowåœ¨å…­å°æ—¶å†…ç”Ÿæˆäº†å®Œæ•´çš„å†œç”°è¾¹ç•Œå±‚ï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜æ•ˆæ€§ã€‚</li>
<li>DelAnyFlowæé«˜äº†è¾¹ç•Œå®Œæ•´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°å‹å’Œåˆ†æ•£çš„ç³»ç»Ÿä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºå…¶ä»–äº§å“å¦‚Sinergise Solutionså’ŒNASA Harvestæœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13417">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29ab5bc9562eb283725152bbe1aaae36" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TripleFDS-Triple-Feature-Disentanglement-and-Synthesis-for-Scene-Text-Editing"><a href="#TripleFDS-Triple-Feature-Disentanglement-and-Synthesis-for-Scene-Text-Editing" class="headerlink" title="TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing"></a>TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing</h2><p><strong>Authors:Yuchen Bao, Yiting Wang, Wenjian Huang, Haowei Wang, Shen Chen, Taiping Yao, Shouhong Ding, Jianguo Zhang</strong></p>
<p>Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the â€œSCB Groupâ€, a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent â€œshortcutâ€ phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: <a target="_blank" rel="noopener" href="https://github.com/yusenbao01/TripleFDS">https://github.com/yusenbao01/TripleFDS</a></p>
<blockquote>
<p>åœºæ™¯æ–‡æœ¬ç¼–è¾‘ï¼ˆSTEï¼‰æ—¨åœ¨è‡ªç„¶ä¿®æ”¹å›¾åƒä¸­çš„æ–‡æœ¬ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¸€è‡´æ€§ï¼Œå…¶å†³å®šå› ç´ å¯åˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼Œå³æ–‡æœ¬æ ·å¼ã€æ–‡æœ¬å†…å®¹å’ŒèƒŒæ™¯ã€‚ä¹‹å‰çš„æ–¹æ³•åœ¨å¯ç¼–è¾‘å±æ€§çš„ä¸å®Œå…¨åˆ†ç¦»æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œé€šå¸¸åªè§£å†³ä¸€ä¸ªæ–¹é¢ï¼Œä¾‹å¦‚ç¼–è¾‘æ–‡æœ¬å†…å®¹ï¼Œä»è€Œé™åˆ¶äº†å¯æ§æ€§å’Œè§†è§‰ä¸€è‡´æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†TripleFDSï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰åˆ†ç¦»æ¨¡å—åŒ–å±æ€§çš„æ–°å‹STEæ¡†æ¶ï¼Œä»¥åŠä¸€ä¸ªåä¸ºSCBåˆæˆçš„æ•°æ®é›†ã€‚SCBåˆæˆé€šè¿‡åˆ©ç”¨â€œSCBç»„â€è¿™ä¸€æ–°é¢–æ„é€ ï¼ˆæ¯å¹…å›¾åƒç»„åˆä¸‰ä¸ªå±æ€§æ¥ç”Ÿæˆå¤šæ ·ã€åˆ†ç¦»çš„è®­ç»ƒç»„ï¼‰ï¼Œä¸ºä¸‰é‡ç‰¹å¾åˆ†ç¦»æä¾›äº†ç¨³å¥çš„è®­ç»ƒæ•°æ®ã€‚åˆ©ç”¨è¿™ä¸€æ„é€ ä½œä¸ºåŸºæœ¬è®­ç»ƒå•å…ƒï¼ŒTripleFDSé¦–å…ˆåˆ†ç¦»ä¸‰é‡ç‰¹å¾ï¼Œé€šè¿‡ç»„é—´å¯¹æ¯”æ­£åˆ™åŒ–ç¡®ä¿è¯­ä¹‰å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡æ ·æœ¬å†…å¤šç‰¹å¾æ­£äº¤æ€§å‡å°‘å†—ä½™ã€‚åœ¨åˆæˆé˜¶æ®µï¼ŒTripleFDSæ‰§è¡Œç‰¹å¾é‡æ˜ å°„ï¼Œä»¥é˜²æ­¢é‡å»ºè¿‡ç¨‹ä¸­çš„â€œæ·å¾„â€ç°è±¡ï¼Œå¹¶å‡è½»æ½œåœ¨çš„ç‰¹å¾æ³„éœ²ã€‚åœ¨125,000ä¸ªSCBç»„çš„è®­ç»ƒä¸‹ï¼ŒTripleFDSåœ¨ä¸»æµSTEåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å›¾åƒä¿çœŸåº¦ï¼ˆSSIMå¾—åˆ†ä¸º44.54ï¼‰å’Œæ–‡æœ¬å‡†ç¡®ç‡ï¼ˆACCå¾—åˆ†ä¸º93.58%ï¼‰ã€‚é™¤äº†å“è¶Šçš„æ€§èƒ½å¤–ï¼ŒTripleFDSçš„æ›´çµæ´»ç¼–è¾‘è¿˜æ”¯æŒæ–°çš„æ“ä½œï¼Œå¦‚æ ·å¼æ›¿æ¢å’ŒèƒŒæ™¯è½¬ç§»ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/yusenbao01/TripleFDS">https://github.com/yusenbao01/TripleFDS</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13399v1">PDF</a> Accepted by AAAI2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœºæ™¯æ–‡æœ¬ç¼–è¾‘ï¼ˆSTEï¼‰çš„ç›®æ ‡ï¼Œæ—¨åœ¨è‡ªç„¶ä¿®æ”¹å›¾åƒä¸­çš„æ–‡æœ¬åŒæ—¶ä¿æŒè§†è§‰ä¸€è‡´æ€§ã€‚é’ˆå¯¹ä»¥å¾€æ–¹æ³•åœ¨ç¼–è¾‘å±æ€§è§£è€¦æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†TripleFDSæ¡†æ¶å’ŒSCBåˆæˆæ•°æ®é›†ã€‚TripleFDSé€šè¿‡è§£è€¦æ¨¡å—å±æ€§å®ç°æ–‡æœ¬é£æ ¼ã€å†…å®¹å’ŒèƒŒæ™¯çš„å…¨é¢ç¼–è¾‘ï¼ŒSCBåˆæˆæ•°æ®é›†åˆ™ä¸ºä¸‰é‡ç‰¹å¾è§£è€¦æä¾›ç¨³å¥çš„è®­ç»ƒæ•°æ®ã€‚è¯¥æ¡†æ¶åœ¨ä¿è¯è¯­ä¹‰å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†å†—ä½™ï¼Œå®ç°äº†çµæ´»ç¼–è¾‘æ“ä½œï¼Œå¦‚é£æ ¼æ›¿æ¢å’ŒèƒŒæ™¯è½¬ç§»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœºæ™¯æ–‡æœ¬ç¼–è¾‘ï¼ˆSTEï¼‰æ—¨åœ¨è‡ªç„¶ä¿®æ”¹å›¾åƒä¸­çš„æ–‡æœ¬ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¸€è‡´æ€§ã€‚</li>
<li>ä»¥å¾€æ–¹æ³•åœ¨ç¼–è¾‘å±æ€§è§£è€¦æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä»…å…³æ³¨å•ä¸€æ–¹é¢çš„ç¼–è¾‘ï¼Œå¦‚æ–‡æœ¬å†…å®¹ã€‚</li>
<li>TripleFDSæ¡†æ¶å®ç°äº†æ–‡æœ¬é£æ ¼ã€å†…å®¹å’ŒèƒŒæ™¯çš„å…¨é¢ç¼–è¾‘ï¼Œé€šè¿‡è§£è€¦æ¨¡å—å±æ€§å…‹æœä»¥å¾€æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>SCBåˆæˆæ•°æ®é›†ä¸ºTripleFDSæ¡†æ¶æä¾›ç¨³å¥çš„è®­ç»ƒæ•°æ®ï¼Œåˆ©ç”¨â€œSCBç»„â€ç”Ÿæˆå¤šæ ·åŒ–ã€è§£è€¦çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>TripleFDSé€šè¿‡ç»„å†…å¯¹æ¯”æ­£åˆ™åŒ–å’Œæ ·æœ¬å†…å¤šç‰¹å¾æ­£äº¤æ€§ï¼Œç¡®ä¿è¯­ä¹‰å‡†ç¡®æ€§å’Œå‡å°‘å†—ä½™ã€‚</li>
<li>åœ¨åˆæˆé˜¶æ®µï¼ŒTripleFDSè¿›è¡Œç‰¹å¾é‡æ˜ å°„ï¼Œé˜²æ­¢é‡å»ºè¿‡ç¨‹ä¸­çš„â€œæ·å¾„â€ç°è±¡å’Œæ½œåœ¨ç‰¹å¾æ³„éœ²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14caf317a1dfa43dfdb2bc158e5a534a" align="middle">
<img src="https://picx.zhimg.com/v2-24db76f48d762425bc3e926ff8712127" align="middle">
<img src="https://picx.zhimg.com/v2-ff450da04b908a2bc152aa2d295ccd78" align="middle">
<img src="https://picx.zhimg.com/v2-e60adbb7b559bea3d146ddc28f8fe8f4" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PDRs4All-XX-Haute-Couture-Spectral-stitching-of-JWST-MIRI-IFU-cubes-with-matrix-completion"><a href="#PDRs4All-XX-Haute-Couture-Spectral-stitching-of-JWST-MIRI-IFU-cubes-with-matrix-completion" class="headerlink" title="PDRs4All XX. Haute Couture: Spectral stitching of JWST MIRI-IFU cubes with matrix completion"></a>PDRs4All XX. Haute Couture: Spectral stitching of JWST MIRI-IFU cubes with matrix completion</h2><p><strong>Authors:AmÃ©lie Canin, CÃ©dric FÃ©votte, Nicolas Dobigeon, Dries Van De Putte, Takashi Onaka, Olivier BernÃ©</strong></p>
<p>MIRI is the imager and spectrograph covering wavelengths from $4.9$ to $27.9$ $Î¼$m onboard the James Webb Space Telescope (JWST). The Medium-Resolution Spectrometer (MRS) consists of four integral field units (IFU), each of which has three sub-channels. The twelve resulting spectral data cubes have different fields of view, spatial, and spectral resolutions. The wavelength range of each cube partially overlaps with the neighboring bands, and the overlap regions typically show flux mismatches which have to be corrected by spectral stitching methods. Stitching methods aim to produce a single data cube incorporating the data of the individual sub-channels, which requires matching the spatial resolution and the flux discrepancies. We present Haute Couture, a novel stitching algorithm which uses non-negative matrix factorization (NMF) to perform a matrix completion, where the available MRS data cubes are treated as twelve sub-matrices of a larger incomplete matrix. Prior to matrix completion, we also introduce a novel pre-processing to homogenize the global intensities of the twelve cubes. Our pre-processing consists in jointly optimizing a set of global scale parameters that maximize the fit between the cubes where spectral overlap occurs. We apply our novel stitching method to JWST data obtained as part of the PDRs4All observing program of the Orion Bar, and produce a uniform cube reconstructed with the best spatial resolution over the full range of wavelengths.</p>
<blockquote>
<p>MIRIæ˜¯è©¹å§†æ–¯Â·éŸ¦ä¼¯ç©ºé—´æœ›è¿œé•œï¼ˆJWSTï¼‰ä¸Šçš„æˆåƒä»ªå’Œå…‰è°±ä»ªï¼Œè¦†ç›–ä»$ 4.9 $åˆ°$ 27.9Î¼m $çš„æ³¢é•¿èŒƒå›´ã€‚ä¸­åˆ†è¾¨ç‡å…‰è°±ä»ªï¼ˆMRSï¼‰ç”±å››ä¸ªæ•´ä½“åœºå•å…ƒï¼ˆIFUï¼‰ç»„æˆï¼Œæ¯ä¸ªå•å…ƒæœ‰ä¸‰ä¸ªå­é€šé“ã€‚ç”±æ­¤äº§ç”Ÿçš„åäºŒä¸ªå…‰è°±æ•°æ®ç«‹æ–¹ä½“å…·æœ‰ä¸åŒçš„è§†åœºã€ç©ºé—´å…‰è°±åˆ†è¾¨ç‡ã€‚æ¯ä¸ªç«‹æ–¹ä½“çš„æ³¢é•¿èŒƒå›´ä¸ç›¸é‚»æ³¢æ®µéƒ¨åˆ†é‡å ï¼Œé‡å åŒºåŸŸé€šå¸¸ä¼šå‡ºç°æµé‡ä¸åŒ¹é…çš„æƒ…å†µï¼Œå¿…é¡»é€šè¿‡å…‰è°±æ‹¼æ¥æ–¹æ³•è¿›è¡Œæ ¡æ­£ã€‚æ‹¼æ¥æ–¹æ³•æ—¨åœ¨ç”Ÿæˆä¸€ä¸ªåŒ…å«å„ä¸ªå­é€šé“æ•°æ®çš„å•ä¸€æ•°æ®ç«‹æ–¹ä½“ï¼Œè¿™è¦æ±‚åŒ¹é…ç©ºé—´åˆ†è¾¨ç‡å’Œæµé‡å·®å¼‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ‹¼æ¥ç®—æ³•â€”â€”é«˜çº§å®šåˆ¶ç®—æ³•ï¼Œè¯¥ç®—æ³•é‡‡ç”¨éè´ŸçŸ©é˜µåˆ†è§£ï¼ˆNMFï¼‰è¿›è¡ŒçŸ©é˜µè¡¥å…¨ï¼Œå°†å¯ç”¨çš„MRSæ•°æ®ç«‹æ–¹ä½“è§†ä¸ºä¸€ä¸ªæ›´å¤§ä¸å®Œæ•´çŸ©é˜µçš„åäºŒä¸ªå­çŸ©é˜µã€‚åœ¨çŸ©é˜µè¡¥å…¨ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹é¢„å¤„ç†æ¥ç»Ÿä¸€åäºŒä¸ªç«‹æ–¹ä½“çš„å…¨å±€å¼ºåº¦ã€‚æˆ‘ä»¬çš„é¢„å¤„ç†åŒ…æ‹¬è”åˆä¼˜åŒ–ä¸€ç»„å…¨å±€å°ºåº¦å‚æ•°ï¼Œä»¥æœ€å¤§åŒ–å‘ç”Ÿå…‰è°±é‡å çš„ç«‹æ–¹ä½“ä¹‹é—´çš„æ‹Ÿåˆåº¦ã€‚æˆ‘ä»¬å°†æ–°å‹æ‹¼æ¥æ–¹æ³•åº”ç”¨äºJWSTè·å–çš„è§‚æµ‹ç¨‹åºOrion Barçš„PDRs4Allæ•°æ®ï¼Œå¹¶ç”Ÿæˆäº†ä¸€ä¸ªåœ¨å…¨æ³¢é•¿èŒƒå›´å†…å…·æœ‰æœ€ä½³ç©ºé—´åˆ†è¾¨ç‡çš„ç»Ÿä¸€é‡å»ºç«‹æ–¹ä½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13377v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è©¹å§†æ–¯éŸ¦ä¼¯å¤ªç©ºæœ›è¿œé•œï¼ˆJWSTï¼‰ä¸Šçš„ä¸­çº¢å¤–æˆåƒä»ªï¼ˆMIRIï¼‰ä¸å…‰è°±ä»ªæ¶µç›–$4.9$è‡³$27.9$å¾®ç±³çš„æ³¢é•¿èŒƒå›´ã€‚å…¶ä¸­ï¼Œä¸­åˆ†è¾¨ç‡å…‰è°±ä»ªï¼ˆMRSï¼‰ç”±å››ä¸ªç§¯åˆ†åœºå•å…ƒï¼ˆIFUï¼‰ç»„æˆï¼Œæ¯ä¸ªå•å…ƒæœ‰ä¸‰ä¸ªå­é€šé“ã€‚é‡‡ç”¨éè´ŸçŸ©é˜µåˆ†è§£ï¼ˆNMFï¼‰çš„æ–°å‹æ‹¼æ¥ç®—æ³•â€”â€”é«˜çº§å®šåˆ¶ï¼ˆHaute Coutureï¼‰ï¼Œå®Œæˆäº†åŒ…å«å„å­é€šé“æ•°æ®çš„å¤§å‹æ•°æ®çŸ©é˜µçš„æ„å»ºã€‚æ­¤ç®—æ³•èƒ½åŒ¹é…ç©ºé—´åˆ†è¾¨ç‡å¹¶ä¿®æ­£æµé‡å·®å¼‚é—®é¢˜ã€‚æˆ‘ä»¬å¯¹JWSTè·å¾—çš„å…³äºçŒæˆ·æ˜Ÿæ£’çŠ¶åŒºåŸŸçš„æ•°æ®è¿›è¡Œäº†å¤„ç†ï¼Œå¹¶æˆåŠŸç”Ÿæˆäº†ä¸€ä¸ªå…¨æ³¢é•¿èŒƒå›´å†…å…·æœ‰æœ€ä½³ç©ºé—´åˆ†è¾¨ç‡çš„ç»Ÿä¸€æ•°æ®ç«‹æ–¹ä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIRIæ˜¯JWSTä¸Šçš„æˆåƒå’Œå…‰è°±ä»ªå·¥å…·ï¼Œè¦†ç›–æ³¢é•¿èŒƒå›´å¹¿ã€‚</li>
<li>MRSåŒ…å«å››ä¸ªIFUï¼Œæ¯ä¸ªIFUæœ‰ä¸‰ä¸ªå­é€šé“ï¼Œäº§ç”Ÿä¸åŒçš„å…‰è°±æ•°æ®ç«‹æ–¹ä½“ã€‚</li>
<li>å…‰è°±æ•°æ®ç«‹æ–¹ä½“ä¹‹é—´å­˜åœ¨æ³¢é•¿é‡å åŒºåŸŸï¼Œé€šå¸¸ä¼šå‡ºç°æµé‡ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>éœ€è¦é‡‡ç”¨å…‰è°±æ‹¼æ¥æ–¹æ³•æ¥ä¿®æ­£æµé‡ä¸åŒ¹é…é—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªåŒ…å«å„å­é€šé“æ•°æ®çš„å¤§å‹æ•°æ®ç«‹æ–¹ä½“ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹æ‹¼æ¥ç®—æ³•â€”â€”é«˜çº§å®šåˆ¶ï¼ˆHaute Coutureï¼‰ï¼Œä½¿ç”¨éè´ŸçŸ©é˜µåˆ†è§£ï¼ˆNMFï¼‰è¿›è¡ŒçŸ©é˜µè¡¥å…¨ã€‚</li>
<li>åœ¨åº”ç”¨æ–°å‹æ‹¼æ¥æ–¹æ³•å¤„ç†JWSTæ•°æ®åï¼ŒæˆåŠŸç”Ÿæˆäº†å…¨æ³¢é•¿èŒƒå›´å†…å…·æœ‰æœ€ä½³ç©ºé—´åˆ†è¾¨ç‡çš„ç»Ÿä¸€æ•°æ®ç«‹æ–¹ä½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8da3327c0cf316501090598991e8acc6" align="middle">
<img src="https://picx.zhimg.com/v2-b573c8ad5964ea160786c72ad246e220" align="middle">
<img src="https://picx.zhimg.com/v2-ddb9e7c72cd8a4901d8170889c6416b0" align="middle">
<img src="https://picx.zhimg.com/v2-e2139c8bc0e1d9101ea62c4fbf812971" align="middle">
<img src="https://picx.zhimg.com/v2-be7d6a7f3f992ffd8cc6426fcc2db6ea" align="middle">
<img src="https://picx.zhimg.com/v2-0cd5c9635030a6d224828dd6023c3983" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PyPeT-A-Python-Perfusion-Tool-for-Automated-Quantitative-Brain-CT-and-MR-Perfusion-Analysis"><a href="#PyPeT-A-Python-Perfusion-Tool-for-Automated-Quantitative-Brain-CT-and-MR-Perfusion-Analysis" class="headerlink" title="PyPeT: A Python Perfusion Tool for Automated Quantitative Brain CT and MR Perfusion Analysis"></a>PyPeT: A Python Perfusion Tool for Automated Quantitative Brain CT and MR Perfusion Analysis</h2><p><strong>Authors:Marijn Borghouts, Ruisheng Su</strong></p>
<p>Computed tomography perfusion (CTP) and magnetic resonance perfusion (MRP) are widely used in acute ischemic stroke assessment and other cerebrovascular conditions to generate quantitative maps of cerebral hemodynamics. While commercial perfusion analysis software exists, it is often costly, closed source, and lacks customizability. This work introduces PyPeT, an openly available Python Perfusion Tool for head CTP and MRP processing. PyPeT is capable of producing cerebral blood flow (CBF), cerebral blood volume (CBV), mean transit time (MTT), time-to-peak (TTP), and time-to-maximum (Tmax) maps from raw four-dimensional perfusion data. PyPeT aims to make perfusion research as accessible and customizable as possible. This is achieved through a unified framework in which both CTP and MRP data can be processed, with a strong focus on modularity, low computational burden, and significant inline documentation. PyPeTâ€™s outputs can be validated through an extensive debug mode in which every step of the process is visualized. Additional validation was performed via visual and quantitative comparison with reference perfusion maps generated by three FDA-approved commercial perfusion tools and a research tool. These comparisons show a mean SSIM around 0.8 for all comparisons, indicating a good and stable correlation with FDA-approved tools. The code for PyPeT is openly available at our GitHub <a target="_blank" rel="noopener" href="https://github.com/Marijn311/CT-and-MR-Perfusion-Tool">https://github.com/Marijn311/CT-and-MR-Perfusion-Tool</a></p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æçŒæ³¨ï¼ˆCTPï¼‰å’Œç£å…±æŒ¯çŒæ³¨ï¼ˆMRPï¼‰å¹¿æ³›åº”ç”¨äºæ€¥æ€§ç¼ºè¡€æ€§å’ä¸­çš„è¯„ä¼°å’Œå…¶ä»–è„‘è¡€ç®¡çŠ¶å†µï¼Œä»¥ç”Ÿæˆè„‘è¡€æµåŠ¨åŠ›å­¦å®šé‡å›¾ã€‚å°½ç®¡å­˜åœ¨å•†ä¸šçŒæ³¨åˆ†æè½¯ä»¶ï¼Œä½†å®ƒé€šå¸¸ä»·æ ¼æ˜‚è´µã€æºä»£ç å°é—­ä¸”ç¼ºä¹å¯å®šåˆ¶æ€§ã€‚è¿™é¡¹å·¥ä½œä»‹ç»äº†PyPeTï¼Œè¿™æ˜¯ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„PythonçŒæ³¨å·¥å…·ï¼Œç”¨äºå¤´éƒ¨CTPå’ŒMRPå¤„ç†ã€‚PyPeTèƒ½å¤Ÿäº§ç”Ÿè„‘è¡€æµé‡ï¼ˆCBFï¼‰ã€è„‘è¡€å®¹é‡ï¼ˆCBVï¼‰ã€å¹³å‡é€šè¿‡æ—¶é—´ï¼ˆMTTï¼‰ã€å³°å€¼æ—¶é—´ï¼ˆTTPï¼‰å’Œæœ€å¤§æ—¶é—´ï¼ˆTmaxï¼‰å›¾ï¼Œè¿™äº›å›¾æ˜¯ä»åŸå§‹çš„å››ç»´çŒæ³¨æ•°æ®ä¸­å¾—å‡ºçš„ã€‚PyPeTæ—¨åœ¨ä½¿çŒæ³¨ç ”ç©¶å°½å¯èƒ½æ˜“äºè®¿é—®å’Œå¯å®šåˆ¶ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å®ç°çš„ï¼Œè¯¥æ¡†æ¶å¯ä»¥å¤„ç†CTPå’ŒMRPæ•°æ®ï¼Œé‡ç‚¹å¼ºè°ƒæ¨¡å—åŒ–ã€ä½è®¡ç®—è´Ÿæ‹…å’Œé‡è¦çš„å†…è”æ–‡æ¡£ã€‚PyPeTçš„è¾“å‡ºå¯ä»¥é€šè¿‡å¹¿æ³›çš„è°ƒè¯•æ¨¡å¼è¿›è¡ŒéªŒè¯ï¼Œåœ¨è¯¥æ¨¡å¼ä¸‹ï¼Œè¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥éƒ½ä¼šè¢«å¯è§†åŒ–ã€‚é€šè¿‡ä¸æˆ‘ä»¬ä½¿ç”¨çš„ä¸‰ä¸ªFDAæ‰¹å‡†çš„å•†ä¸šçŒæ³¨å·¥å…·å’Œä¸€ç§ç ”ç©¶å·¥å…·ç”Ÿæˆçš„å‚è€ƒçŒæ³¨å›¾è¿›è¡Œè§†è§‰å’Œå®šé‡æ¯”è¾ƒï¼Œå¯¹PyPeTè¿›è¡Œäº†é¢å¤–çš„éªŒè¯ã€‚è¿™äº›æ¯”è¾ƒæ˜¾ç¤ºï¼Œæ‰€æœ‰æ¯”è¾ƒçš„å¹³å‡ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰çº¦ä¸º0.8ï¼Œè¡¨æ˜ä¸FDAæ‰¹å‡†çš„å·¥å…·å…·æœ‰è‰¯å¥½çš„ç¨³å®šç›¸å…³æ€§ã€‚PyPeTçš„ä»£ç å¯åœ¨æˆ‘ä»¬çš„GitHubä¸Šå…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Marijn311/CT-and-MR-Perfusion-Tool">https://github.com/Marijn311/CT-and-MR-Perfusion-Tool</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13310v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>PyPeTæ˜¯ä¸€ç§å…¬å¼€å¯ç”¨çš„PythonçŒæ³¨å·¥å…·ï¼Œç”¨äºå¤„ç†å¤´éƒ¨è®¡ç®—æœºæ–­å±‚çŒæ³¨ï¼ˆCTPï¼‰å’Œç£å…±æŒ¯çŒæ³¨ï¼ˆMRPï¼‰æ•°æ®ï¼Œç”Ÿæˆå®šé‡è„‘è¡€æµåŠ¨åŠ›å­¦å›¾ã€‚PyPeTèƒ½å¤Ÿäº§ç”Ÿè„‘è¡€æµé‡ï¼ˆCBFï¼‰ã€è„‘è¡€å®¹é‡ï¼ˆCBVï¼‰ã€å¹³å‡é€šè¿‡æ—¶é—´ï¼ˆMTTï¼‰ã€å³°å€¼æ—¶é—´ï¼ˆTTPï¼‰å’Œæœ€å¤§æ—¶é—´ï¼ˆTmaxï¼‰å›¾ï¼Œä»åŸå§‹å››ç»´çŒæ³¨æ•°æ®ä¸­ã€‚å®ƒçš„ç›®æ ‡æ˜¯ä½¿çŒæ³¨ç ”ç©¶æ˜“äºè®¿é—®å’Œå¯å®šåˆ¶ã€‚é€šè¿‡ä¸FDAæ‰¹å‡†çš„å•†ç”¨çŒæ³¨å·¥å…·å’Œä¸€ç§ç ”ç©¶å·¥å…·çš„æ¯”è¾ƒéªŒè¯ï¼ŒPyPeTçš„è¾“å‡ºç»“æœè¡¨ç°å‡ºè‰¯å¥½çš„ç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PyPeTæ˜¯ä¸€ä¸ªç”¨äºå¤„ç†CTPå’ŒMRPæ•°æ®çš„å¼€æºPythonå·¥å…·ã€‚</li>
<li>å®ƒèƒ½å¤Ÿç”Ÿæˆå¤šç§çŒæ³¨å‚æ•°å›¾ï¼ŒåŒ…æ‹¬CBFã€CBVã€MTTã€TTPå’ŒTmaxã€‚</li>
<li>PyPeTå…·æœ‰ç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯å¤„ç†CTPå’ŒMRPæ•°æ®ã€‚</li>
<li>è¯¥å·¥å…·æ³¨é‡æ¨¡å—åŒ–ã€ä½è®¡ç®—è´Ÿæ‹…å’Œè¯¦ç»†çš„åœ¨çº¿æ–‡æ¡£ã€‚</li>
<li>PyPeTå…·æœ‰å¹¿æ³›çš„è°ƒè¯•æ¨¡å¼ï¼Œå¯ä»¥å¯è§†åŒ–æ¯ä¸€æ­¥çš„å¤„ç†è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡ä¸FDAæ‰¹å‡†çš„å•†ç”¨çŒæ³¨å·¥å…·æ¯”è¾ƒï¼ŒPyPeTè¡¨ç°å‡ºè‰¯å¥½çš„ç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c689cd51336b1490006012a14f98b51" align="middle">
<img src="https://picx.zhimg.com/v2-e50bc8315dabc63312bbe4eca8b768c6" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SF-Recon-Simplification-Free-Lightweight-Building-Reconstruction-via-3D-Gaussian-Splatting"><a href="#SF-Recon-Simplification-Free-Lightweight-Building-Reconstruction-via-3D-Gaussian-Splatting" class="headerlink" title="SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting"></a>SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting</h2><p><strong>Authors:Zihan Li, Tengfei Wang, Wentian Gan, Hao Zhan, Xin Wang, Zongqian Zhan</strong></p>
<p>Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:<a target="_blank" rel="noopener" href="https://lzh282140127-cell.github.io/SF-Recon-project/">https://lzh282140127-cell.github.io/SF-Recon-project/</a></p>
<blockquote>
<p>è½»é‡çº§å»ºç­‘è¡¨é¢æ¨¡å‹å¯¹äºæ•°å­—åŸå¸‚ã€å¯¼èˆªå’Œå¿«é€Ÿåœ°ç†ç©ºé—´åˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¤šè§†è§’å‡ ä½•æµæ°´çº¿ä»ç„¶å¾ˆç¬¨æ‹™ä¸”å¯¹è´¨é‡æ•æ„Ÿï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºå¯†é›†é‡å»ºã€ç½‘æ ¼åŒ–ä»¥åŠéšåçš„ç®€åŒ–ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†SF-Reconæ–¹æ³•ï¼Œä¸€ç§ç›´æ¥ä»å¤šè§†è§’å›¾åƒé‡å»ºè½»é‡çº§å»ºç­‘è¡¨é¢çš„æ–¹æ³•ï¼Œæ— éœ€åç»­çš„ç½‘æ ¼ç®€åŒ–ã€‚æˆ‘ä»¬é¦–å…ˆè®­ç»ƒåˆå§‹çš„3Dé«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰åœºä»¥è·å¾—è§†å›¾ä¸€è‡´è¡¨ç¤ºã€‚ç„¶åï¼Œé€šè¿‡æ³•çº¿æ¢¯åº¦å¼•å¯¼çš„é«˜æ–¯ä¼˜åŒ–æå–å»ºç­‘ç»“æ„ï¼Œè¯¥ä¼˜åŒ–é€‰æ‹©ä¸å±‹é¡¶å’Œå¢™å£è¾¹ç•Œå¯¹é½çš„åŸºæœ¬å½¢çŠ¶ï¼Œéšåé€šè¿‡å¤šè§†è§’è¾¹ç¼˜ä¸€è‡´æ€§ä¿®å‰ªå¢å¼ºç»“æ„æ¸…æ™°åº¦ï¼Œå¹¶åœ¨æ— éœ€å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹æŠ‘åˆ¶éç»“æ„ä¼ªå½±ã€‚æœ€åï¼Œå¤šè§†è§’æ·±åº¦çº¦æŸDelaunayä¸‰è§’å‰–åˆ†å°†ç»“æ„åŒ–é«˜æ–¯åœºè½¬æ¢ä¸ºè½»é‡çº§ã€ç»“æ„å¿ å®çš„å»ºç­‘ç½‘æ ¼ã€‚åŸºäºæå‡ºçš„SFæ•°æ®é›†ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SF-Reconèƒ½å¤Ÿç›´æ¥ä»å¤šè§†è§’å›¾åƒé‡å»ºè½»é‡çº§å»ºç­‘æ¨¡å‹ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°æ›´å°‘çš„é¢å’Œé¡¶ç‚¹ã€‚ç½‘ç«™é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://lzh282140127-cell.github.io/SF-Recon-project/">https://lzh282140127-cell.github.io/SF-Recon-project/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13278v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSF-Reconçš„æ–¹æ³•ï¼Œå¯ä»å¤šè§†è§’å›¾åƒç›´æ¥é‡å»ºè½»é‡çº§å»ºç­‘è¡¨é¢æ¨¡å‹ï¼Œæ— éœ€åç»­ç½‘æ ¼ç®€åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒåˆå§‹3Dé«˜æ–¯å–·å°„åœºè·å¾—è§†è§’ä¸€è‡´çš„è¡¨ç¤ºï¼Œé€šè¿‡æ­£å¸¸æ¢¯åº¦å¼•å¯¼çš„é«˜æ–¯ä¼˜åŒ–é€‰æ‹©ä¸åŸå±‹é¡¶å’Œå¢™å£è¾¹ç•Œå¯¹é½çš„åŸå§‹å…ƒç´ ï¼Œç„¶åè¿›è¡Œå¤šè§†è§’è¾¹ç¼˜ä¸€è‡´æ€§ä¿®å‰ªï¼Œæœ€åé€šè¿‡å¤šè§†è§’æ·±åº¦çº¦æŸDelaunayä¸‰è§’å‰–åˆ†å°†ç»“æ„åŒ–é«˜æ–¯åœºè½¬æ¢ä¸ºè½»é‡çº§ã€ç»“æ„çœŸå®çš„å»ºç­‘ç½‘æ ¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SF-Reconæ–¹æ³•å¯ç›´æ¥ä»å¤šè§†è§’å›¾åƒé‡å»ºè½»é‡çº§å»ºç­‘è¡¨é¢æ¨¡å‹ï¼Œæ— éœ€åç»­ç½‘æ ¼ç®€åŒ–ã€‚</li>
<li>é€šè¿‡è®­ç»ƒåˆå§‹3Dé«˜æ–¯å–·å°„åœºè·å¾—è§†è§’ä¸€è‡´çš„è¡¨ç¤ºï¼Œä¸ºå»ºç­‘ç»“æ„çš„é‡å»ºæä¾›åŸºç¡€ã€‚</li>
<li>æ­£å¸¸æ¢¯åº¦å¼•å¯¼çš„é«˜æ–¯ä¼˜åŒ–é€‰æ‹©ä¸åŸå±‹é¡¶å’Œå¢™å£è¾¹ç•Œå¯¹é½çš„åŸå§‹å…ƒç´ ã€‚</li>
<li>å¤šè§†è§’è¾¹ç¼˜ä¸€è‡´æ€§ä¿®å‰ªå¢å¼ºç»“æ„æ¸…æ™°åº¦ï¼ŒæŠ‘åˆ¶éç»“æ„ä¼ªå½±ã€‚</li>
<li>å¤šè§†è§’æ·±åº¦çº¦æŸDelaunayä¸‰è§’å‰–åˆ†å°†ç»“æ„åŒ–é«˜æ–¯åœºè½¬åŒ–ä¸ºè½»é‡çº§ã€ç»“æ„çœŸå®çš„å»ºç­‘ç½‘æ ¼ã€‚</li>
<li>SF-Reconæ–¹æ³•åœ¨ä¿è¯è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†å»ºç­‘æ¨¡å‹çš„è½»é‡åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88edc0c4650a1e95a3b9c84d60c0d01d" align="middle">
<img src="https://picx.zhimg.com/v2-501970d8f38047e4613096400d828827" align="middle">
<img src="https://picx.zhimg.com/v2-6c2470608ebace34d0f6a532a35acfdf" align="middle">
<img src="https://picx.zhimg.com/v2-df669e7c134bb505a8d2d3313f5d4888" align="middle">
<img src="https://picx.zhimg.com/v2-524daf218b50d79ce901d82b0ff91193" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Referring-Camouflaged-Object-Detection-With-Multi-Context-Overlapped-Windows-Cross-Attention"><a href="#Referring-Camouflaged-Object-Detection-With-Multi-Context-Overlapped-Windows-Cross-Attention" class="headerlink" title="Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention"></a>Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention</h2><p><strong>Authors:Yu Wen, Shuyong Gao, Shuping Zhang, Miao Huang, Lili Tao, Han Yang, Haozhe Xing, Lihe Zhang, Boxue Hou</strong></p>
<p>Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.</p>
<blockquote>
<p>å‚ç…§ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆRef-CODï¼‰æ—¨åœ¨é€šè¿‡åˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬æè¿°ç­‰å‚è€ƒä¿¡æ¯æ¥è¯†åˆ«éšè—çš„ç›®æ ‡ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»å°†å…·æœ‰æ˜¾è‘—ç›®æ ‡çš„å‚è€ƒå›¾åƒè½¬æ¢ä¸ºä¸€ç»´æç¤ºï¼Œå¹¶è·å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚æˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡èåˆä¸°å¯Œæ˜¾è‘—çš„å›¾åƒç‰¹å¾å’Œä¼ªè£…ç›®æ ‡ç‰¹å¾çš„å¤šä¸Šä¸‹æ–‡æ¥å¢å¼ºæ€§èƒ½çš„æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RFMNetï¼Œå®ƒåˆ©ç”¨å‚è€ƒæ˜¾è‘—å›¾åƒå¤šä¸ªç¼–ç é˜¶æ®µçš„ç‰¹å¾ï¼Œå¹¶åœ¨ç›¸åº”çš„ç¼–ç é˜¶æ®µä¸ä¼ªè£…ç‰¹å¾è¿›è¡Œäº¤äº’èåˆã€‚é‰´äºæ˜¾è‘—ç›®æ ‡å›¾åƒä¸­çš„ç‰¹å¾åŒ…å«å¤§é‡çš„ç›®æ ‡ç›¸å…³è¯¦ç»†ä¿¡æ¯ï¼Œåœ¨å±€éƒ¨åŒºåŸŸè¿›è¡Œç‰¹å¾èåˆå¯¹äºæ£€æµ‹ä¼ªè£…ç›®æ ‡æ›´ä¸ºæœ‰åˆ©ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡å çª—å£äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºå‚è€ƒç‰¹å¾æ›´åŠ å…³æ³¨å±€éƒ¨ä¿¡æ¯åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å‚ç…§ç‰¹å¾èšåˆï¼ˆRFAï¼‰æ¨¡å—ï¼Œä»¥é€æ­¥è§£ç å’Œåˆ†å‰²ä¼ªè£…ç›®æ ‡ã€‚åœ¨Ref-CODåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13249v1">PDF</a> 12 pages, 7figures, This work is supported by National Nature Science Foundation of China (Grant No. 62203291)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•é€šè¿‡èåˆå‚è€ƒå›¾åƒçš„å¤šä¸Šä¸‹æ–‡ä¸°å¯Œç‰¹å¾æ¥å¢å¼ºä¼ªè£…å¯¹è±¡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†RFMNetæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å‚è€ƒæ˜¾è‘—å›¾åƒçš„å¤šé˜¶æ®µç¼–ç ç‰¹å¾ï¼Œå¹¶ä¸ä¼ªè£…ç‰¹å¾è¿›è¡Œäº¤äº’å¼èåˆã€‚åŒæ—¶ï¼Œæå‡ºäº†åŸºäºé‡å çª—å£çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œå‚è€ƒç‰¹å¾èšåˆæ¨¡å—ï¼Œä»¥æé«˜å±€éƒ¨ä¿¡æ¯åŒ¹é…çš„å‡†ç¡®æ€§å’Œé€æ­¥è§£ç å’Œåˆ†å‰²ä¼ªè£…å¯¹è±¡çš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Ref-CODåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ref-CODæ—¨åœ¨é€šè¿‡èå…¥å‚è€ƒä¿¡æ¯ï¼Œå¦‚å›¾åƒå’Œæ–‡å­—æè¿°ï¼Œæ¥è¯†åˆ«éšè—ç‰©ä½“ã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶å°†å‚è€ƒå›¾åƒä¸­çš„æ˜¾è‘—ç‰©ä½“è½¬åŒ–ä¸ºä¸€ç»´æç¤ºï¼Œå–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>æå‡ºé€šè¿‡å¤šä¸Šä¸‹æ–‡èåˆå¢å¼ºæ€§èƒ½ï¼Œèåˆä¸°å¯Œæ˜¾è‘—çš„å›¾åƒç‰¹å¾å’Œä¼ªè£…å¯¹è±¡ç‰¹å¾ã€‚</li>
<li>å¼•å…¥RFMNetæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å‚è€ƒæ˜¾è‘—å›¾åƒçš„å¤šé˜¶æ®µç¼–ç ç‰¹å¾ï¼Œå¹¶ä¸ä¼ªè£…ç‰¹å¾è¿›è¡Œäº¤äº’å¼èåˆã€‚</li>
<li>å±€éƒ¨åŒºåŸŸè¿›è¡Œç‰¹å¾èåˆå¯¹äºæ£€æµ‹ä¼ªè£…ç‰©ä½“æ›´æœ‰ç›Šã€‚</li>
<li>æå‡ºäº†åŸºäºé‡å çª—å£çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½æ›´å…³æ³¨åŸºäºå‚è€ƒç‰¹å¾çš„å±€éƒ¨ä¿¡æ¯åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8839d67a8693c8f4b7ff511d03c6e3e5" align="middle">
<img src="https://picx.zhimg.com/v2-60e77b55de521e8a6dc150a7937abc22" align="middle">
<img src="https://picx.zhimg.com/v2-36e8afa508b37d5ae96dc1d0a25e45b0" align="middle">
<img src="https://picx.zhimg.com/v2-5b934e377c5c82e9c589437a32bfcbf1" align="middle">
<img src="https://picx.zhimg.com/v2-ddd7ae75350976f98f3c7d6fac3e9e45" align="middle">
<img src="https://picx.zhimg.com/v2-48b70d4f2b24a7d928285174ff5ab48f" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MRIQT-Physics-Aware-Diffusion-Model-for-Image-Quality-Transfer-in-Neonatal-Ultra-Low-Field-MRI"><a href="#MRIQT-Physics-Aware-Diffusion-Model-for-Image-Quality-Transfer-in-Neonatal-Ultra-Low-Field-MRI" class="headerlink" title="MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI"></a>MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI</h2><p><strong>Authors:Malek Al Abed, Sebiha Demir, Anne Groteklaes, Elodie Germani, Shahrooz Faghihroohi, Hemmen Sabir, Shadi Albarqouni</strong></p>
<p>Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.</p>
<blockquote>
<p>ä¾¿æºå¼è¶…ä½åœºMRIï¼ˆuLF-MRIï¼Œ0.064Tï¼‰ä¸ºæ–°ç”Ÿå„¿æŠ¤ç†æä¾›äº†å¯è®¿é—®çš„ç¥ç»æˆåƒï¼Œä½†ä¸é«˜åœºï¼ˆHFï¼‰MRIç›¸æ¯”ï¼Œå…¶ä¿¡å™ªæ¯”ä½ï¼Œè¯Šæ–­è´¨é‡è¾ƒå·®ã€‚æˆ‘ä»¬æå‡ºäº†MRIQTï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä»uLFåˆ°HF MRIçš„å›¾åƒè´¨é‡è½¬ç§»ï¼ˆIQTï¼‰çš„3Dæ¡ä»¶æ‰©æ•£æ¡†æ¶ã€‚MRIQTç»“åˆäº†é€¼çœŸçš„Kç©ºé—´é€€åŒ–ä»¥å®ç°ç‰©ç†ä¸€è‡´çš„uLFæ¨¡æ‹Ÿã€ç¨³å®šçš„å›¾åƒåˆ°å›¾åƒç”Ÿæˆçš„vé¢„æµ‹æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œä»¥åŠç”¨äºè§£å‰–å­¦çœŸå®æ€§çš„SNRåŠ æƒ3Dæ„ŸçŸ¥æŸå¤±ã€‚è¯¥æ¨¡å‹ä»å™ªå£°uLFè¾“å…¥ä¸­å»é™¤å™ªå£°ï¼Œè¯¥è¾“å…¥ä»¥ç›¸åŒçš„æ‰«æä¸ºæ¡ä»¶ï¼Œåˆ©ç”¨ä½“ç§¯æ³¨æ„åŠ›U-Netæ¶æ„è¿›è¡Œç»“æ„ä¿ç•™ç¿»è¯‘ã€‚åœ¨å…·æœ‰å¤šç§ç—…ç†çš„æ–°ç”Ÿå„¿é˜Ÿåˆ—ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒMRIQTåœ¨PSNRä¸Šè¶…è¶Šäº†æœ€è¿‘çš„GANå’ŒCNNåŸºå‡†æµ‹è¯•ï¼Œè¾¾åˆ°15.3%ï¼Œæ¯”ç°æœ‰æŠ€æœ¯é«˜å‡º1.78%ï¼Œè€ŒåŒ»ç”Ÿè®¤ä¸ºå…¶è¾“å‡ºçš„85%è´¨é‡è‰¯å¥½ï¼Œç—…ç†è¡¨ç°æ¸…æ™°ã€‚MRIQTå®ç°äº†åŸºäºæ‰©æ•£çš„é«˜ä¿çœŸå¢å¼ºä¾¿æºå¼è¶…ä½åœºï¼ˆuLFï¼‰MRIï¼Œå¯ç”¨äºå¯é çš„æ–°ç”Ÿå„¿è„‘è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13232v1">PDF</a> 5 pages, 4 figures</p>
<p><strong>Summary</strong><br>     ä¾¿æºå¼è¶…ä½åœºç£å…±æŒ¯æˆåƒï¼ˆuLF-MRIï¼‰åœ¨æ–°ç”Ÿå„¿æŠ¤ç†ä¸­æä¾›å¯è®¿é—®çš„ç¥ç»æˆåƒï¼Œä½†ä¸é«˜åœºï¼ˆHFï¼‰MRIç›¸æ¯”ï¼Œå­˜åœ¨ä¿¡å·å™ªå£°æ¯”ä½å’Œè¯Šæ–­è´¨é‡å·®çš„é—®é¢˜ã€‚æå‡ºMRIQTï¼Œä¸€ç§ä»uLFåˆ°HF MRIçš„å›¾åƒè´¨é‡è½¬ç§»ï¼ˆIQTï¼‰çš„3Dæ¡ä»¶æ‰©æ•£æ¡†æ¶ã€‚MRIQTç»“åˆé€¼çœŸçš„Kç©ºé—´é€€åŒ–å®ç°ç‰©ç†ä¸€è‡´çš„uLFæ¨¡æ‹Ÿã€vé¢„æµ‹å’Œæ— åˆ†ç±»å™¨æŒ‡å¯¼çš„ç¨³å®šå›¾åƒåˆ°å›¾åƒç”Ÿæˆï¼Œä»¥åŠä¿¡å™ªæ¯”åŠ æƒçš„3Dæ„ŸçŸ¥æŸå¤±ï¼Œä»¥å®ç°è§£å‰–çœŸå®æ€§ã€‚è¯¥æ¨¡å‹ä»å™ªå£°uLFè¾“å…¥ä¸­å­¦ä¹ ï¼Œä»¥ç›¸åŒçš„æ‰«æä¸ºæ¡ä»¶ï¼Œåˆ©ç”¨ä½“ç§¯æ³¨æ„åŠ›UNetæ¶æ„è¿›è¡Œç»“æ„ä¿ç•™ç¿»è¯‘ã€‚åœ¨å…·æœ‰å¤šç§ç—…ç†çš„æ–°ç”Ÿå„¿é˜Ÿåˆ—ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒMRIQTåœ¨PSNRä¸Šè¶…è¶Šæœ€è¿‘çš„GANå’ŒCNNåŸºçº¿ï¼Œé«˜å‡º15.3%ï¼ŒåŒæ—¶åŒ»ç”Ÿè¯„ä»·å…¶è¾“å‡ºçš„85%ä¸ºè´¨é‡è‰¯å¥½ï¼Œç—…ç†æ¸…æ™°ã€‚MRIQTå®ç°äº†åŸºäºæ‰©æ•£çš„ä¾¿æºå¼è¶…ä½åœºï¼ˆuLFï¼‰MRIé«˜è´¨é‡å¢å¼ºï¼Œå¯ç”¨äºå¯é çš„æ–°ç”Ÿå„¿è„‘è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¾¿æºå¼è¶…ä½åœºç£å…±æŒ¯æˆåƒï¼ˆuLF-MRIï¼‰åœ¨æ–°ç”Ÿå„¿æŠ¤ç†ä¸­æœ‰åº”ç”¨ï¼Œä½†ä¿¡å·å™ªå£°æ¯”ä½å’Œè¯Šæ–­è´¨é‡è¾ƒå·®ã€‚</li>
<li>æå‡ºäº†MRIQTæ¡†æ¶ï¼Œé€šè¿‡3Dæ¡ä»¶æ‰©æ•£æŠ€æœ¯æé«˜uLF-MRIçš„å›¾åƒè´¨é‡ã€‚</li>
<li>MRIQTæ¡†æ¶åŒ…æ‹¬é€¼çœŸçš„uLFæ¨¡æ‹Ÿã€ç¨³å®šçš„å›¾åƒç”Ÿæˆå’Œè§£å‰–çœŸå®æ€§çš„æ„ŸçŸ¥æŸå¤±ã€‚</li>
<li>ä½¿ç”¨ä½“ç§¯æ³¨æ„åŠ›UNetæ¶æ„è¿›è¡Œç»“æ„ä¿ç•™ç¿»è¯‘ï¼Œä»å™ªå£°uLFè¾“å…¥ä¸­å­¦ä¹ ã€‚</li>
<li>åœ¨æ–°ç”Ÿå„¿é˜Ÿåˆ—ä¸Šè®­ç»ƒï¼ŒMRIQTåœ¨å›¾åƒè´¨é‡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼ŒåŒ»ç”Ÿè¯„ä»·å…¶è¾“å‡ºè´¨é‡é«˜ä¸”ç—…ç†æ¸…æ™°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5ea30b613d592a111b3c0206bc97ba1" align="middle">
<img src="https://picx.zhimg.com/v2-516fb79028978983af85fdb61db0164c" align="middle">
<img src="https://picx.zhimg.com/v2-b786049795c7e706c7a6e03d8246dbc9" align="middle">
<img src="https://picx.zhimg.com/v2-a21e08218a2541d39553bd6c82eb309c" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CloseUpShot-Close-up-Novel-View-Synthesis-from-Sparse-views-via-Point-conditioned-Diffusion-Model"><a href="#CloseUpShot-Close-up-Novel-View-Synthesis-from-Sparse-views-via-Point-conditioned-Diffusion-Model" class="headerlink" title="CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model"></a>CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model</h2><p><strong>Authors:Yuqi Zhang, Guanying Chen, Jiaxing Chen, Chuanyu Fu, Chuan Huang, Shuguang Cui</strong></p>
<p>Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.</p>
<blockquote>
<p>é‡å»ºä¸‰ç»´åœºæ™¯å¹¶ä»æœªå¯†é›†çš„è¾“å…¥è§†è§’åˆæˆæ–°é¢–è§†è§’æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºç¨€ç–è§†å›¾è®¾ç½®ä¸‹æé«˜é‡å»ºè´¨é‡çš„ç†æƒ³å·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹äºè§†ç‚¹å˜åŒ–è¾ƒå°çš„åœºæ™¯ï¼Œåœ¨è¿‘è·ç¦»åœºæ™¯ä¸­æ•æ‰ç²¾ç»†ç»†èŠ‚æ—¶å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºè¾“å…¥ä¿¡æ¯ä¸¥é‡å—é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç§°ä¸ºCloseUpShotï¼Œå®ƒé€šè¿‡ç‚¹æ¡ä»¶è§†é¢‘æ‰©æ•£ä»ç¨€ç–è¾“å…¥è¿›è¡Œè¿‘è·ç¦»æ–°é¢–è§†è§’åˆæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åƒç´ æ‰­æ›²æ¡ä»¶åœ¨è¿‘è·ç¦»è®¾ç½®ä¸­å­˜åœ¨ä¸¥é‡çš„ç¨€ç–æ€§å’ŒèƒŒæ™¯æ³„éœ²é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåˆ†å±‚æ‰­æ›²å’Œé®æŒ¡æ„ŸçŸ¥å™ªå£°æŠ‘åˆ¶ï¼Œä»¥æé«˜è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è°ƒèŠ‚å›¾åƒçš„è´¨é‡å’Œå®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨å±€ç»“æ„å¼•å¯¼ï¼Œå®ƒåˆ©ç”¨å¯†é›†èåˆçš„ç‚¹äº‘ä¸ºæ‰©æ•£è¿‡ç¨‹æä¾›ä¸€è‡´çš„å‡ ä½•ä¸Šä¸‹æ–‡ï¼Œä»¥å¼¥è¡¥ç¨€ç–è°ƒèŠ‚è¾“å…¥ä¸­ç¼ºä¹å…¨å±€ä¸€è‡´çš„3Dçº¦æŸã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿‘è·ç¦»æ–°é¢–è§†è§’åˆæˆæ–¹é¢ï¼Œè¿™æ¸…æ¥šåœ°éªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13121v1">PDF</a> Project Link: <a target="_blank" rel="noopener" href="https://zyqz97.github.io/CloseUpShot/">https://zyqz97.github.io/CloseUpShot/</a></p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒç ”ç©¶é¢†åŸŸï¼Œç°æœ‰æŠ€æœ¯ä¸»è¦é¢å¯¹ä»ç¨€ç–è¾“å…¥é‡æ„ä¸‰ç»´åœºæ™¯å¹¶åˆæˆæ–°è§†è§’çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨è§†ç‚¹ç›¸è¿‘åœºæ™¯ä¸­é¢ä¸´å›°éš¾ã€‚æ–‡ç« æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶CloseUpShotï¼Œç”¨äºè§£å†³æ­¤é—®é¢˜ã€‚é‡‡ç”¨ç‚¹æ¡ä»¶è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå·¥ä½œï¼Œå¹¶é’ˆå¯¹åƒç´ åç§»ä¸­å­˜åœ¨çš„ä¸¥é‡ç¨€ç–æ€§å’ŒèƒŒæ™¯æ³„æ¼é—®é¢˜æå‡ºè§£å†³æ–¹æ¡ˆã€‚å¼•å…¥å…¨å±€ç»“æ„æŒ‡å¯¼ä»¥å¼¥è¡¥ç¨€ç–æ¡ä»¶è¾“å…¥ä¸­ç¼ºä¹å…¨å±€ä¸€è‡´çš„3Dçº¦æŸã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†ç‚¹ç›¸è¿‘çš„æ–°è§†è§’åˆæˆä¸­è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æŠ€æœ¯é¢ä¸´ä»ç¨€ç–è¾“å…¥é‡æ„ä¸‰ç»´åœºæ™¯çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è§†ç‚¹ç›¸è¿‘åœºæ™¯ä¸­åˆæˆæ–°è§†è§’çš„ä»»åŠ¡ã€‚</li>
<li>ä¸€ç§åä¸ºCloseUpShotçš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶è¢«æå‡ºï¼Œç”¨äºè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºåƒç´ åç§»æ¡ä»¶åœ¨è¿‘è·ç¦»è®¾ç½®ä¸­å­˜åœ¨ä¸¥é‡ç¨€ç–æ€§å’ŒèƒŒæ™¯æ³„æ¼é—®é¢˜ï¼Œå¹¶æå‡ºè§£å†³æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡å¼•å…¥å…¨å±€ç»“æ„æŒ‡å¯¼ï¼Œä¸ºæ‰©æ•£è¿‡ç¨‹æä¾›ä¸€è‡´çš„å‡ ä½•ä¸Šä¸‹æ–‡ï¼Œä»¥å¼¥è¡¥ç¨€ç–è¾“å…¥ä¸­çš„ä¸è¶³ã€‚</li>
<li>å®éªŒè¯æ˜CloseUpShotåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†ç‚¹ç›¸è¿‘çš„æ–°è§†è§’åˆæˆæ–¹é¢ã€‚</li>
<li>è¯¥æ¡†æ¶è®¾è®¡æœ‰æ•ˆï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜é‡å»ºè´¨é‡å’Œåˆæˆæ–°è§†è§’çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13121">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40ee595d7d7fc576197506bce67c0b3f" align="middle">
<img src="https://picx.zhimg.com/v2-99f88b0d9f6faab3a684bf8e06e70dda" align="middle">
<img src="https://picx.zhimg.com/v2-433d7a20a46a48c25861e40a788610e0" align="middle">
<img src="https://picx.zhimg.com/v2-f019d2a92dcf73f6ed3901d06c909caa" align="middle">
<img src="https://picx.zhimg.com/v2-de8cfc70c375ce4cef36ad76a459fcf7" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Wide-Field-X-ray-Polarimetry-for-High-Energy-Astronomical-Transients-First-results-of-the-pathfinder-CXPD-Cubesat-Mission"><a href="#Wide-Field-X-ray-Polarimetry-for-High-Energy-Astronomical-Transients-First-results-of-the-pathfinder-CXPD-Cubesat-Mission" class="headerlink" title="Wide-Field X-ray Polarimetry for High Energy Astronomical Transients: First results of the pathfinder CXPD Cubesat Mission"></a>Wide-Field X-ray Polarimetry for High Energy Astronomical Transients: First results of the pathfinder CXPD Cubesat Mission</h2><p><strong>Authors:Hong-Bang Liu, Zu-Ke Feng, Huan-Bo Feng, Di-Fan Yi, Li-Rong Xie, Yan-Jun Xie, Zong-Wang Fan, Jin Zhang, Wen-Jin Xie, Xue-Feng Huang, Wei Deng, Fei Xie, Dong Wang, Zi-Li Li, Hui Wang, Ran Chen, Shi-Qiang Zhou, Kai Chen, Jin Li, Qian Liu, Shi Chen, Rui-Ting Ma, Bin-Long Wang, Zhen-Yu Tang, Hang-Zhou Li, Bo Peng, Shu-Lin Liu, Xiang-Ming Sun, Yang-Heng Zheng, En-Wei Liang</strong></p>
<p>The Low Energy Polarization Detector (LPD) is a key component of the next-generation large-scale Gamma-Ray Burst polarimeter, POLAR-2. It is designed for polarization observations of transient sources in the soft X-ray energy range with a wide field of view (FOV). To validate the key technologies required for wide-FOV X-ray polarization measurements, the Cosmic X-ray Polarization Detector (CXPD) CubeSat was developed as a prototype for the LPD. The CXPD is equipped with two Gas Microchannel Plate Pixel Detectors (GMPDs) that measure X-ray polarization via the photoelectric effect, where ejected photoelectrons produce ionization tracks in the gas which are imaged to reconstruct their emission directions. Laboratory calibrations of the modulation factor and energy spectra were successfully performed using linear polarized X-ray sources at 2.98 keV, 4.51 keV, 6.40 keV, and 8.05 keV. Since its launch in June 2023, the CXPD has successfully completed critical in-orbit technology verification. It has also performed polarization observations of two bright X-ray sources Sco X-1 and the transient Swift J1727.8-1613 yielding constraints on their polarization degrees and angles. Notably, this was the first time that an anti-coincidence detector had been implemented in an X-ray polarimeter, enabling in-orbit verification of the charged-particle background rejection algorithm. These results demonstrate the feasibility of wide-field soft X-ray polarization measurements and provide essential guidance for the development of the LPD for the POLAR-2 mission, thereby advancing the frontier of X-ray polarization astronomy.</p>
<blockquote>
<p>ä½èƒ½æåŒ–æ¢æµ‹å™¨ï¼ˆLPDï¼‰æ˜¯ä¸‹ä¸€ä»£å¤§å‹ä¼½é©¬å°„çº¿æš´åæŒ¯ä»ªPOLAR-2çš„å…³é”®ç»„ä»¶ã€‚å®ƒè¢«è®¾è®¡ç”¨äºåœ¨è½¯Xå°„çº¿èƒ½é‡èŒƒå›´å†…å¯¹ç¬æ€æºè¿›è¡ŒåæŒ¯è§‚æµ‹ï¼Œå…·æœ‰å¹¿é˜”çš„è§†é‡ï¼ˆFOVï¼‰ã€‚ä¸ºäº†éªŒè¯ç”¨äºå®½è§†åœºXå°„çº¿åæŒ¯æµ‹é‡çš„å…³é”®æŠ€æœ¯ï¼Œå¼€å‘äº†å®‡å®™Xå°„çº¿åæŒ¯æ¢æµ‹å™¨ï¼ˆCXPDï¼‰ç«‹æ–¹ä½“å«æ˜Ÿä½œä¸ºLPDçš„åŸå‹ã€‚CXPDé…å¤‡äº†ä¸¤ä¸ªæ°”ä½“å¾®é€šé“æ¿åƒç´ æ¢æµ‹å™¨ï¼ˆGMPDï¼‰ï¼Œé€šè¿‡å…‰ç”µæ•ˆåº”æµ‹é‡Xå°„çº¿åæŒ¯ï¼Œå…¶ä¸­å‘å°„çš„å…‰ç”µå­åœ¨æ°”ä½“ä¸­äº§ç”Ÿç”µç¦»è½¨è¿¹ï¼Œç„¶åå¯¹å‘å°„æ–¹å‘è¿›è¡Œæˆåƒä»¥é‡å»ºã€‚ä½¿ç”¨çº¿æ€§åæŒ¯Xå°„çº¿æºæˆåŠŸæ‰§è¡Œäº†è°ƒåˆ¶å› å­å’Œèƒ½é‡è°±çš„å®éªŒå®¤æ ¡å‡†ï¼Œèƒ½é‡ä¸º2.98 keVã€4.51 keVã€6.40 keVå’Œ8.05 keVã€‚è‡ª2023å¹´6æœˆå‘å°„ä»¥æ¥ï¼ŒCXPDå·²æˆåŠŸå®Œæˆå…³é”®åœ¨è½¨æŠ€æœ¯éªŒè¯ã€‚å®ƒè¿˜å¯¹ä¸¤ä¸ªæ˜äº®çš„Xå°„çº¿æºSco X-1å’Œç¬æ€Swift J1727.8-1613è¿›è¡Œäº†åæŒ¯è§‚æµ‹ï¼Œå¯¹å®ƒä»¬çš„åæŒ¯åº¦å’Œè§’åº¦è¿›è¡Œäº†çº¦æŸã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨Xå°„çº¿åæŒ¯è®¡ä¸­å®ç°åç¬¦åˆæ¢æµ‹å™¨ï¼Œèƒ½å¤Ÿåœ¨è½¨é“ä¸ŠéªŒè¯å¸¦ç”µç²’å­èƒŒæ™¯æŠ‘åˆ¶ç®—æ³•ã€‚è¿™äº›ç»“æœè¯æ˜äº†å®½è§†åœºè½¯Xå°„çº¿åæŒ¯æµ‹é‡çš„å¯è¡Œæ€§ï¼Œä¸ºLPDçš„å‘å±•æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œä»è€Œæ¨åŠ¨äº†Xå°„çº¿åæŒ¯å¤©æ–‡å­¦çš„å‰æ²¿å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13094v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä½èƒ½é‡åæŒ¯æ£€æµ‹å™¨ï¼ˆLPDï¼‰åœ¨ä¸‹ä¸€ä»£å¤§è§„æ¨¡ä¼½é©¬å°„çº¿çˆ†å‘åæŒ¯ä»ªPOLAR-2ä¸­çš„å…³é”®ä½œç”¨ã€‚ä¸ºéªŒè¯å®½è§†åœºXå°„çº¿åæŒ¯æµ‹é‡çš„å…³é”®æŠ€æœ¯çš„å¯è¡Œæ€§ï¼Œå¼€å‘äº†å®‡å®™Xå°„çº¿åæŒ¯æ¢æµ‹å™¨ï¼ˆCXPDï¼‰ç«‹æ–¹ä½“å«æ˜Ÿä½œä¸ºLPDçš„åŸå‹ã€‚CXPDé…å¤‡äº†ä¸¤ä¸ªæ°”ä½“å¾®é€šé“æ¿åƒç´ æ¢æµ‹å™¨ï¼ˆGMPDsï¼‰ï¼Œé€šè¿‡å…‰ç”µæ•ˆåº”æµ‹é‡Xå°„çº¿åæŒ¯ã€‚å®éªŒå®¤æˆåŠŸä½¿ç”¨çº¿æ€§åæŒ¯Xå°„çº¿æºå¯¹è°ƒåˆ¶å› å­å’Œèƒ½é‡è°±è¿›è¡Œäº†æ ¡å‡†ã€‚è‡ª2023å¹´6æœˆå‘å°„ä»¥æ¥ï¼ŒCXPDæˆåŠŸå®Œæˆäº†å…³é”®åœ¨è½¨æŠ€æœ¯éªŒè¯ï¼Œå¹¶å¯¹ä¸¤ä¸ªæ˜äº®çš„Xå°„çº¿æºè¿›è¡Œäº†åæŒ¯è§‚æµ‹ï¼Œä¸ºLPDçš„å‘å±•æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œæ¨åŠ¨äº†Xå°„çº¿åæŒ¯å¤©æ–‡å­¦çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LPDæ˜¯POLAR-2çš„å…³é”®ç»„ä»¶ï¼Œç”¨äºè½¯Xå°„çº¿èƒ½é‡èŒƒå›´çš„ç¬æ€æºçš„åæŒ¯è§‚æµ‹ï¼Œå…·æœ‰å®½è§†åœºã€‚</li>
<li>CXPDä½œä¸ºLPDçš„åŸå‹ï¼ŒéªŒè¯äº†å®½è§†åœºXå°„çº¿åæŒ¯æµ‹é‡çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>CXPDé‡‡ç”¨GMPDsé€šè¿‡å…‰ç”µæ•ˆåº”æµ‹é‡Xå°„çº¿åæŒ¯ã€‚</li>
<li>å®éªŒå®¤æ ¡å‡†æˆåŠŸï¼Œä½¿ç”¨çº¿æ€§åæŒ¯Xå°„çº¿æºéªŒè¯äº†è°ƒåˆ¶å› å­å’Œèƒ½é‡è°±ã€‚</li>
<li>CXPDæˆåŠŸå®Œæˆåœ¨è½¨æŠ€æœ¯éªŒè¯ï¼Œå¹¶è§‚æµ‹äº†Sco X-1å’ŒSwift J1727.8-1613ä¸¤ä¸ªæ˜äº®Xå°„çº¿æºçš„åæŒ¯ã€‚</li>
<li>é¦–æ¬¡åœ¨Xå°„çº¿åæŒ¯ä»ªä¸­å®ç°åç¬¦åˆæ¢æµ‹å™¨ï¼ŒéªŒè¯äº†å¸¦ç”µç²’å­èƒŒæ™¯æŠ‘åˆ¶ç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f72433576783e060e7cc108347240932" align="middle">
<img src="https://picx.zhimg.com/v2-66d57aec96411b60f3d2e3ec88f4c54c" align="middle">
<img src="https://picx.zhimg.com/v2-85782ea6ff97e5c795df229072220a3e" align="middle">
<img src="https://picx.zhimg.com/v2-63529069b841be994d459cd26b465027" align="middle">
<img src="https://picx.zhimg.com/v2-5b08f8bb6752cfadc03a8dd6c264871c" align="middle">
<img src="https://picx.zhimg.com/v2-4e0c6e5e0e23c312dc6961dfc160b068" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FGNet-Leveraging-Feature-Guided-Attention-to-Refine-SAM2-for-3D-EM-Neuron-Segmentation"><a href="#FGNet-Leveraging-Feature-Guided-Attention-to-Refine-SAM2-for-3D-EM-Neuron-Segmentation" class="headerlink" title="FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation"></a>FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation</h2><p><strong>Authors:Zhenghua Li, Hang Chen, Zihao Sun, Kai Li, Xiaolin Hu</strong></p>
<p>Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.</p>
<blockquote>
<p>ç¥ç»ç»“æ„åœ¨ç”µå­æ˜¾å¾®é•œï¼ˆEMï¼‰å›¾åƒä¸­çš„ç²¾ç¡®åˆ†å‰²å¯¹ç¥ç»ç§‘å­¦è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™ä¸€ä»»åŠ¡é¢ä¸´ç€å¤æ‚çš„å½¢æ€ã€è¾ƒä½çš„ä¿¡å™ªæ¯”å’Œæ ‡æ³¨ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†ç°æœ‰æ–¹æ³•çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è¯•å›¾åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒä¸Šå­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†æ¥æ›´å¥½åœ°è§£å†³è¿™ä¸€ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯ä»¥æœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»é¢„è®­ç»ƒäºè‡ªç„¶å›¾åƒçš„Segment Anything 2ï¼ˆSAM2ï¼‰è½¬ç§»åˆ°EMé¢†åŸŸçš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨SAM2æå–å¼ºå¤§çš„é€šç”¨ç‰¹å¾ã€‚ä¸ºäº†å¼¥åˆé¢†åŸŸå·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç‰¹å¾å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨SAM2çš„è¯­ä¹‰çº¿ç´¢æ¥æŒ‡å¯¼ä¸€ä¸ªè½»é‡çº§çš„ç¼–ç å™¨â€”â€”ç²¾ç»†ç¼–ç å™¨ï¼ˆFGEï¼‰å…³æ³¨è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸã€‚æœ€åï¼Œä¸€ä¸ªåŒäº²å’Œè§£ç å™¨ç”Ÿæˆç²—çš„å’Œç²¾ç»†çš„äº²å’Œå›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å†»ç»“SAM2æƒé‡çš„æƒ…å†µä¸‹å®ç°äº†ä¸å›½å®¶å‰æ²¿æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ã€‚åœ¨EMæ•°æ®ä¸Šè¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒåï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„å›½å®¶å‰æ²¿æŠ€æœ¯æ–¹æ³•ã€‚è¿™é¡¹ç ”ç©¶è¯å®äº†ç»“åˆæœ‰é’ˆå¯¹æ€§çš„é¢†åŸŸè‡ªé€‚åº”æŒ‡å¯¼ï¼Œé¢„å…ˆåœ¨è‡ªç„¶å›¾åƒä¸Šè®­ç»ƒçš„è¡¨ç¤ºè¿ç§»å¯ä»¥æœ‰æ•ˆåœ°è§£å†³ç¥ç»å…ƒåˆ†å‰²ä¸­çš„ç‰¹å®šæŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13063v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”µå­æ˜¾å¾®é•œï¼ˆEMï¼‰å›¾åƒçš„ç¥ç»ç½‘ç»œç»“æ„åˆ†å‰²å¯¹ç¥ç»ç§‘å­¦è‡³å…³é‡è¦ã€‚é’ˆå¯¹å¤æ‚å½¢æ€ã€ä½ä¿¡å™ªæ¯”å’Œæ ‡æ³¨ç¨€ç¼ºç­‰æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºåˆ©ç”¨åœ¨å¤§é‡è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥æ›´å¥½åœ°è§£å†³æ­¤ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥æ–°å‹æ¡†æ¶ï¼Œæœ‰æ•ˆè¿ç§»Segment Anything 2ï¼ˆSAM2ï¼‰çš„çŸ¥è¯†è‡³EMé¢†åŸŸã€‚é¦–å…ˆä½¿ç”¨SAM2æå–é€šç”¨ç‰¹å¾ï¼Œå¹¶å¼•å…¥ç‰¹å¾å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ä»¥ç¼©å°é¢†åŸŸå·®è·ã€‚è¯¥æ¨¡å—åˆ©ç”¨SAM2çš„è¯­ä¹‰çº¿ç´¢å¼•å¯¼è½»é‡çº§ç¼–ç å™¨ï¼ˆFGEï¼‰å…³æ³¨äºæŒ‘æˆ˜æ€§åŒºåŸŸã€‚æœ€åï¼ŒåŒäº²å’ŒåŠ›è§£ç å™¨ç”Ÿæˆç²—ç²’åº¦å’Œç²¾ç»†åŒ–çš„äº²å’ŒåŠ›åœ°å›¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å†»ç»“SAM2æƒé‡çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¸æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ–¹æ³•ç›¸å½“ã€‚åœ¨EMæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰SOTAæ–¹æ³•ã€‚æ­¤ç ”ç©¶éªŒè¯äº†ç»“åˆæœ‰é’ˆå¯¹æ€§çš„é¢†åŸŸè‡ªé€‚åº”æŒ‡å¯¼ï¼Œåœ¨é¢„è®­ç»ƒçš„è‡ªç„¶å›¾åƒä¸Šè¡¨ç¤ºè½¬ç§»å¯ä»¥æœ‰æ•ˆåœ°è§£å†³ç¥ç»å…ƒåˆ†å‰²ä¸­çš„ç‰¹å®šæŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå­æ˜¾å¾®é•œï¼ˆEMï¼‰å›¾åƒçš„ç¥ç»ç½‘ç»œç»“æ„åˆ†å‰²å¯¹ç¥ç»ç§‘å­¦ç ”ç©¶è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚å½¢æ€ã€ä½ä¿¡å™ªæ¯”å’Œæ ‡æ³¨ç¨€ç¼ºç­‰ã€‚</li>
<li>ç ”ç©¶æå‡ºåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚åœ¨è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ï¼‰æ¥æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥æ–°å‹æ¡†æ¶ï¼Œæœ‰æ•ˆè¿ç§»SAM2çš„çŸ¥è¯†è‡³EMé¢†åŸŸã€‚</li>
<li>ä½¿ç”¨SAM2æå–é€šç”¨ç‰¹å¾ï¼Œå¹¶å¼•å…¥ç‰¹å¾å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—æ¥å…³æ³¨æŒ‘æˆ˜æ€§åŒºåŸŸã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¸æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ–¹æ³•ç›¸å½“ï¼Œå¹¶åœ¨å¾®è°ƒåæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-973d8f4e4d795293ac1e18ecd1fb7c5f" align="middle">
<img src="https://picx.zhimg.com/v2-6560f4bffd9bee872fc1aa4788f41371" align="middle">
<img src="https://picx.zhimg.com/v2-22e332ab719e4cc7847486aafd228410" align="middle">
<img src="https://picx.zhimg.com/v2-d391c89582b7bc5200e4962afc67c7eb" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ViSS-R1-Self-Supervised-Reinforcement-Video-Reasoning"><a href="#ViSS-R1-Self-Supervised-Reinforcement-Video-Reasoning" class="headerlink" title="ViSS-R1: Self-Supervised Reinforcement Video Reasoning"></a>ViSS-R1: Self-Supervised Reinforcement Video Reasoning</h2><p><strong>Authors:Bo Fang, Yuxin Song, Qiangqiang Wu, Haoyuan Sun, Wenhao Wu, Antoni B. Chan</strong></p>
<p>Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLMâ€™s R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.</p>
<blockquote>
<p>å¤æ‚è§†é¢‘æ¨ç†å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå½“å‰çš„R1æ–¹æ³•å¾€å¾€ä¼˜å…ˆå¤„ç†åŸºäºæ–‡æœ¬å’Œå›¾åƒå‘å±•çš„æ–‡æœ¬ä¸­å¿ƒæ¨ç†ã€‚åœ¨è§†é¢‘ä»»åŠ¡ä¸­ï¼Œè¿™äº›ç­–ç•¥ç»å¸¸æœªèƒ½å……åˆ†åˆ©ç”¨ä¸°å¯Œçš„è§†è§‰ä¿¡æ¯ï¼Œå¯¼è‡´æ½œåœ¨çš„æ·å¾„å­¦ä¹ å’Œå¯¹å¹»è§‰çš„æ˜“æ„Ÿæ€§å¢åŠ ã€‚ä¸ºäº†ä¿ƒè¿›æ›´ç¨³å¥ã€ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†æ ‡å‡†R1ç®¡é“ä¸­çš„æ–°å‹è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ GRPOç®—æ³•ï¼ˆPretext-GRPOï¼‰ï¼Œè¯¥ç®—æ³•å¯¹ç»è¿‡å˜æ¢çš„è§†è§‰è¾“å…¥çš„é¢„è®­ç»ƒä»»åŠ¡æ­£ç¡®è§£å†³æ—¶ç»™äºˆæ­£é¢å¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿéå¹³å‡¡åœ°å¤„ç†è§†è§‰ä¿¡æ¯ã€‚åŸºäºPretext-GRPOçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ViSS-R1æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç®€åŒ–å’Œé›†æˆäº†åŸºäºé¢„è®­ç»ƒä»»åŠ¡çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œç›´æ¥èå…¥MLLMçš„R1åè®­ç»ƒèŒƒå¼ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸æ˜¯ä»…ä¾èµ–äºç¨€ç–çš„è§†è§‰çº¿ç´¢ï¼Œè€Œæ˜¯è¿«ä½¿æ¨¡å‹é€šè¿‡åŒæ—¶å¤„ç†é¢„è®­ç»ƒé—®é¢˜ï¼ˆå…³äºè½¬æ¢ï¼‰å’ŒçœŸæ­£çš„ç”¨æˆ·æŸ¥è¯¢æ¥å¯¹ç»è¿‡è½¬æ¢çš„è§†è§‰è¾“å…¥è¿›è¡Œæ¨ç†ã€‚è¿™éœ€è¦è¯†åˆ«å‡ºåº”ç”¨çš„è½¬æ¢å¹¶é‡å»ºåŸå§‹è§†é¢‘ä»¥å½¢æˆå‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚åœ¨å…­ä¸ªå¹¿æ³›ä½¿ç”¨çš„è§†é¢‘æ¨ç†å’Œç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†æˆ‘ä»¬çš„Pretext-GRPOå’ŒViSS-R1åœ¨å¤æ‚è§†é¢‘æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13054v1">PDF</a> Our paper was initially titled â€œVideo-SSR1: Self-Supervised Reinforcement Video Reasoning.â€ Upon noticing its close resemblance to the title of a recently released paper, we have decided to rename our work as â€œViSS-R1.â€</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ–°å‹çš„è‡ªæˆ‘ç›‘ç£å¼ºåŒ–å­¦ä¹ GRPOç®—æ³•ï¼ˆPretext-GRPOï¼‰ï¼Œå¹¶å°†å…¶çº³å…¥æ ‡å‡†R1æµç¨‹ä¸­ã€‚Pretext-GRPOé€šè¿‡ä¸ºæ­£ç¡®è§£å†³è½¬æ¢è§†è§‰è¾“å…¥çš„é¢„è®­ç»ƒä»»åŠ¡åˆ†é…æ­£å‘å¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿéå¹³å‡¡åœ°å¤„ç†è§†è§‰ä¿¡æ¯ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æå‡ºäº†ViSS-R1æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç®€åŒ–äº†åŸºäºé¢„è®­ç»ƒä»»åŠ¡çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼Œå¹¶å°†å…¶ç›´æ¥èå…¥MLLMçš„R1åè®­ç»ƒèŒƒå¼ä¸­ã€‚è¯¥æ¡†æ¶è¦æ±‚æ¨¡å‹åŒæ—¶å¤„ç†é¢„è®­ç»ƒé—®é¢˜å’ŒçœŸå®ç”¨æˆ·æŸ¥è¯¢ï¼Œé€šè¿‡å¯¹è½¬æ¢çš„è§†è§‰è¾“å…¥è¿›è¡Œæ¨ç†ï¼Œä»è€Œå‡†ç¡®å›ç­”é—®é¢˜ã€‚ç»è¿‡å…­ä¸ªå¹¿æ³›ä½¿ç”¨çš„è§†é¢‘æ¨ç†å’Œç†è§£åŸºå‡†æµ‹è¯•çš„ç»¼åˆè¯„ä¼°ï¼Œè¯æ˜äº†Pretext-GRPOå’ŒViSS-R1åœ¨å¤æ‚è§†é¢‘æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†è§†é¢‘ä»»åŠ¡æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¤æ‚è§†é¢‘æ¨ç†æ–¹é¢ã€‚</li>
<li>ç°æœ‰çš„R1æ–¹æ³•å€¾å‘äºæ–‡æœ¬ä¸­å¿ƒåŒ–çš„æ¨ç†ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è§†é¢‘ä¸­çš„ä¸°å¯Œè§†è§‰ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥è‡ªæˆ‘ç›‘ç£å¼ºåŒ–å­¦ä¹ GRPOç®—æ³•ï¼ˆPretext-GRPOï¼‰ï¼Œæœ‰æ•ˆå¤„ç†è§†è§‰ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹å¯¹è§†é¢‘çš„è§£è¯»èƒ½åŠ›ã€‚</li>
<li>æå‡ºViSS-R1æ¡†æ¶ï¼Œå°†è‡ªæˆ‘ç›‘ç£å­¦ä¹ ä¸R1åè®­ç»ƒèŒƒå¼ç»“åˆï¼Œè¦æ±‚æ¨¡å‹åŒæ—¶å¤„ç†é¢„è®­ç»ƒé—®é¢˜å’ŒçœŸå®ç”¨æˆ·æŸ¥è¯¢ã€‚</li>
<li>ViSS-R1æ¡†æ¶é€šè¿‡å¤„ç†è½¬æ¢çš„è§†è§‰è¾“å…¥è¿›è¡Œæ¨ç†ï¼Œå‡†ç¡®å›ç­”é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹çš„å®ç”¨æ€§ã€‚</li>
<li>Pretext-GRPOå’ŒViSS-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>å…¬å¼€å¯ç”¨ç›¸å…³ä»£ç å’Œæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9ff3dfac2817434590c8fd0797d8839" align="middle">
<img src="https://picx.zhimg.com/v2-5f5408943d3c1d8a877c4cf80d4ece6a" align="middle">
<img src="https://picx.zhimg.com/v2-cc64abfd88e71043114327b116c204f8" align="middle">
<img src="https://picx.zhimg.com/v2-3d7454212221bf7a496c79d4b229b9f6" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-3D-Object-Centric-Feature-Learning-for-Semantic-Scene-Completion"><a href="#Towards-3D-Object-Centric-Feature-Learning-for-Semantic-Scene-Completion" class="headerlink" title="Towards 3D Object-Centric Feature Learning for Semantic Scene Completion"></a>Towards 3D Object-Centric Feature Learning for Semantic Scene Completion</h2><p><strong>Authors:Weihua Wang, Yubo Cui, Xiangru Lin, Zhiheng Li, Zheng Fang</strong></p>
<p>Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.</p>
<blockquote>
<p>åŸºäºè§†è§‰çš„3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰å› å…¶è‡ªåŠ¨é©¾é©¶æ½œåŠ›è€Œæ—¥ç›Šå—åˆ°å…³æ³¨ã€‚è™½ç„¶å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é‡‡ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ•´ä¸ªåœºæ™¯ä¸Šèšé›†å’Œæ‰©æ•£ç‰¹å¾ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†ç²¾ç»†çš„å¯¹è±¡çº§ç»†èŠ‚ï¼Œå¯¼è‡´è¯­ä¹‰å’Œå‡ ä½•æ¨¡ç³Šï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Oceanï¼Œä¸€ä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„é¢„æµ‹æ¡†æ¶ï¼Œå®ƒå°†åœºæ™¯åˆ†è§£æˆå•ä¸ªå¯¹è±¡å®ä¾‹ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„è¯­ä¹‰å ç”¨é¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨è½»é‡çº§çš„åˆ†å‰²æ¨¡å‹MobileSAMä»è¾“å…¥å›¾åƒä¸­æå–å®ä¾‹æ©ç ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ª3Dè¯­ä¹‰ç»„æ³¨æ„æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨çº¿æ€§æ³¨æ„åŠ›åœ¨3Dç©ºé—´ä¸­èšé›†ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç‰¹å¾ã€‚ä¸ºäº†å¤„ç†åˆ†å‰²é”™è¯¯å’Œç¼ºå¤±çš„å®ä¾‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªå…¨å±€ç›¸ä¼¼æ€§å¼•å¯¼æ³¨æ„æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨åˆ†å‰²ç‰¹å¾è¿›è¡Œå…¨å±€äº¤äº’ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®ä¾‹æ„ŸçŸ¥çš„å±€éƒ¨æ‰©æ•£æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡ç”Ÿæˆè¿‡ç¨‹æ”¹è¿›å®ä¾‹ç‰¹å¾ï¼Œç„¶åç»†åŒ–é¸Ÿç°ç©ºé—´ä¸­çš„åœºæ™¯è¡¨ç¤ºã€‚åœ¨SemanticKITTIå’ŒSSCBench-KITTI360åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOceanè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒmIoUå¾—åˆ†åˆ†åˆ«ä¸º17.40å’Œ20.28ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13031v1">PDF</a> Accept by AAAI-2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é¢å‘å¯¹è±¡çš„é¢„æµ‹æ¡†æ¶Oceanï¼Œç”¨äºè§£å†³åŸºäºè§†è§‰çš„3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰ä¸­çš„è¯­ä¹‰å’Œå‡ ä½•æ¨¡ç³Šé—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†åœºæ™¯åˆ†è§£ä¸ºä¸ªä½“å¯¹è±¡å®ä¾‹ï¼Œå®ç°æ›´ç²¾ç¡®çš„è¯­ä¹‰å ç”¨é¢„æµ‹ã€‚ä½¿ç”¨MobileSAMæå–å®ä¾‹æ©è†œï¼Œå¼•å…¥3Dè¯­ä¹‰ç»„æ³¨æ„åŠ›æ¨¡å—å’Œå…¨å±€ç›¸ä¼¼æ€§å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ä¼˜åŒ–ç‰¹å¾æå–ï¼Œæœ€åé€šè¿‡å®ä¾‹æ„ŸçŸ¥å±€éƒ¨æ‰©æ•£æ¨¡å—å®Œå–„åœºæ™¯è¡¨ç¤ºã€‚åœ¨SemanticKITTIå’ŒSSCBench-KITTI360åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOceanå–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Oceanæ˜¯ä¸€ä¸ªé¢å‘å¯¹è±¡çš„é¢„æµ‹æ¡†æ¶ï¼Œç”¨äºè§£å†³è§†è§‰3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ä¸­çš„è¯­ä¹‰å’Œå‡ ä½•æ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>Oceané€šè¿‡å°†åœºæ™¯åˆ†è§£ä¸ºä¸ªä½“å¯¹è±¡å®ä¾‹å®ç°æ›´ç²¾ç¡®çš„è¯­ä¹‰å ç”¨é¢„æµ‹ã€‚</li>
<li>ä½¿ç”¨MobileSAMæå–å®ä¾‹æ©è†œã€‚</li>
<li>Oceanå¼•å…¥3Dè¯­ä¹‰ç»„æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ©ç”¨çº¿æ€§æ³¨æ„åŠ›åœ¨3Dç©ºé—´ä¸­èšåˆå¯¹è±¡çº§ç‰¹å¾ã€‚</li>
<li>ä¸ºå¤„ç†åˆ†å‰²é”™è¯¯å’Œç¼ºå¤±å®ä¾‹ï¼ŒOceanè®¾è®¡äº†å…¨å±€ç›¸ä¼¼æ€§å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ©ç”¨åˆ†å‰²ç‰¹å¾è¿›è¡Œå…¨å±€äº¤äº’ã€‚</li>
<li>Oceanæå‡ºå®ä¾‹æ„ŸçŸ¥å±€éƒ¨æ‰©æ•£æ¨¡å—ï¼Œé€šè¿‡ç”Ÿæˆè¿‡ç¨‹æ”¹è¿›å®ä¾‹ç‰¹å¾ï¼Œå¹¶ç»†åŒ–BEVç©ºé—´ä¸­çš„åœºæ™¯è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b176e821e18249326a01fcc55c0accbd" align="middle">
<img src="https://picx.zhimg.com/v2-d7decc4adf777792327f5f72d97cd86f" align="middle">
<img src="https://picx.zhimg.com/v2-982fd138e42c45b00c22f4be4173e13a" align="middle">
<img src="https://picx.zhimg.com/v2-ec76fe343ee1fe4328f17e45b77c057c" align="middle">
<img src="https://picx.zhimg.com/v2-3217193d7bd07426126f2d26363fda65" align="middle">
<img src="https://picx.zhimg.com/v2-8a424d82c4a6bb5bc3fe144c40f52759" align="middle">
<img src="https://picx.zhimg.com/v2-cae2a10d2e29f65cde48735b3a300840" align="middle">
<img src="https://picx.zhimg.com/v2-4ca4097146be43768f80bb2ea7a82b02" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="R-2-Seg-Training-Free-OOD-Medical-Tumor-Segmentation-via-Anatomical-Reasoning-and-Statistical-Rejection"><a href="#R-2-Seg-Training-Free-OOD-Medical-Tumor-Segmentation-via-Anatomical-Reasoning-and-Statistical-Rejection" class="headerlink" title="R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection"></a>R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection</h2><p><strong>Authors:Shuaike Shen, Ke Liu, Jiaqing Xie, Shangde Gao, Chunhua Shen, Ge Liu, Mireia Crispin-Ortuzar, Shangqi Gao</strong></p>
<p>Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at <a target="_blank" rel="noopener" href="https://github.com/Eurekashen/R2Seg">https://github.com/Eurekashen/R2Seg</a>.</p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åŸºç¡€æ¨¡å‹åœ¨è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰çš„æƒ…å†µä¸‹ä¼šé‡åˆ°æŒ‘æˆ˜ï¼Œç»å¸¸åœ¨OODè‚¿ç˜¤ä¸Šäº§ç”Ÿåˆ†æ•£çš„è¯¯æŠ¥ã€‚æˆ‘ä»¬æ¨å‡ºäº†R$^{2}$Segï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„é²æ£’OODè‚¿ç˜¤åˆ†å‰²æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤é˜¶æ®µçš„Reason-and-Rejectæµç¨‹è¿›è¡Œå·¥ä½œã€‚é¦–å…ˆï¼ŒReasonæ­¥éª¤é‡‡ç”¨LLMå¼•å¯¼çš„è§£å‰–ç»“æ„æ¨ç†è§„åˆ’å™¨æ¥å®šä½å™¨å®˜é”šç‚¹å¹¶ç”Ÿæˆå¤šå°ºåº¦ROIã€‚å…¶æ¬¡ï¼ŒRejectæ­¥éª¤å¯¹å†»ç»“çš„åŸºç¡€æ¨¡å‹ï¼ˆBiomedParseï¼‰åœ¨è¿™äº›ROIå†…ç”Ÿæˆçš„å€™é€‰å¯¹è±¡åº”ç”¨åŒæ ·æœ¬ç»Ÿè®¡æµ‹è¯•ã€‚è¿™ç§ç»Ÿè®¡æ‹’ç»è¿‡æ»¤å™¨ä»…ä¿ç•™ä¸æ­£å¸¸ç»„ç»‡æœ‰æ˜¾è‘—å·®å¼‚çš„å€™é€‰å¯¹è±¡ï¼Œæœ‰æ•ˆåœ°æŠ‘åˆ¶äº†è¯¯æŠ¥ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ— éœ€æ›´æ–°å‚æ•°ï¼Œä½¿å…¶å…¼å®¹é›¶æ›´æ–°æµ‹è¯•æ—¶é—´å¢å¼ºæŠ€æœ¯å¹¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚åœ¨å¤šä¸­å¿ƒå’Œè·¨æ¨¡æ€è‚¿ç˜¤åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ï¼ŒR$^{2}$Segç›¸è¾ƒäºå¼ºå¤§çš„åŸºå‡†æ¨¡å‹å’ŒåŸå§‹åŸºç¡€æ¨¡å‹åœ¨Diceç³»æ•°ã€ç‰¹å¼‚æ€§å’Œæ•æ„Ÿæ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Eurekashen/R2Seg%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Eurekashen/R2Segæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12691v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>R$^{2}$Segæ¡†æ¶è§£å†³äº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„OODè‚¿ç˜¤åˆ†å‰²é—®é¢˜ã€‚è¯¥æ¡†æ¶æ— éœ€è®­ç»ƒï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„Reason-and-Rejectè¿‡ç¨‹è¿›è¡Œç¨³å¥çš„OODè‚¿ç˜¤åˆ†å‰²ã€‚é¦–å…ˆï¼Œä½¿ç”¨LLMå¼•å¯¼çš„è§£å‰–æ¨ç†è§„åˆ’å™¨å®šä½å™¨å®˜é”šç‚¹å¹¶ç”Ÿæˆå¤šå°ºåº¦ROIï¼›ç„¶åï¼Œåœ¨ROIå†…åº”ç”¨å†»ç»“çš„åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„å€™é€‰æ ·æœ¬è¿›è¡Œä¸¤æ ·æœ¬ç»Ÿè®¡æµ‹è¯•è¿›è¡Œç­›é€‰ã€‚è¯¥æ¡†æ¶æ— éœ€å‚æ•°æ›´æ–°ï¼Œä¸é›¶æ›´æ–°æµ‹è¯•æ—¶é—´å¢å¼ºå…¼å®¹ï¼Œé¿å…äº†ç¾éš¾æ€§é—å¿˜ã€‚åœ¨è·¨ä¸­å¿ƒå’Œè·¨æ¨¡æ€è‚¿ç˜¤åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ï¼ŒR$^{2}$Segè¾ƒåŸºçº¿æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹åœ¨Diceç³»æ•°ã€ç‰¹å¼‚æ€§å’Œæ•æ„Ÿæ€§æ–¹é¢æœ‰äº†æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>R$^{2}$Segæ¡†æ¶è§£å†³äº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„OODè‚¿ç˜¤åˆ†å‰²é—®é¢˜ã€‚</li>
<li>R$^{2}$Segé‡‡ç”¨ä¸¤é˜¶æ®µçš„Reason-and-Rejectè¿‡ç¨‹è¿›è¡Œç¨³å¥çš„OODè‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>æ¡†æ¶ä½¿ç”¨LLMå¼•å¯¼çš„è§£å‰–æ¨ç†è§„åˆ’å™¨å®šä½å™¨å®˜é”šç‚¹å¹¶ç”Ÿæˆå¤šå°ºåº¦ROIã€‚</li>
<li>åœ¨å†»ç»“çš„åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„å€™é€‰æ ·æœ¬ä¸­ï¼Œåˆ©ç”¨ä¸¤æ ·æœ¬ç»Ÿè®¡æµ‹è¯•è¿›è¡Œç­›é€‰ã€‚</li>
<li>R$^{2}$Segæ— éœ€å‚æ•°æ›´æ–°ï¼Œé¿å…äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>ä¸é›¶æ›´æ–°æµ‹è¯•æ—¶é—´å¢å¼ºå…¼å®¹ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒR$^{2}$Segåœ¨Diceç³»æ•°ã€ç‰¹å¼‚æ€§å’Œæ•æ„Ÿæ€§æ–¹é¢æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12691">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60bf74ab5d0ab84e15f53a76005a11c2" align="middle">
<img src="https://picx.zhimg.com/v2-9e55dcbd70122cd0458142cb25022eab" align="middle">
<img src="https://picx.zhimg.com/v2-d998ab0a97627d7ea7a195593cb6cf60" align="middle">
<img src="https://picx.zhimg.com/v2-f896015a7392f273aad9fd66481410f4" align="middle">
<img src="https://picx.zhimg.com/v2-bb5768e9a2eb405622715bf9de880b71" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Medical-Knowledge-Intervention-Prompt-Tuning-for-Medical-Image-Classification"><a href="#Medical-Knowledge-Intervention-Prompt-Tuning-for-Medical-Image-Classification" class="headerlink" title="Medical Knowledge Intervention Prompt Tuning for Medical Image Classification"></a>Medical Knowledge Intervention Prompt Tuning for Medical Image Classification</h2><p><strong>Authors:Ye Du, Nanxi Yu, Shujun Wang</strong></p>
<p>Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at <a target="_blank" rel="noopener" href="https://github.com/usr922/cilmp">https://github.com/usr922/cilmp</a>.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¹¿æ³›çš„åŒ»ç–—ç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºç‰¹å¾è¿ç§»å’Œæ³›åŒ–çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºå‚æ•°ä¼—å¤šï¼Œå¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒéœ€è¦æ¶ˆè€—å¤§é‡èµ„æºã€‚æç¤ºè°ƒæ•´çš„å‡ºç°ä½œä¸ºä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥ç¼“è§£å†…å­˜ä½¿ç”¨å¹¶å‡å°‘è®­ç»ƒæ—¶é—´ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›æ€§èƒ½ã€‚ç„¶è€Œï¼ŒæŒ‘æˆ˜åœ¨äºç°æœ‰çš„æç¤ºè°ƒæ•´æ–¹æ³•æ— æ³•ç²¾ç¡®åŒºåˆ†ä¸åŒçš„åŒ»å­¦æ¦‚å¿µï¼Œè¿™åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­é”™è¿‡äº†å„ç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸‹ç‰¹å®šç–¾ç—…çš„ç›¸å…³ç‰¹å¾ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç‰¹åˆ«æ“…é•¿æä¾›è¿™ç§ä¸“ä¸šåŒ»å­¦çŸ¥è¯†ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºå°†LLMsçº³å…¥æç¤ºè°ƒæ•´è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†CILMPï¼ˆç”¨äºæç¤ºè°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¡ä»¶å¹²é¢„ï¼‰ï¼Œä¸€ç§å°†LLMså’ŒVLMsè”ç³»èµ·æ¥çš„æ–¹æ³•ï¼Œä¾¿äºå°†åŒ»å­¦çŸ¥è¯†è½¬ç§»åˆ°VLMæç¤ºä¸­ã€‚CILMPä»LLMsä¸­æå–ç‰¹å®šç–¾ç—…çš„è¡¨ç¤ºï¼Œåœ¨ä½é˜¶çº¿æ€§å­ç©ºé—´å†…è¿›è¡Œå¹²é¢„ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬æ¥åˆ›å»ºç‰¹å®šç–¾ç—…çš„æç¤ºã€‚æ­¤å¤–ï¼Œè¿˜èå…¥äº†ä¸€ç§æ¡ä»¶æœºåˆ¶ï¼Œä»¥å¯¹æ¯ä¸ªå•ç‹¬çš„åŒ»å­¦å›¾åƒè¿›è¡Œæ¡ä»¶å¹²é¢„è¿‡ç¨‹ï¼Œç”Ÿæˆå®ä¾‹è‡ªé€‚åº”çš„æç¤ºï¼Œä»è€Œæé«˜é€‚åº”æ€§ã€‚åœ¨å¤šç§åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCILMPå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æç¤ºè°ƒæ•´æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ä»£ç å¯è®¿é—®äºï¼š[<a target="_blank" rel="noopener" href="https://github.com/usr9">https://github.com/usr9</a> è„±ç¦»ç®¡æ§å°†é€šè¿‡ç»™äºˆåŒ»ç–—å·¥ä½œè€…æ›´å¤§ç¨‹åº¦çš„è‡ªä¸»å†³ç­–æƒæ¥æ”¹å–„åŒ»ç–—æœåŠ¡çš„è´¨é‡å¹¶æé«˜å·¥ä½œæ•ˆç‡ã€‚ï¼ˆæœºå™¨ç¿»è¯‘çš„ç»“æœå¹¶ä¸ç†æƒ³ï¼Œè¯·å¯¹è¿™å¥è¯è¿›è¡Œæ›´å‡†ç¡®çš„ç¿»è¯‘ã€‚ï¼‰æ­£ç¡®ç¿»è¯‘ä¸ºï¼šâ€œUncontrolled autonomy will improve the quality of healthcare services and increase work efficiency by giving medical workers greater decision-making power.â€</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12639v1">PDF</a> IEEE Transactions on Medical Imaging (Early Access) July 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„æç¤ºè°ƒæ•´è¿‡ç¨‹çš„æ–¹æ³•ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºCILMPçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆLLMså’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œä»LLMsä¸­æå–ç–¾ç—…ç‰¹å¼‚æ€§è¡¨ç¤ºï¼Œå¹¶åœ¨ä½é˜¶çº¿æ€§å­ç©ºé—´å†…è¿›è¡Œå¹²é¢„ï¼Œä»¥åˆ›å»ºç–¾ç—…ç‰¹å¼‚æ€§æç¤ºã€‚CILMPè¿˜èå…¥äº†ä¸€ç§æ¡ä»¶æœºåˆ¶ï¼Œæ ¹æ®æ¯å¼ åŒ»å­¦å›¾åƒç”Ÿæˆå®ä¾‹é€‚åº”æ€§æç¤ºï¼Œä»è€Œæé«˜é€‚åº”æ€§ã€‚å®éªŒè¯æ˜ï¼ŒCILMPåœ¨å¤šç§åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æç¤ºè°ƒæ•´æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨ç‰¹å¾è¿ç§»å’Œæ³›åŒ–æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å‚æ•°ä¼—å¤šå¯¼è‡´èµ„æºå¯†é›†å‹ç²¾ç»†è°ƒæ•´ã€‚</li>
<li>æç¤ºè°ƒæ•´æ˜¯ä¸€ç§é™ä½å†…å­˜ä½¿ç”¨ã€å‡å°‘è®­ç»ƒæ—¶é—´å¹¶ä¿æŒç«äº‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰æç¤ºè°ƒæ•´æ–¹æ³•æ— æ³•ç²¾ç¡®åŒºåˆ†ä¸åŒçš„åŒ»å­¦æ¦‚å¿µï¼Œå¯¼è‡´åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ç‰¹å®šç–¾ç—…ç›¸å…³ç‰¹å¾çš„ä¸¢å¤±ã€‚</li>
<li>LLMsåœ¨æä¾›åŒ»å­¦çŸ¥è¯†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>CILMPæ–¹æ³•ç»“åˆäº†LLMså’ŒVLMsï¼Œé€šè¿‡ä»LLMsä¸­æå–ç–¾ç—…ç‰¹å¼‚æ€§è¡¨ç¤ºå¹¶å¹²é¢„ä½é˜¶çº¿æ€§å­ç©ºé—´ï¼Œåˆ›å»ºç–¾ç—…ç‰¹å¼‚æ€§æç¤ºã€‚</li>
<li>CILMPèå…¥äº†æ¡ä»¶æœºåˆ¶ï¼Œä¸ºæ¯å¼ åŒ»å­¦å›¾åƒç”Ÿæˆå®ä¾‹é€‚åº”æ€§æç¤ºï¼Œæé«˜é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53aec0dec73a33ce38376a079b21f17e" align="middle">
<img src="https://picx.zhimg.com/v2-48e46806c7f15469ec3384922606d43a" align="middle">
<img src="https://picx.zhimg.com/v2-bd2333b5922b77a435c7e4ee1d1886ec" align="middle">
<img src="https://picx.zhimg.com/v2-84526ae71c94b2a21bfb52725250dc55" align="middle">
<img src="https://picx.zhimg.com/v2-22cf0d09b9a9abf99847498971c9403d" align="middle">
<img src="https://picx.zhimg.com/v2-f43c14fb175b6254545a58b295d7aca1" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Explainable-deep-learning-framework-for-cancer-therapeutic-target-prioritization-leveraging-PPI-centrality-and-node-embeddings"><a href="#Explainable-deep-learning-framework-for-cancer-therapeutic-target-prioritization-leveraging-PPI-centrality-and-node-embeddings" class="headerlink" title="Explainable deep learning framework for cancer therapeutic target prioritization leveraging PPI centrality and node embeddings"></a>Explainable deep learning framework for cancer therapeutic target prioritization leveraging PPI centrality and node embeddings</h2><p><strong>Authors:Adham M. Alkhadrawi, Kyungsu Kim, Arif M. Rahman</strong></p>
<p>We developed an explainable deep learning framework integrating protein-protein interaction (PPI) network centrality metrics with node embeddings for cancer therapeutic target prioritization. A high-confidence PPI network was constructed from STRING database interactions, computing six centrality metrics: degree, strength, betweenness, closeness, eigenvector centrality, and clustering coefficient. Node2Vec embeddings captured latent network topology. Combined features trained XGBoost and neural network classifiers using DepMap CRISPR essentiality scores as ground truth. Model interpretability was assessed through GradientSHAP analysis quantifying feature contributions. We developed a novel blended scoring approach combining model probability predictions with SHAP attribution magnitudes for enhanced gene prioritization. Our framework achieved state-of-the-art performance with AUROC of 0.930 and AUPRC of 0.656 for identifying the top 10% most essential genes. GradientSHAP analysis revealed centrality measures contributed significantly to predictions, with degree centrality showing strongest correlation ($Ï$ &#x3D; -0.357) with gene essentiality. The blended scoring approach created robust gene prioritization rankings, successfully identifying known essential genes including ribosomal proteins (RPS27A, RPS17, RPS6) and oncogenes (MYC). This study presents a human-based, combinatorial \textit{in silico} framework successfully integrating network biology with explainable AI for therapeutic target discovery. The framework provides mechanistic transparency through feature attribution analysis while maintaining state-of-the-art predictive performance. Its reproducible design and reliance on human molecular datasets demonstrate a reduction-to-practice example of next-generation, animal-free modeling for cancer therapeutic target discovery and prioritization.</p>
<blockquote>
<p>æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯è§£é‡Šçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰ç½‘ç»œä¸­å¿ƒæ€§åº¦é‡æ ‡å‡†å’ŒèŠ‚ç‚¹åµŒå…¥æŠ€æœ¯ï¼Œç”¨äºç™Œç—‡æ²»ç–—é¶ç‚¹çš„ä¼˜å…ˆæ’åºã€‚æˆ‘ä»¬ä»STRINGæ•°æ®åº“çš„ç›¸äº’ä½œç”¨ä¸­æ„å»ºäº†ä¸€ä¸ªé«˜å¯ä¿¡åº¦çš„PPIç½‘ç»œï¼Œè®¡ç®—äº†å…­ç§ä¸­å¿ƒæ€§åº¦é‡æ ‡å‡†ï¼šåº¦æ•°ã€å¼ºåº¦ã€ä»‹æ•°ã€æ¥è¿‘åº¦ã€ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§å’Œèšç±»ç³»æ•°ã€‚Node2VecåµŒå…¥æŠ€æœ¯æ•æ‰äº†æ½œåœ¨çš„ç½‘ç»œæ‹“æ‰‘ç»“æ„ã€‚ç»“åˆç‰¹å¾è®­ç»ƒäº†XGBoostå’Œç¥ç»ç½‘ç»œåˆ†ç±»å™¨ï¼Œä½¿ç”¨DepMap CRISPRå¿…éœ€æ€§å¾—åˆ†ä½œä¸ºçœŸå®æ ‡ç­¾ã€‚é€šè¿‡GradientSHAPåˆ†æè¯„ä¼°æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œé‡åŒ–ç‰¹å¾è´¡çŒ®ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹æ··åˆè¯„åˆ†æ–¹æ³•ï¼Œç»“åˆæ¨¡å‹æ¦‚ç‡é¢„æµ‹å’ŒSHAPå½’å› å¹…åº¦ï¼Œä»¥ä¼˜åŒ–åŸºå› ä¼˜å…ˆæ’åºã€‚æˆ‘ä»¬çš„æ¡†æ¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è¯†åˆ«å‰10%çš„æœ€å…³é”®åŸºå› æ–¹é¢ï¼ŒAUROCä¸º0.930ï¼ŒAUPRCä¸º0.656ã€‚GradientSHAPåˆ†ææ˜¾ç¤ºï¼Œä¸­å¿ƒæ€§åº¦é‡å¯¹é¢„æµ‹çš„è´¡çŒ®æ˜¾è‘—ï¼Œå…¶ä¸­åº¦æ•°ä¸­å¿ƒæ€§ä¸åŸºå› å¿…éœ€æ€§çš„ç›¸å…³æ€§æœ€å¼ºï¼ˆÏ&#x3D;-0.357ï¼‰ã€‚æ··åˆè¯„åˆ†æ–¹æ³•åˆ›å»ºäº†ç¨³å¥çš„åŸºå› ä¼˜å…ˆæ’åºï¼ŒæˆåŠŸè¯†åˆ«äº†å·²çŸ¥çš„å…³é”®åŸºå› ï¼ŒåŒ…æ‹¬æ ¸ç³–ä½“è›‹ç™½ï¼ˆRPS27Aã€RPS17ã€RPS6ï¼‰å’Œè‡´ç™ŒåŸºå› ï¼ˆMYCï¼‰ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºäººç±»çš„ç»„åˆå¼ä½“å¤–æ¡†æ¶ï¼ŒæˆåŠŸåœ°å°†ç½‘ç»œç”Ÿç‰©å­¦ä¸å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½ç›¸ç»“åˆï¼Œç”¨äºå‘ç°æ²»ç–—é¶ç‚¹ã€‚è¯¥æ¡†æ¶é€šè¿‡ç‰¹å¾å½’å±åˆ†ææä¾›æœºåˆ¶é€æ˜åº¦ï¼ŒåŒæ—¶ä¿æŒæœ€å…ˆè¿›çš„é¢„æµ‹æ€§èƒ½ã€‚å…¶å¯é‡å¤çš„è®¾è®¡å’Œå¯¹äººç±»åˆ†å­æ•°æ®é›†çš„ä¾èµ–ï¼Œå±•ç¤ºäº†ä¸‹ä¸€ä»£æ— åŠ¨ç‰©å»ºæ¨¡åœ¨ç™Œç—‡æ²»ç–—é¶ç‚¹å‘ç°å’Œä¼˜å…ˆæ’åºä¸­çš„å®è·µç¤ºä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12463v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å¼€å‘äº†ä¸€ç§èåˆè›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰ç½‘ç»œä¸­å¿ƒæ€§åº¦é‡ä¸èŠ‚ç‚¹åµŒå…¥çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡æ¡†æ¶ï¼Œç”¨äºç™Œç—‡æ²»ç–—é¶ç‚¹ä¼˜å…ˆæ’åºã€‚ç ”ç©¶æ„å»ºäº†é«˜ç½®ä¿¡åº¦çš„PPIç½‘ç»œï¼Œè®¡ç®—äº†å…­ç§ä¸­å¿ƒæ€§åº¦é‡æŒ‡æ ‡ï¼Œå¹¶ç»“åˆNode2VecåµŒå…¥æŠ€æœ¯æ•æ‰æ½œåœ¨ç½‘ç»œæ‹“æ‰‘ç»“æ„ã€‚é€šè¿‡XGBoostå’Œç¥ç»ç½‘ç»œåˆ†ç±»å™¨ç»“åˆç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œä»¥DepMap CRISPRä¾èµ–æ€§åˆ†æ•°ä½œä¸ºçœŸå®ä¾æ®ã€‚é€šè¿‡GradientSHAPåˆ†æè¯„ä¼°æ¨¡å‹è§£é‡Šæ€§ï¼Œé‡åŒ–ç‰¹å¾è´¡çŒ®ã€‚å¼€å‘äº†ä¸€ç§æ–°å‹æ··åˆè¯„åˆ†æ–¹æ³•ï¼Œç»“åˆæ¨¡å‹é¢„æµ‹æ¦‚ç‡ä¸SHAPå½’å› å¹…åº¦ï¼Œä»¥æé«˜åŸºå› ä¼˜å…ˆæ’åºã€‚è¯¥æ¡†æ¶è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒAUROCä¸º0.930ï¼ŒAUPRCä¸º0.656ï¼Œåœ¨è¯†åˆ«å‰10%æœ€é‡è¦åŸºå› æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚GradientSHAPåˆ†ææ˜¾ç¤ºä¸­å¿ƒæµ‹é‡å¯¹é¢„æµ‹è´¡çŒ®æ˜¾è‘—ï¼Œå…¶ä¸­ç¨‹åº¦ä¸­å¿ƒæ€§ä¸åŸºå› å¿…éœ€æ€§ç›¸å…³æ€§æœ€å¼ºï¼ˆÏ&#x3D;-0.357ï¼‰ã€‚æ··åˆè¯„åˆ†æ–¹æ³•äº§ç”Ÿç¨³å¥çš„åŸºå› ä¼˜å…ˆæ’åºæ’åï¼ŒæˆåŠŸè¯†åˆ«å‡ºå·²çŸ¥çš„å…³é”®åŸºå› ï¼ŒåŒ…æ‹¬æ ¸ç³–ä½“è›‹ç™½ï¼ˆRPS27Aã€RPS17ã€RPS6ï¼‰å’ŒåŸç™ŒåŸºå› ï¼ˆMYCï¼‰ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æˆåŠŸèåˆç½‘ç»œç”Ÿç‰©å­¦ä¸å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„äººç±»ç»„åˆå¼ä½“å¤–æ¡†æ¶ï¼Œç”¨äºæ²»ç–—é¶ç‚¹å‘ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡ç‰¹å¾å½’å› åˆ†ææä¾›æœºåˆ¶é€æ˜åº¦ï¼ŒåŒæ—¶ä¿æŒæœ€å…ˆè¿›çš„é¢„æµ‹æ€§èƒ½ã€‚å…¶å¯é‡å¤çš„è®¾è®¡ä»¥åŠå¯¹äººç±»åˆ†å­æ•°æ®çš„ä¾èµ–ï¼Œå±•ç¤ºäº†æ— åŠ¨ç‰©å»ºæ¨¡çš„ä¸‹ä¸€ä»£ç™Œç—‡æ²»ç–—é¶ç‚¹å‘ç°å’Œä¼˜å…ˆæ’åºçš„å®è·µèŒƒä¾‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼€å‘äº†ä¸€ç§èåˆPPIç½‘ç»œä¸­å¿ƒæ€§åº¦é‡å’ŒèŠ‚ç‚¹åµŒå…¥çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡æ¡†æ¶ï¼Œç”¨äºç™Œç—‡æ²»ç–—é¶ç‚¹ä¼˜å…ˆæ’åºã€‚</li>
<li>æ„å»ºäº†é«˜ç½®ä¿¡åº¦çš„PPIç½‘ç»œï¼Œå¹¶è®¡ç®—äº†å…­ç§ä¸­å¿ƒæ€§åº¦é‡æŒ‡æ ‡æ¥è¯„ä¼°è›‹ç™½è´¨çš„é‡è¦æ€§ã€‚</li>
<li>ä½¿ç”¨Node2VecåµŒå…¥æŠ€æœ¯æ•æ‰ç½‘ç»œæ‹“æ‰‘ç»“æ„ã€‚</li>
<li>ç»“åˆç‰¹å¾è®­ç»ƒçš„XGBoostå’Œç¥ç»ç½‘ç»œåˆ†ç±»å™¨è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>é€šè¿‡GradientSHAPåˆ†æè¯„ä¼°æ¨¡å‹è§£é‡Šæ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ··åˆè¯„åˆ†æ–¹æ³•æ¥å¢å¼ºåŸºå› ä¼˜å…ˆæ’åºçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12463">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb7c07edf775142e5228b7305f054870" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AGGRNet-Selective-Feature-Extraction-and-Aggregation-for-Enhanced-Medical-Image-Classification"><a href="#AGGRNet-Selective-Feature-Extraction-and-Aggregation-for-Enhanced-Medical-Image-Classification" class="headerlink" title="AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification"></a>AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification</h2><p><strong>Authors:Ansh Makwe, Akansh Agrawal, Prateek Jain, Akshan Agrawal, Priyanka Bagade</strong></p>
<p>Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.</p>
<blockquote>
<p>å¯¹äºå¤æ‚æ€§ä»»åŠ¡å¦‚ä¸¥é‡æ€§åˆ†çº§å’Œç–¾ç—…äºšå‹åˆ†ç±»çš„åŒ»å­¦å›¾åƒåˆ†æï¼Œç”±äºç±»åˆ«é—´å¤æ‚ä¸”ç›¸ä¼¼çš„è§†è§‰æ¨¡å¼ã€æ ‡è®°æ•°æ®çš„ç¨€ç¼ºä»¥åŠä¸“å®¶è§£è¯»çš„å·®å¼‚æ€§ï¼Œè¿™å¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰çš„åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹åœ¨æ•æ‰åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„å¤æ‚è§†è§‰æ¨¡å¼æ–¹é¢å¾ˆæœ‰ç”¨ï¼Œä½†åŸºç¡€æ¶æ„é€šå¸¸é¢ä¸´ç€æœ‰æ•ˆåŒºåˆ†ç»†å¾®ç±»åˆ«æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬éš¾ä»¥æ•æ‰ç±»é—´ç›¸ä¼¼æ€§å’Œç±»å†…å˜å¼‚æ€§ï¼Œä»è€Œå¯¼è‡´è¯Šæ–­é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AGGRNetæ¡†æ¶ï¼Œä»¥æå–ä¿¡æ¯æ€§å’Œéä¿¡æ¯æ€§ç‰¹å¾ï¼Œä»è€Œæœ‰æ•ˆåœ°ç†è§£ç»†å¾®çš„è§†è§‰æ¨¡å¼ï¼Œå¹¶æ”¹è¿›å¤æ‚çš„åŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡çš„åˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å„ç§åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨Kvasiræ•°æ®é›†ä¸Šçš„æœ€ä½³æ”¹è¿›è¾¾åˆ°äº†5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12382v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¤æ‚ä»»åŠ¡ï¼Œå¦‚ä¸¥é‡ç¨‹åº¦åˆ†çº§å’Œç–¾ç—…äºšå‹åˆ†ç±»ï¼Œå­˜åœ¨ç±»é—´è§†è§‰æ¨¡å¼å¤æ‚ä¸”ç›¸ä¼¼ã€æ ‡æ³¨æ•°æ®ç¨€ç¼ºä»¥åŠä¸“å®¶è§£è¯»å·®å¼‚ç­‰æŒ‘æˆ˜ã€‚ç°æœ‰åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹åœ¨æ•æ‰åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„å¤æ‚è§†è§‰æ¨¡å¼æ–¹é¢å…·æœ‰ä¸€å®šä½œç”¨ï¼Œä½†åŸºç¡€æ¶æ„åœ¨åŒºåˆ†ç»†å¾®ç±»åˆ«æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥æ•æ‰ç±»é—´ç›¸ä¼¼æ€§å’Œç±»å†…å˜å¼‚æ€§ï¼Œå¯¼è‡´è¯Šæ–­é”™è¯¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºAGGRNetæ¡†æ¶ï¼Œä»¥æå–ä¿¡æ¯æ€§å’Œéä¿¡æ¯æ€§ç‰¹å¾ï¼Œä»è€Œæœ‰æ•ˆç†è§£ç»†å¾®çš„è§†è§‰æ¨¡å¼ï¼Œæé«˜å¤æ‚åŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡çš„åˆ†ç±»æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šç§åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨Kvasiræ•°æ®é›†ä¸Šçš„æœ€ä½³æ”¹è¿›ç‡é«˜è¾¾5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æé¢ä¸´å¤æ‚ä»»åŠ¡æŒ‘æˆ˜ï¼Œå¦‚ç±»é—´è§†è§‰æ¨¡å¼å¤æ‚ã€æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œä¸“å®¶è§£è¯»å·®å¼‚ã€‚</li>
<li>ç°æœ‰æ³¨æ„åŠ›æ¨¡å‹åœ¨æ•æ‰åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„å¤æ‚è§†è§‰æ¨¡å¼æ–¹é¢æœ‰ä¸€å®šä½œç”¨ï¼Œä½†åœ¨åŒºåˆ†ç»†å¾®ç±»åˆ«æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>AGGRNetæ¡†æ¶æ—¨åœ¨é€šè¿‡æå–ä¿¡æ¯æ€§å’Œéä¿¡æ¯æ€§ç‰¹å¾ï¼Œæœ‰æ•ˆç†è§£ç»†å¾®çš„è§†è§‰æ¨¡å¼ã€‚</li>
<li>AGGRNetæ¡†æ¶å¯æé«˜å¤æ‚åŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡çš„åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAGGRNetæ¨¡å‹åœ¨å¤šç§åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>åœ¨Kvasiræ•°æ®é›†ä¸Šçš„æœ€ä½³æ”¹è¿›ç‡é«˜è¾¾5%ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>AGGRNetæ¡†æ¶æœ‰æœ›ä¸ºåŒ»å­¦å›¾åƒåˆ†æå¸¦æ¥æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆçš„è¯Šæ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12382">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b875b46c34376dba05fc5934b3cf150d" align="middle">
<img src="https://picx.zhimg.com/v2-1a8543a3a16dd3257142d2fdcf58b849" align="middle">
<img src="https://picx.zhimg.com/v2-4b52dcce90eaf76712a3427105692bb1" align="middle">
<img src="https://picx.zhimg.com/v2-87013e5387b7fa505706c943453a5817" align="middle">
<img src="https://picx.zhimg.com/v2-4a989b4f76fe5696132633ac7bfeacc3" align="middle">
<img src="https://picx.zhimg.com/v2-70ffbeccb70b9b6a5bf7c9c012016a07" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MTMed3D-A-Multi-Task-Transformer-Based-Model-for-3D-Medical-Imaging"><a href="#MTMed3D-A-Multi-Task-Transformer-Based-Model-for-3D-Medical-Imaging" class="headerlink" title="MTMed3D: A Multi-Task Transformer-Based Model for 3D Medical Imaging"></a>MTMed3D: A Multi-Task Transformer-Based Model for 3D Medical Imaging</h2><p><strong>Authors:Fan Li, Arun Iyengar, Lanyu Xu</strong></p>
<p>In the field of medical imaging, AI-assisted techniques such as object detection, segmentation, and classification are widely employed to alleviate the workload of physicians and doctors. However, single-task models are predominantly used, overlooking the shared information across tasks. This oversight leads to inefficiencies in real-life applications. In this work, we propose MTMed3D, a novel end-to-end Multi-task Transformer-based model to address the limitations of single-task models by jointly performing 3D detection, segmentation, and classification in medical imaging. Our model uses a Transformer as the shared encoder to generate multi-scale features, followed by CNN-based task-specific decoders. The proposed framework was evaluated on the BraTS 2018 and 2019 datasets, achieving promising results across all three tasks, especially in detection, where our method achieves better results than prior works. Additionally, we compare our multi-task model with equivalent single-task variants trained separately. Our multi-task model significantly reduces computational costs and achieves faster inference speed while maintaining comparable performance to the single-task models, highlighting its efficiency advantage. To the best of our knowledge, this is the first work to leverage Transformers for multi-task learning that simultaneously covers detection, segmentation, and classification tasks in 3D medical imaging, presenting its potential to enhance diagnostic processes. The code is available at <a target="_blank" rel="noopener" href="https://github.com/fanlimua/MTMed3D.git">https://github.com/fanlimua/MTMed3D.git</a>.</p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œäººå·¥æ™ºèƒ½è¾…åŠ©æŠ€æœ¯å¦‚ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»è¢«å¹¿æ³›åº”ç”¨äºå‡è½»åŒ»ç”Ÿå’ŒåŒ»å¸ˆçš„å·¥ä½œé‡ã€‚ç„¶è€Œï¼Œç›®å‰ä¸»è¦ä½¿ç”¨å•ä»»åŠ¡æ¨¡å‹ï¼Œå¿½ç•¥äº†ä»»åŠ¡é—´çš„å…±äº«ä¿¡æ¯ã€‚è¿™ç§ç–å¿½å¯¼è‡´äº†åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡ä¸é«˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MTMed3Dï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯çš„åŸºäºå¤šä»»åŠ¡Transformerçš„æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è”åˆæ‰§è¡ŒåŒ»å­¦æˆåƒä¸­çš„3Dæ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»æ¥è§£å†³å•ä»»åŠ¡æ¨¡å‹çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨Transformerä½œä¸ºå…±äº«ç¼–ç å™¨æ¥ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾ï¼Œç„¶åé‡‡ç”¨åŸºäºCNNçš„ä»»åŠ¡ç‰¹å®šè§£ç å™¨ã€‚æ‰€æå‡ºçš„æ¡†æ¶åœ¨BraTS 2018å’Œ2019æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨ä¸‰ä¸ªä»»åŠ¡ä¸­éƒ½å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¥½çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„å¤šä»»åŠ¡æ¨¡å‹ä¸ç­‰æ•ˆçš„å•ä»»åŠ¡å˜ä½“è¿›è¡Œäº†åˆ†åˆ«è®­ç»ƒæ¯”è¾ƒã€‚æˆ‘ä»¬çš„å¤šä»»åŠ¡æ¨¡å‹åœ¨å¤§å¹…é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œè€Œä¸”æ€§èƒ½ä¸å•ä»»åŠ¡æ¨¡å‹ç›¸å½“ï¼Œå‡¸æ˜¾äº†å…¶æ•ˆç‡ä¼˜åŠ¿ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åˆ©ç”¨Transformerè¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ çš„å·¥ä½œï¼ŒåŒæ—¶è¦†ç›–åŒ»å­¦æˆåƒä¸­çš„æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ï¼Œå±•ç°äº†å…¶åœ¨å¢å¼ºè¯Šæ–­è¿‡ç¨‹ä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/fanlimua/MTMed3D.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/fanlimua/MTMed3D.gitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12373v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œäººå·¥æ™ºèƒ½è¾…åŠ©æŠ€æœ¯å¦‚ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»ç­‰å¹¿æ³›åº”ç”¨äºå‡è½»åŒ»ç”Ÿå’ŒåŒ»å¸ˆçš„å·¥ä½œè´Ÿæ‹…ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»è¦ä½¿ç”¨å•ä»»åŠ¡æ¨¡å‹ï¼Œå¿½ç•¥äº†ä»»åŠ¡é—´å…±äº«ä¿¡æ¯çš„é‡è¦æ€§ï¼Œå¯¼è‡´å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºMTMed3Dï¼Œä¸€ç§åŸºäºå¤šä»»åŠ¡Transformerçš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å•ä»»åŠ¡æ¨¡å‹çš„å±€é™æ€§ï¼Œé€šè¿‡è”åˆæ‰§è¡ŒåŒ»å­¦æˆåƒä¸­çš„3Dæ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ¨¡å‹ä½¿ç”¨Transformerä½œä¸ºå…±äº«ç¼–ç å™¨ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶åŸºäºCNNçš„ä»»åŠ¡ç‰¹å®šè§£ç å™¨ã€‚åœ¨BraTS 2018å’Œ2019æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¼˜äºå…ˆå‰å·¥ä½œçš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å¤šä»»åŠ¡æ¨¡å‹ä¸ç­‰æ•ˆçš„å•ä»»åŠ¡å˜ä½“è¿›è¡Œæ¯”è¾ƒï¼Œå‘ç°å¤šä»»åŠ¡æ¨¡å‹åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œæé«˜æ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œä¿æŒä¸å•ä»»åŠ¡æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶æ•ˆç‡ä¼˜åŠ¿ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åˆ©ç”¨Transformerè¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ çš„å·¥ä½œï¼ŒåŒæ—¶æ¶µç›–3DåŒ»å­¦æˆåƒä¸­çš„æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ï¼Œæœ‰æ½œåŠ›å¢å¼ºè¯Šæ–­è¿‡ç¨‹ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/fanlimua/MTMed3D.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/fanlimua/MTMed3D.gitè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AIè¾…åŠ©æŠ€æœ¯åœ¨åŒ»å­¦æˆåƒä¸­å¹¿æ³›åº”ç”¨ï¼Œä¸»è¦ç”¨äºå‡è½»åŒ»ç”Ÿå’ŒåŒ»å¸ˆçš„å·¥ä½œè´Ÿæ‹…ã€‚</li>
<li>å½“å‰æ¨¡å‹ä¸»è¦ä¸ºå•ä»»åŠ¡æ¨¡å‹ï¼Œå¿½ç•¥äº†ä»»åŠ¡é—´å…±äº«ä¿¡æ¯çš„é‡è¦æ€§ï¼Œå¯¼è‡´å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤šä»»åŠ¡Transformerçš„ç«¯åˆ°ç«¯æ¨¡å‹MTMed3Dï¼Œèƒ½è”åˆæ‰§è¡ŒåŒ»å­¦æˆåƒä¸­çš„æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>MTMed3Dä½¿ç”¨Transformerä½œä¸ºå…±äº«ç¼–ç å™¨ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶åŸºäºCNNçš„ä»»åŠ¡ç‰¹å®šè§£ç å™¨ã€‚</li>
<li>åœ¨BraTSæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜MTMed3Dåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½æœ‰è‰¯å¥½è¡¨ç°ã€‚</li>
<li>ä¸å•ä»»åŠ¡æ¨¡å‹ç›¸æ¯”ï¼Œå¤šä»»åŠ¡æ¨¡å‹MTMed3Dèƒ½æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œæé«˜æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d457ccfca353bfdab6b4648767ef975" align="middle">
<img src="https://picx.zhimg.com/v2-a9ae928561bd5a6734bedb56dc5861fb" align="middle">
<img src="https://picx.zhimg.com/v2-4c35c42efc8e08f68db75ec0d518fd32" align="middle">
<img src="https://picx.zhimg.com/v2-5bd3890dcef794ef27226daf2c1e3935" align="middle">
<img src="https://picx.zhimg.com/v2-94b6eac7fe47fb272a9a1a7d3d1b6cdc" align="middle">
<img src="https://picx.zhimg.com/v2-d6a8356db04f36968292ccad765de3f5" align="middle">
<img src="https://picx.zhimg.com/v2-fcf01efdad8fb8e064fe761f2b5b9ca2" align="middle">
<img src="https://picx.zhimg.com/v2-4846899e3e87a052ff087a808bce65c6" align="middle">
<img src="https://picx.zhimg.com/v2-05f12acae7b472d42290ce12948ce118" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4fbf6eae5c80051fc5966dffb97d4896" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  FoleyBench A Benchmark For Video-to-Audio Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2569534174f2934f2d2a780b02a6d8ef" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Free-Form Scene Editor Enabling Multi-Round Object Manipulation like in a 3D Engine
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
