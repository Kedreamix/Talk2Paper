<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
    <meta name="description" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  SAGE Saliency-Guided Contrastive Embeddings">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-31e8e9f9af8e409efde1ee9bf67e343c')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    31 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="SAGE-Saliency-Guided-Contrastive-Embeddings"><a href="#SAGE-Saliency-Guided-Contrastive-Embeddings" class="headerlink" title="SAGE: Saliency-Guided Contrastive Embeddings"></a>SAGE: Saliency-Guided Contrastive Embeddings</h2><p><strong>Authors:Colton R. Crum, Adam Czajka</strong></p>
<p>Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the modelâ€™s latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGEâ€™s effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.</p>
<blockquote>
<p>å°†äººç±»æ„ŸçŸ¥å…ˆéªŒçŸ¥è¯†èå…¥ç¥ç»ç½‘ç»œè®­ç»ƒå·²è¢«è¯æ˜å¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå……å½“æœ‰æ•ˆçš„æ­£åˆ™åŒ–å™¨ï¼Œå¹¶ç”¨äºé«˜é£é™©é¢†åŸŸåº”ç”¨æ—¶ä¸äººç±»ä¸“ä¸šçŸ¥è¯†å¯¹é½ã€‚ç°æœ‰çš„å°†æ˜¾è‘—æ€§æ•´åˆåˆ°æ¨¡å‹è®­ç»ƒä¸­çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå†…éƒ¨æ¨¡å‹æœºåˆ¶ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜è¿™äº›æœºåˆ¶å¯èƒ½ä¸å¯é ã€‚æˆ‘ä»¬çš„è§è§£æ˜¯ï¼Œè®¸å¤šä¸æ˜¾è‘—æ€§å¼•å¯¼è®­ç»ƒç›¸å…³çš„æŒ‘æˆ˜éƒ½æºäºæŒ‡å¯¼æ–¹æ³•ä»…è®¾ç½®åœ¨å›¾åƒç©ºé—´å†…ã€‚ç›¸åï¼Œæˆ‘ä»¬è¿œç¦»å›¾åƒç©ºé—´ï¼Œä½¿ç”¨æ¨¡å‹çš„æ½œåœ¨ç©ºé—´åµŒå…¥æ¥å¼•å¯¼äººç±»æŒ‡å¯¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶æå‡ºSAGEï¼ˆæ˜¾è‘—æ€§å¼•å¯¼å¯¹æ¯”åµŒå…¥ï¼‰ï¼šä¸€ç§ä½¿ç”¨å¯¹æ¯”åµŒå…¥å°†äººç±»æ˜¾è‘—æ€§æ•´åˆåˆ°ç½‘ç»œè®­ç»ƒä¸­çš„æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬å¯¹è¾“å…¥åº”ç”¨æ˜¾è‘—ä¿ç•™å’Œæ˜¾è‘—æ€§é™ä½çš„ä¿¡å·å¢å¼ºï¼Œå¹¶æ•è·åµŒå…¥å’Œæ¨¡å‹é€»è¾‘ä¸­çš„å˜åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨å¯¹æ¯”ä¸‰å…ƒç»„æŸå¤±å¼•å¯¼æ¨¡å‹æœå‘æ˜¾è‘—ç‰¹å¾å¹¶è¿œç¦»éæ˜¾è‘—ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹é€»è¾‘åˆ†å¸ƒæ‰§è¡Œå¥å…¨æ€§æ£€æŸ¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹è¾“å‡ºä¸åŸºäºæ˜¾è‘—æ€§çš„å¢å¼ºç›¸åŒ¹é…ã€‚æˆ‘ä»¬åœ¨å¼€æ”¾å’Œå°é—­åœºæ™¯çš„æƒ…å†µä¸‹ï¼Œä¸æœ€å…ˆè¿›çš„åŸºäºæ˜¾è‘—æ€§çš„æ–¹æ³•ç›¸æ¯”ï¼Œå±•ç¤ºäº†åˆ†ç±»æ€§èƒ½çš„æå‡ï¼Œè¯æ˜äº†SAGEåœ¨ä¸åŒä¸»å¹²ç½‘ç»œä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¿›è¡Œäº†å®éªŒä»¥è¡¨æ˜å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„å¹¿æ³›æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12744v1">PDF</a> 11 pages, 2 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºå°†äººç±»æ„ŸçŸ¥å…ˆéªŒçŸ¥è¯†èå…¥ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥SAGEï¼ˆåŸºäºæ˜¾è‘—æ€§å¼•å¯¼å¯¹æ¯”åµŒå…¥ï¼‰æŸå¤±å‡½æ•°ï¼Œåœ¨æ¨¡å‹è®­ç»ƒçš„æ½œåœ¨ç©ºé—´åµŒå…¥ä¸­å¼•å¯¼äººç±»æŒ‡å¯¼ã€‚è¯¥æ–¹æ³•ä½¿ç”¨æ˜¾è‘—æ€§ä¿ç•™å’Œæ˜¾è‘—æ€§é€€åŒ–ä¿¡å·å¢å¼ºè¾“å…¥ï¼Œé€šè¿‡å¯¹æ¯”ä¸‰å…ƒæŸå¤±å‡½æ•°å¼•å¯¼æ¨¡å‹å…³æ³¨æ˜¾è‘—ç‰¹å¾å¹¶å¿½ç•¥éæ˜¾è‘—ç‰¹å¾ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼€æ”¾å’Œå°é—­åœºæ™¯ä¸‹å‡èƒ½æé«˜åˆ†ç±»æ€§èƒ½ï¼Œé€‚ç”¨äºå¤šç§ä¸»å¹²ç½‘ç»œï¼Œå¹¶å¯åœ¨ä¸åŒä»»åŠ¡ä¸­å¹¿æ³›æ¨å¹¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•´åˆäººç±»æ„ŸçŸ¥å…ˆéªŒçŸ¥è¯†å¯ä»¥æé«˜ç¥ç»ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä½œä¸ºæœ‰æ•ˆçš„æ­£åˆ™åŒ–å™¨ã€‚</li>
<li>ç°æœ‰å°†æ˜¾è‘—æ€§é›†æˆåˆ°æ¨¡å‹è®­ç»ƒä¸­çš„æ–¹æ³•å¸¸å¸¸ä¾èµ–äºå†…éƒ¨æ¨¡å‹æœºåˆ¶ï¼Œä½†æœ€æ–°ç ”ç©¶è¡¨æ˜è¿™å¯èƒ½ä¸å¯é ã€‚</li>
<li>æœ¬æ–‡æå‡ºåœ¨æ¨¡å‹è®­ç»ƒçš„æ½œåœ¨ç©ºé—´åµŒå…¥ä¸­å¼•å¯¼äººç±»æŒ‡å¯¼çš„æ–°æ–¹æ³•ã€‚</li>
<li>å¼•å…¥SAGEæŸå¤±å‡½æ•°ï¼Œé€šè¿‡å¯¹æ¯”åµŒå…¥å°†äººç±»æ˜¾è‘—æ€§èå…¥ç½‘ç»œè®­ç»ƒã€‚</li>
<li>ä½¿ç”¨æ˜¾è‘—æ€§ä¿ç•™å’Œæ˜¾è‘—æ€§é€€åŒ–ä¿¡å·å¢å¼ºè¾“å…¥ï¼Œå¹¶é€šè¿‡å¯¹æ¯”ä¸‰å…ƒæŸå¤±æ¥å¼•å¯¼æ¨¡å‹å…³æ³¨æ˜¾è‘—ç‰¹å¾ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨åˆ†ç±»æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œé€‚ç”¨äºå¤šç§ä¸»å¹²ç½‘ç»œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43b15afab3c40c32c3e042891bc6c43f" align="middle">
<img src="https://picx.zhimg.com/v2-0bab0b99c600d54f99515dd4066b5f30" align="middle">
<img src="https://picx.zhimg.com/v2-f85411a0e8180a6ed67ff222664be6bf" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="C3Net-Context-Contrast-Network-for-Camouflaged-Object-Detection"><a href="#C3Net-Context-Contrast-Network-for-Camouflaged-Object-Detection" class="headerlink" title="C3Net: Context-Contrast Network for Camouflaged Object Detection"></a>C3Net: Context-Contrast Network for Camouflaged Object Detection</h2><p><strong>Authors:Baber Jan, Aiman H. El-Maleh, Abdul Jabbar Siddiqui, Abdul Bais, Saeed Anwar</strong></p>
<p>Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at <a target="_blank" rel="noopener" href="https://github.com/Baber-Jan/C3Net">https://github.com/Baber-Jan/C3Net</a>.</p>
<blockquote>
<p>ä¼ªè£…ç›®æ ‡æ£€æµ‹æ˜¯æŒ‡è¯†åˆ«é‚£äº›é€šè¿‡ç›¸ä¼¼é¢œè‰²ã€çº¹ç†å’Œå›¾æ¡ˆæ— ç¼èå…¥å…¶å‘¨å›´ç¯å¢ƒçš„ç›®æ ‡ã€‚è¿™ä¸€ä»»åŠ¡æ—¢æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„åˆ†å‰²æ–¹æ³•ï¼Œä¹ŸæŒ‘æˆ˜äº†ç°ä»£çš„åŸºç¡€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ä¼ªè£…ç›®æ ‡ä¸Šçš„è¡¨ç°æ€¥å‰§ä¸‹é™ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¼ªè£…ç›®æ ‡æ£€æµ‹ä¸­çš„å…­ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šå†…åœ¨ç›¸ä¼¼æ€§ã€è¾¹ç¼˜ç ´åã€æç«¯å°ºåº¦å˜åŒ–ã€ç¯å¢ƒå¤æ‚æ€§ã€ä¸Šä¸‹æ–‡ä¾èµ–å’Œæ˜¾è‘—ä¼ªè£…ç›®æ ‡æ¶ˆæ­§ã€‚è¿™äº›æŒ‘æˆ˜ç»å¸¸å…±åŒå‘ç”Ÿå¹¶å¢åŠ äº†æ£€æµ‹çš„éš¾åº¦ï¼Œéœ€è¦å…¨é¢çš„æ¶æ„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†C3Netï¼Œå®ƒé€šè¿‡ä¸“é—¨çš„åŒè·¯å¾„è§£ç å™¨æ¶æ„æ¥è§£å†³æ‰€æœ‰æŒ‘æˆ˜ã€‚è¾¹ç¼˜ç»†åŒ–è·¯å¾„é‡‡ç”¨æ¢¯åº¦åˆå§‹åŒ–çš„è¾¹ç¼˜å¢å¼ºæ¨¡å—æ¥ä»æ—©æœŸç‰¹å¾ä¸­æ¢å¤ç²¾ç¡®è¾¹ç•Œã€‚ä¸Šä¸‹æ–‡å®šä½è·¯å¾„åˆ©ç”¨æˆ‘ä»¬åŸºäºå›¾åƒçš„æ–°å‹ä¸Šä¸‹æ–‡æŒ‡å¯¼æœºåˆ¶ï¼Œå®ç°å†…åœ¨æ˜¾è‘—æ€§æŠ‘åˆ¶ï¼Œæ— éœ€å¤–éƒ¨æ¨¡å‹ã€‚æ³¨æ„åŠ›èåˆæ¨¡å—ååŒç»“åˆäº†è¿™ä¸¤ä¸ªè·¯å¾„ï¼Œé€šè¿‡ç©ºé—´é—¨æ§å®ç°ã€‚C3Netåœ¨COD10Kä¸Šè¾¾åˆ°äº†0.898çš„S-measureæŒ‡æ ‡ï¼Œåœ¨CAMOä¸Šè¾¾åˆ°äº†0.904ï¼Œåœ¨NC4Kä¸Šè¾¾åˆ°äº†0.913ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆçš„å¤„ç†é€Ÿåº¦ã€‚C3Netè¯æ˜äº†å¤æ‚çš„ã€å¤šæ–¹é¢çš„æ£€æµ‹æŒ‘æˆ˜éœ€è¦æ¶æ„åˆ›æ–°ï¼Œé€šè¿‡ååŒå·¥ä½œçš„ä¸“ä¸šç»„ä»¶å®ç°å…¨é¢çš„è¦†ç›–ï¼Œè¶…è¶Šå±€éƒ¨æ”¹è¿›ã€‚ç›¸å…³ä»£ç ã€æ¨¡å‹æƒé‡å’Œç»“æœå¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Baber-Jan/C3Net%E3%80%82">https://github.com/Baber-Jan/C3Netã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12627v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰æ‰€é¢ä¸´çš„å…­å¤§æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹è¿™äº›æŒ‘æˆ˜çš„C3Netæ¨¡å‹ã€‚C3Neté€šè¿‡ä¸“é—¨çš„åŒè·¯å¾„è§£ç å™¨æ¶æ„è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¼ªè£…ç›®æ ‡æ£€æµ‹ã€‚è¯¥æ¨¡å‹åœ¨ä¸‰å¤§æ•°æ®é›†ä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œå¹¶å±•ç¤ºäº†å…¶é«˜æ•ˆå¤„ç†å¤æ‚æ£€æµ‹æŒ‘æˆ˜çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ªè£…ç›®æ ‡æ£€æµ‹é¢ä¸´å…­å¤§æŒ‘æˆ˜ï¼šå†…åœ¨ç›¸ä¼¼æ€§ã€è¾¹ç¼˜ç ´åã€æç«¯å°ºåº¦å˜åŒ–ã€ç¯å¢ƒå¤æ‚æ€§ã€ä¸Šä¸‹æ–‡ä¾èµ–æ€§å’Œæ˜¾è‘—ä¼ªè£…ç›®æ ‡æ­§ä¹‰ã€‚</li>
<li>C3Netæ¨¡å‹é€šè¿‡åŒè·¯å¾„è§£ç å™¨æ¶æ„è§£å†³äº†è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>Edge Refinement Pathwayåˆ©ç”¨æ¢¯åº¦åˆå§‹åŒ–çš„è¾¹ç¼˜å¢å¼ºæ¨¡å—ä»æ—©æœŸç‰¹å¾ä¸­æ¢å¤ç²¾ç¡®è¾¹ç•Œã€‚</li>
<li>Contextual Localization Pathwayä½¿ç”¨åŸºäºå›¾åƒä¸Šä¸‹æ–‡æŒ‡å¯¼æœºåˆ¶å®ç°å†…åœ¨æ˜¾è‘—æ€§æŠ‘åˆ¶ã€‚</li>
<li>Attentive Fusion ModuleååŒç»“åˆäº†è¿™ä¸¤ä¸ªè·¯å¾„é€šè¿‡ç©ºé—´é—¨æ§æœºåˆ¶ã€‚</li>
<li>C3Netåœ¨ä¸‰å¤§æ•°æ®é›†ä¸Šå®ç°äº†é¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬COD10Kã€CAMOå’ŒNC4Kçš„S-measureåˆ†åˆ«ä¸º0.898ã€0.904å’Œ0.913ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-933b0c2cce59ebc4fe117ecb0d82aeaf" align="middle">
<img src="https://picx.zhimg.com/v2-4aea9eb45c0f1ee47ad8dfd9a5d28184" align="middle">
<img src="https://picx.zhimg.com/v2-8cdf571537b9ed1d587e1d8c61e9b6bf" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SEMC-Structure-Enhanced-Mixture-of-Experts-Contrastive-Learning-for-Ultrasound-Standard-Plane-Recognition"><a href="#SEMC-Structure-Enhanced-Mixture-of-Experts-Contrastive-Learning-for-Ultrasound-Standard-Plane-Recognition" class="headerlink" title="SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition"></a>SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition</h2><p><strong>Authors:Qing Cai, Guihao Yan, Fan Zhang, Cheng Zhang, Zhi Liu</strong></p>
<p>Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the modelâ€™s ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.</p>
<blockquote>
<p>è¶…å£°æ ‡å‡†å¹³é¢è¯†åˆ«åœ¨ä¸´åºŠä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œå¦‚ç–¾ç—…ç­›æŸ¥ã€å™¨å®˜è¯„ä¼°å’Œç”Ÿç‰©æµ‹é‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æµ…å±‚ç»“æ„ä¿¡æ¯ï¼Œå¹¶ä¸”åœ¨é€šè¿‡å›¾åƒå¢å¼ºç”Ÿæˆå¯¹æ¯”æ ·æœ¬æ—¶ï¼Œéš¾ä»¥æ•æ‰ç»†å¾®çš„è¯­ä¹‰å·®å¼‚ï¼Œæœ€ç»ˆå¯¼è‡´è¶…å£°æ ‡å‡†å¹³é¢çš„ç»“æ„å’Œåˆ¤åˆ«ç»†èŠ‚è¯†åˆ«ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SEMCï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆç»“æ„æ„ŸçŸ¥ç‰¹å¾èåˆå’Œä¸“å®¶æŒ‡å¯¼å¯¹æ¯”å­¦ä¹ çš„æ–°å‹ç»“æ„å¢å¼ºæ··åˆä¸“å®¶å¯¹æ¯”å­¦ä¹ æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯­ä¹‰ç»“æ„èåˆæ¨¡å—ï¼ˆSSFMï¼‰ï¼Œä»¥åˆ©ç”¨å¤šå°ºåº¦ç»“æ„ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æœ‰æ•ˆå¯¹é½æµ…å±‚å’Œæ·±å±‚ç‰¹å¾ï¼Œæé«˜æ¨¡å‹æ„ŸçŸ¥ç»†å¾®ç»“æ„ç»†èŠ‚çš„èƒ½åŠ›ã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„ä¸“å®¶æ··åˆå¯¹æ¯”è¯†åˆ«æ¨¡å—ï¼ˆMCRMï¼‰ï¼Œé€šè¿‡ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æœºåˆ¶åœ¨å¤šå±‚æ¬¡ç‰¹å¾ä¸Šæ‰§è¡Œåˆ†å±‚å¯¹æ¯”å­¦ä¹ å’Œåˆ†ç±»ï¼Œè¿›ä¸€æ­¥æé«˜ç±»é—´å¯åˆ†æ€§å’Œè¯†åˆ«æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¿˜ç²¾å¿ƒåˆ¶ä½œäº†ä¸€ä¸ªåŒ…å«å…­ä¸ªæ ‡å‡†å¹³é¢çš„å¤§è§„æ¨¡è‚è„è¶…å£°æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œäº†ç»†è‡´çš„æ ‡æ³¨ã€‚åœ¨æˆ‘ä»¬è‡ªåˆ¶æ•°æ®é›†å’Œä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒSEMCåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°çš„å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12559v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSEMCçš„æ–°å‹ç»“æ„å¢å¼ºæ··åˆä¸“å®¶å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè¶…å£°æ ‡å‡†å¹³é¢è¯†åˆ«ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç»“æ„æ„ŸçŸ¥ç‰¹å¾èåˆå’Œä¸“å®¶æŒ‡å¯¼å¯¹æ¯”å­¦ä¹ ï¼Œé€šè¿‡è¯­ä¹‰ç»“æ„èåˆæ¨¡å—å’Œå¤šå±‚æ¬¡å¯¹æ¯”è¯†åˆ«æ¨¡å—ï¼Œæé«˜æ¨¡å‹å¯¹ç²¾ç»†ç»“æ„ä¿¡æ¯çš„æ„ŸçŸ¥èƒ½åŠ›å’Œç±»åˆ«å¯åˆ†æ€§ï¼Œè¿›è€Œæå‡è¶…å£°æ ‡å‡†å¹³é¢çš„è¯†åˆ«æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°æ ‡å‡†å¹³é¢è¯†åˆ«åœ¨ä¸´åºŠä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œå¦‚ç–¾ç—…ç­›æŸ¥ã€å™¨å®˜è¯„ä¼°å’Œç”Ÿç‰©æµ‹é‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æµ…å±‚ç»“æ„ä¿¡æ¯ï¼Œå¹¶ä¸”åœ¨é€šè¿‡å›¾åƒå¢å¼ºç”Ÿæˆå¯¹æ¯”æ ·æœ¬æ—¶ï¼Œéš¾ä»¥æ•æ‰ç»†å¾®çš„è¯­ä¹‰å·®å¼‚ã€‚</li>
<li>SEMCæ¡†æ¶ç»“åˆäº†ç»“æ„æ„ŸçŸ¥ç‰¹å¾èåˆå’Œä¸“å®¶æŒ‡å¯¼å¯¹æ¯”å­¦ä¹ ï¼Œä»¥æ”¹å–„è¶…å£°æ ‡å‡†å¹³é¢çš„è¯†åˆ«ã€‚</li>
<li>è¯­ä¹‰ç»“æ„èåˆæ¨¡å—ï¼ˆSSFMï¼‰ç”¨äºæŒ–æ˜å¤šå°ºåº¦ç»“æ„ä¿¡æ¯ï¼Œå¹¶å¢å¼ºæ¨¡å‹å¯¹ç»†å¾®ç»“æ„ä¿¡æ¯çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>æ··åˆä¸“å®¶å¯¹æ¯”è¯†åˆ«æ¨¡å—ï¼ˆMCRMï¼‰æ‰§è¡Œåˆ†å±‚å¯¹æ¯”å­¦ä¹ å’Œåˆ†ç±»ï¼Œä½¿ç”¨æ··åˆä¸“å®¶æœºåˆ¶æé«˜ç±»åˆ«å¯åˆ†æ€§å’Œè¯†åˆ«æ€§èƒ½ã€‚</li>
<li>ç ”å‘äº†ä¸€ä¸ªå¤§å‹ä¸”ç²¾å¿ƒæ ‡æ³¨çš„è‚è„è¶…å£°æ•°æ®é›†ï¼ŒåŒ…å«å…­ä¸ªæ ‡å‡†å¹³é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12559">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-100650bd7296cf0b6e27b7a50f0a5fb5" align="middle">
<img src="https://picx.zhimg.com/v2-b408428ad57dc06c57e01a9fa399ea99" align="middle">
<img src="https://picx.zhimg.com/v2-6d3cfee6036091521956609eaa4100e0" align="middle">
<img src="https://picx.zhimg.com/v2-3efe36678c5d4d29240b07b2744920ec" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FaNe-Towards-Fine-Grained-Cross-Modal-Contrast-with-False-Negative-Reduction-and-Text-Conditioned-Sparse-Attention"><a href="#FaNe-Towards-Fine-Grained-Cross-Modal-Contrast-with-False-Negative-Reduction-and-Text-Conditioned-Sparse-Attention" class="headerlink" title="FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention"></a>FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention</h2><p><strong>Authors:Peng Zhang, Zhihui Lai, Wenting Chen, Xu Wu, Heng Kong</strong></p>
<p>Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.</p>
<blockquote>
<p>åŒ»å­¦è§†è§‰-è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰é€šè¿‡åˆ©ç”¨é…å¯¹å›¾åƒ-æŠ¥å‘Šæ•°æ®ï¼Œåœ¨æ¨è¿›åŒ»å­¦å›¾åƒç†è§£æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å—åˆ°è¯­ä¹‰ç›¸ä¼¼æ–‡æœ¬äº§ç”Ÿçš„å‡é˜´æ€§ï¼ˆFaNeï¼‰å’Œä¸è¶³çš„ç»†ç²’åº¦è·¨æ¨¡æ€å¯¹é½çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FaNeï¼Œä¸€ç§è¯­ä¹‰å¢å¼ºçš„VLPæ¡†æ¶ã€‚ä¸ºäº†å‡è½»å‡é˜´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬-æ–‡æœ¬ç›¸ä¼¼æ€§çš„è¯­ä¹‰æ„ŸçŸ¥æ­£å‘é…å¯¹æŒ–æ˜ç­–ç•¥ï¼Œå¹¶è¿›è¡Œäº†è‡ªé€‚åº”å½’ä¸€åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–‡æœ¬è°ƒèŠ‚çš„ç¨€ç–æ³¨æ„åŠ›æ± æ¨¡å—ï¼Œé€šè¿‡æ–‡æœ¬çº¿ç´¢å¼•å¯¼çš„å±€éƒ¨è§†è§‰è¡¨ç¤ºï¼Œå®ç°ç»†ç²’åº¦çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚ä¸ºäº†åŠ å¼ºæ¨¡æ€å†…åˆ¤åˆ«åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç¡¬è´Ÿæ ·æœ¬æ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼Œè¯¥æŸå¤±å¯ä»¥è‡ªé€‚åº”åœ°é‡æ–°åŠ æƒè¯­ä¹‰ä¸Šç›¸ä¼¼çš„è´Ÿæ ·æœ¬ã€‚åœ¨äº”ä¸ªä¸‹æ¸¸åŒ»å­¦å½±åƒåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFaNeåœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12215v1">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>åŒ»ç–—è§†è§‰-è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰é€šè¿‡åˆ©ç”¨é…å¯¹å›¾åƒæŠ¥å‘Šæ•°æ®ï¼Œä¸ºæ¨è¿›åŒ»ç–—å›¾åƒç†è§£æä¾›äº†æ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å—åˆ°è¯­ä¹‰ç›¸ä¼¼æ–‡æœ¬å¼•èµ·çš„å‡é˜´æ€§ï¼ˆFaNeï¼‰çš„é™åˆ¶ï¼Œä»¥åŠç¼ºä¹ç²¾ç»†çš„è·¨æ¨¡æ€å¯¹é½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FaNeè¯­ä¹‰å¢å¼ºVLPæ¡†æ¶ã€‚é€šè¿‡åŸºäºæ–‡æœ¬-æ–‡æœ¬ç›¸ä¼¼æ€§çš„è¯­ä¹‰æ„ŸçŸ¥æ­£å‘é…å¯¹æŒ–æ˜ç­–ç•¥æ¥ç¼“è§£å‡é˜´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–‡æœ¬æ¡ä»¶ç¨€ç–æ³¨æ„åŠ›æ± æ¨¡å—ï¼Œé€šè¿‡æ–‡æœ¬å¼•å¯¼çš„è§†è§‰è¡¨ç¤ºå®ç°ç²¾ç»†çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚ä¸ºäº†å¢å¼ºæ¨¡æ€å†…åˆ¤åˆ«åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç¡¬è´Ÿæ ·æœ¬æ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼Œè‡ªé€‚åº”åœ°é‡æ–°åŠ æƒè¯­ä¹‰ç›¸ä¼¼çš„è´Ÿæ ·æœ¬ã€‚åœ¨äº”ä¸ªä¸‹æ¸¸åŒ»å­¦å½±åƒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFaNeåœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—è§†è§‰-è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æœ‰æ½œåŠ›é€šè¿‡åˆ©ç”¨é…å¯¹å›¾åƒæŠ¥å‘Šæ•°æ®æå‡åŒ»ç–—å›¾åƒç†è§£ã€‚</li>
<li>ç°æœ‰VLPæ–¹æ³•é¢ä¸´å‡é˜´æ€§ï¼ˆFaNeï¼‰é—®é¢˜å’Œç²¾ç»†è·¨æ¨¡æ€å¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºFaNeè¯­ä¹‰å¢å¼ºVLPæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰æ„ŸçŸ¥æ­£å‘é…å¯¹æŒ–æ˜ç­–ç•¥ç¼“è§£å‡é˜´æ€§ã€‚</li>
<li>è®¾è®¡æ–‡æœ¬æ¡ä»¶ç¨€ç–æ³¨æ„åŠ›æ± æ¨¡å—ï¼Œå®ç°å›¾åƒå’Œæ–‡æœ¬çš„ç²¾ç»†å¯¹é½ã€‚</li>
<li>å¼•å…¥ç¡¬è´Ÿæ ·æœ¬æ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼Œè‡ªé€‚åº”é‡æ–°åŠ æƒè¯­ä¹‰ç›¸ä¼¼çš„è´Ÿæ ·æœ¬ï¼Œå¢å¼ºæ¨¡æ€å†…åˆ¤åˆ«åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªä¸‹æ¸¸åŒ»å­¦å½±åƒåŸºå‡†æµ‹è¯•ä¸Šå–å¾—å…ˆè¿›æ€§èƒ½ï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2be0c57a23c318e9f9427fed38f558d0" align="middle">
<img src="https://picx.zhimg.com/v2-169bbb83596a7ff2bc9c56fe52c621ff" align="middle">
<img src="https://picx.zhimg.com/v2-53b02866d5f929539b9376565b710c94" align="middle">
<img src="https://picx.zhimg.com/v2-6b6607777213acef43037fd8fc7f9f32" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Comparative-Study-of-UNet-based-Architectures-for-Liver-Tumor-Segmentation-in-Multi-Phase-Contrast-Enhanced-Computed-Tomography"><a href="#Comparative-Study-of-UNet-based-Architectures-for-Liver-Tumor-Segmentation-in-Multi-Phase-Contrast-Enhanced-Computed-Tomography" class="headerlink" title="Comparative Study of UNet-based Architectures for Liver Tumor Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography"></a>Comparative Study of UNet-based Architectures for Liver Tumor Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography</h2><p><strong>Authors:Doan-Van-Anh Ly, Thi-Thu-Hien Pham, Thanh-Hai Le</strong></p>
<p>Segmentation of liver structures in multi-phase contrast-enhanced computed tomography (CECT) plays a crucial role in computer-aided diagnosis and treatment planning for liver diseases, including tumor detection. In this study, we investigate the performance of UNet-based architectures for liver tumor segmentation, starting from the original UNet and extending to UNet3+ with various backbone networks. We evaluate ResNet, Transformer-based, and State-space (Mamba) backbones, all initialized with pretrained weights. Surprisingly, despite the advances in modern architecture, ResNet-based models consistently outperform Transformer- and Mamba-based alternatives across multiple evaluation metrics. To further improve segmentation quality, we introduce attention mechanisms into the backbone and observe that incorporating the Convolutional Block Attention Module (CBAM) yields the best performance. ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a Dice score of 0.755 and IoU of 0.662, but also achieved the most precise boundary delineation, evidenced by the lowest HD95 distance of 77.911. The modelâ€™s superiority was further cemented by its leading overall accuracy of 0.925 and specificity of 0.926, showcasing its robust capability in accurately identifying both lesion and healthy tissue. To further enhance interpretability, Grad-CAM visualizations were employed to highlight the regionâ€™s most influential predictions, providing insights into its decision-making process. These findings demonstrate that classical ResNet architecture, when combined with modern attention modules, remain highly competitive for medical image segmentation tasks, offering a promising direction for liver tumor detection in clinical practice.</p>
<blockquote>
<p>åœ¨å¤šæœŸç›¸å¢å¼ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCECTï¼‰ä¸­ï¼Œè‚è„ç»“æ„çš„åˆ†å‰²å¯¹äºè‚è„ç–¾ç—…çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ï¼ŒåŒ…æ‹¬è‚¿ç˜¤æ£€æµ‹ï¼Œèµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†åŸºäºUNetæ¶æ„çš„è‚è„è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ï¼Œä»åŸå§‹UNetæ‰©å±•åˆ°å¸¦æœ‰å„ç§éª¨å¹²ç½‘ç»œçš„UNet3+ã€‚æˆ‘ä»¬è¯„ä¼°äº†ResNetã€åŸºäºTransformerå’ŒState-spaceï¼ˆMambaï¼‰çš„éª¨å¹²ç½‘ç»œï¼Œå®ƒä»¬éƒ½ä»¥é¢„è®­ç»ƒæƒé‡è¿›è¡Œåˆå§‹åŒ–ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå°½ç®¡ç°ä»£æ¶æ„æœ‰æ‰€è¿›æ­¥ï¼Œä½†åŸºäºResNetçš„æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºäºTransformerå’ŒMambaçš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åˆ†å‰²è´¨é‡ï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥åˆ°éª¨å¹²ç½‘ç»œä¸­ï¼Œå¹¶è§‚å¯Ÿåˆ°åŠ å…¥å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰ä¼šå–å¾—æœ€ä½³æ€§èƒ½ã€‚å¸¦æœ‰CBAMæ¨¡å—çš„ResNetUNet3+ä¸ä»…ä»¥Diceå¾—åˆ†0.755å’ŒIoUå¾—åˆ†0.662çš„æœ€ä½³é‡å åº¦æŒ‡æ ‡å±•ç°å‡ºæœ€å¥½çš„æ€§èƒ½ï¼Œè€Œä¸”å®ç°äº†æœ€ç²¾ç¡®çš„è¾¹ç¼˜æç»˜ï¼Œä»¥æœ€ä½çš„HD95è·ç¦»77.911ä¸ºè¯æ˜ã€‚è¯¥æ¨¡å‹çš„æ€»ä½“å‡†ç¡®åº¦è¾¾åˆ°0.925å’Œç‰¹å¼‚æ€§è¾¾åˆ°0.926ï¼Œæ˜¾ç¤ºå‡ºå…¶å‡†ç¡®è¯†åˆ«ç—…ç¶å’Œæ­£å¸¸ç»„ç»‡çš„ç¨³å¥èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢åŠ è§£é‡Šæ€§ï¼Œé‡‡ç”¨äº†Grad-CAMå¯è§†åŒ–æ¥çªå‡ºæ˜¾ç¤ºå¯¹é¢„æµ‹æœ€å…·æœ‰å½±å“åŠ›çš„åŒºåŸŸï¼Œæä¾›äº†å¯¹å…¶å†³ç­–è¿‡ç¨‹çš„è§è§£ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç»å…¸çš„ResNetæ¶æ„ä¸ç°ä»£çš„æ³¨æ„åŠ›æ¨¡å—ç›¸ç»“åˆæ—¶ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä»å…·æœ‰é«˜åº¦çš„ç«äº‰åŠ›ï¼Œä¸ºä¸´åºŠå®è·µä¸­è‚è„è‚¿ç˜¤æ£€æµ‹æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25522v2">PDF</a> 28 pages, 9 figures</p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºUNetæ¶æ„çš„è‚è„è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ï¼Œä»åŸå§‹UNetåˆ°UNet3+çš„ä¸åŒæ¶æ„ï¼Œå¹¶é‡‡ç”¨äº†å¤šç§é¢„è®­ç»ƒçš„ä¸»å¹²ç½‘ç»œï¼Œå¦‚ResNetã€åŸºäºTransformerçš„Mambaç­‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ç°ä»£æ¶æ„æœ‰æ‰€è¿›æ­¥ï¼Œä½†ResNetæ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä»è¡¨ç°æœ€ä½³ã€‚å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶åï¼Œæ€§èƒ½è¿›ä¸€æ­¥æå‡ï¼Œå…¶ä¸­èå…¥å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰çš„ResNetUNet3+è¡¨ç°æœ€ä½³ã€‚è¯¥æ¨¡å‹ä¸ä»…å…·æœ‰æœ€ä½³çš„Diceå’ŒIoUé‡å æŒ‡æ ‡ï¼Œä¸”è¾¹ç•Œåˆ’å®šæœ€ç²¾ç¡®ï¼ŒåŒæ—¶æ•´ä½“å‡†ç¡®åº¦å’Œç‰¹å¼‚æ€§é«˜ï¼Œå±•ç¤ºäº†å…¶åœ¨è¯†åˆ«ç—…å˜å’Œå¥åº·ç»„ç»‡æ–¹é¢çš„ç¨³å¥èƒ½åŠ›ã€‚é‡‡ç”¨Grad-CAMå¯è§†åŒ–æŠ€æœ¯è¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UNet-basedæ¶æ„è¢«ç”¨äºè‚è„è‚¿ç˜¤åˆ†å‰²ç ”ç©¶ï¼ŒåŒ…æ‹¬ä½¿ç”¨å¤šç§é¢„è®­ç»ƒçš„ä¸»å¹²ç½‘ç»œã€‚</li>
<li>ResNetæ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºåŸºäºTransformerå’ŒMambaçš„æ¨¡å‹ã€‚</li>
<li>å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶åï¼Œå°¤å…¶æ˜¯èå…¥CBAMçš„æ¨¡å‹æ€§èƒ½æœ€ä½³ã€‚</li>
<li>æœ€ä½³æ¨¡å‹å…·æœ‰é«˜çš„Diceå’ŒIoUé‡å æŒ‡æ ‡ï¼Œè¾¹ç•Œåˆ’å®šç²¾ç¡®ï¼Œæ•´ä½“å‡†ç¡®åº¦å’Œç‰¹å¼‚æ€§é«˜ã€‚</li>
<li>ä½¿ç”¨Grad-CAMå¯è§†åŒ–æŠ€æœ¯å¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœå±•ç¤ºäº†ResNetæ¶æ„åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f64c46fe896de1d436b8bd8a46d6565f" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SynSeg-Feature-Synergy-for-Multi-Category-Contrastive-Learning-in-End-to-End-Open-Vocabulary-Semantic-Segmentation"><a href="#SynSeg-Feature-Synergy-for-Multi-Category-Contrastive-Learning-in-End-to-End-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="SynSeg: Feature Synergy for Multi-Category Contrastive Learning in End-to-End Open-Vocabulary Semantic Segmentation"></a>SynSeg: Feature Synergy for Multi-Category Contrastive Learning in End-to-End Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Weichen Zhang, Kebin Liu, Fan Dang, Zhui Zhu, Xikai Sun, Yunhao Liu</strong></p>
<p>Semantic segmentation in open-vocabulary scenarios presents significant challenges due to the wide range and granularity of semantic categories. Existing weakly-supervised methods often rely on category-specific supervision and ill-suited feature construction methods for contrastive learning, leading to semantic misalignment and poor performance. In this work, we propose a novel weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a new feature reconstruction framework named Feature Synergy Structure (FSS). Specifically, MCCL strategy robustly combines both intra- and inter-category alignment and separation in order to make the model learn the knowledge of correlations from different categories within the same image. Moreover, FSS reconstructs discriminative features for contrastive learning through prior fusion and semantic-activation-map enhancement, effectively avoiding the foreground bias introduced by the visual encoder. Furthermore, SynSeg is a lightweight end-to-end solution without using any mid-term output from large-scale pretrained models and capable for real-time inference. In general, SynSeg effectively improves the abilities in semantic localization and discrimination under weak supervision in an efficient manner. Extensive experiments on benchmarks demonstrate that our method outperforms state-of-the-art (SOTA) performance. Particularly, SynSeg achieves higher accuracy than SOTA baselines with a ratio from 6.9% up to 26.2%.</p>
<blockquote>
<p>åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸­è¿›è¡Œè¯­ä¹‰åˆ†å‰²é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè¯­ä¹‰ç±»åˆ«çš„èŒƒå›´å¹¿æ³›ä¸”ç²’åº¦ç²¾ç»†ã€‚ç°æœ‰çš„å¼±ç›‘ç£æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šç±»åˆ«çš„ç›‘ç£å’Œå¯¹æ¯”å­¦ä¹ çš„ç‰¹å¾æ„å»ºæ–¹æ³•ä¸å½“ï¼Œå¯¼è‡´è¯­ä¹‰ä¸å¯¹é½å’Œæ€§èƒ½ä¸ä½³ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨æœ¬å·¥ä½œä¸­æå‡ºäº†ä¸€ç§æ–°çš„å¼±ç›‘ç£æ–¹æ³•SynSegã€‚SynSegæ‰§è¡Œå¤šç±»åˆ«å¯¹æ¯”å­¦ä¹ ï¼ˆMCCLï¼‰ä½œä¸ºæ›´å¼ºçš„è®­ç»ƒä¿¡å·ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç‰¹å¾é‡å»ºæ¡†æ¶ï¼Œç§°ä¸ºç‰¹å¾ååŒç»“æ„ï¼ˆFSSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒMCCLç­–ç•¥ç¨³å¥åœ°å°†åŒä¸€å›¾åƒå†…ä¸åŒç±»åˆ«çš„å¯¹é½å’Œåˆ†ç¦»ç»“åˆèµ·æ¥ï¼Œä½¿æ¨¡å‹å­¦ä¹ åˆ°ä¸åŒç±»åˆ«ä¹‹é—´çš„å…³è”çŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒFSSé€šè¿‡å…ˆéªŒèåˆå’Œè¯­ä¹‰æ¿€æ´»å›¾å¢å¼ºï¼Œä¸ºå¯¹æ¯”å­¦ä¹ é‡å»ºåˆ¤åˆ«ç‰¹å¾ï¼Œæœ‰æ•ˆåœ°é¿å…äº†è§†è§‰ç¼–ç å™¨å¸¦æ¥çš„å‰æ™¯åè§ã€‚æ­¤å¤–ï¼ŒSynSegæ˜¯ä¸€ä¸ªè½»é‡çº§çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œä¸éœ€è¦ä½¿ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„ä¸­æœŸè¾“å‡ºï¼Œå¹¶ä¸”æ”¯æŒå®æ—¶æ¨ç†ã€‚æ€»çš„æ¥è¯´ï¼ŒSynSegåœ¨å¼±ç›‘ç£æ¡ä»¶ä¸‹æœ‰æ•ˆåœ°æé«˜äº†è¯­ä¹‰å®šä½å’Œé‰´åˆ«èƒ½åŠ›ï¼Œæ˜¯ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼ŒSynSegçš„å‡†ç¡®ç‡é«˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œæ¯”ç‡ä»6.9%æé«˜åˆ°26.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06115v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£æ–¹æ³•SynSegï¼Œç”¨äºè§£å†³å¼€æ”¾è¯æ±‡è¡¨åœºæ™¯ä¸‹çš„è¯­ä¹‰åˆ†å‰²æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šç±»åˆ«å¯¹æ¯”å­¦ä¹ ï¼ˆMCCLï¼‰å’Œç‰¹å¾ååŒç»“æ„ï¼ˆFSSï¼‰è¿›è¡Œè®­ç»ƒï¼Œæœ‰æ•ˆæé«˜æ¨¡å‹çš„è¯­ä¹‰å®šä½å’Œé‰´åˆ«èƒ½åŠ›ã€‚æ— éœ€ä½¿ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„ä¸­æœŸè¾“å‡ºï¼Œå¯å®ç°å®æ—¶æ¨ç†ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¡¨åœºæ™¯ä¸‹çš„è¯­ä¹‰åˆ†å‰²é¢ä¸´å¹¿æ³›èŒƒå›´å’Œç²’åº¦çš„è¯­ä¹‰ç±»åˆ«æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å¼±ç›‘ç£æ–¹æ³•å­˜åœ¨è¯­ä¹‰ä¸å¯¹é½å’Œæ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£æ–¹æ³•SynSegï¼ŒåŒ…æ‹¬å¤šç±»åˆ«å¯¹æ¯”å­¦ä¹ ï¼ˆMCCLï¼‰å’Œç‰¹å¾ååŒç»“æ„ï¼ˆFSSï¼‰ã€‚</li>
<li>MCCLç­–ç•¥å®ç°äº†åŒä¸€å›¾åƒå†…ä¸åŒç±»åˆ«çš„å…³è”çŸ¥è¯†å­¦ä¹ ã€‚</li>
<li>FSSé€šè¿‡å…ˆéªŒèåˆå’Œè¯­ä¹‰æ¿€æ´»å›¾å¢å¼ºï¼Œæœ‰æ•ˆé¿å…å‰æ™¯åå·®ã€‚</li>
<li>SynSegæ˜¯ç«¯åˆ°ç«¯çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆï¼Œæ— éœ€ä½¿ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸­æœŸè¾“å‡ºï¼Œé€‚åˆå®æ—¶æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31e8e9f9af8e409efde1ee9bf67e343c" align="middle">
<img src="https://picx.zhimg.com/v2-8fd47fa16432ff6e2777a27bb763a383" align="middle">
<img src="https://picx.zhimg.com/v2-fa44bde71f4cffc6bcfda66bee7afceb" align="middle">
<img src="https://picx.zhimg.com/v2-97bfca80d192bdf63d8f94e375d5beed" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MAISI-v2-Accelerated-3D-High-Resolution-Medical-Image-Synthesis-with-Rectified-Flow-and-Region-specific-Contrastive-Loss"><a href="#MAISI-v2-Accelerated-3D-High-Resolution-Medical-Image-Synthesis-with-Rectified-Flow-and-Region-specific-Contrastive-Loss" class="headerlink" title="MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss"></a>MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss</h2><p><strong>Authors:Can Zhao, Pengfei Guo, Dong Yang, Yucheng Tang, Yufan He, Benjamin Simon, Mason Belue, Stephanie Harmon, Baris Turkbey, Daguang Xu</strong></p>
<p>Medical image synthesis is an important topic for both clinical and research applications. Recently, diffusion models have become a leading approach in this area. Despite their strengths, many existing methods struggle with (1) limited generalizability that only work for specific body regions or voxel spacings, (2) slow inference, which is a common issue for diffusion models, and (3) weak alignment with input conditions, which is a critical issue for medical imaging. MAISI, a previously proposed framework, addresses generalizability issues but still suffers from slow inference and limited condition consistency. In this work, we present MAISI-v2, the first accelerated 3D medical image synthesis framework that integrates rectified flow to enable fast and high quality generation. To further enhance condition fidelity, we introduce a novel region-specific contrastive loss to enhance the sensitivity to region of interest. Our experiments show that MAISI-v2 can achieve SOTA image quality with $33 \times$ acceleration for latent diffusion model. We also conducted a downstream segmentation experiment to show that the synthetic images can be used for data augmentation. We release our code, training details, model weights, and a GUI demo to facilitate reproducibility and promote further development within the community.</p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆæˆæ˜¯ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­çš„ä¸€ä¸ªé‡è¦è¯¾é¢˜ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºè¯¥é¢†åŸŸçš„ä¸»æµæ–¹æ³•ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰ä¼˜åŠ¿ï¼Œä½†è®¸å¤šç°æœ‰æ–¹æ³•ä»é¢ä¸´ï¼ˆ1ï¼‰é€šç”¨æ€§æœ‰é™ï¼Œä»…é€‚ç”¨äºç‰¹å®šéƒ¨ä½æˆ–ä½“ç´ é—´è·ï¼›ï¼ˆ2ï¼‰æ¨ç†é€Ÿåº¦æ…¢ï¼Œè¿™æ˜¯æ‰©æ•£æ¨¡å‹çš„å¸¸è§é—®é¢˜ï¼›ï¼ˆ3ï¼‰ä¸è¾“å…¥æ¡ä»¶çš„å¯¹é½è¾ƒå¼±ï¼Œè¿™å¯¹åŒ»å­¦å½±åƒæ¥è¯´æ˜¯å…³é”®é—®é¢˜ã€‚ä¹‹å‰æå‡ºçš„MAISIæ¡†æ¶è§£å†³äº†é€šç”¨æ€§é—®é¢˜ï¼Œä½†ä»å­˜åœ¨æ¨ç†é€Ÿåº¦æ…¢å’Œæ¡ä»¶ä¸€è‡´æ€§æœ‰é™çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MAISI-v2ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŠ é€Ÿçš„3DåŒ»å­¦å›¾åƒåˆæˆæ¡†æ¶ï¼Œå®ƒæ•´åˆäº†æ ¡æ­£æµï¼Œä»¥å®ç°å¿«é€Ÿå’Œé«˜è´¨é‡çš„ç”Ÿæˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¡ä»¶ä¿çœŸåº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŒºåŸŸç‰¹å¼‚æ€§å¯¹æ¯”æŸå¤±ï¼Œä»¥æé«˜å¯¹æ„Ÿå…´è¶£åŒºåŸŸçš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMAISI-v2å¯ä»¥åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸Šå®ç°$33 \times$çš„åŠ é€Ÿï¼ŒåŒæ—¶è¾¾åˆ°æœ€å…ˆè¿›çš„å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸‹æ¸¸åˆ†å‰²å®éªŒï¼Œä»¥è¯æ˜åˆæˆå›¾åƒå¯ç”¨äºæ•°æ®å¢å¼ºã€‚æˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€è®­ç»ƒç»†èŠ‚ã€æ¨¡å‹æƒé‡å’ŒGUIæ¼”ç¤ºï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå†…çš„å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05772v2">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong><br>     åŒ»ç–—å›¾åƒåˆæˆæ˜¯ä¸´åºŠå’Œç ”ç©¶åº”ç”¨çš„é‡è¦è¯¾é¢˜ï¼Œè¿‘æœŸæ‰©æ•£æ¨¡å‹æˆä¸ºè¯¥é¢†åŸŸçš„ä¸»æµæ–¹æ³•ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨é€šç”¨æ€§æœ‰é™ã€æ¨ç†é€Ÿåº¦æ…¢ä»¥åŠä¸è¾“å…¥æ¡ä»¶å¯¹é½ä¸ä½³ç­‰é—®é¢˜ã€‚MAISI-v2æ˜¯é¦–ä¸ªåŠ é€Ÿçš„3DåŒ»ç–—å›¾åƒåˆæˆæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆçŸ«æ­£æµå®ç°å¿«é€Ÿã€é«˜è´¨é‡ç”Ÿæˆã€‚ä¸ºæé«˜æ¡ä»¶ä¿çœŸåº¦ï¼Œå¼•å…¥åŒºåŸŸç‰¹å®šå¯¹æ¯”æŸå¤±ä»¥å¢å¼ºå¯¹æ„Ÿå…´è¶£åŒºåŸŸçš„æ•æ„Ÿæ€§ã€‚å®éªŒæ˜¾ç¤ºï¼ŒMAISI-v2å¯åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸Šå®ç°$33 \times$åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼Œè¿›è¡Œä¸‹æ¸¸åˆ†å‰²å®éªŒè¯æ˜åˆæˆå›¾åƒå¯ç”¨äºæ•°æ®å¢å¼ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆæˆé¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨é€šç”¨æ€§ã€æ¨ç†é€Ÿåº¦ä»¥åŠä¸è¾“å…¥æ¡ä»¶å¯¹é½çš„é—®é¢˜ã€‚</li>
<li>MAISI-v2æ¡†æ¶é€šè¿‡æ•´åˆçŸ«æ­£æµå®ç°å¿«é€Ÿã€é«˜è´¨é‡åŒ»ç–—å›¾åƒç”Ÿæˆã€‚</li>
<li>å¼•å…¥åŒºåŸŸç‰¹å®šå¯¹æ¯”æŸå¤±æé«˜æ¡ä»¶ä¿çœŸåº¦å’Œå¯¹æ„Ÿå…´è¶£åŒºåŸŸçš„æ•æ„Ÿæ€§ã€‚</li>
<li>MAISI-v2åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸Šå®ç°æ˜¾è‘—åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚</li>
<li>åˆæˆå›¾åƒå¯ç”¨äºæ•°æ®å¢å¼ºï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡å¦‚åˆ†å‰²æä¾›æœ‰æ•ˆæ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86530cb42bb998ab7024c4493a7327b5" align="middle">
<img src="https://picx.zhimg.com/v2-bceeb8c7cb53b53392ffa55dda043101" align="middle">
<img src="https://picx.zhimg.com/v2-7e1ab2711b849b75185c25fa9a53526b" align="middle">
<img src="https://picx.zhimg.com/v2-f75d0caf9e3fafcf3fe9c528b14b7c46" align="middle">
<img src="https://picx.zhimg.com/v2-e49072fac54a23ddcd41668d1b3bded5" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AstroECP-towards-more-practical-Electron-Channeling-Contrast-Imaging"><a href="#AstroECP-towards-more-practical-Electron-Channeling-Contrast-Imaging" class="headerlink" title="AstroECP: towards more practical Electron Channeling Contrast Imaging"></a>AstroECP: towards more practical Electron Channeling Contrast Imaging</h2><p><strong>Authors:M. Haroon Qaiser, Lukas Berners, Robin J. Scales, Tianbi Zhang, Martin Heller, Jiri Dluhos, Sandra Korte-Kerzel, T. Ben Britton</strong></p>
<p>Electron channeling contrast imaging (ECCI) is a scanning electron microscopy (SEM) based technique that enables bulk-sample characterization of crystallographic defects (e.g. dislocations, stacking faults, low angle boundaries). Despite its potential, ECCI remains underused for quantitative defect analysis as compared to transmission electron microscope (TEM) based methods. Here, we overcome barriers that limit the use of ECCI including optimizing signal-to-noise contrast, precise determination of the incident beam vector with calibrated and easy to use simulations and experimental selected area electron channeling patterns (SA-ECP). We introduce a systematic ECCI workflow, alongside a new open-source software tool (AstroECP), that includes calibration of stage tilting, SA-ECP field of view, and the energy that forms the ECP&#x2F;ECCI contrast using dynamical simulations. The functionality of this workflow is demonstrated with case studies that include threading dislocations in GaAs and the cross validation of precession based ECCI-contrast, which is otherwise known as Electron Channeling Orientation Determination (eCHORD). To assist the reader, we also provide best practice guidelines for ECCI implementation to promote high-resolution defect imaging in the SEM.</p>
<blockquote>
<p>ç”µå­é€šé“è¡¬åº¦æˆåƒï¼ˆECCIï¼‰æ˜¯ä¸€ç§åŸºäºæ‰«æç”µå­æ˜¾å¾®é•œï¼ˆSEMï¼‰çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿå¯¹æ™¶ä½“ç¼ºé™·ï¼ˆä¾‹å¦‚ä½é”™ã€å±‚é”™ã€å°è§’åº¦è¾¹ç•Œï¼‰è¿›è¡Œæ•´ä½“æ ·å“è¡¨å¾ã€‚å°½ç®¡å®ƒå…·æœ‰æ½œåŠ›ï¼Œä½†ä¸é€å°„ç”µå­æ˜¾å¾®é•œï¼ˆTEMï¼‰æ–¹æ³•ç›¸æ¯”ï¼ŒECCIåœ¨å®šé‡ç¼ºé™·åˆ†ææ–¹é¢çš„åº”ç”¨ä»ç„¶è¾ƒå°‘ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å…‹æœäº†é™åˆ¶ECCIä½¿ç”¨çš„éšœç¢ï¼ŒåŒ…æ‹¬ä¼˜åŒ–ä¿¡å™ªæ¯”è¡¬åº¦ã€ä½¿ç”¨æ ¡å‡†çš„ä¸”æ˜“äºä½¿ç”¨çš„æ¨¡æ‹Ÿä»¥åŠå®éªŒé€‰åŒºç”µå­é€šé“æ¨¡å¼ï¼ˆSA-ECPï¼‰ç²¾ç¡®ç¡®å®šå…¥å°„å…‰æŸçŸ¢é‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç³»ç»Ÿçš„ECCIå·¥ä½œæµç¨‹ï¼Œä»¥åŠä¸€ä¸ªæ–°çš„å¼€æºè½¯ä»¶å·¥å…·ï¼ˆAstroECPï¼‰ï¼ŒåŒ…æ‹¬æ ¡å‡†å€¾æ–œé˜¶æ®µã€SA-ECPè§†é‡ä»¥åŠå½¢æˆECP&#x2F;ECCIè¡¬åº¦çš„èƒ½é‡åŠ¨æ€æ¨¡æ‹Ÿã€‚è¯¥åŠŸèƒ½é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å¾—åˆ°è¯æ˜ï¼ŒåŒ…æ‹¬GaAsä¸­çš„ç©¿çº¿ä½é”™ä»¥åŠåŸºäºç²¾åº¦çš„ECCIè¡¬åº¦ï¼ˆä¹Ÿç§°ä¸ºç”µå­é€šé“å–å‘æµ‹å®šæ³•ï¼ˆeCHORDï¼‰ï¼‰çš„äº¤å‰éªŒè¯ã€‚ä¸ºäº†å¸®åŠ©è¯»è€…ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ECCIå®æ–½çš„æœ€ä½³å®è·µæŒ‡å—ï¼Œä»¥ä¿ƒè¿›SEMä¸­çš„é«˜åˆ†è¾¨ç‡ç¼ºé™·æˆåƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00354v3">PDF</a> as submitted version, post-peer review</p>
<p><strong>Summary</strong></p>
<p>ECCIæŠ€æœ¯æ˜¯ä¸€ç§åŸºäºæ‰«æç”µå­æ˜¾å¾®é•œï¼ˆSEMï¼‰çš„æŠ€æœ¯ï¼Œç”¨äºè¡¨å¾æ™¶ä½“ç¼ºé™·ï¼Œå¦‚ä½é”™ã€å±‚é”™å’Œä½è§’åº¦è¾¹ç•Œã€‚æœ¬æ–‡ä¼˜åŒ–ECCTçš„ä¿¡å·å™ªå£°å¯¹æ¯”ï¼Œç¡®å®šå…¥å°„å…‰æŸçŸ¢é‡ï¼Œå¹¶å¼•å…¥ç³»ç»Ÿçš„å·¥ä½œæµç¨‹å’Œå¼€æºè½¯ä»¶å·¥å…·AstroECPï¼Œä»¥æ ¡å‡†é˜¶æ®µå€¾æ–œã€SA-ECPè§†åœºå’Œå½¢æˆECP&#x2F;ECCTå¯¹æ¯”åº¦çš„èƒ½é‡è¿›è¡ŒåŠ¨åŠ›å­¦æ¨¡æ‹Ÿã€‚æ­¤å¤–ï¼Œè¿˜é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†å…¶åŠŸèƒ½æ€§ï¼ŒåŒ…æ‹¬GaAsä¸­çš„ç©¿çº¿ä½é”™å’ŒåŸºäºç²¾ç¡®åº¦çš„ECCTå¯¹æ¯”åº¦çš„äº¤å‰éªŒè¯ï¼Œä¹Ÿç§°ä¸ºç”µå­é€šé“å–å‘æµ‹å®šï¼ˆeCHORDï¼‰ã€‚æœ¬æ–‡è¿˜æä¾›ECCIå®æ–½çš„æœ€ä½³å®è·µæŒ‡å—ï¼Œä»¥ä¿ƒè¿›SEMä¸­çš„é«˜åˆ†è¾¨ç‡ç¼ºé™·æˆåƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ECCIæ˜¯ä¸€ç§åŸºäºSEMçš„æŠ€æœ¯ï¼Œç”¨äºè¡¨å¾æ™¶ä½“ç¼ºé™·ã€‚</li>
<li>ECCIåœ¨å®šé‡ç¼ºé™·åˆ†ææ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ä¿¡å·å™ªå£°å¯¹æ¯”å’Œç¡®å®šå…¥å°„å…‰æŸçŸ¢é‡æ¥å…‹æœECCIçš„å±€é™æ€§ã€‚</li>
<li>å¼•å…¥ç³»ç»Ÿçš„å·¥ä½œæµç¨‹å’Œå¼€æºè½¯ä»¶å·¥å…·AstroECPï¼Œç”¨äºæ ¡å‡†é˜¶æ®µå€¾æ–œã€SA-ECPè§†åœºå’Œå¯¹æ¯”åº¦çš„å½¢æˆèƒ½é‡ã€‚</li>
<li>å±•ç¤ºäº†ECCIåœ¨æ¡ˆä¾‹ç ”ç©¶ä¸­çš„åŠŸèƒ½æ€§ï¼ŒåŒ…æ‹¬GaAsä¸­çš„ç©¿çº¿ä½é”™å’ŒåŸºäºç²¾ç¡®åº¦çš„ECCTå¯¹æ¯”åº¦çš„äº¤å‰éªŒè¯ã€‚</li>
<li>æä¾›ECCIå®æ–½çš„æœ€ä½³å®è·µæŒ‡å—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00354">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5525d9b7badd0ae89e5312bffaf0ce68" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/Speech/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-113836888b84a1075df6d3a592a99b8e" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  PASE Leveraging the Phonological Prior of WavLM for Low-Hallucination Generative Speech Enhancement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-36e8afa508b37d5ae96dc1d0a25e45b0" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
