<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  PASE Leveraging the Phonological Prior of WavLM for Low-Hallucination Generative Speech Enhancement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-113836888b84a1075df6d3a592a99b8e')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="PASE-Leveraging-the-Phonological-Prior-of-WavLM-for-Low-Hallucination-Generative-Speech-Enhancement"><a href="#PASE-Leveraging-the-Phonological-Prior-of-WavLM-for-Low-Hallucination-Generative-Speech-Enhancement" class="headerlink" title="PASE: Leveraging the Phonological Prior of WavLM for Low-Hallucination Generative Speech Enhancement"></a>PASE: Leveraging the Phonological Prior of WavLM for Low-Hallucination Generative Speech Enhancement</h2><p><strong>Authors:Xiaobin Rong, Qinwen Hu, Mansur Yesilbursa, Kamil Wojcicki, Jing Lu</strong></p>
<p>Generative models have shown remarkable performance in speech enhancement (SE), achieving superior perceptual quality over traditional discriminative approaches. However, existing generative SE approaches often overlook the risk of hallucination under severe noise, leading to incorrect spoken content or inconsistent speaker characteristics, which we term linguistic and acoustic hallucinations, respectively. We argue that linguistic hallucination stems from modelsâ€™ failure to constrain valid phonological structures and it is a more fundamental challenge. While language models (LMs) are well-suited for capturing the underlying speech structure through modeling the distribution of discrete tokens, existing approaches are limited in learning from noise-corrupted representations, which can lead to contaminated priors and hallucinations. To overcome these limitations, we propose the Phonologically Anchored Speech Enhancer (PASE), a generative SE framework that leverages the robust phonological prior embedded in the pre-trained WavLM model to mitigate hallucinations. First, we adapt WavLM into a denoising expert via representation distillation to clean its final-layer features. Guided by the modelâ€™s intrinsic phonological prior, this process enables robust denoising while minimizing linguistic hallucinations. To further reduce acoustic hallucinations, we train the vocoder with a dual-stream representation: the high-level phonetic representation provides clean linguistic content, while a low-level acoustic representation retains speaker identity and prosody. Experimental results demonstrate that PASE not only surpasses state-of-the-art discriminative models in perceptual quality, but also significantly outperforms prior generative models with substantially lower linguistic and acoustic hallucinations.</p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„åˆ¤åˆ«æ–¹æ³•ï¼Œå®ƒä»¬åœ¨æ„ŸçŸ¥è´¨é‡ä¸Šè¾¾åˆ°äº†æ›´é«˜çš„æ°´å¹³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”Ÿæˆå¼SEæ–¹æ³•å¾€å¾€å¿½è§†äº†ä¸¥é‡å™ªå£°ä¸‹çš„å¹»è§‰é£é™©ï¼Œè¿™ä¼šå¯¼è‡´é”™è¯¯çš„è¯­éŸ³å†…å®¹æˆ–ä¸ä¸€è‡´çš„è¯´è¯äººç‰¹å¾ï¼Œæˆ‘ä»¬åˆ†åˆ«ç§°ä¹‹ä¸ºè¯­è¨€å¹»è§‰å’Œå£°éŸ³å¹»è§‰ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¯­è¨€å¹»è§‰æºäºæ¨¡å‹å¯¹æœ‰æ•ˆè¯­éŸ³ç»“æ„çš„çº¦æŸä¸è¶³ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´ä¸ºæ ¹æœ¬çš„æŒ‘æˆ˜ã€‚è™½ç„¶è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰é€šè¿‡å»ºæ¨¡ç¦»æ•£ç¬¦å·çš„åˆ†å¸ƒæ¥æ•æ‰æ½œåœ¨çš„è¯­éŸ³ç»“æ„éå¸¸é€‚åˆï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å­¦ä¹ å™ªå£°æ±¡æŸ“çš„è¡¨ç¤ºæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè¿™å¯èƒ½å¯¼è‡´æ±¡æŸ“çš„å…ˆéªŒçŸ¥è¯†å’Œå¹»è§‰ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºéŸ³ç³»é”šå®šçš„è¯­éŸ³å¢å¼ºå™¨ï¼ˆPASEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆå¼SEæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„WavLMæ¨¡å‹ä¸­åµŒå…¥çš„ç¨³å¥éŸ³ç³»å…ˆéªŒçŸ¥è¯†æ¥å‡è½»å¹»è§‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡è¡¨ç¤ºè’¸é¦å°†WavLMé€‚åº”ä¸ºå»å™ªä¸“å®¶ï¼Œä»¥æ¸…æ´å…¶æœ€ç»ˆå±‚ç‰¹å¾ã€‚åœ¨æ¨¡å‹çš„å†…åœ¨éŸ³ç³»å…ˆéªŒçŸ¥è¯†çš„æŒ‡å¯¼ä¸‹ï¼Œè¿™ä¸ªè¿‡ç¨‹å®ç°äº†ç¨³å¥çš„å»å™ªï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘äº†è¯­è¨€å¹»è§‰ã€‚ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘å£°éŸ³å¹»è§‰ï¼Œæˆ‘ä»¬ç”¨åŒæµè¡¨ç¤ºæ³•è®­ç»ƒäº†vocoderï¼šé«˜çº§è¯­éŸ³è¡¨ç¤ºæ³•æä¾›æ¸…æ™°çš„è¯­éŸ³å†…å®¹ï¼Œè€Œä½çº§å£°éŸ³è¡¨ç¤ºæ³•åˆ™ä¿ç•™äº†è¯´è¯äººçš„èº«ä»½å’Œè¯­è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPASEä¸ä»…è¶…è¿‡äº†å…ˆè¿›åˆ¤åˆ«æ¨¡å‹çš„æ„ŸçŸ¥è´¨é‡ï¼Œè€Œä¸”åœ¨è¯­è¨€å¹»è§‰å’Œå£°éŸ³å¹»è§‰æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„ç”Ÿæˆæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13300v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸï¼Œç”Ÿæˆæ¨¡å‹ç›¸è¾ƒäºä¼ ç»Ÿåˆ¤åˆ«æ¨¡å‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†ç°æœ‰ç”Ÿæˆæ¨¡å‹å­˜åœ¨åœ¨ä¸¥é‡å™ªå£°ä¸‹å‡ºç°å¹»å¬çš„é£é™©ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè¯­è¨€æ¨¡å‹çš„å¹»å¬æºäºå…¶å¯¹æœ‰æ•ˆè¯­éŸ³ç»“æ„çš„çº¦æŸä¸è¶³ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´ä¸ºæ ¹æœ¬çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªåä¸º Phonologically Anchored Speech Enhancerï¼ˆPASEï¼‰çš„ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„WavLMæ¨¡å‹ä¸­çš„ç¨³å¥è¯­éŸ³ç»“æ„å…ˆéªŒæ¥å‡è½»å¹»å¬ç°è±¡ã€‚é€šè¿‡è¡¨ç¤ºè’¸é¦ä½¿WavLMé€‚åº”å»å™ªä¸“å®¶è§’è‰²ï¼Œä»¥å‡€åŒ–å…¶æœ€ç»ˆå±‚ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ¨¡å‹å›ºæœ‰çš„è¯­éŸ³ç»“æ„å…ˆéªŒè¿›è¡Œå¼•å¯¼ï¼Œä»¥å®ç°ç¨³å¥çš„å»å™ªå’Œå‡å°‘è¯­è¨€å¹»å¬ã€‚ä¸ºé™ä½å£°å­¦å¹»å¬ï¼Œè®­ç»ƒäº†ä¸€ä¸ªåŒæµè¡¨ç¤ºçš„vocoderï¼Œå…¶ä¸­é«˜çº§è¯­éŸ³è¡¨ç¤ºæä¾›æ¸…æ™°çš„è¯­éŸ³å†…å®¹ï¼Œè€Œä½çº§å£°å­¦è¡¨ç¤ºä¿ç•™è¯´è¯äººçš„èº«ä»½å’Œè¯­è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPASEä¸ä»…åœ¨æ„ŸçŸ¥è´¨é‡ä¸Šè¶…è¶Šäº†å…ˆè¿›çš„åˆ¤åˆ«æ¨¡å‹ï¼Œè€Œä¸”åœ¨è¯­è¨€å­¦å’Œå£°å­¦å¹»å¬æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„ç”Ÿæˆæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œç›¸è¾ƒäºä¼ ç»Ÿåˆ¤åˆ«æ¨¡å‹å…·æœ‰æ›´é«˜çš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>ç°æœ‰ç”Ÿæˆæ¨¡å‹é¢ä¸´åœ¨ä¸¥é‡å™ªå£°ä¸‹çš„å¹»å¬é£é™©ï¼Œåˆ†ä¸ºè¯­è¨€å­¦å’Œå£°å­¦ä¸¤ç§ã€‚</li>
<li>è¯­è¨€å­¦å¹»å¬æºäºæ¨¡å‹å¯¹è¯­éŸ³ç»“æ„çš„æœ‰æ•ˆçº¦æŸä¸è¶³ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºç¡€æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºPASEæ¡†æ¶åˆ©ç”¨WavLMæ¨¡å‹çš„ç¨³å¥è¯­éŸ³ç»“æ„å…ˆéªŒæ¥å‡è½»å¹»å¬ç°è±¡ã€‚</li>
<li>é€šè¿‡è¡¨ç¤ºè’¸é¦ä½¿WavLMé€‚åº”å»å™ªä¸“å®¶è§’è‰²ï¼Œä»¥å‡€åŒ–ç‰¹å¾å¹¶å‡å°‘è¯­è¨€å¹»å¬ã€‚</li>
<li>è®­ç»ƒäº†ä¸€ä¸ªåŒæµè¡¨ç¤ºçš„vocoderæ¥è¿›ä¸€æ­¥é™ä½å£°å­¦å¹»å¬ï¼ŒåŒæ—¶ä¿ç•™è¯­éŸ³å†…å®¹å’Œè¯´è¯äººçš„èº«ä»½ã€è¯­è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c3099207461775e723f7bf2848e33da" align="middle">
<img src="https://picx.zhimg.com/v2-d693221e9ecd6c2d6199d9d2f0eb68a4" align="middle">
<img src="https://picx.zhimg.com/v2-f96beff29086f708e93b276cf2d00f32" align="middle">
<img src="https://picx.zhimg.com/v2-5d03be946c93565a79041e05aec358ab" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Distinguishing-Repetition-Disfluency-from-Morphological-Reduplication-in-Bangla-ASR-Transcripts-A-Novel-Corpus-and-Benchmarking-Analysis"><a href="#Distinguishing-Repetition-Disfluency-from-Morphological-Reduplication-in-Bangla-ASR-Transcripts-A-Novel-Corpus-and-Benchmarking-Analysis" class="headerlink" title="Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis"></a>Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis</h2><p><strong>Authors:Zaara Zabeen Arpa, Sadnam Sakib Apurbo, Nazia Karim Khan Oishee, Ajwad Abrar</strong></p>
<p>Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error&#x2F;hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.</p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•å†…å®¹ï¼Œç‰¹åˆ«æ˜¯åœ¨å­ŸåŠ æ‹‰è¯­ç­‰ä½èµ„æºè¯­è¨€ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªå…³é”®çš„æ¨¡ç³Šæ€§ï¼šå•è¯é‡å¤å¯èƒ½æ˜¯é‡å¤ä¸æµç•…ï¼ˆæ— æ„çš„ASRé”™è¯¯&#x2F;çŠ¹è±«ï¼‰æˆ–å½¢æ€å¤ç°ï¼ˆæ•…æ„çš„è¯­æ³•ç»“æ„ï¼‰ã€‚æ ‡å‡†çš„æµç•…æ€§ä¿®æ­£ä¼šé”™è¯¯åœ°åˆ é™¤æœ‰æ•ˆçš„è¯­è¨€ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„ã€åŒ…å«2ä¸‡è¡Œæ•°æ®çš„å­ŸåŠ æ‹‰è¯­è¯­æ–™åº“ï¼Œè¿›è¡Œæ‰‹åŠ¨æ³¨é‡Šï¼Œä»¥æ˜ç¡®åŒºåˆ†è¿™ä¸¤ç§ç°è±¡åœ¨å˜ˆæ‚çš„ASRè½¬å½•å†…å®¹ä¸­çš„ä¸åŒã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ç§èŒƒå¼æ¥è¯„ä¼°è¿™ä¸€æ–°é¢–èµ„æºï¼šæœ€å…ˆè¿›çš„å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç¼–ç å™¨æ¨¡å‹å¾®è°ƒã€‚LLMé€šè¿‡å°‘é‡æç¤ºå³å¯å®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ˆæœ€é«˜è¾¾82.68%çš„å‡†ç¡®ç‡ï¼‰ã€‚ç„¶è€Œï¼Œå¾®è°ƒè¯æ˜æ›´ä¼˜è¶Šï¼Œé’ˆå¯¹å­ŸåŠ æ‹‰è¯­çš„ç‰¹å®šè¯­è¨€æ¨¡å‹BanglaBERTå®ç°äº†æœ€é«˜84.78%çš„å‡†ç¡®ç‡å’Œ0.677çš„F1åˆ†æ•°ã€‚è¿™ä¸ºå¼€å‘å¤æ‚ã€è¯­ä¹‰ä¿æŒçš„å­ŸåŠ æ‹‰è¯­æ–‡æœ¬å½’ä¸€åŒ–ç³»ç»Ÿæä¾›äº†å¼ºæœ‰åŠ›çš„è¯­è¨€ä¿¡æ¯åŸºå‡†å’Œå¿…è¦æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13159v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡è§£å†³äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•æœ¬ä¸­è¯é‡å¤ç°è±¡å¼•èµ·çš„æ­§ä¹‰é—®é¢˜ã€‚å¯¹äºå¦‚å­ŸåŠ æ‹‰è¯­ç­‰ä½èµ„æºè¯­è¨€ï¼ŒASRè½¬å½•æœ¬ä¸­çš„è¯é‡å¤å¯èƒ½æ˜¯é‡å¤å¤±æµï¼ˆæ— æ„ä¸­çš„ASRé”™è¯¯&#x2F;çŠ¹è±«ï¼‰æˆ–å½¢æ€å¤ç°ï¼ˆæœ‰æ„çš„è¯­æ³•ç»“æ„ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œå¼•å…¥äº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„æ‰‹åŠ¨æ³¨é‡Šçš„å­ŸåŠ æ‹‰è¯­è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“èƒ½å¤Ÿæ˜ç¡®åŒºåˆ†è¿™ä¸¤ç§ç°è±¡ã€‚é€šè¿‡é‡‡ç”¨æœ€å…ˆè¿›çš„å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹å’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç¼–ç å™¨æ¨¡å‹å¾®è°ƒä¸¤ç§æ–¹æ³•è¯„ä¼°æ­¤æ–°èµ„æºçš„æ•ˆæœï¼Œè¯æ˜äº†è¯­è¨€æ¨¡å‹çš„ä¼˜è¶Šæ€§ï¼Œè€Œå¾®è°ƒæ•ˆæœæ›´ä½³ï¼Œä½¿ç”¨ç‰¹å®šçš„å­ŸåŠ æ‹‰è¯­BERTæ¨¡å‹æœ€é«˜å‡†ç¡®ç‡è¾¾åˆ°äº†84.78%ï¼ŒF1åˆ†æ•°ä¸º0.677ã€‚è¿™ä¸ºå¼€å‘å­ŸåŠ æ‹‰è¯­çš„è¯­ä¹‰ä¿ç•™æ–‡æœ¬å½’ä¸€åŒ–ç³»ç»Ÿæä¾›äº†å¼ºæœ‰åŠ›çš„åŸºçº¿æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRè½¬å½•æœ¬ä¸­çš„è¯é‡å¤ç°è±¡å­˜åœ¨æ­§ä¹‰ï¼Œéœ€åŒºåˆ†é‡å¤å¤±æµå’Œå½¢æ€å¤ç°ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„ã€æ‰‹åŠ¨æ³¨é‡Šçš„å­ŸåŠ æ‹‰è¯­è¯­æ–™åº“ã€‚</li>
<li>é€šè¿‡ä¸¤ç§è¯„ä¼°æ–¹æ³•ï¼šæœ€å…ˆè¿›çš„å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹å’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç¼–ç å™¨æ¨¡å‹å¾®è°ƒã€‚</li>
<li>å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä½†å¾®è°ƒæ•ˆæœæ›´ä½³ã€‚</li>
<li>é‡‡ç”¨ç‰¹å®šè¯­è¨€çš„å­ŸåŠ æ‹‰BERTæ¨¡å‹è¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡84.78%ï¼Œå¹¶å®ç°äº†è¾ƒé«˜çš„F1åˆ†æ•°ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¼€å‘å­ŸåŠ æ‹‰è¯­çš„æ–‡æœ¬å½’ä¸€åŒ–ç³»ç»Ÿæä¾›äº†å¼ºæœ‰åŠ›çš„åŸºçº¿æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f05e27973a12c4809011c2d3697f7355" align="middle">
<img src="https://picx.zhimg.com/v2-249c99f5253d4ee7055be1797731e772" align="middle">
<img src="https://picx.zhimg.com/v2-ba62b6f715374202c816b0bc54650953" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Direct-Persian-English-Speech-to-Speech-Translation-with-Discrete-Units-and-Synthetic-Parallel-Data"><a href="#Improving-Direct-Persian-English-Speech-to-Speech-Translation-with-Discrete-Units-and-Synthetic-Parallel-Data" class="headerlink" title="Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data"></a>Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data</h2><p><strong>Authors:Sina Rashidi, Hossein Sameti</strong></p>
<p>Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English</p>
<blockquote>
<p>ç›´æ¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSpeech-to-Speech Translationï¼ŒS2STï¼‰ï¼Œå…¶æ‰€æœ‰ç»„ä»¶éƒ½è¿›è¡Œè”åˆè®­ç»ƒï¼Œä½œä¸ºçº§è”ç³»ç»Ÿçš„æ›¿ä»£æ–¹æ¡ˆé¢‡å…·å¸å¼•åŠ›ï¼Œå› ä¸ºå®ƒæä¾›äº†æ›´ç®€å•çš„ç®¡é“å’Œæ›´ä½çš„æ¨ç†å»¶è¿Ÿã€‚ç„¶è€Œï¼Œç›´æ¥S2STæ¨¡å‹éœ€è¦å¤§é‡æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„å¹³è¡Œè¯­éŸ³æ•°æ®ï¼Œå¯¹äºæ³¢æ–¯è¯­ç­‰ä½èµ„æºè¯­è¨€æ¥è¯´ï¼Œè¿™äº›æ•°æ®å¾ˆå°‘å¯ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç”¨äºæ³¢æ–¯è¯­è¯­éŸ³ç¿»è¯‘æˆè‹±è¯­è¯­éŸ³çš„ç›´æ¥S2STç³»ç»Ÿï¼Œä»¥åŠä¸€ä¸ªç”¨äºç”Ÿæˆæ³¢æ–¯è¯­-è‹±è¯­å¹³è¡Œè¯­éŸ³çš„åˆæˆç®¡é“ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šï¼ˆ1ï¼‰åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„ç¼–ç å™¨ï¼Œé€šè¿‡è‡ªæˆ‘ç›‘ç£çš„é¢„è®­ç»ƒè¿›è¡Œåˆå§‹åŒ–ï¼Œå°†æºè¯­éŸ³æ˜ å°„åˆ°é«˜çº§å£°å­¦è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰å…·æœ‰ç›¸å¯¹ä½ç½®å¤šå¤´æ³¨æ„åŠ›çš„å› æœå˜å‹å™¨è§£ç å™¨å°†è¿™äº›è¡¨ç¤ºç¿»è¯‘æˆç¦»æ•£çš„ç›®æ ‡è¯­éŸ³å•å…ƒï¼›ï¼ˆ3ï¼‰åŸºäºå•å…ƒçš„ç¥ç»ç½‘ç»œç¼–ç å™¨ä»é¢„æµ‹çš„ç¦»æ•£å•å…ƒç”Ÿæˆæ³¢å½¢ã€‚ä¸ºäº†ç¼“è§£æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ³¢æ–¯è¯­-è‹±è¯­å¹³è¡Œè¯­éŸ³è¯­æ–™åº“ï¼Œé€šè¿‡å°†æ³¢æ–¯è¯­è¯­éŸ³è½¬å½•ç¿»è¯‘æˆè‹±è¯­å¹¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç„¶åä½¿ç”¨æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿåˆæˆç›¸åº”çš„è‹±è¯­è¯­éŸ³ã€‚ç”±æ­¤äº§ç”Ÿçš„è¯­æ–™åº“ä½¿å¯ç”¨çš„å¹³è¡Œè¯­éŸ³æ•°é‡å¢åŠ äº†å¤§çº¦å…­å€ã€‚åœ¨CVSSè¯­æ–™åº“çš„æ³¢æ–¯è¯­-è‹±è¯­éƒ¨åˆ†ä¸Šï¼Œä¸ç›´æ¥åŸºçº¿ç›¸æ¯”ï¼Œä½¿ç”¨åˆæˆæ•°æ®æ‰€æå‡ºçš„æ¨¡å‹åœ¨ASR BLEUä¸Šæé«˜äº†4.6åˆ†ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç»“åˆè‡ªæˆ‘ç›‘ç£çš„é¢„è®­ç»ƒã€ç¦»æ•£è¯­éŸ³å•å…ƒå’Œåˆæˆå¹³è¡Œæ•°æ®å¯¹äºæ”¹è¿›æ³¢æ–¯è¯­-è‹±è¯­ç­‰ä½èµ„æºè¯­è¨€å¯¹çš„ç›´æ¥S2STéå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12690v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºä¸€ç§é’ˆå¯¹æ³¢æ–¯è¯­åˆ°è‹±è¯­ç›´æ¥è¯­éŸ³å¯¹è¯­éŸ³ç¿»è¯‘ï¼ˆS2STï¼‰çš„ç³»ç»Ÿï¼Œä»¥åŠä¸€ä¸ªåˆæˆæ³¢æ–¯è¯­-è‹±è¯­å¹³è¡Œè¯­éŸ³ç”Ÿæˆç®¡é“ã€‚è¯¥ç³»ç»Ÿç”±ä¸‰ä¸ªç»„ä»¶æ„æˆï¼šåŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„ç¼–ç å™¨ã€å…·æœ‰ç›¸å¯¹ä½ç½®å¤šå¤´æ³¨æ„åŠ›çš„å› æœTransformerè§£ç å™¨ä»¥åŠåŸºäºå•å…ƒçš„ç¥ç»ç½‘ç»œç¼–ç å™¨ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œè®ºæ–‡é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å°†æ³¢æ–¯è¯­è¯­éŸ³è½¬å½•ç¿»è¯‘æˆè‹±è¯­ï¼Œç„¶åä½¿ç”¨å…ˆè¿›çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿåˆæˆç›¸åº”çš„è‹±è¯­è¯­éŸ³ï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°çš„æ³¢æ–¯è¯­-è‹±è¯­å¹³è¡Œè¯­éŸ³è¯­æ–™åº“ã€‚åœ¨CVSSè¯­æ–™åº“çš„æ³¢æ–¯è¯­-è‹±è¯­éƒ¨åˆ†ï¼Œä¸ç›´æ¥åŸºçº¿ç›¸æ¯”ï¼Œä½¿ç”¨åˆæˆæ•°æ®çš„æ–°æ¨¡å‹æé«˜äº†4.6çš„ASR BLEUå¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›´æ¥è¯­éŸ³å¯¹è¯­éŸ³ç¿»è¯‘ï¼ˆS2STï¼‰ä¸ºä½èµ„æºè¯­è¨€å¯¹å¦‚æ³¢æ–¯è¯­å’Œè‹±è¯­æä¾›äº†ç®€åŒ–ç®¡é“å’Œè¾ƒä½æ¨ç†å»¶è¿Ÿçš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>S2STæ¨¡å‹éœ€è¦å¤§é‡å¹³è¡Œè¯­éŸ³æ•°æ®ï¼Œä½†ä½èµ„æºè¯­è¨€å¦‚æ³¢æ–¯è¯­çš„æ•°æ®éš¾ä»¥è·å–ã€‚</li>
<li>æœ¬è®ºæ–‡æå‡ºä¸€ä¸ªæ³¢æ–¯è¯­åˆ°è‹±è¯­çš„ç›´æ¥S2STç³»ç»Ÿï¼ŒåŒ…æ‹¬ä¸€ä¸ªåŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„ç¼–ç å™¨ã€å› æœTransformerè§£ç å™¨å’Œç¥ç»ç½‘ç»œç¼–ç å™¨ã€‚</li>
<li>ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œè®ºæ–‡é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºæ³¢æ–¯è¯­-è‹±è¯­å¹³è¡Œè¯­éŸ³è¯­æ–™åº“ï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿåˆæˆç›¸åº”çš„è‹±è¯­è¯­éŸ³ã€‚</li>
<li>æ–°æ„å»ºçš„è¯­æ–™åº“å¢åŠ äº†å¯ç”¨å¹³è¡Œè¯­éŸ³æ•°æ®çš„æ•°é‡ã€‚</li>
<li>åœ¨CVSSè¯­æ–™åº“çš„æ³¢æ–¯è¯­-è‹±è¯­éƒ¨åˆ†è¿›è¡Œçš„å®éªŒä¸­ï¼Œæ–°æ¨¡å‹åœ¨ASR BLEUå¾—åˆ†ä¸Šè¾ƒç›´æ¥åŸºçº¿æé«˜äº†4.6ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62240c21a7518c82d9fa85d2b3f2a4aa" align="middle">
<img src="https://picx.zhimg.com/v2-2d67808ee8a49f90bcbd8ed1ddaf18a9" align="middle">
<img src="https://picx.zhimg.com/v2-b76571a0934c490c3f09cf8e81e16b91" align="middle">
<img src="https://picx.zhimg.com/v2-fa88cbe490ffdadfbe3e974157089a3e" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data"><a href="#Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data" class="headerlink" title="Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data"></a>Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</h2><p><strong>Authors:Yunxin Li, Xinyu Chen, Shenyuan Jiang, Haoyuan Shi, Zhenyu Liu, Xuanyu Zhang, Nanhao Deng, Zhenran Xu, Yicheng Ma, Meishan Zhang, Baotian Hu, Min Zhang</strong></p>
<p>We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lycheeâ€™s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>
<blockquote>
<p>æˆ‘ä»¬åœ¨æ­¤ä»‹ç»æ¥è‡ªLycheeå®¶æ—çš„Uni-MoE 2.0ã€‚ä½œä¸ºä¸€æ¬¾å…¨å¼€æºçš„å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆOLMï¼‰ï¼Œå®ƒæå¤§åœ°æ¨åŠ¨äº†Lycheeçš„Uni-MoEç³»åˆ—åœ¨è¯­è¨€ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚åŸºäºQwen2.5-7Bçš„å¯†é›†æ¶æ„ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒè´¡çŒ®ä»å¤´æ„å»ºäº†Uni-MoE-2.0-Omniï¼šåŠ¨æ€å®¹é‡ä¸“å®¶æ··åˆï¼ˆMoEï¼‰è®¾è®¡ã€é‡‡ç”¨è¿­ä»£å¢å¼ºç­–ç•¥çš„æ¸è¿›è®­ç»ƒç­–ç•¥ä»¥åŠç²¾å¿ƒç­–åˆ’çš„å¤šæ¨¡æ€æ•°æ®åŒ¹é…æŠ€æœ¯ã€‚å®ƒèƒ½å¤Ÿè¿›è¡Œå¤šæ¨¡æ€ç†è§£ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆå›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ã€‚åœ¨ç»“æ„ä¸Šï¼Œæˆ‘ä»¬æ–°çš„MoEæ¡†æ¶é€šè¿‡å…±äº«ã€è·¯ç”±å’Œç©ºä¸“å®¶åœ¨10ä¸ªè·¨æ¨¡æ€è¾“å…¥ä¹‹é—´å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œèƒ½åŠ›ï¼›æˆ‘ä»¬çš„Omni-Modality 3D RoPEåˆ™ç¡®ä¿è‡ªæ³¨æ„åŠ›å±‚ä¸­çš„æ—¶ç©ºè·¨æ¨¡æ€å¯¹é½ã€‚åœ¨è®­ç»ƒæ–¹é¢ï¼Œæˆ‘ä»¬åœ¨è·¨æ¨¡æ€é¢„è®­ç»ƒä¹‹åé‡‡ç”¨æ¸è¿›çš„ç›‘ç£å¾®è°ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿæ¿€æ´»æ¨¡æ€ç‰¹å®šä¸“å®¶ï¼Œå¹¶é€šè¿‡å¹³è¡¡æ•°æ®ç»„åˆå’Œè¿­ä»£GSPO-DPOæ–¹æ³•æ¥ç¨³å®šå¼ºåŒ–å­¦ä¹ è®­ç»ƒå’Œæé«˜æ¨ç†èƒ½åŠ›ã€‚åœ¨æ•°æ®æ–¹é¢ï¼ŒåŸºç¡€æ¨¡å‹åœ¨å¤§çº¦75Bä¸ªå¼€æºå¤šæ¨¡æ€æ•°æ®æ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é…å¤‡äº†ç‰¹æ®Šçš„è¯­éŸ³å’Œå›¾åƒç”Ÿæˆæ ‡è®°ï¼Œè¿™ä½¿å…¶èƒ½å¤Ÿé€šè¿‡è¯­è¨€çº¿ç´¢æ¥ç”Ÿæˆè¿™äº›ä»»åŠ¡è¾“å‡ºã€‚åœ¨85ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³æˆ–åœ¨é¢†å…ˆçš„OLMä¸­å…·æœ‰ç«äº‰åŠ›ï¼Œåœ¨è¶…è¿‡76ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„50å¤šä¸ªæµ‹è¯•ä¸­è¶…è¿‡äº†ä½¿ç”¨1.2Tæ ‡è®°è®­ç»ƒçš„Qwen2.5-Omniã€‚å…³é”®ä¼˜åŠ¿åŒ…æ‹¬è§†é¢‘ç†è§£ï¼ˆ+7%çš„å¹³å‡å€¼è¾¾åˆ°å…«é¡¹æŒ‡æ ‡ï¼‰ã€å¤šæ¨¡æ€ç†è§£ï¼ˆ+ä¸ƒé¡¹å¹³å‡å€¼è¾¾å››é¡¹ï¼‰ã€è§†å¬æ¨ç†ï¼ˆ+4%ï¼‰ç­‰ã€‚åŒæ—¶ï¼Œå®ƒè¿˜æ¨åŠ¨äº†é•¿å½¢å¼è¯­éŸ³å¤„ç†çš„å‘å±•ï¼ˆå°†å­—é”™è¯¯ç‡é™ä½4.2%ï¼‰ï¼Œå¹¶åœ¨äº”ä¸ªæŒ‡æ ‡ä¸­å¤„äºå›¾åƒå¤„ç†å’Œå¯æ§ç”Ÿæˆçš„é¢†å…ˆåœ°ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12609v1">PDF</a> 47 pages,10 Figures, Project Website: <a target="_blank" rel="noopener" href="https://idealistxy.github.io/Uni-MoE-v2.github.io/">https://idealistxy.github.io/Uni-MoE-v2.github.io/</a>; Codes: <a target="_blank" rel="noopener" href="https://github.com/HITsz-TMG/Uni-MoE">https://github.com/HITsz-TMG/Uni-MoE</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºQwen2.5-7Bå¯†é›†æ¶æ„ï¼ŒLycheeå®¶æ—çš„Uni-MoE 2.0ä½œä¸ºå…¨å¼€æºçš„è·¨æ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆOLMï¼‰ï¼Œåœ¨è¯­è¨€ä¸ºä¸­å¿ƒçš„è·¨æ¨¡æ€ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å®ƒé€šè¿‡åŠ¨æ€å®¹é‡çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰è®¾è®¡ã€å¢å¼ºçš„æ¸è¿›è®­ç»ƒç­–ç•¥ä»¥åŠç²¾å¿ƒç­–åˆ’çš„è·¨æ¨¡æ€æ•°æ®åŒ¹é…æŠ€æœ¯ï¼Œæ„å»ºäº†Uni-MoE-2.0-Omniæ¨¡å‹ã€‚è¯¥æ¨¡å‹å…·å¤‡è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆå›¾åƒã€æ–‡æœ¬ã€è¯­éŸ³çš„èƒ½åŠ›ã€‚å…¶æ–°çš„MoEæ¡†æ¶åœ¨è®¡ç®—æ•ˆç‡å’Œå¤„ç†è·¨æ¨¡æ€è¾“å…¥æ–¹é¢å–å¾—äº†å¹³è¡¡ï¼ŒåŒæ—¶Omni-Modality 3D RoPEç¡®ä¿äº†è·¨æ¨¡æ€æ—¶ç©ºçš„è‡ªæ³¨æ„åŠ›å±‚å¯¹é½ã€‚é€šè¿‡æ¸è¿›çš„ç›‘ç£å¾®è°ƒç­–ç•¥å’Œå¹³è¡¡æ•°æ®ç»„åˆä»¥åŠè¿­ä»£GSPO-DPOæ–¹æ³•ï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´åŠ ç¨³å®šå¹¶æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨çº¦75Bæ ‡è®°çš„å¼€æºè·¨æ¨¡æ€æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é…å¤‡äº†ç‰¹æ®Šçš„è¯­éŸ³å’Œå›¾åƒç”Ÿæˆæ ‡è®°ã€‚è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¶…è¿‡ä¸€åŠçš„åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†å…ˆå‰çš„OLMè¡¨ç°ã€‚å…¶ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬è§†é¢‘ç†è§£ã€è·¨æ¨¡æ€ç†è§£å’Œè§†å¬æ¨ç†ç­‰æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Uni-MoE 2.0æ˜¯Lycheeå®¶æ—çš„ä¸€ä¸ªå…¨å¼€æºçš„è·¨æ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆOLMï¼‰ï¼ŒåŸºäºQwen2.5-7Bæ¶æ„ã€‚</li>
<li>Uni-MoE 2.0é€šè¿‡åŠ¨æ€å®¹é‡çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰è®¾è®¡ç­‰æŠ€æœ¯æ„å»ºäº†Uni-MoE-2.0-Omniæ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å…·å¤‡è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆå›¾åƒã€æ–‡æœ¬ã€è¯­éŸ³çš„èƒ½åŠ›ã€‚</li>
<li>æ–°çš„MoEæ¡†æ¶åœ¨è®¡ç®—æ•ˆç‡å’Œå¤„ç†è·¨æ¨¡æ€è¾“å…¥æ–¹é¢å–å¾—äº†å¹³è¡¡ã€‚</li>
<li>Omni-Modality 3D RoPEæŠ€æœ¯ç¡®ä¿äº†è·¨æ¨¡æ€æ—¶ç©ºçš„è‡ªæ³¨æ„åŠ›å±‚å¯¹é½ã€‚</li>
<li>é€šè¿‡æ¸è¿›çš„ç›‘ç£å¾®è°ƒç­–ç•¥å’Œå¹³è¡¡æ•°æ®ç»„åˆä»¥åŠè¿­ä»£GSPO-DPOæ–¹æ³•ï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´åŠ ç¨³å®šå¹¶æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨è§†é¢‘ç†è§£ã€è·¨æ¨¡æ€ç†è§£å’Œè§†å¬æ¨ç†ç­‰æ–¹é¢æœ‰æ˜¾è‘—çš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fbf6eae5c80051fc5966dffb97d4896" align="middle">
<img src="https://picx.zhimg.com/v2-2b72f469dd496a4ebcc1cd796a1db34b" align="middle">
<img src="https://picx.zhimg.com/v2-a964386fcf1c47706e06fde04609f299" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="How-Far-Do-SSL-Speech-Models-Listen-for-Tone-Temporal-Focus-of-Tone-Representation-under-Low-resource-Transfer"><a href="#How-Far-Do-SSL-Speech-Models-Listen-for-Tone-Temporal-Focus-of-Tone-Representation-under-Low-resource-Transfer" class="headerlink" title="How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer"></a>How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer</h2><p><strong>Authors:Minu Kim, Ji Sub Um, Hoirin Kim</strong></p>
<p>Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.</p>
<blockquote>
<p>è¯­éŸ³è¯­è°ƒåœ¨è®¸å¤šè¯­è¨€ä¸­è‡³å…³é‡è¦ï¼Œä½†åœ¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„è¯­éŸ³æ¨¡å‹ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨éæ™®é€šè¯è¯­è¨€ä¸­çš„æ¢ç´¢ä»ç„¶ä¸è¶³ã€‚æˆ‘ä»¬ç ”ç©¶äº†å››ç§å…·æœ‰å¤æ‚å’Œå¤šæ ·åŒ–çš„è¯­è°ƒç³»ç»Ÿçš„è¯­è¨€ï¼šç¼…ç”¸è¯­ã€æ³°è¯­ã€è€æŒè¯­å’Œè¶Šå—è¯­ï¼Œä»¥ç ”ç©¶æ­¤ç±»æ¨¡å‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½å¤Ÿè¯†åˆ«è¯­è°ƒï¼Œä»¥åŠåœ¨èµ„æºåŒ®ä¹çš„æ¡ä»¶ä¸‹å¦‚ä½•è¿›è¡Œè¿ç§»ã€‚ä½œä¸ºåŸºå‡†å‚è€ƒï¼Œæˆ‘ä»¬ä¼°è®¡ç¼…ç”¸è¯­å’Œæ³°è¯­çš„è¯­è°ƒçº¿ç´¢æ—¶é—´è·¨åº¦çº¦ä¸º100æ¯«ç§’ï¼Œè€Œè€æŒè¯­å’Œè¶Šå—è¯­çš„è¯­è°ƒçº¿ç´¢æ—¶é—´è·¨åº¦çº¦ä¸º180æ¯«ç§’ã€‚å¯¹å¾®è°ƒSSLæ¨¡å‹çš„æ¢é’ˆå’Œæ¢¯åº¦åˆ†ææ˜¾ç¤ºï¼Œè¯­è°ƒè¿ç§»å—ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å¾®è°ƒä¸ç‰¹å®šè¯­è¨€çš„è¯­è°ƒçº¿ç´¢å¯¹é½ï¼Œè€ŒéŸµå¾‹å’Œè¯­éŸ³ç›¸å…³ä»»åŠ¡åˆ™ä½¿æ¨¡å‹åå‘äºè¿‡é•¿çš„è·¨åº¦ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè¯­è°ƒè¿ç§»å—ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ï¼Œçªå‡ºäº†ä»»åŠ¡å¯¹è¯­è°ƒå»ºæ¨¡ä¸­æ—¶é—´é‡ç‚¹çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12285v1">PDF</a> 5 pages, 7 figures, submitted to ICASSP 2026</p>
<p><strong>æ€»ç»“</strong></p>
<p>è¯­éŸ³æ¨¡å‹çš„è‡ªç›‘ç£å­¦ä¹ å¯¹éŸ³è°ƒå¤„ç†ä¸å¤Ÿå……åˆ†ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéæ±‰è¯­è¯­ç§çš„ç ”ç©¶å°šæµ…ã€‚æœ¬æ–‡é€šè¿‡æ¢è®¨å››ç§æ‹¥æœ‰å¤æ‚ä¸”å¤šæ ·çš„éŸ³è°ƒç³»ç»Ÿçš„è¯­è¨€ï¼ŒåŒ…æ‹¬ç¼…ç”¸è¯­ã€æ³°è¯­ã€è€æŒè¯­å’Œè¶Šå—è¯­ï¼Œæ¢ç©¶äº†è¯­éŸ³æ¨¡å‹å¦‚ä½•è¯†åˆ«è¯­éŸ³å’Œåœ¨ä¸åŒèµ„æºæ¡ä»¶ä¸‹çš„è¿ç§»æ•ˆæœã€‚æœ¬æ–‡è®¤ä¸ºè¯­éŸ³æ¨¡å‹æ•æ‰éŸ³è°ƒä¿¡æ¯çš„æ—¶é•¿å¤§çº¦ä¸ºç¼…ç”¸è¯­å’Œæ³°è¯­çš„100æ¯«ç§’å·¦å³ï¼Œè€Œè€æŒè¯­å’Œè¶Šå—è¯­åˆ™ä¸ºçº¦180æ¯«ç§’ã€‚å¯¹å¾®è°ƒåçš„è¯­éŸ³æ¨¡å‹çš„æµ‹è¯•å’Œæ¢¯åº¦åˆ†ææ˜¾ç¤ºï¼Œä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„éŸ³è°ƒè¿ç§»å­˜åœ¨å·®å¼‚ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å¾®è°ƒä¸ç‰¹å®šè¯­è¨€çš„éŸ³è°ƒæç¤ºå¯¹é½ï¼Œè€ŒéŸµå¾‹å’Œè¯­éŸ³ç›¸å…³ä»»åŠ¡åˆ™ä½¿æ¨¡å‹åå‘äºè¿‡é•¿çš„æ—¶é•¿ã€‚è¿™è¡¨æ˜ä¸‹æ¸¸ä»»åŠ¡å¯¹éŸ³è°ƒè¿ç§»äº§ç”Ÿå½±å“ï¼Œè¿›è€Œçªæ˜¾äº†è¯­éŸ³æ¨¡å‹å¤„ç†éŸ³è°ƒçš„æ—¶ç©ºç‰¹å¾ã€‚ </p>
<p><strong>å…³é”®å‘ç°</strong></p>
<p>ä»¥ä¸‹æ˜¯ä»æ–‡æœ¬ä¸­å¾—å‡ºçš„ä¸ƒä¸ªä¸»è¦è§‚ç‚¹ï¼š</p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨è¯­éŸ³æ¨¡å‹ä¸­å¯¹éŸ³è°ƒçš„æ¢ç´¢å°šæ˜¾ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å¤šæ ·è¯­è¨€ä½“ç³»æ–¹é¢ï¼Œä¾‹å¦‚ç¼…ç”¸è¯­ã€æ³°è¯­ã€è€æŒè¯­å’Œè¶Šå—è¯­ç­‰ã€‚</li>
<li>éŸ³è°ƒä¿¡æ¯çš„æ•æ‰æ—¶é•¿å› è¯­è¨€è€Œå¼‚ï¼Œé€šå¸¸åœ¨ç¼…ç”¸è¯­å’Œæ³°è¯­ä¸­çº¦ä¸º100æ¯«ç§’å·¦å³ï¼Œè€Œåœ¨è€æŒè¯­å’Œè¶Šå—è¯­ä¸­åˆ™å¤§çº¦ä¸º180æ¯«ç§’ã€‚è¿™è¡¨æ˜ä¸åŒè¯­è¨€çš„éŸ³è°ƒç³»ç»Ÿæœ‰å…¶ç‹¬ç‰¹çš„å£°å­¦ç‰¹å¾ã€‚</li>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å¾®è°ƒä¸ç‰¹å®šè¯­è¨€çš„éŸ³è°ƒæç¤ºå¯¹é½ï¼Œè¿™æœ‰åŠ©äºæå‡è¯­éŸ³æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¸åŒä¸‹æ¸¸ä»»åŠ¡å¯¹è¯­éŸ³æ¨¡å‹çš„éŸ³è°ƒè¿ç§»äº§ç”Ÿå½±å“ï¼Œè¿™åæ˜ äº†è¯­éŸ³æ¨¡å‹åœ¨åº”å¯¹ä¸åŒä»»åŠ¡æ—¶çš„é€‚åº”æ€§å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12285">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6b9ae1cbbf236aada4c9a1ffcb0cd5d" align="middle">
<img src="https://picx.zhimg.com/v2-658eb5ec9b011e078137605aa993a27c" align="middle">
<img src="https://picx.zhimg.com/v2-a2c8d295106f06d6911b8c90974b4ef8" align="middle">
<img src="https://picx.zhimg.com/v2-fd78aad0930db8a6652f3b1a6f47fa0b" align="middle">
<img src="https://picx.zhimg.com/v2-32ae26372073d422b8e441f96267fe94" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Tighter-Truncated-Rectangular-Prism-Approximation-for-RNN-Robustness-Verification"><a href="#Tighter-Truncated-Rectangular-Prism-Approximation-for-RNN-Robustness-Verification" class="headerlink" title="Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification"></a>Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification</h2><p><strong>Authors:Xingqi Lin, Liangyu Chen, Min Wu, Min Zhang, Zhenbing Zeng</strong></p>
<p>Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.</p>
<blockquote>
<p>ç¨³å¥æ€§éªŒè¯æ˜¯è¯æ˜å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ç¨³å¥æ€§çš„æœ‰å‰é€”çš„æŠ€æœ¯ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ç”¨çº¿æ€§çº¦æŸå¯¹éçº¿æ€§æ¿€æ´»å‡½æ•°è¿›è¡Œè¿‡è¿‘ä¼¼ï¼Œè¿™å¯ä»¥å°†éªŒè¯é—®é¢˜è½¬åŒ–ä¸ºå¯é«˜æ•ˆè§£å†³çš„çº¿æ€§è§„åˆ’é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡çº¿æ€§è¾¹ç•Œå¹³é¢å•ç‹¬å¯¹éçº¿æ€§çš„éƒ¨åˆ†è¿›è¡Œè¿‡è¿‘ä¼¼å¤„ç†ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ˜¾è‘—çš„è¿‡åº¦ä¼°è®¡å¹¶å¯¼è‡´éªŒè¯ç²¾åº¦é™ä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä¸ºäº†ç´§å¯†å°é—­ç”±Hadamardäº§å“äº§ç”Ÿçš„ä¸‰ç»´éçº¿æ€§è¡¨é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±ä¸¤ä¸ªçº¿æ€§æ¾å¼›å¹³é¢æ„æˆçš„æ–°å‹æˆªæ–­çŸ©å½¢æ£±æŸ±ï¼Œå¹¶åŸºäºç²¾ç»†åŒ–é©±åŠ¨çš„æ–¹æ³•åŒæ—¶å‡å°å…¶ä½“ç§¯å’Œè¡¨é¢ç§¯æ¥å®ç°æ›´ç´§å¯†çš„è¿‡è¿‘ä¼¼ã€‚åŸºäºè¿™ç§è¿‘ä¼¼æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ç§åä¸ºDeepPrismçš„RNNç¨³å¥æ€§éªŒè¯åŸå‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepPrismç›¸è¾ƒäºæœ€å‰æ²¿çš„å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«å’Œæƒ…æ„Ÿåˆ†æçš„å„ç§ä»»åŠ¡åœ¨å¤„ç†ä¸­å…·æœ‰æ˜¾è‘—æ”¹å–„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11699v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„é²æ£’æ€§éªŒè¯æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰çš„é²æ£’æ€§éªŒè¯ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³éçº¿æ€§æ¿€æ´»å‡½æ•°è¿‡åº¦è¿‘ä¼¼çš„é—®é¢˜ï¼Œé€šè¿‡å°†éçº¿æ€§éƒ¨åˆ†ç´§å¯†åœ°å°è£…åœ¨ä¸€ä¸ªç”±ä¸¤ä¸ªçº¿æ€§æ¾å¼›å¹³é¢å’Œä¸€ä¸ªä¼˜åŒ–é©±åŠ¨çš„æˆªæ–­çŸ©å½¢æ£±æŸ±å†…ï¼Œä»¥æœ€å°åŒ–å…¶ä½“ç§¯å’Œè¡¨é¢ç§¯å®ç°æ›´ç´§å¯†çš„è¿‘ä¼¼ã€‚åŸºäºè¿™ç§è¿‘ä¼¼æ–¹æ³•ï¼Œæ–‡ç« å®ç°äº†ä¸€ä¸ªåä¸ºDeepPrismçš„åŸå‹ç”¨äºRNNé²æ£’æ€§éªŒè¯ï¼Œå¹¶åœ¨å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«å’Œæƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é²æ£’æ€§éªŒè¯æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æŠ€æœ¯ï¼Œç”¨äºä¸¥æ ¼è¯æ˜RNNsçš„é²æ£’æ€§ã€‚</li>
<li>éçº¿æ€§æ¿€æ´»å‡½æ•°çš„è¿‡åº¦è¿‘ä¼¼æ˜¯éªŒè¯è¿‡ç¨‹ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡çº¿æ€§è¾¹ç•Œå¹³é¢è¿›è¡Œä¸ªä½“è¿‡åº¦è¿‘ä¼¼ï¼Œå¯èƒ½å¯¼è‡´æ˜¾è‘—è¿‡åº¦ä¼°è®¡å’Œè¾ƒä½çš„éªŒè¯ç²¾åº¦ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„æˆªæ–­çŸ©å½¢æ£±æŸ±æ–¹æ³•ï¼Œé€šè¿‡ä¸¤ä¸ªçº¿æ€§æ¾å¼›å¹³é¢å’Œæœ€å°åŒ–ä½“ç§¯åŠè¡¨é¢ç§¯å®ç°æ›´ç´§å¯†çš„è¿‘ä¼¼ã€‚</li>
<li>åŸºäºè¿™ç§è¿‘ä¼¼æ–¹æ³•ï¼Œå®ç°äº†åä¸ºDeepPrismçš„RNNé²æ£’æ€§éªŒè¯åŸå‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepPrismåœ¨å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«å’Œæƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ä¸Šç›¸æ¯”æœ€æ–°æŠ€æœ¯æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50257fa6ea93cfc632cf264e618f4330" align="middle">
<img src="https://picx.zhimg.com/v2-b942947f17f32787af52c3e88c21c9b1" align="middle">
<img src="https://picx.zhimg.com/v2-e6e42b8f38ed935082b38dbb5b74ee1b" align="middle">
<img src="https://picx.zhimg.com/v2-81c534a8d2d438f54c72d18d50cdd610" align="middle">
<img src="https://picx.zhimg.com/v2-36715e8b60404f6fa99691ff5369a2d7" align="middle">
<img src="https://picx.zhimg.com/v2-f158958587812d25a14d8ec0c28205a0" align="middle">
<img src="https://picx.zhimg.com/v2-5c6f8b7aa3441f25837e55be8aa57554" align="middle">
<img src="https://picx.zhimg.com/v2-570ebdc4b7a7f65a46ef78cefece13f1" align="middle">
<img src="https://picx.zhimg.com/v2-ebdafd08f5cfbb4d920ed188e7a2d79e" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Regularized-Schrodinger-Alleviating-Distortion-and-Exposure-Bias-in-Solving-Inverse-Problems"><a href="#Regularized-Schrodinger-Alleviating-Distortion-and-Exposure-Bias-in-Solving-Inverse-Problems" class="headerlink" title="Regularized SchrÃ¶dinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems"></a>Regularized SchrÃ¶dinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems</h2><p><strong>Authors:Qing Yao, Lijian Gao, Qirong Mao, Dong Ming</strong></p>
<p>Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized SchrÃ¶dinger Bridge (RSB), an adaptation of SchrÃ¶dinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.</p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æ˜¯è§£å†³é€†é—®é¢˜çš„å¼ºå¤§ç”Ÿæˆæ¡†æ¶ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š1ï¼‰å¤±çœŸä¸æ„ŸçŸ¥ä¹‹é—´çš„æƒè¡¡ï¼Œåœ¨æé«˜æ„ŸçŸ¥è´¨é‡çš„åŒæ—¶å¾€å¾€ä¼šé™ä½é‡å»ºçš„ä¿çœŸåº¦ï¼›2ï¼‰æ›å…‰åå·®é—®é¢˜ï¼Œè®­ç»ƒä¸æ¨ç†ä¹‹é—´çš„è¾“å…¥ä¸åŒ¹é…å¯¼è‡´é¢„æµ‹è¯¯å·®ç´¯ç§¯å¹¶é™ä½äº†é‡å»ºè´¨é‡ã€‚é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§æ­£åˆ™åŒ–SchrÃ¶dinger Bridgeï¼ˆRSBï¼‰ï¼Œè¿™æ˜¯é’ˆå¯¹é€†é—®é¢˜é‡èº«å®šåˆ¶çš„SchrÃ¶dinger Bridgeçš„ä¸€ç§é€‚åº”ã€‚RSBé‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„æ­£åˆ™åŒ–è®­ç»ƒç­–ç•¥ï¼ŒåŒæ—¶æ‰°åŠ¨è¾“å…¥çŠ¶æ€å’Œç›®æ ‡ï¼Œé€šè¿‡ä½¿æ¨¡å‹æš´éœ²äºæ¨¡æ‹Ÿé¢„æµ‹è¯¯å·®æ¥æœ‰æ•ˆç¼“è§£æ›å…‰åå·®ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡åéªŒå‡å€¼è¿›è¡Œæ’å€¼æ¥ç¼“è§£å¤±çœŸã€‚é’ˆå¯¹è¯­éŸ³å¢å¼ºçš„ä¸¤ä¸ªå…¸å‹é€†é—®é¢˜çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒRSBä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å¤±çœŸæŒ‡æ ‡å¹¶æœ‰æ•ˆé™ä½äº†æ›å…‰åå·®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11686v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨è§£å†³åé—®é¢˜ä¸Šæ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç”Ÿæˆæ¡†æ¶ï¼Œä½†ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯å¤±çœŸä¸æ„ŸçŸ¥ä¹‹é—´çš„æƒè¡¡ï¼Œæ”¹å–„æ„ŸçŸ¥è´¨é‡å¾€å¾€ä¼šé™ä½é‡å»ºçš„ä¿çœŸåº¦ï¼›äºŒæ˜¯æ›å…‰åå·®é—®é¢˜ï¼Œè®­ç»ƒä¸æ¨ç†æ—¶è¾“å…¥çš„ä¸åŒ¹é…å¯¼è‡´é¢„æµ‹è¯¯å·®ç´¯ç§¯å’Œé‡å»ºè´¨é‡ä¸‹é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†é’ˆå¯¹åé—®é¢˜çš„æ­£åˆ™åŒ–è–›å®šè°”æ¡¥ï¼ˆRSBï¼‰ï¼Œé€šè¿‡ä¸€ç§æ–°çš„æ­£åˆ™åŒ–è®­ç»ƒç­–ç•¥ï¼Œå¯¹è¾“å…¥çŠ¶æ€å’Œç›®æ ‡è¿›è¡Œæ‰°åŠ¨ï¼Œæœ‰æ•ˆç¼“è§£äº†æ›å…‰åå·®ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ’å€¼æ–¹æ³•ç¼“è§£äº†å¤±çœŸé—®é¢˜ã€‚åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸçš„ä¸¤ä¸ªå…¸å‹åé—®é¢˜ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRSBä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å¤±çœŸæŒ‡æ ‡ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†æ›å…‰åå·®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§£å†³åé—®é¢˜ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é¢ä¸´å¤±çœŸä¸æ„ŸçŸ¥ä¹‹é—´çš„æƒè¡¡ä»¥åŠæ›å…‰åå·®ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>RSBä½œä¸ºé’ˆå¯¹åé—®é¢˜çš„æ‰©æ•£æ¨¡å‹è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ­£åˆ™åŒ–è®­ç»ƒç­–ç•¥ç¼“è§£ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>RSBé€šè¿‡æ‰°åŠ¨è¾“å…¥çŠ¶æ€å’Œç›®æ ‡æ¥ç¼“è§£æ›å…‰åå·®ã€‚</li>
<li>RSBé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ’å€¼æ–¹æ³•ç¼“è§£å¤±çœŸé—®é¢˜ã€‚</li>
<li>åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼ŒRSBåœ¨å¤±çœŸæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdd93f73f5d6d09ed4a297338efca320" align="middle">
<img src="https://picx.zhimg.com/v2-d974758fec148cba5765bc2120209e89" align="middle">
<img src="https://picx.zhimg.com/v2-18728f4146729f9d11f9f01e968fe7ad" align="middle">
<img src="https://picx.zhimg.com/v2-94c3297d74f9226e585929f6ffdec510" align="middle">
<img src="https://picx.zhimg.com/v2-ec6fa9921953e9ce7fa0a952eeb16d02" align="middle">
<img src="https://picx.zhimg.com/v2-113836888b84a1075df6d3a592a99b8e" align="middle">
<img src="https://picx.zhimg.com/v2-7dd859b2732a82e16eaf29ac8690e966" align="middle">
<img src="https://picx.zhimg.com/v2-0d503cdcb2bf381a1a753765035b52a4" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VocalBench-zh-Decomposing-and-Benchmarking-the-Speech-Conversational-Abilities-in-Mandarin-Context"><a href="#VocalBench-zh-Decomposing-and-Benchmarking-the-Speech-Conversational-Abilities-in-Mandarin-Context" class="headerlink" title="VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context"></a>VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context</h2><p><strong>Authors:Heyang Liu, Ziyang Cheng, Yuhao Wang, Hongcheng Liu, Yiqi Li, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</strong></p>
<p>The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at <a target="_blank" rel="noopener" href="https://github.com/SJTU-OmniAgent/VocalBench-zh">https://github.com/SJTU-OmniAgent/VocalBench-zh</a>.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æ¨åŠ¨äº†èƒ½å¤Ÿè¿›è¡Œè¯­éŸ³äº¤äº’çš„æ™ºèƒ½æ–¹æ³•ã€‚ä½œä¸ºä¸–ç•Œä¸Šä½¿ç”¨æœ€å¹¿æ³›çš„è¯­ç§ä¹‹ä¸€ï¼Œæ™®é€šè¯å—åˆ°å¤§å¤šæ•°æ¨¡å‹çš„æ”¯æŒï¼Œå¢å¼ºäº†å…¶é€‚ç”¨æ€§å’Œè¦†ç›–èŒƒå›´ã€‚ç„¶è€Œï¼Œæ™®é€šè¯è¯­å¢ƒä¸­å…¨é¢çš„è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰åŸºå‡†çš„ç¼ºä¹é˜»ç¢äº†å¼€å‘äººå‘˜çš„ç³»ç»Ÿè¯„ä¼°å’Œç”¨æˆ·ä¹‹é—´çš„å…¬å¹³æ¨¡å‹æ¯”è¾ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘æ™®é€šè¯è¯­å¢ƒçš„VocalBench-zhèƒ½åŠ›æ°´å¹³è¯„ä¼°å¥—ä»¶ï¼Œå…¶ä¸­åŒ…æ‹¬ç²¾å¿ƒè®¾è®¡çš„10ä¸ªå­é›†å’Œè¶…è¿‡ä¸€ä¸‡ä¸ªé«˜è´¨é‡å®ä¾‹æ ·æœ¬ã€‚æ­¤å¤–è¿˜è¦†ç›–äº†ç”¨æˆ·æ€§æ ¼å¯¼å‘çš„åäºŒä¸ªè¦ç´ ã€‚å¯¹äºä¸»æµçš„åå››ç§æ¨¡å‹çš„è¯„ä¼°å®éªŒè¡¨æ˜å‡ºäº†å½“å‰æ–¹æ³•çš„æ™®éæŒ‘æˆ˜ï¼Œå¹¶å‡¸æ˜¾å‡ºå¯¹ä¸‹ä¸€ä»£è¯­éŸ³äº¤äº’ç³»ç»Ÿçš„æ–°è§è§£çš„å¿…è¦æ€§ã€‚è¯„ä¼°ä»£ç å’Œæ•°æ®é›†å°†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/SJTU-OmniAgent/VocalBench-zh%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/SJTU-OmniAgent/VocalBench-zhè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08230v2">PDF</a> This article will serve as an extension of the preceding work, â€œVocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Modelsâ€ (arXiv:2505.15727). Therefore, we have chosen to withdraw to avoid potential duplicate publication. We will update the previously open-sourced paper of VocalBench in several weeks to include the content of VocalBench-zh</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æ¨åŠ¨äº†èƒ½å¤Ÿè¿›è¡Œè¯­éŸ³äº¤äº’çš„æ™ºèƒ½æ–¹æ³•çš„åº”ç”¨ã€‚ä¸ºäº†æå‡æ¨¡å‹çš„åº”ç”¨æ€§å’Œè¦†ç›–é¢ï¼Œå¤§å¤šæ•°æ¨¡å‹éƒ½æ”¯æŒå…¨çƒæœ€å¹¿æ³›ä½¿ç”¨çš„è¯­è¨€ä¹‹ä¸€â€”â€”æ™®é€šè¯ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å…¨é¢çš„æ™®é€šè¯è¯­å¢ƒä¸‹çš„è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰åŸºå‡†æµ‹è¯•é›†ï¼Œå¼€å‘è€…åœ¨ç³»ç»Ÿçš„è¯„ä»·æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç”¨æˆ·åœ¨æ¨¡å‹å¯¹æ¯”ä¸­ä¹Ÿéš¾ä»¥åšåˆ°å…¬å¹³ã€‚æœ¬ç ”ç©¶æå‡ºäº†é¢å‘æ™®é€šè¯è¯­å¢ƒçš„VocalBench-zhè¯„ä¼°å¥—ä»¶ï¼Œè¯¥å¥—ä»¶åŒ…å«10ä¸ªç²¾å¿ƒè®¾è®¡çš„å­é›†å’Œè¶…è¿‡1ä¸‡æ¡é«˜è´¨é‡å®ä¾‹ï¼Œæ¶µç›–12ä¸ªé¢å‘ç”¨æˆ·çš„ç‰¹ç‚¹ã€‚å¯¹ä¸»æµæ¨¡å‹çš„è¯„ä¼°å®éªŒæ­ç¤ºäº†å½“å‰è·¯çº¿çš„å¸¸è§æŒ‘æˆ˜ï¼Œå¹¶çªå‡ºäº†å¯¹ä¸‹ä¸€ä»£è¯­éŸ³äº¤äº’ç³»ç»Ÿçš„æ–°è§è§£çš„å¿…è¦æ€§ã€‚è¯„ä¼°ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/SJTU-OmniAgent/VocalBench-zh%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/SJTU-OmniAgent/VocalBench-zhä¸Šæä¾›ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•æ¨åŠ¨äº†è¯­éŸ³äº¤äº’çš„æ™ºèƒ½æ–¹æ³•åº”ç”¨ã€‚</li>
<li>æ™®é€šè¯åœ¨æ¨¡å‹ä¸­çš„åº”ç”¨å¢å¼ºäº†å…¶é€‚ç”¨æ€§å’Œè¦†ç›–é¢ã€‚</li>
<li>ç¼ºä¹æ™®é€šè¯è¯­å¢ƒä¸‹çš„å…¨é¢è¯­éŸ³åˆ°è¯­éŸ³åŸºå‡†æµ‹è¯•é›†å½±å“äº†ç³»ç»Ÿçš„è¯„ä»·ã€‚</li>
<li>VocalBench-zhè¯„ä¼°å¥—ä»¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒ…å«ç²¾å¿ƒè®¾è®¡çš„å­é›†å’Œé«˜è´¨é‡å®ä¾‹ã€‚</li>
<li>è¯¥å¥—ä»¶æ¶µç›–å¤šç§é¢å‘ç”¨æˆ·çš„ç‰¹ç‚¹ã€‚</li>
<li>è¯„ä¼°å®éªŒæ­ç¤ºäº†å½“å‰æ¨¡å‹çš„æŒ‘æˆ˜å¹¶å¼ºè°ƒäº†æ–°è§è§£çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df1274aebf62f49fd64ca2cedcba6792" align="middle">
<img src="https://picx.zhimg.com/v2-5e35906bed44cfaca98718eaabcf630c" align="middle">
<img src="https://picx.zhimg.com/v2-77c3531897a2c40aef171e7226e11ed9" align="middle">
<img src="https://picx.zhimg.com/v2-7ed95890535dbb13bf7c56e00c9108e0" align="middle">
<img src="https://picx.zhimg.com/v2-2b2d4761cd7a0dd97c07bdd6e93bafcc" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Invisible-Ears-at-Your-Fingertips-Acoustic-Eavesdropping-via-Mouse-Sensors"><a href="#Invisible-Ears-at-Your-Fingertips-Acoustic-Eavesdropping-via-Mouse-Sensors" class="headerlink" title="Invisible Ears at Your Fingertips: Acoustic Eavesdropping via Mouse Sensors"></a>Invisible Ears at Your Fingertips: Acoustic Eavesdropping via Mouse Sensors</h2><p><strong>Authors:Mohamad Fakih, Rahul Dharmaji, Youssef Mahmoud, Halima Bouzidi, Mohammad Abdullah Al Faruque</strong></p>
<p>Modern optical mouse sensors, with their advanced precision and high responsiveness, possess an often overlooked vulnerability: they can be exploited for side-channel attacks. This paper introduces Mic-E-Mouse, the first-ever side-channel attack that targets high-performance optical mouse sensors to covertly eavesdrop on users. We demonstrate that audio signals can induce subtle surface vibrations detectable by a mouseâ€™s optical sensor. Remarkably, user-space software on popular operating systems can collect and broadcast this sensitive side channel, granting attackers access to raw mouse data without requiring direct system-level permissions. Initially, the vibration signals extracted from mouse data are of poor quality due to non-uniform sampling, a non-linear frequency response, and significant quantization. To overcome these limitations, Mic-E-Mouse employs a sophisticated end-to-end data filtering pipeline that combines Wiener filtering, resampling corrections, and an innovative encoder-only spectrogram neural filtering technique. We evaluate the attackâ€™s efficacy across diverse conditions, including speaking volume, mouse polling rate and DPI, surface materials, speaker languages, and environmental noise. In controlled environments, Mic-E-Mouse improves the signal-to-noise ratio (SNR) by up to +19 dB for speech reconstruction. Furthermore, our results demonstrate a speech recognition accuracy of roughly 42% to 61% on the AudioMNIST and VCTK datasets. All our code and datasets are publicly accessible on <a target="_blank" rel="noopener" href="https://sites.google.com/view/mic-e-mouse">https://sites.google.com/view/mic-e-mouse</a>.</p>
<blockquote>
<p>ç°ä»£å…‰å­¦é¼ æ ‡ä¼ æ„Ÿå™¨ä»¥å…¶é«˜çº§ç²¾åº¦å’Œé«˜å“åº”æ€§è€Œé—»åï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸€ä¸ªç»å¸¸è¢«å¿½è§†çš„æ¼æ´ï¼šå®ƒä»¬å¯èƒ½è¢«ç”¨äºä¾§ä¿¡é“æ”»å‡»ã€‚æœ¬æ–‡ä»‹ç»äº†Mic-E-Mouseï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹é«˜æ€§èƒ½å…‰å­¦é¼ æ ‡ä¼ æ„Ÿå™¨çš„æ–°å‹ä¾§ä¿¡é“æ”»å‡»ï¼Œå¯ä»¥ç§˜å¯†ç›‘å¬ç”¨æˆ·ã€‚æˆ‘ä»¬è¯æ˜éŸ³é¢‘ä¿¡å·ä¼šè¯±å‘é¼ æ ‡å…‰å­¦ä¼ æ„Ÿå™¨å¯ä»¥æ£€æµ‹åˆ°çš„å¾®å¦™è¡¨é¢æŒ¯åŠ¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæµè¡Œæ“ä½œç³»ç»Ÿä¸Šçš„ç”¨æˆ·ç©ºé—´è½¯ä»¶å¯ä»¥æ”¶é›†å’Œå¹¿æ’­è¿™ç§æ•æ„Ÿçš„ä¾§ä¿¡é“ä¿¡æ¯ï¼Œä»è€Œå…è®¸æ”»å‡»è€…è®¿é—®åŸå§‹é¼ æ ‡æ•°æ®ï¼Œè€Œæ— éœ€è·å¾—ç›´æ¥çš„ç³»ç»Ÿçº§æƒé™ã€‚ç”±äºéå‡åŒ€é‡‡æ ·ã€éçº¿æ€§é¢‘ç‡å“åº”å’Œæ˜¾è‘—çš„é‡åŒ–ï¼Œæœ€åˆä»é¼ æ ‡æ•°æ®ä¸­æå–çš„æŒ¯åŠ¨ä¿¡å·è´¨é‡è¾ƒå·®ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼ŒMic-E-Mouseé‡‡ç”¨äº†ä¸€ç§å…ˆè¿›çš„ç«¯åˆ°ç«¯æ•°æ®è¿‡æ»¤ç®¡é“ï¼Œç»“åˆäº†Wieneræ»¤æ³¢ã€é‡é‡‡æ ·æ ¡æ­£ä»¥åŠåˆ›æ–°çš„ä»…ç¼–ç å™¨å…‰è°±ç¥ç»è¿‡æ»¤æŠ€æœ¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†æ”»å‡»åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è¯´è¯éŸ³é‡ã€é¼ æ ‡è½®è¯¢ç‡å’ŒDPIã€è¡¨é¢ææ–™ã€è¯´è¯äººçš„è¯­è¨€å’Œç¯å¢ƒå™ªéŸ³ã€‚åœ¨å—æ§ç¯å¢ƒä¸­ï¼ŒMic-E-Mouseæé«˜äº†ä¿¡å·ä¸å™ªå£°æ¯”ï¼ˆSNRï¼‰ï¼Œè¯­éŸ³é‡å»ºæé«˜äº†é«˜è¾¾+19åˆ†è´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœåœ¨AudioMNISTå’ŒVCTKæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºçº¦42%è‡³61%çš„è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://sites.google.com/view/mic-e-mouse%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://sites.google.com/view/mic-e-mouseä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13581v2">PDF</a> Appearing in the Annual Computer Security Applications Conference (ACSAC 2025)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç°ä»£é«˜æ€§èƒ½å…‰å­¦é¼ æ ‡ä¼ æ„Ÿå™¨å­˜åœ¨æ˜“è¢«å¿½è§†çš„æ¼æ´ï¼šå¯èƒ½é­å—ä¾§ä¿¡é“æ”»å‡»ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMic-E-Mouseçš„æ–°å‹ä¾§ä¿¡é“æ”»å‡»ï¼Œè¯¥æ”»å‡»é’ˆå¯¹é«˜æ€§èƒ½å…‰å­¦é¼ æ ‡ä¼ æ„Ÿå™¨è¿›è¡Œéšç§˜ç›‘å¬ã€‚æ¼”ç¤ºäº†éŸ³é¢‘ä¿¡å·å¼•èµ·çš„ç»†å¾®è¡¨é¢æŒ¯åŠ¨å¯è¢«é¼ æ ‡å…‰å­¦ä¼ æ„Ÿå™¨æ£€æµ‹åˆ°ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæµè¡Œæ“ä½œç³»ç»Ÿçš„ç”¨æˆ·ç©ºé—´è½¯ä»¶å¯ä»¥æ”¶é›†å’Œå¹¿æ’­è¿™ä¸€æ•æ„Ÿä¾§ä¿¡é“ä¿¡æ¯ï¼Œä½¿æ”»å‡»è€…æ— éœ€ç³»ç»Ÿçº§æƒé™å³å¯è®¿é—®åŸå§‹é¼ æ ‡æ•°æ®ã€‚ä¸ºåº”å¯¹ä»é¼ æ ‡æ•°æ®ä¸­æå–çš„æŒ¯åŠ¨ä¿¡å·è´¨é‡å·®çš„é—®é¢˜ï¼ˆå¦‚éå‡åŒ€é‡‡æ ·ã€éçº¿æ€§é¢‘ç‡å“åº”å’Œé‡å¤§é‡åŒ–ï¼‰ï¼ŒMic-E-Mouseé‡‡ç”¨å…ˆè¿›çš„ç«¯åˆ°ç«¯æ•°æ®è¿‡æ»¤ç®¡é“ï¼Œç»“åˆWienerè¿‡æ»¤ã€é‡æ–°é‡‡æ ·æ ¡æ­£å’Œåˆ›æ–°çš„ä»…ç¼–ç å™¨å…‰è°±ç¥ç»ç½‘ç»œè¿‡æ»¤æŠ€æœ¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†æ”»å‡»åœ¨å¤šç§æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è®²è¯éŸ³é‡ã€é¼ æ ‡è½®è¯¢ç‡å’ŒDPIã€è¡¨é¢ææ–™ã€æ¼”è®²è€…å’Œç¯å¢ƒå™ªéŸ³ã€‚åœ¨æ§åˆ¶ç¯å¢ƒä¸‹ï¼ŒMic-E-Mouseçš„è¯­éŸ³é‡å»ºçš„ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æé«˜äº†é«˜è¾¾+19åˆ†è´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœåœ¨AudioMNISTå’ŒVCTKæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºçº¦42%è‡³61%çš„è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://sites.google.com/view/mic-e-mouse%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://sites.google.com/view/mic-e-mouseä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°ä»£å…‰å­¦é¼ æ ‡ä¼ æ„Ÿå™¨å­˜åœ¨ä¾§ä¿¡é“æ”»å‡»çš„é£é™©ï¼Œæœ¬æ–‡é¦–æ¬¡ä»‹ç»äº†é’ˆå¯¹æ­¤çš„Mic-E-Mouseæ”»å‡»ã€‚</li>
<li>éŸ³é¢‘ä¿¡å·èƒ½é€šè¿‡é¼ æ ‡å…‰å­¦ä¼ æ„Ÿå™¨æ£€æµ‹åˆ°çš„è¡¨é¢æŒ¯åŠ¨è¿›è¡Œéšç§˜ä¼ é€’ã€‚</li>
<li>ç”¨æˆ·ç©ºé—´è½¯ä»¶å¯æ”¶é›†å’Œå¹¿æ’­æ•æ„Ÿä¾§ä¿¡é“ä¿¡æ¯ï¼Œä½¿æ”»å‡»è€…èƒ½è®¿é—®åŸå§‹é¼ æ ‡æ•°æ®ã€‚</li>
<li>Mic-E-Mouseé‡‡ç”¨å¤æ‚çš„æ•°æ®è¿‡æ»¤ç®¡é“ä»¥æé«˜ä»é¼ æ ‡æ•°æ®ä¸­æå–çš„æŒ¯åŠ¨ä¿¡å·è´¨é‡ã€‚</li>
<li>æ”»å‡»åœ¨å¤šç§æ¡ä»¶ä¸‹æœ‰æ•ˆï¼ŒåŒ…æ‹¬è®²è¯éŸ³é‡ã€é¼ æ ‡æ€§èƒ½å’Œç¯å¢ƒå› ç´ ã€‚</li>
<li>åœ¨æ§åˆ¶ç¯å¢ƒä¸‹ï¼ŒMic-E-Mouseçš„è¯­éŸ³é‡å»ºçš„ä¿¡å™ªæ¯”æé«˜æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4980a15314e6d2652c018665a54a2c95" align="middle">
<img src="https://picx.zhimg.com/v2-ba319752925a969a650a736e27e850d7" align="middle">
<img src="https://picx.zhimg.com/v2-115a6422eb1cdc797cbd55220528752b" align="middle">
<img src="https://picx.zhimg.com/v2-9554f96fee565a3c642347c6f538ed05" align="middle">
<img src="https://picx.zhimg.com/v2-f0c95aed9b47de480c4dfb219105a670" align="middle">
<img src="https://picx.zhimg.com/v2-fa2e56d291cb57fb32d97ff5f828236d" align="middle">
<img src="https://picx.zhimg.com/v2-db7c18433778402ec1251c3309397de6" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DualSpeechLM-Towards-Unified-Speech-Understanding-and-Generation-via-Dual-Speech-Token-Modeling-with-Large-Language-Models"><a href="#DualSpeechLM-Towards-Unified-Speech-Understanding-and-Generation-via-Dual-Speech-Token-Modeling-with-Large-Language-Models" class="headerlink" title="DualSpeechLM: Towards Unified Speech Understanding and Generation via Dual Speech Token Modeling with Large Language Models"></a>DualSpeechLM: Towards Unified Speech Understanding and Generation via Dual Speech Token Modeling with Large Language Models</h2><p><strong>Authors:Yuanyuan Wang, Dongchao Yang, Yiwen Shao, Hangting Chen, Jiankun Zhao, Zhiyong Wu, Helen Meng, Xixin Wu</strong></p>
<p>Extending pre-trained text Large Language Models (LLMs)â€™s speech understanding or generation abilities by introducing various effective speech tokens has attracted great attention in the speech community. However, building a unified speech understanding and generation model still faces the following challenges: (1) Due to the huge modality gap between speech and text tokens, extending text LLMs to unified speech LLMs relies on large-scale paired data for fine-tuning, and (2) Generation and understanding tasks prefer information at different levels, e.g., generation benefits from detailed acoustic features, while understanding favors high-level semantics. This divergence leads to difficult performance optimization in one unified model. To solve these challenges, in this paper, we present two key insights in speech tokenization and speech language modeling. Specifically, we first propose an Understanding-driven Speech Tokenizer (USTokenizer), which extracts high-level semantic information essential for accomplishing understanding tasks using text LLMs. In this way, USToken enjoys better modality commonality with text, which reduces the difficulty of modality alignment in adapting text LLMs to speech LLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that concurrently models USToken as input and acoustic token as output within a unified, end-to-end framework, seamlessly integrating speech understanding and generation capabilities. Furthermore, we propose a novel semantic supervision loss and a Chain-of-Condition (CoC) strategy to stabilize model training and enhance speech generation performance. Experimental results demonstrate that our proposed approach effectively fosters a complementary relationship between understanding and generation tasks, highlighting the promising strategy of mutually enhancing both tasks in one unified model.</p>
<blockquote>
<p>å°†é¢„è®­ç»ƒçš„æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¯­éŸ³ç†è§£æˆ–ç”Ÿæˆèƒ½åŠ›é€šè¿‡å¼•å…¥å„ç§æœ‰æ•ˆçš„è¯­éŸ³æ ‡è®°è¿›è¡Œæ‰©å±•ï¼Œåœ¨è¯­éŸ³é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œæ„å»ºç»Ÿä¸€çš„è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ¨¡å‹ä»ç„¶é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç”±äºè¯­éŸ³å’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´å­˜åœ¨å·¨å¤§çš„æ¨¡æ€å·®è·ï¼Œå°†æ–‡æœ¬LLMsæ‰©å±•åˆ°ç»Ÿä¸€çš„è¯­éŸ³LLMsä¾èµ–äºå¤§è§„æ¨¡é…å¯¹æ•°æ®è¿›è¡Œå¾®è°ƒï¼›ï¼ˆ2ï¼‰ç”Ÿæˆå’Œç†è§£ä»»åŠ¡åå¥½ä¸åŒå±‚çº§çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼Œç”Ÿæˆå—ç›Šäºè¯¦ç»†çš„å£°å­¦ç‰¹å¾ï¼Œè€Œç†è§£åˆ™åå‘äºé«˜çº§è¯­ä¹‰ã€‚è¿™ç§åˆ†æ­§å¯¼è‡´åœ¨ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­éš¾ä»¥è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è¯­éŸ³æ ‡è®°åŒ–å’Œè¯­éŸ³è¯­è¨€æ¨¡å‹çš„ä¸¤ä¸ªå…³é”®è§è§£ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§ç†è§£é©±åŠ¨çš„è¯­éŸ³æ ‡è®°å™¨ï¼ˆUSTokenizerï¼‰ï¼Œå®ƒä½¿ç”¨æ–‡æœ¬LLMsæå–å®Œæˆç†è§£ä»»åŠ¡æ‰€éœ€çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒUSTokenä¸æ–‡æœ¬çš„æ¨¡æ€å…±æ€§æ›´å¥½ï¼Œé™ä½äº†å°†æ–‡æœ¬LLMsé€‚åº”åˆ°è¯­éŸ³LLMsæ—¶çš„æ¨¡æ€å¯¹é½éš¾åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†DualSpeechLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŒæ ‡è®°å»ºæ¨¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¸€ä¸ªç»Ÿä¸€ã€ç«¯åˆ°ç«¯çš„æ¡†æ¶å†…ï¼ŒåŒæ—¶ä»¥USTokenä½œä¸ºè¾“å…¥å’ŒéŸ³é¢‘æ ‡è®°ä½œä¸ºè¾“å‡ºè¿›è¡Œå»ºæ¨¡ï¼Œæ— ç¼é›†æˆäº†è¯­éŸ³ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯­ä¹‰ç›‘ç£æŸå¤±å’Œæ¡ä»¶é“¾ï¼ˆCoCï¼‰ç­–ç•¥ï¼Œä»¥ç¨³å®šæ¨¡å‹è®­ç»ƒå¹¶å¢å¼ºè¯­éŸ³ç”Ÿæˆæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æœ‰æ•ˆåœ°ä¿ƒè¿›äº†ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„äº’è¡¥å…³ç³»ï¼Œçªæ˜¾äº†åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ç›¸äº’å¢å¼ºè¿™ä¸¤ä¸ªä»»åŠ¡çš„ç­–ç•¥å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08961v3">PDF</a> Accepted by AAAI 2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©å±•é¢„è®­ç»ƒæ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¯­éŸ³è¯†åˆ«æˆ–ç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡å¼•å…¥å„ç§æœ‰æ•ˆçš„è¯­éŸ³ä»¤ç‰Œï¼Œåœ¨è¯­éŸ³é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œæ„å»ºç»Ÿä¸€çš„è¯­éŸ³ç†è§£ä¸ç”Ÿæˆæ¨¡å‹ä»é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼šä¸€æ˜¯ç”±äºè¯­éŸ³å’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´å­˜åœ¨å·¨å¤§çš„æ¨¡æ€å·®è·ï¼Œå°†æ–‡æœ¬LLMæ‰©å±•åˆ°ç»Ÿä¸€çš„è¯­éŸ³LLMä¾èµ–äºå¤§è§„æ¨¡é…å¯¹æ•°æ®è¿›è¡Œå¾®è°ƒï¼›äºŒæ˜¯ç”Ÿæˆå’Œç†è§£ä»»åŠ¡åå¥½ä¸åŒå±‚çº§çš„ä¿¡æ¯ï¼Œä¾‹å¦‚åœ¨ç”Ÿæˆä¸­å—ç›Šäºè¯¦ç»†çš„å£°å­¦ç‰¹å¾ï¼Œè€Œç†è§£åˆ™æ›´åå‘äºé«˜çº§è¯­ä¹‰ã€‚è¿™ç§åˆ†æ­§å¯¼è‡´åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­éš¾ä»¥è¿›è¡Œä¼˜åŒ–ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªå…³äºè¯­éŸ³ä»¤ç‰ŒåŒ–å’Œè¯­éŸ³è¯­è¨€å»ºæ¨¡çš„å…³é”®è§è§£ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç†è§£é©±åŠ¨çš„è¯­éŸ³åˆ†è¯å™¨ï¼ˆUSTokenizerï¼‰ï¼Œå®ƒä½¿ç”¨æ–‡æœ¬LLMsæå–å¯¹å®Œæˆç†è§£ä»»åŠ¡è‡³å…³é‡è¦çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒUSTokenä¸æ–‡æœ¬çš„æ¨¡æ€å…±æ€§æ›´å¥½ï¼Œé™ä½äº†å°†æ–‡æœ¬LLMsé€‚åº”åˆ°è¯­éŸ³LLMsæ—¶æ¨¡æ€å¯¹é½çš„éš¾åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†DualSpeechLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŒä»¤ç‰Œå»ºæ¨¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¸€ä¸ªç»Ÿä¸€ã€ç«¯åˆ°ç«¯çš„æ¡†æ¶å†…ï¼ŒåŒæ—¶ä»¥USTokenä½œä¸ºè¾“å…¥å’ŒéŸ³é¢‘ä»¤ç‰Œä½œä¸ºè¾“å‡ºè¿›è¡Œå»ºæ¨¡ï¼Œæ— ç¼é›†æˆäº†è¯­éŸ³ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯­ä¹‰ç›‘ç£æŸå¤±å’Œæ¡ä»¶é“¾ï¼ˆCoCï¼‰ç­–ç•¥ï¼Œä»¥ç¨³å®šæ¨¡å‹è®­ç»ƒå¹¶å¢å¼ºè¯­éŸ³ç”Ÿæˆæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æœ‰æ•ˆåœ°ä¿ƒè¿›äº†ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„äº’è¡¥å…³ç³»ï¼Œçªæ˜¾äº†åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ç›¸äº’å¢å¼ºè¿™ä¸¤ä¸ªä»»åŠ¡çš„ç­–ç•¥å‰æ™¯ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ol>
<li>æ‰©å±•é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åˆ°è¯­éŸ³é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å­˜åœ¨æ¨¡æ€å·®è·å’Œç”Ÿæˆç†è§£ä»»åŠ¡åå¥½ä¸åŒå±‚çº§ä¿¡æ¯çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºç†è§£é©±åŠ¨çš„è¯­éŸ³åˆ†è¯å™¨ï¼ˆUSTokenizerï¼‰ï¼Œæå–é«˜çº§è¯­ä¹‰ä¿¡æ¯ç”¨äºç†è§£ä»»åŠ¡ï¼Œå¢å¼ºä¸æ–‡æœ¬çš„æ¨¡æ€å…±æ€§ã€‚</li>
<li>ä»‹ç»DualSpeechLMæ¡†æ¶ï¼Œç»Ÿä¸€å»ºæ¨¡è¯­éŸ³ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½¿ç”¨USTokenå’ŒéŸ³é¢‘ä»¤ç‰Œã€‚</li>
<li>æå‡ºæ–°çš„è¯­ä¹‰ç›‘ç£æŸå¤±å’Œæ¡ä»¶é“¾ï¼ˆCoCï¼‰ç­–ç•¥ï¼Œç¨³å®šæ¨¡å‹è®­ç»ƒå¹¶æå‡è¯­éŸ³ç”Ÿæˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2206b1455c63155755579852387ab85a" align="middle">
<img src="https://picx.zhimg.com/v2-a44d17111f663b828e788cde15e32827" align="middle">
<img src="https://picx.zhimg.com/v2-463362a4c36c7d814fbf850a76648737" align="middle">
<img src="https://picx.zhimg.com/v2-0727f96a0c8fd4a3a910cee56a9f51a0" align="middle">
<img src="https://picx.zhimg.com/v2-8f8fabd84dc1f35c5df7ddcec36c0b77" align="middle">
<img src="https://picx.zhimg.com/v2-587af9e8dec45d69ba126a400bf9f3e1" align="middle">
<img src="https://picx.zhimg.com/v2-7ae9ffca755b7d9b9dfa01ad198c7a06" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PERTINENCE-Input-based-Opportunistic-Neural-Network-Dynamic-Execution"><a href="#PERTINENCE-Input-based-Opportunistic-Neural-Network-Dynamic-Execution" class="headerlink" title="PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution"></a>PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution</h2><p><strong>Authors:Omkar Shende, Gayathri Ananthanarayanan, Marcello Traiola</strong></p>
<p>Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable ability to model complex patterns across various domains such as computer vision, speech recognition, robotics, etc. While large DNN models are often more accurate than simpler, lightweight models, they are also resource- and energy-hungry. Hence, it is imperative to design methods to reduce reliance on such large models without significant degradation in output accuracy. The high computational cost of these models is often necessary only for a reduced set of challenging inputs, while lighter models can handle most simple ones. Thus, carefully combining properties of existing DNN models in a dynamic, input-based way opens opportunities to improve efficiency without impacting accuracy. In this work, we introduce PERTINENCE, a novel online method designed to analyze the complexity of input features and dynamically select the most suitable model from a pre-trained set to process a given input effectively. To achieve this, we employ a genetic algorithm to explore the training space of an ML-based input dispatcher, enabling convergence towards the Pareto front in the solution space that balances overall accuracy and computational efficiency. We showcase our approach on state-of-the-art Convolutional Neural Networks (CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers (ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCEâ€™s ability to provide alternative solutions to existing state-of-the-art models in terms of trade-offs between accuracy and number of operations. By opportunistically selecting among models trained for the same task, PERTINENCE achieves better or comparable accuracy with up to 36% fewer operations.</p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç”±äºå…¶èƒ½å¤Ÿåœ¨è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ã€æœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸå¯¹å¤æ‚æ¨¡å¼è¿›è¡Œå»ºæ¨¡çš„å‡ºè‰²èƒ½åŠ›è€Œæ— å¤„ä¸åœ¨ã€‚è™½ç„¶å¤§å‹DNNæ¨¡å‹çš„å‡†ç¡®æ€§é€šå¸¸é«˜äºç®€å•è½»é‡çº§æ¨¡å‹ï¼Œä½†å®ƒä»¬ä¹Ÿéœ€è¦å¤§é‡çš„èµ„æºå’Œèƒ½æºã€‚å› æ­¤ï¼Œè®¾è®¡æ–¹æ³•æ¥å‡å°‘å¯¹è¿™ç±»å¤§å‹æ¨¡å‹çš„ä¾èµ–ï¼ŒåŒæ—¶ä¸ä¼šæ˜¾è‘—é™ä½è¾“å‡ºå‡†ç¡®æ€§ï¼Œè¿™æ˜¯è‡³å…³é‡è¦çš„ã€‚è¿™äº›æ¨¡å‹çš„é«˜è®¡ç®—æˆæœ¬å¾€å¾€ä»…å¯¹äºå‡å°‘çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„è¾“å…¥é›†æ˜¯å¿…è¦çš„ï¼Œè€Œè¾ƒè½»çš„æ¨¡å‹å¯ä»¥å¤„ç†å¤§å¤šæ•°ç®€å•çš„è¾“å…¥ã€‚å› æ­¤ï¼Œä»¥åŠ¨æ€ã€åŸºäºè¾“å…¥çš„æ–¹å¼è°¨æ…åœ°ç»“åˆç°æœ‰DNNæ¨¡å‹çš„å±æ€§ï¼Œæœ‰åŠ©äºæé«˜æ•ˆç‡è€Œä¸å½±å“å‡†ç¡®æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†PERTINENCEï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åœ¨çº¿æ–¹æ³•ï¼Œæ—¨åœ¨åˆ†æè¾“å…¥ç‰¹å¾å¤æ‚æ€§ï¼Œå¹¶ä»é¢„è®­ç»ƒæ¨¡å‹é›†ä¸­åŠ¨æ€é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹æ¥æœ‰æ•ˆå¤„ç†ç»™å®šè¾“å…¥ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é‡‡ç”¨é—ä¼ ç®—æ³•æ¥æ¢ç´¢åŸºäºæœºå™¨å­¦ä¹ çš„è¾“å…¥è°ƒåº¦å™¨çš„è®­ç»ƒç©ºé—´ï¼Œä½¿è§£å†³æ–¹æ¡ˆåœ¨æ•´ä½“å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œæœç€Paretoå‰æ²¿æ”¶æ•›ã€‚æˆ‘ä»¬åœ¨CIFAR-10å’ŒCIFAR-100ä¸Šç»è¿‡è®­ç»ƒçš„æœ€æ–°å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä»¥åŠTinyImageNetæ•°æ®é›†ä¸Šè®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æŠ¥å‘Šçš„ç»“æœæ˜¾ç¤ºäº†PERTINENCEåœ¨å‡†ç¡®æ€§å’Œè¿ç®—æ¬¡æ•°ä¹‹é—´çš„æƒè¡¡æ–¹é¢ä¸ºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹æä¾›æ›¿ä»£è§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨æœ‰ç›¸åŒä»»åŠ¡çš„æ¨¡å‹ä¸­åšå‡ºé€‰æ‹©ï¼ŒPERTINENCEå¯ä»¥åœ¨ä¿æŒç›¸å½“æˆ–æ›´é«˜çš„å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå‡å°‘é«˜è¾¾36%çš„æ“ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01695v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ã€æœºå™¨äººç­‰é¢†åŸŸï¼Œå…¶å‡ºè‰²çš„æ¨¡å¼è¯†åˆ«èƒ½åŠ›å¼•äººæ³¨ç›®ã€‚ä½†å¤§å‹æ¨¡å‹å¸¸å¸¸æ¶ˆè€—å¤§é‡çš„èµ„æºå’Œèƒ½æºã€‚ç ”ç©¶äººä»‹ç»äº†ä¸€ç§æ–°çš„åœ¨çº¿æ–¹æ³•PERTINENCEï¼Œè¯¥æ–¹æ³•èƒ½æ ¹æ®è¾“å…¥ç‰¹å¾å¤æ‚åº¦åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œä»è€Œæé«˜æ•ˆç‡è€Œä¸å½±å“å‡†ç¡®æ€§ã€‚é€šè¿‡é—ä¼ ç®—æ³•æ¢ç´¢MLè¾“å…¥è°ƒåº¦å™¨çš„è®­ç»ƒç©ºé—´ï¼Œä½¿è§£å†³æ–¹æ¡ˆåœ¨æ€»ä½“å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´è¾¾åˆ°å¹³è¡¡ã€‚åœ¨CIFAR-10ã€CIFAR-100æ•°æ®é›†ä¸Šçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒTinyImageNetæ•°æ®é›†ä¸Šçš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPERTINENCEèƒ½åœ¨ä¿è¯å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œå‡å°‘é«˜è¾¾36%çš„è®¡ç®—æ“ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹è™½ç„¶å‡†ç¡®åº¦é«˜ï¼Œä½†èµ„æºå’Œèƒ½æºæ¶ˆè€—å¤§ã€‚</li>
<li>è¾“å…¥ç‰¹å¾çš„å¤æ‚åº¦å¯¹é€‰æ‹©é€‚åˆçš„æ¨¡å‹å…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>PERTINENCEæ˜¯ä¸€ç§èƒ½å¤Ÿæ ¹æ®è¾“å…¥ç‰¹å¾å¤æ‚åº¦åŠ¨æ€é€‰æ‹©æ¨¡å‹çš„åœ¨çº¿æ–¹æ³•ã€‚</li>
<li>PERTINENCEåˆ©ç”¨é—ä¼ ç®—æ³•åœ¨è®­ç»ƒç©ºé—´ä¸­å¯»æ‰¾å¹³è¡¡å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åœ¨CIFARå’ŒTinyImageNetæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†PERTINENCEçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>PERTINENCEèƒ½åœ¨ä¿è¯å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œå‡å°‘è®¡ç®—æ“ä½œï¼Œæœ€é«˜å¯è¾¾36%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01695">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96fb27a61e5d5d58f3aba25d52bc3db2" align="middle">
<img src="https://picx.zhimg.com/v2-25d28505aed66583e4c1561a3e3ee777" align="middle">
<img src="https://picx.zhimg.com/v2-57112a68c60c8169277f21bb09956daa" align="middle">
<img src="https://picx.zhimg.com/v2-ae5367733e38811edaab83c66310bb52" align="middle">
<img src="https://picx.zhimg.com/v2-a217e1e9babebed7c569a1cf173e1cbb" align="middle">
<img src="https://picx.zhimg.com/v2-92ede9a59e3628a14b006af7e7a7a54b" align="middle">
<img src="https://picx.zhimg.com/v2-4f2a2135ebc9485583e5f72f0329d987" align="middle">
<img src="https://picx.zhimg.com/v2-06072be24aa1254cf5e4124ed0b1637c" align="middle">
<img src="https://picx.zhimg.com/v2-f2c8ef3af4a4c69bf7af7f9a3b4df7b8" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c0a4a53c4138cff44f029033b067c318" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Whose Narrative is it Anyway? A KV Cache Manipulation Attack
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-31e8e9f9af8e409efde1ee9bf67e343c" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  SAGE Saliency-Guided Contrastive Embeddings
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
