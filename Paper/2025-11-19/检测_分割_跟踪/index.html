<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-36e8afa508b37d5ae96dc1d0a25e45b0')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    44 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="Referring-Camouflaged-Object-Detection-With-Multi-Context-Overlapped-Windows-Cross-Attention"><a href="#Referring-Camouflaged-Object-Detection-With-Multi-Context-Overlapped-Windows-Cross-Attention" class="headerlink" title="Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention"></a>Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention</h2><p><strong>Authors:Yu Wen, Shuyong Gao, Shuping Zhang, Miao Huang, Lili Tao, Han Yang, Haozhe Xing, Lihe Zhang, Boxue Hou</strong></p>
<p>Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.</p>
<blockquote>
<p>å‚è€ƒä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆRef-CODï¼‰æ—¨åœ¨é€šè¿‡ç»“åˆå‚è€ƒä¿¡æ¯ï¼ˆå¦‚å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼‰æ¥è¯†åˆ«éšè—çš„ç›®æ ‡ã€‚ä¹‹å‰çš„ç ”ç©¶å°†å«æœ‰æ˜¾è‘—ç›®æ ‡çš„å‚è€ƒå›¾åƒè½¬æ¢ä¸ºä¸€ç»´æç¤ºï¼Œå–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚æˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡èåˆä¸°å¯Œæ˜¾è‘—çš„å›¾åƒç‰¹å¾å’Œä¼ªè£…ç›®æ ‡ç‰¹å¾çš„å¤šä¸Šä¸‹æ–‡æ¥æå‡æ€§èƒ½çš„æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RFMNetï¼Œå®ƒåˆ©ç”¨å‚è€ƒæ˜¾è‘—å›¾åƒå¤šä¸ªç¼–ç é˜¶æ®µçš„ç‰¹ç‚¹ï¼Œå¹¶åœ¨ç›¸åº”çš„ç¼–ç é˜¶æ®µä¸ä¼ªè£…ç‰¹å¾è¿›è¡Œäº¤äº’èåˆã€‚é‰´äºæ˜¾è‘—ç›®æ ‡å›¾åƒä¸­çš„ç‰¹å¾åŒ…å«ä¸°å¯Œçš„ç›®æ ‡ç›¸å…³è¯¦ç»†ä¿¡æ¯ï¼Œåœ¨å±€éƒ¨åŒºåŸŸè¿›è¡Œç‰¹å¾èåˆæ›´æœ‰åˆ©äºæ£€æµ‹ä¼ªè£…ç›®æ ‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡å çª—å£äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºå‚è€ƒç‰¹å¾æ›´åŠ å…³æ³¨å±€éƒ¨ä¿¡æ¯åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å‚è€ƒç‰¹å¾èšåˆï¼ˆRFAï¼‰æ¨¡å—ï¼Œä»¥é€æ­¥è§£ç å’Œåˆ†å‰²ä¼ªè£…çš„ç›®æ ‡ã€‚åœ¨Ref-CODåŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13249v1">PDF</a> 12 pages, 7figures, This work is supported by National Nature Science Foundation of China (Grant No. 62203291)</p>
<p><strong>Summary</strong>ï¼šå‚è€ƒä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆRef-CODï¼‰é€šè¿‡èåˆå‚è€ƒä¿¡æ¯ï¼Œå¦‚å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œè¯†åˆ«éšè—ç›®æ ‡ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†é€šè¿‡èåˆä¸°å¯Œæ˜¾è‘—çš„å›¾åƒç‰¹å¾å’Œä¼ªè£…ç›®æ ‡ç‰¹å¾çš„å¤šä¸Šä¸‹æ–‡å¢å¼ºæ€§èƒ½çš„æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RFMNetï¼Œå®ƒåˆ©ç”¨å‚è€ƒæ˜¾è‘—å›¾åƒå¤šä¸ªç¼–ç é˜¶æ®µçš„ç‰¹ç‚¹ï¼Œä¸ä¼ªè£…ç‰¹å¾åœ¨ç›¸åº”ç¼–ç é˜¶æ®µè¿›è¡Œäº¤äº’èåˆã€‚å±€éƒ¨åŒºåŸŸè¿›è¡Œç‰¹å¾èåˆæœ‰åŠ©äºè·å–æ›´å¤šå…³äºç›®æ ‡çš„è¯¦ç»†ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é‡å çª—å£äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹æ›´åŠ å…³æ³¨åŸºäºå‚è€ƒç‰¹å¾çš„å±€éƒ¨ä¿¡æ¯åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†å‚è€ƒç‰¹å¾èšåˆï¼ˆRFAï¼‰æ¨¡å—ï¼Œé€æ­¥è§£ç å’Œåˆ†å‰²ä¼ªè£…ç›®æ ‡ã€‚åœ¨Ref-CODåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Ref-CODæ—¨åœ¨é€šè¿‡èåˆå‚è€ƒä¿¡æ¯ï¼Œå¦‚å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œæ¥è¯†åˆ«éšè—ç›®æ ‡ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†RFMNetæ¨¡å‹ï¼Œé€šè¿‡å¤šä¸Šä¸‹æ–‡èåˆä¸°å¯Œæ˜¾è‘—çš„å›¾åƒç‰¹å¾å’Œä¼ªè£…ç›®æ ‡ç‰¹å¾æ¥æå‡æ€§èƒ½ã€‚</li>
<li>RFMNetåˆ©ç”¨å‚è€ƒå›¾åƒçš„å¤šä¸ªç¼–ç é˜¶æ®µçš„ç‰¹ç‚¹ï¼Œå¹¶ä¸ä¼ªè£…ç‰¹å¾è¿›è¡Œäº¤äº’èåˆã€‚</li>
<li>å±€éƒ¨åŒºåŸŸç‰¹å¾èåˆæœ‰åŠ©äºè·å–æ›´å¤šå…³äºç›®æ ‡çš„è¯¦ç»†ä¿¡æ¯ã€‚</li>
<li>é‡å çª—å£äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹æ›´åŠ å…³æ³¨åŸºäºå‚è€ƒç‰¹å¾çš„å±€éƒ¨ä¿¡æ¯åŒ¹é…ã€‚</li>
<li>å‚è€ƒç‰¹å¾èšåˆï¼ˆRFAï¼‰æ¨¡å—ç”¨äºé€æ­¥è§£ç å’Œåˆ†å‰²ä¼ªè£…ç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8839d67a8693c8f4b7ff511d03c6e3e5" align="middle">
<img src="https://picx.zhimg.com/v2-60e77b55de521e8a6dc150a7937abc22" align="middle">
<img src="https://picx.zhimg.com/v2-36e8afa508b37d5ae96dc1d0a25e45b0" align="middle">
<img src="https://picx.zhimg.com/v2-5b934e377c5c82e9c589437a32bfcbf1" align="middle">
<img src="https://picx.zhimg.com/v2-ddd7ae75350976f98f3c7d6fac3e9e45" align="middle">
<img src="https://picx.zhimg.com/v2-48b70d4f2b24a7d928285174ff5ab48f" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="C3Net-Context-Contrast-Network-for-Camouflaged-Object-Detection"><a href="#C3Net-Context-Contrast-Network-for-Camouflaged-Object-Detection" class="headerlink" title="C3Net: Context-Contrast Network for Camouflaged Object Detection"></a>C3Net: Context-Contrast Network for Camouflaged Object Detection</h2><p><strong>Authors:Baber Jan, Aiman H. El-Maleh, Abdul Jabbar Siddiqui, Abdul Bais, Saeed Anwar</strong></p>
<p>Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at <a target="_blank" rel="noopener" href="https://github.com/Baber-Jan/C3Net">https://github.com/Baber-Jan/C3Net</a>.</p>
<blockquote>
<p>éšè”½ç‰©ä½“æ£€æµ‹çš„ä»»åŠ¡æ˜¯è¯†åˆ«é€šè¿‡ç›¸ä¼¼é¢œè‰²ã€çº¹ç†å’Œå›¾æ¡ˆæ— ç¼èå…¥å…¶å‘¨å›´ç¯å¢ƒçš„ç‰©ä½“ã€‚è¿™ä¸€ä»»åŠ¡å¯¹ä¼ ç»Ÿåˆ†å‰²æ–¹æ³•å’Œç°ä»£åŸºç¡€æ¨¡å‹éƒ½æ„æˆäº†æŒ‘æˆ˜ï¼Œåœ¨éšè”½ç‰©ä½“æ–¹é¢è¿™äº›æ¨¡å‹çš„è¡¨ç°å°¤ä¸ºç³Ÿç³•ã€‚æˆ‘ä»¬ç¡®å®šäº†CODä¸­çš„å…­ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šå†…åœ¨ç›¸ä¼¼æ€§ã€è¾¹ç¼˜æ–­è£‚ã€æç«¯å°ºåº¦å˜åŒ–ã€ç¯å¢ƒå¤æ‚æ€§ã€ä¸Šä¸‹æ–‡ä¾èµ–æ€§å’Œæ˜¾è‘—éšè”½ç‰©ä½“è¯†åˆ«ã€‚è¿™äº›æŒ‘æˆ˜ç»å¸¸åŒæ—¶å­˜åœ¨å¹¶å¢åŠ äº†æ£€æµ‹çš„éš¾åº¦ï¼Œéœ€è¦å…¨é¢çš„æ¶æ„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†C3Netï¼Œå®ƒé€šè¿‡ä¸“é—¨çš„åŒè·¯å¾„è§£ç å™¨æ¶æ„æ¥è§£å†³æ‰€æœ‰æŒ‘æˆ˜ã€‚è¾¹ç¼˜ç»†åŒ–è·¯å¾„é‡‡ç”¨æ¢¯åº¦åˆå§‹åŒ–çš„è¾¹ç¼˜å¢å¼ºæ¨¡å—æ¥ä»æ—©æœŸç‰¹å¾ä¸­æ¢å¤ç²¾ç¡®è¾¹ç•Œã€‚ä¸Šä¸‹æ–‡å®šä½è·¯å¾„åˆ©ç”¨æˆ‘ä»¬æ–°é¢–çš„åŸºäºå›¾åƒçš„ä¸Šä¸‹æ–‡æŒ‡å¯¼æœºåˆ¶ï¼Œå®ç°å†…åœ¨æ˜¾è‘—æ€§æŠ‘åˆ¶ï¼Œæ— éœ€å¤–éƒ¨æ¨¡å‹ã€‚æ³¨æ„åŠ›èåˆæ¨¡å—ååŒç»“åˆäº†è¿™ä¸¤ä¸ªè·¯å¾„ï¼Œé€šè¿‡ç©ºé—´é—¨æ§å®ç°ã€‚C3Netåœ¨COD10Kï¼ˆS-measureä¸º0.898ï¼‰ã€CAMOï¼ˆS-measureä¸º0.904ï¼‰å’ŒNC4Kï¼ˆS-measureä¸º0.913ï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆçš„å¤„ç†é€Ÿåº¦ã€‚C3Netè¯æ˜ï¼Œå¤æ‚çš„ã€å¤šæ–¹é¢çš„æ£€æµ‹æŒ‘æˆ˜éœ€è¦æ¶æ„åˆ›æ–°ï¼Œéœ€è¦ä¸“é—¨çš„ç»„ä»¶ååŒå·¥ä½œä»¥å®ç°å…¨é¢çš„è¦†ç›–ï¼Œè¶…è¶Šå±€éƒ¨æ”¹è¿›ã€‚ä»£ç ã€æ¨¡å‹æƒé‡å’Œç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Baber-Jan/C3Net%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Baber-Jan/C3Netæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12627v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šä¼ªè£…ç›®æ ‡æ£€æµ‹æ—¨åœ¨è¯†åˆ«ä¸å‘¨å›´ç¯å¢ƒé€šè¿‡é¢œè‰²ã€çº¹ç†å’Œå›¾æ¡ˆæ— ç¼èåˆçš„ç›®æ ‡ã€‚ä¼ ç»Ÿåˆ†å‰²æ–¹æ³•å’Œç°ä»£åŸºç¡€æ¨¡å‹åœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºå…­å¤§æŒ‘æˆ˜ï¼Œå¹¶è®¾è®¡C3Netè§£å†³æ‰€æœ‰æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¾¹ç¼˜ä¼˜åŒ–è·¯å¾„å’Œç¯å¢ƒå®šä½è·¯å¾„ç­‰åˆ›æ–°æ¶æ„ï¼Œå®ç°äº†é«˜æ•ˆã€å‡†ç¡®çš„ä¼ªè£…ç›®æ ‡æ£€æµ‹ã€‚æ¨¡å‹æ€§èƒ½ä¼˜å¼‚ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€æ–°çŠ¶æ€è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>ä¼ªè£…ç›®æ ‡æ£€æµ‹è¯†åˆ«ä¸å‘¨å›´ç¯å¢ƒèåˆçš„ç›®æ ‡ï¼Œæ¶‰åŠé¢œè‰²ã€çº¹ç†å’Œå›¾æ¡ˆçš„ç›¸ä¼¼æ€§ã€‚</li>
<li>ä¼ ç»Ÿåˆ†å‰²æ–¹æ³•å’Œç°ä»£åŸºç¡€æ¨¡å‹åœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†å…­å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å†…åœ¨ç›¸ä¼¼æ€§ã€è¾¹ç¼˜ç ´åç­‰ã€‚</li>
<li>C3Neté€šè¿‡ä¸“ä¸šåŒ–çš„åŒè·¯å¾„è§£ç å™¨æ¶æ„è§£å†³æ‰€æœ‰æŒ‘æˆ˜ã€‚</li>
<li>C3NetåŒ…æ‹¬è¾¹ç¼˜ä¼˜åŒ–è·¯å¾„å’Œç¯å¢ƒå®šä½è·¯å¾„ç­‰åˆ›æ–°ç»„ä»¶ã€‚</li>
<li>C3Netå®ç°äº†é«˜æ•ˆã€å‡†ç¡®çš„ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€æ–°çŠ¶æ€è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-933b0c2cce59ebc4fe117ecb0d82aeaf" align="middle">
<img src="https://picx.zhimg.com/v2-4aea9eb45c0f1ee47ad8dfd9a5d28184" align="middle">
<img src="https://picx.zhimg.com/v2-8cdf571537b9ed1d587e1d8c61e9b6bf" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Evaluation-of-Attention-Mechanisms-in-U-Net-Architectures-for-Semantic-Segmentation-of-Brazilian-Rock-Art-Petroglyphs"><a href="#Evaluation-of-Attention-Mechanisms-in-U-Net-Architectures-for-Semantic-Segmentation-of-Brazilian-Rock-Art-Petroglyphs" class="headerlink" title="Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs"></a>Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs</h2><p><strong>Authors:Leonardi Melo, LuÃ­s Gustavo, Dimmy MagalhÃ£es, Lucciani Vieira, Mauro AraÃºjo</strong></p>
<p>This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the PoÃ§o da Bebidinha Archaeological Complex, PiauÃ­, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.</p>
<blockquote>
<p>æœ¬æ–‡å¯¹æ¯”åˆ†æäº†ä¸‰ç§åŸºäºU-Netçš„æ¶æ„ï¼Œç”¨äºå·´è¥¿è€ƒå¤é—å€å²©çŸ³è‰ºæœ¯æµ®é›•çš„è¯­ä¹‰åˆ†å‰²ã€‚æ‰€ç ”ç©¶çš„æ¶æ„åŒ…æ‹¬ï¼š(1) BEGL-UNetï¼Œå¸¦æœ‰è¾¹ç•Œå¢å¼ºé«˜æ–¯æŸå¤±å‡½æ•°ï¼› (2) èåˆæ®‹å·®å—å’Œé—¨æ§æ³¨æ„æœºåˆ¶çš„Attention-Residual BEGL-UNetï¼›ä»¥åŠ(3) åŸºäºå·ç§¯å—æ³¨æ„æ¨¡å—çš„Spatial Channel Attention BEGL-UNetã€‚æ‰€æœ‰å®ç°éƒ½é‡‡ç”¨äº†ç»“åˆäºŒå…ƒäº¤å‰ç†µä¸é«˜æ–¯è¾¹ç¼˜å¢å¼ºçš„BEGLæŸå¤±å‡½æ•°ã€‚å®éªŒé‡‡ç”¨å·´è¥¿çš®å¥¥å›¾è´æ¯”è¿ªå“ˆè€ƒå¤é—å€çš„å›¾åƒè¿›è¡Œäº”æŠ˜äº¤å‰éªŒè¯ã€‚åœ¨æ‰€æœ‰è¿™äº›æ¶æ„ä¸­ï¼ŒAttention-Residual BEGL-UNetå–å¾—äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ï¼Œå…¶Diceå¾—åˆ†ä¸º0.710ï¼ŒéªŒè¯æŸå¤±ä¸º0.067ï¼Œå¬å›ç‡æœ€é«˜ï¼Œä¸º0.854ã€‚Spatial Channel Attention BEGL-UNetçš„DSCå¾—åˆ†è¾¾åˆ°0.707ï¼Œå¬å›ç‡ä¸º0.857ï¼Œè¡¨ç°ç›¸å½“ã€‚åŸºçº¿BEGL-UNetçš„DSCå¾—åˆ†ä¸º0.690ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯¹äºè€ƒå¤é—äº§æ•°å­—åŒ–ä¿æŠ¤ï¼Œæ³¨æ„åŠ›æœºåˆ¶æ˜¯æœ‰æ•ˆçš„ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒDiceå¾—åˆ†æé«˜äº†2.5-2.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11959v1">PDF</a> 14 pages, 8 figures. Preprint submitted to arXiv</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶å¯¹æ¯”åˆ†æäº†ä¸‰ç§åŸºäºU-Netçš„æ¶æ„ï¼Œç”¨äºå·´è¥¿è€ƒå¤é—å€å²©çŸ³è‰ºæœ¯å²©ç”»çš„è¯­ä¹‰åˆ†å‰²ã€‚å®éªŒåœ¨å·´è¥¿çš®å¥¥ä¼Šå·æ³¢ç§‘è¾¾è´æ¯”è¿ªçº³è€ƒå¤é—å€çš„å›¾åƒä¸Šè¿›è¡Œï¼Œé‡‡ç”¨äº”æŠ˜äº¤å‰éªŒè¯ã€‚å…¶ä¸­ï¼Œèåˆæ®‹å·®å—å’Œé—¨æ§æ³¨æ„åŠ›æœºåˆ¶çš„Attention-Residual BEGL-UNetè·å¾—æœ€ä½³æ€§èƒ½ï¼ŒDiceç³»æ•°ä¸º0.710ï¼ŒéªŒè¯æŸå¤±ä¸º0.067ï¼Œå¬å›ç‡æœ€é«˜ä¸º0.854ã€‚Spatial Channel Attention BEGL-UNetè·å¾—ç›¸è¿‘æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜æ³¨æ„åŠ›æœºåˆ¶åœ¨æ–‡ç‰©æ•°å­—åŒ–ä¿æŠ¤ä¸­æœ‰æ•ˆï¼Œè¾ƒåŸºçº¿æ¨¡å‹Diceç³»æ•°æé«˜äº†2.5%-2.9%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶å¯¹ä¸‰ç§åŸºäºU-Netçš„æ¶æ„è¿›è¡Œäº†å¯¹æ¯”åˆ†æï¼Œç”¨äºå²©çŸ³è‰ºæœ¯å²©ç”»çš„è¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>å®éªŒä¸­é‡‡ç”¨çš„ä¸‰ç§æ¶æ„åŒ…æ‹¬BEGL-UNetã€Attention-Residual BEGL-UNetå’ŒSpatial Channel Attention BEGL-UNetã€‚</li>
<li>å®éªŒåœ¨å·´è¥¿è€ƒå¤é—å€çš„å›¾åƒä¸Šè¿›è¡Œï¼Œå¹¶é‡‡ç”¨äº”æŠ˜äº¤å‰éªŒè¯ã€‚</li>
<li>Attention-Residual BEGL-UNetè·å¾—æœ€ä½³æ€§èƒ½ï¼ŒDiceç³»æ•°ä¸º0.710ï¼ŒéªŒè¯æŸå¤±ä¸º0.067ï¼Œå¬å›ç‡æœ€é«˜ã€‚</li>
<li>Spatial Channel Attention BEGL-UNetæ€§èƒ½ä¸æœ€ä½³æ¨¡å‹ç›¸è¿‘ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜æ³¨æ„åŠ›æœºåˆ¶åœ¨æ–‡ç‰©æ•°å­—åŒ–ä¿æŠ¤ä¸­æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fba3744e1739d3fcce10e97acbf4bfa5" align="middle">
<img src="https://picx.zhimg.com/v2-60daecf0a7114348bbf705ff3c32e38f" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SFFR-Spatial-Frequency-Feature-Reconstruction-for-Multispectral-Aerial-Object-Detection"><a href="#SFFR-Spatial-Frequency-Feature-Reconstruction-for-Multispectral-Aerial-Object-Detection" class="headerlink" title="SFFR: Spatial-Frequency Feature Reconstruction for Multispectral Aerial Object Detection"></a>SFFR: Spatial-Frequency Feature Reconstruction for Multispectral Aerial Object Detection</h2><p><strong>Authors:Xin Zuo, Chenyu Qu, Haibo Zhan, Jifeng Shen, Wankou Yang</strong></p>
<p>Recent multispectral object detection methods have primarily focused on spatial-domain feature fusion based on CNNs or Transformers, while the potential of frequency-domain feature remains underexplored. In this work, we propose a novel Spatial and Frequency Feature Reconstruction method (SFFR) method, which leverages the spatial-frequency feature representation mechanisms of the Kolmogorov-Arnold Network (KAN) to reconstruct complementary representations in both spatial and frequency domains prior to feature fusion. The core components of SFFR are the proposed Frequency Component Exchange KAN (FCEKAN) module and Multi-Scale Gaussian KAN (MSGKAN) module. The FCEKAN introduces an innovative selective frequency component exchange strategy that effectively enhances the complementarity and consistency of cross-modal features based on the frequency feature of RGB and IR images. The MSGKAN module demonstrates excellent nonlinear feature modeling capability in the spatial domain. By leveraging multi-scale Gaussian basis functions, it effectively captures the feature variations caused by scale changes at different UAV flight altitudes, significantly enhancing the modelâ€™s adaptability and robustness to scale variations. It is experimentally validated that our proposed FCEKAN and MSGKAN modules are complementary and can effectively capture the frequency and spatial semantic features respectively for better feature fusion. Extensive experiments on the SeaDroneSee, DroneVehicle and DVTOD datasets demonstrate the superior performance and significant advantages of the proposed method in UAV multispectral object perception task. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/qchenyu1027/SFFR">https://github.com/qchenyu1027/SFFR</a>.</p>
<blockquote>
<p>æœ€è¿‘çš„å¤šå…‰è°±ç›®æ ‡æ£€æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨åŸºäºCNNæˆ–Transformerçš„ç©ºé—´åŸŸç‰¹å¾èåˆï¼Œè€Œé¢‘ç‡åŸŸç‰¹å¾çš„å¯èƒ½æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç©ºé—´ä¸é¢‘ç‡ç‰¹å¾é‡å»ºæ–¹æ³•ï¼ˆSFFRï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰çš„ç©ºé—´é¢‘ç‡ç‰¹å¾è¡¨ç¤ºæœºåˆ¶ï¼Œåœ¨ç‰¹å¾èåˆä¹‹å‰é‡å»ºç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸä¸­çš„äº’è¡¥è¡¨ç¤ºã€‚SFFRçš„æ ¸å¿ƒç»„ä»¶æ˜¯æå‡ºçš„é¢‘ç‡åˆ†é‡äº¤æ¢KANï¼ˆFCEKANï¼‰æ¨¡å—å’Œå¤šå°ºåº¦é«˜æ–¯KANï¼ˆMSGKANï¼‰æ¨¡å—ã€‚FCEKANå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„é€‰æ‹©æ€§é¢‘ç‡åˆ†é‡äº¤æ¢ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºRGBå’Œçº¢å¤–å›¾åƒçš„é¢‘ç‡ç‰¹å¾æœ‰æ•ˆåœ°å¢å¼ºäº†è·¨æ¨¡æ€ç‰¹å¾çš„äº’è¡¥æ€§å’Œä¸€è‡´æ€§ã€‚MSGKANæ¨¡å—åœ¨ç©ºé—´åŸŸè¡¨ç°å‡ºå‡ºè‰²çš„éçº¿æ€§ç‰¹å¾å»ºæ¨¡èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨å¤šå°ºåº¦é«˜æ–¯åŸºå‡½æ•°ï¼Œå®ƒæœ‰æ•ˆåœ°æ•è·äº†ä¸åŒæ— äººæœºé£è¡Œé«˜åº¦å¼•èµ·çš„å°ºåº¦å˜åŒ–æ‰€å¯¼è‡´çš„ç‰¹å¾å˜åŒ–ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹å°ºåº¦å˜åŒ–çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬æå‡ºçš„FCEKANå’ŒMSGKANæ¨¡å—æ˜¯äº’è¡¥çš„ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•è·é¢‘ç‡å’Œç©ºé—´è¯­ä¹‰ç‰¹å¾ï¼Œä»¥å®ç°æ›´å¥½çš„ç‰¹å¾èåˆã€‚åœ¨SeaDroneSeeã€DroneVehicleå’ŒDVTODæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ— äººæœºå¤šå…‰è°±ç›®æ ‡æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/qchenyu1027/SFFR%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/qchenyu1027/SFFRä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06298v3">PDF</a> 11 pages,8 figures, accepted by IEEE TGRS</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå…‰è°±ç›®æ ‡æ£€æµ‹æ–¹æ³•â€”â€”ç©ºé—´ä¸é¢‘ç‡ç‰¹å¾é‡å»ºæ–¹æ³•ï¼ˆSFFRï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰çš„ç©ºé—´é¢‘ç‡ç‰¹å¾è¡¨ç¤ºæœºåˆ¶ï¼Œåœ¨ç©ºé—´å’Œé¢‘ç‡åŸŸè¿›è¡Œç‰¹å¾èåˆå‰çš„ç‰¹å¾é‡å»ºã€‚æ ¸å¿ƒæ¨¡å—åŒ…æ‹¬é¢‘ç‡åˆ†é‡äº¤æ¢KANï¼ˆFCEKANï¼‰å’Œå¤šå°ºåº¦é«˜æ–¯KANï¼ˆMSGKANï¼‰ã€‚FCEKANé€šè¿‡åˆ›æ–°çš„é¢‘ç‡åˆ†é‡é€‰æ‹©æ€§äº¤æ¢ç­–ç•¥ï¼Œå¢å¼ºäº†è·¨æ¨¡æ€ç‰¹å¾çš„äº’è¡¥æ€§å’Œä¸€è‡´æ€§ã€‚MSGKANæ¨¡å—åœ¨ç©ºé—´ä¸­è¡¨ç°å‡ºå“è¶Šçš„éçº¿æ€§ç‰¹å¾å»ºæ¨¡èƒ½åŠ›ï¼Œé€šè¿‡å¤šå°ºåº¦é«˜æ–¯åŸºç¡€å‡½æ•°æ•æ‰ä¸åŒæ— äººæœºé£è¡Œé«˜åº¦å¼•èµ·çš„ç‰¹å¾å˜åŒ–ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹å°ºåº¦å˜åŒ–çš„é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼ŒFCEKANå’ŒMSGKANæ¨¡å—äº’è¡¥æ€§å¼ºï¼Œèƒ½æœ‰æ•ˆæ•æ‰é¢‘ç‡å’Œç©ºé—´è¯­ä¹‰ç‰¹å¾ï¼Œä»è€Œæé«˜æ— äººæœºå¤šå…‰è°±ç›®æ ‡æ„ŸçŸ¥ä»»åŠ¡çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šå…‰è°±ç›®æ ‡æ£€æµ‹æ–¹æ³•â€”â€”ç©ºé—´ä¸é¢‘ç‡ç‰¹å¾é‡å»ºæ–¹æ³•ï¼ˆSFFRï¼‰ï¼Œç»“åˆäº†ç©ºé—´å’Œé¢‘ç‡åŸŸçš„ç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åˆ©ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰çš„ç‰¹æ€§ï¼Œé€šè¿‡å…¶ç©ºé—´é¢‘ç‡ç‰¹å¾è¡¨ç¤ºæœºåˆ¶è¿›è¡Œç‰¹å¾é‡å»ºã€‚</li>
<li>å¼•å…¥äº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šé¢‘ç‡åˆ†é‡äº¤æ¢KANï¼ˆFCEKANï¼‰å’Œå¤šå°ºåº¦é«˜æ–¯KANï¼ˆMSGKANï¼‰ã€‚</li>
<li>FCEKANé€šè¿‡åˆ›æ–°çš„ç­–ç•¥å¢å¼ºè·¨æ¨¡æ€ç‰¹å¾çš„äº’è¡¥æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>MSGKANæ¨¡å—åœ¨ç©ºé—´ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„éçº¿æ€§ç‰¹å¾å»ºæ¨¡èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ•æ‰ä¸åŒé£è¡Œé«˜åº¦ä¸‹çš„ç‰¹å¾å˜åŒ–ã€‚</li>
<li>FCEKANå’ŒMSGKANæ¨¡å—çš„å®éªŒç»“æœè¯æ˜äº†å®ƒä»¬çš„äº’è¡¥æ€§ï¼Œèƒ½æ˜¾è‘—æé«˜æ— äººæœºå¤šå…‰è°±ç›®æ ‡æ„ŸçŸ¥çš„æ•ˆæœã€‚</li>
<li>æ–¹æ³•åœ¨SeaDroneSeeã€DroneVehicleå’ŒDVTODæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6e1e6ea4e66c8b302779fff97d8b3af" align="middle">
<img src="https://picx.zhimg.com/v2-37f64fbfcd3aa72f58bdd2ce2d91233a" align="middle">
<img src="https://picx.zhimg.com/v2-2a5cc7b50aa84a41a01c14ad15eeee96" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Robustness-Analysis-in-High-Dimensional-Space-Application-to-Semantic-Segmentation-Network"><a href="#Probabilistic-Robustness-Analysis-in-High-Dimensional-Space-Application-to-Semantic-Segmentation-Network" class="headerlink" title="Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network"></a>Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network</h2><p><strong>Authors:Navid Hashemi, Samuel Sasaki, Diego Manzanas Lopez, Lars Lindemann, Ipek Oguz, Meiyi Ma, Taylor T. Johnson</strong></p>
<p>Semantic segmentation networks (SSNs) are central to safety-critical applications such as medical imaging and autonomous driving, where robustness under uncertainty is essential. However, existing probabilistic verification methods often fail to scale with the complexity and dimensionality of modern segmentation tasks, producing guarantees that are overly conservative and of limited practical value. We propose a probabilistic verification framework that is architecture-agnostic and scalable to high-dimensional input-output spaces. Our approach employs conformal inference (CI), enhanced by a novel technique that we call the \textbf{clipping block}, to provide provable guarantees while mitigating the excessive conservatism of prior methods. Experiments on large-scale segmentation models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrate that our framework delivers reliable safety guarantees while substantially reducing conservatism compared to state-of-the-art approaches on segmentation tasks. We also provide a public GitHub repository (<a target="_blank" rel="noopener" href="https://github.com/Navidhashemicodes/SSN_Reach_CLP_Surrogate">https://github.com/Navidhashemicodes/SSN_Reach_CLP_Surrogate</a>) for this approach, to support reproducibility.</p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼ˆSSNsï¼‰åœ¨åŒ»å­¦å½±åƒå’Œè‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®é¢†åŸŸåº”ç”¨æ‰®æ¼”æ ¸å¿ƒè§’è‰²ï¼Œè¿™äº›é¢†åŸŸéœ€è¦å¤„ç†ä¸ç¡®å®šæ€§ä¸‹çš„ç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¦‚ç‡éªŒè¯æ–¹æ³•é€šå¸¸æ— æ³•é€‚åº”ç°ä»£åˆ†å‰²ä»»åŠ¡çš„å¤æ‚æ€§å’Œç»´æ•°ï¼Œæä¾›çš„ä¿è¯è¿‡äºä¿å®ˆï¼Œå®ç”¨ä»·å€¼æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¦‚ç‡éªŒè¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸å—æ¶æ„é™åˆ¶ï¼Œå¯æ‰©å±•åˆ°é«˜ç»´è¾“å…¥è¾“å‡ºç©ºé—´ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å½¢å¼åŒ–æ¨æ–­ï¼ˆCIï¼‰ï¼Œå¹¶ç»“åˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œè£å‰ªå—â€çš„æ–°æŠ€æœ¯ï¼Œä»¥æä¾›å¯è¯æ˜çš„ä¿è¯ï¼ŒåŒæ—¶å‡è½»å…ˆå‰æ–¹æ³•çš„è¿‡åº¦ä¿å®ˆæ€§ã€‚åœ¨CamVidã€OCTA-500ã€è‚ºåˆ†å‰²å’ŒåŸå¸‚æ™¯è§‚çš„å¤§è§„æ¨¡åˆ†å‰²æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•æä¾›äº†å¯é çš„ä¿è¯ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†ä¿å®ˆæ€§ã€‚æˆ‘ä»¬è¿˜ä¸ºæ­¤æ–¹æ³•æä¾›äº†å…¬å…±GitHubä»“åº“ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Navidhashemicodes/SSN_Reach_CLP_Surrogate%EF%BC%89%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E5%A4%8D%E5%88%B6%E5%92%8C%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/Navidhashemicodes/SSN_Reach_CLP_Surrogateï¼‰ï¼Œä»¥æ”¯æŒå¤åˆ¶å’Œç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11838v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¦‚ç‡éªŒè¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯¹æ¶æ„å…·æœ‰é€šç”¨æ€§ï¼Œå¹¶èƒ½æ‰©å±•åˆ°é«˜ç»´è¾“å…¥è¾“å‡ºç©ºé—´ã€‚å®ƒé‡‡ç”¨å½¢å¼åŒ–æ¨æ–­ï¼ˆCIï¼‰å’Œä¸€ç§æ–°çš„æŠ€æœ¯â€”â€”è£å‰ªå—ï¼ˆclipping blockï¼‰ï¼Œä»¥æä¾›å¯éªŒè¯çš„ä¿è¯ï¼ŒåŒæ—¶å‡è½»å…ˆå‰æ–¹æ³•çš„è¿‡åº¦ä¿å®ˆæ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨CamVidã€OCTA-500ã€è‚ºåˆ†å‰²å’ŒCityscapesç­‰å¤§å‹åˆ†å‰²æ¨¡å‹ä¸Šçš„å®‰å…¨ä¿è¯æ›´ä¸ºå¯é ï¼Œå¹¶ä¸”åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šç›¸æ¯”ç°æœ‰å…ˆè¿›æŠ€æœ¯å¤§å¹…å‡å°‘äº†ä¿å®ˆæ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†å…¬å…±GitHubä»“åº“ä»¥æ”¯æŒé‡ç°æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SSNsåœ¨åŒ»ç–—æˆåƒå’Œè‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®åº”ç”¨ä¸­æ‰®æ¼”æ ¸å¿ƒè§’è‰²ï¼Œè¦æ±‚å…·å¤‡ä¸ç¡®å®šæ€§ä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>ç°æœ‰æ¦‚ç‡éªŒè¯æ–¹æ³•å¾€å¾€æ— æ³•é€‚åº”ç°ä»£åˆ†å‰²ä»»åŠ¡çš„å¤æ‚æ€§å’Œç»´åº¦ï¼Œæä¾›çš„ä¿è¯è¿‡äºä¿å®ˆä¸”å®ç”¨ä»·å€¼æœ‰é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ¦‚ç‡éªŒè¯æ¡†æ¶é‡‡ç”¨å½¢å¼åŒ–æ¨æ–­ï¼ˆCIï¼‰å¹¶ç»“åˆè£å‰ªå—æŠ€æœ¯ï¼Œä»¥æä¾›å¯é çš„ä¿è¯å¹¶å‡è½»å…ˆå‰æ–¹æ³•çš„è¿‡åº¦ä¿å®ˆæ€§ã€‚</li>
<li>åœ¨å¤šä¸ªå¤§å‹åˆ†å‰²æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ¡†æ¶çš„ä¼˜è¶Šæ€§èƒ½ï¼Œæä¾›äº†æ›´ä¸ºå¯é çš„å®‰å…¨ä¿è¯ï¼Œå¹¶ä¸”ç›¸æ¯”ç°æœ‰æŠ€æœ¯å¤§å¹…é™ä½äº†ä¿å®ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ¶æ„é€šç”¨æ€§ï¼Œèƒ½æ‰©å±•åˆ°é«˜ç»´è¾“å…¥è¾“å‡ºç©ºé—´ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25ddf91ea37a84cebd573f4817381cf5" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SynSeg-Feature-Synergy-for-Multi-Category-Contrastive-Learning-in-End-to-End-Open-Vocabulary-Semantic-Segmentation"><a href="#SynSeg-Feature-Synergy-for-Multi-Category-Contrastive-Learning-in-End-to-End-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="SynSeg: Feature Synergy for Multi-Category Contrastive Learning in End-to-End Open-Vocabulary Semantic Segmentation"></a>SynSeg: Feature Synergy for Multi-Category Contrastive Learning in End-to-End Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Weichen Zhang, Kebin Liu, Fan Dang, Zhui Zhu, Xikai Sun, Yunhao Liu</strong></p>
<p>Semantic segmentation in open-vocabulary scenarios presents significant challenges due to the wide range and granularity of semantic categories. Existing weakly-supervised methods often rely on category-specific supervision and ill-suited feature construction methods for contrastive learning, leading to semantic misalignment and poor performance. In this work, we propose a novel weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a new feature reconstruction framework named Feature Synergy Structure (FSS). Specifically, MCCL strategy robustly combines both intra- and inter-category alignment and separation in order to make the model learn the knowledge of correlations from different categories within the same image. Moreover, FSS reconstructs discriminative features for contrastive learning through prior fusion and semantic-activation-map enhancement, effectively avoiding the foreground bias introduced by the visual encoder. Furthermore, SynSeg is a lightweight end-to-end solution without using any mid-term output from large-scale pretrained models and capable for real-time inference. In general, SynSeg effectively improves the abilities in semantic localization and discrimination under weak supervision in an efficient manner. Extensive experiments on benchmarks demonstrate that our method outperforms state-of-the-art (SOTA) performance. Particularly, SynSeg achieves higher accuracy than SOTA baselines with a ratio from 6.9% up to 26.2%.</p>
<blockquote>
<p>åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸­ï¼Œè¯­ä¹‰åˆ†å‰²é¢ä¸´ç€å¹¿æ³›çš„è¯­ä¹‰ç±»åˆ«å’Œç²’åº¦æ‰€å¸¦æ¥çš„å·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¼±ç›‘ç£æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šç±»åˆ«çš„ç›‘ç£å’Œä¸é€‚åˆå¯¹æ¯”å­¦ä¹ çš„ç‰¹å¾æ„å»ºæ–¹æ³•ï¼Œå¯¼è‡´è¯­ä¹‰ä¸åŒ¹é…å’Œæ€§èƒ½ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£æ–¹æ³•SynSegï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚SynSegæ‰§è¡Œå¤šç±»åˆ«å¯¹æ¯”å­¦ä¹ ï¼ˆMCCLï¼‰ä½œä¸ºæ›´å¼ºçš„è®­ç»ƒä¿¡å·ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªæ–°çš„ç‰¹å¾é‡å»ºæ¡†æ¶ï¼Œç§°ä¸ºç‰¹å¾ååŒç»“æ„ï¼ˆFSSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒMCCLç­–ç•¥ç¨³å¥åœ°å°†åŒä¸€å›¾åƒå†…ä¸åŒç±»åˆ«ä¹‹é—´çš„ç±»åˆ«å†…å’Œç±»åˆ«é—´å¯¹é½å’Œåˆ†ç¦»ç»“åˆèµ·æ¥ï¼Œä½¿æ¨¡å‹å­¦ä¹ ä¸åŒç±»åˆ«ä¹‹é—´çš„å…³è”çŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒFSSé€šè¿‡å…ˆéªŒèåˆå’Œè¯­ä¹‰æ¿€æ´»å›¾å¢å¼ºï¼Œé‡å»ºäº†ç”¨äºå¯¹æ¯”å­¦ä¹ çš„åˆ¤åˆ«ç‰¹å¾ï¼Œæœ‰æ•ˆåœ°é¿å…äº†è§†è§‰ç¼–ç å™¨å¼•å…¥çš„å‰æ™¯åè§ã€‚æ­¤å¤–ï¼ŒSynSegæ˜¯ä¸€ç§è½»é‡çº§çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œæ— éœ€ä½¿ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„ä»»ä½•ä¸­æœŸè¾“å‡ºï¼Œé€‚ç”¨äºå®æ—¶æ¨ç†ã€‚æ€»çš„æ¥è¯´ï¼ŒSynSegåœ¨å¼±ç›‘ç£æƒ…å†µä¸‹æœ‰æ•ˆåœ°æé«˜äº†è¯­ä¹‰å®šä½å’Œé‰´åˆ«çš„èƒ½åŠ›ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¿‡äº†æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼ŒSynSegçš„å‡†ç¡®ç‡é«˜äºSOTAåŸºå‡†çº¿ï¼Œæ¯”ä¾‹ä»6.9%åˆ°26.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06115v2">PDF</a> </p>
<p><strong>Summary</strong><br>è¯­ä¹‰åˆ†å‰²åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸­å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ï¼Œå› ä¸ºè¯­ä¹‰ç±»åˆ«çš„èŒƒå›´å¹¿æ³›ä¸”ç²’åº¦ç²¾ç»†ã€‚ç°æœ‰çš„å¼±ç›‘ç£æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šç±»åˆ«çš„ç›‘ç£å’Œå¯¹æ¯”å­¦ä¹ çš„ç‰¹å¾æ„å»ºæ–¹æ³•ä¸å½“ï¼Œå¯¼è‡´è¯­ä¹‰ä¸åŒ¹é…å’Œæ€§èƒ½ä¸ä½³ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¼±ç›‘ç£æ–¹æ³•SynSegï¼Œé€šè¿‡å¤šç±»åˆ«å¯¹æ¯”å­¦ä¹ ï¼ˆMCCLï¼‰å’Œç‰¹å¾ååŒç»“æ„ï¼ˆFSSï¼‰è§£å†³è¿™äº›é—®é¢˜ã€‚MCCLç­–ç•¥èƒ½å¤Ÿç¨³å¥åœ°ç»“åˆåŒä¸€å›¾åƒå†…ä¸åŒç±»åˆ«çš„å¯¹é½ä¸åˆ†ç¦»ï¼Œä½¿æ¨¡å‹å­¦ä¹ ç›¸å…³æ€§çŸ¥è¯†ã€‚FSSé€šè¿‡å…ˆéªŒèåˆå’Œè¯­ä¹‰æ¿€æ´»å›¾å¢å¼ºé‡å»ºå¯¹æ¯”å­¦ä¹ çš„åˆ¤åˆ«ç‰¹å¾ï¼Œæœ‰æ•ˆé¿å…è§†è§‰ç¼–ç å™¨å¼•å…¥çš„å‰æ™¯åè§ã€‚æ€»ä½“è€Œè¨€ï¼ŒSynSegåœ¨å¼±ç›‘ç£æ¡ä»¶ä¸‹æœ‰æ•ˆæé«˜äº†è¯­ä¹‰å®šä½å’Œé‰´åˆ«èƒ½åŠ›ï¼Œå¹¶ä¸”æ˜¯è½»é‡çº§çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œä¸ä½¿ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„ä¸­é—´è¾“å‡ºï¼Œé€‚ç”¨äºå®æ—¶æ¨ç†ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€æ–°æŠ€æœ¯çŠ¶æ€ï¼Œç‰¹åˆ«æ˜¯ç›¸å¯¹äºæœ€æ–°æŠ€æœ¯åŸºçº¿ï¼Œå‡†ç¡®ç‡æé«˜äº†ä»6.9%åˆ°26.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­ä¹‰åˆ†å‰²åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºè¯­ä¹‰ç±»åˆ«çš„èŒƒå›´å¹¿æ³›ä¸”ç²’åº¦ç²¾ç»†ã€‚</li>
<li>ç°æœ‰å¼±ç›‘ç£æ–¹æ³•å­˜åœ¨è¯­ä¹‰ä¸åŒ¹é…å’Œæ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†SynSegæ–¹æ³•ï¼ŒåŒ…æ‹¬å¤šç±»åˆ«å¯¹æ¯”å­¦ä¹ ï¼ˆMCCLï¼‰å’Œç‰¹å¾ååŒç»“æ„ï¼ˆFSSï¼‰æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>MCCLç¨³å¥åœ°ç»“åˆåŒä¸€å›¾åƒå†…ä¸åŒç±»åˆ«çš„å¯¹é½ä¸åˆ†ç¦»ï¼Œä½¿æ¨¡å‹å­¦ä¹ ç›¸å…³æ€§çŸ¥è¯†ã€‚</li>
<li>FSSé‡å»ºå¯¹æ¯”å­¦ä¹ çš„åˆ¤åˆ«ç‰¹å¾ï¼Œé¿å…å‰æ™¯åè§ã€‚</li>
<li>SynSegæ˜¯è½»é‡çº§çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºå®æ—¶æ¨ç†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31e8e9f9af8e409efde1ee9bf67e343c" align="middle">
<img src="https://picx.zhimg.com/v2-8fd47fa16432ff6e2777a27bb763a383" align="middle">
<img src="https://picx.zhimg.com/v2-fa44bde71f4cffc6bcfda66bee7afceb" align="middle">
<img src="https://picx.zhimg.com/v2-97bfca80d192bdf63d8f94e375d5beed" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SANSA-Unleashing-the-Hidden-Semantics-in-SAM2-for-Few-Shot-Segmentation"><a href="#SANSA-Unleashing-the-Hidden-Semantics-in-SAM2-for-Few-Shot-Segmentation" class="headerlink" title="SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation"></a>SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation</h2><p><strong>Authors:Claudia Cuttano, Gabriele Trivigno, Giuseppe Averta, Carlo Masone</strong></p>
<p>Few-shot segmentation aims to segment unseen object categories from just a handful of annotated examples. This requires mechanisms that can both identify semantically related objects across images and accurately produce segmentation masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, offers both strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art performance on few-shot segmentation benchmarks specifically designed to assess generalization, outperforms generalist methods in the popular in-context setting, supports various prompts flexible interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA">https://github.com/ClaudiaCuttano/SANSA</a>.</p>
<blockquote>
<p>å°æ ·æœ¬åˆ†å‰²ï¼ˆFew-shot segmentationï¼‰æ—¨åœ¨ä»å°‘é‡æ ‡æ³¨çš„æ ·æœ¬ä¸­å¯¹æœªè§è¿‡çš„ç›®æ ‡ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚è¿™éœ€è¦èƒ½å¤Ÿåœ¨å›¾åƒä¸­è¯†åˆ«è¯­ä¹‰ç›¸å…³å¯¹è±¡å¹¶å‡†ç¡®ç”Ÿæˆåˆ†å‰²æ©æ¨¡çš„æœºåˆ¶ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå€ŸåŠ©æç¤ºå’Œä¼ æ’­æœºåˆ¶ï¼ˆprompt-and-propagate mechanismï¼‰ï¼ŒSegment Anything 2ï¼ˆSAM2ï¼‰æä¾›äº†å¼ºå¤§çš„åˆ†å‰²èƒ½åŠ›å’Œå†…ç½®çš„ç‰¹å¾åŒ¹é…è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å±•ç¤ºå…¶è¡¨ç¤ºä¸é’ˆå¯¹å¯¹è±¡è·Ÿè¸ªä¼˜åŒ–çš„ç‰¹å®šä»»åŠ¡çº¿ç´¢äº¤ç»‡åœ¨ä¸€èµ·ï¼Œè¿™æŸå®³å…¶ç”¨äºéœ€è¦æ›´é«˜å±‚æ¬¡è¯­ä¹‰ç†è§£çš„ä»»åŠ¡çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå°½ç®¡å®ƒé‡‡ç”¨äº†ç±»æ— å…³é¢„è®­ç»ƒï¼ˆclass-agnostic pretrainingï¼‰ï¼Œä½†SAM2çš„ç‰¹å¾ä¸­å·²ç»ç¼–ç äº†ä¸°å¯Œçš„è¯­ä¹‰ç»“æ„ã€‚æˆ‘ä»¬æå‡ºSANSAï¼ˆè¯­ä¹‰å¯¹é½åˆ†æ®µä»»ä½•ä¸œè¥¿2ï¼‰ï¼Œå®ƒé€šè¿‡æœ€è½»å¾®çš„ç‰¹å®šä»»åŠ¡ä¿®æ”¹ä½¿è¿™ç§æ½œåœ¨ç»“æ„æ˜ç¡®åŒ–ï¼Œå¹¶é‡æ–°åˆ©ç”¨SAM2è¿›è¡Œå°æ ·æœ¬åˆ†å‰²ã€‚SANSAåœ¨ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°æ³›åŒ–çš„å°‘æ ·æœ¬åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æµè¡Œä¸Šä¸‹æ–‡è®¾ç½®ä¸­çš„é€šç”¨æ–¹æ³•è¡¨ç°ä¼˜å¼‚ï¼Œæ”¯æŒé€šè¿‡ç‚¹ã€æ¡†æˆ–æ¶‚é¸¦è¿›è¡Œå„ç§æç¤ºçµæ´»äº¤äº’ï¼Œå¹¶ä¸”ç›¸æ¯”äºä»¥å‰çš„æ–¹æ³•ä»ç„¶æ˜¾è‘—æ›´å¿«ã€æ›´ç´§å‡‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ClaudiaCuttano/SANSAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21795v2">PDF</a> Accepted to NeurIPS 2025 as Spotlight</p>
<p><strong>Summary</strong></p>
<p>SANSAæ¡†æ¶é’ˆå¯¹å°‘æ ·æœ¬åˆ†å‰²ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡å¾®è°ƒSAM2æ¨¡å‹ä½¿å…¶æ›´é€‚ç”¨äºæ­¤ç±»ä»»åŠ¡ã€‚SANSAå…·æœ‰æ˜¾è‘—æ€§èƒ½è¡¨ç°ï¼Œå®ç°äº†è·¨å›¾åƒè¯†åˆ«è¯­ä¹‰ç›¸å…³å¯¹è±¡çš„èƒ½åŠ›ï¼ŒåŒæ—¶å‡†ç¡®ç”Ÿæˆåˆ†å‰²æ©è†œã€‚æ­¤å¤–ï¼ŒSANSAè¿˜æ”¯æŒå„ç§æç¤ºæ–¹å¼ï¼Œå¦‚ç‚¹ã€æ¡†æˆ–æ¶‚é¸¦ï¼Œå…·æœ‰å¿«é€Ÿä¸”ç´§å‡‘çš„ç‰¹ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å°‘æ ·æœ¬åˆ†å‰²æ—¨åœ¨ä»æœªç»æ ‡æ³¨çš„ä¾‹å­ä¸­å¯¹æœªè§çš„å¯¹è±¡ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>SAM2æ¨¡å‹å…·å¤‡å¼ºå¤§çš„åˆ†å‰²èƒ½åŠ›å’Œå†…ç½®çš„ç‰¹å¾åŒ¹é…è¿‡ç¨‹ï¼Œä½†å…¶è¡¨ç¤ºä¸ä»»åŠ¡ç‰¹å®šçº¿ç´¢çº ç¼ åœ¨ä¸€èµ·ï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦é«˜çº§è¯­ä¹‰ç†è§£çš„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>å°½ç®¡SAM2å…·æœ‰ç±»åˆ«æ— å…³çš„é¢„è®­ç»ƒï¼Œä½†å®ƒå·²ç»åœ¨ç‰¹å¾ä¸­ç¼–ç äº†ä¸°å¯Œçš„è¯­ä¹‰ç»“æ„ã€‚</li>
<li>SANSAæ¡†æ¶é€šè¿‡ä½¿è¿™ç§æ½œåœ¨ç»“æ„æ˜¾æ€§åŒ–å¹¶é‡æ–°åˆ©ç”¨SAM2è¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†å°‘æ ·æœ¬åˆ†å‰²ä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>SANSAåœ¨ä¸“é—¨è®¾è®¡çš„è¯„ä¼°æ³›åŒ–çš„å°‘æ ·æœ¬åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>SANSAåœ¨æµè¡Œçš„ä¸Šä¸‹æ–‡è®¾ç½®ä¸­çš„è¡¨ç°ä¼˜äºé€šç”¨æ–¹æ³•ã€‚</li>
<li>SANSAæ”¯æŒé€šè¿‡ç‚¹ã€æ¡†æˆ–æ¶‚é¸¦ç­‰å¤šç§æç¤ºæ–¹å¼è¿›è¡Œçµæ´»çš„äº¤äº’ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-735af8168299f16ad3afa993d8da6958" align="middle">
<img src="https://picx.zhimg.com/v2-2650d14806296ac49c7a3c61002de01f" align="middle">
<img src="https://picx.zhimg.com/v2-1bf065d9658ddae0703258612f78356f" align="middle">
<img src="https://picx.zhimg.com/v2-f63d21a6ea5759b2dcc4ea6f0220a0cd" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SAM2MOT-A-Novel-Paradigm-of-Multi-Object-Tracking-by-Segmentation"><a href="#SAM2MOT-A-Novel-Paradigm-of-Multi-Object-Tracking-by-Segmentation" class="headerlink" title="SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation"></a>SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation</h2><p><strong>Authors:Junjie Jiang, Zelin Wang, Manqi Zhao, Yin Li, DongSheng Jiang</strong></p>
<p>Inspired by Segment Anything 2, which generalizes segmentation from images to videos, we propose SAM2MOTâ€“a novel segmentation-driven paradigm for multi-object tracking that breaks away from the conventional detection-association framework. In contrast to previous approaches that treat segmentation as auxiliary information, SAM2MOT places it at the heart of the tracking process, systematically tackling challenges like false positives and occlusions. Its effectiveness has been thoroughly validated on major MOT benchmarks. Furthermore, SAM2MOT integrates pre-trained detector, pre-trained segmentor with tracking logic into a zero-shot MOT system that requires no fine-tuning. This significantly reduces dependence on labeled data and paves the way for transitioning MOT research from task-specific solutions to general-purpose systems. Experiments on DanceTrack, UAVDT, and BDD100K show state-of-the-art results. Notably, SAM2MOT outperforms existing methods on DanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT. Code is available at <a target="_blank" rel="noopener" href="https://github.com/TripleJoy/SAM2MOT">https://github.com/TripleJoy/SAM2MOT</a>.</p>
<blockquote>
<p>å—Segment Anything 2çš„å¯å‘ï¼Œå®ƒå°†å›¾åƒåˆ†å‰²æŠ€æœ¯æ³›åŒ–åˆ°è§†é¢‘é¢†åŸŸï¼Œæˆ‘ä»¬æå‡ºäº†SAM2MOTâ€”â€”ä¸€ç§ç”¨äºå¤šç›®æ ‡è·Ÿè¸ªçš„æ–°å‹åˆ†å‰²é©±åŠ¨èŒƒå¼ï¼Œå®ƒæ‰“ç ´äº†ä¼ ç»Ÿçš„æ£€æµ‹å…³è”æ¡†æ¶ã€‚ä¸ä»¥å‰å°†åˆ†å‰²è§†ä¸ºè¾…åŠ©ä¿¡æ¯çš„æ–¹æ³•ä¸åŒï¼ŒSAM2MOTå°†å…¶ç½®äºè·Ÿè¸ªè¿‡ç¨‹çš„æ ¸å¿ƒï¼Œç³»ç»Ÿè§£å†³è¯¯æŠ¥å’Œé®æŒ¡ç­‰æŒ‘æˆ˜ã€‚å®ƒåœ¨ä¸»è¦çš„MOTåŸºå‡†æµ‹è¯•ä¸Šçš„æœ‰æ•ˆæ€§å·²ç»å¾—åˆ°äº†å……åˆ†çš„éªŒè¯ã€‚æ­¤å¤–ï¼ŒSAM2MOTå°†é¢„è®­ç»ƒæ£€æµ‹å™¨ã€é¢„è®­ç»ƒåˆ†å‰²å™¨ä¸è·Ÿè¸ªé€»è¾‘é›†æˆåˆ°ä¸€ä¸ªæ— éœ€å¾®è°ƒå³å¯è¿›è¡Œé›¶ç‚¹å‡»MOTçš„ç³»ç»Ÿï¼Œè¿™æå¤§åœ°å‡å°‘äº†å…¶å¯¹æ ‡è®°æ•°æ®çš„ä¾èµ–ï¼Œä¸ºä»ä»»åŠ¡ç‰¹å®šè§£å†³æ–¹æ¡ˆå‘é€šç”¨ç³»ç»Ÿè¿‡æ¸¡çš„MOTç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚åœ¨DanceTrackã€UAVDTå’ŒBDD100Kä¸Šçš„å®éªŒæ˜¾ç¤ºå…¶å¤„äºé¢†å…ˆæ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSAM2MOTåœ¨DanceTrackä¸Šçš„HOTAé«˜å‡º+2.1ï¼Œ+IDFé«˜å‡º+4.5ï¼Œçªæ˜¾å…¶åœ¨MOTä¸­çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/TripleJoy/SAM2MOT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/TripleJoy/SAM2MOTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04519v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºSegment Anything 2å¯¹å›¾åƒåˆ†å‰²çš„é€šç”¨æ¡†æ¶å¯å‘ï¼Œæå‡ºSAM2MOTè¿™ä¸€æ–°é¢–çš„å¤šç›®æ ‡è·Ÿè¸ªåˆ†å‰²é©±åŠ¨èŒƒå¼ï¼Œçªç ´ä¼ ç»Ÿæ£€æµ‹å…³è”æ¡†æ¶çš„å±€é™ã€‚SAM2MOTå°†åˆ†å‰²ç½®äºè·Ÿè¸ªè¿‡ç¨‹çš„æ ¸å¿ƒï¼Œæœ‰æ•ˆåº”å¯¹è¯¯æŠ¥å’Œé®æŒ¡ç­‰æŒ‘æˆ˜ã€‚å…¶åœ¨ä¸»è¦å¤šç›®æ ‡è·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ°äº†éªŒè¯ã€‚æ­¤å¤–ï¼ŒSAM2MOTé›†æˆäº†é¢„è®­ç»ƒæ£€æµ‹å™¨ã€é¢„è®­ç»ƒåˆ†å‰²å™¨å’Œè·Ÿè¸ªé€»è¾‘ï¼Œæ„å»ºäº†ä¸€ä¸ªæ— éœ€å¾®è°ƒå³èƒ½é€‚åº”å¤šç§ä»»åŠ¡çš„é›¶æ ·æœ¬å¤šç›®æ ‡è·Ÿè¸ªç³»ç»Ÿï¼Œå‡å°‘äº†æ ‡ç­¾æ•°æ®çš„ä¾èµ–ï¼Œä¸ºä»ä»»åŠ¡ç‰¹å®šè§£å†³æ–¹æ¡ˆå‘é€šç”¨ç³»ç»Ÿçš„è¿‡æ¸¡é“ºå¹³äº†é“è·¯ã€‚åœ¨DanceTrackã€UAVDTå’ŒBDD100Kä¸Šçš„å®éªŒæ˜¾ç¤ºå…¶è¡¨ç°å“è¶Šã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAM2MOTæ˜¯åŸºäºSegment Anything 2å¯å‘çš„åˆ†å‰²é©±åŠ¨å¤šç›®æ ‡è·Ÿè¸ªèŒƒå¼ã€‚</li>
<li>å®ƒçªç ´äº†ä¼ ç»Ÿæ£€æµ‹å…³è”æ¡†æ¶çš„é™åˆ¶ï¼Œå°†åˆ†å‰²ç½®äºè·Ÿè¸ªè¿‡ç¨‹çš„æ ¸å¿ƒã€‚</li>
<li>SAM2MOTæœ‰æ•ˆåº”å¯¹è¯¯æŠ¥å’Œé®æŒ¡ç­‰è·Ÿè¸ªè¿‡ç¨‹ä¸­çš„å¸¸è§æŒ‘æˆ˜ã€‚</li>
<li>åœ¨ä¸»è¦çš„å¤šç›®æ ‡è·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ°äº†éªŒè¯å’Œå¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç³»ç»Ÿé›†æˆäº†é¢„è®­ç»ƒæ£€æµ‹å™¨å’Œåˆ†å‰²å™¨ï¼Œå½¢æˆäº†ä¸€ä¸ªé›¶æ ·æœ¬å¤šç›®æ ‡è·Ÿè¸ªç³»ç»Ÿï¼Œæ— éœ€å¾®è°ƒå³å¯é€‚åº”ä¸åŒä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•å‡å°‘äº†æ ‡ç­¾æ•°æ®çš„ä¾èµ–ï¼Œä¸ºä»ä»»åŠ¡ç‰¹å®šè§£å†³æ–¹æ¡ˆå‘é€šç”¨ç³»ç»Ÿçš„è¿‡æ¸¡é“ºå¹³äº†é“è·¯ã€‚</li>
<li>åœ¨DanceTrackã€UAVDTå’ŒBDD100Kç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºå…¶è¡¨ç°å“è¶Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36be44cf84d11b3833dcc91365f25d72" align="middle">
<img src="https://picx.zhimg.com/v2-31d7691b2344bf4453fe933426e2c979" align="middle">
<img src="https://picx.zhimg.com/v2-be004308b4ba168657bb7b3253fb5427" align="middle">
<img src="https://picx.zhimg.com/v2-e33600b1071587327402f58acf25c55d" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SimROD-A-Simple-Baseline-for-Raw-Object-Detection-with-Global-and-Local-Enhancements"><a href="#SimROD-A-Simple-Baseline-for-Raw-Object-Detection-with-Global-and-Local-Enhancements" class="headerlink" title="SimROD: A Simple Baseline for Raw Object Detection with Global and Local Enhancements"></a>SimROD: A Simple Baseline for Raw Object Detection with Global and Local Enhancements</h2><p><strong>Authors:Haiyang Xie, Xi Shen, Shihua Huang, Qirui Wang, Zheng Wang</strong></p>
<p>Most visual models are designed for sRGB images, yet RAW data offers significant advantages for object detection by preserving sensor information before ISP processing. This enables improved detection accuracy and more efficient hardware designs by bypassing the ISP. However, RAW object detection is challenging due to limited training data, unbalanced pixel distributions, and sensor noise. To address this, we propose SimROD, a lightweight and effective approach for RAW object detection. We introduce a Global Gamma Enhancement (GGE) module, which applies a learnable global gamma transformation with only four parameters, improving feature representation while keeping the model efficient. Additionally, we leverage the green channelâ€™s richer signal to enhance local details, aligning with the human eyeâ€™s sensitivity and Bayer filter design. Extensive experiments on multiple RAW object detection datasets and detectors demonstrate that SimROD outperforms state-of-the-art methods like RAW-Adapter and DIAP while maintaining efficiency. Our work highlights the potential of RAW data for real-world object detection. Code is available at <a target="_blank" rel="noopener" href="https://ocean146.github.io/SimROD2025/">https://ocean146.github.io/SimROD2025/</a>.</p>
<blockquote>
<p>å¤§å¤šæ•°è§†è§‰æ¨¡å‹éƒ½æ˜¯ä¸ºsRGBå›¾åƒè®¾è®¡çš„ï¼Œç„¶è€ŒRAWæ•°æ®åœ¨å¯¹è±¡æ£€æµ‹æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå®ƒèƒ½å¤Ÿä¿ç•™ISPå¤„ç†å‰çš„ä¼ æ„Ÿå™¨ä¿¡æ¯ã€‚è¿™é€šè¿‡ç»•è¿‡ISPï¼Œå®ç°äº†æ›´é«˜çš„æ£€æµ‹ç²¾åº¦å’Œæ›´æœ‰æ•ˆçš„ç¡¬ä»¶è®¾è®¡ã€‚ç„¶è€Œï¼ŒRAWå¯¹è±¡æ£€æµ‹é¢ä¸´æœ‰é™çš„è®­ç»ƒæ•°æ®ã€åƒç´ åˆ†å¸ƒä¸å¹³è¡¡å’Œä¼ æ„Ÿå™¨å™ªå£°ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SimRODï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºRAWå¯¹è±¡æ£€æµ‹çš„è½»é‡çº§ä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†å…¨å±€ä¼½é©¬å¢å¼ºï¼ˆGGEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ä»…ä½¿ç”¨å››ä¸ªå‚æ•°åº”ç”¨å¯å­¦ä¹ çš„å…¨å±€ä¼½é©¬å˜æ¢ï¼Œåœ¨ä¿æŒæ¨¡å‹æ•ˆç‡çš„åŒæ—¶ï¼Œæ”¹è¿›äº†ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨ç»¿è‰²é€šé“æ›´ä¸°å¯Œçš„ä¿¡å·å¢å¼ºå±€éƒ¨ç»†èŠ‚ï¼Œè¿™ä¸äººçœ¼çš„æ•æ„Ÿåº¦å’ŒBayeræ»¤é•œè®¾è®¡ç›¸ä¸€è‡´ã€‚åœ¨å¤šä¸ªRAWå¯¹è±¡æ£€æµ‹æ•°æ®é›†å’Œæ£€æµ‹å™¨ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSimRODåœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶ï¼Œä¼˜äºRAW-Adapterå’ŒDIAPç­‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶å·¥ä½œçªå‡ºäº†RAWæ•°æ®åœ¨ç°å®å¯¹è±¡æ£€æµ‹ä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://ocean146.github.io/SimROD2025/%E3%80%82">https://ocean146.github.io/SimROD2025/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07101v3">PDF</a> Accepted by AAAI 2026. Code is available at <a target="_blank" rel="noopener" href="https://ocean146.github.io/SimROD2025/">https://ocean146.github.io/SimROD2025/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰æ¨¡å‹åœ¨RAWæ•°æ®ä¸Šçš„å¯¹è±¡æ£€æµ‹é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè™½ç„¶å¤§å¤šæ•°è§†è§‰æ¨¡å‹æ˜¯ä¸ºsRGBå›¾åƒè®¾è®¡çš„ï¼Œä½†RAWæ•°æ®åœ¨ä¿å­˜ä¼ æ„Ÿå™¨ä¿¡æ¯æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¯ä»¥ç”¨äºæ›´å‡†ç¡®çš„å¯¹è±¡æ£€æµ‹å’Œæ›´æœ‰æ•ˆçš„ç¡¬ä»¶è®¾è®¡ã€‚ä¸ºè§£å†³RAWæ•°æ®å¯¹è±¡æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œå¦‚è®­ç»ƒæ•°æ®æœ‰é™ã€åƒç´ åˆ†å¸ƒä¸å¹³è¡¡å’Œä¼ æ„Ÿå™¨å™ªå£°ç­‰é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†SimRODæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å…¨å±€ä¼½é©¬å¢å¼ºæ¨¡å—å’Œç»¿è‰²é€šé“ä¿¡å·çš„åˆ©ç”¨ï¼Œæé«˜äº†ç‰¹å¾è¡¨ç¤ºå’Œå±€éƒ¨ç»†èŠ‚ï¼Œä»è€Œåœ¨å¤šä¸ªRAWå¯¹è±¡æ£€æµ‹æ•°æ®é›†å’Œæ£€æµ‹å™¨ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºRAW-Adapterå’ŒDIAPç­‰ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ¨¡å‹åœ¨RAWæ•°æ®ä¸Šçš„å¯¹è±¡æ£€æµ‹æ˜¯ä¸€ä¸ªé‡è¦ç ”ç©¶æ–¹å‘ã€‚</li>
<li>RAWæ•°æ®ä¿ç•™äº†ä¼ æ„Ÿå™¨ä¿¡æ¯ï¼Œæœ‰åŠ©äºæé«˜å¯¹è±¡æ£€æµ‹å‡†ç¡®æ€§å’Œç¡¬ä»¶æ•ˆç‡ã€‚</li>
<li>SimRODæ–¹æ³•é€šè¿‡å…¨å±€ä¼½é©¬å¢å¼ºæ¨¡å—å’Œç»¿è‰²é€šé“ä¿¡å·çš„åˆ©ç”¨ï¼Œè§£å†³äº†RAWæ•°æ®å¯¹è±¡æ£€æµ‹çš„æŒ‘æˆ˜ã€‚</li>
<li>SimRODæ–¹æ³•åœ¨å¤šä¸ªRAWå¯¹è±¡æ£€æµ‹æ•°æ®é›†å’Œæ£€æµ‹å™¨ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>SimRODæ–¹æ³•ç›¸å¯¹äºå…¶ä»–æ–¹æ³•å¦‚RAW-Adapterå’ŒDIAPæœ‰æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†åˆ©ç”¨äººç±»è§†è§‰ç³»ç»Ÿçš„ç‰¹æ€§ï¼ˆå¦‚çœ¼ç›å¯¹ç»¿è‰²çš„æ•æ„Ÿåº¦ï¼‰è¿›è¡Œè®¾è®¡çš„è€ƒè™‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fd4e77e6d60439207dcb9de835df136" align="middle">
<img src="https://picx.zhimg.com/v2-ba275e4f5f23a0a03d95349719184310" align="middle">
<img src="https://picx.zhimg.com/v2-9182695c0ba0b578b51c261359ce6b32" align="middle">
<img src="https://picx.zhimg.com/v2-79f85ed019343cbc0fdecafc1b2cb496" align="middle">
<img src="https://picx.zhimg.com/v2-618cdbcf2bd21df123f2e81e43c9328d" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SAQ-SAM-Semantically-Aligned-Quantization-for-Segment-Anything-Model"><a href="#SAQ-SAM-Semantically-Aligned-Quantization-for-Segment-Anything-Model" class="headerlink" title="SAQ-SAM: Semantically-Aligned Quantization for Segment Anything Model"></a>SAQ-SAM: Semantically-Aligned Quantization for Segment Anything Model</h2><p><strong>Authors:Jing Zhang, Zhikai Li, Chengzhi Hu, Xuewen Liu, Qingyi Gu</strong></p>
<p>Segment Anything Model (SAM) exhibits remarkable zero-shot segmentation capability; however, its prohibitive computational costs make edge deployment challenging. Although post-training quantization (PTQ) offers a promising compression solution, existing methods yield unsatisfactory results when applied to SAM, owing to its specialized model components and promptable workflow: (i) The mask decoderâ€™s attention exhibits extreme activation outliers, and we find that aggressive clipping (even 100x), without smoothing or isolation, is effective in suppressing outliers while maintaining performance. Unfortunately, traditional distribution-based metrics (e.g., MSE) fail to provide such large-scale clipping. (ii) Existing quantization reconstruction methods neglect semantic interactivity of SAM, leading to misalignment between image feature and prompt intention. To address the above issues, we propose SAQ-SAM in this paper, which boosts PTQ for SAM from the perspective of semantic alignment. Specifically, we propose Perceptual-Consistency Clipping, which exploits attention focus overlap to promote aggressive clipping while preserving semantic capabilities. Furthermore, we propose Prompt-Aware Reconstruction, which incorporates image-prompt interactions by leveraging cross-attention in mask decoder, thus facilitating alignment in both distribution and semantic. Moreover, to ensure the interaction efficiency, we design a layer-skipping strategy for image tokens in encoder. Extensive experiments are conducted on various SAM sizes and tasks, including instance segmentation, oriented object detection, and semantic segmentation, and the results show that our method consistently exhibits advantages. For example, when quantizing SAM-B to 4-bit, SAQ-SAM achieves 11.7% higher mAP than the baseline in instance segmentation task.</p>
<blockquote>
<p>Segment Anything Modelï¼ˆSAMï¼‰å±•ç°å‡ºäº†å“è¶Šçš„é›¶æ ·æœ¬åˆ†å‰²èƒ½åŠ›ï¼›ç„¶è€Œï¼Œå…¶é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ä½¿å¾—è¾¹ç¼˜éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ã€‚è™½ç„¶åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„å‹ç¼©è§£å†³æ–¹æ¡ˆï¼Œä½†å½“å°†å…¶åº”ç”¨äºSAMæ—¶ï¼Œç°æœ‰æ–¹æ³•äº§ç”Ÿçš„æ•ˆæœå¹¶ä¸ä»¤äººæ»¡æ„ï¼ŒåŸå› åœ¨äºSAMçš„ä¸“é—¨æ¨¡å‹ç»„ä»¶å’Œå¯æç¤ºçš„å·¥ä½œæµç¨‹ï¼šï¼ˆiï¼‰æ©è†œè§£ç å™¨çš„æ³¨æ„åŠ›è¡¨ç°å‡ºæç«¯çš„æ¿€æ´»å¼‚å¸¸å€¼ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨ä¸å¯¹å…¶è¿›è¡Œå¹³æ»‘æˆ–éš”ç¦»çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œæ¿€çƒˆè£å‰ªï¼ˆç”šè‡³æ˜¯100å€ï¼‰åœ¨æŠ‘åˆ¶å¼‚å¸¸å€¼çš„åŒæ—¶èƒ½ç»´æŒæ€§èƒ½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºåˆ†å¸ƒçš„åº¦é‡ï¼ˆä¾‹å¦‚MSEï¼‰æ— æ³•æä¾›å¦‚æ­¤å¤§è§„æ¨¡çš„è£å‰ªã€‚ï¼ˆiiï¼‰ç°æœ‰çš„é‡åŒ–é‡å»ºæ–¹æ³•å¿½ç•¥äº†SAMçš„è¯­ä¹‰äº¤äº’æ€§ï¼Œå¯¼è‡´å›¾åƒç‰¹å¾ä¸æç¤ºæ„å›¾ä¹‹é—´çš„ä¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SAQ-SAMï¼Œå®ƒä»è¯­ä¹‰å¯¹é½çš„è§’åº¦æå‡äº†SAMçš„åè®­ç»ƒé‡åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†æ„ŸçŸ¥ä¸€è‡´æ€§è£å‰ªï¼Œå®ƒåˆ©ç”¨æ³¨æ„åŠ›ç„¦ç‚¹é‡å æ¥ä¿ƒè¿›æ¿€çƒˆè£å‰ªï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æç¤ºæ„ŸçŸ¥é‡å»ºï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ©è†œè§£ç å™¨ä¸­çš„äº¤å‰æ³¨æ„åŠ›æ¥ç»“åˆå›¾åƒæç¤ºäº¤äº’ï¼Œä»è€Œä¿ƒè¿›åˆ†å¸ƒå’Œè¯­ä¹‰çš„å¯¹é½ã€‚ä¸ºäº†ç¡®ä¿äº¤äº’æ•ˆç‡ï¼Œæˆ‘ä»¬è¿˜ä¸ºç¼–ç å™¨ä¸­çš„å›¾åƒä»¤ç‰Œè®¾è®¡äº†ä¸€ç§è·³è¿‡ç­–ç•¥ã€‚åœ¨å¤šç§SAMå¤§å°å’Œä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬å®ä¾‹åˆ†å‰²ã€å®šå‘ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ä¸€è‡´æ€§ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œå½“å°†SAM-Bé‡åŒ–è‡³4ä½æ—¶ï¼ŒSAQ-SAMåœ¨å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­çš„mAPæ¯”åŸºçº¿é«˜å‡º11.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06515v2">PDF</a> AAAI 2026. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jingjing0419/SAQ-SAM">https://github.com/jingjing0419/SAQ-SAM</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Segment Anything Modelï¼ˆSAMï¼‰çš„é›¶æ ·æœ¬åˆ†å‰²èƒ½åŠ›åŠå…¶è¾¹ç¼˜éƒ¨ç½²çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹æ¨¡å‹è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œå°è¯•ä½¿ç”¨é‡åŒ–å‹ç¼©æ–¹æ³•è§£å†³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é‡åŒ–æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºSAMæ¨¡å‹ï¼Œå› ä¸ºå®ƒå…·æœ‰ç‰¹æ®Šçš„æ¨¡å‹ç»„ä»¶å’Œæç¤ºå·¥ä½œæµç¨‹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SAQ-SAMæ–¹æ¡ˆï¼Œä»è¯­ä¹‰å¯¹é½çš„è§’åº¦å¢å¼ºPTQåœ¨SAMä¸Šçš„åº”ç”¨ã€‚åŒ…æ‹¬æ„ŸçŸ¥ä¸€è‡´æ€§è£å‰ªå’Œæç¤ºæ„ŸçŸ¥é‡å»ºæŠ€æœ¯ã€‚é’ˆå¯¹ç¼–ç å™¨çš„å›¾åƒä»¤ç‰Œé‡‡ç”¨å±‚è·³è¿‡ç­–ç•¥ä»¥ç¡®ä¿äº¤äº’æ•ˆç‡ã€‚å®éªŒç»“æœåœ¨å„ç§è§„æ¨¡å’Œä»»åŠ¡çš„SAMæ¨¡å‹ä¸­æ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ã€‚åœ¨å°†SAM-Bé‡åŒ–ä¸ºå››æ¯”ç‰¹çš„æƒ…å†µä¸‹ï¼Œå…¶åœ¨å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­çš„å¹³å‡ç²¾åº¦æ¯”åŸºçº¿é«˜å‡º11.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAMå…·æœ‰é›¶æ ·æœ¬åˆ†å‰²èƒ½åŠ›ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ï¼Œéš¾ä»¥å®ç°è¾¹ç¼˜éƒ¨ç½²ã€‚</li>
<li>ç°æœ‰é‡åŒ–æ–¹æ³•æ— æ³•æ»¡è¶³SAMçš„ç‰¹æ®Šéœ€æ±‚ï¼ŒåŸå› åœ¨äºå…¶ç‹¬ç‰¹çš„æ¨¡å‹ç»„ä»¶å’Œæç¤ºå·¥ä½œæµç¨‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„SAQ-SAMæ–¹æ³•ï¼Œä»è¯­ä¹‰å¯¹é½çš„è§’åº¦å¢å¼ºäº†PTQåœ¨SAMä¸Šçš„åº”ç”¨æ€§èƒ½ã€‚</li>
<li>SAQ-SAMåŒ…å«æ„ŸçŸ¥ä¸€è‡´æ€§è£å‰ªæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆæŠ‘åˆ¶æ³¨æ„åŠ›æ¿€æ´»çš„å¼‚å¸¸å€¼ã€‚</li>
<li>SAQ-SAMå¼•å…¥æç¤ºæ„ŸçŸ¥é‡å»ºæŠ€æœ¯ï¼Œåˆ©ç”¨mask decoderä¸­çš„è·¨æ³¨æ„åŠ›èåˆå›¾åƒæç¤ºäº¤äº’ï¼Œå®ç°åˆ†å¸ƒå’Œè¯­ä¹‰çš„å¯¹é½ã€‚</li>
<li>SAQ-SAMè®¾è®¡äº†ä¸€ç§é’ˆå¯¹å›¾åƒä»¤ç‰Œçš„å±‚è·³è¿‡ç­–ç•¥ï¼Œä»¥æé«˜äº¤äº’æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06515">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b96ab20cbcc7c6436dce28cc9525afb" align="middle">
<img src="https://picx.zhimg.com/v2-ed653e4bb958faa7785c20eef42383a3" align="middle">
<img src="https://picx.zhimg.com/v2-a62daff483df29b1a6eee098f3b42cbd" align="middle">
<img src="https://picx.zhimg.com/v2-06eb37453bc1e236994769d225e6d1be" align="middle">
<img src="https://picx.zhimg.com/v2-87c21661c8b19fda445d0df741e6bd9a" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Framework-for-Real-Time-Volcano-Seismic-Event-Recognition-Based-on-Multi-Station-Seismograms-and-Semantic-Segmentation-Models"><a href="#A-Framework-for-Real-Time-Volcano-Seismic-Event-Recognition-Based-on-Multi-Station-Seismograms-and-Semantic-Segmentation-Models" class="headerlink" title="A Framework for Real-Time Volcano-Seismic Event Recognition Based on Multi-Station Seismograms and Semantic Segmentation Models"></a>A Framework for Real-Time Volcano-Seismic Event Recognition Based on Multi-Station Seismograms and Semantic Segmentation Models</h2><p><strong>Authors:Camilo Espinosa-Curilem, Millaray Curilem, Daniel Basualto</strong></p>
<p>In volcano monitoring, effective recognition of seismic events is essential for understanding volcanic activity and raising timely warning alerts. Traditional methods rely on manual analysis, which can be subjective and labor-intensive. Furthermore, current automatic approaches often tackle detection and classification separately, mostly rely on single station information and generally require tailored preprocessing and representations to perform predictions. These limitations often hinder their application to real-time monitoring and utilization across different volcano conditions. This study introduces a novel approach that utilizes Semantic Segmentation models to automate seismic event recognition by applying a straight forward transformation of multi-channel 1D signals into 2D representations, enabling their use as images. Our framework employs a data-driven, end-to-end design that integrates multi-station seismic data with minimal preprocessing, performing both detection and classification simultaneously for five seismic event classes. We evaluated four state-of-the-art segmentation models (UNet, UNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events recorded at four different Chilean volcanoes: Nevados del ChillÃ¡n Volcanic Complex, Laguna del Maule, Villarrica and Puyehue-CordÃ³n Caulle. Among these models, the UNet architecture was identified as the most effective model, achieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and 0.88, respectively, and demonstrating superior noise robustness and model flexibility to unseen volcano datasets.</p>
<blockquote>
<p>åœ¨ç«å±±ç›‘æµ‹ä¸­ï¼Œæœ‰æ•ˆåœ°è¯†åˆ«åœ°éœ‡äº‹ä»¶å¯¹äºç†è§£ç«å±±æ´»åŠ¨å¹¶åŠæ—¶å‘å‡ºè­¦å‘Šè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨åˆ†æï¼Œè¿™å¯èƒ½ä¼šå¸¦æœ‰ä¸»è§‚æ€§å’ŒåŠ³åŠ¨å¯†é›†å‹ã€‚æ­¤å¤–ï¼Œç›®å‰çš„è‡ªåŠ¨æ–¹æ³•é€šå¸¸åˆ†åˆ«å¤„ç†æ£€æµ‹å’Œåˆ†ç±»ï¼Œä¸»è¦ä¾èµ–å•ä¸€ç«™ç‚¹ä¿¡æ¯ï¼Œé€šå¸¸éœ€è¦è¿›è¡Œä¸“é—¨çš„é¢„å¤„ç†å’Œè¡¨ç¤ºä»¥è¿›è¡Œé¢„æµ‹ã€‚è¿™äº›é™åˆ¶å¸¸å¸¸é˜»ç¢äº†å®ƒä»¬åœ¨ä¸åŒç«å±±æ¡ä»¶ä¸‹çš„å®æ—¶ç›‘æµ‹å’Œåº”ç”¨ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨è¯­ä¹‰åˆ†å‰²æ¨¡å‹é€šè¿‡ç›´æ¥å°†å¤šé€šé“ä¸€ç»´ä¿¡å·è½¬æ¢ä¸ºäºŒç»´è¡¨ç¤ºï¼Œå®ç°åœ°éœ‡äº‹ä»¶çš„è‡ªåŠ¨è¯†åˆ«ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨æ•°æ®é©±åŠ¨ã€ç«¯åˆ°ç«¯çš„è®¾è®¡ï¼Œå°†å¤šç«™åœ°éœ‡æ•°æ®æ•´åˆåœ¨ä¸€èµ·ï¼Œè¿›è¡Œæœ€å°çš„é¢„å¤„ç†ï¼ŒåŒæ—¶å®Œæˆäº”ä¸ªåœ°éœ‡äº‹ä»¶ç±»åˆ«çš„æ£€æµ‹å’Œåˆ†ç±»ã€‚æˆ‘ä»¬å¯¹å››ç§æœ€æ–°åˆ†å‰²æ¨¡å‹ï¼ˆUNetã€UNet++ã€DeepLabV3+å’ŒSwinUNetï¼‰åœ¨æ™ºåˆ©å››ä¸ªä¸åŒç«å±±è®°å½•çš„å¤§çº¦25,000ä¸ªåœ°éœ‡äº‹ä»¶è¿›è¡Œäº†è¯„ä¼°ï¼šNevados del ChillÃ¡nç«å±±ç¾¤ã€Laguna del Mauleã€Villarricaå’ŒPuyehue-CordÃ³n Caulleç«å±±ã€‚åœ¨è¿™äº›æ¨¡å‹ä¸­ï¼ŒUNetæ¶æ„è¢«è®¤å®šä¸ºæœ€æœ‰æ•ˆçš„æ¨¡å‹ï¼Œå¹³å‡F1åˆ†æ•°å’Œäº¤å¹¶æ¯”ï¼ˆIoUï¼‰å¾—åˆ†é«˜è¾¾0.91å’Œ0.88ï¼Œæ˜¾ç¤ºå‡ºå¯¹æœªè§è¿‡çš„ç«å±±æ•°æ®é›†çš„å‡ºè‰²å™ªå£°é²æ£’æ€§å’Œæ¨¡å‹çµæ´»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20595v4">PDF</a> 10 pages, 9 figures. This is a pre-print, it is currently under review for publication</p>
<p><strong>Summary</strong>ï¼šè¯¥ç ”ç©¶é‡‡ç”¨è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œé€šè¿‡ç›´æ¥å°†å¤šé€šé“ä¸€ç»´ä¿¡å·è½¬åŒ–ä¸ºäºŒç»´å›¾åƒè¡¨ç¤ºï¼Œå®ç°äº†åœ°éœ‡äº‹ä»¶çš„è‡ªåŠ¨è¯†åˆ«ã€‚è¯¥ç ”ç©¶é‡‡ç”¨æ•°æ®é©±åŠ¨çš„ç«¯åˆ°ç«¯è®¾è®¡ï¼Œæ•´åˆå¤šç«™åœ°éœ‡æ•°æ®ï¼Œæœ€å°é™åº¦é¢„å¤„ç†ï¼ŒåŒæ—¶å®Œæˆæ£€æµ‹å’Œåˆ†ç±»äº”ä¸ªåœ°éœ‡äº‹ä»¶ç±»åˆ«ã€‚åœ¨æ™ºåˆ©å››ä¸ªä¸åŒç«å±±çš„åœ°éœ‡äº‹ä»¶æ•°æ®ä¸Šè¯„ä¼°äº†å››ç§æœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ï¼Œå…¶ä¸­UNetæ¶æ„è¡¨ç°æœ€ä½³ï¼Œå…·æœ‰å™ªå£°é²æ£’æ€§å’Œæ¨¡å‹çµæ´»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶èƒŒæ™¯ï¼šç«å±±ç›‘æµ‹ä¸­åœ°éœ‡äº‹ä»¶çš„æœ‰æ•ˆè¯†åˆ«å¯¹äºç†è§£ç«å±±æ´»åŠ¨å’ŒåŠæ—¶å‘å‡ºè­¦å‘Šè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äººå·¥åˆ†æï¼Œå­˜åœ¨ä¸»è§‚æ€§å’ŒåŠ³åŠ¨å¯†é›†æ€§çš„é—®é¢˜ã€‚</li>
<li>å½“å‰æŒ‘æˆ˜ï¼šç°æœ‰è‡ªåŠ¨æ–¹æ³•å¸¸å°†æ£€æµ‹å’Œåˆ†ç±»åˆ†å¼€å¤„ç†ï¼Œä¾èµ–äºå•ä¸€ç«™ä¿¡æ¯ï¼Œéœ€è¦å®šåˆ¶é¢„å¤„ç†å’Œè¡¨ç¤ºæ¥è¿›è¡Œé¢„æµ‹ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†å®ƒä»¬åœ¨å®æ—¶ç›‘æµ‹å’Œä¸åŒç«å±±æ¡ä»¶ä¸‹çš„åº”ç”¨ã€‚</li>
<li>æ–°æ–¹æ³•ä»‹ç»ï¼šæœ¬ç ”ç©¶ä½¿ç”¨è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œé€šè¿‡ç®€å•è½¬æ¢å¤šé€šé“ä¸€ç»´ä¿¡å·ä¸ºäºŒç»´å›¾åƒè¡¨ç¤ºï¼Œå®ç°åœ°éœ‡äº‹ä»¶çš„è‡ªåŠ¨è¯†åˆ«ã€‚</li>
<li>æ•°æ®é©±åŠ¨æ–¹æ³•ï¼šé‡‡ç”¨ç«¯åˆ°ç«¯è®¾è®¡ï¼Œæ•´åˆå¤šç«™åœ°éœ‡æ•°æ®ï¼Œç®€åŒ–é¢„å¤„ç†è¿‡ç¨‹ï¼ŒåŒæ—¶è¿›è¡Œäº”ä¸ªåœ°éœ‡äº‹ä»¶ç±»åˆ«çš„æ£€æµ‹å’Œåˆ†ç±»ã€‚</li>
<li>æ¨¡å‹è¯„ä¼°ï¼šåœ¨æ™ºåˆ©å››ä¸ªä¸åŒç«å±±çš„åœ°éœ‡äº‹ä»¶æ•°æ®ä¸Šæµ‹è¯•äº†å››ç§åˆ†å‰²æ¨¡å‹ï¼ŒUNetæ¶æ„è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ€§èƒ½æŒ‡æ ‡ï¼šUNetæ¨¡å‹è¾¾åˆ°å¹³å‡F1åˆ†æ•°å’Œäº¤å¹¶æ¯”ï¼ˆIoUï¼‰é«˜è¾¾0.91å’Œ0.88ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20595">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca7a6f0a741c873aa0752daca337fe92" align="middle">
<img src="https://picx.zhimg.com/v2-32057fce7e813ca8eb849ff51743a637" align="middle">
<img src="https://picx.zhimg.com/v2-05e5f87648a96f07a862db764ccc310e" align="middle">
<img src="https://picx.zhimg.com/v2-30712bae1e37cd51ca998682000182ab" align="middle">
<img src="https://picx.zhimg.com/v2-b60b3f1082cbe2f9b5d80e40a95f49d7" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-31e8e9f9af8e409efde1ee9bf67e343c" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  SAGE Saliency-Guided Contrastive Embeddings
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e07c52a6ebb5464c9cea2c1ebe0160e9" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  MergeSlide Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
