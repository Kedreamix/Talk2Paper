<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1df2db90f43fb9813d581375e6e3e1c6')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    73 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="Training-Free-Multi-View-Extension-of-IC-Light-for-Textual-Position-Aware-Scene-Relighting"><a href="#Training-Free-Multi-View-Extension-of-IC-Light-for-Textual-Position-Aware-Scene-Relighting" class="headerlink" title="Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting"></a>Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting</h2><p><strong>Authors:Jiangnan Ye, Jiedong Zhuang, Lianrui Mu, Wenjie Zheng, Jiaqi Hu, Xingze Zou, Jing Wang, Haoji Hu</strong></p>
<p>We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.</p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†GS-Lightï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆã€å¯¹æ–‡æœ¬ä½ç½®æœ‰æ„ŸçŸ¥çš„ç®¡é“ï¼Œç”¨äºå¯¹é€šè¿‡é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºçš„3Dåœºæ™¯è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„é‡ç…§æ˜ã€‚GS-Lightå®ç°äº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å•è¾“å…¥æ‰©æ•£æ¨¡å‹çš„æ‰©å±•ï¼Œä»¥å¤„ç†å¤šè§†å›¾è¾“å…¥ã€‚ç»™å®šç”¨æˆ·æç¤ºï¼Œå¯èƒ½æŒ‡å®šç…§æ˜æ–¹å‘ã€é¢œè‰²ã€å¼ºåº¦æˆ–å‚è€ƒå¯¹è±¡ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å°†æç¤ºè§£æä¸ºç…§æ˜ä¼˜å…ˆäº‹é¡¹ã€‚æˆ‘ä»¬ä½¿ç”¨ç°æˆçš„å‡ ä½•å’Œè¯­ä¹‰ä¼°è®¡å™¨ï¼ˆæ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œè¯­ä¹‰åˆ†å‰²ï¼‰å°†è¿™äº›ç…§æ˜ä¼˜å…ˆäº‹é¡¹ä¸è§†å›¾å‡ ä½•çº¦æŸèåˆï¼Œä»¥è®¡ç®—ç…§æ˜åœ°å›¾å¹¶ä¸ºæ¯ä¸ªè§†å›¾ç”Ÿæˆåˆå§‹æ½œåœ¨ä»£ç ã€‚è¿™äº›ç²¾å¿ƒå¾—å‡ºçš„åˆå§‹æ½œç å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®åœ°åæ˜ ç”¨æˆ·æœŸæœ›çš„ç…§æ˜è¾“å‡ºï¼Œå°¤å…¶æ˜¯åœ¨ç…§æ˜æ–¹å‘æ–¹é¢ã€‚é€šè¿‡å‘æˆ‘ä»¬çš„å¤šè§†å›¾é‡ç…§æ˜æ¨¡å‹è¾“å…¥å¤šè§†å›¾æ¸²æŸ“å›¾åƒä»¥åŠåˆå§‹æ½œç ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é«˜ä¿çœŸã€è‰ºæœ¯åŒ–çš„é‡ç…§æ˜å›¾åƒã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨é‡ç…§æ˜çš„å¤–è§‚å¯¹3DGSåœºæ™¯è¿›è¡Œå¾®è°ƒï¼Œä»¥è·å¾—å®Œå…¨é‡ç…§æ˜çš„3Dåœºæ™¯ã€‚æˆ‘ä»¬åœ¨å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸Šè¯„ä¼°GS-Lightï¼Œå°†å…¶ä¸æœ€æ–°æŠ€æœ¯åŸºå‡†çº¿ï¼ˆåŒ…æ‹¬è§†å›¾é‡ç…§æ˜ã€è§†é¢‘é‡ç…§æ˜å’Œåœºæ™¯ç¼–è¾‘æ–¹æ³•ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚é€šè¿‡å®šé‡æŒ‡æ ‡ï¼ˆå¤šè§†å›¾ä¸€è‡´æ€§ã€æˆåƒè´¨é‡ã€ç¾å­¦åˆ†æ•°ã€è¯­ä¹‰ç›¸ä¼¼æ€§ç­‰ï¼‰å’Œå®šæ€§è¯„ä¼°ï¼ˆç”¨æˆ·ç ”ç©¶ï¼‰ï¼ŒGS-Lightåœ¨åŸºå‡†çº¿ä¸Šè¡¨ç°å‡ºæŒç»­æ”¹è¿›ã€‚ä»£ç å’Œèµ„äº§å°†åœ¨å‘å¸ƒæ—¶æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13684v1">PDF</a> Submitting for Neurocomputing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GS-Lightï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºçš„æ–‡æœ¬å¼•å¯¼å¼ä¸‰ç»´åœºæ™¯é‡ç…§æ˜çš„é«˜æ•ˆã€æ–‡æœ¬ä½ç½®æ„ŸçŸ¥ç®¡é“ã€‚GS-Lighté€šè¿‡æ— è®­ç»ƒçš„å•è¾“å…¥æ‰©æ•£æ¨¡å‹å¤„ç†å¤šè§†å›¾è¾“å…¥ï¼Œå®ç°ç”¨æˆ·æç¤ºè§£æä¸ºç…§æ˜å…ˆéªŒã€‚ç»“åˆç°æˆçš„å‡ ä½•å’Œè¯­ä¹‰ä¼°ç®—å™¨ï¼ˆæ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ï¼Œå°†è¿™äº›ç…§æ˜å…ˆéªŒä¸è§†å›¾å‡ ä½•çº¦æŸèåˆä»¥è®¡ç®—ç…§æ˜å›¾å¹¶ä¸ºæ¯ä¸ªè§†å›¾ç”Ÿæˆåˆå§‹æ½œåœ¨ä»£ç ã€‚è¿™äº›ç²¾å¿ƒå¾—å‡ºçš„åˆå§‹æ½œåœ¨ä»£ç æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®åœ°åæ˜ ç”¨æˆ·æœŸæœ›çš„ç…§æ˜è¾“å‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨ç…§æ˜æ–¹å‘æ–¹é¢ã€‚é€šè¿‡å‘å¤šè§†å›¾é‡ç…§æ˜æ¨¡å‹è¾“å…¥å¤šè§†å›¾æ¸²æŸ“å›¾åƒä»¥åŠåˆå§‹æ½œåœ¨ä»£ç ï¼Œç”Ÿæˆé«˜è´¨é‡ã€è‰ºæœ¯æ€§é‡ç…§æ˜å›¾åƒã€‚æœ€åï¼Œä½¿ç”¨é‡ç…§æ˜å¤–è§‚å¾®è°ƒ3DGSåœºæ™¯ï¼Œè·å¾—å®Œå…¨é‡ç…§æ˜çš„ä¸‰ç»´åœºæ™¯ã€‚GS-Lightåœ¨å®¤å†…å¤–åœºæ™¯ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼ŒåŒ…æ‹¬è§†å›¾é‡ç…§æ˜ã€è§†é¢‘é‡ç…§æ˜å’Œåœºæ™¯ç¼–è¾‘æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GS-Lightæ˜¯ä¸€ç§ç”¨äºæ–‡æœ¬å¼•å¯¼çš„ä¸‰ç»´åœºæ™¯é‡ç…§æ˜çš„ç®¡é“ï¼ŒåŸºäºé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºã€‚</li>
<li>GS-Lighté€šè¿‡æ— è®­ç»ƒçš„å•è¾“å…¥æ‰©æ•£æ¨¡å‹å¤„ç†å¤šè§†å›¾è¾“å…¥ã€‚</li>
<li>ç”¨æˆ·æç¤ºè¢«è§£æä¸ºç…§æ˜å…ˆéªŒï¼Œå¹¶ç»“åˆå‡ ä½•å’Œè¯­ä¹‰ä¼°ç®—å™¨è¿›è¡Œè®¡ç®—ã€‚</li>
<li>åˆå§‹æ½œåœ¨ä»£ç æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆåæ˜ ç”¨æˆ·æœŸæœ›çš„ç…§æ˜è¾“å‡ºï¼Œç‰¹åˆ«æ˜¯ç…§æ˜æ–¹å‘ã€‚</li>
<li>å¤šè§†å›¾æ¸²æŸ“å›¾åƒå’Œåˆå§‹æ½œåœ¨ä»£ç ç”¨äºç”Ÿæˆé«˜è´¨é‡ã€è‰ºæœ¯æ€§é‡ç…§æ˜å›¾åƒã€‚</li>
<li>é€šè¿‡å¾®è°ƒè·å¾—å®Œå…¨é‡ç…§æ˜çš„ä¸‰ç»´åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5de1aded6bd287d2278b1b06d3ab4d1e" align="middle">
<img src="https://picx.zhimg.com/v2-cf50de6bd2a2b021c48557a52d59e1ee" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Opt3DGS-Optimizing-3D-Gaussian-Splatting-with-Adaptive-Exploration-and-Curvature-Aware-Exploitation"><a href="#Opt3DGS-Optimizing-3D-Gaussian-Splatting-with-Adaptive-Exploration-and-Curvature-Aware-Exploitation" class="headerlink" title="Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation"></a>Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation</h2><p><strong>Authors:Ziyang Huang, Jiagang Chen, Jin Liu, Shunping Ji</strong></p>
<p>3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.</p>
<blockquote>
<p>3Dé«˜æ–¯æ··åˆæŠ€æœ¯ï¼ˆ3DGSï¼‰å·²æˆä¸ºæ–°è§†è§’åˆæˆçš„é‡è¦æ¡†æ¶ï¼Œä½†å…¶æ ¸å¿ƒä¼˜åŒ–æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æˆ‘ä»¬å‘ç°äº†3DGSä¼˜åŒ–ä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šé™·å…¥æ¬¡ä¼˜å±€éƒ¨æœ€ä¼˜å’Œæ”¶æ•›è´¨é‡ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Opt3DGSï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è‡ªé€‚åº”æ¢ç´¢å’Œæ›²ç‡å¼•å¯¼åˆ©ç”¨çš„ä¸¤é˜¶æ®µä¼˜åŒ–è¿‡ç¨‹æ¥å¢å¼º3DGSçš„ç¨³å¥æ¡†æ¶ã€‚åœ¨æ¢ç´¢é˜¶æ®µï¼Œè‡ªé€‚åº”åŠ æƒéšæœºæ¢¯åº¦æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ï¼ˆSGLDï¼‰æ–¹æ³•å¢å¼ºäº†å…¨å±€æœç´¢ï¼Œä»¥é€ƒé¿å±€éƒ¨æœ€ä¼˜ã€‚åœ¨å¼€å‘é˜¶æ®µï¼Œå±€éƒ¨æ‹Ÿç‰›é¡¿æ–¹å‘å¼•å¯¼çš„Adamä¼˜åŒ–å™¨åˆ©ç”¨æ›²ç‡ä¿¡æ¯å®ç°ç²¾ç¡®é«˜æ•ˆçš„æ”¶æ•›ã€‚åœ¨å¤šç§åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOpt3DGSé€šè¿‡æ”¹è¿›3DGSä¼˜åŒ–è¿‡ç¨‹ï¼ˆä¸ä¿®æ”¹å…¶åº•å±‚è¡¨ç¤ºï¼‰å®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13571v1">PDF</a> Accepted at AAAI 2026 as a Conference Paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäº3Dé«˜æ–¯å¡«å……ï¼ˆ3DGSï¼‰è¿›è¡Œæ–°é¢–è§†å›¾åˆæˆæ—¶é¢ä¸´çš„æ ¸å¿ƒä¼˜åŒ–æŒ‘æˆ˜ï¼Œå¹¶é’ˆå¯¹è¿™äº›é—®é¢˜æå‡ºäº†Opt3DGSæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªé€‚åº”æ¢ç´¢å’Œæ›²ç‡å¼•å¯¼åˆ©ç”¨çš„ä¸¤é˜¶æ®µä¼˜åŒ–è¿‡ç¨‹æ¥å¢å¼º3DGSã€‚åœ¨æ¢ç´¢é˜¶æ®µï¼Œé‡‡ç”¨è‡ªé€‚åº”åŠ æƒéšæœºæ¢¯åº¦æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ï¼ˆSGLDï¼‰æ–¹æ³•è¿›è¡Œå…¨å±€æœç´¢ï¼Œä»¥é€ƒç¦»å±€éƒ¨æœ€ä¼˜è§£ï¼›åœ¨åˆ©ç”¨é˜¶æ®µï¼Œåˆ©ç”¨å±€éƒ¨æ‹Ÿç‰›é¡¿æ–¹å‘å¼•å¯¼çš„Adamä¼˜åŒ–å™¨è¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„æ”¶æ•›ã€‚å®éªŒè¯æ˜ï¼ŒOpt3DGSåœ¨ä¸æ”¹å˜å…¶åº•å±‚è¡¨ç¤ºçš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGSä½œä¸ºæ–°é¢–è§†å›¾åˆæˆçš„é¢†å…ˆæ¡†æ¶ï¼Œä½†å…¶æ ¸å¿ƒä¼˜åŒ–æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†Opt3DGSæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”æ¢ç´¢ä¸æ›²ç‡å¼•å¯¼åˆ©ç”¨çš„ä¸¤é˜¶æ®µä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>æ¢ç´¢é˜¶æ®µé‡‡ç”¨è‡ªé€‚åº”åŠ æƒéšæœºæ¢¯åº¦æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ï¼ˆSGLDï¼‰è¿›è¡Œå…¨å±€æœç´¢ä»¥é€ƒç¦»å±€éƒ¨æœ€ä¼˜è§£ã€‚</li>
<li>åˆ©ç”¨é˜¶æ®µåˆ©ç”¨å±€éƒ¨æ‹Ÿç‰›é¡¿æ–¹å‘å¼•å¯¼çš„Adamä¼˜åŒ–å™¨è¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„æ”¶æ•›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4dda3f53801d61fccbf1451885bc480" align="middle">
<img src="https://picx.zhimg.com/v2-c6d944e560a08a7d42f000b978f12ff5" align="middle">
<img src="https://picx.zhimg.com/v2-e9831f9e31c693bf589c922e36755523" align="middle">
<img src="https://picx.zhimg.com/v2-e847b473cd2c2b3b554b71a88032de00" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SF-Recon-Simplification-Free-Lightweight-Building-Reconstruction-via-3D-Gaussian-Splatting"><a href="#SF-Recon-Simplification-Free-Lightweight-Building-Reconstruction-via-3D-Gaussian-Splatting" class="headerlink" title="SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting"></a>SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting</h2><p><strong>Authors:Zihan Li, Tengfei Wang, Wentian Gan, Hao Zhan, Xin Wang, Zongqian Zhan</strong></p>
<p>Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:<a target="_blank" rel="noopener" href="https://lzh282140127-cell.github.io/SF-Recon-project/">https://lzh282140127-cell.github.io/SF-Recon-project/</a></p>
<blockquote>
<p>è½»é‡çº§å»ºç­‘è¡¨é¢æ¨¡å‹å¯¹äºæ•°å­—åŸå¸‚ã€å¯¼èˆªå’Œå¿«é€Ÿåœ°ç†ç©ºé—´åˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¤šè§†è§’å‡ ä½•æµç¨‹ä»ç„¶å¾ˆç¹çä¸”å¯¹è´¨é‡æ•æ„Ÿï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºå¯†é›†é‡å»ºã€ç½‘æ ¼ç”Ÿæˆå’Œéšåçš„ç®€åŒ–ã€‚æœ¬ç ”ç©¶æå‡ºäº†SF-Reconæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥ä»å¤šè§†è§’å›¾åƒé‡å»ºè½»é‡çº§å»ºç­‘è¡¨é¢ï¼Œæ— éœ€åç»­ç½‘æ ¼ç®€åŒ–ã€‚æˆ‘ä»¬é¦–å…ˆè®­ç»ƒåˆå§‹çš„3Dé«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰åœºä»¥è·å¾—ä¸€è‡´çš„è§†å›¾è¡¨ç¤ºã€‚ç„¶åï¼Œé€šè¿‡æ³•çº¿æ¢¯åº¦å¼•å¯¼çš„é«˜æ–¯ä¼˜åŒ–æå–å»ºç­‘ç»“æ„ï¼Œè¯¥ä¼˜åŒ–é€‰æ‹©ä¸å±‹é¡¶å’Œå¢™å£è¾¹ç•Œå¯¹é½çš„åŸºæœ¬å½¢çŠ¶ï¼Œéšåè¿›è¡Œå¤šè§†è§’è¾¹ç¼˜ä¸€è‡´æ€§ä¿®å‰ªï¼Œä»¥æé«˜ç»“æ„æ¸…æ™°åº¦å¹¶æŠ‘åˆ¶éç»“æ„ä¼ªå½±ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚æœ€åï¼Œå¤šè§†è§’æ·±åº¦çº¦æŸçš„Delaunayä¸‰è§’å‰–åˆ†å°†ç»“æ„åŒ–é«˜æ–¯åœºè½¬åŒ–ä¸ºè½»é‡çº§ã€ç»“æ„å¿ å®çš„å»ºç­‘ç½‘æ ¼ã€‚åŸºäºæå‡ºçš„SFæ•°æ®é›†ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SF-Reconå¯ä»¥ç›´æ¥ä»å¤šè§†è§’å›¾åƒé‡å»ºè½»é‡çº§å»ºç­‘æ¨¡å‹ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°æ›´å°‘çš„é¢å’Œé¡¶ç‚¹ã€‚ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://lzh282140127-cell.github.io/SF-Recon-project/">https://lzh282140127-cell.github.io/SF-Recon-project/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13278v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSF-Reconçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»å¤šè§†è§’å›¾åƒç›´æ¥é‡å»ºè½»é‡çº§å»ºç­‘è¡¨é¢æ¨¡å‹ï¼Œæ— éœ€åç»­ç½‘æ ¼ç®€åŒ–ã€‚é€šè¿‡åˆå§‹çš„3Dé«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰åœºè·å¾—è§†è§’ä¸€è‡´è¡¨ç¤ºï¼Œéšåé€šè¿‡æ­£å¸¸æ¢¯åº¦å¼•å¯¼çš„é«˜æ–¯ä¼˜åŒ–å’ŒåŸºäºå¤šè§†è§’è¾¹ç¼˜ä¸€è‡´æ€§çš„ä¿®å‰ªæ¥å¢å¼ºç»“æ„é”åº¦å’ŒæŠ‘åˆ¶éç»“æ„ä¼ªå½±ï¼Œæœ€åé€šè¿‡å¤šè§†è§’æ·±åº¦çº¦æŸDelaunayä¸‰è§’å‰–åˆ†å°†ç»“æ„åŒ–é«˜æ–¯åœºè½¬æ¢ä¸ºè½»é‡çº§ã€ç»“æ„çœŸå®çš„å»ºç­‘ç½‘æ ¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SF-Reconæ–¹æ³•å¯ç›´æ¥ä»å¤šè§†è§’å›¾åƒé‡å»ºè½»é‡çº§å»ºç­‘è¡¨é¢æ¨¡å‹ï¼Œæ— éœ€åç»­ç½‘æ ¼ç®€åŒ–ã€‚</li>
<li>åˆå§‹é˜¶æ®µé€šè¿‡3Dé«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰åœºè·å¾—è§†è§’ä¸€è‡´è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡æ­£å¸¸æ¢¯åº¦å¼•å¯¼çš„é«˜æ–¯ä¼˜åŒ–ï¼Œé€‰æ‹©ä¸å±‹é¡¶å’Œå¢™å£è¾¹ç•Œå¯¹é½çš„åŸå§‹å…ƒç´ ã€‚</li>
<li>å¤šè§†è§’è¾¹ç¼˜ä¸€è‡´æ€§ä¿®å‰ªå¢å¼ºç»“æ„é”åº¦ï¼ŒæŠ‘åˆ¶éç»“æ„ä¼ªå½±ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºSFçš„æ•°æ®é›†ï¼Œç”¨äºå®éªŒè¯„ä¼°ã€‚</li>
<li>SF-Reconæ–¹æ³•å®ç°äº†å»ºç­‘æ¨¡å‹çš„è½»é‡åŒ–ï¼Œå‡å°‘äº†é¢æ•°å’Œé¡¶ç‚¹æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88edc0c4650a1e95a3b9c84d60c0d01d" align="middle">
<img src="https://picx.zhimg.com/v2-501970d8f38047e4613096400d828827" align="middle">
<img src="https://picx.zhimg.com/v2-6c2470608ebace34d0f6a532a35acfdf" align="middle">
<img src="https://picx.zhimg.com/v2-df669e7c134bb505a8d2d3313f5d4888" align="middle">
<img src="https://picx.zhimg.com/v2-524daf218b50d79ce901d82b0ff91193" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SymGS-Leveraging-Local-Symmetries-for-3D-Gaussian-Splatting-Compression"><a href="#SymGS-Leveraging-Local-Symmetries-for-3D-Gaussian-Splatting-Compression" class="headerlink" title="SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression"></a>SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression</h2><p><strong>Authors:Keshav Gupta, Akshat Sanghvi, Shreyas Reddy Palley, Astitva Srivastava, Charu Sharma, Avinash Sharma</strong></p>
<p>3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}</p>
<blockquote>
<p>3Dé«˜æ–¯å»¶å±•æŠ€æœ¯å·²æˆä¸ºæ–°å‹è§†è§’åˆæˆä¸­çš„å˜é©æ€§æŠ€æœ¯ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå…¶é«˜é€Ÿæ¸²æŸ“å’Œé€¼çœŸçš„ä¿çœŸåº¦ã€‚ç„¶è€Œï¼Œå…¶å†…å­˜å ç”¨éšç€åœºæ™¯çš„å¤æ‚æ€§è€Œè¿…é€Ÿæ‰©å±•ï¼Œç»å¸¸è¾¾åˆ°æ•°GBã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å¼•å…¥åˆ©ç”¨åŸºäºåŸå§‹çº§åˆ«çš„å†—ä½™çš„å‹ç¼©ç­–ç•¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡ç›¸ä¼¼æ€§æ£€æµ‹å’Œé‡åŒ–æ¥å®ç°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡èå…¥å¯¹ç§°æ„ŸçŸ¥æŠ€æœ¯æ¥è¶…è¶Šæ­¤ç±»æ–¹æ³•çš„å‹ç¼©æé™ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é•œåƒå¯¹ç§°æ¥æ¶ˆé™¤å†—ä½™åŸå§‹æ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å‹ç¼©æ¡†æ¶â€œSymGSâ€ï¼Œå°†å¯å­¦ä¹ çš„é•œåƒå¼•å…¥åœºæ™¯ï¼Œä»è€Œæ¶ˆé™¤å±€éƒ¨å’Œå…¨å±€åå°„å†—ä½™ä»¥å®ç°å‹ç¼©ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥ä½œä¸ºæœ€æ–°å‹ç¼©æ–¹æ³•çš„å³æ’å³ç”¨å¢å¼ºåŠŸèƒ½ï¼ˆä¾‹å¦‚HACï¼‰ï¼Œä»¥å®ç°è¿›ä¸€æ­¥çš„å‹ç¼©ã€‚ä¸HACç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†1.66å€çš„å‹ç¼©ï¼ˆå¤§è§„æ¨¡åœºæ™¯ä¸Šå¯è¾¾3å€ï¼‰ã€‚å¹³å‡è€Œè¨€ï¼ŒSymGSå®ç°äº†å¯¹3DGSåœºæ™¯çš„108å€å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒäº†æ¸²æŸ“è´¨é‡ã€‚é¡¹ç›®é¡µé¢å’Œè¡¥å……ææ–™å¯åœ¨â€‹â€‹symgs.github.ioâ€‹â€‹æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13264v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://symgs.github.io/">https://symgs.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸‰ç»´é«˜æ–¯å±•å¹³æŠ€æœ¯ä¸­çš„å¯¹ç§°æ„ŸçŸ¥å‹ç¼©æ¡†æ¶SymGSã€‚å®ƒé€šè¿‡å¼•å…¥å­¦ä¹ é•œåƒæ¥æ¶ˆé™¤åœºæ™¯ä¸­çš„å±€éƒ¨å’Œå…¨å±€åå°„å†—ä½™ï¼Œä»¥æé«˜å‹ç¼©æ•ˆç‡å¹¶é™ä½å†…å­˜å ç”¨ã€‚SymGSå¯ä½œä¸ºä¸€ä¸ªæ’ä»¶å¢å¼ºç°æœ‰å‹ç¼©æ–¹æ³•ï¼ˆå¦‚HACï¼‰ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°æ›´é«˜çš„å‹ç¼©ç‡ï¼Œå¹³å‡å‹ç¼©ç‡è¾¾åˆ°108å€ï¼ŒåŒæ—¶ä¿æŒæ¸²æŸ“è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dé«˜æ–¯å±•å¹³æŠ€æœ¯åœ¨æ–°å‹è§†å›¾åˆæˆä¸­å…·æœ‰å˜é©æ€§å½±å“ï¼Œå…·æœ‰é«˜æ¸²æŸ“é€Ÿåº¦å’Œé€¼çœŸçš„ä¿çœŸåº¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„å†…å­˜å ç”¨éšåœºæ™¯å¤æ‚åº¦è€Œå¿«é€Ÿå¢åŠ ï¼Œè¾¾åˆ°æ•°GBã€‚</li>
<li>å¯¹ç§°æ„ŸçŸ¥æŠ€æœ¯æ—¨åœ¨è¶…è¶Šç°æœ‰æ–¹æ³•çš„å‹ç¼©é™åˆ¶ï¼Œé€šè¿‡æ¶ˆé™¤å†—ä½™åŸºæœ¬å…ƒç´ æ¥æé«˜å‹ç¼©æ•ˆç‡ã€‚</li>
<li>SymGSæ¡†æ¶å¼•å…¥å­¦ä¹ é•œåƒæ¥æ¶ˆé™¤å±€éƒ¨å’Œå…¨å±€åå°„å†—ä½™ï¼Œå®ç°é«˜æ•ˆå‹ç¼©ã€‚</li>
<li>SymGSå¯ä½œä¸ºæ’ä»¶å¢å¼ºç°æœ‰å‹ç¼©æ–¹æ³•ï¼ˆå¦‚HACï¼‰ï¼Œå®ç°æ›´é«˜çš„å‹ç¼©ç‡ã€‚</li>
<li>ä¸HACç›¸æ¯”ï¼ŒSymGSåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾1.66å€çš„å‹ç¼©ç‡ï¼ˆåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸Šå¯è¾¾3å€ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b7219d3a0bef1d466d039f35cef0ea4" align="middle">
<img src="https://picx.zhimg.com/v2-beae98e5e734e749cbe638fb0a5b748d" align="middle">
<img src="https://picx.zhimg.com/v2-b5cfd02ec4dcb68f30c87ff8a9697c99" align="middle">
<img src="https://picx.zhimg.com/v2-0224069b58cb50d6d13c0dce2732dc84" align="middle">
<img src="https://picx.zhimg.com/v2-3179574c884315b7422b8a2c0c0ff366" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Monocular-3D-Lane-Detection-via-Structure-Uncertainty-Aware-Network-with-Curve-Point-Queries"><a href="#Monocular-3D-Lane-Detection-via-Structure-Uncertainty-Aware-Network-with-Curve-Point-Queries" class="headerlink" title="Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries"></a>Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries</h2><p><strong>Authors:Ruixin Liu, Zejian Yuan</strong></p>
<p>Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a birdâ€™s-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at <a target="_blank" rel="noopener" href="https://github.com/lrx02/MonoUnc">https://github.com/lrx02/MonoUnc</a>.</p>
<blockquote>
<p>å•ç›®3Dè½¦é“æ£€æµ‹é¢ä¸´ç”±å›ºæœ‰è§‚æµ‹å™ªå£°å¼•èµ·çš„å¶ç„¶ä¸ç¡®å®šæ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºç®€åŒ–çš„å‡ ä½•å‡è®¾ï¼Œå¦‚ç‹¬ç«‹ç‚¹é¢„æµ‹æˆ–å…¨å±€å¹³é¢å»ºæ¨¡ï¼Œæ— æ³•æ•æ‰çœŸå®åœºæ™¯ä¸­çš„ç»“æ„å˜åŒ–å’Œå¶ç„¶ä¸ç¡®å®šæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MonoUncï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é¸Ÿç°å›¾ï¼ˆBEVï¼‰çš„3Dè½¦é“æ£€æµ‹å™¨ï¼Œèƒ½å¤Ÿæ˜¾å¼åœ°é€šè¿‡å±€éƒ¨è½¦é“ç»“æ„å¯¹å¶ç„¶ä¸ç¡®å®šæ€§è¿›è¡Œå»ºæ¨¡ã€‚å…·ä½“è€Œè¨€ï¼Œ3Dè½¦é“è¢«æŠ•å½±åˆ°å‰è§†ï¼ˆFVï¼‰ç©ºé—´ï¼Œå¹¶ç”±å‚æ•°æ›²çº¿è¿‘ä¼¼è¡¨ç¤ºã€‚åœ¨æ›²çº¿é¢„æµ‹çš„å¼•å¯¼ä¸‹ï¼Œä¸º3Dç©ºé—´ä¸­çš„è½¦é“ç‚¹é¢„æµ‹åŠ¨æ€ç”Ÿæˆæ›²çº¿ç‚¹æŸ¥è¯¢åµŒå…¥ã€‚ç”±ä¸¤ä¸ªç›¸é‚»ç‚¹å½¢æˆçš„æ¯ä¸ªçº¿æ®µéƒ½è¢«å»ºæ¨¡ä¸º3Dé«˜æ–¯åˆ†å¸ƒï¼Œé€šè¿‡å±€éƒ¨ç»“æ„å’Œä¸ç¡®å®šæ€§ä¼°è®¡è¿›è¡Œå‚æ•°åŒ–ã€‚å› æ­¤ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„3Dé«˜æ–¯åŒ¹é…æŸå¤±ï¼Œä»¥è”åˆçº¦æŸè¿™äº›å‚æ•°ã€‚åœ¨ONCE-3DLaneså’ŒOpenLaneæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMonoUncåœ¨æ›´ä¸¥æ ¼çš„è¯„ä¼°æ ‡å‡†ä¸‹ï¼Œåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„ï¼ˆSoTAï¼‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºONCE-3DLanesæå‡ºäº†ä¸¤é¡¹ç»¼åˆè¯„ä»·æŒ‡æ ‡ï¼Œé€šè¿‡è®¡ç®—å¹³å‡å’Œæœ€å¤§åŒå‘Chamferè·ç¦»æ¥é‡åŒ–å…¨å±€å’Œå±€éƒ¨è¯¯å·®ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/lrx02/MonoUnc%E3%80%82">https://github.com/lrx02/MonoUncã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13055v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMonoUncçš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰è‡ªç”±3Dè½¦é“æ£€æµ‹å™¨ï¼Œå®ƒæ˜¾å¼å»ºæ¨¡ç”±å±€éƒ¨è½¦é“ç»“æ„ä¿¡æ¯å¼•èµ·çš„å¶ç„¶ä¸ç¡®å®šæ€§ã€‚é€šè¿‡æŠ•å½±3Dè½¦é“åˆ°å‰è§†ï¼ˆFVï¼‰ç©ºé—´å¹¶ä½¿ç”¨å‚æ•°æ›²çº¿è¿‘ä¼¼è¡¨ç¤ºï¼ŒåŠ¨æ€ç”Ÿæˆæ›²çº¿ç‚¹æŸ¥è¯¢åµŒå…¥ä»¥è¿›è¡Œ3Dç©ºé—´ä¸­çš„è½¦é“ç‚¹é¢„æµ‹ã€‚æ¯ä¸ªç”±ç›¸é‚»ä¸¤ç‚¹å½¢æˆçš„çº¿æ®µè¢«å»ºæ¨¡ä¸º3Dé«˜æ–¯åˆ†å¸ƒï¼Œç”±å±€éƒ¨ç»“æ„å’Œä¸ç¡®å®šæ€§ä¼°è®¡å‚æ•°åŒ–ã€‚åœ¨ONCE-3DLaneså’ŒOpenLaneæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMonoUncåœ¨æ›´ä¸¥æ ¼çš„è¯„ä¼°æ ‡å‡†ä¸‹å…¨é¢è¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MonoUncæ˜¯ä¸€ç§æ–°å‹çš„3Dè½¦é“æ£€æµ‹æ–¹æ³•ï¼Œèƒ½å¤Ÿæ˜¾å¼å»ºæ¨¡å¶ç„¶ä¸ç¡®å®šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨å‰è§†ç©ºé—´æŠ•å½±å’Œå‚æ•°æ›²çº¿è¿‘ä¼¼è¡¨ç¤º3Dè½¦é“ã€‚</li>
<li>é€šè¿‡åŠ¨æ€ç”Ÿæˆæ›²çº¿ç‚¹æŸ¥è¯¢åµŒå…¥è¿›è¡Œ3Dç©ºé—´ä¸­çš„è½¦é“ç‚¹é¢„æµ‹ã€‚</li>
<li>æ¯ä¸ªçº¿æ®µè¢«è§†ä¸º3Dé«˜æ–¯åˆ†å¸ƒï¼Œè€ƒè™‘äº†å±€éƒ¨ç»“æ„å’Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
<li>è®¾è®¡äº†æ–°å‹çš„3Dé«˜æ–¯åŒ¹é…æŸå¤±ä»¥è”åˆçº¦æŸå‚æ•°ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMonoUncæ€§èƒ½ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6de62e9f6cd9285ecc2f3ad28678a1a7" align="middle">
<img src="https://picx.zhimg.com/v2-4c1b9aba05282c0ad3ec5d86a0fde862" align="middle">
<img src="https://picx.zhimg.com/v2-69aaf58889af0b42a185a6f2d4fa3e40" align="middle">
<img src="https://picx.zhimg.com/v2-b3c3bdbba0b0b4db625d0cd97b6a0c33" align="middle">
<img src="https://picx.zhimg.com/v2-73d5a52053e9c2418ecb094211ce8df7" align="middle">
<img src="https://picx.zhimg.com/v2-0dae72f5e47e7901fa7df727e8a9b065" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Darkness-Thermal-Supervised-3D-Gaussian-Splatting-for-Low-Light-Novel-View-Synthesis"><a href="#Beyond-Darkness-Thermal-Supervised-3D-Gaussian-Splatting-for-Low-Light-Novel-View-Synthesis" class="headerlink" title="Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis"></a>Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis</h2><p><strong>Authors:Qingsen Ma, Chen Zou, Dianyun Wang, Jia Wang, Liuyu Xiang, Zhaofeng He</strong></p>
<p>Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.</p>
<blockquote>
<p>åœ¨æä½å…‰æ¡ä»¶ä¸‹ï¼Œæ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰åœ¨å‡ ä½•ã€è‰²å½©ä¸€è‡´æ€§å’Œè¾å°„ç¨³å®šæ€§æ–¹é¢é¢ä¸´ä¸¥é‡é€€åŒ–ã€‚å½“ç›´æ¥åº”ç”¨äºæ›å…‰ä¸è¶³çš„è¾“å…¥æ—¶ï¼Œæ ‡å‡†çš„ä¸‰ç»´é«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰æµæ°´çº¿ä¼šå¤±æ•ˆï¼Œå› ä¸ºç‹¬ç«‹çš„è§†å›¾å¢å¼ºä¼šå¯¼è‡´ç…§æ˜ä¸ä¸€è‡´å’Œå‡ ä½•å¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DTGSï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒé€šè¿‡Retinexå¯å‘å¼çš„ç…§æ˜åˆ†è§£ä¸çƒ­å¼•å¯¼çš„ä¸‰ç»´é«˜æ–¯å–·ç»˜ç´§å¯†è€¦åˆï¼Œå®ç°ç…§æ˜ä¸å˜é‡å»ºã€‚ä¸åŒäºå°†å¢å¼ºä½œä¸ºé¢„å¤„ç†æ­¥éª¤çš„å…ˆå‰æ–¹æ³•ï¼ŒDTGSé€šè¿‡å¾ªç¯å¢å¼º-é‡å»ºæœºåˆ¶ï¼Œåœ¨å¢å¼ºã€å‡ ä½•å’Œçƒ­ç›‘ç£ä¹‹é—´è¿›è¡Œè”åˆä¼˜åŒ–ã€‚çƒ­ç›‘ç£åˆ†æ”¯é€šè¿‡åŠ¨æ€å¹³è¡¡å¢å¼ºã€ç»“æ„å’Œçƒ­æŸå¤±ï¼Œç¨³å®šé¢œè‰²æ¢å¤å’Œå‡ ä½•å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒåµŒå…¥åœ¨3DGSå¾ªç¯å†…çš„åŸºäºRetinexçš„åˆ†è§£æ¨¡å—æä¾›ç‰©ç†å¯è§£é‡Šçš„åå°„-ç…§æ˜åˆ†ç¦»ï¼Œç¡®ä¿è·¨è§†ç‚¹çš„é¢œè‰²å’Œçº¹ç†ä¸€è‡´æ€§ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†RGBT-LOWï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šè§†è§’ä½å…‰çƒ­æ•°æ®é›†ï¼Œæ•æ‰ä¸¥é‡çš„ç…§æ˜é€€åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDTGSåœ¨æç«¯ç…§æ˜æ¡ä»¶ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä½å…‰å¢å¼ºå’Œ3Dé‡å»ºåŸºçº¿ï¼Œå®ç°äº†å“è¶Šçš„è¾å°„ä¸€è‡´æ€§ã€å‡ ä½•ä¿çœŸåº¦å’Œé¢œè‰²ç¨³å®šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13011v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æè¿°äº†åœ¨æä½å…‰ç…§æ¡ä»¶ä¸‹ï¼Œæ ‡å‡†ä¸‰ç»´é«˜æ–¯æ•£æ–‘ï¼ˆ3DGSï¼‰åœ¨å¤„ç†æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†DTGSç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡Retinexçµæ„Ÿç…§æ˜åˆ†è§£ä¸çƒ­å¼•å¯¼ä¸‰ç»´é«˜æ–¯æ•£æ–‘æŠ€æœ¯ç´§å¯†ç»“åˆï¼Œå®ç°äº†ç…§æ˜ä¸å˜é‡å»ºã€‚æ­¤å¤–ï¼Œæ–°æ„å»ºçš„RGBT-LOWå¤šè§†è§’ä½å…‰çƒ­æ•°æ®é›†ç”¨äºè¯„ä¼°æ–°æ–¹æ³•åœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚å®éªŒè¯æ˜ï¼ŒDTGSåœ¨è¾å°„åº¦é‡ä¸€è‡´æ€§ã€å‡ ä½•ä¿çœŸåº¦å’Œé¢œè‰²ç¨³å®šæ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨æä½å…‰ç…§æ¡ä»¶ä¸‹ï¼Œæ ‡å‡†ä¸‰ç»´é«˜æ–¯æ•£æ–‘æŠ€æœ¯é¢ä¸´å‡ ä½•ã€é¢œè‰²ä¸€è‡´æ€§å’Œè¾å°„ç¨³å®šæ€§æ–¹é¢çš„ä¸¥é‡é€€åŒ–é—®é¢˜ã€‚</li>
<li>DTGSæ¡†æ¶è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå®ƒé€šè¿‡Retinexçµæ„Ÿç…§æ˜åˆ†è§£ä¸çƒ­å¼•å¯¼ä¸‰ç»´é«˜æ–¯æ•£æ–‘æŠ€æœ¯å®ç°ç…§æ˜ä¸å˜é‡å»ºã€‚</li>
<li>DTGSé‡‡ç”¨å¾ªç¯å¢å¼ºé‡å»ºæœºåˆ¶ï¼Œè”åˆä¼˜åŒ–å¢å¼ºã€å‡ ä½•å’Œçƒ­ç›‘ç£ã€‚</li>
<li>çƒ­ç›‘ç£åˆ†æ”¯é€šè¿‡åŠ¨æ€å¹³è¡¡å¢å¼ºã€ç»“æ„å’Œçƒ­æŸå¤±æ¥ç¨³å®šé¢œè‰²æ¢å¤å’Œå‡ ä½•å­¦ä¹ ã€‚</li>
<li>RetinexåŸºäºåˆ†è§£æ¨¡å—çš„åµŒå…¥æä¾›äº†ç‰©ç†å¯è§£é‡Šçš„åå°„ç…§æ˜åˆ†ç¦»ï¼Œç¡®ä¿è·¨è§†ç‚¹çš„é¢œè‰²å’Œçº¹ç†ä¸€è‡´æ€§ã€‚</li>
<li>æ–°æ„å»ºçš„RGBT-LOWå¤šè§†è§’ä½å…‰çƒ­æ•°æ®é›†ç”¨äºè¯„ä¼°æ–¹æ³•åœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e759a7113e547b9023aed20180f312ac" align="middle">
<img src="https://picx.zhimg.com/v2-7e5ac18908d3af89a8e54e7d9f11a7dd" align="middle">
<img src="https://picx.zhimg.com/v2-8518a37dc1b596708ac4a24740ed4d4d" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TR-Gaussians-High-fidelity-Real-time-Rendering-of-Planar-Transmission-and-Reflection-with-3D-Gaussian-Splatting"><a href="#TR-Gaussians-High-fidelity-Real-time-Rendering-of-Planar-Transmission-and-Reflection-with-3D-Gaussian-Splatting" class="headerlink" title="TR-Gaussians: High-fidelity Real-time Rendering of Planar Transmission and Reflection with 3D Gaussian Splatting"></a>TR-Gaussians: High-fidelity Real-time Rendering of Planar Transmission and Reflection with 3D Gaussian Splatting</h2><p><strong>Authors:Yong Liu, Keyang Ye, Tianjia Shao, Kun Zhou</strong></p>
<p>We propose Transmission-Reflection Gaussians (TR-Gaussians), a novel 3D-Gaussian-based representation for high-fidelity rendering of planar transmission and reflection, which are ubiquitous in indoor scenes. Our method combines 3D Gaussians with learnable reflection planes that explicitly model the glass planes with view-dependent reflectance strengths. Real scenes and transmission components are modeled by 3D Gaussians and the reflection components are modeled by the mirrored Gaussians with respect to the reflection plane. The transmission and reflection components are blended according to a Fresnel-based, view-dependent weighting scheme, allowing for faithful synthesis of complex appearance effects under varying viewpoints. To effectively optimize TR-Gaussians, we develop a multi-stage optimization framework incorporating color and geometry constraints and an opacity perturbation mechanism. Experiments on different datasets demonstrate that TR-Gaussians achieve real-time, high-fidelity novel view synthesis in scenes with planar transmission and reflection, and outperform state-of-the-art approaches both quantitatively and qualitatively.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¼ è¾“-åå°„é«˜æ–¯ï¼ˆTR-Gaussiansï¼‰è¿™ä¸€æ–°é¢–çš„ä¸‰ç»´é«˜æ–¯åŸºè¡¨ç¤ºæ–¹æ³•ï¼Œç”¨äºåœ¨å®¤å†…åœºæ™¯ä¸­æ™®éå­˜åœ¨çš„å¹³é¢ä¼ è¾“å’Œåå°„çš„é«˜ä¿çœŸæ¸²æŸ“ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¸‰ç»´é«˜æ–¯å’Œå¯å­¦ä¹ çš„åå°„å¹³é¢ï¼Œåè€…å¯æ˜¾å¼åœ°æ¨¡æ‹Ÿç»ç’ƒå¹³é¢çš„è§†è§’ç›¸å…³åå°„å¼ºåº¦ã€‚ä¸‰ç»´é«˜æ–¯æ¨¡æ‹ŸçœŸå®åœºæ™¯å’Œä¼ è¾“æˆåˆ†ï¼Œè€Œåå°„æˆåˆ†åˆ™æ ¹æ®åå°„å¹³é¢é•œåƒå¯¹ç§°çš„é«˜æ–¯è¿›è¡Œæ¨¡æ‹Ÿã€‚ä¼ è¾“å’Œåå°„æˆåˆ†æ ¹æ®åŸºäºFresnelçš„è§†è§’ç›¸å…³åŠ æƒæ–¹æ¡ˆè¿›è¡Œæ··åˆï¼Œèƒ½å¤Ÿåœ¨ä¸åŒè§†è§’ä¸‹åˆæˆå¤æ‚çš„æ•ˆæœã€‚ä¸ºäº†æœ‰æ•ˆåœ°ä¼˜åŒ–TR-Gaussiansï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šé˜¶æ®µä¼˜åŒ–æ¡†æ¶ï¼Œèåˆäº†é¢œè‰²å’Œå‡ ä½•çº¦æŸä»¥åŠä¸€ä¸ªé€æ˜åº¦æ‰°åŠ¨æœºåˆ¶ã€‚åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTR-Gaussianså®ç°äº†å¹³é¢ä¼ è¾“å’Œåå°„åœºæ™¯çš„é«˜ä¿çœŸå®æ—¶æ–°è§†è§’åˆæˆï¼Œå¹¶åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯çš„å‰æ²¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13009v1">PDF</a> 15 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åŸºäº3Dé«˜æ–¯çš„æ–°å‹è¡¨ç¤ºæ–¹æ³•â€”â€”ä¼ è¾“åå°„é«˜æ–¯ï¼ˆTR-Gaussiansï¼‰ï¼Œç”¨äºå®¤å†…åœºæ™¯ä¸­å¹³é¢ä¼ è¾“å’Œåå°„çš„é«˜ä¿çœŸæ¸²æŸ“ã€‚è¯¥æ–¹æ³•ç»“åˆäº†3Dé«˜æ–¯å’Œå¯å­¦ä¹ çš„åå°„å¹³é¢ï¼Œé€šè¿‡è§†å›¾ç›¸å…³çš„åå°„å¼ºåº¦æ˜ç¡®å»ºæ¨¡ç»ç’ƒå¹³é¢ã€‚é€šè¿‡Fresnel-basedçš„è§†å›¾ç›¸å…³åŠ æƒæ–¹æ¡ˆæ··åˆä¼ è¾“å’Œåå°„æˆåˆ†ï¼Œå®ç°ä¸åŒè§†ç‚¹ä¸‹çš„å¤æ‚å¤–è§‚æ•ˆæœçš„å¿ å®åˆæˆã€‚å¼€å‘çš„å¤šé˜¶æ®µä¼˜åŒ–æ¡†æ¶çº³å…¥é¢œè‰²å’Œå‡ ä½•çº¦æŸä»¥åŠä¸é€æ˜åº¦æ‰°åŠ¨æœºåˆ¶ï¼Œæœ‰æ•ˆä¼˜åŒ–TR-Gaussiansã€‚å®éªŒè¯æ˜ï¼ŒTR-Gaussiansåœ¨å…·æœ‰å¹³é¢ä¼ è¾“å’Œåå°„çš„åœºæ™¯ä¸­å®ç°å®æ—¶é«˜ä¿çœŸæ–°é¢–è§†å›¾åˆæˆï¼Œå¹¶åœ¨å®šé‡å’Œå®šæ€§ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäº3Dé«˜æ–¯çš„æ–¹æ³•â€”â€”ä¼ è¾“åå°„é«˜æ–¯ï¼ˆTR-Gaussiansï¼‰ï¼Œç”¨äºå®¤å†…åœºæ™¯çš„å¹³é¢ä¼ è¾“å’Œåå°„çš„é«˜ä¿çœŸæ¸²æŸ“ã€‚</li>
<li>ç»“åˆäº†3Dé«˜æ–¯å’Œå¯å­¦ä¹ çš„åå°„å¹³é¢ï¼Œæ˜ç¡®å»ºæ¨¡ç»ç’ƒå¹³é¢çš„è§†å›¾ç›¸å…³åå°„å¼ºåº¦ã€‚</li>
<li>é€šè¿‡Fresnel-basedçš„è§†å›¾ç›¸å…³åŠ æƒæ–¹æ¡ˆæ··åˆä¼ è¾“å’Œåå°„æˆåˆ†ï¼Œå®ç°å¤æ‚å¤–è§‚æ•ˆæœçš„å¿ å®åˆæˆã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªå¤šé˜¶æ®µä¼˜åŒ–æ¡†æ¶ï¼Œçº³å…¥é¢œè‰²å’Œå‡ ä½•çº¦æŸä»¥åŠä¸é€æ˜åº¦æ‰°åŠ¨æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–TR-Gaussiansã€‚</li>
<li>TR-Gaussiansèƒ½å¤Ÿå®ç°å®æ—¶é«˜ä¿çœŸæ–°é¢–è§†å›¾åˆæˆï¼Œåœ¨å…·æœ‰å¹³é¢ä¼ è¾“å’Œåå°„çš„åœºæ™¯ä¸­æœ‰å‡ºè‰²è¡¨ç°ã€‚</li>
<li>TR-Gaussiansåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6657002f0de75bd52ff9c8241ab7f54" align="middle">
<img src="https://picx.zhimg.com/v2-94ebc09aaad6bfa4beee1cf032d14fc1" align="middle">
<img src="https://picx.zhimg.com/v2-82a78895f0a9af0e06dcd93fac36f8b1" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SplatSearch-Instance-Image-Goal-Navigation-for-Mobile-Robots-using-3D-Gaussian-Splatting-and-Diffusion-Models"><a href="#SplatSearch-Instance-Image-Goal-Navigation-for-Mobile-Robots-using-3D-Gaussian-Splatting-and-Diffusion-Models" class="headerlink" title="SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models"></a>SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models</h2><p><strong>Authors:Siddarth Narasimhan, Matthew Lisondra, Haitong Wang, Goldie Nejat</strong></p>
<p>The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.</p>
<blockquote>
<p>å®ä¾‹å›¾åƒç›®æ ‡å¯¼èˆªï¼ˆIINï¼‰é—®é¢˜è¦æ±‚éƒ¨ç½²åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„ç§»åŠ¨æœºå™¨äººä»…ä½¿ç”¨å•ä¸ªç›®æ ‡å‚è€ƒå›¾åƒæ¥æœç´¢ç‰¹å®šå¯¹è±¡æˆ–æ„Ÿå…´è¶£çš„äººã€‚å½“é¢ä¸´ä»¥ä¸‹æƒ…å†µæ—¶ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼š1ï¼‰å‚è€ƒå›¾åƒæ˜¯ä»ä»»æ„è§†è§’æ•è·çš„ï¼›2ï¼‰æœºå™¨äººå¿…é¡»åœ¨ç¨€ç–è§†å›¾åœºæ™¯é‡å»ºä¸­è¿›è¡Œæ“ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥SplatSearchæ¥è§£å†³IINé—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒåˆ©ç”¨ç¨€ç–è§†å›¾ä¸‰ç»´é«˜æ–¯æ‘Šé“ºé‡å»ºæŠ€æœ¯ï¼ˆSparse-view 3D Gaussian Splatting, ç®€ç§°3DGSï¼‰ã€‚SplatSearchä½¿ç”¨ç¨€ç–åœ¨çº¿3DGSåœ°å›¾æ¸²æŸ“å€™é€‰å¯¹è±¡å‘¨å›´çš„å¤šè§†è§’å›¾åƒï¼Œå¹¶ä½¿ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹æ¥è¡¥å……æ¸²æŸ“å›¾åƒçš„ç¼ºå¤±åŒºåŸŸï¼Œä»è€Œå®ç°ä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„ç¨³å¥ç‰¹å¾åŒ¹é…ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹å‰æ²¿æ¢ç´¢ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨åˆæˆè§†è§’çš„è§†è§‰ä¸Šä¸‹æ–‡å’Œç›®æ ‡å›¾åƒä¸­çš„è¯­ä¹‰ä¸Šä¸‹æ–‡æ¥è¯„ä¼°å‰æ²¿ä½ç½®ï¼Œä»è€Œä½¿æœºå™¨äººèƒ½å¤Ÿä¼˜å…ˆæ¢ç´¢åœ¨è¯­ä¹‰å’Œè§†è§‰ä¸Šä¸ç›®æ ‡å›¾åƒç›¸å…³çš„å‰æ²¿ã€‚åœ¨é€¼çœŸçš„å®¶åº­å’ŒçœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„å¤§é‡å®éªŒéªŒè¯äº†SplatSearchç›¸å¯¹äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•å…·æœ‰æ›´é«˜çš„æˆåŠŸç‡å’ŒæˆåŠŸè·¯å¾„é•¿åº¦æ–¹é¢çš„æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¯å®äº†SplatSearchçš„è®¾è®¡é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12972v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://splat-search.github.io/">https://splat-search.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹å®ä¾‹å›¾åƒå¯¼èˆªï¼ˆIINï¼‰é—®é¢˜æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼šSplatSearchã€‚å®ƒåˆ©ç”¨ç¨€ç–è§†è§’çš„3Dé«˜æ–¯Splattingï¼ˆ3DGSï¼‰é‡å»ºæŠ€æœ¯ï¼Œé€šè¿‡æ¸²æŸ“ç›®æ ‡å¯¹è±¡çš„å¤šè§†è§’å›¾åƒæ¥è§£å†³é—®é¢˜ã€‚SplatSearchåˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹å®Œæˆæ¸²æŸ“å›¾åƒçš„ç¼ºå¤±åŒºåŸŸï¼Œä½¿å…¶ä¸ç›®æ ‡å›¾åƒè¿›è¡Œç¨³å¥çš„ç‰¹å¾åŒ¹é…ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„è¾¹ç•Œæ¢ç´¢ç­–ç•¥ï¼Œç»“åˆåˆæˆè§†è§’çš„è§†è§‰ä¸Šä¸‹æ–‡å’Œç›®æ ‡å›¾åƒçš„è¯­ä¹‰ä¸Šä¸‹æ–‡æ¥è¯„ä¼°è¾¹ç•Œä½ç½®ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿä¼˜å…ˆæ¢ç´¢è¯­ä¹‰ä¸Šå’Œè§†è§‰ä¸Šä¸ç›®æ ‡å›¾åƒç›¸å…³çš„è¾¹ç•Œã€‚åœ¨é€¼çœŸçš„å®¶åº­å’ŒçœŸå®ç¯å¢ƒä¸­çš„å®éªŒéªŒè¯äº†SplatSearchåœ¨æˆåŠŸç‡å’ŒæˆåŠŸè·¯å¾„é•¿åº¦æ–¹é¢çš„é«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ä¾‹å›¾åƒå¯¼èˆªï¼ˆIINï¼‰é—®é¢˜è¦æ±‚æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­ä»…ä½¿ç”¨å•ä¸ªç›®æ ‡å‚è€ƒå›¾åƒæœç´¢ç‰¹å®šå¯¹è±¡æˆ–äººç‰©ã€‚</li>
<li>SplatSearchæ˜¯ä¸€ç§è§£å†³IINé—®é¢˜çš„æ–°æ¶æ„ï¼Œåˆ©ç”¨ç¨€ç–è§†è§’çš„3Dé«˜æ–¯Splattingï¼ˆ3DGSï¼‰é‡å»ºã€‚</li>
<li>SplatSearché€šè¿‡æ¸²æŸ“ç›®æ ‡å¯¹è±¡çš„å¤šè§†è§’å›¾åƒï¼Œå¹¶åˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹å®Œæˆå›¾åƒç¼ºå¤±åŒºåŸŸçš„å¡«å……ï¼Œå®ç°ä¸ç›®æ ‡å›¾åƒçš„ç¨³å®šç‰¹å¾åŒ¹é…ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è¾¹ç•Œæ¢ç´¢ç­–ç•¥ï¼Œç»“åˆè§†è§‰å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡æ¥è¯„ä¼°è¾¹ç•Œä½ç½®ï¼ŒæŒ‡å¯¼æœºå™¨äººçš„æ¢ç´¢æ–¹å‘ã€‚</li>
<li>åœ¨çœŸå®å’Œä»¿çœŸç¯å¢ƒä¸‹çš„å®éªŒéªŒè¯äº†SplatSearchåœ¨æˆåŠŸç‡å’Œè·¯å¾„è§„åˆ’ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>Ablationç ”ç©¶ç¡®è®¤äº†SplatSearchçš„è®¾è®¡é€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>SplatSearchèƒ½å¤Ÿä¸ºç§»åŠ¨æœºå™¨äººæä¾›æ›´ç²¾ç¡®çš„å¯¼èˆªï¼Œå°¤å…¶åœ¨ç¯å¢ƒä¿¡æ¯ä¸å®Œå…¨å·²çŸ¥çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e400049e19e2fa458c2be8534c816e2" align="middle">
<img src="https://picx.zhimg.com/v2-4974c1c02830db9f482c2ebb7da7cba8" align="middle">
<img src="https://picx.zhimg.com/v2-a6e879fd3fdb7f2fb81ceec6c69aa0b6" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Neo-Real-Time-On-Device-3D-Gaussian-Splatting-with-Reuse-and-Update-Sorting-Acceleration"><a href="#Neo-Real-Time-On-Device-3D-Gaussian-Splatting-with-Reuse-and-Update-Sorting-Acceleration" class="headerlink" title="Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration"></a>Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration</h2><p><strong>Authors:Changhun Oh, Seongryong Oh, Jinwoo Hwang, Yoonsung Kim, Hardik Sharma, Jongse Park</strong></p>
<p>3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR&#x2F;VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.</p>
<blockquote>
<p>å®æ—¶åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šè¿›è¡Œä¸‰ç»´é«˜æ–¯å±•å¼€ï¼ˆ3DGSï¼‰æ¸²æŸ“å¯¹äºæä¾›æ²‰æµ¸å¼å¢å¼ºå’Œè™šæ‹Ÿç°å®ï¼ˆAR&#x2F;VRï¼‰ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆéš¾ä»¥å®ç°é«˜å¸§ç‡ï¼Œå°¤å…¶æ˜¯åœ¨é«˜åˆ†è¾¨ç‡æ¸²æŸ“æ–¹é¢ã€‚æˆ‘ä»¬çš„åˆ†æå°†3DGSæ¸²æŸ“æµæ°´çº¿ä¸­çš„æ’åºé˜¶æ®µè¯†åˆ«ä¸ºä¸»è¦ç“¶é¢ˆï¼ŒåŸå› åœ¨äºå…¶å¯¹é«˜å†…å­˜å¸¦å®½çš„éœ€æ±‚ã€‚æœ¬æ–‡ä»‹ç»äº†Neoï¼Œå®ƒå¼•å…¥äº†ä¸€ç§é‡ç”¨ä»¥æ›´æ–°æ’åºç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨è¿ç»­å¸§ä¸­é«˜æ–¯æ’åºçš„æ—¶é—´å†—ä½™æ€§ï¼Œå¹¶ä¸ºæ­¤ç®—æ³•è®¾è®¡äº†ä¸€ä¸ªç¡¬ä»¶åŠ é€Ÿå™¨ã€‚é€šè¿‡æœ‰æ•ˆåœ°è·Ÿè¸ªå’Œæ›´æ–°é«˜æ–¯æ·±åº¦æ’åºï¼Œè€Œä¸æ˜¯ä»é›¶å¼€å§‹é‡æ–°æ’åºï¼ŒNeoæ˜¾è‘—å‡å°‘äº†å†—ä½™è®¡ç®—å’Œå†…å­˜åœ¨å¸¦å®½æ–¹é¢çš„å‹åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNeoçš„ååé‡æ¯”æœ€æ–°çš„è¾¹ç¼˜GPUå’ŒASICè§£å†³æ–¹æ¡ˆåˆ†åˆ«é«˜å‡ºé«˜è¾¾10.0å€å’Œ5.6å€ï¼ŒåŒæ—¶å°†DRAMæµé‡å‡å°‘äº†94.5%å’Œ81.3%ã€‚è¿™äº›æ”¹è¿›ä½¿å¾—é«˜è´¨é‡å’Œä½å»¶è¿Ÿçš„æœ¬åœ°è®¾å¤‡ä¸‰ç»´æ¸²æŸ“æ›´åŠ å®ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12930v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†åä¸ºNeoçš„å®æ—¶ä¸‰ç»´é«˜æ–¯æ¸²æŸ“æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é’ˆå¯¹ç°æœ‰è§£å†³æ–¹æ¡ˆåœ¨é«˜åˆ†è¾¨ç‡æ¸²æŸ“æ—¶å¸§ç‡ä½çš„é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚ä¸»è¦æ”¹è¿›åœ¨äºè¯†åˆ«å¹¶è§£å†³äº†æ¸²æŸ“ç®¡çº¿ä¸­çš„æ’åºé˜¶æ®µç“¶é¢ˆï¼Œé€šè¿‡åˆ©ç”¨é«˜æ–¯æ’åºçš„æ—¶é—´å†—ä½™ä¿¡æ¯ï¼Œæå‡ºäº†ä¸€ç§é‡ç”¨å’Œæ›´æ–°æ’åºç®—æ³•ï¼Œå¹¶ä¸ºæ­¤ç®—æ³•è®¾è®¡äº†ä¸€ç§ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚é€šè¿‡æœ‰æ•ˆè¿½è¸ªå’Œæ›´æ–°é«˜æ–¯æ·±åº¦æ’åºï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹é‡æ–°æ’åºï¼ŒNeoæ˜¾è‘—å‡å°‘äº†å†—ä½™è®¡ç®—å’Œå†…å­˜å¸¦å®½å‹åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNeoè¾ƒæœ€æ–°çš„è¾¹ç¼˜GPUå’ŒASICè§£å†³æ–¹æ¡ˆåˆ†åˆ«æé«˜äº†é«˜è¾¾10å€å’Œ5.6å€çš„ååé‡ï¼ŒåŒæ—¶DRAMæµé‡å‡å°‘äº†94.5%å’Œ81.3%ã€‚è¿™äº›æ”¹è¿›ä½¿å¾—é«˜è´¨é‡ã€ä½å»¶è¿Ÿçš„æœ¬åœ°è®¾å¤‡ä¸‰ç»´æ¸²æŸ“æ›´åŠ å®ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶ä¸‰ç»´é«˜æ–¯æ¸²æŸ“æŠ€æœ¯å¯¹äºå¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ä½“éªŒè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆåœ¨é«˜åˆ†è¾¨ç‡æ¸²æŸ“æ—¶é¢ä¸´å¸§ç‡é—®é¢˜ã€‚</li>
<li>æ’åºé˜¶æ®µæ˜¯ä¸‰ç»´é«˜æ–¯æ¸²æŸ“çš„ä¸»è¦ç“¶é¢ˆï¼Œå…¶é«˜å†…å­˜å¸¦å®½éœ€æ±‚é™åˆ¶äº†æ€§èƒ½ã€‚</li>
<li>NeoæŠ€æœ¯åˆ©ç”¨é«˜æ–¯æ’åºçš„æ—¶é—´å†—ä½™ä¿¡æ¯æ¥ä¼˜åŒ–æ¸²æŸ“è¿‡ç¨‹ã€‚</li>
<li>Neoæå‡ºäº†ä¸€ç§é‡ç”¨å’Œæ›´æ–°æ’åºç®—æ³•ï¼Œå¹¶é€šè¿‡ç¡¬ä»¶åŠ é€Ÿå™¨å®ç°äº†ä¼˜åŒ–ã€‚</li>
<li>Neoé€šè¿‡æœ‰æ•ˆè¿½è¸ªå’Œæ›´æ–°é«˜æ–¯æ·±åº¦æ’åºï¼Œæ˜¾è‘—å‡å°‘äº†å†—ä½™è®¡ç®—å’Œå†…å­˜å¸¦å®½å‹åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc27e82f1a17bce6d203334f307432a4" align="middle">
<img src="https://picx.zhimg.com/v2-2d01bcefdbd38403497986118e50832b" align="middle">
<img src="https://picx.zhimg.com/v2-5403f4079b004af4165c6cb04175be1b" align="middle">
<img src="https://picx.zhimg.com/v2-9d7cf6c732b798ddf52dd3ed925df286" align="middle">
<img src="https://picx.zhimg.com/v2-160de1fb838ed5cf7848400dfb35b6ba" align="middle">
<img src="https://picx.zhimg.com/v2-9b31aeb1b07d7907f6bd83d21c68646c" align="middle">
<img src="https://picx.zhimg.com/v2-1afdc6a1c8207b1982dd1f67f27e4ee4" align="middle">
<img src="https://picx.zhimg.com/v2-ae07ae26cf13b564232a26d472cc05bd" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Reconstructing-3D-Scenes-in-Native-High-Dynamic-Range"><a href="#Reconstructing-3D-Scenes-in-Native-High-Dynamic-Range" class="headerlink" title="Reconstructing 3D Scenes in Native High Dynamic Range"></a>Reconstructing 3D Scenes in Native High Dynamic Range</h2><p><strong>Authors:Kaixuan Zhang, Minxian Li, Mingwu Ren, Jiankang Deng, Xiatian Zhu</strong></p>
<p>High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.</p>
<blockquote>
<p>é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰æˆåƒå¯¹äºä¸“ä¸šæ•°å­—åª’ä½“åˆ›ä½œï¼Œä¾‹å¦‚ç”µå½±åˆ¶ä½œã€è™šæ‹Ÿåˆ¶ä½œå’ŒçœŸå®æ„Ÿæ¸²æŸ“ï¼Œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç›®å‰çš„3Dåœºæ™¯é‡å»ºä¸»è¦é›†ä¸­åœ¨ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰æ•°æ®ä¸Šï¼Œé™åˆ¶äº†å…¶åœ¨ä¸“ä¸šå·¥ä½œæµç¨‹ä¸­çš„åº”ç”¨ã€‚ç°æœ‰çš„ä»LDRè§‚æµ‹é‡å»ºHDRåœºæ™¯çš„æ–¹æ³•ä¾èµ–äºå¤šæ›å…‰èåˆæˆ–åå‘è‰²è°ƒæ˜ å°„ï¼Œè¿™å¢åŠ äº†æ•è·çš„å¤æ‚æ€§å¹¶ä¾èµ–äºåˆæˆç›‘ç£ã€‚éšç€èƒ½ç›´æ¥æ•è·å•æ¬¡æ›å…‰ä¸‹çš„åŸç”ŸHDRæ•°æ®çš„ç›¸æœºæœ€è¿‘çš„å‡ºç°ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ç›´æ¥å¯¹åŸç”ŸHDRè§‚æµ‹è¿›è¡Œå»ºæ¨¡çš„3Dåœºæ™¯é‡å»ºæ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†â€œNative High dynamic range 3D Gaussian Splattingï¼ˆNH-3DGSï¼‰â€ï¼Œåœ¨é‡å»ºè¿‡ç¨‹ä¸­ä¿ç•™äº†å…¨åŠ¨æ€èŒƒå›´ã€‚æˆ‘ä»¬çš„ä¸»è¦æŠ€æœ¯è´¡çŒ®æ˜¯å¯¹é¢œè‰²è¡¨ç¤ºè¿›è¡Œæ–°å‹äº®åº¦-è‰²åº¦åˆ†è§£ï¼Œèƒ½å¤Ÿä»åŸç”ŸHDRç›¸æœºæ•°æ®ä¸­ç›´æ¥è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®çš„å¤šè§†å›¾HDRæ•°æ®é›†ä¸Šçš„æ¼”ç¤ºè¡¨æ˜ï¼ŒNH-3DGSåœ¨é‡å»ºè´¨é‡å’ŒåŠ¨æ€èŒƒå›´ä¿ç•™æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°ç›´æ¥ä»åŸç”ŸHDRæ•è·è¿›è¡Œä¸“ä¸šçº§åˆ«çš„3Dé‡å»ºã€‚ä»£ç å’Œæ•°æ®é›†å°†æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12895v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>HDRæˆåƒåœ¨ä¸“ä¸šæ•°å­—åª’ä½“åˆ›å»ºä¸­è‡³å…³é‡è¦ï¼Œå¦‚ç”µå½±åˆ¶ä½œã€è™šæ‹Ÿåˆ¶ä½œå’Œé€¼çœŸæ¸²æŸ“ç­‰ã€‚ç„¶è€Œï¼Œ3Dåœºæ™¯é‡å»ºä¸»è¦é›†ä¸­åœ¨ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰æ•°æ®ä¸Šï¼Œé™åˆ¶äº†å…¶åœ¨ä¸“ä¸šå·¥ä½œæµç¨‹ä¸­çš„åº”ç”¨ã€‚ç°æœ‰ä»LDRè§‚æµ‹é‡å»ºHDRåœºæ™¯çš„æ–¹æ³•ä¾èµ–äºå¤šæ›å…‰èåˆæˆ–é€†è‰²è°ƒæ˜ å°„ï¼Œè¿™å¢åŠ äº†æ•è·çš„å¤æ‚æ€§å¹¶ä¾èµ–äºåˆæˆç›‘ç£ã€‚éšç€ç›´æ¥æ•è·åŸç”ŸHDRæ•°æ®çš„ç›¸æœºè¿‘æœŸå‡ºç°ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ç›´æ¥å»ºæ¨¡åŸç”ŸHDRè§‚æµ‹çš„3Dåœºæ™¯é‡å»ºæ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºNative High dynamic range 3D Gaussian Splattingï¼ˆNH-3DGSï¼‰ï¼Œåœ¨é‡å»ºè¿‡ç¨‹ä¸­ä¿ç•™äº†å…¨åŠ¨æ€èŒƒå›´ã€‚æˆ‘ä»¬çš„å…³é”®æŠ€æœ¯è´¡çŒ®æ˜¯é¢œè‰²è¡¨ç¤ºçš„æ–°é¢–äº®åº¦-è‰²åº¦åˆ†è§£ï¼Œèƒ½å¤Ÿç›´æ¥ä»åŸç”ŸHDRç›¸æœºæ•°æ®è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®çš„å¤šè§†è§’HDRæ•°æ®é›†ä¸Šçš„æ¼”ç¤ºè¡¨æ˜ï¼ŒNH-3DGSåœ¨é‡å»ºè´¨é‡å’ŒåŠ¨æ€èŒƒå›´ä¿ç•™æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°ç›´æ¥ä»åŸç”ŸHDRæ•è·çš„ä¸“ä¸šçº§3Dé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>HDRæˆåƒå¯¹ä¸“ä¸šæ•°å­—åª’ä½“åˆ›å»ºè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰3Dåœºæ™¯é‡å»ºæ–¹æ³•ä¸»è¦å…³æ³¨LDRæ•°æ®ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºNative High dynamic range 3D Gaussian Splattingï¼ˆNH-3DGSï¼‰æ–¹æ³•ï¼Œç›´æ¥å»ºæ¨¡åŸç”ŸHDRè§‚æµ‹ã€‚</li>
<li>NH-3DGSä¿ç•™äº†å…¨åŠ¨æ€èŒƒå›´ï¼Œå¹¶åœ¨é‡å»ºè¿‡ç¨‹ä¸­å®ç°äº†è´¨é‡æå‡ã€‚</li>
<li>æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®åœ¨äºé¢œè‰²è¡¨ç¤ºçš„æ–°é¢–äº®åº¦-è‰²åº¦åˆ†è§£ã€‚</li>
<li>NH-3DGSåœ¨åˆæˆå’ŒçœŸå®çš„å¤šè§†è§’HDRæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-685ca78539a714fba58abe9d4d7cfdc1" align="middle">
<img src="https://picx.zhimg.com/v2-1c40894b82c2a234e18b3110628ee982" align="middle">
<img src="https://picx.zhimg.com/v2-bfba5575ee515ef35777612f3eb1eaf8" align="middle">
<img src="https://picx.zhimg.com/v2-a2caf188a8608ce91dc1910a262492ce" align="middle">
<img src="https://picx.zhimg.com/v2-15547b1d49c3d68e049544dea9446af9" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Changes-in-Real-Time-Online-Scene-Change-Detection-with-Multi-View-Fusion"><a href="#Changes-in-Real-Time-Online-Scene-Change-Detection-with-Multi-View-Fusion" class="headerlink" title="Changes in Real Time: Online Scene Change Detection with Multi-View Fusion"></a>Changes in Real Time: Online Scene Change Detection with Multi-View Fusion</h2><p><strong>Authors:Chamuditha Jayanga Galappaththige, Jason Lai, Lloyd Windrim, Donald Dansereau, Niko SÃ¼nderhauf, Dimity Miller</strong></p>
<p>Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.</p>
<blockquote>
<p>åœ¨çº¿åœºæ™¯å˜åŒ–æ£€æµ‹ï¼ˆSCDï¼‰æ˜¯ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå®ƒè¦æ±‚æ™ºèƒ½ä½“åœ¨ä»éçº¦æŸè§†è§’è§‚å¯Ÿåœºæ™¯æ—¶å®æ—¶æ£€æµ‹ç›¸å…³çš„å˜åŒ–ã€‚ç°æœ‰çš„åœ¨çº¿SCDæ–¹æ³•çš„å‡†ç¡®æ€§è¿œè¿œä½äºç¦»çº¿æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªåœ¨çº¿SCDæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰å§¿æ€æ— å…³æ€§ã€æ— éœ€æ ‡ç­¾ï¼ŒåŒæ—¶ä¿è¯å¤šè§†è§’ä¸€è‡´æ€§ï¼Œåœ¨è¶…è¿‡æ¯ç§’10å¸§çš„æ“ä½œä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†æœ€ä½³çš„ç¦»çº¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°çš„è‡ªç›‘ç£èåˆæŸå¤±ï¼Œé€šè¿‡å¤šä¸ªçº¿ç´¢å’Œè§‚å¯Ÿæ¨æ–­åœºæ™¯å˜åŒ–ï¼ŒåŸºäºPnPçš„å¿«é€Ÿå§¿æ€ä¼°è®¡å‚è€ƒåœºæ™¯ï¼Œä»¥åŠé’ˆå¯¹3Dé«˜æ–¯å¹³é“ºåœºæ™¯è¡¨ç¤ºçš„å¿«é€Ÿå˜åŒ–å¼•å¯¼æ›´æ–°ç­–ç•¥ã€‚åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåœ¨çº¿å’Œç¦»çº¿çš„åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12370v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨çº¿åœºæ™¯å˜åŒ–æ£€æµ‹ï¼ˆSCDï¼‰æ˜¯ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨ä»ä¸å—çº¦æŸçš„è§†è§’è§‚å¯Ÿåœºæ™¯æ—¶æ£€æµ‹ç›¸å…³çš„å˜åŒ–ã€‚ç°æœ‰çš„åœ¨çº¿SCDæ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸Šè¿œè¿œè½åäºç¦»çº¿æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºäº†é¦–ä¸ªåœ¨çº¿SCDæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰å§¿æ€æ— å…³æ€§ã€æ— éœ€æ ‡ç­¾ï¼Œå¹¶ç¡®ä¿äº†å¤šè§†è§’ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä»¥è¶…è¿‡10 FPSçš„é€Ÿåº¦è¿è¡Œï¼Œå®ç°äº†è¶…è¶Šç°æœ‰ç¦»çº¿æ–¹æ³•çš„æœ€æ–°æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥æ–°çš„è‡ªç›‘ç£èåˆæŸå¤±æ¥ä»å¤šä¸ªçº¿ç´¢å’Œè§‚å¯Ÿä¸­æ¨æ–­åœºæ™¯å˜åŒ–ï¼Œé‡‡ç”¨åŸºäºPnPçš„å¿«é€Ÿå§¿æ€ä¼°è®¡ä»¥åŒ¹é…å‚è€ƒåœºæ™¯ï¼Œä»¥åŠé’ˆå¯¹å¿«é€Ÿåœºæ™¯å˜åŒ–çš„æŒ‡å¯¼æ›´æ–°ç­–ç•¥è¿›è¡Œå¿«é€Ÿé‡å»ºåœºæ™¯çš„æ›´æ–°ï¼Œæ–¹æ³•æ˜¾ç¤ºå‡ºæå¤§çš„ä¼˜åŠ¿ã€‚å¤§é‡å®éªŒè¯æ˜è¯¥æ–¹æ³•ä¼˜äºåœ¨çº¿å’Œç¦»çº¿åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çº¿åœºæ™¯å˜åŒ–æ£€æµ‹ï¼ˆSCDï¼‰æ˜¯ä¸€ä¸ªéœ€è¦å®æ—¶æ£€æµ‹åœºæ™¯å˜åŒ–çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>å½“å‰åœ¨çº¿SCDæ–¹æ³•çš„å‡†ç¡®æ€§ä½äºç¦»çº¿æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æå‡ºäº†ä¸€ç§åœ¨çº¿SCDæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰å§¿æ€æ— å…³æ€§ã€æ— éœ€æ ‡ç­¾å’Œå¤šè§†è§’ä¸€è‡´æ€§ç‰¹ç‚¹ã€‚</li>
<li>è¯¥æ–¹æ³•å¼•å…¥äº†è‡ªç›‘ç£èåˆæŸå¤±æ¥æ¨æ–­åœºæ™¯å˜åŒ–ã€‚</li>
<li>é‡‡ç”¨åŸºäºPnPçš„å¿«é€Ÿå§¿æ€ä¼°è®¡æŠ€æœ¯åŒ¹é…å‚è€ƒåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57c9367e36a5edc82afb0b3d8e79adbf" align="middle">
<img src="https://picx.zhimg.com/v2-358c6561e46c29ff69756a2255c27ad9" align="middle">
<img src="https://picx.zhimg.com/v2-487bb99be299ad1e7789159e5c177efe" align="middle">
<img src="https://picx.zhimg.com/v2-a9b76e3974a8a6db71a6cea7b3707549" align="middle">
<img src="https://picx.zhimg.com/v2-56593fab2ac495799d897b4bf904f54d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LiDAR-GS-Improving-LiDAR-Gaussian-Reconstruction-via-Diffusion-Priors"><a href="#LiDAR-GS-Improving-LiDAR-Gaussian-Reconstruction-via-Diffusion-Priors" class="headerlink" title="LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors"></a>LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors</h2><p><strong>Authors:Qifeng Chen, Jiarun Liu, Rengan Xie, Tao Tang, Sicong Du, Yiru Zhao, Yuchi Huo, Sheng Yang</strong></p>
<p>Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.</p>
<blockquote>
<p>æœ€è¿‘çš„åŸºäºGSçš„æ¸²æŸ“åœ¨æ¿€å…‰é›·è¾¾æŠ€æœ¯ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåœ¨è´¨é‡å’Œé€Ÿåº¦æ–¹é¢éƒ½è¶…è¶Šäº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€‚ç„¶è€Œï¼Œç”±äºå•æ¬¡éå†æ‰«æçš„é‡å»ºä¸å®Œæ•´ï¼Œè¿™äº›æ–¹æ³•åœ¨æ¨æ–­æ–°é¢–è§†å›¾åˆæˆæ—¶ä¼šå‡ºç°ä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LiDAR-GS++ï¼Œè¿™æ˜¯ä¸€ç§ç”±æ‰©æ•£å…ˆéªŒå¢å¼ºçš„æ¿€å…‰é›·è¾¾é«˜æ–¯å–·æ¶‚é‡å»ºæ–¹æ³•ï¼Œå¯ç”¨äºå…¬å…±åŸå¸‚é“è·¯è¿›è¡Œå®æ—¶é«˜ä¿çœŸé‡æ–°æ¨¡æ‹Ÿã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯æ§çš„æ¿€å…‰é›·è¾¾ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥ç²—ç•¥æ¨æ–­çš„æ¸²æŸ“ä¸ºæ¡ä»¶ï¼Œç”Ÿæˆé¢å¤–çš„å‡ ä½•ä¸€è‡´æ€§æ‰«æï¼Œå¹¶é‡‡ç”¨äº†æœ‰æ•ˆçš„è’¸é¦æœºåˆ¶è¿›è¡Œæ‰©å±•é‡å»ºã€‚é€šè¿‡å°†é‡å»ºæ‰©å±•åˆ°æœªæ‹ŸåˆåŒºåŸŸï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†æ¨æ–­æ–°é¢–è§†å›¾çš„å…¨å±€å‡ ä½•ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†ä¼ æ„Ÿå™¨æ•æ‰åˆ°çš„è¯¦ç»†åœºæ™¯è¡¨é¢ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLiDAR-GS++åœ¨æ¨æ–­å’Œæ¨æ–­çš„è§†ç‚¹ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„GSå’ŒNeRFæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12304v1">PDF</a> Accepted by AAAI-26</p>
<p><strong>Summary</strong></p>
<p>åŸºäºGSçš„æ¸²æŸ“æ–¹æ³•åœ¨LiDARé¢†åŸŸåœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šéƒ½è¶…è¶Šäº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€‚ç„¶è€Œï¼Œç”±äºå•éæ‰«æçš„é‡å»ºä¸å®Œæ•´ï¼Œè¿™äº›æ–¹æ³•åœ¨æ¨æ–­æ–°é¢–è§†å›¾åˆæˆæ—¶ä¼šå‡ºç°ä¼ªå½±ã€‚ä¸ºè§£å†³æ­¤é™åˆ¶ï¼Œæˆ‘ä»¬æ¨å‡ºLiDAR-GS++ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆæ‰©æ•£å…ˆéªŒå¢å¼ºçš„LiDARé«˜æ–¯å¹³é“ºé‡å»ºæ–¹æ³•ï¼Œé€‚ç”¨äºå…¬å…±åŸå¸‚é“è·¯çš„é«˜ä¿çœŸå®æ—¶æ¨¡æ‹Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GS-based rendering for LiDAR has advanced significantly, surpassing NeRF in quality and speed.</li>
<li>å•éæ‰«æçš„é‡å»ºä¸å®Œæ•´å¯¼è‡´æ¨æ–­æ–°é¢–è§†å›¾åˆæˆæ—¶å‡ºç°ä¼ªå½±ã€‚</li>
<li>LiDAR-GS++é‡‡ç”¨å¯æ§çš„LiDARç”Ÿæˆæ¨¡å‹ï¼Œä»¥ç²—ç•¥æ¨æ–­çš„æ¸²æŸ“ä¸ºæ¡ä»¶ï¼Œäº§ç”Ÿé¢å¤–çš„å‡ ä½•ä¸€è‡´æ‰«æã€‚</li>
<li>LiDAR-GS++é‡‡ç”¨æœ‰æ•ˆçš„è’¸é¦æœºåˆ¶è¿›è¡Œæ‰©å±•é‡å»ºï¼Œå°†é‡å»ºæ‰©å±•åˆ°æ¬ é…åŒºåŸŸã€‚</li>
<li>LiDAR-GS++æ–¹æ³•ç¡®ä¿æ¨æ–­æ–°é¢–è§†å›¾çš„å…¨å±€å‡ ä½•ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™ä¼ æ„Ÿå™¨æ•æ‰åˆ°çš„è¯¦ç»†åœºæ™¯è¡¨é¢ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLiDAR-GS++åœ¨æ’å€¼å’Œæ¨æ–­è§‚ç‚¹ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02ec822cfb37ff04ee6cebf453be6278" align="middle">
<img src="https://picx.zhimg.com/v2-2e58d7e516b1480813b022420d133e0d" align="middle">
<img src="https://picx.zhimg.com/v2-07b23fc2fb421c369f3d1393d55004fe" align="middle">
<img src="https://picx.zhimg.com/v2-9876e4630f8492a1e0e0b365d37015f5" align="middle">
<img src="https://picx.zhimg.com/v2-a0d04f5ab406fb769c176e9e9e1ad117" align="middle">
<img src="https://picx.zhimg.com/v2-18d7d291ab66d2129875e5d5e6df27a4" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SRSplat-Feed-Forward-Super-Resolution-Gaussian-Splatting-from-Sparse-Multi-View-Images"><a href="#SRSplat-Feed-Forward-Super-Resolution-Gaussian-Splatting-from-Sparse-Multi-View-Images" class="headerlink" title="SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images"></a>SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images</h2><p><strong>Authors:Xinyuan Hu, Changyue Shi, Chuxiao Yang, Minghao Chen, Jiajun Ding, Tao Wei, Chen Wei, Zhou Yu, Min Tan</strong></p>
<p>Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.</p>
<blockquote>
<p>ä»ç¨€ç–ã€ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒè¿›è¡Œå‰é¦ˆ3Dé‡å»ºæ˜¯è‡ªåŠ¨é©¾é©¶å’ŒåµŒå…¥å¼AIç­‰ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å…³é”®èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ¢å¤ç²¾ç»†çš„çº¹ç†ç»†èŠ‚ã€‚è¿™ç§é™åˆ¶æºäºLRè¾“å…¥ä¸­ç¼ºä¹é«˜é¢‘ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SRSplatï¼Œè¿™æ˜¯ä¸€ä¸ªå‰é¦ˆæ¡†æ¶ï¼Œä»…ä»å‡ ä¸ªLRè§†è§’é‡å»ºé«˜åˆ†è¾¨ç‡çš„3Dåœºæ™¯ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯é€šè¿‡è”åˆåˆ©ç”¨å¤–éƒ¨é«˜è´¨é‡å‚è€ƒå›¾åƒå’Œå†…éƒ¨çº¹ç†çº¿ç´¢æ¥å¼¥è¡¥çº¹ç†ä¿¡æ¯çš„ä¸è¶³ã€‚æˆ‘ä»¬é¦–å…ˆé’ˆå¯¹æ¯ä¸ªåœºæ™¯ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹æ„å»ºåœºæ™¯ç‰¹å®šå‚è€ƒç”»å»Šã€‚ä¸ºäº†æ•´åˆè¿™äº›å¤–éƒ¨ä¿¡æ¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‚è€ƒå¼•å¯¼ç‰¹å¾å¢å¼ºï¼ˆRGFEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯¹é½å¹¶èåˆæ¥è‡ªLRè¾“å…¥å›¾åƒå’Œå…¶å‚è€ƒå­ªç”Ÿå›¾åƒçš„ç‰¹å¾ã€‚éšåï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªè§£ç å™¨ï¼Œä½¿ç”¨ä»RGFEè·å¾—çš„å¤šè§†å›¾èåˆç‰¹å¾æ¥é¢„æµ‹é«˜æ–¯åŸºå…ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥ç»†åŒ–é¢„æµ‹çš„é«˜æ–¯åŸºå…ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†çº¹ç†æ„ŸçŸ¥å¯†åº¦æ§åˆ¶ï¼ˆTADCï¼‰ï¼Œå®ƒå¯ä»¥æ ¹æ®LRè¾“å…¥çš„å†…éƒ¨çº¹ç†ä¸°å¯Œç¨‹åº¦è‡ªé€‚åº”åœ°è°ƒæ•´é«˜æ–¯å¯†åº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SRSplatåœ¨åŒ…æ‹¬RealEstate10Kã€ACIDå’ŒDTUç­‰å„ç§æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„è·¨æ•°æ®é›†å’Œè·¨åˆ†è¾¨ç‡æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12040v1">PDF</a> AAAI2026-Oral. Project Page: <a target="_blank" rel="noopener" href="https://xinyuanhu66.github.io/SRSplat/">https://xinyuanhu66.github.io/SRSplat/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSRSplatçš„åé¦ˆå‰3Dé‡å»ºæ¡†æ¶ï¼Œèƒ½å¤Ÿä»å°‘é‡ä½åˆ†è¾¨ç‡å›¾åƒé‡å»ºå‡ºé«˜åˆ†è¾¨ç‡çš„3Dåœºæ™¯ã€‚é€šè¿‡ç»“åˆå¤–éƒ¨é«˜è´¨é‡å‚è€ƒå›¾åƒå’Œå†…éƒ¨çº¹ç†çº¿ç´¢æ¥å¼¥è¡¥çº¹ç†ä¿¡æ¯çš„ç¼ºå¤±ã€‚åˆ›æ–°ç‚¹åŒ…æ‹¬åœºæ™¯ç‰¹å®šå‚è€ƒç”»å»Šçš„æ„å»ºã€å‚è€ƒå¼•å¯¼ç‰¹å¾å¢å¼ºï¼ˆRGFEï¼‰æ¨¡å—çš„ä½¿ç”¨ä»¥åŠçº¹ç†æ„ŸçŸ¥å¯†åº¦æ§åˆ¶ï¼ˆTADCï¼‰çš„å¼•å…¥ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰è·¨æ•°æ®é›†å’Œè·¨åˆ†è¾¨ç‡çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SRSplatæ˜¯ä¸€ä¸ªèƒ½å¤Ÿä»å°‘é‡ä½åˆ†è¾¨ç‡å›¾åƒé‡å»ºé«˜åˆ†è¾¨ç‡3Dåœºæ™¯çš„åé¦ˆå‰æ¡†æ¶ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤–éƒ¨é«˜è´¨é‡å‚è€ƒå›¾åƒå’Œå†…éƒ¨çº¹ç†çº¿ç´¢æ¥å¼¥è¡¥çº¹ç†ä¿¡æ¯çš„ç¼ºå¤±ã€‚</li>
<li>æ„å»ºäº†åœºæ™¯ç‰¹å®šå‚è€ƒç”»å»Šï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¸ºæ¯ä¸ªåœºæ™¯ç”Ÿæˆå‚è€ƒå›¾åƒã€‚</li>
<li>å¼•å…¥å‚è€ƒå¼•å¯¼ç‰¹å¾å¢å¼ºï¼ˆRGFEï¼‰æ¨¡å—ï¼Œæ•´åˆLRè¾“å…¥å›¾åƒå’Œå‚è€ƒå›¾åƒçš„ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨å¤šè§†è§’èåˆç‰¹å¾æ¥é¢„æµ‹é«˜æ–¯åŸºæœ¬ä½“ï¼Œé€šè¿‡çº¹ç†æ„ŸçŸ¥å¯†åº¦æ§åˆ¶ï¼ˆTADCï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–é¢„æµ‹ç»“æœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f48590dff4a907472a45bc6f68b3026" align="middle">
<img src="https://picx.zhimg.com/v2-504f8d959af63dc82644d46174858279" align="middle">
<img src="https://picx.zhimg.com/v2-a14838af9ef9ec071f376bebe48e7b52" align="middle">
<img src="https://picx.zhimg.com/v2-c8e99771e8da8980db6e40fb5c96ccb7" align="middle">
<img src="https://picx.zhimg.com/v2-24ccd46b9483d4a25975d968c6abe326" align="middle">
<img src="https://picx.zhimg.com/v2-6714c4e366d1da626ad50c06b554b1ff" align="middle">
<img src="https://picx.zhimg.com/v2-e0735f92c9185289d096da5b39e7be9d" align="middle">
<img src="https://picx.zhimg.com/v2-ae13c837523da88eb26df86aecb9c02e" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="REALM-An-MLLM-Agent-Framework-for-Open-World-3D-Reasoning-Segmentation-and-Editing-on-Gaussian-Splatting"><a href="#REALM-An-MLLM-Agent-Framework-for-Open-World-3D-Reasoning-Segmentation-and-Editing-on-Gaussian-Splatting" class="headerlink" title="REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting"></a>REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting</h2><p><strong>Authors:Changyue Shi, Minghao Chen, Yiping Mao, Chuxiao Yang, Xinyuan Hu, Jiajun Ding, Zhou Yu</strong></p>
<p>Bridging the gap between complex human instructions and precise 3D object grounding remains a significant challenge in vision and robotics. Existing 3D segmentation methods often struggle to interpret ambiguous, reasoning-based instructions, while 2D vision-language models that excel at such reasoning lack intrinsic 3D spatial understanding. In this paper, we introduce REALM, an innovative MLLM-agent framework that enables open-world reasoning-based segmentation without requiring extensive 3D-specific post-training. We perform segmentation directly on 3D Gaussian Splatting representations, capitalizing on their ability to render photorealistic novel views that are highly suitable for MLLM comprehension. As directly feeding one or more rendered views to the MLLM can lead to high sensitivity to viewpoint selection, we propose a novel Global-to-Local Spatial Grounding strategy. Specifically, multiple global views are first fed into the MLLM agent in parallel for coarse-level localization, aggregating responses to robustly identify the target object. Then, several close-up novel views of the object are synthesized to perform fine-grained local segmentation, yielding accurate and consistent 3D masks. Extensive experiments show that REALM achieves remarkable performance in interpreting both explicit and implicit instructions across LERF, 3D-OVS, and our newly introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly supports a range of 3D interaction tasks, including object removal, replacement, and style transfer, demonstrating its practical utility and versatility. Project page: <a target="_blank" rel="noopener" href="https://changyueshi.github.io/REALM">https://ChangyueShi.github.io/REALM</a>.</p>
<blockquote>
<p>åœ¨è§†è§‰å’Œæœºå™¨äººé¢†åŸŸï¼Œå¼¥åˆå¤æ‚äººç±»æŒ‡ä»¤å’Œç²¾ç¡®ä¸‰ç»´ç‰©ä½“å®šä½ä¹‹é—´çš„é¸¿æ²Ÿä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„ä¸‰ç»´åˆ†å‰²æ–¹æ³•å¾€å¾€éš¾ä»¥è§£é‡Šæ¨¡ç³Šã€åŸºäºæ¨ç†çš„æŒ‡ä»¤ï¼Œè€Œæ“…é•¿æ­¤ç±»æ¨ç†çš„äºŒç»´è§†è§‰è¯­è¨€æ¨¡å‹åˆ™ç¼ºä¹å†…åœ¨çš„çš„ä¸‰ç»´ç©ºé—´ç†è§£èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†REALMï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„MLLM-agentæ¡†æ¶ï¼Œå®ƒå¯ä»¥è¿›è¡ŒåŸºäºæ¨ç†çš„å¼€æ”¾å¼ä¸–ç•Œåˆ†å‰²ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§é‡çš„é’ˆå¯¹ä¸‰ç»´çš„ç‰¹å®šåè®­ç»ƒã€‚æˆ‘ä»¬ç›´æ¥åœ¨ä¸‰ç»´é«˜æ–¯å¹³é“ºè¡¨ç¤ºä¸Šè¿›è¡Œåˆ†å‰²ï¼Œå……åˆ†åˆ©ç”¨å…¶å‘ˆç°é«˜åº¦é€¼çœŸçš„æ–°è§†è§’çš„èƒ½åŠ›ï¼Œéå¸¸é€‚åˆMLLMç†è§£ã€‚ç”±äºç›´æ¥å°†ä¸€ä¸ªæˆ–å¤šä¸ªæ¸²æŸ“è§†å›¾ç›´æ¥è¾“å…¥åˆ°MLLMä¸­å¯èƒ½å¯¼è‡´å¯¹è§†ç‚¹é€‰æ‹©çš„æ•æ„Ÿæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å…¨å±€åˆ°å±€éƒ¨ç©ºé—´å®šä½ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆå°†å¤šä¸ªå…¨å±€è§†å›¾å¹¶è¡Œè¾“å…¥åˆ°MLLMä»£ç†ä¸­ï¼Œè¿›è¡Œç²—ç•¥å®šä½ï¼Œå¹¶æ±‡æ€»å“åº”ä»¥ç¨³å¥åœ°è¯†åˆ«ç›®æ ‡å¯¹è±¡ã€‚ç„¶åï¼Œåˆæˆè¯¥å¯¹è±¡çš„ä¸€äº›ç‰¹å†™æ–°é¢–è§†å›¾ä»¥æ‰§è¡Œç²¾ç»†çš„å±€éƒ¨åˆ†å‰²ï¼Œç”Ÿæˆå‡†ç¡®ä¸”ä¸€è‡´çš„3Dè’™ç‰ˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒREALMåœ¨è§£é‡ŠLERFã€3D-OVSå’Œæˆ‘ä»¬æ–°å¼•å…¥çš„REALM3DåŸºå‡†æµ‹è¯•ä¸­çš„æ˜ç¡®å’Œéšå«æŒ‡ä»¤æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä»£ç†æ¡†æ¶æ— ç¼æ”¯æŒä¸€ç³»åˆ—ä¸‰ç»´äº¤äº’ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¯¹è±¡ç§»é™¤ã€æ›¿æ¢å’Œé£æ ¼è½¬æ¢ï¼Œè¯æ˜äº†å…¶å®ç”¨æ€§ã€é€šç”¨æ€§å’Œå¤šç”¨é€”æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://changyueshi.github.io/REALM%E3%80%82">https://ChangyueShi.github.io/REALMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16410v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºREALMçš„åˆ›æ–°MLLMä»£ç†æ¡†æ¶ï¼Œç”¨äºåœ¨æ— éœ€å¤§é‡é’ˆå¯¹3Dçš„åæœŸè®­ç»ƒæƒ…å†µä¸‹ï¼Œå®ç°åŸºäºæ¨ç†çš„åˆ†å‰²ã€‚è¯¥æ¡†æ¶ç›´æ¥åœ¨3Dé«˜æ–¯æŠ•å½±è¡¨ç¤ºä¸Šè¿›è¡Œåˆ†å‰²ï¼Œå¹¶åˆ©ç”¨å…¶æ¸²æŸ“é€¼çœŸæ–°è§†è§’çš„èƒ½åŠ›ï¼Œé«˜åº¦é€‚åˆMLLMç†è§£ã€‚ä¸ºè§£å†³ç›´æ¥å–‚é£Ÿä¸€ä¸ªæˆ–å¤šä¸ªæ¸²æŸ“è§†å›¾å¯èƒ½å¯¼è‡´çš„è§†è§’é€‰æ‹©æ•æ„Ÿæ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨çƒåˆ°å±€éƒ¨çš„ç©ºé—´å®šä½ç­–ç•¥ã€‚é¦–å…ˆï¼Œå°†å¤šä¸ªå…¨å±€è§†å›¾å¹¶è¡Œè¾“å…¥åˆ°MLLMä»£ç†ä¸­ï¼Œè¿›è¡Œç²—ç•¥å®šä½ï¼Œå¹¶é€šè¿‡èšåˆå“åº”æ¥ç¨³å¥åœ°è¯†åˆ«ç›®æ ‡å¯¹è±¡ã€‚ç„¶åï¼Œåˆæˆç›®æ ‡å¯¹è±¡çš„è¿‘è·ç¦»æ–°è§†è§’ä»¥è¿›è¡Œç²¾ç»†å±€éƒ¨åˆ†å‰²ï¼Œç”Ÿæˆå‡†ç¡®ä¸”ä¸€è‡´çš„3Dæ©è†œã€‚REALMåœ¨è§£é‡Šæ˜ç¡®å’Œéšå«æŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨LERFã€3D-OVSå’Œæœ€æ–°å¼•å…¥çš„REALM3DåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚æ­¤å¤–ï¼Œè¯¥ä»£ç†æ¡†æ¶æ— ç¼æ”¯æŒä¸€ç³»åˆ—3Däº¤äº’ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¯¹è±¡ç§»é™¤ã€æ›¿æ¢å’Œé£æ ¼è½¬æ¢ï¼Œå±•ç¤ºäº†å…¶å®ç”¨æ€§ã€é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>REALMæ˜¯ä¸€ä¸ªMLLM-agentæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€å¤§é‡é’ˆå¯¹3Dçš„åæœŸè®­ç»ƒæƒ…å†µä¸‹å®ç°åŸºäºæ¨ç†çš„åˆ†å‰²ã€‚</li>
<li>REALMåˆ©ç”¨3Dé«˜æ–¯æŠ•å½±è¡¨ç¤ºè¿›è¡Œåˆ†å‰²ï¼Œå¹¶å€ŸåŠ©å…¶æ¸²æŸ“é€¼çœŸæ–°è§†è§’çš„èƒ½åŠ›ä¿ƒè¿›MLLMç†è§£ã€‚</li>
<li>å…¨çƒåˆ°å±€éƒ¨çš„ç©ºé—´å®šä½ç­–ç•¥è§£å†³äº†è§†è§’é€‰æ‹©æ•æ„Ÿæ€§é—®é¢˜ã€‚</li>
<li>REALMé€šè¿‡ç»“åˆå…¨å±€å’Œå±€éƒ¨è§†å›¾ï¼Œå®ç°äº†ç›®æ ‡å¯¹è±¡çš„ç²—ç•¥å’Œç²¾ç»†å®šä½ã€‚</li>
<li>REALMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬LERFã€3D-OVSå’Œæœ€æ–°å¼•å…¥çš„REALM3Dã€‚</li>
<li>REALMæ¡†æ¶æ”¯æŒä¸€ç³»åˆ—3Däº¤äº’ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¯¹è±¡ç§»é™¤ã€æ›¿æ¢å’Œé£æ ¼è½¬æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c93170823a67927c226919a794b5018" align="middle">
<img src="https://picx.zhimg.com/v2-bf54b12520e3ce0d487048a0220e9ee5" align="middle">
<img src="https://picx.zhimg.com/v2-c3e13b703a73d7cb06ee97c998d77898" align="middle">
<img src="https://picx.zhimg.com/v2-f7dfbad9214c40e4bcea79adc512defc" align="middle">
<img src="https://picx.zhimg.com/v2-5d7d345b45ebceb3d0c2613b22631cef" align="middle">
<img src="https://picx.zhimg.com/v2-3cc5025172504c7fe41e1ad43a0bd542" align="middle">
<img src="https://picx.zhimg.com/v2-153bc1e0ba1549a747271ec3095207e6" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Dream-Lift-Animate-From-Single-Images-to-Animatable-Gaussian-Avatars"><a href="#Dream-Lift-Animate-From-Single-Images-to-Animatable-Gaussian-Avatars" class="headerlink" title="Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars"></a>Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars</h2><p><strong>Authors:Marcel C. BÃ¼hler, Ye Yuan, Xueting Li, Yangyi Huang, Koki Nagano, Umar Iqbal</strong></p>
<p>We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs animatable 3D human avatars from a single image. This is achieved by leveraging multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of 3D Gaussians. Given an image, we first dream plausible multi-views using a video diffusion model, capturing rich geometric and appearance details. These views are then lifted into unstructured 3D Gaussians. To enable animation, we propose a transformer-based encoder that models global spatial relationships and projects these Gaussians into a structured latent representation aligned with the UV space of a parametric body model. This latent code is decoded into UV-space Gaussians that can be animated via body-driven deformation and rendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV manifold, our method ensures consistency during animation while preserving fine visual details. DLA enables real-time rendering and intuitive editing without requiring post-processing. Our method outperforms state-of-the-art approaches on the ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric accuracy. By combining the generative strengths of video diffusion models with a pose-aware UV-space Gaussian mapping, DLA bridges the gap between unstructured 3D representations and high-fidelity, animation-ready avatars.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Dreamã€Liftã€Animateï¼ˆDLAï¼‰è¿™ä¸€å…¨æ–°æ¡†æ¶ï¼Œå®ƒèƒ½ä»å•å¼ å›¾ç‰‡é‡å»ºå¯åŠ¨ç”»çš„3Däººç±»è§’è‰²ã€‚è¿™æ˜¯é€šè¿‡åˆ©ç”¨å¤šè§†è§’ç”Ÿæˆã€3Dé«˜æ–¯æå‡å’Œå§¿æ€æ„ŸçŸ¥çš„UVç©ºé—´é«˜æ–¯æ˜ å°„æ¥å®ç°çš„ã€‚ç»™å®šä¸€å¼ å›¾ç‰‡ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹æ¢¦è§å¯èƒ½çš„å¤šè§†è§’ï¼Œæ•æ‰ä¸°å¯Œçš„å‡ ä½•å’Œå¤–è§‚ç»†èŠ‚ã€‚ç„¶åï¼Œè¿™äº›è§†è§’è¢«æå‡ä¸ºæ— ç»“æ„çš„3Dé«˜æ–¯ã€‚ä¸ºäº†å®ç°åŠ¨ç”»æ•ˆæœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå˜å‹å™¨çš„ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨èƒ½å¤Ÿæ¨¡æ‹Ÿå…¨å±€ç©ºé—´å…³ç³»å¹¶å°†è¿™äº›é«˜æ–¯æŠ•å½±åˆ°ä¸å‚æ•°åŒ–èº«ä½“æ¨¡å‹çš„UVç©ºé—´å¯¹é½çš„ç»“æ„åŒ–æ½œåœ¨è¡¨ç¤ºä¸­ã€‚è¿™ä¸ªæ½œåœ¨ä»£ç è¢«è§£ç ä¸ºUVç©ºé—´é«˜æ–¯ï¼Œå¯ä»¥é€šè¿‡èº«ä½“é©±åŠ¨çš„å˜å½¢è¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼Œå¹¶æ ¹æ®å§¿æ€å’Œè§†ç‚¹è¿›è¡Œæ¸²æŸ“ã€‚é€šè¿‡å°†é«˜æ–¯é”šå®šåˆ°UVæµå½¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†åŠ¨ç”»è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†ç²¾ç»†çš„è§†è§‰ç»†èŠ‚ã€‚DLAæ— éœ€åæœŸå¤„ç†å³å¯å®ç°å®æ—¶æ¸²æŸ“å’Œç›´è§‚ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ActorsHQå’Œ4D-Dressæ•°æ®é›†ä¸Šçš„æ„ŸçŸ¥è´¨é‡å’Œå…‰åº¦å‡†ç¡®æ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚é€šè¿‡å°†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸å§¿æ€æ„ŸçŸ¥çš„UVç©ºé—´é«˜æ–¯æ˜ å°„ç›¸ç»“åˆï¼ŒDLAç¼©å°äº†æ— ç»“æ„3Dè¡¨ç¤ºä¸é«˜ä¿çœŸã€å¯åŠ¨ç”»è§’è‰²ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15979v2">PDF</a> Accepted to 3DV 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Dreamã€Liftã€Animateï¼ˆDLAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»å•ä¸€å›¾åƒé‡å»ºå¯åŠ¨ç”»çš„3Däººç±»è§’è‰²ã€‚é€šè¿‡åˆ©ç”¨å¤šè§†è§’ç”Ÿæˆã€3Dé«˜æ–¯æå‡å’Œå§¿æ€æ„ŸçŸ¥UVç©ºé—´æ˜ å°„ç­‰æŠ€æœ¯å®ç°ã€‚ç»™å®šå›¾åƒï¼Œé¦–å…ˆé€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šä¸ªè§†è§’çš„å›¾åƒï¼Œå†å°†è¿™äº›è§†è§’æå‡ä¸ºæ— ç»“æ„çš„3Dé«˜æ–¯æ•°æ®ã€‚ä¸ºå®ç°åŠ¨ç”»æ•ˆæœï¼Œæå‡ºåŸºäºè½¬æ¢ç¼–ç å™¨çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿå…¨å±€ç©ºé—´å…³ç³»å¹¶å°†è¿™äº›é«˜æ–¯æ•°æ®æŠ•å½±åˆ°å‚æ•°åŒ–èº«ä½“æ¨¡å‹çš„UVç©ºé—´ä¸­ã€‚è¿™ç§æ½œåœ¨ä»£ç è¢«è§£ç ä¸ºUVç©ºé—´é«˜æ–¯æ•°æ®ï¼Œå¯ä»¥é€šè¿‡èº«ä½“é©±åŠ¨å˜å½¢è¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼Œå¹¶æ ¹æ®å§¿æ€å’Œè§†è§’è¿›è¡Œæ¸²æŸ“ã€‚DLAæ–¹æ³•ç¡®ä¿äº†åŠ¨ç”»è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†ç²¾ç»†çš„è§†è§‰ç»†èŠ‚ï¼Œå®ç°äº†å®æ—¶æ¸²æŸ“å’Œç›´è§‚ç¼–è¾‘ï¼Œæ— éœ€åæœŸå¤„ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DLAæ¡†æ¶èƒ½å¤Ÿä»å•ä¸€å›¾åƒé‡å»º3Däººç±»è§’è‰²ã€‚</li>
<li>åˆ©ç”¨å¤šè§†è§’ç”Ÿæˆã€3Dé«˜æ–¯æå‡å’Œå§¿æ€æ„ŸçŸ¥UVç©ºé—´æ˜ å°„ç­‰æŠ€æœ¯å®ç°é‡å»ºã€‚</li>
<li>é€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šä¸ªè§†è§’çš„å›¾åƒã€‚</li>
<li>åŸºäºè½¬æ¢ç¼–ç å™¨çš„æ¨¡å‹å®ç°åŠ¨ç”»æ•ˆæœï¼Œæ¨¡æ‹Ÿå…¨å±€ç©ºé—´å…³ç³»ã€‚</li>
<li>å°†é«˜æ–¯æ•°æ®æŠ•å½±åˆ°å‚æ•°åŒ–èº«ä½“æ¨¡å‹çš„UVç©ºé—´ï¼Œå®ç°æ½œåœ¨ä»£ç çš„è§£ç ã€‚</li>
<li>DLAæ–¹æ³•æ”¯æŒå®æ—¶æ¸²æŸ“å’Œç›´è§‚ç¼–è¾‘ï¼Œæ— éœ€åæœŸå¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-726482a654e1201edaef760743b1f52b" align="middle">
<img src="https://picx.zhimg.com/v2-519f5325ab868268153826982b44b13d" align="middle">
<img src="https://picx.zhimg.com/v2-6007b567d017fcd819b0297e629bb13f" align="middle">
<img src="https://picx.zhimg.com/v2-59efe94fa1f3462a86bee0c9545d103c" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ZPressor-Bottleneck-Aware-Compression-for-Scalable-Feed-Forward-3DGS"><a href="#ZPressor-Bottleneck-Aware-Compression-for-Scalable-Feed-Forward-3DGS" class="headerlink" title="ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS"></a>ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS</h2><p><strong>Authors:Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang</strong></p>
<p>Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their models, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: <a target="_blank" rel="noopener" href="https://lhmd.top/zpressor">https://lhmd.top/zpressor</a>.</p>
<blockquote>
<p>å‰é¦ˆä¸‰ç»´é«˜æ–¯æ‘Šé“ºï¼ˆ3DGSï¼‰æ¨¡å‹æœ€è¿‘ä½œä¸ºåˆæˆæ–°è§†è§’çš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ï¼Œæ— éœ€å¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œå•ç‹¬çš„ä¼˜åŒ–å°±å¯ä»¥è¿›è¡Œä¸€æ¬¡æ¨ç†æ¨æ–­ã€‚ç„¶è€Œï¼Œæ¨¡å‹çš„æœ‰é™å®¹é‡ä»æ ¹æœ¬ä¸Šé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ï¼Œéšç€è¾“å…¥è§†å›¾æ•°é‡çš„å¢åŠ ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™æˆ–å†…å­˜æ¶ˆè€—è¿‡å¤§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¿¡æ¯ç“¶é¢ˆåŸç†æ¥åˆ†æå‰é¦ˆ3DGSæ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†ZPressorï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ¶æ„æ— å…³æ¨¡å—ï¼Œèƒ½å¤Ÿå°†å¤šè§†å›¾è¾“å…¥æœ‰æ•ˆåœ°å‹ç¼©æˆç´§å‡‘çš„æ½œåœ¨çŠ¶æ€Zï¼ŒåŒæ—¶ä¿ç•™å…³é”®åœºæ™¯ä¿¡æ¯å¹¶ä¸¢å¼ƒå†—ä½™ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼ŒZPressoré€šè¿‡å°†è§†å›¾åˆ’åˆ†ä¸ºé”šç‚¹é›†å’Œæ”¯æŒé›†å¹¶ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å°†æ”¯æŒè§†å›¾çš„ä¿¡æ¯å‹ç¼©åˆ°é”šè§†å›¾ä¸­æ¥å½¢æˆå‹ç¼©çš„æ½œåœ¨çŠ¶æ€Zï¼Œä»è€Œå®ç°äº†åœ¨å®¹é‡ä¸º80GBçš„GPUä¸Šæ”¯æŒé«˜è¾¾ä¸€ç™¾ä¸ªè¾“å…¥è§†å›¾åœ¨480Påˆ†è¾¨ç‡ä¸‹çš„æ‰©å±•èƒ½åŠ›ã€‚æˆ‘ä»¬å±•ç¤ºäº†å°†ZPressoré›†æˆåˆ°å‡ ç§å…ˆè¿›çš„å‰é¦ˆ3DGSæ¨¡å‹ä¸­ï¼Œåœ¨å¤§å‹åŸºå‡†æµ‹è¯•DL3DV-10Kå’ŒRealEstate10Kä¸Šçš„ä¸€è‡´æ€§èƒ½æ”¹è¿›ä»¥åŠåœ¨ä¸­ç­‰è¾“å…¥è§†å›¾ä¸‹çš„ç¨³å¥æ€§å¢å¼ºã€‚è§†é¢‘ç»“æœã€ä»£ç å’Œè®­ç»ƒæ¨¡å‹å‡å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æŸ¥çœ‹ï¼š[ç½‘å€é“¾æ¥]ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23734v4">PDF</a> NeurIPS 2025, Project Page: <a target="_blank" rel="noopener" href="https://lhmd.top/zpressor">https://lhmd.top/zpressor</a>, Code: <a target="_blank" rel="noopener" href="https://github.com/ziplab/ZPressor">https://github.com/ziplab/ZPressor</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨è§†å›¾åˆæˆé¢†åŸŸä¸­ï¼Œå‰é¦ˆ3Dé«˜æ–¯æ‰©å±•æ¨¡å‹åœ¨å¤„ç†å¤§é‡è§†å›¾æ—¶çš„å±€é™æ€§é—®é¢˜ã€‚ä¸ºäº†æå‡æ¨¡å‹å¤„ç†æ€§èƒ½å¹¶è§£å†³å†…å­˜æ¶ˆè€—é—®é¢˜ï¼Œè¯¥æ–‡é€šè¿‡ä¿¡æ¯ç“¶é¢ˆåŸç†å¯¹å‰é¦ˆ3DGSæ¡†æ¶è¿›è¡Œåˆ†æï¼Œå¹¶å¼•å…¥äº†åä¸ºZPressorçš„è½»é‡çº§æ¶æ„æ— å…³æ¨¡å—ã€‚è¯¥æ¨¡å—å¯å°†å¤šè§†å›¾è¾“å…¥å‹ç¼©æˆç´§å‡‘çš„æ½œåœ¨çŠ¶æ€Zï¼Œä¿ç•™å…³é”®åœºæ™¯ä¿¡æ¯å¹¶å»é™¤å†—ä½™ä¿¡æ¯ã€‚é€šè¿‡æ•´åˆZPressoræ¨¡å—ï¼Œç°æœ‰å‰é¦ˆ3DGSæ¨¡å‹èƒ½åœ¨æœ‰é™çš„GPUå†…å­˜ä¸‹å¤„ç†è¶…è¿‡ç™¾ä¸ªè§†å›¾ã€‚åœ¨å¤§å‹æ•°æ®é›†ä¸Šæµ‹è¯•æ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆæ€§èƒ½è¡¨ç°ä¼˜è¶Šä¸”é²æ£’æ€§å¼ºã€‚è¯¦æƒ…å¯è§ç›¸å…³é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://lhmd.top/zpressor">https://lhmd.top/zpressor</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å‰é¦ˆ3Dé«˜æ–¯æ‰©å±•ï¼ˆ3DGSï¼‰æ¨¡å‹å¯ç”¨äºå¿«é€Ÿè§†å›¾åˆæˆï¼Œæ— éœ€ä¸ºæ¯ä¸ªåœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>éšç€è¾“å…¥è§†å›¾æ•°é‡çš„å¢åŠ ï¼Œç°æœ‰å‰é¦ˆ3DGSæ¨¡å‹çš„æ€§èƒ½å—é™ï¼Œå¹¶å¯èƒ½å‡ºç°å†…å­˜æ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ã€‚</li>
<li>ZPressoræ¨¡å—é€šè¿‡å‹ç¼©å¤šè§†å›¾è¾“å…¥åˆ°ç´§å‡‘çš„æ½œåœ¨çŠ¶æ€Zï¼Œæé«˜äº†æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>ZPressoræ¨¡å—å°†è§†å›¾åˆ†ä¸ºé”šç‚¹å’Œæ”¯æŒé›†ï¼Œå¹¶ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å°†æ”¯æŒè§†å›¾çš„ä¿¡æ¯å‹ç¼©åˆ°é”šè§†å›¾ä¸­ã€‚</li>
<li>é›†æˆZPressoræ¨¡å—çš„å‰é¦ˆ3DGSæ¨¡å‹èƒ½åœ¨æœ‰é™çš„GPUå†…å­˜ä¸‹å¤„ç†æ›´å¤šè§†å›¾ï¼Œå¹¶ä¸”åœ¨å¤§å‹æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°å“è¶Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f00fc84cc31fa65dc51ef79f696a22ba" align="middle">
<img src="https://picx.zhimg.com/v2-84b900ea8684066f5dad1e155b102fed" align="middle">
<img src="https://picx.zhimg.com/v2-0ebc2eef1f677c7d84622a0895011b4a" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GaussianFocus-Constrained-Attention-Focus-for-3D-Gaussian-Splatting"><a href="#GaussianFocus-Constrained-Attention-Focus-for-3D-Gaussian-Splatting" class="headerlink" title="GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting"></a>GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting</h2><p><strong>Authors:Zexu Huang, Min Xu, Stuart Perry</strong></p>
<p>Recent developments in 3D reconstruction and neural rendering have significantly propelled the capabilities of photo-realistic 3D scene rendering across various academic and industrial fields. The 3D Gaussian Splatting technique, alongside its derivatives, integrates the advantages of primitive-based and volumetric representations to deliver top-tier rendering quality and efficiency. Despite these advancements, the method tends to generate excessive redundant noisy Gaussians overfitted to every training view, which degrades the rendering quality. Additionally, while 3D Gaussian Splatting excels in small-scale and object-centric scenes, its application to larger scenes is hindered by constraints such as limited video memory, excessive optimization duration, and variable appearance across views. To address these challenges, we introduce GaussianFocus, an innovative approach that incorporates a patch attention algorithm to refine rendering quality and implements a Gaussian constraints strategy to minimize redundancy. Moreover, we propose a subdivision reconstruction strategy for large-scale scenes, dividing them into smaller, manageable blocks for individual training. Our results indicate that GaussianFocus significantly reduces unnecessary Gaussians and enhances rendering quality, surpassing existing State-of-The-Art (SoTA) methods. Furthermore, we demonstrate the capability of our approach to effectively manage and render large scenes, such as urban environments, whilst maintaining high fidelity in the visual output.</p>
<blockquote>
<p>è¿‘æœŸä¸‰ç»´é‡å»ºå’Œç¥ç»æ¸²æŸ“çš„å‘å±•æå¤§åœ°æ¨åŠ¨äº†å¤šå°ºåº¦å…¨æ™¯æŠ€æœ¯åœ¨å„å­¦æœ¯å’Œäº§ä¸šé¢†åŸŸä¸­çš„é€¼çœŸä¸‰ç»´åœºæ™¯æ¸²æŸ“èƒ½åŠ›ã€‚ä¸‰ç»´é«˜æ–¯å±•å¼€æŠ€æœ¯åŠå…¶è¡ç”ŸæŠ€æœ¯ç»“åˆäº†åŸºäºåŸå§‹æ•°æ®å’Œä½“ç§¯è¡¨ç¤ºçš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜è´¨é‡çš„æ¸²æŸ“æ•ˆæœå’Œæ•ˆç‡ã€‚ç„¶è€Œï¼Œè¯¥æŠ€æœ¯å¾€å¾€ä¼šäº§ç”Ÿå¤§é‡å†—ä½™çš„å™ªå£°é«˜æ–¯åˆ†å¸ƒï¼Œè¿‡åº¦æ‹Ÿåˆæ¯ä¸ªè®­ç»ƒè§†å›¾ï¼Œå¯¼è‡´æ¸²æŸ“è´¨é‡ä¸‹é™ã€‚æ­¤å¤–ï¼Œè™½ç„¶ä¸‰ç»´é«˜æ–¯å±•å¼€åœ¨å°è§„æ¨¡ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åº”ç”¨äºå¤§å‹åœºæ™¯æ—¶å—åˆ°è§†é¢‘å†…å­˜é™åˆ¶ã€ä¼˜åŒ–æ—¶é—´è¿‡é•¿ä»¥åŠä¸åŒè§†å›¾ä¹‹é—´å¤–è§‚å˜åŒ–ç­‰çº¦æŸçš„é˜»ç¢ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GaussianFocusè¿™ä¸€åˆ›æ–°æ–¹æ³•ï¼Œå®ƒç»“åˆäº†è¡¥ä¸æ³¨æ„åŠ›ç®—æ³•æ¥ä¼˜åŒ–æ¸²æŸ“è´¨é‡ï¼Œå¹¶å®æ–½äº†é«˜æ–¯çº¦æŸç­–ç•¥æ¥å‡å°‘å†—ä½™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå¤§è§„æ¨¡åœºæ™¯çš„ç»†åˆ†é‡å»ºç­–ç•¥ï¼Œå°†å®ƒä»¬åˆ’åˆ†ä¸ºè¾ƒå°çš„ã€å¯ç®¡ç†çš„å—è¿›è¡Œå•ç‹¬è®­ç»ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGaussianFocusæ˜¾è‘—å‡å°‘äº†ä¸å¿…è¦çš„é«˜æ–¯åˆ†å¸ƒï¼Œæé«˜äº†æ¸²æŸ“è´¨é‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç®¡ç†å’Œæ¸²æŸ“å¤§å‹åœºæ™¯ï¼ˆå¦‚åŸå¸‚ç¯å¢ƒï¼‰æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒè§†è§‰è¾“å‡ºçš„é«˜ä¿çœŸåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17798v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è¿‘æœŸä¸‰ç»´é‡å»ºå’Œç¥ç»æ¸²æŸ“æŠ€æœ¯çš„å‘å±•å¯¹å…‰æ …åŒ–ä¸‰ç»´åœºæ™¯æ¸²æŸ“çš„å½±å“ã€‚ç‰¹åˆ«ä»‹ç»äº†3Dé«˜æ–¯å–·ç»˜æŠ€æœ¯åŠå…¶è¡ç”Ÿçš„é›†æˆæ–¹æ³•çš„ä¼˜ç‚¹ï¼Œå®ƒä»¬èåˆäº†åŸºäºåŸå§‹çš„å’Œä½“ç§¯è¡¨ç¤ºæ–¹æ³•çš„ä¼˜ç‚¹ï¼Œå®ç°äº†é«˜è´¨é‡çš„æ¸²æŸ“æ•ˆæœã€‚ç„¶è€Œï¼Œè¯¥æŠ€æœ¯ä»é¢ä¸´ä¸€äº›é—®é¢˜ï¼Œå¦‚è¿‡åº¦ç”Ÿæˆå†—ä½™å™ªå£°é«˜æ–¯å¹¶é€‚åº”æ¯ä¸ªè®­ç»ƒè§†å›¾è€Œå¯¼è‡´æ¸²æŸ“è´¨é‡ä¸‹é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†GaussianFocusæ–¹æ³•ï¼Œé‡‡ç”¨è¡¥ä¸æ³¨æ„åŠ›ç®—æ³•æ”¹è¿›æ¸²æŸ“è´¨é‡ï¼Œå¹¶å®æ–½äº†é«˜æ–¯çº¦æŸç­–ç•¥æ¥å‡å°‘å†—ä½™ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡åœºæ™¯çš„ç»†åˆ†é‡å»ºç­–ç•¥ï¼Œå°†åœºæ™¯åˆ†ä¸ºè¾ƒå°çš„å¯ç®¡ç†å—è¿›è¡Œå•ç‹¬è®­ç»ƒã€‚æ€»ç»“æ¥è¯´ï¼ŒGaussianFocusæé«˜äº†æ¸²æŸ“è´¨é‡å¹¶é™ä½äº†ä¸å¿…è¦çš„å†—ä½™é«˜æ–¯ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤§å‹åœºæ™¯ç®¡ç†æ¸²æŸ“æ–¹é¢çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3Dé‡å»ºå’Œç¥ç»æ¸²æŸ“æŠ€æœ¯çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†å…‰æ …åŒ–ä¸‰ç»´åœºæ™¯æ¸²æŸ“çš„è¿›æ­¥ã€‚</li>
<li>3Dé«˜æ–¯å–·ç»˜æŠ€æœ¯èåˆäº†åŸºäºåŸå§‹å’Œä½“ç§¯çš„è¡¨ç¤ºæ–¹æ³•çš„ä¼˜ç‚¹ã€‚</li>
<li>å­˜åœ¨ç”Ÿæˆå†—ä½™å™ªå£°é«˜æ–¯çš„é—®é¢˜ï¼Œå½±å“æ¸²æŸ“è´¨é‡ã€‚</li>
<li>GaussianFocusæ–¹æ³•é€šè¿‡è¡¥ä¸æ³¨æ„åŠ›ç®—æ³•æ”¹è¿›æ¸²æŸ“è´¨é‡ï¼Œå‡å°‘å†—ä½™ã€‚</li>
<li>GaussianFocusé‡‡ç”¨é«˜æ–¯çº¦æŸç­–ç•¥æ¥æœ€å°åŒ–å†—ä½™ã€‚</li>
<li>é’ˆå¯¹å¤§è§„æ¨¡åœºæ™¯æå‡ºäº†ç»†åˆ†é‡å»ºç­–ç•¥ï¼Œå°†åœºæ™¯åˆ†ä¸ºè¾ƒå°çš„å—è¿›è¡Œå•ç‹¬è®­ç»ƒã€‚</li>
<li>GaussianFocusæé«˜äº†æ¸²æŸ“è´¨é‡å¹¶è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e741fdf8343fbbffbb409d2fccc77d1" align="middle">
<img src="https://picx.zhimg.com/v2-1df2db90f43fb9813d581375e6e3e1c6" align="middle">
<img src="https://picx.zhimg.com/v2-b11705ca9a9c93c55f5e54e44afd1567" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DehazeGS-Seeing-Through-Fog-with-3D-Gaussian-Splatting"><a href="#DehazeGS-Seeing-Through-Fog-with-3D-Gaussian-Splatting" class="headerlink" title="DehazeGS: Seeing Through Fog with 3D Gaussian Splatting"></a>DehazeGS: Seeing Through Fog with 3D Gaussian Splatting</h2><p><strong>Authors:Jinze Yu, Yiqun Wang, Aiheng Jiang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang</strong></p>
<p>Current novel view synthesis methods are typically designed for high-quality and clean input images. However, in foggy scenes, scattering and attenuation can significantly degrade the quality of rendering. Although NeRF-based dehazing approaches have been developed, their reliance on deep fully connected neural networks and per-ray sampling strategies leads to high computational costs. Furthermore, NeRFâ€™s implicit representation limits its ability to recover fine-grained details from hazy scenes. To overcome these limitations, we propose learning an explicit Gaussian representation to explain the formation mechanism of foggy images through a physically forward rendering process. Our method, DehazeGS, reconstructs and renders fog-free scenes using only multi-view foggy images as input. Specifically, based on the atmospheric scattering model, we simulate the formation of fog by establishing the transmission function directly onto Gaussian primitives via depth-to-transmission mapping. During training, we jointly learn the atmospheric light and scattering coefficients while optimizing the Gaussian representation of foggy scenes. At inference time, we remove the effects of scattering and attenuation in Gaussian distributions and directly render the scene to obtain dehazed views. Experiments on both real-world and synthetic foggy datasets demonstrate that DehazeGS achieves state-of-the-art performance. visualizations are available at <a target="_blank" rel="noopener" href="https://dehazegs.github.io/">https://dehazegs.github.io/</a></p>
<blockquote>
<p>å½“å‰çš„æ–°å‹è§†å›¾åˆæˆæ–¹æ³•é€šå¸¸é’ˆå¯¹é«˜è´¨é‡å’Œæ¸…æ™°çš„è¾“å…¥å›¾åƒè¿›è¡Œè®¾è®¡ã€‚ç„¶è€Œï¼Œåœ¨é›¾å¤©åœºæ™¯ä¸­ï¼Œæ•£å°„å’Œè¡°å‡ä¼šæ˜¾è‘—é™ä½æ¸²æŸ“è´¨é‡ã€‚å°½ç®¡å·²ç»å¼€å‘äº†åŸºäºNeRFçš„å»é›¾æ–¹æ³•ï¼Œä½†å®ƒä»¬ä¾èµ–äºæ·±åº¦å…¨è¿æ¥ç¥ç»ç½‘ç»œå’ŒæŒ‰å°„çº¿é‡‡æ ·ç­–ç•¥ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æ­¤å¤–ï¼ŒNeRFçš„éšå¼è¡¨ç¤ºé™åˆ¶äº†å…¶ä»é›¾å¤©åœºæ™¯ä¸­æ¢å¤ç»†èŠ‚çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ æ˜¾å¼é«˜æ–¯è¡¨ç¤ºï¼Œé€šè¿‡ç‰©ç†å‰å‘æ¸²æŸ“è¿‡ç¨‹æ¥è§£é‡Šé›¾å¤©å›¾åƒçš„å½¢æˆæœºåˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•DehazeGSä»…ä½¿ç”¨å¤šè§†è§’é›¾å¤©å›¾åƒä½œä¸ºè¾“å…¥ï¼Œé‡å»ºå’Œæ¸²æŸ“æ— é›¾åœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºå¤§æ°”æ•£å°„æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡æ·±åº¦åˆ°ä¼ è¾“æ˜ å°„ç›´æ¥åœ¨é«˜æ–¯åŸå§‹æ•°æ®ä¸Šå»ºç«‹ä¼ è¾“å‡½æ•°æ¥æ¨¡æ‹Ÿé›¾çš„å½¢æˆã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è”åˆå­¦ä¹ å¤§æ°”å…‰å’Œæ•£å°„ç³»æ•°ï¼ŒåŒæ—¶ä¼˜åŒ–é›¾å¤©åœºæ™¯çš„é«˜æ–¯è¡¨ç¤ºã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†é«˜æ–¯åˆ†å¸ƒä¸­çš„æ•£å°„å’Œè¡°å‡å½±å“ï¼Œç›´æ¥æ¸²æŸ“åœºæ™¯ä»¥è·å¾—å»é›¾çš„è§†å›¾ã€‚åœ¨ç°å®ä¸–ç•Œå’Œåˆæˆé›¾æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDehazeGSè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚å¯è§†åŒ–è¯·è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://dehazegs.github.io/]">https://dehazegs.github.io/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03659v5">PDF</a> 9 pages,5 figures. Accepted by AAAI2026. visualizations are available at <a target="_blank" rel="noopener" href="https://dehazegs.github.io/">https://dehazegs.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é›¾å¤©åœºæ™¯åˆæˆè§†å›¾çš„æ–¹æ³•ã€‚ä¼ ç»Ÿçš„NeRFå»é›¾æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”éš¾ä»¥æ¢å¤ç»†èŠ‚ã€‚æ–‡ç« æå‡ºä½¿ç”¨æ˜ç¡®çš„Gaussianè¡¨è¾¾å­¦ä¹ é›¾å›¾åƒçš„å½¢æˆæœºåˆ¶ï¼ŒåŸºäºå¤§æ°”æ•£å°„æ¨¡å‹ï¼Œç›´æ¥å»ºç«‹ä¼ è¾“å‡½æ•°è‡³GaussianåŸå§‹æ•°æ®ä¸Šï¼Œå»é›¾æ¸²æŸ“æ¸…æ™°åœºæ™¯ã€‚æ–°æ–¹æ³•DehazeGSä»…ä½¿ç”¨å¤šè§†è§’é›¾å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè®­ç»ƒæ—¶è”åˆå­¦ä¹ å¤§æ°”å…‰å’Œæ•£å°„ç³»æ•°ï¼Œä¼˜åŒ–Gaussianè¡¨è¾¾ã€‚å®éªŒè¯æ˜å…¶åœ¨çœŸå®å’Œåˆæˆé›¾å¤©æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰è§†å›¾åˆæˆæ–¹æ³•ä¸»è¦é’ˆå¯¹é«˜è´¨é‡æ¸…æ™°å›¾åƒï¼Œé›¾å¤©åœºæ™¯ä¸­çš„æ•£å°„å’Œè¡°å‡ä¼šé™ä½æ¸²æŸ“è´¨é‡ã€‚</li>
<li>NeRF-basedå»é›¾æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”éš¾ä»¥æ¢å¤ç»†èŠ‚ã€‚</li>
<li>æå‡ºä½¿ç”¨æ˜ç¡®çš„Gaussianè¡¨è¾¾å­¦ä¹ é›¾å›¾åƒçš„å½¢æˆæœºåˆ¶ã€‚</li>
<li>åŸºäºå¤§æ°”æ•£å°„æ¨¡å‹ï¼Œç›´æ¥å»ºç«‹ä¼ è¾“å‡½æ•°è‡³GaussianåŸå§‹æ•°æ®ä¸Šã€‚</li>
<li>DehazeGSæ–¹æ³•ä»…ä½¿ç”¨å¤šè§†è§’é›¾å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè¿›è¡Œå»é›¾æ¸²æŸ“ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­è”åˆå­¦ä¹ å¤§æ°”å…‰å’Œæ•£å°„ç³»æ•°ï¼Œä¼˜åŒ–Gaussianè¡¨è¾¾ã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜DehazeGSåœ¨çœŸå®å’Œåˆæˆé›¾å¤©æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-490255692d2002c08ce2a05a288d18e9" align="middle">
<img src="https://picx.zhimg.com/v2-b55f0ffe2cd9393834c9e411160bcd53" align="middle">
<img src="https://picx.zhimg.com/v2-4e8b35c5bd815388a139c395184ad82a" align="middle">
<img src="https://picx.zhimg.com/v2-f871051d79f3e470460f669fff2bcc80" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EnerVerse-Envisioning-Embodied-Future-Space-for-Robotics-Manipulation"><a href="#EnerVerse-Envisioning-Embodied-Future-Space-for-Robotics-Manipulation" class="headerlink" title="EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation"></a>EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation</h2><p><strong>Authors:Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Yue Liao, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren</strong></p>
<p>We introduce EnerVerse, a generative robotics foundation model that constructs and interprets embodied spaces. EnerVerse employs a chunk-wise autoregressive video diffusion framework to predict future embodied spaces from instructions, enhanced by a sparse context memory for long-term reasoning. To model the 3D robotics world, we adopt a multi-view video representation, providing rich perspectives to address challenges like motion ambiguity and 3D grounding. Additionally, EnerVerse-D, a data engine pipeline combining generative modeling with 4D Gaussian Splatting, forms a self-reinforcing data loop to reduce the sim-to-real gap. Leveraging these innovations, EnerVerse translates 4D world representations into physical actions via a policy head (EnerVerse-A), achieving state-of-the-art performance in both simulation and real-world tasks. For efficiency, EnerVerse-A reuses features from the first denoising step and predicts action chunks, achieving about 280 ms per 8-step action chunk on a single RTX 4090. Further video demos, dataset samples could be found in our project page.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†EnerVerseï¼Œè¿™æ˜¯ä¸€ä¸ªæ„å»ºå’Œè§£é‡Šå®ä½“ç©ºé—´çš„ç”Ÿæˆå¼æœºå™¨äººåŸºç¡€æ¨¡å‹ã€‚EnerVerseé‡‡ç”¨åˆ†å—è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œæ ¹æ®æŒ‡ä»¤é¢„æµ‹æœªæ¥çš„å®ä½“ç©ºé—´ï¼Œå¹¶é€šè¿‡ç¨€ç–ä¸Šä¸‹æ–‡è®°å¿†è¿›è¡Œé•¿æœŸæ¨ç†æ¥å¢å¼ºé¢„æµ‹ã€‚ä¸ºäº†æ„å»ºä¸‰ç»´æœºå™¨äººä¸–ç•Œæ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šè§†è§’è§†é¢‘è¡¨ç¤ºï¼Œæä¾›ä¸°å¯Œçš„è§†è§’æ¥è§£å†³è¿åŠ¨æ¨¡ç³Šå’Œä¸‰ç»´å®šä½ç­‰æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒEnerVerse-Dæ˜¯ä¸€ä¸ªæ•°æ®å¼•æ“ç®¡é“ï¼Œå°†ç”Ÿæˆå»ºæ¨¡ä¸4Dé«˜æ–¯æ¶‚æŠ¹ç›¸ç»“åˆï¼Œå½¢æˆä¸€ä¸ªè‡ªæˆ‘åŠ å¼ºçš„æ•°æ®å¾ªç¯ï¼Œä»¥ç¼©å°ä»¿çœŸä¸ç°å®çš„å·®è·ã€‚åˆ©ç”¨è¿™äº›åˆ›æ–°æŠ€æœ¯ï¼ŒEnerVerseé€šè¿‡ç­–ç•¥å¤´ï¼ˆEnerVerse-Aï¼‰å°†4Dä¸–ç•Œè¡¨ç¤ºè½¬åŒ–ä¸ºç‰©ç†åŠ¨ä½œï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­éƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼ŒEnerVerse-Aé‡ç”¨äº†ç¬¬ä¸€æ¬¡å»å™ªæ­¥éª¤çš„ç‰¹å¾ï¼Œå¹¶é¢„æµ‹åŠ¨ä½œå—ï¼Œåœ¨å•ä¸ªRTX 4090ä¸Šå®ç°æ¯8æ­¥åŠ¨ä½œå—çº¦280æ¯«ç§’çš„å“åº”æ—¶é—´ã€‚æ›´å¤šè§†é¢‘æ¼”ç¤ºã€æ•°æ®é›†æ ·æœ¬å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01895v3">PDF</a> Accepted by NeurIPS 2025. Website: <a target="_blank" rel="noopener" href="https://sites.google.com/view/enerverse">https://sites.google.com/view/enerverse</a></p>
<p><strong>Summary</strong></p>
<p>EnerVerseæ˜¯ä¸€ä¸ªç”Ÿæˆå¼æœºå™¨äººåŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ„å»ºå¹¶è§£è¯»ä½“ç°ç©ºé—´ã€‚å®ƒé‡‡ç”¨åˆ†å—è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œä»æŒ‡ä»¤é¢„æµ‹æœªæ¥çš„ä½“ç°ç©ºé—´ï¼Œå¹¶é€šè¿‡ç¨€ç–ä¸Šä¸‹æ–‡è®°å¿†è¿›è¡Œé•¿æœŸæ¨ç†ã€‚é€šè¿‡é‡‡ç”¨å¤šè§†è§’è§†é¢‘è¡¨ç¤ºæ¥å»ºæ¨¡3Dæœºå™¨äººä¸–ç•Œï¼Œè§£å†³äº†è¿åŠ¨æ¨¡ç³Šå’Œ3Då®šä½ç­‰æŒ‘æˆ˜ã€‚EnerVerse-Dæ•°æ®å¼•æ“ç®¡é“ç»“åˆç”Ÿæˆå»ºæ¨¡ä¸4Dé«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯ï¼Œå½¢æˆè‡ªæˆ‘åŠ å¼ºçš„æ•°æ®å¾ªç¯ï¼Œç¼©å°æ¨¡æ‹Ÿä¸çœŸå®ä¹‹é—´çš„å·®è·ã€‚EnerVerseå°†4Dä¸–ç•Œè¡¨ç¤ºè½¬åŒ–ä¸ºç‰©ç†åŠ¨ä½œï¼Œé€šè¿‡ç­–ç•¥å¤´ï¼ˆEnerVerse-Aï¼‰å®ç°ä»¿çœŸå’ŒçœŸå®ä»»åŠ¡çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EnerVerseæ˜¯ä¸€ä¸ªç”Ÿæˆå¼æœºå™¨äººåŸºç¡€æ¨¡å‹ï¼Œèƒ½æ„å»ºå¹¶è§£è¯»ä½“ç°ç©ºé—´ã€‚</li>
<li>é‡‡ç”¨åˆ†å—è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¡†æ¶è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>é€šè¿‡ç¨€ç–ä¸Šä¸‹æ–‡è®°å¿†è¿›è¡Œé•¿æœŸæ¨ç†ã€‚</li>
<li>é‡‡ç”¨å¤šè§†è§’è§†é¢‘è¡¨ç¤ºå»ºæ¨¡3Dæœºå™¨äººä¸–ç•Œï¼Œè§£å†³è¿åŠ¨æ¨¡ç³Šå’Œ3Då®šä½æŒ‘æˆ˜ã€‚</li>
<li>EnerVerse-Dæ•°æ®å¼•æ“ç®¡é“ç»“åˆç”Ÿæˆå»ºæ¨¡ä¸4Dé«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯ã€‚</li>
<li>è‡ªæˆ‘åŠ å¼ºçš„æ•°æ®å¾ªç¯æœ‰åŠ©äºç¼©å°æ¨¡æ‹Ÿä¸çœŸå®ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b09f64cb74b8061d208a275a97c13dd8" align="middle">
<img src="https://picx.zhimg.com/v2-1c62c2b7392d57e364162218ea7e4057" align="middle">
<img src="https://picx.zhimg.com/v2-43764fb1f2a79a3f1dab473e5c7c451e" align="middle">
<img src="https://picx.zhimg.com/v2-0f2fa9a4d12010d2f67ffd5dbdd6d309" align="middle">
<img src="https://picx.zhimg.com/v2-a3392b0e40718dbc4f7b24be1f5eb9a5" align="middle">
<img src="https://picx.zhimg.com/v2-f8fd13a3da74be3598dfca5bfdcfe792" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-18d7d291ab66d2129875e5d5e6df27a4" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  PFAvatar Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5c8c04429049eeef36d71747b460ea83" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  PFAvatar Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
