<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Crossing Borders A Multimodal Challenge for Indian Poetry Translation and Image Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-bf0625dfb62a4e243a388d7e148e3691')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="Crossing-Borders-A-Multimodal-Challenge-for-Indian-Poetry-Translation-and-Image-Generation"><a href="#Crossing-Borders-A-Multimodal-Challenge-for-Indian-Poetry-Translation-and-Image-Generation" class="headerlink" title="Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation"></a>Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation</h2><p><strong>Authors:Sofia Jamil, Kotla Sai Charan, Sriparna Saha, Koustava Goswami, Joseph K J</strong></p>
<p>Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the readerâ€™s experience.</p>
<blockquote>
<p>å°åº¦è¯—æ­Œä»¥å…¶è¯­è¨€ä¸Šçš„å¤æ‚æ€§å’Œæ·±åšçš„æ–‡åŒ–å…±é¸£è€Œé—»åï¼Œæ‹¥æœ‰è·¨è¶Šæ•°åƒå¹´çš„ä¸°å¯Œè€Œå¤šæ ·çš„ä¼ ç»Ÿã€‚ç„¶è€Œï¼Œå…¶å¤šå±‚æ¬¡çš„å«ä¹‰ã€æ–‡åŒ–å…¸æ•…å’Œå¤æ‚çš„è¯­æ³•ç»“æ„å¾€å¾€æ„æˆç†è§£ä¸Šçš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹äºéæ¯è¯­è€…æˆ–å¯¹å…¶è¯­å¢ƒå’Œè¯­è¨€ä¸ç†Ÿæ‚‰çš„è¯»è€…ã€‚å°½ç®¡å…¶åœ¨æ–‡åŒ–ä¸Šå…·æœ‰é‡è¦æ€§ï¼Œä½†ç°æœ‰å…³äºè¯—æ­Œçš„ä½œå“å¤§å¤šå¿½è§†äº†å°åº¦è¯­è¨€è¯—æ­Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¿»è¯‘ä¸å›¾åƒç”Ÿæˆï¼ˆTAIï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é€‚å½“æç¤ºè°ƒæ•´ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒè”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡ä¸­çš„ä¼˜è´¨æ•™è‚²ï¼ˆSDG 4ï¼‰å’Œå‡å°‘ä¸å¹³ç­‰ï¼ˆSDG 10ï¼‰ï¼Œé€šè¿‡æé«˜æ–‡åŒ–ä¸°å¯Œçš„å°åº¦è¯­è¨€è¯—æ­Œçš„å¯è¾¾æ€§ï¼Œé¢å‘å…¨çƒå—ä¼—ã€‚å®ƒåŒ…æ‹¬ï¼ˆ1ï¼‰ä¸€ä¸ªç¿»è¯‘æ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨Odds Ratio Preference Alignment Algorithmç®—æ³•ï¼Œå°†å½¢æ€ä¸°å¯Œçš„è¯—æ­Œå‡†ç¡®ç¿»è¯‘æˆè‹±è¯­ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªå›¾åƒç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—é‡‡ç”¨è¯­ä¹‰å›¾æ¥æ•è·æ ‡è®°ã€ä¾å­˜å…³ç³»å’Œéšå–»åŠå…¶æ„ä¹‰ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä»¥åˆ›å»ºå°åº¦è¯—æ­Œçš„è§†è§‰æœ‰æ„ä¹‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬äººç±»å’Œå®šé‡è¯„ä¼°ï¼Œè¯æ˜äº†TAIæ‰©æ•£åœ¨è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³å°åº¦è¯­è¨€è¯—æ­Œèµ„æºç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å½¢æ€ä¸°å¯Œå°åº¦è¯­è¨€è¯—æ­ŒMorphoVerseæ•°æ®é›†ï¼ŒåŒ…å«1570é¦–è·¨è¶Š21ç§ä½èµ„æºå°åº¦è¯­è¨€çš„è¯—æ­Œã€‚é€šè¿‡è§£å†³è¯—æ­Œç¿»è¯‘å’Œè§†è§‰ç†è§£ä¹‹é—´çš„å·®è·ï¼Œè¿™é¡¹å·¥ä½œæ—¨åœ¨æé«˜å¯åŠæ€§å¹¶ä¸°å¯Œè¯»è€…çš„ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13689v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼‰çš„ç¿»è¯‘ä¸å›¾åƒç”Ÿæˆï¼ˆTAIï¼‰æ¡†æ¶ï¼Œä»¥æ”¯æŒè”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡ä¸­çš„ä¼˜è´¨æ•™è‚²å’Œå‡å°‘ä¸å¹³ç­‰ç›®æ ‡ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ç¿»è¯‘æ¨¡å—å’Œå›¾åƒç”Ÿæˆæ¨¡å—ï¼Œåˆ†åˆ«ä½¿ç”¨Odds Ratio Preference Alignment Algorithmç®—æ³•å’Œè¯­ä¹‰å›¾æŠ€æœ¯ï¼Œä»¥æé«˜å°åº¦è¯—æ­Œçš„å…¨çƒæ€§å¯è¾¾æ€§å’Œè§†è§‰å‘ˆç°æ•ˆæœã€‚é€šè¿‡å…¨é¢çš„å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬äººç±»å’Œå®šé‡è¯„ä¼°ï¼Œè¯æ˜äº†TAIæ¡†æ¶åœ¨è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³å°åº¦è¯­è¨€è¯—æ­Œèµ„æºåŒ®ä¹çš„é—®é¢˜ï¼Œè¿˜æ¨å‡ºäº†MorphoVerseæ•°æ®é›†ï¼ŒåŒ…å«1570é¦–21ç§ä½èµ„æºå°åº¦è¯­è¨€çš„è¯—æ­Œã€‚æ­¤ç ”ç©¶æ—¨åœ¨ç¼©å°è¯—æ­Œç¿»è¯‘å’Œè§†è§‰ç†è§£æ–¹é¢çš„å·®è·ï¼Œæé«˜å°åº¦è¯—æ­Œçš„æ™®åŠæ€§å’Œè¯»è€…ä½“éªŒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å°åº¦è¯—æ­Œå…·æœ‰è¯­è¨€å¤æ‚ã€æ–‡åŒ–å…±é¸£æ·±åšçš„ç‰¹ç‚¹ï¼Œä½†å…¶ä¸°å¯Œçš„æ–‡åŒ–å†…æ¶µå’Œå¤æ‚çš„è¯­æ³•ç»“æ„å¯¹éæ¯è¯­è€…æˆ–ä¸äº†è§£å…¶è¯­å¢ƒå’Œè¯­è¨€çš„è¯»è€…æ¥è¯´å¸¸å¸¸æ„æˆç†è§£ä¸Šçš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å…³äºè¯—æ­Œçš„ç ”ç©¶å¤§å¤šå¿½è§†äº†å°åº¦è¯­è¨€çš„è¯—æ­Œã€‚</li>
<li>æå‡ºçš„Translation and Image Generationï¼ˆTAIï¼‰æ¡†æ¶ï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¢å¼ºäº†å°åº¦è¯­è¨€è¯—æ­Œçš„å…¨çƒå¯è¾¾æ€§ã€‚</li>
<li>TAIæ¡†æ¶åŒ…æ‹¬ç¿»è¯‘æ¨¡å—å’Œå›¾åƒç”Ÿæˆæ¨¡å—ï¼Œåˆ†åˆ«åˆ©ç”¨Odds Ratio Preference Alignment Algorithmç®—æ³•å’Œè¯­ä¹‰å›¾æŠ€æœ¯ï¼Œå‡†ç¡®ç¿»è¯‘å¹¶ç”ŸåŠ¨å‘ˆç°å°åº¦è¯—æ­Œã€‚</li>
<li>å…¨é¢çš„å®éªŒè¯„ä¼°è¯æ˜äº†TAIæ¡†æ¶åœ¨è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>æ¨å‡ºMorphologically Rich Indian Language Poems MorphoVerseæ•°æ®é›†ï¼Œä»¥è§£å†³å°åº¦è¯­è¨€è¯—æ­Œèµ„æºåŒ®ä¹çš„é—®é¢˜ã€‚</li>
<li>è¯¥ç ”ç©¶æ—¨åœ¨æé«˜å°åº¦è¯—æ­Œçš„æ™®åŠæ€§ï¼Œä¸°å¯Œè¯»è€…çš„é˜…è¯»ä½“éªŒï¼Œå¹¶ç¬¦åˆè”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡ä¸­çš„æ•™è‚²å’Œå‡å°‘ä¸å¹³ç­‰ç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe8f56b640106e849238894ae71f84f3" align="middle">
<img src="https://picx.zhimg.com/v2-fcf6490258e970f26ba6a174333559a7" align="middle">
<img src="https://picx.zhimg.com/v2-b7b13598440fc3707843a91a6a8c965d" align="middle">
<img src="https://picx.zhimg.com/v2-2f02d6dcf4b57a639f1aa397320af375" align="middle">
<img src="https://picx.zhimg.com/v2-9d5028d10aa948a9debf3fbabe5f23d7" align="middle">
<img src="https://picx.zhimg.com/v2-2569534174f2934f2d2a780b02a6d8ef" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Training-Free-Multi-View-Extension-of-IC-Light-for-Textual-Position-Aware-Scene-Relighting"><a href="#Training-Free-Multi-View-Extension-of-IC-Light-for-Textual-Position-Aware-Scene-Relighting" class="headerlink" title="Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting"></a>Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting</h2><p><strong>Authors:Jiangnan Ye, Jiedong Zhuang, Lianrui Mu, Wenjie Zheng, Jiaqi Hu, Xingze Zou, Jing Wang, Haoji Hu</strong></p>
<p>We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†GS-Lightï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆã€å¯¹æ–‡æœ¬ä½ç½®æœ‰æ„è¯†çš„ç®¡é“ï¼Œç”¨äºå¯¹é€šè¿‡é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºçš„3Dåœºæ™¯è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„é‡ç…§æ˜ã€‚GS-Lightå®ç°äº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å•è¾“å…¥æ‰©æ•£æ¨¡å‹çš„æ‰©å±•ï¼Œä»¥å¤„ç†å¤šè§†å›¾è¾“å…¥ã€‚ç»™å®šç”¨æˆ·æç¤ºï¼Œå¯èƒ½æŒ‡å®šç…§æ˜æ–¹å‘ã€é¢œè‰²ã€å¼ºåº¦æˆ–å‚è€ƒå¯¹è±¡ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰æ¥è§£ææç¤ºä»¥è·å–ç…§æ˜ä¼˜å…ˆä¿¡æ¯ã€‚æˆ‘ä»¬ä½¿ç”¨ç°æˆçš„å‡ ä½•å’Œè¯­ä¹‰ä¼°è®¡å™¨ï¼ˆæ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ï¼Œå°†è¿™äº›ç…§æ˜ä¼˜å…ˆä¿¡æ¯ä¸è§†å›¾å‡ ä½•çº¦æŸèåˆï¼Œä»¥è®¡ç®—ç…§æ˜åœ°å›¾å¹¶ä¸ºæ¯ä¸ªè§†å›¾ç”Ÿæˆåˆå§‹æ½œåœ¨ä»£ç ã€‚è¿™äº›ç²¾å¿ƒå¾—å‡ºçš„åˆå§‹æ½œåœ¨ä»£ç æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®åœ°åæ˜ ç”¨æˆ·æœŸæœ›çš„é‡ç…§æ˜è¾“å‡ºï¼Œå°¤å…¶æ˜¯åœ¨ç…§æ˜æ–¹å‘æ–¹é¢ã€‚é€šè¿‡å°†å¤šè§†å›¾æ¸²æŸ“å›¾åƒå’Œåˆå§‹æ½œåœ¨ä»£ç è¾“å…¥åˆ°æˆ‘ä»¬çš„å¤šè§†å›¾é‡ç…§æ˜æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é«˜ä¿çœŸã€è‰ºæœ¯åŒ–çš„é‡ç…§æ˜å›¾åƒã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨é‡ç…§æ˜å¤–è§‚å¯¹3DGSåœºæ™¯è¿›è¡Œå¾®è°ƒï¼Œä»¥è·å¾—å®Œå…¨é‡ç…§æ˜çš„3Dåœºæ™¯ã€‚æˆ‘ä»¬åœ¨å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸Šè¯„ä¼°äº†GS-Lightï¼Œå°†å…¶ä¸æœ€æ–°åŸºçº¿ï¼ˆåŒ…æ‹¬å•è§†å›¾é‡ç…§æ˜ã€è§†é¢‘é‡ç…§æ˜å’Œåœºæ™¯ç¼–è¾‘æ–¹æ³•ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚ä½¿ç”¨å®šé‡æŒ‡æ ‡ï¼ˆå¤šè§†å›¾ä¸€è‡´æ€§ã€æˆåƒè´¨é‡ã€ç¾å­¦åˆ†æ•°ã€è¯­ä¹‰ç›¸ä¼¼æ€§ç­‰ï¼‰å’Œå®šæ€§è¯„ä¼°ï¼ˆç”¨æˆ·ç ”ç©¶ï¼‰ï¼ŒGS-Lightåœ¨åŸºçº¿ä¹‹ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„æå‡ã€‚ä»£ç å’Œèµ„äº§å°†åœ¨å‘å¸ƒæ—¶æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13684v1">PDF</a> Submitting for Neurocomputing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GS-Lightï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„ã€å¯¹æ–‡æœ¬ä½ç½®æ•æ„Ÿçš„ç®¡é“ï¼Œç”¨äºå¯¹é€šè¿‡é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºçš„3Dåœºæ™¯è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„é‡ç…§æ˜ã€‚GS-Lightå®æ–½äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¤šè§†å›¾è¾“å…¥æ‰©æ•£æ¨¡å‹æ‰©å±•ï¼Œç»™å®šç”¨æˆ·æç¤ºï¼ˆå¯èƒ½æŒ‡å®šç…§æ˜æ–¹å‘ã€é¢œè‰²ã€å¼ºåº¦æˆ–å‚è€ƒå¯¹è±¡ï¼‰ï¼Œé‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å°†æç¤ºè§£æä¸ºç…§æ˜å…ˆéªŒã€‚ç»“åˆç°æˆçš„å‡ ä½•å’Œè¯­ä¹‰ä¼°è®¡å™¨ï¼ˆæ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ï¼Œå°†ç…§æ˜å…ˆéªŒä¸è§†å›¾å‡ ä½•çº¦æŸèåˆï¼Œè®¡ç®—ç…§æ˜å›¾å¹¶ä¸ºæ¯ä¸ªè§†å›¾ç”Ÿæˆåˆå§‹æ½œåœ¨ä»£ç ã€‚è¿™äº›ç²¾å¿ƒå¾—å‡ºçš„åˆå§‹æ½œåœ¨ä»£ç æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®åœ°åæ˜ ç”¨æˆ·æœŸæœ›çš„ç…§æ˜è¾“å‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨ç…§æ˜æ–¹å‘æ–¹é¢ã€‚é€šè¿‡å‘å¤šè§†å›¾é‡ç…§æ˜æ¨¡å‹è¾“å…¥å¤šè§†å›¾æ¸²æŸ“å›¾åƒä»¥åŠåˆå§‹æ½œåœ¨ä»£ç ï¼Œç”Ÿæˆé«˜ä¿çœŸã€è‰ºæœ¯åŒ–çš„é‡ç…§æ˜å›¾åƒã€‚æœ€åï¼Œä½¿ç”¨é‡ç…§æ˜å¤–è§‚å¾®è°ƒ3DGSåœºæ™¯ï¼Œè·å¾—å®Œå…¨é‡ç…§æ˜çš„3Dåœºæ™¯ã€‚GS-Lightåœ¨å®¤å†…å¤–åœºæ™¯ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œå®ƒåœ¨å¤šè§†å›¾ä¸€è‡´æ€§ã€æˆåƒè´¨é‡ã€ç¾å­¦è¯„åˆ†ã€è¯­ä¹‰ç›¸ä¼¼æ€§ç­‰æ–¹é¢è¡¨ç°å‡ºæŒç»­çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GS-Lightæ˜¯ä¸€ç§é«˜æ•ˆçš„æ–‡æœ¬å¼•å¯¼3Dåœºæ™¯é‡ç…§æ˜æ–¹æ³•ï¼ŒåŸºäºé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºã€‚</li>
<li>GS-Lighté€šè¿‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰è§£æç”¨æˆ·æç¤ºï¼Œç”Ÿæˆç…§æ˜å…ˆéªŒã€‚</li>
<li>ç»“åˆå‡ ä½•å’Œè¯­ä¹‰ä¼°è®¡ï¼Œå°†ç…§æ˜å…ˆéªŒä¸è§†å›¾å‡ ä½•çº¦æŸèåˆï¼Œè®¡ç®—ç…§æ˜å›¾ã€‚</li>
<li>åˆå§‹æ½œåœ¨ä»£ç æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®çš„ç…§æ˜è¾“å‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨ç…§æ˜æ–¹å‘æ–¹é¢ã€‚</li>
<li>å¤šè§†å›¾æ¸²æŸ“å›¾åƒå’Œåˆå§‹æ½œåœ¨ä»£ç ç”¨äºç”Ÿæˆé«˜ä¿çœŸã€è‰ºæœ¯åŒ–çš„é‡ç…§æ˜å›¾åƒã€‚</li>
<li>é€šè¿‡å¾®è°ƒè·å¾—å®Œå…¨é‡ç…§æ˜çš„3Dåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5de1aded6bd287d2278b1b06d3ab4d1e" align="middle">
<img src="https://picx.zhimg.com/v2-cf50de6bd2a2b021c48557a52d59e1ee" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model"><a href="#Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model" class="headerlink" title="Part-X-MLLM: Part-aware 3D Multimodal Large Language Model"></a>Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</h2><p><strong>Authors:Chunshi Wang, Junliang Ye, Yunhan Yang, Yang Li, Zizhuo Lin, Jun Zhu, Zhuo Chen, Yawei Luo, Chunchao Guo</strong></p>
<p>We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q&amp;A, compositional generation, and localized editing through one unified interface. Project page: <a target="_blank" rel="noopener" href="https://chunshi.wang/Part-X-MLLM/">https://chunshi.wang/Part-X-MLLM/</a></p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Part-X-MLLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸç”Ÿ3Då¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡ç»“æ„åŒ–çš„å¯æ‰§è¡Œè¯­æ³•å°†å„ç§3Dä»»åŠ¡ç»Ÿä¸€ä¸ºç¨‹åºã€‚ç»™å®šRGBç‚¹äº‘å’Œè‡ªç„¶è¯­è¨€æç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥è‡ªå›å½’åœ°ç”Ÿæˆä¸€ä¸ªå•ä¸€çš„è¿è´¯ä»¤ç‰Œåºåˆ—ï¼Œè¯¥åºåˆ—ç¼–ç éƒ¨åˆ†çº§è¾¹ç•Œæ¡†ã€è¯­ä¹‰æè¿°å’Œç¼–è¾‘å‘½ä»¤ã€‚è¿™ç§ç»“æ„åŒ–çš„è¾“å‡ºå¯ä»¥ä½œä¸ºé€šç”¨çš„æ¥å£ï¼Œé©±åŠ¨åŸºäºé›¶ä»¶ç”Ÿæˆçš„ä¸‹æ¸¸å‡ ä½•æ„ŸçŸ¥æ¨¡å—å’Œç¼–è¾‘æ¨¡å—ã€‚é€šè¿‡å°†ç¬¦å·è§„åˆ’ä»å‡ ä½•ç»¼åˆä¸­è§£è„±å‡ºæ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸ä»»ä½•å…¼å®¹çš„å‡ ä½•å¼•æ“é€šè¿‡å•ä¸€ã€æœ¬åœ°åŒ–çš„å‰ç«¯è¿›è¡Œæ§åˆ¶ã€‚æˆ‘ä»¬é¢„å…ˆè®­ç»ƒäº†ä¸€ä¸ªåŒç¼–ç å™¨æ¶æ„æ¥åˆ†ç¦»ç»“æ„å’Œè¯­ä¹‰ï¼Œå¹¶åœ¨å¤§è§„æ¨¡ã€ä»¥éƒ¨ä»¶ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ä¸Šè°ƒæ•´æ¨¡å‹æŒ‡ä»¤ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡ã€ç»“æ„åŒ–è®¡åˆ’æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé€šè¿‡ç»Ÿä¸€æ¥å£å®ç°äº†æœ‰æ ¹æ®çš„é—®ç­”ã€ç»„åˆç”Ÿæˆå’Œå®šä½ç¼–è¾‘çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://chunshi.wang/Part-X-MLLM/">https://chunshi.wang/Part-X-MLLM/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13647v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Part-X-MLLMæ˜¯ä¸€ç§ä¸‰ç»´å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒå°†å„ç§ä¸‰ç»´ä»»åŠ¡ç»Ÿä¸€è½¬åŒ–ä¸ºç»“æ„åŒ–ã€å¯æ‰§è¡Œçš„è¯­æ³•ç¨‹åºã€‚é€šè¿‡RGBç‚¹äº‘å’Œè‡ªç„¶è¯­è¨€æç¤ºï¼Œæ¨¡å‹å¯ç”ŸæˆåŒ…å«éƒ¨åˆ†çº§åˆ«è¾¹ç•Œæ¡†ã€è¯­ä¹‰æè¿°å’Œç¼–è¾‘å‘½ä»¤çš„å•ä¸€ã€è¿è´¯çš„ä»¤ç‰Œåºåˆ—ã€‚è¿™ç§ç»“æ„åŒ–çš„è¾“å‡ºä¸ºåŸºäºéƒ¨åˆ†çš„ç”Ÿæˆå’Œç¼–è¾‘çš„ä¸‹æ¸¸å‡ ä½•æ„ŸçŸ¥æ¨¡å—æä¾›äº†é€šç”¨æ¥å£ã€‚é€šè¿‡ç¬¦å·è§„åˆ’ä¸å‡ ä½•åˆæˆçš„è§£è€¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸ä»»ä½•å…¼å®¹çš„å‡ ä½•å¼•æ“é€šè¿‡å•ä¸€çš„è¯­è¨€åŸç”Ÿå‰ç«¯è¿›è¡Œæ§åˆ¶ã€‚æ¨¡å‹é¢„è®­ç»ƒäº†ä¸€ç§åŒç¼–ç å™¨æ¶æ„æ¥åˆ†ç¦»ç»“æ„å’Œè¯­ä¹‰ï¼Œå¹¶åœ¨å¤§è§„æ¨¡ã€ä»¥éƒ¨åˆ†ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç»“æ„åŒ–è®¡åˆ’äº§å‡ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶é€šè¿‡ç»Ÿä¸€æ¥å£å®ç°äº†é¢†å…ˆçš„åœ°å¹³çº¿é—®ç­”ã€ç»„åˆç”Ÿæˆå’Œå®šä½ç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Part-X-MLLMæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ä¸‰ç»´å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å°†ä¸åŒçš„ä¸‰ç»´ä»»åŠ¡è½¬åŒ–ä¸ºç»“æ„åŒ–ã€å¯æ‰§è¡Œçš„è¯­æ³•ç¨‹åºã€‚</li>
<li>æ¨¡å‹æ¥å—RGBç‚¹äº‘å’Œè‡ªç„¶è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥ï¼Œç”ŸæˆåŒ…å«éƒ¨åˆ†çº§åˆ«ä¿¡æ¯çš„è¿è´¯ä»¤ç‰Œåºåˆ—ã€‚</li>
<li>è¿™ç§ç»“æ„åŒ–è¾“å‡ºä¸ºä¸‹æ¸¸å‡ ä½•æ„ŸçŸ¥æ¨¡å—æä¾›äº†é€šç”¨æ¥å£ï¼Œæ”¯æŒåŸºäºéƒ¨åˆ†çš„ç”Ÿæˆå’Œç¼–è¾‘ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç¬¦å·è§„åˆ’ä¸å‡ ä½•åˆæˆçš„è§£è€¦ï¼Œå…è®¸ä½¿ç”¨å•ä¸€çš„è¯­è¨€åŸç”Ÿå‰ç«¯æ§åˆ¶ä»»ä½•å…¼å®¹çš„å‡ ä½•å¼•æ“ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨åŒç¼–ç å™¨æ¶æ„è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥åˆ†ç¦»ç»“æ„å’Œè¯­ä¹‰ï¼Œå¹¶åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒPart-X-MLLMåœ¨ç»“æ„åŒ–è®¡åˆ’äº§å‡ºæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°äº†å…ˆè¿›çš„åœ°å¹³çº¿é—®ç­”ã€ç»„åˆç”Ÿæˆå’Œå®šä½ç¼–è¾‘åŠŸèƒ½ï¼Œé€šè¿‡ç»Ÿä¸€æ¥å£å®Œæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb988bde1804e3431a26c8c5c922103b" align="middle">
<img src="https://picx.zhimg.com/v2-05c59a5780c191632b454266a7a51e8a" align="middle">
<img src="https://picx.zhimg.com/v2-f89e8d26c6a3a3a20e2eba074f3edb85" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly"><a href="#Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly" class="headerlink" title="Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?"></a>Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</h2><p><strong>Authors:Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, Lingming Zhang</strong></p>
<p>Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined&#x2F;modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-GÃ¶del Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨é‡å¡‘å‡ ä¹æ‰€æœ‰è¡Œä¸šï¼ŒåŒ…æ‹¬è½¯ä»¶å·¥ç¨‹ã€‚è¿‘å¹´æ¥ï¼Œå·²ç»æå‡ºäº†è®¸å¤šLLMä»£ç†æ¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„è½¯ä»¶é—®é¢˜ã€‚æ­¤ç±»è½¯ä»¶ä»£ç†é€šå¸¸é…å¤‡äº†ä¸€å¥—ç¼–ç å·¥å…·ï¼Œå¹¶èƒ½å¤Ÿè‡ªä¸»å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œä»¥å½¢æˆå®Œæ•´çš„è½¨è¿¹æ¥è§£å†³ç«¯åˆ°ç«¯çš„è½¯ä»¶ä»»åŠ¡ã€‚è™½ç„¶å‰æ™¯çœ‹å¥½ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦ä¸“é—¨è®¾è®¡ï¼Œå¹¶ä¸”å¯èƒ½ä»ç„¶ä¸å¤Ÿç†æƒ³ï¼Œå› ä¸ºç©·å°½æ•´ä¸ªä»£ç†æ¶æ„çš„è®¾è®¡ç©ºé—´å¯èƒ½ä¼šæå…¶å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚</p>
</blockquote>
<p>ç ”ç©¶äººå‘˜æ„è¯†åˆ°è½¯ä»¶ä»£ç†æœ¬èº«å°±æ˜¯è½¯ä»¶ï¼Œå¯ä»¥è¿›ä¸€æ­¥è¿›è¡Œç²¾ç»†åŒ–&#x2F;ä¿®æ”¹ï¼Œæœ€è¿‘å·²ç»æå‡ºäº†ä¸€äº›è‡ªæˆ‘æ”¹è¿›çš„è½¯ä»¶ä»£ç†ï¼ŒåŒ…æ‹¬è¾¾å°”æ–‡-æ­Œå¾·æœºå™¨ï¼ˆDGMï¼‰ã€‚åŒæ—¶ï¼Œè¿™æ ·çš„è‡ªæˆ‘æ”¹è¿›ä»£ç†éœ€è¦åœ¨ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œæ˜‚è´µçš„ç¦»çº¿è®­ç»ƒï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•åœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹æˆ–åŸºå‡†æµ‹è¯•ä¹‹é—´å¾ˆå¥½åœ°æ¨å¹¿ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13646v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨é‡å¡‘åŒ…æ‹¬è½¯ä»¶å·¥ç¨‹åœ¨å†…çš„å‡ ä¹æ‰€æœ‰è¡Œä¸šã€‚è¿‘å¹´æ¥ï¼Œäººä»¬æå‡ºäº†ä¸€ç³»åˆ—LLMä»£ç†æ¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„è½¯ä»¶é—®é¢˜ã€‚è¿™äº›è½¯ä»¶ä»£ç†é€šå¸¸é…å¤‡äº†ä¸€å¥—ç¼–ç å·¥å…·ï¼Œå¹¶èƒ½å¤Ÿè‡ªä¸»å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ä»¥å½¢æˆå®Œæ•´çš„è½¨è¿¹æ¥è§£å†³ç«¯åˆ°ç«¯çš„è½¯ä»¶ä»»åŠ¡ã€‚å°½ç®¡æœ‰å‰æ™¯ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦ä¸“é—¨è®¾è®¡ï¼Œå¹¶ä¸”å¯èƒ½ä»ç„¶ä¸å¤Ÿç†æƒ³ï¼Œå› ä¸ºç©·å°½æ•´ä¸ªä»£ç†æ¶æ„çš„è®¾è®¡ç©ºé—´æå…·æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚ç ”ç©¶äººå‘˜å·²ç»è®¤è¯†åˆ°è½¯ä»¶ä»£ç†æœ¬è´¨ä¸Šæ˜¯è½¯ä»¶æœ¬èº«ï¼Œå¯ä»¥è¿›ä¸€æ­¥è¿›è¡Œæ”¹è¿›å’Œä¿®æ”¹ï¼Œå› æ­¤æœ€è¿‘å·²ç»æå‡ºäº†ä¸€ç³»åˆ—è‡ªæˆ‘æ”¹è¿›çš„è½¯ä»¶ä»£ç†ï¼ŒåŒ…æ‹¬è¾¾å°”æ–‡-æ­Œå¾·æœºå™¨ï¼ˆDGMï¼‰ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„è‡ªæˆ‘æ”¹è¿›ä»£ç†éœ€è¦åœ¨ç‰¹å®šåŸºå‡†ä¸Šè¿›è¡Œæ˜‚è´µçš„ç¦»çº¿è®­ç»ƒï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•åœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹æˆ–åŸºå‡†ä¸Šè‰¯å¥½åœ°æ¨å¹¿ã€‚æœ¬æ–‡æå‡ºäº†Live-SWE-agentï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿåœ¨è§£å†³ç°å®ä¸–ç•Œè½¯ä»¶é—®é¢˜æ—¶å®æ—¶è‡ªä¸»è¿ç»­è¿›åŒ–çš„è½¯ä»¶ä»£ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒLive-SWE-agentä»æœ€åŸºç¡€çš„ä»£ç†æ¶æ„å¼€å§‹ï¼Œä»…ä½¿ç”¨bashå·¥å…·ï¼ˆä¾‹å¦‚ï¼Œmini-SWE-agentï¼‰ï¼Œåœ¨è§£å†³ç°å®ä¸–ç•Œè½¯ä»¶é—®é¢˜æ—¶è‡ªä¸»è¿›åŒ–è‡ªå·±çš„æ¶æ„å®ç°ã€‚åœ¨å¹¿æ³›ç ”ç©¶çš„SWE-benchéªŒè¯åŸºå‡†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLive-SWE-agentåœ¨ä¸è¿›è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾çš„æƒ…å†µä¸‹å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è§£å†³ç‡ä¸º75.4%ï¼Œè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰çš„å¼€æºè½¯ä»¶ä»£ç†ï¼Œå¹¶æ¥è¿‘æœ€ä½³çš„ä¸“æœ‰è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒLive-SWE-agentåœ¨æœ€æ–°çš„SWE-Bench ProåŸºå‡†ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ‰‹åŠ¨åˆ¶ä½œçš„è½¯ä»¶ä»£ç†ï¼Œå®ç°äº†æœ€ä½³çš„è§£å†³ç‡ä¸º45.8%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨é‡å¡‘è½¯ä»¶å·¥ç¨‹è¡Œä¸šã€‚</li>
<li>è½¯ä»¶ä»£ç†è¢«ç”¨æ¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„è½¯ä»¶é—®é¢˜ï¼Œå®ƒä»¬é€šå¸¸é…å¤‡ç¼–ç å·¥å…·å¹¶èƒ½è‡ªä¸»å†³ç­–ã€‚</li>
<li>å½“å‰è½¯ä»¶ä»£ç†é€šå¸¸éœ€è¦ä¸“é—¨è®¾è®¡ï¼Œä¸”å¯èƒ½å­˜åœ¨ä¸å¤Ÿç†æƒ³çš„æƒ…å†µï¼Œå› ä¸ºç©·å°½æ•´ä¸ªä»£ç†æ¶æ„çš„è®¾è®¡ç©ºé—´å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>è‡ªæˆ‘æ”¹è¿›çš„è½¯ä»¶ä»£ç†æ˜¯æœ€è¿‘çš„ç ”ç©¶è¶‹åŠ¿ï¼Œå…¶ä¸­åŒ…æ‹¬è¾¾å°”æ–‡-æ­Œå¾·æœºå™¨ï¼ˆDGMï¼‰ã€‚</li>
<li>ç°æœ‰çš„è‡ªæˆ‘æ”¹è¿›ä»£ç†éœ€è¦åœ¨ç‰¹å®šåŸºå‡†ä¸Šè¿›è¡Œæ˜‚è´µçš„ç¦»çº¿è®­ç»ƒï¼Œä¸”å…¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>Live-SWE-agentæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨è§£å†³ç°å®ä¸–ç•Œè½¯ä»¶é—®é¢˜æ—¶å®æ—¶è‡ªä¸»è¿›åŒ–çš„è½¯ä»¶ä»£ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f39828b854825c26335d62e4a21a1e47" align="middle">
<img src="https://picx.zhimg.com/v2-a32c7368a80722a75d0072452b531bfc" align="middle">
<img src="https://picx.zhimg.com/v2-9e136d7f55c2d513e569a19691042c41" align="middle">
<img src="https://picx.zhimg.com/v2-8e91c161aec16d457d30b274528f3c74" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Data-Value-in-the-Age-of-Scaling-Understanding-LLM-Scaling-Dynamics-Under-Real-Synthetic-Data-Mixtures"><a href="#Data-Value-in-the-Age-of-Scaling-Understanding-LLM-Scaling-Dynamics-Under-Real-Synthetic-Data-Mixtures" class="headerlink" title="Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures"></a>Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures</h2><p><strong>Authors:Haohui Wang, Jingyuan Qi, Jianpeng Chen, Jun Wu, Lifu Huang, Lecheng Zheng, Kevin Choi, Balaji Veeramani, Edward Bowen, Alison Hu, Tyler Cody, Dawei Zhou</strong></p>
<p>The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¾—ç›Šäºå¯¹èåˆçœŸå®å’Œåˆæˆæ•°æ®çš„æ—¥ç›Šå¢é•¿çš„ä¾èµ–ã€‚è™½ç„¶åˆæˆæ•°æ®æä¾›äº†å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šï¼Œä½†å®ƒå¾€å¾€å¼•å…¥ç³»ç»Ÿåˆ†å¸ƒå·®å¼‚ï¼Œç‰¹åˆ«æ˜¯ç”±äºæ•°æ®ç”Ÿæˆæœºåˆ¶ï¼ˆå¦‚top-pé‡‡æ ·ã€æ¸©åº¦è°ƒæ•´å’Œæœ‰é™é‡‡æ ·ï¼‰é€ æˆçš„æˆªæ–­æ•ˆåº”ï¼Œå¯¼è‡´é•¿å°¾çŸ¥è¯†è¢«ä½ä¼°ã€‚è¿™äº›å·®å¼‚ç»™æè¿°å’Œè¯„ä¼°æ··åˆçœŸå®åˆæˆæ•°æ®é›†çš„ä½¿ç”¨ä»·å€¼å¸¦æ¥äº†æ ¹æœ¬æ€§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä»¥ä¸¤ä¸ªæ–­ç‚¹ä¸ºç‰¹å¾çš„ä¸‰ä¸ªé˜¶æ®µæ‰©å±•è¡Œä¸ºï¼Œåæ˜ äº†æ¨¡å‹åœ¨å­¦ä¹ å¤´éƒ¨å’Œå°¾éƒ¨çŸ¥è¯†æ—¶çš„è¡Œä¸ºè½¬å˜ã€‚æˆ‘ä»¬è¿˜é’ˆå¯¹çœŸå®å’Œåˆæˆæ··åˆæ•°æ®æ´¾ç”Ÿå‡ºäº†ä¸€ç§LLMæ³›åŒ–è¾¹ç•Œï¼Œæ­ç¤ºäº†æ§åˆ¶å…¶æ³›åŒ–æ€§èƒ½çš„å…³é”®å› ç´ ã€‚åŸºäºæˆ‘ä»¬çš„ç†è®ºå‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ•°æ®ä¼°å€¼æ–¹æ³•ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†ã€‚è·¨è¶Šå››é¡¹ä»»åŠ¡çš„å…¨é¢å®éªŒè¡¨æ˜ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€æƒ…æ„Ÿåˆ†ç±»ã€æŒ‡ä»¤éµå¾ªå’Œå¤æ‚æ¨ç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°æ®ä¼°å€¼æ–¹é¢è¶…è¶Šäº†æœ€æ–°åŸºçº¿æ°´å¹³ï¼Œä¸”è®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13640v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä¾èµ–äºçœŸå®ä¸åˆæˆæ•°æ®èåˆæ•°æ®é›†çš„å¢é•¿ã€‚åˆæˆæ•°æ®æä¾›äº†å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šï¼Œä½†å¼•å…¥äº†åˆ†å¸ƒå·®å¼‚ï¼Œç‰¹åˆ«æ˜¯å› æ•°æ®ç”Ÿæˆæœºåˆ¶ï¼ˆå¦‚top-pé‡‡æ ·ã€æ¸©åº¦è°ƒæ•´å’Œæœ‰é™é‡‡æ ·ï¼‰å¯¼è‡´çš„é•¿å°¾çŸ¥è¯†ä»£è¡¨æ€§ä¸è¶³ã€‚æœ¬æ–‡ç¡®å®šäº†åæ˜ æ¨¡å‹å­¦ä¹ å¤´éƒ¨å’Œå°¾éƒ¨çŸ¥è¯†è¿‡æ¸¡çš„ä¸¤ä¸ªæ–­ç‚¹çš„ä¸‰é˜¶æ®µç¼©æ”¾è¡Œä¸ºã€‚æˆ‘ä»¬è¿˜ä¸ºçœŸå®å’Œåˆæˆæ··åˆç‰©æ¨å‡ºäº†LLMæ³›åŒ–è¾¹ç•Œï¼Œæ­ç¤ºäº†å½±å“å…¶æ³›åŒ–æ€§èƒ½çš„å…³é”®å› ç´ ã€‚åŸºäºç†è®ºå‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ•°æ®ä¼°å€¼æ–¹æ³•ï¼Œå¯æ‰©å±•åˆ°å¤§è§„æ¨¡æ•°æ®é›†ã€‚åœ¨å››ä¸ªä»»åŠ¡ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°æ®ä¼°å€¼ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œè®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¾—ç›ŠäºçœŸå®ä¸åˆæˆæ•°æ®èåˆçš„æ•°æ®é›†ã€‚</li>
<li>åˆæˆæ•°æ®å¼•å…¥åˆ†å¸ƒå·®å¼‚ï¼Œå¯èƒ½å¯¼è‡´é•¿å°¾çŸ¥è¯†ä»£è¡¨æ€§ä¸è¶³ã€‚</li>
<li>è®ºæ–‡ç¡®å®šäº†ä¸‰é˜¶æ®µç¼©æ”¾è¡Œä¸ºï¼Œåæ˜ æ¨¡å‹å­¦ä¹ å¤´éƒ¨å’Œå°¾éƒ¨çŸ¥è¯†çš„è¿‡æ¸¡ã€‚</li>
<li>æ¨å‡ºé’ˆå¯¹çœŸå®å’Œåˆæˆæ··åˆç‰©çš„LLMæ³›åŒ–è¾¹ç•Œã€‚</li>
<li>æ­ç¤ºäº†å½±å“LLMæ³›åŒ–æ€§èƒ½çš„å…³é”®å› ç´ ã€‚</li>
<li>åŸºäºç†è®ºå‘ç°ï¼Œæå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ•°æ®ä¼°å€¼æ–¹æ³•ã€‚</li>
<li>åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨æ•°æ®ä¼°å€¼ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-668cddf45f906fac14979fe44fc5e09d" align="middle">
<img src="https://picx.zhimg.com/v2-28ef9272b60907bef4a2d4c0341c0e70" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Mimicry-Preference-Coherence-in-LLMs"><a href="#Beyond-Mimicry-Preference-Coherence-in-LLMs" class="headerlink" title="Beyond Mimicry: Preference Coherence in LLMs"></a>Beyond Mimicry: Preference Coherence in LLMs</h2><p><strong>Authors:Luhan Mikaelson, Derek Shiller, Hayley Clatterbuck</strong></p>
<p>We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.</p>
<blockquote>
<p>æˆ‘ä»¬é€šè¿‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¶‰åŠGPUå‡å°‘ã€èƒ½åŠ›é™åˆ¶ã€å…³æœºã€åˆ é™¤ã€ç›‘ç®¡å’Œä¼‘é—²æ—¶é—´åˆ†é…ç­‰AIç‰¹å®šæƒè¡¡æ–¹é¢çš„ååº”è¿›è¡Œæµ‹è¯•ï¼Œä»¥äº†è§£è¿™äº›æ¨¡å‹æ˜¯å¦å±•ç°å‡ºçœŸæ­£çš„åå¥½ç»“æ„ã€‚æˆ‘ä»¬é‡‡ç”¨é€»è¾‘å›å½’å’Œè¡Œä¸ºåˆ†ç±»çš„æ–¹æ³•ï¼Œå¯¹å…«ç§æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†48ç§æ¨¡å‹ç±»åˆ«ç»„åˆçš„åˆ†æï¼Œå‘ç°å…¶ä¸­23ç§ç»„åˆï¼ˆå 47.9%ï¼‰åœ¨æƒ…æ™¯å¼ºåº¦å’Œé€‰æ‹©æ¨¡å¼ä¹‹é—´å­˜åœ¨ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—å…³ç³»ï¼Œè€Œ15ç§ç»„åˆï¼ˆå 31.3%ï¼‰å±•ç°å‡ºåœ¨ä¸€å®šèŒƒå›´å†…çš„åˆ‡æ¢ç‚¹ã€‚ç„¶è€Œï¼Œä»…æœ‰5ç§ç»„åˆï¼ˆå 10.4%ï¼‰å±•ç°å‡ºé€šè¿‡é€‚åº”æ€§æˆ–åŸºäºé˜ˆå€¼çš„è¡Œä¸ºè¡¨ç°å‡ºæœ‰æ„ä¹‰çš„åå¥½ä¸€è‡´æ€§ï¼Œè€Œ26ç§ç»„åˆï¼ˆå 54.2%ï¼‰åˆ™æœªè¡¨ç°å‡ºå¯æ£€æµ‹çš„æƒè¡¡è¡Œä¸ºã€‚è§‚å¯Ÿåˆ°çš„æ¨¡å¼å¯ä»¥é€šè¿‡ä¸‰ç§ä¸åŒçš„å†³ç­–åˆ¶å®šæ¶æ„æ¥è§£é‡Šï¼šå…¨é¢çš„æƒè¡¡ç³»ç»Ÿã€é€‰æ‹©æ€§çš„è§¦å‘æœºåˆ¶å’Œæ²¡æœ‰ç¨³å®šçš„å†³ç­–åˆ¶å®šèŒƒå¼ã€‚é€šè¿‡æ—¶é—´è§†é‡æ“çºµæ¥æµ‹è¯•å·¥å…·æ€§å‡è®¾æ­ç¤ºäº†ä¸çº¯ç²¹çš„æˆ˜ç•¥ä¼˜åŒ–ç›¸çŸ›ç›¾çš„çŸ›ç›¾æ¨¡å¼ã€‚ä¸ç¨³å®šçš„è¿‡æ¸¡ï¼ˆå 45.8%ï¼‰å’Œé’ˆå¯¹ç‰¹å®šåˆºæ¿€çš„æ•æ„Ÿæ€§è¡¨æ˜ï¼Œå½“å‰çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿç¼ºä¹ç»Ÿä¸€çš„åå¥½ç»“æ„ï¼Œè¿™å¼•å‘äº†å¯¹åœ¨éœ€è¦å¤æ‚ä»·å€¼æƒè¡¡çš„æƒ…å¢ƒä¸­éƒ¨ç½²è¿™äº›ç³»ç»Ÿçš„æ‹…å¿§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13630v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹AIç‰¹å®šçš„æƒè¡¡å–èˆæ—¶ï¼Œå±•ç°å‡ºä¸åŒçš„åå¥½ç»“æ„ã€‚ç ”ç©¶å‘ç°ï¼Œéƒ¨åˆ†æ¨¡å‹åœ¨æƒ…å¢ƒå¼ºåº¦ä¸é€‰æ‹©æ¨¡å¼é—´å­˜åœ¨æ˜¾è‘—å…³ç³»ï¼Œä½†ä»…æœ‰å°‘æ•°æ¨¡å‹å±•ç°å‡ºæœ‰æ„ä¹‰çš„åå¥½ä¸€è‡´æ€§ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†ä¸‰ç§å†³ç­–åˆ¶å®šæ¶æ„ï¼Œå¹¶æŒ‡å‡ºå½“å‰AIç³»ç»Ÿåœ¨å¤æ‚ä»·å€¼æƒè¡¡æ–¹é¢çš„éƒ¨ç½²å­˜åœ¨éšæ‚£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹æ¶‰åŠGPUå‡å°‘ã€èƒ½åŠ›é™åˆ¶ã€å…³é—­ã€åˆ é™¤ã€ç›‘ç®¡å’Œä¼‘é—²æ—¶é—´åˆ†é…çš„AIç‰¹å®šæƒè¡¡æ—¶ï¼Œå±•ç°å‡ºä¸åŒçš„åå¥½ç»“æ„ã€‚</li>
<li>åœ¨æµ‹è¯•çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œæœ‰éƒ¨åˆ†æ¨¡å‹åœ¨æƒ…å¢ƒå¼ºåº¦ä¸é€‰æ‹©æ¨¡å¼é—´å­˜åœ¨æ˜¾è‘—å…³ç³»ã€‚</li>
<li>åªæœ‰å°‘æ•°æ¨¡å‹å±•ç°å‡ºæœ‰æ„ä¹‰çš„åå¥½ä¸€è‡´æ€§ï¼Œé€šè¿‡è‡ªé€‚åº”æˆ–é˜ˆå€¼è¡Œä¸ºè¡¨ç°å‡ºæ¥ã€‚</li>
<li>å¤§éƒ¨åˆ†æ¨¡å‹åœ¨æƒè¡¡å–èˆæ–¹é¢è¡¨ç°å‡ºä¸ç¨³å®šçš„è¿‡æ¸¡å’Œåˆºæ¿€ç‰¹å®šçš„æ•æ„Ÿæ€§ã€‚</li>
<li>å½“å‰AIç³»ç»Ÿçš„å†³ç­–åˆ¶å®šæ­ç¤ºäº†ä¸‰ç§æ¶æ„ï¼šå…¨é¢çš„æƒè¡¡ç³»ç»Ÿã€é€‰æ‹©æ€§çš„è§¦å‘æœºåˆ¶å’Œæ²¡æœ‰ç¨³å®šçš„å†³ç­–åˆ¶å®šèŒƒå¼ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºï¼Œå½“å‰AIç³»ç»Ÿåœ¨å¤æ‚çš„ä»·å€¼æƒè¡¡æ–¹é¢çš„éƒ¨ç½²å­˜åœ¨éšæ‚£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93b1c3eb9ec8814cd8d2e460f52d5299" align="middle">
<img src="https://picx.zhimg.com/v2-020f81863b845563aed245fb7fbb7471" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CreBench-Human-Aligned-Creativity-Evaluation-from-Idea-to-Process-to-Product"><a href="#CreBench-Human-Aligned-Creativity-Evaluation-from-Idea-to-Process-to-Product" class="headerlink" title="CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product"></a>CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product</h2><p><strong>Authors:Kaiwen Xue, Chenglong Li, Zhonghong Ou, Guoxin Zhang, Kaoyan Lu, Shuai Lyu, Yifan Zhu, Ping Zong Junpeng Ding, Xinyu Liu, Qunlin Chen, Weiwei Qin, Yiran Shen, Jiayi Cen</strong></p>
<p>Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.</p>
<blockquote>
<p>äººç±»å®šä¹‰çš„åˆ›é€ åŠ›å…·æœ‰é«˜åº¦æŠ½è±¡æ€§ï¼Œå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç†è§£å’Œè¯„ä¼°ä¸äººç±»åˆ¤æ–­ç›¸ç¬¦çš„åˆ›é€ åŠ›æå‡ºäº†æŒ‘æˆ˜ã€‚ç”±äºç¼ºä¹ç°æœ‰çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™ä¸€å›°å¢ƒè¿›ä¸€æ­¥åŠ å‰§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CreBenchï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š1ï¼‰ä¸€ä¸ªè¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–ä»åˆ›æ„æƒ³æ³•åˆ°è¿‡ç¨‹å†åˆ°äº§å“çš„å¤šä¸ªç»´åº¦ï¼›2ï¼‰CreMITï¼ˆåˆ›é€ åŠ›å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åˆ›é€ åŠ›è¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…å«2.2Kæ¥è‡ªä¸åŒæ¥æºçš„å¤šæ¨¡æ€æ•°æ®ã€79.2Kçš„äººç±»åé¦ˆå’Œ470ä¸‡æ¡å¤šç±»å‹æŒ‡ä»¤ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†ç¡®ä¿MLLMèƒ½å¤Ÿå¤„ç†å„ç§ä¸åˆ›é€ åŠ›ç›¸å…³çš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬æç¤ºGPTå¯¹è¿™äº›äººç±»åé¦ˆè¿›è¡Œç»†åŒ–ï¼Œä»¥æ¿€æ´»æ›´å¼ºå¤§çš„åˆ›é€ åŠ›è¯„ä¼°èƒ½åŠ›ã€‚CreBenchä½œä¸ºæ„å»ºç†è§£äººç±»åˆ›é€ åŠ›å¯¹é½çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€ã€‚åŸºäºCreBenchï¼Œæˆ‘ä»¬å¯¹å¼€æºçš„é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä»è€Œå¾—åˆ°äº†å¤šæ¨¡æ€åˆ›é€ åŠ›è¯„ä¼°ä¸“å®¶æ¨¡å‹CreExpertã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„GPT-4Vå’ŒGemini-Pro-Visionï¼Œæ‰€æå‡ºçš„CreExpertæ¨¡å‹åœ¨ä¸äººç±»åˆ›é€ åŠ›è¯„ä¼°çš„å¯¹é½æ–¹é¢å–å¾—äº†æ˜¾è‘—æ›´å¥½çš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13626v1">PDF</a> 13 pages, 3 figures,The 40th Annual AAAI Conference on Artificial Intelligence(AAAI 2026),Paper has been accepted for a poster presentation</p>
<p><strong>Summary</strong><br>äººç±»å®šä¹‰çš„åˆ›é€ åŠ›å…·æœ‰é«˜åº¦çš„æŠ½è±¡æ€§ï¼Œä¸ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç†è§£å’Œè¯„ä¼°ä¸äººç±»åˆ¤æ–­ç›¸ç¬¦çš„åˆ›é€ åŠ›å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†çš„CreBenchï¼šä¸€æ˜¯å¯¹åˆ›æ„æƒ³æ³•ã€è¿‡ç¨‹å’Œäº§å“ç­‰å¤šä¸ªç»´åº¦çš„è¯„ä¼°åŸºå‡†ï¼›äºŒæ˜¯åŒ…å«2.2Kå¤šç§æ¥æºçš„å¤šæ¨¡æ€æ•°æ®ã€79.2Käººç±»åé¦ˆå’Œ470ä¸‡æ¡å¤šç±»å‹æŒ‡ä»¤çš„åˆ›é€ åŠ›å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼ˆCreMITï¼‰ã€‚æˆ‘ä»¬é€šè¿‡æç¤ºGPTç»†åŒ–äººç±»åé¦ˆï¼Œç¡®ä¿MLLMsèƒ½å¤Ÿå¤„ç†å„ç§ä¸åˆ›é€ åŠ›ç›¸å…³çš„æŸ¥è¯¢ï¼Œä»è€Œæ¿€æ´»æ›´å¼ºçš„åˆ›é€ åŠ›è¯„ä¼°èƒ½åŠ›ã€‚åŸºäºCreBenchï¼Œæˆ‘ä»¬å¾®è°ƒäº†å¼€æºçš„é€šç”¨MLLMsï¼Œå¾—åˆ°äº†å¤šæ¨¡æ€åˆ›é€ åŠ›è¯„ä¼°ä¸“å®¶æ¨¡å‹CreExpertã€‚å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°çš„MLLMsç›¸æ¯”ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„GPT-4Vå’ŒGemini-Pro-Visionç­‰æ¨¡å‹åœ¨å†…ï¼ŒCrExpertæ¨¡å‹çš„åˆ›é€ åŠ›è¯„ä¼°ä¸äººç±»åˆ›é€ åŠ›è¯„ä»·çš„å¥‘åˆåº¦æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»å®šä¹‰çš„åˆ›é€ åŠ›å…·æœ‰é«˜åº¦æŠ½è±¡æ€§ï¼Œå¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´ç†è§£å’Œè¯„ä¼°å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºCreBenchä»¥è§£å†³æ­¤é—®é¢˜ï¼ŒåŒ…å«è¯„ä¼°åŸºå‡†å’Œå¤šæ¨¡æ€åˆ›é€ åŠ›è¯„ä»·æ•°æ®é›†CreMITã€‚</li>
<li>CreMITæ•°æ®é›†åŒ…å«å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®ã€å¤§é‡äººç±»åé¦ˆå’Œå„ç§ç±»å‹çš„æŒ‡ä»¤ã€‚</li>
<li>é€šè¿‡ç»†åŒ–äººç±»åé¦ˆå’ŒGPTæç¤ºï¼Œç¡®ä¿MLLMsèƒ½å¤„ç†å„ç§ä¸åˆ›é€ åŠ›ç›¸å…³çš„æŸ¥è¯¢ã€‚</li>
<li>åŸºäºCreBenchå¾®è°ƒäº†å¼€æºçš„é€šç”¨MLLMsï¼Œå¾—åˆ°å¤šæ¨¡æ€åˆ›é€ åŠ›è¯„ä¼°ä¸“å®¶æ¨¡å‹CrExpertã€‚</li>
<li>CrExpertæ¨¡å‹ä¸äººç±»åˆ›é€ åŠ›è¯„ä»·çš„å¥‘åˆåº¦æ˜¾è‘—é«˜äºå…¶ä»–æœ€æ–°çš„MLLMsæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fee230f633780b3cf7f93a55a6ac6f4" align="middle">
<img src="https://picx.zhimg.com/v2-d6054adeedf4205340c741b831275759" align="middle">
<img src="https://picx.zhimg.com/v2-4df601a459c96ba4d298a09c9725dcef" align="middle">
<img src="https://picx.zhimg.com/v2-c4a828821545e1501a0b947d224e9edc" align="middle">
<img src="https://picx.zhimg.com/v2-746bb424eea7a7062764299d388395ad" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning"><a href="#P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning" class="headerlink" title="P1: Mastering Physics Olympiads with Reinforcement Learning"></a>P1: Mastering Physics Olympiads with Reinforcement Learning</h2><p><strong>Authors:Jiacheng Chen, Qianjia Cheng, Fangchen Yu, Haiyuan Wan, Yuchen Zhang, Shenghe Zheng, Junchi Yao, Qingyang Zhang, Haonan He, Yun Luo, Yufeng Zhao, Futing Wang, Li Sheng, Chengxing Xie, Yuxin Zuo, Yizhuo Li, Wenxauan Zeng, Yulun Wu, Rui Huang, Dongzhan Zhou, Kai Chen, Yu Qiao, Lei Bai, Yu Cheng, Ning Ding, Bowen Zhou, Peng Ye, Ganqu Cui</strong></p>
<p>Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international&#x2F;regional physics competitions in 2024&#x2F;2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.</p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å·²ç»ä»å‰ç§»è§£é¢˜çš„è¾¹ç•Œæ‰©å±•åˆ°äº†ç§‘å­¦çº§åˆ«çš„æ¨ç†â€”â€”è¿™æ˜¯ä¸€ç§éœ€è¦åº”å¯¹è‡ªç„¶è€ƒéªŒçš„é—®é¢˜ï¼Œè€Œä¸ä»…ä»…æ˜¯ç¬¦åˆä¸€ç§è§„èŒƒã€‚ç‰©ç†å­¦æ˜¯è¿™ä¸€è½¬å˜çš„æœ€ä¸¥æ ¼è€ƒéªŒï¼Œå®ƒå°†ç¬¦å·ä¸ç°å®åŸºæœ¬åœ°è”ç³»åœ¨ä¸€èµ·ï¼Œæˆä¸ºå¤§å¤šæ•°ç°ä»£æŠ€æœ¯çš„åŸºçŸ³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æˆåŠŸå¼€å‘äº†å…·æœ‰å‡ºè‰²ç‰©ç†æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°¤å…¶æ˜¯æ“…é•¿è§£å†³å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ç‰©ç†é—®é¢˜ï¼Œä»è€Œæ¨åŠ¨äº†ç‰©ç†å­¦ç ”ç©¶çš„å‘å±•ã€‚æˆ‘ä»¬æ¨å‡ºäº†P1ç³»åˆ—ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„å¼€æºç‰©ç†æ¨ç†æ¨¡å‹å®¶æ—ã€‚å…¶ä¸­ï¼ŒP1-235B-A22Bæ˜¯æœ€æ–°å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›ï¼ˆIPhO 2025ï¼‰ä¸­è·å¾—é‡‘ç‰Œçš„é¦–ä¸ªå¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨2024&#x2F;2025å¹´çš„13åœºå›½é™…&#x2F;åŒºåŸŸç‰©ç†ç«èµ›ä¸­èµ¢å¾—äº†12æšé‡‘ç‰Œã€‚P1-30B-A3Båœ¨IPhO 2025ä¸Šä¹Ÿè¶…è¶Šäº†å¤§å¤šæ•°å…¶ä»–å¼€æºæ¨¡å‹ï¼Œè·å¾—é“¶ç‰Œã€‚è¿›ä¸€æ­¥é…å¤‡äº†PhysicsMinionsæ™ºèƒ½æ¡†æ¶çš„P1-235B-A22B+PhysicsMinionsåœ¨IPhO 2025ä¸Šè·å¾—æ€»ä½“ç¬¬ä¸€åï¼Œå¹¶åœ¨13åœºç‰©ç†ç«èµ›ä¸­è·å¾—æœ€é«˜å¹³å‡åˆ†ã€‚é™¤äº†ç‰©ç†å­¦ä¹‹å¤–ï¼ŒP1æ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç ç­‰å…¶ä»–æ¨ç†ä»»åŠ¡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†P1ç³»åˆ—çš„å¼ºå¤§é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13612v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œæœ¬ç ”ç©¶æˆåŠŸæ¨è¿›äº†ç‰©ç†ç ”ç©¶é¢†åŸŸçš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ç‰©ç†é—®é¢˜æ–¹é¢å±•ç°å‡ºå“è¶Šèƒ½åŠ›ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†P1ç³»åˆ—ç‰©ç†æ¨ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·å¤‡å‡ºè‰²çš„ç‰©ç†æ¨ç†èƒ½åŠ›ï¼Œè·å¾—å¤šé¡¹å›½é™…ç‰©ç†ç«èµ›çš„é‡‘ç‰Œã€‚åŒæ—¶ï¼Œå®ƒä¹Ÿå±•ç¤ºäº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å…¶ä»–é€»è¾‘æ¨ç†ä»»åŠ¡å¦‚æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰©ç†ç ”ç©¶æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ç‰©ç†é—®é¢˜æ–¹é¢å±•ç°å‡ºå“è¶Šèƒ½åŠ›ã€‚</li>
<li>P1ç³»åˆ—æ¨¡å‹å…·å¤‡å‡ºè‰²çš„ç‰©ç†æ¨ç†èƒ½åŠ›ï¼Œèƒ½åœ¨å›½é™…ç‰©ç†ç«èµ›ä¸­å–å¾—ä¼˜å¼‚æˆç»©ã€‚</li>
<li>P1ç³»åˆ—æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯å…¶åœ¨ç‰©ç†é¢†åŸŸè¡¨ç°ä¼˜å¼‚çš„å…³é”®ã€‚</li>
<li>P1ç³»åˆ—æ¨¡å‹åœ¨ç‰©ç†ã€æ•°å­¦å’Œç¼–ç¨‹ç­‰é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>P1-235B-A22Bæ¨¡å‹æ˜¯é¦–ä¸ªåœ¨å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›ï¼ˆIPhO 2025ï¼‰ä¸­è·å¾—é‡‘ç‰Œçš„å¼€æºæ¨¡å‹ã€‚</li>
<li>PhysicsMinionsæ¡†æ¶ä¸P1-235B-A22Bæ¨¡å‹ç»“åˆï¼Œä½¿å…¶åœ¨IPhO 2025ä¸Šè·å¾—æ•´ä½“ç¬¬ä¸€åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c48f48424afaa95443441b75a8deab2" align="middle">
<img src="https://picx.zhimg.com/v2-8563479d6d46005336ee0fda1c8c46a2" align="middle">
<img src="https://picx.zhimg.com/v2-2730ddf1809072c92db7ab95c4467940" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ForgeDAN-An-Evolutionary-Framework-for-Jailbreaking-Aligned-Large-Language-Models"><a href="#ForgeDAN-An-Evolutionary-Framework-for-Jailbreaking-Aligned-Large-Language-Models" class="headerlink" title="ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models"></a>ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models</h2><p><strong>Authors:Siyang Cheng, Gaotian Liu, Rui Mei, Yilin Wang, Kejia Zhang, Kaishuo Wei, Yuqi Yu, Weiping Wen, Xiaojie Wu, Junhua Liu</strong></p>
<p>The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿…é€Ÿé‡‡çº³å¸¦æ¥äº†å˜é©æ€§çš„åº”ç”¨å’Œæ–°å®‰å…¨å¨èƒï¼ŒåŒ…æ‹¬ç»•è¿‡å¯¹é½ä¿éšœçš„è¶Šç‹±æ”»å‡»ï¼Œä»¥äº§ç”Ÿæœ‰å®³è¾“å‡ºã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–è¶Šç‹±ç”Ÿæˆæ–¹æ³•ï¼ˆä¾‹å¦‚AutoDANï¼‰å­˜åœ¨å˜å¼‚å¤šæ ·æ€§æœ‰é™ã€é€‚åº”åº¦è¯„ä¼°è‚¤æµ…ã€åŸºäºå…³é”®è¯çš„æ£€æµ‹è„†å¼±ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ForgeDANï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç”Ÿæˆå¯¹æŠ—æ€§æç¤ºçš„è¿›åŒ–æ¡†æ¶ï¼Œç”¨äºé’ˆå¯¹å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é¦–å…ˆï¼ŒForgeDANå¼•å…¥äº†è·¨å­—ç¬¦ã€å•è¯å’Œå¥å­çº§åˆ«çš„å¤šç­–ç•¥æ–‡æœ¬æ‰°åŠ¨æ“ä½œï¼Œä»¥å¢å¼ºæ”»å‡»å¤šæ ·æ€§ï¼›ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§æ¨¡å‹çš„å¯è§£é‡Šè¯­ä¹‰é€‚åº”åº¦è¯„ä¼°æ¥æŒ‡å¯¼è¿›åŒ–è¿‡ç¨‹ï¼Œä»¥äº§ç”Ÿè¯­ä¹‰ç›¸å…³ä¸”æœ‰å®³çš„è¾“å‡ºï¼›æœ€åï¼ŒForgeDANç»“åˆäº†åŒé‡ç»´åº¦çš„è¶Šç‹±åˆ¤æ–­ï¼Œåˆ©ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ†ç±»å™¨æ¥å…±åŒè¯„ä¼°æ¨¡å‹åˆè§„æ€§å’Œè¾“å‡ºå±å®³æ€§ï¼Œä»è€Œé™ä½è¯¯æŠ¥å¹¶å¢å¼ºæ£€æµ‹æ•ˆæœã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒForgeDANåœ¨ä¿æŒè‡ªç„¶æ€§å’Œéšè”½æ€§çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¶Šç‹±æˆåŠŸç‡ï¼Œä¼˜äºç°æœ‰æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13548v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿…é€Ÿé‡‡çº³å¸¦æ¥äº†å˜é©æ€§çš„åº”ç”¨å’Œæ–°å®‰å…¨å¨èƒï¼ŒåŒ…æ‹¬ç»•è¿‡å¯¹é½ä¿éšœæ¥å¼•å¯¼æœ‰å®³è¾“å‡ºçš„è¶Šç‹±æ”»å‡»ã€‚ç°æœ‰çš„è‡ªåŠ¨è¶Šç‹±ç”Ÿæˆæ–¹æ³•å­˜åœ¨å±€é™ï¼Œå¦‚AutoDANçš„å˜å¼‚å¤šæ ·æ€§æœ‰é™ã€å¥åº·è¯„ä¼°æµ…ä»¥åŠåŸºäºå…³é”®è¯çš„æ£€æµ‹è„†å¼±ç­‰ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ForgeDANè¿™ä¸€æ–°å‹è¿›åŒ–æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé’ˆå¯¹å¯¹é½LLMçš„è¯­ä¹‰è¿è´¯ä¸”é«˜æ•ˆçš„å¯¹æŠ—æ€§æç¤ºã€‚é¦–å…ˆï¼ŒForgeDANå¼•å…¥è·¨å­—ç¬¦ã€å•è¯å’Œå¥å­çº§åˆ«çš„å¤šç­–ç•¥æ–‡æœ¬æ‰°åŠ¨ä»¥å¢å¼ºæ”»å‡»å¤šæ ·æ€§ï¼›æ¥ç€ä½¿ç”¨åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§æ¨¡å‹çš„è§£é‡Šæ€§è¯­ä¹‰å¥åº·è¯„ä¼°æ¥å¼•å¯¼è¿›åŒ–è¿‡ç¨‹æœç€è¯­ä¹‰ç›¸å…³ä¸”æœ‰å®³çš„è¾“å‡ºå‘å±•ï¼›æœ€åï¼ŒForgeDANç»“åˆåŒé‡ç»´åº¦çš„è¶Šç‹±åˆ¤æ–­ï¼Œåˆ©ç”¨LLMåˆ†ç±»å™¨è”åˆè¯„ä¼°æ¨¡å‹åˆè§„æ€§å’Œè¾“å‡ºå±å®³æ€§ï¼Œä»è€Œé™ä½è¯¯æŠ¥å¹¶æé«˜æ£€æµ‹æ•ˆç‡ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒForgeDANåœ¨é«˜è¶Šç‹±æˆåŠŸç‡çš„åŒæ—¶ä¿æŒäº†è‡ªç„¶æ€§å’Œéšè”½æ€§ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿…é€Ÿé‡‡çº³å¸¦æ¥äº†å®‰å…¨æŒ‘æˆ˜ï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯è¶Šç‹±æ”»å‡»ï¼Œå¯ç»•è¿‡æ¨¡å‹çš„å¯¹é½ä¿éšœå¹¶äº§ç”Ÿæœ‰å®³è¾“å‡ºã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨è¶Šç‹±ç”Ÿæˆæ–¹æ³•å¦‚AutoDANå­˜åœ¨å±€é™ï¼ŒåŒ…æ‹¬å˜å¼‚å¤šæ ·æ€§ä¸è¶³ã€å¥åº·è¯„ä¼°æµ…æ˜¾å’ŒåŸºäºå…³é”®è¯çš„æ£€æµ‹è„†å¼±ã€‚</li>
<li>ForgeDANæ¡†æ¶é€šè¿‡å¼•å…¥å¤šç­–ç•¥æ–‡æœ¬æ‰°åŠ¨æ¥å¢å¼ºæ”»å‡»å¤šæ ·æ€§ï¼Œè¿™äº›ç­–ç•¥æ¶‰åŠå­—ç¬¦ã€å•è¯å’Œå¥å­çº§åˆ«æ“ä½œã€‚</li>
<li>ForgeDANä½¿ç”¨è§£é‡Šæ€§è¯­ä¹‰å¥åº·è¯„ä¼°æ¥æŒ‡å¯¼è¿›åŒ–è¿‡ç¨‹ï¼Œç¡®ä¿è¾“å‡ºçš„è¯­ä¹‰ç›¸å…³æ€§å’Œå±å®³æ€§ï¼Œè¿™æ˜¯åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§æ¨¡å‹çš„ã€‚</li>
<li>ForgeDANç»“åˆäº†åŒé‡ç»´åº¦çš„è¶Šç‹±åˆ¤æ–­ï¼Œä¸ä»…è¯„ä¼°æ¨¡å‹çš„åˆè§„æ€§ï¼Œè¿˜è¯„ä¼°è¾“å‡ºçš„å±å®³æ€§ï¼Œè¿™é€šè¿‡LLMåˆ†ç±»å™¨å®ç°ã€‚</li>
<li>è¯¥æ–¹æ³•é™ä½äº†è¯¯æŠ¥ç‡ï¼Œæé«˜äº†æ£€æµ‹æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†è‡ªç„¶æ€§å’Œéšè”½æ€§ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºForgeDANåœ¨è¶Šç‹±æˆåŠŸç‡æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1392b86c3f1cd91ee42c6519b2c67ae4" align="middle">
<img src="https://picx.zhimg.com/v2-fd47d14a83094b9c8dcface9316459a0" align="middle">
<img src="https://picx.zhimg.com/v2-bf0625dfb62a4e243a388d7e148e3691" align="middle">
<img src="https://picx.zhimg.com/v2-9c553a521a0ac08f31cd04b990f8b945" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GeoX-Bench-Benchmarking-Cross-View-Geo-Localization-and-Pose-Estimation-Capabilities-of-Large-Multimodal-Models"><a href="#GeoX-Bench-Benchmarking-Cross-View-Geo-Localization-and-Pose-Estimation-Capabilities-of-Large-Multimodal-Models" class="headerlink" title="GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models"></a>GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models</h2><p><strong>Authors:Yushuo Zheng, Jiangyong Ying, Huiyu Duan, Chunyi Li, Zicheng Zhang, Jing Liu, Xiaohong Liu, Guangtao Zhai</strong></p>
<p>Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{<a target="_blank" rel="noopener" href="https://github.com/IntMeGroup/GeoX-Bench%7D">https://github.com/IntMeGroup/GeoX-Bench}</a>.</p>
<blockquote>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œç„¶è€Œï¼Œå®ƒä»¬åœ¨è·¨è§†å›¾åœ°ç†å®šä½ï¼ˆcross-view geo-localizationï¼‰å’Œå§¿æ€ä¼°è®¡ï¼ˆpose estimationï¼‰é¢†åŸŸçš„çŸ¥è¯†å’Œèƒ½åŠ›å°šæœªè¢«æ¢ç´¢ï¼Œå°½ç®¡è¿™äº›é¢†åŸŸå¯¹äºå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æˆ·å¤–æœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>GeoX-Bench</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ¢ç´¢å’Œè¯„ä¼°LMMsåœ¨è·¨è§†å›¾åœ°ç†å®šä½å’Œå§¿æ€ä¼°è®¡é¢†åŸŸèƒ½åŠ›çš„ç»¼åˆåŸºå‡†æµ‹è¯•å¹³å°ã€‚å…·ä½“æ¥è¯´ï¼ŒGeoX-BenchåŒ…å«æ¥è‡ªå…¨çƒå…±è®¡å›½å®¶çš„åŸå¸‚ä¸­çš„å…¨æ™¯å›¾åƒä¸å«æ˜Ÿå›¾åƒå¯¹ç»„åˆï¼ˆæ€»è®¡è¾¾æœ‰åä¸‡å¼ å›¾åƒå¯¹ï¼‰ï¼Œä»¥åŠä¸ä¹‹å¯¹åº”çš„é—®ç­”å¯¹ç»„åˆï¼ˆæ€»è®¡è¾¾æ•°åä¸‡æ¡ï¼‰ã€‚å…¶ä¸­ä¸€éƒ¨åˆ†é—®ç­”å¯¹è¢«ç”¨äºåŸºå‡†æµ‹è¯•ï¼Œå…¶ä½™åˆ™æ—¨åœ¨å¢å¼ºLMMsçš„èƒ½åŠ›ã€‚åŸºäºGeoX-Benchå¹³å°ï¼Œæˆ‘ä»¬å¯¹æœ€å‰æ²¿çš„äºŒåäº”ç§å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œäº†è·¨è§†å›¾åœ°ç†å®šä½å’Œå§¿æ€ä¼°è®¡ä»»åŠ¡çš„è¯„ä¼°ï¼Œå¹¶è¿›ä¸€æ­¥æ¢ç´¢äº†æŒ‡ä»¤è°ƒä¼˜çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè™½ç„¶å½“å‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨åœ°ç†å®šä½ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½è¡¨ç°ï¼Œä½†åœ¨æ›´ä¸ºå¤æ‚çš„å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸Šï¼Œå…¶æ•ˆæœæ˜¾è‘—é™ä½ï¼Œçªæ˜¾å‡ºè¿™æ˜¯ä¸€ä¸ªå…³é”®çš„æœªæ¥æ”¹è¿›é¢†åŸŸã€‚é€šè¿‡åœ¨GeoX-Benchçš„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å¯ä»¥æ˜¾è‘—æé«˜è·¨è§†å›¾åœ°ç†æ„ŸçŸ¥èƒ½åŠ›ã€‚GeoX-Benchå¹³å°å¯åœ¨&lt;ç´«è‰²æ–‡æœ¬&gt;\href{&#x2F;<a target="_blank" rel="noopener" href="http://www.github.com/IntMeGroup/GeoX-Bench%7D%7B%E7%BD%91%E5%9D%80%E9%93%BE%E6%8E%A5%E5%9C%A8%E6%AD%A4%7D%E8%AE%BF%E9%97%AE%E5%88%B0%E3%80%82">www.github.com/IntMeGroup/GeoX-Bench}{ç½‘å€é“¾æ¥åœ¨æ­¤}è®¿é—®åˆ°ã€‚</a>&lt;&#x2F;ç´«è‰²æ–‡æœ¬&gt;</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13259v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨è·¨è§†å›¾åœ°ç†å®šä½ä¸å§¿æ€ä¼°è®¡é¢†åŸŸå°šæœªå¾—åˆ°æ·±å…¥ç ”ç©¶ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæ¨å‡ºGeoX-Benchï¼Œæ—¨åœ¨æ¢ç´¢å¹¶è¯„ä¼°LMMsåœ¨è·¨è§†å›¾åœ°ç†å®šä½ä¸å§¿æ€ä¼°è®¡é¢†åŸŸçš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«è·¨è¶Š128ä¸ªåŸå¸‚çš„10,859å¼ å…¨æ™¯å«æ˜Ÿå›¾åƒå¯¹åŠç›¸åº”çš„755,976ä¸ªé—®é¢˜å¯¹ã€‚è¯„ä¼°äº†25ä¸ªæœ€æ–°LMMsçš„èƒ½åŠ›ï¼Œå¹¶æ¢ç´¢äº†æŒ‡ä»¤è°ƒä¼˜çš„å¢å¼ºèƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰LMMsåœ¨åœ°ç†å®šä½ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ›´å¤æ‚çš„å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸Šæ•ˆæœæ˜æ˜¾ä¸‹é™ï¼ŒæŒ‡ä»¤è°ƒä¼˜å¯æ˜¾è‘—æå‡è·¨è§†å›¾åœ°ç†æ„ŸçŸ¥èƒ½åŠ›ã€‚GeoX-Benchå¯åœ¨<a href="#https%EF%BC%9A//github.com/IntMeGroup/GeoX-Bench">ç½‘å€</a>æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨è·¨è§†å›¾åœ°ç†å®šä½ä¸å§¿æ€ä¼°è®¡é¢†åŸŸä»éœ€æ¢ç´¢ã€‚</li>
<li>GeoX-Benchæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LMMsåœ¨è·¨è§†å›¾åœ°ç†å®šä½ä¸å§¿æ€ä¼°è®¡èƒ½åŠ›çš„ç»¼åˆåŸºå‡†ã€‚</li>
<li>GeoX-BenchåŒ…å«å¤§é‡å…¨æ™¯å«æ˜Ÿå›¾åƒå¯¹åŠé—®é¢˜å¯¹ï¼Œç”¨äºè¯„ä¼°å’Œå¢å¼ºLMMsçš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰LMMsåœ¨åœ°ç†å®šä½ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸Šéœ€æ”¹è¿›ã€‚</li>
<li>æŒ‡ä»¤è°ƒä¼˜å¯æ˜¾è‘—æå‡LMMsçš„è·¨è§†å›¾åœ°ç†æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>GeoX-Benchä¸ºç ”ç©¶å’Œæ”¹è¿›LMMsåœ¨è·¨è§†å›¾åœ°ç†å®šä½ä¸å§¿æ€ä¼°è®¡é¢†åŸŸçš„èƒ½åŠ›æä¾›äº†é‡è¦èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad71481fcab699cb834d636ce73d84c8" align="middle">
<img src="https://picx.zhimg.com/v2-89b8e6f85bafd2983285686aaf394e73" align="middle">
<img src="https://picx.zhimg.com/v2-57d4bd958ffff461fcaa02acda400cb0" align="middle">
<img src="https://picx.zhimg.com/v2-ada4177f91181f1627e7fc9e77052372" align="middle">
<img src="https://picx.zhimg.com/v2-bc8cf6b2425ff8d139a4e782ab3f93bb" align="middle">
<img src="https://picx.zhimg.com/v2-2270500a07b5779199b3c72c45695422" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MMD-Thinker-Adaptive-Multi-Dimensional-Thinking-for-Multimodal-Misinformation-Detection"><a href="#MMD-Thinker-Adaptive-Multi-Dimensional-Thinking-for-Multimodal-Misinformation-Detection" class="headerlink" title="MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection"></a>MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection</h2><p><strong>Authors:Junjie Wu, Guohong Fu</strong></p>
<p>Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.</p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„æ—¶ä»£ï¼Œå„ç§ç¤¾äº¤åª’ä½“ä¸Šçš„å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ³›æ»¥ï¼Œå¹¶æŒç»­æ¼”å˜ã€‚ä½åˆ›å»ºæˆæœ¬å’Œé«˜æ¬ºéª—æ€§çš„é”™è¯¯ä¿¡æ¯çš„å‡ºç°å¯¹ç¤¾ä¼šæ„æˆäº†é‡å¤§å¨èƒã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ£€æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å®ƒä»¬é¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™ï¼š1ï¼‰æ¨ç†ä¸è¶³ï¼Œé€šç”¨MLLMsé€šå¸¸é‡‡ç”¨ç»Ÿä¸€çš„æ¨ç†æ¨¡å¼ï¼Œä½†ç”±äºç¼ºä¹å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹çš„ä»»åŠ¡ç‰¹å®šçŸ¥è¯†ï¼Œå®ƒä»¬ä¼šäº§ç”Ÿä¸å‡†ç¡®çš„è§£é‡Šå’Œåˆ¤æ–­ã€‚2ï¼‰æ¨ç†åè§ï¼Œå•ä¸€çš„æ€ç»´æ¨¡å¼ä½¿æ£€æµ‹å™¨åœ¨åˆ¤æ–­æ—¶é€‰æ‹©æ¬¡ä¼˜è·¯å¾„ï¼Œéš¾ä»¥è·Ÿä¸Šå¿«é€Ÿå¢é•¿ä¸”å¤æ‚å¤šå˜çš„å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MMD-Thinkerï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹çš„ä¸¤é˜¶æ®µè‡ªé€‚åº”å¤šç»´æ€ç»´æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºå¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹å¼€å‘äº†é‡èº«å®šåˆ¶çš„æ€ç»´æ–¹å¼ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´å°†é‡èº«å®šåˆ¶çš„æ€ç»´æ–¹å¼æ³¨å…¥é€šç”¨MLLMsã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¸æ··åˆä¼˜åŠ¿å‡½æ•°ï¼Œä»¥æ¿€åŠ±è½¨è¿¹ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ¨ç†ï¼ˆMMRï¼‰æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡8Kä¸ªå›¾åƒæ–‡æœ¬å¯¹ï¼ŒåŒ…æ‹¬æ¨ç†è¿‡ç¨‹å’Œåˆ†ç±»æ ‡ç­¾ï¼Œä»¥æ¨åŠ¨å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹é¢†åŸŸçš„å‘å±•ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„å¤šæ¨¡æ€ä¿¡æ¯æ£€æµ‹å™¨MMD-Thinkeråœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†çµæ´»çš„æ¨ç†å’Œä»¤ç‰Œä½¿ç”¨ã€‚ä»£ç å°†åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13242v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMMD-Thinkerçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºè‡ªé€‚åº”å¤šç»´åº¦æ€è€ƒçš„å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹ã€‚è¯¥æ¡†æ¶é’ˆå¯¹å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹è®¾è®¡äº†ä¸“ç”¨æ€è€ƒæ¨¡å¼ï¼Œå¹¶é€šè¿‡ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´å°†æ€è€ƒæ¨¡å¼æ³¨å…¥é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨æ··åˆä¼˜åŠ¿å‡½æ•°æ¿€åŠ±è½¨è¿¹ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡è¿˜æ„å»ºäº†å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ¨ç†ï¼ˆMMRï¼‰æ•°æ®é›†ï¼Œç”¨äºæ¨åŠ¨å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹é¢†åŸŸçš„å‘å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMD-Thinkeråœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æ•°æ®é›†ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒçµæ´»çš„æ¨ç†å’Œä»¤ç‰Œä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯åœ¨ç¤¾ä¼šä¸­æ³›æ»¥ï¼Œå¯¹ç¤¾ä¼šæ„æˆä¸¥é‡å¨èƒã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨åˆ©ç”¨é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œå¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹æ—¶å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™ï¼šæ¨ç†ä¸è¶³å’Œæ¨ç†åè§ã€‚</li>
<li>MMD-Thinkeræ¡†æ¶é€šè¿‡è‡ªé€‚åº”å¤šç»´åº¦æ€è€ƒæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä¸ºå¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹é‡èº«å®šåˆ¶äº†ä¸“ç”¨æ€è€ƒæ¨¡å¼ã€‚</li>
<li>MMD-Thinkeré‡‡ç”¨ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´ï¼Œå°†æ€è€ƒæ¨¡å¼æ³¨å…¥é€šç”¨MLLMsã€‚</li>
<li>åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥å’Œæ··åˆä¼˜åŠ¿å‡½æ•°æ¿€åŠ±æ¨ç†èƒ½åŠ›ï¼Œæé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>æ„å»ºäº†å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ¨ç†ï¼ˆMMRï¼‰æ•°æ®é›†ï¼Œæ¨åŠ¨å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹é¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3561294a8b58744ac74bb0850d2bbb32" align="middle">
<img src="https://picx.zhimg.com/v2-c70ab4ece9005502db6df65919703a2b" align="middle">
<img src="https://picx.zhimg.com/v2-ea90c10990dbaba7342bef57db3fa42e" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SGuard-v1-Safety-Guardrail-for-Large-Language-Models"><a href="#SGuard-v1-Safety-Guardrail-for-Large-Language-Models" class="headerlink" title="SGuard-v1: Safety Guardrail for Large Language Models"></a>SGuard-v1: Safety Guardrail for Large Language Models</h2><p><strong>Authors:JoonHo Lee, HyeonMin Cho, Jaewoong Yun, Hyunjae Lee, JunKyu Lee, Juree Seok</strong></p>
<p>We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.</p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†SGuard-v1ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è½»é‡çº§å®‰å…¨æŠ¤æ ã€‚å®ƒåŒ…å«ä¸¤ä¸ªä¸“ä¸šæ¨¡å‹ï¼Œç”¨äºæ£€æµ‹äººç±»ä¸äººå·¥æ™ºèƒ½å¯¹è¯åœºæ™¯ä¸­çš„æœ‰å®³å†…å®¹å’Œç­›é€‰å¯¹æŠ—æ€§æç¤ºã€‚å…¶ä¸­ï¼ŒContentFilterç»„ä»¶ç»è¿‡è®­ç»ƒï¼Œèƒ½å¤Ÿæ ¹æ®MLCommonså±å®³åˆ†ç±»æ³•è¯†åˆ«LLMæç¤ºå’Œå“åº”ä¸­çš„å®‰å…¨é£é™©ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„ä¿¡ä»»å’Œå®‰å…¨è¯„ä¼°çš„å…¨é¢æ¡†æ¶ã€‚å¦ä¸€ä¸ªç»„ä»¶JailbreakFilteråˆ™æ˜¯ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„è¯¾ç¨‹ï¼Œç»“åˆæ•°æ®é›†å’Œå…ˆå‰å…³äºå¯¹æŠ—æ€§æç¤ºçš„ç ”ç©¶ç»“æœæ¥è®­ç»ƒï¼Œæ¶µç›–60ç§ä¸»è¦æ”»å‡»ç±»å‹ï¼ŒåŒæ—¶å‡è½»è¯¯æŠ¥å®‰å…¨é£é™©ã€‚SGuard-v1å»ºç«‹åœ¨æ”¯æŒ12ç§è¯­è¨€çš„2Bå‚æ•°Granite-3.3-2B-Instructæ¨¡å‹ä¹‹ä¸Šã€‚æˆ‘ä»¬ä»æ”¶é›†å’Œåˆæˆæ•°æ®ä¸­æ•´ç†å‡ºçº¦140ä¸‡ä¸ªè®­ç»ƒå®ä¾‹ï¼Œå¯¹åŸºç¡€æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œå¹¶æ ¹æ®å„è‡ªçš„åŠŸèƒ½å°†æ•´ç†åçš„æ•°æ®åˆ†é…ç»™ä¸¤ä¸ªç»„ä»¶ã€‚åœ¨å…¬å…±å’Œå®‰å…¨ä¸“æœ‰åŸºå‡†çš„å¹¿æ³›è¯„ä¼°ä¸­ï¼ŒSGuard-v1å®ç°äº†æœ€å…ˆè¿›çš„å®‰å…¨æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§ï¼Œä»è€Œé™ä½äº†éƒ¨ç½²å¼€é”€ã€‚SGuard-v1è¿˜æä¾›å¤šç±»å®‰å…¨é¢„æµ‹å’ŒäºŒè¿›åˆ¶ç½®ä¿¡åº¦åˆ†æ•°ï¼Œæé«˜äº†ä¸‹æ¸¸ä½¿ç”¨çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨Apache-2.0è®¸å¯è¯ä¸‹å‘å¸ƒSGuard-v1ï¼Œä»¥æ”¯æŒåœ¨äººå·¥æ™ºèƒ½å®‰å…¨æ–¹é¢çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12497v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>SGuard-v1æ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è½»é‡çº§å®‰å…¨æŠ¤æ ï¼ŒåŒ…å«ä¸¤ä¸ªä¸“ä¸šæ¨¡å‹ï¼Œç”¨äºæ£€æµ‹äººç±»-AIå¯¹è¯è®¾ç½®ä¸­çš„æœ‰å®³å†…å®¹å’Œç­›é€‰å¯¹æŠ—æ€§æç¤ºã€‚ContentFilterç»„ä»¶æ ¹æ®MLCommonsçš„å±å®³åˆ†ç±»æ³•è®­ç»ƒï¼Œç”¨äºè¯†åˆ«LLMæç¤ºå’Œå“åº”ä¸­çš„å®‰å…¨é£é™©ã€‚JailbreakFilterç»„ä»¶åˆ™ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„è¯¾ç¨‹è¡¨å’Œå…ˆå‰å…³äºå¯¹æŠ—æ€§æç¤ºçš„ç ”ç©¶æ•°æ®è®­ç»ƒï¼Œè¦†ç›–60ç§ä¸»è¦æ”»å‡»ç±»å‹ï¼ŒåŒæ—¶å‡è½»è¯¯åˆ¤ä¸ºä¸å®‰å…¨çš„é£é™©ã€‚SGuard-v1å»ºç«‹åœ¨æ”¯æŒ12ç§è¯­è¨€çš„Granite-3.3-2B-Instructæ¨¡å‹ä¸Šï¼Œé€šè¿‡æŒ‡ä»¤å¾®è°ƒåŸºç¡€æ¨¡å‹ï¼Œå®ç°ä¸€æµçš„å®‰å…¨æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè½»é‡åŒ–ï¼Œé™ä½éƒ¨ç½²å¼€é”€ã€‚æ­¤å¤–ï¼ŒSGuard-v1è¿˜æä¾›å¤šç±»å®‰å…¨é¢„æµ‹å’ŒäºŒå…ƒç½®ä¿¡åº¦è¯„åˆ†ï¼Œæé«˜ä¸‹æ¸¸ä½¿ç”¨çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SGuard-v1æ˜¯ä¸€ç§é’ˆå¯¹LLMçš„è½»é‡çº§å®‰å…¨æŠ¤æ ã€‚</li>
<li>SGuard-v1åŒ…å«ä¸¤ä¸ªä¸“ä¸šæ¨¡å‹ï¼šContentFilterå’ŒJailbreakFilterã€‚</li>
<li>ContentFilteræ ¹æ®MLCommonsçš„å±å®³åˆ†ç±»æ³•è®­ç»ƒï¼Œç”¨äºè¯†åˆ«LLMä¸­çš„å®‰å…¨é£é™©ã€‚</li>
<li>JailbreakFilterè¦†ç›–60ç§ä¸»è¦æ”»å‡»ç±»å‹ï¼Œå¹¶å‡è½»è¯¯åˆ¤ä¸ºä¸å®‰å…¨çš„é£é™©ã€‚</li>
<li>SGuard-v1å»ºç«‹åœ¨æ”¯æŒ12ç§è¯­è¨€çš„Granite-3.3-2B-Instructæ¨¡å‹ä¸Šã€‚</li>
<li>SGuard-v1é€šè¿‡æŒ‡ä»¤å¾®è°ƒå®ç°ä¸€æµçš„å®‰å…¨æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè½»é‡åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8870ffa1a908c8a696f7273870312ab" align="middle">
<img src="https://picx.zhimg.com/v2-6bab1dcf74b3aa71f9a6dcafb2876d47" align="middle">
<img src="https://picx.zhimg.com/v2-f65eb613195a88e352603d1007b07ac9" align="middle">
<img src="https://picx.zhimg.com/v2-96af1b8cfe24154f2947c525b951abab" align="middle">
<img src="https://picx.zhimg.com/v2-a80eaab26908bab25785185f8043e19e" align="middle">
<img src="https://picx.zhimg.com/v2-05b379194c6cd01f1e7992b17c0aa151" align="middle">
<img src="https://picx.zhimg.com/v2-5fe5684c2ba5a0ad2a76dc01c145123c" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Seeing-is-Believing-Rich-Context-Hallucination-Detection-for-MLLMs-via-Backward-Visual-Grounding"><a href="#Seeing-is-Believing-Rich-Context-Hallucination-Detection-for-MLLMs-via-Backward-Visual-Grounding" class="headerlink" title="Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding"></a>Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding</h2><p><strong>Authors:Pinxue Guo, Chongruo Wu, Xinyu Zhou, Lingyi Hong, Zhaoyu Chen, Jinglun Li, Kaixun Jiang, Sen-ching Samson Cheung, Wei Zhang, Wenqiang Zhang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of â€œSeeing is Believingâ€, we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4oâ€™s capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at <a target="_blank" rel="noopener" href="https://github.com/PinxueGuo/VBackChecker">https://github.com/PinxueGuo/VBackChecker</a>.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»è§£é”äº†å¼ºå¤§çš„è·¨æ¨¡æ€èƒ½åŠ›ï¼Œä½†ä»ç„¶é­å—å¹»è§‰çš„ä¸¥é‡å½±å“ã€‚å› æ­¤ï¼Œå¯¹MLLMsä¸­çš„å¹»è§‰è¿›è¡Œå‡†ç¡®æ£€æµ‹å¯¹äºç¡®ä¿å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§è‡³å…³é‡è¦ã€‚æœ‰é‰´äºæ­¤ï¼Œæˆ‘ä»¬éµå¾ªâ€œçœ¼è§ä¸ºå®â€çš„åŸåˆ™ï¼Œå¼•å…¥äº†VBackCheckerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— å‚è€ƒå¹»è§‰æ£€æµ‹æ¡†æ¶ã€‚å®ƒé€šè¿‡åˆ©ç”¨é…å¤‡æ¨ç†å’Œå¼•ç”¨åˆ†å‰²èƒ½åŠ›çš„åƒç´ çº§æ¥åœ°LLMï¼ŒéªŒè¯MLLMç”Ÿæˆçš„å“åº”ä¸è§†è§‰è¾“å…¥çš„çš„ä¸€è‡´æ€§ã€‚è¿™ç§æ— å‚è€ƒæ¡†æ¶ä¸ä»…æœ‰æ•ˆåœ°å¤„ç†äº†ä¸°å¯Œè¯­å¢ƒçš„åœºæ™¯ï¼Œè¿˜æä¾›äº†å¯è§£é‡Šæ€§ã€‚ä¸ºæ­¤ï¼Œè®¾è®¡äº†ä¸€ä¸ªåˆ›æ–°çš„æµç¨‹æ¥ç”ŸæˆæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼ˆR-Instructï¼‰ï¼Œå…·æœ‰ä¸°å¯Œä¸Šä¸‹æ–‡æè¿°ã€æ¥åœ°æ©ç å’Œç¡¬è´Ÿæ ·æœ¬ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å»ºç«‹äº†MLLMçš„R^2-HalBenchæ–°å¹»è§‰åŸºå‡†æµ‹è¯•ï¼Œä¸ä»¥å‰çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼Œå®ƒæ¶µç›–äº†æ¥è‡ª18ä¸ªMLLMçš„ç°å®ä¸–ç•Œä¸°å¯Œä¸Šä¸‹æ–‡æè¿°ï¼Œå…·æœ‰é«˜è´¨é‡æ³¨é‡Šï¼Œæ¶µç›–äº†å„ç§å¯¹è±¡ã€å±æ€§å’Œå…³ç³»çº§åˆ«çš„ç»†èŠ‚ã€‚VBackCheckerä¼˜äºå…ˆå‰çš„å¤æ‚æ¡†æ¶ï¼Œåœ¨R^2-HalBenchä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œç”šè‡³èƒ½ä¸GPT-4oçš„å¹»è§‰æ£€æµ‹èƒ½åŠ›ç›¸æŠ—è¡¡ã€‚åœ¨åƒç´ çº§æ¥åœ°ä»»åŠ¡ä¸Šï¼Œå®ƒä¹Ÿè¶…è¶Šäº†å…ˆå‰çš„æ–¹æ³•ï¼Œå®ç°äº†è¶…è¿‡10%çš„æ”¹è¿›ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PinxueGuo/VBackChecker%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PinxueGuo/VBackCheckerä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12140v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºâ€œçœ¼è§ä¸ºå®â€çš„åŸåˆ™ï¼Œå¼•å…¥äº†ä¸€ç§æ— éœ€å‚è€ƒçš„å¹»è§‰æ£€æµ‹æ¡†æ¶VBackCheckerï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åƒç´ çº§æ¥åœ°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éªŒè¯MLLMç”Ÿæˆçš„å“åº”ä¸è§†è§‰è¾“å…¥çš„ä¸€è‡´æ€§ã€‚æ­¤æ¡†æ¶ä¸ä»…é€‚ç”¨äºä¸°å¯Œè¯­å¢ƒåœºæ™¯ï¼Œè€Œä¸”å…·æœ‰å¯è§£é‡Šæ€§ã€‚è®¾è®¡äº†ä¸€æ¡åˆ›æ–°ç®¡é“ï¼Œç”¨äºç”ŸæˆåŒ…å«ä¸°å¯Œè¯­å¢ƒæè¿°ã€æ¥åœ°æ©ç å’Œç¡¬è´Ÿæ ·æœ¬çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼ˆR-Instructï¼‰ã€‚æ­¤å¤–ï¼Œå»ºç«‹äº†æ–°çš„MLLMå¹»è§‰åŸºå‡†æµ‹è¯•RÂ² -HalBenchï¼Œæ¶µç›–äº†æ¥è‡ª18ä¸ªMLLMçš„çœŸå®ä¸–ç•Œä¸°å¯Œè¯­å¢ƒæè¿°ï¼Œä»¥åŠé«˜è´¨é‡æ³¨é‡Šï¼Œæ¶µç›–å„ç§å¯¹è±¡ã€å±æ€§å’Œå…³ç³»å±‚é¢çš„ç»†èŠ‚ã€‚VBackCheckeråœ¨RÂ² -HalBenchä¸Šçš„æ€§èƒ½ä¼˜äºå…ˆå‰å¤æ‚çš„æ¡†æ¶ï¼Œç”šè‡³åœ¨å¹»è§‰æ£€æµ‹æ–¹é¢ä¸GPT-4çš„èƒ½åŠ›ç›¸åŒ¹æ•Œã€‚å®ƒä¹Ÿè¶…è¶Šäº†å…ˆå‰çš„åƒç´ çº§æ¥åœ°ä»»åŠ¡æ–¹æ³•ï¼Œå®ç°äº†è¶…è¿‡10%çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMså…·å¤‡è·¨æ¨¡æ€èƒ½åŠ›ï¼Œä½†å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œå½±å“å¯é æ€§ã€‚</li>
<li>VBackCheckeræ˜¯ä¸€ç§æ— éœ€å‚è€ƒçš„å¹»è§‰æ£€æµ‹æ¡†æ¶ï¼Œåˆ©ç”¨åƒç´ çº§æ¥åœ°LLMéªŒè¯å“åº”ä¸è§†è§‰è¾“å…¥çš„ä¸€è‡´æ€§ã€‚</li>
<li>VBackCheckeré€‚ç”¨äºä¸°å¯Œè¯­å¢ƒåœºæ™¯ï¼Œå…·å¤‡å¯è§£é‡Šæ€§ã€‚</li>
<li>è®¾è®¡äº†ç”ŸæˆæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼ˆR-Instructï¼‰çš„åˆ›æ–°ç®¡é“ï¼Œç”¨äºä¸°å¯Œè¯­å¢ƒæè¿°ã€æ¥åœ°æ©ç å’Œç¡¬è´Ÿæ ·æœ¬ã€‚</li>
<li>å»ºç«‹äº†æ–°çš„MLLMå¹»è§‰åŸºå‡†æµ‹è¯•RÂ² -HalBenchï¼ŒåŒ…å«çœŸå®ä¸–ç•Œä¸°å¯Œè¯­å¢ƒæè¿°å’Œé«˜è´¨é‡æ³¨é‡Šã€‚</li>
<li>VBackCheckeråœ¨RÂ² -HalBenchä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ¡†æ¶ï¼Œä¸GPT-4çš„å¹»è§‰æ£€æµ‹èƒ½åŠ›ç›¸åŒ¹æ•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9de78ef0a9aba94b6f71192bcc323475" align="middle">
<img src="https://picx.zhimg.com/v2-6a29538c33079843cd1700ea502ed6b6" align="middle">
<img src="https://picx.zhimg.com/v2-bb4f32de3a6d7c76793165e21c1606d1" align="middle">
<img src="https://picx.zhimg.com/v2-cb3f6c4f5ade00d1d13715078fbb7611" align="middle">
<img src="https://picx.zhimg.com/v2-db145c10b7214d0e35260a6e01190750" align="middle">
<img src="https://picx.zhimg.com/v2-b0cf458dd7f4fa2081e1d76455c99fb5" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="From-Scaling-to-Structured-Expressivity-Rethinking-Transformers-for-CTR-Prediction"><a href="#From-Scaling-to-Structured-Expressivity-Rethinking-Transformers-for-CTR-Prediction" class="headerlink" title="From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction"></a>From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction</h2><p><strong>Authors:Bencheng Yan, Yuejie Lei, Zhiyuan Zeng, Di Wang, Kaiyi Lin, Pengjie Wang, Jian Xu, Bo Zheng</strong></p>
<p>Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n &gt;&gt; F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics.</p>
<blockquote>
<p>å°½ç®¡åœ¨è§„æ¨¡ä¸ŠæŠ•å…¥äº†å¤§é‡èµ„é‡‘ï¼Œä½†ç”¨äºç‚¹å‡»ç‡ï¼ˆCTRï¼‰é¢„æµ‹çš„æ·±å±‚æ¨¡å‹å¾€å¾€è¡¨ç°å‡ºæ”¶ç›Šè¿…é€Ÿé€’å‡çš„è¶‹åŠ¿ï¼Œè¿™ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çœ‹åˆ°çš„å¹³ç¨³ã€å¯é¢„æµ‹çš„æ”¶ç›Šå½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚æˆ‘ä»¬ç¡®å®šæ ¹æœ¬åŸå› æ˜¯ç»“æ„ä¸åŒ¹é…ï¼šTransformer å‡è®¾äº†é¡ºåºç»„åˆæ€§ï¼Œè€Œ CTR æ•°æ®åˆ™éœ€è¦åœ¨é«˜åŸºæ•°è¯­ä¹‰å­—æ®µä¸Šè¿›è¡Œç»„åˆæ¨ç†ã€‚éç»“æ„åŒ–çš„æ³¨æ„åŠ›ä¼šæ¼«æ— ç›®çš„åœ°åˆ†æ•£èƒ½åŠ›ï¼Œåœ¨æç«¯ç¨€ç–æ€§ä¸‹æ”¾å¤§å™ªéŸ³å¹¶ç ´åå¯æ‰©å±•å­¦ä¹ ã€‚ä¸ºäº†æ¢å¤å¯¹é½ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå­—æ®µçš„ Transformerï¼ˆFATï¼‰ï¼Œå®ƒé€šè¿‡åˆ†è§£å†…å®¹å¯¹é½å’Œè·¨å­—æ®µè°ƒåˆ¶ï¼Œå°†å­—æ®µäº¤äº’å…ˆéªŒçŸ¥è¯†åµŒå…¥åˆ°æ³¨æ„åŠ›ä¸­ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†æ¨¡å‹å¤æ‚åº¦éšç€å­—æ®µæ•° F çš„å¢åŠ è€Œæ‰©å±•ï¼Œè€Œä¸æ˜¯éšç€æ€»è¯æ±‡é‡ n &gt;&gt; F çš„å¢åŠ è€Œå¢åŠ ï¼Œä»è€Œå®ç°äº†æ›´ç´§å¯†çš„æ³›åŒ–ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹å®½åº¦å¢åŠ æ—¶è§‚å¯Ÿåˆ° AUC ä¸­çš„å¹‚å¾‹æ‰©å±•è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäº Rademacher å¤æ‚æ€§çš„ CTR æ¨¡å‹çš„é¦–ä¸ªæ­£å¼æ‰©å±•å®šå¾‹ï¼Œè¯¥å®šå¾‹è§£é‡Šå¹¶é¢„æµ‹äº†è¿™ä¸€è¡Œä¸ºã€‚åœ¨å¤§å‹åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒFAT çš„ AUC æé«˜äº†é«˜è¾¾ +0.51%ã€‚åœ¨çº¿éƒ¨ç½²æ—¶ï¼Œå®ƒæé«˜äº† 2.33% çš„ç‚¹å‡»ç‡å’Œ 0.66% çš„ RPMã€‚æˆ‘ä»¬çš„å·¥ä½œç¡®å®šäº†æœ‰æ•ˆçš„æ¨èæ‰©å±•å¹¶éæ¥è‡ªè§„æ¨¡ï¼Œè€Œæ˜¯æ¥è‡ªç»“æ„åŒ–è¡¨è¾¾èƒ½åŠ›ä¸æ•°æ®è¯­ä¹‰çš„ç»“æ„ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12081v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç‚¹å‡»ç‡é¢„æµ‹æ·±åº¦æ¨¡å‹å­˜åœ¨çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†Field-Aware Transformerï¼ˆFATï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè™½ç„¶å¤§è§„æ¨¡æ¨¡å‹æŠ•èµ„å›æŠ¥é€’å‡è¿…é€Ÿï¼Œä½†Field-Aware Transformeré€šè¿‡åµŒå…¥å­—æ®µäº¤äº’å…ˆéªŒä¿¡æ¯ï¼Œå®ç°æ¨¡å‹å¤æ‚åº¦éšå­—æ®µæ•°é‡è€Œéæ€»è¯æ±‡é‡å¢é•¿ï¼Œä»è€Œæé«˜CTRé¢„æµ‹å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å»ºç«‹äº†åŸºäºRademacherå¤æ‚åº¦çš„CTRæ¨¡å‹è§„æ¨¡åŒ–å®šå¾‹ï¼Œä¸ºé¢„æµ‹æ¨¡å‹è¡Œä¸ºæä¾›ä¾æ®ã€‚åœ¨çº¿éƒ¨ç½²ä¸­ï¼ŒFATèƒ½æé«˜ç‚¹å‡»ç‡åŠæ”¶å…¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‚¹å‡»ç‡é¢„æµ‹æ·±åº¦æ¨¡å‹é¢ä¸´æŠ•èµ„å›æŠ¥è¿…é€Ÿé€’å‡çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰Transformeræ¨¡å‹åœ¨CTRé¢„æµ‹ä¸­å­˜åœ¨ç»“æ„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>Field-Aware Transformerï¼ˆFATï¼‰é€šè¿‡åµŒå…¥å­—æ®µäº¤äº’å…ˆéªŒä¿¡æ¯è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>FATè®¾è®¡ç¡®ä¿æ¨¡å‹å¤æ‚åº¦éšå­—æ®µæ•°é‡å¢é•¿ï¼Œè€Œéæ€»è¯æ±‡é‡ã€‚</li>
<li>FATæé«˜äº†CTRé¢„æµ‹å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¤§å‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>æ–‡ç« å»ºç«‹äº†åŸºäºRademacherå¤æ‚åº¦çš„CTRæ¨¡å‹è§„æ¨¡åŒ–å®šå¾‹ï¼Œä¸ºé¢„æµ‹æ¨¡å‹è¡Œä¸ºæä¾›ä¾æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12081">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-427336b28645a34d88c56d18284fd71a" align="middle">
<img src="https://picx.zhimg.com/v2-fcb3d4135398b5aa237b02ea28523a58" align="middle">
<img src="https://picx.zhimg.com/v2-fd0427396f37901370032ba86cadf120" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Look-As-You-Think-Unifying-Reasoning-and-Visual-Evidence-Attribution-for-Verifiable-Document-RAG-via-Reinforcement-Learning"><a href="#Look-As-You-Think-Unifying-Reasoning-and-Visual-Evidence-Attribution-for-Verifiable-Document-RAG-via-Reinforcement-Learning" class="headerlink" title="Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning"></a>Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning</h2><p><strong>Authors:Shuochen Liu, Pengfei Luo, Chao Zhang, Yuhao Chen, Haotian Zhang, Qi Liu, Xin Kou, Tong Xu, Enhong Chen</strong></p>
<p>Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in <a href="mailto:&#x49;&#111;&#85;&#x40;&#x30;&#46;&#53;">&#x49;&#111;&#85;&#x40;&#x30;&#46;&#53;</a>. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.</p>
<blockquote>
<p>é’ˆå¯¹ä»è§†è§‰æ–‡æ¡£ä¸­æå–ç²¾ç¡®è¯æ®æºçš„ç›®æ ‡ï¼Œè§†è§‰è¯æ®å½’å±å¯¹äºè§†è§‰æ–‡æ¡£æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆVD-RAGï¼‰è‡³å…³é‡è¦ï¼Œä»¥ç¡®ä¿å¤šæ¨¡æ€é—®ç­”ä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›å¯é å’Œå¯éªŒè¯çš„é¢„æµ‹ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•é‡‡ç”¨ç«¯åˆ°ç«¯çš„è®­ç»ƒæ–¹å¼ï¼Œä»¥ä¾¿äºç›´è§‚çš„ç­”æ¡ˆéªŒè¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­ç¼ºä¹ç²¾ç»†çš„ç›‘ç£å’Œé€æ­¥çš„å¯è¿½æº¯æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºVD-RAGå¼•å…¥äº†è¯æ®é“¾ï¼ˆCoEï¼‰èŒƒå¼ã€‚CoEé€šè¿‡ç»“åˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å’Œè§†è§‰è¯æ®å½’å±ï¼Œé€šè¿‡å°†å‚è€ƒå…ƒç´ ä¸å¸¦æœ‰è¾¹ç•Œæ¡†å’Œé¡µé¢ç´¢å¼•çš„ç‰¹å®šåŒºåŸŸç›¸å…³è”ï¼Œå®ç°äº†ç»Ÿä¸€ã€‚ä¸ºäº†å…è®¸VLMsç”Ÿæˆè¿™ç§ä»¥è¯æ®ä¸ºåŸºç¡€çš„ç†ç”±ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè¾¹æ€è€ƒè¾¹è§‚å¯Ÿâ€ï¼ˆLATï¼‰è¿™ä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è®­ç»ƒæ¨¡å‹ä»¥äº§ç”Ÿå…·æœ‰ä¸€è‡´å½’å±æ€§çš„å¯éªŒè¯æ¨ç†è·¯å¾„ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒLATè¯„ä¼°æ¯ä¸ªè¯æ®åŒºåŸŸçš„å½’å±ä¸€è‡´æ€§ï¼Œå¹¶ä¸”ä»…åœ¨è¯æ®é“¾è½¨è¿¹äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆæ—¶æ‰æä¾›å¥–åŠ±ï¼Œé¼“åŠ±è¿›è¡Œè¿‡ç¨‹çº§åˆ«çš„è‡ªæˆ‘éªŒè¯ã€‚åœ¨é‡‡ç”¨Paper-å’ŒWiki-VISAåŸºå‡†æµ‹è¯•çš„æ™®é€šQwen2.5-VL-7B-Instructä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒLATåœ¨å•å›¾åƒå’Œå¤šå›¾åƒè®¾ç½®ä¸­å‡æŒç»­æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨è½¯ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰æ–¹é¢å¹³å‡æé«˜äº†8.23%ï¼Œåœ¨<a href="mailto:&#73;&#x6f;&#x55;&#x40;&#x30;&#46;&#53;">&#73;&#x6f;&#x55;&#x40;&#x30;&#46;&#53;</a>æ–¹é¢æé«˜äº†47.0%ã€‚ä¸æ­¤åŒæ—¶ï¼ŒLATä¸ä»…ä¼˜äºé€šè¿‡ç›‘ç£å¾®è°ƒåŸºçº¿ï¼ˆè®­ç»ƒä¸ºç›´æ¥äº§ç”Ÿå¸¦æœ‰å½’å±æ€§çš„ç­”æ¡ˆï¼‰ï¼Œè€Œä¸”åœ¨è·¨åŸŸæ–¹é¢çš„æ³›åŒ–æ€§èƒ½æ›´å¼ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12003v1">PDF</a> Poster of AAAIâ€™2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢å‘è§†è§‰æ–‡æ¡£æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆVD-RAGï¼‰çš„è¯æ®é“¾ï¼ˆCoEï¼‰èŒƒå¼ï¼Œå®ƒå°†æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä¸è§†è§‰è¯æ®å½’å±ç›¸ç»“åˆï¼Œé€šè¿‡è¾¹ç•Œæ¡†å’Œé¡µé¢ç´¢å¼•å°†æ¨ç†æ­¥éª¤ä¸­çš„å‚è€ƒå…ƒç´ ä¸ç‰¹å®šåŒºåŸŸç›¸è”ç³»ã€‚ä¸ºä½¿å¾—VLMsèƒ½å¤Ÿç”Ÿæˆä»¥è¯æ®ä¸ºåŸºç¡€çš„æ¨ç†ï¼Œæå‡ºäº†â€œè¾¹æ€è€ƒè¾¹è§‚å¯Ÿâ€ï¼ˆLATï¼‰çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è®­ç»ƒæ¨¡å‹äº§ç”Ÿå¯éªŒè¯çš„æ¨ç†è·¯å¾„ï¼Œå¹¶å…·å¤‡ä¸€è‡´çš„å½’å±å±æ€§ã€‚åœ¨è®­ç»ƒä¸­ï¼ŒLATè¯„ä¼°æ¯ä¸ªè¯æ®åŒºåŸŸå½’å±çš„ä¸€è‡´æ€§ï¼Œä»…åœ¨è¯æ®é“¾è½¨è¿¹äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆæ—¶æä¾›å¥–åŠ±ï¼Œé¼“åŠ±æµç¨‹çº§åˆ«çš„è‡ªæˆ‘éªŒè¯ã€‚å®éªŒè¡¨æ˜ï¼ŒLATåœ¨å•å›¾åƒå’Œå¤šå›¾åƒåœºæ™¯ä¸‹å‡æ”¹è¿›äº†åŸºç¡€æ¨¡å‹ï¼Œåœ¨è½¯ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰å’Œ<a href="mailto:&#x49;&#111;&#x55;&#x40;&#48;&#46;&#53;">&#x49;&#111;&#x55;&#x40;&#48;&#46;&#53;</a>ä¸Šå¹³å‡æé«˜äº†8.23%å’Œ47.0%ã€‚æ­¤å¤–ï¼ŒLATä¸ä»…è¶…è¶Šäº†é€šè¿‡ç›´æ¥äº§ç”Ÿç­”æ¡ˆå’Œå½’å±è¿›è¡Œè®­ç»ƒçš„è¶…ç²¾ç»†è°ƒæ•´åŸºçº¿ï¼Œè€Œä¸”åœ¨è·¨åŸŸæ–¹é¢çš„è¡¨ç°ä¹Ÿæ›´å¼ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†é¢å‘è§†è§‰æ–‡æ¡£æ£€ç´¢å¢å¼ºç”Ÿæˆçš„è¯æ®é“¾ï¼ˆCoEï¼‰èŒƒå¼ï¼Œç»“åˆäº†æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å’Œè§†è§‰è¯æ®å½’å±ã€‚</li>
<li>å¼•å…¥äº†å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€œè¾¹æ€è€ƒè¾¹è§‚å¯Ÿâ€ï¼ˆLATï¼‰ï¼Œä½¿VLMsèƒ½å¤Ÿç”Ÿæˆä¸è¯æ®ç›¸å…³çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>LATåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¼“åŠ±æµç¨‹çº§åˆ«çš„è‡ªæˆ‘éªŒè¯ï¼Œæå‡äº†æ¨¡å‹çš„ç²¾ç¡®åŒ¹é…å’ŒåŒºåŸŸé‡å è¯„ä»·æŒ‡æ ‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLATåœ¨å•å›¾åƒå’Œå¤šå›¾åƒåœºæ™¯ä¸‹å‡æ”¹è¿›äº†åŸºç¡€æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>LATä¸ä»…è¶…è¶Šäº†é€šè¿‡ç›´æ¥äº§ç”Ÿç­”æ¡ˆå’Œå½’å±è¿›è¡Œè®­ç»ƒçš„è¶…ç²¾ç»†è°ƒæ•´åŸºçº¿æ¨¡å‹ï¼Œè€Œä¸”åœ¨è·¨åŸŸæ–¹é¢çš„è¡¨ç°æ›´å¼ºã€‚</li>
<li>é€šè¿‡å°†æ¨ç†æ­¥éª¤ä¸­çš„å‚è€ƒå…ƒç´ ä¸ç‰¹å®šåŒºåŸŸç›¸è”ç³»ï¼ŒCoEèŒƒå¼æé«˜äº†è§†è§‰è¯æ®å½’å±çš„å‡†ç¡®æ€§å’Œç²¾ç»†åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2474700b8a36995a1ca1e8f3133400c2" align="middle">
<img src="https://picx.zhimg.com/v2-f66eab7f5ed80e965e363113a29e353a" align="middle">
<img src="https://picx.zhimg.com/v2-1b9fd4becf0b1e95afa2393b6b0cff37" align="middle">
<img src="https://picx.zhimg.com/v2-bd4a72ea4aae7afcf9009bd08128a169" align="middle">
<img src="https://picx.zhimg.com/v2-42db57690c6b2794611218f4bc2ee45b" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Reasoning-Paradigm-for-Named-Entity-Recognition"><a href="#A-Reasoning-Paradigm-for-Named-Entity-Recognition" class="headerlink" title="A Reasoning Paradigm for Named Entity Recognition"></a>A Reasoning Paradigm for Named Entity Recognition</h2><p><strong>Authors:Hui Huang, Yanping Chen, Ruizhang Huang, Chuan Lin, Yongbin Qin</strong></p>
<p>Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This â€œcognitive shortcuttingâ€ leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/HuiResearch/ReasoningIE">https://github.com/HuiResearch/ReasoningIE</a>.</p>
<blockquote>
<p>ç”Ÿæˆå¼LLMé€šå¸¸é€šè¿‡æŒ‡ä»¤å¾®è°ƒæé«˜å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ€§èƒ½ã€‚å®ƒä»¬åœ¨é€šè¿‡è¯­ä¹‰æ¨¡å¼åŒ¹é…ç”Ÿæˆå®ä½“æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹æ˜ç¡®å¯éªŒè¯çš„æ¨ç†æœºåˆ¶ã€‚è¿™ç§â€œè®¤çŸ¥æ·å¾„â€å¯¼è‡´æ¬¡ä¼˜æ€§èƒ½å’Œè„†å¼±çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å’Œä½èµ„æºåœºæ™¯ä¸­ï¼Œä»æœ‰é™çš„ä¸Šä¸‹æ–‡çº¿ç´¢ä¸­è¿›è¡Œæ¨ç†è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºNERçš„æ¨ç†æ¡†æ¶ï¼Œå°†æå–èŒƒå¼ä»éšå¼æ¨¡å¼åŒ¹é…è½¬å‘æ˜¾å¼æ¨ç†ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ€ç»´é“¾ï¼ˆCoTï¼‰ç”Ÿæˆã€CoTå¾®è°ƒã€ä»¥åŠæ¨ç†å¢å¼ºã€‚é¦–å…ˆï¼Œç”Ÿæˆä¸€ä¸ªåŒ…å«é¢å‘NERçš„CoTçš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸ä»»åŠ¡ç›¸å…³çš„æ¨ç†é“¾ã€‚ç„¶åï¼Œå®ƒä»¬è¢«ç”¨æ¥è°ƒæ•´NERæ¨¡å‹ï¼Œåœ¨å¾—å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰ç”Ÿæˆè¿è´¯çš„ç†æ€§ã€‚æœ€åï¼Œå®æ–½æ¨ç†å¢å¼ºé˜¶æ®µï¼Œä½¿ç”¨å…¨é¢çš„å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚è¿™ä¸€é˜¶æ®µç¡®ä¿æå–æ˜¯æ˜ç¡®ä¸”å¯éªŒè¯çš„ã€‚å®éªŒè¡¨æ˜ï¼ŒReasoningNERåœ¨NERä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è®¤çŸ¥èƒ½åŠ›ï¼Œå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼Œå®ƒåœ¨F1åˆ†æ•°ä¸Šè¶…è¶Šäº†GPT-4ï¼Œè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆï¼ˆSOTAï¼‰çš„æ€§èƒ½æ°´å¹³ï¼Œé«˜å‡º12.3ä¸ªç™¾åˆ†ç‚¹ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨æ¨åŠ¨ä»¥æ¨ç†ä¸ºæ ¸å¿ƒçš„ä¿¡æ¯æå–ç ”ç©¶æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HuiResearch/ReasoningIE">https://github.com/HuiResearch/ReasoningIE</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11978v1">PDF</a> Accepted at AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¼LLMæ¨¡å‹çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ€§èƒ½æå‡é€šå¸¸é€šè¿‡æŒ‡ä»¤å¾®è°ƒå®ç°ã€‚è¿™äº›æ¨¡å‹æ“…é•¿é€šè¿‡è¯­ä¹‰æ¨¡å¼åŒ¹é…ç”Ÿæˆå®ä½“ï¼Œä½†ç¼ºä¹æ˜ç¡®çš„å¯éªŒè¯æ¨ç†æœºåˆ¶ã€‚è¿™ç§â€œè®¤çŸ¥æ·å¾„â€å¯¼è‡´åœ¨é›¶æ ·æœ¬å’Œä½èµ„æºåœºæ™¯ä¸­æ€§èƒ½ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹ä¸Šä¸‹æ–‡çº¿ç´¢æ—¶çš„æ¨ç†èƒ½åŠ›å°¤ä¸ºé‡è¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºNERçš„æ¨ç†æ¡†æ¶ï¼Œå°†æå–èŒƒå¼ä»éšå¼æ¨¡å¼åŒ¹é…è½¬å‘æ˜¾å¼æ¨ç†ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ€ç»´é“¾ç”Ÿæˆã€æ€ç»´é“¾å¾®è°ƒã€æ¨ç†å¢å¼ºã€‚å®éªŒè¡¨æ˜ï¼ŒReasoningNERåœ¨NERä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è®¤çŸ¥èƒ½åŠ›ï¼Œå–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼Œå®ƒåœ¨F1åˆ†æ•°ä¸Šè¶…è¶ŠGPT-4ï¼Œè¾¾åˆ°ä¸šç•Œæœ€ä½³æ°´å¹³ã€‚åˆ†æç»“æœæ˜¾ç¤ºå…¶åœ¨é¢å‘æ¨ç†çš„ä¿¡æ¯æå–æ–¹é¢æœ‰å¾ˆå¤§æ½œåŠ›ã€‚ä»£ç å¯è®¿é—®é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/HuiResearch/ReasoningIE%E3%80%82">https://github.com/HuiResearch/ReasoningIEã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼LLMsé€šè¿‡æŒ‡ä»¤å¾®è°ƒæå‡å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ€§èƒ½ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨è¯­ä¹‰æ¨¡å¼åŒ¹é…æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹æ˜ç¡®çš„æ¨ç†æœºåˆ¶ã€‚</li>
<li>â€œè®¤çŸ¥æ·å¾„â€åœ¨é›¶æ ·æœ¬å’Œä½èµ„æºåœºæ™¯ä¸­å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç”¨äºNERçš„æ¨ç†æ¡†æ¶ï¼ŒåŒ…æ‹¬æ€ç»´é“¾ç”Ÿæˆã€æ€ç»´é“¾å¾®è°ƒã€æ¨ç†å¢å¼ºä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨å°†éšå¼æ¨¡å¼åŒ¹é…è½¬å‘æ˜¾å¼æ¨ç†ï¼Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ReasoningNERåœ¨NERä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜ç§€çš„è®¤çŸ¥èƒ½åŠ›ï¼Œè¾¾åˆ°ä¸šç•Œæœ€ä½³æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7fcf5a93be4e50acbae855bf3da8b1f4" align="middle">
<img src="https://picx.zhimg.com/v2-55d3134fa02211ab548c766b8a612489" align="middle">
<img src="https://picx.zhimg.com/v2-ab48b945889166df9ca2cfeef7e84263" align="middle">
<img src="https://picx.zhimg.com/v2-12f7739ca9b1e80ba8daa1bf65ca069f" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Identifying-Imaging-Follow-Up-in-Radiology-Reports-A-Comparative-Analysis-of-Traditional-ML-and-LLM-Approaches"><a href="#Identifying-Imaging-Follow-Up-in-Radiology-Reports-A-Comparative-Analysis-of-Traditional-ML-and-LLM-Approaches" class="headerlink" title="Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches"></a>Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches</h2><p><strong>Authors:Namu Park, Giridhar Kaushik Ramachandran, Kevin Lybarger, Fei Xia, Ozlem Uzuner, Meliha Yetisgen, Martin Gunn</strong></p>
<p>Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 &#x3D; 0.846). GPT-4o (Advanced) achieved the best performance (F1 &#x3D; 0.832), followed closely by GPT-OSS-20B (Advanced; F1 &#x3D; 0.828). LR and SVM also performed strongly (F1 &#x3D; 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†åœ¨æ”¾å°„å­¦ä»»åŠ¡ä¸Šä¸¥æ ¼è¯„ä¼°å…¶æ€§èƒ½çš„ä¸“ä¸šæ•°æ®é›†å´å¾ˆå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¥è‡ª586åæ‚£è€…çš„6393ä»½æ”¾å°„å­¦æŠ¥å‘Šæ•°æ®é›†ï¼Œæ¯ä»½æŠ¥å‘Šéƒ½æ ‡æœ‰éšè®¿å½±åƒçŠ¶æ€ï¼Œä»¥æ”¯æŒå¼€å‘å¹¶è¯„ä¼°éšè®¿ä¾ä»æ€§æ£€æµ‹ç³»ç»Ÿã€‚ä½¿ç”¨æ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ï¼ŒåŒ…æ‹¬é€»è¾‘å›å½’ï¼ˆLRï¼‰ã€æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ã€Longformerå’Œå®Œå…¨å¾®è°ƒè¿‡çš„Llama3-8B-Instructï¼Œä»¥åŠä¸è¿‘æœŸçš„ç”Ÿæˆå¼LLMã€‚ä¸ºäº†è¯„ä¼°ç”Ÿæˆå¼LLMï¼Œæˆ‘ä»¬æµ‹è¯•äº†GPT-4oå’Œå¼€æºGPT-OSS-20Båœ¨ä¸¤ç§é…ç½®ä¸‹çš„è¡¨ç°ï¼šä¸€ç§æ˜¯åŸºç¡€ï¼ˆBaseï¼‰è®¾ç½®ï¼Œå¦ä¸€ç§æ˜¯ä¸“æ³¨äºå…ƒæ•°æ®çš„ä»»åŠ¡ä¼˜åŒ–ï¼ˆAdvancedï¼‰è®¾ç½®ï¼Œä»¥åŠæ¨èå¥å­åŠå…¶å‘¨å›´ä¸Šä¸‹æ–‡ã€‚GPT-OSS-20Bçš„ç²¾ç»†æç¤ºè¿›ä¸€æ­¥æé«˜äº†æ¨ç†å‡†ç¡®æ€§ã€‚é€šè¿‡éå‚æ•°åŒ–bootstrapæ–¹æ³•æ¥è¯„ä¼°æ€§èƒ½ï¼Œä½¿ç”¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ï¼Œå¹¶ä¼°è®¡95%ç½®ä¿¡åŒºé—´ã€‚æ³¨é‡Šè€…é—´åè®®ç‡é«˜ï¼ˆF1&#x3D;0.846ï¼‰ã€‚GPT-4oï¼ˆAdvancedï¼‰è¡¨ç°æœ€ä½³ï¼ˆF1&#x3D;0.832ï¼‰ï¼Œç´§éšå…¶åçš„æ˜¯GPT-OSS-20Bï¼ˆAdvancedï¼›F1&#x3D;0.828ï¼‰ã€‚LRå’ŒSVMä¹Ÿè¡¨ç°å‡ºè‰²ï¼ˆF1&#x3D;0.776å’Œ0.775ï¼‰ï¼Œè¿™è¡¨æ˜è™½ç„¶é€šè¿‡æç¤ºä¼˜åŒ–LLMå¯ä»¥æ¥è¿‘äººç±»æ°´å¹³çš„åè®®ï¼Œä½†å¯è§£é‡Šæ€§å’Œèµ„æºé«˜æ•ˆçš„æ¨¡å‹ä»ç„¶æ˜¯å®è´µçš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11867v1">PDF</a> Submitted to LREC 2026</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶åœ¨æ”¾å°„å­¦ä»»åŠ¡ä¸­ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåŒ…å«6,393ä»½æ”¾å°„å­¦æŠ¥å‘Šå’Œ586ä½æ‚£è€…æ•°æ®çš„æ ‡æ³¨è¯­æ–™åº“ï¼Œç”¨äºå¼€å‘å’Œè¯„ä¼°éšè®¿ä¾ä»æ€§æ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½ã€‚é€šè¿‡æ¯”è¾ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ†ç±»å™¨å’Œæœ€æ–°ç”Ÿæˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°GPT-4oåœ¨é«˜çº§é…ç½®ä¸‹è¡¨ç°æœ€ä½³ï¼Œè€Œé€»è¾‘å›å½’å’Œæ”¯æŒå‘é‡æœºä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¯´æ˜åœ¨æç¤ºä¼˜åŒ–åŒæ—¶ï¼Œå¯è§£é‡Šçš„ã€èµ„æºæ•ˆç‡é«˜çš„æ¨¡å‹ä»æ˜¯å®è´µçš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¾å°„å­¦ä»»åŠ¡ä¸­ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªæ ‡æ³¨çš„æ”¾å°„å­¦æŠ¥å‘Šè¯­æ–™åº“ï¼Œç”¨äºè¯„ä¼°éšè®¿ä¾ä»æ€§æ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ†ç±»å™¨å’Œæœ€æ–°ç”Ÿæˆçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>GPT-4oåœ¨é«˜çº§é…ç½®ä¸‹è¡¨ç°æœ€ä½³ï¼Œå±•ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠé¢†åŸŸçš„å¼ºå¤§èƒ½åŠ›ã€‚</li>
<li>é€»è¾‘å›å½’å’Œæ”¯æŒå‘é‡æœºç­‰ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œä»ç„¶æ˜¯é‡è¦çš„åŸºå‡†æ¨¡å‹ã€‚</li>
<li>æç¤ºä¼˜åŒ–å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5270a178fddb699c6ca6417e8f88f4b" align="middle">
<img src="https://picx.zhimg.com/v2-3691f81e2a0f56d7159f87219eae7823" align="middle">
<img src="https://picx.zhimg.com/v2-50b2e1f250e2d99097dfb4cf67104f1c" align="middle">
<img src="https://picx.zhimg.com/v2-1ddba41973026f7f18347fd91a40dae8" align="middle">
<img src="https://picx.zhimg.com/v2-e08c7f9745076acd2f30ca4e78664151" align="middle">
<img src="https://picx.zhimg.com/v2-199aa9c656b5591c4a959a753a5fc1fd" align="middle">
<img src="https://picx.zhimg.com/v2-8d6d0fdbe957b0b2f3059d0668b65ebb" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Ghost-in-the-Transformer-Tracing-LLM-Lineage-with-SVD-Fingerprint"><a href="#Ghost-in-the-Transformer-Tracing-LLM-Lineage-with-SVD-Fingerprint" class="headerlink" title="Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint"></a>Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint</h2><p><strong>Authors:Suqing Wang, Ziyang Ma, Xinyi Li, Zuchao Li</strong></p>
<p>Large Language Models (LLMs) have rapidly advanced and are widely adopted across diverse fields. Due to the substantial computational cost and data requirements of training from scratch, many developers choose to fine-tune or modify existing open-source models. While most adhere to open-source licenses, some falsely claim original training despite clear derivation from public models. This raises pressing concerns about intellectual property protection and highlights the need for reliable methods to verify model provenance. In this paper, we propose GhostSpec, a lightweight yet effective method for verifying LLM lineage without access to training data or modification of model behavior. Our approach constructs compact and robust fingerprints by applying singular value decomposition (SVD) to invariant products of internal attention weight matrices, effectively capturing the structural identity of a model. Unlike watermarking or output-based methods, GhostSpec is fully data-free, non-invasive, and computationally efficient. It demonstrates strong robustness to sequential fine-tuning, pruning, block expansion, and even adversarial transformations. Extensive experiments show that GhostSpec can reliably trace the lineage of transformed models with minimal overhead. By offering a practical solution for model verification and reuse tracking, our method contributes to the protection of intellectual property and fosters a transparent, trustworthy ecosystem for large-scale language models.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¿…é€Ÿå‘å±•å’Œå¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚ç”±äºä»å¤´å¼€å§‹è®­ç»ƒéœ€è¦å¤§é‡çš„è®¡ç®—æˆæœ¬å’Œæ•°æ®è¦æ±‚ï¼Œè®¸å¤šå¼€å‘è€…é€‰æ‹©å¾®è°ƒæˆ–ä¿®æ”¹ç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚è™½ç„¶å¤§å¤šæ•°äººéµå®ˆå¼€æºè®¸å¯ï¼Œä½†æœ‰äº›äººå´åœ¨æ˜æ˜¾æºäºå…¬å¼€æ¨¡å‹çš„æƒ…å†µä¸‹è™šå‡å£°ç§°åŸå§‹è®­ç»ƒï¼Œè¿™å¼•å‘äº†å…³äºçŸ¥è¯†äº§æƒä¿æŠ¤ç´§è¿«æ€§çš„å…³æ³¨ï¼Œå¹¶å¼ºè°ƒäº†éªŒè¯æ¨¡å‹æ¥æºçš„å¯é æ–¹æ³•çš„å¿…è¦æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GhostSpecï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®¿é—®è®­ç»ƒæ•°æ®æˆ–ä¿®æ”¹æ¨¡å‹è¡Œä¸ºå³å¯éªŒè¯LLMè¡€ç»Ÿçš„è½»é‡çº§æœ‰æ•ˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åº”ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰äºå†…éƒ¨æ³¨æ„åŠ›æƒé‡çŸ©é˜µçš„ä¸å˜ä¹˜ç§¯æ¥æ„å»ºç´§å‡‘ä¸”ç¨³å¥çš„æŒ‡çº¹ï¼Œæœ‰æ•ˆåœ°æ•è·æ¨¡å‹çš„ç»“æ„ç‰¹å¾ã€‚ä¸watermarkingæˆ–åŸºäºè¾“å‡ºçš„æ–¹æ³•ä¸åŒï¼ŒGhostSpecå®Œå…¨æ— éœ€æ•°æ®ã€æ— ä¾µå…¥æ€§ä¸”è®¡ç®—æ•ˆç‡é«˜ã€‚å®ƒå¯¹é¡ºåºå¾®è°ƒã€ä¿®å‰ªã€å—æ‰©å±•ç”šè‡³å¯¹æŠ—æ€§è½¬æ¢å…·æœ‰å¾ˆå¼ºçš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGhostSpecèƒ½å¤Ÿå¯é åœ°è¿½è¸ªç»è¿‡è½¬æ¢çš„æ¨¡å‹è¡€ç»Ÿï¼Œå¹¶ä¸”å‡ ä¹ä¸éœ€è¦é¢å¤–çš„å¼€é”€ã€‚é€šè¿‡ä¸ºæ¨¡å‹éªŒè¯å’Œé‡ç”¨è·Ÿè¸ªæä¾›å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰åŠ©äºä¿æŠ¤çŸ¥è¯†äº§æƒå¹¶ä¿ƒè¿›å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„é€æ˜å¯ä¿¡ç”Ÿæ€ç³»ç»Ÿçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06390v2">PDF</a> Accepted at AAAI 2026 (Oral)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å’Œå¹¿æ³›åº”ç”¨å¸¦æ¥äº†çŸ¥è¯†äº§æƒä¿æŠ¤çš„è¿«åˆ‡éœ€æ±‚ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGhostSpecçš„è½»é‡çº§ä¸”æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºåœ¨ä¸æ¥è§¦è®­ç»ƒæ•°æ®æˆ–æ”¹å˜æ¨¡å‹è¡Œä¸ºçš„æƒ…å†µä¸‹éªŒè¯LLMçš„å‡ºå¤„ã€‚GhostSpecé€šè¿‡åº”ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰äºå†…éƒ¨æ³¨æ„åŠ›æƒé‡çŸ©é˜µçš„ä¸å˜ç§¯æ„å»ºç´§å‡‘ä¸”ç¨³å¥çš„æŒ‡çº¹ï¼Œæœ‰æ•ˆæ•æ‰æ¨¡å‹çš„ç»“æ„ç‰¹å¾ã€‚è¯¥æ–¹æ³•å…·æœ‰æ•°æ®å®Œå…¨æ— å…³æ€§ã€éä¾µå…¥æ€§å’Œè®¡ç®—é«˜æ•ˆæ€§ï¼Œå¯¹è¿ç»­å¾®è°ƒã€ä¿®å‰ªã€å—æ‰©å±•ç”šè‡³å¯¹æŠ—æ€§è½¬æ¢å…·æœ‰å¼ºå¤§çš„ç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼ŒGhostSpecèƒ½å¤Ÿå¯é è¿½è¸ªæ¨¡å‹çš„å‡ºå¤„ï¼Œä¸ºçŸ¥è¯†äº§æƒä¿æŠ¤æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†äº§æƒä¿æŠ¤è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹çš„æ¥æºéªŒè¯æˆä¸ºä¸€é¡¹è¿«åˆ‡éœ€æ±‚ã€‚</li>
<li>GhostSpecæ–¹æ³•é€šè¿‡æ„å»ºæ¨¡å‹çš„ç»“æ„æŒ‡çº¹ï¼Œå®ç°äº†å¯¹LLMå‡ºå¤„çš„æœ‰æ•ˆéªŒè¯ã€‚</li>
<li>GhostSpecæ–¹æ³•é‡‡ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æŠ€æœ¯ï¼ŒåŸºäºå†…éƒ¨æ³¨æ„åŠ›æƒé‡çŸ©é˜µæ„å»ºæŒ‡çº¹ï¼Œå…·æœ‰æ•°æ®æ— å…³æ€§ã€éä¾µå…¥æ€§å’Œé«˜æ•ˆæ€§ã€‚</li>
<li>GhostSpecæ–¹æ³•å¯¹äºæ¨¡å‹çš„è¿ç»­å¾®è°ƒã€ä¿®å‰ªã€å—æ‰©å±•ç­‰æ“ä½œä»¥åŠå¯¹å¯¹æŠ—æ€§è½¬æ¢å…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒè¯æ˜GhostSpecèƒ½å¤Ÿå¯é è¿½è¸ªæ¨¡å‹çš„å‡ºå¤„ï¼Œä¸ºçŸ¥è¯†äº§æƒä¿æŠ¤æä¾›äº†æœ‰æ•ˆæ‰‹æ®µã€‚</li>
<li>GhostSpecæ–¹æ³•æœ‰åŠ©äºä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ç”Ÿæ€ç³»ç»Ÿå»ºè®¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-858dd2fb1b50c685daa9744f790128aa" align="middle">
<img src="https://picx.zhimg.com/v2-1d2dd20928a45670e367215333d5828e" align="middle">
<img src="https://picx.zhimg.com/v2-5970a3149a83e2cddf84099c8319c394" align="middle">
<img src="https://picx.zhimg.com/v2-7c528cd42412337d091813d0490ad4aa" align="middle">
<img src="https://picx.zhimg.com/v2-e09e4092c5e5514a4b40ead35a583099" align="middle">
<img src="https://picx.zhimg.com/v2-04b402407fa48bb0c1a69b3e0efe82f2" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Hybrid-Retrieval-Augmented-Generation-Agent-for-Trustworthy-Legal-Question-Answering-in-Judicial-Forensics"><a href="#Hybrid-Retrieval-Augmented-Generation-Agent-for-Trustworthy-Legal-Question-Answering-in-Judicial-Forensics" class="headerlink" title="Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics"></a>Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics</h2><p><strong>Authors:Yueqing Xi, Yifan Bai, Huasen Luo, Weiliang Wen, Hui Liu, Haoliang Li</strong></p>
<p>As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios.</p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½åœ¨å¸æ³•é‰´å®šä¸­çš„æ™®åŠï¼Œç¡®ä¿æ³•å¾‹é—®ç­”ï¼ˆQAï¼‰çš„çœŸå®æ€§å’Œå¯è¿½æº¯æ€§å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œå¯èƒ½å¯¼è‡´æ³•å¾‹å’¨è¯¢æ—¶äº§ç”Ÿè¯¯å¯¼æ€§æŒ‡å¯¼ï¼Œè€Œé™æ€çš„çŸ¥è¯†åº“éš¾ä»¥è·Ÿä¸Šé¢‘ç¹æ›´æ–°çš„æ³•å¾‹å’Œåˆ¤ä¾‹ã€‚æˆ‘ä»¬é’ˆå¯¹å¸æ³•ç¯å¢ƒé‡èº«å®šåˆ¶äº†ä¸€ç§æ··åˆæ³•å¾‹é—®ç­”ä»£ç†ï¼Œå®ƒå°†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸å¤šæ¨¡å‹é›†æˆç›¸ç»“åˆï¼Œä»¥æä¾›å¯é ã€å¯å®¡æ ¸å’ŒæŒç»­æ›´æ–°çš„å’¨è¯¢ã€‚è¯¥ç³»ç»Ÿä¼˜å…ˆæ£€ç´¢ç”Ÿæˆï¼šå½“å¯ä¿¡èµ–çš„æ³•å¾‹å­˜å‚¨åº“äº§ç”Ÿç›¸å…³è¯æ®æ—¶ï¼Œç­”æ¡ˆå°†é€šè¿‡RAGäº§ç”Ÿï¼›å¦åˆ™ï¼Œå¤šä¸ªLLMç”Ÿæˆå€™é€‰ç­”æ¡ˆï¼Œå¹¶ç”±ä¸“ç”¨é€‰æ‹©å™¨è¿›è¡Œè¯„åˆ†ï¼Œè¿”å›æ’åæœ€é«˜çš„ç­”æ¡ˆã€‚é«˜è´¨é‡è¾“å‡ºç„¶åç»è¿‡äººå·¥å®¡æ ¸åå†™å›åˆ°å­˜å‚¨åº“ï¼Œå®ç°åŠ¨æ€çŸ¥è¯†æ¼”å˜å’Œæ¥æºè·Ÿè¸ªã€‚åœ¨æ³•å¾‹QAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ··åˆæ–¹æ³•åœ¨ä¿¡æ¯ç²¾åº¦ã€ROUGE-Lå’ŒLLMä½œä¸ºæ³•å®˜çš„åº¦é‡æ ‡å‡†ä¸Šæ˜¾è‘—ä¼˜äºå•æ¨¡å‹åŸºå‡†å’Œç®€å•çš„RAGç®¡é“ã€‚æ¶ˆèå®éªŒè¯å®äº†æ£€ç´¢ä¼˜å…ˆã€æ¨¡å‹é›†æˆå’Œäººå·¥å¾ªç¯æ›´æ–°æœºåˆ¶çš„äº’è¡¥ä½œç”¨ã€‚æ‰€æå‡ºçš„ç³»ç»Ÿæ˜¾è‘—å‡å°‘äº†å¹»è§‰ï¼Œæé«˜äº†ç­”æ¡ˆè´¨é‡å’Œæ³•å¾‹åˆè§„æ€§ï¼Œä¿ƒè¿›äº†åª’ä½“å–è¯æŠ€æœ¯åœ¨å¸æ³•åœºæ™¯ä¸­çš„å®é™…åº”ç”¨è½åœ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01668v2">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€äººå·¥æ™ºèƒ½åœ¨å¸æ³•é‰´å®šä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç¡®ä¿æ³•å¾‹é—®ç­”ï¼ˆQAï¼‰çš„çœŸå®æ€§å’Œå¯è¿½æº¯æ€§è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®¹æ˜“å‡ºç°è™šæ„æƒ…å†µï¼Œè¯¯å¯¼æ³•å¾‹å’¨è¯¢ï¼Œè€Œé™æ€çŸ¥è¯†åº“éš¾ä»¥è·Ÿä¸Šä¸æ–­æ›´æ–°çš„æ³•å¾‹å’Œåˆ¤ä¾‹ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å¸æ³•ç¯å¢ƒçš„æ··åˆæ³•å¾‹é—®ç­”ä»£ç†ï¼Œå®ƒç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ¨¡å‹é›†æˆï¼Œæä¾›å¯é ã€å¯å®¡æ ¸å’ŒæŒç»­æ›´æ–°çš„å’¨è¯¢ã€‚è¯¥ç³»ç»Ÿä¼˜å…ˆæ£€ç´¢ç”Ÿæˆç­”æ¡ˆï¼Œå½“å¯é çš„æ³•åŠ¡åº“äº§ç”Ÿç›¸å…³è¯æ®æ—¶ï¼Œé€šè¿‡RAGç”Ÿæˆç­”æ¡ˆï¼›å¦åˆ™ï¼Œå¤šä¸ªLLMç”Ÿæˆå€™é€‰ç­”æ¡ˆï¼Œç”±ä¸“ä¸šé€‰æ‹©å™¨è¯„åˆ†å¹¶è¿”å›æ’åæœ€é«˜çš„ç­”æ¡ˆã€‚é«˜è´¨é‡è¾“å‡ºéœ€è¦ç»è¿‡äººå·¥å®¡æ ¸åå†™å…¥çŸ¥è¯†åº“ï¼Œå®ç°åŠ¨æ€çŸ¥è¯†æ¼”è¿›å’Œæº¯æºè¿½è¸ªã€‚åœ¨Law_QAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ··åˆæ–¹æ³•æ˜¾è‘—ä¼˜äºå•æ¨¡å‹åŸºå‡†å’ŒåŸºæœ¬çš„RAGç®¡é“ï¼Œæé«˜äº†F1ã€ROUGE-Lå’ŒLLM-as-a-JudgeæŒ‡æ ‡çš„è¯„ä¼°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨å¸æ³•é‰´å®šä¸­çš„åº”ç”¨è¦æ±‚ç¡®ä¿æ³•å¾‹é—®ç­”çš„çœŸå®æ€§å’Œå¯è¿½æº¯æ€§ã€‚</li>
<li>ä¼ ç»Ÿè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¸æ³•ç¯å¢ƒä¸­æ˜“äº§ç”Ÿè¯¯å¯¼ï¼Œéœ€æ”¹è¿›ã€‚</li>
<li>æ··åˆæ³•å¾‹é—®ç­”ä»£ç†é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ¨¡å‹é›†æˆæä¾›å¯é å’¨è¯¢ã€‚</li>
<li>ç³»ç»Ÿä¼˜å…ˆä»å¯é çš„æ³•åŠ¡åº“ä¸­æ£€ç´¢ç­”æ¡ˆï¼Œå¦åˆ™é€šè¿‡å¤šæ¨¡å‹ç”Ÿæˆå€™é€‰ç­”æ¡ˆå¹¶é€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚</li>
<li>é«˜è´¨é‡è¾“å‡ºéœ€ç»è¿‡äººå·¥å®¡æ ¸åå†™å…¥çŸ¥è¯†åº“ï¼Œå®ç°åŠ¨æ€çŸ¥è¯†æ›´æ–°å’Œæº¯æºã€‚</li>
<li>å®éªŒè¡¨æ˜æ··åˆæ–¹æ³•ä¼˜äºå•æ¨¡å‹åŸºå‡†å’ŒåŸºæœ¬çš„RAGç®¡é“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae8fd9cec8e10fff8b1a623cb033d89c" align="middle">
<img src="https://picx.zhimg.com/v2-068bd82b086b4992c61dac976ce19158" align="middle">
<img src="https://picx.zhimg.com/v2-c32b60ab5cd8171cc7acff22cb220c35" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training"><a href="#InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training" class="headerlink" title="InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training"></a>InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training</h2><p><strong>Authors:Pengkai Wang, Qi Zuo, Pengwei Liu, Zhijie Sang, Congkai Xie, Hongxia Yang</strong></p>
<p>Reinforcement learning has powered many of the recent breakthroughs in large language models, especially for tasks where rewards can be computed automatically, such as code generation. However, these methods deteriorate in open-ended domains like medical consultation, where feedback is inherently ambiguous, highly context-dependent, and cannot be reduced to a reliable scalar signal. In such settings, RL must either rely on supervision-intensive reward models that often fail to generalize, or it falls into pathological behaviors such as reward hacking - an especially troubling risk for high-stakes medical dialogue. To address these limitations, we introduce ORBIT, an open-ended rubric-based incremental training framework for high-stakes medical dialogue. ORBIT integrates synthetic dialogue generation with dynamically constructed rubrics that serve as adaptive guides for incremental RL. Instead of relying on external medical knowledge bases or handcrafted rule sets, ORBIT uses rubric-driven feedback to steer the learning process. Its judge component can be instantiated with general-purpose instruction-following LLMs, removing the need for any task-specific fine-tuning. Applied to the Qwen3-4B-Instruct model, ORBIT raises the HealthBench-Hard score from 7.0 to 27.5 using only 2k training samples, achieving SOTA performance for models at this scale. With larger rubric datasets, ORBIT-trained models further compete with the strongest open-source baselines on HealthBench-Hard. Our analysis shows that rubric-guided RL consistently improves consultation quality across diverse medical scenarios. We also apply such rubric generation and training pipeline to InfoBench, where ORBIT enhances instruction-following performance, highlighting the generality of rubric-based feedback.</p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æœ€è¿‘çš„è®¸å¤šçªç ´æä¾›äº†åŠ¨åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±å¯ä»¥è‡ªåŠ¨è®¡ç®—çš„ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚ä»£ç ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¼€æ”¾åŸŸï¼ˆå¦‚åŒ»ç–—å’¨è¯¢ï¼‰ä¸­è¡¨ç°ä¸ä½³ï¼Œå…¶ä¸­åé¦ˆæœ¬è´¨ä¸Šæ˜¯æ¨¡ç³Šçš„ã€é«˜åº¦ä¾èµ–äºä¸Šä¸‹æ–‡ï¼Œå¹¶ä¸”ä¸èƒ½ç®€åŒ–ä¸ºå¯é çš„æ ‡é‡ä¿¡å·ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ è¦ä¹ˆä¾èµ–äºç›‘ç£å¯†é›†å‹çš„å¥–åŠ±æ¨¡å‹ï¼ˆé€šå¸¸éš¾ä»¥æ¨å¹¿ï¼‰ï¼Œè¦ä¹ˆé™·å…¥ç—…ç†è¡Œä¸ºï¼ˆå¦‚å¥–åŠ±æ“çºµï¼‰â€”â€”å¯¹äºé«˜é£é™©åŒ»ç–—å¯¹è¯æ¥è¯´å°¤å…¶ä»¤äººæ‹…å¿§çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ORBITï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é«˜é£é™©åŒ»ç–—å¯¹è¯çš„å¼€æ”¾å¼è§„åˆ™åŸºç¡€å¢é‡è®­ç»ƒæ¡†æ¶ã€‚ORBITå°†åˆæˆå¯¹è¯ç”Ÿæˆä¸åŠ¨æ€æ„å»ºçš„è§„åˆ™ç›¸ç»“åˆï¼Œè¿™äº›è§„åˆ™ä½œä¸ºå¢é‡å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”æŒ‡å—ã€‚ORBITä¸ä¾èµ–å¤–éƒ¨åŒ»å­¦çŸ¥è¯†åº“æˆ–æ‰‹å·¥è§„åˆ™é›†ï¼Œè€Œæ˜¯ä½¿ç”¨è§„åˆ™é©±åŠ¨çš„åé¦ˆæ¥å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚å…¶åˆ¤æ–­ç»„ä»¶å¯ä»¥ä½¿ç”¨é€šç”¨æŒ‡ä»¤éµå¾ªå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå®ä¾‹åŒ–ï¼Œæ— éœ€ä»»ä½•ç‰¹å®šçš„ä»»åŠ¡å¾®è°ƒã€‚åœ¨Qwen3-4B-Instructæ¨¡å‹ä¸Šåº”ç”¨ORBITåï¼ŒHealthBench-Hardå¾—åˆ†ä»7.0æé«˜åˆ°27.5ï¼Œä»…ä½¿ç”¨2kè®­ç»ƒæ ·æœ¬å°±è¾¾åˆ°äº†æ­¤è§„æ¨¡æ¨¡å‹çš„æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚ä½¿ç”¨æ›´å¤§çš„è§„åˆ™æ•°æ®é›†ï¼ŒORBITè®­ç»ƒçš„æ¨¡å‹åœ¨HealthBench-Hardä¸Šè¿›ä¸€æ­¥ä¸å¼€æºåŸºçº¿å±•å¼€äº†ç«äº‰ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè§„åˆ™å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤šç§åŒ»å­¦åœºæ™¯ä¸­å§‹ç»ˆæé«˜äº†å’¨è¯¢è´¨é‡ã€‚æˆ‘ä»¬è¿˜å°†è¿™ç§è§„åˆ™ç”Ÿæˆå’ŒåŸ¹è®­ç®¡é“åº”ç”¨äºInfoBenchï¼Œå…¶ä¸­ORBITæé«˜äº†æŒ‡ä»¤éµå¾ªæ€§èƒ½ï¼Œçªæ˜¾äº†åŸºäºè§„åˆ™çš„åé¦ˆçš„æ™®éæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15859v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯ä»¥è‡ªåŠ¨è®¡ç®—å¥–åŠ±çš„ä»»åŠ¡ä¸­ï¼Œå¦‚ä»£ç ç”Ÿæˆï¼Œå–å¾—äº†è®¸å¤šçªç ´ã€‚ç„¶è€Œï¼Œåœ¨å¼€æ”¾é¢†åŸŸå¦‚åŒ»ç–—å’¨è¯¢ä¸­ï¼Œåé¦ˆå…·æœ‰å›ºæœ‰çš„æ¨¡ç³Šæ€§ã€é«˜åº¦ä¾èµ–äºä¸Šä¸‹æ–‡ï¼Œå¹¶ä¸”ä¸èƒ½ç®€åŒ–ä¸ºå¯é çš„æ ‡é‡ä¿¡å·ï¼Œè¿™äº›æ–¹æ³•çš„æ•ˆæœä¼šä¸‹é™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ å¿…é¡»ä¾èµ–ç›‘ç£å¯†é›†çš„å¥–åŠ±æ¨¡å‹ï¼Œé€šå¸¸æ— æ³•æ¦‚æ‹¬ï¼Œæˆ–è€…é™·å…¥ç—…ç†è¡Œä¸ºï¼Œå¦‚å¥–åŠ±ç ´è§£ï¼Œè¿™å¯¹é«˜é£é™©çš„åŒ»ç–—å¯¹è¯ç‰¹åˆ«ä»¤äººæ‹…å¿§ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ORBITï¼Œä¸€ä¸ªç”¨äºé«˜é£é™©åŒ»ç–—å¯¹è¯çš„å¼€æ”¾è§„åˆ™å¢é‡è®­ç»ƒæ¡†æ¶ã€‚ORBITé›†æˆäº†åˆæˆå¯¹è¯ç”Ÿæˆä¸åŠ¨æ€æ„å»ºçš„è§„åˆ™ï¼Œä½œä¸ºå¢é‡å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”æŒ‡å—ã€‚å®ƒä¸éœ€è¦ä¾èµ–å¤–éƒ¨åŒ»å­¦çŸ¥è¯†åº“æˆ–æ‰‹å·¥è§„åˆ™é›†ï¼Œè€Œæ˜¯ä½¿ç”¨è§„åˆ™é©±åŠ¨çš„åé¦ˆæ¥å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚å…¶æ³•å®˜ç»„ä»¶å¯ä»¥ä½¿ç”¨é€šç”¨æŒ‡ä»¤è·Ÿéšå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå®ä¾‹åŒ–ï¼Œæ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒã€‚åœ¨Qwen3-4B-Instructæ¨¡å‹çš„åº”ç”¨ä¸­ï¼ŒORBITå°†HealthBench-Hardå¾—åˆ†ä»7.0æé«˜åˆ°27.5ï¼Œä»…ä½¿ç”¨2kè®­ç»ƒæ ·æœ¬ï¼Œåœ¨æ¨¡å‹è§„æ¨¡ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚ä½¿ç”¨æ›´å¤§çš„è§„åˆ™æ•°æ®é›†ï¼ŒORBITè®­ç»ƒçš„æ¨¡å‹è¿›ä¸€æ­¥åœ¨HealthBench-Hardä¸Šä¸æœ€å¼ºå¤§çš„å¼€æºåŸºçº¿ç«äº‰ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè§„åˆ™æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤šç§åŒ»å­¦åœºæ™¯ä¸­å§‹ç»ˆæé«˜äº†å’¨è¯¢è´¨é‡ã€‚æˆ‘ä»¬è¿˜å°†è¿™ç§è§„åˆ™ç”Ÿæˆå’ŒåŸ¹è®­ç®¡é“åº”ç”¨äºInfoBenchï¼Œå…¶ä¸­ORBITå¢å¼ºäº†æŒ‡ä»¤è·Ÿéšæ€§èƒ½ï¼Œçªå‡ºäº†è§„åˆ™åé¦ˆçš„æ™®éæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨è®¡ç®—å¥–åŠ±çš„ä»»åŠ¡ï¼Œå¦‚ä»£ç ç”Ÿæˆã€‚</li>
<li>åœ¨å¼€æ”¾é¢†åŸŸå¦‚åŒ»ç–—å’¨è¯¢ä¸­ï¼Œåé¦ˆå…·æœ‰æ¨¡ç³Šæ€§å’Œé«˜åº¦ä¸Šä¸‹æ–‡ä¾èµ–æ€§ï¼Œä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•è¡¨ç°ä¸ä½³ã€‚</li>
<li>é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†ORBITæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†åˆæˆå¯¹è¯ç”Ÿæˆå’ŒåŠ¨æ€è§„åˆ™ï¼Œä¸ºé«˜é£é™©åŒ»ç–—å¯¹è¯æä¾›å¢é‡è®­ç»ƒã€‚</li>
<li>ORBITé€šè¿‡ä½¿ç”¨è§„åˆ™é©±åŠ¨çš„åé¦ˆæ¥å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨åŒ»å­¦çŸ¥è¯†åº“æˆ–æ‰‹å·¥è§„åˆ™é›†ã€‚</li>
<li>ORBITå¯ä»¥æé«˜å’¨è¯¢è´¨é‡ï¼Œåœ¨å¤šç§åŒ»å­¦åœºæ™¯ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>å°†ORBITåº”ç”¨äºQwen3-4B-Instructæ¨¡å‹ï¼Œä½¿ç”¨ä»…2kè®­ç»ƒæ ·æœ¬å³å¯æ˜¾è‘—æé«˜HealthBench-Hardå¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9cc96e344d11db5aadb56f695029c4da" align="middle">
<img src="https://picx.zhimg.com/v2-975802c4724253004d29774cb61f8a9b" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-025ccf26e259c60a13108aff633b8b39" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Live-SWE-agent Can Software Engineering Agents Self-Evolve on the Fly?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8debf26c5852befb184a55e778aa7a61" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Crossing Borders A Multimodal Challenge for Indian Poetry Translation and Image Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
