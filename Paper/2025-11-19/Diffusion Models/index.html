<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Free-Form Scene Editor Enabling Multi-Round Object Manipulation like in a 3D Engine">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2569534174f2934f2d2a780b02a6d8ef')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="Free-Form-Scene-Editor-Enabling-Multi-Round-Object-Manipulation-like-in-a-3D-Engine"><a href="#Free-Form-Scene-Editor-Enabling-Multi-Round-Object-Manipulation-like-in-a-3D-Engine" class="headerlink" title="Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine"></a>Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine</h2><p><strong>Authors:Xincheng Shuai, Zhenyuan Qin, Henghui Ding, Dacheng Tao</strong></p>
<p>Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.</p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•åœ¨è¯­ä¹‰å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•åœ¨3Dæ„ŸçŸ¥å¯¹è±¡æ“ä½œæ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FFSEï¼Œè¿™æ˜¯ä¸€ä¸ª3Dæ„ŸçŸ¥çš„è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°åœ¨çœŸå®ä¸–ç•Œå›¾åƒä¸Šç›´æ¥è¿›è¡Œç›´è§‚ã€ç‰©ç†ä¸€è‡´çš„å¯¹è±¡ç¼–è¾‘ã€‚ä¸åŒäºä¹‹å‰åœ¨å›¾åƒç©ºé—´æ“ä½œæˆ–éœ€è¦ç¼“æ…¢ä¸”æ˜“å‡ºé”™çš„3Dé‡å»ºçš„æ–¹æ³•ï¼ŒFFSEå°†ç¼–è¾‘å»ºæ¨¡ä¸ºä¸€ç³»åˆ—å­¦ä¹ çš„3Dè½¬æ¢ï¼Œå…è®¸ç”¨æˆ·æ‰§è¡Œä»»æ„æ“ä½œï¼Œå¦‚å¹³ç§»ã€ç¼©æ”¾å’Œæ—‹è½¬ï¼ŒåŒæ—¶ä¿æŒèƒŒæ™¯æ•ˆæœï¼ˆå¦‚é˜´å½±ã€åå°„ï¼‰çš„çœŸå®æ€§ï¼Œå¹¶åœ¨å¤šæ¬¡ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿æŒå…¨å±€åœºæ™¯çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†æ”¯æŒå¤šè½®3Dæ„ŸçŸ¥å¯¹è±¡æ“ä½œçš„å­¦ä¹ ï¼Œæˆ‘ä»¬å¼•å…¥äº†3DObjectEditorï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆæ•°æ®é›†ï¼Œç”±å„ç§å¯¹è±¡å’Œåœºæ™¯æ¨¡æ‹Ÿç¼–è¾‘åºåˆ—æ„å»ºè€Œæˆï¼Œèƒ½å¤Ÿåœ¨å¤šè½®å’ŒåŠ¨æ€æ¡ä»¶ä¸‹è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„FFSEåœ¨å•è½®å’Œå¤šè½®3Dæ„ŸçŸ¥ç¼–è¾‘åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13713v1">PDF</a> AAAI 2026, Project Page: <a target="_blank" rel="noopener" href="https://henghuiding.com/FFSE/">https://henghuiding.com/FFSE/</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æå¤§åœ°æ¨åŠ¨äº†è¯­ä¹‰å›¾åƒç¼–è¾‘çš„å‘å±•ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•åœ¨å®ç°3Dæ„ŸçŸ¥ç‰©ä½“æ“ä½œæ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFFSEçš„3Dæ„ŸçŸ¥è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç›´è§‚ã€ç‰©ç†ä¸€è‡´çš„ç‰©ä½“ç¼–è¾‘ï¼Œå¯ç›´æ¥åº”ç”¨äºçœŸå®ä¸–ç•Œå›¾åƒã€‚ä¸åŒäºåœ¨å›¾åƒç©ºé—´æ“ä½œæˆ–éœ€è¦ç¼“æ…¢ã€å®¹æ˜“å‡ºç°é”™è¯¯çš„3Dé‡å»ºæ–¹æ³•ï¼ŒFFSEå°†ç¼–è¾‘å»ºæ¨¡ä¸ºä¸€ç³»åˆ—å­¦ä¹ çš„3Dè½¬æ¢ï¼Œå…è®¸ç”¨æˆ·è¿›è¡Œä»»æ„æ“ä½œï¼Œå¦‚å¹³ç§»ã€ç¼©æ”¾å’Œæ—‹è½¬ï¼ŒåŒæ—¶ä¿æŒèƒŒæ™¯æ•ˆæœï¼ˆå¦‚é˜´å½±ã€åå°„ï¼‰çš„çœŸå®æ€§ï¼Œå¹¶åœ¨å¤šæ¬¡ç¼–è¾‘ä¸­ä¿æŒå…¨å±€åœºæ™¯ä¸€è‡´æ€§ã€‚ä¸ºäº†æ”¯æŒå¤šè½®æ„ŸçŸ¥ç‰©ä½“çš„å­¦ä¹ ç¼–è¾‘ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†æ•°æ®é›†â€”â€”ç»“åˆæ¨¡æ‹Ÿçš„ç¼–è¾‘åºåˆ—ç”¨äºè®­ç»ƒå¤æ‚çš„æ„ŸçŸ¥æ¨¡å‹å’Œæƒ…å¢ƒåˆ›å»ºç­‰ç›®çš„æ„å»ºèµ·æ¥çš„ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤šè½®å’Œå¤šåŠ¨æ€æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFFSEåœ¨å•è½®å’Œå¤šè½®æ„ŸçŸ¥ç‰©ä½“ç¼–è¾‘åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨è¯­ä¹‰å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—è¿›å±•ã€‚</li>
<li>å½“å‰æ–¹æ³•éš¾ä»¥è¿›è¡Œå¤šè½®æ„ŸçŸ¥ç‰©ä½“ç¼–è¾‘ï¼Œå°¤å…¶æ˜¯æ¶‰åŠçœŸå®ä¸–ç•Œå›¾åƒçš„åœºæ™¯ã€‚</li>
<li>FFSEæ¡†æ¶å®ç°äº†ç›´è§‚ã€ç‰©ç†ä¸€è‡´çš„ç‰©ä½“ç¼–è¾‘ï¼Œç›´æ¥åœ¨çœŸå®ä¸–ç•Œå›¾åƒä¸Šåº”ç”¨ã€‚</li>
<li>FFSEå°†ç¼–è¾‘å»ºæ¨¡ä¸ºä¸€ç³»åˆ—å­¦ä¹ çš„3Dè½¬æ¢ï¼Œå…è®¸ä»»æ„æ“ä½œå¦‚å¹³ç§»ã€ç¼©æ”¾å’Œæ—‹è½¬ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e486ec630255a9e088d4dcef0df4d2b" align="middle">
<img src="https://picx.zhimg.com/v2-e2057da5dc3605910014efe68f3c499c" align="middle">
<img src="https://picx.zhimg.com/v2-30ce8ae4a7b0023ac92f38006a856676" align="middle">
<img src="https://picx.zhimg.com/v2-4649d3cb51767b6901c20a70be5cf876" align="middle">
<img src="https://picx.zhimg.com/v2-047708640f62851445a559dc7e453b84" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Crossing-Borders-A-Multimodal-Challenge-for-Indian-Poetry-Translation-and-Image-Generation"><a href="#Crossing-Borders-A-Multimodal-Challenge-for-Indian-Poetry-Translation-and-Image-Generation" class="headerlink" title="Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation"></a>Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation</h2><p><strong>Authors:Sofia Jamil, Kotla Sai Charan, Sriparna Saha, Koustava Goswami, Joseph K J</strong></p>
<p>Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the readerâ€™s experience.</p>
<blockquote>
<p>å°åº¦è¯—æ­Œä»¥å…¶è¯­è¨€å¤æ‚æ€§å’Œæ·±åšçš„æ–‡åŒ–å…±é¸£è€Œé—»åï¼Œæ‹¥æœ‰è·¨è¶Šæ•°åƒå¹´çš„ä¸°å¯Œè€Œå¤šæ ·çš„é—äº§ã€‚ç„¶è€Œï¼Œå…¶å±‚æ¬¡ä¸°å¯Œçš„å«ä¹‰ã€æ–‡åŒ–å…¸æ•…å’Œå¤æ‚çš„è¯­æ³•ç»“æ„å¸¸å¸¸æ„æˆç†è§£ä¸Šçš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹äºéæ¯è¯­è€…æˆ–å¯¹å…¶è¯­å¢ƒå’Œè¯­è¨€ä¸ç†Ÿæ‚‰çš„è¯»è€…ã€‚å°½ç®¡å…¶åœ¨æ–‡åŒ–ä¸Šå…·æœ‰é‡è¦æ€§ï¼Œä½†ç°æœ‰å…³äºè¯—æ­Œçš„ä½œå“å¤§å¤šå¿½è§†äº†å°åº¦è¯­è¨€è¯—æ­Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¿»è¯‘å’Œå›¾åƒç”Ÿæˆï¼ˆTAIï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é€‚å½“çš„æç¤ºè°ƒæ•´ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒè”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡ä¸­çš„ä¼˜è´¨æ•™è‚²ï¼ˆSDG 4ï¼‰å’Œå‡å°‘ä¸å¹³ç­‰ï¼ˆSDG 10ï¼‰ï¼Œé€šè¿‡æé«˜æ–‡åŒ–ä¸°å¯Œçš„å°åº¦è¯­è¨€è¯—æ­Œçš„æ™®åŠæ€§ï¼Œé¢å‘å…¨çƒå—ä¼—ã€‚å®ƒåŒ…æ‹¬ï¼ˆ1ï¼‰ä¸€ä¸ªç¿»è¯‘æ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨Odds Ratio Preference Alignment Algorithmç®—æ³•ï¼Œå°†å½¢æ€ä¸°å¯Œçš„è¯—æ­Œå‡†ç¡®åœ°ç¿»è¯‘æˆè‹±è¯­ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªå›¾åƒç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—é‡‡ç”¨è¯­ä¹‰å›¾æ¥æ•è·æ ‡è®°ã€ä¾èµ–å…³ç³»å’Œéšå–»åŠå…¶æ„ä¹‰ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä»¥åˆ›å»ºå°åº¦è¯—æ­Œçš„è§†è§‰æ„ä¹‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å…¨é¢å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬äººç±»å’Œå®šé‡è¯„ä¼°ï¼Œè¯æ˜äº†TAI Diffusionåœ¨è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³å°åº¦è¯­è¨€è¯—æ­Œèµ„æºç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å½¢æ€ä¸°å¯Œå°åº¦è¯­è¨€è¯—æ­ŒMorphoVerseæ•°æ®é›†ï¼ŒåŒ…å«1570é¦–è·¨è¶Š21ç§ä½èµ„æºå°åº¦è¯­è¨€çš„è¯—æ­Œã€‚é€šè¿‡è§£å†³è¯—æ­Œç¿»è¯‘å’Œè§†è§‰ç†è§£ä¹‹é—´çš„å·®è·ï¼Œè¿™é¡¹å·¥ä½œæ—¨åœ¨æé«˜æ™®åŠæ€§å¹¶ä¸°å¯Œè¯»è€…çš„ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13689v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºç¿»è¯‘ä¸å›¾åƒç”Ÿæˆï¼ˆTAIï¼‰æ¡†æ¶ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡é€‚å½“çš„æç¤ºè°ƒæ•´ï¼Œæ”¯æŒå°åº¦è¯—æ­Œçš„ç¿»è¯‘å’Œå›¾åƒç”Ÿæˆã€‚è¯¥æ¡†æ¶æ—¨åœ¨æé«˜ä¸°å¯Œæ–‡åŒ–å†…è•´çš„å°åº¦è¯­è¨€è¯—æ­Œå¯¹å…¨çƒå—ä¼—çš„æ™®åŠæ€§ï¼ŒåŒ…æ‹¬ç¿»è¯‘æ¨¡å—å’Œå›¾åƒç”Ÿæˆæ¨¡å—ã€‚å‰è€…ä½¿ç”¨Odds Ratio Preference Alignment Algorithmå‡†ç¡®ç¿»è¯‘å½¢æ€ä¸°å¯Œçš„è¯—æ­Œï¼Œåè€…é‡‡ç”¨è¯­ä¹‰å›¾æ•æ‰è¯—æ­Œä¸­çš„æ ‡è®°ã€ä¾èµ–å…³ç³»å’Œéšå–»ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä¸ºå°åº¦è¯—æ­Œåˆ›å»ºè§†è§‰ä¸Šæœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒTAIæ¡†æ¶åœ¨è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³å°åº¦è¯­è¨€è¯—æ­Œèµ„æºç¨€ç¼ºçš„é—®é¢˜ï¼Œè¿˜æ¨å‡ºäº†å½¢æ€ä¸°å¯Œå°åº¦è¯­è¨€è¯—æ­ŒMorphoVerseæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°åº¦è¯—æ­Œå…·æœ‰å¤æ‚è¯­è¨€å’Œæ·±åšæ–‡åŒ–èƒŒæ™¯ï¼Œå¯¹äºéæ¯è¯­è€…æˆ–ä¸äº†è§£å…¶è¯­å¢ƒå’Œè¯­è¨€çš„è¯»è€…æ¥è¯´ï¼Œç†è§£å…¶å«ä¹‰å¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰å¯¹è¯—æ­Œçš„ç ”ç©¶å¤§å¤šå¿½ç•¥äº†å°åº¦è¯­è¨€è¯—æ­Œã€‚</li>
<li>æå‡ºäº†ç¿»è¯‘ä¸å›¾åƒç”Ÿæˆï¼ˆTAIï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬ç¿»è¯‘æ¨¡å—å’Œå›¾åƒç”Ÿæˆæ¨¡å—ï¼Œæ—¨åœ¨æé«˜å°åº¦è¯­è¨€è¯—æ­Œçš„å…¨çƒæ™®åŠæ€§ã€‚</li>
<li>ç¿»è¯‘æ¨¡å—é‡‡ç”¨Odds Ratio Preference Alignment Algorithmå‡†ç¡®ç¿»è¯‘å½¢æ€ä¸°å¯Œçš„è¯—æ­Œã€‚</li>
<li>å›¾åƒç”Ÿæˆæ¨¡å—é‡‡ç”¨è¯­ä¹‰å›¾æŠ€æœ¯ï¼Œä¸ºå°åº¦è¯—æ­Œåˆ›å»ºè§†è§‰ä¸Šæœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚</li>
<li>ç»¼åˆå®éªŒè¯„ä¼°è¯æ˜TAIæ¡†æ¶åœ¨è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe8f56b640106e849238894ae71f84f3" align="middle">
<img src="https://picx.zhimg.com/v2-fcf6490258e970f26ba6a174333559a7" align="middle">
<img src="https://picx.zhimg.com/v2-b7b13598440fc3707843a91a6a8c965d" align="middle">
<img src="https://picx.zhimg.com/v2-2f02d6dcf4b57a639f1aa397320af375" align="middle">
<img src="https://picx.zhimg.com/v2-9d5028d10aa948a9debf3fbabe5f23d7" align="middle">
<img src="https://picx.zhimg.com/v2-2569534174f2934f2d2a780b02a6d8ef" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Training-Free-Multi-View-Extension-of-IC-Light-for-Textual-Position-Aware-Scene-Relighting"><a href="#Training-Free-Multi-View-Extension-of-IC-Light-for-Textual-Position-Aware-Scene-Relighting" class="headerlink" title="Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting"></a>Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting</h2><p><strong>Authors:Jiangnan Ye, Jiedong Zhuang, Lianrui Mu, Wenjie Zheng, Jiaqi Hu, Xingze Zou, Jing Wang, Haoji Hu</strong></p>
<p>We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†GS-Lightï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆã€å¯¹æ–‡æœ¬ä½ç½®æ•æ„Ÿçš„ç®¡é“ï¼Œç”¨äºå¯¹é€šè¿‡é«˜æ–¯æ‹¼è´´(3DGS)è¡¨ç¤ºçš„3Dåœºæ™¯è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„é‡ç…§æ˜ã€‚GS-Lightå®ç°äº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å•è¾“å…¥æ‰©æ•£æ¨¡å‹çš„æ‰©å±•ï¼Œä»¥å¤„ç†å¤šè§†å›¾è¾“å…¥ã€‚ç»™å®šç”¨æˆ·æç¤ºï¼Œå¯èƒ½æŒ‡å®šç…§æ˜æ–¹å‘ã€é¢œè‰²ã€å¼ºåº¦æˆ–å‚è€ƒå¯¹è±¡ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å°†æç¤ºè§£æä¸ºç…§æ˜å…ˆéªŒã€‚æˆ‘ä»¬ä½¿ç”¨ç°æˆçš„ä¼°è®¡å™¨è¿›è¡Œå‡ ä½•å’Œè¯­ä¹‰ï¼ˆæ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ï¼Œå°†è¿™äº›ç…§æ˜å…ˆéªŒä¸è§†å›¾å‡ ä½•çº¦æŸèåˆï¼Œä»¥è®¡ç®—ç…§æ˜åœ°å›¾å¹¶ä¸ºæ¯ä¸ªè§†å›¾ç”Ÿæˆåˆå§‹æ½œåœ¨ä»£ç ã€‚è¿™äº›ç²¾å¿ƒæ¨å¯¼çš„åˆå§‹æ½œç å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®åæ˜ ç”¨æˆ·æœŸæœ›çš„é‡ç…§æ˜è¾“å‡ºï¼Œå°¤å…¶æ˜¯åœ¨ç…§æ˜æ–¹å‘æ–¹é¢ã€‚é€šè¿‡å°†å¤šè§†å›¾æ¸²æŸ“å›¾åƒå’Œåˆå§‹æ½œç è¾“å…¥åˆ°æˆ‘ä»¬çš„å¤šè§†å›¾é‡ç…§æ˜æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é«˜ä¿çœŸã€è‰ºæœ¯åŒ–çš„é‡ç…§æ˜å›¾åƒã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨é‡ç…§æ˜çš„å¤–è§‚å¯¹3DGSåœºæ™¯è¿›è¡Œå¾®è°ƒï¼Œä»¥è·å¾—å®Œå…¨é‡ç…§æ˜çš„3Dåœºæ™¯ã€‚æˆ‘ä»¬åœ¨å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸Šè¯„ä¼°äº†GS-Lightï¼Œå°†å…¶ä¸æœ€æ–°æŠ€æœ¯åŸºå‡†çº¿ï¼ˆåŒ…æ‹¬è§†å›¾é‡ç…§æ˜ã€è§†é¢‘é‡ç…§æ˜å’Œåœºæ™¯ç¼–è¾‘æ–¹æ³•ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚ä½¿ç”¨å®šé‡æŒ‡æ ‡ï¼ˆå¤šè§†å›¾ä¸€è‡´æ€§ã€æˆåƒè´¨é‡ã€ç¾å­¦åˆ†æ•°ã€è¯­ä¹‰ç›¸ä¼¼æ€§ç­‰ï¼‰å’Œå®šæ€§è¯„ä¼°ï¼ˆç”¨æˆ·ç ”ç©¶ï¼‰ï¼ŒGS-Lightåœ¨åŸºå‡†çº¿ä¹‹ä¸Šè¡¨ç°å‡ºæŒç»­æ”¹è¿›ã€‚ä»£ç å’Œèµ„æºå°†åœ¨å‘å¸ƒæ—¶æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13684v1">PDF</a> Submitting for Neurocomputing</p>
<p><strong>æ‘˜è¦</strong></p>
<p>GS-Lightæ˜¯ä¸€ç§é«˜æ•ˆã€æ–‡æœ¬å¼•å¯¼çš„ä¸‰ç»´åœºæ™¯é‡æ–°ç…§æ˜æ–¹æ³•ã€‚å®ƒé€šè¿‡å¤šè§†å›¾è¾“å…¥ï¼Œæ— éœ€è®­ç»ƒæ‰©å±•äº†å•è¾“å…¥æ‰©æ•£æ¨¡å‹ã€‚ç»™å®šç”¨æˆ·æç¤ºå…³äºç…§æ˜æ–¹å‘ã€é¢œè‰²ã€å¼ºåº¦æˆ–å‚è€ƒå¯¹è±¡çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å°†æç¤ºè§£æä¸ºç…§æ˜å…ˆéªŒã€‚ç»“åˆç°æˆçš„å‡ ä½•å’Œè¯­ä¹‰ä¼°è®¡å™¨ï¼ˆæ·±åº¦ã€è¡¨é¢æ³•çº¿å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ï¼Œæˆ‘ä»¬å°†è¿™äº›ç…§æ˜å…ˆéªŒä¸è§†å›¾å‡ ä½•çº¦æŸèåˆï¼Œè®¡ç®—ç…§æ˜åœ°å›¾å¹¶ä¸ºæ¯ä¸ªè§†å›¾ç”Ÿæˆåˆå§‹æ½œåœ¨ä»£ç ã€‚è¿™äº›ç²¾å¿ƒå¾—å‡ºçš„åˆå§‹æ½œåœ¨ä»£ç æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®åœ°åæ˜ ç”¨æˆ·æœŸæœ›çš„é‡æ–°ç…§æ˜è¾“å‡ºï¼Œå°¤å…¶æ˜¯åœ¨ç…§æ˜æ–¹å‘æ–¹é¢ã€‚é€šè¿‡å¤šè§†å›¾æ¸²æŸ“å›¾åƒå’Œåˆå§‹æ½œåœ¨ä»£ç ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é«˜è´¨é‡çš„è‰ºæœ¯é‡æ–°ç…§æ˜å›¾åƒã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ä½¿ç”¨é‡æ–°ç…§æ˜å¤–è§‚çš„3DGSåœºæ™¯è¿›è¡Œå¾®è°ƒï¼Œä»¥è·å¾—å®Œå…¨é‡æ–°ç…§æ˜çš„ä¸‰ç»´åœºæ™¯ã€‚GS-Lightåœ¨å®¤å†…å¤–åœºæ™¯ä¸Šçš„è¡¨ç°å‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åŸºå‡†çº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬è§†å›¾é‡æ–°ç…§æ˜ã€è§†é¢‘é‡æ–°ç…§æ˜å’Œåœºæ™¯ç¼–è¾‘æ–¹æ³•ã€‚é€šè¿‡å®šé‡æŒ‡æ ‡ï¼ˆå¤šè§†å›¾ä¸€è‡´æ€§ã€æˆåƒè´¨é‡ã€ç¾å­¦è¯„åˆ†ã€è¯­ä¹‰ç›¸ä¼¼æ€§ï¼‰å’Œå®šæ€§è¯„ä¼°ï¼ˆç”¨æˆ·ç ”ç©¶ï¼‰ï¼ŒGS-Lightå±•ç°å‡ºå¯¹åŸºå‡†çº¿çš„ä¸€è‡´æ”¹è¿›ã€‚å°†åœ¨å‘è¡¨æ—¶å…¬å¼€ä»£ç å’Œèµ„æºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GS-Lightæ˜¯ä¸€ç§ç”¨äºæ–‡æœ¬å¼•å¯¼çš„ä¸‰ç»´åœºæ™¯é‡æ–°ç…§æ˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å¤šè§†å›¾è¾“å…¥ï¼ŒGS-Lightæ‰©å±•äº†å•è¾“å…¥æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ï¼Œèƒ½å¤Ÿè§£æç”¨æˆ·å…³äºç…§æ˜æ–¹å‘çš„æç¤ºã€‚</li>
<li>ç»“åˆå‡ ä½•å’Œè¯­ä¹‰ä¼°è®¡å™¨è®¡ç®—ç…§æ˜åœ°å›¾ï¼Œç”Ÿæˆæ¯ä¸ªè§†å›¾çš„åˆå§‹æ½œåœ¨ä»£ç ã€‚</li>
<li>é‡æ–°ç…§æ˜è¾“å‡ºåæ˜ ç”¨æˆ·æœŸæœ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç…§æ˜æ–¹å‘æ–¹é¢ã€‚</li>
<li>é€šè¿‡å¤šè§†å›¾æ¸²æŸ“å›¾åƒå’Œåˆå§‹æ½œåœ¨ä»£ç ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è‰ºæœ¯é‡æ–°ç…§æ˜å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5de1aded6bd287d2278b1b06d3ab4d1e" align="middle">
<img src="https://picx.zhimg.com/v2-cf50de6bd2a2b021c48557a52d59e1ee" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generalized-Denoising-Diffusion-Codebook-Models-gDDCM-Tokenizing-images-using-a-pre-trained-diffusion-model"><a href="#Generalized-Denoising-Diffusion-Codebook-Models-gDDCM-Tokenizing-images-using-a-pre-trained-diffusion-model" class="headerlink" title="Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model"></a>Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model</h2><p><strong>Authors:Fei Kong</strong></p>
<p>Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.</p>
<blockquote>
<p>æœ€è¿‘ï¼Œæå‡ºäº†å»å™ªæ‰©æ•£ç¼–ç æœ¬æ¨¡å‹ï¼ˆDDCMï¼‰ã€‚DDCMåˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œæ ¹æ®é¢„è®¾è§„åˆ™ï¼Œå°†åå‘è¿‡ç¨‹ä¸­çš„éšæœºå™ªå£°æ›¿æ¢ä¸ºä»ç‰¹å®šé›†åˆä¸­é‡‡æ ·çš„å™ªå£°ï¼Œä»è€Œå®ç°å›¾åƒå‹ç¼©ã€‚ä½†æ˜¯ï¼ŒDDCMä¸èƒ½åº”ç”¨äºé™¤DDPMä¹‹å¤–çš„æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¹¿ä¹‰å»å™ªæ‰©æ•£å‹ç¼©æ¨¡å‹ï¼ˆgDDCMï¼‰ï¼Œå®ƒå°†DDCMæ‰©å±•åˆ°ä¸»æµæ‰©æ•£æ¨¡å‹åŠå…¶å˜ä½“ï¼ŒåŒ…æ‹¬DDPMã€åŸºäºåˆ†æ•°çš„æ¨¡å‹ã€ä¸€è‡´æ€§æ¨¡å‹å’Œæ ¡æ­£æµã€‚æˆ‘ä»¬åœ¨CIFAR-10å’ŒLSUNå§å®¤æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å°†DDCMæ¨å¹¿åˆ°äº†ä¸Šè¿°æ¨¡å‹ï¼Œå¹¶å®ç°äº†æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13387v1">PDF</a> in Chinese language</p>
<p><strong>Summary</strong><br>     è¿‘æœŸæå‡ºäº†å»å™ªæ‰©æ•£ç¼–ç æœ¬æ¨¡å‹ï¼ˆDDCMï¼‰ï¼Œå®ƒåˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰å¹¶æ›¿æ¢åå‘è¿‡ç¨‹ä¸­çš„éšæœºå™ªå£°ï¼Œé‡‡ç”¨ç‰¹å®šé›†åˆé‡‡æ ·å™ªå£°çš„æ–¹å¼å®ç°å›¾åƒå‹ç¼©ã€‚æœ¬ç ”ç©¶è¿›ä¸€æ­¥æ¨å¹¿äº†DDCMè‡³ä¸»æµæ‰©æ•£æ¨¡å‹åŠå…¶å˜ä½“ï¼ŒåŒ…æ‹¬DDPMã€åŸºäºåˆ†æ•°çš„æ¨¡å‹ã€ä¸€è‡´æ€§æ¨¡å‹å’Œæ ¡æ­£æµï¼Œå¹¶åœ¨CIFAR-10å’ŒLSUNå§å®¤æ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æˆåŠŸæ€§ï¼Œå®ç°äº†æ€§èƒ½çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DDCMåˆ©ç”¨DDPMæ¨¡å‹ï¼Œé€šè¿‡æ›¿æ¢åå‘è¿‡ç¨‹ä¸­çš„éšæœºå™ªå£°å®ç°å›¾åƒå‹ç¼©ã€‚</li>
<li>gDDCMæ˜¯DDCMçš„å¹¿ä¹‰ç‰ˆæœ¬ï¼Œé€‚ç”¨äºä¸»æµæ‰©æ•£æ¨¡å‹åŠå…¶å˜ä½“ï¼ŒåŒ…æ‹¬DDPMã€åŸºäºåˆ†æ•°çš„æ¨¡å‹ã€ä¸€è‡´æ€§æ¨¡å‹å’Œæ ¡æ­£æµã€‚</li>
<li>gDDCMåœ¨CIFAR-10å’ŒLSUNå§å®¤æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒgDDCMæˆåŠŸæ¨å¹¿äº†DDCMè‡³å…¶ä»–æ¨¡å‹ï¼Œå¹¶å®ç°äº†æ€§èƒ½çš„æå‡ã€‚</li>
<li>gDDCMçš„æå‡ºæ‰©å¤§äº†å›¾åƒå‹ç¼©çš„åº”ç”¨èŒƒå›´ï¼Œä¸ºä¸åŒç±»å‹çš„æ‰©æ•£æ¨¡å‹æä¾›äº†ç»Ÿä¸€çš„å‹ç¼©æ¡†æ¶ã€‚</li>
<li>å»å™ªæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå‹ç¼©é¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼å’Œå‘å±•å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9f270504587e4f3eb8e2ef20e7e6270" align="middle">
<img src="https://picx.zhimg.com/v2-ed515e6490ffa03dc1745d7a682e9a65" align="middle">
<img src="https://picx.zhimg.com/v2-3a04df4ee769f58838d7027a23d412c4" align="middle">
<img src="https://picx.zhimg.com/v2-a9bd143b2e8ca03340ecf8aefc81ac6c" align="middle">
<img src="https://picx.zhimg.com/v2-34a334b32a9bb53aef0fb4fcd4751123" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MRIQT-Physics-Aware-Diffusion-Model-for-Image-Quality-Transfer-in-Neonatal-Ultra-Low-Field-MRI"><a href="#MRIQT-Physics-Aware-Diffusion-Model-for-Image-Quality-Transfer-in-Neonatal-Ultra-Low-Field-MRI" class="headerlink" title="MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI"></a>MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI</h2><p><strong>Authors:Malek Al Abed, Sebiha Demir, Anne Groteklaes, Elodie Germani, Shahrooz Faghihroohi, Hemmen Sabir, Shadi Albarqouni</strong></p>
<p>Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.</p>
<blockquote>
<p>ä¾¿æºå¼è¶…ä½åœºç£å…±æŒ¯æˆåƒï¼ˆuLF-MRIï¼Œ0.064Tï¼‰ä¸ºæ–°ç”Ÿå„¿æŠ¤ç†æä¾›äº†å¯åŠçš„ç¥ç»æˆåƒï¼Œä½†ä¸å…¶é«˜åœºï¼ˆHFï¼‰MRIç›¸æ¯”ï¼Œå­˜åœ¨ä¿¡å™ªæ¯”ä½å’Œè¯Šæ–­è´¨é‡å·®çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†MRIQTï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä»uLFåˆ°HF MRIçš„å›¾åƒè´¨é‡è½¬ç§»ï¼ˆIQTï¼‰çš„3Dæ¡ä»¶æ‰©æ•£æ¡†æ¶ã€‚MRIQTç»“åˆäº†é€¼çœŸçš„Kç©ºé—´é€€åŒ–è¿›è¡Œç‰©ç†ä¸€è‡´çš„uLFæ¨¡æ‹Ÿã€vé¢„æµ‹ä¸æ— åˆ†ç±»å™¨å¼•å¯¼çš„ç¨³å®šå›¾åƒåˆ°å›¾åƒç”Ÿæˆï¼Œä»¥åŠç”¨äºè§£å‰–å­¦ä¿çœŸåº¦çš„SNRåŠ æƒ3Dæ„ŸçŸ¥æŸå¤±ã€‚è¯¥æ¨¡å‹ä»å—å™ªå£°å½±å“çš„uLFè¾“å…¥ä¸­è¿›è¡Œå»å™ªï¼Œä»¥ç›¸åŒçš„æ‰«æä¸ºæ¡ä»¶ï¼Œåˆ©ç”¨ä½“ç§¯æ³¨æ„åŠ›UNetæ¶æ„è¿›è¡Œç»“æ„ä¿ç•™ç¿»è¯‘ã€‚åœ¨å…·æœ‰å¤šç§ç—…ç†çš„æ–°ç”Ÿå„¿é˜Ÿåˆ—ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒMRIQTåœ¨PSNRä¸Šè¶…è¶Šäº†æœ€è¿‘çš„GANå’ŒCNNåŸºçº¿ï¼Œæé«˜äº†15.3%ï¼Œè¶…å‡ºæœ€æ–°æŠ€æœ¯1.78%ï¼ŒåŒæ—¶åŒ»ç”Ÿè¯„ä»·å…¶è¾“å‡ºä¸­æœ‰85%è´¨é‡è‰¯å¥½ï¼Œç—…ç†æ¸…æ™°å¯è§ã€‚MRIQTå®ç°äº†åŸºäºæ‰©æ•£çš„é«˜ä¿çœŸå¢å¼ºä¾¿æºå¼è¶…ä½åœºï¼ˆuLFï¼‰MRIï¼Œå¯ç”¨äºå¯é çš„æ–°ç”Ÿå„¿è„‘è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13232v1">PDF</a> 5 pages, 4 figures</p>
<p><strong>Summary</strong><br>     ä¾¿æºå¼è¶…ä½åœºç£å…±æŒ¯æˆåƒï¼ˆuLF-MRIï¼Œ0.064Tï¼‰åœ¨æ–°ç”Ÿå„¿æŠ¤ç†ä¸­æä¾›å¯è®¿é—®çš„ç¥ç»æˆåƒï¼Œä½†ä¸é«˜åœºï¼ˆHFï¼‰MRIç›¸æ¯”ï¼Œå­˜åœ¨ä¿¡å·å™ªå£°æ¯”ä½å’Œè¯Šæ–­è´¨é‡å·®çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºMRIQTï¼Œä¸€ç§ç”¨äºä»uLFåˆ°HF MRIçš„å›¾åƒè´¨é‡è½¬ç§»ï¼ˆIQTï¼‰çš„3Dæ¡ä»¶æ‰©æ•£æ¡†æ¶ã€‚MRIQTç»“åˆé€¼çœŸçš„Kç©ºé—´é€€åŒ–è¿›è¡Œç‰©ç†ä¸€è‡´çš„uLFæ¨¡æ‹Ÿã€vé¢„æµ‹ä¸æ— åˆ†ç±»å™¨å¼•å¯¼çš„ç¨³å®šå›¾åƒåˆ°å›¾åƒç”Ÿæˆï¼Œä»¥åŠåŸºäºSNRçš„3Dæ„ŸçŸ¥æŸå¤±æ¥ä¿è¯è§£å‰–çœŸå®æ€§ã€‚è¯¥æ¨¡å‹ä»å™ªå£°uLFè¾“å…¥ä¸­å­¦ä¹ å¹¶è½¬æ¢ä¸ºåŒä¸€æ‰«ææ¡ä»¶ä¸‹çš„é«˜é¢‘å›¾åƒï¼Œåˆ©ç”¨ä½“ç§¯æ³¨æ„åŠ›UNetæ¶æ„è¿›è¡Œç»“æ„ä¿ç•™ç¿»è¯‘ã€‚åœ¨å…·æœ‰ä¸åŒç—…ç†çš„æ–°ç”Ÿå„¿é˜Ÿåˆ—ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒMRIQTåœ¨PSNRä¸Šè¶…è¿‡äº†æœ€æ–°çš„GANå’ŒCNNåŸºå‡†æµ‹è¯•ï¼Œæå‡äº†15.3%ï¼Œè¾ƒå½“å‰æœ€é«˜æ°´å¹³é«˜å‡º1.78%ï¼Œè€ŒåŒ»ç”Ÿå¯¹å…¶è¾“å‡ºçš„è¯„ä»·ä¸­ï¼Œæœ‰85%è®¤ä¸ºè´¨é‡è‰¯å¥½ä¸”ç—…ç†æ¸…æ™°ã€‚MRIQTå®ç°äº†åŸºäºæ‰©æ•£çš„ä¾¿æºå¼è¶…ä½åœºï¼ˆuLFï¼‰MRIé«˜è´¨é‡å¢å¼ºï¼Œä¸ºæ–°ç”Ÿå„¿å¤§è„‘è¯„ä¼°æä¾›äº†å¯é ä¿éšœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>uLF-MRIè™½ç„¶ä¾¿æºå¹¶é€‚ç”¨äºæ–°ç”Ÿå„¿æŠ¤ç†ï¼Œä½†åœ¨ä¿¡å·å™ªå£°æ¯”å’Œè¯Šæ–­è´¨é‡æ–¹é¢è¡¨ç°ä¸ä½³ã€‚</li>
<li>MRIQTæ˜¯é¦–ä¸ªåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹uLFåˆ°HF MRIçš„å›¾åƒè´¨é‡è½¬ç§»æ–¹æ³•ã€‚</li>
<li>MRIQTé€šè¿‡é€¼çœŸæ¨¡æ‹ŸuLFåˆ°HFçš„è¿‡ç¨‹ä»¥åŠåŸºäºç»“æ„ä¿æŒç¿»è¯‘çš„èƒ½åŠ›æé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>ç»“åˆå¤šç§æŠ€æœ¯ï¼ˆåŒ…æ‹¬ç‰©ç†æ¨¡æ‹Ÿã€ç¨³å®šå›¾åƒç”Ÿæˆã€æ„ŸçŸ¥æŸå¤±ç­‰ï¼‰å®ç°äº†å‡ºè‰²çš„å›¾åƒè½¬æ¢æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹ç»è¿‡å¤šæ ·ç—…ç†æ–°ç”Ÿå„¿é˜Ÿåˆ—è®­ç»ƒï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®åº¦å’Œè¯Šæ–­èƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒMRIQTæ˜¾è‘—æé«˜å›¾åƒè´¨é‡å¹¶è·å¾—åŒ»ç”Ÿçš„é«˜åº¦è¯„ä»·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5ea30b613d592a111b3c0206bc97ba1" align="middle">
<img src="https://picx.zhimg.com/v2-516fb79028978983af85fdb61db0164c" align="middle">
<img src="https://picx.zhimg.com/v2-b786049795c7e706c7a6e03d8246dbc9" align="middle">
<img src="https://picx.zhimg.com/v2-a21e08218a2541d39553bd6c82eb309c" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GenTract-Generative-Global-Tractography"><a href="#GenTract-Generative-Global-Tractography" class="headerlink" title="GenTract: Generative Global Tractography"></a>GenTract: Generative Global Tractography</h2><p><strong>Authors:Alec Sargood, Lemuel Puglisi, Elinor Thompson, Mirco Musolesi, Daniel C. Alexander</strong></p>
<p>Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTractâ€™s performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.</p>
<blockquote>
<p>ç¥ç»çº¤ç»´è¿½è¸ªï¼ˆTractographyï¼‰æ˜¯ä»æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰æ¨æ–­å¤§è„‘ç™½è´¨è·¯å¾„è½¨è¿¹çš„è¿‡ç¨‹ã€‚å±€éƒ¨è¿½è¸ªæ–¹æ³•é€šè¿‡é€æ­¥è·Ÿè¸ªå›¾åƒä¸­çš„å±€éƒ¨çº¤ç»´æ–¹å‘ä¼°è®¡æ¥æ„å»ºæµçº¿ï¼Œå®¹æ˜“ç´¯ç§¯è¯¯å·®å¹¶å‡ºç°è¾ƒé«˜çš„å‡é˜³æ€§ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°è¾ƒå¤§æˆ–åˆ†è¾¨ç‡è¾ƒä½çš„æ•°æ®ä¸Šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¨å±€æ–¹æ³•è¯•å›¾ä¼˜åŒ–ä¸€ç»„æµçº¿ï¼Œä»¥æœ€å¤§åŒ–å…¶ä¸åŸºç¡€çº¤ç»´æ–¹å‘ä¼°è®¡çš„å…¼å®¹æ€§ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GenTractï¼Œè¿™æ˜¯å…¨çƒé¦–ä¸ªç”¨äºç¥ç»çº¤ç»´è¿½è¸ªçš„ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬å°†ç¥ç»çº¤ç»´è¿½è¸ªä½œä¸ºç”Ÿæˆä»»åŠ¡ï¼Œå­¦ä¹ ä»dMRIåˆ°å®Œæ•´ã€è§£å‰–ä¸Šåˆç†çš„æµçº¿çš„ç›´æ¥æ˜ å°„ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºæ‰©æ•£å’ŒæµåŒ¹é…çš„æ¨¡å¼ï¼Œå¹¶è¯„ä¼°äº†GenTractä¸æœ€æ–°åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGenTractçš„ç²¾ç¡®åº¦æ¯”ç¬¬äºŒåTractOracleé«˜å‡º2.1å€ã€‚è¿™ä¸€ä¼˜åŠ¿åœ¨åˆ†è¾¨ç‡è¾ƒä½å’Œå™ªå£°è¾ƒå¤§çš„æŒ‘æˆ˜ç¯å¢ƒä¸­æ›´ä¸ºæ˜æ˜¾ï¼Œæ­¤æ—¶å®ƒæ¯”æœ€æ¥è¿‘çš„ç«äº‰å¯¹æ‰‹é«˜å‡ºæ•°å€ã€‚GenTractèƒ½å¤Ÿåœ¨ç ”ç©¶çº§æ•°æ®ä¸Šäº§ç”Ÿé«˜ç²¾åº¦çš„è¿½è¸ªå›¾ï¼ŒåŒæ—¶åœ¨å¤„ç†ä¸å®Œç¾ã€ä½åˆ†è¾¨ç‡æ•°æ®æ—¶ä¹Ÿèƒ½ä¿æŒå¯é æ€§ï¼Œå› æ­¤ä»£è¡¨ç€å…¨å±€è¿½è¸ªçš„ä¸€ä¸ªæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13183v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„è„‘ç™½è´¨è·¯å¾„æ¨æ–­æŠ€æœ¯ï¼ˆTractographyï¼‰ã€‚å±€éƒ¨è¿½è¸ªæ³•æ˜“ç§¯ç´¯è¯¯å·®ä¸”æ˜“å‡ºç°è¯¯æŠ¥ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°æˆ–ä½åˆ†è¾¨ç‡æ•°æ®ä¸‹ã€‚è€Œå…¨å±€æ–¹æ³•è™½ç„¶è®¡ç®—é‡å¤§ï¼Œä½†ä¼˜åŒ–æ•ˆæœæ›´ä½³ã€‚ç ”ç©¶æå‡ºäº†åŸºäºç”Ÿæˆæ¨¡å‹çš„å…¨å±€è¿½è¸ªæ–¹æ³•GenTractï¼Œå®ƒèƒ½ä»æ‰©æ•£ç£å…±æŒ¯æˆåƒæ•°æ®ä¸­ç›´æ¥ç”Ÿæˆå®Œæ•´ã€è§£å‰–ä¸Šåˆç†çš„æµçº¿ã€‚ç›¸è¾ƒäºå…¶ä»–é¡¶å°–æ–¹æ³•ï¼ŒGenTractçš„ç²¾å‡†åº¦é«˜å‡ºä¸¤å€ä»¥ä¸Šï¼Œä¸”åœ¨ä½åˆ†è¾¨ç‡å’Œå™ªå£°ç¯å¢ƒä¸‹è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Tractography æ˜¯ä»æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰æ¨æ–­å¤§è„‘ç™½è´¨è·¯å¾„çš„è¿‡ç¨‹ã€‚</li>
<li>å±€éƒ¨è¿½è¸ªæ³•åœ¨å™ªå£°æˆ–ä½åˆ†è¾¨ç‡æ•°æ®ä¸‹æ˜“ç§¯ç´¯è¯¯å·®å’Œäº§ç”Ÿé«˜è¯¯æŠ¥ç‡ã€‚</li>
<li>å…¨å±€æ–¹æ³•å°è¯•ä¼˜åŒ–æµçº¿é›†åˆä»¥æœ€å¤§é™åº¦åœ°ä¸åº•å±‚çº¤ç»´æ–¹å‘ä¼°è®¡ç›¸ç¬¦ï¼Œä½†è®¡ç®—é‡å¤§ã€‚</li>
<li>GenTract æ˜¯é¦–ä¸ªç”¨äºå…¨å±€è¿½è¸ªçš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½ä» dMRI ç›´æ¥ç”Ÿæˆå®Œæ•´ã€è§£å‰–ä¸Šåˆç†çš„æµçº¿ã€‚</li>
<li>GenTract é‡‡ç”¨äº†æ‰©æ•£å’ŒæµåŒ¹é…ä¸¤ç§èŒƒå¼è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>GenTract çš„æ€§èƒ½ä¼˜äºå…¶ä»–é¡¶å°–æ–¹æ³•ï¼Œç²¾å‡†åº¦é«˜å‡ºä¸¤å€ä»¥ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc1aad352d8c2a4b623a53c55c8e8bef" align="middle">
<img src="https://picx.zhimg.com/v2-768d234729b38f04179cf59fddaa23bb" align="middle">
<img src="https://picx.zhimg.com/v2-70af671c05f8ecab5adb25a1543534cf" align="middle">
<img src="https://picx.zhimg.com/v2-c23f0279eecd719278ddf3d205e1f4cf" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CloseUpShot-Close-up-Novel-View-Synthesis-from-Sparse-views-via-Point-conditioned-Diffusion-Model"><a href="#CloseUpShot-Close-up-Novel-View-Synthesis-from-Sparse-views-via-Point-conditioned-Diffusion-Model" class="headerlink" title="CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model"></a>CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model</h2><p><strong>Authors:Yuqi Zhang, Guanying Chen, Jiaxing Chen, Chuanyu Fu, Chuan Huang, Shuguang Cui</strong></p>
<p>Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.</p>
<blockquote>
<p>ä»ç¨€ç–çš„è¾“å…¥è§†è§’é‡å»º3Dåœºæ™¯å¹¶åˆæˆæ–°çš„è§†è§’æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºç¨€ç–è§†å›¾è®¾ç½®ä¸‹æé«˜é‡å»ºè´¨é‡çš„æœ‰å‰é€”çš„å·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦è®¾è®¡ç”¨äºé€‚åº¦çš„è§†ç‚¹å˜åŒ–ï¼Œå®ƒä»¬åœ¨æ•æ‰è¿‘è·ç¦»åœºæ™¯çš„ç²¾ç»†ç»†èŠ‚æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå› ä¸ºè¾“å…¥ä¿¡æ¯ä¸¥é‡å—é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç§°ä¸ºCloseUpShotï¼Œé€šè¿‡ç‚¹æ¡ä»¶è§†é¢‘æ‰©æ•£è¿›è¡Œè¿‘è·ç¦»æ–°é¢–è§†å›¾åˆæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°åƒç´ warpæ¡ä»¶åœ¨è¿‘è·ç¦»è®¾ç½®ä¸­å­˜åœ¨ä¸¥é‡çš„ç¨€ç–æ€§å’ŒèƒŒæ™¯æ³„æ¼é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å±‚æ¬¡warpå’Œé®æŒ¡æ„ŸçŸ¥å™ªå£°æŠ‘åˆ¶ï¼Œæé«˜äº†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶å›¾åƒçš„è´¨é‡å’Œå®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨å±€ç»“æ„æŒ‡å¯¼ï¼Œåˆ©ç”¨å¯†é›†èåˆç‚¹äº‘ä¸ºæ‰©æ•£è¿‡ç¨‹æä¾›ä¸€è‡´çš„å‡ ä½•ä¸Šä¸‹æ–‡ï¼Œä»¥å¼¥è¡¥ç¨€ç–æ¡ä»¶è¾“å…¥ä¸­ç¼ºä¹å…¨å±€ä¸€è‡´çš„3Dçº¦æŸã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿‘è·ç¦»æ–°é¢–è§†å›¾åˆæˆæ–¹é¢ï¼Œè¿™æ¸…æ¥šåœ°éªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13121v1">PDF</a> Project Link: <a target="_blank" rel="noopener" href="https://zyqz97.github.io/CloseUpShot/">https://zyqz97.github.io/CloseUpShot/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>é‡å»ºä¸‰ç»´åœºæ™¯å¹¶ä»æœªçŸ¥è§†è§’åˆæˆæ–°å‹å›¾åƒæ˜¯ä¸€ä¸ªæå¯ŒæŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ€æ–°çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ç¨€ç–è§†è§’æ¡ä»¶ä¸‹å¢å¼ºé‡å»ºè´¨é‡æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹è§†è§’å˜åŒ–ä¸å¤§çš„æƒ…å†µï¼Œéš¾ä»¥æ•æ‰è¿‘è·ç¦»åœºæ™¯ä¸­çš„ç²¾ç»†ç»†èŠ‚ï¼Œå› ä¸ºè¾“å…¥ä¿¡æ¯ä¸¥é‡å—é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶CloseUpShotï¼Œé€šè¿‡ç‚¹æ¡ä»¶è§†é¢‘æ‰©æ•£ä»ç¨€ç–è¾“å…¥è¿›è¡Œè¿‘è·ç¦»æ–°å‹è§†è§’åˆæˆã€‚æˆ‘ä»¬å‘ç°åƒç´ çº§æ‰­æ›²æ¡ä»¶åœ¨è¿‘è·ç¦»è®¾ç½®ä¸­å­˜åœ¨ä¸¥é‡ç¨€ç–æ€§å’ŒèƒŒæ™¯æ³„éœ²é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå±‚æ¬¡æ‰­æ›²å’Œé®æŒ¡æ„ŸçŸ¥å™ªå£°æŠ‘åˆ¶æŠ€æœ¯ï¼Œæå‡è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å‚è€ƒå›¾åƒè´¨é‡å’Œå®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥å…¨å±€ç»“æ„å¼•å¯¼ï¼Œåˆ©ç”¨å¯†é›†èåˆç‚¹äº‘ä¸ºæ‰©æ•£è¿‡ç¨‹æä¾›ä¸€è‡´çš„å‡ ä½•ä¸Šä¸‹æ–‡ï¼Œä»¥å¼¥è¡¥ç¨€ç–æ¡ä»¶è¾“å…¥ä¸­ç¼ºä¹å…¨å±€ä¸€è‡´çš„3Dçº¦æŸã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿‘è·ç¦»æ–°é¢–è§†è§’åˆæˆæ–¹é¢ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é‡å»ºä¸‰ç»´åœºæ™¯å¹¶åˆæˆç¨€ç–è¾“å…¥çš„æ–°è§†è§’å›¾åƒæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œåœ¨ç¨€ç–è§†è§’æ¡ä»¶ä¸‹æœ‰å¢å¼ºé‡å»ºè´¨é‡çš„æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å¤„ç†è§†è§’å˜åŒ–ä¸å¤§çš„æƒ…å†µï¼Œéš¾ä»¥æ•æ‰è¿‘è·ç¦»åœºæ™¯ä¸­çš„ç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>CloseUpShotæ¡†æ¶é€šè¿‡ç‚¹æ¡ä»¶è§†é¢‘æ‰©æ•£è§£å†³ä»ç¨€ç–è¾“å…¥è¿›è¡Œè¿‘è·ç¦»æ–°å‹è§†è§’åˆæˆçš„é—®é¢˜ã€‚</li>
<li>åƒç´ çº§æ‰­æ›²æ¡ä»¶åœ¨è¿‘è·ç¦»åˆæˆä¸­å­˜åœ¨ç¨€ç–æ€§å’ŒèƒŒæ™¯æ³„éœ²é—®é¢˜ã€‚</li>
<li>å±‚æ¬¡æ‰­æ›²å’Œé®æŒ¡æ„ŸçŸ¥å™ªå£°æŠ‘åˆ¶æŠ€æœ¯æå‡å‚è€ƒå›¾åƒè´¨é‡å’Œå®Œæ•´æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥å…¨å±€ç»“æ„å¼•å¯¼ï¼Œåˆ©ç”¨å¯†é›†èåˆç‚¹äº‘æä¾›ä¸€è‡´çš„å‡ ä½•ä¸Šä¸‹æ–‡ï¼Œå¼¥è¡¥ç¨€ç–è¾“å…¥ä¸­çš„å…¨å±€3Dçº¦æŸç¼ºå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13121">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40ee595d7d7fc576197506bce67c0b3f" align="middle">
<img src="https://picx.zhimg.com/v2-99f88b0d9f6faab3a684bf8e06e70dda" align="middle">
<img src="https://picx.zhimg.com/v2-433d7a20a46a48c25861e40a788610e0" align="middle">
<img src="https://picx.zhimg.com/v2-f019d2a92dcf73f6ed3901d06c909caa" align="middle">
<img src="https://picx.zhimg.com/v2-de8cfc70c375ce4cef36ad76a459fcf7" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DGS-Net-Distillation-Guided-Gradient-Surgery-for-CLIP-Fine-Tuning-in-AI-Generated-Image-Detection"><a href="#DGS-Net-Distillation-Guided-Gradient-Surgery-for-CLIP-Fine-Tuning-in-AI-Generated-Image-Detection" class="headerlink" title="DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection"></a>DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection</h2><p><strong>Authors:Jiazhen Yan, Ziqiang Li, Fan Wang, Boyu Wang, Zhangjie Fu</strong></p>
<p>The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.</p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANå’Œæ‰©æ•£æ¨¡å‹ï¼‰çš„å¿«é€Ÿå‘å±•å¯¼è‡´äº†AIç”Ÿæˆå›¾åƒçš„å¹¿æ³›ä¼ æ’­ï¼Œå¼•å‘äº†å…³äºæ•°å­—åª’ä½“ä¸­çš„è¯¯å¯¼ä¿¡æ¯ã€ä¾µçŠ¯éšç§å’Œå¯¹ä¿¡ä»»çš„ä¾µèš€çš„æ‹…å¿§ã€‚è™½ç„¶åƒCLIPè¿™æ ·çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ä¸ºæ£€æµ‹åˆæˆå†…å®¹æä¾›äº†å¼ºå¤§çš„å¯è¿ç§»è¡¨ç¤ºï¼Œä½†å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒå¸¸å¸¸ä¼šå¼•å‘ç¾éš¾æ€§é—å¿˜ï¼Œè¿™é™ä½äº†é¢„è®­ç»ƒå…ˆéªŒçŸ¥è¯†å¹¶é™åˆ¶äº†è·¨åŸŸæ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Distillation-guided Gradient Surgery Networkï¼ˆDGS-Netï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿä¿ç•™å¯è¿ç§»çš„é¢„è®­ç»ƒå…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶æŠ‘åˆ¶ä¸ä»»åŠ¡æ— å…³çš„æˆåˆ†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¢¯åº¦ç©ºé—´åˆ†è§£ï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆ†ç¦»æœ‰å®³å’Œæœ‰ç›Šçš„ä¸‹é™æ–¹å‘ã€‚é€šè¿‡å°†ä»»åŠ¡æ¢¯åº¦æŠ•å½±åˆ°æœ‰å®³æ–¹å‘çš„æ­£äº¤è¡¥é›†ä¸Šï¼Œå¹¶ä¸ä»å†»ç»“çš„CLIPç¼–ç å™¨ä¸­æç‚¼å‡ºçš„æœ‰ç›Šæ¢¯åº¦å¯¹é½ï¼ŒDGS-Netå®ç°äº†å…ˆéªŒçŸ¥è¯†ä¿ç•™å’Œæ— å…³æŠ‘åˆ¶çš„ç»Ÿä¸€ä¼˜åŒ–ã€‚åœ¨50ä¸ªç”Ÿæˆæ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€æ–°æ–¹æ³•å¹³å‡é«˜å‡º6.6ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨å¤šç§ç”ŸæˆæŠ€æœ¯ä¸Šå®ç°äº†æ›´å‡ºè‰²çš„æ£€æµ‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13108v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç”Ÿæˆæ¨¡å‹å¦‚GANå’Œæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´AIç”Ÿæˆå›¾åƒçš„å¤§é‡æ¶Œç°ï¼Œå¼•å‘äº†å¯¹æ•°å­—åª’ä½“ä¸­è™šå‡ä¿¡æ¯ã€éšç§ä¾µçŠ¯å’Œä¿¡ä»»ä¾µèš€çš„æ‹…å¿§ã€‚ä¸ºè§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å¦‚CLIPåœ¨æ£€æµ‹åˆæˆå†…å®¹æ—¶é¢ä¸´çš„å¾®è°ƒç¾éš¾é—å¿˜é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºDistillation-guided Gradient Surgery Networkï¼ˆDGS-Netï¼‰æ¡†æ¶ã€‚å®ƒèƒ½ä¿ç•™å¯è¿ç§»çš„é¢„è®­ç»ƒå…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶æŠ‘åˆ¶ä¸ä»»åŠ¡æ— å…³çš„æˆåˆ†ã€‚å®éªŒè¯æ˜ï¼ŒDGS-Netåœ¨50ä¸ªç”Ÿæˆæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹³å‡æé«˜6.6ä¸ªç™¾åˆ†ç‚¹çš„æ£€æµ‹æ€§èƒ½å’Œè·¨ä¸åŒç”ŸæˆæŠ€æœ¯çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANså’Œæ‰©æ•£æ¨¡å‹ï¼‰çš„å¿«é€Ÿå‘å±•å¯¼è‡´äº†AIç”Ÿæˆå›¾åƒçš„å¤§é‡å‡ºç°ã€‚</li>
<li>AIç”Ÿæˆå›¾åƒå¼•å‘å¯¹æ•°å­—åª’ä½“ä¸­è™šå‡ä¿¡æ¯ã€éšç§ä¾µçŠ¯å’Œä¿¡ä»»é—®é¢˜çš„æ‹…å¿§ã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å¦‚CLIPåœ¨æ£€æµ‹åˆæˆå†…å®¹æ—¶é¢ä¸´å¾®è°ƒç¾éš¾é—å¿˜é—®é¢˜ã€‚</li>
<li>DGS-Netæ¡†æ¶èƒ½ä¿ç•™å¯è¿ç§»çš„é¢„è®­ç»ƒå…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶æŠ‘åˆ¶ä¸ä»»åŠ¡æ— å…³çš„æˆåˆ†ã€‚</li>
<li>DGS-Neté€šè¿‡æ¢¯åº¦ç©ºé—´åˆ†è§£æ¥åˆ†ç¦»æœ‰å®³å’Œæœ‰ç›Šçš„ä¸‹é™æ–¹å‘ï¼Œå®ç°ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å…ˆéªŒä¿ç•™å’Œæ— å…³æˆåˆ†æŠ‘åˆ¶ã€‚</li>
<li>DGS-Netä½¿ç”¨è’¸é¦æŠ€æœ¯ä»å†»ç»“çš„CLIPç¼–ç å™¨ä¸­æå–æœ‰ç›Šæ¢¯åº¦ï¼Œå¹¶ä¸ä»»åŠ¡æ¢¯åº¦å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70eb3bd09555c682bed34cb7e12ff78d" align="middle">
<img src="https://picx.zhimg.com/v2-a26bddc030412426a9e079f86239a542" align="middle">
<img src="https://picx.zhimg.com/v2-c8de52c5ff175ba0ab9426bc16b69f81" align="middle">
<img src="https://picx.zhimg.com/v2-8a08fa5ffd849071874565bf7eff06da" align="middle">
<img src="https://picx.zhimg.com/v2-e9f719e9b7c6df4e4e82ec8249d884ba" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-3D-Object-Centric-Feature-Learning-for-Semantic-Scene-Completion"><a href="#Towards-3D-Object-Centric-Feature-Learning-for-Semantic-Scene-Completion" class="headerlink" title="Towards 3D Object-Centric Feature Learning for Semantic Scene Completion"></a>Towards 3D Object-Centric Feature Learning for Semantic Scene Completion</h2><p><strong>Authors:Weihua Wang, Yubo Cui, Xiangru Lin, Zhiheng Li, Zheng Fang</strong></p>
<p>Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.</p>
<blockquote>
<p>åŸºäºè§†è§‰çš„3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰å› å…¶åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨æ½œåŠ›è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚è™½ç„¶å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é‡‡ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ•´ä¸ªåœºæ™¯ä¸Šèšåˆå’Œæ‰©æ•£ç‰¹å¾ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†ç²¾ç»†çš„å¯¹è±¡çº§ç»†èŠ‚ï¼Œå¯¼è‡´è¯­ä¹‰å’Œå‡ ä½•æ­§ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Oceanï¼Œä¸€ä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„é¢„æµ‹æ¡†æ¶ï¼Œå®ƒå°†åœºæ™¯åˆ†è§£ä¸ºå•ä¸ªå¯¹è±¡å®ä¾‹ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„è¯­ä¹‰å ç”¨é¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨è½»é‡çº§åˆ†å‰²æ¨¡å‹MobileSAMä»è¾“å…¥å›¾åƒä¸­æå–å®ä¾‹æ©ç ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ª3Dè¯­ä¹‰ç»„æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ©ç”¨çº¿æ€§æ³¨æ„åŠ›åœ¨3Dç©ºé—´ä¸­èšåˆä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç‰¹å¾ã€‚ä¸ºäº†å¤„ç†åˆ†å‰²é”™è¯¯å’Œç¼ºå¤±çš„å®ä¾‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªå…¨å±€ç›¸ä¼¼åº¦å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ©ç”¨åˆ†å‰²ç‰¹å¾è¿›è¡Œå…¨å±€äº¤äº’ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®ä¾‹æ„ŸçŸ¥å±€éƒ¨æ‰©æ•£æ¨¡å—ï¼Œé€šè¿‡ç”Ÿæˆè¿‡ç¨‹æ”¹è¿›å®ä¾‹ç‰¹å¾ï¼Œç„¶åç»†åŒ–BEVç©ºé—´ä¸­çš„åœºæ™¯è¡¨ç¤ºã€‚åœ¨SemanticKITTIå’ŒSSCBench-KITTI360åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOceanè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒmIoUå¾—åˆ†åˆ†åˆ«ä¸º17.40å’Œ20.28ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13031v1">PDF</a> Accept by AAAI-2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åŸºäºè§†è§‰çš„3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½ç•¥å¯¹è±¡çº§åˆ«çš„ç»†èŠ‚å¯¼è‡´çš„è¯­ä¹‰å’Œå‡ ä½•æ¨¡ç³Šé—®é¢˜ï¼Œæå‡ºäº†Oceanæ¡†æ¶ã€‚å®ƒé‡‡ç”¨å¯¹è±¡ä¸­å¿ƒé¢„æµ‹ï¼Œå°†åœºæ™¯åˆ†è§£ä¸ºç‹¬ç«‹å¯¹è±¡å®ä¾‹ï¼Œå®ç°æ›´å‡†ç¡®çš„è¯­ä¹‰å ç”¨é¢„æµ‹ã€‚é€šè¿‡æ¨¡å—è®¾è®¡ï¼Œå®ç°äº†åœºæ™¯åœ¨å®ä¾‹çº§åˆ«ä¸Šçš„ç²¾ç»†åŒ–ç†è§£å’Œè¡¨è¾¾ï¼Œæé«˜äº†åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚åœ¨SemanticKITTIå’ŒSSCBench-KITTI360åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOceanå–å¾—äº†æœ€æ–°æ€§èƒ½ï¼ŒmIoUå¾—åˆ†åˆ†åˆ«ä¸º17.4å’Œ20.28ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒåŸºäºè§†è§‰çš„3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰å—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤šé‡‡ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„èŒƒå¼ï¼Œå¯¼è‡´è¯­ä¹‰å’Œå‡ ä½•æ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>Oceanæ¡†æ¶é‡‡ç”¨å¯¹è±¡ä¸­å¿ƒé¢„æµ‹ï¼Œåˆ†è§£åœºæ™¯ä¸ºç‹¬ç«‹å¯¹è±¡å®ä¾‹ï¼Œå®ç°æ›´å‡†ç¡®è¯­ä¹‰å ç”¨é¢„æµ‹ã€‚</li>
<li>Oceanè®¾è®¡äº†å¤šä¸ªæ¨¡å—å¤„ç†åˆ†å‰²é”™è¯¯å’Œç¼ºå¤±å®ä¾‹é—®é¢˜ï¼Œæé«˜æ€§èƒ½ã€‚</li>
<li>Oceané€šè¿‡æ¨¡å—è®¾è®¡å®ç°åœºæ™¯åœ¨å®ä¾‹çº§åˆ«ä¸Šçš„ç²¾ç»†åŒ–ç†è§£å’Œè¡¨è¾¾ã€‚</li>
<li>Oceanåœ¨SemanticKITTIå’ŒSSCBench-KITTI360åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€æ–°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b176e821e18249326a01fcc55c0accbd" align="middle">
<img src="https://picx.zhimg.com/v2-d7decc4adf777792327f5f72d97cd86f" align="middle">
<img src="https://picx.zhimg.com/v2-982fd138e42c45b00c22f4be4173e13a" align="middle">
<img src="https://picx.zhimg.com/v2-ec76fe343ee1fe4328f17e45b77c057c" align="middle">
<img src="https://picx.zhimg.com/v2-3217193d7bd07426126f2d26363fda65" align="middle">
<img src="https://picx.zhimg.com/v2-8a424d82c4a6bb5bc3fe144c40f52759" align="middle">
<img src="https://picx.zhimg.com/v2-cae2a10d2e29f65cde48735b3a300840" align="middle">
<img src="https://picx.zhimg.com/v2-4ca4097146be43768f80bb2ea7a82b02" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Infinite-Story-A-Training-Free-Consistent-Text-to-Image-Generation"><a href="#Infinite-Story-A-Training-Free-Consistent-Text-to-Image-Generation" class="headerlink" title="Infinite-Story: A Training-Free Consistent Text-to-Image Generation"></a>Infinite-Story: A Training-Free Consistent Text-to-Image Generation</h2><p><strong>Authors:Jihun Park, Kyoungmin Lee, Jongmin Gim, Hyeonseo Jo, Minseok Oh, Wonhyeok Choi, Kyumin Hwang, Jaeyeul Kim, Minwoo Choi, Sunghoon Im</strong></p>
<p>We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Infinite-Storyï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¡†æ¶ï¼Œä¸“ä¸ºå¤šæç¤ºå™äº‹åœºæ™¯å®šåˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨è§„æ¨¡è‡ªé€‚åº”å›å½’æ¨¡å‹ä¹‹ä¸Šï¼Œè§£å†³äº†æ–‡æœ¬ä¸€è‡´æ€§ç”Ÿæˆä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šèº«ä»½ä¸ä¸€è‡´å’Œé£æ ¼ä¸ä¸€è‡´ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ç§äº’è¡¥æŠ€æœ¯ï¼šèº«ä»½æç¤ºæ›¿æ¢ï¼Œè¿™å‡è½»äº†æ–‡æœ¬ç¼–ç å™¨çš„ä¸Šä¸‹æ–‡åè§ï¼Œä½¿è·¨æç¤ºçš„èº«ä»½å±æ€§å¯¹é½ï¼›ä»¥åŠåŒ…å«è‡ªé€‚åº”é£æ ¼æ³¨å…¥å’ŒåŒæ­¥æŒ‡å¯¼é€‚åº”çš„ç»Ÿä¸€æ³¨æ„åŠ›æŒ‡å¯¼æœºåˆ¶ï¼Œå®ƒä»¬å…±åŒæ‰§è¡Œå…¨å±€é£æ ¼å’Œèº«ä»½å¤–è§‚çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒæç¤ºä¿çœŸåº¦ã€‚ä¸å…ˆå‰éœ€è¦å¾®è°ƒæˆ–é¢ä¸´æ…¢é€Ÿæ¨ç†çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ä¸åŒï¼ŒInfinite-Storyå®Œå…¨åœ¨æµ‹è¯•é˜¶æ®µè¿è¡Œï¼Œå®ç°åœ¨ä¸åŒæç¤ºä¸‹é«˜èº«ä»½å’Œé£æ ¼çš„ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆæ€§èƒ½ï¼ŒåŒæ—¶æä¾›æ¯”ç°æœ‰æœ€å¿«çš„ä¸€è‡´T2Iæ¨¡å‹å¿«6å€ä»¥ä¸Šçš„æ¨ç†é€Ÿåº¦ï¼ˆæ¯ç§’ç”Ÿæˆ1.72å¼ å›¾åƒï¼‰ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„è§†è§‰å™äº‹ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13002v1">PDF</a> 18pages, 13 figures, AAAI 2026 Oral</p>
<p><strong>Summary</strong></p>
<p>æ— é™æ•…äº‹ï¼šæ— éœ€è®­ç»ƒçš„ä¸€è‡´æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•é’ˆå¯¹å¤šæç¤ºå™äº‹åœºæ™¯ï¼Œè§£å†³èº«ä»½ä¸ä¸€è‡´å’Œé£æ ¼ä¸ä¸€è‡´ä¸¤å¤§æŒ‘æˆ˜ã€‚å¼•å…¥ä¸‰ç§äº’è¡¥æŠ€æœ¯ï¼šèº«ä»½æç¤ºæ›¿æ¢ã€è‡ªé€‚åº”é£æ ¼æ³¨å…¥å’ŒåŒæ­¥æŒ‡å¯¼é€‚åº”çš„ç»Ÿä¸€æ³¨æ„åŠ›å¼•å¯¼æœºåˆ¶ï¼Œå®ç°å…¨å±€é£æ ¼å’Œèº«ä»½å¤–è§‚çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒæç¤ºçš„ä¿çœŸåº¦ã€‚ä¸éœ€è¦å¾®è°ƒæˆ–æ¨ç†é€Ÿåº¦æ…¢çš„æ‰©æ•£æ–¹æ³•ä¸åŒï¼Œæ— é™æ•…äº‹å®Œå…¨åœ¨æµ‹è¯•é˜¶æ®µè¿è¡Œï¼Œå®ç°é«˜èº«ä»½å’Œé£æ ¼çš„ä¸€è‡´æ€§ï¼Œæä¾›è¶…è¿‡ç°æœ‰æœ€å¿«ä¸€è‡´T2Iæ¨¡å‹6å€ä»¥ä¸Šçš„æ¨ç†é€Ÿåº¦ï¼ˆæ¯ç§’ç”Ÿæˆ1.72å¼ å›¾åƒï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†æ— é™æ•…äº‹æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤šæç¤ºå™äº‹åœºæ™¯ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šèº«ä»½ä¸ä¸€è‡´å’Œé£æ ¼ä¸ä¸€è‡´ã€‚</li>
<li>é€šè¿‡å¼•å…¥èº«ä»½æç¤ºæ›¿æ¢æŠ€æœ¯ï¼Œå‡è½»äº†æ–‡æœ¬ç¼–ç å™¨ä¸­çš„ä¸Šä¸‹æ–‡åè§ï¼Œä½¿ä¸åŒæç¤ºé—´çš„èº«ä»½å±æ€§ä¿æŒä¸€è‡´ã€‚</li>
<li>æå‡ºçš„è‡ªé€‚åº”é£æ ¼æ³¨å…¥å’ŒåŒæ­¥æŒ‡å¯¼é€‚åº”çš„ç»Ÿä¸€æ³¨æ„åŠ›å¼•å¯¼æœºåˆ¶ï¼Œå…±åŒä¿è¯äº†å…¨å±€é£æ ¼å’Œèº«ä»½å¤–è§‚çš„ä¸€è‡´æ€§ã€‚</li>
<li>æ— é™æ•…äº‹æ¡†æ¶å®Œå…¨åœ¨æµ‹è¯•é˜¶æ®µè¿è¡Œï¼Œæ— éœ€è®­ç»ƒï¼Œå®ç°äº†å¿«é€Ÿæ¨ç†ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆæ€§èƒ½æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å®é™…è§†è§‰å™äº‹ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a7de20c428568bb44a5244e10b0cd8e" align="middle">
<img src="https://picx.zhimg.com/v2-34495cf12d4a27c4f74a97fb7ef405a3" align="middle">
<img src="https://picx.zhimg.com/v2-c2c715f4700c0b2a3bcb4aa616e04295" align="middle">
<img src="https://picx.zhimg.com/v2-649edf44e85588c504c9f95f07d1ad33" align="middle">
<img src="https://picx.zhimg.com/v2-e2e5d193b335db3b11fe753b621eb75c" align="middle">
<img src="https://picx.zhimg.com/v2-a52332db0c01dfff94577547803cfee4" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SplatSearch-Instance-Image-Goal-Navigation-for-Mobile-Robots-using-3D-Gaussian-Splatting-and-Diffusion-Models"><a href="#SplatSearch-Instance-Image-Goal-Navigation-for-Mobile-Robots-using-3D-Gaussian-Splatting-and-Diffusion-Models" class="headerlink" title="SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models"></a>SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models</h2><p><strong>Authors:Siddarth Narasimhan, Matthew Lisondra, Haitong Wang, Goldie Nejat</strong></p>
<p>The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.</p>
<blockquote>
<p>å®ä¾‹å›¾åƒç›®æ ‡å¯¼èˆªï¼ˆIINï¼‰é—®é¢˜è¦æ±‚éƒ¨ç½²åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„ç§»åŠ¨æœºå™¨äººä»…ä½¿ç”¨å•ä¸ªç›®æ ‡å‚è€ƒå›¾åƒæ¥æœç´¢ç‰¹å®šå¯¹è±¡æˆ–æ„Ÿå…´è¶£çš„äººã€‚å½“é¢ä¸´ä»¥ä¸‹æƒ…å†µæ—¶ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼š1ï¼‰å‚è€ƒå›¾åƒæ˜¯ä»ä»»æ„è§†è§’æ•è·çš„ï¼›2ï¼‰æœºå™¨äººå¿…é¡»åœ¨ç¨€ç–è§†å›¾åœºæ™¯é‡å»ºä¸­è¿›è¡Œæ“ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹IINé—®é¢˜ï¼Œé€šè¿‡å¼•å…¥SplatSearchè¿™ä¸€æ–°å‹æ¶æ„æ¥è§£å†³è¯¥é—®é¢˜ï¼Œè¯¥æ¶æ„åˆ©ç”¨ç¨€ç–è§†å›¾3Dé«˜æ–¯å±•å¸ƒï¼ˆ3DGSï¼‰é‡å»ºã€‚SplatSearchä½¿ç”¨ç¨€ç–åœ¨çº¿3DGSåœ°å›¾åœ¨å€™é€‰å¯¹è±¡å‘¨å›´å‘ˆç°å¤šä¸ªè§†ç‚¹ï¼Œå¹¶ä½¿ç”¨å¤šè§†å›¾æ‰©æ•£æ¨¡å‹å®Œæˆæ¸²æŸ“å›¾åƒçš„ç¼ºå¤±åŒºåŸŸï¼Œä»è€Œå®ç°ä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„ç¨³å¥ç‰¹å¾åŒ¹é…ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„å‰æ²¿æ¢ç´¢ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨åˆæˆè§†ç‚¹çš„è§†è§‰ä¸Šä¸‹æ–‡å’Œç›®æ ‡å›¾åƒçš„è¯­ä¹‰ä¸Šä¸‹æ–‡æ¥è¯„ä¼°å‰æ²¿ä½ç½®ï¼Œä»è€Œä½¿æœºå™¨äººèƒ½å¤Ÿä¼˜å…ˆå¤„ç†åœ¨è¯­ä¹‰å’Œè§†è§‰æ–¹é¢ä¸ç›®æ ‡å›¾åƒç›¸å…³çš„å‰æ²¿ã€‚åœ¨é€¼çœŸå®¶å±…å’ŒçœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„å¤§é‡å®éªŒéªŒè¯äº†SplatSearchç›¸è¾ƒäºå½“å‰æœ€æ–°æ–¹æ³•åœ¨æˆåŠŸç‡å’ŒæˆåŠŸè·¯å¾„é•¿åº¦æ–¹é¢çš„æ›´é«˜æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¯å®äº†SplatSearchçš„è®¾è®¡é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12972v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://splat-search.github.io/">https://splat-search.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ç ”ç©¶äº†Instance Image Goal Navigationï¼ˆIINï¼‰é—®é¢˜ï¼Œå³ç§»åŠ¨æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­ä»…ä½¿ç”¨ç›®æ ‡å›¾åƒè¿›è¡Œç‰¹å®šå¯¹è±¡æˆ–äººç‰©çš„æœç´¢ä»»åŠ¡ã€‚é’ˆå¯¹å‚è€ƒå›¾åƒè§†è§’ä»»æ„åŠæœºå™¨äººéœ€æ“ä½œç¨€ç–è§†å›¾åœºæ™¯é‡å»ºçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†SplatSearchæ¶æ„ã€‚è¯¥æ¶æ„åˆ©ç”¨ç¨€ç–è§†å›¾3Dé«˜æ–¯æ˜ å°„ï¼ˆ3DGSï¼‰é‡å»ºï¼Œé€šè¿‡æ¸²æŸ“å€™é€‰å¯¹è±¡çš„å¤šè§†è§’å¹¶ä½¿ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹å®Œæˆå›¾åƒç¼ºå¤±åŒºåŸŸçš„è¡¥å……ï¼Œä»è€Œå®ç°ä¸ç›®æ ‡å›¾åƒçš„é²æ£’ç‰¹å¾åŒ¹é…ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„è¾¹ç•Œæ¢ç´¢ç­–ç•¥ï¼Œç»“åˆåˆæˆè§†è§’çš„è§†è§‰ä¸Šä¸‹æ–‡ä¸ç›®æ ‡å›¾åƒçš„è¯­ä¹‰ä¸Šä¸‹æ–‡æ¥è¯„ä¼°è¾¹ç•Œä½ç½®ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿä¼˜å…ˆæ¢ç´¢è¯­ä¹‰å’Œè§†è§‰ä¸Šä¸ç›®æ ‡å›¾åƒç›¸å…³çš„è¾¹ç•Œã€‚å®éªŒè¯æ˜ï¼ŒSplatSearchåœ¨çœŸå®å®¶å±…å’ŒçœŸå®ç¯å¢ƒä¸­è¾ƒå½“å‰å…ˆè¿›æ–¹æ³•å…·æœ‰æ›´é«˜çš„æˆåŠŸç‡å’Œæ›´çŸ­çš„è·¯å¾„é•¿åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IINé—®é¢˜è¦æ±‚æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­ä»…ä½¿ç”¨å•ä¸€ç›®æ ‡å›¾åƒè¿›è¡Œç‰¹å®šå¯¹è±¡æˆ–äººç‰©çš„æœç´¢ã€‚</li>
<li>SplatSearchæ¶æ„è§£å†³äº†IINé—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨ç¨€ç–è§†å›¾3Dé«˜æ–¯æ˜ å°„ï¼ˆ3DGSï¼‰è¿›è¡Œé‡å»ºã€‚</li>
<li>SplatSearché€šè¿‡æ¸²æŸ“å€™é€‰å¯¹è±¡çš„å¤šè§†è§’å¹¶ä½¿ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹å®Œæˆå›¾åƒç¼ºå¤±åŒºåŸŸçš„è¡¥å……ï¼Œå®ç°é²æ£’ç‰¹å¾åŒ¹é…ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è¾¹ç•Œæ¢ç´¢ç­–ç•¥ï¼Œç»“åˆè§†è§‰å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡æ¥è¯„ä¼°è¾¹ç•Œä½ç½®çš„ä¼˜å…ˆçº§ã€‚</li>
<li>å®éªŒè¯æ˜SplatSearchåœ¨çœŸå®ç¯å¢ƒä¸­è¾ƒå½“å‰æ–¹æ³•æ€§èƒ½æ›´é«˜ã€‚</li>
<li>SplatSearchçš„æˆåŠŸç‡å’Œè·¯å¾„é•¿åº¦å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e400049e19e2fa458c2be8534c816e2" align="middle">
<img src="https://picx.zhimg.com/v2-4974c1c02830db9f482c2ebb7da7cba8" align="middle">
<img src="https://picx.zhimg.com/v2-a6e879fd3fdb7f2fb81ceec6c69aa0b6" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GrOCE-Graph-Guided-Online-Concept-Erasure-for-Text-to-Image-Diffusion-Models"><a href="#GrOCE-Graph-Guided-Online-Concept-Erasure-for-Text-to-Image-Diffusion-Models" class="headerlink" title="GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models"></a>GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models</h2><p><strong>Authors:Ning Han, Zhenyu Ge, Feng Han, Yuhua Sun, Chengqing Li, Jingjing Chen</strong></p>
<p>Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and FrÃ©chet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.</p>
<blockquote>
<p>æ¦‚å¿µæ“¦é™¤æ—¨åœ¨ä»æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸­åˆ é™¤æœ‰å®³ã€ä¸åˆé€‚æˆ–ç‰ˆæƒå†…å®¹ï¼ŒåŒæ—¶ä¿ç•™éç›®æ ‡è¯­ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–äºæ˜‚è´µçš„å¾®è°ƒï¼Œè¦ä¹ˆåº”ç”¨ç²—ç³™çš„è¯­ä¹‰åˆ†ç¦»ï¼Œç»å¸¸å¯¼è‡´æ— å…³æ¦‚å¿µçš„é€€åŒ–ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹ä¸æ–­å˜åŒ–çš„æ¦‚å¿µé›†çš„é€‚åº”æ€§ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒçš„Graph-Guided Online Concept Erasureï¼ˆGrOCEï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸºäºå›¾çš„è¯­ä¹‰æ¨ç†è¿›è¡Œç²¾ç¡®å’Œè‡ªé€‚åº”çš„æ¦‚å¿µåˆ é™¤ã€‚GrOCEå°†æ¦‚å¿µåŠå…¶ç›¸äº’å…³ç³»å»ºæ¨¡ä¸ºåŠ¨æ€è¯­ä¹‰å›¾ï¼Œå®ç°å¯¹ä¾èµ–å…³ç³»çš„åŸåˆ™æ€§æ¨ç†å’Œå¯¹ä¸éœ€è¦å†…å®¹çš„ç²¾ç»†ç²’åº¦éš”ç¦»ã€‚å®ƒåŒ…æ‹¬ä¸‰ä¸ªç»„ä»¶ï¼š1ï¼‰åŠ¨æ€æ‹“æ‰‘å›¾æ„å»ºï¼Œç”¨äºå¢é‡å›¾æ„å»ºï¼›2ï¼‰è‡ªé€‚åº”é›†ç¾¤è¯†åˆ«ï¼Œç”¨äºå¤šè·³éå†ä¸ç›¸ä¼¼æ€§è¡°å‡è¯„åˆ†ï¼›3ï¼‰é€‰æ‹©æ€§è¾¹ç¼˜åˆ‡æ–­ï¼Œç”¨äºç›®æ ‡è¾¹ç¼˜åˆ é™¤ï¼ŒåŒæ—¶ä¿ç•™å…¨å±€è¯­ä¹‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGrOCEåœ¨æ¦‚å¿µç›¸ä¼¼æ€§å’ŒFrÃ©chet Inception Distanceï¼ˆFIDï¼‰æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œæä¾›äº†é«˜æ•ˆã€å‡†ç¡®ã€ç¨³å®šçš„æ¦‚å¿µæ“¦é™¤ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12968v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ¦‚å¿µæ“¦é™¤æŠ€æœ¯åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨å»é™¤æœ‰å®³ã€ä¸åˆé€‚æˆ–ç‰ˆæƒå†…å®¹ï¼ŒåŒæ—¶ä¿ç•™éç›®æ ‡è¯­ä¹‰ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGraph-Guided Online Concept Erasure (GrOCE)çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŸºäºå›¾çš„è¯­ä¹‰æ¨ç†è¿›è¡Œç²¾ç¡®å’Œè‡ªé€‚åº”çš„æ¦‚å¿µå»é™¤ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯åº”å¯¹ä¸æ–­å˜åŒ–çš„æ¦‚å¿µé›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µæ“¦é™¤çš„ç›®æ ‡æ˜¯ä»æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸­ç§»é™¤æœ‰å®³ã€ä¸åˆé€‚æˆ–ç‰ˆæƒå†…å®¹ï¼ŒåŒæ—¶ä¿ç•™éç›®æ ‡è¯­ä¹‰ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ä¾èµ–æ˜‚è´µçš„å¾®è°ƒæˆ–åº”ç”¨ç²—ç³™è¯­ä¹‰åˆ†ç¦»çš„é—®é¢˜ï¼Œç»å¸¸å¯¼è‡´æ— å…³æ¦‚å¿µé€€åŒ–ï¼Œç¼ºä¹é€‚åº”ä¸æ–­å˜åŒ–æ¦‚å¿µé›†çš„èƒ½åŠ›ã€‚</li>
<li>GrOCEæ¡†æ¶æ˜¯ä¸€ç§åŸºäºå›¾çš„è¯­ä¹‰æ¨ç†è¿›è¡Œç²¾ç¡®å’Œè‡ªé€‚åº”æ¦‚å¿µå»é™¤çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>GrOCEæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šåŠ¨æ€æ‹“æ‰‘å›¾æ„å»ºã€è‡ªé€‚åº”é›†ç¾¤è¯†åˆ«å’Œé€‰æ‹©æ€§è¾¹ç¼˜åˆ‡æ–­ã€‚</li>
<li>åŠ¨æ€æ‹“æ‰‘å›¾æ„å»ºç”¨äºå¢é‡æ„å»ºå›¾ã€‚</li>
<li>è‡ªé€‚åº”é›†ç¾¤è¯†åˆ«é€šè¿‡å¤šè·³éå†å’Œç›¸ä¼¼æ€§è¡°å‡è¯„åˆ†æ¥è¯†åˆ«å¹¶å¤„ç†æ¦‚å¿µé›†ç¾¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12968">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57a29b42cd6e4ff586a94686b4adc7ad" align="middle">
<img src="https://picx.zhimg.com/v2-b544e1fcffc10f98a6554f3168f23c57" align="middle">
<img src="https://picx.zhimg.com/v2-50e536bdd270f4b7bff8a18b5f7123ac" align="middle">
<img src="https://picx.zhimg.com/v2-519447c7396b57402f97d9e12deb275a" align="middle">
<img src="https://picx.zhimg.com/v2-a28c8f5cf7feb6bd2805ce653f4b729e" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PFAvatar-Pose-Fusion-3D-Personalized-Avatar-Reconstruction-from-Real-World-Outfit-of-the-Day-Photos"><a href="#PFAvatar-Pose-Fusion-3D-Personalized-Avatar-Reconstruction-from-Real-World-Outfit-of-the-Day-Photos" class="headerlink" title="PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos"></a>PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos</h2><p><strong>Authors:Dianbing Xi, Guoyuan An, Jingsen Zhu, Zhijian Liu, Yuan Liu, Ruiyuan Zhang, Jiayuan Lu, Rui Wang, Yuchi Huo</strong></p>
<p>We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from &#96;&#96;Outfit of the Dayâ€™â€™ (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions&#x2F;truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†PFAvatarï¼ˆå§¿æ€èåˆåŒ–èº«ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥ä»â€œæ—¥å¸¸ç©¿æ­â€ï¼ˆOOTDï¼‰ç…§ç‰‡é‡å»ºé«˜è´¨é‡çš„ä¸‰ç»´åŒ–èº«ã€‚è¿™äº›ç…§ç‰‡å±•ç°å‡ºå¤šç§å§¿æ€ã€é®æŒ¡å’Œå¤æ‚èƒŒæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰é€šè¿‡å°‘é‡OOTDæ ·æœ¬å¯¹å§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›ï¼ˆ2ï¼‰é€šè¿‡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¡¨ç¤ºä¸‰ç»´åŒ–èº«å¹¶è¿›è¡Œæç‚¼ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä¸ä»¥å¾€å°†å›¾åƒåˆ†å‰²æˆèµ„äº§ï¼ˆå¦‚æœè£…ã€é…é¥°ï¼‰è¿›è¡Œä¸‰ç»´ç»„è£…çš„æ–¹æ³•ä¸åŒï¼Œè¿™ç§æ–¹æ³•å®¹æ˜“å¯¼è‡´ä¸ä¸€è‡´æ€§ã€‚æˆ‘ä»¬é¿å…åˆ†è§£ï¼Œç›´æ¥å¯¹å…¨èº«å¤–è§‚è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡é›†æˆé¢„è®­ç»ƒçš„ControlNetè¿›è¡Œå§¿æ€ä¼°è®¡å’Œæ–°é¢–çš„æ¡ä»¶å…ˆéªŒä¿ç•™æŸå¤±ï¼ˆCPPLï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç«¯åˆ°ç«¯å­¦ä¹ ä¸­ç²¾ç»†ç»†èŠ‚ï¼ŒåŒæ—¶å‡è½»å°‘é‡è®­ç»ƒä¸­çš„è¯­è¨€æ¼‚ç§»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»…5åˆ†é’Ÿå†…å®Œæˆä¸ªæ€§åŒ–è®¾ç½®ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”å®ç°äº†48å€çš„åŠ é€Ÿã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºNeRFçš„åŒ–èº«è¡¨ç¤ºï¼Œé€šè¿‡è§„èŒƒçš„SMPL-Xç©ºé—´é‡‡æ ·å’Œå¤šåˆ†è¾¨ç‡3D-SDSè¿›è¡Œä¼˜åŒ–ã€‚ä¸åŸºäºç½‘æ ¼çš„è¡¨ç¤ºæ–¹æ³•ç›¸æ¯”ï¼Œåè€…å—é™äºåˆ†è¾¨ç‡çš„ç¦»æ•£åŒ–å’Œé®æŒ¡çš„å‡ ä½•é”™è¯¯ï¼Œæˆ‘ä»¬è¿ç»­çš„è¾å°„åœºå¯ä»¥ä¿ç•™é«˜é¢‘çº¹ç†ï¼ˆä¾‹å¦‚å¤´å‘ï¼‰ï¼Œå¹¶é€šè¿‡é€æ˜åº¦æ­£ç¡®åœ°å¤„ç†é®æŒ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒPFAvataråœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™å’ŒæŠ—é®æŒ¡&#x2F;æˆªæ–­æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæ¨åŠ¨äº†ä»ç°å®ä¸–ç•ŒOOTDç›¸å†Œç”Ÿæˆå®ç”¨ä¸‰ç»´åŒ–èº«çš„å‘å±•ã€‚æ­¤å¤–ï¼Œé‡å»ºçš„ä¸‰ç»´åŒ–èº«æ”¯æŒä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚è™šæ‹Ÿè¯•ç©¿ã€åŠ¨ç”»å’Œäººä½“è§†é¢‘é‡æ’­ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„é€šç”¨æ€§å’Œå®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12935v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong><br>    æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºPFAvatarï¼ˆå§¿æ€èåˆåŒ–èº«ï¼‰çš„æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»å¤šæ ·åŒ–çš„å§¿æ€ã€é®æŒ¡å’Œå¤æ‚èƒŒæ™¯çš„â€œæ¯æ—¥ç©¿æ­â€ï¼ˆOOTDï¼‰ç…§ç‰‡ä¸­é‡å»ºé«˜è´¨é‡çš„ä¸‰ç»´åŒ–èº«ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šä¸€æ˜¯å¾®è°ƒå§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ä»¥é€‚åº”å°‘é‡OOTDç¤ºä¾‹ï¼ŒäºŒæ˜¯é€šè¿‡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¡¨ç¤ºä¸‰ç»´åŒ–èº«ã€‚è¯¥æ–¹æ³•é¿å…äº†èµ„äº§åˆ†è§£ï¼Œç›´æ¥å¯¹å…¨èº«å¤–è§‚è¿›è¡Œå»ºæ¨¡ï¼Œèƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…å®Œæˆä¸ªæ€§åŒ–å®šåˆ¶ï¼Œå¹¶å®ç°äº†é«˜åˆ†è¾¨ç‡çº¹ç†å’Œæ­£ç¡®é®æŒ¡å¤„ç†ã€‚å®éªŒç»“æœè¯æ˜äº†PFAvataråœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™ä»¥åŠå¯¹é®æŒ¡å’Œæˆªæ–­æƒ…å†µçš„é²æ£’æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œé‡å»ºçš„ä¸‰ç»´åŒ–èº«æ”¯æŒä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚è™šæ‹Ÿè¯•ç©¿ã€åŠ¨ç”»å’Œäººç±»è§†é¢‘é‡æ–°æ¼”ç»ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„é€šç”¨æ€§å’Œå®ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PFAvataræ–¹æ³•å¯ä»¥ä»OOTDç…§ç‰‡ä¸­é‡å»ºé«˜è´¨é‡çš„ä¸‰ç»´åŒ–èº«ã€‚</li>
<li>æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¾®è°ƒå§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹å’Œé€šè¿‡NeRFè¡¨ç¤ºä¸‰ç»´åŒ–èº«ã€‚</li>
<li>é¿å…èµ„äº§åˆ†è§£ï¼Œç›´æ¥å¯¹å…¨èº«å¤–è§‚è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°ä¸ªæ€§åŒ–å®šåˆ¶ã€‚</li>
<li>ç«¯åˆ°ç«¯å­¦ä¹ æ–¹æ³•å¯ä»¥ç²¾ç»†ç»†èŠ‚ï¼ŒåŒæ—¶å‡è½»è¯­è¨€æ¼‚ç§»é—®é¢˜ã€‚</li>
<li>NeRFè¡¨ç¤ºèƒ½å¤Ÿä¿ç•™é«˜åˆ†è¾¨ç‡çº¹ç†å¹¶æ­£ç¡®å¤„ç†é®æŒ¡æƒ…å†µã€‚</li>
<li>PFAvataråœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™å’Œå¯¹é®æŒ¡&#x2F;æˆªæ–­æƒ…å†µçš„é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7babe9aa8f4e76891007ba85aec4735" align="middle">
<img src="https://picx.zhimg.com/v2-f69f246eef6a79116dd8a1c3c0437772" align="middle">
<img src="https://picx.zhimg.com/v2-dc4cce51cb10c3588fd127745a8e1a41" align="middle">
<img src="https://picx.zhimg.com/v2-5c8c04429049eeef36d71747b460ea83" align="middle">
<img src="https://picx.zhimg.com/v2-79f18b03f7c81325d79e755ba0fe6749" align="middle">
<img src="https://picx.zhimg.com/v2-87c8e6052af98dbaef6dec80e7c738af" align="middle">
<img src="https://picx.zhimg.com/v2-a8d8f82dc104053190c27ed1c2c01c14" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="cryoSENSE-Compressive-Sensing-Enables-High-throughput-Microscopy-with-Sparse-and-Generative-Priors-on-the-Protein-Cryo-EM-Image-Manifold"><a href="#cryoSENSE-Compressive-Sensing-Enables-High-throughput-Microscopy-with-Sparse-and-Generative-Priors-on-the-Protein-Cryo-EM-Image-Manifold" class="headerlink" title="cryoSENSE: Compressive Sensing Enables High-throughput Microscopy with Sparse and Generative Priors on the Protein Cryo-EM Image Manifold"></a>cryoSENSE: Compressive Sensing Enables High-throughput Microscopy with Sparse and Generative Priors on the Protein Cryo-EM Image Manifold</h2><p><strong>Authors:Zain Shabeeb, Daniel Saeedi, Darin Tsui, Vida Jamali, Amirali Aghazadeh</strong></p>
<p>Cryo-electron microscopy (cryo-EM) enables the atomic-resolution visualization of biomolecules; however, modern direct detectors generate data volumes that far exceed the available storage and transfer bandwidth, thereby constraining practical throughput. We introduce cryoSENSE, the computational realization of a hardware-software co-designed framework for compressive cryo-EM sensing and acquisition. We show that cryo-EM images of proteins lie on low-dimensional manifolds that can be independently represented using sparse priors in predefined bases and generative priors captured by a denoising diffusion model. cryoSENSE leverages these low-dimensional manifolds to enable faithful image reconstruction from spatial and Fourier-domain undersampled measurements while preserving downstream structural resolution. In experiments, cryoSENSE increases acquisition throughput by up to 2.5$\times$ while retaining the original 3D resolution, offering controllable trade-offs between the number of masked measurements and the level of downsampling. Sparse priors favor faithful reconstruction from Fourier-domain measurements and moderate compression, whereas generative diffusion priors achieve accurate recovery from pixel-domain measurements and more severe undersampling. Project website: <a target="_blank" rel="noopener" href="https://cryosense.github.io/">https://cryosense.github.io</a>.</p>
<blockquote>
<p>å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰èƒ½å¤Ÿå®ç°ç”Ÿç‰©åˆ†å­çš„åŸå­åˆ†è¾¨ç‡å¯è§†åŒ–ï¼›ç„¶è€Œï¼Œç°ä»£ç›´æ¥æ¢æµ‹å™¨ç”Ÿæˆçš„æ•°æ®é‡è¿œè¿œè¶…è¿‡äº†å¯ç”¨çš„å­˜å‚¨å’Œä¼ è¾“å¸¦å®½ï¼Œä»è€Œé™åˆ¶äº†å®é™…ååé‡ã€‚æˆ‘ä»¬å¼•å…¥äº†cryoSENSEï¼Œè¿™æ˜¯ç¡¬ä»¶å’Œè½¯ä»¶ååŒè®¾è®¡æ¡†æ¶çš„è®¡ç®—å®ç°ï¼Œç”¨äºå‹ç¼©å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰æ„ŸçŸ¥å’Œé‡‡é›†ã€‚æˆ‘ä»¬è¯æ˜è›‹ç™½è´¨å†·å†»ç”µå­æ˜¾å¾®é•œå›¾åƒä½äºä½ç»´æµå½¢ä¸Šï¼Œå¯ä»¥ä½¿ç”¨é¢„å®šä¹‰åŸºä¸­çš„ç¨€ç–å…ˆéªŒå’Œç”±é™å™ªæ‰©æ•£æ¨¡å‹æ•è·çš„ç”Ÿæˆå…ˆéªŒè¿›è¡Œç‹¬ç«‹è¡¨ç¤ºã€‚cryoSENSEåˆ©ç”¨è¿™äº›ä½ç»´æµå½¢å®ç°ä»ç©ºé—´å’Œå‚…é‡Œå¶åŸŸæ¬ é‡‡æ ·æµ‹é‡è¿›è¡Œå¿ å®å›¾åƒé‡å»ºï¼ŒåŒæ—¶ä¿ç•™ä¸‹æ¸¸ç»“æ„åˆ†è¾¨ç‡ã€‚åœ¨å®éªŒä¸­ï¼ŒcryoSENSEå°†é‡‡é›†é€Ÿåº¦æé«˜äº†é«˜è¾¾2.5å€ï¼ŒåŒæ—¶ä¿æŒäº†åŸå§‹çš„3Dåˆ†è¾¨ç‡ï¼Œåœ¨æ©è†œæµ‹é‡çš„æ•°é‡å’Œæ¬ é‡‡æ ·çš„ç¨‹åº¦ä¹‹é—´å®ç°äº†å¯æ§çš„æƒè¡¡ã€‚ç¨€ç–å…ˆéªŒæœ‰åˆ©äºä»å‚…é‡Œå¶åŸŸæµ‹é‡å’Œé€‚åº¦å‹ç¼©ä¸­å®ç°å¿ å®é‡å»ºï¼Œè€Œç”Ÿæˆæ‰©æ•£å…ˆéªŒåˆ™å¯ä»åƒç´ åŸŸæµ‹é‡å’Œæ›´ä¸¥é‡çš„æ¬ é‡‡æ ·ä¸­å®ç°å‡†ç¡®æ¢å¤ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://cryosense.github.io./">https://cryosense.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12931v1">PDF</a> </p>
<p><strong>Summary</strong><br>     cryoSENSE æ˜¯è½¯ç¡¬ä»¶ååŒè®¾è®¡çš„è®¡ç®—æ¡†æ¶ï¼Œç”¨äºå‹ç¼©ä½æ¸©ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰å›¾åƒé‡‡é›†ä¸é‡å»ºã€‚æ¡†æ¶æ”¯æŒå›¾åƒç¨€ç–å…ˆéªŒç”Ÿæˆå’Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹é‡å»ºè¿‡ç¨‹ä»¥ä»æ¬ é‡‡æ ·æ•°æ®ä¸­å¿ å®é‡å»ºå›¾åƒå¹¶ä¿æŒç»“æ„åˆ†è¾¨ç‡ã€‚å®éªŒæ˜¾ç¤ºï¼ŒcryoSENSE å¯æé«˜é‡‡é›†é€Ÿåº¦è¾¾ 2.5 å€åŒæ—¶ä¿æŒåŸæœ‰ 3D åˆ†è¾¨ç‡ã€‚è¯¥æ¡†æ¶å¯æ ¹æ®æ©è†œæµ‹é‡æ•°é‡å’Œæ¬ é‡‡æ ·ç¨‹åº¦è¿›è¡Œæ§åˆ¶æƒè¡¡ã€‚æ¡†æ¶å…è®¸åœ¨æ›´æç«¯çš„æ¬ é‡‡æ ·æ¡ä»¶ä¸‹ï¼Œä»åƒç´ åŸŸæµ‹é‡ä¸­è·å¾—å‡†ç¡®çš„é‡å»ºç»“æœã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://cryosense.github.io/">https://cryosense.github.io</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>cryoSENSE æ˜¯ç”¨äºå‹ç¼©ä½æ¸©ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰å›¾åƒé‡‡é›†ä¸é‡å»ºçš„è®¡ç®—æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨å›¾åƒç¨€ç–å…ˆéªŒå’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé‡å»ºã€‚</li>
<li>cryoSENSE å¯æé«˜é‡‡é›†é€Ÿåº¦è¾¾ 2.5 å€ï¼ŒåŒæ—¶ä¿æŒåŸæœ‰ 3D åˆ†è¾¨ç‡ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒä»æ¬ é‡‡æ ·æ•°æ®ä¸­å¿ å®é‡å»ºå›¾åƒã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨ç¡¬ä»¶å’Œè½¯ä»¶ååŒè®¾è®¡ä»¥å®ç°é«˜æ•ˆæ•°æ®é‡‡é›†å’Œé‡å»ºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯åœ¨æç«¯çš„æ¬ é‡‡æ ·æ¡ä»¶ä¸‹è·å¾—å‡†ç¡®çš„é‡å»ºç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef2a87cf25b86188ed1485f3ca9ac1cd" align="middle">
<img src="https://picx.zhimg.com/v2-038b5e44f1910f0ec2af4ff16417eab1" align="middle">
<img src="https://picx.zhimg.com/v2-8ef69c11ab792ed08de9fa9f8b104593" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Algorithms-Trained-on-Normal-Chest-X-rays-Can-Predict-Health-Insurance-Types"><a href="#Algorithms-Trained-on-Normal-Chest-X-rays-Can-Predict-Health-Insurance-Types" class="headerlink" title="Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types"></a>Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types</h2><p><strong>Authors:Chi-Yu Chen, Rawan Abulibdeh, Arash Asgari, Leo Anthony Celi, Deirdre Goode, Hassan Hamidi, Laleh Seyyed-Kalantari, Ned McCague, Thomas Sounack, Po-Chih Kuo</strong></p>
<p>Artificial intelligence is revealing what medicine never intended to encode. Deep vision models, trained on chest X-rays, can now detect not only disease but also invisible traces of social inequality. In this study, we show that state-of-the-art architectures (DenseNet121, SwinV2-B, MedMamba) can predict a patientâ€™s health insurance type, a strong proxy for socioeconomic status, from normal chest X-rays with significant accuracy (AUC around 0.67 on MIMIC-CXR-JPG, 0.68 on CheXpert). The signal persists even when age, race, and sex are controlled for, and remains detectable when the model is trained exclusively on a single racial group. Patch-based occlusion reveals that the signal is diffuse rather than localized, embedded in the upper and mid-thoracic regions. This suggests that deep networks may be internalizing subtle traces of clinical environments, equipment differences, or care pathways; learning socioeconomic segregation itself. These findings challenge the assumption that medical images are neutral biological data. By uncovering how models perceive and exploit these hidden social signatures, this work reframes fairness in medical AI: the goal is no longer only to balance datasets or adjust thresholds, but to interrogate and disentangle the social fingerprints embedded in clinical data itself.</p>
<blockquote>
<p>äººå·¥æ™ºèƒ½æ­£åœ¨æ­ç¤ºåŒ»å­¦ä»æœªæ‰“ç®—ç¼–ç çš„ä¿¡æ¯ã€‚åŸºäºèƒ¸éƒ¨Xå°„å›¾åƒçš„æ·±åº¦è§†è§‰æ¨¡å‹ç°åœ¨ä¸ä»…èƒ½å¤Ÿæ£€æµ‹ç–¾ç—…ï¼Œè¿˜èƒ½å‘ç°ç¤¾ä¼šä¸å¹³ç­‰çš„éšå½¢ç—•è¿¹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ¶æ„ï¼ˆDenseNet121ã€SwinV2-Bã€MedMambaï¼‰èƒ½ä»æ­£å¸¸çš„èƒ¸éƒ¨Xå°„å›¾åƒä¸­å‡†ç¡®é¢„æµ‹æ‚£è€…çš„å¥åº·ä¿é™©ç±»å‹ï¼ˆä½œä¸ºç¤¾ä¼šç»æµåœ°ä½çš„å¼ºçƒˆä»£ç†æŒ‡æ ‡ï¼‰ï¼Œå…¶å‡†ç¡®åº¦è¾ƒé«˜ï¼ˆåœ¨MIMIC-CXR-JPGä¸Šä¸ºAUCçº¦0.67ï¼Œåœ¨CheXpertä¸Šä¸º0.68ï¼‰ã€‚å³ä½¿åœ¨æ§åˆ¶å¹´é¾„ã€ç§æ—å’Œæ€§åˆ«åï¼Œè¿™ä¸€ä¿¡å·ä¾ç„¶å­˜åœ¨ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹ä»…é’ˆå¯¹å•ä¸€ç§æ—ç¾¤ä½“è¿›è¡Œè®­ç»ƒæ—¶ä»ç„¶å¯æ£€æµ‹ã€‚åŸºäºè¡¥ä¸çš„é®æŒ¡è¡¨æ˜ä¿¡å·æ˜¯åˆ†æ•£çš„è€Œéå±€éƒ¨çš„ï¼Œä½äºä¸Šèƒ¸éƒ¨å’Œä¸­èƒ¸éƒ¨åŒºåŸŸã€‚è¿™è¡¨æ˜æ·±åº¦ç½‘ç»œå¯èƒ½æ­£åœ¨å†…åŒ–ä¸´åºŠç¯å¢ƒã€è®¾å¤‡å·®å¼‚æˆ–æŠ¤ç†è·¯å¾„çš„å¾®å¦™ç—•è¿¹ï¼›å­¦ä¹ ç¤¾ä¼šç»æµåˆ†å‰²æœ¬èº«ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†åŒ»å­¦å›¾åƒæ˜¯ä¸­ç«‹ç”Ÿç‰©æ•°æ®çš„å‡è®¾ã€‚é€šè¿‡æ­ç¤ºæ¨¡å‹å¦‚ä½•æ„ŸçŸ¥å’Œåˆ©ç”¨è¿™äº›éšè—çš„ç¤¾ä¼šç­¾åï¼Œè¿™é¡¹å·¥ä½œé‡æ–°å®šä¹‰äº†åŒ»ç–—äººå·¥æ™ºèƒ½çš„å…¬å¹³æ€§ï¼šç›®æ ‡ä¸å†ä»…ä»…æ˜¯å¹³è¡¡æ•°æ®é›†æˆ–è°ƒæ•´é˜ˆå€¼ï¼Œè€Œæ˜¯è´¨ç–‘å’Œåˆ†è§£åµŒå…¥åœ¨ä¸´åºŠæ•°æ®ä¸­çš„ç¤¾ä¼šæŒ‡çº¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11030v2">PDF</a> Submitting to MIDL 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶çš„æ¨¡å‹å±•ç¤ºäº†æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å¯¹åŒ»å­¦å½±åƒæ•°æ®è§£è¯»çš„è¿‡ç¨‹ä¸­å¯èƒ½å¸¦æœ‰çš„ç¤¾ä¼šå±æ€§åè§ï¼Œå³ä¾¿æ˜¯é¢å¯¹ä¸æ¶‰åŠç—…ç†çš„èƒ¸éƒ¨Xå…‰ç‰‡ä¹Ÿèƒ½å¤Ÿåˆ†æå‡ºæ‚£è€…æ‰€äº«å—çš„åŒ»ç–—ä¿é™©ç­‰çº§å’ŒèƒŒåæ½œåœ¨çš„ç¤¾ä¼šç»æµçŠ¶å†µç­‰éšç§ä¿¡æ¯ã€‚åˆ†æè¿‡ç¨‹ä¸­é‡‡ç”¨äº†ä¸€ç³»åˆ—çš„æ·±åº¦è§†è§‰æ¨¡å‹ï¼ˆå¦‚DenseNet121ã€SwinV2-Bå’ŒMedMambaç­‰ï¼‰ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒçš„ä¸åŒåŒºåŸŸæ•æ‰åˆ°ä¸æŸç§å¥åº·ä¿éšœç›¸å…³è”çš„ä¿¡æ¯ä¿¡å·ã€‚å› æ­¤è¦æ±‚é‡æ–°å®¡è§†åŒ»ç–—å½±åƒæ•°æ®çš„å…¬å¹³æ€§é—®é¢˜ï¼Œä¸ä»…ä»…æ˜¯æ•°æ®é›†å¹³è¡¡æˆ–é˜ˆå€¼è°ƒæ•´çš„é—®é¢˜ï¼Œæ›´æ¶‰åŠåˆ°ä¸´åºŠæ•°æ®ä¸­éšè—çš„ä¸ç¤¾ä¼šæ€§è”ç³»å¤æ‚çš„æŒ‡ç—•å‰–æä¸åˆ†ç¦»çš„ä»»åŠ¡ã€‚é€šè¿‡åˆ†æè¿™ç§ç°è±¡å¯äº†è§£å¦‚ä½•ä¿ƒè¿›å…¬æ­£ä¸”åˆä¹ä¼¦ç†çš„åŒ»ç–—AIç³»ç»Ÿè®¾è®¡ä¸åº”ç”¨ã€‚å¯¹æ­¤æ·±å…¥ç ”ç©¶æä¾›äº†æŒ‘æˆ˜è§†è§’çš„æ·±å…¥ç ”ç©¶æ„è§åŒæ—¶å¯¹æ•´ä¸ªåŒ»å­¦è¡Œä¸šå…³äºæ•°æ®å’ŒAIä½¿ç”¨çš„çœ‹æ³•å¸¦æ¥æ·±è¿œçš„å½±å“ã€‚åŒæ—¶ä¹Ÿæ„å‘³ç€ç®—æ³•å¼€å‘è€…å¿…é¡»å¯¹AIæŠ€æœ¯èƒ½å¤Ÿç¼–ç ä¸è§£è¯»å‡ºä¸ªä½“éš¾ä»¥å¯Ÿè§‰çš„ç¤¾ä¼šåœ°ä½èƒŒæ™¯æœ‰æ‰€äº†è§£ã€‚å½“å‰çš„ç»“æœæ„å‘³ç€æ— æ³•å¿½ç•¥ç”±äºåŒ»å­¦æˆåƒç¯å¢ƒï¼Œè®¾å¤‡å·®å¼‚æˆ–è€…æŠ¤ç†é€”å¾„çš„å·®å¼‚è€Œå¯èƒ½é€ æˆç¤¾ä¼šèº«ä»½è¢«æš´éœ²çš„æƒ…å†µã€‚æ¨¡å‹ä¸ä»…é€šè¿‡è¯†åˆ«ç—…ç†ç‰¹å¾ï¼Œè¿˜é€šè¿‡æ•æ‰å¾®å¦™çš„ä¸´åºŠç¯å¢ƒå·®å¼‚æ¥æ­ç¤ºæ‚£è€…çš„ç¤¾ä¼šç»æµçŠ¶å†µã€‚è¿™è¿›ä¸€æ­¥æ­ç¤ºäº†åŒ»å­¦å›¾åƒå¹¶éå•çº¯åæ˜ ç”Ÿç‰©å­¦ä¿¡æ¯çš„è§‚ç‚¹ã€‚è¿™ä¸€å‘ç°å°†é‡å¡‘åŒ»ç–—é¢†åŸŸå¯¹å½±åƒæ•°æ®åŠå…¶ä½¿ç”¨æ–¹å¼çš„çœ‹æ³•ï¼Œä¿ƒä½¿æœªæ¥æ›´åŠ æ³¨é‡å¯¹æ•°æ®ä½¿ç”¨çš„ç›‘ç®¡åŠç ”ç©¶åº”ç”¨çš„å…¬å¹³æ€§å®¡æŸ¥å·¥ä½œã€‚æ€»è€Œè¨€ä¹‹ï¼Œè¯¥é¡¹ç ”ç©¶å¼ºè°ƒï¼Œç°ä»£æŠ€æœ¯ä½¿å¾—æˆåƒæ‰€å¸¦æ¥é™¤äº†æ²»ç–—æ€§çš„å†…å®¹è¿˜åŒ…æ‹¬æ— å½¢çš„ç¤¾ä¼šå­¦å¤æ‚æ€§ç‰¹å¾çš„æ³„éœ²å€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶ä»¥ä¿è¯ä¿¡æ¯åˆ©ç”¨æ—¢ç¬¦åˆå…¬å¹³æ­£ä¹‰åˆå…·æœ‰äººæ€§åŒ–ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæœªæ¥çš„åŒ»ç–—AIç³»ç»Ÿéœ€è¦æ›´åŠ å…³æ³¨å¦‚ä½•é¿å…è¿™ç§ç¤¾ä¼šåè§çš„åµŒå…¥å’Œä¼ æ’­ã€‚åŒæ—¶ï¼Œè¿™ä¹Ÿæé†’æˆ‘ä»¬é‡æ–°å®¡è§†äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ä¼¦ç†é—®é¢˜ï¼Œä»¥ç¡®ä¿æŠ€æœ¯çš„å…¬å¹³æ€§å’Œå…¬æ­£æ€§ã€‚åŒæ—¶éœ€è¦é‡‡å–æ›´å¤šæªæ–½ç¡®ä¿éšç§ä¿æŠ¤å’Œæ•°æ®å®‰å…¨ï¼Œé¿å…å¼•å‘ç¤¾ä¼šæ­§è§†ç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œä¹Ÿéœ€è¦åŠ å¼ºå¯¹äºç›¸å…³é¢†åŸŸçš„ä¼¦ç†å®¡æŸ¥å·¥ä½œï¼Œä»¥ç¡®ä¿æ–°æŠ€æœ¯çš„å‘å±•ä¸ä¼šå¯¹ç¤¾ä¼šå…¬å¹³å’Œå…¬æ­£é€ æˆå¨èƒã€‚å¯¹æ­¤å‘ç°çš„è®¤è¯†ä¸åç»­åº”ç”¨ç ”ç©¶å°†æ˜¯åŒ»ç–—äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡ç‚¹ä¹‹ä¸€ã€‚æœ¬ç ”ç©¶çš„ç»“æœä¸ºæˆ‘ä»¬æä¾›äº†é‡æ–°è®¤è¯†AIåº”ç”¨çš„è§†è§’ã€‚åªæœ‰å½»åº•ç†è§£äº†åŒ»å­¦å½±åƒæ•°æ®ä¸ç¤¾ä¼šç»æµç°è±¡é—´çš„æ½œåœ¨è”ç³»æ‰èƒ½çœŸæ­£é©¾é©­è¿™ç±»å¼ºå¤§æŠ€æœ¯çš„æ½œèƒ½ä¸ºç¤¾ä¼šæœåŠ¡å¹¶ä¸”ä¿éšœå…¬ä¼—çš„åˆ©ç›Šã€‚é€šè¿‡ä¸€ç³»åˆ—æ·±å…¥ç»†è‡´çš„ç ”ç©¶æ­¥éª¤å¯¹å…¬å¹³æ€§å’Œä¼¦ç†æ€§çš„æ¢ç´¢ä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½çš„è¿›æ­¥å¥ å®šäº†åšå®çš„åŸºç¡€ä¹Ÿä¸ºæœªæ¥æ›´å¹¿æ³›åœ°æ¨è¿›æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„ç»“åˆé“ºå¹³äº†é“è·¯ä»è€Œæ›´åŠ å…¨é¢æ·±åˆ»åœ°æ¨è¿›ç§‘æŠ€æœåŠ¡äºäººç±»ç¤¾ä¼šè¿™ä¸€ç›®æ ‡çš„å®ç°ä¹Ÿå¼€è¾Ÿäº†æ–°æ€è·¯æ¨åŠ¨äº†æŠ€æœ¯çš„å…¬æ­£æ€§ä¸æ™®åŠåŒ–çš„å…±åŒè¿›å±•å› æ­¤å®ƒçš„å‡ºç°å¿…å°†å¸¦æ¥æ·±è¿œçš„ç§¯æå½±å“å¹¶å°†ç»§ç»­æˆä¸ºè¡Œä¸šå†…çš„ç ”ç©¶çƒ­ç‚¹æ–¹å‘ä¹‹ä¸€å¹¶ä¸ºåŒ»å­¦ç§‘æŠ€ä¸ç¤¾ä¼šä¼¦ç†å¸¦æ¥é•¿è¿œçš„ç§¯æè´¡çŒ®æœªæ¥ç ”ç©¶è€…å°†èƒ½å¤Ÿå……åˆ†åˆ©ç”¨æ­¤ç±»å·¥å…·è¿›ä¸€æ­¥æ¨åŠ¨åŒ»å­¦äººå·¥æ™ºèƒ½é¢†åŸŸçš„è¿›æ­¥ä¸å‘å±•ä¸ºå…¬ä¼—çš„å¥åº·ç¦ç¥‰åšå‡ºæ›´å¤§çš„è´¡çŒ®ã€‚æˆ‘ä»¬é¢ä¸´çš„æŒ‘æˆ˜æ˜¯å¦‚ä½•ç¡®ä¿æŠ€æœ¯çš„å…¬å¹³æ€§å’Œå…¬æ­£æ€§å¹¶æ¶ˆé™¤å…¶å¯èƒ½å¸¦æ¥çš„åè§å’Œæ­§è§†ä»¥ç¡®ä¿æ‰€æœ‰äººéƒ½èƒ½å…¬å¹³åœ°äº«å—æŠ€æœ¯å¸¦æ¥çš„å¥½å¤„ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦ç»§ç»­æ·±å…¥ç ”ç©¶å¹¶æ¢ç´¢æ–°çš„è§£å†³æ–¹æ¡ˆä»¥ç¡®ä¿äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨èƒ½å¤ŸçœŸæ­£é€ ç¦äºäººç±»å¹¶æ¨åŠ¨ç§‘æŠ€è¿›æ­¥çš„æ­¥ä¼åŒæ—¶ä¹Ÿè¦éµå¾ªå…¬å¹³å…¬æ­£çš„åŸºæœ¬åŸåˆ™æ¨è¿›æŠ€æœ¯åº”ç”¨æ›´åŠ å¹¿æ³›åœ°è¦†ç›–å„ä¸ªé¢†åŸŸå’Œè¡Œä¸šä»¥è§£å†³æˆ‘ä»¬é¢ä¸´çš„å„ç±»é—®é¢˜å’ŒæŒ‘æˆ˜åŒæ—¶è¿˜è¦é‡è§†æ–°æŠ€æœ¯åº”ç”¨å¸¦æ¥çš„ä¼¦ç†é“å¾·é—®é¢˜ç¡®ä¿äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¥åº·å‘å±•èƒ½å¤Ÿä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥é•¿è¿œçš„ç§¯æå½±å“ä¸ºæ„å»ºæ›´åŠ å…¬å¹³å’Œè°çš„ç¤¾ä¼šè´¡çŒ®åŠ›é‡åŒæ—¶è¯¥ç ”ç©¶è¿˜ä¸ºæˆ‘ä»¬æä¾›äº†åœ¨ç®—æ³•è®¾è®¡ä¹‹åˆå°±æ³¨é‡å…¬å¹³æ€§å’Œä¼¦ç†æ€§çš„æ€è€ƒæ¡†æ¶å¸®åŠ©æˆ‘ä»¬é¿å…åè§å’Œæ­§è§†çš„å‡ºç°ä»è€Œä¿è¯ç®—æ³•å…¬æ­£æ€§å’Œé€æ˜åº¦ä»¥å®ç°æ›´åŠ å¹¿æ³›çš„äººå·¥æ™ºèƒ½æŠ€æœ¯åº”ç”¨åŠå…¶æ‰€å¸¦æ¥çš„ç§¯æå½±å“ä¸ºæˆ‘ä»¬æ„å»ºæ›´åŠ å…¬æ­£å’Œå…¬å¹³çš„ç¤¾ä¼šæä¾›äº†æ–°çš„è§†è§’å’Œæ€è€ƒæ–¹å‘ç¡®ä¿äº†æŠ€æœ¯åœ¨å‘å±•åº”ç”¨è¿‡ç¨‹ä¸­å§‹ç»ˆä¿æŒå…¬å¹³æ­£ä¹‰çš„åº•çº¿ç¡®ä¿äº†ç¤¾ä¼šå…¬å¹³å…¬æ­£æ€§åŒæ—¶ä¹Ÿèƒ½ä¸ºç¤¾ä¼šå…¬å…±åˆ©ç›Šå¸¦æ¥æ›´å¤šæ­£å‘æ„ä¹‰å¹¶å®ç°ç§‘å­¦æŠ€æœ¯ä¼¦ç†åŸåˆ™çš„å…±äº«ä½¿å¾—æ–°çš„æ™ºèƒ½æŠ€æœ¯å’Œç§‘æŠ€åˆ›æ–°åº”ç”¨è¶Šæ¥è¶Šå®‰å…¨å¯é çš„ååº”ç¤¾ä¼šå„é˜¶å±‚éœ€æ±‚çš„å…±åŒå¯Œè£•åŸåˆ™ä¸ºæœªæ¥æ›´å…ˆè¿›çš„ç®—æ³•æŠ€æœ¯å’Œç ”ç©¶é“è·¯å¥ å®šäº†åŸºç¡€æ”¯æ’‘ç€æˆ‘ä»¬ä¸æ–­æ¢ç´¢å¦‚ä½•ä¼˜åŒ–æœºå™¨å­¦ä¹ ç®—æ³•å’Œç¤¾ä¼šç¦åˆ©éœ€æ±‚çš„åè°ƒå‘å±•é€šè¿‡ä¸æ–­æ”¹è¿›æ¨¡å‹çš„åº”ç”¨æ–¹å¼æ–¹æ³•å’ŒæŠ€æœ¯å¤„ç†æªæ–½é€æ­¥æ¶ˆé™¤äº†ä¸å…¬å¹³ä¸å…¬æ­£çš„å¹²æ‰°å› ç´ åŒæ—¶ç»§ç»­ä¿æŒæ¨åŠ¨ç§‘æŠ€çš„å¿«é€Ÿå‘å±•ä¿ƒè¿›ç¤¾ä¼šæ•´ä½“çš„ç¹è£ä¸è¿›æ­¥ä¿éšœäº†äººä»¬çš„å…¬å¹³è·å¾—æœåŠ¡å®ç°ä»¥äººä¸ºæœ¬çš„ç›®æ ‡ä¸ºæˆ‘ä»¬çš„åŒ»ç–—ç§‘ç ”å‘å±•å’Œä¸´åºŠå®è·µå¸¦æ¥äº†æ–°çš„å¥‘æœºä¸å‘å±•æœºé‡æ‹“å±•äº†ç§‘å­¦ç ”ç©¶çš„è§†é‡ä¿ƒè¿›äº†äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨åŒ»å­¦é¢†åŸŸçš„æ·±åº¦åº”ç”¨ä¸ºæ„å»ºå’Œè°ç¤¾ä¼šè´¡çŒ®å‡ºæ›´å¤§çš„åŠ›é‡ä¹Ÿä¸ºäººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å…¶ä»–é¢†åŸŸçš„åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„ç†è®ºæ”¯æ’‘å’Œç§‘å­¦ä¾æ®æ˜ç¡®äº†ç ”ç©¶æ–¹å‘å…·æœ‰é‡è¦çš„é‡Œç¨‹ç¢‘æ„ä¹‰<br>    &#96;&#96;&#96;summary&#96;&#96;ï¼ˆè¿™é‡Œå°†ç»™å‡ºå¯¹æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹è¿›è¡Œçš„ç®€åŒ–æ¦‚è¿°ï¼‰ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2a061b700de5889a086aa0a865da236" align="middle">
<img src="https://picx.zhimg.com/v2-745b0f92dcaf92bbf88f598197c3ee19" align="middle">
<img src="https://picx.zhimg.com/v2-b95e1e429731f5711d49c948ab2a617b" align="middle">
<img src="https://picx.zhimg.com/v2-743fda727ec2d9e013128f701a6f5aea" align="middle">
<img src="https://picx.zhimg.com/v2-4ced01ae56d78a6be7fb7bffccd3c788" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Virtual-Multiplex-Staining-for-Histological-Images-using-a-Marker-wise-Conditioned-Diffusion-Model"><a href="#Virtual-Multiplex-Staining-for-Histological-Images-using-a-Marker-wise-Conditioned-Diffusion-Model" class="headerlink" title="Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model"></a>Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model</h2><p><strong>Authors:Hyun-Jic Oh, Junsik Kim, Zhiyi Shi, Yichen Wu, Yu-An Chen, Peter K. Sorger, Hanspeter Pfister, Won-Ki Jeong</strong></p>
<p>Multiplex imaging is revolutionizing pathology by enabling the simultaneous visualization of multiple biomarkers within tissue samples, providing molecular-level insights that traditional hematoxylin and eosin (H&amp;E) staining cannot provide. However, the complexity and cost of multiplex data acquisition have hindered its widespread adoption. Additionally, most existing large repositories of H&amp;E images lack corresponding multiplex images, limiting opportunities for multimodal analysis. To address these challenges, we leverage recent advances in latent diffusion models (LDMs), which excel at modeling complex data distributions by utilizing their powerful priors for fine-tuning to a target domain. In this paper, we introduce a novel framework for virtual multiplex staining that utilizes pretrained LDM parameters to generate multiplex images from H&amp;E images using a conditional diffusion model. Our approach enables marker-by-marker generation by conditioning the diffusion model on each marker, while sharing the same architecture across all markers. To tackle the challenge of varying pixel value distributions across different marker stains and to improve inference speed, we fine-tune the model for single-step sampling, enhancing both color contrast fidelity and inference efficiency through pixel-level loss functions. We validate our framework on two publicly available datasets, notably demonstrating its effectiveness in generating up to 18 different marker types with improved accuracy, a substantial increase over the 2-3 marker types achieved in previous approaches. This validation highlights the potential of our framework, pioneering virtual multiplex staining. Finally, this paper bridges the gap between H&amp;E and multiplex imaging, potentially enabling retrospective studies and large-scale analyses of existing H&amp;E image repositories.</p>
<blockquote>
<p>å¤šè·¯æˆåƒæŠ€æœ¯èƒ½å¤Ÿé€šè¿‡åœ¨ç»„ç»‡æ ·æœ¬ä¸­åŒæ—¶å¯è§†åŒ–å¤šä¸ªç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œä¸ºç—…ç†å­¦å¸¦æ¥é©å‘½æ€§çš„å˜é©ï¼Œæä¾›åˆ†å­æ°´å¹³çš„è§è§£ï¼Œè¿™æ˜¯ä¼ ç»Ÿçš„è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰æŸ“è‰²æ— æ³•æä¾›çš„ã€‚ç„¶è€Œï¼Œå¤šè·¯æ•°æ®è·å–çš„å¤æ‚æ€§å’Œæˆæœ¬é˜»ç¢äº†å…¶å¹¿æ³›é‡‡ç”¨ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰çš„H&amp;Eå›¾åƒå¤§å‹å­˜å‚¨åº“ç¼ºå°‘ç›¸åº”çš„å¤šè·¯å›¾åƒï¼Œé™åˆ¶äº†å¤šæ¨¡å¼åˆ†æçš„æœºä¼šã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œè¯¥æ¨¡å‹é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†å¯¹ç›®æ ‡é¢†åŸŸè¿›è¡Œå¾®è°ƒï¼Œæ“…é•¿å¯¹å¤æ‚æ•°æ®åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒçš„LDMå‚æ•°ç”Ÿæˆå¤šè·¯å›¾åƒçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ä»H&amp;Eå›¾åƒç”Ÿæˆå¤šè·¯å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é’ˆå¯¹æ¯ä¸ªæ ‡å¿—ç‰©å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶å¤„ç†æ¥ç”Ÿæˆæ ‡å¿—ç‰©ï¼ŒåŒæ—¶å…±äº«æ‰€æœ‰æ ‡å¿—ç‰©çš„ç›¸åŒæ¶æ„ã€‚ä¸ºäº†è§£å†³ä¸åŒæ ‡å¿—æŸ“è‰²ä¹‹é—´åƒç´ å€¼åˆ†å¸ƒçš„å˜åŒ–å¹¶åŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œäº†å•æ­¥é‡‡æ ·çš„å¾®è°ƒï¼Œé€šè¿‡åƒç´ çº§æŸå¤±å‡½æ•°æé«˜äº†é¢œè‰²å¯¹æ¯”åº¦å’Œä¿çœŸåº¦ä»¥åŠæ¨ç†æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œæ˜¾è‘—è¯æ˜äº†å…¶åœ¨ç”Ÿæˆå¤šè¾¾18ç§ä¸åŒç±»å‹çš„æ ‡å¿—ç‰©æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå‡†ç¡®æ€§æœ‰æ‰€æé«˜ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”å®ç°äº†å¤§é‡çš„æ ‡å¿—ç‰©ç±»å‹å¢é•¿ã€‚è¿™ä¸€éªŒè¯çªæ˜¾äº†æˆ‘ä»¬æ¡†æ¶çš„æ½œåŠ›ï¼Œå¼€åˆ›äº†è™šæ‹Ÿå¤šè·¯æŸ“è‰²çš„å…ˆæ²³ã€‚æœ€åï¼Œæœ¬æ–‡æ¶èµ·äº†H&amp;Eå’Œå¤šè·¯æˆåƒä¹‹é—´çš„æ¡¥æ¢ï¼Œæœ‰æœ›å®ç°å¯¹ç°æœ‰H&amp;Eå›¾åƒå­˜å‚¨åº“çš„å›é¡¾æ€§ç ”ç©¶å’Œå¤§è§„æ¨¡åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14681v2">PDF</a> AAAI 2026 accepted</p>
<p><strong>Summary</strong></p>
<p>åœ¨ç—…ç†å­¦ä¸­ï¼Œå¤šé‡æˆåƒæŠ€æœ¯èƒ½å¤Ÿé€šè¿‡åŒæ—¶å¯è§†åŒ–ç»„ç»‡æ ·æœ¬ä¸­çš„å¤šä¸ªç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œä¸ºåˆ†å­æ°´å¹³çš„æ´å¯Ÿæä¾›å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ï¼Œè¿™æ˜¯ä¼ ç»Ÿçš„è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰æŸ“è‰²æ— æ³•å®ç°çš„ã€‚ç„¶è€Œï¼Œå¤šé‡æ•°æ®è·å–çš„å¤æ‚æ€§å’Œæˆæœ¬é˜»ç¢äº†å…¶å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶åˆ©ç”¨æ½œä¼æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§è™šæ‹Ÿå¤šé‡æŸ“è‰²çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»H&amp;Eå›¾åƒç”Ÿæˆå¤šé‡å›¾åƒã€‚æœ¬ç ”ç©¶çš„æ–¹æ³•èƒ½å¤Ÿé€šè¿‡é’ˆå¯¹æ¯ä¸ªæ ‡è®°ç‰©å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼ŒåŒæ—¶åœ¨æ‰€æœ‰æ ‡è®°ç‰©ä¹‹é—´å…±äº«ç›¸åŒçš„æ¶æ„ï¼Œå®ç°é€æ ‡è®°çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜è§£å†³äº†ä¸åŒæ ‡è®°ç‰©æŸ“è‰²åƒç´ å€¼åˆ†å¸ƒä¸å‡çš„é—®é¢˜ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆäº†å¤šè¾¾18ç§ä¸åŒçš„æ ‡è®°ç±»å‹ï¼Œå‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼Œè¿œè¶…ä»¥å‰æ–¹æ³•å®ç°çš„2-3ç§æ ‡è®°ç±»å‹ã€‚è¯¥ç ”ç©¶ä¸ºè™šæ‹Ÿå¤šé‡æŸ“è‰²å¼€è¾Ÿäº†æ–°çš„é€”å¾„ï¼Œå¹¶æœ‰æœ›ç¼©å°H&amp;EæŸ“è‰²ä¸å¤šé‡æˆåƒä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šé‡æˆåƒæŠ€æœ¯èƒ½å¤Ÿåœ¨åˆ†å­å±‚é¢æä¾›ä¼ ç»ŸæŸ“è‰²æ— æ³•è·å¾—çš„æ´å¯ŸåŠ›ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®è·å–çš„å¤æ‚æ€§å’Œæˆæœ¬é—®é¢˜ã€‚</li>
<li>æ½œä¼æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ç”¨äºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå®ƒé€šè¿‡å¼ºå¤§çš„å…ˆéªŒæ¨¡å‹å¯¹ç›®æ ‡é¢†åŸŸè¿›è¡Œå¾®è°ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è™šæ‹Ÿå¤šé‡æŸ“è‰²æ¡†æ¶ï¼Œèƒ½å¤Ÿä»H&amp;Eå›¾åƒç”Ÿæˆå¤šé‡å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†é€æ ‡è®°çš„ç”Ÿæˆï¼Œé€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œã€‚</li>
<li>ç ”ç©¶è§£å†³äº†ä¸åŒæ ‡è®°ç‰©æŸ“è‰²åƒç´ å€¼åˆ†å¸ƒä¸å‡çš„é—®é¢˜ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦å’Œé¢œè‰²å¯¹æ¯”åº¦çš„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91100b4371ec36b59bb014a31c5f3194" align="middle">
<img src="https://picx.zhimg.com/v2-4b229528f2e8d496b70ec9ccd08d4f5b" align="middle">
<img src="https://picx.zhimg.com/v2-051898a6763848de1fc7927cda1a7dfb" align="middle">
<img src="https://picx.zhimg.com/v2-11194d78cef729cf51ca442281912197" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DogFit-Domain-guided-Fine-tuning-for-Efficient-Transfer-Learning-of-Diffusion-Models"><a href="#DogFit-Domain-guided-Fine-tuning-for-Efficient-Transfer-Learning-of-Diffusion-Models" class="headerlink" title="DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models"></a>DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models</h2><p><strong>Authors:Yara Bahram, Mohammadhadi Shateri, Eric Granger</strong></p>
<p>Transfer learning of diffusion models to smaller target domains is challenging, as naively fine-tuning the model often results in poor generalization. Test-time guidance methods help mitigate this by offering controllable improvements in image fidelity through a trade-off with sample diversity. However, this benefit comes at a high computational cost, typically requiring dual forward passes during sampling. We propose the Domain-guided Fine-tuning (DogFit) method, an effective guidance mechanism for diffusion transfer learning that maintains controllability without incurring additional computational overhead. DogFit injects a domain-aware guidance offset into the training loss, effectively internalizing the guided behavior during the fine-tuning process. The domain-aware design is motivated by our observation that during fine-tuning, the unconditional source model offers a stronger marginal estimate than the target model. To support efficient controllable fidelity-diversity trade-offs at inference, we encode the guidance strength value as an additional model input through a lightweight conditioning mechanism. We further investigate the optimal placement and timing of the guidance offset during training and propose two simple scheduling strategies, i.e., late-start and cut-off, which improve generation quality and training stability. Experiments on DiT and SiT backbones across six diverse target domains show that DogFit can outperform prior guidance methods in transfer learning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling TFLOPS. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yaramohamadi/DogFit">https://github.com/yaramohamadi/DogFit</a>.</p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„è¿ç§»å­¦ä¹ åœ¨è½¬ç§»åˆ°è¾ƒå°çš„ç›®æ ‡åŸŸæ—¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç®€å•åœ°å¾®è°ƒæ¨¡å‹é€šå¸¸ä¼šå¯¼è‡´æ³›åŒ–æ€§èƒ½ä¸ä½³ã€‚æµ‹è¯•æ—¶çš„æŒ‡å¯¼æ–¹æ³•æœ‰åŠ©äºé€šè¿‡æ ·æœ¬å¤šæ ·æ€§çš„æƒè¡¡æ¥æä¾›å¯æ§çš„å›¾åƒä¿çœŸåº¦æ”¹è¿›ã€‚ç„¶è€Œï¼Œè¿™ç§å¥½å¤„éœ€è¦ä»˜å‡ºé«˜æ˜‚çš„è®¡ç®—æˆæœ¬ï¼Œé€šå¸¸åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­éœ€è¦ä¸¤æ¬¡æ­£å‘ä¼ é€’ã€‚æˆ‘ä»¬æå‡ºäº†Domain-guided Fine-tuningï¼ˆDogFitï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ‰©æ•£è¿ç§»å­¦ä¹ æŒ‡å¯¼æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ é¢å¤–è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ä¿æŒå¯æ§æ€§ã€‚DogFitå°†åŸŸæ„ŸçŸ¥æŒ‡å¯¼åç§»é‡æ³¨å…¥è®­ç»ƒæŸå¤±ä¸­ï¼Œä»è€Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°å†…åŒ–æŒ‡å¯¼è¡Œä¸ºã€‚åŸŸæ„ŸçŸ¥è®¾è®¡æ˜¯å—åˆ°æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„å¯å‘ï¼Œå³åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ— æ¡ä»¶æºæ¨¡å‹æä¾›çš„è¾¹é™…ä¼°è®¡æ¯”ç›®æ ‡æ¨¡å‹æ›´å¼ºã€‚ä¸ºäº†æ”¯æŒåœ¨æ¨ç†æ—¶è¿›è¡Œé«˜æ•ˆçš„å¯æ§ä¿çœŸåº¦-å¤šæ ·æ€§æƒè¡¡ï¼Œæˆ‘ä»¬é€šè¿‡è½»é‡çº§æ¡ä»¶æœºåˆ¶å°†æŒ‡å¯¼å¼ºåº¦å€¼ç¼–ç ä¸ºé™„åŠ æ¨¡å‹è¾“å…¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç ”ç©¶äº†æŒ‡å¯¼åç§»åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³æ”¾ç½®ä½ç½®å’Œæ—¶æœºï¼Œå¹¶æå‡ºäº†ä¸¤ç§ç®€å•çš„è°ƒåº¦ç­–ç•¥ï¼Œå³å»¶è¿Ÿå¯åŠ¨å’Œæˆªæ­¢ï¼Œè¿™æé«˜äº†ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚åœ¨å…­ä¸ªä¸åŒç›®æ ‡åŸŸä¸Šçš„DiTå’ŒSiTä¸»å¹²ç½‘ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDogFitåœ¨è¿ç§»å­¦ä¹ çš„FIDå’ŒFDDINOV2æ–¹é¢å¯ä»¥ä¼˜äºå…ˆå‰çš„æŒ‡å¯¼æ–¹æ³•ï¼ŒåŒæ—¶é‡‡æ ·TFLOPSå‡å°‘äº†ä¸€åŠã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yaramohamadi/DogFit%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yaramohamadi/DogFitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05685v4">PDF</a> Accepted for poster presentation at AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨è½¬ç§»åˆ°å°å‹ç›®æ ‡åŸŸæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶ä»‹ç»äº†Domain-guided Fine-tuningï¼ˆDogFitï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æŒ‡å¯¼æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ‰©æ•£è½¬ç§»å­¦ä¹ ä¸­ä¿æŒå¯æ§æ€§ï¼ŒåŒæ—¶ä¸ä¼šå¢åŠ é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚DogFité€šè¿‡æ³¨å…¥é¢†åŸŸæ„ŸçŸ¥æŒ‡å¯¼åç§»é‡æ¥ä¼˜åŒ–è®­ç»ƒæŸå¤±ï¼Œåˆ©ç”¨æ— æ¡ä»¶æºæ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æä¾›æ›´å¼ºå¤§çš„è¾¹é™…ä¼°è®¡ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡è½»é‡çº§æ¡ä»¶æœºåˆ¶å°†æŒ‡å¯¼å¼ºåº¦å€¼ç¼–ç ä¸ºé™„åŠ æ¨¡å‹è¾“å…¥ï¼Œä»¥å®ç°å¯æ§çš„ä¿çœŸåº¦ä¸å¤šæ ·æ€§çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†æŒ‡å¯¼åç§»åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³æ”¾ç½®å’Œæ—¶æœºï¼Œå¹¶æå‡ºäº†ä¸¤ç§ç®€å•çš„è°ƒåº¦ç­–ç•¥ï¼Œå³æ™šæœŸå¼€å§‹å’Œæˆªæ­¢ç­–ç•¥ï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDogFitåœ¨å…­ä¸ªä¸åŒçš„ç›®æ ‡åŸŸä¸Šä¼˜äºå…ˆå‰çš„æŒ‡å¯¼æ–¹æ³•ï¼ŒåŒæ—¶å‡å°‘äº†é‡‡æ ·è®¡ç®—é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è½¬ç§»åˆ°å°å‹ç›®æ ‡åŸŸæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç›´æ¥å¾®è°ƒå¯èƒ½å¯¼è‡´è¾ƒå·®çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•æ—¶çš„æŒ‡å¯¼æ–¹æ³•å¯ä»¥é€šè¿‡æ§åˆ¶å›¾åƒä¿çœŸåº¦ä¸æ ·æœ¬å¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æå‡ºäº†Domain-guided Fine-tuningï¼ˆDogFitï¼‰æ–¹æ³•ï¼Œèƒ½åœ¨ä¿æŒå¯æ§æ€§çš„åŒæ—¶ï¼Œä¸å¢åŠ é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚</li>
<li>DogFité€šè¿‡æ³¨å…¥é¢†åŸŸæ„ŸçŸ¥æŒ‡å¯¼åç§»é‡æ¥ä¼˜åŒ–è®­ç»ƒæŸå¤±ï¼Œåˆ©ç”¨æ— æ¡ä»¶æºæ¨¡å‹çš„æ›´å¼ºè¾¹é™…ä¼°è®¡ã€‚</li>
<li>DogFité‡‡ç”¨è½»é‡çº§æ¡ä»¶æœºåˆ¶ï¼Œå°†æŒ‡å¯¼å¼ºåº¦å€¼ä½œä¸ºé™„åŠ æ¨¡å‹è¾“å…¥ï¼Œä»¥å®ç°å¯æ§çš„ä¿çœŸåº¦ä¸å¤šæ ·æ€§çš„æƒè¡¡ã€‚</li>
<li>DogFitæ¢è®¨äº†æŒ‡å¯¼åç§»åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³æ”¾ç½®å’Œæ—¶æœºï¼Œå¹¶æå‡ºäº†ä¸¤ç§ç®€å•çš„è°ƒåº¦ç­–ç•¥ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDogFitåœ¨å¤šä¸ªç›®æ ‡åŸŸä¸Šä¼˜äºå…¶ä»–æŒ‡å¯¼æ–¹æ³•ï¼Œé™ä½äº†é‡‡æ ·è®¡ç®—é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd4b0fd34074e812504ecd7942b65889" align="middle">
<img src="https://picx.zhimg.com/v2-27b6adb0bad827280ac1f42c0a1bb31d" align="middle">
<img src="https://picx.zhimg.com/v2-1afb772025c8da57f4a876b9fe8e8ae3" align="middle">
<img src="https://picx.zhimg.com/v2-d2497b28e7107707eddcdd91e7348960" align="middle">
<img src="https://picx.zhimg.com/v2-187d49a67fd2966b255978999de3b7b2" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="HierarchicalPrune-Position-Aware-Compression-for-Large-Scale-Diffusion-Models"><a href="#HierarchicalPrune-Position-Aware-Compression-for-Large-Scale-Diffusion-Models" class="headerlink" title="HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models"></a>HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models</h2><p><strong>Authors:Young D. Kwon, Rui Li, Sijia Li, Da Li, Sourav Bhattacharya, Stylianos I. Venieris</strong></p>
<p>State-of-the-art text-to-image diffusion models (DMs) achieve remarkable quality, yet their massive parameter scale (8-11B) poses significant challenges for inferences on resource-constrained devices. In this paper, we present HierarchicalPrune, a novel compression framework grounded in a key observation: DM blocks exhibit distinct functional hierarchies, where early blocks establish semantic structures while later blocks handle texture refinements. HierarchicalPrune synergistically combines three techniques: (1) Hierarchical Position Pruning, which identifies and removes less essential later blocks based on position hierarchy; (2) Positional Weight Preservation, which systematically protects early model portions that are essential for semantic structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts knowledge-transfer intensity based on our discovery of block-wise sensitivity variations. As a result, our framework brings billion-scale diffusion models into a range more suitable for on-device inference, while preserving the quality of the output images. Specifically, combined with INT4 weight quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction (e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score and 7% in HPSv2 score compared to the original model. Finally, our comprehensive user study with 85 participants demonstrates that HierarchicalPrune maintains perceptual quality comparable to the original model while significantly outperforming prior works.</p>
<blockquote>
<p>å…ˆè¿›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è¾¾åˆ°äº†æ˜¾è‘—çš„è´¨é‡æ°´å¹³ï¼Œç„¶è€Œå…¶åºå¤§çš„å‚æ•°è§„æ¨¡ï¼ˆ8-11Bï¼‰å¯¹èµ„æºå—é™è®¾å¤‡ä¸Šçš„æ¨ç†æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HierarchicalPruneï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å‹ç¼©æ¡†æ¶ï¼Œå®ƒåŸºäºä¸€ä¸ªå…³é”®è§‚å¯Ÿï¼šDMå—è¡¨ç°å‡ºä¸åŒçš„åŠŸèƒ½å±‚æ¬¡ï¼Œæ—©æœŸå—å»ºç«‹è¯­ä¹‰ç»“æ„ï¼Œè€ŒåæœŸå—å¤„ç†çº¹ç†ç»†åŒ–ã€‚HierarchicalPruneååŒç»“åˆäº†ä¸‰ç§æŠ€æœ¯ï¼šï¼ˆ1ï¼‰å±‚æ¬¡ä½ç½®å‰ªæï¼Œå®ƒæ ¹æ®ä½ç½®å±‚æ¬¡è¯†åˆ«å¹¶ç§»é™¤ä¸å¤ªé‡è¦çš„åæœŸå—ï¼›ï¼ˆ2ï¼‰ä½ç½®æƒé‡ä¿ç•™ï¼Œå®ƒç³»ç»Ÿåœ°ä¿æŠ¤æ—©æœŸæ¨¡å‹éƒ¨åˆ†ï¼Œå¯¹äºè¯­ä¹‰ç»“æ„å®Œæ•´æ€§è‡³å…³é‡è¦ï¼›ï¼ˆ3ï¼‰æ•æ„Ÿåº¦å¼•å¯¼è’¸é¦ï¼Œå®ƒæ ¹æ®æˆ‘ä»¬å‘ç°çš„å—çº§æ•æ„Ÿåº¦å˜åŒ–è°ƒæ•´çŸ¥è¯†è½¬ç§»å¼ºåº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å°†è§„æ¨¡åºå¤§çš„æ‰©æ•£æ¨¡å‹å¼•å…¥åˆ°æ›´é€‚åˆäºè®¾å¤‡ç«¯æ¨ç†çš„èŒƒå›´ï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºå›¾åƒçš„è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œç»“åˆINT4æƒé‡é‡åŒ–ï¼ŒHierarchicalPruneå®ç°äº†77.5-80.4%çš„å†…å­˜å ç”¨å‡å°‘ï¼ˆä¾‹å¦‚ï¼Œä»15.8 GBå‡å°‘åˆ°3.2 GBï¼‰ï¼Œåœ¨æœåŠ¡å™¨å’Œæ¶ˆè´¹çº§GPUä¸Šæµ‹é‡çš„å»¶è¿Ÿå‡å°‘äº†27.9-38.0%ï¼Œä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼ŒGenEvalå¾—åˆ†æœ€ä½ä¸‹é™äº†2.6%ï¼ŒHPSv2å¾—åˆ†ä¸‹é™äº†7%ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç»¼åˆç”¨æˆ·ç ”ç©¶ï¼Œå…±æœ‰85åå‚ä¸è€…ï¼Œè¯æ˜HierarchicalPruneåœ¨ä¿æŒä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„å¯æ„ŸçŸ¥è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04663v2">PDF</a> Accepted at AAAI 2026 (Main Technical Track)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„å‹ç¼©æ¡†æ¶â€”â€”HierarchicalPruneã€‚è¯¥æ¡†æ¶åˆ©ç”¨DMå—çš„åŠŸèƒ½å±‚æ¬¡ç»“æ„ç‰¹ç‚¹ï¼Œç»“åˆä¸‰ç§æŠ€æœ¯å®ç°æ¨¡å‹å‹ç¼©ï¼Œæ—¨åœ¨å°†å¤§è§„æ¨¡çš„æ‰©æ•£æ¨¡å‹é€‚åº”äºèµ„æºå—é™è®¾å¤‡ä¸Šçš„æ¨ç†è®¡ç®—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆINT4æƒé‡é‡åŒ–ï¼ŒHierarchicalPruneåœ¨ä¿æŒå›¾åƒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå¤§å¹…å‡å°‘äº†æ¨¡å‹çš„å†…å­˜å ç”¨å’Œæ¨ç†å»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HierarchicalPruneæ˜¯ä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å‹ç¼©æ¡†æ¶ï¼ŒåŸºäºæ¨¡å‹å—çš„åŠŸèƒ½å±‚æ¬¡ç»“æ„è¿›è¡Œè®¾è®¡ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†ä¸‰ç§æŠ€æœ¯ï¼šHierarchical Position Pruningã€Positional Weight Preservationå’ŒSensitivity-Guided Distillationï¼Œä»¥å®ç°æ¨¡å‹çš„æœ‰æ•ˆå‹ç¼©ã€‚</li>
<li>HierarchicalPruneèƒ½å¤Ÿå°†å¤§è§„æ¨¡çš„æ‰©æ•£æ¨¡å‹é€‚åº”äºèµ„æºå—é™è®¾å¤‡ä¸Šçš„æ¨ç†è®¡ç®—ï¼Œå‡å°äº†å†…å­˜å ç”¨å’Œæ¨ç†å»¶è¿Ÿã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ç»“åˆINT4æƒé‡é‡åŒ–åï¼Œå®ç°äº†æ˜¾è‘—çš„å†…å­˜å ç”¨å‡å°‘ï¼ˆä¾‹å¦‚ï¼Œä»15.8 GBå‡å°‘åˆ°3.2 GBï¼‰å’Œæ¨ç†å»¶è¿Ÿé™ä½ã€‚</li>
<li>HierarchicalPruneåœ¨ä¿æŒå›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼ŒGenEvalå¾—åˆ†ä»…ä¸‹é™2.6%ï¼ŒHPSv2å¾—åˆ†ä¸‹é™7%ã€‚</li>
<li>é€šè¿‡ä¸85åå‚ä¸è€…çš„ç»¼åˆç”¨æˆ·ç ”ç©¶ï¼Œè¯æ˜äº†HierarchicalPruneåœ¨ä¿æŒæ„ŸçŸ¥è´¨é‡ä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04663">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a08876e9cfb8e78e06cca2075f768c90" align="middle">
<img src="https://picx.zhimg.com/v2-bab7c203c26dd0f49f3828443314b1a4" align="middle">
<img src="https://picx.zhimg.com/v2-4d01a485688bf9caa8a13adfecfc8b0c" align="middle">
<img src="https://picx.zhimg.com/v2-ffd6c53725a1bf1c9b35add2668e31c8" align="middle">
<img src="https://picx.zhimg.com/v2-72f106a28e0f5fa017a910c804eee6e0" align="middle">
<img src="https://picx.zhimg.com/v2-d956754309ee6cb188082cbe078b0169" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Dream-Lift-Animate-From-Single-Images-to-Animatable-Gaussian-Avatars"><a href="#Dream-Lift-Animate-From-Single-Images-to-Animatable-Gaussian-Avatars" class="headerlink" title="Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars"></a>Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars</h2><p><strong>Authors:Marcel C. BÃ¼hler, Ye Yuan, Xueting Li, Yangyi Huang, Koki Nagano, Umar Iqbal</strong></p>
<p>We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs animatable 3D human avatars from a single image. This is achieved by leveraging multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of 3D Gaussians. Given an image, we first dream plausible multi-views using a video diffusion model, capturing rich geometric and appearance details. These views are then lifted into unstructured 3D Gaussians. To enable animation, we propose a transformer-based encoder that models global spatial relationships and projects these Gaussians into a structured latent representation aligned with the UV space of a parametric body model. This latent code is decoded into UV-space Gaussians that can be animated via body-driven deformation and rendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV manifold, our method ensures consistency during animation while preserving fine visual details. DLA enables real-time rendering and intuitive editing without requiring post-processing. Our method outperforms state-of-the-art approaches on the ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric accuracy. By combining the generative strengths of video diffusion models with a pose-aware UV-space Gaussian mapping, DLA bridges the gap between unstructured 3D representations and high-fidelity, animation-ready avatars.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Dreamã€Liftã€Animateï¼ˆDLAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒèƒ½ä»å•å¼ å›¾ç‰‡é‡å»ºå¯åŠ¨æ€è°ƒæ•´çš„3Däººç±»è§’è‰²ã€‚è¿™é€šè¿‡åˆ©ç”¨å¤šè§†è§’ç”Ÿæˆã€3Dé«˜æ–¯æå‡å’Œå§¿æ€æ„ŸçŸ¥çš„UVç©ºé—´é«˜æ–¯æ˜ å°„æ¥å®ç°ã€‚ç»™å®šä¸€å¼ å›¾ç‰‡ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹æ¢¦è§å¯èƒ½çš„å¤šè§†è§’ï¼Œæ•æ‰ä¸°å¯Œçš„å‡ ä½•å’Œå¤–è§‚ç»†èŠ‚ã€‚ç„¶åï¼Œè¿™äº›è§†è§’è¢«æå‡ä¸ºæ— ç»“æ„çš„3Dé«˜æ–¯ã€‚ä¸ºäº†å®ç°åŠ¨ç”»æ•ˆæœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå˜å‹å™¨çš„ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨èƒ½å¤Ÿæ¨¡æ‹Ÿå…¨å±€ç©ºé—´å…³ç³»å¹¶å°†è¿™äº›é«˜æ–¯æŠ•å½±åˆ°ä¸å‚æ•°åŒ–èº«ä½“æ¨¡å‹çš„UVç©ºé—´å¯¹é½çš„ç»“æ„åŒ–æ½œåœ¨è¡¨ç¤ºä¸­ã€‚è¿™ä¸ªæ½œåœ¨ä»£ç è¢«è§£ç ä¸ºUVç©ºé—´é«˜æ–¯ï¼Œå¯ä»¥é€šè¿‡èº«ä½“é©±åŠ¨å˜å½¢è¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼Œå¹¶æ ¹æ®å§¿æ€å’Œè§†ç‚¹è¿›è¡Œæ¸²æŸ“ã€‚é€šè¿‡å°†é«˜æ–¯é”šå®šåˆ°UVæµå½¢ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†åŠ¨ç”»è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†ç²¾ç»†çš„è§†è§‰ç»†èŠ‚ã€‚DLAæ— éœ€åæœŸå¤„ç†å³å¯å®ç°å®æ—¶æ¸²æŸ“å’Œç›´è§‚ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ActorsHQå’Œ4D-Dressæ•°æ®é›†ä¸Šçš„æ„ŸçŸ¥è´¨é‡å’Œå…‰åº¦ç²¾åº¦æ–¹é¢éƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚é€šè¿‡å°†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸å§¿æ€æ„ŸçŸ¥çš„UVç©ºé—´é«˜æ–¯æ˜ å°„ç›¸ç»“åˆï¼ŒDLAåœ¨æ„å»ºæ— ç»“æ„çš„ä¸‰ç»´è¡¨ç¤ºå’Œé«˜ä¿çœŸåŠ¨ç”»è§’è‰²ä¹‹é—´æ¶èµ·äº†ä¸€åº§æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15979v2">PDF</a> Accepted to 3DV 2026</p>
<p><strong>Summary</strong></p>
<p>æ¢¦ã€æå‡ã€åŠ¨ç”»ï¼ˆDLAï¼‰æ¡†æ¶èƒ½ä»å•å¼ å›¾åƒé‡å»ºå¯åŠ¨åŒ–çš„3Däººç±»è§’è‰²ã€‚åˆ©ç”¨å¤šè§†è§’ç”Ÿæˆã€3Dé«˜æ–¯æå‡å’Œå§¿æ€æ„ŸçŸ¥UVç©ºé—´æ˜ å°„ï¼Œå®Œæˆç”Ÿæˆæ„ŸçŸ¥è§†é¢‘å’Œæ˜ å°„åŠ¨ç”»çš„æ“ä½œã€‚è¯¥æ¡†æ¶å¯å®ç°ä¸°å¯Œçš„å‡ ä½•å’Œå¤–è§‚ç»†èŠ‚ï¼Œå¹¶èƒ½åœ¨åŠ¨ç”»è¿‡ç¨‹ä¸­ä¿æŒä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†çš„è§†è§‰ç»†èŠ‚ã€‚ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼ŒDLAåœ¨æ„ŸçŸ¥è´¨é‡å’Œå…‰åº¦å‡†ç¡®æ€§ä¸Šæ›´èƒœä¸€ç­¹ã€‚DLAæ¡†æ¶å®ç°äº†ä»éç»“æ„åŒ–åˆ°åŠ¨ç”»å°±ç»ªçš„é«˜ä¿çœŸè§’è‰²çš„è½¬å˜ã€‚è¿™ä¸€æŠ€æœ¯çš„è¿ç”¨èŒƒå›´éå¸¸å¹¿æ³›ï¼Œèƒ½å¤Ÿä¸ºåŠ¨ç”»åˆ›ä½œè€…æä¾›æ— é™çš„å¯èƒ½æ€§ã€‚æ¢¦ç³»åˆ—æ¡†æ¶æˆåŠŸåœ°æŠŠè§†è§‰æ‰©æ•£æ¨¡å‹çš„åŠ›é‡ä¸å§¿æ€æ„ŸçŸ¥UVç©ºé—´æ˜ å°„æŠ€æœ¯ç»“åˆï¼Œå¼€åˆ›äº†æ–°çš„åŠ¨ç”»é¢†åŸŸã€‚å®ƒæ”¹å˜äº†ä¼ ç»Ÿçš„åŠ¨ç”»åˆ¶ä½œæ–¹å¼ï¼Œä¸ºåˆ›ä½œè€…å¸¦æ¥å…¨æ–°çš„ä½“éªŒã€‚æ€»ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶å¯¹äºåŠ¨ç”»åˆ¶ä½œå…·æœ‰åˆ’æ—¶ä»£çš„æ„ä¹‰ã€‚<br>  Dream, Lift, Animate (DLA) framework is capable of reconstructing animated 3D human avatars from a single image, achieving rich geometric and appearance details through multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping. With its ability to maintain consistency during animation and preserve fine visual details, DLA outperforms state-of-the-art approaches in perceptual quality and photometric accuracy. It bridges the gap between unstructured 3D representations and high-fidelity animation-ready avatars, revolutionizing the way animation is created. In short, the DLA framework represents a milestone in animation production.</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DLAæ¡†æ¶èƒ½å¤Ÿä»å•ä¸€å›¾åƒé‡å»ºå‡ºå¯åŠ¨åŒ–çš„3Däººç±»è§’è‰²ã€‚</li>
<li>åˆ©ç”¨å¤šè§†è§’ç”ŸæˆæŠ€æœ¯æ•æ‰ä¸°å¯Œçš„å‡ ä½•å’Œå¤–è§‚ç»†èŠ‚ã€‚</li>
<li>é€šè¿‡ç»“åˆè§†é¢‘æ‰©æ•£æ¨¡å‹å’Œå§¿æ€æ„ŸçŸ¥UVç©ºé—´æ˜ å°„æŠ€æœ¯ï¼Œå®ç°äº†åŠ¨ç”»çš„ç”Ÿæˆå’Œæ¸²æŸ“ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨åŠ¨ç”»è¿‡ç¨‹ä¸­ä¿æŒä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†çš„è§†è§‰ç»†èŠ‚ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDLAåœ¨æ„ŸçŸ¥è´¨é‡å’Œå…‰åº¦å‡†ç¡®æ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>DLAæŠ€æœ¯å°†éç»“æ„åŒ–3Dè¡¨ç¤ºä¸é«˜è´¨é‡åŠ¨ç”»è§’è‰²ç›¸ç»“åˆï¼Œå¡«è¡¥äº†ä¸¤è€…ä¹‹é—´çš„é¸¿æ²Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-726482a654e1201edaef760743b1f52b" align="middle">
<img src="https://picx.zhimg.com/v2-519f5325ab868268153826982b44b13d" align="middle">
<img src="https://picx.zhimg.com/v2-6007b567d017fcd819b0297e629bb13f" align="middle">
<img src="https://picx.zhimg.com/v2-59efe94fa1f3462a86bee0c9545d103c" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Self-NPO-Data-Free-Diffusion-Model-Enhancement-via-Truncated-Diffusion-Fine-Tuning"><a href="#Self-NPO-Data-Free-Diffusion-Model-Enhancement-via-Truncated-Diffusion-Fine-Tuning" class="headerlink" title="Self-NPO: Data-Free Diffusion Model Enhancement via Truncated Diffusion Fine-Tuning"></a>Self-NPO: Data-Free Diffusion Model Enhancement via Truncated Diffusion Fine-Tuning</h2><p><strong>Authors:Fu-Yun Wang, Keqiang Sun, Yao Teng, Xihui Liu, Jiale Yuan, Jiaming Song, Hongsheng Li</strong></p>
<p>Diffusion models have demonstrated remarkable success in various visual generation tasks, including image, video, and 3D content generation. Preference optimization (PO) is a prominent and growing area of research that aims to align these models with human preferences. While existing PO methods primarily concentrate on producing favorable outputs, they often overlook the significance of classifier-free guidance (CFG) in mitigating undesirable results. Diffusion-NPO addresses this gap by introducing negative preference optimization (NPO), training models to generate outputs opposite to human preferences and thereby steering them away from unfavorable outcomes through CFG. However, prior NPO approaches rely on costly and fragile procedures for obtaining explicit preference annotations (e.g., manual pairwise labeling or reward model training), limiting their practicality in domains where such data are scarce or difficult to acquire. In this work, we propose Self-NPO, specifically truncated diffusion fine-tuning, a data-free approach of negative preference optimization by directly learning from the model itself, eliminating the need for manual data labeling or reward model training. This data-free approach is highly efficient (less than 1% training cost of Diffusion-NPO) and achieves comparable performance to Diffusion-NPO in a data-free manner. We demonstrate that Self-NPO integrates seamlessly into widely used diffusion models, including SD1.5, SDXL, and CogVideoX, as well as models already optimized for human preferences, consistently enhancing both their generation quality and alignment with human preferences. Code is available at <a target="_blank" rel="noopener" href="https://github.com/G-U-N/Diffusion-NPO">https://github.com/G-U-N/Diffusion-NPO</a>.</p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å„ç§è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œ3Då†…å®¹ç”Ÿæˆã€‚åå¥½ä¼˜åŒ–ï¼ˆPOï¼‰æ˜¯ä¸€ä¸ªçªå‡ºä¸”ä¸æ–­å‘å±•çš„ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨å°†è¿™äº›æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚è™½ç„¶ç°æœ‰çš„POæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨äº§ç”Ÿæœ‰åˆ©çš„è¾“å‡ºï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†æ— åˆ†ç±»å¼•å¯¼ï¼ˆCFGï¼‰åœ¨ç¼“è§£ä¸è‰¯ç»“æœä¸­çš„é‡è¦ä½œç”¨ã€‚Diffusion-NPOé€šè¿‡å¼•å…¥è´Ÿé¢åå¥½ä¼˜åŒ–ï¼ˆNPOï¼‰æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œè®­ç»ƒæ¨¡å‹ä»¥äº§ç”Ÿä¸äººç±»åå¥½ç›¸åçš„è¾“å‡ºæ¥æŠµåˆ¶ä¸è‰¯ç»“æœï¼Œå¹¶é€šè¿‡CFGå¼•å¯¼æ¨¡å‹é¿å…è¿™äº›ä¸åˆ©ç»“æœã€‚ç„¶è€Œï¼Œå…ˆå‰çš„NPOæ–¹æ³•ä¾èµ–äºè·å–æ˜¾æ€§åå¥½æ³¨é‡Šçš„æ˜‚è´µä¸”è„†å¼±çš„è¿‡ç¨‹ï¼ˆä¾‹å¦‚æ‰‹åŠ¨é…å¯¹æ ‡ç­¾æˆ–å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼‰ï¼Œè¿™åœ¨ç¼ºä¹æ­¤ç±»æ•°æ®æˆ–éš¾ä»¥è·å–æ•°æ®çš„é¢†åŸŸä¸­é™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Self-NPOï¼Œç‰¹åˆ«æ˜¯æˆªæ–­æ‰©æ•£å¾®è°ƒï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç›´æ¥ä»æ¨¡å‹æœ¬èº«å­¦ä¹ æ¥è¿›è¡Œè´Ÿé¢åå¥½ä¼˜åŒ–çš„æ— æ•°æ®æ–¹æ³•ï¼Œæ— éœ€æ‰‹åŠ¨æ•°æ®æ ‡æ³¨æˆ–å¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚è¿™ç§æ— æ•°æ®çš„æ–¹æ³•éå¸¸é«˜æ•ˆï¼ˆä»…ä¸ºDiffusion-NPOçš„ä¸åˆ°1%çš„è®­ç»ƒæˆæœ¬ï¼‰ï¼Œå¹¶ä¸”ä»¥æ— æ•°æ®çš„æ–¹å¼å®ç°äº†ä¸Diffusion-NPOç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¯æ˜äº†Self-NPOå¯ä»¥æ— ç¼é›†æˆåˆ°å¹¿æ³›ä½¿ç”¨çš„æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒåŒ…æ‹¬SD1.5ã€SDXLå’ŒCogVideoXï¼Œä»¥åŠå·²ç»ä¼˜åŒ–ä¸ºäººç±»åå¥½çš„æ¨¡å‹ï¼ŒæŒç»­æé«˜å®ƒä»¬çš„ç”Ÿæˆè´¨é‡å’Œä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/G-U-N/Diffusion-NPO%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/G-U-N/Diffusion-NPOæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11777v2">PDF</a> accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œä»¥åŠåå¥½ä¼˜åŒ–ï¼ˆPOï¼‰åœ¨è¿™ä¸€é¢†åŸŸçš„é‡è¦æ€§ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†Diffusion-NPOä¸­çš„è´Ÿåå¥½ä¼˜åŒ–ï¼ˆNPOï¼‰ï¼Œå®ƒé€šè¿‡è®­ç»ƒæ¨¡å‹ç”Ÿæˆä¸äººç±»åå¥½ç›¸åçš„è¾“å‡ºï¼Œé€šè¿‡åˆ†ç±»å™¨å…è´¹çš„æŒ‡å¯¼ï¼ˆCFGï¼‰æ¥é¿å…ä¸è‰¯ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰çš„NPOæ–¹æ³•ä¾èµ–äºè·å–æ˜ç¡®åå¥½æ³¨é‡Šçš„æ˜‚è´µå’Œè„†å¼±ç¨‹åºï¼Œé™åˆ¶äº†å…¶åœ¨æ•°æ®ç¨€ç¼ºæˆ–éš¾ä»¥è·å–çš„é¢†åŸŸä¸­çš„å®ç”¨æ€§ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Self-NPOï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç›´æ¥å­¦ä¹ æ¨¡å‹æœ¬èº«çš„è´Ÿåå¥½ä¼˜åŒ–çš„æ•°æ®å…è´¹æ–¹æ³•ï¼Œæ— éœ€æ‰‹åŠ¨æ•°æ®æ ‡è®°æˆ–å¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚è¯¥æ–¹æ³•é«˜æ•ˆä¸”æ€§èƒ½ä¸Diffusion-NPOç›¸å½“ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°å¹¿æ³›ä½¿ç”¨çš„æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒåŒ…æ‹¬SD1.5ã€SDXLå’ŒCogVideoXç­‰ï¼Œæé«˜ç”Ÿæˆè´¨é‡å’Œä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œ3Då†…å®¹ç”Ÿæˆã€‚</li>
<li>åå¥½ä¼˜åŒ–ï¼ˆPOï¼‰æ˜¯æ‰©æ•£æ¨¡å‹ç ”ç©¶çš„ä¸€ä¸ªé‡è¦æ–¹å‘ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>Diffusion-NPOå¼•å…¥è´Ÿåå¥½ä¼˜åŒ–ï¼ˆNPOï¼‰ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹ç”Ÿæˆä¸äººç±»åå¥½ç›¸åçš„è¾“å‡ºï¼Œä½¿ç”¨åˆ†ç±»å™¨å…è´¹çš„æŒ‡å¯¼ï¼ˆCFGï¼‰é¿å…ä¸è‰¯ç»“æœã€‚</li>
<li>ç°æœ‰NPOæ–¹æ³•ä¾èµ–è·å–æ˜ç¡®åå¥½æ³¨é‡Šçš„æ˜‚è´µå’Œè„†å¼±ç¨‹åºï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠã€‚</li>
<li>Self-NPOæ˜¯ä¸€ç§æ•°æ®å…è´¹çš„æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥å­¦ä¹ æ¨¡å‹æœ¬èº«è¿›è¡Œè´Ÿåå¥½ä¼˜åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨æ•°æ®æ ‡è®°æˆ–å¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚</li>
<li>Self-NPOæ–¹æ³•é«˜æ•ˆï¼Œæ€§èƒ½ä¸Diffusion-NPOç›¸å½“ï¼Œå¹¶èƒ½æé«˜ç”Ÿæˆè´¨é‡å’Œä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-edc021645e0b0dba8cc4af5391137325" align="middle">
<img src="https://picx.zhimg.com/v2-6a06300c72ea3601b4d4ce8ce963dbd1" align="middle">
<img src="https://picx.zhimg.com/v2-2a1999a5ae7b1b919be05f36628c5a2b" align="middle">
<img src="https://picx.zhimg.com/v2-3b77f0ece3df92a687f5dd12b4c18094" align="middle">
<img src="https://picx.zhimg.com/v2-1bad8b51dab23737d9823ddb507a370b" align="middle">
<img src="https://picx.zhimg.com/v2-b543f37492f410bb9fb862cd38f8a8ce" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d998ab0a97627d7ea7a195593cb6cf60" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Free-Form Scene Editor Enabling Multi-Round Object Manipulation like in a 3D Engine
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-18d7d291ab66d2129875e5d5e6df27a4" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  PFAvatar Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
