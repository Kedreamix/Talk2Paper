<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  CacheFlow Compressive Streaming Memory for Efficient Long-Form Video Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-676b1004d755069f28ec465bf0bbf6de')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="CacheFlow-Compressive-Streaming-Memory-for-Efficient-Long-Form-Video-Understanding"><a href="#CacheFlow-Compressive-Streaming-Memory-for-Efficient-Long-Form-Video-Understanding" class="headerlink" title="CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding"></a>CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding</h2><p><strong>Authors:Shrenik Patel, Daivik Patel</strong></p>
<p>Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each oneâ€™s keys are summarized by a tiny recurrent encoder to form a retrieval index, while the blockâ€™s full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.</p>
<blockquote>
<p>é•¿è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰ç»™å½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå› ä¸ºéšç€è¿è¡Œæ—¶çš„å¢é•¿ï¼Œæ³¨æ„åŠ›æœºåˆ¶å’Œé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ä¹Ÿéšä¹‹å¢é•¿ï¼Œè¿™è¿«ä½¿æ¨¡å‹è¿›è¡Œæ˜‚è´µçš„æ¨ç†æˆ–ä½¿ç”¨ç›®å…‰çŸ­æµ…çš„æ»‘åŠ¨çª—å£ã€‚æˆ‘ä»¬å¼•å…¥äº†CacheFlowï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ç®¡é“ï¼Œå®ƒç»“åˆäº†åŠ¨æ€ä»¤ç‰Œä¸¢å¼ƒï¼ˆDTDï¼‰å’Œå‹ç¼©é•¿æœŸå†…å­˜ã€‚DTDé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦åœ¨çº¿å‰”é™¤æ¯å—ä»¤ç‰Œçš„å‰ä¸€å¸§æ•°æ®ï¼Œå°†å‰©ä½™ä»¤ç‰Œæ‰“åŒ…æˆå›ºå®šå¤§å°çš„å—ã€‚è¿™ç§åœ¨çº¿ã€é€å¸§å¤„ç†æ–¹å¼ä½¿æˆ‘ä»¬çš„æ–¹æ³•ä»æ ¹æœ¬ä¸Šé€‚ç”¨äºæµåª’ä½“è§†é¢‘é—®ç­”ã€‚åœ¨å¤„ç†å—çš„è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªå—çš„é”®éƒ½è¢«å¾®å°çš„å¾ªç¯ç¼–ç å™¨æ€»ç»“æˆæ£€ç´¢ç´¢å¼•ï¼Œè€Œå—çš„å®Œæ•´é”®å€¼å¯¹è¢«å¸è½½å¹¶åœ¨åç»­é‡æ–°åŠ è½½ä»¥ç”Ÿæˆç­”æ¡ˆï¼Œç¡®ä¿ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒåŸºäºå…±è¯†çš„æ£€ç´¢æœºåˆ¶åªæ£€ç´¢å‡ºæœ€ç›¸å…³çš„å‰Kä¸ªå—ï¼Œå¹¶åœ¨æ£€ç´¢ç»“æœå’Œæœ¬åœ°ä¸Šä¸‹æ–‡ä¸Šè¿›è¡Œç²¾å‡†çš„é•¿ç¨‹æ¨ç†ã€‚CacheFlowæ˜¯éšæ’éšç”¨çš„ï¼Œä¸æ¶æ„æ— å…³ï¼Œä¸éœ€è¦å¾®è°ƒã€‚åœ¨ç¦»çº¿ä»¥åŠæµåª’ä½“è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCacheFlowè¶…è¶Šäº†å½“å‰çš„ä¼˜ç§€åŸºå‡†æµ‹è¯•æ°´å¹³ï¼ŒåŒæ—¶å¤„ç†çš„ä»¤ç‰Œå‡å°‘äº†é«˜è¾¾87%ã€‚æˆ‘ä»¬çš„åŒé‡æ–¹æ³•ä½¿VLMæ—¢é«˜æ•ˆåˆå…·å¤‡ä¸Šä¸‹æ–‡æ„è¯†ï¼Œä¸ºå®é™…çš„é•¿è§†é¢‘ç†è§£é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13644v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é•¿è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰çš„CacheFlowæ–¹æ³•ï¼Œè§£å†³äº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†é•¿è§†é¢‘æ—¶çš„å›°å¢ƒã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŠ¨æ€ä»¤ç‰Œä¸¢å¼ƒï¼ˆDTDï¼‰å’Œå‹ç¼©é•¿æœŸå†…å­˜æŠ€æœ¯ï¼Œå®ç°åœ¨çº¿ã€æŒ‰å¸§å¤„ç†ï¼Œé€‚ç”¨äºå®æ—¶æµåª’ä½“VQAã€‚é€šè¿‡å‹ç¼©ç¼–ç å—å¹¶ä¿ç•™å…³é”®ä¿¡æ¯ï¼ŒCacheFlowåœ¨æ¨ç†é˜¶æ®µèƒ½å¤Ÿé«˜æ•ˆæ£€ç´¢ç›¸å…³å—ï¼ŒåŒæ—¶å…³æ³¨æœ¬åœ°å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œå®ç°ç²¾ç¡®çš„é•¿ç¨‹æ¨ç†ã€‚CacheFlowå…·æœ‰é€šç”¨æ€§ï¼Œæ— éœ€å¾®è°ƒå³å¯åº”ç”¨äºå„ç§æ¶æ„ï¼Œå¹¶åœ¨ç¦»çº¿åŠæµåª’ä½“VQAæµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶Šç°æœ‰åŸºå‡†çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CacheFlowè§£å†³äº†é•¿è§†é¢‘é—®ç­”ä¸­è§†è§‰è¯­è¨€æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ç»“åˆåŠ¨æ€ä»¤ç‰Œä¸¢å¼ƒï¼ˆDTDï¼‰å’Œå‹ç¼©é•¿æœŸå†…å­˜æŠ€æœ¯ï¼ŒCacheFlowå®ç°äº†é«˜æ•ˆçš„è§†é¢‘å¤„ç†ã€‚</li>
<li>CacheFlowé‡‡ç”¨åœ¨çº¿ã€æŒ‰å¸§å¤„ç†æ–¹å¼ï¼Œé€‚ç”¨äºå®æ—¶æµåª’ä½“VQAã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å‹ç¼©ç¼–ç å—å¹¶ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œæé«˜äº†æ£€ç´¢æ•ˆç‡å’Œç­”æ¡ˆç²¾åº¦ã€‚</li>
<li>CacheFlowé‡‡ç”¨å…±è¯†æ£€ç´¢æœºåˆ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ£€ç´¢æœ€ç›¸å…³çš„å—ï¼Œå¹¶å…³æ³¨æœ¬åœ°å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œå®ç°ç²¾ç¡®çš„é•¿ç¨‹æ¨ç†ã€‚</li>
<li>CacheFlowå…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºå„ç§æ¶æ„ï¼Œæ— éœ€å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5572afcb0a61d906445a075cfea77add" align="middle">
<img src="https://picx.zhimg.com/v2-b419b39489c9d2b055bcbf98693a43d7" align="middle">
<img src="https://picx.zhimg.com/v2-d44affe2f4d7912619e1d94afd005f01" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ReaSon-Reinforced-Causal-Search-with-Information-Bottleneck-for-Video-Understanding"><a href="#ReaSon-Reinforced-Causal-Search-with-Information-Bottleneck-for-Video-Understanding" class="headerlink" title="ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding"></a>ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding</h2><p><strong>Authors:Yuan Zhou, Litao Hua, Shilong Jin, Wentao Huang, Haoran Duan</strong></p>
<p>Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.</p>
<blockquote>
<p>åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è§†é¢‘ç†è§£ä¸­ï¼Œå…³é”®å¸§é€‰æ‹©å˜å¾—è‡³å…³é‡è¦ï¼Œå› ä¸ºå­˜åœ¨è¾“å…¥æ ‡è®°æœ‰é™å’Œè§†é¢‘å¸§ä¹‹é—´ç›¸å…³ä¿¡æ¯çš„æ—¶åºç¨€ç–æ€§ã€‚è§†é¢‘ç†è§£é€šå¸¸ä¾èµ–äºæ—¢å…·æœ‰ä¿¡æ¯æ€§åˆå…·æœ‰å› æœå†³å®šæ€§çš„æœ‰æ•ˆå…³é”®å¸§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¿¡æ¯ç“¶é¢ˆçš„å¼ºåŒ–å› æœæœç´¢ï¼ˆReaSonï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å…³é”®å¸§é€‰æ‹©å…¬å¼åŒ–ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œå€ŸåŠ©æ–°é¢–çš„å› æœä¿¡æ¯ç“¶é¢ˆï¼ˆCIBï¼‰ï¼Œæ˜ç¡®åœ°å°†å…³é”®å¸§å®šä¹‰ä¸ºæ»¡è¶³é¢„æµ‹å……åˆ†æ€§å’Œå› æœå¿…è¦æ€§çš„å¸§ã€‚å…·ä½“æ¥è¯´ï¼ŒReaSonä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„ç­–ç•¥ç½‘ç»œä»è§†è§‰ç›¸å…³çš„å€™é€‰å¸§æ± ä¸­é€‰å–å…³é”®å¸§ï¼Œä»¥æ•æ‰é¢„æµ‹å……åˆ†æ€§ï¼Œç„¶åé€šè¿‡åäº‹å®å¹²é¢„æ¥è¯„ä¼°å› æœå¿…è¦æ€§ã€‚æœ€åï¼Œè®¾è®¡äº†ä¸€ç§ä¸CIBåŸåˆ™ä¸€è‡´çš„å¤åˆå¥–åŠ±ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¼•å¯¼é€‰æ‹©ç­–ç•¥ã€‚åœ¨NExT-QAã€EgoSchemaå’Œè§†é¢‘MMEä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨æœ‰é™å¸§è®¾ç½®ä¸‹ï¼ŒReaSonå§‹ç»ˆä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12530v1">PDF</a> Accepted to AAAI 2026. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/robin-hlt/AAAI26-ReaSon">https://github.com/robin-hlt/AAAI26-ReaSon</a></p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘ç†è§£ä¸­ï¼Œå…³é”®å¸§çš„é€‰æ‹©å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è‡³å…³é‡è¦ã€‚é¢å¯¹æœ‰é™çš„è¾“å…¥æ ‡è®°å’Œè·¨è§†é¢‘å¸§çš„æ—¶ç©ºä¿¡æ¯ç¨€ç–æ€§ï¼Œæœ‰æ•ˆçš„å…³é”®å¸§ä¸ä»…éœ€è¦åŒ…å«ä¿¡æ¯ï¼Œè¿˜éœ€è¦å…·æœ‰å†³å®šæ€§çš„å› æœä½œç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¿¡æ¯ç“¶é¢ˆçš„å¼ºåŒ–å› æœæœç´¢ï¼ˆReaSonï¼‰æ¡†æ¶ï¼Œå®ƒå°†å…³é”®å¸§é€‰æ‹©å…¬å¼åŒ–ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å€ŸåŠ©æ–°é¢–çš„å› æœä¿¡æ¯ç“¶é¢ˆï¼ˆCIBï¼‰æ˜ç¡®åœ°å°†å…³é”®å¸§å®šä¹‰ä¸ºåŒæ—¶æ»¡è¶³é¢„æµ‹å……åˆ†æ€§å’Œå› æœå¿…è¦æ€§çš„å¸§ã€‚å…·ä½“æ¥è¯´ï¼ŒReaSonä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„ç­–ç•¥ç½‘ç»œä»è§†è§‰ç›¸å…³çš„å€™é€‰å¸§æ± ä¸­é€‰å–å…³é”®å¸§ï¼Œä»¥æ•æ‰é¢„æµ‹å……åˆ†æ€§ï¼Œç„¶åé€šè¿‡åäº‹å®å¹²é¢„è¯„ä¼°å› æœå¿…è¦æ€§ã€‚æœ€åï¼Œè®¾è®¡äº†ä¸€ä¸ªä¸CIBåŸåˆ™ä¸€è‡´çš„å¤åˆå¥–åŠ±æ¥é€šè¿‡å¼ºåŒ–å­¦ä¹ å¼•å¯¼é€‰æ‹©ç­–ç•¥ã€‚åœ¨NExT-QAã€EgoSchemaå’ŒVideo-MMEä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨æœ‰é™å¸§è®¾ç½®ä¸‹ï¼ŒReaSonå§‹ç»ˆä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…³é”®å¸§é€‰æ‹©åœ¨è§†é¢‘ç†è§£ä¸­å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>æœ‰é™çš„è¾“å…¥æ ‡è®°å’Œè·¨è§†é¢‘å¸§çš„æ—¶ç©ºä¿¡æ¯ç¨€ç–æ€§æ˜¯æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>ReaSonæ¡†æ¶é€šè¿‡ä¼˜åŒ–é—®é¢˜å¤„ç†å…³é”®å¸§é€‰æ‹©ã€‚</li>
<li>é¢„æµ‹å……åˆ†æ€§å’Œå› æœå¿…è¦æ€§æ˜¯å®šä¹‰å…³é”®å¸§çš„ä¸¤ä¸ªæ ¸å¿ƒè¦ç´ ã€‚</li>
<li>ç­–ç•¥ç½‘ç»œè´Ÿè´£ä»è§†è§‰ç›¸å…³å€™é€‰å¸§ä¸­é€‰æ‹©å…³é”®å¸§ã€‚</li>
<li>åäº‹å®å¹²é¢„ç”¨äºè¯„ä¼°å› æœå¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d966fce05fefdb1469f66b6a158a1531" align="middle">
<img src="https://picx.zhimg.com/v2-77410bd5c54d8b8ce59584a5887b360c" align="middle">
<img src="https://picx.zhimg.com/v2-af56e5ea671a427ddd6e7fdd5aa11c07" align="middle">
<img src="https://picx.zhimg.com/v2-01439857f5a059370b61da68ec049cf6" align="middle">
<img src="https://picx.zhimg.com/v2-5e89409549fa04a8570483af50b1ac85" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GCAgent-Long-Video-Understanding-via-Schematic-and-Narrative-Episodic-Memory"><a href="#GCAgent-Long-Video-Understanding-via-Schematic-and-Narrative-Episodic-Memory" class="headerlink" title="GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory"></a>GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory</h2><p><strong>Authors:Jeong Hun Yeo, Sangyun Chung, Sungjune Park, Dae Hoe Kim, Jinyoung Moon, Yong Man Ro</strong></p>
<p>Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4% accuracy on the Long split and the highest overall average (71.9%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.</p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå›ºæœ‰çš„æ ‡è®°é™åˆ¶ä»¥åŠæ•æ‰é•¿æœŸæ—¶é—´ä¾èµ–å…³ç³»çš„å¤æ‚æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰åˆ°å…¨å±€ä¸Šä¸‹æ–‡å’Œå¤æ‚çš„äº‹ä»¶å…³ç³»ï¼Œè¿™å¯¹äºæ·±åº¦è§†é¢‘æ¨ç†æ˜¯å¿…è¦çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GCAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä»£ç†æ¡†æ¶ï¼Œå®ç°äº†å…¨é¢çš„é•¿è§†é¢‘ç†è§£ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°æ˜¯åœºæ™¯å’Œå™äº‹æ€§æƒ…èŠ‚è®°å¿†ã€‚è¿™ç§è®°å¿†ä»¥ç»“æ„åŒ–çš„æ–¹å¼å»ºæ¨¡äº‹ä»¶åŠå…¶å› æœå’Œæ—¶é—´å…³ç³»ï¼Œå½¢æˆä¸€ä¸ªç®€æ´ã€æœ‰ç»„ç»‡åŒ–çš„ä¸Šä¸‹æ–‡ï¼Œä»æ ¹æœ¬ä¸Šè§£å†³äº†é•¿æœŸä¾èµ–é—®é¢˜ã€‚æˆ‘ä»¬çš„GCAgentåœ¨ä¸€ä¸ªå¤šé˜¶æ®µçš„æ„ŸçŸ¥-è¡ŒåŠ¨-åæ€å¾ªç¯ä¸­è¿è¡Œï¼Œåˆ©ç”¨å†…å­˜ç®¡ç†å™¨æ£€ç´¢ç›¸å…³çš„æƒ…èŠ‚ä¸Šä¸‹æ–‡ï¼Œè¿›è¡Œç¨³å¥çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ã€‚å¤§é‡å®éªŒè¯å®ï¼ŒGCAgentæ˜¾è‘—å¢å¼ºäº†é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œåœ¨Video-MME Long splitä¸Šç›¸è¾ƒäºå…ˆè¿›çš„MLLMåŸºå‡†æµ‹è¯•æé«˜äº†é«˜è¾¾23.5%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¯æ¯”çš„7Bè§„æ¨¡MLLMä¸­å»ºç«‹äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨Long splitä¸Šè¾¾åˆ°äº†73.4%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨Video-MMEåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†æœ€é«˜çš„æ€»ä½“å¹³å‡å‡†ç¡®ç‡ï¼ˆ71.9%ï¼‰ï¼Œè¿™éªŒè¯äº†æˆ‘ä»¬çš„åŸºäºä»£ç†çš„æ¨ç†èŒƒå¼å’Œç»“æ„åŒ–è®°å¿†å¯¹è®¤çŸ¥å¯å‘é•¿è§†é¢‘ç†è§£çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12027v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†é•¿è§†é¢‘ç†è§£æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§å…¨æ–°çš„å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä»£ç†æ¡†æ¶GCAgentï¼Œå®ç°äº†å…¨é¢çš„é•¿è§†é¢‘ç†è§£ã€‚å…¶æ ¸å¿ƒåˆ›æ–°æ˜¯â€œå›¾å¼å™äº‹æ€§æƒ…æ™¯è®°å¿†â€ï¼Œè¯¥è®°å¿†ç»“æ„æ€§åœ°æ¨¡æ‹Ÿäº‹ä»¶åŠå…¶å› æœå’Œæ—¶é—´å…³ç³»ï¼Œä»¥è§£å†³é•¿æœŸä¾èµ–é—®é¢˜ã€‚GCAgentåœ¨å¤šé˜¶æ®µæ„ŸçŸ¥-è¡ŒåŠ¨-åæ€å¾ªç¯ä¸­è¿è¡Œï¼Œé€šè¿‡å†…å­˜ç®¡ç†å™¨æ£€ç´¢ç›¸å…³æƒ…æ™¯ä¸Šä¸‹æ–‡è¿›è¡Œç¨³å¥çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ã€‚å®éªŒè¯æ˜ï¼ŒGCAgentåœ¨Video-MMEé•¿åˆ†å‰²ä¸Šæé«˜äº†é«˜è¾¾23.5%çš„å‡†ç¡®ç‡ï¼Œåœ¨å¯æ¯”çš„7Bè§„æ¨¡MLLMsä¸Šå»ºç«‹äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†åŸºäºä»£ç†çš„æ¨ç†èŒƒå¼å’Œç»“æ„è®°å¿†å¯¹è®¤çŸ¥å¯å‘é•¿è§†é¢‘ç†è§£çš„æ•ˆç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿è§†é¢‘ç†è§£å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³å†…åœ¨æ ‡è®°é™åˆ¶å’Œé•¿æœŸæ—¶é—´ä¾èµ–æ€§é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡å’Œå¤æ‚äº‹ä»¶å…³ç³»ï¼Œå¯¹äºæ·±åº¦è§†é¢‘æ¨ç†è‡³å…³é‡è¦ã€‚</li>
<li>GCAgentæ˜¯ä¸€ä¸ªå…¨æ–°çš„å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä»£ç†æ¡†æ¶ï¼Œå®ç°äº†å…¨é¢çš„é•¿è§†é¢‘ç†è§£ã€‚</li>
<li>GCAgentçš„æ ¸å¿ƒåˆ›æ–°æ˜¯â€œå›¾å¼å™äº‹æ€§æƒ…æ™¯è®°å¿†â€ï¼Œç”¨äºè§£å†³é•¿æœŸä¾èµ–é—®é¢˜å¹¶ç»“æ„æ€§åœ°æ¨¡æ‹Ÿäº‹ä»¶åŠå…¶å…³ç³»ã€‚</li>
<li>GCAgentè¿è¡Œåœ¨å¤šé˜¶æ®µæ„ŸçŸ¥-è¡ŒåŠ¨-åæ€å¾ªç¯ä¸­ï¼Œå¹¶é€šè¿‡å†…å­˜ç®¡ç†å™¨æ£€ç´¢ç›¸å…³æƒ…æ™¯ä¸Šä¸‹æ–‡è¿›è¡Œç¨³å¥çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGCAgentåœ¨Video-MMEé•¿åˆ†å‰²ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾23.5%ï¼Œå¹¶åœ¨ç±»ä¼¼è§„æ¨¡çš„MLLMsä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a72e8f30dafb0730ace5311f47d2147a" align="middle">
<img src="https://picx.zhimg.com/v2-2722ff68ac65c5e747b41d2ab42b98e9" align="middle">
<img src="https://picx.zhimg.com/v2-d7af8b055fbb94ee122b25f06b6363f7" align="middle">
<img src="https://picx.zhimg.com/v2-f9df47b269c48e1312dedea5bc6e33fd" align="middle">
<img src="https://picx.zhimg.com/v2-ce223576c220b81cfd03e7179f7ad6f5" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction"><a href="#VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction" class="headerlink" title="VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction"></a>VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</h2><p><strong>Authors:Hao Wang, Eiki Murata, Lingfang Zhang, Ayako Sato, So Fukuda, Ziqi Yin, Wentao Hu, Keisuke Nakao, Yusuke Nakamura, Sebastian Zwirner, Yi-Chia Chen, Hiroyuki Otomo, Hiroki Ouchi, Daisuke Kawahara</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMsâ€™ geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agentâ€™s markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.</p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥æå¤§åœ°å¢å¼ºäº†è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œä¸ºå®é™…åº”ç”¨å¼€è¾Ÿäº†æ–°å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºå®¤å†…åœºæ™¯æˆ–è¿‘è·ç¦»æˆ·å¤–æ´»åŠ¨ï¼Œä¸é•¿é€”æ—…è¡Œç›¸å…³çš„æŒ‘æˆ˜åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚æŒæ¡æ‰©å±•çš„åœ°ç†æ—¶ç©ºè½¨è¿¹å¯¹äºä¸‹ä¸€ä»£MLLMsè‡³å…³é‡è¦ï¼Œå®ƒä¸ºå®ä½“AIè§„åˆ’å’Œå¯¼èˆªç­‰ç°å®ä»»åŠ¡æä¾›äº†æ”¯æŒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VIR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”±200ä¸ªæ—…è¡Œè§†é¢‘ç»„æˆçš„æ–°åŸºå‡†æµ‹è¯•ï¼Œå®ƒå°†è¡Œç¨‹é‡å»ºè®¾å®šä¸ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ¨åŠ¨MLLMsçš„åœ°ç†æ—¶ç©ºæ™ºèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒ…æ‹¬ä¸“æœ‰æ¨¡å‹åœ¨å†…çš„æœ€å…ˆè¿›MLLMsåœ¨å–å¾—é«˜åˆ†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™è¡¨æ˜å¤„ç†è·¨è¶Šå¹¿é˜”ç©ºé—´å’Œæ—¶é—´çš„è§†é¢‘éå¸¸å›°éš¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ·±å…¥çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå¼€å‘äº†ä¸€ä¸ªåˆ©ç”¨VIR-Benchæ‰€è·å¾—è§è§£çš„åŸå‹æ—…è¡Œè§„åˆ’ä»£ç†ã€‚è¯¥ä»£ç†çš„è¡Œç¨‹æ¨èæ˜¾è‘—æ”¹å–„ï¼Œè¯æ˜æˆ‘ä»¬çš„è¯„ä¼°åè®®ä¸ä»…æœ‰æ•ˆåœ°è¯„ä¼°äº†æ¨¡å‹ï¼Œè€Œä¸”è¿˜è½¬åŒ–ä¸ºé¢å‘ç”¨æˆ·çš„åº”ç”¨ä¸­çš„å…·ä½“æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19002v2">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥æå¤§åœ°æå‡äº†è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œä¸ºå®é™…åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œå½“å‰è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨å®¤å†…åœºæ™¯æˆ–è¿‘è·ç¦»æˆ·å¤–æ´»åŠ¨ï¼Œå¯¹äºè¿œè·ç¦»æ—…è¡Œç›¸å…³çš„æŒ‘æˆ˜ä»å¾…æ¢ç´¢ã€‚æŒæ¡æ‰©å±•çš„åœ°ç†æ—¶ç©ºè½¨è¿¹å¯¹äºä¸‹ä¸€ä»£MLLMsè‡³å…³é‡è¦ï¼Œè¿™æ˜¯å®ç°å®ä½“AIè§„åˆ’å’Œå¯¼èˆªç­‰ç°å®ä»»åŠ¡çš„åŸºç¡€ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VIR-BenchåŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬200ä¸ªæ—…è¡Œè§†é¢‘ï¼Œå°†è¡Œç¨‹é‡å»ºè®¾å®šä¸ºä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ¨åŠ¨MLLMsçš„åœ°ç†æ—¶ç©ºæ™ºèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŒ…æ‹¬ä¸“æœ‰æ¨¡å‹åœ¨å†…çš„æœ€æ–°MLLMså¾—åˆ†ä¸é«˜ï¼Œè¯´æ˜å¤„ç†å¤§èŒƒå›´ç©ºé—´å’Œæ—¶é—´å°ºåº¦çš„è§†é¢‘éå¸¸å›°éš¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ·±å…¥çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå¼€å‘äº†ä¸€ä¸ªåˆ©ç”¨VIR-Benchè§è§£çš„æ—…è¡Œè§„åˆ’ä»£ç†åŸå‹ï¼Œå…¶è¡Œç¨‹æ¨èæ˜¾è‘—æ”¹å–„ï¼Œè¯æ˜æˆ‘ä»¬çš„è¯„ä¼°åè®®ä¸ä»…æœ‰æ•ˆåœ°è¯„ä¼°äº†æ¨¡å‹ï¼Œè€Œä¸”è½¬åŒ–ä¸ºé¢å‘ç”¨æˆ·çš„åº”ç”¨ä¸­çš„å®é™…æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨å®¤å†…å’Œè¿‘è·ç¦»æ´»åŠ¨ï¼Œç¼ºä¹é’ˆå¯¹è¿œè·ç¦»æ—…è¡Œçš„ç ”ç©¶ã€‚</li>
<li>æŒæ¡æ‰©å±•çš„åœ°ç†æ—¶ç©ºè½¨è¿¹å¯¹ä¸‹ä¸€ä»£MLLMsè‡³å…³é‡è¦ï¼Œè¿™æ˜¯å®ç°å®ä½“AIè§„åˆ’å’Œå¯¼èˆªç­‰ä»»åŠ¡çš„åŸºç¡€ã€‚</li>
<li>VIR-BenchåŸºå‡†æµ‹è¯•åŒ…å«200ä¸ªæ—…è¡Œè§†é¢‘ï¼Œæ—¨åœ¨è¯„ä¼°MLLMså¤„ç†åœ°ç†æ—¶ç©ºæ™ºèƒ½çš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰MLLMsåœ¨å¤„ç†å¤§èŒƒå›´ç©ºé—´å’Œæ—¶é—´å°ºåº¦çš„è§†é¢‘æ—¶è¡¨ç°å›°éš¾ã€‚</li>
<li>æ·±å…¥æ¡ˆä¾‹ç ”ç©¶æ˜¾ç¤ºï¼Œåˆ©ç”¨VIR-Benchè§è§£å¼€å‘çš„æ—…è¡Œè§„åˆ’ä»£ç†èƒ½æ˜¾è‘—æ”¹å–„è¡Œç¨‹æ¨èã€‚</li>
<li>VIR-BenchåŸºå‡†æµ‹è¯•ä¸ä»…æœ‰æ•ˆè¯„ä¼°äº†æ¨¡å‹æ€§èƒ½ï¼Œè€Œä¸”æé«˜äº†é¢å‘ç”¨æˆ·çš„åº”ç”¨çš„å®é™…æ€§èƒ½ã€‚</li>
<li>å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¤„ç†å¤æ‚åœ°ç†æ—¶ç©ºä¿¡æ¯çš„é•¿æœŸæ—…è¡Œè§†é¢‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2d50a16a2dc7a7726fe401558e67d8a" align="middle">
<img src="https://picx.zhimg.com/v2-7f91c56d602c419f2d90546cd012e73b" align="middle">
<img src="https://picx.zhimg.com/v2-601078ad9d95b6f2afedf77c5d333e99" align="middle">
<img src="https://picx.zhimg.com/v2-03aa3d578823faa2b06bb6ff25df97ad" align="middle">
<img src="https://picx.zhimg.com/v2-52ad7639e5e598555898bc770ce9a1dd" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ViMoNet-A-Multimodal-Vision-Language-Framework-for-Human-Behavior-Understanding-from-Motion-and-Video"><a href="#ViMoNet-A-Multimodal-Vision-Language-Framework-for-Human-Behavior-Understanding-from-Motion-and-Video" class="headerlink" title="ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video"></a>ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video</h2><p><strong>Authors:Rajan Das Gupta, Md Yeasin Rahat, Nafiz Fahad, Abir Ahmed, Liew Tze Hui</strong></p>
<p>This study investigates how large language models (LLMs) can be used to understand human behavior using motion and video data. We think that mixing both types is essential to completely capture the nuanced movements and meanings of human actions, in contrast to recent models that simply concentrate on motion data or films. To address this, we provide ViMoNet, a straightforward yet effective framework for comprehending, characterizing, and deducing human action. ViMoNet employs a joint training strategy that leverages the advantages of two data types: detailed motion-text data, which is more exact, and generic video-text data, which is more comprehensive but less detailed. This aids in the modelâ€™s acquisition of rich data regarding time and space in human behavior. Additionally, we provide a brand new dataset named VIMOS that contains a variety of films, motion sequences, instructions, and subtitles. We developed ViMoNet-Bench, a standardized benchmark with carefully labeled samples, to evaluate how well models understand human behavior. Our tests show that ViMoNet outperforms existing methods in caption generation, motion understanding, and behavior interpretation.</p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åŠ¨ä½œå’Œè§†é¢‘æ•°æ®æ¥ç†è§£äººç±»è¡Œä¸ºã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæ··åˆä¸¤ç§ç±»å‹çš„æ•°æ®å¯¹äºå®Œå…¨æ•æ‰äººç±»è¡Œä¸ºçš„ç»†å¾®åŠ¨ä½œå’Œå«ä¹‰è‡³å…³é‡è¦ï¼Œä¸æœ€è¿‘ä»…ä¸“æ³¨äºåŠ¨ä½œæ•°æ®æˆ–ç”µå½±çš„æ¨¡å‹å½¢æˆå¯¹æ¯”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æä¾›äº†ViMoNetï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºç†è§£ã€è¡¨å¾å’Œæ¨æ–­äººç±»è¡Œä¸ºã€‚ViMoNeté‡‡ç”¨è”åˆè®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨ä¸¤ç§æ•°æ®ç±»å‹çš„ä¼˜ç‚¹ï¼šè¯¦ç»†çš„åŠ¨ä½œæ–‡æœ¬æ•°æ®æ›´åŠ ç²¾ç¡®ï¼Œè€Œé€šç”¨çš„è§†é¢‘æ–‡æœ¬æ•°æ®æ›´åŠ å…¨é¢ä½†ä¸å¤Ÿè¯¦ç»†ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹è·å–æœ‰å…³äººç±»è¡Œä¸ºçš„æ—¶é—´å’Œç©ºé—´çš„ä¸°å¯Œæ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…¨æ–°çš„æ•°æ®é›†VIMOSï¼Œå…¶ä¸­åŒ…å«å„ç§ç”µå½±ã€è¿åŠ¨åºåˆ—ã€æŒ‡ä»¤å’Œå­—å¹•ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹å¯¹äººç±»è¡Œä¸ºçš„ç†è§£ç¨‹åº¦ï¼Œæˆ‘ä»¬å¼€å‘äº†ViMoNet-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰ç²¾å¿ƒæ ‡æ³¨æ ·æœ¬çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„æµ‹è¯•è¡¨æ˜ï¼Œåœ¨å­—å¹•ç”Ÿæˆã€åŠ¨ä½œç†è§£å’Œè¡Œä¸ºè§£é‡Šæ–¹é¢ï¼ŒViMoNetçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09818v2">PDF</a> This is the preprint version of the manuscript. It is currently being prepared for submission to an academic conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åŠ¨ä½œå’Œè§†é¢‘æ•°æ®ç†è§£äººç±»è¡Œä¸ºã€‚ç ”ç©¶å‘ç°ï¼Œæ··åˆä¸¤ç§ç±»å‹çš„æ•°æ®å¯¹äºå…¨é¢æ•æ‰äººç±»åŠ¨ä½œçš„ç»†å¾®åŠ¨ä½œå’Œå«ä¹‰è‡³å…³é‡è¦ï¼Œä¸æœ€è¿‘ä»…ä¸“æ³¨äºè¿åŠ¨æ•°æ®æˆ–ç”µå½±çš„æ¨¡å‹å½¢æˆå¯¹æ¯”ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ViMoNetï¼Œä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºç†è§£ã€è¡¨å¾å’Œæ¨æ–­äººç±»è¡Œä¸ºã€‚ViMoNeté‡‡ç”¨è”åˆè®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨ä¸¤ç§æ•°æ®ç±»å‹çš„ä¼˜åŠ¿ï¼šè¯¦ç»†çš„è¿åŠ¨æ–‡æœ¬æ•°æ®æ›´ç²¾ç¡®ï¼Œé€šç”¨çš„è§†é¢‘æ–‡æœ¬æ•°æ®æ›´å…¨é¢ä½†ä¸å¤ªè¯¦ç»†ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹è·å¾—æœ‰å…³äººç±»è¡Œä¸ºæ—¶é—´å’Œç©ºé—´çš„ä¸°å¯Œæ•°æ®ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¨å‡ºäº†å…¨æ–°çš„VIMOSæ•°æ®é›†ï¼ŒåŒ…å«å„ç§ç”µå½±ã€è¿åŠ¨åºåˆ—ã€æŒ‡ä»¤å’Œå­—å¹•ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹å¯¹äººç±»è¡Œä¸ºçš„ç†è§£ç¨‹åº¦ï¼Œç ”ç©¶è¿˜å¼€å‘äº†æ ‡å‡†åŒ–çš„ViMoNet-BenchåŸºå‡†æµ‹è¯•ï¼Œå¸¦æœ‰ç²¾å¿ƒæ ‡è®°çš„æ ·æœ¬ã€‚æµ‹è¯•è¡¨æ˜ï¼ŒViMoNetåœ¨ç”Ÿæˆå­—å¹•ã€ç†è§£è¿åŠ¨å’Œè§£é‡Šè¡Œä¸ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºé€šè¿‡åŠ¨ä½œå’Œè§†é¢‘æ•°æ®ç†è§£äººç±»è¡Œä¸ºã€‚</li>
<li>æ··åˆè¿åŠ¨æ–‡æœ¬æ•°æ®å’Œè§†é¢‘æ–‡æœ¬æ•°æ®å¯¹äºå…¨é¢ç†è§£äººç±»è¡Œä¸ºè‡³å…³é‡è¦ã€‚</li>
<li>ViMoNetæ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºç†è§£ã€è¡¨å¾å’Œæ¨æ–­äººç±»è¡Œä¸ºï¼Œé‡‡ç”¨è”åˆè®­ç»ƒç­–ç•¥ã€‚</li>
<li>ViMoNetåœ¨ç”Ÿæˆå­—å¹•ã€ç†è§£è¿åŠ¨å’Œè§£é‡Šè¡Œä¸ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ¨å‡ºäº†å…¨æ–°çš„VIMOSæ•°æ®é›†ï¼ŒåŒ…å«å„ç§ç”µå½±ã€è¿åŠ¨åºåˆ—ã€æŒ‡ä»¤å’Œå­—å¹•ï¼Œä»¥æ”¯æŒäººç±»è¡Œä¸ºç ”ç©¶ã€‚</li>
<li>ç ”ç©¶è¿˜å¼€å‘äº†æ ‡å‡†åŒ–çš„ViMoNet-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹å¯¹äººç±»è¡Œä¸ºçš„ç†è§£ç¨‹åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef2031bb315f2ff13227251ffcbd2394" align="middle">
<img src="https://picx.zhimg.com/v2-d7112567789877d5c7aaf095c2e652ea" align="middle">
<img src="https://picx.zhimg.com/v2-c18b5a70f67b355199f206bb4ea7b0da" align="middle">
<img src="https://picx.zhimg.com/v2-bf57b29e3940243e80ed61984af856e9" align="middle">
<img src="https://picx.zhimg.com/v2-e0c3202912cdb0ea90a93eb5cfa976ea" align="middle">
<img src="https://picx.zhimg.com/v2-a0d12fb07b9d76801a41f615f60bdc8d" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="APVR-Hour-Level-Long-Video-Understanding-with-Adaptive-Pivot-Visual-Information-Retrieval"><a href="#APVR-Hour-Level-Long-Video-Understanding-with-Adaptive-Pivot-Visual-Information-Retrieval" class="headerlink" title="APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval"></a>APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval</h2><p><strong>Authors:Hong Gao, Yiming Bao, Xuezhen Tu, Bin Zhong, Linan Yue, Minling Zhang</strong></p>
<p>Current multimodal large language models (MLLMs) struggle with hour-level video understanding, facing significant challenges not only in modeling the substantial information volume of long videos but also in overcoming the memory wall and resource constraints during both training and inference. Although recent training-free approaches have alleviated resource demands by compressing visual features, their reliance on incomplete visual information limits the performance potential. To address these limitations, we propose Adaptive Pivot Visual information Retrieval (APVR), a training-free framework that hierarchically retrieves and retains sufficient and important visual information. It breakthroughs the memory wall limitation via two complementary components: Pivot Frame Retrieval employs query expansion and iterative spatio-semantic confidence scoring to identify relevant video frames, and Pivot Token Retrieval performs query-aware attention-driven token selection within up to 1024 pivot frames. This dual granularity approach enables the processing of hour-long videos while maintaining semantic fidelity. Experimental validations on three different baseline MLLMs demonstrate significant performance improvements up to 9.5%, 4.6% and 9.7% on LongVideoBench, VideoMME and MLVU, respectively. APVR achieves state-of-the-art results for both training-free and training-based approaches.</p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å°æ—¶çº§åˆ«çš„è§†é¢‘ç†è§£ä¸Šé‡åˆ°äº†å›°éš¾ï¼Œä¸ä»…é¢ä¸´å¯¹é•¿è§†é¢‘å¤§é‡ä¿¡æ¯çš„å»ºæ¨¡æŒ‘æˆ˜ï¼Œè€Œä¸”åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­éƒ½é¢ä¸´ç€çªç ´å†…å­˜é™åˆ¶å’Œèµ„æºçº¦æŸçš„æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘çš„æ— è®­ç»ƒæ–¹æ³•é€šè¿‡å‹ç¼©è§†è§‰ç‰¹å¾å‡è½»äº†èµ„æºéœ€æ±‚ï¼Œä½†å®ƒä»¬å¯¹ä¸å®Œæ•´è§†è§‰ä¿¡æ¯çš„ä¾èµ–é™åˆ¶äº†æ€§èƒ½æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”æ¢è½´è§†è§‰ä¿¡æ¯æ£€ç´¢ï¼ˆAPVRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒæ¡†æ¶ï¼Œå¯ä»¥åˆ†å±‚æ£€ç´¢å’Œä¿ç•™å……è¶³ä¸”é‡è¦çš„è§†è§‰ä¿¡æ¯ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªäº’è¡¥çš„ç»„ä»¶çªç ´äº†å†…å­˜å¢™çš„é™åˆ¶ï¼šæ¢è½´å¸§æ£€ç´¢åˆ©ç”¨æŸ¥è¯¢æ‰©å±•å’Œè¿­ä»£çš„ç©ºé—´è¯­ä¹‰ç½®ä¿¡åº¦æ‰“åˆ†æ¥è¯†åˆ«ç›¸å…³çš„è§†é¢‘å¸§ï¼Œè€Œæ¢è½´ä»¤ç‰Œæ£€ç´¢åˆ™åœ¨ä¸Šè‡³1 024ä¸ªæ¢è½´å¸§å†…æ‰§è¡ŒæŸ¥è¯¢æ„ŸçŸ¥çš„æ³¨æ„åŠ›é©±åŠ¨ä»¤ç‰Œé€‰æ‹©ã€‚è¿™ç§åŒé‡ç²’åº¦çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒè¯­ä¹‰ä¿çœŸåº¦çš„åŒæ—¶å¤„ç†é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ã€‚åœ¨LongVideoBenchã€VideoMMEå’ŒMLVUä¸‰ä¸ªä¸åŒçš„åŸºçº¿MLLMä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒAPVRçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†9.5%ã€4.6%å’Œ9.7%ã€‚APVRåœ¨æ— è®­ç»ƒå’ŒåŸºäºè®­ç»ƒçš„æ–¹æ³•ä¸­éƒ½è¾¾åˆ°äº†æœ€æ–°çš„ç ”ç©¶ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04953v3">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è¾¾æ•°å°æ—¶çš„è§†é¢‘æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºè‡ªé€‚åº”æ¢è½´è§†è§‰ä¿¡æ¯æ£€ç´¢ï¼ˆAPVRï¼‰çš„è®­ç»ƒå¤–æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå±‚æ¬¡åœ°æ£€ç´¢å’Œä¿ç•™é‡è¦ä¸”å……è¶³çš„è§†è§‰ä¿¡æ¯ï¼Œçªç ´å†…å­˜é™åˆ¶ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªäº’è¡¥ç»„ä»¶â€”â€”æ¢è½´å¸§æ£€ç´¢å’Œæ¢è½´ä»¤ç‰Œæ£€ç´¢æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸‰ä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯•é›†ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤„ç†å¤§é‡ä¿¡æ¯å’Œå…‹æœå†…å­˜é™åˆ¶ã€‚</li>
<li>è®­ç»ƒå¤–æ–¹æ³•å¯ä»¥ç¼“è§£èµ„æºéœ€æ±‚ï¼Œä½†ä¾èµ–äºä¸å®Œå…¨çš„è§†è§‰ä¿¡æ¯é™åˆ¶äº†æ€§èƒ½æ½œåŠ›ã€‚</li>
<li>è‡ªé€‚åº”æ¢è½´è§†è§‰ä¿¡æ¯æ£€ç´¢ï¼ˆAPVRï¼‰æ˜¯ä¸€ç§è®­ç»ƒå¤–æ¡†æ¶ï¼Œå¯ä»¥å±‚æ¬¡åœ°æ£€ç´¢å’Œä¿ç•™é‡è¦çš„è§†è§‰ä¿¡æ¯ã€‚</li>
<li>APVRé€šè¿‡ä¸¤ä¸ªäº’è¡¥ç»„ä»¶å®ç°ï¼šæ¢è½´å¸§æ£€ç´¢å’Œæ¢è½´ä»¤ç‰Œæ£€ç´¢ã€‚</li>
<li>æ¢è½´å¸§æ£€ç´¢é‡‡ç”¨æŸ¥è¯¢æ‰©å±•å’Œè¿­ä»£æ—¶ç©ºè¯­ä¹‰ç½®ä¿¡åº¦è¯„åˆ†æ¥è¯†åˆ«ç›¸å…³è§†é¢‘å¸§ã€‚</li>
<li>æ¢è½´ä»¤ç‰Œæ£€ç´¢æ‰§è¡ŒæŸ¥è¯¢æ„ŸçŸ¥çš„æ³¨æ„åŠ›é©±åŠ¨ä»¤ç‰Œé€‰æ‹©åœ¨æœ€å¤š1024ä¸ªæ¢è½´å¸§å†…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4048995d24c8844043c83895684574d" align="middle">
<img src="https://picx.zhimg.com/v2-2415c525bc5da391d995ab085a959090" align="middle">
<img src="https://picx.zhimg.com/v2-14796060c3f1fa2653cd71aa247d87b7" align="middle">
<img src="https://picx.zhimg.com/v2-43304cccd9fb2396c90a87f0b72e4205" align="middle">
<img src="https://picx.zhimg.com/v2-92122352323be84caa8e6f7d836bd16a" align="middle">
<img src="https://picx.zhimg.com/v2-e81f85b40d185cff2f6839131a9c3758" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="OmniVDiff-Omni-Controllable-Video-Diffusion-for-Generation-and-Understanding"><a href="#OmniVDiff-Omni-Controllable-Video-Diffusion-for-Generation-and-Understanding" class="headerlink" title="OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding"></a>OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding</h2><p><strong>Authors:Dianbing Xi, Jiepeng Wang, Yuanzhi Liang, Xi Qiu, Yuchi Huo, Rui Wang, Chi Zhang, Xuelong Li</strong></p>
<p>In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff , aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. Our framework supports three key capabilities: (1) Text-conditioned video generation, where all modalities are jointly synthesized from a textual prompt; (2) Video understanding, where structural modalities are predicted from rgb inputs in a coherent manner; and (3) X-conditioned video generation, where video synthesis is guided by finegrained inputs such as depth, canny and segmentation. Extensive experiments demonstrate that OmniVDiff achieves state-of-the-art performance in video generation tasks and competitive results in video understanding. Its flexibility and scalability make it well-suited for downstream applications such as video-to-video translation, modality adaptation for visual tasks, and scene reconstruction.</p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯æ§è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œåä¸ºOmniVDiffï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªå•ä¸€çš„æ‰©æ•£æ¨¡å‹ä¸­åˆæˆå’Œç†è§£å¤šä¸ªè§†é¢‘è§†è§‰å†…å®¹ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼ŒOmniVDiffå¤„ç†å½©è‰²ç©ºé—´ä¸­çš„æ‰€æœ‰è§†é¢‘è§†è§‰æ¨¡å¼æ¥å­¦ä¹ è”åˆåˆ†å¸ƒï¼ŒåŒæ—¶é‡‡ç”¨è‡ªé€‚åº”æ§åˆ¶ç­–ç•¥ï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ¯ç§è§†è§‰æ¨¡å¼çš„ä½œç”¨ï¼Œæ—¢å¯ä»¥ä½œä¸ºç”Ÿæˆæ¨¡å¼ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºæ¡ä»¶æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒä¸‰ç§å…³é”®åŠŸèƒ½ï¼šï¼ˆ1ï¼‰æ–‡æœ¬æ§åˆ¶è§†é¢‘ç”Ÿæˆï¼Œå…¶ä¸­æ‰€æœ‰æ¨¡å¼éƒ½ä»ä¸€ä¸ªæ–‡æœ¬æç¤ºä¸­è”åˆåˆæˆï¼›ï¼ˆ2ï¼‰è§†é¢‘ç†è§£ï¼Œå…¶ä¸­ç»“æ„æ¨¡å¼ä»¥è¿è´¯çš„æ–¹å¼ä»RGBè¾“å…¥ä¸­è¿›è¡Œé¢„æµ‹ï¼›ï¼ˆ3ï¼‰Xæ§åˆ¶è§†é¢‘ç”Ÿæˆï¼Œå…¶ä¸­è§†é¢‘åˆæˆç”±æ·±åº¦ã€Cannyå’Œåˆ†æ®µç­‰ç²¾ç»†è¾“å…¥å¼•å¯¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOmniVDiffåœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚å…¶çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶éå¸¸é€‚åˆä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚è§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘ã€è§†è§‰ä»»åŠ¡çš„æ¨¡æ€é€‚åº”å’Œåœºæ™¯é‡å»ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10825v2">PDF</a> Accepted by AAAI 2026. Our project page: <a target="_blank" rel="noopener" href="https://tele-ai.github.io/OmniVDiff/">https://tele-ai.github.io/OmniVDiff/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯æ§è§†é¢‘æ‰©æ•£æ¡†æ¶OmniVDiffï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªå•ä¸€çš„æ‰©æ•£æ¨¡å‹ä¸­åˆæˆå’Œç†è§£å¤šç§è§†é¢‘è§†è§‰å†…å®¹ã€‚OmniVDiffå¤„ç†æ‰€æœ‰è§†é¢‘è§†è§‰æ¨¡å¼æ¥å­¦ä¹ è”åˆåˆ†å¸ƒï¼Œé‡‡ç”¨è‡ªé€‚åº”æ§åˆ¶ç­–ç•¥åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å„ç§è§†è§‰æ¨¡å¼çš„ä½œç”¨ã€‚è¯¥æ¡†æ¶æ”¯æŒæ–‡æœ¬å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆã€è§†é¢‘ç†è§£å’ŒXæ¡ä»¶è§†é¢‘ç”Ÿæˆç­‰ä¸‰ä¸ªæ ¸å¿ƒèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒOmniVDiffåœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚å…¶çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶æˆä¸ºè§†é¢‘åˆ°è§†é¢‘ç¿»è¯‘ã€è§†è§‰ä»»åŠ¡çš„æ¨¡æ€é€‚åº”å’Œåœºæ™¯é‡å»ºç­‰ä¸‹æ¸¸åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>OmniVDiffæ˜¯ä¸€ä¸ªæ–°å‹å¯æ§è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œå¯åœ¨ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ä¸­åŒæ—¶åˆæˆå’Œç†è§£å¤šç§è§†é¢‘è§†è§‰å†…å®¹ã€‚</li>
<li>è¯¥æ¡†æ¶å¤„ç†æ‰€æœ‰è§†é¢‘è§†è§‰æ¨¡å¼æ¥å­¦ä¹ è”åˆåˆ†å¸ƒï¼Œå®ç°å¤šæ¨¡æ€è§†é¢‘å†…å®¹çš„ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>OmniVDiffé‡‡ç”¨è‡ªé€‚åº”æ§åˆ¶ç­–ç•¥ï¼Œæ ¹æ®éœ€è¦åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å„ç§è§†è§‰æ¨¡å¼çš„ä½œç”¨ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒæ–‡æœ¬å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆã€è§†é¢‘ç†è§£å’ŒXæ¡ä»¶è§†é¢‘ç”Ÿæˆç­‰æ ¸å¿ƒèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒOmniVDiffåœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå¹¶åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</li>
<li>OmniVDiffå…·æœ‰çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚è§†é¢‘åˆ°è§†é¢‘ç¿»è¯‘ã€è§†è§‰ä»»åŠ¡çš„æ¨¡æ€é€‚åº”å’Œåœºæ™¯é‡å»ºç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e482f48dd8dee1862255f2bd18aff00" align="middle">
<img src="https://picx.zhimg.com/v2-b889e4c5e609ea3be06b5a796e4c9871" align="middle">
<img src="https://picx.zhimg.com/v2-0cf3a107caa6aa9d2d6cf9d2dd894d87" align="middle">
<img src="https://picx.zhimg.com/v2-676b1004d755069f28ec465bf0bbf6de" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TEMPLE-Incentivizing-Temporal-Understanding-of-Video-Large-Language-Models-via-Progressive-Pre-SFT-Alignment"><a href="#TEMPLE-Incentivizing-Temporal-Understanding-of-Video-Large-Language-Models-via-Progressive-Pre-SFT-Alignment" class="headerlink" title="TEMPLE: Incentivizing Temporal Understanding of Video Large Language Models via Progressive Pre-SFT Alignment"></a>TEMPLE: Incentivizing Temporal Understanding of Video Large Language Models via Progressive Pre-SFT Alignment</h2><p><strong>Authors:Shicheng Li, Lei Li, Kun Ouyang, Shuhuai Ren, Yuanxin Liu, Yuanxing Zhang, Fuzheng Zhang, Lingpeng Kong, Qi Liu, Xu Sun</strong></p>
<p>Video Large Language Models (Video LLMs) have achieved significant success by adopting the paradigm of large-scale pre-training followed by supervised fine-tuning (SFT). However, existing approaches struggle with temporal reasoning due to weak temporal correspondence in the data and over-reliance on the next-token prediction paradigm}, which collectively result in the absence temporal supervision. To address these limitations, we propose TEMPLE (TEMporal Preference LEarning), a systematic framework that enhances temporal reasoning capabilities through Direct Preference Optimization (DPO). To address temporal information scarcity in data, we introduce an automated pipeline for systematically constructing temporality-intensive preference pairs comprising three steps: selecting temporally rich videos, designing video-specific perturbation strategies, and evaluating model responses on clean and perturbed inputs. Complementing this data pipeline, we provide additional supervision signals via preference learning and propose a novel Progressive Pre-SFT Alignment strategy featuring two key innovations: a curriculum learning strategy which progressively increases perturbation difficulty to maximize data efficiency; and applying preference optimization before instruction tuning to incentivize fundamental temporal alignment. Extensive experiments demonstrate that our approach consistently improves Video LLM performance across multiple benchmarks with a relatively small set of self-generated DPO data. Our findings highlight TEMPLE as a scalable and efficient complement to SFT-based methods, paving the way for developing reliable Video LLMs.</p>
<blockquote>
<p>è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰é€šè¿‡é‡‡ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒåç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ¨¡å¼å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç”±äºæ•°æ®ä¸­çš„æ—¶é—´å¯¹åº”æ€§è¾ƒå¼±ä»¥åŠå¯¹ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æ¨¡å¼çš„è¿‡åº¦ä¾èµ–ï¼Œä»è€Œåœ¨æ—¶é—´æ¨ç†æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œè¿™äº›å› ç´ å…±åŒå¯¼è‡´äº†æ—¶é—´ç›‘ç£çš„ç¼ºå¤±ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†TEMPLEï¼ˆæ—¶ç©ºåå¥½å­¦ä¹ ï¼‰ç³»ç»Ÿæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¢å¼ºæ—¶é—´æ¨ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³æ•°æ®ä¸­æ—¶é—´ä¿¡æ¯åŒ®ä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºç³»ç»Ÿåœ°æ„å»ºåŒ…å«ä¸‰ä¸ªæ­¥éª¤çš„æ—¶é—´å¯†é›†åå¥½å¯¹ï¼šé€‰æ‹©æ—¶é—´ä¸°å¯Œçš„è§†é¢‘ã€è®¾è®¡é’ˆå¯¹è§†é¢‘çš„æ‰°åŠ¨ç­–ç•¥ï¼Œå¹¶åœ¨å¹²å‡€å’Œå—å¹²æ‰°çš„è¾“å…¥ä¸Šè¯„ä¼°æ¨¡å‹å“åº”ã€‚è¡¥å……è¿™ä¸€æ•°æ®ç®¡é“çš„åŒæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡åå¥½å­¦ä¹ æä¾›é¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œå¹¶æå‡ºä¸€ç§æ–°å‹æ¸è¿›å¼Pre-SFTå¯¹é½ç­–ç•¥ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥å¢åŠ æ‰°åŠ¨éš¾åº¦ä»¥æœ€å¤§åŒ–æ•°æ®æ•ˆç‡ï¼›ä»¥åŠåœ¨æŒ‡ä»¤è°ƒæ•´ä¹‹å‰åº”ç”¨åå¥½ä¼˜åŒ–ï¼Œä»¥æ¿€åŠ±åŸºæœ¬çš„æ—¶é—´å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½¿ç”¨è‡ªæˆ‘ç”Ÿæˆçš„å°‘é‡DPOæ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­æé«˜è§†é¢‘LLMçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒTEMPLEå¯ä»¥ä½œä¸ºåŸºäºSFTçš„æ–¹æ³•çš„å¯æ‰©å±•å’Œæœ‰æ•ˆçš„è¡¥å……ï¼Œä¸ºå¼€å‘å¯é çš„è§†é¢‘LLMé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16929v3">PDF</a> Accepted to AAAI 2026. Code available at <a target="_blank" rel="noopener" href="https://github.com/lscpku/TEMPLE">https://github.com/lscpku/TEMPLE</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒä¸ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨æ—¶é—´æ¨ç†æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºæ”¹å–„è¿™ä¸€çŠ¶å†µï¼Œæœ¬æ–‡æå‡ºTEMPLEæ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¢å¼ºæ¨¡å‹çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚ä¸ºåº”å¯¹æ•°æ®ä¸­æ—¶é—´ä¿¡æ¯çš„ç¨€ç¼ºæ€§ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†è‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºç³»ç»Ÿåœ°æ„å»ºæ—¶é—´å¯†é›†å‹çš„åå¥½å¯¹ã€‚æ­¤å¤–ï¼Œé€šè¿‡åå¥½å­¦ä¹ æä¾›é¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œå¹¶æå‡ºä¸€ç§æ–°å‹çš„æ¸è¿›å¼é¢„SFTå¯¹é½ç­–ç•¥ï¼ŒåŒ…å«æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ä»¥åŠåå¥½ä¼˜åŒ–å‰çš„æŒ‡ä»¤å¾®è°ƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æé«˜äº†Video LLMçš„æ€§èƒ½ï¼Œä¸”ä»…ä½¿ç”¨å°‘é‡è‡ªç”Ÿæˆçš„DPOæ•°æ®ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼ŒTEMPLEå¯ä½œä¸ºSFTæ–¹æ³•çš„å¯é è¡¥å……ï¼Œä¸ºå¼€å‘å¯é çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹é“ºå¹³é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video LLMså·²å–å¾—æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨æ—¶é—´æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>TEMPLEæ¡†æ¶æ—¨åœ¨é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¢å¼ºæ¨¡å‹çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸ºåº”å¯¹æ•°æ®ä¸­çš„æ—¶é—´ä¿¡æ¯ç¨€ç¼ºé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†è‡ªåŠ¨åŒ–ç®¡é“æ¥ç³»ç»Ÿåœ°åˆ›å»ºæ—¶é—´å¯†é›†å‹çš„åå¥½å¯¹ã€‚</li>
<li>é€šè¿‡åå¥½å­¦ä¹ æä¾›é¢å¤–ç›‘ç£ä¿¡å·ã€‚</li>
<li>æ–°å‹æ¸è¿›å¼é¢„SFTå¯¹é½ç­–ç•¥åŒ…æ‹¬æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ä»¥åŠæ—©æœŸçš„åå¥½ä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒTEMPLEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†Video LLMçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16929">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f222e213fb002fd7d9e5fd6897b3f185" align="middle">
<img src="https://picx.zhimg.com/v2-42acb237382b2f5c7b4ea6907835c477" align="middle">
<img src="https://picx.zhimg.com/v2-49ffa460e9804f798d401279ee791b5a" align="middle">
<img src="https://picx.zhimg.com/v2-9cd1d6ef5b41534f41d38425a33370fc" align="middle">
<img src="https://picx.zhimg.com/v2-712d71a2df80e06bd24e48b8180610cd" align="middle">
<img src="https://picx.zhimg.com/v2-d708f34f8b40480a8006d989d8baf07a" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SVBench-A-Benchmark-with-Temporal-Multi-Turn-Dialogues-for-Streaming-Video-Understanding"><a href="#SVBench-A-Benchmark-with-Temporal-Multi-Turn-Dialogues-for-Streaming-Video-Understanding" class="headerlink" title="SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding"></a>SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding</h2><p><strong>Authors:Zhenyu Yang, Yuhang Hu, Zemin Du, Dizhan Xue, Shengsheng Qian, Jiahong Wu, Fan Yang, Weiming Dong, Changsheng Xu</strong></p>
<p>Despite the significant advancements of Large Vision-Language Models (LVLMs) on established benchmarks, there remains a notable gap in suitable evaluation regarding their applicability in the emerging domain of long-context streaming video understanding. Current benchmarks for video understanding typically emphasize isolated single-instance text inputs and fail to evaluate the capacity to sustain temporal reasoning throughout the entire duration of video streams. To address these limitations, we introduce SVBench, a pioneering benchmark with temporal multi-turn question-answering chains specifically designed to thoroughly assess the capabilities of streaming video understanding of current LVLMs. We design a semi-automated annotation pipeline to obtain 49,979 Question-Answer (QA) pairs of 1,353 streaming videos, which includes generating QA chains that represent a series of consecutive multi-turn dialogues over video segments and constructing temporal linkages between successive QA chains. Our experimental results, obtained from 14 models in dialogue and streaming evaluations, reveal that while the closed-source GPT-4o outperforms others, most open-source LVLMs struggle with long-context streaming video understanding. We also construct a StreamingChat model, which significantly outperforms open-source LVLMs on our SVBench and achieves comparable performance on diverse vision-language benchmarks. We expect SVBench to advance the research of streaming video understanding by providing a comprehensive and in-depth analysis of current LVLMs. Our benchmark and model can be accessed at <a target="_blank" rel="noopener" href="https://github.com/sotayang/SVBench">https://github.com/sotayang/SVBench</a>.</p>
<blockquote>
<p>å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æ—¢å®šåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é•¿è¯­å¢ƒæµåª’ä½“è§†é¢‘ç†è§£ç­‰æ–°å…´é¢†åŸŸï¼Œå…¶é€‚ç”¨æ€§ä»å­˜åœ¨æ˜æ˜¾çš„è¯„ä»·å·®è·ã€‚å½“å‰è§†é¢‘ç†è§£çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾§é‡äºå­¤ç«‹çš„å•ä¸€å®ä¾‹æ–‡æœ¬è¾“å…¥ï¼Œæ— æ³•è¯„ä¼°åœ¨æ•´ä¸ªè§†é¢‘æµè¿‡ç¨‹ä¸­ç»´æŒæ—¶é—´æ¨ç†çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†SVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æ—¶é—´å¤šè½®é—®ç­”é“¾çš„å¼€åˆ›æ€§åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå…¨é¢è¯„ä¼°å½“å‰LVLMsçš„æµåª’ä½“è§†é¢‘ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŠè‡ªåŠ¨æ³¨é‡Šç®¡é“ï¼Œè·å¾—äº†49979ä¸ªé—®ç­”å¯¹ï¼Œæ¶‰åŠ1353ä¸ªæµåª’ä½“è§†é¢‘ï¼ŒåŒ…æ‹¬ç”Ÿæˆä»£è¡¨è§†é¢‘ç‰‡æ®µä¸Šè¿ç»­å¤šè½®å¯¹è¯çš„QAé“¾ï¼Œä»¥åŠåœ¨è¿ç»­QAé“¾ä¹‹é—´æ„å»ºæ—¶é—´é“¾æ¥ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ¥è‡ª14ä¸ªæ¨¡å‹åœ¨å¯¹è¯å’Œæµåª’ä½“è¯„ä¼°ä¸­çš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡é—­æºGPT-4oè¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œä½†å¤§å¤šæ•°å¼€æºLVLMsåœ¨é•¿æŒ‰ä¸Šä¸‹æ–‡æµåª’ä½“è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å›°éš¾ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªStreamingChatæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æˆ‘ä»¬çš„SVBenchä¸Šæ˜¾è‘—ä¼˜äºå¼€æºLVLMsï¼Œå¹¶åœ¨å„ç§è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ç›¸å½“ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡SVBenchæ¨è¿›æµåª’ä½“è§†é¢‘ç†è§£çš„ç ”ç©¶ï¼Œä¸ºå½“å‰LVLMsæä¾›å…¨é¢æ·±å…¥çš„åˆ†æã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/sotayang/SVBench%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/sotayang/SVBenchè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10810v2">PDF</a> ICLR 2025 Accepted (Spotlight)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹é’ˆå¯¹é•¿è§†é¢‘ç†è§£é¢†åŸŸçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¯„ä¼°çš„æ–°åŸºå‡†SVBenchã€‚è¯¥åŸºå‡†é€šè¿‡è®¾è®¡åŒ…å«è¿ç»­å¤šè½®å¯¹è¯çš„è§†é¢‘ç‰‡æ®µé—®ç­”é“¾ï¼Œè¯„ä¼°LVLMsåœ¨æµå¼è§†é¢‘ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4oè¡¨ç°æœ€ä½³ï¼Œè€Œå¤§å¤šæ•°å¼€æºLVLMsåœ¨å¤„ç†é•¿è§†é¢‘ç†è§£æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åŒæ—¶ï¼Œæå‡ºäº†StreamingChatæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨SVBenchä¸Šçš„è¡¨ç°ä¼˜äºå¼€æºLVLMså¹¶åœ¨å¤šç§è§†è§‰è¯­è¨€åŸºå‡†ä¸Šå®ç°äº†è‰¯å¥½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SVBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹é•¿è§†é¢‘ç†è§£çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°çš„æ–°åŸºå‡†ã€‚</li>
<li>SVBenché€šè¿‡è®¾è®¡åŒ…å«è¿ç»­å¤šè½®å¯¹è¯çš„è§†é¢‘ç‰‡æ®µé—®ç­”é“¾ï¼Œå¼ºè°ƒå¯¹è§†é¢‘æµå…¨ç¨‹çš„æ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>å½“å‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æµå¼è§†é¢‘ç†è§£æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>GPT-4oåœ¨SVBenchä¸Šçš„è¡¨ç°æœ€ä½³ï¼Œå¤šæ•°å¼€æºæ¨¡å‹å¤„ç†é•¿è§†é¢‘å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºçš„StreamingChatæ¨¡å‹åœ¨SVBenchä¸Šè¡¨ç°ä¼˜äºå¤§å¤šæ•°å¼€æºæ¨¡å‹ã€‚</li>
<li>SVBenchæœŸæœ›èƒ½æ¨åŠ¨é•¿è§†é¢‘ç†è§£çš„æ·±å…¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-821f46c5c79577dae01357073dd0f32f" align="middle">
<img src="https://picx.zhimg.com/v2-8ae00f7f0b2979bcaeed612533736e63" align="middle">
<img src="https://picx.zhimg.com/v2-8e1a2b315c12e89051c2b13abb727b89" align="middle">
<img src="https://picx.zhimg.com/v2-1ff8e02cb3a55eff35491ce630661849" align="middle">
<img src="https://picx.zhimg.com/v2-aec1e8a08835a98ad50329ed4cb3e483" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e07c52a6ebb5464c9cea2c1ebe0160e9" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  MergeSlide Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-516fb79028978983af85fdb61db0164c" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Free-Form Scene Editor Enabling Multi-Round Object Manipulation like in a 3D Engine
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
