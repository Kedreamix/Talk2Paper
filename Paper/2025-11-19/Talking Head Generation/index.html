<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b942947f17f32787af52c3e88c21c9b1')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    5.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    21 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-19-æ›´æ–°"><a href="#2025-11-19-æ›´æ–°" class="headerlink" title="2025-11-19 æ›´æ–°"></a>2025-11-19 æ›´æ–°</h1><h2 id="Tighter-Truncated-Rectangular-Prism-Approximation-for-RNN-Robustness-Verification"><a href="#Tighter-Truncated-Rectangular-Prism-Approximation-for-RNN-Robustness-Verification" class="headerlink" title="Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification"></a>Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification</h2><p><strong>Authors:Xingqi Lin, Liangyu Chen, Min Wu, Min Zhang, Zhenbing Zeng</strong></p>
<p>Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.</p>
<blockquote>
<p>ç¨³å¥æ€§éªŒè¯æ˜¯è¯æ˜å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ç¨³å¥æ€§çš„æœ‰å‰é€”çš„æŠ€æœ¯ã€‚å…³é”®æŒ‘æˆ˜åœ¨äºç”¨çº¿æ€§çº¦æŸå¯¹éçº¿æ€§æ¿€æ´»å‡½æ•°è¿›è¡Œè¿‡è¿‘ä¼¼å¤„ç†ï¼Œè¿™å¯ä»¥å°†éªŒè¯é—®é¢˜è½¬åŒ–ä¸ºå¯é«˜æ•ˆè§£å†³çš„çº¿æ€§è§„åˆ’é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡çº¿æ€§è¾¹ç•Œå¹³é¢å•ç‹¬å¯¹éçº¿æ€§éƒ¨åˆ†è¿›è¡Œè¿‡è¿‘ä¼¼å¤„ç†ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´è¾ƒå¤§çš„è¿‡åº¦ä¼°è®¡ï¼Œä»è€Œå¯¼è‡´éªŒè¯ç²¾åº¦é™ä½ã€‚æœ¬æ–‡ä¸­ï¼Œä¸ºäº†ç´§å¯†å°é—­ç”±Hadamardä¹˜ç§¯äº§ç”Ÿçš„ä¸‰ç»´éçº¿æ€§è¡¨é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±ä¸¤ä¸ªçº¿æ€§æ¾å¼›å¹³é¢å½¢æˆçš„æ–°å‹æˆªæ–­çŸ©å½¢æ£±æŸ±ï¼Œä»¥åŠä¸€ç§ä»¥ç»†åŒ–é©±åŠ¨çš„æ–¹æ³•æ¥æœ€å°åŒ–å…¶ä½“ç§¯å’Œè¡¨é¢ç§¯ï¼Œä»¥å®ç°æ›´ç´§å¯†çš„è¿‡è¿‘ä¼¼ã€‚åŸºäºè¿™ç§è¿‘ä¼¼æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†ç”¨äºRNNç¨³å¥æ€§éªŒè¯çš„DeepPrismåŸå‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«å’Œæƒ…æ„Ÿåˆ†æç­‰å„é¡¹ä»»åŠ¡ä¸­ç›¸æ¯”ï¼ŒDeepPrismå…·æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11699v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç”¨äºå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ç¨³å¥æ€§éªŒè¯çš„æ–°æ–¹æ³•ã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºéçº¿æ€§æ¿€æ´»å‡½æ•°çš„çº¿æ€§çº¦æŸè¿‡åº¦è¿‘ä¼¼ï¼Œè¯¥é—®é¢˜å¯ä»¥é€šè¿‡é‡‡ç”¨ç”±ä¸¤ä¸ªçº¿æ€§æ¾å¼›å¹³é¢æ„æˆçš„æˆªæ–­çŸ©å½¢æ£±æŸ±æ¥è§£å†³ã€‚æ–°æ–¹æ³•æ—¨åœ¨ç´§å¯†åŒ…è£¹ç”±Hadamardäº§å“ç”Ÿæˆçš„ä¸‰ç»´éçº¿æ€§è¡¨é¢ï¼Œé€šè¿‡æœ€å°åŒ–å…¶ä½“ç§¯å’Œè¡¨é¢ç§¯æ¥å®ç°æ›´ç´§å¯†çš„ä¸Šç•Œè¿‘ä¼¼ã€‚åŸºäºè¿™ç§è¿‘ä¼¼ï¼Œå®ç°äº†åä¸ºDeepPrismçš„åŸå‹ç”¨äºRNNç¨³å¥æ€§éªŒè¯ï¼Œåœ¨å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«å’Œæƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨³å¥æ€§éªŒè¯æ˜¯è¯æ˜å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ç¨³å¥æ€§çš„æœ‰å‰é€”çš„æŠ€æœ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡çº¿æ€§è¾¹ç•Œå¹³é¢è¿›è¡Œéçº¿æ€§éƒ¨åˆ†çš„è¿‡åº¦è¿‘ä¼¼ï¼Œå¯èƒ½å¯¼è‡´æ˜¾è‘—è¿‡åº¦ä¼°è®¡å’Œè¾ƒä½çš„éªŒè¯ç²¾åº¦ã€‚</li>
<li>ä¸ºç´§å¯†åŒ…è£¹ç”±Hadamardäº§å“ç”Ÿæˆçš„ä¸‰ç»´éçº¿æ€§è¡¨é¢ï¼Œæå‡ºäº†ç”±ä¸¤ä¸ªçº¿æ€§æ¾å¼›å¹³é¢æ„æˆçš„æˆªæ–­çŸ©å½¢æ£±æŸ±ã€‚</li>
<li>æœ€å°åŒ–æˆªæ–­çŸ©å½¢æ£±æŸ±çš„ä½“ç§¯å’Œè¡¨é¢ç§¯ä»¥å®ç°æ›´ç´§å¯†çš„ä¸Šç•Œè¿‘ä¼¼ã€‚</li>
<li>åŸºäºè¿™ç§æ–°çš„è¿‘ä¼¼æ–¹æ³•ï¼Œå®ç°äº†åä¸ºDeepPrismçš„RNNç¨³å¥æ€§éªŒè¯åŸå‹ã€‚</li>
<li>åœ¨å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«å’Œæƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ä¸Šï¼ŒDeepPrismç›¸è¾ƒäºæœ€æ–°æ–¹æ³•å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50257fa6ea93cfc632cf264e618f4330" align="middle">
<img src="https://picx.zhimg.com/v2-b942947f17f32787af52c3e88c21c9b1" align="middle">
<img src="https://picx.zhimg.com/v2-e6e42b8f38ed935082b38dbb5b74ee1b" align="middle">
<img src="https://picx.zhimg.com/v2-81c534a8d2d438f54c72d18d50cdd610" align="middle">
<img src="https://picx.zhimg.com/v2-36715e8b60404f6fa99691ff5369a2d7" align="middle">
<img src="https://picx.zhimg.com/v2-f158958587812d25a14d8ec0c28205a0" align="middle">
<img src="https://picx.zhimg.com/v2-5c6f8b7aa3441f25837e55be8aa57554" align="middle">
<img src="https://picx.zhimg.com/v2-570ebdc4b7a7f65a46ef78cefece13f1" align="middle">
<img src="https://picx.zhimg.com/v2-ebdafd08f5cfbb4d920ed188e7a2d79e" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DialogGraph-LLM-Graph-Informed-LLMs-for-End-to-End-Audio-Dialogue-Intent-Recognition"><a href="#DialogGraph-LLM-Graph-Informed-LLMs-for-End-to-End-Audio-Dialogue-Intent-Recognition" class="headerlink" title="DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition"></a>DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition</h2><p><strong>Authors:HongYu Liu, Junxin Li, Changxi Guo, Hao Chen, Yaqian Huang, Yifu Guo, Huan Yang, Lihua Cai</strong></p>
<p>Recognizing speaker intent in long audio dialogues among speakers has a wide range of applications, but is a non-trivial AI task due to complex inter-dependencies in speaker utterances and scarce annotated data. To address these challenges, an end-to-end framework, namely DialogGraph-LLM, is proposed in the current work. DialogGraph-LLM combines a novel Multi-Relational Dialogue Attention Network (MR-DAN) architecture with multimodal foundation models (e.g., Qwen2.5-Omni-7B) for direct acoustic-to-intent inference. An adaptive semi-supervised learning strategy is designed using LLM with a confidence-aware pseudo-label generation mechanism based on dual-threshold filtering using both global and class confidences, and an entropy-based sample selection process that prioritizes high-information unlabeled instances. Extensive evaluations on the proprietary MarketCalls corpus and the publicly available MIntRec 2.0 benchmark demonstrate DialogGraph-LLMâ€™s superiority over strong audio and text-driven baselines. The framework demonstrates strong performance and efficiency in intent recognition in real world scenario audio dialogues, proving its practical value for audio-rich domains with limited supervision. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/david188888/DialogGraph-LLM">https://github.com/david188888/DialogGraph-LLM</a>.</p>
<blockquote>
<p>è¯†åˆ«é•¿éŸ³é¢‘å¯¹è¯ä¸­è¯´è¯è€…çš„æ„å›¾å…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œä½†ç”±äºè¯´è¯è€…è¯è¯­ä¹‹é—´çš„å¤æ‚ç›¸äº’ä¾èµ–å…³ç³»å’Œæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºï¼Œè¿™æ˜¯ä¸€é¡¹éå¹³å‡¡çš„AIä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå½“å‰å·¥ä½œæå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œå³DialogGraph-LLMã€‚DialogGraph-LLMç»“åˆäº†ä¸€ç§æ–°å‹çš„å¤šå…³ç³»å¯¹è¯æ³¨æ„ç½‘ç»œï¼ˆMR-DANï¼‰æ¶æ„å’Œå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚Qwen2.5-Omni-7Bï¼‰è¿›è¡Œç›´æ¥çš„å£°å­¦æ„å›¾æ¨æ–­ã€‚è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”çš„åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œä½¿ç”¨LLMï¼ŒåŸºäºåŒé‡é˜ˆå€¼è¿‡æ»¤çš„å…¨å±€å’Œç±»åˆ«ç½®ä¿¡åº¦ï¼Œä»¥åŠåŸºäºç†µçš„æ ·æœ¬é€‰æ‹©è¿‡ç¨‹æ¥ä¼˜å…ˆå¤„ç†é«˜ä¿¡æ¯é‡çš„æœªæ ‡è®°å®ä¾‹çš„ç½®ä¿¡åº¦æ„ŸçŸ¥ä¼ªæ ‡ç­¾ç”Ÿæˆæœºåˆ¶ã€‚åœ¨ä¸“æœ‰MarketCallsè¯­æ–™åº“å’Œå…¬å¼€çš„MIntRec 2.0åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDialogGraph-LLMä¼˜äºå¼ºå¤§çš„éŸ³é¢‘å’Œæ–‡æœ¬é©±åŠ¨åŸºå‡†æµ‹è¯•ã€‚è¯¥æ¡†æ¶åœ¨ç°å®åœºæ™¯éŸ³é¢‘å¯¹è¯çš„æ„å›¾è¯†åˆ«ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œè¯æ˜äº†å…¶åœ¨ç›‘ç£æœ‰é™çš„ä¸°å¯ŒéŸ³é¢‘é¢†åŸŸçš„å®ç”¨ä»·å€¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/david188888/DialogGraph-LLM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/david188888/DialogGraph-LLMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11000v2">PDF</a> 8 pages, 2 figures. To appear in: Proceedings of the 28th European Conference on Artificial Intelligence (ECAI 2025), Frontiers in Artificial Intelligence and Applications, Vol. 413. DOI: 10.3233&#x2F;FAIA251182</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹é•¿éŸ³é¢‘å¯¹è¯ä¸­çš„è¯´è¯äººæ„å›¾è¯†åˆ«è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDialogGraph-LLMçš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šå…³ç³»å¯¹è¯æ³¨æ„åŠ›ç½‘ç»œï¼ˆMR-DANï¼‰å’Œå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Qwen2.5-Omni-7Bï¼‰ï¼Œç”¨äºç›´æ¥è¿›è¡Œå£°éŸ³åˆ°æ„å›¾çš„æ¨ç†ã€‚è¯¥æ¡†æ¶è¿˜ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œä½¿ç”¨åŸºäºåŒé‡é˜ˆå€¼è¿‡æ»¤å’Œç†µæ ·æœ¬é€‰æ‹©çš„ç½®ä¿¡åº¦æ„ŸçŸ¥ä¼ªæ ‡ç­¾ç”Ÿæˆæœºåˆ¶ã€‚åœ¨MarketCallsè¯­æ–™åº“å’Œå…¬å¼€çš„MIntRec 2.0åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒDialogGraph-LLMåœ¨éŸ³é¢‘å¯¹è¯ä¸­çš„æ„å›¾è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DialogGraph-LLMæ˜¯ä¸€ä¸ªç”¨äºéŸ³é¢‘å¯¹è¯ä¸­è¯´è¯äººæ„å›¾è¯†åˆ«çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†MR-DANå’Œå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹è¿›è¡Œå£°éŸ³åˆ°æ„å›¾çš„ç›´æ¥æ¨ç†ã€‚</li>
<li>DialogGraph-LLMé‡‡ç”¨äº†è‡ªé€‚åº”åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè®¾è®¡ã€‚</li>
<li>è¯¥ç­–ç•¥åŒ…æ‹¬åŸºäºåŒé‡é˜ˆå€¼è¿‡æ»¤å’Œç†µæ ·æœ¬é€‰æ‹©çš„ç½®ä¿¡åº¦æ„ŸçŸ¥ä¼ªæ ‡ç­¾ç”Ÿæˆæœºåˆ¶ã€‚</li>
<li>åœ¨MarketCallsè¯­æ–™åº“å’ŒMIntRec 2.0åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¯æ˜äº†DialogGraph-LLMçš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>DialogGraph-LLMåœ¨å®é™…åœºæ™¯ä¸­çš„éŸ³é¢‘å¯¹è¯æ„å›¾è¯†åˆ«ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-402ee8eed96d746857661115340a68e4" align="middle">
<img src="https://picx.zhimg.com/v2-ddc8bd36edc20fd7024f85e80482d9d9" align="middle">
<img src="https://picx.zhimg.com/v2-7a7b04ad0ae0a8e18eb83eb130118903" align="middle">
<img src="https://picx.zhimg.com/v2-2d101ba0534b2bb3aed5c38c919420b8" align="middle">
<img src="https://picx.zhimg.com/v2-00b2a3ce01b7cec6f1b632dade976223" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reinforcing-Trustworthiness-in-Multimodal-Emotional-Support-Systems"><a href="#Reinforcing-Trustworthiness-in-Multimodal-Emotional-Support-Systems" class="headerlink" title="Reinforcing Trustworthiness in Multimodal Emotional Support Systems"></a>Reinforcing Trustworthiness in Multimodal Emotional Support Systems</h2><p><strong>Authors:Huy M. Le, Dat Tien Nguyen, Ngan T. T. Vo, Tuan D. Q. Nguyen, Nguyen Binh Le, Duy Minh Ho Nguyen, Daniel Sonntag, Lizi Liao, Binh T. Nguyen</strong></p>
<p>In todayâ€™s world, emotional support is increasingly essential, yet it remains challenging for both those seeking help and those offering it. Multimodal approaches to emotional support show great promise by integrating diverse data sources to provide empathetic, contextually relevant responses, fostering more effective interactions. However, current methods have notable limitations, often relying solely on text or converting other data types into text, or providing emotion recognition only, thus overlooking the full potential of multimodal inputs. Moreover, many studies prioritize response generation without accurately identifying critical emotional support elements or ensuring the reliability of outputs. To overcome these issues, we introduce \textsc{ MultiMood}, a new framework that (i) leverages multimodal embeddings from video, audio, and text to predict emotional components and to produce responses responses aligned with professional therapeutic standards. To improve trustworthiness, we (ii) incorporate novel psychological criteria and apply Reinforcement Learning (RL) to optimize large language models (LLMs) for consistent adherence to these standards. We also (iii) analyze several advanced LLMs to assess their multimodal emotional support capabilities. Experimental results show that MultiMood achieves state-of-the-art on MESC and DFEW datasets while RL-driven trustworthiness improvements are validated through human and LLM evaluations, demonstrating its superior capability in applying a multimodal framework in this domain.</p>
<blockquote>
<p>åœ¨ç°ä»Šä¸–ç•Œï¼Œæƒ…æ„Ÿæ”¯æŒå˜å¾—æ„ˆå‘é‡è¦ï¼Œä½†å¯¹äºå¯»æ±‚å¸®åŠ©å’Œæä¾›å¸®åŠ©çš„äººæ¥è¯´éƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚é€šè¿‡æ•´åˆå„ç§æ•°æ®æºï¼Œæƒ…æ„Ÿæ”¯æŒçš„å¤šæ¨¡å¼æ–¹æ³•å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æä¾›å¯Œæœ‰åŒæƒ…å¿ƒå’Œä¸Šä¸‹æ–‡ç›¸å…³çš„å›åº”ï¼Œä¿ƒè¿›æ›´æœ‰æ•ˆçš„äº’åŠ¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•æœ‰æ˜æ˜¾çš„å±€é™æ€§ï¼Œé€šå¸¸ä»…ä¾èµ–æ–‡æœ¬æˆ–å°†å…¶ä»–æ•°æ®ç±»å‹è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œæˆ–è€…åªæä¾›æƒ…ç»ªè¯†åˆ«ï¼Œä»è€Œå¿½ç•¥äº†å¤šæ¨¡æ€è¾“å…¥çš„å…¨é¢æ½œåŠ›ã€‚æ­¤å¤–ï¼Œè®¸å¤šç ”ç©¶ä¼˜å…ˆæ³¨é‡å“åº”ç”Ÿæˆï¼Œè€Œæ²¡æœ‰å‡†ç¡®è¯†åˆ«å…³é”®çš„æƒ…æ„Ÿæ”¯æŒå…ƒç´ æˆ–ç¡®ä¿è¾“å‡ºçš„å¯é æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†\textsc{MultiMood}æ–°æ¡†æ¶ï¼Œå®ƒï¼ˆiï¼‰åˆ©ç”¨è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„å¤šæ¨¡å¼åµŒå…¥æ¥é¢„æµ‹æƒ…æ„Ÿæˆåˆ†ï¼Œå¹¶äº§ç”Ÿä¸ä¸“ä¸šæ²»ç–—æ ‡å‡†ç›¸ç¬¦çš„å“åº”ã€‚ä¸ºäº†æé«˜å¯ä¿¡åº¦ï¼Œæˆ‘ä»¬ï¼ˆiiï¼‰ç»“åˆäº†æ–°é¢–çš„å¿ƒç†æ ‡å‡†å’Œåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿å…¶å§‹ç»ˆéµå¾ªè¿™äº›æ ‡å‡†ã€‚æˆ‘ä»¬è¿˜ï¼ˆiiiï¼‰åˆ†æäº†å¤šä¸ªå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è¯„ä¼°å…¶å¤šæ¨¡å¼æƒ…æ„Ÿæ”¯æŒèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMultiMoodåœ¨MESCå’ŒDFEWæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè€Œç”±å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„å¯é æ€§æ”¹è¿›é€šè¿‡äººç±»å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜å…¶åœ¨è¯¥é¢†åŸŸåº”ç”¨å¤šæ¨¡å¼æ¡†æ¶çš„å“è¶Šèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10011v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåœ¨å¦‚ä»Šä¸–ç•Œï¼Œæƒ…æ„Ÿæ”¯æŒå˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œç„¶è€Œï¼Œå¯¹å¯»æ±‚å¸®åŠ©è€…å’Œæä¾›å¸®åŠ©è€…æ¥è¯´ä»é¢ä¸´æŒ‘æˆ˜ã€‚å¤šæ¨¡å¼æƒ…æ„Ÿæ”¯æŒæ–¹æ³•é€šè¿‡æ•´åˆå„ç§æ•°æ®æºæä¾›å¯Œæœ‰åŒæƒ…å¿ƒçš„ã€è¯­å¢ƒç›¸å…³çš„å›åº”ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•å­˜åœ¨è¯¸å¤šå±€é™æ€§ï¼Œé€šå¸¸åªä¾èµ–æ–‡æœ¬æˆ–å°†å…¶ä»–æ•°æ®ç±»å‹è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œä»…æä¾›æƒ…æ„Ÿè¯†åˆ«ï¼Œå¿½ç•¥äº†å¤šæ¨¡å¼è¾“å…¥çš„æ½œåŠ›ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶â€œMultiMoodâ€ï¼Œå®ƒèƒ½åˆ©ç”¨è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„å¤šå…ƒåµŒå…¥æ¥é¢„æµ‹æƒ…æ„Ÿæˆåˆ†å¹¶äº§ç”Ÿä¸ä¸“ä¸šçš„æ²»ç–—æ ‡å‡†ç›¸ç¬¦çš„å“åº”ã€‚ä¸ºäº†æé«˜å¯ä¿¡åº¦ï¼Œæˆ‘ä»¬è¿˜ç»“åˆäº†æ–°å‹å¿ƒç†æ ‡å‡†å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚é€šè¿‡å¤šæ¨¡å¼åˆ†æéªŒè¯äº†MultiMoodæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”åœ¨MESCå’ŒDFEWæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœéªŒè¯äº†å…¶åœ¨RLé©±åŠ¨ä¸‹å¯ä¿¡åº¦çš„æé«˜ã€‚<strong>å…³é”®è¦ç‚¹</strong>ï¼š</p>
<ol>
<li>å½“å‰æƒ…æ„Ÿæ”¯æŒé¢ä¸´çš„æŒ‘æˆ˜æ˜¯æœªèƒ½å‡†ç¡®ç†è§£éœ€æ±‚å¹¶æœ‰æ•ˆåœ°å›åº”ã€‚å¤šæ¨¡å¼æƒ…æ„Ÿæ”¯æŒæ–¹æ³•å¯ä»¥èåˆä¸åŒæ•°æ®æºæ¥æä¾›æœ‰é’ˆå¯¹æ€§çš„æ”¯æŒã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å’Œè¡¨è¾¾æƒ…æ„Ÿä¸Šå±€é™æ€§è¾ƒå¤§ï¼Œå¯èƒ½å¿½è§†äº†å…¨é¢çš„å¤šæ¨¡å¼ä¿¡æ¯æ½œåŠ›ã€‚ä¸€äº›ç°æœ‰çš„æƒ…æ„Ÿè¡¨è¾¾ä»…ä»…ä¾èµ–æ–‡å­—æ•°æ®æˆ–å¯¹çœŸå®æ„Ÿæƒ…åé¦ˆçš„è¡¨ç¤ºæ–¹æ³•æœ‰æ‰€ç®€åŒ–ã€‚éœ€è¦æ•´åˆå„ç§å½¢å¼çš„ä¿¡æ¯è¾“å…¥ä»¥æé«˜å¯¹æƒ…ç»ªå¤æ‚æ€§åŠå…¶å®é™…åº”ç”¨çš„æŒæ¡ç¨‹åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd8953a670e1bd6271553979f9db5831" align="middle">
<img src="https://picx.zhimg.com/v2-7bc066546c269ec69c7b05f996f2e830" align="middle">
<img src="https://picx.zhimg.com/v2-9093b195770eee5e6b22cd46940dcf9b" align="middle">
<img src="https://picx.zhimg.com/v2-394ea0f16b404caae4985cfce5ee0615" align="middle">
<img src="https://picx.zhimg.com/v2-a62daa8f80c2faff6fd25d79d4eeb96d" align="middle">
<img src="https://picx.zhimg.com/v2-eaf1777cc16931543332505a8d001057" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DualSpeechLM-Towards-Unified-Speech-Understanding-and-Generation-via-Dual-Speech-Token-Modeling-with-Large-Language-Models"><a href="#DualSpeechLM-Towards-Unified-Speech-Understanding-and-Generation-via-Dual-Speech-Token-Modeling-with-Large-Language-Models" class="headerlink" title="DualSpeechLM: Towards Unified Speech Understanding and Generation via Dual Speech Token Modeling with Large Language Models"></a>DualSpeechLM: Towards Unified Speech Understanding and Generation via Dual Speech Token Modeling with Large Language Models</h2><p><strong>Authors:Yuanyuan Wang, Dongchao Yang, Yiwen Shao, Hangting Chen, Jiankun Zhao, Zhiyong Wu, Helen Meng, Xixin Wu</strong></p>
<p>Extending pre-trained text Large Language Models (LLMs)â€™s speech understanding or generation abilities by introducing various effective speech tokens has attracted great attention in the speech community. However, building a unified speech understanding and generation model still faces the following challenges: (1) Due to the huge modality gap between speech and text tokens, extending text LLMs to unified speech LLMs relies on large-scale paired data for fine-tuning, and (2) Generation and understanding tasks prefer information at different levels, e.g., generation benefits from detailed acoustic features, while understanding favors high-level semantics. This divergence leads to difficult performance optimization in one unified model. To solve these challenges, in this paper, we present two key insights in speech tokenization and speech language modeling. Specifically, we first propose an Understanding-driven Speech Tokenizer (USTokenizer), which extracts high-level semantic information essential for accomplishing understanding tasks using text LLMs. In this way, USToken enjoys better modality commonality with text, which reduces the difficulty of modality alignment in adapting text LLMs to speech LLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that concurrently models USToken as input and acoustic token as output within a unified, end-to-end framework, seamlessly integrating speech understanding and generation capabilities. Furthermore, we propose a novel semantic supervision loss and a Chain-of-Condition (CoC) strategy to stabilize model training and enhance speech generation performance. Experimental results demonstrate that our proposed approach effectively fosters a complementary relationship between understanding and generation tasks, highlighting the promising strategy of mutually enhancing both tasks in one unified model.</p>
<blockquote>
<p>å°†é¢„è®­ç»ƒçš„æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³ç†è§£å’Œç”Ÿæˆèƒ½åŠ›é€šè¿‡å¼•å…¥å„ç§æœ‰æ•ˆçš„è¯­éŸ³æ ‡è®°è¿›è¡Œæ‰©å±•ï¼Œåœ¨è¯­éŸ³é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œæ„å»ºç»Ÿä¸€çš„è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ¨¡å‹ä»ç„¶é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç”±äºè¯­éŸ³å’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´å­˜åœ¨å·¨å¤§çš„æ¨¡æ€å·®è·ï¼Œå°†æ–‡æœ¬LLMæ‰©å±•åˆ°ç»Ÿä¸€çš„è¯­éŸ³LLMä¾èµ–äºå¤§è§„æ¨¡é…å¯¹æ•°æ®è¿›è¡Œå¾®è°ƒï¼›ï¼ˆ2ï¼‰ç”Ÿæˆå’Œç†è§£ä»»åŠ¡åå¥½ä¸åŒå±‚çº§çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼Œç”Ÿæˆå—ç›Šäºè¯¦ç»†çš„å£°å­¦ç‰¹å¾ï¼Œè€Œç†è§£åˆ™æ›´å€¾å‘äºé«˜çº§è¯­ä¹‰ã€‚è¿™ç§åˆ†æ­§å¯¼è‡´åœ¨ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­ä¼˜åŒ–æ€§èƒ½å˜å¾—å›°éš¾ã€‚</p>
</blockquote>
<p>ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è¯­éŸ³æ ‡è®°åŒ–å’Œè¯­éŸ³è¯­è¨€æ¨¡å‹çš„ä¸¤ä¸ªå…³é”®è§è§£ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§ç†è§£é©±åŠ¨çš„è¯­éŸ³æ ‡è®°å™¨ï¼ˆUSTokenizerï¼‰ï¼Œå®ƒä½¿ç”¨æ–‡æœ¬LLMå®Œæˆç†è§£ä»»åŠ¡æ‰€å¿…éœ€çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒUSTokenä¸æ–‡æœ¬çš„æ¨¡æ€å…±æ€§æ›´å¥½ï¼Œé™ä½äº†å°†æ–‡æœ¬LLMé€‚åº”åˆ°è¯­éŸ³LLMæ—¶çš„æ¨¡æ€å¯¹é½éš¾åº¦ã€‚</p>
<p>å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†DualSpeechLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŒæ ‡è®°å»ºæ¨¡æ¡†æ¶ï¼Œå®ƒåœ¨ä¸€ä¸ªç»Ÿä¸€ã€ç«¯åˆ°ç«¯çš„æ¡†æ¶å†…ï¼ŒåŒæ—¶ä»¥USTokenä½œä¸ºè¾“å…¥ã€å£°å­¦æ ‡è®°ä½œä¸ºè¾“å‡ºè¿›è¡Œå»ºæ¨¡ï¼Œæ— ç¼é›†æˆäº†è¯­éŸ³ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯­ä¹‰ç›‘ç£æŸå¤±å’Œé“¾å¼æ¡ä»¶ï¼ˆCoCï¼‰ç­–ç•¥ï¼Œä»¥ç¨³å®šæ¨¡å‹è®­ç»ƒå¹¶æé«˜è¯­éŸ³ç”Ÿæˆæ€§èƒ½ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08961v3">PDF</a> Accepted by AAAI 2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©å±•é¢„è®­ç»ƒæ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¯­éŸ³ç†è§£æˆ–ç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡å¼•å…¥å„ç§æœ‰æ•ˆçš„è¯­éŸ³ä»¤ç‰Œï¼Œåœ¨è¯­éŸ³é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œæ„å»ºç»Ÿä¸€çš„è¯­éŸ³ç†è§£ä¸ç”Ÿæˆæ¨¡å‹ä»é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼šä¸€æ˜¯ç”±äºè¯­éŸ³å’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´å­˜åœ¨å·¨å¤§çš„æ¨¡æ€å·®è·ï¼Œå°†æ–‡æœ¬LLMæ‰©å±•åˆ°ç»Ÿä¸€è¯­éŸ³LLMä¾èµ–äºå¤§è§„æ¨¡é…å¯¹æ•°æ®è¿›è¡Œå¾®è°ƒï¼›äºŒæ˜¯ç”Ÿæˆå’Œç†è§£ä»»åŠ¡åå¥½ä¸åŒå±‚çº§çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ç”Ÿæˆå—ç›Šäºè¯¦ç»†çš„å£°å­¦ç‰¹å¾ï¼Œè€Œç†è§£åˆ™åå¥½é«˜çº§è¯­ä¹‰ã€‚è¿™å¯¼è‡´åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ä¼˜åŒ–æ€§èƒ½å˜å¾—å›°éš¾ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªå…³äºè¯­éŸ³ä»¤ç‰ŒåŒ–å’Œè¯­éŸ³è¯­è¨€å»ºæ¨¡çš„å…³é”®è§è§£ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç†è§£é©±åŠ¨çš„è¯­éŸ³åˆ†è¯å™¨ï¼ˆUSTokenizerï¼‰ï¼Œå…¶åˆ©ç”¨æ–‡æœ¬LLMså®Œæˆç†è§£ä»»åŠ¡æ—¶æå–çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯è‡³å…³é‡è¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒUSTokenä¸æ–‡æœ¬çš„æ¨¡æ€å…±æ€§æ›´å¥½ï¼Œé™ä½äº†å°†æ–‡æœ¬LLMsé€‚åº”åˆ°è¯­éŸ³LLMsæ—¶æ¨¡æ€å¯¹é½çš„éš¾åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†DualSpeechLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŒä»¤ç‰Œå»ºæ¨¡æ¡†æ¶ï¼Œå°†USTokenä½œä¸ºè¾“å…¥ã€å£°å­¦ä»¤ç‰Œä½œä¸ºè¾“å‡ºåœ¨ç»Ÿä¸€ç«¯åˆ°ç«¯æ¡†æ¶å†…å»ºæ¨¡ï¼Œæ— ç¼é›†æˆè¯­éŸ³ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¯­ä¹‰ç›‘ç£æŸå¤±å’Œé“¾å¼æ¡ä»¶ï¼ˆCoCï¼‰ç­–ç•¥ï¼Œä»¥ç¨³å®šæ¨¡å‹è®­ç»ƒå¹¶å¢å¼ºè¯­éŸ³ç”Ÿæˆæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æœ‰æ•ˆåœ°ä¿ƒè¿›äº†ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„äº’è¡¥å…³ç³»ï¼Œçªæ˜¾å‡ºåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ç›¸äº’å¢å¼ºä¸¤ä¸ªä»»åŠ¡çš„ç­–ç•¥å‰æ™¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³å’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´å­˜åœ¨å·¨å¤§çš„æ¨¡æ€å·®è·ï¼Œæ‰©å±•æ–‡æœ¬LLMsåˆ°ç»Ÿä¸€è¯­éŸ³LLMséœ€è¦å¤§è§„æ¨¡é…å¯¹æ•°æ®è¿›è¡Œå¾®è°ƒã€‚</li>
<li>ç”Ÿæˆå’Œç†è§£ä»»åŠ¡åå¥½ä¸åŒå±‚çº§çš„ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ä¼˜åŒ–æ€§èƒ½å˜å¾—å›°éš¾ã€‚</li>
<li>æå‡ºç†è§£é©±åŠ¨çš„è¯­éŸ³åˆ†è¯å™¨ï¼ˆUSTokenizerï¼‰ï¼Œæå–é«˜çº§è¯­ä¹‰ä¿¡æ¯ï¼Œé™ä½æ¨¡æ€å¯¹é½éš¾åº¦ã€‚</li>
<li>å¼•å…¥DualSpeechLMæ¡†æ¶ï¼Œç»Ÿä¸€å»ºæ¨¡è¯­éŸ³ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æå‡ºæ–°é¢–çš„è¯­ä¹‰ç›‘ç£æŸå¤±ï¼Œç¨³å®šæ¨¡å‹è®­ç»ƒï¼Œæé«˜è¯­éŸ³ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨é“¾å¼æ¡ä»¶ï¼ˆCoCï¼‰ç­–ç•¥ï¼Œæœ‰æ•ˆä¿ƒè¿›ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„äº’è¡¥å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2206b1455c63155755579852387ab85a" align="middle">
<img src="https://picx.zhimg.com/v2-a44d17111f663b828e788cde15e32827" align="middle">
<img src="https://picx.zhimg.com/v2-463362a4c36c7d814fbf850a76648737" align="middle">
<img src="https://picx.zhimg.com/v2-0727f96a0c8fd4a3a910cee56a9f51a0" align="middle">
<img src="https://picx.zhimg.com/v2-8f8fabd84dc1f35c5df7ddcec36c0b77" align="middle">
<img src="https://picx.zhimg.com/v2-587af9e8dec45d69ba126a400bf9f3e1" align="middle">
<img src="https://picx.zhimg.com/v2-7ae9ffca755b7d9b9dfa01ad198c7a06" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="READ-Real-time-and-Efficient-Asynchronous-Diffusion-for-Audio-driven-Talking-Head-Generation"><a href="#READ-Real-time-and-Efficient-Asynchronous-Diffusion-for-Audio-driven-Talking-Head-Generation" class="headerlink" title="READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation"></a>READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation</h2><p><strong>Authors:Haotian Wang, Yuzhe Weng, Jun Du, Haoran Xu, Xiaoyan Wu, Shan He, Bing Yin, Cong Liu, Jianqing Gao, Qingfeng Liu</strong></p>
<p>The introduction of diffusion models has brought significant advances to the field of audio-driven talking head generation. However, the extremely slow inference speed severely limits the practical implementation of diffusion-based talking head generation models. In this study, we propose READ, a real-time diffusion-transformer-based talking head generation framework. Our approach first learns a spatiotemporal highly compressed video latent space via a temporal VAE, significantly reducing the token count to accelerate generation. To achieve better audio-visual alignment within this compressed latent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to generate temporally compressed speech latent codes corresponding to the video latent space. These latent representations are then modeled by a carefully designed Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient talking head synthesis. Furthermore, to ensure temporal consistency and accelerated inference in extended generation, we propose a novel asynchronous noise scheduler (ANS) for both the training and inference processes of our framework. The ANS leverages asynchronous add-noise and asynchronous motion-guided generation in the latent space, ensuring consistency in generated video clips. Experimental results demonstrate that READ outperforms state-of-the-art methods by generating competitive talking head videos with significantly reduced runtime, achieving an optimal balance between quality and speed while maintaining robust metric stability in long-time generation.</p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å¼•å…¥ä¸ºéŸ³é¢‘é©±åŠ¨çš„å¤´åƒç”Ÿæˆé¢†åŸŸå¸¦æ¥äº†é‡å¤§çªç ´ã€‚ç„¶è€Œï¼Œææ…¢çš„æ¨ç†é€Ÿåº¦ä¸¥é‡é™åˆ¶äº†åŸºäºæ‰©æ•£çš„å¤´åƒç”Ÿæˆæ¨¡å‹çš„å®é™…åº”ç”¨ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†READï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå®æ—¶æ‰©æ•£å˜æ¢å™¨çš„å¤´åƒç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡æ—¶é—´VAEå­¦ä¹ ä¸€ä¸ªæ—¶ç©ºé«˜åº¦å‹ç¼©çš„è§†é¢‘æ½œåœ¨ç©ºé—´ï¼Œæ˜¾è‘—å‡å°‘ä»¤ç‰Œè®¡æ•°ä»¥åŠ é€Ÿç”Ÿæˆã€‚ä¸ºäº†åœ¨è¿™ä¸ªå‹ç¼©çš„æ½œåœ¨ç©ºé—´å†…å®ç°æ›´å¥½çš„éŸ³é¢‘è§†é¢‘å¯¹é½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­éŸ³è‡ªç¼–ç å™¨ï¼ˆSpeechAEï¼‰æ¥ç”Ÿæˆä¸è§†é¢‘æ½œåœ¨ç©ºé—´ç›¸å¯¹åº”çš„æ—¶ç©ºå‹ç¼©è¯­éŸ³æ½œåœ¨ä»£ç ã€‚ç„¶åï¼Œè¿™äº›æ½œåœ¨è¡¨ç¤ºç”±ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„éŸ³é¢‘åˆ°è§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼ˆA2V-DiTï¼‰ä¸»å¹²è¿›è¡Œå»ºæ¨¡ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¤´åƒåˆæˆã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿æ‰©å±•ç”Ÿæˆçš„æ—¶åºä¸€è‡´æ€§å’ŒåŠ é€Ÿæ¨ç†ï¼Œæˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„æ¡†æ¶çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹æå‡ºäº†æ–°å‹å¼‚æ­¥å™ªå£°è°ƒåº¦å™¨ï¼ˆANSï¼‰ã€‚ANSåˆ©ç”¨æ½œåœ¨ç©ºé—´ä¸­çš„å¼‚æ­¥æ·»åŠ å™ªå£°å’Œå¼‚æ­¥è¿åŠ¨å¼•å¯¼ç”Ÿæˆï¼Œç¡®ä¿ç”Ÿæˆè§†é¢‘ç‰‡æ®µçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒREADä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç”Ÿæˆçš„å¤´åƒè§†é¢‘å…·æœ‰æ˜¾è‘—å‡å°‘çš„è¿è¡Œæ—¶é—´ï¼Œåœ¨è´¨é‡å’Œé€Ÿåº¦ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼ŒåŒæ—¶åœ¨é•¿æ—¶é—´ç”Ÿæˆä¸­ä¿æŒäº†ç¨³å¥çš„åº¦é‡ç¨³å®šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03457v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://readportrait.github.io/READ/">https://readportrait.github.io/READ/</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘é©±åŠ¨çš„å¤´ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ¨ç†é€Ÿåº¦ææ…¢ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ‰©æ•£è½¬æ¢å™¨çš„å®æ—¶å¤´ç”Ÿæˆæ¡†æ¶READã€‚é€šè¿‡æ—¶ç©ºé™ç»´è§†é¢‘æ½œåœ¨ç©ºé—´å­¦ä¹ å’Œé¢„è®­ç»ƒè¯­éŸ³è‡ªç¼–ç å™¨ï¼Œå®ç°éŸ³é¢‘ä¸è§†é¢‘çš„æ›´å¥½å¯¹é½ã€‚è®¾è®¡ç²¾å·§çš„éŸ³é¢‘åˆ°è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ä¿è¯äº†é«˜æ•ˆå¤´åˆæˆã€‚æ­¤å¤–ï¼Œä¸ºç¡®ä¿é•¿æœŸç”Ÿæˆçš„æ—¶åºä¸€è‡´æ€§å’ŒåŠ é€Ÿæ¨ç†ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°å‹å¼‚æ­¥å™ªå£°è°ƒåº¦å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒREADåœ¨ç”Ÿæˆå…·æœ‰ç«äº‰åŠ›çš„å¤´è§†é¢‘æ—¶ï¼Œè¿è¡Œæ—¶æ˜¾è‘—å‡å°‘ï¼Œå®ç°äº†è´¨é‡ä¸é€Ÿåº¦çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘é©±åŠ¨çš„å¤´ç”Ÿæˆé¢†åŸŸæœ‰é‡å¤§è¿›å±•ï¼Œä½†æ¨ç†é€Ÿåº¦æ…¢ã€‚</li>
<li>READæ¡†æ¶åŸºäºæ‰©æ•£è½¬æ¢å™¨å®ç°å®æ—¶å¤´ç”Ÿæˆã€‚</li>
<li>é€šè¿‡æ—¶ç©ºé™ç»´è§†é¢‘æ½œåœ¨ç©ºé—´å­¦ä¹ æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>é¢„è®­ç»ƒè¯­éŸ³è‡ªç¼–ç å™¨å®ç°éŸ³é¢‘ä¸è§†é¢‘çš„æ›´å¥½å¯¹é½ã€‚</li>
<li>ç²¾å¿ƒè®¾è®¡çš„éŸ³é¢‘åˆ°è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ä¿è¯é«˜æ•ˆå¤´åˆæˆã€‚</li>
<li>å¼‚æ­¥å™ªå£°è°ƒåº¦å™¨ç¡®ä¿é•¿æœŸç”Ÿæˆçš„æ—¶åºä¸€è‡´æ€§å’ŒåŠ é€Ÿæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a532b8468daf339f42803efdd68b7c0" align="middle">
<img src="https://picx.zhimg.com/v2-731d5dbc10c5ac9d73a7726d8ed3ebf0" align="middle">
<img src="https://picx.zhimg.com/v2-7eae0d8a1445ea860f62593723a3237a" align="middle">
<img src="https://picx.zhimg.com/v2-79207cfd1edc5c217efa7f43aa7abdc9" align="middle">
<img src="https://picx.zhimg.com/v2-ecdb31a262bf872fb8ee3cfb127a4685" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-19/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b2103e0612f304f8fda470659851e2bf" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  UniGen-1.5 Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4dd000714d18daebbe4a74e4beda29ea" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Skeletons Speak Louder than Text A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
