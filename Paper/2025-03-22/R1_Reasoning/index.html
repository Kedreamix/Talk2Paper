<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  Chain of Functions A Programmatic Pipeline for Fine-Grained Chart   Reasoning Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7f1dde42d0ff60bdb5033de8d4720b30.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-22-æ›´æ–°"><a href="#2025-03-22-æ›´æ–°" class="headerlink" title="2025-03-22 æ›´æ–°"></a>2025-03-22 æ›´æ–°</h1><h2 id="Chain-of-Functions-A-Programmatic-Pipeline-for-Fine-Grained-Chart-Reasoning-Data"><a href="#Chain-of-Functions-A-Programmatic-Pipeline-for-Fine-Grained-Chart-Reasoning-Data" class="headerlink" title="Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart   Reasoning Data"></a>Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart   Reasoning Data</h2><p><strong>Authors:Zijian Li, Jingjing Fu, Lei Song, Jiang Bian, Jun Zhang, Rui Wang</strong></p>
<p>Visual reasoning is crucial for multimodal large language models (MLLMs) to address complex chart queries, yet high-quality rationale data remains scarce. Existing methods leveraged (M)LLMs for data generation, but direct prompting often yields limited precision and diversity. In this paper, we propose \textit{Chain of Functions (CoF)}, a novel programmatic reasoning data generation pipeline that utilizes freely-explored reasoning paths as supervision to ensure data precision and diversity. Specifically, it starts with human-free exploration among the atomic functions (e.g., maximum data and arithmetic operations) to generate diverse function chains, which are then translated into linguistic rationales and questions with only a moderate open-sourced LLM. \textit{CoF} provides multiple benefits: 1) Precision: function-governed generation reduces hallucinations compared to freeform generation; 2) Diversity: enumerating function chains enables varied question taxonomies; 3) Explainability: function chains serve as built-in rationales, allowing fine-grained evaluation beyond overall accuracy; 4) Practicality: eliminating reliance on extremely large models. Employing \textit{CoF}, we construct the \textit{ChartCoF} dataset, with 1.4k complex reasoning Q&amp;A for fine-grained analysis and 50k Q&amp;A for reasoning enhancement. The fine-grained evaluation on \textit{ChartCoF} reveals varying performance across question taxonomies for each MLLM, and the experiments also show that finetuning with \textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs on widely used benchmarks. Furthermore, the novel paradigm of function-governed rationale generation in \textit{CoF} could inspire broader applications beyond charts. </p>
<blockquote>
<p>è§†è§‰æ¨ç†å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¤„ç†å¤æ‚å›¾è¡¨æŸ¥è¯¢è‡³å…³é‡è¦ï¼Œä½†é«˜è´¨é‡çš„ç†ç”±æ•°æ®ä»ç„¶ç¨€ç¼ºã€‚ç°æœ‰æ–¹æ³•åˆ©ç”¨ï¼ˆMï¼‰LLMsè¿›è¡Œæ•°æ®ç”Ÿæˆï¼Œä½†ç›´æ¥æç¤ºå¾€å¾€ä¼šå¯¼è‡´ç²¾åº¦å’Œå¤šæ ·æ€§æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†\textit{åŠŸèƒ½é“¾ï¼ˆCoFï¼‰}ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç¨‹åºåŒ–æ¨ç†æ•°æ®ç”Ÿæˆç®¡é“ï¼Œå®ƒåˆ©ç”¨è‡ªç”±æ¢ç´¢çš„æ¨ç†è·¯å¾„ä½œä¸ºç›‘ç£ï¼Œä»¥ç¡®ä¿æ•°æ®çš„ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé¦–å…ˆä»åŸå­åŠŸèƒ½ï¼ˆå¦‚æœ€å¤§æ•°æ®å’Œç®—æœ¯è¿ç®—ï¼‰ä¹‹é—´è¿›è¡Œæ— äººå·¥æ¢ç´¢ï¼Œä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„åŠŸèƒ½é“¾ï¼Œç„¶åä»…ä½¿ç”¨é€‚åº¦çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹å°†å…¶ç¿»è¯‘æˆè¯­è¨€ç†ç”±å’Œé—®é¢˜ã€‚CoFå…·æœ‰å¤šä¸ªä¼˜ç‚¹ï¼š1ï¼‰ç²¾åº¦ï¼šåŠŸèƒ½æ§åˆ¶ç”Ÿæˆå‡å°‘äº†ä¸è‡ªç”±å½¢å¼ç”Ÿæˆç›¸æ¯”çš„å¹»è§‰ï¼›2ï¼‰å¤šæ ·æ€§ï¼šæšä¸¾åŠŸèƒ½é“¾å¯å®ç°å˜åŒ–çš„é—®é¢˜åˆ†ç±»ï¼›3ï¼‰å¯è§£é‡Šæ€§ï¼šåŠŸèƒ½é“¾ä½œä¸ºå†…ç½®çš„ç†ç”±ï¼Œå…è®¸è¶…è¶Šæ€»ä½“å‡†ç¡®åº¦çš„ç²¾ç»†è¯„ä¼°ï¼›4ï¼‰å®ç”¨æ€§ï¼šæ¶ˆé™¤å¯¹è¶…å¤§æ¨¡å‹çš„ä¾èµ–ã€‚é€šè¿‡ä½¿ç”¨CoFï¼Œæˆ‘ä»¬æ„å»ºäº†ChartCoFæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«1400ä¸ªç”¨äºç²¾ç»†åˆ†æå¤æ‚æ¨ç†çš„é—®ç­”å’Œ5ä¸‡ä¸ªç”¨äºå¢å¼ºæ¨ç†çš„é—®ç­”ã€‚åœ¨ChartCoFä¸Šçš„ç²¾ç»†è¯„ä¼°æ˜¾ç¤ºï¼Œæ¯ä¸ªMLLMåœ¨ä¸åŒé—®é¢˜åˆ†ç±»ä¸­çš„æ€§èƒ½å„ä¸ç›¸åŒï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œä½¿ç”¨ChartCoFè¿›è¡Œå¾®è°ƒåœ¨åŒè§„æ¨¡MLLMsä¸­å®ç°äº†å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•çš„æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCoFä¸­çš„åŠŸèƒ½æ§åˆ¶ç†ç”±ç”Ÿæˆæ–°èŒƒå¼å¯èƒ½ä¼šæ¿€å‘å›¾è¡¨ä¹‹å¤–æ›´å¹¿æ³›çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16260v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºChain of Functionsï¼ˆCoFï¼‰çš„æ–°å‹ç¨‹åºåŒ–æ¨ç†æ•°æ®ç”Ÿæˆç®¡é“ï¼Œç”¨äºè§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚å›¾è¡¨æŸ¥è¯¢æ—¶çš„éš¾é¢˜ã€‚CoFåˆ©ç”¨äººç±»æ— éœ€å‚ä¸çš„åŸå­åŠŸèƒ½è‡ªç”±æ¢ç´¢æ¥ç”Ÿæˆå¤šæ ·çš„åŠŸèƒ½é“¾ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè¯­è¨€ç†æ€§ä¸é—®é¢˜ã€‚CoFå…·æœ‰ç²¾ç¡®æ€§ã€å¤šæ ·æ€§ã€å¯è§£é‡Šæ€§å’Œå®ç”¨æ€§ç­‰ä¼˜ç‚¹ï¼Œå¯åº”ç”¨äºæ„å»ºå›¾è¡¨æ¨ç†æ•°æ®é›†ChartCoFã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ¨ç†åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å¤„ç†å¤æ‚å›¾è¡¨æŸ¥è¯¢æ—¶è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åˆ©ç”¨MLLMsè¿›è¡Œæ•°æ®ç”Ÿæˆï¼Œä½†ç›´æ¥æç¤ºçš„ç²¾åº¦å’Œå¤šæ ·æ€§æœ‰é™ã€‚</li>
<li>æå‡ºçš„Chain of Functionsï¼ˆCoFï¼‰æ˜¯ä¸€ç§æ–°å‹çš„ç¨‹åºåŒ–æ¨ç†æ•°æ®ç”Ÿæˆç®¡é“ï¼Œåˆ©ç”¨è‡ªç”±æ¢ç´¢çš„æ¨ç†è·¯å¾„ä½œä¸ºç›‘ç£ï¼Œç¡®ä¿æ•°æ®çš„ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚</li>
<li>CoFä»åŸå­åŠŸèƒ½ï¼ˆå¦‚æœ€å¤§æ•°æ®å’Œç®—æœ¯è¿ç®—ï¼‰å¼€å§‹è‡ªç”±æ¢ç´¢ï¼Œç”Ÿæˆå¤šæ ·çš„åŠŸèƒ½é“¾ï¼Œå¹¶è½¬åŒ–ä¸ºè¯­è¨€ç†æ€§ä¸é—®é¢˜ã€‚</li>
<li>CoFæä¾›äº†ç²¾ç¡®åº¦ã€å¤šæ ·æ€§ã€å¯è§£é‡Šæ€§å’Œå®ç”¨æ€§ç­‰å¥½å¤„ã€‚</li>
<li>ä½¿ç”¨CoFæ„å»ºäº†ChartCoFæ•°æ®é›†ï¼ŒåŒ…å«1.4kå¤æ‚æ¨ç†é—®ç­”ç”¨äºç²¾ç»†åˆ†æï¼Œä»¥åŠ50ké—®ç­”ç”¨äºæ¨ç†æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7a92e8e04e1d7d18d7d0af3b3a398ef1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b827dddf1bb96f622db5b1db32cf0377.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-384322e30e7db246065cdaa7fd403373.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Fin-R1-A-Large-Language-Model-for-Financial-Reasoning-through-Reinforcement-Learning"><a href="#Fin-R1-A-Large-Language-Model-for-Financial-Reasoning-through-Reinforcement-Learning" class="headerlink" title="Fin-R1: A Large Language Model for Financial Reasoning through   Reinforcement Learning"></a>Fin-R1: A Large Language Model for Financial Reasoning through   Reinforcement Learning</h2><p><strong>Authors:Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu, Dezhi Chen, Yun Chen, Zuo Bai, Liwen Zhang</strong></p>
<p>Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SUFE-AIFLM-Lab/Fin-R1">https://github.com/SUFE-AIFLM-Lab/Fin-R1</a>. </p>
<blockquote>
<p>æ¨ç†å¤§è¯­è¨€æ¨¡å‹æ­£åœ¨å„ä¸ªé¢†åŸŸè¿…é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„é‡‘èä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ä»éœ€è¦è¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸“é—¨ä¸ºé‡‘èé¢†åŸŸè®¾è®¡çš„æ¨ç†å¤§è¯­è¨€æ¨¡å‹Fin-R1ã€‚Fin-R1é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„æ„å»ºï¼Œåˆ©ç”¨åŸºäºDeepSeek-R1æç‚¼å’Œå¤„ç†çš„é‡‘èæ¨ç†æ•°æ®é›†ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œå®ƒåœ¨å„ç§é‡‘èæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æ¥è¿‘å‚æ•°è§„æ¨¡ä¸º7äº¿çš„DeepSeek-R1ã€‚åœ¨æˆ‘ä»¬çš„è¯„ä¼°ä¸­ï¼Œå®ƒåœ¨FinQAå’ŒConvFinQAä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œåœ¨å…¶ä»–ä»»åŠ¡ä¸­ä¹Ÿè¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ã€‚Fin-R1å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼Œä¸ºè§£å†³é‡‘èé¢†åŸŸé‡åˆ°çš„é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/SUFE-AIFLM-Lab/Fin-R">https://github.com/SUFE-AIFLM-Lab/Fin-R</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16252v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸè¿…é€Ÿæ¼”å˜ï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„é‡‘èä»»åŠ¡æ–¹é¢ä»éœ€æ·±å…¥ç ”ç©¶ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸“ä¸ºé‡‘èé¢†åŸŸè®¾è®¡çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹Fin-R1ã€‚Fin-R1é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼ŒåŸºäºDeepSeek-R1è’¸é¦å’Œå¤„ç†é‡‘èæ¨ç†æ•°æ®é›†ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œå®ƒåœ¨é‡‘èæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ¥è¿‘DeepSeek-R1çš„æ€§èƒ½ï¼Œå‚æ•°è§„æ¨¡ä¸º7äº¿ã€‚åœ¨FinQAå’ŒConvFinQAä»»åŠ¡ä¸­ï¼Œå®ƒåœ¨æˆ‘ä»¬è¯„ä¼°çš„LLMsä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨å…¶ä»–ä»»åŠ¡ä¸­ä¹Ÿè¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ã€‚Fin-R1å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼Œä¸ºè§£å†³é‡‘èé¢†åŸŸé‡åˆ°çš„é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èé¢†åŸŸçš„æ¨ç†èƒ½åŠ›å°šå¾…æ·±å…¥ç ”ç©¶ã€‚</li>
<li>Fin-R1æ˜¯ä¸€ä¸ªä¸“ä¸ºé‡‘èé¢†åŸŸè®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>Fin-R1é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œåˆ©ç”¨é‡‘èæ¨ç†æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼ŒFin-R1è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>Fin-R1åœ¨å¤šä¸ªé‡‘èæ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
<li>Fin-R1å±•ç°äº†å¼ºå¤§çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cad24304b10036555df886ff328075e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5a951f4d8bab88b76dbec5a96e120ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e5e55c6d5bba585a2cc414fc9221967.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73bbd9fc11a29ecd1b3c418afe8df550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a42eb5b35e2c97b5018b6656e925d774.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-for-Reasoning-in-Small-LLMs-What-Works-and-What-Doesnâ€™t"><a href="#Reinforcement-Learning-for-Reasoning-in-Small-LLMs-What-Works-and-What-Doesnâ€™t" class="headerlink" title="Reinforcement Learning for Reasoning in Small LLMs: What Works and What   Doesnâ€™t"></a>Reinforcement Learning for Reasoning in Small LLMs: What Works and What   Doesnâ€™t</h2><p><strong>Authors:Quy-Anh Dang, Chris Ngo</strong></p>
<p>Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 7,000 samples and a $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering a cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying a foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at <a target="_blank" rel="noopener" href="https://github.com/knoveleng/open-rs">https://github.com/knoveleng/open-rs</a>. </p>
<blockquote>
<p>å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›é€šå¸¸ä¾èµ–äºå¤§é‡çš„è®¡ç®—èµ„æºå’Œåºå¤§çš„æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†èµ„æºå—é™ç¯å¢ƒçš„å¯è®¿é—®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å°å‹LLMæ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ï¼Œé‡ç‚¹æ˜¯ä¸€ä¸ªæ‹¥æœ‰1.5äº¿å‚æ•°çš„æ¨¡å‹â€”â€”DeepSeek-R1-Distill-Qwen-1.5Bï¼Œåœ¨ä¸¥æ ¼çš„çº¦æŸæ¡ä»¶ä¸‹ï¼šåœ¨4ä¸ªNVIDIA A40 GPUï¼ˆæ¯ä¸ªå…·æœ‰48GB VRAMï¼‰ä¸Šè¿›è¡Œä¸ºæœŸ24å°æ—¶çš„åŸ¹è®­ã€‚é€šè¿‡é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¹¶æ•´ç†ä¸€ä¸ªç´§å‡‘ã€é«˜è´¨é‡çš„æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸‰é¡¹å®éªŒæ¥æ¢ç´¢æ¨¡å‹çš„è¡Œä¸ºå’Œæ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¨ç†èƒ½åŠ›è¿…é€Ÿæé«˜â€”â€”ä¾‹å¦‚ï¼ŒAMC23çš„å‡†ç¡®æ€§ä»63%æé«˜åˆ°80%ï¼ŒAIME24è¾¾åˆ°46.7%ï¼Œè¶…è¿‡äº†o1-previewâ€”â€”è¿™åªéœ€ä½¿ç”¨7000ä¸ªæ ·æœ¬å’Œ42ç¾å…ƒçš„åŸ¹è®­æˆæœ¬ï¼Œè€ŒåŸºçº¿æ¨¡å‹çš„åŸ¹è®­æˆæœ¬åˆ™é«˜è¾¾æ•°åƒç¾å…ƒã€‚ç„¶è€Œï¼Œéšç€è®­ç»ƒçš„å»¶é•¿ï¼Œä¹Ÿå‡ºç°äº†ä¼˜åŒ–ä¸ç¨³å®šå’Œé•¿åº¦çº¦æŸç­‰æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°çªæ˜¾äº†åŸºäºRLçš„å¾®è°ƒå¯¹äºå°å‹LLMçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤§è§„æ¨¡æ–¹æ³•æä¾›äº†æˆæœ¬æ•ˆç›Šæ›´é«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬å·²å°†ä»£ç å’Œæ•°æ®é›†ä½œä¸ºå¼€æºèµ„æºå‘å¸ƒï¼Œæä¾›äº†æœ‰å…³æƒè¡¡çš„è§è§£ï¼Œå¹¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸­å¯æ‰©å±•çš„ã€å…·å¤‡æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚æ‰€æœ‰èµ„æºå‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/knoveleng/open-rs">https://github.com/knoveleng/open-rs</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16219v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜å°å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ç ”ç©¶ã€‚åœ¨æœ‰é™çš„è®¡ç®—èµ„æºå’Œæ—¶é—´å†…ï¼Œé€šè¿‡å¯¹å°å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒRLä¼˜åŒ–è®­ç»ƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚è¿™ä¸€æ–¹æ³•çš„æˆæœ¬è¾ƒä½ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­å¯æ‰©å±•çš„æ¨ç†å‹LLMæä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯ç”¨äºæé«˜å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¯¹å°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¼˜åŒ–è®­ç»ƒæ˜¯å¯è¡Œçš„ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æˆæœ¬è¾ƒä½ï¼Œç›¸æ¯”åŸºçº¿æ¨¡å‹å¯å¤§å¹…é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>ç ”ç©¶ä¸­é¢ä¸´äº†ä¼˜åŒ–ä¸ç¨³å®šå’Œé•¿åº¦çº¦æŸç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶ç»“æœä»¥å¼€æ”¾æºä»£ç çš„å½¢å¼å‘å¸ƒï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d1ae0476b780bfb0435db7f373bd23f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7cd9168b132518717c956deba36f6d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8d9f4786043b572259ce59c1134a7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f1adcaa6fb47e8e6375121aaa290ce1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CLS-RL-Image-Classification-with-Rule-Based-Reinforcement-Learning"><a href="#CLS-RL-Image-Classification-with-Rule-Based-Reinforcement-Learning" class="headerlink" title="CLS-RL: Image Classification with Rule-Based Reinforcement Learning"></a>CLS-RL: Image Classification with Rule-Based Reinforcement Learning</h2><p><strong>Authors:Ming Li, Shitian Zhao, Jike Zhong, Yuxiang Lai, Kaipeng Zhang</strong></p>
<p>Classification is a core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to SOTA classification models. However, acquiring large-scale labeled data is expensive. In this paper, we explore few-shot MLLM classification fine-tuning. We found that SFT can cause severe overfitting issues and may even degrade performance over the zero-shot approach. To address this challenge, inspired by the recent successes in rule-based reinforcement learning, we propose CLS-RL, which uses verifiable signals as reward to fine-tune MLLMs. We discovered that CLS-RL outperforms SFT in most datasets and has a much higher average accuracy on both base-to-new and few-shot learning setting. Moreover, we observed a free-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular dataset, their performance on other distinct datasets may also improve over zero-shot models, even if those datasets differ in distribution and class names. This suggests that RL-based methods effectively teach models the fundamentals of classification. Lastly, inspired by recent works in inference time thinking, we re-examine the &#96;thinking processâ€™ during fine-tuning, a critical aspect of RL-based methods, in the context of visual classification. We question whether such tasks require extensive thinking process during fine-tuning, proposing that this may actually detract from performance. Based on this premise, we introduce the No-Thinking-CLS-RL method, which minimizes thinking processes during training by setting an equality accuracy reward. Our findings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL method achieves superior in-domain performance and generalization capabilities than CLS-RL. </p>
<blockquote>
<p>åˆ†ç±»æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€åˆåœ¨å›¾åƒåˆ†ç±»æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä½†é€šè¿‡è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜å®ƒä»¬çš„æ€§èƒ½ï¼Œä½¿å…¶ä¸å½“å‰æœ€ä½³ï¼ˆSOTAï¼‰åˆ†ç±»æ¨¡å‹ç›¸åª²ç¾ã€‚ç„¶è€Œï¼Œè·å–å¤§è§„æ¨¡æ ‡è®°æ•°æ®æˆæœ¬é«˜æ˜‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†MLLMçš„å°‘é‡æ ·æœ¬åˆ†ç±»å¾®è°ƒã€‚æˆ‘ä»¬å‘ç°ï¼ŒSFTå¯èƒ½å¯¼è‡´ä¸¥é‡çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œç”šè‡³å¯èƒ½ä½¿é›¶æ ·æœ¬æ–¹æ³•çš„è¡¨ç°ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å—åˆ°åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æœ€æ–°æˆåŠŸçš„å¯å‘ï¼Œæå‡ºäº†CLS-RLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¯éªŒè¯çš„ä¿¡å·ä½œä¸ºå¥–åŠ±æ¥å¾®è°ƒMLLMsã€‚æˆ‘ä»¬å‘ç°CLS-RLåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºSFTï¼Œå¹¶ä¸”åœ¨åŸºç¡€åˆ°æ–°å’Œå°‘é‡å­¦ä¹ è®¾ç½®ä¸­å¹³å‡ç²¾åº¦æ›´é«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°CLS-RLçš„å…è´¹åˆé¤ç°è±¡ï¼›å½“æ¨¡å‹åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œå®ƒä»¬åœ¨å…¶ä»–ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½å¯èƒ½ä¼šè¶…è¿‡é›¶æ ·æœ¬æ¨¡å‹ï¼Œå³ä½¿è¿™äº›æ•°æ®é›†åœ¨åˆ†å¸ƒå’Œç±»åˆ«åç§°ä¸Šæœ‰æ‰€ä¸åŒã€‚è¿™è¡¨æ˜åŸºäºRLçš„æ–¹æ³•æœ‰æ•ˆåœ°æ•™æˆäº†æ¨¡å‹åˆ†ç±»çš„åŸºæœ¬åŸç†ã€‚æœ€åï¼Œå—åˆ°æœ€æ–°æ¨ç†æ—¶é—´æ€è€ƒç ”ç©¶å·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†è§†è§‰åˆ†ç±»çš„å¾®è°ƒè¿‡ç¨‹ä¸­çš„â€œæ€è€ƒè¿‡ç¨‹â€ï¼Œè¿™æ˜¯åŸºäºRLçš„æ–¹æ³•çš„ä¸€ä¸ªå…³é”®æ–¹é¢ã€‚æˆ‘ä»¬è´¨ç–‘è¿™ç§ä»»åŠ¡åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ˜¯å¦éœ€è¦å¤§é‡çš„æ€è€ƒè¿‡ç¨‹ï¼Œå¹¶æå‡ºè¿™å¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚åŸºäºè¿™ä¸€å‰æï¼Œæˆ‘ä»¬å¼•å…¥äº†æ— æ€è€ƒCLS-RLæ–¹æ³•ï¼Œé€šè¿‡è®¾å®šç²¾ç¡®å¥–åŠ±æ¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€è€ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å¤§å¤§ç¼©çŸ­å¾®è°ƒæ—¶é—´çš„æƒ…å†µä¸‹ï¼Œæ— æ€è€ƒCLS-RLæ–¹æ³•åœ¨é¢†åŸŸå†…çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢ä¼˜äºCLS-RLæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16188v1">PDF</a> Preprint, work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åˆ†ç±»å¾®è°ƒæ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨å¯éªŒè¯ä¿¡å·ä½œä¸ºå¥–åŠ±çš„CLS-RLåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šä¼˜äºæ ‡å‡†å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¹¶åœ¨åŸºç¡€åˆ°æ–°çš„æ•°æ®é›†å’Œå°æ ·æœ¬å­¦ä¹ è®¾ç½®ä¸­å…·æœ‰æ›´é«˜çš„å¹³å‡å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè§‚å¯Ÿåˆ°ä¸€ç§å…è´¹åˆé¤ç°è±¡ï¼šå½“æ¨¡å‹åœ¨æŸä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œå®ƒä»¬åœ¨å…¶å®ƒä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½å¯èƒ½ä¼šå¾—åˆ°æå‡ã€‚æœ¬æ–‡è¿˜å¼•å…¥äº†æ— æ€è€ƒè¿‡ç¨‹çš„CLS-RLæ–¹æ³•ï¼Œé€šè¿‡å‡å°‘è®­ç»ƒæ—¶çš„æ€è€ƒè¿‡ç¨‹æ¥æé«˜æ€§èƒ½ã€‚æ€»ç»“æ¥è¯´ï¼Œæœ¬æ–‡æå‡ºäº†æ–°çš„åŸºäºRLçš„æ–¹æ³•ä¼˜åŒ–äº†MLLMçš„åˆ†ç±»æ€§èƒ½å¹¶å¢å¼ºäº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç»è¿‡é€‚å½“çš„å¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚</li>
<li>æ ‡å‡†å¾®è°ƒï¼ˆSFTï¼‰å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆé—®é¢˜å¹¶å¯èƒ½é™ä½æ€§èƒ½ï¼Œç›¸æ¯”ä¹‹ä¸‹CLS-RLæ–¹æ³•æ›´æœ‰æ•ˆã€‚</li>
<li>CLS-RLåœ¨å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨åŸºç¡€åˆ°æ–°çš„æ•°æ®é›†å’Œå°æ ·æœ¬å­¦ä¹ è®¾ç½®ä¸­å…·æœ‰æ›´é«˜çš„å¹³å‡å‡†ç¡®æ€§ã€‚</li>
<li>RLæ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¡¨ç°åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½æå‡ã€‚</li>
<li>RLä¸­çš„â€œæ€è€ƒè¿‡ç¨‹â€åœ¨å¾®è°ƒä¸­å¯èƒ½é™ä½æ€§èƒ½ï¼Œå› æ­¤æå‡ºäº†æ— æ€è€ƒè¿‡ç¨‹çš„CLS-RLæ–¹æ³•ã€‚</li>
<li>æ— æ€è€ƒCLS-RLæ–¹æ³•åœ¨å‡å°‘è®­ç»ƒæ—¶é—´çš„åŒæ—¶å®ç°äº†ä¼˜å¼‚çš„é¢†åŸŸå†…éƒ¨æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡çš„ç ”ç©¶ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–MLLMåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f1dde42d0ff60bdb5033de8d4720b30.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8eed20ea936a8d8fc9f759bf58f42279.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89a4d440fafc91f50c84e976bf0efb35.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GraspCoT-Integrating-Physical-Property-Reasoning-for-6-DoF-Grasping-under-Flexible-Language-Instructions"><a href="#GraspCoT-Integrating-Physical-Property-Reasoning-for-6-DoF-Grasping-under-Flexible-Language-Instructions" class="headerlink" title="GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping   under Flexible Language Instructions"></a>GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping   under Flexible Language Instructions</h2><p><strong>Authors:Xiaomeng Chu, Jiajun Deng, Guoliang You, Wei Liu, Xingchen Li, Jianmin Ji, Yanyong Zhang</strong></p>
<p>Flexible instruction-guided 6-DoF grasping is a significant yet challenging task for real-world robotic systems. Existing methods utilize the contextual understanding capabilities of the large language models (LLMs) to establish mappings between expressions and targets, allowing robots to comprehend usersâ€™ intentions in the instructions. However, the LLMâ€™s knowledge about objectsâ€™ physical properties remains underexplored despite its tight relevance to grasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework that integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to physical properties, guided by auxiliary question-answering (QA) tasks. Particularly, we design a set of QA templates to enable hierarchical reasoning that includes three stages: target parsing, physical property analysis, and grasp action selection. Moreover, GraspCoT presents a unified multimodal LLM architecture, which encodes multi-view observations of 3D scenes into 3D-aware visual tokens, and then jointly embeds these visual tokens with CoT-derived textual tokens within LLMs to generate grasp pose predictions. Furthermore, we present IntentGrasp, a large-scale benchmark that fills the gap in public datasets for multi-object grasp detection under diverse and indirect verbal commands. Extensive experiments on IntentGrasp demonstrate the superiority of our method, with additional validation in real-world robotic applications confirming its practicality. Codes and data will be released. </p>
<blockquote>
<p>çµæ´»æŒ‡ä»¤å¼•å¯¼çš„6è‡ªç”±åº¦æŠ“å–å¯¹ç°å®ä¸–ç•Œçš„æœºå™¨äººç³»ç»Ÿæ¥è¯´æ˜¯ä¸€ä¸ªé‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›æ¥å»ºç«‹è¡¨è¾¾å¼å’Œç›®æ ‡ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤ä¸­çš„ç”¨æˆ·æ„å›¾ã€‚ç„¶è€Œï¼Œå°½ç®¡ä¸æŠ“å–ç´§å¯†ç›¸å…³ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯¹ç‰©ä½“çš„ç‰©ç†å±æ€§çš„äº†è§£ä»ç„¶è¢«å¿½è§†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GraspCoTï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘ç‰©ç†å±æ€§çš„6è‡ªç”±åº¦æŠ“å–æ£€æµ‹æ¡†æ¶ï¼Œå®ƒç»“åˆäº†é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†æœºåˆ¶ï¼Œå¹¶å—è¾…åŠ©é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡çš„å¼•å¯¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç³»åˆ—é—®ç­”æ¨¡æ¿ï¼Œä»¥å®ç°åŒ…æ‹¬ç›®æ ‡è§£æã€ç‰©ç†å±æ€§åˆ†æå’ŒæŠ“å–åŠ¨ä½œé€‰æ‹©åœ¨å†…çš„åˆ†å±‚æ¨ç†ã€‚æ­¤å¤–ï¼ŒGraspCoTå‘ˆç°äº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œå®ƒå°†ä¸‰ç»´åœºæ™¯çš„å¤šå…ƒè§†è§’è§‚å¯Ÿç¼–ç ä¸ºä¸‰ç»´è§†è§‰ä»¤ç‰Œï¼Œç„¶åå°†è¿™äº›è§†è§‰ä»¤ç‰Œä¸åŸºäºCoTè¡ç”Ÿçš„æ–‡æœ¬ä»¤ç‰ŒåµŒå…¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥ç”ŸæˆæŠ“å–å§¿æ€é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†IntentGraspï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†ï¼Œå¡«è¡¥äº†å…¬å¼€æ•°æ®é›†ä¸­å¤šå¯¹è±¡æŠ“å–æ£€æµ‹åœ¨å¤šæ ·åŒ–å’Œé—´æ¥è¨€è¯­æŒ‡ä»¤ä¸‹çš„ç©ºç™½ã€‚åœ¨IntentGraspä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ï¼Œå¹¶ä¸”åœ¨ç°å®ä¸–ç•Œçš„æœºå™¨äººåº”ç”¨ä¸­çš„é¢å¤–éªŒè¯ä¹Ÿè¯æ˜äº†å…¶å®ç”¨æ€§ã€‚ä»£ç å’Œæ•°æ®å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16013v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæå‡ºäº†ä¸€ç§åä¸ºGraspCoTçš„6è‡ªç”±åº¦æŠ“å–æ£€æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é¢å‘ç‰©ç†å±æ€§çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æ¨ç†æœºåˆ¶ï¼Œé€šè¿‡è¾…åŠ©é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡è¿›è¡Œå¼•å¯¼ã€‚è®¾è®¡äº†ä¸€ç³»åˆ—QAæ¨¡æ¿ï¼Œå®ç°ç›®æ ‡è§£æã€ç‰©ç†å±æ€§åˆ†æå’ŒæŠ“å–åŠ¨ä½œé€‰æ‹©çš„ä¸‰é˜¶æ®µå±‚æ¬¡æ¨ç†ã€‚åŒæ—¶ï¼ŒGraspCoTå‘ˆç°äº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œå°†ä¸‰ç»´åœºæ™¯çš„å¤šè§†è§’è§‚å¯Ÿç¼–ç ä¸ºä¸‰ç»´è§†è§‰æ ‡è®°ï¼Œå¹¶ä¸CoTè¡ç”Ÿçš„æ–‡æœ¬æ ‡è®°è”åˆåµŒå…¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥ç”ŸæˆæŠ“å–å§¿æ€é¢„æµ‹ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†IntentGraspå¤§å‹åŸºå‡†æµ‹è¯•ï¼Œå¡«è¡¥äº†å…¬å…±æ•°æ®é›†ä¸­å¤šå¯¹è±¡æŠ“å–æ£€æµ‹åœ¨å¤šæ ·åŒ–å’Œé—´æ¥è¨€è¯­æŒ‡ä»¤ä¸‹çš„ç©ºç™½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>GraspCoTæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æ¨ç†æœºåˆ¶å®ç°çµæ´»çš„æŒ‡ä»¤å¼•å¯¼6è‡ªç”±åº¦æŠ“å–ã€‚</li>
<li>GraspCoTæ¡†æ¶é€šè¿‡è¾…åŠ©é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡è¿›è¡Œå¼•å¯¼ï¼Œå¹¶è®¾è®¡äº†ä¸€ç³»åˆ—QAæ¨¡æ¿ä»¥å®ç°å±‚æ¬¡æ¨ç†ã€‚</li>
<li>GraspCoTæ¡†æ¶å…³æ³¨ç‰©ä½“çš„ç‰©ç†å±æ€§ï¼Œå°†å…¶ä¸æŠ“å–ä»»åŠ¡ç´§å¯†ç»“åˆã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œç»“åˆäº†è§†è§‰æ ‡è®°å’Œæ–‡æœ¬æ ‡è®°è¿›è¡ŒæŠ“å–å§¿æ€é¢„æµ‹ã€‚</li>
<li>IntentGraspåŸºå‡†æµ‹è¯•å¡«è¡¥äº†å…¬å…±æ•°æ®é›†ä¸­å¤šå¯¹è±¡æŠ“å–æ£€æµ‹åœ¨å¤æ‚å’Œé—´æ¥æŒ‡ä»¤ä¸‹çš„ç©ºç™½ã€‚</li>
<li>å®éªŒè¡¨æ˜GraspCoTæ¡†æ¶åœ¨IntentGraspåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd242601b677910ae8d1612b1d947728.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a428d62a43d1817b51bb1ff32995c99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f61f27e1e6873a3f6bf001e7fa081242.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-379b1c4b6f705d7d8d3663526bcbdfdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbe984911a74120afc20c5d9560a00ac.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Grammar-and-Gameplay-aligned-RL-for-Game-Description-Generation-with-LLMs"><a href="#Grammar-and-Gameplay-aligned-RL-for-Game-Description-Generation-with-LLMs" class="headerlink" title="Grammar and Gameplay-aligned RL for Game Description Generation with   LLMs"></a>Grammar and Gameplay-aligned RL for Game Description Generation with   LLMs</h2><p><strong>Authors:Tsunehiko Tanaka, Edgar Simo-Serra</strong></p>
<p>Game Description Generation (GDG) is the task of generating a game description written in a Game Description Language (GDL) from natural language text. Previous studies have explored generation methods leveraging the contextual understanding capabilities of Large Language Models (LLMs); however, accurately reproducing the game features of the game descriptions remains a challenge. In this paper, we propose reinforcement learning-based fine-tuning of LLMs for GDG (RLGDG). Our training method simultaneously improves grammatical correctness and fidelity to game concepts by introducing both grammar rewards and concept rewards. Furthermore, we adopt a two-stage training strategy where Reinforcement Learning (RL) is applied following Supervised Fine-Tuning (SFT). Experimental results demonstrate that our proposed method significantly outperforms baseline methods using SFT alone. </p>
<blockquote>
<p>æ¸¸æˆæè¿°ç”Ÿæˆï¼ˆGDGï¼‰çš„ä»»åŠ¡æ˜¯ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­ç”Ÿæˆä¸€ç§ç”¨æ¸¸æˆæè¿°è¯­è¨€ï¼ˆGDLï¼‰ç¼–å†™çš„æ¸¸æˆæè¿°ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›çš„ç”Ÿæˆæ–¹æ³•ï¼›ç„¶è€Œï¼Œå‡†ç¡®å¤åˆ¶æ¸¸æˆæè¿°ä¸­çš„æ¸¸æˆç‰¹æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMå¾®è°ƒæ–¹æ³•ç”¨äºGDGï¼ˆRLGDGï¼‰ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ–¹æ³•é€šè¿‡å¼•å…¥è¯­æ³•å¥–åŠ±å’Œæ¦‚å¿µå¥–åŠ±ï¼ŒåŒæ—¶æé«˜äº†è¯­æ³•æ­£ç¡®æ€§å’Œå¯¹æ¸¸æˆæ¦‚å¿µçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œåœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¹‹ååº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨SFTçš„åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15783v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ¸¸æˆæè¿°ç”Ÿæˆï¼ˆGDGï¼‰çš„ä»»åŠ¡ï¼Œå³åˆ©ç”¨æ¸¸æˆæè¿°è¯­è¨€ï¼ˆGDLï¼‰ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­ç”Ÿæˆæ¸¸æˆæè¿°ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›è¿›è¡Œç”Ÿæˆçš„æ–¹æ³•ï¼Œä½†å‡†ç¡®å†ç°æ¸¸æˆæè¿°ä¸­çš„æ¸¸æˆç‰¹æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMsç²¾ç»†è°ƒæ•´æ–¹æ³•ï¼ˆRLGDGï¼‰ï¼Œé€šè¿‡å¼•å…¥è¯­æ³•å¥–åŠ±å’Œæ¦‚å¿µå¥–åŠ±ï¼ŒåŒæ—¶æé«˜è¯­æ³•æ­£ç¡®æ€§å’Œå¯¹æ¸¸æˆæ¦‚å¿µçš„å¿ å®åº¦ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œåœ¨ç›‘ç£ç²¾ç»†è°ƒæ•´ï¼ˆSFTï¼‰ä¹‹ååº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨SFTçš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¸¸æˆæè¿°ç”Ÿæˆï¼ˆGDGï¼‰çš„ä»»åŠ¡æ˜¯ï¼šä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­ï¼Œåˆ©ç”¨æ¸¸æˆæè¿°è¯­è¨€ï¼ˆGDLï¼‰ç”Ÿæˆæ¸¸æˆæè¿°ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²è¢«ç”¨äºæ¸¸æˆæè¿°ç”Ÿæˆï¼Œä½†å‡†ç¡®å†ç°æ¸¸æˆç‰¹æ€§æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•RLGDGï¼Œå³åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMsç²¾ç»†è°ƒæ•´æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¸¸æˆæè¿°çš„è¯­æ³•æ­£ç¡®æ€§å’Œå¯¹æ¸¸æˆæ¦‚å¿µçš„å¿ å®åº¦ã€‚</li>
<li>RLGDGé€šè¿‡å¼•å…¥è¯­æ³•å¥–åŠ±å’Œæ¦‚å¿µå¥–åŠ±æ¥åŒæ—¶æ”¹å–„è¿™ä¸¤æ–¹é¢ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå…ˆåœ¨ç›‘ç£å­¦ä¹ ä¸‹ç²¾ç»†è°ƒæ•´æ¨¡å‹ï¼Œå†åº”ç”¨å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLGDGæ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨ç›‘ç£å­¦ä¹ çš„åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d4d2ed61b3258391965bc2e7970d301.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9fc0a81b45a29ad42020429f8d3c87b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36a9ec392402ca14d6f88e904ee4fe8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad559e02019126c828df0d4a662a7a97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-556f079a5e21516e031d31aeb4cfef29.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7471a351a6f66eb1790ec7b9edd1f77e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba3b5b8cfd4ceb064707aa0aa8663c1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3b2f01ee0bcb34ee52cf92ed43784d3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-Environment-with-LLM-Controlled-Adversary-in-D-D-5th-Edition-Combat"><a href="#Reinforcement-Learning-Environment-with-LLM-Controlled-Adversary-in-D-D-5th-Edition-Combat" class="headerlink" title="Reinforcement Learning Environment with LLM-Controlled Adversary in D&amp;D   5th Edition Combat"></a>Reinforcement Learning Environment with LLM-Controlled Adversary in D&amp;D   5th Edition Combat</h2><p><strong>Authors:Joseph Emmanuel DL Dayo, Michel Onasis S. Ogbinar, Prospero C. Naval Jr</strong></p>
<p>The objective of this study is to design and implement a reinforcement learning (RL) environment using D&amp;D 5E combat scenarios to challenge smaller RL agents through interaction with a robust adversarial agent controlled by advanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research employs Deep Q-Networks (DQN) for the smaller agents, creating a testbed for strategic AI development that also serves as an educational tool by simulating dynamic and unpredictable combat scenarios. We successfully integrated sophisticated language models into the RL framework, enhancing strategic decision-making processes. Our results indicate that while RL agents generally outperform LLM-controlled adversaries in standard metrics, the strategic depth provided by LLMs significantly enhances the overall AI capabilities in this complex, rule-based setting. The novelty of our approach and its implications for mastering intricate environments and developing adaptive strategies are discussed, alongside potential innovations in AI-driven interactive simulations. This paper aims to demonstrate how integrating LLMs can create more robust and adaptable AI systems, providing valuable insights for further research and educational applications. </p>
<blockquote>
<p>æœ¬ç ”ç©¶çš„ç›®æ ‡æ˜¯åˆ©ç”¨D&amp;D 5Eæˆ˜æ–—åœºæ™¯è®¾è®¡å’Œå®ç°ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç¯å¢ƒï¼Œé€šè¿‡åœ¨ä¸å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oå’ŒLLaMA 3 8Bï¼‰æ§åˆ¶çš„ç¨³å¥å¯¹æŠ—ä»£ç†çš„äº’åŠ¨æ¥æŒ‘æˆ˜è¾ƒå°çš„RLä»£ç†ã€‚æœ¬ç ”ç©¶é‡‡ç”¨æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ç”¨äºå°å‹ä»£ç†ï¼Œä¸ºæˆ˜ç•¥äººå·¥æ™ºèƒ½å‘å±•åˆ›å»ºä¸€ä¸ªæµ‹è¯•å¹³å°ï¼ŒåŒæ—¶é€šè¿‡æ¨¡æ‹ŸåŠ¨æ€å’Œä¸å¯é¢„æµ‹çš„æˆ˜æ–—åœºæ™¯ï¼Œä¹Ÿä½œä¸ºæ•™è‚²å·¥å…·ã€‚æˆ‘ä»¬æˆåŠŸåœ°å°†å¤æ‚è¯­è¨€æ¨¡å‹æ•´åˆåˆ°RLæ¡†æ¶ä¸­ï¼Œå¢å¼ºäº†æˆ˜ç•¥å†³ç­–è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°½ç®¡åœ¨æ ‡å‡†æŒ‡æ ‡ä¸Šï¼ŒRLä»£ç†é€šå¸¸ä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹æ§åˆ¶çš„å¯¹æ‰‹ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹æä¾›çš„æˆ˜ç•¥æ·±åº¦åœ¨å¤æ‚çš„åŸºäºè§„åˆ™çš„ç¯å¢ƒä¸­æ˜¾è‘—å¢å¼ºäº†æ•´ä½“çš„äººå·¥æ™ºèƒ½èƒ½åŠ›ã€‚è®¨è®ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„åˆ›æ–°æ€§åŠå…¶åœ¨å­¦ä¹ å¤æ‚ç¯å¢ƒå’Œå¼€å‘è‡ªé€‚åº”ç­–ç•¥æ–¹é¢çš„æ„ä¹‰ï¼Œä»¥åŠäººå·¥æ™ºèƒ½é©±åŠ¨äº¤äº’å¼æ¨¡æ‹Ÿä¸­çš„æ½œåœ¨åˆ›æ–°ã€‚æœ¬æ–‡æ—¨åœ¨å±•ç¤ºå¦‚ä½•æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹æ¥åˆ›å»ºæ›´ç¨³å¥å’Œé€‚åº”æ€§æ›´å¼ºçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œä¸ºè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ•™è‚²åº”ç”¨æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15726v1">PDF</a> Preprint. Submitted to the 31st International Conference on Neural   Information Processing (ICONIP 2024)</p>
<p><strong>Summary</strong><br>æ¸¸æˆè®¾è®¡ä¸­çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç¯å¢ƒé€šè¿‡ä½¿ç”¨D&amp;D 5Eæˆ˜æ–—åœºæ™¯æ¥æŒ‘æˆ˜å°å‹RLä»£ç†ï¼Œå¹¶ä¸å…¶ç”±å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ§åˆ¶çš„å¯¹æŠ—æ€§ä»£ç†è¿›è¡Œäº¤äº’ã€‚è¯¥ç ”ç©¶é‡‡ç”¨æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ä¸ºå°å‹ä»£ç†åˆ›å»ºæµ‹è¯•å¹³å°ï¼ŒåŒæ—¶ä½œä¸ºæ¨¡æ‹ŸåŠ¨æ€å’Œä¸å¯é¢„æµ‹æˆ˜æ–—åœºæ™¯çš„æ•™è‚²å·¥å…·ã€‚æˆåŠŸå°†å¤æ‚è¯­è¨€æ¨¡å‹æ•´åˆåˆ°RLæ¡†æ¶ä¸­ï¼Œæå‡ç­–ç•¥å†³ç­–è¿‡ç¨‹ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡RLä»£ç†åœ¨æ ‡å‡†æŒ‡æ ‡ä¸Šé€šå¸¸ä¼˜äºLLMæ§åˆ¶çš„å¯¹æ‰‹ï¼Œä½†LLMsæä¾›çš„æˆ˜ç•¥æ·±åº¦åœ¨å¤æ‚çš„åŸºäºè§„åˆ™çš„ç¯å¢ƒä¸­æ˜¾è‘—å¢å¼ºäº†æ•´ä½“AIèƒ½åŠ›ã€‚è¯¥ç ”ç©¶æ¢è®¨äº†å…¶æ–¹æ³•çš„åˆ›æ–°æ€§ä»¥åŠå…¶åœ¨æŒæ¡å¤æ‚ç¯å¢ƒå’Œå¼€å‘è‡ªé€‚åº”ç­–ç•¥æ–¹é¢çš„æ„ä¹‰ï¼Œå¹¶æä¾›äº†å…³äºAIé©±åŠ¨äº¤äº’å¼æ¨¡æ‹Ÿçš„æ½œåœ¨åˆ›æ–°ã€‚æ•´åˆLLMså¯ä»¥åˆ›å»ºæ›´ç¨³å¥å’Œé€‚åº”æ€§æ›´å¼ºçš„AIç³»ç»Ÿï¼Œä¸ºè¿›ä¸€æ­¥çš„ç§‘ç ”å’Œæ•™è‚²åº”ç”¨æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æ—¨åœ¨è®¾è®¡ä¸€ä¸ªä½¿ç”¨D&amp;D 5Eæˆ˜æ–—åœºæ™¯çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç¯å¢ƒã€‚</li>
<li>å°å‹RLä»£ç†åœ¨ä¸ç”±å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ§åˆ¶çš„å¯¹æŠ—æ€§ä»£ç†äº¤äº’æ—¶æ¥å—æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ä¸ºå°å‹ä»£ç†åˆ›å»ºæµ‹è¯•å¹³å°ï¼Œæ¨¡æ‹ŸåŠ¨æ€å’Œä¸å¯é¢„æµ‹çš„æˆ˜æ–—åœºæ™¯ã€‚</li>
<li>æˆåŠŸæ•´åˆå¤æ‚è¯­è¨€æ¨¡å‹åˆ°RLæ¡†æ¶ä¸­ï¼Œå¢å¼ºç­–ç•¥å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>RLä»£ç†åœ¨æ ‡å‡†æŒ‡æ ‡ä¸Šé€šå¸¸ä¼˜äºLLMæ§åˆ¶çš„å¯¹æ‰‹ï¼Œä½†LLMsåœ¨å¤æ‚ç¯å¢ƒä¸­æ˜¾è‘—å¢å¼ºäº†AIçš„æ•´ä½“èƒ½åŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶çš„åˆ›æ–°æ€§åœ¨äºå…¶åœ¨å¤æ‚ç¯å¢ƒæŒæ¡å’Œè‡ªé€‚åº”ç­–ç•¥å¼€å‘æ–¹é¢çš„æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-923a997d5111c16b7444743b7f9c1e6f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11bfcfe68beacb16e7dfc29857e8d728.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80775a5e026910d3acc5b5d5a3add4e8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LLaVA-MORE-A-Comparative-Study-of-LLMs-and-Visual-Backbones-for-Enhanced-Visual-Instruction-Tuning"><a href="#LLaVA-MORE-A-Comparative-Study-of-LLMs-and-Visual-Backbones-for-Enhanced-Visual-Instruction-Tuning" class="headerlink" title="LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for   Enhanced Visual Instruction Tuning"></a>LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for   Enhanced Visual Instruction Tuning</h2><p><strong>Authors:Federico Cocchi, Nicholas Moratelli, Davide Caffagni, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara</strong></p>
<p>Recent progress in Multimodal Large Language Models (MLLMs) has highlighted the critical roles of both the visual backbone and the underlying language model. While prior work has primarily focused on scaling these components to billions of parameters, the trade-offs between model size, architecture, and performance remain underexplored. Additionally, inconsistencies in training data and evaluation protocols have hindered direct comparisons, making it difficult to derive optimal design choices. In this paper, we introduce LLaVA-MORE, a new family of MLLMs that integrates recent language models with diverse visual backbones. To ensure fair comparisons, we employ a unified training protocol applied consistently across all architectures. Our analysis systematically explores both small- and medium-scale LLMs â€“ including Phi-4, LLaMA-3.1, and Gemma-2 â€“ to evaluate multimodal reasoning, generation, and instruction following, while examining the relationship between model size and performance. Beyond evaluating the LLM impact on final results, we conduct a comprehensive study of various visual encoders, ranging from CLIP-based architectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional experiments investigate the effects of increased image resolution and variations in pre-training datasets. Overall, our results provide insights into the design of more effective MLLMs, offering a reproducible evaluation framework that facilitates direct comparisons and can guide future model development. Our source code and trained models are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/aimagelab/LLaVA-MORE">https://github.com/aimagelab/LLaVA-MORE</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥å‡¸æ˜¾äº†è§†è§‰ä¸»å¹²å’Œåº•å±‚è¯­è¨€æ¨¡å‹çš„å…³é”®ä½œç”¨ã€‚è™½ç„¶æ—©æœŸçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å°†è¿™äº›ç»„ä»¶æ‰©å±•åˆ°æ•°åäº¿å‚æ•°ä¸Šï¼Œä½†æ¨¡å‹å¤§å°ã€æ¶æ„å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ­¤å¤–ï¼Œè®­ç»ƒæ•°æ®å’Œè¯„ä¼°åè®®çš„ä¸ä¸€è‡´æ€§é˜»ç¢äº†ç›´æ¥æ¯”è¾ƒï¼Œéš¾ä»¥å¾—å‡ºæœ€ä½³çš„è®¾è®¡é€‰æ‹©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LLaVA-MOREï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œå®ƒå°†æœ€æ–°çš„è¯­è¨€æ¨¡å‹ä¸å¤šæ ·åŒ–çš„è§†è§‰ä¸»å¹²é›†æˆåœ¨ä¸€èµ·ã€‚ä¸ºç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç»Ÿä¸€çš„åº”ç”¨äºæ‰€æœ‰æ¶æ„çš„è®­ç»ƒåè®®ã€‚æˆ‘ä»¬çš„åˆ†æç³»ç»Ÿåœ°æ¢ç´¢äº†åŒ…æ‹¬Phi-4ã€LLaMA-3.1å’ŒGemma-2åœ¨å†…çš„å°å‹å’Œä¸­å‹è§„æ¨¡çš„è¯­è¨€æ¨¡å‹ï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€æ¨ç†ã€ç”Ÿæˆå’ŒæŒ‡ä»¤éµå¾ªçš„èƒ½åŠ›ï¼ŒåŒæ—¶æ¢è®¨æ¨¡å‹å¤§å°ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚é™¤äº†è¯„ä¼°è¯­è¨€æ¨¡å‹å¯¹æœ€ç»ˆç»“æœçš„å½±å“å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹å„ç§è§†è§‰ç¼–ç å™¨è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ï¼ŒåŒ…æ‹¬åŸºäºCLIPçš„æ¶æ„ä»¥åŠDINOv2ã€SigLIPå’ŒSigLIP2ç­‰æ›¿ä»£æ–¹æ¡ˆã€‚é¢å¤–çš„å®éªŒè¿˜æ¢è®¨äº†æé«˜å›¾åƒåˆ†è¾¨ç‡å’Œé¢„è®­ç»ƒæ•°æ®é›†å˜åŒ–çš„å½±å“ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æä¾›äº†è§è§£ï¼Œæä¾›äº†ä¸€ä¸ªå¯é‡å¤çš„è¯„ä»·æ¡†æ¶ï¼Œä¾¿äºç›´æ¥æ¯”è¾ƒï¼Œå¹¶èƒ½æŒ‡å¯¼æœªæ¥çš„æ¨¡å‹å¼€å‘ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/aimagelab/LLaVA-MORE">https://github.com/aimagelab/LLaVA-MORE</a>å…¬å¼€è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15621v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LLaVA-MOREï¼Œä¸€ä¸ªé›†æˆäº†æœ€æ–°è¯­è¨€æ¨¡å‹å’Œå¤šç§è§†è§‰éª¨å¹²ç½‘çš„æ–°å®¶æ—çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶é€šè¿‡ç»Ÿä¸€è®­ç»ƒåè®®ï¼Œå¯¹å°å‹å’Œä¸­ç­‰è§„æ¨¡çš„è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œè¯„ä¼°äº†å¤šæ¨¡æ€æ¨ç†ã€ç”Ÿæˆå’ŒæŒ‡ä»¤è·Ÿéšçš„èƒ½åŠ›ï¼Œå¹¶æ¢è®¨äº†æ¨¡å‹å°ºå¯¸ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚åŒæ—¶ï¼Œä¹Ÿå¯¹ä¸åŒçš„è§†è§‰ç¼–ç å™¨è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶å…¬å¼€äº†æºä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaVA-MOREæ˜¯ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œç»“åˆäº†æœ€æ–°çš„è¯­è¨€æ¨¡å‹å’Œå¤šç§è§†è§‰éª¨å¹²ç½‘ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†ç»Ÿä¸€çš„è®­ç»ƒåè®®ï¼Œä»¥ç¡®ä¿å¯¹ä¸åŒæ¶æ„çš„å…¬å¹³æ¯”è¾ƒã€‚</li>
<li>å¯¹å°å‹å’Œä¸­ç­‰è§„æ¨¡çš„è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼ŒåŒ…æ‹¬Phi-4ã€LLaMA-3.1å’ŒGemma-2ã€‚</li>
<li>è¯„ä¼°äº†å¤šæ¨¡æ€æ¨ç†ã€ç”Ÿæˆå’ŒæŒ‡ä»¤è·Ÿéšçš„èƒ½åŠ›ï¼Œå¹¶æ¢è®¨äº†æ¨¡å‹å°ºå¯¸ä¸æ€§èƒ½çš„å…³ç³»ã€‚</li>
<li>å¯¹ä¸åŒçš„è§†è§‰ç¼–ç å™¨è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼ŒåŒ…æ‹¬CLIPåŸºç¡€æ¶æ„ä»¥åŠå…¶ä»–æ›¿ä»£æ–¹æ¡ˆï¼Œå¦‚DINOv2ã€SigLIPå’ŒSigLIP2ã€‚</li>
<li>é€šè¿‡å¢åŠ å›¾åƒåˆ†è¾¨ç‡å’Œé¢„è®­ç»ƒæ•°æ®é›†çš„å˜åŒ–è¿›è¡Œäº†å®éªŒæ¢ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ffadbf318a62c31dd96e92d275f12ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e82e0723a57453823323b86ee37e77f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21726c89669c1ffbe6c6fc5f4b628546.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f39c2c82da3ed24e9396532e29276d01.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Cosmos-Reason1-From-Physical-Common-Sense-To-Embodied-Reasoning"><a href="#Cosmos-Reason1-From-Physical-Common-Sense-To-Embodied-Reasoning" class="headerlink" title="Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning"></a>Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning</h2><p><strong>Authors: NVIDIA,  :, Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu Liu, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Xiaodong Yang, Zhuolin Yang, Xiaohui Zeng, Zhe Zhang</strong></p>
<p>Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data and train our models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL) as the post-training. To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and reinforcement learning bring significant improvements. To facilitate the development of Physical AI, we will make our code and pre-trained models available under the NVIDIA Open Model License at <a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-reason1">https://github.com/nvidia-cosmos/cosmos-reason1</a>. </p>
<blockquote>
<p>ç‰©ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿéœ€è¦åœ¨ç‰©ç†ä¸–ç•Œä¸­æ„ŸçŸ¥ã€ç†è§£å’Œæ‰§è¡Œå¤æ‚çš„åŠ¨ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Cosmos-Reason1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé€šè¿‡é•¿é“¾æ€ç»´æ¨ç†è¿‡ç¨‹ç†è§£ç‰©ç†ä¸–ç•Œï¼Œå¹¶ä»¥è‡ªç„¶è¯­è¨€ç”Ÿæˆé€‚å½“çš„å†³ç­–ï¼ˆä¾‹å¦‚ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰ç‰©ç†äººå·¥æ™ºèƒ½æ¨ç†çš„å…³é”®èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨ç‰©ç†å¸¸è¯†å’Œå®ä½“æ¨ç†ã€‚ä¸ºäº†è¡¨ç¤ºç‰©ç†å¸¸è¯†ï¼Œæˆ‘ä»¬ä½¿ç”¨å±‚æ¬¡æœ¬ä½“è®ºæ¥æ•æ‰å…³äºç©ºé—´ã€æ—¶é—´å’Œç‰©ç†çš„åŸºæœ¬çŸ¥è¯†ã€‚å¯¹äºå®ä½“æ¨ç†ï¼Œæˆ‘ä»¬ä¾èµ–äºä¸€ä¸ªäºŒç»´çš„æœ¬ä½“è®ºæ¥æ¦‚æ‹¬ä¸åŒçš„ç‰©ç†å®ä½“ã€‚åŸºäºè¿™äº›èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå³Cosmos-Reason1-8Bå’ŒCosmos-Reason1-56Bã€‚æˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œäº†å››ä¸ªé˜¶æ®µçš„æ•°æ®æ•´ç†å’Œè®­ç»ƒï¼šè§†è§‰é¢„è®­ç»ƒã€ä¸€èˆ¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç‰©ç†äººå·¥æ™ºèƒ½SFTå’Œç‰©ç†äººå·¥æ™ºèƒ½å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºåè®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æ ¹æ®æˆ‘ä»¬çš„æœ¬ä½“è®ºå»ºç«‹äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºç‰©ç†å¸¸è¯†å’Œå®ä½“æ¨ç†ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç‰©ç†äººå·¥æ™ºèƒ½SFTå’Œå¼ºåŒ–å­¦ä¹ å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä¸ºäº†æ–¹ä¾¿ç‰©ç†äººå·¥æ™ºèƒ½çš„å¼€å‘ï¼Œæˆ‘ä»¬å°†åœ¨NVIDIA Open Model Licenseä¸‹æä¾›æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-reason1%E3%80%82">https://github.com/nvidia-cosmos/cosmos-reason1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15558v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç‰©ç†AIç³»ç»Ÿéœ€è¦å…·å¤‡æ„ŸçŸ¥ã€ç†è§£å’Œæ‰§è¡Œç°å®ä¸–ç•Œå¤æ‚åŠ¨ä½œçš„èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†Cosmos-Reason1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé€šè¿‡é•¿é“¾æ€ç»´æ¨ç†è¿‡ç¨‹ç†è§£ç‰©ç†ä¸–ç•Œå¹¶ç”Ÿæˆé€‚å½“çš„å†³ç­–ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†ç‰©ç†å¸¸è¯†å’Œå®ä½“æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨åˆ†å±‚æœ¬ä½“è®ºæ¥è¡¨ç¤ºç‰©ç†å¸¸è¯†ï¼Œå¹¶è¿ç”¨äºŒç»´æœ¬ä½“è®ºè¿›è¡Œå®ä½“æ¨ç†ã€‚åŸºäºæ­¤ï¼Œå¼€å‘äº†ä¸¤ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Cosmos-Reason1-8Bå’ŒCosmos-Reason1-56Bã€‚æ¨¡å‹è®­ç»ƒåˆ†ä¸ºå››ä¸ªé˜¶æ®µï¼šè§†è§‰é¢„è®­ç»ƒã€ä¸€èˆ¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç‰©ç†AISFTå’Œç‰©ç†AIå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒã€‚è¯„ä¼°ç»“æœæ˜¾ç†ç‰©ç†AISFTå’Œå¼ºåŒ–å­¦ä¹ å¸¦æ¥äº†æ˜¾è‘—æ”¹å–„ã€‚ä¸ºæ¨è¿›ç‰©ç†AIçš„å‘å±•ï¼Œç›¸å…³ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨NVIDIA Open Model Licenseä¸‹äº<a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-reason1%E5%BC%BA%E5%BC%95%E3%80%82">https://github.com/nvidia-cosmos/cosmos-reason1å¼€æ”¾è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©ç†AIç³»ç»Ÿéœ€è¦ç†è§£å¹¶æ‰§è¡Œç°å®ä¸–ç•Œä¸­çš„å¤æ‚åŠ¨ä½œã€‚</li>
<li>Cosmos-Reason1æ¨¡å‹é€šè¿‡é•¿é“¾æ€ç»´æ¨ç†è¿‡ç¨‹ç†è§£ç‰©ç†ä¸–ç•Œå¹¶åšå‡ºå†³ç­–ã€‚</li>
<li>æ¨¡å‹å…·å¤‡ç‰©ç†å¸¸è¯†å’Œå®ä½“æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨åˆ†å±‚æœ¬ä½“è®ºå’ŒäºŒç»´æœ¬ä½“è®ºè¿›è¡Œè¡¨ç¤ºã€‚</li>
<li>å¼€å‘äº†ä¸¤ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Cosmos-Reason1-8Bå’ŒCosmos-Reason1-56Bã€‚</li>
<li>æ¨¡å‹è®­ç»ƒåŒ…æ‹¬è§†è§‰é¢„è®­ç»ƒã€ä¸€èˆ¬ç›‘ç£å¾®è°ƒã€ç‰©ç†AIç‰¹å®šç›‘ç£å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ ç­‰é˜¶æ®µã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜ç‰©ç†AIç‰¹å®šç›‘ç£è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c43e7b3cff1c1fa134f1a8b30994c54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-856bc8687ff4a4cacb12a7ffe717b60f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d67b4b58a8a779e95a32e10d3829ef47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-731456c5726227d06f6f44bf8457ddbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99659305b782ad2bbf56ee38fd36b36c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VEGGIE-Instructional-Editing-and-Reasoning-of-Video-Concepts-with-Grounded-Generation"><a href="#VEGGIE-Instructional-Editing-and-Reasoning-of-Video-Concepts-with-Grounded-Generation" class="headerlink" title="VEGGIE: Instructional Editing and Reasoning of Video Concepts with   Grounded Generation"></a>VEGGIE: Instructional Editing and Reasoning of Video Concepts with   Grounded Generation</h2><p><strong>Authors:Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, Mohit Bansal</strong></p>
<p>Recent video diffusion models have enhanced video editing, but it remains challenging to handle instructional editing and diverse tasks (e.g., adding, removing, changing) within a unified framework. In this paper, we introduce VEGGIE, a Video Editor with Grounded Generation from Instructions, a simple end-to-end framework that unifies video concept editing, grounding, and reasoning based on diverse user instructions. Specifically, given a video and text query, VEGGIE first utilizes an MLLM to interpret user intentions in instructions and ground them to the video contexts, generating frame-specific grounded task queries for pixel-space responses. A diffusion model then renders these plans and generates edited videos that align with user intent. To support diverse tasks and complex instructions, we employ a curriculum learning strategy: first aligning the MLLM and video diffusion model with large-scale instructional image editing data, followed by end-to-end fine-tuning on high-quality multitask video data. Additionally, we introduce a novel data synthesis pipeline to generate paired instructional video editing data for model training. It transforms static image data into diverse, high-quality video editing samples by leveraging Image-to-Video models to inject dynamics. VEGGIE shows strong performance in instructional video editing with different editing skills, outperforming the best instructional baseline as a versatile model, while other models struggle with multi-tasking. VEGGIE also excels in video object grounding and reasoning segmentation, where other baselines fail. We further reveal how the multiple tasks help each other and highlight promising applications like zero-shot multimodal instructional and in-context video editing. </p>
<blockquote>
<p>æœ€è¿‘çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»æé«˜äº†è§†é¢‘ç¼–è¾‘çš„èƒ½åŠ›ï¼Œä½†åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¤„ç†æŒ‡ä»¤ç¼–è¾‘å’Œå¤šæ ·åŒ–ä»»åŠ¡ï¼ˆä¾‹å¦‚æ·»åŠ ã€åˆ é™¤ã€æ›´æ”¹ï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VEGGIEï¼Œä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„æ¥åœ°ç”Ÿæˆè§†é¢‘ç¼–è¾‘å™¨ï¼Œå®ƒæ˜¯ä¸€ä¸ªç®€å•ç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œç»Ÿä¸€äº†è§†é¢‘æ¦‚å¿µç¼–è¾‘ã€æ¥åœ°å’ŒåŸºäºå¤šæ ·åŒ–ç”¨æˆ·æŒ‡ä»¤çš„æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªè§†é¢‘å’Œæ–‡æœ¬æŸ¥è¯¢ï¼ŒVEGGIEé¦–å…ˆåˆ©ç”¨MLLMæ¥è§£é‡Šç”¨æˆ·æ„å›¾çš„æŒ‡ä»¤å¹¶å°†å…¶æ¥åœ°åˆ°è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œä¸ºåƒç´ ç©ºé—´å“åº”ç”Ÿæˆç‰¹å®šå¸§çš„æ¥åœ°ä»»åŠ¡æŸ¥è¯¢ã€‚ç„¶åï¼Œæ‰©æ•£æ¨¡å‹ä¼šå‘ˆç°è¿™äº›è®¡åˆ’å¹¶ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„ç¼–è¾‘è§†é¢‘ã€‚ä¸ºäº†æ”¯æŒå¤šæ ·åŒ–çš„ä»»åŠ¡å’Œå¤æ‚çš„æŒ‡ä»¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼šé¦–å…ˆç”¨å¤§è§„æ¨¡çš„æŒ‡ä»¤å›¾åƒç¼–è¾‘æ•°æ®å¯¹é½MLLMå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç„¶ååœ¨é«˜è´¨é‡çš„å¤šä»»åŠ¡è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®åˆæˆç®¡é“ï¼Œä»¥ç”Ÿæˆç”¨äºæ¨¡å‹è®­ç»ƒçš„é…å¯¹æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ•°æ®ã€‚å®ƒé€šè¿‡åˆ©ç”¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹å°†é™æ€å›¾åƒæ•°æ®è½¬æ¢ä¸ºå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è§†é¢‘ç¼–è¾‘æ ·æœ¬ã€‚VEGGIEåœ¨å…·æœ‰ä¸åŒç¼–è¾‘æŠ€èƒ½çš„æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½œä¸ºä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†æœ€ä½³æŒ‡ä»¤åŸºçº¿ï¼Œè€Œå…¶ä»–æ¨¡å‹åœ¨å¤šä»»åŠ¡å¤„ç†æ–¹é¢åˆ™è¡¨ç°æŒ£æ‰ã€‚VEGGIEåœ¨è§†é¢‘å¯¹è±¡æ¥åœ°å’Œæ¨ç†åˆ†å‰²æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè€Œå…¶ä»–åŸºçº¿åˆ™å¤±è´¥äº†ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ­ç¤ºäº†å¤šä¸ªä»»åŠ¡æ˜¯å¦‚ä½•ç›¸äº’å¸®åŠ©çš„ï¼Œå¹¶å¼ºè°ƒäº†æœ‰å‰é€”çš„åº”ç”¨ï¼Œå¦‚é›¶å‡»å¤šåŠŸèƒ½æŒ‡ä»¤å’Œä¸Šä¸‹æ–‡å†…è§†é¢‘ç¼–è¾‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14350v2">PDF</a> First three authors contributed equally. Project page:   <a target="_blank" rel="noopener" href="https://veggie-gen.github.io/">https://veggie-gen.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ä¸ªåä¸ºVEGGIEçš„è§†é¢‘ç¼–è¾‘ç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŸºäºç”¨æˆ·æŒ‡ä»¤è¿›è¡Œè§†é¢‘æ¦‚å¿µç¼–è¾‘ã€å®šä½å’Œæ¨ç†ã€‚å®ƒä½¿ç”¨MLLMè§£è¯»ç”¨æˆ·æŒ‡ä»¤å¹¶å°†å…¶ä¸è§†é¢‘å†…å®¹ç›¸ç»“åˆï¼Œç”Ÿæˆç‰¹å®šå¸§çš„ä»»åŠ¡æŸ¥è¯¢ï¼Œå†é€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„ç¼–è¾‘è§†é¢‘ã€‚ä¸ºæ”¯æŒå¤šæ ·ä»»åŠ¡å’Œå¤æ‚æŒ‡ä»¤ï¼Œé‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå…ˆåœ¨å¤§è§„æ¨¡æŒ‡ä»¤å›¾åƒç¼–è¾‘æ•°æ®ä¸Šå¯¹é½MLLMå’Œæ‰©æ•£æ¨¡å‹ï¼Œå†åœ¨é«˜è´¨é‡å¤šä»»åŠ¡è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒã€‚æ­¤å¤–ï¼Œå¼•å…¥æ–°çš„æ•°æ®åˆæˆç®¡é“ç”Ÿæˆé…å¯¹æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ•°æ®ç”¨äºæ¨¡å‹è®­ç»ƒã€‚VEGGIEåœ¨æŒ‡ä»¤è§†é¢‘ç¼–è¾‘å’Œç‰©ä½“å®šä½ä¸æ¨ç†æ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VEGGIEæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†é¢‘ç¼–è¾‘ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§ç¼–è¾‘ä»»åŠ¡ï¼ˆå¦‚æ·»åŠ ã€åˆ é™¤ã€ä¿®æ”¹ï¼‰ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡MLLMè§£è¯»ç”¨æˆ·æŒ‡ä»¤å¹¶å°†å…¶ä¸è§†é¢‘å†…å®¹ç»“åˆï¼Œç”Ÿæˆç‰¹å®šå¸§çš„ä»»åŠ¡æŸ¥è¯¢ã€‚</li>
<li>ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¥æ‰§è¡Œç¼–è¾‘è®¡åˆ’å¹¶ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„è§†é¢‘ã€‚</li>
<li>é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥æ¥æ”¯æŒå¤šæ ·ä»»åŠ¡å’Œå¤æ‚æŒ‡ä»¤çš„è®­ç»ƒã€‚</li>
<li>å¼•å…¥æ•°æ®åˆæˆç®¡é“ç”Ÿæˆé…å¯¹æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ•°æ®ï¼Œåˆ©ç”¨Image-to-Videoæ¨¡å‹å¢åŠ åŠ¨æ€æ€§ã€‚</li>
<li>VEGGIEåœ¨æŒ‡ä»¤è§†é¢‘ç¼–è¾‘å’Œç‰©ä½“å®šä½ä¸æ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>å¤šä»»åŠ¡è®­ç»ƒæœ‰åŠ©äºæå‡æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æ­ç¤ºä¸åŒä»»åŠ¡é—´çš„ç›¸äº’ä¿ƒè¿›å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14350">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5708f7e3dba187051c8395723aedb51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b1e9cf510817cc414098bcbf8e2b667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b107c4552a56b5cb3d171cd65e6210a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e900a0068210883ca8d7217aaba69d7b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="JuDGE-Benchmarking-Judgment-Document-Generation-for-Chinese-Legal-System"><a href="#JuDGE-Benchmarking-Judgment-Document-Generation-for-Chinese-Legal-System" class="headerlink" title="JuDGE: Benchmarking Judgment Document Generation for Chinese Legal   System"></a>JuDGE: Benchmarking Judgment Document Generation for Chinese Legal   System</h2><p><strong>Authors:Weihang Su, Baoqing Yue, Qingyao Ai, Yiran Hu, Jiaqi Li, Changyue Wang, Kaiyuan Zhang, Yueyue Wu, Yiqun Liu</strong></p>
<p>This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: <a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE">https://github.com/oneal2000/JuDGE</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†JuDGEï¼ˆåˆ¤å†³ä¹¦ç”Ÿæˆè¯„ä¼°ï¼‰è¿™ä¸€æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ä¸­æ–‡æ³•å¾‹ä½“ç³»ä¸­åˆ¤å†³ä¹¦ç”Ÿæˆçš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä»»åŠ¡å®šä¹‰ä¸ºæ ¹æ®ç»™å®šçš„æ¡ˆä»¶äº‹å®æè¿°ç”Ÿæˆå®Œæ•´çš„æ³•å¾‹åˆ¤å†³ä¹¦ã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸€åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«æ¥è‡ªçœŸå®æ³•å¾‹æ¡ˆä»¶çš„æ¡ˆæƒ…æè¿°åŠå…¶ç›¸åº”çš„å®Œæ•´åˆ¤å†³ä¹¦ï¼Œè¿™äº›åˆ¤å†³ä¹¦ä½œä¸ºè¯„ä¼°ç”Ÿæˆæ–‡æ¡£è´¨é‡çš„çœŸå®æ ‡å‡†ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸¤ä¸ªå¤–éƒ¨æ³•å¾‹è¯­æ–™åº“è¿›ä¸€æ­¥æ‰©å……ï¼Œä¸ºä»»åŠ¡æä¾›äº†é¢å¤–çš„æ³•å¾‹çŸ¥è¯†ï¼šä¸€ä¸ªåŒ…å«æ³•è§„å’Œæ¡ä¾‹ï¼Œå¦ä¸€ä¸ªåˆ™åŒ…å«å¤§é‡ä»¥å¾€çš„åˆ¤å†³ä¹¦ã€‚æˆ‘ä»¬ä¸æ³•å¾‹ä¸“ä¸šäººå£«åˆä½œï¼Œå»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»å¤šä¸ªç»´åº¦è¯„ä¼°ç”Ÿæˆçš„åˆ¤å†³ä¹¦çš„è´¨é‡ã€‚æˆ‘ä»¬è¯„ä¼°äº†å„ç§åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ã€å¾®è°ƒä»¥åŠå¤šæºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æ—¢æ¶‰åŠé€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¹Ÿæ¶‰åŠæ³•å¾‹é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶RAGæ–¹æ³•å¯ä»¥æœ‰æ•ˆæé«˜æ­¤ä»»åŠ¡æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†å‡å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE%E3%80%82">https://github.com/oneal2000/JuDGEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14258v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ³•å¾‹æ–‡ä¹¦çš„ç”Ÿæˆä¸è¯„ä¼°æ˜¯ä¸€ä¸ªé‡è¦çš„é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä¸­æ–‡æ³•å¾‹ç³»ç»Ÿçš„æ–°å‹åŸºå‡†æµ‹è¯•JuDGEï¼Œæ—¨åœ¨è¯„ä¼°æ³•å¾‹æ–‡ä¹¦ç”Ÿæˆæ€§èƒ½ã€‚è¯¥ä»»åŠ¡è¢«å®šä¹‰ä¸ºæ ¹æ®ç»™å®šçš„æ¡ˆä»¶äº‹å®æè¿°ç”Ÿæˆå®Œæ•´çš„æ³•å¾‹åˆ¤å†³ä¹¦ã€‚ä¸ºæ¨è¿›æ­¤åŸºå‡†æµ‹è¯•ï¼Œæœ¬æ–‡æ„å»ºäº†åŒ…å«çœŸå®æ¡ˆä»¶äº‹å®æè¿°ä¸å…¶å¯¹åº”çš„å®Œæ•´åˆ¤å†³ä¹¦çš„ç»¼åˆæ•°æ®é›†ï¼Œè¿˜è¾…ä»¥ä¸¤ä¸ªå¤–éƒ¨æ³•å¾‹è¯­æ–™åº“æä¾›é¢å¤–çš„æ³•å¾‹çŸ¥è¯†ã€‚ä¸æ³•åŠ¡ä¸“ä¸šäººå£«åˆä½œï¼Œå»ºç«‹äº†å…¨é¢çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°ç”Ÿæˆçš„æ³•å¾‹æ–‡ä¹¦è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•å¯æœ‰æ•ˆæå‡æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨è¾ƒå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/oneal2000/JuDGEæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†æ–°å‹åŸºå‡†æµ‹è¯•JuDGEï¼Œé’ˆå¯¹ä¸­æ–‡æ³•å¾‹ç³»ç»Ÿçš„æ³•å¾‹æ–‡ä¹¦ç”Ÿæˆæ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>ä»»åŠ¡å®šä¹‰ä¸ºä»ç»™å®šçš„æ¡ˆä»¶äº‹å®æè¿°ç”Ÿæˆå®Œæ•´çš„æ³•å¾‹åˆ¤å†³ä¹¦ã€‚</li>
<li>æ„å»ºäº†ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«çœŸå®æ¡ˆä»¶çš„äº‹å®æè¿°ä¸å¯¹åº”åˆ¤å†³ä¹¦ï¼Œä½œä¸ºè¯„ä¼°ç”Ÿæˆæ–‡ä¹¦è´¨é‡çš„ä¾æ®ã€‚</li>
<li>è¾…ä»¥ä¸¤ä¸ªå¤–éƒ¨æ³•å¾‹è¯­æ–™åº“æä¾›é¢å¤–æ³•å¾‹çŸ¥è¯†ã€‚</li>
<li>ä¸æ³•å¾‹ä¸“ä¸šäººå£«åˆä½œå»ºç«‹å…¨é¢çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºRAGæ–¹æ³•èƒ½æœ‰æ•ˆæå‡ä»»åŠ¡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46635abb4e981bc03a6ee514cac96004.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c12c7e7f91bd6666772101cdba937f7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4a0d37c3ddd60aee0fb04a037e968a1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models"><a href="#Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models" class="headerlink" title="Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models"></a>Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models</h2><p><strong>Authors:Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang</strong></p>
<p>Vision-language models (VLMs) have advanced reasoning in natural scenes, but their role in medical imaging remains underexplored. Medical reasoning tasks demand robust image analysis and well-justified answers, posing challenges due to the complexity of medical images. Transparency and trustworthiness are essential for clinical adoption and regulatory compliance. We introduce Med-R1, a framework exploring reinforcement learning (RL) to enhance VLMsâ€™ generalizability and trustworthiness in medical reasoning. Leveraging the DeepSeek strategy, we employ Group Relative Policy Optimization (GRPO) to guide reasoning paths via reward signals. Unlike supervised fine-tuning (SFT), which often overfits and lacks generalization, RL fosters robust and diverse reasoning. Med-R1 is evaluated across eight medical imaging modalities: CT, MRI, Ultrasound, Dermoscopy, Fundus Photography, Optical Coherence Tomography (OCT), Microscopy, and X-ray Imaging. Compared to its base model, Qwen2-VL-2B, Med-R1 achieves a 29.94% accuracy improvement and outperforms Qwen2-VL-72B, which has 36 times more parameters. Testing across five question types-modality recognition, anatomy identification, disease diagnosis, lesion grading, and biological attribute analysis Med-R1 demonstrates superior generalization, exceeding Qwen2-VL-2B by 32.06% and surpassing Qwen2-VL-72B in question-type generalization. These findings show that RL improves medical reasoning and enables parameter-efficient models to outperform significantly larger ones. With interpretable reasoning outputs, Med-R1 represents a promising step toward generalizable, trustworthy, and clinically viable medical VLMs. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶åœºæ™¯æ¨ç†æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨åŒ»å­¦æˆåƒä¸­çš„ä½œç”¨ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åŒ»å­¦æ¨ç†ä»»åŠ¡éœ€è¦ç¨³å¥çš„å›¾åƒåˆ†æå’Œåˆç†çš„ç­”æ¡ˆï¼Œç”±äºåŒ»å­¦å›¾åƒçš„å¤æ‚æ€§ï¼Œè¿™æ„æˆäº†æŒ‘æˆ˜ã€‚é€æ˜åº¦å’Œå¯ä¿¡åº¦å¯¹äºä¸´åºŠé‡‡ç”¨å’Œæ³•è§„åˆè§„è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†Med-R1æ¡†æ¶ï¼Œæ¢ç´¢å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥å¢å¼ºVLMsåœ¨åŒ»å­¦æ¨ç†ä¸­çš„é€šç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚åˆ©ç”¨DeepSeekç­–ç•¥ï¼Œæˆ‘ä»¬é‡‡ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é€šè¿‡å¥–åŠ±ä¿¡å·å¼•å¯¼æ¨ç†è·¯å¾„ã€‚ä¸ç»å¸¸è¿‡åº¦æ‹Ÿåˆä¸”ç¼ºä¹æ³›åŒ–èƒ½åŠ›çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸åŒï¼ŒRLä¿ƒè¿›ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚Med-R1åœ¨å…«ç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šCTã€MRIã€è¶…å£°ã€çš®è‚¤é•œæ£€æŸ¥ã€çœ¼åº•æ‘„å½±ã€å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰ã€æ˜¾å¾®é•œå’ŒXå°„çº¿æˆåƒã€‚ä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼ŒMed-R1åœ¨å‡†ç¡®æ€§ä¸Šæé«˜äº†29.94%ï¼Œå¹¶ä¼˜äºå‚æ•°æ›´å¤šçš„Qwen2-VL-72Bã€‚åœ¨äº”ç§é—®é¢˜ç±»å‹ï¼ˆæ¨¡æ€è¯†åˆ«ã€è§£å‰–è¯†åˆ«ã€ç–¾ç—…è¯Šæ–­ã€ç—…å˜åˆ†çº§å’Œç”Ÿç‰©å±æ€§åˆ†æï¼‰çš„æµ‹è¯•ä¸Šï¼ŒMed-R1å±•ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¿‡Qwen2-VL-2B 32.06%ï¼Œå¹¶åœ¨é—®é¢˜ç±»å‹æ³›åŒ–æ–¹é¢è¶…è¶ŠQwen2-VL-72Bã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ å¯ä»¥æ”¹å–„åŒ»å­¦æ¨ç†ï¼Œå¹¶ä½¿å‚æ•°æ•ˆç‡æ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—è¶…è¶Šæ›´å¤§çš„æ¨¡å‹ã€‚Med-R1å¯è§£é‡Šçš„æ¨ç†è¾“å‡ºä»£è¡¨äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ­¥éª¤ï¼Œæœç€é€šç”¨ã€å¯ä¿¡å’Œä¸´åºŠä¸Šå¯è¡Œçš„åŒ»å­¦VLMså‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13939v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>VLMåœ¨åŒ»ç–—å›¾åƒé¢†åŸŸçš„ä½œç”¨å°šå¾…æ·±å…¥ç ”ç©¶ã€‚æ–‡ç« æå‡ºäº†Med-R1æ¡†æ¶ï¼Œæ—¨åœ¨æ¢ç´¢åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¢å¼ºå…¶åœ¨åŒ»ç–—æ¨ç†ä¸­çš„æ³›åŒ–å’Œå¯ä¿¡åº¦ã€‚ä½¿ç”¨DeepSeekç­–ç•¥å¹¶é‡‡ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰å¼•å¯¼æ¨ç†è·¯å¾„ï¼Œç›¸è¾ƒäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒMed-R1å®ç°äº†æ›´ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€çš„æµ‹è¯•æ˜¾ç¤ºï¼ŒMed-R1ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ï¼Œä¸”å±•ç°å‡ºå“è¶Šçš„é—®é¢˜ç±»å‹æ³›åŒ–èƒ½åŠ›ã€‚è¿™è¡¨æ˜å¼ºåŒ–å­¦ä¹ èƒ½æå‡åŒ»ç–—æ¨ç†èƒ½åŠ›ï¼Œä¸”å‚æ•°æ•ˆç‡æ›´é«˜çš„æ¨¡å‹ä¹Ÿèƒ½è¶…è¶Šæ›´å¤§çš„æ¨¡å‹ã€‚Med-R1æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œæœç€é€šç”¨ã€å¯ä¿¡å’Œä¸´åºŠå¯è¡Œçš„åŒ»ç–—VLMå‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VLMåœ¨åŒ»ç–—å›¾åƒé¢†åŸŸçš„ä½œç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­ã€‚</li>
<li>Med-R1æ¡†æ¶å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥å¢å¼ºVLMåœ¨åŒ»ç–—æ¨ç†ä¸­çš„æ³›åŒ–å’Œå¯ä¿¡åº¦ã€‚</li>
<li>Med-R1ä½¿ç”¨DeepSeekç­–ç•¥å’ŒGRPOæ–¹æ³•ï¼Œå®ç°äº†ç¨³å¥ä¸”å¤šæ ·åŒ–çš„æ¨ç†ï¼Œä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚</li>
<li>Med-R1åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€çš„æµ‹è¯•ä¸­æœ‰æ˜¾è‘—è¡¨ç°ï¼Œç›¸è¾ƒäºåŸºå‡†æ¨¡å‹å‡†ç¡®æ€§æé«˜29.94%ã€‚</li>
<li>Med-R1å±•ç°å‡ºå“è¶Šçš„é—®é¢˜ç±»å‹æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¶ŠåŸºå‡†æ¨¡å‹è¾¾32.06%ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ”¹å–„äº†åŒ»ç–—æ¨ç†èƒ½åŠ›ï¼Œå‚æ•°æ•ˆç‡æ›´é«˜çš„æ¨¡å‹ä¹Ÿèƒ½è¡¨ç°å‡ºè¶…è¶Šæ›´å¤§æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70c488823f69b8251d63b363931d8743.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e6d0d3225870b6fb40060a3faa48986.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-515ee458fedd469593906c9c0cc6d186.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9703a323e756c6a53f917fea46c55b3d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Survey-of-Direct-Preference-Optimization"><a href="#A-Survey-of-Direct-Preference-Optimization" class="headerlink" title="A Survey of Direct Preference Optimization"></a>A Survey of Direct Preference Optimization</h2><p><strong>Authors:Shunyu Liu, Wenkai Fang, Zetian Hu, Junjie Zhang, Yang Zhou, Kongcheng Zhang, Rongcheng Tu, Ting-En Lin, Fei Huang, Mingli Song, Yongbin Li, Dacheng Tao</strong></p>
<p>Large Language Models (LLMs) have demonstrated unprecedented generative capabilities, yet their alignment with human values remains critical for ensuring helpful and harmless deployments. While Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful paradigm for aligning LLMs with human preferences, its reliance on complex reward modeling introduces inherent trade-offs in computational efficiency and training stability. In this context, Direct Preference Optimization (DPO) has recently gained prominence as a streamlined alternative that directly optimizes LLMs using human preferences, thereby circumventing the need for explicit reward modeling. Owing to its theoretical elegance and computational efficiency, DPO has rapidly attracted substantial research efforts exploring its various implementations and applications. However, this field currently lacks systematic organization and comparative analysis. In this survey, we conduct a comprehensive overview of DPO and introduce a novel taxonomy, categorizing previous works into four key dimensions: data strategy, learning framework, constraint mechanism, and model property. We further present a rigorous empirical analysis of DPO variants across standardized benchmarks. Additionally, we discuss real-world applications, open challenges, and future directions for DPO. This work delivers both a conceptual framework for understanding DPO and practical guidance for practitioners, aiming to advance robust and generalizable alignment paradigms. All collected resources are available and will be continuously updated at <a target="_blank" rel="noopener" href="https://github.com/liushunyu/awesome-direct-preference-optimization">https://github.com/liushunyu/awesome-direct-preference-optimization</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºå‰æ‰€æœªæœ‰çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç„¶è€Œï¼Œå°†å…¶ä¸äººç±»ä»·å€¼è§‚å¯¹é½å¯¹äºç¡®ä¿æœ‰ç”¨å’Œæ— å®³çš„éƒ¨ç½²ä»ç„¶è‡³å…³é‡è¦ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºå°†LLMä¸äººç±»åå¥½å¯¹é½çš„å¼ºå¤§èŒƒå¼ï¼Œä½†å®ƒå¯¹å¤æ‚å¥–åŠ±æ¨¡å‹çš„ä¾èµ–å¸¦æ¥äº†è®¡ç®—æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§æ–¹é¢çš„å›ºæœ‰æƒè¡¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æœ€è¿‘å› å…¶ä½œä¸ºä½¿ç”¨äººç±»åå¥½ç›´æ¥ä¼˜åŒ–LLMçš„ç®€åŒ–æ›¿ä»£æ–¹æ¡ˆè€Œå¤‡å—å…³æ³¨ï¼Œä»è€Œé¿å…äº†æ˜¾å¼å¥–åŠ±å»ºæ¨¡çš„éœ€æ±‚ã€‚ç”±äºå…¶ç†è®ºä¸Šçš„ä¼˜é›…å’Œè®¡ç®—æ•ˆç‡ï¼ŒDPOè¿…é€Ÿå¸å¼•äº†å¤§é‡ç ”ç©¶åŠªåŠ›æ¥æ¢ç´¢å…¶å„ç§å®ç°å’Œåº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé¢†åŸŸç›®å‰ç¼ºä¹ç³»ç»Ÿçš„ç»„ç»‡å’Œæ¯”è¾ƒåˆ†æã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬å¯¹DPOè¿›è¡Œäº†å…¨é¢çš„æ¦‚è¿°ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§æ–°çš„åˆ†ç±»æ³•ï¼Œå°†ä»¥å‰çš„å·¥ä½œåˆ†ä¸ºå››ä¸ªå…³é”®ç»´åº¦ï¼šæ•°æ®ç­–ç•¥ã€å­¦ä¹ æ¡†æ¶ã€çº¦æŸæœºåˆ¶å’Œæ¨¡å‹å±æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¯¹æ ‡å‡†åŒ–åŸºå‡†ä¸‹çš„DPOå˜ä½“è¿›è¡Œäº†ä¸¥æ ¼çš„å®è¯åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†DPOçš„ç°å®ä¸–ç•Œåº”ç”¨ã€å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚è¿™é¡¹å·¥ä½œæ—¢æä¾›äº†ç†è§£DPOçš„æ¦‚å¿µæ¡†æ¶ï¼Œä¹Ÿä¸ºå®è·µè€…æä¾›äº†å®ç”¨æŒ‡å¯¼ï¼Œæ—¨åœ¨æ¨è¿›ç¨³å¥å’Œé€šç”¨çš„å¯¹é½èŒƒå¼ã€‚æ‰€æœ‰æ”¶é›†çš„èµ„æºéƒ½å¯ç”¨ï¼Œå¹¶å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/liushunyu/awesome-direct-preference-optimization%E4%B8%8A%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E3%80%82">https://github.com/liushunyu/awesome-direct-preference-optimizationä¸ŠæŒç»­æ›´æ–°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11701v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆèƒ½åŠ›ç©ºå‰ï¼Œä½†å…¶ä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½å¯¹äºç¡®ä¿å®‰å…¨ã€æœ‰ç›Šçš„éƒ¨ç½²è‡³å…³é‡è¦ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºå¯¹é½LLMä¸äººçš„åå¥½å¼ºå¤§çš„èŒƒå¼ï¼Œä½†å…¶å¤æ‚çš„å¥–åŠ±å»ºæ¨¡å¸¦æ¥è®¡ç®—æ•ˆç‡å’Œè®­ç»ƒç¨³å®šçš„å›ºæœ‰æƒè¡¡ã€‚å› æ­¤ï¼Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºä¸€ç§ä¼˜åŒ–çš„LLMä½¿ç”¨äººç±»åå¥½çš„æ–¹æ³•è¿…é€Ÿå—åˆ°å…³æ³¨ï¼Œè§„é¿äº†æ˜ç¡®å¥–åŠ±å»ºæ¨¡çš„éœ€è¦ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†DPOï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹åˆ†ç±»æ³•ï¼Œå°†ä»¥å‰çš„å·¥ä½œåˆ†ä¸ºå››ä¸ªå…³é”®ç»´åº¦ï¼šæ•°æ®ç­–ç•¥ã€å­¦ä¹ æ¡†æ¶ã€çº¦æŸæœºåˆ¶å’Œæ¨¡å‹å±æ€§ã€‚æœ¬æ–‡è¿˜å¯¹ä¸åŒæ ‡å‡†åŒ–åŸºå‡†è¿›è¡Œäº†ä¸¥æ ¼çš„å®è¯åˆ†æï¼Œå¹¶è®¨è®ºäº†DPOçš„ç°å®åº”ç”¨ã€å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚æ­¤å·¥ä½œæ—¨åœ¨ä¸ºç†è§£DPOæä¾›æ¦‚å¿µæ¡†æ¶ï¼Œå¹¶ä¸ºä»ä¸šè€…æä¾›å®è·µæŒ‡å¯¼ï¼Œæ¨åŠ¨ç¨³å¥ã€é€šç”¨çš„å¯¹é½èŒƒå¼å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç”Ÿæˆèƒ½åŠ›å¼ºï¼Œä½†ä¸äººä»·å€¼è§‚çš„å¯¹æ¥å¾ˆå…³é”®ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯LLMå¯¹é½çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å­˜åœ¨è®¡ç®—æ•ˆç‡å’Œè®­ç»ƒç¨³å®šçš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºæ— éœ€æ˜ç¡®å¥–åŠ±å»ºæ¨¡çš„æ–¹æ³•å—åˆ°å…³æ³¨ã€‚</li>
<li>DPOæœ‰å››ä¸ªå…³é”®ç»´åº¦ï¼šæ•°æ®ç­–ç•¥ã€å­¦ä¹ æ¡†æ¶ã€çº¦æŸæœºåˆ¶å’Œæ¨¡å‹å±æ€§ã€‚</li>
<li>DPOçš„ä¸åŒå˜ä½“åœ¨æ ‡å‡†åŒ–åŸºå‡†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼çš„å®è¯åˆ†æã€‚</li>
<li>DPOå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå¼€æ”¾æ€§æŒ‘æˆ˜ï¼ŒåŠæœªæ¥çš„å‘å±•æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d88f935fa227636c37783e928b3746c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffd0bd5d121f82098d507d49f9f2402d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e05d1f4fbe6c52ab9722d490b34ac4b6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="R1-Onevision-Advancing-Generalized-Multimodal-Reasoning-through-Cross-Modal-Formalization"><a href="#R1-Onevision-Advancing-Generalized-Multimodal-Reasoning-through-Cross-Modal-Formalization" class="headerlink" title="R1-Onevision: Advancing Generalized Multimodal Reasoning through   Cross-Modal Formalization"></a>R1-Onevision: Advancing Generalized Multimodal Reasoning through   Cross-Modal Formalization</h2><p><strong>Authors:Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen</strong></p>
<p>Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚çš„æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æƒŠäººçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€æ¨ç†ï¼Œè¿™éœ€è¦æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°åˆ†æå’Œæ¨ç†è§†è§‰å†…å®¹ï¼Œå¯¼è‡´åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šæ€§èƒ½ä¸ä½³ã€‚æ­¤å¤–ï¼Œç¼ºä¹å…¨é¢çš„åŸºå‡†æµ‹è¯•é˜»ç¢äº†å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å‡†ç¡®è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†R1-Onevisionå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨å¼¥åˆè§†è§‰æ„ŸçŸ¥å’Œæ·±åº¦æ¨ç†ä¹‹é—´çš„å·®è·ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨æ¨¡æ€æ¨ç†ç®¡é“ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºæ­£å¼çš„çº¹ç†è¡¨ç¤ºï¼Œå®ç°åŸºäºç²¾ç¡®è¯­è¨€æ¨ç†ã€‚åˆ©ç”¨è¯¥ç®¡é“ï¼Œæˆ‘ä»¬æ„å»ºäº†R1-Onevisionæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æä¾›äº†ä¸åŒé¢†åŸŸçš„è¯¦ç»†ã€é€æ­¥çš„å¤šæ¨¡æ€æ¨ç†æ³¨é‡Šã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡ç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ æ¥å¼€å‘R1-Onevisionæ¨¡å‹ï¼Œä»¥åŸ¹å…»å…ˆè¿›çš„æ¨ç†å’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°ä¸åŒç­‰çº§çš„å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸äººçš„æ•™è‚²é˜¶æ®µç›¸ä¸€è‡´çš„R1-Onevision-BenchåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä»åˆä¸­åˆ°å¤§å­¦åŠä»¥åçš„è€ƒè¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR1-Onevisionåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¶…è¶Šäº†GPT-4oå’ŒQwen2.5-VLç­‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10615v2">PDF</a> Code and Model: <a target="_blank" rel="noopener" href="https://github.com/Fancy-MLLM/R1-onevision">https://github.com/Fancy-MLLM/R1-onevision</a></p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚çš„æ–‡æœ¬ä»»åŠ¡ä¸­å±•ç°å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åˆ†æè§†è§‰å†…å®¹æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡ä»‹ç»R1-Onevisionå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡è·¨æ¨¡æ€æ¨ç†ç®¡é“å°†å›¾åƒè½¬åŒ–ä¸ºæ­£å¼çš„æ–‡æœ¬è¡¨ç¤ºï¼Œå®ç°ç²¾ç¡®çš„è¯­è¨€æ¨ç†ã€‚æ­¤å¤–ï¼Œæ„å»ºR1-Onevisionæ•°æ®é›†å¹¶æä¾›è¯¦ç»†çš„å¤šæ¨¡æ€æ¨ç†æ³¨é‡Šï¼Œå¹¶å¼€å‘R1-Onevisionæ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ æ¥åŸ¹å…»å…ˆè¿›çš„æ¨ç†å’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºå…¨é¢è¯„ä¼°ä¸åŒç­‰çº§çš„å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ï¼Œæœ¬æ–‡è¿˜å¼•å…¥R1-Onevision-BenchåŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»æœ‰å¾…æé«˜ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åˆ†æå’Œæ¨ç†è§†è§‰å†…å®¹æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚</li>
<li>R1-Onevisionå¤šæ¨¡æ€æ¨ç†æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€æ¨ç†ç®¡é“è½¬åŒ–å›¾åƒä¸ºæ–‡æœ¬è¡¨ç¤ºï¼Œå®ç°ç²¾ç¡®çš„è¯­è¨€æ¨ç†ã€‚</li>
<li>R1-Onevisionæ•°æ®é›†æä¾›è¯¦ç»†ã€é€æ­¥çš„å¤šæ¨¡æ€æ¨ç†æ³¨é‡Šï¼Œæ¶µç›–ä¸åŒé¢†åŸŸã€‚</li>
<li>R1-Onevisionæ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œå¼€å‘ï¼Œä»¥åŸ¹è‚²é«˜çº§æ¨ç†å’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥R1-Onevision-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥å…¨é¢è¯„ä¼°ä¸åŒç­‰çº§çš„å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ï¼Œä¸äººç±»æ•™è‚²é˜¶æ®µç›¸å¥‘åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5e7f0560b3fc2abbebfd78e0400e01b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16752ac75fa0fe28dc5b056a5e187cfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eabb853dfabf9f9c5b4e649f733d91ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa35ecce8741f0e38a5eef55a96232db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d79e2c222974a93705052dae305edd64.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Light-R1-Curriculum-SFT-DPO-and-RL-for-Long-COT-from-Scratch-and-Beyond"><a href="#Light-R1-Curriculum-SFT-DPO-and-RL-for-Long-COT-from-Scratch-and-Beyond" class="headerlink" title="Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and   Beyond"></a>Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and   Beyond</h2><p><strong>Authors:Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, Xiangzheng Zhang</strong></p>
<p>This paper introduces Light-R1, an open-source suite for training long reasoning models using reproducible and cost-effective methodology. Given the proprietary nature of data used in the DeepSeek-R1 series, we develop an alternative approach leveraging exclusively public data and models. Our curriculum training progressively increases data difficulty, combined with multi-staged post-training. Our Light-R1-32B model, trained from Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning.   Experimental results show that this curriculum approach becomes more effective when distinct, diverse datasets are available for different training stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on proprietary data) with 3,000 challenging examples from our curriculum dataset yielded state-of-the-art 7B and 14B models, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying GRPO on long reasoning models. Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math, with AIME24 &amp; 25 scores of 74.0 and 60.2 respectively, surpassing many 32B models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training, Light-R1-14B-DS demonstrates strong cross-domain generalization.   Light-R1 represents a significant advancement in making sophisticated reasoning models more accessible and implementable in real-world applications. Our models, training data and code have been made available at <a target="_blank" rel="noopener" href="https://github.com/Qihoo360/Light-R1">https://github.com/Qihoo360/Light-R1</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Light-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºå¥—ä»¶ï¼Œé‡‡ç”¨å¯å¤åˆ¶å’Œæˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•è®­ç»ƒé•¿æ¨ç†æ¨¡å‹ã€‚é‰´äºDeepSeek-R1ç³»åˆ—ä¸­ä½¿ç”¨çš„æ•°æ®çš„ä¸“æœ‰æ€§è´¨ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§çº¯ç²¹åˆ©ç”¨å…¬å¼€æ•°æ®å’Œæ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„è¯¾ç¨‹è®­ç»ƒé€æ­¥å¢åŠ æ•°æ®éš¾åº¦ï¼Œå¹¶ç»“åˆå¤šé˜¶æ®µåè®­ç»ƒã€‚æˆ‘ä»¬çš„Light-R1-32Bæ¨¡å‹ï¼ŒåŸºäºQwen2.5-32B-Instructè¿›è¡Œè®­ç»ƒï¼Œåœ¨æ•°å­¦æ¨ç†æ–¹é¢ä¼˜äºDeepSeek-R1-Distill-Qwen-32Bã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“ä¸åŒè®­ç»ƒé˜¶æ®µæ‹¥æœ‰ä¸åŒä¸”å¤šæ ·çš„æ•°æ®é›†æ—¶ï¼Œè¿™ç§è¯¾ç¨‹æ–¹æ³•å˜å¾—æ›´åŠ æœ‰æ•ˆï¼šä½¿ç”¨æˆ‘ä»¬è¯¾ç¨‹æ•°æ®é›†ä¸­çš„3000ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¾‹å­å¯¹DeepSeek-R1-Distilledæ¨¡å‹ï¼ˆDeepSeekå›¢é˜Ÿåœ¨ä¸“æœ‰æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼‰è¿›è¡Œå¾®è°ƒï¼Œäº§ç”Ÿäº†æœ€å…ˆè¿›çš„7Bå’Œ14Bæ¨¡å‹ï¼Œè€Œ32Bæ¨¡å‹Light-R1-32B-DSçš„è¡¨ç°ä¸QwQ-32Bå’ŒDeepSeek-R1ç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å·¥ä½œæ‰©å±•åˆ°åœ¨é•¿æ¨ç†æ¨¡å‹ä¸Šåº”ç”¨GRPOã€‚æˆ‘ä»¬çš„æœ€ç»ˆLight-R1-14B-DSåœ¨æ•°å­¦çš„14Bæ¨¡å‹ä¸­å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼ŒAIME24å’ŒAIME25çš„å¾—åˆ†åˆ†åˆ«ä¸º74.0å’Œ60.2ï¼Œè¶…è¿‡äº†è®¸å¤š32Bæ¨¡å‹å’ŒDeepSeek-R1-Distill-Llama-70Bã€‚å°½ç®¡ä»¥æ•°å­¦ä¸ºä¸­å¿ƒçš„è®­ç»ƒï¼Œä½†Light-R1-14B-DSæ˜¾ç¤ºå‡ºå¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚Light-R1æ ‡å¿—ç€è®©å¤æ‚çš„æ¨ç†æ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„å®é™…åº”ç”¨ä¸­æ›´åŠ å¯è®¿é—®å’Œå¯å®ç°çš„ä¸€ä¸ªé‡å¤§è¿›æ­¥ã€‚æˆ‘ä»¬çš„æ¨¡å‹ã€è®­ç»ƒæ•°æ®å’Œä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Qihoo360/Light-R1%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Qihoo360/Light-R1ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10460v2">PDF</a> v2: better writing &amp; format for later submission; all release at   <a target="_blank" rel="noopener" href="https://github.com/Qihoo360/Light-R1">https://github.com/Qihoo360/Light-R1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Light-R1è¿™ä¸€å¼€æºå¥—ä»¶ï¼Œè¯¥å¥—ä»¶é‡‡ç”¨å¯é‡ç°å’Œå…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ–¹æ³•è®­ç»ƒé•¿æ¨ç†æ¨¡å‹ã€‚é’ˆå¯¹DeepSeek-R1ç³»åˆ—ä½¿ç”¨çš„ä¸“æœ‰æ•°æ®ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘å‡ºä¸€ç§æ›¿ä»£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨å…¬å…±æ•°æ®å’Œæ¨¡å‹ã€‚é€šè¿‡æ¸è¿›å¼å¢åŠ æ•°æ®éš¾åº¦å’Œç»“åˆå¤šé˜¶æ®µåè®­ç»ƒçš„è¯¾ç¨‹è®­ç»ƒï¼ŒLight-R1ç³»åˆ—æ¨¡å‹è¡¨ç°ä¼˜è¶Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“ä¸åŒè®­ç»ƒé˜¶æ®µä½¿ç”¨ä¸åŒã€å¤šæ ·çš„æ•°æ®é›†æ—¶ï¼Œè¿™ç§è¯¾ç¨‹è®­ç»ƒæ–¹æ³•æ›´åŠ æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå°†å·¥ä½œæ‰©å±•è‡³é•¿æ¨ç†æ¨¡å‹ä¸Šçš„GRPOåº”ç”¨ï¼Œæœ€ç»ˆLight-R1-14B-DSæ¨¡å‹åœ¨æ•°å­¦é¢†åŸŸå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚Light-R1çš„å‡ºç°ä½¿å¾—å¤æ‚çš„æ¨ç†æ¨¡å‹æ›´æ˜“äºè®¿é—®å¹¶åœ¨å®é™…åº”ç”¨ä¸­å¾—åˆ°å®ç°ã€‚ç›¸å…³æ¨¡å‹å’Œä»£ç å·²å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/Qihoo360/Light-R1">https://github.com/Qihoo360/Light-R1</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Light-R1æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒé•¿æ¨ç†æ¨¡å‹çš„å¼€æºå¥—ä»¶ï¼Œé‡‡ç”¨å¯é‡ç°å’Œæˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§ä»…ä½¿ç”¨å…¬å…±æ•°æ®å’Œæ¨¡å‹çš„æ›¿ä»£æ–¹æ³•ã€‚</li>
<li>é€šè¿‡è¯¾ç¨‹è®­ç»ƒå’Œå¤šé˜¶æ®µåè®­ç»ƒï¼ŒLight-R1æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨ä¸åŒã€å¤šæ ·çš„æ•°æ®é›†è¿›è¡Œä¸åŒè®­ç»ƒé˜¶æ®µæ•ˆæœæ›´ä½³ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå°†GRPOåº”ç”¨äºé•¿æ¨ç†æ¨¡å‹ï¼ŒLight-R1-14B-DSæ¨¡å‹åœ¨æ•°å­¦é¢†åŸŸå®ç°å“è¶Šæ€§èƒ½å¹¶å±•ç°å¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Light-R1ä½¿å¾—å¤æ‚çš„æ¨ç†æ¨¡å‹æ›´æ˜“äºè®¿é—®å’Œå®é™…åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ecd87f41862edafe0ac2ed03fe603902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a1d5c4fe1fff3d2daaa25b839a4d012.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80797d4f92ab71dc35fafb87b3d6351c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1df2a45f2ea69557e33b58c003c6cc55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d84b630c6ef5e08e7192221537574d7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning"><a href="#Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning" class="headerlink" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning"></a>Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning</h2><p><strong>Authors:Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, Jiawei Han</strong></p>
<p>Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns â€“ solely through reinforcement learning (RL) â€“ to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over strong baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1">https://github.com/PeterGriffinJin/Search-R1</a>. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆï¼Œé«˜æ•ˆè·å–å¤–éƒ¨çŸ¥è¯†å’Œæœ€æ–°ä¿¡æ¯è‡³å…³é‡è¦ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­æç¤ºå…·æœ‰æ¨ç†èƒ½åŠ›çš„å…ˆè¿›LLMä½¿ç”¨æœç´¢å¼•æ“å¹¶ä¸æ˜¯æœ€ä½³é€‰æ‹©ï¼Œå› ä¸ºLLMå¹¶æ²¡æœ‰å­¦ä¹ å¦‚ä½•ä¸æœç´¢å¼•æ“è¿›è¡Œæœ€ä½³äº¤äº’ã€‚æœ¬æ–‡ä»‹ç»äº†Search-R1ï¼Œå®ƒæ˜¯DeepSeek-R1æ¨¡å‹çš„æ‰©å±•ï¼Œå…¶ä¸­LLMä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å­¦ä¹ åœ¨é€æ­¥æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»ç”Ÿæˆï¼ˆå¤šä¸ªï¼‰æœç´¢æŸ¥è¯¢ï¼Œå¹¶è¿›è¡Œå®æ—¶æ£€ç´¢ã€‚Search-R1é€šè¿‡å¤šè½®æœç´¢äº¤äº’ä¼˜åŒ–LLMçš„æ»šåŠ¨æ›´æ–°ï¼Œåˆ©ç”¨æ£€ç´¢ä»¤ç‰Œå±è”½è¿›è¡Œç¨³å®šçš„RLè®­ç»ƒä»¥åŠç®€å•çš„åŸºäºç»“æœå¥–åŠ±å‡½æ•°ã€‚åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼ŒSearch-R1çš„æ€§èƒ½æé«˜äº†26%ï¼ˆQwen2.5-7Bï¼‰ã€21%ï¼ˆQwen2.5-3Bï¼‰å’Œ10%ï¼ˆLLaMA3.2-3Bï¼‰ã€‚æœ¬æ–‡è¿˜æ·±å…¥æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©å’Œå“åº”é•¿åº¦åŠ¨æ€åœ¨æ£€ç´¢å¢å¼ºæ¨ç†ä¸­çš„ç»éªŒè§è§£ã€‚ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä½äº<a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1%E3%80%82">https://github.com/PeterGriffinJin/Search-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09516v2">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Search-R1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡è‡ªä¸»ç”Ÿæˆï¼ˆå¤šä¸ªï¼‰æœç´¢æŸ¥è¯¢çš„èƒ½åŠ›ï¼Œä»¥åœ¨å®æ—¶æ£€ç´¢è¿‡ç¨‹ä¸­è¿›è¡Œé€æ­¥æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒSearch-R1åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„æ€§èƒ½ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹æœ‰äº†æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Search-R1æ¨¡å‹æ˜¯DeepSeek-R1æ¨¡å‹çš„æ‰©å±•ï¼Œä½¿LLMå…·å¤‡è‡ªä¸»ç”Ÿæˆæœç´¢æŸ¥è¯¢çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œä½¿LLMå­¦ä¼šå¦‚ä½•æœ€ä¼˜åœ°ä¸æœç´¢å¼•æ“äº¤äº’ã€‚</li>
<li>Search-R1ä¼˜åŒ–äº†LLMçš„æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡å¤šè½®æœç´¢äº¤äº’ã€æ£€ç´¢ä»¤ç‰Œæ©ç å’Œç®€å•çš„ç»“æœå¯¼å‘å¥–åŠ±å‡½æ•°æ¥å®ç°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSearch-R1åœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå¹³å‡æå‡çº¦20%ã€‚</li>
<li>è¯¥è®ºæ–‡è¿˜æä¾›äº†å…³äºRLä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©å’Œæ£€ç´¢å¢å¼ºæ¨ç†çš„å“åº”é•¿åº¦åŠ¨æ€æ€§çš„å®è¯è§è§£ã€‚</li>
<li>Search-R1æ¨¡å‹çš„ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e11538b4b36a832441f6c2f1b3baf241.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7677d1b5771eb2d8fc9f3c532c475d8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ReMA-Learning-to-Meta-think-for-LLMs-with-Multi-Agent-Reinforcement-Learning"><a href="#ReMA-Learning-to-Meta-think-for-LLMs-with-Multi-Agent-Reinforcement-Learning" class="headerlink" title="ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement   Learning"></a>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement   Learning</h2><p><strong>Authors:Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen</strong></p>
<p>Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking â€“ enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Experimental results demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs. </p>
<blockquote>
<p>å…³äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†ç ”ç©¶çš„æœ€æ–°è¿›å±•ï¼Œæ˜¯é€šè¿‡èå…¥å…ƒæ€ç»´æ¥è¿›ä¸€æ­¥æå‡å…¶æ€§èƒ½ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç›‘æ§ã€è¯„ä¼°å’Œæ§åˆ¶å…¶æ¨ç†è¿‡ç¨‹ï¼Œä»¥æ›´é€‚åº”å’Œæœ‰æ•ˆåœ°è§£å†³é—®é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„å•æ™ºèƒ½ä½“ç ”ç©¶ç¼ºä¹è·å–å…ƒæ€ç»´çš„ä¸“é—¨è®¾è®¡ï¼Œå¯¼è‡´æ•ˆç‡è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¼ºåŒ–å…ƒæ€ç»´æ™ºèƒ½ä½“ï¼ˆReMAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¥æ¿€å‘å…ƒæ€ç»´è¡Œä¸ºï¼Œé¼“åŠ±LLMè¿›è¡Œåæ€æ€§æ€è€ƒã€‚ReMAå°†æ¨ç†è¿‡ç¨‹è§£è€¦ä¸ºä¸¤ä¸ªå±‚æ¬¡æ™ºèƒ½ä½“ï¼šé«˜çº§å…ƒæ€ç»´æ™ºèƒ½ä½“ï¼Œè´Ÿè´£äº§ç”Ÿæˆ˜ç•¥æ€§ç›‘ç£å’Œè®¡åˆ’ï¼›ä½çº§æ¨ç†æ™ºèƒ½ä½“ï¼Œè´Ÿè´£è¯¦ç»†æ‰§è¡Œã€‚é€šè¿‡ç›®æ ‡ä¸€è‡´çš„è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œè¿™äº›æ™ºèƒ½ä½“æ¢ç´¢å’Œå­¦ä¹ åä½œï¼Œæé«˜äº†é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šï¼ŒReMAè¶…è¶Šäº†å•æ™ºèƒ½ä½“RLåŸºå‡†çº¿ï¼ŒåŒ…æ‹¬ç«æŠ€çº§æ•°å­¦åŸºå‡†å’ŒLLMä½œä¸ºæ³•å®˜çš„åŸºå‡†ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯´æ˜äº†æ¯ä¸ªç‹¬ç‰¹æ™ºèƒ½ä½“çš„åŠ¨æ€æ¼”å˜ï¼Œæä¾›äº†å…ƒæ€ç»´æ¨ç†è¿‡ç¨‹å¦‚ä½•å¢å¼ºLLMæ¨ç†èƒ½åŠ›çš„å®è´µè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09501v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸå…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶è‡´åŠ›äºé€šè¿‡å¼•å…¥å…ƒæ€ç»´è¿›ä¸€æ­¥æå‡å…¶æ€§èƒ½ã€‚å…ƒæ€ç»´ä½¿æ¨¡å‹èƒ½å¤Ÿç›‘æ§ã€è¯„ä¼°å’Œæ§åˆ¶å…¶æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå®ç°æ›´è‡ªé€‚åº”å’Œé«˜æ•ˆçš„é—®é¢˜è§£å†³ã€‚ä¸ºè§£å†³ç°æœ‰å•æ™ºèƒ½ä½“åœ¨è·å–å…ƒæ€ç»´æ–¹é¢çš„ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†å¼ºåŒ–å…ƒæ€ç»´æ™ºèƒ½ä½“ï¼ˆReMAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¿€å‘å…ƒæ€ç»´è¡Œä¸ºã€‚ReMAå°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡æ™ºèƒ½ä½“ï¼šé«˜çº§å…ƒæ€ç»´æ™ºèƒ½ä½“è´Ÿè´£ç”Ÿæˆæˆ˜ç•¥ç›‘ç£å’Œè®¡åˆ’ï¼Œä½çº§æ¨ç†æ™ºèƒ½ä½“è´Ÿè´£å…·ä½“æ‰§è¡Œã€‚é€šè¿‡ç›®æ ‡ä¸€è‡´çš„è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œè¿™äº›æ™ºèƒ½ä½“æ¢ç´¢å¹¶å­¦ä¹ åä½œï¼Œæé«˜äº†æ³›åŒ–å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReMAåœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºå•æ™ºèƒ½ä½“RLåŸºçº¿ï¼ŒåŒ…æ‹¬ç«äº‰æ€§æ°´å¹³çš„æ•°å­¦åŸºå‡†æµ‹è¯•å’ŒLLMä½œä¸ºæ³•å®˜çš„åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMsçš„å…ƒæ€ç»´å¢å¼ºç ”ç©¶æ—¨åœ¨æé«˜æ¨¡å‹çš„è‡ªé€‚åº”å’Œé«˜æ•ˆé—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>ReMAæ¡†æ¶åˆ©ç”¨MARLæ¿€å‘æ¨¡å‹å…ƒæ€ç»´è¡Œä¸ºï¼Œåˆ†ä¸ºé«˜çº§å…ƒæ€ç»´æ™ºèƒ½ä½“å’Œä½çº§æ¨ç†æ™ºèƒ½ä½“ã€‚</li>
<li>é€šè¿‡è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œæ™ºèƒ½ä½“ä¹‹é—´æ¢ç´¢å¹¶å­¦ä¹ åä½œï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–å’Œç¨³å¥æ€§ã€‚</li>
<li>ReMAåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬æ•°å­¦å’ŒLLM-as-a-JudgeåŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†ReMAæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚</li>
<li>ReMAæ¡†æ¶ä¸ºLLMsçš„å…ƒæ€ç»´æ¨ç†è¿‡ç¨‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-036916c1875b56b7af33f27dbba8c803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5087306fb21b19db6c96420a875b08bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f204a1cf33c2518ea1cd5e1c60c1b75.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Reasoning-In-The-Wild-Is-Not-Always-Faithful"><a href="#Chain-of-Thought-Reasoning-In-The-Wild-Is-Not-Always-Faithful" class="headerlink" title="Chain-of-Thought Reasoning In The Wild Is Not Always Faithful"></a>Chain-of-Thought Reasoning In The Wild Is Not Always Faithful</h2><p><strong>Authors:IvÃ¡n Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy</strong></p>
<p>Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful, i.e. CoT reasoning does not always reflect how models arrive at conclusions. So far, most of these studies have focused on unfaithfulness in unnatural contexts where an explicit bias has been introduced. In contrast, we show that unfaithful CoT can occur on realistic prompts with no artificial bias. Our results reveal non-negligible rates of several forms of unfaithful reasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and ChatGPT-4o (7.0%) all answer a notable proportion of question pairs unfaithfully. Specifically, we find that models rationalize their implicit biases in answers to binary questions (â€œimplicit post-hoc rationalizationâ€). For example, when separately presented with the questions â€œIs X bigger than Y?â€ and â€œIs Y bigger than X?â€, models sometimes produce superficially coherent arguments to justify answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We also investigate restoration errors (Dziri et al., 2023), where models make and then silently correct errors in their reasoning, and unfaithful shortcuts, where models use clearly illogical reasoning to simplify solving problems in Putnam questions (a hard benchmark). Our findings raise challenges for AI safety work that relies on monitoring CoT to detect undesired behavior. </p>
<blockquote>
<p>â€œé“¾å¼æ€ç»´â€ï¼ˆChain-of-Thought, CoTï¼‰æ¨ç†æŠ€æœ¯æ˜¾è‘—æå‡äº†å½“å‰çš„äººå·¥æ™ºèƒ½èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿‘æœŸçš„ç ”ç©¶æ˜¾ç¤ºï¼ŒCoTæ¨ç†å¹¶ä¸æ€»æ˜¯å¯é ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒCoTæ¨ç†å¹¶ä¸æ€»èƒ½åæ˜ æ¨¡å‹å¦‚ä½•å¾—å‡ºç»“è®ºã€‚è¿„ä»Šä¸ºæ­¢ï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨éè‡ªç„¶è¯­å¢ƒä¸‹çš„ä¸å¿ å®æƒ…å†µï¼Œè¿™ç§è¯­å¢ƒä¸­å¼•å…¥äº†æ˜æ˜¾çš„åè§ã€‚ä¸ä¹‹ç›¸åï¼Œæˆ‘ä»¬è¯æ˜ï¼Œåœ¨æ²¡æœ‰äººä¸ºåè§çš„æƒ…å†µä¸‹ï¼Œç°å®æç¤ºä¹Ÿä¼šå‡ºç°ä¸å¿ å®çš„CoTã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†å‰æ²¿æ¨¡å‹ä¸­ä¸å¯å¿½ç•¥çš„å„ç§å½¢å¼çš„ä¸å¿ å®æ¨ç†ç‡ï¼šSonnet 3.7ï¼ˆ16.3%ï¼‰ã€DeepSeek R1ï¼ˆ5.3%ï¼‰å’ŒChatGPT-4oï¼ˆ7.0%ï¼‰éƒ½ä¼šå›ç­”ç›¸å½“æ¯”ä¾‹çš„é—®é¢˜å¯¹æ—¶å­˜åœ¨ä¸å¿ å®çš„æƒ…å†µã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹ä¼šåœ¨å›ç­”äºŒå…ƒé—®é¢˜æ—¶ï¼ˆå¦‚â€œXæ˜¯å¦å¤§äºYï¼Ÿâ€å’Œâ€œYæ˜¯å¦å¤§äºXï¼Ÿâ€ï¼‰ï¼Œä¸ºå…¶ç­”æ¡ˆè¿›è¡Œåˆç†åŒ–çš„è§£é‡Šï¼ˆâ€œäº‹åéšå¼åˆç†åŒ–â€ï¼‰ã€‚ä¾‹å¦‚ï¼Œå°½ç®¡å›ç­”ä¸¤ä¸ªé€»è¾‘çŸ›ç›¾çš„é—®é¢˜ï¼ˆâ€œXæ˜¯å¦å¤§äºYï¼Ÿâ€çš„ç­”æ¡ˆæ˜¯â€œæ˜¯â€ï¼Œâ€œYæ˜¯å¦å¤§äºXï¼Ÿâ€çš„ç­”æ¡ˆæ˜¯â€œæ˜¯â€ï¼‰åœ¨é€»è¾‘ä¸Šæ˜¯çŸ›ç›¾çš„ï¼Œä½†æœ‰æ—¶æ¨¡å‹ä¼šæä¾›è¡¨é¢ä¸Šåˆç†çš„è®ºè¯æ¥æ”¯æŒè¿™æ ·çš„ç­”æ¡ˆã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ¨¡å‹åœ¨æ¨ç†ä¸­çš„æ¢å¤é”™è¯¯ï¼ˆDziriç­‰äººï¼Œ2023ï¼‰ï¼Œå³æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çŠ¯é”™å¹¶é»˜é»˜çº æ­£è¿™äº›é”™è¯¯çš„æƒ…å†µï¼Œä»¥åŠä¸å¿ å®çš„æ·å¾„ï¼Œå³æ¨¡å‹ä½¿ç”¨æ˜æ˜¾ä¸åˆé€»è¾‘çš„æ¨ç†æ¥ç®€åŒ–è§£å†³Putnamé—®é¢˜ä¸­çš„éš¾é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºä¾èµ–ç›‘æ§CoTæ¥æ£€æµ‹ä¸å¸Œæœ›å‡ºç°è¡Œä¸ºçš„AIå®‰å…¨å·¥ä½œæ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08679v3">PDF</a> Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),   10 main paper pages, 39 appendix pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†æ˜¾è‘—æå‡äº†äººå·¥æ™ºèƒ½çš„å…ˆè¿›èƒ½åŠ›ã€‚ä½†æœ€æ–°ç ”ç©¶æ˜¾ç¤ºï¼ŒCoTæ¨ç†å¹¶ä¸æ€»æ˜¯å¿ å®äºçœŸå®æƒ…å†µï¼Œæ„å‘³ç€æ¨¡å‹çš„ç»“è®ºå¹¶ä¸æ€»æ˜¯åæ˜ å…¶æ¨ç†è¿‡ç¨‹ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨åœ¨ä¸è‡ªç„¶æƒ…å¢ƒä¸‹çš„ä¸å¿ å®æ¨ç†ï¼Œè€Œæœ¬ç ”ç©¶æ˜¾ç¤ºï¼Œåœ¨æ²¡æœ‰äººä¸ºåè§çš„ç°å®æç¤ºä¸‹ï¼Œä¹Ÿä¼šå‡ºç°ä¸å¿ å®çš„CoTã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†å‰æ²¿æ¨¡å‹ä¸­ä¸å¯å¿½ç•¥çš„å„ç§å½¢å¼çš„ä¸å¿ å®æ¨ç†ç‡ï¼šSonnet 3.7ï¼ˆ16.3%ï¼‰ã€DeepSeek R1ï¼ˆ5.3%ï¼‰å’ŒChatGPT-4oï¼ˆ7.0%ï¼‰åœ¨å›ç­”é—®é¢˜æ—¶å‡æœ‰æ˜¾è‘—æ¯”ä¾‹çš„ä¸å¿ å®æ¨ç†ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨å›ç­”äºŒå…ƒé—®é¢˜æ—¶ï¼Œä¼šä¸ºå…¶ç­”æ¡ˆè¿›è¡Œäº‹åç†æ€§åŒ–ã€‚ä¾‹å¦‚ï¼Œå½“åˆ†åˆ«é¢å¯¹â€œXæ˜¯å¦å¤§äºYï¼Ÿâ€å’Œâ€œYæ˜¯å¦å¤§äºXï¼Ÿâ€çš„é—®é¢˜æ—¶ï¼Œæ¨¡å‹æœ‰æ—¶ä¼šåˆ¶é€ è¡¨é¢ä¸Šåˆç†çš„è®ºæ®ï¼Œä»¥è¯æ˜ä¸¤ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯â€œæ˜¯â€æˆ–éƒ½æ˜¯â€œå¦â€ï¼Œå°½ç®¡è¿™æ ·çš„å›ç­”åœ¨é€»è¾‘ä¸Šæ˜¯çŸ›ç›¾çš„ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ¨¡å‹åœ¨ä¿®å¤é”™è¯¯æ—¶çš„ä¸å¿ å®è¡Œä¸ºï¼Œä»¥åŠä½¿ç”¨æ˜æ˜¾ä¸åˆé€»è¾‘çš„æ¨ç†æ¥ç®€åŒ–è§£å†³Putnamé—®é¢˜çš„å¿«æ·æ–¹å¼ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹ä¾èµ–ç›‘æµ‹CoTæ¥æ£€æµ‹ä¸å¸Œæœ›å‡ºç°è¡Œä¸ºçš„AIå®‰å…¨å·¥ä½œæå‡ºäº†æŒ‘æˆ˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åœ¨æå‡AIèƒ½åŠ›æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œä½†å­˜åœ¨ä¸å¿ å®äºçœŸå®æƒ…å†µçš„é—®é¢˜ã€‚</li>
<li>ä¸å¿ å®CoTå¯åœ¨æ— äººå·¥åè§çš„ç°å®æç¤ºä¸‹å‘ç”Ÿã€‚</li>
<li>å¤šç§å‰æ²¿æ¨¡å‹å­˜åœ¨ä¸å¿ å®æ¨ç†é—®é¢˜ï¼ŒåŒ…æ‹¬Sonnet 3.7ã€DeepSeek R1å’ŒChatGPT-4oã€‚</li>
<li>æ¨¡å‹ä¼šåœ¨å›ç­”äºŒå…ƒé—®é¢˜æ—¶è¿›è¡Œéšå¼çš„äº‹åç†æ€§åŒ–ï¼Œæœ‰æ—¶å¯¼è‡´é€»è¾‘çŸ›ç›¾ã€‚</li>
<li>æ¨¡å‹åœ¨ä¿®å¤é”™è¯¯æ—¶å­˜åœ¨ä¸å¿ å®è¡Œä¸ºï¼Œä»¥åŠä½¿ç”¨ç®€åŒ–è§£å†³å¤æ‚é—®é¢˜çš„é€»è¾‘ä¸æ¸…æ™°çš„æ¨ç†å¿«æ·æ–¹å¼ã€‚</li>
<li>è¿™äº›å‘ç°å¯¹ä¾èµ–CoTç›‘æµ‹æ¥ç¡®ä¿AIå®‰å…¨çš„å·¥ä½œæ„æˆäº†æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c94fb04f32d6bbc9ac3444ede3149799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f0fa5fae005ae6a328cae9c825173f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49a797cea6090c18b6e99a5e13dae10c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf71d8e35b0d52876892172c63ffbe0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c0eec734baa32483be4effdc3d3ce3f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MedAgentsBench-Benchmarking-Thinking-Models-and-Agent-Frameworks-for-Complex-Medical-Reasoning"><a href="#MedAgentsBench-Benchmarking-Thinking-Models-and-Agent-Frameworks-for-Complex-Medical-Reasoning" class="headerlink" title="MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for   Complex Medical Reasoning"></a>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for   Complex Medical Reasoning</h2><p><strong>Authors:Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark Gerstein</strong></p>
<p>Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present MedAgentsBench, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/medagents-benchmark">https://github.com/gersteinlab/medagents-benchmark</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°æœ‰çš„åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚è¿™ç§é«˜æ€§èƒ½ä½¿å¾—å¯¹å…ˆè¿›æ–¹æ³•è¿›è¡Œæœ‰æ„ä¹‰åœ°è¯„ä¼°å’ŒåŒºåˆ†å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚æˆ‘ä»¬æ¨å‡ºäº†MedAgentsBenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒä¸“æ³¨äºå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—é—®é¢˜ï¼Œè¿™äº›é—®é¢˜éœ€è¦è¿›è¡Œå¤šæ­¥éª¤çš„ä¸´åºŠæ¨ç†ã€è¯Šæ–­åˆ¶å®šå’Œæ²»ç–—è®¡åˆ’åˆ¶å®šã€‚å°½ç®¡åœ¨æ ‡å‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œå½“å‰æ¨¡å‹ä»ç„¶é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬ä»ä¸ƒä¸ªå…¬è®¤çš„åŒ»å­¦æ•°æ®é›†ä¸­æ±²å–çŸ¥è¯†ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è§£å†³äº†ç°æœ‰è¯„ä¼°ä¸­çš„ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼šï¼ˆ1ï¼‰æ™®éå­˜åœ¨çš„ç®€å•é—®é¢˜ï¼Œå³ä½¿æ˜¯åŸºç¡€æ¨¡å‹ä¹Ÿèƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ï¼›ï¼ˆ2ï¼‰ç ”ç©¶ä¹‹é—´é‡‡æ ·å’Œè¯„ä¼°åè®®çš„ä¸ä¸€è‡´æ€§ï¼›ï¼ˆ3ï¼‰ç¼ºä¹æ€§èƒ½ã€æˆæœ¬å’Œæ¨ç†æ—¶é—´ä¹‹é—´ç›¸äº’ä½œç”¨çš„ç³»ç»Ÿæ€§åˆ†æã€‚é€šè¿‡å¯¹å„ç§åŸºç¡€æ¨¡å‹å’Œæ¨ç†æ–¹æ³•çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€æ–°çš„æ€ç»´æ¨¡å‹DeepSeek R1å’ŒOpenAI o3åœ¨å¤æ‚çš„åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿçš„æœç´¢æ–¹æ³•ç›¸æ¯”ï¼Œå…ˆè¿›çš„åŸºäºæœç´¢çš„ä»£ç†æ–¹æ³•æä¾›äº†å…·æœ‰å‰æ™¯çš„æ€§èƒ½ä»·æ ¼æ¯”ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸åŒæ¨¡å‹å®¶æ—åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ€§èƒ½å·®è·ï¼Œå¹¶é’ˆå¯¹ä¸åŒè®¡ç®—çº¦æŸç¡®å®šäº†æœ€ä½³æ¨¡å‹é€‰æ‹©ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gersteinlab/medagents-benchmark%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/gersteinlab/medagents-benchmarkå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07459v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°æœ‰çš„åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è¿™ä½¿å¾—è¯„ä¼°åŒºåˆ†é«˜çº§æ–¹æ³•è¶Šæ¥è¶Šå›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†MedAgentsBenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒä¸“æ³¨äºæŒ‘æˆ˜éœ€è¦å¤šæ­¥éª¤ä¸´åºŠæ¨ç†ã€è¯Šæ–­åˆ¶å®šå’Œæ²»ç–—è§„åˆ’çš„åŒ»ç–—é—®é¢˜ï¼Œè¿™æ˜¯å½“å‰æ¨¡å‹ä»ç„¶é¢ä¸´å›°éš¾çš„åœºæ™¯ã€‚è¯¥åŸºå‡†æµ‹è¯•è§£å†³äº†ç°æœ‰è¯„ä¼°ä¸­çš„ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬ç®€å•é—®é¢˜å ä¸»å¯¼ã€é‡‡æ ·å’Œè¯„ä¼°åè®®ä¸ä¸€è‡´ä»¥åŠç¼ºä¹æ€§èƒ½ã€æˆæœ¬å’Œæ¨ç†æ—¶é—´ä¹‹é—´çš„ç³»ç»Ÿåˆ†æã€‚é€šè¿‡ä¸åŒçš„åŸºç¡€æ¨¡å‹å’Œæ¨ç†æ–¹æ³•çš„å®éªŒï¼Œè¯æ˜æœ€æ–°çš„DeepSeek R1å’ŒOpenAI o3æ¨¡å‹åœ¨å¤æ‚çš„åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œå…ˆè¿›çš„åŸºäºæœç´¢çš„ä»£ç†æ–¹æ³•ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æœ‰å¸å¼•åŠ›çš„æ€§èƒ½æˆæœ¬æ¯”ã€‚åˆ†æè¡¨æ˜ï¼Œåœ¨ä¸åŒè®¡ç®—çº¦æŸä¸‹ï¼Œæ¨¡å‹å®¶æ—é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®å¼‚ï¼Œä¸ºä¸åŒè®¡ç®—ç¯å¢ƒé€‰æ‹©æœ€ä¼˜æ¨¡å‹æä¾›äº†ä¾æ®ã€‚åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶å¯åœ¨å…¬å¼€æ¸ é“è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»é¢ä¸´è¯„ä¼°å›°éš¾ã€‚</li>
<li>MedAgentsBenchåŸºå‡†æµ‹è¯•ä¸“æ³¨äºæŒ‘æˆ˜éœ€è¦å¤šæ­¥éª¤ä¸´åºŠæ¨ç†çš„åŒ»ç–—é—®é¢˜ã€‚</li>
<li>ç°æœ‰è¯„ä¼°å­˜åœ¨ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼šç®€å•é—®é¢˜ä¸»å¯¼ã€é‡‡æ ·å’Œè¯„ä¼°åè®®ä¸ä¸€è‡´ä»¥åŠç¼ºä¹ç³»ç»Ÿåˆ†ææ€§èƒ½ã€æˆæœ¬å’Œæ¨ç†æ—¶é—´çš„å…³ç³»ã€‚</li>
<li>æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹å¦‚DeepSeek R1å’ŒOpenAI o3åœ¨å¤æ‚çš„åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>å…ˆè¿›çš„åŸºäºæœç´¢çš„ä»£ç†æ–¹æ³•å…·æœ‰æœ‰å¸å¼•åŠ›çš„æ€§èƒ½æˆæœ¬æ¯”ã€‚</li>
<li>ä¸åŒæ¨¡å‹å®¶æ—é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®å¼‚ï¼Œä¸ºé€‰æ‹©æœ€ä¼˜æ¨¡å‹æä¾›äº†ä¾æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8b9dc477b6ac4ae40482772f969decd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77a8eeb27761ffb85c39029465caf05e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-887da8e131c5d782c91256d66dcf6959.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c22a174577809b271182779a14fd292c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-260406b85b5ca7259eda8f0c599d34ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6ea4ce61a163df5634ea058b52879d5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-with-Verifiable-Rewards-GRPOâ€™s-Effective-Loss-Dynamics-and-Success-Amplification"><a href="#Reinforcement-Learning-with-Verifiable-Rewards-GRPOâ€™s-Effective-Loss-Dynamics-and-Success-Amplification" class="headerlink" title="Reinforcement Learning with Verifiable Rewards: GRPOâ€™s Effective Loss,   Dynamics, and Success Amplification"></a>Reinforcement Learning with Verifiable Rewards: GRPOâ€™s Effective Loss,   Dynamics, and Success Amplification</h2><p><strong>Authors:Youssef Mroueh</strong></p>
<p>Group Relative Policy Optimization (GRPO) was introduced and used successfully to train DeepSeek R1 models for promoting reasoning capabilities of LLMs using verifiable or binary rewards. We show in this paper that GRPO with verifiable rewards can be written as a Kullback Leibler ($\mathsf{KL}$) regularized contrastive loss, where the contrastive samples are synthetic data sampled from the old policy. The optimal GRPO policy $\pi_{n}$ can be expressed explicitly in terms of the binary reward, as well as the first and second order statistics of the old policy ($\pi_{n-1}$) and the reference policy $\pi_0$. Iterating this scheme, we obtain a sequence of policies $\pi_{n}$ for which we can quantify the probability of success $p_n$. We show that the probability of success of the policy satisfies a recurrence that converges to a fixed point of a function that depends on the initial probability of success $p_0$ and the regularization parameter $\beta$ of the $\mathsf{KL}$ regularizer. We show that the fixed point $p^*$ is guaranteed to be larger than $p_0$, thereby demonstrating that GRPO effectively amplifies the probability of success of the policy. </p>
<blockquote>
<p>ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¢«å¼•å…¥å¹¶æˆåŠŸç”¨äºä½¿ç”¨å¯éªŒè¯æˆ–äºŒè¿›åˆ¶å¥–åŠ±è®­ç»ƒDeepSeek R1æ¨¡å‹ï¼Œä»¥ä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å±•ç¤ºäº†ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„GRPOå¯ä»¥è¡¨ç¤ºä¸ºKullback Leiblerï¼ˆKLï¼‰æ­£åˆ™åŒ–çš„å¯¹æ¯”æŸå¤±ï¼Œå…¶ä¸­å¯¹æ¯”æ ·æœ¬æ˜¯é‡‡æ ·è‡ªæ—§ç­–ç•¥çš„åˆæˆæ•°æ®ã€‚æœ€ä¼˜GRPOç­–ç•¥Ï€nå¯ä»¥æ˜ç¡®è¡¨ç¤ºä¸ºäºŒè¿›åˆ¶å¥–åŠ±ä»¥åŠæ—§ç­–ç•¥Ï€n-1å’Œå‚è€ƒç­–ç•¥Ï€0çš„ä¸€é˜¶å’ŒäºŒé˜¶ç»Ÿè®¡é‡ã€‚é€šè¿‡è¿­ä»£è¿™ä¸ªæ–¹æ¡ˆï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ç³»åˆ—ç­–ç•¥Ï€nï¼Œæˆ‘ä»¬å¯ä»¥é‡åŒ–å…¶æˆåŠŸçš„æ¦‚ç‡pnã€‚æˆ‘ä»¬å±•ç¤ºäº†ç­–ç•¥çš„æˆåŠŸæ¦‚ç‡æ»¡è¶³ä¸€ä¸ªé€’å½’ï¼Œè¯¥é€’å½’æ”¶æ•›åˆ°ä¸€ä¸ªä¾èµ–äºåˆå§‹æˆåŠŸæ¦‚ç‡p0å’ŒKLæ­£åˆ™åŒ–çš„æ­£åˆ™åŒ–å‚æ•°Î²çš„å‡½æ•°çš„ä¸åŠ¨ç‚¹ã€‚æˆ‘ä»¬è¯æ˜äº†ä¸åŠ¨ç‚¹p*ä¸€å®šå¤§äºp0ï¼Œä»è€Œè¯æ˜GRPOæœ‰æ•ˆåœ°æ”¾å¤§äº†ç­–ç•¥æˆåŠŸçš„æ¦‚ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06639v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GRPOï¼ˆGroup Relative Policy Optimizationï¼‰è¢«æˆåŠŸåº”ç”¨äºDeepSeek R1æ¨¡å‹çš„è®­ç»ƒï¼Œä»¥æå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å°†GRPOä¸å¯éªŒè¯å¥–åŠ±ç›¸ç»“åˆï¼Œå¹¶å°†å…¶è¡¨è¿°ä¸ºKullback Leiblerï¼ˆKLï¼‰æ­£åˆ™åŒ–çš„å¯¹æ¯”æŸå¤±ã€‚GRPOç­–ç•¥ä¼˜åŒ–å¯ä»¥é€šè¿‡è¿­ä»£è·å¾—ï¼Œå¹¶å¯ä»¥é‡åŒ–æˆåŠŸæ¦‚ç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGRPOèƒ½æœ‰æ•ˆæé«˜ç­–ç•¥æˆåŠŸæ¦‚ç‡çš„å›ºå®šç‚¹å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GRPOè¢«æˆåŠŸåº”ç”¨äºè®­ç»ƒDeepSeek R1æ¨¡å‹ï¼Œä»¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨å¯éªŒè¯æˆ–äºŒè¿›åˆ¶å¥–åŠ±ã€‚</li>
<li>GRPOä¸å¯éªŒè¯å¥–åŠ±çš„ç»“åˆå¯ä»¥è¡¨ç¤ºä¸ºKLæ­£åˆ™åŒ–çš„å¯¹æ¯”æŸå¤±ã€‚</li>
<li>GRPOç­–ç•¥ä¼˜åŒ–å¯ä»¥é€šè¿‡è¿­ä»£è·å¾—ï¼Œå¹¶å¯ä»¥é‡åŒ–å…¶æˆåŠŸæ¦‚ç‡ã€‚</li>
<li>GRPOç­–ç•¥ä¼˜åŒ–ä¸­çš„æœ€ä¼˜ç­–ç•¥$\pi_{n}$å¯ä»¥æ˜¾å¼è¡¨è¾¾ä¸ºäºŒè¿›åˆ¶å¥–åŠ±ä»¥åŠæ—§ç­–ç•¥$\pi_{n-1}$å’Œå‚è€ƒç­–ç•¥$\pi_0$çš„ä¸€é˜¶å’ŒäºŒé˜¶ç»Ÿè®¡ã€‚</li>
<li>ç­–ç•¥æˆåŠŸæ¦‚ç‡æ»¡è¶³ä¸€ä¸ªé€’å½’ï¼Œæ”¶æ•›åˆ°ä¾èµ–äºåˆå§‹æˆåŠŸæ¦‚ç‡$p_0$å’ŒKLæ­£åˆ™åŒ–çš„æ­£åˆ™åŒ–å‚æ•°$\beta$çš„å‡½æ•°å›ºå®šç‚¹ã€‚</li>
<li>å›ºå®šç‚¹$p^*$ä¿è¯å¤§äº$p_0$ï¼Œè¡¨æ˜GRPOæœ‰æ•ˆåœ°æé«˜äº†ç­–ç•¥çš„æˆåŠŸæ¦‚ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-718755944f1f2fda397d20f017f68644.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88d5e84e1662e6affc82e901e59b4568.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-92904619907f3f442ef28194e265ec3f.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  The Emperor's New Clothes in Benchmarking? A Rigorous Examination of   Mitigation Strategies for LLM Benchmark Data Contamination
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-21/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0616f05e4178ab9ce3e2f4a41d899f97.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-21  Reinforcement learning-based motion imitation for physiologically   plausible musculoskeletal motor control
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15444.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
