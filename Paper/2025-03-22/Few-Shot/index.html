<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language   Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-cd6ab3d5cd16a2668137ec591aa485a6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-22-æ›´æ–°"><a href="#2025-03-22-æ›´æ–°" class="headerlink" title="2025-03-22 æ›´æ–°"></a>2025-03-22 æ›´æ–°</h1><h2 id="Generalized-Few-shot-3D-Point-Cloud-Segmentation-with-Vision-Language-Model"><a href="#Generalized-Few-shot-3D-Point-Cloud-Segmentation-with-Vision-Language-Model" class="headerlink" title="Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language   Model"></a>Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language   Model</h2><p><strong>Authors:Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Junlin Han, Ender Konukoglu, Serge Belongie</strong></p>
<p>Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to new classes with few support samples while retaining base class segmentation. Existing GFS-PCS methods enhance prototypes via interacting with support or query features but remain limited by sparse knowledge from few-shot samples. Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world novel classes, contain rich but noisy novel class knowledge. In this work, we introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label selection to filter low-quality regions, followed by an adaptive infilling strategy that combines knowledge from pseudo-label contexts and few-shot samples to adaptively label the filtered, unlabeled areas. Additionally, we design a novel-base mix strategy to embed few-shot samples into training scenes, preserving essential context for improved novel class learning. Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we introduce two challenging benchmarks with diverse novel classes for comprehensive generalization evaluation. Experiments validate the effectiveness of our framework across models and datasets. Our approach and benchmarks provide a solid foundation for advancing GFS-PCS in the real world. The code is at <a target="_blank" rel="noopener" href="https://github.com/ZhaochongAn/GFS-VL">https://github.com/ZhaochongAn/GFS-VL</a> </p>
<blockquote>
<p>å¹¿ä¹‰å°‘æ ·æœ¬3Dç‚¹äº‘åˆ†å‰²ï¼ˆGFS-PCSï¼‰èƒ½å¤Ÿåœ¨ä¿ç•™åŸºç¡€ç±»åˆ«åˆ†å‰²çš„åŒæ—¶ï¼Œé€‚åº”æ–°çš„ç±»åˆ«å¹¶å…·æœ‰å°‘é‡çš„æ”¯æ’‘æ ·æœ¬ã€‚ç°æœ‰çš„GFS-PCSæ–¹æ³•é€šè¿‡ä¸æ”¯æŒæˆ–æŸ¥è¯¢ç‰¹å¾è¿›è¡Œäº¤äº’æ¥å¢å¼ºåŸå‹ï¼Œä½†ä»å—åˆ°å°‘é‡æ ·æœ¬ç¨€ç–çŸ¥è¯†çš„é™åˆ¶ã€‚åŒæ—¶ï¼Œ3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆ3D VLMsï¼‰åœ¨å¼€æ”¾ä¸–ç•Œçš„æ–°ç±»åˆ«ä¸­å…·æœ‰ä¸°å¯Œçš„ä½†å¸¦æœ‰å™ªå£°çš„æ–°ç±»åˆ«çŸ¥è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªGFS-PCSæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒåˆ©ç”¨æ¥è‡ª3D VLMsçš„å¯†é›†ä½†å¸¦æœ‰å™ªå£°çš„ä¼ªæ ‡ç­¾å’Œç²¾ç¡®çš„ä½†ç¨€ç–çš„å°‘é‡æ ·æœ¬ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‘æŒ¥ä¸¤è€…çš„ä¼˜åŠ¿ï¼Œç§°ä¸ºGFS-VLã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸå‹å¼•å¯¼ä¼ªæ ‡ç­¾é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºè¿‡æ»¤ä½è´¨é‡åŒºåŸŸï¼Œéšåæ˜¯ä¸€ç§è‡ªé€‚åº”å¡«å……ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç»“åˆäº†ä¼ªæ ‡ç­¾ä¸Šä¸‹æ–‡å’Œå°‘é‡æ ·æœ¬çš„çŸ¥è¯†ï¼Œå¯¹è¿‡æ»¤åçš„æœªæ ‡è®°åŒºåŸŸè¿›è¡Œè‡ªé€‚åº”æ ‡è®°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹åŸºç¡€æ··åˆç­–ç•¥ï¼Œå°†å°‘é‡æ ·æœ¬åµŒå…¥åˆ°è®­ç»ƒåœºæ™¯ä¸­ï¼Œä¿ç•™é‡è¦ä¸Šä¸‹æ–‡ï¼Œä»¥æ”¹è¿›æ–°ç±»åˆ«çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¤è¯†åˆ°å½“å‰GFS-PCSåŸºå‡†æµ‹è¯•å¤šæ ·æ€§çš„å±€é™æ€§ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå…·æœ‰å¤šæ ·æ–°ç±»åˆ«çš„æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ï¼Œä»¥è¿›è¡Œå…¨é¢çš„æ³›åŒ–è¯„ä¼°ã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ä¸ºæ¨è¿›GFS-PCSåœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ã€‚ä»£ç åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/ZhaochongAn/GFS-VL">https://github.com/ZhaochongAn/GFS-VL</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16282v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç»“åˆ3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆ3D VLMsï¼‰ä¸å¹¿ä¹‰å°‘æ ·æœ¬3Dç‚¹äº‘åˆ†å‰²ï¼ˆGFS-PCSï¼‰çš„æ–°æ¡†æ¶GFS-VLã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸°å¯Œçš„ä¼ªæ ‡ç­¾å’Œç²¾ç¡®çš„å°‘æ ·æœ¬æ•°æ®ï¼Œé€šè¿‡åŸå‹å¼•å¯¼ä¼ªæ ‡ç­¾é€‰æ‹©å’Œè‡ªé€‚åº”å¡«å……ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å¼•å…¥äº†ä¸¤ä¸ªæŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºå…¨é¢çš„æ³›åŒ–è¯„ä¼°ã€‚å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GFS-VLç»“åˆä¸°å¯Œçš„ä¼ªæ ‡ç­¾ä¸ç²¾ç¡®å°‘æ•°æ ·æœ¬ï¼Œä¼˜åŒ–äº†ç°æœ‰çš„å¹¿ä¹‰å°‘æ ·æœ¬ç‚¹äº‘åˆ†å‰²ï¼ˆGFS-PCSï¼‰ã€‚</li>
<li>åˆ©ç”¨åŸå‹å¼•å¯¼ä¼ªæ ‡ç­¾é€‰æ‹©æ–¹æ³•ç­›é€‰ä½è´¨é‡åŒºåŸŸï¼Œç»“åˆè‡ªé€‚åº”å¡«å……ç­–ç•¥å¡«å……å¹¶æ ‡æ³¨ç­›é€‰å‡ºçš„æœªæ ‡è®°åŒºåŸŸã€‚</li>
<li>åˆ›æ–°æ€§åœ°å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºæ··åˆç­–ç•¥å°†å°‘æ•°æ ·æœ¬åµŒå…¥è®­ç»ƒåœºæ™¯çš„æ–¹æ³•ï¼Œå¢å¼ºäº†é‡è¦ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åŒæ—¶å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-37035712fcadb54bf90e93f5b5fa5304.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c1249e2e6bb7508adca5e077f48f3d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0605d2dc9fc4a10ad1374de047600aa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb68326f2c93c82c51a6dd449804308.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CLS-RL-Image-Classification-with-Rule-Based-Reinforcement-Learning"><a href="#CLS-RL-Image-Classification-with-Rule-Based-Reinforcement-Learning" class="headerlink" title="CLS-RL: Image Classification with Rule-Based Reinforcement Learning"></a>CLS-RL: Image Classification with Rule-Based Reinforcement Learning</h2><p><strong>Authors:Ming Li, Shitian Zhao, Jike Zhong, Yuxiang Lai, Kaipeng Zhang</strong></p>
<p>Classification is a core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to SOTA classification models. However, acquiring large-scale labeled data is expensive. In this paper, we explore few-shot MLLM classification fine-tuning. We found that SFT can cause severe overfitting issues and may even degrade performance over the zero-shot approach. To address this challenge, inspired by the recent successes in rule-based reinforcement learning, we propose CLS-RL, which uses verifiable signals as reward to fine-tune MLLMs. We discovered that CLS-RL outperforms SFT in most datasets and has a much higher average accuracy on both base-to-new and few-shot learning setting. Moreover, we observed a free-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular dataset, their performance on other distinct datasets may also improve over zero-shot models, even if those datasets differ in distribution and class names. This suggests that RL-based methods effectively teach models the fundamentals of classification. Lastly, inspired by recent works in inference time thinking, we re-examine the &#96;thinking processâ€™ during fine-tuning, a critical aspect of RL-based methods, in the context of visual classification. We question whether such tasks require extensive thinking process during fine-tuning, proposing that this may actually detract from performance. Based on this premise, we introduce the No-Thinking-CLS-RL method, which minimizes thinking processes during training by setting an equality accuracy reward. Our findings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL method achieves superior in-domain performance and generalization capabilities than CLS-RL. </p>
<blockquote>
<p>åˆ†ç±»æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€åˆåœ¨å›¾åƒåˆ†ç±»æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä½†é€šè¿‡è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜å®ƒä»¬çš„æ€§èƒ½ï¼Œä½¿å…¶ä¸æœ€å…ˆè¿›çš„åˆ†ç±»æ¨¡å‹ç›¸åª²ç¾ã€‚ç„¶è€Œï¼Œè·å–å¤§è§„æ¨¡æ ‡è®°æ•°æ®æˆæœ¬é«˜æ˜‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†åŸºäºå°‘é‡æ•°æ®çš„MLLMåˆ†ç±»å¾®è°ƒæŠ€æœ¯ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ ‡å‡†å¾®è°ƒï¼ˆSFTï¼‰å¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶ä¸”å¯èƒ½åœ¨é›¶æ ·æœ¬æ–¹æ³•ä¸Šçš„æ€§èƒ½è¡¨ç°æ›´å·®ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å—åˆ°åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ è¿‘æœŸæˆåŠŸçš„å¯å‘ï¼Œæå‡ºäº†CLS-RLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¯éªŒè¯çš„ä¿¡å·ä½œä¸ºå¥–åŠ±æ¥å¾®è°ƒMLLMsã€‚æˆ‘ä»¬å‘ç°CLS-RLåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæ ‡å‡†å¾®è°ƒï¼Œå¹¶ä¸”åœ¨åŸºç¡€åˆ°æ–°çš„æ•°æ®é›†å’Œå°æ ·æœ¬å­¦ä¹ ç¯å¢ƒä¸­å…·æœ‰æ›´é«˜çš„å¹³å‡å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°CLS-RLå­˜åœ¨ä¸€ä¸ªå…è´¹åˆé¤ç°è±¡ï¼›å½“æ¨¡å‹åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œå®ƒä»¬åœ¨å…¶ä»–ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½å¯èƒ½ä¼šè¶…è¿‡é›¶æ ·æœ¬æ¨¡å‹ï¼Œå³ä½¿è¿™äº›æ•°æ®é›†åœ¨åˆ†å¸ƒå’Œç±»åˆ«åç§°ä¸Šæœ‰æ‰€ä¸åŒã€‚è¿™è¡¨æ˜åŸºäºRLçš„æ–¹æ³•æœ‰æ•ˆåœ°æ•™æˆäº†æ¨¡å‹åˆ†ç±»çš„åŸºæœ¬åŸç†ã€‚æœ€åï¼Œå—åˆ°æœ€è¿‘æ¨ç†æ—¶é—´æ€è€ƒç ”ç©¶å·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†åŸºäºRLçš„æ–¹æ³•ä¸­å¾®è°ƒè¿‡ç¨‹çš„â€œæ€è€ƒè¿‡ç¨‹â€ï¼Œè¿™æ˜¯åœ¨è§†è§‰åˆ†ç±»èƒŒæ™¯ä¸‹å…³é”®çš„ä¸€ä¸ªæ–¹é¢ã€‚æˆ‘ä»¬è´¨ç–‘è¿™æ ·çš„ä»»åŠ¡æ˜¯å¦éœ€è¦å¤§é‡çš„æ€è€ƒè¿‡ç¨‹æ¥è¿›è¡Œå¾®è°ƒï¼Œå¹¶æå‡ºè¿™å®é™…ä¸Šå¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚åŸºäºè¿™ä¸€å‰æï¼Œæˆ‘ä»¬å¼•å…¥äº†æ— æ€è€ƒCLS-RLæ–¹æ³•ï¼Œé€šè¿‡è®¾ç½®ç›¸ç­‰çš„å‡†ç¡®ç‡å¥–åŠ±æ¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€è€ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨è¾ƒå°‘çš„å¾®è°ƒæ—¶é—´å†…ï¼Œæ— æ€è€ƒCLS-RLæ–¹æ³•åœ¨åŸŸå†…æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢ä¼˜äºCLS-RLã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16188v1">PDF</a> Preprint, work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„å¾®è°ƒç­–ç•¥ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¼ ç»ŸåŸºäºæ•°æ®çš„å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶ä¸”å¯èƒ½å½±å“é›¶æ ·æœ¬æ–¹æ³•çš„è¡¨ç°ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºå¯éªŒè¯ä¿¡å·çš„å¥–åŠ±å¾®è°ƒç­–ç•¥CLS-RLï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºç¡€åˆ°æ–°çš„æ•°æ®é›†å’Œå°‘æ ·æœ¬å­¦ä¹ ç¯å¢ƒä¸­ã€‚æ­¤å¤–ï¼Œè¿˜è§‚å¯Ÿåˆ°ä¸€ä¸ªç°è±¡ï¼šå½“æ¨¡å‹åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¾®è°ƒæ—¶ï¼Œå…¶åœ¨å…¶ä»–ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¹Ÿå¯èƒ½è¶…è¿‡é›¶æ ·æœ¬æ¨¡å‹ã€‚å—è¿‘æœŸæ¨ç†æ—¶é—´æ€è€ƒç ”ç©¶å¯å‘ï¼Œæœ¬æ–‡é‡æ–°å®¡è§†äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ€è€ƒè¿‡ç¨‹ï¼Œå¹¶å¼•å…¥äº†æ— æ€è€ƒè¿‡ç¨‹çš„CLS-RLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è®¾å®šå‡†ç¡®æ€§å¥–åŠ±æ¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€è€ƒè¿‡ç¨‹ï¼Œå®ç°äº†ä¼˜è¶Šçš„é¢†åŸŸå†…æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è™½ç„¶åˆè¡¨ç°ä¸ä½³ï¼Œä½†é€šè¿‡æ•°æ®å¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶å½±å“é›¶æ ·æœ¬æ–¹æ³•çš„è¡¨ç°ã€‚</li>
<li>CLS-RLç­–ç•¥ä½¿ç”¨å¯éªŒè¯ä¿¡å·ä½œä¸ºå¥–åŠ±è¿›è¡Œå¾®è°ƒï¼Œåœ¨å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>CLS-RLç­–ç•¥åœ¨æé«˜ç‰¹å®šæ•°æ®é›†æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿèƒ½æå‡å…¶ä»–ä¸åŒæ•°æ®é›†çš„è¡¨ç°ï¼Œæ˜¾ç¤ºå…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å—æ¨ç†æ—¶é—´æ€è€ƒç ”ç©¶å¯å‘ï¼Œé‡æ–°å®¡è§†äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ€è€ƒè¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥æ— æ€è€ƒè¿‡ç¨‹çš„CLS-RLæ–¹æ³•ï¼Œé€šè¿‡è®¾å®šå‡†ç¡®æ€§å¥–åŠ±å‡å°‘è®­ç»ƒæ€è€ƒè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f1dde42d0ff60bdb5033de8d4720b30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eed20ea936a8d8fc9f759bf58f42279.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89a4d440fafc91f50c84e976bf0efb35.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Human-or-LLM-A-Comparative-Study-on-Accessible-Code-Generation-Capability"><a href="#Human-or-LLM-A-Comparative-Study-on-Accessible-Code-Generation-Capability" class="headerlink" title="Human or LLM? A Comparative Study on Accessible Code Generation   Capability"></a>Human or LLM? A Comparative Study on Accessible Code Generation   Capability</h2><p><strong>Authors:Hyunjae Suh, Mahan Tafreshipour, Sam Malek, Iftekhar Ahmed</strong></p>
<p>Web accessibility is essential for inclusive digital experiences, yet the accessibility of LLM-generated code remains underexplored. This paper presents an empirical study comparing the accessibility of web code generated by GPT-4o and Qwen2.5-Coder-32B-Instruct-AWQ against human-written code. Results show that LLMs often produce more accessible code, especially for basic features like color contrast and alternative text, but struggle with complex issues such as ARIA attributes. We also assess advanced prompting strategies (Zero-Shot, Few-Shot, Self-Criticism), finding they offer some gains but are limited. To address these gaps, we introduce FeedA11y, a feedback-driven ReAct-based approach that significantly outperforms other methods in improving accessibility. Our work highlights the promise of LLMs for accessible code generation and emphasizes the need for feedback-based techniques to address persistent challenges. </p>
<blockquote>
<p>Webå¯åŠæ€§å¯¹äºåŒ…å®¹æ€§æ•°å­—ä½“éªŒè‡³å…³é‡è¦ï¼Œç„¶è€Œå…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„ä»£ç çš„å¯åŠæ€§ä»ç„¶ç¼ºä¹æ·±å…¥ç ”ç©¶ã€‚æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶å¯¹æ¯”äº†GPT-4oå’ŒQwen2.5-Coder-32B-Instruct-AWQç”Ÿæˆçš„ç½‘é¡µä»£ç ä¸äººç±»ç¼–å†™çš„ä»£ç çš„å¯åŠæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMsç”Ÿæˆçš„ä»£ç é€šå¸¸æ›´å…·å¯åŠæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢œè‰²å¯¹æ¯”åº¦å’Œæ›¿ä»£æ–‡æœ¬ç­‰åŸºæœ¬åŠŸèƒ½æ–¹é¢ï¼Œä½†åœ¨å¤„ç†ARIAå±æ€§ç­‰å¤æ‚é—®é¢˜ä¸Šå­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†é«˜çº§æç¤ºç­–ç•¥ï¼ˆé›¶æ ·æœ¬ã€å°æ ·æœ¬ã€è‡ªæˆ‘æ‰¹è¯„ï¼‰ï¼Œå‘ç°å®ƒä»¬è™½ç„¶å¯ä»¥æä¾›ä¸€äº›å¸®åŠ©ï¼Œä½†ä»æœ‰å±€é™æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†FeedA11yï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåé¦ˆçš„ReActæ–¹æ³•ï¼Œåœ¨æ”¹è¿›å¯åŠæ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œçªå‡ºäº†LLMsåœ¨ç”Ÿæˆå¯è®¿é—®ä»£ç æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒäº†åŸºäºåé¦ˆçš„æŠ€æœ¯åœ¨è§£å†³æŒç»­æŒ‘æˆ˜æ–¹é¢çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15885v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†LLMç”Ÿæˆçš„ç½‘é¡µä»£ç çš„å¯è®¿é—®æ€§ï¼Œå¹¶ä¸äººç±»ç¼–å†™çš„ä»£ç è¿›è¡Œäº†æ¯”è¾ƒã€‚ç ”ç©¶å‘ç°ï¼ŒLLMç”Ÿæˆçš„ä»£ç åœ¨åŸºæœ¬åŠŸèƒ½æ–¹é¢æ›´å…·å¯è®¿é—®æ€§ï¼Œä½†åœ¨å¤æ‚é—®é¢˜ä¸Šå­˜åœ¨ç¼ºé™·ã€‚åŒæ—¶ï¼Œè¯„ä¼°äº†å…ˆè¿›çš„æç¤ºç­–ç•¥ï¼Œå‘ç°å®ƒä»¬è™½ç„¶æœ‰æ‰€åŠ©ç›Šä½†ä»æœ‰å±€é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†FeedA11yæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºåé¦ˆå’ŒReactï¼Œæ˜¾è‘—æé«˜äº†ä»£ç çš„å¯è®¿é—®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMç”Ÿæˆçš„ä»£ç åœ¨åŸºæœ¬åŠŸèƒ½æ–¹é¢æ›´å…·å¯è®¿é—®æ€§ã€‚</li>
<li>LLMåœ¨å¤„ç†å¤æ‚çš„å¯è®¿é—®æ€§é—®é¢˜ï¼ˆå¦‚ARIAå±æ€§ï¼‰æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>å…ˆè¿›çš„æç¤ºç­–ç•¥ï¼ˆå¦‚Zero-Shotã€Few-Shotã€Self-Criticismï¼‰å¯¹äºæé«˜LLMçš„ä»£ç ç”Ÿæˆèƒ½åŠ›æœ‰ä¸€å®šå¸®åŠ©ï¼Œä½†ä»æœ‰å±€é™ã€‚</li>
<li>åé¦ˆé©±åŠ¨çš„æ–¹æ³•ï¼ˆå¦‚FeedA11yï¼‰èƒ½æ˜¾è‘—æé«˜LLMç”Ÿæˆä»£ç çš„å¯è®¿é—®æ€§ã€‚</li>
<li>LLMåœ¨ç”Ÿæˆå¯è®¿é—®çš„ä»£ç æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>éœ€è¦æ›´å¤šçš„ç ”ç©¶æ¥æ”¹è¿›LLMåœ¨å¤„ç†å¤æ‚çš„å¯è®¿é—®æ€§é—®é¢˜æ–¹é¢çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-992ea3c2b7387b01bc1418f4321ace0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c78e3f113a4eb486006fb4c1f04ca12a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83a34b4346c99a2f3e69abd61ca993d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45fcd22fe9d5ee60781b53f6f1c53a2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ede70ce05f9bb98af8489fb2fb83af19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3734b09a3987d379ba92ae5310b3bae.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Sparseformer-a-Transferable-Transformer-with-Multi-granularity-Token-Sparsification-for-Medical-Time-Series-Classification"><a href="#Sparseformer-a-Transferable-Transformer-with-Multi-granularity-Token-Sparsification-for-Medical-Time-Series-Classification" class="headerlink" title="Sparseformer: a Transferable Transformer with Multi-granularity Token   Sparsification for Medical Time Series Classification"></a>Sparseformer: a Transferable Transformer with Multi-granularity Token   Sparsification for Medical Time Series Classification</h2><p><strong>Authors:Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Fugee Tsung</strong></p>
<p>Medical time series (MedTS) classification is crucial for improved diagnosis in healthcare, and yet it is challenging due to the varying granularity of patterns, intricate inter-channel correlation, information redundancy, and label scarcity. While existing transformer-based models have shown promise in time series analysis, they mainly focus on forecasting and fail to fully exploit the distinctive characteristics of MedTS data. In this paper, we introduce Sparseformer, a transformer specifically designed for MedTS classification. We propose a sparse token-based dual-attention mechanism that enables global modeling and token compression, allowing dynamic focus on the most informative tokens while distilling redundant features. This mechanism is then applied to the multi-granularity, cross-channel encoding of medical signals, capturing intra- and inter-granularity correlations and inter-channel connections. The sparsification design allows our model to handle heterogeneous inputs of varying lengths and channels directly. Further, we introduce an adaptive label encoder to address label space misalignment across datasets, equipping our model with cross-dataset transferability to alleviate the medical label scarcity issue. Our model outperforms 12 baselines across seven medical datasets under supervised learning. In the few-shot learning experiments, our model also achieves superior average results. In addition, the in-domain and cross-domain experiments among three diagnostic scenarios demonstrate our modelâ€™s zero-shot learning capability. Collectively, these findings underscore the robustness and transferability of our model in various medical applications. </p>
<blockquote>
<p>åŒ»ç–—æ—¶é—´åºåˆ—ï¼ˆMedTSï¼‰åˆ†ç±»å¯¹äºæé«˜åŒ»ç–—ä¿å¥ä¸­çš„è¯Šæ–­è‡³å…³é‡è¦ï¼Œç„¶è€Œç”±äºæ¨¡å¼ç²’åº¦ä¸ä¸€ã€å¤æ‚è·¨é€šé“ç›¸å…³æ€§ã€ä¿¡æ¯å†—ä½™å’Œæ ‡ç­¾ç¨€ç¼ºï¼Œå®ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰çš„åŸºäºtransformerçš„æ¨¡å‹åœ¨æ—¶é—´åºåˆ—åˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ä¸»è¦é›†ä¸­åœ¨é¢„æµ‹æ–¹é¢ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨MedTSæ•°æ®çš„ç‹¬ç‰¹ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Sparseformerï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºMedTSåˆ†ç±»è®¾è®¡çš„transformerã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¨€ç–ä»¤ç‰Œçš„åŒé‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒèƒ½å¤Ÿå®ç°å…¨å±€å»ºæ¨¡å’Œä»¤ç‰Œå‹ç¼©ï¼Œå…è®¸åŠ¨æ€å…³æ³¨æœ€å…·ä¿¡æ¯é‡çš„ä»¤ç‰Œï¼ŒåŒæ—¶æç‚¼å†—ä½™ç‰¹å¾ã€‚è¯¥æœºåˆ¶éšååº”ç”¨äºåŒ»ç–—ä¿¡å·çš„å¤šç²’åº¦è·¨é€šé“ç¼–ç ï¼Œæ•è·ç²’åº¦å’Œè·¨ç²’åº¦ç›¸å…³æ€§ä»¥åŠè·¨é€šé“è¿æ¥ã€‚ç¨€ç–åŒ–è®¾è®¡å…è®¸æˆ‘ä»¬çš„æ¨¡å‹ç›´æ¥å¤„ç†ä¸åŒé•¿åº¦å’Œé€šé“çš„å¼‚æ„è¾“å…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”æ ‡ç­¾ç¼–ç å™¨ï¼Œä»¥è§£å†³æ•°æ®é›†é—´æ ‡ç­¾ç©ºé—´çš„ä¸å¯¹é½é—®é¢˜ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹å…·å¤‡è·¨æ•°æ®é›†çš„å¯è½¬ç§»æ€§ï¼Œä»¥ç¼“è§£åŒ»ç–—æ ‡ç­¾ç¨€ç¼ºçš„é—®é¢˜ã€‚åœ¨ä¸ƒä¸ªåŒ»ç–—æ•°æ®é›†çš„ç›‘ç£å­¦ä¹ ä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äº12ç§åŸºçº¿æ¨¡å‹ã€‚åœ¨å°‘æ ·æœ¬å­¦ä¹ å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¹Ÿå–å¾—äº†å¹³å‡æˆç»©é¢†å…ˆçš„ä¼˜ç§€ç»“æœã€‚æ­¤å¤–ï¼Œä¸‰ä¸ªè¯Šæ–­åœºæ™¯ä¸­çš„åŸŸå†…å’Œè·¨åŸŸå®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›å‘ç°çªæ˜¾äº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨å„ç§åŒ»ç–—åº”ç”¨ä¸­çš„ç¨³å¥æ€§å’Œå¯è½¬ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15578v1">PDF</a> 3 figures, 16 pages, 5 tables</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºSparseformeræ¨¡å‹ï¼Œä¸“ä¸ºåŒ»ç–—æ—¶é—´åºåˆ—åˆ†ç±»è®¾è®¡ã€‚é€šè¿‡ç¨€ç–æ ‡è®°åŒæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå…¨å±€å»ºæ¨¡å’Œæ ‡è®°å‹ç¼©ï¼Œå¯åŠ¨æ€å…³æ³¨æœ€å…·æœ‰ä¿¡æ¯é‡çš„æ ‡è®°å¹¶æç‚¼å†—ä½™ç‰¹å¾ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸åŒé•¿åº¦å’Œé€šé“çš„å¼‚è´¨è¾“å…¥ï¼Œé€šè¿‡è‡ªé€‚åº”æ ‡ç­¾ç¼–ç å™¨è§£å†³æ ‡ç­¾ç©ºé—´åœ¨ä¸åŒæ•°æ®é›†ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œå®ç°è·¨æ•°æ®é›†è¿ç§»å­¦ä¹ ï¼Œç¼“è§£åŒ»ç–—æ ‡ç­¾ç¨€ç¼ºé—®é¢˜ã€‚Sparseformeråœ¨å¤šä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè¶…è¶ŠåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç°å‡ºåœ¨ä¸åŒåŒ»ç–—åº”ç”¨ä¸­çš„ç¨³å¥æ€§å’Œè¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sparseformeræ˜¯ä¸“ä¸ºåŒ»ç–—æ—¶é—´åºåˆ—åˆ†ç±»è®¾è®¡çš„å˜å‹å™¨æ¨¡å‹ã€‚</li>
<li>Sparseformeré‡‡ç”¨ç¨€ç–æ ‡è®°åŒæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå…¨å±€å»ºæ¨¡å’Œæ ‡è®°å‹ç¼©ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸åŒé•¿åº¦å’Œé€šé“çš„å¼‚è´¨è¾“å…¥ã€‚</li>
<li>è‡ªé€‚åº”æ ‡ç­¾ç¼–ç å™¨è§£å†³äº†æ ‡ç­¾ç©ºé—´åœ¨ä¸åŒæ•°æ®é›†ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>Sparseformeråœ¨å¤šä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šå®ç°äº†è¶…è¶ŠåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>Sparseformeråœ¨å°‘æ ·æœ¬å­¦ä¹ å®éªŒä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„å¹³å‡ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e4fcc0937a09da756b464e50b1c3ac1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-877890552ba6985eacf592cc28a42f70.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Conjuring-Positive-Pairs-for-Efficient-Unification-of-Representation-Learning-and-Image-Synthesis"><a href="#Conjuring-Positive-Pairs-for-Efficient-Unification-of-Representation-Learning-and-Image-Synthesis" class="headerlink" title="Conjuring Positive Pairs for Efficient Unification of Representation   Learning and Image Synthesis"></a>Conjuring Positive Pairs for Efficient Unification of Representation   Learning and Image Synthesis</h2><p><strong>Authors:Imanol G. Estepa, JesÃºs M. RodrÃ­guez-de-Vera, Ignacio SarasÃºa, Bhalaji Nagarajan, Petia Radeva</strong></p>
<p>While representation learning and generative modeling seek to understand visual data, unifying both domains remains unexplored. Recent Unified Self-Supervised Learning (SSL) methods have started to bridge the gap between both paradigms. However, they rely solely on semantic token reconstruction, which requires an external tokenizer during training â€“ introducing a significant overhead. In this work, we introduce Sorcen, a novel unified SSL framework, incorporating a synergic Contrastive-Reconstruction objective. Our Contrastive objective, â€œEcho Contrastâ€, leverages the generative capabilities of Sorcen, eliminating the need for additional image crops or augmentations during training. Sorcen â€œgeneratesâ€ an echo sample in the semantic token space, forming the contrastive positive pair. Sorcen operates exclusively on precomputed tokens, eliminating the need for an online token transformation during training, thereby significantly reducing computational overhead. Extensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the previous Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear probing, unconditional image generation, few-shot learning, and transfer learning, respectively, while being 60.8% more efficient. Additionally, Sorcen surpasses previous single-crop MIM SoTA in linear probing and achieves SoTA performance in unconditional image generation, highlighting significant improvements and breakthroughs in Unified SSL models. </p>
<blockquote>
<p>è¡¨ç¤ºå­¦ä¹ å’Œç”Ÿæˆå»ºæ¨¡éƒ½åœ¨åŠªåŠ›ç†è§£è§†è§‰æ•°æ®ï¼Œä½†ç»Ÿä¸€è¿™ä¸¤ä¸ªé¢†åŸŸä»ç„¶æœªè¢«æ¢ç´¢ã€‚æœ€è¿‘çš„ç»Ÿä¸€è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆSSLï¼‰å·²ç»å¼€å§‹å¼¥åˆä¸¤ç§èŒƒå¼ä¹‹é—´çš„é¸¿æ²Ÿã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»…ä¾èµ–äºè¯­ä¹‰ä»¤ç‰Œé‡å»ºï¼Œè¿™éœ€è¦è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¤–éƒ¨ä»¤ç‰Œå™¨â€”â€”å¸¦æ¥äº†ç›¸å½“å¤§çš„å¼€é”€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Sorcenï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç»Ÿä¸€çš„SSLæ¡†æ¶ï¼Œå®ƒç»“åˆäº†ååŒå¯¹æ¯”é‡å»ºç›®æ ‡ã€‚æˆ‘ä»¬çš„å¯¹æ¯”ç›®æ ‡â€œå›å£°å¯¹æ¯”â€åˆ©ç”¨äº†Sorcençš„ç”Ÿæˆèƒ½åŠ›ï¼Œæ¶ˆé™¤äº†è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é¢å¤–å›¾åƒè£å‰ªæˆ–å¢å¼ºçš„éœ€æ±‚ã€‚Sorcenåœ¨è¯­ä¹‰ä»¤ç‰Œç©ºé—´ä¸­â€œç”Ÿæˆâ€ä¸€ä¸ªå›å£°æ ·æœ¬ï¼Œå½¢æˆå¯¹æ¯”æ­£å¯¹ã€‚Sorcenåªåœ¨é¢„è®¡ç®—ä»¤ç‰Œä¸Šè¿è¡Œï¼Œæ¶ˆé™¤äº†è®­ç»ƒè¿‡ç¨‹ä¸­åœ¨çº¿ä»¤ç‰Œè½¬æ¢çš„éœ€è¦ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚åœ¨ImageNet-1kä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ç›´æ¥çº¿æ€§æ¢æµ‹ã€æ— æ¡ä»¶å›¾åƒç”Ÿæˆã€å°‘æ ·æœ¬å­¦ä¹ å’Œè¿ç§»å­¦ä¹ æ–¹é¢ï¼Œç´¢æ£®åˆ†åˆ«æ¯”ä¹‹å‰çš„ç»Ÿä¸€SSLæŠ€æœ¯é¢†å…ˆäº†0.4%ã€1.48 FIDã€1.76%å’Œ1.53%ï¼ŒåŒæ—¶åœ¨æ•ˆç‡ä¸Šé«˜å‡º60.8%ã€‚æ­¤å¤–ï¼Œç´¢æ£®åœ¨ç›´æ¥çº¿æ€§æ¢æµ‹ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„å•è£å‰ªMIMæŠ€æœ¯ï¼Œå¹¶åœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè¿™çªæ˜¾äº†ç»Ÿä¸€SSLæ¨¡å‹çš„æ˜¾è‘—æ”¹è¿›å’Œçªç ´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15060v2">PDF</a> The source code is available in <a target="_blank" rel="noopener" href="https://github.com/ImaGonEs/Sorcen">https://github.com/ImaGonEs/Sorcen</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ç»Ÿä¸€è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶Sorcenï¼Œå®ƒç»“åˆäº†å¯¹æ¯”å’Œé‡å»ºç›®æ ‡ï¼Œæ— éœ€é¢å¤–çš„å›¾åƒè£å‰ªæˆ–å¢å¼ºå³å¯è¿›è¡Œè®­ç»ƒã€‚Sorcenåœ¨é¢„è®¡ç®—ä»¤ç‰Œä¸Šè¿è¡Œï¼Œæ¶ˆé™¤äº†åœ¨çº¿ä»¤ç‰Œè½¬æ¢çš„éœ€è¦ï¼Œä»è€Œå¤§å¤§æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚åœ¨ImageNet-1kä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSorcenåœ¨ç»Ÿä¸€SSLé¢†åŸŸä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sorcenæ˜¯ä¸€ä¸ªæ–°å‹çš„ç»Ÿä¸€SSLæ¡†æ¶ï¼Œç»“åˆäº†å¯¹æ¯”å’Œé‡å»ºç›®æ ‡ã€‚</li>
<li>Sorcenåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„å›¾åƒè£å‰ªæˆ–å¢å¼ºå³å¯è¿›è¡Œè®­ç»ƒã€‚</li>
<li>Sorcené€šè¿‡ç”Ÿæˆè¯­ä¹‰ä»¤ç‰Œç©ºé—´ä¸­çš„å›å£°æ ·æœ¬ï¼Œå½¢æˆå¯¹æ¯”æ­£é…å¯¹ã€‚</li>
<li>Sorcenåœ¨é¢„è®¡ç®—ä»¤ç‰Œä¸Šè¿è¡Œï¼Œæ— éœ€åœ¨çº¿ä»¤ç‰Œè½¬æ¢ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>Sorcenåœ¨ImageNet-1kä¸Šçš„å®éªŒè¡¨ç°å‡ºè‰²ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¿‡äº†ä¹‹å‰çš„SSLæ–¹æ³•ã€‚</li>
<li>Sorcenåœ¨ç»Ÿä¸€SSLé¢†åŸŸå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›å’Œçªç ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a880d1ae28ddbb8e432db5b9f355f952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c0439c7a81af2941727063a1f161805.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7752ace7b23f3430fc3279a1c0e402f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a0eeba2eca04982030c2592cc91ec63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c34fc7cdff00c2f9b0dedb17067c890f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="JuDGE-Benchmarking-Judgment-Document-Generation-for-Chinese-Legal-System"><a href="#JuDGE-Benchmarking-Judgment-Document-Generation-for-Chinese-Legal-System" class="headerlink" title="JuDGE: Benchmarking Judgment Document Generation for Chinese Legal   System"></a>JuDGE: Benchmarking Judgment Document Generation for Chinese Legal   System</h2><p><strong>Authors:Weihang Su, Baoqing Yue, Qingyao Ai, Yiran Hu, Jiaqi Li, Changyue Wang, Kaiyuan Zhang, Yueyue Wu, Yiqun Liu</strong></p>
<p>This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: <a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE">https://github.com/oneal2000/JuDGE</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†JuDGEï¼ˆåˆ¤å†³æ–‡ä¹¦ç”Ÿæˆè¯„ä¼°ï¼‰è¿™ä¸€æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°ä¸­æ–‡æ³•å¾‹ä½“ç³»ä¸­åˆ¤å†³æ–‡ä¹¦ç”Ÿæˆçš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä»»åŠ¡å®šä¹‰ä¸ºæ ¹æ®ç»™å®šçš„æ¡ˆä»¶äº‹å®æè¿°ç”Ÿæˆå®Œæ•´çš„æ³•å¾‹åˆ¤å†³ä¹¦ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«æ¥è‡ªçœŸå®æ³•å¾‹æ¡ˆä»¶çš„æ¡ˆæƒ…æè¿°ä»¥åŠä¸å…¶ç›¸åº”çš„å®Œæ•´åˆ¤å†³ä¹¦ï¼Œè¿™äº›åˆ¤å†³ä¹¦ä½œä¸ºè¯„ä¼°ç”Ÿæˆæ–‡æ¡£è´¨é‡çš„çœŸå®æ ‡å‡†ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸¤ä¸ªå¤–éƒ¨æ³•å¾‹è¯­æ–™åº“è¿›è¡Œæ‰©å……ï¼Œä¸ºä»»åŠ¡æä¾›äº†é¢å¤–çš„æ³•å¾‹çŸ¥è¯†ï¼šä¸€ä¸ªåŒ…å«æ³•è§„å’Œæ¡ä¾‹ï¼Œå¦ä¸€ä¸ªåˆ™åŒ…å«å¤§é‡è¿‡å»çš„åˆ¤å†³ä¹¦ã€‚æˆ‘ä»¬ä¸æ³•å¾‹ä¸“ä¸šäººå£«åˆä½œï¼Œå»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»å„ä¸ªç»´åº¦è¯„ä¼°ç”Ÿæˆçš„åˆ¤å†³ä¹¦çš„è´¨é‡ã€‚æˆ‘ä»¬è¯„ä¼°äº†å„ç§åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬å°æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ã€å¾®è°ƒä»¥åŠå¤šæºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å‡ä½¿ç”¨é€šç”¨å’Œæ³•å¾‹é¢†åŸŸçš„LLMsã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶RAGæ–¹æ³•å¯ä»¥æœ‰æ•ˆæé«˜æ­¤ä»»åŠ¡æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/oneal2000/JuDGEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14258v2">PDF</a> </p>
<p><strong>Summary</strong><br>ä¸­å›½æ³•å¾‹ç³»ç»Ÿä¸‹åˆ¤å†³æ–‡ä¹¦ç”Ÿæˆè¯„ä¼°çš„æ–°åŸºå‡†JuDGEè¢«ä»‹ç»ã€‚è¯¥ä»»åŠ¡è¢«å®šä¹‰ä¸ºä»ç»™å®šæ¡ˆä¾‹çš„äº‹å®æè¿°ç”Ÿæˆå®Œæ•´çš„æ³•å¾‹åˆ¤å†³ä¹¦ã€‚ä¸ºæ¨è¿›æ­¤åŸºå‡†ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«çœŸå®æ¡ˆä¾‹äº‹å®æè¿°ä¸å…¶å®Œæ•´åˆ¤å†³ä¹¦çš„é…å¯¹æ•°æ®é›†ï¼Œä½œä¸ºç”Ÿæˆæ–‡æ¡£è´¨é‡çš„è¯„ä¼°åŸºå‡†ã€‚æ­¤å¤–ï¼Œè¿˜é€šè¿‡ä¸¤ä¸ªå¤–éƒ¨æ³•å¾‹è¯­æ–™åº“æä¾›é¢å¤–çš„æ³•å¾‹çŸ¥è¯†ã€‚ä¸å¾‹å¸ˆåˆä½œï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°ç”Ÿæˆåˆ¤å†³ä¹¦çš„è´¨é‡ã€‚å¯¹å¤šç§åŸºçº¿æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å°æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ã€å¾®è°ƒä»¥åŠå¤šæºæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œä½¿ç”¨é€šç”¨å’Œæ³•å¾‹é¢†åŸŸçš„LLMã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶RAGæ–¹æ³•èƒ½æå‡ä»»åŠ¡è¡¨ç°ï¼Œä½†ä»æœ‰è®¸å¤šæ”¹è¿›ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†æ–°çš„åˆ¤å†³æ–‡ä¹¦ç”Ÿæˆè¯„ä¼°åŸºå‡†JuDGEã€‚</li>
<li>ä»»åŠ¡å®šä¹‰ä¸ºä»æ¡ˆä¾‹äº‹å®æè¿°ç”Ÿæˆå®Œæ•´çš„æ³•å¾‹åˆ¤å†³ä¹¦ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåŒ…å«çœŸå®æ¡ˆä¾‹äº‹å®æè¿°å’Œåˆ¤å†³ä¹¦çš„é…å¯¹æ•°æ®é›†ã€‚</li>
<li>åˆ©ç”¨ä¸¤ä¸ªå¤–éƒ¨æ³•å¾‹è¯­æ–™åº“æä¾›æ³•å¾‹çŸ¥è¯†ã€‚</li>
<li>ä¸å¾‹å¸ˆåˆä½œå»ºç«‹äº†å…¨é¢çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>è¯„ä¼°äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬å°æ ·æœ¬å­¦ä¹ ã€å¾®è°ƒåŠå¤šæºæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46635abb4e981bc03a6ee514cac96004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c12c7e7f91bd6666772101cdba937f7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4a0d37c3ddd60aee0fb04a037e968a1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rationalization-Models-for-Text-to-SQL"><a href="#Rationalization-Models-for-Text-to-SQL" class="headerlink" title="Rationalization Models for Text-to-SQL"></a>Rationalization Models for Text-to-SQL</h2><p><strong>Authors:Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian</strong></p>
<p>We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç”Ÿæˆæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰ç†ç”±çš„æ¡†æ¶ï¼Œä»¥å¢å¼ºæ–‡æœ¬åˆ°SQLæ¨¡å‹çš„å¾®è°ƒã€‚è¿™äº›ç†ç”±åŒ…æ‹¬ä¸­é—´çš„SQLè¯­å¥å’Œè§£é‡Šï¼Œä½œä¸ºæ„å»ºæœ€ç»ˆSQLæŸ¥è¯¢çš„å¢é‡æ­¥éª¤ã€‚è¿™ä¸ªè¿‡ç¨‹ä»æ‰‹åŠ¨æ ‡æ³¨ä¸€å°éƒ¨åˆ†ä¾‹å­å¼€å§‹ï¼Œç„¶åç”¨è¿™äº›ä¾‹å­æ¥æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¿­ä»£ã€åŠ¨æ€çš„å°‘é‡çŸ¥è¯†è’¸é¦è¿‡ç¨‹ï¼ˆä»æ•™å¸ˆæ¨¡å‹å¼€å§‹ï¼‰ã€‚éšååœ¨éªŒè¯è¿‡çš„åˆ†è§£æŸ¥è¯¢ä¸Šè®­ç»ƒåˆç†åŒ–æ¨¡å‹ï¼Œä¸ºæ–‡æœ¬åˆ°SQLæ•°æ®é›†æä¾›ä¸°å¯Œçš„åˆæˆCoTæ³¨é‡Šã€‚ä¸ºäº†è¯„ä¼°è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨BIRDæ•°æ®é›†ä¸Šå¯¹å¸¦æœ‰å’Œä¸å¸¦è¿™äº›ç†ç”±çš„å°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚ç»“æœè¡¨æ˜ï¼Œé€æ­¥æŸ¥è¯¢ç”Ÿæˆæé«˜äº†æ‰§è¡Œå‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä¸­ç­‰å’Œé«˜åº¦å¤æ‚çš„æŸ¥è¯¢ï¼ŒåŒæ—¶æé«˜äº†å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06759v4">PDF</a> Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs</p>
<p><strong>Summary</strong></p>
<p>åŸºäºChain-of-Thoughtï¼ˆCoTï¼‰æ¡†æ¶ç”Ÿæˆå¢å¼ºæ–‡æœ¬åˆ°SQLæ¨¡å‹å¾®è°ƒçš„ç†ç”±ã€‚è¿™äº›ç†ç”±åŒ…æ‹¬ä¸­é—´SQLè¯­å¥å’Œè§£é‡Šï¼Œä½œä¸ºæ„å»ºæœ€ç»ˆSQLæŸ¥è¯¢çš„é€æ­¥æ­¥éª¤ã€‚é€šè¿‡æ‰‹åŠ¨æ ‡æ³¨å°‘é‡ç¤ºä¾‹ï¼Œç”¨äºæç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿­ä»£ã€åŠ¨æ€çš„å°‘æ ·æœ¬çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­ä»æ•™å¸ˆæ¨¡å‹å­¦ä¹ ã€‚éšååœ¨éªŒè¯çš„åˆ†è§£æŸ¥è¯¢ä¸Šè®­ç»ƒè§£é‡Šæ¨¡å‹ï¼Œä¸ºæ–‡æœ¬åˆ°SQLæ•°æ®é›†æä¾›ä¸°å¯Œçš„åˆæˆCoTæ³¨é‡Šã€‚åœ¨BIRDæ•°æ®é›†ä¸Šå¾®è°ƒå¸¦æœ‰å’Œä¸å¸¦æœ‰è¿™äº›ç†ç”±çš„å°å‹è¯­è¨€æ¨¡å‹çš„ç»“æœè¡¨æ˜ï¼Œé€æ­¥ç”ŸæˆæŸ¥è¯¢æé«˜äº†æ‰§è¡Œå‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä¸­ç­‰å’Œé«˜åº¦å¤æ‚çš„æŸ¥è¯¢ï¼ŒåŒæ—¶æé«˜äº†å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥Chain-of-Thoughtï¼ˆCoTï¼‰æ¡†æ¶ç”Ÿæˆç†ç”±ä»¥å¢å¼ºæ–‡æœ¬åˆ°SQLæ¨¡å‹çš„å¾®è°ƒã€‚</li>
<li>è¿™äº›ç†ç”±åŒ…æ‹¬ä¸­é—´SQLè¯­å¥å’Œè§£é‡Šï¼Œä½œä¸ºæ„å»ºæœ€ç»ˆSQLæŸ¥è¯¢çš„é€æ­¥æ­¥éª¤ã€‚</li>
<li>é€šè¿‡æ‰‹åŠ¨æ ‡æ³¨å°‘é‡ç¤ºä¾‹æ¥å¯åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨è¿­ä»£ã€åŠ¨æ€çš„å°‘æ ·æœ¬çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä»æ•™å¸ˆæ¨¡å‹å­¦ä¹ ã€‚</li>
<li>è®­ç»ƒä¸€ä¸ªè§£é‡Šæ¨¡å‹æ¥å¤„ç†éªŒè¯åçš„åˆ†è§£æŸ¥è¯¢ã€‚</li>
<li>æ­¤æ–¹æ³•èƒ½ç”Ÿæˆä¸°å¯Œçš„åˆæˆCoTæ³¨é‡Šï¼Œé€‚ç”¨äºæ–‡æœ¬åˆ°SQLæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-722cf4bfe3f8c158fdfe9b38739b89ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac7591afcabb4de48b07038449651318.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffb3a4fac446037e8b054310b875c4fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-465989f087ba4c2ed99c1b84af82473f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion"><a href="#Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion" class="headerlink" title="Synthetic Prior for Few-Shot Drivable Head Avatar Inversion"></a>Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</h2><p><strong>Authors:Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart</strong></p>
<p>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle three major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, the use of real data is strictly regulated (e.g., under the General Data Protection Regulation, which mandates frequent deletion of models and data to accommodate a situation when a participantâ€™s consent is withdrawn). Synthetic data, free from these constraints, is an appealing alternative. Third, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to SOTA monocular and GAN-based methods, SynShot significantly improves novel view and expression synthesis. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SynShotï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆæˆå…ˆéªŒçš„æ–°é¢–æ–¹æ³•ï¼Œç”¨äºè¿›è¡ŒåŸºäºå°‘é‡æ ·æœ¬çš„é©¾é©¶å¤´éƒ¨è‚–åƒå€’åºç”Ÿæˆã€‚æˆ‘ä»¬è§£å†³äº†ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œè®­ç»ƒå¯æ§çš„3Dç”Ÿæˆç½‘ç»œéœ€è¦å¤§é‡çš„å„ç§åºåˆ—ï¼Œè€Œè¿™äº›åºåˆ—çš„å›¾åƒå’Œé«˜è´¨é‡è·Ÿè¸ªç½‘æ ¼å¹¶ä¸æ€»æ˜¯å¯ç”¨ã€‚å…¶æ¬¡ï¼ŒçœŸå®æ•°æ®çš„ä½¿ç”¨å—åˆ°ä¸¥æ ¼ç›‘ç®¡ï¼ˆä¾‹å¦‚ï¼Œæ ¹æ®ã€Šé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ã€‹ï¼Œå½“å‚ä¸è€…æ’¤å›åŒæ„æ—¶ï¼Œå¿…é¡»é¢‘ç¹åˆ é™¤æ¨¡å‹å’Œæ•°æ®è¿›è¡Œé€‚åº”ï¼‰ã€‚ä¸å—è¿™äº›çº¦æŸçš„åˆæˆæ•°æ®æ˜¯ä¸€ä¸ªå¸å¼•äººçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç¬¬ä¸‰ï¼Œæœ€å…ˆè¿›çš„å•çœ¼è‚–åƒæ¨¡å‹åœ¨æ¨å¹¿åˆ°æ–°çš„è§†è§’å’Œè¡¨æƒ…æ—¶é‡åˆ°å›°éš¾ï¼Œç¼ºä¹å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†å¹¶ä¸”ç»å¸¸è¿‡åº¦æ‹Ÿåˆç‰¹å®šçš„è§†è§’åˆ†å¸ƒã€‚å—åªä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»åŒ…å«å„ç§èº«ä»½ã€è¡¨æƒ…å’Œè§†è§’çš„å¤§é‡åˆæˆå¤´éƒ¨æ•°æ®ä¸­å­¦ä¹ å…ˆéªŒæ¨¡å‹ã€‚å‡­å€Ÿå°‘é‡çš„è¾“å…¥å›¾åƒï¼ŒSynShotå¯¹é¢„è®­ç»ƒçš„åˆæˆå…ˆéªŒè¿›è¡Œå¾®è°ƒï¼Œä»¥å¼¥åˆé¢†åŸŸå·®è·ï¼Œä»è€Œå»ºç«‹ä¸€ä¸ªé€¼çœŸçš„å¤´éƒ¨è‚–åƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ¨å¹¿åˆ°æ–°çš„è¡¨æƒ…å’Œè§†è§’ã€‚æˆ‘ä»¬ä½¿ç”¨3Dé«˜æ–¯å–·ç»˜å’Œå·ç§¯ç¼–ç å™¨è§£ç å™¨æ¥å»ºç«‹å¤´éƒ¨è‚–åƒæ¨¡å‹ï¼Œè¾“å‡ºUVçº¹ç†ç©ºé—´çš„é«˜æ–¯å‚æ•°ã€‚ä¸ºäº†å¤„ç†å¤´éƒ¨å„éƒ¨åˆ†çš„å»ºæ¨¡å¤æ‚æ€§å·®å¼‚ï¼ˆä¾‹å¦‚çš®è‚¤å’Œå¤´å‘ï¼‰ï¼Œæˆ‘ä»¬å°†å…ˆéªŒå€¼åµŒå…¥å…·æœ‰é’ˆå¯¹å„éƒ¨åˆ†åŸå§‹æ•°é‡è¿›è¡Œä¸Šé‡‡æ ·çš„æ˜¾å¼æ§åˆ¶åŠŸèƒ½ã€‚ä¸æœ€å…ˆè¿›å•çœ¼ä»¥åŠåŸºäºGANçš„æ–¹æ³•ç›¸æ¯”ï¼ŒSynShotæå¤§åœ°æé«˜äº†æ–°å‹è§†è§’å’Œè¡¨æƒ…çš„åˆæˆæ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06903v2">PDF</a> Accepted to CVPR25 Website: <a target="_blank" rel="noopener" href="https://zielon.github.io/synshot/">https://zielon.github.io/synshot/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºåˆæˆå…ˆéªŒçš„SynShotæ–¹æ³•ï¼Œç”¨äºå°‘æ ·æœ¬é©±åŠ¨å¤´éƒ¨åŒ–èº«åè½¬ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šç¼ºä¹å¤šæ ·åºåˆ—å›¾åƒå’Œé«˜å“è´¨è¿½è¸ªç½‘æ ¼çš„è®­ç»ƒæ•°æ®ã€çœŸå®æ•°æ®ä½¿ç”¨å—é™ä»¥åŠæœ€æ–°å•çœ¼åŒ–èº«æ¨¡å‹ç¼ºä¹é€šç”¨æ€§å’Œè§†è§’å˜åŒ–çš„èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨åˆæˆæ•°æ®ä½œä¸ºå…ˆéªŒæ¨¡å‹ï¼Œå¹¶ç»“åˆåœ¨å¤§å‹åˆæˆå¤´éƒ¨æ•°æ®é›†ä¸Šè®­ç»ƒçš„å…ˆéªŒæ¨¡å‹ï¼ŒSynShotèƒ½å¤Ÿç²¾ç»†è°ƒæ•´å‚æ•°ä»¥ç¼©å°é¢†åŸŸå·®è·ï¼Œå»ºç«‹é€¼çœŸå¤´éƒ¨åŒ–èº«ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”æ–°çš„è¡¨è¾¾å’Œè§†è§’ã€‚ä½¿ç”¨3Dé«˜æ–¯å¹³é“ºæŠ€æœ¯å’Œå·ç§¯ç¼–ç å™¨è§£ç å™¨è¿›è¡Œå¤´éƒ¨å»ºæ¨¡ï¼Œå¹¶é‡‡ç”¨æ˜ç¡®æ§åˆ¶çš„å…ˆéªŒåµŒå…¥æ¥å¤„ç†å¤´éƒ¨ä¸åŒéƒ¨åˆ†çš„å»ºæ¨¡å¤æ‚æ€§ã€‚ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›å•çœ¼å’ŒåŸºäºGANçš„æ–¹æ³•ï¼ŒSynShotå¤§å¹…æé«˜äº†æ–°å‹è¡¨è¾¾å’Œè§†è§’çš„åˆæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SynShotæ˜¯ä¸€ç§åŸºäºåˆæˆå…ˆéªŒçš„å°‘æ•°æ ·æœ¬é©±åŠ¨å¤´éƒ¨åŒ–èº«åè½¬æ–°æ–¹æ³•ã€‚</li>
<li>è§£å†³äº†è®­ç»ƒå¯æ§çš„3Dç”Ÿæˆç½‘ç»œä¸­çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹å¤šæ ·åºåˆ—æ•°æ®ã€çœŸå®æ•°æ®çš„ä½¿ç”¨å—é™ä»¥åŠæ¨¡å‹ç¼ºä¹é€šç”¨æ€§å’Œè§†è§’å˜åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨åˆæˆæ•°æ®ä½œä¸ºå…ˆéªŒæ¨¡å‹ï¼Œå¹¶é€šè¿‡å¤§å‹åˆæˆå¤´éƒ¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ç»“åˆ3Dé«˜æ–¯å¹³é“ºæŠ€æœ¯å’Œå·ç§¯ç¼–ç å™¨è§£ç å™¨è¿›è¡Œå¤´éƒ¨å»ºæ¨¡ï¼Œå¹¶å¤„ç†ä¸åŒéƒ¨åˆ†çš„å»ºæ¨¡å¤æ‚æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd0cc9a3a4a5ca64e316c0f6919c49d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc4fd6a640e4f770326012aab1b0575e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34b5f656b91512006dff6313ea06256e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91869d7ecc0a53622008866a9ae4c5fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8020a341dbd40737d91ef6aa6c3efdeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfb08e011b3ab317551b561dbc247e94.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Defending-Multimodal-Backdoored-Models-by-Repulsive-Visual-Prompt-Tuning"><a href="#Defending-Multimodal-Backdoored-Models-by-Repulsive-Visual-Prompt-Tuning" class="headerlink" title="Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning"></a>Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning</h2><p><strong>Authors:Zhifang Zhang, Shuo He, Haobo Wang, Bingquan Shen, Lei Feng</strong></p>
<p>Multimodal contrastive learning models (e.g., CLIP) can learn high-quality representations from large-scale image-text datasets, yet they exhibit significant vulnerabilities to backdoor attacks, raising serious safety concerns. In this paper, we disclose that CLIPâ€™s vulnerabilities primarily stem from its excessive encoding of class-irrelevant features, which can compromise the modelâ€™s visual feature resistivity to input perturbations, making it more susceptible to capturing the trigger patterns inserted by backdoor attacks. Inspired by this finding, we propose Repulsive Visual Prompt Tuning (RVPT), a novel defense approach that employs specially designed deep visual prompt tuning and feature-repelling loss to eliminate excessive class-irrelevant features while simultaneously optimizing cross-entropy loss to maintain clean accuracy. Unlike existing multimodal backdoor defense methods that typically require the availability of poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot downstream clean samples and only tunes a small number of parameters. Empirical results demonstrate that RVPT tunes only 0.27% of the parameters relative to CLIP, yet it significantly outperforms state-of-the-art baselines, reducing the attack success rate from 67.53% to 2.76% against SoTA attacks and effectively generalizing its defensive capabilities across multiple datasets. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰å¯ä»¥ä»å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸­å­¦ä¹ é«˜è´¨é‡è¡¨ç¤ºï¼Œä½†å®ƒä»¬å¯¹åé—¨æ”»å‡»è¡¨ç°å‡ºæ˜¾è‘—è„†å¼±æ€§ï¼Œè¿™å¼•å‘äº†ä¸¥é‡çš„å®‰å…¨æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŠ«éœ²CLIPçš„è„†å¼±æ€§ä¸»è¦æºäºå…¶å¯¹ä¸ç±»åˆ«æ— å…³ç‰¹å¾çš„è¿‡åº¦ç¼–ç ï¼Œè¿™å¯èƒ½ä¼šæŸå®³æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨çš„è§†è§‰ç‰¹å¾æŠ—æ€§ï¼Œä½¿å…¶æ›´å®¹æ˜“æ•è·ç”±åé—¨æ”»å‡»æ’å…¥çš„è§¦å‘æ¨¡å¼ã€‚å—æ­¤å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºRepulsive Visual Prompt Tuningï¼ˆRVPTï¼‰çš„æ–°å‹é˜²å¾¡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨ä¸“é—¨è®¾è®¡çš„æ·±åº¦è§†è§‰æç¤ºè°ƒæ•´å’Œç‰¹å¾æ’æ–¥æŸå¤±ï¼Œä»¥æ¶ˆé™¤è¿‡å¤šçš„ä¸ç±»åˆ«æ— å…³çš„ç‰¹å¾ï¼ŒåŒæ—¶ä¼˜åŒ–äº¤å‰ç†µæŸå¤±ä»¥ä¿æŒæ¸…æ´å‡†ç¡®æ€§ã€‚ä¸é€šå¸¸éœ€è¦ä¸­æ¯’æ•°æ®æˆ–æ¶‰åŠå¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„ä¼ ç»Ÿå¤šæ¨¡æ€åé—¨é˜²å¾¡æ–¹æ³•ä¸åŒï¼ŒRVPTåˆ©ç”¨ä¸‹æ¸¸æ¸…æ´æ ·æœ¬çš„å°‘é‡é•œå¤´ï¼Œå¹¶ä¸”åªè°ƒæ•´å°‘æ•°å‚æ•°ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒRVPTä»…è°ƒæ•´CLIPçš„0.27%å‚æ•°ï¼Œå´æ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œå°†æ”»å‡»æˆåŠŸç‡ä»67.53%é™ä½åˆ°2.76%ï¼Œæœ‰æ•ˆæŠµå¾¡äº†SoTAæ”»å‡»ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é˜²å¾¡èƒ½åŠ›çš„æœ‰æ•ˆæ³›åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20392v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ­ç¤ºäº†CLIPç­‰å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹å¯¹åé—¨æ”»å‡»çš„è„†å¼±æ€§ï¼Œå…¶ä¸»è¦æºäºå¯¹ç±»æ— å…³ç‰¹å¾çš„è¿‡åº¦ç¼–ç ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åä¸ºRVPTçš„æ–°å‹é˜²å¾¡ç­–ç•¥ï¼Œå®ƒé€šè¿‡æ·±åº¦è§†è§‰æç¤ºè°ƒæ•´å’Œç‰¹å¾æ’æ–¥æŸå¤±ï¼Œå‡å°‘æ— å…³ç‰¹å¾çš„å¹²æ‰°ï¼ŒåŒæ—¶ä¼˜åŒ–äº¤å‰ç†µæŸå¤±ä»¥ä¿æŒæ¸…æ´å‡†ç¡®æ€§ã€‚ä¸å…¶ä»–éœ€è¦ä¸­æ¯’æ•°æ®æˆ–å¾®è°ƒæ•´ä¸ªæ¨¡å‹çš„å¤šæ¨¡æ€åé—¨é˜²å¾¡æ–¹æ³•ä¸åŒï¼ŒRVPTåˆ©ç”¨ä¸‹æ¸¸çš„å°‘é‡æ¸…æ´æ ·æœ¬ï¼Œä»…è°ƒæ•´å°‘é‡å‚æ•°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒRVPTä»…è°ƒæ•´CLIPçš„0.27%å‚æ•°ï¼Œå³å¯æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æŠ€æœ¯ï¼Œå°†æ”»å‡»æˆåŠŸç‡ä»67.53%é™ä½åˆ°2.76%ï¼Œå¹¶èƒ½åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæœ‰æ•ˆæ¨å¹¿å…¶é˜²å¾¡èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ˜“å—åé—¨æ”»å‡»å½±å“ï¼Œå­˜åœ¨å®‰å…¨éšæ‚£ã€‚</li>
<li>CLIPçš„è„†å¼±æ€§ä¸»è¦æºäºå¯¹ç±»æ— å…³ç‰¹å¾çš„è¿‡åº¦ç¼–ç ã€‚</li>
<li>RVPTæ˜¯ä¸€ç§æ–°å‹é˜²å¾¡ç­–ç•¥ï¼Œé€šè¿‡æ·±åº¦è§†è§‰æç¤ºè°ƒæ•´å’Œç‰¹å¾æ’æ–¥æŸå¤±æ¥å‡å°‘ç±»æ— å…³ç‰¹å¾çš„å¹²æ‰°ã€‚</li>
<li>RVPTä»…éœ€å°‘é‡ä¸‹æ¸¸æ¸…æ´æ ·æœ¬ï¼Œä»…è°ƒæ•´å°‘é‡æ¨¡å‹å‚æ•°ã€‚</li>
<li>RVPTæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æŠ€æœ¯ï¼Œå¤§å¹…é™ä½æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>RVPTçš„é˜²å¾¡èƒ½åŠ›åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå¾—åˆ°æœ‰æ•ˆæ¨å¹¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b763a267742c8983aa15a5e13ccb41d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e86bfffa15963e09982bbe81f24a171.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20bb51639e72792372438854d6293bba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a173b65f31f7e5b6d7db1209500388bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddca483dcb49ffe70ef429fe81b764e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c1b3e359f6d3b60ae272fe74626456.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FOCUS-Knowledge-enhanced-Adaptive-Visual-Compression-for-Few-shot-Whole-Slide-Image-Classification"><a href="#FOCUS-Knowledge-enhanced-Adaptive-Visual-Compression-for-Few-shot-Whole-Slide-Image-Classification" class="headerlink" title="FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole   Slide Image Classification"></a>FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole   Slide Image Classification</h2><p><strong>Authors:Zhengrui Guo, Conghao Xiong, Jiabo Ma, Qichen Sun, Lishuang Feng, Jinzhuo Wang, Hao Chen</strong></p>
<p>Few-shot learning presents a critical solution for cancer diagnosis in computational pathology (CPath), addressing fundamental limitations in data availability, particularly the scarcity of expert annotations and patient privacy constraints. A key challenge in this paradigm stems from the inherent disparity between the limited training set of whole slide images (WSIs) and the enormous number of contained patches, where a significant portion of these patches lacks diagnostically relevant information, potentially diluting the modelâ€™s ability to learn and focus on critical diagnostic features. While recent works attempt to address this by incorporating additional knowledge, several crucial gaps hinder further progress: (1) despite the emergence of powerful pathology foundation models (FMs), their potential remains largely untapped, with most approaches limiting their use to basic feature extraction; (2) current language guidance mechanisms attempt to align text prompts with vast numbers of WSI patches all at once, struggling to leverage rich pathological semantic information. To this end, we introduce the knowledge-enhanced adaptive visual compression framework, dubbed FOCUS, which uniquely combines pathology FMs with language prior knowledge to enable a focused analysis of diagnostically relevant regions by prioritizing discriminative WSI patches. Our approach implements a progressive three-stage compression strategy: we first leverage FMs for global visual redundancy elimination, and integrate compressed features with language prompts for semantic relevance assessment, then perform neighbor-aware visual token filtering while preserving spatial coherence. Extensive experiments on pathological datasets spanning breast, lung, and ovarian cancers demonstrate its superior performance in few-shot pathology diagnosis. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/dddavid4real/FOCUS">https://github.com/dddavid4real/FOCUS</a>. </p>
<blockquote>
<p>åœ¨è®¡ç®—ç—…ç†å­¦ï¼ˆCPathï¼‰ä¸­ï¼Œå°æ ·æœ¬å­¦ä¹ ä¸ºç™Œç—‡è¯Šæ–­æä¾›äº†å…³é”®è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†æ•°æ®å¯ç”¨æ€§æ–¹é¢çš„æ ¹æœ¬æ€§é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯ä¸“å®¶æ³¨é‡Šçš„ç¨€ç¼ºå’Œæ‚£è€…éšç§çº¦æŸã€‚è¯¥èŒƒå¼çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ¥è‡ªäºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰æœ‰é™è®­ç»ƒé›†ä¸æ‰€å«è¡¥ä¸çš„å·¨å¤§æ•°é‡ä¹‹é—´çš„å›ºæœ‰å·®å¼‚ï¼Œå…¶ä¸­å¾ˆå¤§ä¸€éƒ¨åˆ†è¡¥ä¸ç¼ºä¹è¯Šæ–­ç›¸å…³ä¿¡æ¯ï¼Œå¯èƒ½ä¼šç¨€é‡Šæ¨¡å‹å­¦ä¹ å’Œå…³æ³¨å…³é”®è¯Šæ–­ç‰¹å¾çš„èƒ½åŠ›ã€‚å°½ç®¡æœ€è¿‘çš„å·¥ä½œè¯•å›¾é€šè¿‡èå…¥é¢å¤–çŸ¥è¯†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å‡ ä¸ªå…³é”®å·®è·ä»ç„¶é˜»ç¢äº†è¿›ä¸€æ­¥çš„è¿›æ­¥ï¼šï¼ˆ1ï¼‰å°½ç®¡ç—…ç†åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰å·²ç»å‡ºç°ï¼Œä½†å®ƒä»¬çš„æ½œåŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æŒ–æ˜ï¼Œå¤§å¤šæ•°æ–¹æ³•ä»…é™äºåŸºæœ¬ç‰¹å¾æå–ï¼›ï¼ˆ2ï¼‰å½“å‰çš„è¯­è¨€æŒ‡å¯¼æœºåˆ¶è¯•å›¾ä¸€æ¬¡æ€§å°†æ–‡æœ¬æç¤ºä¸å¤§é‡çš„WSIè¡¥ä¸å¯¹é½ï¼Œéš¾ä»¥åˆ©ç”¨ä¸°å¯Œçš„ç—…ç†è¯­ä¹‰ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†çŸ¥è¯†å¢å¼ºè‡ªé€‚åº”è§†è§‰å‹ç¼©æ¡†æ¶ï¼Œåä¸ºFOCUSï¼Œå®ƒç‹¬ç‰¹åœ°ç»“åˆäº†ç—…ç†FMså’Œè¯­è¨€å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡ä¼˜å…ˆç­›é€‰å…·æœ‰é‰´åˆ«åŠ›çš„WSIè¡¥ä¸ï¼Œå®ç°å¯¹è¯Šæ–­ç›¸å…³åŒºåŸŸçš„èšç„¦åˆ†æã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†åˆ†é˜¶æ®µçš„é€æ­¥ä¸‰çº§å‹ç¼©ç­–ç•¥ï¼šæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨FMsè¿›è¡Œå…¨å±€è§†è§‰å†—ä½™æ¶ˆé™¤ï¼Œå¹¶å°†å‹ç¼©ç‰¹å¾ä¸è¯­è¨€æç¤ºç›¸ç»“åˆè¿›è¡Œè¯­ä¹‰ç›¸å…³æ€§è¯„ä¼°ï¼Œç„¶åæ‰§è¡Œè€ƒè™‘é‚»å±…çš„è§†è§‰ä»¤ç‰Œè¿‡æ»¤ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´ä¸€è‡´æ€§ã€‚åœ¨æ¶µç›–ä¹³è…ºç™Œã€è‚ºç™Œå’Œåµå·¢ç™Œçš„ç—…ç†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜ï¼Œå…¶åœ¨å°æ ·æœ¬ç—…ç†è¯Šæ–­ä¸­çš„å“è¶Šæ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dddavid4real/FOCUS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dddavid4real/FOCUSä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14743v2">PDF</a> Accepted by CVPRâ€™2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»ç–—è®¡ç®—ç—…ç†å­¦ï¼ˆCPathï¼‰é¢†åŸŸä¸­ï¼Œå°‘æ ·æœ¬å­¦ä¹ ä¸ºè§£å†³ç™Œç—‡è¯Šæ–­ä¸­çš„å…³é”®æŒ‘æˆ˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æ–‡ç« æŒ‡å‡ºå½“å‰é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®å¯ç”¨æ€§çš„é™åˆ¶å’Œå¤§é‡æ— å…³ä¿¡æ¯çš„å¹²æ‰°ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶â€”â€”FOCUSã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç—…ç†å­¦åŸºç¡€æ¨¡å‹å’Œè¯­è¨€å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡é€æ­¥å‹ç¼©ç­–ç•¥æ¥ä¸“æ³¨äºè¯Šæ–­ç›¸å…³åŒºåŸŸçš„åˆ†æï¼Œå¹¶å…·æœ‰è¾ƒé«˜çš„è¡¨ç°æ€§èƒ½ã€‚æ–‡ç« å¼ºè°ƒäº†å……åˆ†åˆ©ç”¨ç—…ç†åŸºç¡€æ¨¡å‹çš„ä»·å€¼å’Œè¯­è¨€æŒ‡å¯¼æœºåˆ¶çš„é‡è¦æ€§ã€‚ç›®å‰ç ”ç©¶å·²åº”ç”¨äºä¹³è…ºç™Œã€è‚ºç™Œå’Œåµå·¢ç™Œç­‰å¤šç§ç™Œç—‡çš„ç—…ç†æ•°æ®é›†ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šå…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-907d604341cc27facca3ba4d6299f4cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edcecbfdfb0ddfc846414175831c83c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a38b0e62edd5977398606831e9e96678.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Continual-Gesture-Learning-without-Data-via-Synthetic-Feature-Sampling"><a href="#Continual-Gesture-Learning-without-Data-via-Synthetic-Feature-Sampling" class="headerlink" title="Continual Gesture Learning without Data via Synthetic Feature Sampling"></a>Continual Gesture Learning without Data via Synthetic Feature Sampling</h2><p><strong>Authors:Zhenyu Lu, Hao Tang</strong></p>
<p>Data-Free Class Incremental Learning (DFCIL) aims to enable models to continuously learn new classes while retraining knowledge of old classes, even when the training data for old classes is unavailable. Although explored primarily with image datasets by researchers, this study focuses on investigating DFCIL for skeleton-based gesture classification due to its significant real-world implications, particularly considering the growing prevalence of VR&#x2F;AR headsets where gestures serve as the primary means of control and interaction. In this work, we made an intriguing observation: skeleton models trained with base classes(even very limited) demonstrate strong generalization capabilities to unseen classes without requiring additional training. Building on this insight, we developed Synthetic Feature Replay (SFR) that can sample synthetic features from class prototypes to replay for old classes and augment for new classes (under a few-shot setting). Our proposed method showcases significant advancements over the state-of-the-art, achieving up to 15% enhancements in mean accuracy across all steps and largely mitigating the accuracy imbalance between base classes and new classes. </p>
<blockquote>
<p>æ— æ•°æ®ç±»å¢é‡å­¦ä¹ ï¼ˆDFCILï¼‰æ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æ—§ç±»è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œç»§ç»­å­¦ä¹ æ–°ç±»å¹¶é‡æ–°è®­ç»ƒæ—§ç±»çš„çŸ¥è¯†ã€‚å°½ç®¡ç ”ç©¶è€…ä¸»è¦å¯¹å›¾åƒæ•°æ®é›†è¿›è¡Œäº†æ¢ç´¢ï¼Œä½†ç”±äºå…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„é‡å¤§åº”ç”¨æ„ä¹‰ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°VR&#x2F;ARå¤´ç›”æ—¥ç›Šæ™®åŠï¼Œæ‰‹åŠ¿ä½œä¸ºä¸»è¦æ§åˆ¶å’Œäº¤äº’æ‰‹æ®µï¼Œæœ¬ç ”ç©¶ä¸“æ³¨äºç ”ç©¶åŸºäºéª¨æ¶çš„æ‰‹åŠ¿åˆ†ç±»çš„DFCILã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šä½¿ç”¨åŸºç¡€ç±»è®­ç»ƒçš„éª¨æ¶æ¨¡å‹ï¼ˆå³ä½¿éå¸¸æœ‰é™ï¼‰åœ¨æœªè¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å¯¹æœªè§è¿‡çš„ç±»å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆæˆç‰¹å¾å›æ”¾ï¼ˆSFRï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»ç±»åŸå‹ä¸­é‡‡æ ·åˆæˆç‰¹å¾ä»¥å›æ”¾æ—§ç±»å¹¶ä¸ºæ–°ç±»æä¾›å¢å¼ºåŠŸèƒ½ï¼ˆåœ¨å°‘é‡è®¾ç½®ä¸‹ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€å…ˆè¿›çš„æŠ€æœ¯æœ‰æ˜¾è‘—çš„è¿›æ­¥ï¼Œåœ¨æ‰€æœ‰æ­¥éª¤ä¸Šçš„å¹³å‡å‡†ç¡®åº¦æé«˜äº†é«˜è¾¾15%ï¼Œå¹¶å¤§å¤§ç¼“è§£äº†åŸºç¡€ç±»å’Œæ–°ç±»ä¹‹é—´çš„å‡†ç¡®åº¦ä¸å¹³è¡¡é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12629v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ•°æ®æ— ä¾èµ–ç±»å¢é‡å­¦ä¹ ï¼ˆDFCILï¼‰æ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æ—§ç±»è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒæŒç»­å­¦ä¹ æ–°ç±»å¹¶é‡æ–°è®­ç»ƒæ—§ç±»çš„çŸ¥è¯†ã€‚è™½ç„¶ç ”ç©¶è€…ä¸»è¦ä½¿ç”¨å›¾åƒæ•°æ®é›†è¿›è¡Œæ¢ç´¢ï¼Œä½†æœ¬ç ”ç©¶å…³æ³¨äºå°†DFCILåº”ç”¨äºåŸºäºéª¨éª¼çš„æ‰‹åŠ¿åˆ†ç±»ï¼Œè¿™åœ¨è™šæ‹Ÿç°å®&#x2F;å¢å¼ºç°å®å¤´ç›”æ—¥ç›Šæ™®åŠçš„ç°å®ä¸–ç•Œä¸­å…·æœ‰é‡å¤§å®é™…æ„ä¹‰ã€‚ç ”ç©¶ä¸­å‘ç°ï¼Œä½¿ç”¨åŸºç¡€ç±»åˆ«è®­ç»ƒçš„éª¨éª¼æ¨¡å‹ï¼ˆå³ä½¿éå¸¸æœ‰é™ï¼‰åœ¨æœªè¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾ç¤ºå‡ºå¯¹æœªè§ç±»åˆ«çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†åˆæˆç‰¹å¾å›æ”¾ï¼ˆSFRï¼‰æŠ€æœ¯ï¼Œå¯ä»¥ä»ç±»åˆ«åŸå‹ä¸­é‡‡æ ·åˆæˆç‰¹å¾ä»¥å›æ”¾æ—§ç±»åˆ«å¹¶å¢å¼ºæ–°ç±»åˆ«ï¼ˆåœ¨å°‘é‡æ ·æœ¬è®¾ç½®ä¸‹ï¼‰ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨å¹³å‡å‡†ç¡®åº¦ä¸Šå®ç°äº†é«˜è¾¾15%çš„æå‡ï¼Œå¹¶æå¤§åœ°ç¼“è§£äº†åŸºç¡€ç±»åˆ«ä¸æ–°ç±»åˆ«ä¹‹é—´çš„å‡†ç¡®åº¦ä¸å¹³è¡¡é—®é¢˜ï¼Œè¶…è¿‡äº†ç°æœ‰æŠ€æœ¯çš„æ˜¾è‘—è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®æ— ä¾èµ–ç±»å¢é‡å­¦ä¹ ï¼ˆDFCILï¼‰èƒ½å¤Ÿåœ¨æ— æ—§ç±»è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä½¿æ¨¡å‹æŒç»­å­¦ä¹ æ–°ç±»åˆ«å¹¶é‡æ–°è®­ç»ƒæ—§ç±»çš„çŸ¥è¯†ã€‚</li>
<li>æœ¬ç ”ç©¶å…³æ³¨å°†DFCILåº”ç”¨äºåŸºäºéª¨éª¼çš„æ‰‹åŠ¿åˆ†ç±»ï¼Œå…·æœ‰ç°å®ä¸–ç•Œçš„é‡å¤§æ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨è™šæ‹Ÿç°å®&#x2F;å¢å¼ºç°å®å¤´ç›”çš„æ™®åŠèƒŒæ™¯ä¸‹ã€‚</li>
<li>éª¨éª¼æ¨¡å‹åœ¨æœªè§ç±»åˆ«ä¸Šæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå³ä½¿ä½¿ç”¨æœ‰é™çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>æå‡ºåˆæˆç‰¹å¾å›æ”¾ï¼ˆSFRï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿä»ç±»åˆ«åŸå‹ä¸­é‡‡æ ·åˆæˆç‰¹å¾ï¼Œä»¥å›æ”¾æ—§ç±»åˆ«å¹¶å¢å¼ºæ–°ç±»åˆ«çš„å­¦ä¹ ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨å¹³å‡å‡†ç¡®åº¦ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ï¼Œè¶…è¿‡äº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿç¼“è§£åŸºç¡€ç±»åˆ«ä¸æ–°ç±»åˆ«ä¹‹é—´çš„å‡†ç¡®åº¦ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6233e46195057f0d0667db54ecd01655.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bb85bfe8632255f3b34cb3df2ec2b84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-662b247c542ee78472cdc670cc2dc4a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce937734df816d99f2de213c9ad86b88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1006dbe7474cdc38a76d276e207bd251.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acd56e059c9e543e85fb1850061948ee.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Tiny-models-from-tiny-data-Textual-and-null-text-inversion-for-few-shot-distillation"><a href="#Tiny-models-from-tiny-data-Textual-and-null-text-inversion-for-few-shot-distillation" class="headerlink" title="Tiny models from tiny data: Textual and null-text inversion for few-shot   distillation"></a>Tiny models from tiny data: Textual and null-text inversion for few-shot   distillation</h2><p><strong>Authors:Erik Landolsi, Fredrik Kahl</strong></p>
<p>Few-shot learning deals with problems such as image classification using very few training examples. Recent vision foundation models show excellent few-shot transfer abilities, but are large and slow at inference. Using knowledge distillation, the capabilities of high-performing but slow models can be transferred to tiny, efficient models. However, common distillation methods require a large set of unlabeled data, which is not available in the few-shot setting. To overcome this lack of data, there has been a recent interest in using synthetic data. We expand on this line of research by presenting a novel diffusion model inversion technique (TINT) combining the diversity of textual inversion with the specificity of null-text inversion. Using this method in a few-shot distillation pipeline leads to state-of-the-art accuracy among small student models on popular benchmarks, while being significantly faster than prior work. Popular few-shot benchmarks involve evaluation over a large number of episodes, which is computationally cumbersome for methods involving synthetic data generation. We also present a theoretical analysis on how the accuracy estimator variance depends on the number of episodes and query examples, and use these results to lower the computational effort required for method evaluation. Finally, to further motivate the use of generative models in few-shot distillation, we demonstrate that our method outperforms training on real data mined from the dataset used in the original diffusion model training. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/pixwse/tiny2">https://github.com/pixwse/tiny2</a>. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬å­¦ä¹ å¤„ç†çš„æ˜¯ä½¿ç”¨æå°‘è®­ç»ƒæ ·æœ¬è¿›è¡Œå›¾åƒåˆ†ç±»ç­‰é—®é¢˜ã€‚æœ€è¿‘çš„è§†è§‰åŸºç¡€æ¨¡å‹è¡¨ç°å‡ºå‡ºè‰²çš„å°‘é‡æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œä½†åœ¨æ¨ç†æ—¶ä½“ç§¯åºå¤§ä¸”é€Ÿåº¦è¾ƒæ…¢ã€‚åˆ©ç”¨çŸ¥è¯†è’¸é¦ï¼Œé«˜æ€§èƒ½ä½†é€Ÿåº¦è¾ƒæ…¢çš„æ¨¡å‹çš„èƒ½åŠ›å¯ä»¥è½¬ç§»åˆ°å¾®å°ã€é«˜æ•ˆçš„æ¨¡å‹ä¸Šã€‚ç„¶è€Œï¼Œå¸¸è§çš„è’¸é¦æ–¹æ³•éœ€è¦å¤§é‡æœªæ ‡è®°æ•°æ®ï¼Œè¿™åœ¨å°‘é‡æ ·æœ¬è®¾ç½®ä¸­æ˜¯æ— æ³•è·å¾—çš„ã€‚ä¸ºäº†å…‹æœæ•°æ®ç¼ºä¹çš„é—®é¢˜ï¼Œæœ€è¿‘å¼€å§‹æœ‰å…´è¶£ä½¿ç”¨åˆæˆæ•°æ®ã€‚æˆ‘ä»¬é€šè¿‡å¯¹æ–‡æœ¬åè½¬çš„å¤šæ ·æ€§å’Œç©ºæ–‡æœ¬åè½¬çš„ç‰¹å¼‚æ€§ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ‰©æ•£æ¨¡å‹åè½¬æŠ€æœ¯ï¼ˆTINTï¼‰ã€‚å°†è¿™ç§æŠ€æœ¯åº”ç”¨äºå°‘é‡æ ·æœ¬è’¸é¦ç®¡é“ä¸­ï¼Œå¯ä»¥åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šå®ç°å°å‹å­¦ç”Ÿæ¨¡å‹çš„æœ€æ–°å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—å¿«äºå…ˆå‰çš„å·¥ä½œã€‚æµè¡Œçš„å°‘é‡æ ·æœ¬åŸºå‡†æµ‹è¯•æ¶‰åŠå¤§é‡å›åˆçš„è¯„ä»·ï¼Œè¿™å¯¹äºæ¶‰åŠåˆæˆæ•°æ®ç”Ÿæˆçš„æ–¹æ³•åœ¨è®¡ç®—ä¸Šæ˜¯å¾ˆç¹ççš„ã€‚æˆ‘ä»¬è¿˜ä»ç†è®ºä¸Šåˆ†æäº†å‡†ç¡®åº¦ä¼°ç®—å™¨çš„æ–¹å·®å¦‚ä½•å–å†³äºå›åˆæ•°å’ŒæŸ¥è¯¢æ ·æœ¬æ•°ï¼Œå¹¶åˆ©ç”¨è¿™äº›ç»“æœé™ä½äº†æ–¹æ³•è¯„ä¼°æ‰€éœ€çš„è®¡ç®—é‡ã€‚æœ€åï¼Œä¸ºäº†è¿›ä¸€æ­¥æ¨åŠ¨ç”Ÿæˆæ¨¡å‹åœ¨å°‘é‡æ ·æœ¬è’¸é¦ä¸­çš„åº”ç”¨ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸå§‹æ‰©æ•£æ¨¡å‹è®­ç»ƒæ‰€ç”¨çš„æ•°æ®é›†ä¸­æŒ–æ˜å‡ºçš„çœŸå®æ•°æ®è®­ç»ƒè¡¨ç°ä¸Šæ›´èƒœä¸€ç­¹ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/pixwse/tiny2%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/pixwse/tiny2è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03146v2">PDF</a> 24 pages (13 main pages + references and appendix)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°æ ·å­¦ä¹ çš„ç›¸å…³é—®é¢˜ï¼Œå¦‚ä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚å°½ç®¡å½“å‰çš„å¤§å‹è§†è§‰æ¨¡å‹åœ¨å°æ ·å­¦ä¹ ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç§»èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä½“ç§¯åºå¤§ï¼Œæ¨æ–­é€Ÿåº¦è¾ƒæ…¢ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯å°†é«˜æ€§èƒ½ä½†ç¼“æ…¢çš„æ¨¡å‹çš„èƒ½åŠ›è½¬ç§»åˆ°å°å‹é«˜æ•ˆæ¨¡å‹ä¸Šã€‚ä½†ç”±äºå¸¸è§çš„è’¸é¦æ–¹æ³•éœ€è¦å¤§é‡æœªæ ‡è®°æ•°æ®ï¼Œè¿™åœ¨å°æ ·è®¾ç½®ä¸­æ˜¯ä¸å¯ç”¨çš„ï¼Œå› æ­¤ç ”ç©¶äººå‘˜å¼€å§‹å…³æ³¨ä½¿ç”¨åˆæˆæ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ‰©æ•£æ¨¡å‹åæ¼”æŠ€æœ¯ï¼ˆTINTï¼‰ï¼Œç»“åˆäº†æ–‡æœ¬åæ¼”çš„å¤šæ ·æ€§å’Œç©ºæ–‡æœ¬åæ¼”çš„ç‰¹å¼‚æ€§ã€‚åœ¨å°‘é‡æ ·æœ¬è’¸é¦ç®¡é“ä¸­ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œå°å‹å­¦ç”Ÿæ¨¡å‹åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¿«ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¯¹å‡†ç¡®ä¼°è®¡å™¨æ–¹å·®ä¸äº‹ä»¶æ•°é‡å’ŒæŸ¥è¯¢ç¤ºä¾‹ä¹‹é—´çš„å…³ç³»è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œä»¥é™ä½æ–¹æ³•è¯„ä¼°çš„è®¡ç®—è´Ÿæ‹…ã€‚æœ€åï¼Œä¸ºäº†æ¨åŠ¨ç”Ÿæˆæ¨¡å‹åœ¨å°‘é‡æ ·æœ¬è’¸é¦ä¸­çš„åº”ç”¨ï¼Œæœ¬æ–‡è¯æ˜äº†è¯¥æ–¹æ³•ä¼˜äºåœ¨åŸå§‹æ‰©æ•£æ¨¡å‹è®­ç»ƒæ‰€ä½¿ç”¨çš„æ•°æ®é›†ä¸ŠæŒ–æ˜çœŸå®æ•°æ®çš„è®­ç»ƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot learningä¸»è¦è§£å†³ä½¿ç”¨æå°‘é‡è®­ç»ƒæ ·æœ¬è¿›è¡Œå›¾åƒåˆ†ç±»ç­‰é—®é¢˜ã€‚</li>
<li>ç°æœ‰çš„å¤§å‹è§†è§‰æ¨¡å‹åœ¨å°æ ·å­¦ä¹ ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å­˜åœ¨ä½“ç§¯åºå¤§ã€æ¨æ–­é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æŠ€æœ¯å¯å°†é«˜æ€§èƒ½æ¨¡å‹çš„æ€§èƒ½è½¬ç§»åˆ°å°å‹é«˜æ•ˆæ¨¡å‹ä¸Šã€‚</li>
<li>ç”±äºç¼ºä¹æœªæ ‡è®°æ•°æ®ï¼Œå¸¸è§çš„è’¸é¦æ–¹æ³•åœ¨å°æ ·è®¾ç½®ä¸­æœ‰å±€é™æ€§ã€‚</li>
<li>ä¸€ç§æ–°é¢–çš„æ‰©æ•£æ¨¡å‹åæ¼”æŠ€æœ¯ï¼ˆTINTï¼‰ç»“åˆäº†æ–‡æœ¬åæ¼”çš„å¤šæ ·æ€§å’Œç©ºæ–‡æœ¬åæ¼”çš„ç‰¹å¼‚æ€§ï¼Œåœ¨å°‘é‡æ ·æœ¬è’¸é¦ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>TINTæ–¹æ³•åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.03146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cd6ab3d5cd16a2668137ec591aa485a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5be455852856e3c83c059532e49e3b44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40cf26792c35b11cb8c120150181abdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ceb7cc15d77dfec92d7646eda551a3f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf381ec319aa7cf4fd10cf8af3b82bac.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SelfReplay-Adapting-Self-Supervised-Sensory-Models-via-Adaptive-Meta-Task-Replay"><a href="#SelfReplay-Adapting-Self-Supervised-Sensory-Models-via-Adaptive-Meta-Task-Replay" class="headerlink" title="SelfReplay: Adapting Self-Supervised Sensory Models via Adaptive   Meta-Task Replay"></a>SelfReplay: Adapting Self-Supervised Sensory Models via Adaptive   Meta-Task Replay</h2><p><strong>Authors:Hyungjun Yoon, Jaehyun Kwak, Biniyam Aschalew Tolera, Gaole Dai, Mo Li, Taesik Gong, Kimin Lee, Sung-Ju Lee</strong></p>
<p>Self-supervised learning has emerged as a method for utilizing massive unlabeled data for pre-training models, providing an effective feature extractor for various mobile sensing applications. However, when deployed to end-users, these models encounter significant domain shifts attributed to user diversity. We investigate the performance degradation that occurs when self-supervised models are fine-tuned in heterogeneous domains. To address the issue, we propose SelfReplay, a few-shot domain adaptation framework for personalizing self-supervised models. SelfReplay proposes self-supervised meta-learning for initial model pre-training, followed by a user-side model adaptation by replaying the self-supervision with user-specific data. This allows models to adjust their pre-trained representations to the user with only a few samples. Evaluation with four benchmarks demonstrates that SelfReplay outperforms existing baselines by an average F1-score of 8.8%p. Our on-device computational overhead analysis on a commodity off-the-shelf (COTS) smartphone shows that SelfReplay completes adaptation within an unobtrusive latency (in three minutes) with only a 9.54% memory consumption, demonstrating the computational efficiency of the proposed method. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ å·²ç»æˆä¸ºä¸€ç§åˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®è¿›è¡Œé¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œä¸ºå„ç§ç§»åŠ¨æ„ŸçŸ¥åº”ç”¨æä¾›äº†æœ‰æ•ˆçš„ç‰¹å¾æå–å™¨ã€‚ç„¶è€Œï¼Œå½“è¿™äº›æ¨¡å‹éƒ¨ç½²ç»™æœ€ç»ˆç”¨æˆ·æ—¶ï¼Œç”±äºç”¨æˆ·å¤šæ ·æ€§ï¼Œå®ƒä»¬ä¼šé‡åˆ°æ˜¾è‘—çš„é¢†åŸŸåç§»ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†è‡ªç›‘ç£æ¨¡å‹åœ¨å¼‚æ„é¢†åŸŸè¿›è¡Œå¾®è°ƒæ—¶å‘ç”Ÿçš„æ€§èƒ½ä¸‹é™æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SelfReplayï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¸ªæ€§åŒ–è‡ªç›‘ç£æ¨¡å‹çš„Few-ShotåŸŸè‡ªé€‚åº”æ¡†æ¶ã€‚SelfReplayæå‡ºè‡ªç›‘ç£å…ƒå­¦ä¹ è¿›è¡Œåˆå§‹æ¨¡å‹é¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡é‡æ’­ç”¨æˆ·ç‰¹å®šæ•°æ®çš„è‡ªç›‘ç£æ¥è¿›è¡Œç”¨æˆ·ä¾§æ¨¡å‹é€‚åº”ã€‚è¿™å…è®¸æ¨¡å‹ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬è°ƒæ•´å…¶é¢„è®­ç»ƒè¡¨ç¤ºä»¥é€‚åº”ç”¨æˆ·ã€‚ä½¿ç”¨å››ä¸ªåŸºå‡†ç‚¹çš„è¯„ä¼°è¡¨æ˜ï¼ŒSelfReplayçš„å¹³å‡F1åˆ†æ•°æ¯”ç°æœ‰åŸºçº¿é«˜å‡º8.8%ã€‚æˆ‘ä»¬åœ¨å•†å“ç°è´§æ™ºèƒ½æ‰‹æœºä¸Šè¿›è¡Œçš„åœ¨çº¿è®¾å¤‡è®¡ç®—å¼€é”€åˆ†æè¡¨æ˜ï¼ŒSelfReplayåœ¨ä¸è¢«å¹²æ‰°çš„å»¶è¿Ÿï¼ˆä¸‰åˆ†é’Ÿå†…ï¼‰å†…å®Œæˆé€‚åº”ï¼Œå†…å­˜æ¶ˆè€—ä»…ä¸º9.54%ï¼Œè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„è®¡ç®—æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15305v2">PDF</a> Accepted to the 23rd ACM Conference on Embedded Networked Sensor   Systems (ACM SenSys 2025)</p>
<p><strong>Summary</strong><br>     è‡ªç›‘ç£å­¦ä¹ æ˜¯åˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒçš„ä¸€ç§æ–¹æ³•ï¼Œä¸ºå„ç§ç§»åŠ¨æ„ŸçŸ¥åº”ç”¨æä¾›æœ‰æ•ˆçš„ç‰¹å¾æå–ã€‚ç„¶è€Œï¼Œåœ¨éƒ¨ç½²ç»™ç»ˆç«¯ç”¨æˆ·æ—¶ï¼Œè¿™äº›æ¨¡å‹ç”±äºç”¨æˆ·å¤šæ ·æ€§è€Œé‡åˆ°æ˜¾è‘—çš„é¢†åŸŸåç§»é—®é¢˜ã€‚æœ¬æ–‡ç ”ç©¶äº†è‡ªç›‘ç£æ¨¡å‹åœ¨å¼‚æ„é¢†åŸŸå¾®è°ƒæ—¶å‡ºç°çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SelfReplayï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä¸ªæ€§åŒ–è‡ªç›‘ç£æ¨¡å‹çš„å°‘é•œå¤´åŸŸé€‚åº”æ¡†æ¶ã€‚SelfReplayå…ˆè¿›è¡Œè‡ªç›‘ç£å…ƒå­¦ä¹ è¿›è¡Œåˆå§‹æ¨¡å‹é¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡é‡æ’­ç”¨æˆ·ç‰¹å®šæ•°æ®çš„è‡ªç›‘ç£è¿›è¡Œç”¨æˆ·ä¾§æ¨¡å‹é€‚åº”ã€‚è¿™å…è®¸æ¨¡å‹ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬å°±èƒ½è°ƒæ•´å…¶é¢„è®­ç»ƒè¡¨ç¤ºä»¥é€‚åº”ç”¨æˆ·ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSelfReplayè¾ƒç°æœ‰åŸºçº¿å¹³å‡F1åˆ†æ•°æé«˜äº†8.8%ã€‚æˆ‘ä»¬åœ¨å•†å“ç°è´§æ™ºèƒ½æ‰‹æœºä¸Šè¿›è¡Œäº†åœ¨çº¿è®¾å¤‡è®¡ç®—å¼€é”€åˆ†æï¼Œç»“æœæ˜¾ç¤ºSelfReplayçš„é€‚åº”è¿‡ç¨‹åœ¨ä¸‰åˆ†é’Ÿå†…å®Œæˆï¼Œå†…å­˜æ¶ˆè€—ä»…ä¸º9.54%ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong><br>     1. è‡ªç›‘ç£å­¦ä¹ èƒ½åˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®è¿›è¡Œæ¨¡å‹é¢„è®­ç»ƒï¼Œä¸ºç§»åŠ¨æ„ŸçŸ¥åº”ç”¨æä¾›æœ‰æ•ˆç‰¹å¾æå–ã€‚<br>     2. éƒ¨ç½²è‡ªç›‘ç£æ¨¡å‹ç»™ç»ˆç«¯ç”¨æˆ·æ—¶ï¼Œä¼šé‡åˆ°ç”±äºç”¨æˆ·å¤šæ ·æ€§å¯¼è‡´çš„é¢†åŸŸåç§»é—®é¢˜ã€‚<br>     3. SelfReplayæ˜¯ä¸€ä¸ªå°‘é•œå¤´åŸŸé€‚åº”æ¡†æ¶ï¼Œç”¨äºä¸ªæ€§åŒ–è‡ªç›‘ç£æ¨¡å‹ã€‚<br>     4. SelfReplayé€šè¿‡è‡ªç›‘ç£å…ƒå­¦ä¹ è¿›è¡Œåˆå§‹æ¨¡å‹é¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡é‡æ’­ç”¨æˆ·ç‰¹å®šæ•°æ®çš„è‡ªç›‘ç£è¿›è¡Œé€‚åº”ã€‚<br>     5. SelfReplayå…è®¸æ¨¡å‹ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬å°±èƒ½è°ƒæ•´é¢„è®­ç»ƒè¡¨ç¤ºä»¥é€‚åº”ç”¨æˆ·ã€‚<br>     6. åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒSelfReplayè¾ƒç°æœ‰åŸºçº¿å¹³å‡F1åˆ†æ•°æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15305">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9adee7ffb197321fc936a879492fd2eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41dde59913f0283eff6d27a056fa6ca3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c2354c39372e2376d1374cb9218f80b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-044fa892472861bdae6b2a27356e00ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0c842ecbf9c31c0c7bb03010b6373a3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8bf95cb57bafbcb09567f4babb446322.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  LaPIG Cross-Modal Generation of Paired Thermal and Visible Facial   Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d96a848d2fab7dd231af034b0c52c753.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  Do Visual Imaginations Improve Vision-and-Language Navigation Agents?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25219.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
