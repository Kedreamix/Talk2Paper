<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  Attentional Triple-Encoder Network in Spatiospectral Domains for Medical   Image Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1cf4dc24bc373ca0d837435871375477.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-22-æ›´æ–°"><a href="#2025-03-22-æ›´æ–°" class="headerlink" title="2025-03-22 æ›´æ–°"></a>2025-03-22 æ›´æ–°</h1><h2 id="Attentional-Triple-Encoder-Network-in-Spatiospectral-Domains-for-Medical-Image-Segmentation"><a href="#Attentional-Triple-Encoder-Network-in-Spatiospectral-Domains-for-Medical-Image-Segmentation" class="headerlink" title="Attentional Triple-Encoder Network in Spatiospectral Domains for Medical   Image Segmentation"></a>Attentional Triple-Encoder Network in Spatiospectral Domains for Medical   Image Segmentation</h2><p><strong>Authors:Kristin Qi, Xinhan Di</strong></p>
<p>Retinal Optical Coherence Tomography (OCT) segmentation is essential for diagnosing pathology. Traditional methods focus on either spatial or spectral domains, overlooking their combined dependencies. We propose a triple-encoder network that integrates CNNs for spatial features, Fast Fourier Convolution (FFC) for spectral features, and attention mechanisms to capture global relationships across both domains. Attention fusion modules integrate convolution and cross-attention to further enhance features. Our method achieves an average Dice score improvement from 0.855 to 0.864, outperforming prior work. </p>
<blockquote>
<p>è§†ç½‘è†œå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰åˆ†å‰²å¯¹äºç—…ç†è¯Šæ–­è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾§é‡äºç©ºé—´åŸŸæˆ–å…‰è°±åŸŸï¼Œå¿½ç•¥äº†å®ƒä»¬çš„ç»„åˆä¾èµ–æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰é‡ç¼–ç å™¨ç½‘ç»œï¼Œè¯¥ç½‘ç»œç»“åˆäº†ç”¨äºç©ºé—´ç‰¹å¾çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€ç”¨äºå…‰è°±ç‰¹å¾çš„å¿«å‚…é‡Œå¶å·ç§¯ï¼ˆFFCï¼‰ä»¥åŠæ•æ‰ä¸¤ä¸ªé¢†åŸŸå…¨å±€å…³ç³»çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚æ³¨æ„åŠ›èåˆæ¨¡å—ç»“åˆäº†å·ç§¯å’Œäº¤å‰æ³¨æ„åŠ›ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å¹³å‡Diceå¾—åˆ†ä»0.855æé«˜åˆ°0.864ï¼Œä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16389v1">PDF</a> IEEE Conference on Artificial Intelligence (IEEE CAI)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè§†ç½‘è†œå…‰å­¦ç›¸å¹²å±‚ææˆåƒï¼ˆOCTï¼‰åˆ†å‰²çš„ç—…ç†è¯Šæ–­æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸‰é‡ç¼–ç å™¨ç½‘ç»œï¼Œç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æå–ç©ºé—´ç‰¹å¾ã€å¿«é€Ÿå‚…é‡Œå¶å·ç§¯ï¼ˆFFCï¼‰æå–å…‰è°±ç‰¹å¾ï¼Œä»¥åŠæ³¨æ„åŠ›æœºåˆ¶æ•æ‰ä¸¤ä¸ªé¢†åŸŸçš„å…¨å±€å…³ç³»ã€‚é€šè¿‡å·ç§¯å’Œäº¤å‰æ³¨æ„åŠ›çš„èåˆæ¨¡å—è¿›ä¸€æ­¥å¢å¼ºäº†ç‰¹å¾ã€‚è¯¥æ–¹æ³•å¹³å‡Diceå¾—åˆ†ä»0.855æé«˜åˆ°0.864ï¼Œä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†ç½‘è†œå…‰å­¦ç›¸å¹²å±‚ææˆåƒï¼ˆOCTï¼‰åˆ†å‰²å¯¹ç—…ç†è¯Šæ–­è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨ç©ºé—´æˆ–å…‰è°±é¢†åŸŸï¼Œå¿½ç•¥äº†äºŒè€…çš„ç»¼åˆä¾èµ–æ€§ã€‚</li>
<li>æå‡ºä¸€ç§ä¸‰é‡ç¼–ç å™¨ç½‘ç»œï¼Œç»“åˆCNNã€FFCå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶æ•æ‰ç©ºé—´å’Œå…‰è°±ç‰¹å¾ä»¥åŠä¸¤è€…ä¹‹é—´çš„å…¨å±€å…³ç³»ã€‚</li>
<li>é€šè¿‡æ³¨æ„åŠ›èåˆæ¨¡å—ï¼Œå°†å·ç§¯å’Œäº¤å‰æ³¨æ„åŠ›ç»“åˆï¼Œå¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å¹³å‡Diceå¾—åˆ†çš„æé«˜ï¼Œä»0.855æé«˜åˆ°0.864ã€‚</li>
<li>æ‰€ææ–¹æ³•ä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb72be7192d8a0a2ca680f21de22f0d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-baaae5d639fe2ef018c72e16d8206a9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6eacc1944fcd226b8fca2c9cf68c5eae.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Rapid-patient-specific-neural-networks-for-intraoperative-X-ray-to-volume-registration"><a href="#Rapid-patient-specific-neural-networks-for-intraoperative-X-ray-to-volume-registration" class="headerlink" title="Rapid patient-specific neural networks for intraoperative X-ray to   volume registration"></a>Rapid patient-specific neural networks for intraoperative X-ray to   volume registration</h2><p><strong>Authors:Vivek Gopalakrishnan, Neel Dey, David-Dimitris Chlorogiannis, Andrew Abumoussa, Anna M. Larson, Darren B. Orbach, Sarah Frisken, Polina Golland</strong></p>
<p>The integration of artificial intelligence in image-guided interventions holds transformative potential, promising to extract 3D geometric and quantitative information from conventional 2D imaging modalities during complex procedures. Achieving this requires the rapid and precise alignment of 2D intraoperative images (e.g., X-ray) with 3D preoperative volumes (e.g., CT, MRI). However, current 2D&#x2F;3D registration methods fail across the broad spectrum of procedures dependent on X-ray guidance: traditional optimization techniques require custom parameter tuning for each subject, whereas neural networks trained on small datasets do not generalize to new patients or require labor-intensive manual annotations, increasing clinical burden and precluding application to new anatomical targets. To address these challenges, we present xvr, a fully automated framework for training patient-specific neural networks for 2D&#x2F;3D registration. xvr uses physics-based simulation to generate abundant high-quality training data from a patientâ€™s own preoperative volumetric imaging, thereby overcoming the inherently limited ability of supervised models to generalize to new patients and procedures. Furthermore, xvr requires only 5 minutes of training per patient, making it suitable for emergency interventions as well as planned procedures. We perform the largest evaluation of a 2D&#x2F;3D registration algorithm on real X-ray data to date and find that xvr robustly generalizes across a diverse dataset comprising multiple anatomical structures, imaging modalities, and hospitals. Across surgical tasks, xvr achieves submillimeter-accurate registration at intraoperative speeds, improving upon existing methods by an order of magnitude. xvr is released as open-source software freely available at <a target="_blank" rel="noopener" href="https://github.com/eigenvivek/xvr">https://github.com/eigenvivek/xvr</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½åœ¨å›¾åƒå¼•å¯¼å¹²é¢„ä¸­çš„èåˆå…·æœ‰å˜é©æ€§æ½œåŠ›ï¼Œæœ‰æœ›åœ¨è¿›è¡Œå¤æ‚æ‰‹æœ¯æ—¶ä»ä¼ ç»Ÿçš„äºŒç»´æˆåƒæ¨¡å¼ä¸­æå–ä¸‰ç»´å‡ ä½•å’Œå®šé‡ä¿¡æ¯ã€‚å®ç°è¿™ä¸€ç‚¹éœ€è¦å¿«é€Ÿç²¾ç¡®åœ°åŒ¹é…äºŒç»´æœ¯ä¸­å›¾åƒï¼ˆå¦‚Xå°„çº¿ï¼‰ä¸ä¸‰ç»´æœ¯å‰ä½“ç§¯ï¼ˆå¦‚CTã€MRIï¼‰ã€‚ç„¶è€Œï¼Œå½“å‰äºŒç»´å’Œä¸‰ç»´çš„æ³¨å†Œæ–¹æ³•åœ¨å…¨æµç¨‹ä¸­å¹¶ä¸èƒ½å¹¿æ³›åº”ç”¨ï¼šä¼ ç»Ÿä¼˜åŒ–æŠ€æœ¯éœ€è¦æ ¹æ®æ¯ä¸ªæ‚£è€…æ‰‹åŠ¨è°ƒæ•´å‚æ•°ï¼Œè€Œè®­ç»ƒåœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šçš„ç¥ç»ç½‘ç»œå¹¶ä¸èƒ½é€‚åº”æ–°æ‚£è€…ï¼Œæˆ–éœ€è¦å¯†é›†çš„åŠ³åŠ›è¿›è¡Œæ‰‹åŠ¨æ³¨é‡Šï¼Œä»è€Œå¢åŠ äº†ä¸´åºŠè´Ÿæ‹…å¹¶é™åˆ¶äº†å…¶åœ¨æ–°è§£å‰–ç›®æ ‡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†xvræ¡†æ¶ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºäºŒç»´å’Œä¸‰ç»´æ³¨å†Œçš„é’ˆå¯¹æ‚£è€…çš„ç¥ç»ç½‘ç»œçš„å…¨è‡ªåŠ¨è®­ç»ƒæ¡†æ¶ã€‚xvrä½¿ç”¨åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿä»æ‚£è€…è‡ªèº«çš„æœ¯å‰ä½“ç§¯æˆåƒç”Ÿæˆå¤§é‡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œå…‹æœäº†ç›‘ç£æ¨¡å‹å¯¹é€‚åº”æ–°æ‚£è€…å’Œç¨‹åºçš„æ–°é—®é¢˜çš„å›ºæœ‰å±€é™æ€§ã€‚æ­¤å¤–ï¼Œxvrå¯¹æ¯ä½æ‚£è€…çš„è®­ç»ƒæ—¶é—´ä»…éœ€äº”åˆ†é’Ÿï¼Œä½¿å…¶æ—¢é€‚ç”¨äºç´§æ€¥å¹²é¢„ä¹Ÿé€‚ç”¨äºè®¡åˆ’ä¸­çš„ç¨‹åºã€‚æˆ‘ä»¬å¯¹è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„çœŸå®Xå°„çº¿æ•°æ®çš„äºŒç»´å’Œä¸‰ç»´æ³¨å†Œç®—æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°xvråœ¨åŒ…å«å¤šä¸ªè§£å‰–ç»“æ„ã€æˆåƒæ¨¡å¼å’ŒåŒ»é™¢çš„å¤šæ ·åŒ–æ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚åœ¨å„ç§æ‰‹æœ¯ä»»åŠ¡ä¸­ï¼Œxvrå®ç°äº†äºšæ¯«ç±³çº§çš„ç²¾ç¡®æ³¨å†Œé€Ÿåº¦åœ¨æœ¯ä¸­å¤§å¹…æé«˜ï¼Œæ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ã€‚xvè¢«å‘å¸ƒä¸ºå¼€æºè½¯ä»¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/eigenvivek/xvr%E8%BF%9D%E8%B4%B9%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/eigenvivek/xvrå…è´¹è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16309v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½åœ¨å›¾åƒå¼•å¯¼å¹²é¢„ä¸­çš„æ•´åˆå…·æœ‰å˜é©æ½œåŠ›ï¼Œæœ‰æœ›ä»ä¼ ç»Ÿçš„äºŒç»´æˆåƒæ¨¡å¼ä¸­æå–ä¸‰ç»´å‡ ä½•å’Œå®šé‡ä¿¡æ¯ï¼Œåœ¨å¤æ‚è¿‡ç¨‹ä¸­è¾…åŠ©è¿›è¡Œç²¾ç¡®æ“ä½œã€‚å®ç°è¿™ä¸€ç›®æ ‡éœ€è¦å¿«é€Ÿç²¾ç¡®åœ°å°†äºŒç»´æœ¯ä¸­å›¾åƒï¼ˆå¦‚Xå°„çº¿ï¼‰ä¸ä¸‰ç»´æœ¯å‰ä½“ç§¯ï¼ˆå¦‚CTã€MRIï¼‰è¿›è¡Œå¯¹é½ã€‚å½“å‰äºŒç»´&#x2F;ä¸‰ç»´æ³¨å†Œæ–¹æ³•åœ¨ä½¿ç”¨Xå°„çº¿å¼•å¯¼çš„å¤šç§æ‰‹æœ¯ä¸­æ•ˆæœä¸ç†æƒ³ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨æ¡†æ¶xvrï¼Œç”¨äºè®­ç»ƒæ‚£è€…ç‰¹å¼‚æ€§ç¥ç»ç½‘ç»œè¿›è¡ŒäºŒç»´&#x2F;ä¸‰ç»´æ³¨å†Œã€‚xvråˆ©ç”¨åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿç”Ÿæˆå¤§é‡çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œä»æ‚£è€…è‡ªèº«çš„æœ¯å‰ä½“ç§¯æˆåƒå¼€å§‹ï¼Œå…‹æœç›‘ç£æ¨¡å‹å¯¹æ–°æ‚£è€…å’Œæ‰‹æœ¯æ™®éæ€§çš„å†…åœ¨é™åˆ¶ã€‚æ­¤å¤–ï¼Œxvrå¯¹æ‚£è€…è®­ç»ƒçš„æ—¶é•¿ä»…éœ€äº”åˆ†é’Ÿï¼Œæ—¢é€‚ç”¨äºç´§æ€¥å¹²é¢„ä¹Ÿé€‚ç”¨äºè®¡åˆ’ç¨‹åºã€‚æœ¬æ–‡æ‰§è¡Œè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„äºŒç»´&#x2F;ä¸‰ç»´æ³¨å†Œç®—æ³•åœ¨çœŸå®Xå°„çº¿æ•°æ®ä¸Šçš„è¯„ä¼°ï¼Œå‘ç°xvrèƒ½å¤Ÿè·¨è¶ŠåŒ…å«å¤šä¸ªè§£å‰–ç»“æ„ã€æˆåƒæ¨¡å¼åŠåŒ»é™¢çš„å¤šæ ·åŒ–æ•°æ®é›†è¿›è¡Œç¨³å¥çš„æ™®éæ€§åº”ç”¨ã€‚åœ¨å„ç§æ‰‹æœ¯ä¸­ï¼Œxvrå®ç°äº†äºšæ¯«ç±³çº§çš„ç²¾ç¡®æ³¨å†Œï¼Œå…¶é€Ÿåº¦ç¬¦åˆæœ¯ä¸­è¦æ±‚ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰äº†æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œå·²å°†xvrä½œä¸ºå¼€æºè½¯ä»¶å‘å¸ƒåœ¨ç›¸å…³ç½‘ç«™ä¾›å¤§å®¶å…è´¹ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨å›¾åƒå¼•å¯¼æ‰‹æœ¯ä¸­çš„æ½œåŠ›ï¼šèƒ½å¤Ÿä»ä¼ ç»Ÿçš„äºŒç»´æˆåƒæ¨¡å¼ä¸­æå–ä¸‰ç»´å‡ ä½•å’Œå®šé‡ä¿¡æ¯ã€‚</li>
<li>å½“å‰äºŒç»´&#x2F;ä¸‰ç»´æ³¨å†Œæ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼šä¼ ç»Ÿä¼˜åŒ–æŠ€æœ¯éœ€è¦ä¸ºæ¯ä¸ªæ‚£è€…å®šåˆ¶å‚æ•°è°ƒæ•´ï¼Œè€ŒåŸºäºå°æ•°æ®é›†è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ— æ³•å¾ˆå¥½åœ°åº”ç”¨äºæ–°æ‚£è€…ã€‚</li>
<li>xvræ¡†æ¶ä»‹ç»ï¼šåˆ©ç”¨åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œå®ç°æ‚£è€…ç‰¹å¼‚æ€§ç¥ç»ç½‘ç»œçš„å¿«é€Ÿè®­ç»ƒï¼ˆä»…äº”åˆ†é’Ÿï¼‰ã€‚</li>
<li>xvræ¡†æ¶çš„ä¼˜åŠ¿ï¼šèƒ½å¤Ÿé€‚ç”¨äºç´§æ€¥å¹²é¢„å’Œè®¡åˆ’æ‰‹æœ¯ï¼Œå¹¶ä¸”èƒ½å¤„ç†å¤šç§è§£å‰–ç»“æ„ã€æˆåƒæ¨¡å¼å’ŒåŒ»é™¢çš„å¤šæ ·åŒ–æ•°æ®ã€‚</li>
<li>xvrçš„æ³¨å†Œç²¾åº¦é«˜ï¼šå®ç°äºšæ¯«ç±³çº§çš„ç²¾ç¡®æ³¨å†Œã€‚</li>
<li>xvrå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼šåœ¨å„ç§æ‰‹æœ¯ä¸­éƒ½èƒ½å®ç°å¿«é€Ÿç²¾ç¡®çš„æ³¨å†Œï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16309">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e3192b7ea715ca672055824c8743df40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b17d30adb0b2b773dcd5c282a6ea491.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cf4dc24bc373ca0d837435871375477.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237d90ad5a29f452f57937815e6c75c3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OpenMIBOOD-Open-Medical-Imaging-Benchmarks-for-Out-Of-Distribution-Detection"><a href="#OpenMIBOOD-Open-Medical-Imaging-Benchmarks-for-Out-Of-Distribution-Detection" class="headerlink" title="OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution   Detection"></a>OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution   Detection</h2><p><strong>Authors:Max Gutbrod, David Rauber, Danilo Weber Nunes, Christoph Palm</strong></p>
<p>The growing reliance on Artificial Intelligence (AI) in critical domains such as healthcare demands robust mechanisms to ensure the trustworthiness of these systems, especially when faced with unexpected or anomalous inputs. This paper introduces the Open Medical Imaging Benchmarks for Out-Of-Distribution Detection (OpenMIBOOD), a comprehensive framework for evaluating out-of-distribution (OOD) detection methods specifically in medical imaging contexts. OpenMIBOOD includes three benchmarks from diverse medical domains, encompassing 14 datasets divided into covariate-shifted in-distribution, near-OOD, and far-OOD categories. We evaluate 24 post-hoc methods across these benchmarks, providing a standardized reference to advance the development and fair comparison of OOD detection methods. Results reveal that findings from broad-scale OOD benchmarks in natural image domains do not translate to medical applications, underscoring the critical need for such benchmarks in the medical field. By mitigating the risk of exposing AI models to inputs outside their training distribution, OpenMIBOOD aims to support the advancement of reliable and trustworthy AI systems in healthcare. The repository is available at <a target="_blank" rel="noopener" href="https://github.com/remic-othr/OpenMIBOOD">https://github.com/remic-othr/OpenMIBOOD</a>. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åŒ»ç–—ç­‰å…³é”®é¢†åŸŸçš„ä¾èµ–ç¨‹åº¦ä¸æ–­å¢é•¿ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹æ„å¤–æˆ–å¼‚å¸¸è¾“å…¥æ—¶ï¼Œéœ€è¦å»ºç«‹ç¨³å¥çš„æœºåˆ¶æ¥ç¡®ä¿è¿™äº›ç³»ç»Ÿçš„å¯ä¿¡åº¦ã€‚æœ¬æ–‡ä»‹ç»äº†å¼€æ”¾åŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ï¼ˆOpenMIBOODï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°åŒ»å­¦æˆåƒä¸Šä¸‹æ–‡ä¸­åˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆOut-Of-Distribution Detectionï¼Œç®€ç§°OODï¼‰æ–¹æ³•çš„ç»¼åˆæ¡†æ¶ã€‚OpenMIBOODåŒ…å«æ¥è‡ªä¸åŒåŒ»å­¦é¢†åŸŸçš„ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–14ä¸ªæ•°æ®é›†ï¼Œåˆ†ä¸ºåå˜é‡åç§»å†…åˆ†å¸ƒã€æ¥è¿‘OODå’Œè¿œç¦»OODä¸‰ç±»ã€‚æˆ‘ä»¬åœ¨è¿™ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†2de9åå¤„ç†éªŒè¯æ–¹æ³•ï¼Œæä¾›äº†æ ‡å‡†åŒ–å‚è€ƒï¼Œä»¥ä¿ƒè¿›OODæ£€æµ‹æ–¹æ³•çš„å¼€å‘å’Œå…¬å¹³æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œè‡ªç„¶å›¾åƒé¢†åŸŸçš„å¤§è§„æ¨¡OODåŸºå‡†æµ‹è¯•ç»“æœå¹¶ä¸èƒ½åº”ç”¨äºåŒ»å­¦åº”ç”¨ï¼Œè¿™å¼ºè°ƒäº†åŒ»å­¦é¢†åŸŸå¯¹è¿™ç§åŸºå‡†æµ‹è¯•çš„å…³é”®éœ€æ±‚ã€‚é€šè¿‡é™ä½AIæ¨¡å‹æš´éœ²äºè®­ç»ƒåˆ†å¸ƒä¹‹å¤–è¾“å…¥çš„é£é™©ï¼ŒOpenMIBOODæ—¨åœ¨æ”¯æŒåŒ»ç–—ä¿å¥ä¸­å¯é å’Œå¯ä¿¡çš„AIç³»ç»Ÿçš„å‘å±•ã€‚è¯¥å­˜å‚¨åº“å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/remic-othr/OpenMIBOOD">https://github.com/remic-othr/OpenMIBOOD</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16247v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¼€æ”¾åŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ï¼ˆOpenMIBOODï¼‰ä¸ºè¯„ä¼°åŒ»ç–—æˆåƒä¸­å¼‚å¸¸åˆ†å¸ƒæ£€æµ‹æ–¹æ³•æä¾›äº†å…¨é¢æ¡†æ¶ã€‚å®ƒåŒ…å«ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–åå››ç»„æ•°æ®é›†ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„å‚è€ƒèµ„æºï¼Œæœ‰åŠ©äºæ¨åŠ¨ç¨³å¥å’Œå¯ä¿¡èµ–çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„å‘å±•ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†ä¸ºåŒ»ç–—è¡Œä¸šå¼€å‘ä¸“é—¨åŸºå‡†æµ‹è¯•çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenMIBOODæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°åŒ»ç–—æˆåƒä¸­å¼‚å¸¸åˆ†å¸ƒæ£€æµ‹æ–¹æ³•çš„æ¡†æ¶ã€‚</li>
<li>å®ƒåŒ…å«ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šç§åŒ»å­¦é¢†åŸŸçš„æ•°æ®é›†ã€‚</li>
<li>OpenMIBOODæä¾›äº†æ ‡å‡†åŒ–çš„å‚è€ƒèµ„æºï¼Œä¾¿äºå¯¹å¼‚å¸¸æ£€æµ‹æ–¹æ³•è¿›è¡Œè¯„ä¼°å’Œæ¯”è¾ƒã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œè‡ªç„¶å›¾åƒé¢†åŸŸçš„å¹¿æ³›å¼‚å¸¸åˆ†å¸ƒåŸºå‡†æµ‹è¯•å¹¶ä¸é€‚ç”¨äºåŒ»ç–—åº”ç”¨ã€‚</li>
<li>OpenMIBOODæ—¨åœ¨é™ä½äººå·¥æ™ºèƒ½æ¨¡å‹é¢ä¸´éè®­ç»ƒåˆ†å¸ƒè¾“å…¥çš„é£é™©ï¼Œæ¨åŠ¨åŒ»ç–—ä¿å¥é¢†åŸŸå¯é å’Œå¯ä¿¡çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚</li>
<li>è¯¥æ¡†æ¶çš„æ¨å‡ºå¼ºè°ƒäº†é’ˆå¯¹åŒ»ç–—è¡Œä¸šå¼€å‘ä¸“é—¨çš„åŸºå‡†æµ‹è¯•çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16247">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b52801d6643dfe17234765a12c4c45cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-481a84e72b1f2be42e8d02e1e089bd50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad7651f131228a7be9f0ef23ab12a978.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Iterative-Optimal-Attention-and-Local-Model-for-Single-Image-Rain-Streak-Removal"><a href="#Iterative-Optimal-Attention-and-Local-Model-for-Single-Image-Rain-Streak-Removal" class="headerlink" title="Iterative Optimal Attention and Local Model for Single Image Rain Streak   Removal"></a>Iterative Optimal Attention and Local Model for Single Image Rain Streak   Removal</h2><p><strong>Authors:Xiangyu Li, Wanshu Fan, Yue Shen, Cong Wang, Wei Wang, Xin Yang, Qiang Zhang, Dongsheng Zhou</strong></p>
<p>High-fidelity imaging is crucial for the successful safety supervision and intelligent deployment of vision-based measurement systems (VBMS). It ensures high-quality imaging in VBMS, which is fundamental for reliable visual measurement and analysis. However, imaging quality can be significantly impaired by adverse weather conditions, particularly rain, leading to blurred images and reduced contrast. Such impairments increase the risk of inaccurate evaluations and misinterpretations in VBMS. To address these limitations, we propose an Expectation Maximization Reconstruction Transformer (EMResformer) for single image rain streak removal. The EMResformer retains the key self-attention values for feature aggregation, enhancing local features to produce superior image reconstruction. Specifically, we propose an Expectation Maximization Block seamlessly integrated into the single image rain streak removal network, enhancing its ability to eliminate superfluous information and restore a cleaner background image. Additionally, to further enhance local information for improved detail rendition, we introduce a Local Model Residual Block, which integrates two local model blocks along with a sequence of convolutions and activation functions. This integration synergistically facilitates the extraction of more pertinent features for enhanced single image rain streak removal. Extensive experiments validate that our proposed EMResformer surpasses current state-of-the-art single image rain streak removal methods on both synthetic and real-world datasets, achieving an improved balance between model complexity and single image deraining performance. Furthermore, we evaluate the effectiveness of our method in VBMS scenarios, demonstrating that high-quality imaging significantly improves the accuracy and reliability of VBMS tasks. </p>
<blockquote>
<p>é«˜ä¿çœŸæˆåƒå¯¹äºåŸºäºè§†è§‰çš„æµ‹é‡ç³»ç»Ÿï¼ˆVBMSï¼‰çš„å®‰å…¨ç›‘ç£ä¸æ™ºèƒ½éƒ¨ç½²è‡³å…³é‡è¦ã€‚å®ƒç¡®ä¿äº†VBMSä¸­çš„é«˜è´¨é‡æˆåƒï¼Œè¿™æ˜¯å¯é è§†è§‰æµ‹é‡ä¸åˆ†æçš„åŸºç¡€ã€‚ç„¶è€Œï¼Œæ¶åŠ£çš„å¤©æ°”æ¡ä»¶ï¼Œç‰¹åˆ«æ˜¯é›¨å¤©ï¼Œä¼šä¸¥é‡æŸå®³æˆåƒè´¨é‡ï¼Œå¯¼è‡´å›¾åƒæ¨¡ç³Šå’Œå¯¹æ¯”åº¦é™ä½ã€‚è¿™äº›ç¼ºé™·å¢åŠ äº†VBMSä¸­è¯„ä¼°å’Œè¯¯è§£çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœŸæœ›æœ€å¤§åŒ–é‡å»ºè½¬æ¢å™¨ï¼ˆEMResformerï¼‰ï¼Œç”¨äºå•å›¾åƒé›¨çº¹å»é™¤ã€‚EMResformerä¿ç•™äº†å…³é”®çš„è‡ªæ³¨æ„åŠ›å€¼è¿›è¡Œç‰¹å¾èšåˆï¼Œå¢å¼ºå±€éƒ¨ç‰¹å¾ä»¥äº§ç”Ÿä¼˜è´¨çš„å›¾åƒé‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç¼é›†æˆåˆ°å•å›¾åƒé›¨çº¹å»é™¤ç½‘ç»œä¸­çš„æœŸæœ›æœ€å¤§åŒ–å—ï¼Œæé«˜å…¶æ¶ˆé™¤å¤šä½™ä¿¡æ¯å¹¶æ¢å¤æ›´å¹²å‡€èƒŒæ™¯å›¾åƒçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå±€éƒ¨ä¿¡æ¯ä»¥æé«˜ç»†èŠ‚å‘ˆç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±€éƒ¨æ¨¡å‹æ®‹å·®å—ï¼Œè¯¥å—å°†ä¸¤ä¸ªå±€éƒ¨æ¨¡å‹å—ä¸ä¸€ç³»åˆ—å·ç§¯å’Œæ¿€æ´»å‡½æ•°ç›¸ç»“åˆã€‚è¿™ç§é›†æˆååŒä¿ƒè¿›äº†æ›´å¤šç›¸å…³ç‰¹å¾çš„æå–ï¼Œä»¥æé«˜å•å›¾åƒé›¨çº¹å»é™¤æ•ˆæœã€‚å¤§é‡å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬æå‡ºçš„EMResformeråœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å•å›¾åƒé›¨çº¹å»é™¤æ–¹æ³•å‡è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œåœ¨æ¨¡å‹å¤æ‚æ€§ä¸å•å›¾åƒå»é›¨æ€§èƒ½ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨VBMSåœºæ™¯ä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜é«˜è´¨é‡æˆåƒå¯ä»¥æ˜¾è‘—æé«˜VBMSä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16165v1">PDF</a> 14 pages, 14 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†é«˜ä¿çœŸæˆåƒåœ¨åŸºäºè§†è§‰çš„æµ‹é‡ç³»ç»Ÿï¼ˆVBMSï¼‰ä¸­çš„é‡è¦ä½œç”¨ï¼Œå¹¶æå‡ºä¸€ç§æœŸæœ›æœ€å¤§åŒ–é‡å»ºå˜å‹å™¨ï¼ˆEMResformerï¼‰æ¥è§£å†³æ¶åŠ£å¤©æ°”æ¡ä»¶å¯¹æˆåƒè´¨é‡çš„å¹²æ‰°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å•å›¾åƒé›¨ç—•å»é™¤ã€‚é€šè¿‡å¼•å…¥æœŸæœ›æœ€å¤§åŒ–å—å’Œå±€éƒ¨æ¨¡å‹æ®‹å·®å—ï¼ŒEMResformerèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å»é™¤å¤šä½™ä¿¡æ¯å¹¶æ¢å¤æ¸…æ™°çš„èƒŒæ™¯å›¾åƒï¼Œä»è€Œæé«˜å›¾åƒè´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šï¼ŒEMResformerçš„å•å›¾åƒé›¨ç—•å»é™¤æ•ˆæœä¼˜äºå½“å‰æœ€å…ˆè¿›çš„å•å›¾åƒé›¨ç—•å»é™¤æ–¹æ³•ã€‚åœ¨VBMSåœºæ™¯ä¸­ï¼ŒéªŒè¯äº†é«˜è´¨é‡æˆåƒèƒ½æ˜¾è‘—æé«˜VBMSä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é«˜ä¿çœŸæˆåƒå¯¹åŸºäºè§†è§‰çš„æµ‹é‡ç³»ç»Ÿï¼ˆVBMSï¼‰çš„å®‰å…¨ç›‘æ§å’Œæ™ºèƒ½éƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
<li>æ¶åŠ£å¤©æ°”æ¡ä»¶ï¼Œå°¤å…¶æ˜¯é›¨å¤©ï¼Œä¼šæ˜¾è‘—å½±å“æˆåƒè´¨é‡ï¼Œå¢åŠ VBMSè¯„ä¼°å¤±è¯¯çš„é£é™©ã€‚</li>
<li>æå‡ºäº†æœŸæœ›æœ€å¤§åŒ–é‡å»ºå˜å‹å™¨ï¼ˆEMResformerï¼‰æ¥è§£å†³å•å›¾åƒé›¨ç—•å»é™¤é—®é¢˜ã€‚</li>
<li>EMResformeré€šè¿‡å¼•å…¥æœŸæœ›æœ€å¤§åŒ–å—å’Œå±€éƒ¨æ¨¡å‹æ®‹å·®å—ï¼Œæé«˜äº†å›¾åƒé‡å»ºçš„è´¨é‡å’Œæ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒEMResformeråœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å•å›¾åƒé›¨ç—•å»é™¤æ•ˆæœä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-476d02bfce55cace1d4826c791cc0e40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e33f9630e30510419d0a927a33a6e56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b487438a4097b9f6c91ac52eac06279d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea6cd9dffcc9f5a28686a80988e7061b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9958ff6bcc351a19b00b67a9acb273ed.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Selective-Complementary-Feature-Fusion-and-Modal-Feature-Compression-Interaction-for-Brain-Tumor-Segmentation"><a href="#Selective-Complementary-Feature-Fusion-and-Modal-Feature-Compression-Interaction-for-Brain-Tumor-Segmentation" class="headerlink" title="Selective Complementary Feature Fusion and Modal Feature Compression   Interaction for Brain Tumor Segmentation"></a>Selective Complementary Feature Fusion and Modal Feature Compression   Interaction for Brain Tumor Segmentation</h2><p><strong>Authors:Dong Chen, Boyue Zhao, Yi Zhang, Meng Zhao</strong></p>
<p>Efficient modal feature fusion strategy is the key to achieve accurate segmentation of brain glioma. However, due to the specificity of different MRI modes, it is difficult to carry out cross-modal fusion with large differences in modal features, resulting in the model ignoring rich feature information. On the other hand, the problem of multi-modal feature redundancy interaction occurs in parallel networks due to the proliferation of feature dimensions, further increase the difficulty of multi-modal feature fusion at the bottom end. In order to solve the above problems, we propose a noval complementary feature compression interaction network (CFCI-Net), which realizes the complementary fusion and compression interaction of multi-modal feature information with an efficient mode fusion strategy. Firstly, we propose a selective complementary feature fusion (SCFF) module, which adaptively fuses rich cross-modal feature information by complementary soft selection weights. Secondly, a modal feature compression interaction (MFCI) transformer is proposed to deal with the multi-mode fusion redundancy problem when the feature dimension surges. The MFCI transformer is composed of modal feature compression (MFC) and modal feature interaction (MFI) to realize redundancy feature compression and multi-mode feature interactive learning. %In MFI, we propose a hierarchical interactive attention mechanism based on multi-head attention. Evaluations on the BraTS2019 and BraTS2020 datasets demonstrate that CFCI-Net achieves superior results compared to state-of-the-art models. Code: <a target="_blank" rel="noopener" href="https://github.com/CDmm0/CFCI-Net">https://github.com/CDmm0/CFCI-Net</a> </p>
<blockquote>
<p>é«˜æ•ˆæ¨¡æ€ç‰¹å¾èåˆç­–ç•¥æ˜¯å®ç°è„‘èƒ¶è´¨ç˜¤å‡†ç¡®åˆ†å‰²çš„å…³é”®ã€‚ç„¶è€Œï¼Œç”±äºä¸åŒMRIæ¨¡å¼çš„ç‰¹æ®Šæ€§ï¼Œåœ¨æ¨¡æ€ç‰¹å¾å­˜åœ¨è¾ƒå¤§å·®å¼‚æ—¶ï¼Œè¿›è¡Œè·¨æ¨¡æ€èåˆå˜å¾—å›°éš¾ï¼Œå¯¼è‡´æ¨¡å‹å¿½ç•¥äº†ä¸°å¯Œçš„ç‰¹å¾ä¿¡æ¯ã€‚å¦ä¸€æ–¹é¢ï¼Œç”±äºç‰¹å¾ç»´åº¦çš„å¢åŠ ï¼Œå¹¶è¡Œç½‘ç»œä¸­å‡ºç°äº†å¤šæ¨¡æ€ç‰¹å¾å†—ä½™äº¤äº’çš„é—®é¢˜ï¼Œè¿›ä¸€æ­¥å¢åŠ äº†åº•å±‚å¤šæ¨¡æ€ç‰¹å¾èåˆçš„éš¾åº¦ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹äº’è¡¥ç‰¹å¾å‹ç¼©äº¤äº’ç½‘ç»œï¼ˆCFCI-Netï¼‰ï¼Œå®ƒé‡‡ç”¨é«˜æ•ˆçš„æ¨¡å¼èåˆç­–ç•¥ï¼Œå®ç°äº†å¤šæ¨¡æ€ç‰¹å¾ä¿¡æ¯çš„äº’è¡¥èåˆå’Œå‹ç¼©äº¤äº’ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†é€‰æ‹©æ€§äº’è¡¥ç‰¹å¾èåˆï¼ˆSCFFï¼‰æ¨¡å—ï¼Œé€šè¿‡äº’è¡¥çš„è½¯é€‰æ‹©æƒé‡è‡ªé€‚åº”åœ°èåˆä¸°å¯Œçš„è·¨æ¨¡æ€ç‰¹å¾ä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡æ€ç‰¹å¾å‹ç¼©äº¤äº’ï¼ˆMFCIï¼‰è½¬æ¢å™¨ï¼Œä»¥å¤„ç†ç‰¹å¾ç»´åº¦æ¿€å¢æ—¶çš„å¤šæ¨¡å¼èåˆå†—ä½™é—®é¢˜ã€‚MFCIè½¬æ¢å™¨ç”±æ¨¡æ€ç‰¹å¾å‹ç¼©ï¼ˆMFCï¼‰å’Œæ¨¡æ€ç‰¹å¾äº¤äº’ï¼ˆMFIï¼‰ç»„æˆï¼Œå®ç°å†—ä½™ç‰¹å¾å‹ç¼©å’Œå¤šæ¨¡å¼ç‰¹å¾äº¤äº’å­¦ä¹ ã€‚åœ¨MFIä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šå¤´æ³¨æ„åŠ›çš„åˆ†å±‚äº¤äº’æ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨BraTS2019å’ŒBraTS2020æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCFCI-Netç›¸è¾ƒäºæœ€æ–°æ¨¡å‹å–å¾—äº†ä¼˜è¶Šçš„ç»“æœã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/CDmm0/CFCI-Net">https://github.com/CDmm0/CFCI-Net</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16149v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„äº’è¡¥ç‰¹å¾å‹ç¼©äº¤äº’ç½‘ç»œï¼ˆCFCI-Netï¼‰ï¼Œå®ç°å¤šæ¨¡æ€ç‰¹å¾ä¿¡æ¯çš„äº’è¡¥èåˆä¸å‹ç¼©äº¤äº’ï¼Œä»¥æé«˜è„‘èƒ¶è´¨ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚ä¸ºè§£å†³ä¸åŒMRIæ¨¡å¼åœ¨ç‰¹å¾èåˆä¸Šçš„éš¾ç‚¹åŠå¤šæ¨¡æ€ç‰¹å¾å†—ä½™é—®é¢˜ï¼Œç½‘ç»œä¸­åŒ…å«é€‰æ‹©æ€§äº’è¡¥ç‰¹å¾èåˆæ¨¡å—ä¸æ¨¡æ€ç‰¹å¾å‹ç¼©äº¤äº’å˜å‹å™¨ï¼Œé€šè¿‡é«˜æ•ˆçš„æ¨¡æ€èåˆç­–ç•¥å®ç°è·¨æ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆèåˆã€‚è¯„ä»·æ•°æ®è¡¨ç°CFCI-Netæ€§èƒ½ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº’è¡¥ç‰¹å¾å‹ç¼©äº¤äº’ç½‘ç»œï¼ˆCFCI-Netï¼‰ç”¨äºè„‘èƒ¶è´¨ç˜¤åˆ†å‰²ã€‚</li>
<li>é’ˆå¯¹ä¸åŒMRIæ¨¡å¼çš„ç‰¹æ®Šæ€§ï¼Œå®ç°è·¨æ¨¡æ€ç‰¹å¾èåˆçš„æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨é€‰æ‹©æ€§äº’è¡¥ç‰¹å¾èåˆï¼ˆSCFFï¼‰æ¨¡å—è‡ªé€‚åº”èåˆè·¨æ¨¡æ€ç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>æå‡ºæ¨¡æ€ç‰¹å¾å‹ç¼©äº¤äº’ï¼ˆMFCIï¼‰å˜å‹å™¨åº”å¯¹ç‰¹å¾ç»´åº¦å¢é•¿å¸¦æ¥çš„å†—ä½™é—®é¢˜ã€‚</li>
<li>MFCIå˜å‹å™¨åŒ…å«æ¨¡æ€ç‰¹å¾å‹ç¼©ï¼ˆMFCï¼‰ä¸æ¨¡æ€ç‰¹å¾äº¤äº’ï¼ˆMFIï¼‰ã€‚</li>
<li>å¼•å…¥å±‚æ¬¡åŒ–äº¤äº’æ³¨æ„åŠ›æœºåˆ¶ä»¥å®ç°å¤šæ¨¡æ€ç‰¹å¾çš„äº¤äº’å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d089c274450b1d9fb5fc741a96b697d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2370f4b31ab330fcc576b1cdce59baad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2324777655323e42861679ee3fed65ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebdef2305f4a5413eb02d0e3ba4ab65f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Disentangled-and-Interpretable-Multimodal-Attention-Fusion-for-Cancer-Survival-Prediction"><a href="#Disentangled-and-Interpretable-Multimodal-Attention-Fusion-for-Cancer-Survival-Prediction" class="headerlink" title="Disentangled and Interpretable Multimodal Attention Fusion for Cancer   Survival Prediction"></a>Disentangled and Interpretable Multimodal Attention Fusion for Cancer   Survival Prediction</h2><p><strong>Authors:Aniek Eijpe, Soufyan Lakbir, Melis Erdal Cesur, Sara P. Oliveira, Sanne Abeln, Wilson Silva</strong></p>
<p>To improve the prediction of cancer survival using whole-slide images and transcriptomics data, it is crucial to capture both modality-shared and modality-specific information. However, multimodal frameworks often entangle these representations, limiting interpretability and potentially suppressing discriminative features. To address this, we propose Disentangled and Interpretable Multimodal Attention Fusion (DIMAF), a multimodal framework that separates the intra- and inter-modal interactions within an attention-based fusion mechanism to learn distinct modality-specific and modality-shared representations. We introduce a loss based on Distance Correlation to promote disentanglement between these representations and integrate Shapley additive explanations to assess their relative contributions to survival prediction. We evaluate DIMAF on four public cancer survival datasets, achieving a relative average improvement of 1.85% in performance and 23.7% in disentanglement compared to current state-of-the-art multimodal models. Beyond improved performance, our interpretable framework enables a deeper exploration of the underlying interactions between and within modalities in cancer biology. </p>
<blockquote>
<p>ä¸ºæé«˜åˆ©ç”¨å…¨æ»‘å›¾åƒå’Œè½¬å½•ç»„æ•°æ®é¢„æµ‹ç™Œç—‡å­˜æ´»ç‡çš„æ•ˆæœï¼Œæ•è·æ¨¡æ€å…±äº«å’Œæ¨¡æ€ç‰¹å®šä¿¡æ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€æ¡†æ¶ç»å¸¸çº ç¼ è¿™äº›è¡¨ç¤ºï¼Œé™åˆ¶äº†å¯è§£é‡Šæ€§å¹¶å¯èƒ½æŠ‘åˆ¶è¾¨åˆ«ç‰¹å¾ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè§£è€¦å¯è§£é‡Šå¤šæ¨¡æ€æ³¨æ„åŠ›èåˆâ€ï¼ˆDIMAFï¼‰è¿™ä¸€å¤šæ¨¡æ€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŸºäºæ³¨æ„åŠ›çš„èåˆæœºåˆ¶æ¥åˆ†ç¦»æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„äº¤äº’ä½œç”¨ï¼Œä»¥å­¦ä¹ ç‹¬ç‰¹çš„æ¨¡æ€ç‰¹å®šå’Œæ¨¡æ€å…±äº«è¡¨ç¤ºã€‚æˆ‘ä»¬å¼•å…¥åŸºäºè·ç¦»ç›¸å…³çš„æŸå¤±æ¥ä¿ƒè¿›è¿™äº›è¡¨ç¤ºä¹‹é—´çš„è§£è€¦ï¼Œå¹¶æ•´åˆæ²™æ™®åˆ©åŠ æ³•è§£é‡Šæ¥è¯„ä¼°å®ƒä»¬å¯¹ç”Ÿå­˜é¢„æµ‹çš„ç›¸å¯¹è´¡çŒ®ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…¬å…±ç™Œç—‡ç”Ÿå­˜æ•°æ®é›†ä¸Šè¯„ä¼°äº†DIMAFçš„è¡¨ç°ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½å¹³å‡æé«˜äº†1.85%ï¼Œè§£è€¦ç¨‹åº¦æé«˜äº†23.7%ã€‚é™¤äº†æ€§èƒ½æå‡å¤–ï¼Œæˆ‘ä»¬çš„å¯è§£é‡Šæ¡†æ¶è¿˜èƒ½å¤Ÿæ›´æ·±å…¥åœ°æ¢ç´¢ç™Œç—‡ç”Ÿç‰©å­¦ä¸­æ¨¡æ€ä¹‹é—´å’Œæ¨¡æ€å†…éƒ¨çš„æ½œåœ¨ç›¸äº’ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16069v1">PDF</a> 11 pages, 1 figure, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºDIMAFçš„è§£çº ç¼ å¯è§£é‡Šå¤šæ¨¡æ€æ³¨æ„åŠ›èåˆæ¡†æ¶ï¼Œç”¨äºæ”¹å–„åˆ©ç”¨å…¨æ»‘å›¾åƒå’Œè½¬å½•ç»„å­¦æ•°æ®å¯¹ç™Œç—‡ç”Ÿå­˜ç‡çš„é¢„æµ‹ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆ†ç¦»æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„äº¤äº’ä½œç”¨ï¼Œå­¦ä¹ ç‹¬ç‰¹çš„æ¨¡æ€ç‰¹å®šå’Œæ¨¡æ€å…±äº«è¡¨ç¤ºï¼ŒåŒæ—¶åŸºäºè·ç¦»ç›¸å…³æ€§å¼•å…¥æŸå¤±æ¥ä¿ƒè¿›è¿™äº›è¡¨ç¤ºçš„è§£çº ç¼ ï¼Œå¹¶ä½¿ç”¨ShapleyåŠ æ³•è§£é‡Šæ¥è¯„ä¼°å®ƒä»¬å¯¹ç”Ÿå­˜ç‡é¢„æµ‹çš„ç›¸å¯¹è´¡çŒ®ã€‚åœ¨å››ä¸ªå…¬å…±ç™Œç—‡ç”Ÿå­˜æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼ŒDIMAFçš„æ€§èƒ½å¹³å‡æé«˜äº†1.85%ï¼Œè§£çº ç¼ ç¨‹åº¦æé«˜äº†23.7%ã€‚é™¤äº†æé«˜æ€§èƒ½å¤–ï¼Œæˆ‘ä»¬çš„å¯è§£é‡Šæ¡†æ¶è¿˜èƒ½å¤Ÿæ›´æ·±å…¥åœ°æ¢ç´¢ç™Œç—‡ç”Ÿç‰©å­¦ä¸­æ¨¡æ€ä¹‹é—´å’Œæ¨¡æ€å†…éƒ¨çš„ç›¸äº’ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIMAFæ¡†æ¶æ—¨åœ¨æé«˜åˆ©ç”¨å…¨æ»‘å›¾åƒå’Œè½¬å½•ç»„å­¦æ•°æ®é¢„æµ‹ç™Œç—‡ç”Ÿå­˜ç‡çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡åˆ†ç¦»æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„äº¤äº’ä½œç”¨ï¼Œä»¥å­¦ä¹ ç‹¬ç‰¹çš„æ¨¡æ€ç‰¹å®šå’Œæ¨¡æ€å…±äº«è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥åŸºäºè·ç¦»ç›¸å…³æ€§çš„æŸå¤±æ¥ä¿ƒè¿›è¡¨ç¤ºçš„è§£çº ç¼ ã€‚</li>
<li>ä½¿ç”¨ShapleyåŠ æ³•è§£é‡Šè¯„ä¼°ä¸åŒè¡¨ç¤ºå¯¹ç”Ÿå­˜ç‡é¢„æµ‹çš„ç›¸å¯¹è´¡çŒ®ã€‚</li>
<li>åœ¨å››ä¸ªå…¬å…±ç™Œç—‡ç”Ÿå­˜æ•°æ®é›†ä¸Šï¼ŒDIMAFçš„æ€§èƒ½ç›¸è¾ƒäºç°æœ‰æ¨¡å‹æœ‰æ‰€æå‡ã€‚</li>
<li>DIMAFä¸ä»…æé«˜äº†é¢„æµ‹æ€§èƒ½ï¼Œè€Œä¸”æä¾›äº†å¯¹ç™Œç—‡ç”Ÿç‰©å­¦ä¸­æ¨¡æ€ç›¸äº’ä½œç”¨æ›´æ·±å…¥çš„ç†è§£ã€‚</li>
<li>è¯¥æ¡†æ¶çš„æå‡ºä¸ºç™Œç—‡ç”Ÿå­˜é¢„æµ‹å¸¦æ¥äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d7a77aa5034b87117402e6dd6fbf71c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a14c4cf3f2124027adf1e0a255eb0672.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SALT-Singular-Value-Adaptation-with-Low-Rank-Transformation"><a href="#SALT-Singular-Value-Adaptation-with-Low-Rank-Transformation" class="headerlink" title="SALT: Singular Value Adaptation with Low-Rank Transformation"></a>SALT: Singular Value Adaptation with Low-Rank Transformation</h2><p><strong>Authors:Abdelrahman Elsayed, Sarim Hashmi, Mohammed Elseiagy, Hu Wang, Mohammad Yaqub, Ibrahim Almakky</strong></p>
<p>The complex nature of medical image segmentation calls for models that are specifically designed to capture detailed, domain-specific features. Large foundation models offer considerable flexibility, yet the cost of fine-tuning these models remains a significant barrier. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model weights with low-rank matrices but may suffer from underfitting when the chosen rank is insufficient to capture domain-specific nuances. Conversely, full-rank Singular Value Decomposition (SVD) based methods provide comprehensive updates by modifying all singular values, yet they often lack flexibility and exhibit variable performance across datasets. We propose SALT (Singular Value Adaptation with Low-Rank Transformation), a method that selectively adapts the most influential singular values using trainable scale and shift parameters while complementing this with a low-rank update for the remaining subspace. This hybrid approach harnesses the advantages of both LoRA and SVD, enabling effective adaptation without relying on increasing model size or depth. Evaluated on 5 challenging medical datasets, ranging from as few as 20 samples to 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in Dice with only 3.9% trainable parameters, demonstrating robust adaptation even in low-resource settings. The code for SALT is available at: <a target="_blank" rel="noopener" href="https://github.com/BioMedIA-MBZUAI/SALT">https://github.com/BioMedIA-MBZUAI/SALT</a> </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤æ‚æ€§è¦æ±‚ä¸“é—¨è®¾è®¡çš„æ¨¡å‹æ¥æ•æ‰è¯¦ç»†ã€ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾ã€‚å¤§å‹åŸºç¡€æ¨¡å‹æä¾›äº†å¾ˆå¤§çš„çµæ´»æ€§ï¼Œä½†å¾®è°ƒè¿™äº›æ¨¡å‹çš„æˆæœ¬ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§éšœç¢ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ›´æ–°æ¨¡å‹æƒé‡ä½¿ç”¨ä½ç§©çŸ©é˜µï¼Œä½†å½“æ‰€é€‰çš„ç§©ä¸è¶³ä»¥æ•æ‰ç‰¹å®šé¢†åŸŸçš„ç»†å¾®å·®åˆ«æ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°æ¬ æ‹Ÿåˆçš„æƒ…å†µã€‚ç›¸åï¼Œå…¨ç§©å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰åŸºäºçš„æ–¹æ³•é€šè¿‡ä¿®æ”¹æ‰€æœ‰å¥‡å¼‚å€¼æ¥æä¾›å…¨é¢çš„æ›´æ–°ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹çµæ´»æ€§ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°å„ä¸ç›¸åŒã€‚æˆ‘ä»¬æå‡ºäº†SALTï¼ˆå¸¦æœ‰ä½ç§©å˜æ¢çš„å¥‡å¼‚å€¼é€‚åº”ï¼‰ï¼Œä¸€ç§æ–¹æ³•ï¼Œå®ƒé€‰æ‹©æ€§åœ°é€‚åº”æœ€æœ‰å½±å“åŠ›çš„å¥‡å¼‚å€¼ï¼Œä½¿ç”¨å¯è®­ç»ƒçš„ç¼©æ”¾å’Œç§»ä½å‚æ•°ï¼Œå¹¶ç”¨ä½ç§©æ›´æ–°å¯¹å‰©ä½™çš„å­ç©ºé—´è¿›è¡Œè¡¥å……ã€‚è¿™ç§æ··åˆæ–¹æ³•ç»“åˆäº†LoRAå’ŒSVDçš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ¨¡å‹å¤§å°æˆ–æ·±åº¦çš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆçš„é€‚åº”ã€‚åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ ·æœ¬æ•°é‡ä»æœ€å°‘çš„20ä¸ªåˆ°1000ä¸ªä¸ç­‰ï¼ŒSALTåœ¨Diceä¸Šæ¯”æœ€æ–°çš„PEFTï¼ˆLoRAå’ŒSVDï¼‰é«˜å‡º2%è‡³5%ï¼Œå¹¶ä¸”åªæœ‰3.9%çš„å¯è®­ç»ƒå‚æ•°ï¼Œè¯æ˜äº†å³ä½¿åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿå…·æœ‰ç¨³å¥çš„é€‚åº”èƒ½åŠ›ã€‚SALTçš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/BioMedIA-MBZUAI/SALT%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/BioMedIA-MBZUAI/SALTæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16055v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤æ‚æ€§ï¼Œéœ€è¦ä¸“é—¨è®¾è®¡çš„æ¨¡å‹æ¥æ•æ‰è¯¦ç»†çš„é¢†åŸŸç‰¹å®šç‰¹å¾ã€‚æ–‡ç« æ¢è®¨äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å’Œå…¨ç§©å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•SALTï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ä¸¤è€…çš„ä¼˜ç‚¹ï¼Œé€šè¿‡é€‰æ‹©æ€§é€‚åº”æœ€å…·å½±å“åŠ›çš„å¥‡å¼‚å€¼å¹¶ç»“åˆä½ç§©æ›´æ–°å…¶ä½™å­ç©ºé—´ï¼Œå®ç°äº†æœ‰æ•ˆçš„æ¨¡å‹é€‚åº”ã€‚åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒSALTåœ¨ä»…æœ‰å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ä»è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºç°æœ‰çš„PEFTæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²éœ€è¦ä¸“é—¨è®¾è®¡çš„æ¨¡å‹ä»¥æ•æ‰è¯¦ç»†çš„é¢†åŸŸç‰¹å®šç‰¹å¾ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹é€‚åº”çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>LoRAæ–¹æ³•é€šè¿‡ä½ç§©çŸ©é˜µæœ‰æ•ˆæ›´æ–°æ¨¡å‹æƒé‡ï¼Œä½†å¯èƒ½å› æ‰€é€‰æ’åä¸è¶³è€Œå‡ºç°è¿‡æ‹Ÿåˆã€‚</li>
<li>SVDæ–¹æ³•é€šè¿‡ä¿®æ”¹æ‰€æœ‰å¥‡å¼‚å€¼æä¾›å…¨é¢çš„æ›´æ–°ï¼Œä½†ç¼ºä¹çµæ´»æ€§å¹¶ä¸”åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸ç¨³å®šã€‚</li>
<li>SALTæ–¹æ³•ç»“åˆäº†LoRAå’ŒSVDçš„ä¼˜ç‚¹ï¼Œé€šè¿‡é€‰æ‹©æ€§é€‚åº”æœ€å…·å½±å“åŠ›çš„å¥‡å¼‚å€¼å¹¶ç»“åˆä½ç§©æ›´æ–°å‰©ä½™å­ç©ºé—´ï¼Œå®ç°äº†æœ‰æ•ˆé€‚åº”ã€‚</li>
<li>SALTåœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„PEFTæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-053ce2021c63d6b400eeb6d150fe9faa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e806a6d919da1e611a14e2825fcc19b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c5094406e8d62178a3d8a242eb861f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35ea530d98a2899c7daf74fcc680a4b0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Closer-to-Ground-Truth-Realistic-Shape-and-Appearance-Labeled-Data-Generation-for-Unsupervised-Underwater-Image-Segmentation"><a href="#Closer-to-Ground-Truth-Realistic-Shape-and-Appearance-Labeled-Data-Generation-for-Unsupervised-Underwater-Image-Segmentation" class="headerlink" title="Closer to Ground Truth: Realistic Shape and Appearance Labeled Data   Generation for Unsupervised Underwater Image Segmentation"></a>Closer to Ground Truth: Realistic Shape and Appearance Labeled Data   Generation for Unsupervised Underwater Image Segmentation</h2><p><strong>Authors:Andrei Jelea, Ahmed Nabil Belbachir, Marius Leordeanu</strong></p>
<p>Solving fish segmentation in underwater videos, a real-world problem of great practical value in marine and aquaculture industry, is a challenging task due to the difficulty of the filming environment, poor visibility and limited existing annotated underwater fish data. In order to overcome these obstacles, we introduce a novel two stage unsupervised segmentation approach that requires no human annotations and combines artificially created and real images. Our method generates challenging synthetic training data, by placing virtual fish in real-world underwater habitats, after performing fish transformations such as Thin Plate Spline shape warping and color Histogram Matching, which realistically integrate synthetic fish into the backgrounds, making the generated images increasingly closer to the real world data with every stage of our approach. While we validate our unsupervised method on the popular DeepFish dataset, obtaining a performance close to a fully-supervised SoTA model, we further show its effectiveness on the specific case of salmon segmentation in underwater videos, for which we introduce DeepSalmon, the largest dataset of its kind in the literature (30 GB). Moreover, on both datasets we prove the capability of our approach to boost the performance of the fully-supervised SoTA model. </p>
<blockquote>
<p>è§£å†³æ°´ä¸‹è§†é¢‘ä¸­çš„é±¼ç±»åˆ†å‰²é—®é¢˜æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™ä¸€ç°å®ä¸–ç•Œçš„éš¾é¢˜åœ¨æµ·æ´‹å’Œæ°´äº§å…»æ®–ä¸šä¸­å…·æœ‰å·¨å¤§çš„å®ç”¨ä»·å€¼ã€‚ç”±äºæ‹æ‘„ç¯å¢ƒå›°éš¾ã€èƒ½è§åº¦å·®ä»¥åŠç°æœ‰çš„æ°´ä¸‹é±¼ç±»æ ‡æ³¨æ•°æ®æœ‰é™ï¼Œæˆ‘ä»¬é¢ä¸´è¯¸å¤šéš¾é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ— ç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€äººå·¥æ ‡æ³¨ï¼Œç»“åˆäº†äººå·¥åˆ›å»ºå’ŒçœŸå®å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„åˆæˆè®­ç»ƒæ•°æ®æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡åœ¨çœŸå®çš„æ°´ä¸‹ç¯å¢ƒä¸­æ”¾ç½®è™šæ‹Ÿé±¼ï¼Œå¹¶è¿›è¡Œè¯¸å¦‚è–„æ¿æ ·æ¡å½¢çŠ¶å˜æ¢å’Œé¢œè‰²ç›´æ–¹å›¾åŒ¹é…ç­‰é±¼ç±»å˜æ¢æ“ä½œï¼Œå°†åˆæˆçš„é±¼ç°å®åœ°èå…¥åˆ°èƒŒæ™¯ä¸­ï¼Œä½¿å¾—ç”Ÿæˆçš„å›¾åƒéšç€æˆ‘ä»¬çš„æ–¹æ³•çš„æ¯ä¸€é˜¶æ®µè€Œè¶Šæ¥è¶Šæ¥è¿‘çœŸå®ä¸–ç•Œæ•°æ®ã€‚æˆ‘ä»¬åœ¨æµè¡Œçš„DeepFishæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ— ç›‘ç£æ–¹æ³•ï¼Œå…¶æ€§èƒ½æ¥è¿‘å®Œå…¨ç›‘ç£çš„å½“å‰æœ€ä½³æ¨¡å‹ï¼Œå¹¶è¿›ä¸€æ­¥å±•ç¤ºäº†å…¶åœ¨æ°´ä¸‹è§†é¢‘ä¸­çš„é²‘é±¼åˆ†å‰²è¿™ä¸€ç‰¹å®šæ¡ˆä¾‹ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ä¸ºæ­¤æˆ‘ä»¬å¼•å…¥äº†DeepSalmonæ•°æ®é›†ï¼Œå®ƒæ˜¯æ–‡çŒ®ä¸­åŒç±»ä¸­æœ€å¤§çš„æ•°æ®é›†ï¼ˆ30GBï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬éƒ½è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æå‡å®Œå…¨ç›‘ç£çš„å½“å‰æœ€ä½³æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16051v1">PDF</a> Proceedings of ECCVW 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è§£å†³æ°´ä¸‹è§†é¢‘é±¼ç±»åˆ†å‰²é—®é¢˜çš„æ–°å‹ä¸¤é˜¶æ®µæ— ç›‘ç£åˆ†å‰²æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäººå·¥åˆ›å»ºå’ŒçœŸå®å›¾åƒï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼Œé€šè¿‡é±¼ç±»çš„å½¢æ€å˜åŒ–å’Œè‰²å½©åŒ¹é…æŠ€æœ¯ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ã€‚è¯¥æ–¹æ³•åœ¨DeepFishæ•°æ®é›†ä¸Šçš„è¡¨ç°æ¥è¿‘å…¨ç›‘ç£çš„å½“å‰æœ€ä½³æ¨¡å‹ï¼Œå¹¶åœ¨æœ€å¤§çš„æ°´ä¸‹ä¸‰æ–‡é±¼åˆ†å‰²æ•°æ®é›†DeepSalmonä¸Šå±•ç°äº†å…¶æœ‰æ•ˆæ€§ï¼Œèƒ½æå‡å…¨ç›‘ç£å½“å‰æœ€ä½³æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹ä¸¤é˜¶æ®µæ— ç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œç”¨äºè§£å†³æ°´ä¸‹è§†é¢‘é±¼ç±»åˆ†å‰²é—®é¢˜ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†äººå·¥åˆ›å»ºå’ŒçœŸå®å›¾åƒï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚</li>
<li>é€šè¿‡é±¼ç±»çš„å½¢æ€å˜åŒ–å’Œè‰²å½©åŒ¹é…æŠ€æœ¯ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ã€‚</li>
<li>åœ¨DeepFishæ•°æ®é›†ä¸Šçš„è¡¨ç°æ¥è¿‘å…¨ç›‘ç£çš„å½“å‰æœ€ä½³æ¨¡å‹ã€‚</li>
<li>å¼•å…¥äº†DeepSalmonæ•°æ®é›†ï¼Œä¸ºä¸‰æ–‡é±¼åˆ†å‰²é¢†åŸŸæä¾›äº†æœ€å¤§çš„æ•°æ®é›†ã€‚</li>
<li>åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡è¯æ˜äº†è¯¥æ–¹æ³•èƒ½æå‡å…¨ç›‘ç£å½“å‰æœ€ä½³æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-772d586d04ca13c1ecc0b8a0a1296712.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6369a95d94bd3cd55a6ddde8c7566b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47522443e7a2862bb2af8ba886c14bda.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DIPLI-Deep-Image-Prior-Lucky-Imaging-for-Blind-Astronomical-Image-Restoration"><a href="#DIPLI-Deep-Image-Prior-Lucky-Imaging-for-Blind-Astronomical-Image-Restoration" class="headerlink" title="DIPLI: Deep Image Prior Lucky Imaging for Blind Astronomical Image   Restoration"></a>DIPLI: Deep Image Prior Lucky Imaging for Blind Astronomical Image   Restoration</h2><p><strong>Authors:Suraj Singh, Anastasia Batsheva, Oleg Y. Rogov, Ahmed Bouridane</strong></p>
<p>Contemporary image restoration and super-resolution techniques effectively harness deep neural networks, markedly outperforming traditional methods. However, astrophotography presents unique challenges for deep learning due to limited training data. This work explores hybrid strategies, such as the Deep Image Prior (DIP) model, which facilitates blind training but is susceptible to overfitting, artifact generation, and instability when handling noisy images. We propose enhancements to the DIP modelâ€™s baseline performance through several advanced techniques. First, we refine the model to process multiple frames concurrently, employing the Back Projection method and the TVNet model. Next, we adopt a Markov approach incorporating Monte Carlo estimation, Langevin dynamics, and a variational input technique to achieve unbiased estimates with minimal variance and counteract overfitting effectively. Collectively, these modifications reduce the likelihood of noise learning and mitigate loss function fluctuations during training, enhancing result stability. We validated our algorithm across multiple image sets of astronomical and celestial objects, achieving performance that not only mitigates limitations of Lucky Imaging, a classical computer vision technique that remains a standard in astronomical image reconstruction but surpasses the original DIP model, state of the art transformer- and diffusion-based models, underscoring the significance of our improvements. </p>
<blockquote>
<p>å½“å‰å›¾åƒæ¢å¤å’Œè¶…åˆ†è¾¨ç‡æŠ€æœ¯æœ‰æ•ˆåœ°åˆ©ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºå¤©æ–‡æ‘„å½±çš„è®­ç»ƒæ•°æ®æœ‰é™ï¼Œç»™æ·±åº¦å­¦ä¹ å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œæ¢ç´¢äº†æ··åˆç­–ç•¥ï¼Œä¾‹å¦‚æ·±åº¦å›¾åƒå…ˆéªŒï¼ˆDIPï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¾¿äºç›²è®­ç»ƒï¼Œä½†å¤„ç†å¸¦å™ªå£°çš„å›¾åƒæ—¶å®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆã€ä¼ªå½±ç”Ÿæˆå’Œä¸ç¨³å®šçš„æƒ…å†µã€‚æˆ‘ä»¬æå‡ºé€šè¿‡å‡ ç§å…ˆè¿›æŠ€æœ¯å¢å¼ºDIPæ¨¡å‹çš„åŸºçº¿æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ”¹è¿›æ¨¡å‹ä»¥åŒæ—¶å¤„ç†å¤šå¸§å›¾åƒï¼Œé‡‡ç”¨åå‘æŠ•å½±æ–¹æ³•å’ŒTVNetæ¨¡å‹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é‡‡ç”¨é©¬å°”å¯å¤«æ–¹æ³•ï¼Œç»“åˆè’™ç‰¹å¡ç½—ä¼°è®¡ã€æœ—æ ¼æ–‡åŠ¨åŠ›å­¦å’Œå˜å¼‚è¾“å…¥æŠ€æœ¯ï¼Œä»¥å®ç°å…·æœ‰æœ€å°æ–¹å·®çš„æ— åä¼°è®¡ï¼Œå¹¶æœ‰æ•ˆå¯¹æŠ—è¿‡æ‹Ÿåˆã€‚è¿™äº›ä¿®æ”¹æ€»ä½“ä¸Šé™ä½äº†å™ªå£°å­¦ä¹ çš„å¯èƒ½æ€§ï¼Œå‡å°‘äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°æ³¢åŠ¨ï¼Œå¢å¼ºäº†ç»“æœçš„ç¨³å®šæ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¤©æ–‡å’Œå¤©ä½“å›¾åƒé›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„ç®—æ³•ï¼Œä¸ä»…å®ç°äº†æ€§èƒ½çš„æå‡ï¼Œç¼“è§£äº†ä»ç„¶æ˜¯å¤©æ–‡å›¾åƒé‡å»ºæ ‡å‡†çš„ç»å…¸è®¡ç®—æœºè§†è§‰æŠ€æœ¯â€”â€”å¹¸è¿æˆåƒçš„å±€é™æ€§ï¼Œè¿˜è¶…è¶Šäº†åŸå§‹çš„DIPæ¨¡å‹ã€æœ€å…ˆè¿›çš„åŸºäºè½¬æ¢å’Œæ‰©æ•£çš„æ¨¡å‹ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æ”¹è¿›çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15984v1">PDF</a> 10 pages, 7 figures, 2 tables</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†æ·±åº¦å­¦ä¹ åœ¨å¤©æ–‡æ‘„å½±ä¸­çš„å›¾åƒä¿®å¤ä¸è¶…åˆ†è¾¨ç‡æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶é’ˆå¯¹Deep Image Priorï¼ˆDIPï¼‰æ¨¡å‹æå‡ºäº†æ”¹è¿›ç­–ç•¥ã€‚é€šè¿‡ç»“åˆå¤šç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤šå¸§å¤„ç†ã€Markovæ–¹æ³•ä»¥åŠè’™ç‰¹å¡æ´›ä¼°è®¡ç­‰ï¼Œæœ¬æ–‡æå‡ºçš„ç®—æ³•ä¸ä»…è§£å†³äº†DIPæ¨¡å‹çš„å±€é™æ€§ï¼Œå¦‚è¿‡åº¦æ‹Ÿåˆå’Œå™ªå£°ç”Ÿæˆé—®é¢˜ï¼Œè€Œä¸”è¶…è¶Šäº†å½“å‰ä¸»æµçš„è½¬æ¢å™¨æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨å¤©æ–‡æ‘„å½±çš„å›¾åƒæ¢å¤å’Œè¶…åˆ†è¾¨ç‡å¤„ç†ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>Deep Image Priorï¼ˆDIPï¼‰æ¨¡å‹åœ¨å¤©æ–‡æ‘„å½±ä¸­é¢ä¸´è®­ç»ƒæ•°æ®æœ‰é™ã€æ˜“è¿‡æ‹ŸåˆåŠä¸ç¨³å®šç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ç»“åˆå¤šç§å…ˆè¿›æŠ€æœ¯çš„æ”¹è¿›ç­–ç•¥ï¼Œå¦‚å¤šå¸§å¤„ç†ã€Back Projectionæ–¹æ³•å’ŒTVNetæ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨Markovæ–¹æ³•ç»“åˆè’™ç‰¹å¡æ´›ä¼°è®¡ã€æœ—ç»´åŠ¨åŠ›å­¦å’Œå˜åˆ†è¾“å…¥æŠ€æœ¯ï¼Œå®ç°æ— åä¼°è®¡å¹¶é™ä½æ–¹å·®ã€‚</li>
<li>æ”¹è¿›ç­–ç•¥æœ‰åŠ©äºå‡å°‘å™ªå£°å­¦ä¹ ï¼Œå¹¶å‡è½»è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°æ³¢åŠ¨ï¼Œå¢å¼ºç»“æœçš„ç¨³å®šæ€§ã€‚</li>
<li>éªŒè¯ç®—æ³•åœ¨å¤šä¸ªå¤©æ–‡å’Œå¤©ä½“å›¾åƒé›†ä¸Šçš„æ€§èƒ½ï¼Œä¸ä»…è¶…è¶Šäº†ä¼ ç»Ÿçš„Lucky ImagingæŠ€æœ¯ï¼Œä¹Ÿè¶…è¶Šäº†åŸå§‹çš„DIPæ¨¡å‹å’Œå½“å‰ä¸»æµæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-770d4d61aa51cd0d13307db13cc3b20a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7393888281158c1bc4faeaf7474cf61f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8e0622e7cc5c23d211f7258255a8004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cbc7fce9294d6e1759d9274c1e5d8c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CausalCLIPSeg-Unlocking-CLIPâ€™s-Potential-in-Referring-Medical-Image-Segmentation-with-Causal-Intervention"><a href="#CausalCLIPSeg-Unlocking-CLIPâ€™s-Potential-in-Referring-Medical-Image-Segmentation-with-Causal-Intervention" class="headerlink" title="CausalCLIPSeg: Unlocking CLIPâ€™s Potential in Referring Medical Image   Segmentation with Causal Intervention"></a>CausalCLIPSeg: Unlocking CLIPâ€™s Potential in Referring Medical Image   Segmentation with Causal Intervention</h2><p><strong>Authors:Yaxiong Chen, Minghong Wei, Zixuan Zheng, Jingliang Hu, Yilei Shi, Shengwu Xiong, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Referring medical image segmentation targets delineating lesions indicated by textual descriptions. Aligning visual and textual cues is challenging due to their distinct data properties. Inspired by large-scale pre-trained vision-language models, we propose CausalCLIPSeg, an end-to-end framework for referring medical image segmentation that leverages CLIP. Despite not being trained on medical data, we enforce CLIPâ€™s rich semantic space onto the medical domain by a tailored cross-modal decoding method to achieve text-to-pixel alignment. Furthermore, to mitigate confounding bias that may cause the model to learn spurious correlations instead of meaningful causal relationships, CausalCLIPSeg introduces a causal intervention module which self-annotates confounders and excavates causal features from inputs for segmentation judgments. We also devise an adversarial min-max game to optimize causal features while penalizing confounding ones. Extensive experiments demonstrate the state-of-the-art performance of our proposed method. Code is available at <a target="_blank" rel="noopener" href="https://github.com/WUTCM-Lab/CausalCLIPSeg">https://github.com/WUTCM-Lab/CausalCLIPSeg</a>. </p>
<blockquote>
<p>å…³äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ç›®æ ‡æ˜¯æ ¹æ®æ–‡æœ¬æè¿°æ¥æç»˜ç—…å˜ã€‚ç”±äºè§†è§‰å’Œæ–‡æœ¬æç¤ºçš„ä¸åŒæ•°æ®å±æ€§ï¼Œå¯¹é½å®ƒä»¬æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚å—å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†CausalCLIPSegï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå‚è€ƒåŒ»å­¦å›¾åƒåˆ†å‰²çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨CLIPã€‚å°½ç®¡æ²¡æœ‰åœ¨åŒ»ç–—æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†æˆ‘ä»¬é€šè¿‡å®šåˆ¶çš„è·¨æ¨¡æ€è§£ç æ–¹æ³•å°†CLIPä¸°å¯Œçš„è¯­ä¹‰ç©ºé—´å¼ºåˆ¶æ–½åŠ åˆ°åŒ»ç–—é¢†åŸŸï¼Œä»¥å®ç°æ–‡æœ¬åˆ°åƒç´ çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»å¯èƒ½å¯¼è‡´æ¨¡å‹å­¦ä¹ è™šå‡å…³è”è€Œéæœ‰æ„ä¹‰çš„å› æœå…³ç³»çš„æ··æ·†åå·®ï¼ŒCausalCLIPSegå¼•å…¥äº†ä¸€ä¸ªå› æœå¹²é¢„æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯¹æ··æ·†å› ç´ è¿›è¡Œè‡ªæˆ‘æ³¨é‡Šï¼Œå¹¶ä»è¾“å…¥ä¸­æŒ–æ˜å› æœç‰¹å¾ä»¥è¿›è¡Œåˆ†å‰²åˆ¤æ–­ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªå¯¹æŠ—æ€§çš„æœ€å°æœ€å¤§æ¸¸æˆæ¥ä¼˜åŒ–å› æœç‰¹å¾ï¼ŒåŒæ—¶æƒ©ç½šæ··æ·†ç‰¹å¾ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„å…ˆè¿›æ°´å¹³ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WUTCM-Lab/CausalCLIPSeg%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/WUTCM-Lab/CausalCLIPSegè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15949v1">PDF</a> MICCAI 2024</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬æè¿°çš„åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´è§†è§‰ä¸æ–‡æœ¬çº¿ç´¢å¯¹é½çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºCausalCLIPSegçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨CLIPä¸°å¯Œçš„è¯­ä¹‰ç©ºé—´å¹¶é€šè¿‡å®šåˆ¶åŒ–çš„è·¨æ¨¡æ€è§£ç æ–¹æ³•å®ç°æ–‡æœ¬åˆ°åƒç´ çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œä¸ºç¼“è§£å¯èƒ½äº§ç”Ÿæ¨¡å‹å­¦ä¹ è¡¨é¢å…³è”è€ŒéçœŸå®å› æœå…³ç³»çš„æ··æ·†åå·®ï¼ŒCausalCLIPSegå¼•å…¥äº†å› æœå¹²é¢„æ¨¡å—ï¼Œä»è¾“å…¥ä¸­è‡ªæˆ‘æ ‡æ³¨æ··æ·†å› ç´ å¹¶æŒ–æ˜å› æœç‰¹å¾ç”¨äºåˆ†å‰²åˆ¤æ–­ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§å¯¹æŠ—æœ€å°æœ€å¤§æ¸¸æˆä»¥ä¼˜åŒ–å› æœç‰¹å¾å¹¶æŠ‘åˆ¶æ··æ·†ç‰¹å¾ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•è¡¨ç°ä¼˜å¼‚ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²éœ€ç»“åˆæ–‡æœ¬æè¿°è¿›è¡Œï¼Œå­˜åœ¨è§†è§‰ä¸æ–‡æœ¬çº¿ç´¢å¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºCausalCLIPSegæ¡†æ¶ï¼Œåˆ©ç”¨CLIPè¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>é€šè¿‡å®šåˆ¶åŒ–çš„è·¨æ¨¡æ€è§£ç æ–¹æ³•å®ç°æ–‡æœ¬åˆ°åƒç´ çš„å¯¹é½ã€‚</li>
<li>å¼•å…¥å› æœå¹²é¢„æ¨¡å—ä»¥ç¼“è§£æ··æ·†åå·®ï¼ŒåŒºåˆ†çœŸå®å› æœå…³ç³»ä¸è¡¨é¢å…³è”ã€‚</li>
<li>å› æœå¹²é¢„æ¨¡å—èƒ½ä»è¾“å…¥ä¸­è‡ªæˆ‘æ ‡æ³¨æ··æ·†å› ç´ å¹¶æŒ–æ˜å› æœç‰¹å¾ã€‚</li>
<li>è®¾è®¡å¯¹æŠ—æœ€å°æœ€å¤§æ¸¸æˆä»¥ä¼˜åŒ–å› æœç‰¹å¾ï¼ŒåŒæ—¶æŠ‘åˆ¶æ··æ·†ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b03d505d1d1b7e4c96b74f096373bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b6b8a7fc5d24dd43a1b8896f0391d36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-791f77bb3ecc0d679de80afa13d13e12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5414196dc66bfdc46e5b803eb626b8ea.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UniCrossAdapter-Multimodal-Adaptation-of-CLIP-for-Radiology-Report-Generation"><a href="#UniCrossAdapter-Multimodal-Adaptation-of-CLIP-for-Radiology-Report-Generation" class="headerlink" title="UniCrossAdapter: Multimodal Adaptation of CLIP for Radiology Report   Generation"></a>UniCrossAdapter: Multimodal Adaptation of CLIP for Radiology Report   Generation</h2><p><strong>Authors:Yaxiong Chen, Chuang Du, Chunlei Li, Jingliang Hu, Yilei Shi, Shengwu Xiong, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Automated radiology report generation aims to expedite the tedious and error-prone reporting process for radiologists. While recent works have made progress, learning to align medical images and textual findings remains challenging due to the relative scarcity of labeled medical data. For example, datasets for this task are much smaller than those used for image captioning in computer vision. In this work, we propose to transfer representations from CLIP, a large-scale pre-trained vision-language model, to better capture cross-modal semantics between images and texts. However, directly applying CLIP is suboptimal due to the domain gap between natural images and radiology. To enable efficient adaptation, we introduce UniCrossAdapter, lightweight adapter modules that are incorporated into CLIP and fine-tuned on the target task while keeping base parameters fixed. The adapters are distributed across modalities and their interaction to enhance vision-language alignment. Experiments on two public datasets demonstrate the effectiveness of our approach, advancing state-of-the-art in radiology report generation. The proposed transfer learning framework provides a means of harnessing semantic knowledge from large-scale pre-trained models to tackle data-scarce medical vision-language tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/chauncey-tow/MRG-CLIP">https://github.com/chauncey-tow/MRG-CLIP</a>. </p>
<blockquote>
<p>è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ—¨åœ¨åŠ é€Ÿæ”¾å°„ç§‘åŒ»ç”Ÿæ¯ç‡¥ä¸”æ˜“å‡ºé”™çš„æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚å°½ç®¡è¿‘æœŸçš„ç ”ç©¶æœ‰æ‰€è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹æ ‡è®°çš„åŒ»å­¦æ•°æ®ï¼Œå­¦ä¹ å¯¹é½åŒ»å­¦å›¾åƒå’Œæ–‡æœ¬å‘ç°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¾‹å¦‚ï¼Œç”¨äºæ­¤ä»»åŠ¡çš„æ•°æ®é›†è¿œè¿œå°äºè®¡ç®—æœºè§†è§‰é¢†åŸŸç”¨äºå›¾åƒæè¿°çš„æ•°æ®é›†ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨CLIPï¼ˆä¸€ç§å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰çš„è¡¨å¾æ¥è¿›è¡Œè·¨æ¨¡æ€è¯­ä¹‰æ•æ‰å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å…³è”ã€‚ç„¶è€Œï¼Œç”±äºè‡ªç„¶å›¾åƒä¸æ”¾å°„å­¦ä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·ï¼Œç›´æ¥åº”ç”¨CLIPå¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†å®ç°æœ‰æ•ˆçš„é€‚åº”ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniCrossAdapterï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„é€‚é…å™¨æ¨¡å—ï¼Œå®ƒèå…¥äº†CLIPå¹¶åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶ä¿æŒåŸºç¡€å‚æ•°ä¸å˜ã€‚è¿™äº›é€‚é…å™¨åˆ†å¸ƒåœ¨ä¸åŒçš„æ¨¡æ€ä¹‹é—´ï¼Œé€šè¿‡å®ƒä»¬ä¹‹é—´çš„äº¤äº’å¢å¼ºè§†è§‰è¯­è¨€çš„å¯¹é½ã€‚åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚æ‰€æå‡ºçš„è¿ç§»å­¦ä¹ æ¡†æ¶æä¾›äº†ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„è¯­ä¹‰çŸ¥è¯†æ¥è§£å†³æ•°æ®ç¨€ç¼ºçš„åŒ»å­¦è§†è§‰è¯­è¨€ä»»åŠ¡çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chauncey-tow/MRG-CLIP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/chauncey-tow/MRG-CLIPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15940v1">PDF</a> MICCAI 2024 Workshop</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹CLIPçš„è¿ç§»è¡¨ç¤ºï¼ŒåŠ©åŠ›åŒ»å­¦å›¾åƒä¸æ–‡æœ¬é—´çš„è·¨æ¨¡æ€è¯­ä¹‰æ•æ‰ï¼Œåœ¨è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚é’ˆå¯¹ç›´æ¥åº”ç”¨CLIPå­˜åœ¨çš„é¢†åŸŸå·®è·é—®é¢˜ï¼Œæå‡ºUniCrossAdapterï¼Œå®ç°é«˜æ•ˆé€‚åº”ã€‚é€‚é…å™¨æ¨¡å—èå…¥CLIPï¼Œå¹¶åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶ä¿æŒåŸºç¡€å‚æ•°ä¸å˜ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ¨è¿›äº†è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆé¢†åŸŸçš„å‰æ²¿ã€‚æä¾›ä»£ç å¼€æºä¸‹è½½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªåŠ¨åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ—¨åœ¨åŠ å¿«æ”¾å°„ç§‘åŒ»ç”Ÿç¹çä¸”æ˜“å‡ºé”™çš„æŠ¥å‘Šè¿‡ç¨‹ã€‚</li>
<li>è¿ç§»å­¦ä¹ å¯¹äºè§£å†³åŒ»å­¦å›¾åƒä¸æ–‡æœ¬å¯¹é½çš„æŒ‘æˆ˜è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨CLIPæ¨¡å‹è¿›è¡Œè·¨æ¨¡æ€è¯­ä¹‰æ•æ‰ï¼Œä½†ç›´æ¥åº”ç”¨é¢ä¸´é¢†åŸŸå·®å¼‚æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºUniCrossAdapterï¼Œå®ç°é«˜æ•ˆé€‚åº”ï¼Œå°†é€‚é…å™¨æ¨¡å—èå…¥CLIPå¹¶å¾®è°ƒç›®æ ‡ä»»åŠ¡ã€‚</li>
<li>é€‚é…å™¨æ¨¡å—åˆ†å¸ƒåœ¨ä¸åŒçš„æ¨¡æ€ä¹‹é—´ï¼Œé€šè¿‡äº¤äº’å¢å¼ºè§†è§‰è¯­è¨€å¯¹é½ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ¨è¿›äº†æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bde777bb8ed44f4e90a0a519222052b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a808342082d3e4596e0e2fb62517fd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99a50f0a4c58f2853d921aec048c1a4e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Jasmine-Harnessing-Diffusion-Prior-for-Self-supervised-Depth-Estimation"><a href="#Jasmine-Harnessing-Diffusion-Prior-for-Self-supervised-Depth-Estimation" class="headerlink" title="Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation"></a>Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation</h2><p><strong>Authors:Jiyuan Wang, Chunyu Lin, Cheng Guan, Lang Nie, Jing He, Haodong Li, Kang Liao, Yao Zhao</strong></p>
<p>In this paper, we propose Jasmine, the first Stable Diffusion (SD)-based self-supervised framework for monocular depth estimation, which effectively harnesses SDâ€™s visual priors to enhance the sharpness and generalization of unsupervised prediction. Previous SD-based methods are all supervised since adapting diffusion models for dense prediction requires high-precision supervision. In contrast, self-supervised reprojection suffers from inherent challenges (e.g., occlusions, texture-less regions, illumination variance), and the predictions exhibit blurs and artifacts that severely compromise SDâ€™s latent priors. To resolve this, we construct a novel surrogate task of hybrid image reconstruction. Without any additional supervision, it preserves the detail priors of SD models by reconstructing the images themselves while preventing depth estimation from degradation. Furthermore, to address the inherent misalignment between SDâ€™s scale and shift invariant estimation and self-supervised scale-invariant depth estimation, we build the Scale-Shift GRU. It not only bridges this distribution gap but also isolates the fine-grained texture of SD output against the interference of reprojection loss. Extensive experiments demonstrate that Jasmine achieves SoTA performance on the KITTI benchmark and exhibits superior zero-shot generalization across multiple datasets. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†Jasmineï¼Œè¿™æ˜¯åŸºäºStable Diffusionï¼ˆSDï¼‰çš„å•çœ¼æ·±åº¦ä¼°è®¡è‡ªç›‘ç£æ¡†æ¶ï¼Œå®ƒæœ‰æ•ˆåœ°åˆ©ç”¨äº†SDçš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜äº†æ— ç›‘ç£é¢„æµ‹çš„æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¹‹å‰çš„åŸºäºSDçš„æ–¹æ³•éƒ½æ˜¯æœ‰ç›‘ç£çš„ï¼Œå› ä¸ºå°†æ‰©æ•£æ¨¡å‹ç”¨äºå¯†é›†é¢„æµ‹éœ€è¦é«˜ç²¾åº¦ç›‘ç£ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè‡ªç›‘ç£é‡æŠ•å½±é¢ä¸´å›ºæœ‰çš„æŒ‘æˆ˜ï¼ˆä¾‹å¦‚é®æŒ¡ã€æ— çº¹ç†åŒºåŸŸã€å…‰ç…§å˜åŒ–ï¼‰ï¼Œé¢„æµ‹ç»“æœå‡ºç°æ¨¡ç³Šå’Œä¼ªå½±ï¼Œä¸¥é‡æŸå®³SDçš„æ½œåœ¨å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ··åˆå›¾åƒé‡å»ºçš„æ–°å‹æ›¿ä»£ä»»åŠ¡ã€‚åœ¨æ²¡æœ‰ä»»ä½•é¢å¤–ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œå®ƒé€šè¿‡é‡å»ºå›¾åƒæœ¬èº«æ¥ä¿ç•™SDæ¨¡å‹çš„ç»†èŠ‚å…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶é˜²æ­¢æ·±åº¦ä¼°è®¡é€€åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³SDå°ºåº¦å’Œç§»ä½ä¸å˜ä¼°è®¡ä¸è‡ªç›‘ç£å°ºåº¦ä¸å˜æ·±åº¦ä¼°è®¡ä¹‹é—´çš„å›ºæœ‰ä¸åŒ¹é…é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†Scale-Shift GRUã€‚å®ƒä¸ä»…å¼¥è¡¥äº†åˆ†å¸ƒå·®è·ï¼Œè€Œä¸”éš”ç¦»äº†SDè¾“å‡ºçš„ç²¾ç»†çº¹ç†ï¼Œä¸å—é‡æŠ•å½±æŸå¤±çš„å¹²æ‰°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒJasmineåœ¨KITTIåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15905v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Jasmineï¼Œé¦–ä¸ªåŸºäºStable Diffusionï¼ˆSDï¼‰çš„è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåˆ©ç”¨SDçš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜äº†æ— ç›‘ç£é¢„æµ‹çš„æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä»¥å¾€SDæ–¹æ³•å‡éœ€ç›‘ç£ï¼Œè€Œè‡ªç›‘ç£é‡æŠ•å½±é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æ„å»ºäº†æ··åˆå›¾åƒé‡å»ºçš„æ›¿ä»£ä»»åŠ¡ï¼Œæ— éœ€é¢å¤–ç›‘ç£ï¼Œä¿ç•™äº†SDæ¨¡å‹çš„ç»†èŠ‚å…ˆéªŒï¼Œå¹¶é˜²æ­¢æ·±åº¦ä¼°è®¡é€€åŒ–ã€‚åŒæ—¶ï¼Œä¸ºè§£å†³SDå°ºåº¦ä¸è‡ªç›‘ç£æ·±åº¦ä¼°è®¡ä¹‹é—´çš„å›ºæœ‰ä¸åŒ¹é…é—®é¢˜ï¼Œæ„å»ºäº†Scale-Shift GRUï¼Œä¸ä»…å¼¥è¡¥äº†åˆ†å¸ƒå·®è·ï¼Œè€Œä¸”éš”ç¦»äº†SDè¾“å‡ºçš„ç»†ç²’åº¦çº¹ç†ï¼Œå‡å°‘äº†é‡æŠ•å½±æŸå¤±çš„å¹²æ‰°ã€‚å®éªŒè¡¨æ˜ï¼ŒJasmineåœ¨KITTIåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Jasmineæ˜¯åŸºäºStable Diffusionçš„è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œæé«˜äº†é¢„æµ‹çš„æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¹‹å‰çš„SDæ–¹æ³•éƒ½éœ€è¦ç›‘ç£ï¼Œè€ŒJasmineå®ç°äº†æ— ç›‘ç£çš„æ·±åº¦ä¼°è®¡ã€‚</li>
<li>è‡ªç›‘ç£é‡æŠ•å½±é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚é®æŒ¡ã€æ— çº¹ç†åŒºåŸŸã€å…‰ç…§å˜åŒ–ç­‰ã€‚</li>
<li>æ„å»ºäº†æ··åˆå›¾åƒé‡å»ºçš„æ›¿ä»£ä»»åŠ¡ï¼Œæ— éœ€é¢å¤–ç›‘ç£ï¼Œä¿ç•™äº†SDæ¨¡å‹çš„ç»†èŠ‚å…ˆéªŒã€‚</li>
<li>è§£å†³SDå°ºåº¦ä¸è‡ªç›‘ç£æ·±åº¦ä¼°è®¡ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œæ„å»ºäº†Scale-Shift GRUã€‚</li>
<li>Jasmineåœ¨KITTIåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fea0a823a3d0e6d8fd792dbac2bdfb2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e9e0a924e01262778ffb3e244cb9388.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-677cff62fa6ad345e1a008c81d85fe7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d08308e41099c6c6b784e94dbef00638.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="UMIT-Unifying-Medical-Imaging-Tasks-via-Vision-Language-Models"><a href="#UMIT-Unifying-Medical-Imaging-Tasks-via-Vision-Language-Models" class="headerlink" title="UMIT: Unifying Medical Imaging Tasks via Vision-Language Models"></a>UMIT: Unifying Medical Imaging Tasks via Vision-Language Models</h2><p><strong>Authors:Haiyang Yu, Siyang Yi, Ke Niu, Minghan Zhuo, Bin Li</strong></p>
<p>With the rapid advancement of deep learning, particularly in the field of medical image analysis, an increasing number of Vision-Language Models (VLMs) are being widely applied to solve complex health and biomedical challenges. However, existing research has primarily focused on specific tasks or single modalities, which limits their applicability and generalization across diverse medical scenarios. To address this challenge, we propose UMIT, a unified multi-modal, multi-task VLM designed specifically for medical imaging tasks. UMIT is able to solve various tasks, including visual question answering, disease detection, and medical report generation. In addition, it is applicable to multiple imaging modalities (e.g., X-ray, CT and PET), covering a wide range of applications from basic diagnostics to complex lesion analysis. Moreover, UMIT supports both English and Chinese, expanding its applicability globally and ensuring accessibility to healthcare services in different linguistic contexts. To enhance the modelâ€™s adaptability and task-handling capability, we design a unique two-stage training strategy and fine-tune UMIT with designed instruction templates. Through extensive empirical evaluation, UMIT outperforms previous methods in five tasks across multiple datasets. The performance of UMIT indicates that it can significantly enhance diagnostic accuracy and workflow efficiency, thus providing effective solutions for medical imaging applications. </p>
<blockquote>
<p>éšç€æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸï¼Œè¶Šæ¥è¶Šå¤šçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¢«å¹¿æ³›åº”ç”¨äºè§£å†³å¤æ‚çš„å¥åº·å’Œç”Ÿç‰©åŒ»å­¦æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šä»»åŠ¡æˆ–å•ä¸€æ¨¡æ€ä¸Šï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤šæ ·åŒ»ç–—åœºæ™¯ä¸­çš„åº”ç”¨å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UMITï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤šä»»åŠ¡VLMï¼Œä¸“ä¸ºåŒ»å­¦æˆåƒä»»åŠ¡è€Œè®¾è®¡ã€‚UMITèƒ½å¤Ÿè§£å†³å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰é—®ç­”ã€ç–¾ç—…æ£€æµ‹å’ŒåŒ»ç–—æŠ¥å‘Šç”Ÿæˆã€‚æ­¤å¤–ï¼Œå®ƒé€‚ç”¨äºå¤šç§æˆåƒæ¨¡æ€ï¼ˆå¦‚Xå°„çº¿ã€CTå’ŒPETï¼‰ï¼Œæ¶µç›–ä»åŸºæœ¬è¯Šæ–­åˆ°å¤æ‚ç—…ç¶åˆ†æçš„å„ç§åº”ç”¨ã€‚è€Œä¸”ï¼ŒUMITæ”¯æŒè‹±æ–‡å’Œä¸­æ–‡ï¼Œæ‰©å¤§äº†å…¶å…¨çƒåº”ç”¨èŒƒå›´ï¼Œç¡®ä¿äº†ä¸åŒè¯­è¨€ç¯å¢ƒä¸‹çš„åŒ»ç–—ä¿å¥æœåŠ¡å¯åŠæ€§ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„é€‚åº”æ€§å’Œä»»åŠ¡å¤„ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è®¾è®¡äº†ç‹¬ç‰¹çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¹¶ä½¿ç”¨è®¾è®¡å¥½çš„æŒ‡ä»¤æ¨¡æ¿å¯¹UMITè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼ŒUMITåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„äº”ä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè¶…è¶Šå…ˆå‰æ–¹æ³•çš„æ•ˆæœã€‚UMITçš„æ€§èƒ½è¡¨æ˜ï¼Œå®ƒå¯ä»¥æ˜¾è‘—æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œå·¥ä½œæµç¨‹æ•ˆç‡ï¼Œä»è€Œä¸ºåŒ»å­¦æˆåƒåº”ç”¨æä¾›æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15892v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œè¶Šæ¥è¶Šå¤šçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¢«å¹¿æ³›åº”ç”¨äºè§£å†³å¤æ‚çš„åŒ»ç–—å’Œç”Ÿç‰©åŒ»å­¦æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤šæ ·åŒ»ç–—åœºæ™¯ä¸‹çš„åº”ç”¨å±€é™ï¼Œæå‡ºäº†UMITè¿™ä¸€ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤šä»»åŠ¡VLMæ¨¡å‹ï¼Œç”¨äºåŒ»å­¦æˆåƒä»»åŠ¡ã€‚UMITå¯è§£å†³è§†è§‰é—®ç­”ã€ç–¾ç—…æ£€æµ‹ã€åŒ»å­¦æŠ¥å‘Šç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ï¼Œå¹¶é€‚ç”¨äºå¤šç§æˆåƒæ¨¡æ€ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒä¸­è‹±æ–‡ï¼Œå…¨çƒé€‚ç”¨ã€‚é€šè¿‡ç‹¬ç‰¹çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œç²¾ç»†è°ƒæ•´ï¼ŒUMITåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„äº”ä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè¶…è¶Šå…ˆå‰æ–¹æ³•çš„æ€§èƒ½ã€‚UMITçš„å‡ºè‰²è¡¨ç°æ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œå·¥ä½œæµç¨‹æ•ˆç‡ï¼Œä¸ºåŒ»å­¦æˆåƒåº”ç”¨æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UMITæ˜¯ä¸€ä¸ªé’ˆå¯¹åŒ»å­¦æˆåƒçš„å¤šæ¨¡æ€å¤šä»»åŠ¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚</li>
<li>UMITèƒ½å¤Ÿå¤„ç†è§†è§‰é—®ç­”ã€ç–¾ç—…æ£€æµ‹ã€åŒ»å­¦æŠ¥å‘Šç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ã€‚</li>
<li>UMITé€‚ç”¨äºå¤šç§æˆåƒæ¨¡æ€ï¼Œå¦‚Xå…‰ã€CTå’ŒPETã€‚</li>
<li>UMITæ”¯æŒä¸­è‹±æ–‡ï¼Œå…¨çƒé€‚ç”¨ï¼Œä¾¿äºä¸åŒè¯­è¨€ç¯å¢ƒä¸‹çš„åŒ»ç–—æœåŠ¡è·å–ã€‚</li>
<li>UMITé‡‡ç”¨ç‹¬ç‰¹çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å¹¶è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œä»¥æé«˜é€‚åº”æ€§å’Œä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚</li>
<li>UMITåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„äº”ä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºè¶…è¶Šå…ˆå‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd143311ff135de509b96813339f5658.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df8bb7f601c6725cf1020431fa7fd332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-438d6313edbdea0542d9d3de6fcf0218.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9d897152fdb2a29a46e73f0dc86b149.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Supernova-production-of-axion-like-particles-coupling-to-electrons-reloaded"><a href="#Supernova-production-of-axion-like-particles-coupling-to-electrons-reloaded" class="headerlink" title="Supernova production of axion-like particles coupling to electrons,   reloaded"></a>Supernova production of axion-like particles coupling to electrons,   reloaded</h2><p><strong>Authors:Damiano F. G. Fiorillo, Tetyana Pitik, Edoardo Vitagliano</strong></p>
<p>We revisit the production of axion-like particles (ALPs) coupled to electrons at tree-level in a relativistic plasma. We explicitly demonstrate the equivalence between pseudoscalar and derivative couplings, incorporate previously neglected processes for the first time-namely, semi-Compton production ($\gamma e^-\rightarrow a e^-$) and pair annihilation ($e^+e^-\rightarrow a\gamma$)-and derive analytical expressions for the bremsstrahlung ($e^- N\to e^- N a$) production rate, enabling a more computationally efficient evaluation of the ALP flux. Additionally, we assess uncertainties in the production rate arising from electron thermal mass corrections, electron-electron Coulomb interactions, and the Landau-Pomeranchuk-Migdal effect. The ALP emissivity is made available in a public repository as a function of the ALP mass, the temperature, and the electron chemical potential of the plasma. Finally, we examine the impact of ALP production and subsequent decays on astrophysical observables, deriving the leading bounds on ALPs coupling to electrons. At small couplings, the dominant constraints come from the previously neglected decay $a\to e^+ e^-\gamma$, except for a region of fireball formation where SN~1987A X-ray observations offer the best probe. At large couplings, bounds are dominated by the energy deposition argument, with a recently developed new prescription for the trapping regime. </p>
<blockquote>
<p>æˆ‘ä»¬é‡æ–°å®¡è§†åœ¨ç›¸å¯¹è®ºæ€§ç­‰ç¦»å­ä½“ä¸­ä»¥æ ‘çŠ¶å±‚çº§ä¸ç”µå­è€¦åˆäº§ç”Ÿçš„è½´çªå­æ ·ç²’å­ï¼ˆALPsï¼‰ã€‚æˆ‘ä»¬æ˜ç¡®å±•ç¤ºäº†ä¼ªæ ‡é‡å’Œå¯¼æ•°è€¦åˆä¹‹é—´çš„ç­‰ä»·æ€§ï¼Œé¦–æ¬¡è€ƒè™‘äº†ä¹‹å‰è¢«å¿½ç•¥çš„è¿‡ç¨‹ï¼Œå³åŠåº·æ™®é¡¿äº§ç”Ÿï¼ˆÎ³e^-â†’ae^-ï¼‰å’Œé…å¯¹æ¶ˆç­ï¼ˆe^+e^-â†’aÎ³ï¼‰ï¼Œå¹¶æ¨å¯¼å‡ºåˆ¶åŠ¨è¾å°„ï¼ˆe^-Nâ†’e^-Naï¼‰äº§ç”Ÿç‡çš„åˆ†æè¡¨è¾¾å¼ï¼Œä»¥ä¾¿æ›´é«˜æ•ˆåœ°è¯„ä¼°ALPæµé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ç”±äºç”µå­çƒ­è´¨é‡æ ¡æ­£ã€ç”µå­-ç”µå­åº“ä»‘ç›¸äº’ä½œç”¨ä»¥åŠæœ—é“-åºå¾·ç‘-ç±³æ ¼å°”æ•ˆåº”è€Œäº§ç”Ÿçš„ç”Ÿäº§ç‡ä¸ç¡®å®šæ€§ã€‚ALPçš„å‘å°„ç‡ä½œä¸ºALPè´¨é‡ã€æ¸©åº¦å’Œç­‰ç¦»å­ä½“ç”µå­åŒ–å­¦åŠ¿çš„å‡½æ•°å­˜å‚¨åœ¨å…¬å…±å­˜å‚¨åº“ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†ALPäº§ç”ŸåŠå…¶éšåçš„è¡°å˜å¯¹å¤©æ–‡è§‚æµ‹çš„å½±å“ï¼Œæ¨å¯¼å‡ºä¸ç”µå­è€¦åˆçš„ALPçš„ä¸»è¦ç•Œé™ã€‚åœ¨å°è€¦åˆçš„æƒ…å†µä¸‹ï¼Œä¸»è¦çš„çº¦æŸæ¥è‡ªäºä¹‹å‰è¢«å¿½ç•¥çš„aâ†’e^+e^-Î³è¡°å˜ï¼Œä½†åœ¨ç«çƒå½¢æˆçš„åŒºåŸŸä¸­ï¼ŒSN 1987A Xå°„çº¿è§‚æµ‹æä¾›äº†æœ€ä½³çš„æ¢æµ‹æ‰‹æ®µã€‚åœ¨å¤§è€¦åˆçš„æƒ…å†µä¸‹ï¼Œè¾¹ç•Œä¸»è¦å—èƒ½é‡æ²‰ç§¯è®ºæ®çš„æ”¯é…ï¼Œè€Œé’ˆå¯¹æ•è·çŠ¶æ€æœ‰ä¸€å¥—æ–°åˆ¶å®šçš„è§„åˆ™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15630v1">PDF</a> 36 pages, including 8 figures and 4 appendices</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡é‡æ–°æ¢è®¨äº†è½´çªç²’å­ï¼ˆALPsï¼‰ä¸ç”µå­åœ¨ç›¸å¯¹è®ºç­‰ç¦»å­ä½“ä¸­çš„è€¦åˆç”Ÿäº§é—®é¢˜ã€‚æ–‡ç« å±•ç¤ºäº†ä¼ªæ ‡é‡å’Œå¯¼æ•°è€¦åˆçš„ç­‰ä»·æ€§ï¼Œé¦–æ¬¡è€ƒè™‘äº†ä¹‹å‰è¢«å¿½ç•¥çš„è¿‡ç¨‹ï¼Œå¦‚åŠåº·æ™®é¡¿ç”Ÿäº§å’Œé…å¯¹æ¶ˆç­ï¼Œå¹¶æ¨å¯¼äº†è¾å°„å±‚ç”Ÿäº§çš„è§£æè¡¨è¾¾å¼ï¼Œæé«˜äº†ALPæµé‡çš„è®¡ç®—æ•ˆç‡ã€‚æ–‡ç« è¿˜è¯„ä¼°äº†ç”Ÿäº§é€Ÿç‡çš„ä¸ç¡®å®šæ€§ï¼Œæ¥æºäºç”µå­çƒ­è´¨é‡ä¿®æ­£ã€ç”µå­-ç”µå­åº“ä»‘ç›¸äº’ä½œç”¨ä»¥åŠæœ—é“-æ³¢é»˜å…°å…‹-ç±³æ ¼å°”æ•ˆåº”ã€‚ALPçš„å‘å°„ç‡ä½œä¸ºç²’å­è´¨é‡ã€æ¸©åº¦å’Œç”µå­åŒ–å­¦åŠ¿çš„å‡½æ•°è¢«å…¬å¼€å­˜å‚¨ã€‚æœ€åï¼Œæ–‡ç« æ¢è®¨äº†ALPç”Ÿäº§å’Œéšåçš„è¡°å˜å¯¹å¤©æ–‡è§‚æµ‹çš„å½±å“ï¼Œå¹¶å¾—å‡ºäº†ç”µå­è€¦åˆALPsçš„ä¸»å¯¼çº¦æŸã€‚å°è€¦åˆæƒ…å†µä¸‹ï¼Œä¸»è¦çš„é™åˆ¶æ¥æºäºå…ˆå‰è¢«å¿½ç•¥çš„è¡°å˜aâ†’e+eâˆ’Î³ï¼Œä½†å¯¹äºç«çƒå½¢æˆåŒºåŸŸï¼ŒSN 1987A Xå°„çº¿è§‚æµ‹æä¾›äº†æœ€ä½³æ¢æµ‹ã€‚å¤§è€¦åˆæƒ…å†µä¸‹ï¼Œé™åˆ¶ä¸»è¦æ¥è‡ªäºèƒ½é‡æ²‰ç§¯è§‚ç‚¹ï¼Œå¹¶ä¸”æœ€è¿‘å¼€å‘çš„æ–°å¤„æ–¹å¯¹äºæ•è·çŠ¶æ€åŒæ ·é€‚ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« é‡æ–°æ¢è®¨äº†è½´çªç²’å­ï¼ˆALPsï¼‰ä¸ç”µå­åœ¨ç›¸å¯¹è®ºç­‰ç¦»å­ä½“ä¸­çš„è€¦åˆç”Ÿäº§é—®é¢˜ï¼Œå¹¶å±•ç¤ºäº†ä¼ªæ ‡é‡å’Œå¯¼æ•°è€¦åˆçš„ç­‰ä»·æ€§ã€‚</li>
<li>æ–‡ç« è€ƒè™‘äº†ä¹‹å‰è¢«å¿½ç•¥çš„è¿‡ç¨‹ï¼ŒåŒ…æ‹¬åŠåº·æ™®é¡¿ç”Ÿäº§å’Œé…å¯¹æ¶ˆç­ï¼Œå¹¶æ¨å¯¼äº†è¾å°„å±‚ç”Ÿäº§çš„è§£æè¡¨è¾¾å¼ã€‚</li>
<li>æ–‡ç« è¯„ä¼°äº†ç”Ÿäº§é€Ÿç‡çš„ä¸ç¡®å®šæ€§æ¥æºï¼ŒåŒ…æ‹¬ç”µå­çƒ­è´¨é‡ä¿®æ­£ã€ç”µå­-ç”µå­åº“ä»‘ç›¸äº’ä½œç”¨ä»¥åŠæœ—é“-æ³¢é»˜å…°å…‹-ç±³æ ¼å°”æ•ˆåº”ã€‚</li>
<li>æ–‡ç« æä¾›äº†ALPçš„å‘å°„ç‡ä½œä¸ºç²’å­è´¨é‡ã€æ¸©åº¦å’Œç”µå­åŒ–å­¦åŠ¿çš„å‡½æ•°ï¼Œå¹¶å°†å…¶å…¬å¼€å­˜å‚¨ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†ALPç”Ÿäº§å’Œéšåçš„è¡°å˜å¯¹å¤©æ–‡è§‚æµ‹çš„å½±å“ï¼Œå‘ç°å°è€¦åˆå’Œå¤§è€¦åˆæƒ…å†µä¸‹æœ‰ä¸åŒçš„ä¸»å¯¼çº¦æŸã€‚</li>
<li>å¯¹äºå°è€¦åˆæƒ…å†µï¼Œå…ˆå‰è¢«å¿½ç•¥çš„è¡°å˜è¿‡ç¨‹aâ†’e+eâˆ’Î³æˆä¸ºä¸»è¦çš„é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨ç«çƒå½¢æˆåŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-946a1863a94dd81ad3046c1ea9552253.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93d5fa12daa19fe1bd1e96c186f60ac4.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models"><a href="#Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models" class="headerlink" title="Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models"></a>Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models</h2><p><strong>Authors:Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang</strong></p>
<p>Vision-language models (VLMs) have advanced reasoning in natural scenes, but their role in medical imaging remains underexplored. Medical reasoning tasks demand robust image analysis and well-justified answers, posing challenges due to the complexity of medical images. Transparency and trustworthiness are essential for clinical adoption and regulatory compliance. We introduce Med-R1, a framework exploring reinforcement learning (RL) to enhance VLMsâ€™ generalizability and trustworthiness in medical reasoning. Leveraging the DeepSeek strategy, we employ Group Relative Policy Optimization (GRPO) to guide reasoning paths via reward signals. Unlike supervised fine-tuning (SFT), which often overfits and lacks generalization, RL fosters robust and diverse reasoning. Med-R1 is evaluated across eight medical imaging modalities: CT, MRI, Ultrasound, Dermoscopy, Fundus Photography, Optical Coherence Tomography (OCT), Microscopy, and X-ray Imaging. Compared to its base model, Qwen2-VL-2B, Med-R1 achieves a 29.94% accuracy improvement and outperforms Qwen2-VL-72B, which has 36 times more parameters. Testing across five question types-modality recognition, anatomy identification, disease diagnosis, lesion grading, and biological attribute analysis Med-R1 demonstrates superior generalization, exceeding Qwen2-VL-2B by 32.06% and surpassing Qwen2-VL-72B in question-type generalization. These findings show that RL improves medical reasoning and enables parameter-efficient models to outperform significantly larger ones. With interpretable reasoning outputs, Med-R1 represents a promising step toward generalizable, trustworthy, and clinically viable medical VLMs. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›å·²ç»å¾—åˆ°äº†æå‡ï¼Œä½†å®ƒä»¬åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿã€‚åŒ»å­¦æ¨ç†ä»»åŠ¡éœ€è¦å¯é çš„å›¾åƒåˆ†æå’Œæœ‰å……åˆ†ç†ç”±çš„ç­”æ¡ˆï¼Œç”±äºåŒ»å­¦å›¾åƒçš„å¤æ‚æ€§ï¼Œè¿™æ„æˆäº†æŒ‘æˆ˜ã€‚é€æ˜åº¦å’Œå¯ä¿¡åº¦å¯¹äºä¸´åºŠé‡‡ç”¨å’Œæ³•è§„åˆè§„è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†Med-R1æ¡†æ¶ï¼Œæ¢ç´¢å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æé«˜VLMsåœ¨åŒ»å­¦æ¨ç†ä¸­çš„é€šç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚åˆ©ç”¨DeepSeekç­–ç•¥ï¼Œæˆ‘ä»¬é‡‡ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰é€šè¿‡å¥–åŠ±ä¿¡å·æ¥å¼•å¯¼æ¨ç†è·¯å¾„ã€‚ä¸ç»å¸¸è¿‡åº¦æ‹Ÿåˆä¸”ç¼ºä¹æ³›åŒ–èƒ½åŠ›çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸åŒï¼ŒRLä¿ƒè¿›äº†ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚Med-R1åœ¨å…«ç§åŒ»å­¦æˆåƒæ¨¡å¼ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šCTã€MRIã€è¶…å£°ã€çš®è‚¤é•œæ£€æŸ¥ã€çœ¼åº•æ‘„å½±ã€å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰ã€æ˜¾å¾®é•œå’ŒXå°„çº¿æˆåƒã€‚ä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼ŒMed-R1å®ç°äº†29.94%çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶è¶…è¶Šäº†å‚æ•°æ›´å¤šçš„Qwen2-VL-72Bã€‚åœ¨äº”ç§é—®é¢˜ç±»å‹ï¼ˆæ¨¡æ€è¯†åˆ«ã€è§£å‰–ç»“æ„è¯†åˆ«ã€ç–¾ç—…è¯Šæ–­ã€ç—…å˜åˆ†çº§å’Œç”Ÿç‰©å±æ€§åˆ†æï¼‰çš„æµ‹è¯•ä¸Šï¼ŒMed-R1å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œç›¸è¾ƒäºQwen2-VL-2Bæé«˜32.06%ï¼Œå¹¶åœ¨é—®é¢˜ç±»å‹æ³›åŒ–ä¸Šè¶…è¶Šäº†Qwen2-VL-72Bã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒRLèƒ½å¤Ÿæ”¹å–„åŒ»å­¦æ¨ç†ï¼Œå¹¶ä½¿å¾—å‚æ•°æ•ˆç‡é«˜çš„æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šè§„æ¨¡æ›´å¤§çš„æ¨¡å‹ã€‚Med-R1çš„å¯è§£é‡Šæ¨ç†è¾“å‡ºä»£è¡¨äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ­¥éª¤ï¼Œæœç€é€šç”¨ã€å¯ä¿¡å’Œä¸´åºŠä¸Šå¯è¡Œçš„åŒ»å­¦VLMså‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13939v2">PDF</a> </p>
<p><strong>Summary</strong><br>    é€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæå‡ºMed-R1æ¡†æ¶ï¼Œæé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—æ¨ç†ä¸­çš„é€šç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚Med-R1é‡‡ç”¨DeepSeekç­–ç•¥ï¼Œåˆ©ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é€šè¿‡å¥–åŠ±ä¿¡å·å¼•å¯¼æ¨ç†è·¯å¾„ã€‚ä¸ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç›¸æ¯”ï¼ŒRLä¿ƒè¿›ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚åœ¨å…«ç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šè¯„ä¼°Med-R1ï¼Œç›¸æ¯”åŸºå‡†æ¨¡å‹ï¼Œå…¶å‡†ç¡®åº¦æé«˜äº†29.94%ï¼Œå¹¶åœ¨å‚æ•°è¾ƒå°‘çš„æ¨¡å‹ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚Med-R1å±•ç°å‡ºå“è¶Šçš„é—®é¢˜ç±»å‹æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¨¡æ€è¯†åˆ«ã€è§£å‰–ç»“æ„è¯†åˆ«ã€ç–¾ç—…è¯Šæ–­ã€ç—…å˜åˆ†çº§å’Œç”Ÿç‰©å±æ€§åˆ†æç­‰æ–¹é¢ã€‚è¿™è¡¨æ˜å¼ºåŒ–å­¦ä¹ æœ‰åŠ©äºæ”¹è¿›åŒ»ç–—æ¨ç†ï¼Œä½¿å‚æ•°æ•ˆç‡æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šæ›´å¤§çš„æ¨¡å‹ã€‚Med-R1çš„å¯è§£é‡Šæ€§æ¨ç†è¾“å‡ºæ˜¯æœç€é€šç”¨ã€å¯ä¿¡å’Œä¸´åºŠå¯è¡Œçš„åŒ»ç–—VLMsè¿ˆå‡ºçš„æœ‰å¸Œæœ›çš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-R1æ¡†æ¶æ¢ç´¢å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åŒ»ç–—æ¨ç†ä¸­çš„åº”ç”¨ï¼Œä»¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é€šç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚</li>
<li>é‡‡ç”¨DeepSeekç­–ç•¥å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥å¼•å¯¼æ¨ç†è·¯å¾„ã€‚</li>
<li>ä¸ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç›¸æ¯”ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¿ƒè¿›æ›´ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚</li>
<li>Med-R1åœ¨å…«ç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>Med-R1å±•ç°å‡ºä¼˜ç§€çš„é—®é¢˜ç±»å‹æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¨¡æ€è¯†åˆ«ã€è§£å‰–ç»“æ„è¯†åˆ«ç­‰äº”ä¸ªæ–¹é¢çš„é—®é¢˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æœ‰åŠ©äºæ”¹è¿›åŒ»ç–—æ¨ç†ï¼Œå‚æ•°æ•ˆç‡æ¨¡å‹å¯è¶…è¶Šæ›´å¤§æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70c488823f69b8251d63b363931d8743.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e6d0d3225870b6fb40060a3faa48986.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-515ee458fedd469593906c9c0cc6d186.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9703a323e756c6a53f917fea46c55b3d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CLIMB-Data-Foundations-for-Large-Scale-Multimodal-Clinical-Foundation-Models"><a href="#CLIMB-Data-Foundations-for-Large-Scale-Multimodal-Clinical-Foundation-Models" class="headerlink" title="CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation   Models"></a>CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation   Models</h2><p><strong>Authors:Wei Dai, Peilin Chen, Malinda Lu, Daniel Li, Haowen Wei, Hejie Cui, Paul Pu Liang</strong></p>
<p>Recent advances in clinical AI have enabled remarkable progress across many clinical domains. However, existing benchmarks and models are primarily limited to a small set of modalities and tasks, which hinders the development of large-scale multimodal methods that can make holistic assessments of patient health and well-being. To bridge this gap, we introduce Clinical Large-Scale Integrative Multimodal Benchmark (CLIMB), a comprehensive clinical benchmark unifying diverse clinical data across imaging, language, temporal, and graph modalities. CLIMB comprises 4.51 million patient samples totaling 19.01 terabytes distributed across 2D imaging, 3D video, time series, graphs, and multimodal data. Through extensive empirical evaluation, we demonstrate that multitask pretraining significantly improves performance on understudied domains, achieving up to 29% improvement in ultrasound and 23% in ECG analysis over single-task learning. Pretraining on CLIMB also effectively improves modelsâ€™ generalization capability to new tasks, and strong unimodal encoder performance translates well to multimodal performance when paired with task-appropriate fusion strategies. Our findings provide a foundation for new architecture designs and pretraining strategies to advance clinical AI research. Code is released at <a target="_blank" rel="noopener" href="https://github.com/DDVD233/climb">https://github.com/DDVD233/climb</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä¸´åºŠäººå·¥æ™ºèƒ½çš„è¿›æ­¥åœ¨è®¸å¤šä¸´åºŠé¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ä¸»è¦å±€é™äºå°‘æ•°å‡ ç§æ¨¡æ€å’Œä»»åŠ¡ï¼Œè¿™é˜»ç¢äº†èƒ½å¤Ÿå…¨é¢è¯„ä¼°æ‚£è€…å¥åº·å’Œç¦ç¥‰çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ–¹æ³•çš„å‘å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸´åºŠå¤§è§„æ¨¡ç»¼åˆå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼ˆCLIMBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ä¸´åºŠåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†æˆåƒã€è¯­è¨€ã€æ—¶é—´å’Œå›¾å½¢ç­‰å¤šç§ä¸´åºŠæ•°æ®æ¨¡æ€ã€‚CLIMBåŒ…å«451ä¸‡æ‚£è€…æ ·æœ¬ï¼Œæ€»è®¡19.01ä¸‡äº¿å­—èŠ‚çš„æ•°æ®ï¼Œåˆ†å¸ƒåœ¨äºŒç»´æˆåƒã€ä¸‰ç»´è§†é¢‘ã€æ—¶é—´åºåˆ—ã€å›¾å½¢å’Œå¤šæ¨¡æ€æ•°æ®ä¸­ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜å¤šä»»åŠ¡é¢„è®­ç»ƒåœ¨è¾ƒå°‘ç ”ç©¶çš„é¢†åŸŸå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œåœ¨è¶…å£°å’Œå¿ƒç”µå›¾åˆ†æä¸­åˆ†åˆ«å®ç°äº†é«˜è¾¾29%å’Œ23%çš„æ”¹è¿›ï¼Œè¶…è¿‡å•ä»»åŠ¡å­¦ä¹ ã€‚åœ¨CLIMBä¸Šè¿›è¡Œé¢„è®­ç»ƒè¿˜å¯ä»¥æœ‰æ•ˆæé«˜æ¨¡å‹å¯¹æ–°ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¼ºå¤§çš„å•æ¨¡æ€ç¼–ç å™¨æ€§èƒ½åœ¨ä¸ä»»åŠ¡é€‚å½“çš„èåˆç­–ç•¥é…å¯¹æ—¶ï¼Œå¯ä»¥å¾ˆå¥½åœ°è½¬åŒ–ä¸ºå¤šæ¨¡æ€æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ–°çš„æ¶æ„è®¾è®¡å’Œé¢„è®­ç»ƒç­–ç•¥æä¾›äº†åŸºç¡€ï¼Œä»¥æ¨åŠ¨ä¸´åºŠäººå·¥æ™ºèƒ½ç ”ç©¶çš„å‘å±•ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/DDVD233/climb%E3%80%82">https://github.com/DDVD233/climbã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07667v2">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¸´åºŠäººå·¥æ™ºèƒ½çš„æœ€æ–°è¿›å±•å·²åœ¨å¤šä¸ªä¸´åºŠé¢†åŸŸå–å¾—æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ä¸»è¦å±€é™äºå°‘æ•°å‡ ç§æ¨¡æ€å’Œä»»åŠ¡ï¼Œè¿™é˜»ç¢äº†èƒ½å¤Ÿè¿›è¡Œæ‚£è€…å¥åº·ä¸ç¦ç¥‰å…¨é¢è¯„ä¼°çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ–¹æ³•çš„å‘å±•ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸´åºŠå¤§è§„æ¨¡ç»¼åˆå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼ˆCLIMBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€äº†æˆåƒã€è¯­è¨€ã€æ—¶é—´å’Œå›¾å½¢ç­‰å¤šç§ä¸´åºŠæ•°æ®çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚CLIMBåŒ…å«451ä¸‡æ‚£è€…æ ·æœ¬ï¼Œæ€»è®¡19.01ä¸‡äº¿å­—èŠ‚çš„æ•°æ®ï¼Œæ¶µç›–äº†2Dæˆåƒã€3Dè§†é¢‘ã€æ—¶é—´åºåˆ—ã€å›¾å½¢å’Œå¤šæ¨¡æ€æ•°æ®ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†å¤šä»»åŠ¡é¢„è®­ç»ƒèƒ½æ˜¾è‘—æ”¹å–„ä»¥å¾€ç ”ç©¶è¾ƒå°‘çš„é¢†åŸŸçš„æ€§èƒ½ï¼Œåœ¨è¶…å£°å’Œå¿ƒç”µå›¾åˆ†æä¸­åˆ†åˆ«å®ç°äº†é«˜è¾¾29%å’Œ23%çš„æ”¹è¿›ã€‚åœ¨CLIMBä¸Šè¿›è¡Œé¢„è®­ç»ƒè¿˜èƒ½æœ‰æ•ˆæé«˜æ¨¡å‹å¯¹æ–°ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¼ºå¤§çš„å•æ¨¡æ€ç¼–ç å™¨æ€§èƒ½é…åˆé€‚å½“çš„èåˆç­–ç•¥åœ¨å¤šæ¨¡æ€æ€§èƒ½ä¸Šä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºä¸´åºŠäººå·¥æ™ºèƒ½ç ”ç©¶çš„æ–°æ¶æ„è®¾è®¡å’Œé¢„è®­ç»ƒç­–ç•¥æä¾›äº†åŸºç¡€ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒäº<a target="_blank" rel="noopener" href="https://github.com/DDVD233/climb%E3%80%82">https://github.com/DDVD233/climbã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹å±€é™äºç‰¹å®šæ¨¡æ€å’Œä»»åŠ¡ï¼Œé˜»ç¢å¤§è§„æ¨¡å¤šæ¨¡æ€æ–¹æ³•çš„å‘å±•ã€‚</li>
<li>æ¨å‡ºCLIMBç»¼åˆä¸´åºŠåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šç§ä¸´åºŠæ•°æ®æ¨¡æ€ï¼ŒåŒ…æ‹¬æˆåƒã€è¯­è¨€ã€æ—¶é—´å’Œå›¾å½¢ç­‰ã€‚</li>
<li>CLIMBåŒ…å«å¤§é‡æ‚£è€…æ ·æœ¬å’Œå¹¿æ³›çš„æ•°æ®é‡ï¼Œä¸ºä¸´åºŠAIç ”ç©¶æä¾›ä¸°å¯Œèµ„æºã€‚</li>
<li>å¤šä»»åŠ¡é¢„è®­ç»ƒèƒ½æœ‰æ•ˆæé«˜åœ¨ä»¥å¾€ç ”ç©¶è¾ƒå°‘çš„é¢†åŸŸçš„æ€§èƒ½ï¼Œå¦‚è¶…å£°å’Œå¿ƒç”µå›¾åˆ†æã€‚</li>
<li>åœ¨CLIMBä¸Šè¿›è¡Œé¢„è®­ç»ƒæœ‰åŠ©äºæé«˜æ¨¡å‹å¯¹æ–°ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼ºå¤§çš„å•æ¨¡æ€ç¼–ç å™¨æ€§èƒ½é…åˆé€‚å½“çš„èåˆç­–ç•¥åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-efa0e369ab92911ebd627abb0ba03cd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-012f055d86cb307f988b4fd64f30db77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c0d70f811bd87001d519c37caa20a5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf95cb57bafbcb09567f4babb446322.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-782322a10abeaca2443c7253de268580.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-245d0576b27e09effad9d07e813725cb.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Retrospective-Systematic-Study-on-Hierarchical-Sparse-Query-Transformer-assisted-Ultrasound-Screening-for-Early-Hepatocellular-Carcinoma"><a href="#A-Retrospective-Systematic-Study-on-Hierarchical-Sparse-Query-Transformer-assisted-Ultrasound-Screening-for-Early-Hepatocellular-Carcinoma" class="headerlink" title="A Retrospective Systematic Study on Hierarchical Sparse Query   Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma"></a>A Retrospective Systematic Study on Hierarchical Sparse Query   Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma</h2><p><strong>Authors:Chaoyin She, Ruifang Lu, Danni He, Jiayi Lv, Yadan Lin, Meiqing Cheng, Hui Huang, Fengyu Ye, Lida Chen, Wei Wang, Qinghua Huang</strong></p>
<p>Hepatocellular carcinoma (HCC), ranking as the third leading cause of cancer-related mortality worldwide, demands urgent improvements in early detection to enhance patient survival. While ultrasound remains the preferred screening modality due to its cost-effectiveness and real-time capabilities, its sensitivity (59%-78%) heavily relies on radiologistsâ€™ expertise, leading to inconsistent diagnostic outcomes and operational inefficiencies. Recent advancements in AI technology offer promising solutions to bridge this gap. This study introduces the Hierarchical Sparse Query Transformer (HSQformer), a novel hybrid architecture that synergizes CNNsâ€™ local feature extraction with Vision Transformersâ€™ global contextual awareness through latent space representation and sparse learning. By dynamically activating task-specific experts via a Mixture-of-Experts (MoE) framework, HSQformer achieves hierarchical feature integration without structural redundancy. Evaluated across three clinical scenarios: single-center, multi-center, and high-risk patient cohorts, HSQformer outperforms state-of-the-art models (e.g., 95.38% AUC in multi-center testing) and matches senior radiologistsâ€™ diagnostic accuracy while significantly surpassing junior counterparts. These results highlight the potential of AI-assisted tools to standardize HCC screening, reduce dependency on human expertise, and improve early diagnosis rates. The full code is available at <a target="_blank" rel="noopener" href="https://github.com/Asunatan/HSQformer">https://github.com/Asunatan/HSQformer</a>. </p>
<blockquote>
<p>è‚ç»†èƒç™Œï¼ˆHCCï¼‰ä½œä¸ºå…¨çƒç¬¬ä¸‰å¤§ç™Œç—‡è‡´æ­»åŸå› ï¼Œæ€¥éœ€æ”¹è¿›æ—©æœŸæ£€æµ‹æ–¹æ³•ä»¥æé«˜æ‚£è€…å­˜æ´»ç‡ã€‚è™½ç„¶è¶…å£°å› å…¶æˆæœ¬æ•ˆç›Šå’Œå®æ—¶èƒ½åŠ›ä»æ˜¯é¦–é€‰ç­›æŸ¥æ–¹å¼ï¼Œä½†å…¶æ•æ„Ÿæ€§ï¼ˆ59%-78%ï¼‰ä¸¥é‡ä¾èµ–äºæ”¾å°„ç§‘ä¸“å®¶çš„ç»éªŒï¼Œå¯¼è‡´è¯Šæ–­ç»“æœä¸ä¸€è‡´å’Œè¿è¥æ•ˆç‡ä½ä¸‹ã€‚æœ€è¿‘äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›å±•ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶ä»‹ç»äº†åˆ†å±‚ç¨€ç–æŸ¥è¯¢è½¬æ¢å™¨ï¼ˆHSQformerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ··åˆæ¶æ„ï¼Œå®ƒé€šè¿‡æ½œåœ¨ç©ºé—´è¡¨ç¤ºå’Œç¨€ç–å­¦ä¹ ï¼ŒååŒCNNçš„å±€éƒ¨ç‰¹å¾æå–å’Œè§†è§‰å˜å‹å™¨çš„å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€‚HSQformeré€šè¿‡ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¡†æ¶åŠ¨æ€æ¿€æ´»ä»»åŠ¡ç‰¹å®šä¸“å®¶ï¼Œå®ç°åˆ†å±‚ç‰¹å¾é›†æˆï¼Œæ— ç»“æ„å†—ä½™ã€‚åœ¨ä¸‰ç§ä¸´åºŠæƒ…æ™¯ï¼šå•ä¸­å¿ƒã€å¤šä¸­å¿ƒå’Œé«˜å±æ‚£è€…ç¾¤ä½“ä¸­è¿›è¡Œè¯„ä¼°ï¼ŒHSQformerçš„æ€§èƒ½è¶…è¿‡äº†æœ€å…ˆè¿›æ¨¡å‹ï¼ˆä¾‹å¦‚åœ¨å¤šä¸­å¿ƒæµ‹è¯•ä¸­çš„95.38% AUCï¼‰ï¼Œå¹¶è¾¾åˆ°äº†èµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¯Šæ–­ç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—è¶…è¶Šäº†åˆçº§åŒ»ç”Ÿçš„æ°´å¹³ã€‚è¿™äº›ç»“æœçªå‡ºäº†äººå·¥æ™ºèƒ½è¾…åŠ©å·¥å…·åœ¨æ ‡å‡†åŒ–è‚ç»†èƒç™Œç­›æŸ¥ã€å‡å°‘å¯¹äººç±»ä¸“å®¶çš„ä¾èµ–ä»¥åŠæé«˜æ—©æœŸè¯Šæ–­ç‡æ–¹é¢çš„æ½œåŠ›ã€‚å®Œæ•´ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Asunatan/HSQformer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Asunatan/HSQformeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03772v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‚ç»†èƒç™Œï¼ˆHCCï¼‰çš„æ—©æœŸæ£€æµ‹ç°çŠ¶å’ŒæŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨æå‡è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆæ¶æ„HSQformerï¼Œå®ƒé€šè¿‡ç»“åˆCNNsçš„å±€éƒ¨ç‰¹å¾æå–å’ŒVision Transformersçš„å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œå®ç°äº†å±‚æ¬¡åŒ–çš„ç‰¹å¾èåˆã€‚HSQformeråœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œç”šè‡³è¾¾åˆ°èµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¯Šæ–­æ°´å¹³ï¼Œæœ‰æœ›æ ‡å‡†åŒ–HCCç­›æŸ¥ï¼Œå‡å°‘å¯¹äººåŠ›çš„ä¾èµ–ï¼Œæé«˜æ—©æœŸè¯Šæ–­ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ç»†èƒç™Œï¼ˆHCCï¼‰æ˜¯å…¨çƒç¬¬ä¸‰å¤§è‡´ç™Œæ­»å› ï¼Œæ€¥éœ€æ”¹è¿›æ—©æœŸæ£€æµ‹æŠ€æœ¯ä»¥æé«˜æ‚£è€…å­˜æ´»ç‡ã€‚</li>
<li>è¶…å£°æ³¢æ˜¯ç›®å‰é¦–é€‰çš„ç­›æŸ¥æ–¹å¼ï¼Œä½†å…¶æ•æ„Ÿæ€§ä¾èµ–äºæ”¾å°„ç§‘ä¸“å®¶çš„ç»éªŒï¼Œå¯¼è‡´è¯Šæ–­ç»“æœçš„ä¸ä¸€è‡´å’Œè¿è¥æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>HSQformeræ˜¯ä¸€ç§æ–°å‹æ··åˆæ¶æ„ï¼Œç»“åˆäº†CNNå’ŒVision Transformerçš„ä¼˜ç‚¹ï¼Œå®ç°å±‚æ¬¡åŒ–çš„ç‰¹å¾æå–å’Œèåˆã€‚</li>
<li>HSQformeråœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œä¸”ä¸èµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹æ›´ä½³ã€‚</li>
<li>HSQformeræœ‰æœ›æ ‡å‡†åŒ–HCCç­›æŸ¥ï¼Œå‡å°‘å¯¹äººåŠ›çš„ä¾èµ–ï¼Œæé«˜æ—©æœŸè¯Šæ–­ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc1cc280bc455f957a8d8a4e923af440.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b656808b9fdb3b10b8e0c4cf384cede.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae724697cac6e7ded595910604974a41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c25670817262634c2e50e7ee0eef7d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-052bf32bd566a98358e9175adbd6bcef.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Repurposing-Stable-Diffusion-Attention-for-Training-Free-Unsupervised-Interactive-Segmentation"><a href="#Repurposing-Stable-Diffusion-Attention-for-Training-Free-Unsupervised-Interactive-Segmentation" class="headerlink" title="Repurposing Stable Diffusion Attention for Training-Free Unsupervised   Interactive Segmentation"></a>Repurposing Stable Diffusion Attention for Training-Free Unsupervised   Interactive Segmentation</h2><p><strong>Authors:Markus Karmann, Onay Urfalioglu</strong></p>
<p>Recent progress in interactive point prompt based Image Segmentation allows to significantly reduce the manual effort to obtain high quality semantic labels. State-of-the-art unsupervised methods use self-supervised pre-trained models to obtain pseudo-labels which are used in training a prompt-based segmentation model. In this paper, we propose a novel unsupervised and training-free approach based solely on the self-attention of Stable Diffusion. We interpret the self-attention tensor as a Markov transition operator, which enables us to iteratively construct a Markov chain. Pixel-wise counting of the required number of iterations along the Markov chain to reach a relative probability threshold yields a Markov-iteration-map, which we simply call a Markov-map. Compared to the raw attention maps, we show that our proposed Markov-map has less noise, sharper semantic boundaries and more uniform values within semantically similar regions. We integrate the Markov-map in a simple yet effective truncated nearest neighbor framework to obtain interactive point prompt based segmentation. Despite being training-free, we experimentally show that our approach yields excellent results in terms of Number of Clicks (NoC), even outperforming state-of-the-art training based unsupervised methods in most of the datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mkarmann/m2n2">https://github.com/mkarmann/m2n2</a>. </p>
<blockquote>
<p>è¿‘æœŸåŸºäºäº¤äº’å¼ç‚¹æç¤ºçš„å›¾åƒåˆ†å‰²æŠ€æœ¯çš„è¿›å±•ï¼Œæ˜¾è‘—å‡å°‘äº†è·å¾—é«˜è´¨é‡è¯­ä¹‰æ ‡ç­¾æ‰€éœ€çš„äººå·¥åŠªåŠ›ã€‚æœ€å…ˆè¿›çš„æ— ç›‘ç£æ–¹æ³•ä½¿ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹æ¥è·å¾—ä¼ªæ ‡ç­¾ï¼Œè¿™äº›ä¼ªæ ‡ç­¾ç”¨äºè®­ç»ƒåŸºäºæç¤ºçš„åˆ†å‰²æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ— ç›‘ç£ä¸”æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…åŸºäºStable Diffusionçš„è‡ªæ³¨æ„åŠ›ã€‚æˆ‘ä»¬å°†è‡ªæ³¨æ„åŠ›å¼ é‡è§£é‡Šä¸ºé©¬å°”å¯å¤«è½¬ç§»ç®—å­ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ„å»ºé©¬å°”å¯å¤«é“¾ã€‚æ²¿ç€é©¬å°”å¯å¤«é“¾è¿›è¡Œæ‰€éœ€çš„è¿­ä»£æ¬¡æ•°çš„åƒç´ çº§è®¡æ•°ï¼Œä»¥è¾¾åˆ°ç›¸å¯¹æ¦‚ç‡é˜ˆå€¼ï¼Œä»è€Œäº§ç”Ÿé©¬å°”å¯å¤«è¿­ä»£å›¾ï¼Œæˆ‘ä»¬å°†å…¶ç®€ç§°ä¸ºé©¬å°”å¯å¤«å›¾ã€‚ä¸åŸå§‹æ³¨æ„åŠ›å›¾ç›¸æ¯”ï¼Œæˆ‘ä»¬æ˜¾ç¤ºæ‰€æå‡ºçš„é©¬å°”å¯å¤«å›¾å…·æœ‰è¾ƒå°‘çš„å™ªå£°ã€æ›´æ¸…æ™°çš„è¯­ä¹‰è¾¹ç•Œå’Œè¯­ä¹‰ç›¸ä¼¼åŒºåŸŸå†…æ›´å‡åŒ€çš„å€¼ã€‚æˆ‘ä»¬å°†é©¬å°”å¯å¤«å›¾é›†æˆåˆ°ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æˆªæ–­æœ€è¿‘é‚»æ¡†æ¶ä¸­ï¼Œä»¥è·å¾—åŸºäºäº¤äº’å¼ç‚¹çš„æç¤ºåˆ†å‰²ã€‚å°½ç®¡æ— éœ€è®­ç»ƒï¼Œä½†å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç‚¹å‡»æ¬¡æ•°ï¼ˆNoCï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šè¶…è¶Šäº†åŸºäºè®­ç»ƒçš„æ— ç›‘ç£æ–¹æ³•çš„æœ€æ–°æŠ€æœ¯ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mkarmann/m2n2%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mkarmann/m2n2å¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10411v2">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºäº¤äº’å¼ç‚¹æç¤ºçš„å›¾åƒåˆ†å‰²æ–°æ–¹æ³•ï¼Œåˆ©ç”¨Stable Diffusionçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ„å»ºMarkové“¾ï¼Œæå‡ºä¸€ç§æ— ç›‘ç£ä¸”æ— éœ€è®­ç»ƒçš„æ–°æ–¹æ³•ç”ŸæˆMarkovå›¾ï¼Œç”¨äºå›¾åƒåˆ†å‰²ã€‚è¯¥æ–¹æ³•åœ¨æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œå‡å°‘äº†æ‰‹åŠ¨æ“ä½œçš„å¤æ‚æ€§ï¼Œä¸”ç‚¹å‡»æ•°è¾ƒä½ï¼ˆNoCï¼‰ï¼Œç”šè‡³åœ¨å¤šæ•°æ•°æ®é›†ä¸Šè¶…è¿‡äº†åŸºäºè®­ç»ƒçš„æ— ç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–¹æ³•åŸºäºStable Diffusionçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>é€šè¿‡æ„å»ºMarkové“¾ï¼Œæå‡ºäº†åä¸ºMarkovå›¾çš„æ¨¡å‹ã€‚</li>
<li>Markovå›¾ç›¸æ¯”åŸå§‹æ³¨æ„åŠ›å›¾å…·æœ‰æ›´ä½çš„å™ªå£°å’Œæ›´æ¸…æ™°çš„è¯­ä¹‰è¾¹ç•Œã€‚</li>
<li>è¯¥æ–¹æ³•æ•´åˆäº†Markovå›¾åˆ°æˆªæ–­æœ€è¿‘é‚»æ¡†æ¶ä¸­ï¼Œå®ç°äº†åŸºäºäº¤äº’å¼ç‚¹æç¤ºçš„å›¾åƒåˆ†å‰²ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒå³å¯ç”Ÿæˆè‰¯å¥½çš„ç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ç‚¹å‡»æ•°ï¼ˆNoCï¼‰æ–¹é¢çš„è¡¨ç°ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6194648b82ec46c399c78ea059869d2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cc0c3452edf33c8be5d93680d0266f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5606b9e08fcc922dcfe9f4cb73b93f3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd4821f94007a812f0433db2dac30c0e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Label-efficient-multi-organ-segmentation-with-a-diffusion-model"><a href="#Label-efficient-multi-organ-segmentation-with-a-diffusion-model" class="headerlink" title="Label-efficient multi-organ segmentation with a diffusion model"></a>Label-efficient multi-organ segmentation with a diffusion model</h2><p><strong>Authors:Yongzhi Huang, Fengjun Xi, Liyun Tu, Jinxin Zhu, Haseeb Hassan, Liyilei Su, Yun Peng, Jingyu Li, Jun Ma, Bingding Huang</strong></p>
<p>Accurate segmentation of multiple organs in Computed Tomography (CT) images plays a vital role in computer-aided diagnosis systems. While various supervised learning approaches have been proposed recently, these methods heavily depend on a large amount of high-quality labeled data, which are expensive to obtain in practice. To address this challenge, we propose a label-efficient framework using knowledge transfer from a pre-trained diffusion model for CT multi-organ segmentation. Specifically, we first pre-train a denoising diffusion model on 207,029 unlabeled 2D CT slices to capture anatomical patterns. Then, the model backbone is transferred to the downstream multi-organ segmentation task, followed by fine-tuning with few labeled data. In fine-tuning, two fine-tuning strategies, linear classification and fine-tuning decoder, are employed to enhance segmentation performance while preserving learned representations. Quantitative results show that the pre-trained diffusion model is capable of generating diverse and realistic 256x256 CT images (Fr&#39;echet inception distance (FID): 11.32, spatial Fr&#39;echet inception distance (sFID): 46.93, F1-score: 73.1%). Compared to state-of-the-art methods for multi-organ segmentation, our method achieves competitive performance on the FLARE 2022 dataset, particularly in limited labeled data scenarios. After fine-tuning with 1% and 10% labeled data, our method achieves dice similarity coefficients (DSCs) of 71.56% and 78.51%, respectively. Remarkably, the method achieves a DSC score of 51.81% using only four labeled CT slices. These results demonstrate the efficacy of our approach in overcoming the limitations of supervised learning approaches that is highly dependent on large-scale labeled data. </p>
<blockquote>
<p>åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒä¸­ï¼Œå¤šå™¨å®˜ç²¾å‡†åˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å°½ç®¡æœ€è¿‘å·²ç»æå‡ºäº†å„ç§ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¤§é‡é«˜è´¨é‡æ ‡è®°æ•°æ®ï¼Œè€Œåœ¨å®è·µä¸­è¿™äº›æ•°æ®è·å–æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨çŸ¥è¯†è½¬ç§»çš„CTå¤šå™¨å®˜åˆ†å‰²æ ‡ç­¾æœ‰æ•ˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨207,029å¼ æ— æ ‡ç­¾çš„2D CTåˆ‡ç‰‡ä¸Šé¢„è®­ç»ƒä¸€ä¸ªå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œä»¥æ•æ‰è§£å‰–æ¨¡å¼ã€‚ç„¶åï¼Œå°†æ¨¡å‹ä¸»å¹²è½¬ç§»åˆ°ä¸‹æ¸¸å¤šå™¨å®˜åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œå¹¶ä½¿ç”¨å°‘é‡æ ‡è®°æ•°æ®è¿›è¡Œå¾®è°ƒã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ç§å¾®è°ƒç­–ç•¥ï¼Œå³çº¿æ€§åˆ†ç±»å’Œå¾®è°ƒè§£ç å™¨ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½åŒæ—¶ä¿ç•™å­¦ä¹ åˆ°çš„è¡¨ç¤ºã€‚å®šé‡ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é€¼çœŸçš„256x256 CTå›¾åƒï¼ˆFrÃ©chet inceptionè·ç¦»ï¼ˆFIDï¼‰ï¼š11.32ï¼Œç©ºé—´FrÃ©chet inceptionè·ç¦»ï¼ˆsFIDï¼‰ï¼š46.93ï¼ŒF1åˆ†æ•°ï¼š73.1%ï¼‰ã€‚ä¸å¤šå™¨å®˜åˆ†å‰²çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨FLARE 2022æ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ã€‚ä½¿ç”¨1%å’Œ10%çš„æ ‡è®°æ•°æ®è¿›è¡Œå¾®è°ƒåï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è¿ªå…‹ç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰åˆ†åˆ«ä¸º71.56%å’Œ78.51%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨å››ä¸ªæ ‡è®°çš„CTåˆ‡ç‰‡å°±è¾¾åˆ°äº†51.81%çš„DSCå¾—åˆ†ã€‚è¿™äº›ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…‹æœé«˜åº¦ä¾èµ–äºå¤§è§„æ¨¡æ ‡è®°æ•°æ®çš„ç›‘ç£å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.15216v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸï¼Œå¯¹è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒä¸­å¤šä¸ªå™¨å®˜çš„å‡†ç¡®åˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚é’ˆå¯¹ç›‘ç£å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡ŒCTå¤šå™¨å®˜åˆ†å‰²çš„æ ‡ç­¾æœ‰æ•ˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆåœ¨å¤§è§„æ¨¡æœªæ ‡è®°çš„CTåˆ‡ç‰‡ä¸Šé¢„è®­ç»ƒå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œç„¶åå°†å…¶åº”ç”¨äºä¸‹æ¸¸å¤šå™¨å®˜åˆ†å‰²ä»»åŠ¡ï¼Œå¹¶åœ¨å°‘é‡æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é€¼çœŸçš„CTå›¾åƒï¼Œå¹¶åœ¨æœ‰é™æ ‡è®°æ•°æ®åœºæ™¯ä¸‹å®ç°äº†ä¸æœ€æ–°å¤šå™¨å®˜åˆ†å‰²æ–¹æ³•ç›¸ç«äº‰çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­éœ€è¦å¤§é‡é«˜è´¨é‡æ ‡æ³¨æ•°æ®ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ ‡ç­¾æœ‰æ•ˆæ¡†æ¶ï¼Œç”¨äºCTå¤šå™¨å®˜åˆ†å‰²ã€‚</li>
<li>æ¡†æ¶é¦–å…ˆåœ¨å¤§é‡æœªæ ‡è®°çš„CTåˆ‡ç‰‡ä¸Šé¢„è®­ç»ƒå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ è§£å‰–æ¨¡å¼ã€‚</li>
<li>æ¡†æ¶éšåå°†é¢„è®­ç»ƒçš„æ¨¡å‹åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶åœ¨å°‘é‡æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚</li>
<li>é‡‡ç”¨äº†ä¸¤ç§å¾®è°ƒç­–ç•¥ï¼šçº¿æ€§åˆ†ç±»å’Œå¾®è°ƒè§£ç å™¨ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½å¹¶ä¿ç•™å­¦ä¹ åˆ°çš„è¡¨ç¤ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„CTå›¾åƒï¼Œå¹¶åœ¨FLARE 2022æ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.15216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d2cd6024660c3b22c96a5dcd28a10f8.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ae6447232f8dbbef2e71dab4cffebc4a.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  UniSync A Unified Framework for Audio-Visual Synchronization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a6be095d67f3ecb8599584ca442a882b.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  VerbDiff Text-Only Diffusion Models with Enhanced Interaction Awareness
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
