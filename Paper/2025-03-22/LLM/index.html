<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  The Emperor&#39;s New Clothes in Benchmarking? A Rigorous Examination of   Mitigation Strategies for LLM Benchmark Data Contamination">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-92904619907f3f442ef28194e265ec3f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-22-æ›´æ–°"><a href="#2025-03-22-æ›´æ–°" class="headerlink" title="2025-03-22 æ›´æ–°"></a>2025-03-22 æ›´æ–°</h1><h2 id="The-Emperorâ€™s-New-Clothes-in-Benchmarking-A-Rigorous-Examination-of-Mitigation-Strategies-for-LLM-Benchmark-Data-Contamination"><a href="#The-Emperorâ€™s-New-Clothes-in-Benchmarking-A-Rigorous-Examination-of-Mitigation-Strategies-for-LLM-Benchmark-Data-Contamination" class="headerlink" title="The Emperorâ€™s New Clothes in Benchmarking? A Rigorous Examination of   Mitigation Strategies for LLM Benchmark Data Contamination"></a>The Emperorâ€™s New Clothes in Benchmarking? A Rigorous Examination of   Mitigation Strategies for LLM Benchmark Data Contamination</h2><p><strong>Authors:Yifan Sun, Han Wang, Dongbai Li, Gang Wang, Huan Zhang</strong></p>
<p>Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples in the training set-has raised increasing concerns in Large Language Model (LLM) evaluation, leading to falsely inflated performance estimates and undermining evaluation reliability. To address this, researchers have proposed various mitigation strategies to update existing benchmarks, including modifying original questions or generating new ones based on them. However, a rigorous examination of the effectiveness of these mitigation strategies remains lacking. In this paper, we design a systematic and controlled pipeline along with two novel metrics-fidelity and contamination resistance-to provide a fine-grained and comprehensive assessment of existing BDC mitigation strategies. Previous assessment methods, such as accuracy drop and accuracy matching, focus solely on aggregate accuracy, often leading to incomplete or misleading conclusions. Our metrics address this limitation by emphasizing question-level evaluation result matching. Extensive experiments with 10 LLMs, 5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios reveal that no existing strategy significantly improves resistance over the vanilla case (i.e., no benchmark update) across all benchmarks, and none effectively balances fidelity and contamination resistance. These findings underscore the urgent need for designing more effective BDC mitigation strategies. Our code repository is available at <a target="_blank" rel="noopener" href="https://github.com/ASTRAL-Group/BDC_mitigation_assessment">https://github.com/ASTRAL-Group/BDC_mitigation_assessment</a>. </p>
<blockquote>
<p>åŸºå‡†æ•°æ®æ±¡æŸ“ï¼ˆBDCï¼‰â€”â€”å³å°†åŸºå‡†æµ‹è¯•æ ·æœ¬çº³å…¥è®­ç»ƒé›†â€”â€”åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°ä¸­å¼•å‘äº†è¶Šæ¥è¶Šå¤šçš„æ‹…å¿§ï¼Œå¯¼è‡´äº†æ€§èƒ½è¯„ä¼°ç»“æœè™šé«˜ï¼Œè¯„ä¼°å¯é æ€§é™ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å·²ç»æå‡ºäº†å„ç§ç¼“è§£ç­–ç•¥æ¥æ›´æ–°ç°æœ‰åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¿®æ”¹åŸå§‹é—®é¢˜æˆ–åŸºäºå®ƒä»¬ç”Ÿæˆæ–°çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå¯¹äºè¿™äº›ç¼“è§£ç­–ç•¥çš„æœ‰æ•ˆæ€§è¿›è¡Œä¸¥è°¨æ£€éªŒä»ç„¶ç¼ºä¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç³»ç»Ÿä¸”å—æ§çš„ç®¡é“ï¼Œå¹¶å¼•å…¥äº†ä¸¤ç§æ–°å‹æŒ‡æ ‡â€”â€”ä¿çœŸåº¦å’ŒæŠ—æ±¡æŸ“åº¦ï¼Œä»¥å¯¹ç°æœ‰çš„BDCç¼“è§£ç­–ç•¥è¿›è¡Œç²¾ç»†ä¸”å…¨é¢çš„è¯„ä¼°ã€‚ä¹‹å‰çš„è¯„ä¼°æ–¹æ³•ï¼Œå¦‚å‡†ç¡®åº¦ä¸‹é™å’Œå‡†ç¡®åº¦åŒ¹é…ï¼Œåªå…³æ³¨æ€»ä½“å‡†ç¡®åº¦ï¼Œå¾€å¾€å¯¼è‡´ä¸å®Œæ•´æˆ–è¯¯å¯¼æ€§çš„ç»“è®ºã€‚æˆ‘ä»¬çš„æŒ‡æ ‡é€šè¿‡å¼ºè°ƒé—®é¢˜çº§åˆ«çš„è¯„ä¼°ç»“æœåŒ¹é…æ¥è§£å†³è¿™ä¸€å±€é™æ€§ã€‚å¯¹10ä¸ªLLMã€5ä¸ªåŸºå‡†æµ‹è¯•ã€20ä¸ªBDCç¼“è§£ç­–ç•¥å’Œ2ä¸ªæ±¡æŸ“åœºæ™¯çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ²¡æœ‰ä»»ä½•ç°æœ‰ç­–ç•¥èƒ½æ˜¾è‘—æé«˜å¯¹æ±¡æŸ“çš„æŠµæŠ—åŠ›ï¼Œä¹Ÿæ²¡æœ‰ç­–ç•¥èƒ½æœ‰æ•ˆåœ°å¹³è¡¡ä¿çœŸåº¦å’ŒæŠ—æ±¡æŸ“åº¦ã€‚è¿™äº›å‘ç°çªæ˜¾äº†è®¾è®¡æ›´æœ‰æ•ˆçš„BDCç¼“è§£ç­–ç•¥çš„ç´§è¿«éœ€æ±‚ã€‚æˆ‘ä»¬çš„ä»£ç ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ASTRAL-Group/BDC_mitigation_assessment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ASTRAL-Group/BDC_mitigation_assessmentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16402v1">PDF</a> 23 pages</p>
<p><strong>Summary</strong>ï¼šè®­ç»ƒé›†ä¸­æ··å…¥åŸºå‡†æµ‹è¯•æ ·æœ¬çš„é—®é¢˜ï¼Œå¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°æ€§èƒ½è¢«è™šå‡æŠ¬é«˜å¹¶å½±å“è¯„ä¼°å¯é æ€§ã€‚ç ”ç©¶äººå‘˜æå‡ºäº†å¤šç§ç¼“è§£ç­–ç•¥æ›´æ–°ç°æœ‰åŸºå‡†ï¼Œä½†ç¼ºä¹å¯¹å…¶æœ‰æ•ˆæ€§çš„ä¸¥æ ¼å®¡æŸ¥ã€‚æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªç³»ç»ŸåŒ–ç®¡é“å’Œä¸¤ä¸ªæ–°æŒ‡æ ‡ï¼Œä»¥ç²¾ç»†å’Œç»¼åˆåœ°è¯„ä¼°ç°æœ‰ç¼“è§£ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰ç­–ç•¥åœ¨æ”¹å–„æŠµæŠ—åŠ›æ–¹é¢å¹¶ä¸æ˜¾è‘—ï¼Œéœ€è®¾è®¡æ›´æœ‰æ•ˆçš„ç¼“è§£ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŸºå‡†æ•°æ®æ±¡æŸ“ï¼ˆBDCï¼‰é—®é¢˜åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°ä¸­è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚</li>
<li>BDCä¼šå¯¼è‡´æ€§èƒ½è¯„ä¼°è¢«è™šå‡æŠ¬é«˜ï¼Œå½±å“è¯„ä¼°çš„å¯é æ€§ã€‚</li>
<li>ç ”ç©¶äººå‘˜å·²ç»æå‡ºäº†å¤šç§ç­–ç•¥æ¥ç¼“è§£BDCé—®é¢˜ï¼Œæ›´æ–°ç°æœ‰çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªç³»ç»ŸåŒ–ç®¡é“å’Œä¸¤ä¸ªæ–°æŒ‡æ ‡ï¼ˆä¿çœŸåº¦å’Œæ±¡æŸ“æŠµæŠ—åŠ›ï¼‰æ¥å…¨é¢è¯„ä¼°è¿™äº›ç¼“è§£ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„ç¼“è§£ç­–ç•¥å¹¶æœªæ˜¾è‘—æé«˜æ¨¡å‹çš„æŠµæŠ—åŠ›ã€‚</li>
<li>ç›®å‰æ²¡æœ‰ä»»ä½•ç­–ç•¥èƒ½å¤Ÿåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆå¹³è¡¡ä¿çœŸåº¦å’Œæ±¡æŸ“æŠµæŠ—åŠ›ã€‚</li>
<li>éœ€è¦è®¾è®¡æ›´æœ‰æ•ˆçš„BDCç¼“è§£ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dad96941a801a34af77df2bc6d758e9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdb29714499027190f4840058110bd6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c709036b018fb97b443b759682ce423.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0cf458abc4ec6d06f7730e6d22be603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ec7bc188f3a092e0acce4797151c829.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LaPIG-Cross-Modal-Generation-of-Paired-Thermal-and-Visible-Facial-Images"><a href="#LaPIG-Cross-Modal-Generation-of-Paired-Thermal-and-Visible-Facial-Images" class="headerlink" title="LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial   Images"></a>LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial   Images</h2><p><strong>Authors:Leyang Wang, Joice Lin</strong></p>
<p>The success of modern machine learning, particularly in facial translation networks, is highly dependent on the availability of high-quality, paired, large-scale datasets. However, acquiring sufficient data is often challenging and costly. Inspired by the recent success of diffusion models in high-quality image synthesis and advancements in Large Language Models (LLMs), we propose a novel framework called LLM-assisted Paired Image Generation (LaPIG). This framework enables the construction of comprehensive, high-quality paired visible and thermal images using captions generated by LLMs. Our method encompasses three parts: visible image synthesis with ArcFace embedding, thermal image translation using Latent Diffusion Models (LDMs), and caption generation with LLMs. Our approach not only generates multi-view paired visible and thermal images to increase data diversity but also produces high-quality paired data while maintaining their identity information. We evaluate our method on public datasets by comparing it with existing methods, demonstrating the superiority of LaPIG. </p>
<blockquote>
<p>ç°ä»£æœºå™¨å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨ç¿»è¯‘ç½‘ç»œæ–¹é¢çš„æˆåŠŸï¼Œåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé«˜è´¨é‡ã€é…å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†çš„å¯ç”¨æ€§ã€‚ç„¶è€Œï¼Œè·å–è¶³å¤Ÿçš„æ•°æ®å¾€å¾€å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚å—æœ€è¿‘é«˜è´¨é‡å›¾åƒåˆæˆä¸­æ‰©æ•£æ¨¡å‹æˆåŠŸä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºLLMè¾…åŠ©é…å¯¹å›¾åƒç”Ÿæˆï¼ˆLaPIGï¼‰çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨LLMç”Ÿæˆçš„æ ‡é¢˜æ„å»ºå…¨é¢ã€é«˜è´¨é‡é…å¯¹çš„å¯è§å…‰å’Œçƒ­å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼šä½¿ç”¨ArcFaceåµŒå…¥çš„å¯è§å›¾åƒåˆæˆã€ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„çƒ­å›¾åƒç¿»è¯‘ã€ä»¥åŠä½¿ç”¨LLMçš„æ ‡é¢˜ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ç”Ÿæˆå¤šè§†è§’é…å¯¹å¯è§å…‰å’Œçƒ­å›¾åƒä»¥å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œè€Œä¸”è¿˜åœ¨ä¿æŒèº«ä»½ä¿¡æ¯çš„æ¡ä»¶ä¸‹äº§ç”Ÿé«˜è´¨é‡é…å¯¹æ•°æ®ã€‚æˆ‘ä»¬åœ¨å…¬å…±æ•°æ®é›†ä¸Šé€šè¿‡å°†å…¶ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œå¯¹æ¯”ï¼Œè¯æ˜äº†LaPIGçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16376v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°ä»£æœºå™¨å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨ç¿»è¯‘ç½‘ç»œæ–¹é¢çš„æˆåŠŸé«˜åº¦ä¾èµ–äºé«˜è´¨é‡ã€é…å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†çš„å¯è·å¾—æ€§ã€‚ç„¶è€Œï¼Œè·å–è¶³å¤Ÿçš„æ•°æ®å¾€å¾€å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚å—æ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒåˆæˆå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢çš„æœ€æ–°æˆåŠŸçš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLLMè¾…åŠ©é…å¯¹å›¾åƒç”Ÿæˆï¼ˆLaPIGï¼‰çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨LLMç”Ÿæˆçš„æ ‡é¢˜ï¼Œèƒ½å¤Ÿå®ç°å…¨é¢ã€é«˜è´¨é‡é…å¯¹å¯è§å’Œçº¢å¤–å›¾åƒçš„æ„å»ºã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼šä½¿ç”¨ArcFaceåµŒå…¥çš„å¯è§å›¾åƒåˆæˆã€ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„çº¢å¤–å›¾åƒç¿»è¯‘ï¼Œä»¥åŠä½¿ç”¨LLMçš„æ ‡é¢˜ç”Ÿæˆã€‚è¯¥æ–¹æ³•ä¸ä»…ç”Ÿæˆå¤šè§†è§’é…å¯¹å¯è§å’Œçº¢å¤–å›¾åƒä»¥å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œè€Œä¸”åœ¨ä¿æŒèº«ä»½ä¿¡æ¯çš„åŒäº‹äº§ç”Ÿé«˜è´¨é‡é…å¯¹æ•°æ®ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°è¯¥æ–¹æ³•ï¼Œä¸ç°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œè¯æ˜äº†LaPIGçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£æœºå™¨å­¦ä¹ çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé«˜è´¨é‡ã€é…å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>è·å–è¶³å¤Ÿçš„æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>LLMè¾…åŠ©é…å¯¹å›¾åƒç”Ÿæˆï¼ˆLaPIGï¼‰æ¡†æ¶èƒ½å¤Ÿæ„å»ºå…¨é¢ã€é«˜è´¨é‡çš„é…å¯¹å›¾åƒã€‚</li>
<li>LaPIGæ¡†æ¶åŒ…æ‹¬å¯è§å›¾åƒåˆæˆã€çº¢å¤–å›¾åƒç¿»è¯‘å’Œæ ‡é¢˜ç”Ÿæˆä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤šè§†è§’é…å¯¹å›¾åƒæ¥å¢åŠ æ•°æ®å¤šæ ·æ€§ã€‚</li>
<li>LaPIGåœ¨ä¿æŒèº«ä»½ä¿¡æ¯çš„åŒæ—¶ï¼Œäº§ç”Ÿé«˜è´¨é‡é…å¯¹æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96b8777b07b88bc21179988f93d98e12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa64d1ba110ac4383c0c0334af3d1f0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b78173cdb01b03f3a9314f192627e972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5df613a31fcd249d5d5072355c031678.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CaKE-Circuit-aware-Editing-Enables-Generalizable-Knowledge-Learners"><a href="#CaKE-Circuit-aware-Editing-Enables-Generalizable-Knowledge-Learners" class="headerlink" title="CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners"></a>CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners</h2><p><strong>Authors:Yunzhi Yao, Jizhan Fang, Jia-Chen Gu, Ningyu Zhang, Shumin Deng, Huajun Chen, Nanyun Peng</strong></p>
<p>Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they struggle to generalize these updates to multi-hop reasoning tasks that depend on the modified knowledge. Through an analysis of reasoning circuits â€“ the neural pathways LLMs use for knowledge-based inference, we observe that current layer-localized KE approaches, such as MEMIT and WISE, which edit only single or a few model layers, struggle to effectively incorporate updated information into these reasoning pathways. To address this limitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method that enables more effective integration of updated knowledge in LLMs. CaKE leverages strategically curated data, guided by our circuits-based analysis, that enforces the model to utilize the modified knowledge, stimulating the model to develop appropriate reasoning circuits for newly integrated knowledge. Experimental results show that CaKE enables more accurate and consistent use of updated knowledge across related reasoning tasks, leading to an average of 20% improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to existing KE methods. We release the code and data in <a target="_blank" rel="noopener" href="https://github.com/zjunlp/CaKE">https://github.com/zjunlp/CaKE</a>. </p>
<blockquote>
<p>çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰èƒ½å¤Ÿä¿®æ”¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿‡æ—¶æˆ–é”™è¯¯çš„ä¿¡æ¯ã€‚è™½ç„¶ç°æœ‰çš„KEæ–¹æ³•èƒ½å¤Ÿæ›´æ–°å­¤ç«‹çš„äº‹å®ï¼Œä½†å®ƒä»¬éš¾ä»¥å°†è¿™äº›æ›´æ–°æ¨å¹¿åˆ°ä¾èµ–äºä¿®æ”¹åçš„çŸ¥è¯†çš„å¤šè·³æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å¯¹è¯­è¨€æ¨¡å‹ç”¨äºçŸ¥è¯†æ¨ç†çš„ç¥ç»é€”å¾„ï¼ˆå³æ¨ç†ç”µè·¯ï¼‰çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„å±‚å±€éƒ¨åŒ–KEæ–¹æ³•ï¼Œå¦‚MEMITå’ŒWISEï¼Œå®ƒä»¬åªç¼–è¾‘å•ä¸ªæˆ–å°‘æ•°å‡ ä¸ªæ¨¡å‹å±‚ï¼Œéš¾ä»¥æœ‰æ•ˆåœ°å°†æ›´æ–°åçš„ä¿¡æ¯èå…¥è¿™äº›æ¨ç†é€”å¾„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CaKEï¼ˆç”µè·¯æ„ŸçŸ¥çŸ¥è¯†ç¼–è¾‘ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿æ›´æ–°åçš„çŸ¥è¯†åœ¨LLMä¸­æ›´æœ‰æ•ˆåœ°é›†æˆçš„æ–°æ–¹æ³•ã€‚CaKEåˆ©ç”¨æˆ‘ä»¬åŸºäºç”µè·¯çš„åˆ†ææŒ‡å¯¼çš„æˆ˜ç•¥å®šåˆ¶æ•°æ®ï¼Œå¼ºåˆ¶æ¨¡å‹åˆ©ç”¨ä¿®æ”¹åçš„çŸ¥è¯†ï¼Œåˆºæ¿€æ¨¡å‹ä¸ºåˆšåˆšé›†æˆçš„æ–°çŸ¥è¯†å¼€å‘é€‚å½“çš„æ¨ç†ç”µè·¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCaKEèƒ½å¤Ÿæ›´å‡†ç¡®ã€æ›´ä¸€è‡´åœ°åœ¨ç›¸å…³æ¨ç†ä»»åŠ¡ä¸­ä½¿ç”¨æ›´æ–°çš„çŸ¥è¯†ï¼Œä¸ç°æœ‰çš„KEæ–¹æ³•ç›¸æ¯”ï¼Œåœ¨MQuAKEæ•°æ®é›†ä¸Šçš„å¤šè·³æ¨ç†å‡†ç¡®åº¦å¹³å‡æé«˜äº†20%ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/CaKE%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/zjunlp/CaKEä¸Šå‘å¸ƒäº†ä»£ç å’Œæ•°æ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16356v1">PDF</a> Work in progress</p>
<p><strong>æ‘˜è¦</strong></p>
<p>çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰èƒ½å¤Ÿä¿®æ”¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿‡æ—¶æˆ–é”™è¯¯çš„ä¿¡æ¯ã€‚ç°æœ‰KEæ–¹æ³•è™½ç„¶å¯ä»¥æ›´æ–°å­¤ç«‹çš„äº‹å®ï¼Œä½†åœ¨ä¾èµ–äºä¿®æ”¹åçš„çŸ¥è¯†çš„å¤šè·³æ¨ç†ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬éš¾ä»¥å°†è¿™äº›æ›´æ–°æ¨å¹¿ã€‚é€šè¿‡åˆ†æLLMç”¨äºçŸ¥è¯†æ¨ç†çš„ç¥ç»é€šè·¯â€”â€”æ¨ç†ç”µè·¯ï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰çš„å±‚å±€éƒ¨åŒ–KEæ–¹æ³•ï¼Œå¦‚MEMITå’ŒWISEï¼Œä»…åœ¨å•ä¸ªæˆ–å°‘æ•°æ¨¡å‹å±‚è¿›è¡Œç¼–è¾‘ï¼Œéš¾ä»¥å°†æ›´æ–°åçš„ä¿¡æ¯æœ‰æ•ˆåœ°èå…¥è¿™äº›æ¨ç†ç”µè·¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CaKEï¼ˆç”µè·¯æ„ŸçŸ¥çŸ¥è¯†ç¼–è¾‘ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿æ›´æ–°åçš„çŸ¥è¯†æ›´æœ‰æ•ˆåœ°æ•´åˆåˆ°LLMä¸­çš„æ–°æ–¹æ³•ã€‚CaKEåˆ©ç”¨æˆ‘ä»¬åŸºäºç”µè·¯çš„åˆ†æç²¾å¿ƒç­–åˆ’çš„æ•°æ®ï¼Œå¼ºåˆ¶æ¨¡å‹åˆ©ç”¨ä¿®æ”¹åçš„çŸ¥è¯†ï¼Œåˆºæ¿€æ¨¡å‹ä¸ºåˆšåˆšæ•´åˆçš„çŸ¥è¯†å‘å±•é€‚å½“çš„æ¨ç†ç”µè·¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰KEæ–¹æ³•ç›¸æ¯”ï¼ŒCaKEåœ¨MQuAKEæ•°æ®é›†ä¸Šçš„å¤šè·³æ¨ç†å‡†ç¡®æ€§æé«˜äº†å¹³å‡20%ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/CaKE%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/zjunlp/CaKEä¸Šå‘å¸ƒäº†ä»£ç å’Œæ•°æ®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰èƒ½å¤Ÿä¿®æ”¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰KEæ–¹æ³•åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­éš¾ä»¥æ¨å¹¿çŸ¥è¯†æ›´æ–°ã€‚</li>
<li>é€šè¿‡å¯¹LLMçš„æ¨ç†ç”µè·¯è¿›è¡Œåˆ†æï¼Œå‘ç°ç°æœ‰KEæ–¹æ³•éš¾ä»¥å°†æ›´æ–°åçš„çŸ¥è¯†èå…¥è¿™äº›ç”µè·¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†ç¼–è¾‘æ–¹æ³•â€”â€”CaKEï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æ•´åˆæ›´æ–°åçš„çŸ¥è¯†åˆ°LLMä¸­ã€‚</li>
<li>CaKEåˆ©ç”¨æˆ˜ç•¥ç­–åˆ’çš„æ•°æ®ï¼Œå¼ºåˆ¶æ¨¡å‹åˆ©ç”¨ä¿®æ”¹åçš„çŸ¥è¯†ï¼Œå¹¶åˆºæ¿€å…¶å½¢æˆé€‚å½“çš„æ¨ç†ç”µè·¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCaKEåœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨MQuAKEæ•°æ®é›†ä¸Šçš„æ¨ç†å‡†ç¡®æ€§å¹³å‡æé«˜äº†20%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a298e17baa78aebf234487e71df76fdd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37ec7305a769642a232fbc13bc55c010.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a72e0dc732f0fcd6decdb754c5ae0e17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d2267512312a59b03b4130c106589fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b2c36e98b157457494cc856b5eeb818.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c7c0a88466debfa3036e537483fadd4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLM-Braces-Straightening-Out-LLM-Predictions-with-Relevant-Sub-Updates"><a href="#LLM-Braces-Straightening-Out-LLM-Predictions-with-Relevant-Sub-Updates" class="headerlink" title="LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates"></a>LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates</h2><p><strong>Authors:Ying Shen, Lifu Huang</strong></p>
<p>Recent findings reveal that much of the knowledge in a Transformer-based Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where each FNN layer can be interpreted as the summation of sub-updates, each corresponding to a weighted column vector from the FFNâ€™s value parameter matrix that often encodes human-interpretable concepts. In light of this, we hypothesize that model performance and behaviors can be further enhanced and controlled by modulating the contributions of these sub-updates based on their relevance to the input or target output style, and propose LLMBRACES, a novel and efficient method that computes relevance scores associated with value vectors in FFN layers and leverages these scores to dynamically adjust the contribution of sub-updates. By optimizing sub-update contributions, LLMBRACES refines the prediction process, leading to more accurate and reliable outputs, much like a â€˜braceâ€™ providing support and stability. Moreover, LLMBRACES can be extended to support conditional control over generation characteristics, such as sentiment, thereby offering fine-grained steering of LLM outputs. Extensive experiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and Llama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both fine-tuning and zero-shot settings while requiring significantly fewer tunable parameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in sentiment-controlled generation and toxicity reduction, highlighting its potential for flexible, controlled text generation across applications. </p>
<blockquote>
<p>æœ€æ–°ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¤§éƒ¨åˆ†çŸ¥è¯†éƒ½ç¼–ç åœ¨å…¶å‰é¦ˆï¼ˆFFNï¼‰å±‚ä¸­ã€‚åœ¨è¿™äº›FFNå±‚ä¸­ï¼Œæ¯ä¸€å±‚éƒ½å¯ä»¥è¢«è§£é‡Šä¸ºå­æ›´æ–°çš„æ€»å’Œï¼Œæ¯ä¸ªå­æ›´æ–°å¯¹åº”äºFFNå€¼å‚æ•°çŸ©é˜µä¸­çš„åŠ æƒåˆ—å‘é‡ï¼Œè¿™äº›åˆ—å‘é‡é€šå¸¸ç¼–ç äº†äººç±»å¯è§£é‡Šçš„æ¦‚å¿µã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬å‡è®¾å¯ä»¥é€šè¿‡è°ƒèŠ‚è¿™äº›å­æ›´æ–°å¯¹æ¨¡å‹æ€§èƒ½å’Œè¡Œä¸ºçš„è´¡çŒ®æ¥è¿›ä¸€æ­¥æ”¹å–„å’Œæ§åˆ¶æ¨¡å‹æ€§èƒ½å’Œè¡Œä¸ºï¼Œè¿™äº›è°ƒèŠ‚åŸºäºå­æ›´æ–°ä¸è¾“å…¥æˆ–ç›®æ ‡è¾“å‡ºé£æ ¼çš„å…³è”æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³•LLMBRACESï¼Œè¯¥æ–¹æ³•è®¡ç®—FFNå±‚ä¸­å€¼å‘é‡çš„ç›¸å…³æ€§åˆ†æ•°ï¼Œå¹¶åˆ©ç”¨è¿™äº›åˆ†æ•°æ¥åŠ¨æ€è°ƒæ•´å­æ›´æ–°çš„è´¡çŒ®ã€‚é€šè¿‡ä¼˜åŒ–å­æ›´æ–°çš„è´¡çŒ®ï¼ŒLLMBRACESå¯ä»¥æ”¹è¿›é¢„æµ‹è¿‡ç¨‹ï¼Œä»è€Œå¾—åˆ°æ›´å‡†ç¡®ã€æ›´å¯é çš„è¾“å‡ºï¼Œå°±åƒâ€œæ”¯æ’‘â€å’Œâ€œç¨³å®šâ€çš„â€œæ”¯æ’‘å¸¦â€ä¸€æ ·ã€‚æ­¤å¤–ï¼ŒLLMBRACESå¯ä»¥æ‰©å±•ä»¥æ”¯æŒå¯¹ç”Ÿæˆç‰¹æ€§çš„æ¡ä»¶æ§åˆ¶ï¼Œä¾‹å¦‚æƒ…æ„Ÿï¼Œä»è€Œå®ç°å¯¹LLMè¾“å‡ºçš„ç²¾ç»†æ§åˆ¶ã€‚åœ¨åŒ…æ‹¬Qwen2.5-1.5Bã€Llama2-7Bå’ŒLlama3-8Bç­‰å„ç§LLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLLMBRACESåœ¨å¾®è°ƒå’Œé›¶æ ·æœ¬è®¾ç½®ä¸­éƒ½ä¼˜äºåŸºå‡†æ–¹æ³•ï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„å¯è°ƒå‚æ•°ï¼Œä¸LoRAç›¸æ¯”ï¼Œæœ€å¤šå¯å‡å°‘75%ã€‚æ­¤å¤–ï¼ŒLLMBRACESåœ¨æƒ…æ„Ÿæ§åˆ¶ç”Ÿæˆå’Œæ¯’æ€§é™ä½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œçªæ˜¾å…¶åœ¨è·¨åº”ç”¨ç¨‹åºçš„çµæ´»ã€å—æ§æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16334v1">PDF</a> 16 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶å‘ç°ï¼ŒåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„çŸ¥è¯†ä¸»è¦ç¼–ç åœ¨å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰å±‚ä¸­ã€‚æ¯ä¸ªFFNå±‚å¯è§£é‡Šä¸ºå­æ›´æ–°çš„æ€»å’Œï¼Œæ¯ä¸ªå­æ›´æ–°å¯¹åº”äºFFNå€¼å‚æ•°çŸ©é˜µçš„åŠ æƒåˆ—å‘é‡ï¼Œè¿™äº›åˆ—å‘é‡é€šå¸¸ç¼–ç äººç±»å¯è§£é‡Šçš„æ¦‚å¿µã€‚å› æ­¤ï¼Œé€šè¿‡è°ƒèŠ‚è¿™äº›å­æ›´æ–°å¯¹æ¨¡å‹æ€§èƒ½å’Œè¡Œä¸ºçš„è´¡çŒ®ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œå¯æ§æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†LLMBRACESæ–¹æ³•ï¼Œè¯¥æ–¹æ³•è®¡ç®—FFNå±‚ä¸­å€¼å‘é‡çš„ç›¸å…³æ€§åˆ†æ•°ï¼Œå¹¶åˆ©ç”¨è¿™äº›åˆ†æ•°åŠ¨æ€è°ƒæ•´å­æ›´æ–°çš„è´¡çŒ®ã€‚é€šè¿‡ä¼˜åŒ–å­æ›´æ–°çš„è´¡çŒ®ï¼ŒLLMBRACESèƒ½å¤Ÿæ›´ç²¾ç»†åœ°æ§åˆ¶è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒLLMBRACESåœ¨å¤šç§LLMä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿæ§åˆ¶ç”Ÿæˆå’Œæ¯’æ€§é™ä½ï¼Œå‡¸æ˜¾å…¶åœ¨å„ç§åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä¸­çš„çŸ¥è¯†ä¸»è¦ç¼–ç åœ¨FFNå±‚ä¸­ï¼Œæ¯ä¸ªFFNå±‚åŒ…å«ç¼–ç äººç±»å¯è§£é‡Šæ¦‚å¿µçš„å€¼å‚æ•°çŸ©é˜µçš„åŠ æƒåˆ—å‘é‡ã€‚</li>
<li>LLMBRACESæ–¹æ³•é€šè¿‡è®¡ç®—å€¼å‘é‡çš„ç›¸å…³æ€§åˆ†æ•°æ¥åŠ¨æ€è°ƒæ•´å­æ›´æ–°çš„è´¡çŒ®ã€‚</li>
<li>LLMBRACESèƒ½å¤Ÿæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§å¹¶æ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºã€‚</li>
<li>LLMBRACESé€‚ç”¨äºå¤šç§LLMï¼ŒåŒ…æ‹¬æƒ…æ„Ÿæ§åˆ¶ç”Ÿæˆå’Œæ¯’æ€§é™ä½ç­‰åº”ç”¨ã€‚</li>
<li>LLMBRACESåœ¨ç²¾ç»†è°ƒæ•´æ¨¡å‹è¾“å‡ºæ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒLLMBRACESåœ¨å‚æ•°æ•°é‡æ–¹é¢æ›´ä¸ºé«˜æ•ˆï¼Œå¯å‡å°‘å¤šè¾¾75%çš„å¯è°ƒæ•´å‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e493607def497621f26e000277d505cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89a2f4db81f6cfcba7e514381d73dd17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4207fe061c827f710e48542a231ece4b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OmniGeo-Towards-a-Multimodal-Large-Language-Models-for-Geospatial-Artificial-Intelligence"><a href="#OmniGeo-Towards-a-Multimodal-Large-Language-Models-for-Geospatial-Artificial-Intelligence" class="headerlink" title="OmniGeo: Towards a Multimodal Large Language Models for Geospatial   Artificial Intelligence"></a>OmniGeo: Towards a Multimodal Large Language Models for Geospatial   Artificial Intelligence</h2><p><strong>Authors:Long Yuan, Fengran Mo, Kaiyu Huang, Wenjie Wang, Wangyuxuan Zhai, Xiaoyu Zhu, You Li, Jinan Xu, Jian-Yun Nie</strong></p>
<p>The rapid advancement of multimodal large language models (LLMs) has opened new frontiers in artificial intelligence, enabling the integration of diverse large-scale data types such as text, images, and spatial information. In this paper, we explore the potential of multimodal LLMs (MLLM) for geospatial artificial intelligence (GeoAI), a field that leverages spatial data to address challenges in domains including Geospatial Semantics, Health Geography, Urban Geography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo) tailored to geospatial applications, capable of processing and analyzing heterogeneous data sources, including satellite imagery, geospatial metadata, and textual descriptions. By combining the strengths of natural language understanding and spatial reasoning, our model enhances the ability of instruction following and the accuracy of GeoAI systems. Results demonstrate that our model outperforms task-specific models and existing LLMs on diverse geospatial tasks, effectively addressing the multimodality nature while achieving competitive results on the zero-shot geospatial tasks. Our code will be released after publication. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºäººå·¥æ™ºèƒ½å¼€å¯äº†æ–°çš„å‰æ²¿ï¼Œèƒ½å¤Ÿå®ç°æ–‡æœ¬ã€å›¾åƒå’Œç©ºé—´ä¿¡æ¯ç­‰ä¸åŒå¤§è§„æ¨¡æ•°æ®ç±»å‹çš„é›†æˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰åœ¨åœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½ï¼ˆGeoAIï¼‰ä¸­çš„æ½œåŠ›ã€‚GeoAIæ˜¯ä¸€ä¸ªåˆ©ç”¨ç©ºé—´æ•°æ®æ¥è§£å†³åŒ…æ‹¬åœ°ç†ç©ºé—´è¯­ä¹‰ã€å¥åº·åœ°ç†å­¦ã€åŸå¸‚åœ°ç†å­¦ã€åŸå¸‚æ„ŸçŸ¥å’Œé¥æ„Ÿç­‰é¢†åŸŸçš„æŒ‘æˆ˜çš„é¢†åŸŸã€‚æˆ‘ä»¬é’ˆå¯¹åœ°ç†ç©ºé—´åº”ç”¨æå‡ºäº†ä¸€ç§MLLMï¼ˆOmniGeoï¼‰ï¼Œå®ƒèƒ½å¤Ÿå¤„ç†å’Œåˆ†æåŒ…æ‹¬å«æ˜Ÿå›¾åƒã€åœ°ç†ç©ºé—´å…ƒæ•°æ®ä»¥åŠæ–‡æœ¬æè¿°åœ¨å†…çš„å¼‚è´¨æ•°æ®æºã€‚é€šè¿‡ç»“åˆè‡ªç„¶è¯­è¨€ç†è§£å’Œç©ºé—´æ¨ç†çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¢å¼ºäº†æŒ‡ä»¤è·Ÿéšèƒ½åŠ›å’ŒGeoAIç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šç§åœ°ç†ç©ºé—´ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç‰¹å®šä»»åŠ¡æ¨¡å‹å’Œç°æœ‰çš„LLMï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å¤šæ¨¡æ€æ€§è´¨ï¼Œå¹¶åœ¨é›¶æ ·æœ¬åœ°ç†ç©ºé—´ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚ä»£ç å°†åœ¨å‘è¡¨åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16326v1">PDF</a> 15 pages, Under review</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºäººå·¥æ™ºèƒ½å¼€å¯äº†æ–°çš„å‰æ²¿ï¼Œä½¿å¾—èƒ½å¤Ÿæ•´åˆæ–‡æœ¬ã€å›¾åƒå’Œç©ºé—´ä¿¡æ¯ç­‰å¤šæ ·åŒ–çš„å¤§è§„æ¨¡æ•°æ®ç±»å‹ã€‚æœ¬æ–‡æ¢ç´¢äº†å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰åœ¨åœ°ç¼˜ç©ºé—´äººå·¥æ™ºèƒ½ï¼ˆGeoAIï¼‰é¢†åŸŸçš„æ½œåŠ›ï¼Œè¯¥é¢†åŸŸåˆ©ç”¨ç©ºé—´æ•°æ®åº”å¯¹åŒ…æ‹¬åœ°ç†è¯­ä¹‰ã€å¥åº·åœ°ç†å­¦ã€åŸå¸‚åœ°ç†å­¦ã€åŸå¸‚æ„ŸçŸ¥å’Œé¥æ„Ÿç­‰é¢†åŸŸçš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹åœ°ç†ç©ºé—´åº”ç”¨ç¨‹åºçš„MLLMï¼ˆOmniGeoï¼‰ï¼Œèƒ½å¤Ÿå¤„ç†å’Œåˆ†æåŒ…æ‹¬å«æ˜Ÿå›¾åƒã€åœ°ç†ç©ºé—´å…ƒæ•°æ®ä»¥åŠæ–‡æœ¬æè¿°åœ¨å†…çš„å¼‚è´¨æ•°æ®æºã€‚é€šè¿‡ç»“åˆè‡ªç„¶è¯­è¨€ç†è§£å’Œç©ºé—´æ¨ç†çš„ä¼˜åŠ¿ï¼Œè¯¥æ¨¡å‹æé«˜äº†éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ä»¥åŠGeoAIç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚ç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§åœ°ç†ç©ºé—´ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç‰¹å®šä»»åŠ¡æ¨¡å‹å’Œç°æœ‰LLMï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å¤šæ¨¡æ€æ€§è´¨é—®é¢˜ï¼Œå¹¶åœ¨é›¶æ ·æœ¬åœ°ç†ç©ºé—´ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºäººå·¥æ™ºèƒ½æ•´åˆå¤šæ ·åŒ–æ•°æ®ç±»å‹æä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>åœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½ï¼ˆGeoAIï¼‰é¢†åŸŸåˆ©ç”¨ç©ºé—´æ•°æ®åº”å¯¹å¤šä¸ªé¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹åœ°ç†ç©ºé—´åº”ç”¨ç¨‹åºçš„å¤šæ¨¡æ€LLMæ¨¡å‹ï¼ˆOmniGeoï¼‰ã€‚</li>
<li>OmniGeoèƒ½å¤Ÿå¤„ç†å’Œåˆ†æå¼‚è´¨æ•°æ®æºï¼ŒåŒ…æ‹¬å«æ˜Ÿå›¾åƒã€åœ°ç†ç©ºé—´å…ƒæ•°æ®å’Œæ–‡æœ¬æè¿°ã€‚</li>
<li>OmniGeoç»“åˆäº†è‡ªç„¶è¯­è¨€ç†è§£å’Œç©ºé—´æ¨ç†çš„ä¼˜åŠ¿ï¼Œæé«˜äº†æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’ŒGeoAIç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šç§åœ°ç†ç©ºé—´ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç‰¹å®šä»»åŠ¡æ¨¡å‹å’Œç°æœ‰LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33799f5bf0353071c8b0274571f62abc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d52f073f9d2631c1bec8ce30699fa0df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cf49d8e68557adcf1c9c9b08558c296.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b6d5ea6fe094572465cbd9ae036bca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5fc39163427b2bfa0b832e571c59cea8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Uni-3DAR-Unified-3D-Generation-and-Understanding-via-Autoregression-on-Compressed-Spatial-Tokens"><a href="#Uni-3DAR-Unified-3D-Generation-and-Understanding-via-Autoregression-on-Compressed-Spatial-Tokens" class="headerlink" title="Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on   Compressed Spatial Tokens"></a>Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on   Compressed Spatial Tokens</h2><p><strong>Authors:Shuqi Lu, Haowei Lin, Lin Yao, Zhifeng Gao, Xiaohong Ji, Weinan E, Linfeng Zhang, Guolin Ke</strong></p>
<p>Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates {3D GU} tasks via autoregressive prediction. At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse {3D GU} tasks within a single autoregressive framework. Extensive experiments across multiple microscopic {3D GU} tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/dptech-corp/Uni-3DAR">https://github.com/dptech-corp/Uni-3DAR</a>. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹åŠå…¶å¤šæ¨¡æ€æ‰©å±•çš„è¿›å±•è¯æ˜äº†é€šè¿‡è‡ªå›å½’ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æ¥ç»Ÿä¸€ç”Ÿæˆå’Œç†è§£çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡ä¸‰ç»´ç»“æ„ç”Ÿæˆä¸ç†è§£ï¼ˆ3D GUï¼‰åœ¨äººå·¥æ™ºèƒ½ç§‘å­¦ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†è¿™äº›ä»»åŠ¡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç‹¬ç«‹å‘å±•çš„ï¼Œè‡ªå›å½’æ–¹æ³•ä»ç„¶é²œæœ‰ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Uni-3DARï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è‡ªå›å½’é¢„æµ‹æ— ç¼é›†æˆä¸‰ç»´ç»“æ„ç”Ÿæˆä¸ç†è§£ä»»åŠ¡çš„ç»Ÿä¸€æ¡†æ¶ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨äº†ä¸€ç§æ–°å‹å±‚æ¬¡ä»¤ç‰ŒåŒ–æŠ€æœ¯ï¼Œä½¿ç”¨å…«å‰æ ‘å‹ç¼©ä¸‰ç»´ç©ºé—´ï¼Œåˆ©ç”¨ä¸‰ç»´ç»“æ„å›ºæœ‰çš„ç¨€ç–æ€§ã€‚ç„¶åï¼Œå®ƒé’ˆå¯¹ç²¾ç»†ç»“æ„ç»†èŠ‚åº”ç”¨é¢å¤–çš„ä»¤ç‰ŒåŒ–ï¼Œæ•è·è¯¸å¦‚åŸå­ç±»å‹å’Œå¾®è§‚ä¸‰ç»´ç»“æ„ä¸­çš„ç²¾ç¡®ç©ºé—´åæ ‡ç­‰å…³é”®å±æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ•ˆç‡å¹¶å¢å¼ºæ•ˆæœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹ä¼˜åŒ–æªæ–½ã€‚é¦–å…ˆæ˜¯ä¸¤çº§å­æ ‘å‹ç¼©ç­–ç•¥ï¼Œå¯å°†å…«å‰æ ‘ä»¤ç‰Œåºåˆ—å‡å°‘åˆ°åŸæ¥çš„å…«åˆ†ä¹‹ä¸€ä»¥ä¸‹ã€‚å…¶æ¬¡æ˜¯é’ˆå¯¹åŠ¨æ€å˜åŒ–çš„ä»¤ç‰Œä½ç½®çš„è‡ªå®šä¹‰ä¸‹ä¸€ä¸ªè¢«æ©ç›–ä»¤ç‰Œçš„é¢„æµ‹æœºåˆ¶ï¼Œæå¤§åœ°æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡ç»“åˆè¿™äº›ç­–ç•¥ï¼ŒUni-3DARæˆåŠŸåœ°åœ¨å•ä¸€è‡ªå›å½’æ¡†æ¶å†…ç»Ÿä¸€äº†å¤šç§ä¸‰ç»´ç»“æ„ç”Ÿæˆä¸ç†è§£ä»»åŠ¡ã€‚åœ¨å¤šä¸ªå¾®è§‚çš„ä¸‰ç»´ç»“æ„ç”Ÿæˆä¸ç†è§£ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬åˆ†å­ã€è›‹ç™½è´¨ã€èšåˆç‰©å’Œæ™¶ä½“ç­‰ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒUni-3DARåœ¨å¤šä¸ªæ–¹é¢å¤§å¤§è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç›¸å¯¹æ”¹å–„ç‡é«˜è¾¾256%çš„åŒæ—¶ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†é«˜è¾¾21.8å€ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/dptech-corp/Uni-3DAR%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/dptech-corp/Uni-3DARä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16278v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ‰©å±•å±•ç°äº†é€šè¿‡è‡ªå›å½’é¢„æµ‹æå‡ç”Ÿæˆä¸ç†è§£çš„æˆæ•ˆã€‚ç„¶è€Œï¼Œåœ¨é¢å‘ç§‘å­¦çš„AIé¢†åŸŸä¸­çš„ä¸‰ç»´ç»“æ„ç”Ÿæˆä¸ç†è§£ï¼ˆ3D GUï¼‰ä»»åŠ¡ä¾æ—§å¤§å¤šç‹¬ç«‹æ¼”è¿›ï¼Œè‡ªå›å½’æ–¹æ³•åº”ç”¨æœ‰é™ã€‚ä¸ºå¼¥åˆæ­¤å·®è·ï¼Œå¼•å…¥äº†Uni-3DARç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ— ç¼æ•´åˆå„ç±»3D GUä»»åŠ¡è¿›è¡Œè‡ªå›å½’é¢„æµ‹ã€‚Uni-3DARåˆ©ç”¨ä¸€ç§æ–°é¢–çš„å±‚æ¬¡åŒ–æ ‡è®°æ³•å‹ç¼©ä¸‰ç»´ç©ºé—´ï¼Œå¹¶é‡‡ç”¨é’ˆå¯¹ç²¾ç»†ç»“æ„ç‰¹å¾çš„é¢å¤–æ ‡è®°æ³•æ•è·å…³é”®å±æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºä¸¤é¡¹ä¼˜åŒ–ç­–ç•¥ä»¥æé«˜æ•ˆç‡ä¸æ€§èƒ½ï¼šé€šè¿‡ä¸¤å±‚çº§å­æ ‘å‹ç¼©ç­–ç•¥å°†å‹ç¼©é‡æ ‘ä»¤ç‰Œåºåˆ—é™ä½è‡³åŸæ¥çš„å…«åˆ†ä¹‹ä¸€ä»¥å†…ï¼›é€šè¿‡é’ˆå¯¹åŠ¨æ€å˜åŒ–ä»¤ç‰Œä½ç½®çš„æ©ç ä»¤ç‰Œé¢„æµ‹æœºåˆ¶æå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒUni-3DARåœ¨å¤šç§å¾®è§‚ä¸‰ç»´ç»“æ„ç†è§£ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ä¸çµæ´»æ€§ï¼Œç›¸å¯¹äºæœ€æ–°æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡å¹¶é™ä½äº†æˆæœ¬ã€‚å…¶ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ‰©å±•é€šè¿‡è‡ªå›å½’é¢„æµ‹æå‡äº†ç”Ÿæˆä¸ç†è§£çš„æ•ˆèƒ½ã€‚</li>
<li>å½“å‰çš„ä¸‰ç»´ç»“æ„ç”Ÿæˆä¸ç†è§£ï¼ˆ3D GUï¼‰ä»»åŠ¡åœ¨é¢å‘ç§‘å­¦çš„AIé¢†åŸŸä¸­å¤§å¤šç‹¬ç«‹æ¼”è¿›ï¼Œè€Œè‡ªå›å½’æ–¹æ³•åœ¨è¿™æ–¹é¢çš„åº”ç”¨å°šå¾…å¼€å‘ã€‚</li>
<li>Uni-3DARæ¡†æ¶å¯ä»¥æ•´åˆä¸åŒçš„ä¸‰ç»´GUä»»åŠ¡è¿›è¡Œè‡ªå›å½’é¢„æµ‹ã€‚å®ƒè¿ç”¨æ–°é¢–çš„å±‚æ¬¡åŒ–æ ‡è®°æŠ€æœ¯ï¼Œå¯é’ˆå¯¹ä¸‰ç»´ç©ºé—´è¿›è¡Œç²¾ç»†æ“ä½œå’Œæ•è·å…³é”®ç‰¹å¾ã€‚è¿™ç§å±‚æ¬¡åŒ–æ ‡è®°æ³•åˆ©ç”¨å…«å‰æ ‘å‹ç¼©ä¸‰ç»´ç©ºé—´ï¼Œæé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ã€‚åŒæ—¶å¼•å…¥ç²¾ç»†ç»“æ„ç‰¹å¾çš„é¢å¤–æ ‡è®°æ³•ä»¥æ•è·å…³é”®å±æ€§ã€‚</li>
<li>Uni-3DARæ¡†æ¶å…·æœ‰ä¸¤å¤§ä¼˜åŒ–ç­–ç•¥ï¼šä¸¤å±‚çº§å­æ ‘å‹ç¼©ç­–ç•¥å’Œæ©ç ä»¤ç‰Œé¢„æµ‹æœºåˆ¶ã€‚è¿™ä¸¤å¤§ç­–ç•¥ä¸ä»…æ˜¾è‘—æé«˜äº†Uni-3DARçš„æ€§èƒ½å’Œæ•ˆç‡ï¼ŒåŒæ—¶ä¹Ÿä¿è¯äº†å…¶é’ˆå¯¹å¤šç§ä»»åŠ¡çš„çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚é€šè¿‡å‡å°‘ä»¤ç‰Œåºåˆ—å’Œæé«˜æ¨¡å‹æ€§èƒ½ï¼Œè¿™äº›ä¼˜åŒ–ç­–ç•¥æœ‰åŠ©äºUni-3DARè¶…è¶Šç°æœ‰çš„æŠ€æœ¯ç“¶é¢ˆã€‚</li>
<li>Uni-3DARæ¡†æ¶å·²åœ¨å¤šç§å¾®è§‚ä¸‰ç»´ç»“æ„ç†è§£ä»»åŠ¡ä¸­è¿›è¡Œäº†å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬åˆ†å­ã€è›‹ç™½è´¨ã€èšåˆç‰©å’Œæ™¶ä½“ç­‰ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚ç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼ŒUni-3DARåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æå‡ã€‚ä¾‹å¦‚ç›¸å¯¹äºæœ€å…ˆè¿›æŠ€æœ¯æå‡è¾¾è‡³ç™¾åˆ†ä¹‹äºŒç™¾äº”åå…­çš„ç›¸å¯¹æ”¹å–„æ•ˆæœã€‚è¿™æ„å‘³ç€Uni-3DARåœ¨å„ç§ä»»åŠ¡ä¸­çš„åº”ç”¨éƒ½å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚åŒæ—¶å…¶æ¨ç†é€Ÿåº¦ä¹Ÿæ˜¾è‘—æé«˜ï¼Œæœ€é«˜å¯è¾¾äºŒåä¸€å€åŠçš„é€Ÿåº¦æå‡ã€‚è¿™è¡¨æ˜Uni-3DARä¸ä»…å…·æœ‰é«˜æ•ˆæ€§ï¼Œè€Œä¸”åœ¨å®é™…åº”ç”¨ä¸­èƒ½å¤Ÿå¿«é€Ÿå“åº”å’Œå¤„ç†æ•°æ®ã€‚è¿™ä¸ºUni-3DARåœ¨å„ç§åœºæ™¯ä¸‹çš„åº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€å’Œå¹¿é˜”çš„å‰æ™¯ã€‚</li>
<li>Uni-3DARçš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨è¿™ä¸€é‡è¦ä¿¡æ¯ä¸ºè¯¥é¡¹ç›®çš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†å¯èƒ½ä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†åŸºç¡€æ¡†æ¶å’ŒæŠ€æœ¯æ”¯æŒä»è€Œä½¿å¾—æ›´å¤šç ”ç©¶äººå‘˜å’Œè¡Œä¸šå‚ä¸è€…å¾—ä»¥åˆ©ç”¨å…¶æ¡†æ¶å’Œèµ„æºæ¥å®ç°åˆ›æ–°çš„æ¢ç´¢å’Œæ”¹è¿›æ—¢æœ‰çš„æŠ€æœ¯å’Œæˆæœå±•ç°å‡ºå®½å¹¿çš„å‘å±•æ½œåŠ›å’Œé€‚ç”¨æ€§å€¼å¾—è¿›ä¸€æ­¥å…³æ³¨å’Œç ”ç©¶ã€‚è¿™å¯¹äºæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚åŒæ—¶å…¬å¼€ä»£ç ä¹Ÿä½“ç°äº†ç ”ç©¶è€…çš„å¼€æ”¾ç§‘å­¦ç²¾ç¥ä¿ƒè¿›äº†å­¦æœ¯äº¤æµå’Œåˆä½œæ¨åŠ¨ç§‘å­¦ç ”ç©¶çš„è¿›æ­¥å’Œå‘å±•å…·æœ‰ç§¯ææ„ä¹‰ã€‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0041516a37153f6090a81606192c175c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdbe617e7eb5d6a7fc4d23e20b4ab959.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Chain-of-Functions-A-Programmatic-Pipeline-for-Fine-Grained-Chart-Reasoning-Data"><a href="#Chain-of-Functions-A-Programmatic-Pipeline-for-Fine-Grained-Chart-Reasoning-Data" class="headerlink" title="Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart   Reasoning Data"></a>Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart   Reasoning Data</h2><p><strong>Authors:Zijian Li, Jingjing Fu, Lei Song, Jiang Bian, Jun Zhang, Rui Wang</strong></p>
<p>Visual reasoning is crucial for multimodal large language models (MLLMs) to address complex chart queries, yet high-quality rationale data remains scarce. Existing methods leveraged (M)LLMs for data generation, but direct prompting often yields limited precision and diversity. In this paper, we propose \textit{Chain of Functions (CoF)}, a novel programmatic reasoning data generation pipeline that utilizes freely-explored reasoning paths as supervision to ensure data precision and diversity. Specifically, it starts with human-free exploration among the atomic functions (e.g., maximum data and arithmetic operations) to generate diverse function chains, which are then translated into linguistic rationales and questions with only a moderate open-sourced LLM. \textit{CoF} provides multiple benefits: 1) Precision: function-governed generation reduces hallucinations compared to freeform generation; 2) Diversity: enumerating function chains enables varied question taxonomies; 3) Explainability: function chains serve as built-in rationales, allowing fine-grained evaluation beyond overall accuracy; 4) Practicality: eliminating reliance on extremely large models. Employing \textit{CoF}, we construct the \textit{ChartCoF} dataset, with 1.4k complex reasoning Q&amp;A for fine-grained analysis and 50k Q&amp;A for reasoning enhancement. The fine-grained evaluation on \textit{ChartCoF} reveals varying performance across question taxonomies for each MLLM, and the experiments also show that finetuning with \textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs on widely used benchmarks. Furthermore, the novel paradigm of function-governed rationale generation in \textit{CoF} could inspire broader applications beyond charts. </p>
<blockquote>
<p>è§†è§‰æ¨ç†å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è§£å†³å¤æ‚å›¾è¡¨æŸ¥è¯¢è‡³å…³é‡è¦ï¼Œä½†é«˜è´¨é‡çš„ç†ç”±æ•°æ®ä»ç„¶ç¨€ç¼ºã€‚ç°æœ‰æ–¹æ³•åˆ©ç”¨ï¼ˆMï¼‰LLMsè¿›è¡Œæ•°æ®ç”Ÿæˆï¼Œä½†ç›´æ¥æç¤ºå¾€å¾€ç²¾åº¦å’Œå¤šæ ·æ€§æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†\emph{å‡½æ•°é“¾ï¼ˆCoFï¼‰}ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç¨‹åºåŒ–æ¨ç†æ•°æ®ç”Ÿæˆç®¡é“ï¼Œå®ƒåˆ©ç”¨è‡ªç”±æ¢ç´¢çš„æ¨ç†è·¯å¾„ä½œä¸ºç›‘ç£æ¥ä¿è¯æ•°æ®çš„ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå§‹äºåŸå­åŠŸèƒ½ï¼ˆå¦‚æœ€å¤§æ•°æ®å’Œç®—æœ¯è¿ç®—ï¼‰ä¹‹é—´çš„äººç±»è‡ªç”±æ¢ç´¢ï¼Œä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„å‡½æ•°é“¾ï¼Œç„¶åä»…ä½¿ç”¨é€‚åº¦çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹å°†å…¶è½¬æ¢ä¸ºè¯­è¨€ç†ç”±å’Œé—®é¢˜ã€‚\emph{CoF}å¸¦æ¥äº†å¤šä¸ªå¥½å¤„ï¼š1ï¼‰ç²¾åº¦ï¼šå‡½æ•°æ§åˆ¶ç”Ÿæˆå‡å°‘äº†ä¸è‡ªç”±å½¢å¼ç”Ÿæˆç›¸æ¯”çš„å¹»è§‰ï¼›2ï¼‰å¤šæ ·æ€§ï¼šæšä¸¾å‡½æ•°é“¾å¯å®ç°å„ç§é—®é¢˜åˆ†ç±»ï¼›3ï¼‰å¯è§£é‡Šæ€§ï¼šå‡½æ•°é“¾ä½œä¸ºå†…ç½®ç†ç”±ï¼Œå…è®¸è¶…è¶Šæ€»ä½“å‡†ç¡®åº¦çš„ç²¾ç»†è¯„ä¼°ï¼›4ï¼‰å®ç”¨æ€§ï¼šä¸ä¾èµ–æç«¯å¤§å‹æ¨¡å‹ã€‚é€šè¿‡é‡‡ç”¨\emph{CoF}ï¼Œæˆ‘ä»¬æ„å»ºäº†\emph{ChartCoF}æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«1400ä¸ªç”¨äºç²¾ç»†åˆ†æå¤æ‚æ¨ç†çš„é—®ç­”å’Œ5ä¸‡ä¸ªç”¨äºå¢å¼ºæ¨ç†çš„é—®ç­”ã€‚\emph{ChartCoF}ä¸Šçš„ç²¾ç»†è¯„ä¼°æ­ç¤ºäº†æ¯ä¸ªMLLMåœ¨ä¸åŒé—®é¢˜åˆ†ç±»ä¸Šçš„æ€§èƒ½å·®å¼‚ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œä½¿ç”¨\emph{ChartCoF}è¿›è¡Œå¾®è°ƒåœ¨åŒè§„æ¨¡MLLMsä¸­è¾¾åˆ°äº†å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­çš„æœ€æ–°æ°´å¹³ã€‚æ­¤å¤–ï¼Œ\emph{CoF}ä¸­å‡½æ•°æ§åˆ¶ç†ç”±ç”Ÿæˆçš„æ–°èŒƒå¼å¯èƒ½ä¼šæ¿€å‘æ›´å¹¿æ³›çš„å›¾è¡¨ä¹‹å¤–çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16260v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰æ¨ç†å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¤„ç†å¤æ‚å›¾è¡¨æŸ¥è¯¢è‡³å…³é‡è¦ï¼Œä½†é«˜è´¨é‡æ¨ç†æ•°æ®ä»ç„¶ç¨€ç¼ºã€‚ç°æœ‰æ–¹æ³•åˆ©ç”¨ï¼ˆMï¼‰LLMsè¿›è¡Œæ•°æ®é‡‡é›†ç”Ÿæˆï¼Œä½†ç›´æ¥æç¤ºå¾€å¾€ç²¾åº¦å’Œå¤šæ ·æ€§æœ‰é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºChain of Functionsï¼ˆCoFï¼‰çš„æ–°å‹ç¨‹åºåŒ–æ¨ç†æ•°æ®ç”Ÿæˆç®¡é“ï¼Œå®ƒåˆ©ç”¨è‡ªç”±æ¢ç´¢çš„æ¨ç†è·¯å¾„ä½œä¸ºç›‘ç£ï¼Œç¡®ä¿æ•°æ®çš„ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚CoFå…·æœ‰å¤šé‡ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬ç²¾åº¦æå‡ã€å¤šæ ·æ€§å¢å¼ºã€è§£é‡Šæ€§å¢å¼ºå’Œå®è·µæ€§å¹¿æ³›ç­‰ã€‚åˆ©ç”¨CoFï¼Œæˆ‘ä»¬æ„å»ºäº†ChartCoFæ•°æ®é›†ï¼ŒåŒ…å«ç”¨äºç²¾ç»†åˆ†æçš„1400ä¸ªå¤æ‚æ¨ç†é—®ç­”å’Œç”¨äºå¢å¼ºæ¨ç†çš„5ä¸‡ä¸ªé—®ç­”ã€‚åœ¨ChartCoFä¸Šçš„ç²¾ç»†è¯„ä¼°æ˜¾ç¤ºï¼Œæ¯ä¸ªMLLMåœ¨ä¸åŒé—®é¢˜åˆ†ç±»ä¸­çš„æ€§èƒ½å„ä¸ç›¸åŒï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œä½¿ç”¨ChartCoFè¿›è¡Œå¾®è°ƒå¯ä»¥åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCoFä¸­çš„å‡½æ•°æ§åˆ¶æ¨ç†æ•°æ®ç”Ÿæˆæ–°æ¨¡å¼å¯å¯å‘å›¾è¡¨ä¹‹å¤–æ›´å¹¿æ³›çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ¨ç†åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å¤æ‚å›¾è¡¨æŸ¥è¯¢ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†é«˜è´¨é‡æ•°æ®ç¨€ç¼ºã€‚</li>
<li>ç°æœ‰æ•°æ®ç”Ÿæˆæ–¹æ³•ç²¾åº¦å’Œå¤šæ ·æ€§æœ‰é™ã€‚</li>
<li>æå‡ºçš„Chain of Functionsï¼ˆCoFï¼‰æ–¹æ³•åˆ©ç”¨è‡ªç”±æ¢ç´¢çš„æ¨ç†è·¯å¾„ï¼Œæé«˜æ•°æ®ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚</li>
<li>CoFå…·æœ‰ç²¾åº¦ã€å¤šæ ·æ€§ã€è§£é‡Šæ€§å’Œå®ç”¨æ€§ç­‰å¤šé‡ä¼˜åŠ¿ã€‚</li>
<li>ä½¿ç”¨CoFæ„å»ºäº†ChartCoFæ•°æ®é›†ï¼ŒåŒ…å«å¤æ‚æ¨ç†é—®ç­”ï¼Œç”¨äºç²¾ç»†åˆ†æå’Œæ€§èƒ½è¯„ä¼°ã€‚</li>
<li>åœ¨ChartCoFä¸Šçš„è¯„ä¼°æ˜¾ç¤ºä¸åŒMLLMåœ¨ä¸åŒé—®é¢˜åˆ†ç±»ä¸­çš„æ€§èƒ½å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a92e8e04e1d7d18d7d0af3b3a398ef1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b827dddf1bb96f622db5b1db32cf0377.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-384322e30e7db246065cdaa7fd403373.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fin-R1-A-Large-Language-Model-for-Financial-Reasoning-through-Reinforcement-Learning"><a href="#Fin-R1-A-Large-Language-Model-for-Financial-Reasoning-through-Reinforcement-Learning" class="headerlink" title="Fin-R1: A Large Language Model for Financial Reasoning through   Reinforcement Learning"></a>Fin-R1: A Large Language Model for Financial Reasoning through   Reinforcement Learning</h2><p><strong>Authors:Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu, Dezhi Chen, Yun Chen, Zuo Bai, Liwen Zhang</strong></p>
<p>Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SUFE-AIFLM-Lab/Fin-R1">https://github.com/SUFE-AIFLM-Lab/Fin-R1</a>. </p>
<blockquote>
<p>æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹æ­£åœ¨å„ä¸ªé¢†åŸŸä¸­è¿…é€Ÿæ¼”å˜ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„é‡‘èä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ä»ç„¶éœ€è¦æ·±å…¥æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸“ä¸ºé‡‘èé¢†åŸŸè®¾è®¡çš„Fin-R1æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ã€‚Fin-R1é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„æ„å»ºï¼Œåˆ©ç”¨åŸºäºDeepSeek-R1æç‚¼å’Œå¤„ç†çš„é‡‘èæ¨ç†æ•°æ®é›†ã€‚é€šè¿‡æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œå®ƒåœ¨å„ç§é‡‘èæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æ¥è¿‘æ‹¥æœ‰7äº¿å‚æ•°çš„DeepSeek-R1ã€‚åœ¨è¯„ä¼°çš„LLMä¸­ï¼Œå®ƒåœ¨FinQAå’ŒConvFinQAä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨å…¶ä»–ä»»åŠ¡ä¸­ä¹Ÿè¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ã€‚Fin-R1å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼Œä¸ºè§£å†³é‡‘èé¢†åŸŸé‡åˆ°çš„å„ç§é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SUFE-AIFLM-Lab/Fin-R1">https://github.com/SUFE-AIFLM-Lab/Fin-R1</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16252v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸè¿…é€Ÿè¿›åŒ–ï¼Œä½†åœ¨å¤„ç†å¤æ‚é‡‘èä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ä»éœ€æ·±å…¥ç ”ç©¶ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾ä¸“ä¸ºé‡‘èé¢†åŸŸè®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹Fin-R1ã€‚Fin-R1é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼ŒåŸºäºDeepSeek-R1è’¸é¦å’Œå¤„ç†é‡‘èæ¨ç†æ•°æ®é›†ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œå®ƒåœ¨é‡‘èæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ¥è¿‘DeepSeek-R1çš„æ€§èƒ½ï¼Œå‚æ•°è§„æ¨¡ä¸º7äº¿ã€‚å®ƒåœ¨FinQAå’ŒConvFinQAä»»åŠ¡ä¸Šè¾¾åˆ°äº†å½“æ—¶æœ€ä½³æ°´å¹³ï¼Œå¹¶åœ¨å…¶ä»–ä»»åŠ¡ä¸­è¶…è¶Šäº†å¤§å‹æ¨¡å‹ã€‚Fin-R1å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼Œä¸ºé‡‘èé¢†åŸŸçš„é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå¿«é€Ÿå‘å±•ï¼Œé‡‘èé¢†åŸŸçš„ä¸“ç”¨æ¨¡å‹éœ€æ±‚é€æ¸å‡¸æ˜¾ã€‚</li>
<li>Fin-R1æ˜¯ä¸€æ¬¾ä¸“ä¸ºé‡‘èé¢†åŸŸè®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚</li>
<li>Fin-R1é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼ŒåŸºäºDeepSeek-R1è¿›è¡Œæ”¹è¿›å’Œä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼ŒFin-R1åœ¨å¤šä¸ªé‡‘èæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Fin-R1åœ¨FinQAå’ŒConvFinQAä»»åŠ¡ä¸Šè¾¾åˆ°äº†å½“æ—¶æœ€ä½³æ°´å¹³ï¼ˆSOTAï¼‰ã€‚</li>
<li>Fin-R1åœ¨æŸäº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¶Šäº†å…¶ä»–å¤§å‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cad24304b10036555df886ff328075e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5a951f4d8bab88b76dbec5a96e120ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e5e55c6d5bba585a2cc414fc9221967.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73bbd9fc11a29ecd1b3c418afe8df550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a42eb5b35e2c97b5018b6656e925d774.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-for-Reasoning-in-Small-LLMs-What-Works-and-What-Doesnâ€™t"><a href="#Reinforcement-Learning-for-Reasoning-in-Small-LLMs-What-Works-and-What-Doesnâ€™t" class="headerlink" title="Reinforcement Learning for Reasoning in Small LLMs: What Works and What   Doesnâ€™t"></a>Reinforcement Learning for Reasoning in Small LLMs: What Works and What   Doesnâ€™t</h2><p><strong>Authors:Quy-Anh Dang, Chris Ngo</strong></p>
<p>Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 7,000 samples and a $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering a cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying a foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at <a target="_blank" rel="noopener" href="https://github.com/knoveleng/open-rs">https://github.com/knoveleng/open-rs</a>. </p>
<blockquote>
<p>å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›é€šå¸¸ä¾èµ–äºå¤§é‡çš„è®¡ç®—èµ„æºå’Œå¹¿æ³›çš„æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†èµ„æºå—é™ç¯å¢ƒçš„å¯è®¿é—®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å°å‹LLMä¸­æé«˜æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ï¼Œé‡ç‚¹å…³æ³¨ä¸€ä¸ª1.5äº¿å‚æ•°çš„æ¨¡å‹â€”â€”DeepSeek-R1-Distill-Qwen-1.5Bï¼Œåœ¨ä¸¥æ ¼çš„çº¦æŸæ¡ä»¶ä¸‹ï¼šåœ¨4ä¸ªNVIDIA A40 GPUï¼ˆæ¯ä¸ªå…·æœ‰48GB VRAMï¼‰ä¸Šè¿›è¡Œä¸ºæœŸ24å°æ—¶çš„è®­ç»ƒã€‚é€šè¿‡é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¹¶æ•´ç†ä¸€ä¸ªç´§å‡‘ã€é«˜è´¨é‡çš„æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸‰é¡¹å®éªŒæ¥æ¢ç´¢æ¨¡å‹çš„è¡Œä¸ºå’Œæ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¨ç†èƒ½åŠ›è¿…é€Ÿæå‡â€”â€”ä¾‹å¦‚AMC23å‡†ç¡®ç‡ä»63%æé«˜åˆ°80%ï¼ŒAIME24è¾¾åˆ°46.7%ï¼Œä»…ä½¿ç”¨7000ä¸ªæ ·æœ¬å’Œ42ç¾å…ƒçš„è®­ç»ƒæˆæœ¬å°±è¶…è¿‡äº†o1-previewçš„åŸºå‡†æ¨¡å‹ï¼ˆé€šå¸¸éœ€è¦æ•°åƒç¾å…ƒï¼‰ã€‚ç„¶è€Œï¼Œéšç€è®­ç»ƒçš„å»¶é•¿ï¼Œä¹Ÿå‡ºç°äº†ä¼˜åŒ–ä¸ç¨³å®šå’Œé•¿åº¦çº¦æŸç­‰æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°çªæ˜¾äº†åŸºäºRLçš„å¾®è°ƒå¯¹äºå°å‹LLMçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤§è§„æ¨¡æ–¹æ³•æä¾›äº†æˆæœ¬æ•ˆç›Šæ›´é«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬å·²å°†ä»£ç å’Œæ•°æ®é›†ä½œä¸ºå¼€æºèµ„æºå‘å¸ƒï¼Œä¸ºæƒè¡¡æä¾›è§è§£ï¼Œå¹¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸­å¯æ‰©å±•çš„ã€å…·å¤‡æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹å¥ å®šåŸºç¡€ã€‚æ‰€æœ‰èµ„æºéƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/knoveleng/open-rs%E4%B8%8A%E8%8E%B7%E3%80%82">https://github.com/knoveleng/open-rsä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16219v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢ç´¢äº†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜å°å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚åœ¨ä¸¥æ ¼çš„èµ„æºçº¦æŸä¸‹ï¼Œç ”ç©¶å›¢é˜Ÿå¯¹ä»…æœ‰1.5äº¿å‚æ•°çš„æ¨¡å‹DeepSeek-R1-Distill-Qwen-1.5Bè¿›è¡Œäº†è®­ç»ƒï¼Œä»…ä½¿ç”¨4ä¸ªNVIDIA A40 GPUåœ¨24å°æ—¶å†…å®Œæˆã€‚é€šè¿‡é‡‡ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªç´§å‡‘çš„é«˜è´¨é‡æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œç ”ç©¶è¿›è¡Œäº†ä¸‰é¡¹å®éªŒä»¥è§‚å¯Ÿæ¨¡å‹çš„è¡Œä¸ºå’Œæ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒåï¼Œæ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œå¦‚AMC23å‡†ç¡®ç‡ä»63%æé«˜åˆ°80%ï¼ŒAIME24è¾¾åˆ°46.7%ï¼Œä¸”ä»…ä½¿ç”¨7000ä¸ªæ ·æœ¬å’Œ42ç¾å…ƒçš„è®­ç»ƒæˆæœ¬ï¼Œç›¸è¾ƒäºåŸºå‡†æ¨¡å‹å¤§å¤§èŠ‚çœäº†æˆæœ¬ã€‚ç„¶è€Œï¼Œé•¿æœŸè®­ç»ƒä¹Ÿé¢ä¸´ä¼˜åŒ–ä¸ç¨³å®šå’Œé•¿åº¦çº¦æŸç­‰æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°è¯æ˜äº†å¼ºåŒ–å­¦ä¹ å¯¹å°è§„æ¨¡LLMç²¾ç»†è°ƒæ•´çš„æœ‰æ•ˆæ€§ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­å¯ä¼¸ç¼©ã€å…·å¤‡æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶æä¾›äº†æ·±åˆ»çš„è§è§£ã€‚è¯¦æƒ…å¯è®¿é—®ç ”ç©¶å›¢é˜Ÿå…¬å¼€çš„å¼€æºèµ„æºç½‘ç«™æŸ¥çœ‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜äº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹åœ¨èµ„æºä¸¥æ ¼é™åˆ¶ä¸‹è¿›è¡Œï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ çš„é€‚ç”¨æ€§ã€‚</li>
<li>ä½¿ç”¨GRPOç®—æ³•å’Œç‰¹å®šæ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒå®éªŒã€‚</li>
<li>æ¨¡å‹æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œå¦‚AMC23å‡†ç¡®ç‡æå‡å’ŒAIME24å¾—åˆ†å¢åŠ ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æˆæœ¬æ•ˆç›Šæ˜¾è‘—ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å¤§å¤§èŠ‚çœäº†æˆæœ¬ã€‚</li>
<li>é•¿æœŸè®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´ä¼˜åŒ–ä¸ç¨³å®šå’Œé•¿åº¦çº¦æŸçš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d1ae0476b780bfb0435db7f373bd23f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7cd9168b132518717c956deba36f6d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8d9f4786043b572259ce59c1134a7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f1adcaa6fb47e8e6375121aaa290ce1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DocVideoQA-Towards-Comprehensive-Understanding-of-Document-Centric-Videos-through-Question-Answering"><a href="#DocVideoQA-Towards-Comprehensive-Understanding-of-Document-Centric-Videos-through-Question-Answering" class="headerlink" title="DocVideoQA: Towards Comprehensive Understanding of Document-Centric   Videos through Question Answering"></a>DocVideoQA: Towards Comprehensive Understanding of Document-Centric   Videos through Question Answering</h2><p><strong>Authors:Haochen Wang, Kai Hu, Liangcai Gao</strong></p>
<p>Remote work and online courses have become important methods of knowledge dissemination, leading to a large number of document-based instructional videos. Unlike traditional video datasets, these videos mainly feature rich-text images and audio that are densely packed with information closely tied to the visual content, requiring advanced multimodal understanding capabilities. However, this domain remains underexplored due to dataset availability and its inherent complexity. In this paper, we introduce the DocVideoQA task and dataset for the first time, comprising 1454 videos across 23 categories with a total duration of about 828 hours. The dataset is annotated with 154k question-answer pairs generated manually and via GPT, assessing modelsâ€™ comprehension, temporal awareness, and modality integration capabilities. Initially, we establish a baseline using open-source MLLMs. Recognizing the challenges in modality comprehension for document-centric videos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances unimodal feature extraction with diverse instruction-tuning data and employs contrastive learning to strengthen modality integration. Through fine-tuning, the LLM is equipped with audio-visual capabilities, leading to significant improvements in document-centric video understanding. Extensive testing on the DocVideoQA dataset shows that DV-LLaMA significantly outperforms existing models. Weâ€™ll release the code and dataset to facilitate future research. </p>
<blockquote>
<p>è¿œç¨‹å·¥ä½œå’Œåœ¨çº¿è¯¾ç¨‹å·²æˆä¸ºçŸ¥è¯†ä¼ æ’­çš„é‡è¦æ–¹æ³•ï¼Œå¯¼è‡´å¤§é‡åŸºäºæ–‡æ¡£çš„æŒ‡ä»¤è§†é¢‘å‡ºç°ã€‚ä¸ä¼ ç»Ÿè§†é¢‘æ•°æ®é›†ä¸åŒï¼Œè¿™äº›è§†é¢‘ä¸»è¦å‘ˆç°ä¸°å¯Œçš„æ–‡æœ¬å›¾åƒå’ŒéŸ³é¢‘ï¼Œä¸è§†è§‰å†…å®¹ç´§å¯†ç›¸å…³çš„ä¿¡æ¯å¯†é›†ï¼Œéœ€è¦é«˜çº§çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®é›†å¯ç”¨æ€§å’Œå…¶å›ºæœ‰çš„å¤æ‚æ€§ï¼Œè¿™ä¸ªé¢†åŸŸä»ç„¶é²œæœ‰ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å¼•å…¥DocVideoQAä»»åŠ¡å’Œæ•°æ®é›†ï¼ŒåŒ…å«1454ä¸ªè·¨è¶Š23ä¸ªç±»åˆ«çš„è§†é¢‘ï¼Œæ€»æ—¶é•¿çº¦828å°æ—¶ã€‚æ•°æ®é›†ä½¿ç”¨æ‰‹åŠ¨å’ŒGPTç”Ÿæˆçš„é—®é¢˜ç­”æ¡ˆå¯¹è¿›è¡Œæ ‡æ³¨ï¼Œè¯„ä¼°æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€æ—¶é—´æ„è¯†å’Œæ¨¡æ€èåˆèƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å»ºç«‹åŸºå‡†çº¿ã€‚ä¸ºäº†åº”å¯¹ä»¥æ–‡æ¡£ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­çš„æ¨¡æ€ç†è§£æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DV-LLaMAè¿™ä¸€ç¨³å¥çš„è§†é¢‘LLMåŸºå‡†æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¤šæ ·çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®å¢å¼ºå•æ¨¡æ€ç‰¹å¾æå–ï¼Œå¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ¥åŠ å¼ºæ¨¡æ€èåˆã€‚é€šè¿‡å¾®è°ƒï¼ŒLLMå…·å¤‡äº†è§†å¬èƒ½åŠ›ï¼Œæå¤§åœ°æ”¹è¿›äº†å¯¹ä»¥æ–‡æ¡£ä¸ºä¸­å¿ƒçš„è§†é¢‘çš„ç†è§£ã€‚åœ¨DocVideoQAæ•°æ®é›†ä¸Šçš„å¹¿æ³›æµ‹è¯•è¡¨æ˜ï¼ŒDV-LLaMAæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬å°†å‘å¸ƒä»£ç å’Œæ•°æ®é›†ä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15887v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>éšç€è¿œç¨‹å·¥ä½œå’Œåœ¨çº¿è¯¾ç¨‹çš„æ™®åŠï¼Œæ–‡æ¡£å‹æ•™å­¦è§†é¢‘æˆä¸ºé‡è¦çš„çŸ¥è¯†ä¼ æ’­æ–¹å¼ã€‚æœ¬æ–‡é¦–æ¬¡å¼•å…¥DocVideoQAä»»åŠ¡å’Œæ•°æ®é›†ï¼ŒåŒ…å«1454ä¸ªè§†é¢‘å’Œçº¦828å°æ—¶çš„æ—¶é•¿ï¼Œæ ‡æ³¨æœ‰15.4ä¸‡ç»„é—®ç­”å¯¹ã€‚å»ºç«‹åŸºçº¿åï¼Œæå‡ºé’ˆå¯¹æ–‡æ¡£å‹è§†é¢‘çš„DV-LLaMAæ¨¡å‹ï¼Œé‡‡ç”¨å¤šæ ·æŒ‡ä»¤å¾®è°ƒæ•°æ®å’Œå¯¹æ¯”å­¦ä¹ å¼ºåŒ–æ¨¡æ€èåˆã€‚æ¨¡å‹åœ¨æ–‡æ¡£å‹è§†é¢‘ç†è§£ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>è¿œç¨‹å·¥ä½œå’Œåœ¨çº¿è¯¾ç¨‹æ¨åŠ¨äº†æ–‡æ¡£å‹æ•™å­¦è§†é¢‘çš„æ™®åŠã€‚</li>
<li>DocVideoQAæ•°æ®é›†åŒ…å«å¤§é‡æ–‡æ¡£å‹è§†é¢‘ï¼Œå¹¶æ ‡æ³¨æœ‰é—®ç­”å¯¹ä»¥è¯„ä¼°æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥DV-LLaMAæ¨¡å‹ä½œä¸ºæ–‡æ¡£å‹è§†é¢‘ç†è§£çš„åŸºçº¿æ–¹æ³•ã€‚</li>
<li>DV-LLaMAæ¨¡å‹é‡‡ç”¨å¤šæ ·æŒ‡ä»¤å¾®è°ƒæ•°æ®å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯å¢å¼ºæ¨¡æ€èåˆèƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç²¾ç»†è°ƒæ•´åœ¨æ–‡æ¡£å‹è§†é¢‘ç†è§£ä¸Šè¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3746f09e0d700edcbc7420c9e1f974fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-107a3f739d1a6a13f5e47e95ff9df104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2565216810abda4d187352db6fcb5840.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6014c3a4a8905403357be276ab58be1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa4b12e342a65c059af0ff0f7a702a6f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Leveraging-MoE-based-Large-Language-Model-for-Zero-Shot-Multi-Task-Semantic-Communication"><a href="#Leveraging-MoE-based-Large-Language-Model-for-Zero-Shot-Multi-Task-Semantic-Communication" class="headerlink" title="Leveraging MoE-based Large Language Model for Zero-Shot Multi-Task   Semantic Communication"></a>Leveraging MoE-based Large Language Model for Zero-Shot Multi-Task   Semantic Communication</h2><p><strong>Authors:Sin-Yu Huang, Renjie Liao, Vincent W. S. Wong</strong></p>
<p>Multi-task semantic communication (SC) can reduce the computational resources in wireless systems since retraining is not required when switching between tasks. However, existing approaches typically rely on task-specific embeddings to identify the intended task, necessitating retraining the entire model when given a new task. Consequently, this drives the need for a multi-task SC system that can handle new tasks without additional training, known as zero-shot learning. Inspired by the superior zero-shot capabilities of large language models (LLMs), we leverage pre-trained instruction-tuned LLMs, referred to as fine-tuned language net (FLAN), to improve the generalization capability. We incorporate a mixture-of-experts (MoE) architecture in the FLAN model and propose MoE-FLAN-SC architecture for multi-task SC systems. Our proposed MoE-FLAN-SC architecture can further improve the performance of FLAN-T5 model without increasing the computational cost. Moreover, we design a multi-task feature extraction module (FEM) which can adaptively extract relevant features across various tasks given the provided features and signal-to-noise ratio (SNR). Simulation results show that our proposed MoE-FLAN-SC architecture outperforms three state-of-the-art models in terms of the average accuracy on four different unseen tasks. </p>
<blockquote>
<p>å¤šä»»åŠ¡è¯­ä¹‰é€šä¿¡ï¼ˆSCï¼‰èƒ½å¤ŸèŠ‚çœæ— çº¿ç³»ç»Ÿä¸­çš„è®¡ç®—èµ„æºï¼Œå› ä¸ºä»»åŠ¡é—´åˆ‡æ¢æ—¶æ— éœ€é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„åµŒå…¥æ¥è¯†åˆ«æ„å›¾ä»»åŠ¡ï¼Œå½“ç»™å®šæ–°ä»»åŠ¡æ—¶éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ã€‚å› æ­¤ï¼Œè¿™å¼•å‘äº†å¯¹èƒ½å¤Ÿå¤„ç†æ–°ä»»åŠ¡è€Œæ— éœ€é¢å¤–è®­ç»ƒçš„å¤šä»»åŠ¡SCç³»ç»Ÿçš„éœ€æ±‚ï¼Œè¿™è¢«ç§°ä¸ºé›¶å°„å‡»å­¦ä¹ ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å“è¶Šé›¶å°„å‡»èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„æŒ‡ä»¤è°ƒæ•´LLMï¼Œç§°ä¸ºç²¾ç»†è°ƒæ•´è¯­è¨€ç½‘ç»œï¼ˆFLANï¼‰ï¼Œä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å°†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„èå…¥FLANæ¨¡å‹ä¸­ï¼Œå¹¶æå‡ºMoE-FLAN-SCæ¶æ„ç”¨äºå¤šä»»åŠ¡SCç³»ç»Ÿã€‚æˆ‘ä»¬æå‡ºçš„MoE-FLAN-SCæ¶æ„å¯ä»¥åœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹è¿›ä¸€æ­¥æé«˜FLAN-T5æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šä»»åŠ¡ç‰¹å¾æå–æ¨¡å—ï¼ˆFEMï¼‰ï¼Œè¯¥æ¨¡å—å¯ä»¥è‡ªé€‚åº”åœ°æå–ç»™å®šç‰¹å¾å’Œä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ä¸‹å„ç§ä»»åŠ¡çš„ç›¸å…³ç‰¹å¾ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MoE-FLAN-SCæ¶æ„åœ¨å››ä¸ªä¸åŒæœªè§ä»»åŠ¡ä¸Šçš„å¹³å‡å‡†ç¡®æ€§æ–¹é¢ä¼˜äºä¸‰ç§æœ€å…ˆè¿›æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15722v1">PDF</a> Accepted by ICC 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šä»»åŠ¡è¯­ä¹‰é€šä¿¡ï¼ˆSCï¼‰èƒ½å‡å°‘æ— çº¿ç³»ç»Ÿä¸­çš„è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡åµŒå…¥æ¥è¯†åˆ«ç›®æ ‡ä»»åŠ¡ï¼Œè¿™è¦æ±‚å¯¹æ–°ä»»åŠ¡è¿›è¡Œæ•´ä¸ªæ¨¡å‹çš„é‡æ–°è®­ç»ƒã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤„ç†æ–°ä»»åŠ¡è€Œæ— éœ€é¢å¤–è®­ç»ƒçš„å¤šä»»åŠ¡SCç³»ç»Ÿï¼Œå³é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡ºè‰²é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„æŒ‡ä»¤è°ƒæ•´LLMï¼ˆç§°ä¸ºç²¾ç»†è°ƒæ•´è¯­è¨€ç½‘ç»œï¼ˆFLANï¼‰ï¼‰æ¥æé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å°†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„èå…¥FLANæ¨¡å‹ï¼Œå¹¶æå‡ºMoE-FLAN-SCæ¶æ„ç”¨äºå¤šä»»åŠ¡SCç³»ç»Ÿã€‚è¯¥æ¶æ„èƒ½åœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œè¿›ä¸€æ­¥æé«˜FLAN-T5æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šä»»åŠ¡ç‰¹å¾æå–æ¨¡å—ï¼ˆFEMï¼‰ï¼Œè¯¥æ¨¡å—å¯ä»¥è‡ªé€‚åº”åœ°æå–å„ç§ä»»åŠ¡çš„ç›¸å…³ç‰¹å¾ï¼Œå¹¶è€ƒè™‘æä¾›çš„ç‰¹å¾å’Œä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MoE-FLAN-SCæ¶æ„åœ¨å››ä¸ªä¸åŒæœªè§ä»»åŠ¡ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä¼˜äºä¸‰ç§æœ€æ–°æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä»»åŠ¡è¯­ä¹‰é€šä¿¡(SC)èƒ½é€šè¿‡å‡å°‘é‡æ–°è®­ç»ƒçš„éœ€æ±‚æ¥èŠ‚çº¦æ— çº¿ç³»ç»Ÿä¸­çš„è®¡ç®—èµ„æºã€‚</li>
<li>ç°æœ‰æ–¹æ³•éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹æ¥å¤„ç†æ–°ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„çµæ´»æ€§ã€‚</li>
<li>éœ€è¦ä¸€ç§å…·å¤‡é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›çš„å¤šä»»åŠ¡SCç³»ç»Ÿï¼Œä»¥å¤„ç†æ–°ä»»åŠ¡è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„æŒ‡ä»¤è°ƒæ•´LLMï¼ˆFLANï¼‰æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œç»“åˆæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å½¢æˆMoE-FLAN-SCæ¶æ„ã€‚</li>
<li>MoE-FLAN-SCæ¶æ„èƒ½æé«˜æ€§èƒ½ä¸”ä¸ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚</li>
<li>å¼•å…¥å¤šä»»åŠ¡ç‰¹å¾æå–æ¨¡å—ï¼ˆFEMï¼‰ï¼Œèƒ½è‡ªé€‚åº”æå–ä¸åŒä»»åŠ¡çš„ç›¸å…³ç‰¹å¾å¹¶è€ƒè™‘ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d4a7add4ce7f829842a5d2d644d99b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ac83f6ab5cd9785f5cba1910eda3bb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe92b68231391ecc7ad906534068be2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d1f369b724789f119cec5941d99fb4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04e770da0e96338e2d6785093b505f1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca3cfe37918e796e5148de6d2477c106.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Neuronal-Activation-States-as-Sample-Embeddings-for-Data-Selection-in-Task-Specific-Instruction-Tuning"><a href="#Neuronal-Activation-States-as-Sample-Embeddings-for-Data-Selection-in-Task-Specific-Instruction-Tuning" class="headerlink" title="Neuronal Activation States as Sample Embeddings for Data Selection in   Task-Specific Instruction Tuning"></a>Neuronal Activation States as Sample Embeddings for Data Selection in   Task-Specific Instruction Tuning</h2><p><strong>Authors:Da Ma, Gonghu Shang, Zhi Chen, Libo Qin, Yijie Luo, Lei Pan, Shuai Fan, Lu Chen, Kai Yu</strong></p>
<p>Task-specific instruction tuning enhances the performance of large language models (LLMs) on specialized tasks, yet efficiently selecting relevant data for this purpose remains a challenge. Inspired by neural coactivation in the human brain, we propose a novel data selection method called NAS, which leverages neuronal activation states as embeddings for samples in the feature space. Extensive experiments show that NAS outperforms classical data selection methods in terms of both effectiveness and robustness across different models, datasets, and selection ratios. </p>
<blockquote>
<p>é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æŒ‡ä»¤å¾®è°ƒæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸“ä¸šåŒ–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä½†å¦‚ä½•æœ‰æ•ˆåœ°é€‰æ‹©ç›¸å…³æ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å—äººç±»å¤§è„‘ä¸­ç¥ç»ååŒæ¿€æ´»çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œåä¸ºNASï¼ˆç¥ç»æ¿€æ´»çŠ¶æ€ï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç¥ç»æ¿€æ´»çŠ¶æ€ä½œä¸ºç‰¹å¾ç©ºé—´ä¸­æ ·æœ¬çš„åµŒå…¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNASåœ¨æœ‰æ•ˆæ€§ã€é²æ£’æ€§æ–¹é¢ä¼˜äºç»å…¸çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œé€‚ç”¨äºä¸åŒçš„æ¨¡å‹ã€æ•°æ®é›†å’Œé€‰æ‹©æ¯”ä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15573v1">PDF</a> preprint</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æŒ‡ä»¤å¾®è°ƒå¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸“ä¸šåŒ–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä½†å¦‚ä½•é€‰æ‹©ç›¸å…³æ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å—åˆ°äººç±»å¤§è„‘ç¥ç»å…±æ¿€æ´»çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©æ–¹æ³•NASï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç¥ç»å…ƒæ¿€æ´»çŠ¶æ€ä½œä¸ºç‰¹å¾ç©ºé—´ä¸­æ ·æœ¬çš„åµŒå…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒNASåœ¨æœ‰æ•ˆæ€§ã€ç¨³å¥æ€§æ–¹é¢ä¼˜äºç»å…¸çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œä¸”åœ¨ä¸åŒæ¨¡å‹ã€æ•°æ®é›†å’Œé€‰æ‹©æ¯”ä¾‹ä¸Šéƒ½è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤å¾®è°ƒå¯¹LLMåœ¨ä¸“ä¸šåŒ–ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡è‡³å…³é‡è¦ã€‚</li>
<li>æ•°æ®é€‰æ‹©å¯¹äºLLMæ€§èƒ½çš„æå‡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>NASæ•°æ®é€‰æ‹©æ–¹æ³•åˆ©ç”¨ç¥ç»å…ƒæ¿€æ´»çŠ¶æ€ä½œä¸ºåµŒå…¥ï¼Œåœ¨ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œæ•°æ®é€‰æ‹©ã€‚</li>
<li>NASåœ¨æœ‰æ•ˆæ€§ã€ç¨³å¥æ€§æ–¹é¢ä¼˜äºç»å…¸çš„æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚</li>
<li>NASåœ¨ä¸åŒæ¨¡å‹ã€æ•°æ®é›†å’Œé€‰æ‹©æ¯”ä¾‹ä¸Šéƒ½æœ‰è‰¯å¥½çš„è¡¨ç°ã€‚</li>
<li>æœ¬ç ”ç©¶å—åˆ°äººç±»å¤§è„‘ç¥ç»å…±æ¿€æ´»çš„å¯å‘ï¼Œä½“ç°å‡ºäº†ç”Ÿç‰©çµæ„Ÿåœ¨è®¡ç®—é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-24d91ce961b36a9f3b9eb519b60a64d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac5680ba1e42593261a0afe5222cd0a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb0ce0e7c2125a1d12475380fc33d6e5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BigO-Bench-â€“-Can-LLMs-Generate-Code-with-Controlled-Time-and-Space-Complexity"><a href="#BigO-Bench-â€“-Can-LLMs-Generate-Code-with-Controlled-Time-and-Space-Complexity" class="headerlink" title="BigO(Bench) â€“ Can LLMs Generate Code with Controlled Time and Space   Complexity?"></a>BigO(Bench) â€“ Can LLMs Generate Code with Controlled Time and Space   Complexity?</h2><p><strong>Authors:Pierre Chambon, Baptiste Roziere, Benoit Sagot, Gabriel Synnaeve</strong></p>
<p>We introduce BigO(Bench), a novel coding benchmark designed to evaluate the capabilities of generative language models in understanding and generating code with specified time and space complexities. This benchmark addresses the gap in current evaluations that often overlook the ability of models to comprehend and produce code constrained by computational complexity. BigO(Bench) includes tooling to infer the algorithmic complexity of any Python function from profiling measurements, including human- or LLM-generated solutions. BigO(Bench) also includes of set of 3,105 coding problems and 1,190,250 solutions from Code Contests annotated with inferred (synthetic) time and space complexity labels from the complexity framework, as well as corresponding runtime and memory footprint values for a large set of input sizes. We present results from evaluating multiple state-of-the-art language models on this benchmark, highlighting their strengths and weaknesses in handling complexity requirements. In particular, token-space reasoning models are unrivaled in code generation but not in complexity understanding, hinting that they may not generalize well to tasks for which no reward was given at training time. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†BigO(Bench)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç¼–ç åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆå…·æœ‰ç‰¹å®šæ—¶é—´å’Œç©ºé—´å¤æ‚åº¦çš„ä»£ç çš„èƒ½åŠ›ã€‚è¿™ä¸ªåŸºå‡†æµ‹è¯•å¼¥è¡¥äº†å½“å‰è¯„ä¼°ä¸­çš„ç©ºç™½ï¼Œå½“å‰çš„è¯„ä¼°å¾€å¾€å¿½è§†äº†æ¨¡å‹ç†è§£å’Œäº§ç”Ÿå—è®¡ç®—å¤æ‚åº¦çº¦æŸçš„ä»£ç çš„èƒ½åŠ›ã€‚BigO(Bench)åŒ…æ‹¬ä»åˆ†ææµ‹é‡æ¨æ–­ä»»ä½•Pythonå‡½æ•°ç®—æ³•å¤æ‚åº¦çš„å·¥å…·ï¼ŒåŒ…æ‹¬äººç±»æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆã€‚BigO(Bench)è¿˜åŒ…æ‹¬ä¸€ç»„æ¥è‡ªä»£ç ç«èµ›çš„3105ä¸ªç¼–ç é—®é¢˜å’Œ1190250ä¸ªè§£å†³æ–¹æ¡ˆï¼Œè¿™äº›é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆè¢«æ ‡æ³¨äº†ä»å¤æ‚åº¦æ¡†æ¶æ¨æ–­å‡ºçš„ï¼ˆåˆæˆï¼‰æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦æ ‡ç­¾ï¼Œä»¥åŠå¯¹åº”çš„å¤§è§„æ¨¡è¾“å…¥å¤§å°çš„è¿è¡Œæ—¶é—´å’Œå†…å­˜å ç”¨å€¼ã€‚æˆ‘ä»¬åœ¨æ­¤åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†å¤šä¸ªæœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹çš„ç»“æœï¼Œçªå‡ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚åº¦è¦æ±‚æ–¹é¢çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè™½ç„¶åœ¨ä»£ç ç”Ÿæˆæ–¹é¢æ— ä¸ä¼¦æ¯”ï¼Œä½†åœ¨å¤æ‚æ€§ç†è§£æ–¹é¢ï¼Œæ ‡è®°ç©ºé—´æ¨ç†æ¨¡å‹å¯èƒ½å¹¶ä¸é€‚ç”¨äºé‚£äº›åœ¨è®­ç»ƒæ—¶æœªç»™äºˆå¥–åŠ±çš„ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15242v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§Oï¼ˆBenchï¼‰æ˜¯ä¸€ä¸ªæ–°å‹ç¼–ç åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆå…·æœ‰ç‰¹å®šæ—¶é—´å’Œç©ºé—´å¤æ‚åº¦çš„ä»£ç æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•å¼¥è¡¥äº†å½“å‰è¯„ä¼°ä¸­çš„ç©ºç™½ï¼Œå½“å‰çš„è¯„ä¼°å¸¸å¸¸å¿½è§†æ¨¡å‹ç†è§£å’Œç”Ÿæˆå—è®¡ç®—å¤æ‚åº¦çº¦æŸçš„ä»£ç çš„èƒ½åŠ›ã€‚å¤§Oï¼ˆBenchï¼‰åŒ…æ‹¬å·¥å…·ï¼Œå¯ä»¥ä»åˆ†ææµ‹é‡ä¸­æ¨æ–­ä»»ä½•Pythonå‡½æ•°çš„ç®—æ³•å¤æ‚åº¦ï¼ŒåŒ…æ‹¬äººç±»æˆ–LLMç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…æ‹¬ä¸€ç»„æ¥è‡ªä»£ç ç«èµ›çš„3105ä¸ªç¼–ç¨‹é—®é¢˜å’Œ1190250ä¸ªè§£å†³æ–¹æ¡ˆï¼Œè¿™äº›é—®é¢˜å’Œç­”æ¡ˆéƒ½è¢«æ ‡æ³¨äº†æ¨æ–­çš„ï¼ˆåˆæˆï¼‰æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦æ ‡ç­¾ä»¥åŠç›¸åº”çš„å¤§è§„æ¨¡è¾“å…¥é›†çš„è¿è¡Œæ—¶å†…å­˜å ç”¨å€¼ã€‚æˆ‘ä»¬åœ¨æ­¤åŸºå‡†ä¸Šè¯„ä¼°äº†å¤šä¸ªå…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼Œçªå‡ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚åº¦è¦æ±‚æ–¹é¢çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚å°¤å…¶æ˜¯ï¼Œä»¤ç‰Œç©ºé—´æ¨ç†æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢æ— ä¸ä¼¦æ¯”ï¼Œä½†åœ¨å¤æ‚æ€§ç†è§£æ–¹é¢å¹¶ä¸çªå‡ºï¼Œè¿™è¡¨æ˜å®ƒä»¬å¯èƒ½æ— æ³•å¾ˆå¥½åœ°æ¨å¹¿åˆ°è®­ç»ƒæ—¶æœªç»™äºˆå¥–åŠ±çš„ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BigO(Bench)æ˜¯ä¸€ä¸ªè¯„ä¼°è¯­è¨€æ¨¡å‹ç†è§£å’Œç”Ÿæˆå…·æœ‰ç‰¹å®šæ—¶é—´å’Œç©ºé—´å¤æ‚åº¦çš„ä»£ç èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å½“å‰è¯„ä¼°å­˜åœ¨å¿½ç•¥æ¨¡å‹ç†è§£å’Œç”Ÿæˆå—è®¡ç®—å¤æ‚åº¦çº¦æŸçš„ä»£ç çš„ç©ºç™½ã€‚</li>
<li>BigO(Bench)èƒ½ä»åˆ†ææµ‹é‡ä¸­æ¨æ–­Pythonå‡½æ•°çš„ç®—æ³•å¤æ‚åº¦ã€‚</li>
<li>å®ƒåŒ…å«æ¥è‡ªä»£ç ç«èµ›çš„ç¼–ç¨‹é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Œå¹¶æ ‡æ³¨äº†æ¨æ–­çš„æ—¶ç©ºå¤æ‚åº¦åŠè¿è¡Œæ—¶å†…å­˜å ç”¨ã€‚</li>
<li>ä»¤ç‰Œç©ºé—´æ¨ç†æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚æ€§ç†è§£ä¸Šä»æœ‰ä¸è¶³ã€‚</li>
<li>è¿™äº›æ¨¡å‹å¯èƒ½æ— æ³•å¾ˆå¥½åœ°æ¨å¹¿åˆ°è®­ç»ƒæ—¶æœªç»™äºˆå¥–åŠ±çš„ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e684ceab81e95163501705b5889c0d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcb74e6b45161e7ae0f66991f5a9bbab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2ac6b2712be4bffed8f40f70affa252.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a28e4c02b33560e67917ec0f5fa392d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c64e5a0ca769d0b8552c2e995fec262.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LegalCore-A-Dataset-for-Event-Coreference-Resolution-in-Legal-Documents"><a href="#LegalCore-A-Dataset-for-Event-Coreference-Resolution-in-Legal-Documents" class="headerlink" title="LegalCore: A Dataset for Event Coreference Resolution in Legal Documents"></a>LegalCore: A Dataset for Event Coreference Resolution in Legal Documents</h2><p><strong>Authors:Kangda Wei, Xi Shi, Jonathan Tong, Sai Ramana Reddy, Anandhavelu Natarajan, Rajiv Jain, Aparna Garimella, Ruihong Huang</strong></p>
<p>Recognizing events and their coreferential mentions in a document is essential for understanding semantic meanings of text. The existing research on event coreference resolution is mostly limited to news articles. In this paper, we present the first dataset for the legal domain, LegalCore, which has been annotated with comprehensive event and event coreference information. The legal contract documents we annotated in this dataset are several times longer than news articles, with an average length of around 25k tokens per document. The annotations show that legal documents have dense event mentions and feature both short-distance and super long-distance coreference links between event mentions. We further benchmark mainstream Large Language Models (LLMs) on this dataset for both event detection and event coreference resolution tasks, and find that this dataset poses significant challenges for state-of-the-art open-source and proprietary LLMs, which perform significantly worse than a supervised baseline. We will publish the dataset as well as the code. </p>
<blockquote>
<p>è¯†åˆ«å’Œè§£ææ–‡æ¡£ä¸­äº‹ä»¶åŠå…¶æ ¸å¿ƒå¼•ç”¨æåŠå¯¹äºç†è§£æ–‡æœ¬è¯­ä¹‰è‡³å…³é‡è¦ã€‚ç°æœ‰çš„äº‹ä»¶æ ¸å¿ƒå¼•ç”¨è§£æç ”ç©¶å¤§å¤šå±€é™äºæ–°é—»æ–‡ç« ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¨å‡ºäº†æ³•å¾‹é¢†åŸŸçš„æ•°æ®é›†LegalCoreï¼Œè¯¥æ•°æ®é›†å·²è¿›è¡Œäº†å…¨é¢çš„äº‹ä»¶å’Œäº‹ä»¶æ ¸å¿ƒå¼•ç”¨ä¿¡æ¯æ³¨é‡Šã€‚æˆ‘ä»¬åœ¨è¯¥æ•°æ®é›†ä¸­æ³¨é‡Šçš„æ³•å¾‹åˆåŒæ–‡æ¡£æ˜¯æ–°é—»æ–‡ç« çš„æ•°å€é•¿ï¼Œå¹³å‡æ¯ä¸ªæ–‡æ¡£çº¦åŒ…å«25,000ä¸ªä»¤ç‰Œã€‚æ³¨é‡Šè¡¨æ˜ï¼Œæ³•å¾‹æ–‡æ¡£å…·æœ‰å¯†é›†çš„äº‹ä»¶æåŠï¼Œå¹¶ä¸”äº‹ä»¶æåŠä¹‹é—´å…·æœ‰çŸ­è·ç¦»å’Œè¶…è¿œè·ç¦»çš„æ ¸å¿ƒå¼•ç”¨é“¾æ¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨æ­¤æ•°æ®é›†ä¸Šè¯„ä¼°ä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº‹ä»¶æ£€æµ‹å’Œäº‹ä»¶æ ¸å¿ƒå¼•ç”¨è§£æä»»åŠ¡ï¼Œå‘ç°è¯¥æ•°æ®é›†å¯¹æœ€æ–°çš„å¼€æºå’Œä¸“æœ‰LLMæ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œå®ƒä»¬çš„æ€§èƒ½æ˜¾è‘—å·®äºå—ç›‘ç£çš„åŸºçº¿ã€‚æˆ‘ä»¬å°†å‘å¸ƒæ•°æ®é›†ä»¥åŠä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12509v4">PDF</a> Need company internal approval before public release</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†äº‹ä»¶åŠå…¶æ ¸å¿ƒå¼•ç”¨åœ¨æ–‡æ¡£ä¸­çš„é‡è¦æ€§ï¼Œå¹¶å¼ºè°ƒå¯¹æ³•å¾‹é¢†åŸŸäº‹ä»¶æ ¸å¿ƒå¼•ç”¨è§£æçš„ç ”ç©¶ã€‚ç°æœ‰ç ”ç©¶å¤šå±€é™äºæ–°é—»æ–‡ç« ï¼Œè€Œæ³•å¾‹æ–‡æ¡£æ›´å¯†é›†åœ°æ¶‰åŠåˆ°äº‹ä»¶æåŠï¼Œå¹¶æœ‰å¤šç§å¤æ‚çš„æ ¸å¿ƒå¼•ç”¨é“¾æ¥ã€‚æœ¬ç ”ç©¶æ„å»ºäº†é¦–ä¸ªæ³•å¾‹é¢†åŸŸçš„æ ¸å¿ƒäº‹ä»¶æ•°æ®é›†â€”â€”LegalCoreï¼Œå¹¶æ ‡æ³¨äº†äº‹ä»¶åŠå…¶æ ¸å¿ƒå¼•ç”¨ä¿¡æ¯ã€‚è¯¥æ•°æ®é›†åŒ…å«çš„æ³•å¾‹åˆåŒæ–‡æ¡£é•¿åº¦è¿œè¶…æ–°é—»æ–‡ç« ï¼Œå¹³å‡æ¯ä¸ªæ–‡æ¡£çº¦å«25kä¸ªæ ‡è®°ã€‚åœ¨æ­¤æ•°æ®é›†ä¸Šè¿›è¡Œçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å°†å…¬å¸ƒæ•°æ®é›†åŠä»£ç ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äº‹ä»¶åŠå…¶æ ¸å¿ƒå¼•ç”¨çš„è¯†åˆ«å¯¹äºç†è§£æ–‡æœ¬è¯­ä¹‰è‡³å…³é‡è¦ã€‚</li>
<li>æ³•å¾‹é¢†åŸŸçš„äº‹ä»¶æ ¸å¿ƒå¼•ç”¨è§£æç ”ç©¶å°šæœªå¾—åˆ°å……åˆ†å…³æ³¨ã€‚</li>
<li>LegalCoreæ•°æ®é›†æ˜¯é¦–ä¸ªé’ˆå¯¹æ³•å¾‹é¢†åŸŸçš„æ ¸å¿ƒäº‹ä»¶æ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡æ ‡æ³¨çš„äº‹ä»¶åŠå…¶æ ¸å¿ƒå¼•ç”¨ä¿¡æ¯ã€‚</li>
<li>æ³•å¾‹æ–‡æ¡£ä¸­çš„äº‹ä»¶æåŠæ›´ä¸ºå¯†é›†ï¼Œå­˜åœ¨å¤šç§å¤æ‚çš„æ ¸å¿ƒå¼•ç”¨é“¾æ¥ã€‚</li>
<li>ä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´è¯¥æ•°æ®é›†çš„æ˜¾è‘—æŒ‘æˆ˜ï¼Œæ€§èƒ½è¿œä½äºç›‘ç£åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ•°æ®é›†å…¬å¼€å°†ä¿ƒè¿›å¯¹è¯¥é¢†åŸŸçš„æ·±å…¥ç ”ç©¶ä¸åº”ç”¨ã€‚</li>
<li>ç ”ç©¶å°†å…¬å¼€æ•°æ®é›†åŠä»£ç ï¼Œä¾¿äºåç»­ç ”ç©¶ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97b92f64b41cb0a1f6ae17bbfb1236bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bedd9e7460e75b356b8fb599c492da99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ad504dd4853c2fcaf06c60111a192c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a95d76ac9aa5fc8521ca7a6e21e3ed9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-838fae44b4a8fd2876b2c8234f063db2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00739e0e02e84d7a842ef544c521a40d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ReLearn-Unlearning-via-Learning-for-Large-Language-Models"><a href="#ReLearn-Unlearning-via-Learning-for-Large-Language-Models" class="headerlink" title="ReLearn: Unlearning via Learning for Large Language Models"></a>ReLearn: Unlearning via Learning for Large Language Models</h2><p><strong>Authors:Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang</strong></p>
<p>Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/unlearn">https://github.com/zjunlp/unlearn</a>. </p>
<blockquote>
<p>å½“å‰é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„é—å¿˜å­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–äºåå‘ä¼˜åŒ–æ¥é™ä½ç›®æ ‡ä»¤ç‰Œçš„æ¦‚ç‡ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨¡å¼ä¼šç ´ååç»­çš„ä»¤ç‰Œé¢„æµ‹ï¼Œé™ä½æ¨¡å‹æ€§èƒ½å’Œè¯­è¨€è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡è¿‡åˆ†å¼ºè°ƒä¸Šä¸‹æ–‡é—å¿˜ï¼Œè€Œä¸è¶³ä»¥è¯„ä¼°å“åº”çš„æµç•…æ€§å’Œç›¸å…³æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReLearnï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæœ‰æ•ˆé—å¿˜çš„æ•°æ®å¢å¼ºå’Œå¾®è°ƒç®¡é“ï¼Œä»¥åŠä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†çŸ¥è¯†é—å¿˜ç‡ï¼ˆKFRï¼‰å’ŒçŸ¥è¯†ä¿ç•™ç‡ï¼ˆKRRï¼‰æ¥è¡¡é‡çŸ¥è¯†å±‚é¢çš„ä¿ç•™æƒ…å†µï¼Œä»¥åŠè¯­è¨€å¾—åˆ†ï¼ˆLSï¼‰æ¥è¯„ä¼°ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReLearnæˆåŠŸå®ç°äº†æœ‰é’ˆå¯¹æ€§çš„é—å¿˜ï¼ŒåŒæ—¶ä¿ç•™äº†é«˜è´¨é‡çš„è¾“å‡ºã€‚é€šè¿‡æœºåˆ¶åˆ†æï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜äº†åå‘ä¼˜åŒ–æ˜¯å¦‚ä½•ç ´åè¿è´¯çš„æ–‡æœ¬ç”Ÿæˆçš„ï¼Œè€ŒReLearnèƒ½å¤Ÿä¿æŒè¿™ç§é‡è¦èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/unlearn%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjunlp/unlearnæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11190v2">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ReLearnæ–¹æ³•ï¼Œé‡‡ç”¨æ•°æ®å¢å¼ºå’Œå¾®è°ƒç®¡é“è¿›è¡Œæœ‰æ•ˆé—å¿˜ã€‚è¯¥æ–¹æ³•å…‹æœäº†åå‘ä¼˜åŒ–åœ¨çŸ¥è¯†é—å¿˜è¿‡ç¨‹ä¸­çš„ç¼ºç‚¹ï¼Œä¿ç•™äº†æ–‡æœ¬çš„è¿è´¯æ€§å’Œæ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§ç»¼åˆè¯„ä»·ä½“ç³»ï¼ŒåŒ…æ‹¬çŸ¥è¯†é—å¿˜ç‡ï¼ˆKFRï¼‰ã€çŸ¥è¯†ä¿ç•™ç‡ï¼ˆKRRï¼‰å’Œè¯­è¨€å­¦è¯„åˆ†ï¼ˆLSï¼‰ï¼Œæ—¨åœ¨æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹ç”Ÿæˆè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒReLearnæ–¹æ³•åœ¨æˆåŠŸå®ç°ç›®æ ‡é—å¿˜çš„åŒæ—¶ï¼Œä¿æŒäº†é«˜è´¨é‡è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„é—å¿˜æ–¹æ³•ä¸»è¦ä¾èµ–åå‘ä¼˜åŒ–æ¥é™ä½ç›®æ ‡è¯æ±‡æ¦‚ç‡ï¼Œä½†è¿™ç§æ–¹æ³•ä¼šç ´åæ–‡æœ¬çš„è¿è´¯æ€§å’Œæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ReLearnæ–¹æ³•é‡‡ç”¨æ•°æ®å¢å¼ºå’Œå¾®è°ƒç®¡é“è¿›è¡Œæ›´æœ‰æ•ˆçš„é—å¿˜ï¼Œé¿å…äº†åå‘ä¼˜åŒ–çš„ç¼ºç‚¹ã€‚</li>
<li>ç»¼åˆè¯„ä»·ä½“ç³»åŒ…æ‹¬çŸ¥è¯†é—å¿˜ç‡ï¼ˆKFRï¼‰ã€çŸ¥è¯†ä¿ç•™ç‡ï¼ˆKRRï¼‰å’Œè¯­è¨€å­¦è¯„åˆ†ï¼ˆLSï¼‰ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>ReLearnæ–¹æ³•æˆåŠŸå®ç°äº†ç›®æ ‡é—å¿˜ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡è¾“å‡ºã€‚</li>
<li>çŸ¥è¯†é—å¿˜ç‡çš„è¯„ä¼°å¯ä»¥æ›´å¥½åœ°è¡¡é‡æ¨¡å‹åœ¨é—å¿˜ç‰¹å®šçŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ReLearnæ–¹æ³•é€šè¿‡ä¿ç•™è¯­è¨€è¿è´¯æ€§ï¼Œæ”¹å–„äº†æ–‡æœ¬ç”Ÿæˆçš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6cc2fde7f3f4fd3b3d310a616a823f60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92904619907f3f442ef28194e265ec3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45f5eab8589e6e7907f2278998607069.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a530860130991a349ea1e6bb1a7d7c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-908142ac864b1fdbcdc23e5a5f92b50b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-428c7d42310230bc3899d33c252e8a62.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Is-Long-Context-All-You-Need-Leveraging-LLMâ€™s-Extended-Context-for-NL2SQL"><a href="#Is-Long-Context-All-You-Need-Leveraging-LLMâ€™s-Extended-Context-for-NL2SQL" class="headerlink" title="Is Long Context All You Need? Leveraging LLMâ€™s Extended Context for   NL2SQL"></a>Is Long Context All You Need? Leveraging LLMâ€™s Extended Context for   NL2SQL</h2><p><strong>Authors:Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.   In this work, we explore the performance and the latency trade-offs of the extended context window (a.k.a., long context) offered by Googleâ€™s state-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Googleâ€™s \textit{gemini-pro-1.5} achieve strong performances on various benchmark datasets without finetuning and expensive self-consistency based techniques. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ¨ç†èƒ½åŠ›çš„æ”¹è¿›å’Œä¸Šä¸‹æ–‡çª—å£çš„æ‰©å±•å¼€è¾Ÿäº†åˆ©ç”¨è¿™äº›å¼ºå¤§æ¨¡å‹çš„æ–°é€”å¾„ã€‚NL2SQLçš„æŒ‘æˆ˜åœ¨äºè‡ªç„¶è¯­è¨€é—®é¢˜æœ¬è´¨ä¸Šæ˜¯æ¨¡ç³Šçš„ï¼Œè€ŒSQLç”Ÿæˆéœ€è¦ç²¾ç¡®ç†è§£å¤æ‚çš„æ•°æ®ç»“æ„å’Œè¯­ä¹‰ã€‚è§£å†³è¿™ç§è¯­ä¹‰æ¨¡ç³Šé—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯æä¾›æ›´å¤šå’Œè¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†è°·æ­Œæœ€æ–°LLMï¼ˆåŒå­åº§-ä¸“ä¸šç‰ˆ1.5ï¼‰æä¾›çš„æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼ˆä¹Ÿç§°ä¸ºé•¿ä¸Šä¸‹æ–‡ï¼‰çš„æ€§èƒ½å’Œå»¶è¿Ÿæƒè¡¡ã€‚æˆ‘ä»¬ç ”ç©¶äº†å„ç§ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å½±å“ï¼ŒåŒ…æ‹¬åˆ—ç¤ºä¾‹å€¼ã€é—®é¢˜å’ŒSQLæŸ¥è¯¢å¯¹ã€ç”¨æˆ·æä¾›çš„æç¤ºã€SQLæ–‡æ¡£å’Œæ¨¡å¼ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹ç ”ç©¶æ‰©å±•ä¸Šä¸‹æ–‡çª—å£å’Œé¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä½•å¸®åŠ©NL2SQLç”Ÿæˆå·¥ä½œï¼ŒåŒæ—¶è€ƒè™‘å‡†ç¡®æ€§å’Œå»¶è¿Ÿæˆæœ¬ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé•¿ä¸Šä¸‹æ–‡LLMæ˜¯ç¨³å¥çš„ï¼Œä¸ä¼šåœ¨æ‰©å±•çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­è¿·å¤±æ–¹å‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åŸºäºè°·æ­ŒåŒå­åº§ä¸“ä¸šç‰ˆ-åŸºäºä¸Šä¸‹æ–‡çš„é•¿NL2SQLç®¡é“åœ¨å„ç§åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œæ— éœ€å¾®è°ƒå’Œä½¿ç”¨æ˜‚è´µçš„åŸºäºè‡ªæˆ‘ä¸€è‡´æ€§ç­‰æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12372v5">PDF</a> 13 pages, 6 figures, VLDB 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†èƒ½åŠ›å’Œä¸Šä¸‹æ–‡çª—å£æ‰©å±•æ–¹é¢ã€‚é’ˆå¯¹NL2SQLçš„è¯­ä¹‰æ¨¡ç³Šé—®é¢˜ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†é€šè¿‡æä¾›æ›´å……è¶³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥è§£å†³è¯¥é—®é¢˜çš„æ–¹æ³•ã€‚é€šè¿‡æ¢ç´¢è°·æ­Œå‰æ²¿LLMï¼ˆåŒå­åº§-ä¸“ä¸šç‰ˆï¼‰æä¾›çš„æ‰©å±•ä¸Šä¸‹æ–‡çª—å£çš„æ€§èƒ½å’Œå»¶è¿Ÿæƒè¡¡ï¼Œæœ¬ç ”ç©¶è€ƒè™‘äº†å¤šç§ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å½±å“ï¼ŒåŒ…æ‹¬åˆ—ç¤ºä¾‹å€¼ã€é—®ç­”å¯¹ã€ç”¨æˆ·æç¤ºã€SQLæ–‡æ¡£å’Œæ¨¡å¼ç­‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé•¿ä¸Šä¸‹æ–‡LLMåœ¨NL2SQLç”Ÿæˆæ–¹é¢æ—¢ç¨³å¥åˆé«˜æ•ˆï¼Œèƒ½å¤Ÿåœ¨å„ç§åŸºå‡†æ•°æ®é›†ä¸Šå®ç°å¼ºå¤§çš„æ€§èƒ½ï¼Œæ— éœ€å¾®è°ƒæˆ–æ˜‚è´µçš„è‡ªä¸€è‡´æ€§æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>ä¸Šä¸‹æ–‡çª—å£çš„æ‰©å±•å¢å¼ºäº†LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>NL2SQLçš„æŒ‘æˆ˜åœ¨äºè‡ªç„¶è¯­è¨€çš„å›ºæœ‰æ¨¡ç³Šæ€§å’Œå¯¹å¤æ‚æ•°æ®æ¶æ„ç²¾ç¡®ç†è§£çš„éœ€æ±‚ã€‚</li>
<li>æä¾›æ›´å¤šå……è¶³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ˜¯è§£å†³NL2SQLè¯­ä¹‰æ¨¡ç³Šé—®é¢˜çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç ”ç©¶äº†è°·æ­Œå‰æ²¿LLMï¼ˆåŒå­åº§-ä¸“ä¸šç‰ˆï¼‰åœ¨æ‰©å±•ä¸Šä¸‹æ–‡çª—å£æ–¹é¢çš„æ€§èƒ½å’Œå»¶è¿Ÿæƒè¡¡ã€‚</li>
<li>ç»¼åˆè€ƒè™‘äº†å¤šç§ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹NL2SQLç”Ÿæˆçš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13e3da97623e0ac78e4ef0df59a0bbe4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c434039c1c98a4a6b7f7d0679b4bf28a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-980303f450f3031ad0534f5d6562b1a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58fa67f98abfc5e41e9f3e1d80ad1e0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33d2b71e27900d911eff29d0577fc7a4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="When-Text-Embedding-Meets-Large-Language-Model-A-Comprehensive-Survey"><a href="#When-Text-Embedding-Meets-Large-Language-Model-A-Comprehensive-Survey" class="headerlink" title="When Text Embedding Meets Large Language Model: A Comprehensive Survey"></a>When Text Embedding Meets Large Language Model: A Comprehensive Survey</h2><p><strong>Authors:Zhijie Nie, Zhangchi Feng, Mingxin Li, Cunwang Zhang, Yanzhao Zhang, Dingkun Long, Richong Zhang</strong></p>
<p>Text embedding has become a foundational technology in natural language processing (NLP) during the deep learning era, driving advancements across a wide array of downstream tasks. While many natural language understanding challenges can now be modeled using generative paradigms and leverage the robust generative and comprehension capabilities of large language models (LLMs), numerous practical applications - such as semantic matching, clustering, and information retrieval - continue to rely on text embeddings for their efficiency and effectiveness. Therefore, integrating LLMs with text embeddings has become a major research focus in recent years. In this survey, we categorize the interplay between LLMs and text embeddings into three overarching themes: (1) LLM-augmented text embedding, enhancing traditional embedding methods with LLMs; (2) LLMs as text embedders, adapting their innate capabilities for high-quality embedding; and (3) Text embedding understanding with LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing recent works based on interaction patterns rather than specific downstream applications, we offer a novel and systematic overview of contributions from various research and application domains in the era of LLMs. Furthermore, we highlight the unresolved challenges that persisted in the pre-LLM era with pre-trained language models (PLMs) and explore the emerging obstacles brought forth by LLMs. Building on this analysis, we outline prospective directions for the evolution of text embedding, addressing both theoretical and practical opportunities in the rapidly advancing landscape of NLP. </p>
<blockquote>
<p>æ–‡æœ¬åµŒå…¥åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå·²æˆä¸ºä¸€é¡¹åŸºç¡€æŠ€æœ¯ï¼Œå¹¶åœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£æ¨åŠ¨äº†ä¼—å¤šä¸‹æ¸¸ä»»åŠ¡çš„è¿›æ­¥ã€‚è™½ç„¶ç°åœ¨å¾ˆå¤šè‡ªç„¶è¯­è¨€ç†è§£æŒ‘æˆ˜éƒ½å¯ä»¥é‡‡ç”¨ç”Ÿæˆæ€§èŒƒå¼è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¨³å¥ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¦‚è¯­ä¹‰åŒ¹é…ã€èšç±»å’Œä¿¡æ¯æ£€ç´¢ç­‰ä»ä¾èµ–äºæ–‡æœ¬åµŒå…¥çš„é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚å› æ­¤ï¼Œå°†LLMä¸æ–‡æœ¬åµŒå…¥ç›¸ç»“åˆå·²æˆä¸ºè¿‘å¹´æ¥çš„ä¸»è¦ç ”ç©¶é‡ç‚¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†LLMä¸æ–‡æœ¬åµŒå…¥çš„ç›¸äº’ä½œç”¨å½’çº³ä¸ºä¸‰ä¸ªä¸»è¦ä¸»é¢˜ï¼šï¼ˆ1ï¼‰LLMå¢å¼ºçš„æ–‡æœ¬åµŒå…¥ï¼Œå³ç”¨LLMå¢å¼ºä¼ ç»ŸåµŒå…¥æ–¹æ³•ï¼›ï¼ˆ2ï¼‰LLMä½œä¸ºæ–‡æœ¬åµŒå…¥å™¨ï¼Œåˆ©ç”¨å…¶å›ºæœ‰èƒ½åŠ›è¿›è¡Œé«˜è´¨é‡åµŒå…¥ï¼›ï¼ˆ3ï¼‰ä½¿ç”¨LLMç†è§£æ–‡æœ¬åµŒå…¥ï¼Œå€ŸåŠ©LLMåˆ†æå’Œè§£é‡ŠåµŒå…¥ã€‚æˆ‘ä»¬æ ¹æ®äº¤äº’æ¨¡å¼è€Œéç‰¹å®šçš„ä¸‹æ¸¸åº”ç”¨æ¥ç»„ç»‡è¿‘æœŸçš„å·¥ä½œï¼Œæä¾›äº†ä¸€ä¸ªå…³äºLLMæ—¶ä»£å„ç§ç ”ç©¶å’Œåº”ç”¨é¢†åŸŸè´¡çŒ®çš„æ–°é¢–ä¸”ç³»ç»Ÿçš„æ¦‚è¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰æ—¶ä»£æŒç»­å­˜åœ¨çš„æœªè§£å†³æŒ‘æˆ˜ï¼Œå¹¶æ¢è®¨äº†LLMå¸¦æ¥çš„æ–°å…´éšœç¢ã€‚åŸºäºè¿™äº›åˆ†æï¼Œæˆ‘ä»¬å±•æœ›äº†æ–‡æœ¬åµŒå…¥çš„æœªæ¥å‘å±•ï¼Œå¹¶é’ˆå¯¹NLPé¢†åŸŸçš„å¿«é€Ÿè¿›æ­¥æå‡ºç†è®ºå’Œå®é™…æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09165v3">PDF</a> Version 3: We added some latest works of LLM-based Embedders and   MLLM-based Embedders</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬åµŒå…¥åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå·²æˆä¸ºä¸€é¡¹åŸºç¡€æŠ€æœ¯ï¼Œå¹¶åœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£æ¨åŠ¨äº†ä¼—å¤šä¸‹æ¸¸ä»»åŠ¡çš„å‘å±•ã€‚è™½ç„¶è®¸å¤šè‡ªç„¶è¯­è¨€ç†è§£æŒ‘æˆ˜ç°åœ¨å¯ä»¥ä½¿ç”¨ç”Ÿæˆæ€§èŒƒå¼å»ºæ¨¡ï¼Œå¹¶å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¨³å¥ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ï¼Œä½†è®¸å¤šå®é™…åº”ç”¨ï¼ˆå¦‚è¯­ä¹‰åŒ¹é…ã€èšç±»å’Œä¿¡æ¯æ£€ç´¢ï¼‰ä»ç„¶ä¾èµ–äºæ–‡æœ¬åµŒå…¥çš„é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚å› æ­¤ï¼Œå°†LLMä¸æ–‡æœ¬åµŒå…¥ç›¸ç»“åˆå·²æˆä¸ºè¿‘å¹´æ¥çš„ç ”ç©¶é‡ç‚¹ã€‚æœ¬æ–‡å°†å…¶äº’åŠ¨åˆ†ç±»ä¸ºä¸‰å¤§ä¸»é¢˜ï¼šLLMå¢å¼ºçš„æ–‡æœ¬åµŒå…¥ã€LLMä½œä¸ºæ–‡æœ¬åµŒå…¥å™¨ä»¥åŠä½¿ç”¨LLMç†è§£æ–‡æœ¬åµŒå…¥ã€‚é€šè¿‡åŸºäºäº¤äº’æ¨¡å¼è€Œéç‰¹å®šä¸‹æ¸¸åº”ç”¨çš„ç»„ç»‡æ–¹å¼ï¼Œæœ¬æ–‡æä¾›äº†å¯¹LLMæ—¶ä»£å„ç§ç ”ç©¶åº”ç”¨é¢†åŸŸè´¡çŒ®çš„æ–°é¢–ä¸”ç³»ç»Ÿçš„æ¦‚è¿°ï¼Œå¹¶å¼ºè°ƒäº†ä¸é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰æ—¶ä»£å­˜åœ¨çš„æœªè§£å†³æŒ‘æˆ˜ä»¥åŠLLMå¸¦æ¥çš„æ–°å…´éšœç¢ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡å±•æœ›äº†æ–‡æœ¬åµŒå…¥çš„æœªæ¥å‘å±•ï¼Œå¹¶é’ˆå¯¹ç†è®ºå’Œå®è·µæœºä¼šæå‡ºäº†åœ¨å¿«é€Ÿå‘å±•çš„NLPé¢†åŸŸä¸­çš„å»ºè®®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åµŒå…¥åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæ˜¯è®¸å¤šä¸‹æ¸¸ä»»åŠ¡çš„åŸºç¡€æŠ€æœ¯ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ä¸ºè‡ªç„¶è¯­è¨€ç†è§£æŒ‘æˆ˜æä¾›äº†æ–°çš„å»ºæ¨¡æ–¹æ³•ã€‚</li>
<li>LLMä¸æ–‡æœ¬åµŒå…¥çš„ç»“åˆæˆä¸ºç ”ç©¶çƒ­ç‚¹ï¼Œä¸»è¦åŒ…æ‹¬LLMå¢å¼ºçš„æ–‡æœ¬åµŒå…¥ã€LLMä½œä¸ºæ–‡æœ¬åµŒå…¥å™¨ä»¥åŠä½¿ç”¨LLMç†è§£æ–‡æœ¬åµŒå…¥ä¸‰å¤§ä¸»é¢˜ã€‚</li>
<li>é€šè¿‡äº¤äº’æ¨¡å¼ç»„ç»‡ç ”ç©¶è´¡çŒ®ï¼Œæä¾›äº†ä¸€ä¸ªç³»ç»Ÿçš„æ¦‚è¿°ï¼Œæ¶µç›–äº†å„ç§ç ”ç©¶åº”ç”¨é¢†åŸŸã€‚</li>
<li>ä¸é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰æ—¶ä»£ç›¸æ¯”ï¼Œä»å­˜åœ¨æœªè§£å†³æŒ‘æˆ˜å’Œæ–°å…´éšœç¢ã€‚</li>
<li>æ–‡ç« å±•æœ›äº†æ–‡æœ¬åµŒå…¥çš„æœªæ¥å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è®ºå’Œå®è·µæœºä¼šæ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d3ea8e6c4cce7590a78ab4744c7adbc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7cf915f9bcc4c2a7279452e4e86655f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffd68e5456de6395c8732115d4b02cf2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dfe7865b453379e858bfd0597d05f78.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Immune-Improving-Safety-Against-Jailbreaks-in-Multi-modal-LLMs-via-Inference-Time-Alignment"><a href="#Immune-Improving-Safety-Against-Jailbreaks-in-Multi-modal-LLMs-via-Inference-Time-Alignment" class="headerlink" title="Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via   Inference-Time Alignment"></a>Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via   Inference-Time Alignment</h2><p><strong>Authors:Soumya Suvra Ghosal, Souradip Chakraborty, Vaibhav Singh, Tianrui Guan, Mengdi Wang, Ahmad Beirami, Furong Huang, Alvaro Velasquez, Dinesh Manocha, Amrit Singh Bedi</strong></p>
<p>With the widespread deployment of Multimodal Large Language Models (MLLMs) for visual-reasoning tasks, improving their safety has become crucial. Recent research indicates that despite training-time safety alignment, these models remain vulnerable to jailbreak attacks. In this work, we first highlight an important safety gap to describe that alignment achieved solely through safety training may be insufficient against jailbreak attacks. To address this vulnerability, we propose Immune, an inference-time defense framework that leverages a safe reward model through controlled decoding to defend against jailbreak attacks. Additionally, we provide a mathematical characterization of Immune, offering insights on why it improves safety against jailbreaks. Extensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal that Immune effectively enhances model safety while preserving the modelâ€™s original capabilities. For instance, against text-based jailbreak attacks on LLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared to the base MLLM and state-of-the-art defense strategy, respectively. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæé«˜å…¶å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œäº†å®‰å…¨å¯¹é½ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¼ºè°ƒäº†ä¸€ä¸ªé‡è¦çš„å®‰å…¨å·®è·ï¼Œè¯´æ˜ä»…é€šè¿‡å®‰å…¨è®­ç»ƒå®ç°çš„å¯¹é½å¯èƒ½ä¸è¶³ä»¥æŠµå¾¡è¶Šç‹±æ”»å‡»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ¼æ´ï¼Œæˆ‘ä»¬æå‡ºäº†Immuneï¼Œä¸€ä¸ªæ¨ç†æ—¶é—´é˜²å¾¡æ¡†æ¶ï¼Œå®ƒé€šè¿‡å—æ§è§£ç åˆ©ç”¨å®‰å…¨å¥–åŠ±æ¨¡å‹æ¥æŠµå¾¡è¶Šç‹±æ”»å‡»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹Immuneè¿›è¡Œäº†æ•°å­¦è¡¨å¾ï¼Œæä¾›äº†å…³äºå®ƒä¸ºä»€ä¹ˆèƒ½æé«˜æŠµå¾¡è¶Šç‹±æ”»å‡»å®‰å…¨æ€§çš„è§è§£ã€‚åœ¨å¤šç§è¶Šç‹±åŸºå‡†æµ‹è¯•ä¸Šå¯¹æœ€è¿‘çš„MLLMsè¿›è¡Œçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒImmuneæœ‰æ•ˆåœ°æé«˜äº†æ¨¡å‹çš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„åŸå§‹èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨é’ˆå¯¹LLaVA-1.6çš„æ–‡æœ¬è¶Šç‹±æ”»å‡»ä¸­ï¼Œä¸åŸºç¡€MLLMå’Œæœ€æ–°çš„é˜²å¾¡ç­–ç•¥ç›¸æ¯”ï¼ŒImmuneå°†æ”»å‡»æˆåŠŸç‡é™ä½äº†57.82%å’Œ16.78%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18688v3">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ä½¿å¾—å…¶å®‰å…¨æ€§å˜å¾—è‡³å…³é‡è¦ã€‚å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œäº†å®‰å…¨å¯¹é½ï¼Œä½†è¿™äº›æ¨¡å‹ä»å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»çš„å½±å“ã€‚æœ¬æ–‡é¦–å…ˆå¼ºè°ƒäº†ä»…é€šè¿‡å®‰å…¨åŸ¹è®­å®ç°çš„å®‰å…¨å¯¹é½å¯èƒ½ä¸è¶³ä»¥æŠµå¾¡è¶Šç‹±æ”»å‡»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ¼æ´ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºImmuneçš„æ¨ç†æ—¶é—´é˜²å¾¡æ¡†æ¶ï¼Œé€šè¿‡å—æ§è§£ç åˆ©ç”¨å®‰å…¨å¥–åŠ±æ¨¡å‹æ¥é˜²å¾¡è¶Šç‹±æ”»å‡»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æ•°å­¦ç‰¹å¾è¿›è¡Œäº†è¡¨å¾ï¼Œæ­ç¤ºäº†å®ƒä¸ºä½•èƒ½æé«˜å¯¹è¶Šç‹±çš„å®‰å…¨é˜²æŠ¤èƒ½åŠ›ã€‚å¯¹å¤šæ ·è¶Šç‹±åŸºå‡†çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒImmuneåœ¨æé«˜æ¨¡å‹å®‰å…¨æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„åŸå§‹èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå¯¹äºLLaVA-1.6ä¸Šçš„æ–‡æœ¬è¶Šç‹±æ”»å‡»ï¼ŒImmuneç›¸è¾ƒäºåŸºç¡€MLLMå’Œç°æœ‰æœ€å…ˆè¿›çš„é˜²å¾¡ç­–ç•¥ï¼Œåˆ†åˆ«å°†æ”»å‡»æˆåŠŸç‡é™ä½äº†57.82%å’Œ16.78%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´å®‰å…¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»çš„å½±å“ã€‚</li>
<li>å•çº¯é€šè¿‡è®­ç»ƒæ—¶çš„å®‰å…¨å¯¹é½å¯èƒ½ä¸è¶³ä»¥æŠµå¾¡è¶Šç‹±æ”»å‡»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºImmuneçš„æ¨ç†æ—¶é—´é˜²å¾¡æ¡†æ¶ï¼Œç»“åˆå®‰å…¨å¥–åŠ±æ¨¡å‹å’Œå—æ§è§£ç æ¥å¢å¼ºæ¨¡å‹å¯¹è¶Šç‹±æ”»å‡»çš„æŠµå¾¡èƒ½åŠ›ã€‚</li>
<li>Immuneæ¡†æ¶çš„æ•°å­¦è¡¨å¾æ­ç¤ºäº†å…¶æé«˜å®‰å…¨é˜²æŠ¤èƒ½åŠ›çš„åŸå› ã€‚</li>
<li>å¹¿æ³›è¯„ä¼°è¯æ˜ï¼ŒImmuneåœ¨æé«˜æ¨¡å‹å®‰å…¨æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„åŸå§‹æ€§èƒ½ã€‚</li>
<li>åœ¨LLaVA-1.6æ¨¡å‹ä¸Šè¿›è¡Œçš„æ–‡æœ¬è¶Šç‹±æ”»å‡»æµ‹è¯•è¡¨æ˜ï¼ŒImmuneç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å…·æœ‰æ›´é«˜çš„é˜²å¾¡æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f265bba44669ed8116354975182f7511.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07b74ad05f0ac8f3926e3c595a6c78c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19a133f0548e1c050c3769e99f171c86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64b553bd4f122d069e3b6330c62ef232.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="On-the-Consistency-of-Video-Large-Language-Models-in-Temporal-Comprehension"><a href="#On-the-Consistency-of-Video-Large-Language-Models-in-Temporal-Comprehension" class="headerlink" title="On the Consistency of Video Large Language Models in Temporal   Comprehension"></a>On the Consistency of Video Large Language Models in Temporal   Comprehension</h2><p><strong>Authors:Minjoon Jung, Junbin Xiao, Byoung-Tak Zhang, Angela Yao</strong></p>
<p>Video large language models (Video-LLMs) can temporally ground language queries and retrieve video moments. Yet, such temporal comprehension capabilities are neither well-studied nor understood. So we conduct a study on prediction consistency â€“ a key indicator for robustness and trustworthiness of temporal grounding. After the model identifies an initial moment within the video content, we apply a series of probes to check if the modelâ€™s responses align with this initial grounding as an indicator of reliable comprehension. Our results reveal that current Video-LLMs are sensitive to variations in video contents, language queries, and task settings, unveiling severe deficiencies in maintaining consistency. We further explore common prompting and instruction-tuning methods as potential solutions, but find that their improvements are often unstable. To that end, we propose event temporal verification tuning that explicitly accounts for consistency, and demonstrate significant improvements for both grounding and consistency. Our data and code are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/minjoong507/Consistency-of-Video-LLM">https://github.com/minjoong507/Consistency-of-Video-LLM</a>. </p>
<blockquote>
<p>è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰å¯ä»¥å°†è¯­è¨€æŸ¥è¯¢ä¸è§†é¢‘ç‰‡æ®µè¿›è¡Œæ—¶é—´ä¸Šçš„åŒ¹é…å¹¶æ£€ç´¢å‡ºå¯¹åº”çš„è§†é¢‘ç‰‡æ®µã€‚ç„¶è€Œï¼Œè¿™æ ·çš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹é¢„æµ‹ä¸€è‡´æ€§è¿›è¡Œäº†ç ”ç©¶ï¼Œè¿™æ˜¯ä¸€é¡¹å…³äºæ—¶é—´åŒ¹é…æ¨¡å‹ç¨³å¥æ€§å’Œå¯ä¿¡åº¦çš„å…³é”®æŒ‡æ ‡ã€‚æ¨¡å‹åœ¨è§†é¢‘å†…å®¹ä¸­è¯†åˆ«å‡ºä¸€ä¸ªåˆå§‹ç‰‡æ®µåï¼Œæˆ‘ä»¬åº”ç”¨ä¸€ç³»åˆ—æ¢é’ˆæ¥æ£€æŸ¥æ¨¡å‹çš„å“åº”æ˜¯å¦ä¸è¿™ä¸€åˆå§‹åŒ¹é…ç›¸ç¬¦ï¼Œä»¥æ­¤æ¥åˆ¤æ–­å…¶ç†è§£çš„å¯é æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„Video-LLMså¯¹è§†é¢‘å†…å®¹ã€è¯­è¨€æŸ¥è¯¢å’Œä»»åŠ¡è®¾ç½®çš„å˜åŠ¨éå¸¸æ•æ„Ÿï¼Œåœ¨ä¿æŒä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢è®¨äº†å¸¸è§çš„æç¤ºå’Œè°ƒæ•´æŒ‡ä»¤çš„æ–¹æ³•ä½œä¸ºå¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å‘ç°å…¶æ”¹è¿›å¾€å¾€ä¸ç¨³å®šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†äº‹ä»¶æ—¶é—´éªŒè¯è°ƒæ•´æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜ç¡®åœ°è€ƒè™‘äº†ä¸€è‡´æ€§ï¼Œå¹¶å±•ç¤ºäº†åœ¨åŒ¹é…å’Œä¸€è‡´æ€§æ–¹é¢çš„æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å·²å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://github.com/minjoong507/Consistency-of-Video-LLM%E3%80%82">https://github.com/minjoong507/Consistency-of-Video-LLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12951v2">PDF</a> Accepted to CVPRâ€™25</p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰å…·å¤‡å¯¹è¯­è¨€æŸ¥è¯¢è¿›è¡Œæ—¶é—´å®šä½å¹¶æ£€ç´¢è§†é¢‘ç‰‡æ®µçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶æ—¶é—´ç†è§£èƒ½åŠ›çš„æ·±å…¥ç ”ç©¶å’Œç†è§£å°šä¸è¶³ã€‚æœ¬ç ”ç©¶å…³æ³¨é¢„æµ‹ä¸€è‡´æ€§â€”â€”è¡¡é‡æ¨¡å‹æ—¶é—´å®šä½ç¨³å¥æ€§å’Œå¯ä¿¡åº¦çš„é‡è¦æŒ‡æ ‡ã€‚åœ¨æ¨¡å‹åˆæ­¥è¯†åˆ«è§†é¢‘å†…å®¹ä¸­çš„æŸä¸€æ—¶åˆ»åï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—æ¢é’ˆæµ‹è¯•å…¶å“åº”æ˜¯å¦ä¸åˆå§‹å®šä½ä¸€è‡´ï¼Œä»¥è¯„ä¼°å…¶ç†è§£èƒ½åŠ›å¯é æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œå½“å‰Video-LLMså¯¹è§†é¢‘å†…å®¹ã€è¯­è¨€æŸ¥è¯¢å’Œä»»åŠ¡è®¾ç½®çš„å˜åŠ¨éå¸¸æ•æ„Ÿï¼Œç»´æŒä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚å°è¯•å¸¸è§æç¤ºå’ŒæŒ‡ä»¤å¾®è°ƒæ–¹æ³•ä½œä¸ºæ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œä½†å‘ç°å…¶æ”¹å–„æ•ˆæœå¹¶ä¸ç¨³å®šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº‹ä»¶æ—¶é—´éªŒè¯è°ƒæ•´æ–¹æ³•ï¼Œä¸“é—¨è€ƒè™‘ä¸€è‡´æ€§å› ç´ ï¼Œå¹¶åœ¨å®šä½å’Œä¸€è‡´æ€§æ–¹é¢å–å¾—æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-LLMså…·æœ‰æ—¶é—´å®šä½è¯­è¨€æŸ¥è¯¢å’Œè§†é¢‘ç‰‡æ®µçš„èƒ½åŠ›ï¼Œä½†å¯¹å…¶æ—¶é—´ç†è§£èƒ½åŠ›çš„æ·±å…¥ç ”ç©¶å°šä¸è¶³ã€‚</li>
<li>é¢„æµ‹ä¸€è‡´æ€§æ˜¯è¡¡é‡Video-LLMsæ—¶é—´å®šä½ç¨³å¥æ€§å’Œå¯ä¿¡åº¦çš„é‡è¦æŒ‡æ ‡ã€‚</li>
<li>å½“å‰Video-LLMsåœ¨ç»´æŒä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œå¯¹è§†é¢‘å†…å®¹ã€è¯­è¨€æŸ¥è¯¢å’Œä»»åŠ¡è®¾ç½®çš„å˜åŠ¨éå¸¸æ•æ„Ÿã€‚</li>
<li>å¸¸è§æç¤ºå’ŒæŒ‡ä»¤å¾®è°ƒæ–¹æ³•å¯¹æ”¹å–„Video-LLMsçš„é¢„æµ‹ä¸€è‡´æ€§æ•ˆæœä¸ç¨³å®šã€‚</li>
<li>æå‡ºäº†äº‹ä»¶æ—¶é—´éªŒè¯è°ƒæ•´æ–¹æ³•ï¼Œä¸“é—¨è€ƒè™‘ä¸€è‡´æ€§å› ç´ ã€‚</li>
<li>äº‹ä»¶æ—¶é—´éªŒè¯è°ƒæ•´æ–¹æ³•åœ¨å®šä½å’Œä¸€è‡´æ€§æ–¹é¢å–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e3bebb8f109d4f16d2c0138ee645472d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-566e38b10848a74ba7deac0127486ce2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fe23913ff8b73b1964b8aab0cfb2876.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd26700dfd7f5360adc072c55dd927a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0412efc6bdfdb5716838898af05be6b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GPT-for-Games-An-Updated-Scoping-Review-2020-2024"><a href="#GPT-for-Games-An-Updated-Scoping-Review-2020-2024" class="headerlink" title="GPT for Games: An Updated Scoping Review (2020-2024)"></a>GPT for Games: An Updated Scoping Review (2020-2024)</h2><p><strong>Authors:Daijin Yang, Erica Kleinman, Casper Harteveld</strong></p>
<p>Due to GPTâ€™s impressive generative capabilities, its applications in games are expanding rapidly. To offer researchers a comprehensive understanding of the current applications and identify both emerging trends and unexplored areas, this paper introduces an updated scoping review of 177 articles, 122 of which were published in 2024, to explore GPTâ€™s potential for games. By coding and synthesizing the papers, we identify five prominent applications of GPT in current game research: procedural content generation, mixed-initiative game design, mixed-initiative gameplay, playing games, and game user research. Drawing on insights from these application areas and emerging research, we propose future studies should focus on expanding the technical boundaries of the GPT models and exploring the complex interaction dynamics between them and users. This review aims to illustrate the state of the art in innovative GPT applications in games, offering a foundation to enrich game development and enhance player experiences through cutting-edge AI innovations. </p>
<blockquote>
<p>ç”±äºGPTçš„ç”Ÿæˆèƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œå…¶åœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æ­£åœ¨è¿…é€Ÿæ‰©å±•ã€‚ä¸ºäº†ä¸ºç ”ç©¶äººå‘˜æä¾›å¯¹å½“å‰åº”ç”¨çš„å…¨é¢çš„ç†è§£ï¼Œå¹¶è¯†åˆ«æ–°å…´è¶‹åŠ¿å’Œæœªæ¢ç´¢çš„é¢†åŸŸï¼Œæœ¬æ–‡ä»‹ç»äº†å¯¹177ç¯‡æ–‡ç« çš„æœ€æ–°èŒƒå›´å®¡æŸ¥ï¼Œå…¶ä¸­122ç¯‡äº2024å¹´å‡ºç‰ˆï¼Œä»¥æ¢ç´¢GPTå¯¹æ¸¸æˆçš„æ½œåŠ›ã€‚é€šè¿‡å¯¹æ–‡ç« è¿›è¡Œç¼–ç å’Œåˆæˆï¼Œæˆ‘ä»¬ç¡®å®šäº†GPTåœ¨å½“å‰æ¸¸æˆç ”ç©¶ä¸­äº”ä¸ªä¸»è¦çš„åº”ç”¨ï¼šç¨‹åºå†…å®¹ç”Ÿæˆã€æ··åˆå€¡è®®æ¸¸æˆè®¾è®¡ã€æ··åˆå€¡è®®æ¸¸æˆç©æ³•ã€ç©æ¸¸æˆä»¥åŠæ¸¸æˆç”¨æˆ·ç ”ç©¶ã€‚åŸºäºè¿™äº›åº”ç”¨é¢†åŸŸå’Œæ–°å…´ç ”ç©¶çš„è§è§£ï¼Œæˆ‘ä»¬æå‡ºæœªæ¥çš„ç ”ç©¶åº”é›†ä¸­åœ¨æ‰©å±•GPTæ¨¡å‹çš„æŠ€æœ¯è¾¹ç•Œï¼Œå¹¶æ¢ç´¢å®ƒä»¬ä¸ç”¨æˆ·ä¹‹é—´å¤æ‚çš„äº¤äº’åŠ¨æ€ã€‚æœ¬ç»¼è¿°æ—¨åœ¨è¯´æ˜æ¸¸æˆä¸­åˆ›æ–°æ€§GPTåº”ç”¨çš„ç°çŠ¶ï¼Œé€šè¿‡æœ€å‰æ²¿çš„äººå·¥æ™ºèƒ½åˆ›æ–°ä¸ºæ¸¸æˆå¼€å‘å’Œå¢å¼ºç©å®¶ä½“éªŒæä¾›åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00308v2">PDF</a> Submitted to IEEE Transactions on Games</p>
<p><strong>Summary</strong><br>åŸºäºGPTçš„å‡ºè‰²ç”Ÿæˆèƒ½åŠ›ï¼Œå…¶åœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æ­£åœ¨è¿…é€Ÿæ‰©å±•ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æœ€æ–°çš„æ–‡çŒ®ç»¼è¿°ï¼Œæ¶µç›–äº†GPTåœ¨æ¸¸æˆé¢†åŸŸçš„æ½œåœ¨åº”ç”¨ï¼Œå¹¶å¯¹å½“å‰çš„ç ”ç©¶è¶‹åŠ¿å’Œæœªæ¥å‘å±•æ–¹å‘è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚æ–‡ç« é€šè¿‡åˆ†æå’Œåˆæˆå¤§é‡æ–‡çŒ®ï¼Œç¡®å®šäº†GPTåœ¨æ¸¸æˆç ”ç©¶ä¸­çš„äº”å¤§ä¸»è¦åº”ç”¨æ–¹å‘ï¼ŒåŒ…æ‹¬ç¨‹åºå†…å®¹ç”Ÿæˆã€æ··åˆä¸»åŠ¨æ¸¸æˆè®¾è®¡ã€æ··åˆä¸»åŠ¨æ¸¸æˆç©æ³•ã€æ¸¸æˆç©å®¶å’Œæ¸¸æˆç”¨æˆ·ç ”ç©¶ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶åº”å…³æ³¨GPTæ¨¡å‹çš„æŠ€æœ¯è¾¹ç•Œæ‰©å±•ä»¥åŠå…¶ä¸ç”¨æˆ·ä¹‹é—´çš„å¤æ‚äº¤äº’åŠ¨æ€ã€‚æœ¬æ–‡æ—¨åœ¨å±•ç¤ºGPTåœ¨æ¸¸æˆé¢†åŸŸçš„æœ€æ–°åº”ç”¨æ°´å¹³ï¼Œä¸ºé€šè¿‡å‰æ²¿äººå·¥æ™ºèƒ½åˆ›æ–°ä¸°å¯Œæ¸¸æˆå¼€å‘å’Œæå‡ç©å®¶ä½“éªŒæä¾›åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPTåœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æ­£åœ¨è¿…é€Ÿæ‰©å±•ã€‚</li>
<li>GPTä¸»è¦åº”ç”¨äºç¨‹åºå†…å®¹ç”Ÿæˆã€æ··åˆä¸»åŠ¨æ¸¸æˆè®¾è®¡ã€æ··åˆä¸»åŠ¨æ¸¸æˆç©æ³•ã€æ¸¸æˆç©å®¶å’Œæ¸¸æˆç”¨æˆ·ç ”ç©¶ç­‰é¢†åŸŸã€‚</li>
<li>å½“å‰ç ”ç©¶è¶‹åŠ¿å’Œæœªæ¥å‘å±•æ–¹å‘éœ€è¦æ›´æ·±å…¥çš„æ¢è®¨ã€‚</li>
<li>GPTæ¨¡å‹çš„æŠ€æœ¯è¾¹ç•Œæ‰©å±•æ˜¯æœªæ¥çš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚</li>
<li>GPTä¸ç”¨æˆ·çš„å¤æ‚äº¤äº’åŠ¨æ€éœ€è¦æ›´å¤šçš„ç ”ç©¶å…³æ³¨ã€‚</li>
<li>æœ¬æ–‡æ—¨åœ¨ä¸ºé€šè¿‡å‰æ²¿äººå·¥æ™ºèƒ½åˆ›æ–°ä¸°å¯Œæ¸¸æˆå¼€å‘å’Œæå‡ç©å®¶ä½“éªŒæä¾›åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d3fa25b45b66fb37d4505ab74d6dd97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b9d710f472312b64d204e348396268e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f62912a62aaa7d74c3eb81deadc90b3.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d96a848d2fab7dd231af034b0c52c753.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  Do Visual Imaginations Improve Vision-and-Language Navigation Agents?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7f1dde42d0ff60bdb5033de8d4720b30.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  Chain of Functions A Programmatic Pipeline for Fine-Grained Chart   Reasoning Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
