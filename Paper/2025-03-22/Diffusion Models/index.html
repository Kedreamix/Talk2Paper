<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  VerbDiff Text-Only Diffusion Models with Enhanced Interaction Awareness">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a6be095d67f3ecb8599584ca442a882b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-22-æ›´æ–°"><a href="#2025-03-22-æ›´æ–°" class="headerlink" title="2025-03-22 æ›´æ–°"></a>2025-03-22 æ›´æ–°</h1><h2 id="VerbDiff-Text-Only-Diffusion-Models-with-Enhanced-Interaction-Awareness"><a href="#VerbDiff-Text-Only-Diffusion-Models-with-Enhanced-Interaction-Awareness" class="headerlink" title="VerbDiff: Text-Only Diffusion Models with Enhanced Interaction Awareness"></a>VerbDiff: Text-Only Diffusion Models with Enhanced Interaction Awareness</h2><p><strong>Authors:SeungJu Cha, Kwanyoung Lee, Ye-Chan Kim, Hyunwoo Oh, Dong-Jin Kim</strong></p>
<p>Recent large-scale text-to-image diffusion models generate photorealistic images but often struggle to accurately depict interactions between humans and objects due to their limited ability to differentiate various interaction words. In this work, we propose VerbDiff to address the challenge of capturing nuanced interactions within text-to-image diffusion models. VerbDiff is a novel text-to-image generation model that weakens the bias between interaction words and objects, enhancing the understanding of interactions. Specifically, we disentangle various interaction words from frequency-based anchor words and leverage localized interaction regions from generated images to help the model better capture semantics in distinctive words without extra conditions. Our approach enables the model to accurately understand the intended interaction between humans and objects, producing high-quality images with accurate interactions aligned with specified verbs. Extensive experiments on the HICO-DET dataset demonstrate the effectiveness of our method compared to previous approaches. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆäº†é€¼çœŸçš„å›¾åƒï¼Œä½†ç”±äºå…¶åŒºåˆ†å„ç§äº’åŠ¨è¯çš„èƒ½åŠ›æœ‰é™ï¼Œå¾€å¾€éš¾ä»¥å‡†ç¡®æç»˜äººä¸ç‰©ä½“ä¹‹é—´çš„äº’åŠ¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VerbDiffæ¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å†…æ•æ‰å¾®å¦™äº’åŠ¨çš„éš¾é¢˜ã€‚VerbDiffæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡å‡å¼±äº’åŠ¨è¯ä¸ç‰©ä½“ä¹‹é—´çš„åè§ï¼Œå¢å¼ºäº†äº’åŠ¨çš„ç†è§£èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»åŸºäºé¢‘ç‡çš„é”šå®šè¯ä¸­åˆ†ç¦»å‡ºå„ç§äº’åŠ¨è¯ï¼Œå¹¶åˆ©ç”¨ç”Ÿæˆçš„å›¾åƒçš„å±€éƒ¨äº’åŠ¨åŒºåŸŸæ¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ç‹¬ç‰¹è¯æ±‡çš„è¯­ä¹‰ï¼Œè€Œæ— éœ€é¢å¤–çš„æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°ç†è§£äººç±»å’Œç‰©ä½“ä¹‹é—´çš„é¢„æœŸäº’åŠ¨ï¼Œç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œå…¶ä¸­çš„äº’åŠ¨ä¸æŒ‡å®šçš„åŠ¨è¯å‡†ç¡®å¯¹åº”ã€‚åœ¨HICO-DETæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”éå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16406v1">PDF</a> Accepted at CVPR 2025, code :   <a target="_blank" rel="noopener" href="https://github.com/SeungJuCha/VerbDiff.git">https://github.com/SeungJuCha/VerbDiff.git</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºVerbDiffçš„æ–°å‹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æç»˜äººç±»ä¸ç‰©ä½“ä¹‹é—´å¾®å¦™äº’åŠ¨æ—¶çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é€šè¿‡å‰Šå¼±äº’åŠ¨è¯æ±‡ä¸ç‰©ä½“ä¹‹é—´çš„åè§ï¼Œå¢å¼ºå¯¹äº’åŠ¨çš„ç†è§£ã€‚å®ƒåœ¨ç”Ÿæˆå›¾åƒæ—¶ï¼Œèƒ½å‡†ç¡®æ•æ‰ç‰¹å®šè¯æ±‡çš„è¯­ä¹‰ï¼Œå¹¶ä¾æ®æŒ‡å®šçš„åŠ¨è¯ç”Ÿæˆé«˜è´¨é‡ã€äº’åŠ¨å‡†ç¡®çš„å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VerbDiffæ¨¡å‹æ—¨åœ¨è§£å†³å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æç»˜äººç±»ä¸ç‰©ä½“äº’åŠ¨æ—¶çš„å±€é™ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å‰Šå¼±äº’åŠ¨è¯æ±‡ä¸ç‰©ä½“é—´çš„åè§ï¼Œæé«˜æ¨¡å‹å¯¹äº’åŠ¨çš„ç†è§£ã€‚</li>
<li>VerbDiffèƒ½å¤ŸåŒºåˆ†å„ç§äº’åŠ¨è¯æ±‡ï¼Œå¹¶ä»é¢‘ç‡é”šè¯ä¸­åˆ†ç¦»å‡ºæ¥ã€‚</li>
<li>åˆ©ç”¨ç”Ÿæˆçš„å›¾åƒä¸­çš„å±€éƒ¨äº’åŠ¨åŒºåŸŸï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°æ•æ‰ç‰¹å®šè¯æ±‡çš„è¯­ä¹‰ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸é¢å¤–è®¾å®šæ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®ç†è§£äººç±»ä¸ç‰©ä½“çš„é¢„æœŸäº’åŠ¨ã€‚</li>
<li>åœ¨HICO-DETæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œä¸å‰æœŸæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c42b0a7c495cd22ffa91b1004cb2bdda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7366a4163b88c69e4294c11cdad66e5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75f84a4164c474b596df410a7eb7bd8d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Scale-wise-Distillation-of-Diffusion-Models"><a href="#Scale-wise-Distillation-of-Diffusion-Models" class="headerlink" title="Scale-wise Distillation of Diffusion Models"></a>Scale-wise Distillation of Diffusion Models</h2><p><strong>Authors:Nikita Starodubcev, Denis Kuznedelev, Artem Babenko, Dmitry Baranchuk</strong></p>
<p>We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion processes to the implicit spectral autoregression. We suppose that DMs can initiate generation at lower data resolutions and gradually upscale the samples at each denoising step without loss in performance while significantly reducing computational costs. SwD naturally integrates this idea into existing diffusion distillation methods based on distribution matching. Also, we enrich the family of distribution matching approaches by introducing a novel patch loss enforcing finer-grained similarity to the target distribution. When applied to state-of-the-art text-to-image diffusion models, SwD approaches the inference times of two full resolution steps and significantly outperforms the counterparts under the same computation budget, as evidenced by automated metrics and human preference studies. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SwDï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„è§„æ¨¡è’¸é¦æ¡†æ¶ï¼Œå®ƒæœ‰æ•ˆåœ°é‡‡ç”¨äº†åŸºäºæ‰©æ•£çš„å‡ æ­¥ç”Ÿæˆå™¨çš„ä¸‹ä¸€å°ºåº¦é¢„æµ‹æ€æƒ³ã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒSwDçš„çµæ„Ÿæ¥æºäºæœ€è¿‘å…³äºæ‰©æ•£è¿‡ç¨‹ä¸éšå¼è°±è‡ªå›å½’çš„ç›¸å…³è§è§£ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒDMså¯ä»¥åœ¨è¾ƒä½çš„æ•°æ®åˆ†è¾¨ç‡ä¸‹å¼€å§‹ç”Ÿæˆï¼Œå¹¶åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­é€æ­¥æ”¾å¤§æ ·æœ¬ï¼Œè€Œä¸ä¼šæŸå¤±æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§é™ä½è®¡ç®—æˆæœ¬ã€‚SwDè‡ªç„¶åœ°å°†è¿™ä¸€æƒ³æ³•é›†æˆåˆ°åŸºäºåˆ†å¸ƒåŒ¹é…çš„ç°æœ‰æ‰©æ•£è’¸é¦æ–¹æ³•ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„è¡¥ä¸æŸå¤±æ–¹æ³•ï¼Œä½¿åˆ†å¸ƒåŒ¹é…æ–¹æ³•æ›´åŠ ä¸°å¯Œï¼Œè¯¥æ–¹æ³•å¼ºåˆ¶å¯¹ç›®æ ‡åˆ†å¸ƒçš„ç»†ç²’åº¦ç›¸ä¼¼æ€§ã€‚å½“åº”ç”¨äºæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ—¶ï¼ŒSwDæ¥è¿‘ä¸¤ä¸ªå…¨åˆ†è¾¨ç‡æ­¥éª¤çš„æ¨ç†æ—¶é—´ï¼Œå¹¶åœ¨ç›¸åŒçš„è®¡ç®—é¢„ç®—ä¸‹æ˜¾è‘—ä¼˜äºåŒç±»æ¨¡å‹ï¼Œè¿™ç”±è‡ªåŠ¨æŒ‡æ ‡å’Œäººç±»åå¥½ç ”ç©¶æ‰€è¯æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16397v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†SwDï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„è§„æ¨¡è’¸é¦æ¡†æ¶ï¼Œå®ƒæœ‰æ•ˆåœ°é‡‡ç”¨äº†ä¸‹ä¸€æ­¥é¢„æµ‹çš„æ€æƒ³ï¼Œç”¨äºåŸºäºæ‰©æ•£çš„å‡ æ­¥ç”Ÿæˆå™¨ã€‚SwDå—åˆ°æ‰©æ•£è¿‡ç¨‹ä¸éšå¼è°±è‡ªå›å½’ç›¸å…³çš„æ–°è§è§£çš„å¯å‘ï¼Œè®¤ä¸ºDMså¯ä»¥ä»è¾ƒä½çš„æ•°æ®åˆ†è¾¨ç‡å¼€å§‹ç”Ÿæˆï¼Œå¹¶åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­é€æ­¥ä¸Šé‡‡æ ·æ ·æœ¬ï¼ŒåŒæ—¶ä¸ä¼šæŸå¤±æ€§èƒ½ï¼Œå¹¶ä¸”èƒ½æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚SwDè‡ªç„¶åœ°å°†è¿™ä¸€æƒ³æ³•é›†æˆåˆ°åŸºäºåˆ†å¸ƒåŒ¹é…çš„ç°æœ‰æ‰©æ•£è’¸é¦æ–¹æ³•ä¸­ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„è¡¥ä¸æŸå¤±ï¼Œä»¥å®ç°å¯¹ç›®æ ‡åˆ†å¸ƒçš„ç²¾ç»†ç²’åº¦ç›¸ä¼¼æ€§ã€‚åº”ç”¨äºå…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ—¶ï¼ŒSwDæ¥è¿‘ä¸¤ä¸ªå…¨åˆ†è¾¨ç‡æ­¥éª¤çš„æ¨ç†æ—¶é—´ï¼Œå¹¶åœ¨ç›¸åŒçš„è®¡ç®—é¢„ç®—ä¸‹æ˜¾è‘—ä¼˜äºåŒç±»äº§å“ï¼Œè¿™ç”±è‡ªåŠ¨åº¦é‡æŒ‡æ ‡å’Œäººç±»åå¥½ç ”ç©¶æ‰€è¯æ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SwDæ˜¯ä¸€ä¸ªç”¨äºæ‰©æ•£æ¨¡å‹çš„è§„æ¨¡è’¸é¦æ¡†æ¶ï¼Œç»“åˆäº†ä¸‹ä¸€æ­¥é¢„æµ‹æ€æƒ³ï¼Œé€‚ç”¨äºåŸºäºæ‰©æ•£çš„å‡ æ­¥ç”Ÿæˆå™¨ã€‚</li>
<li>SwDå—åˆ°æ‰©æ•£è¿‡ç¨‹ä¸éšå¼è°±è‡ªå›å½’ç›¸å…³æ–°è§è§£çš„å¯å‘ï¼Œå¯ä»ä½åˆ†è¾¨ç‡æ•°æ®å¯åŠ¨ç”Ÿæˆï¼Œé€æ­¥ä¸Šé‡‡æ ·æ ·æœ¬ï¼Œä¿æŒæ€§èƒ½åŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>SwDæ•´åˆäº†ç°æœ‰åŸºäºåˆ†å¸ƒåŒ¹é…çš„æ‰©æ•£è’¸é¦æ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è¡¥ä¸æŸå¤±ï¼Œä»¥å¢å¼ºå¯¹ç›®æ ‡åˆ†å¸ƒçš„ç²¾ç»†ç²’åº¦ç›¸ä¼¼æ€§ã€‚</li>
<li>SwDåº”ç”¨äºå…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ—¶è¡¨ç°å‡ºè‰²ã€‚</li>
<li>SwDåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œæ¥è¿‘ä¸¤ä¸ªå…¨åˆ†è¾¨ç‡æ­¥éª¤çš„æ¨ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16397">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91136922319c76aedc67678f1b08609a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2beb21be65fd6cd64d6b5b6889401cb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34370b27271c616672ed65d65154081c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb03c1dcccf51cba833c4349da043475.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f9d1f733e8bd652a82e58e12a623335.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea7241dcb0ef4726bede86a77d80147f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba41c8456c5e19010a992021b332ceda.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Do-Visual-Imaginations-Improve-Vision-and-Language-Navigation-Agents"><a href="#Do-Visual-Imaginations-Improve-Vision-and-Language-Navigation-Agents" class="headerlink" title="Do Visual Imaginations Improve Vision-and-Language Navigation Agents?"></a>Do Visual Imaginations Improve Vision-and-Language Navigation Agents?</h2><p><strong>Authors:Akhil Perincherry, Jacob Krantz, Stefan Lee</strong></p>
<p>Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or imaginations, we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of around 1 point and up to 0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone. Code and data for our work can be found at <a target="_blank" rel="noopener" href="https://www.akhilperincherry.com/VLN-Imagine-website/">https://www.akhilperincherry.com/VLN-Imagine-website/</a>. </p>
<blockquote>
<p>è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»£ç†çš„ä»»åŠ¡æ˜¯ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨æœªçŸ¥ç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶æŒ‡ä»¤æ‰€éšå«çš„å­ç›®æ ‡çš„è§†è§‰è¡¨ç¤ºæ˜¯å¦å¯ä»¥ä½œä¸ºå¯¼èˆªçº¿ç´¢ï¼Œä»è€Œæé«˜å¯¼èˆªæ€§èƒ½ã€‚ä¸ºäº†åˆæˆè¿™äº›è§†è§‰è¡¨ç¤ºæˆ–æƒ³è±¡ï¼Œæˆ‘ä»¬åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå¯¹åˆ†æ®µæŒ‡ä»¤ä¸­åŒ…å«çš„æ ‡å¿—æ€§å‚è€ƒç‰©è¿›è¡Œå¤„ç†ã€‚è¿™äº›æƒ³è±¡ä¸ºVLNä»£ç†æä¾›äº†ä½œä¸ºæ ‡å¿—æ€§çº¿ç´¢çš„é™„åŠ æ¨¡å¼ï¼Œå¹¶æ·»åŠ äº†ä¸€ä¸ªè¾…åŠ©æŸå¤±ï¼Œä»¥æ˜ç¡®é¼“åŠ±å…¶ä¸ç›¸åº”çš„æŒ‡ä»£è¡¨è¾¾å¼ç›¸å…³è”ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä¸ä»…ä¾é è¯­è¨€æŒ‡ä»¤ç›¸æ¯”ï¼Œä»£ç†çš„æˆåŠŸç‡ï¼ˆSRï¼‰æé«˜äº†å¤§çº¦1ä¸ªç‚¹ï¼ŒæŒ‰é€†è·¯å¾„é•¿åº¦ï¼ˆSPLï¼‰è®¡ç®—çš„æˆåŠŸç‡æé«˜äº†é«˜è¾¾0.5ä¸ªç‚¹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åŠ å¼ºäº†è§†è§‰ç†è§£ã€‚æˆ‘ä»¬çš„å·¥ä½œç›¸å…³çš„ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://www.akhilperincherry.com/VLN-Imagine-website/%E6%89%BE%E5%88%B0%E3%80%82">https://www.akhilperincherry.com/VLN-Imagine-website/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16394v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä¸­ï¼Œé€šè¿‡æ–‡æœ¬æŒ‡ä»¤éšå«çš„å­ç›®æ ‡è§†è§‰è¡¨å¾ä½œä¸ºå¯¼èˆªçº¿ç´¢ï¼Œä»¥æé«˜å¯¼èˆªæ€§èƒ½çš„æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼ŒåŸºäºåˆ†æ®µæŒ‡ä»¤ä¸­çš„åœ°æ ‡å‚ç…§ç”Ÿæˆæƒ³è±¡å›¾åƒï¼Œå°†å…¶ä½œä¸ºVLNä»£ç†çš„é™„åŠ æ¨¡æ€ï¼Œä¸è¯­è¨€æŒ‡ä»¤ä¸€åŒå¼•å¯¼ä»£ç†è¿›è¡Œå¯¼èˆªã€‚å®éªŒç»“æœè¡¨æ˜æ˜¾ç¤ºï¼Œæ–°æ–¹æ³•çš„æˆåŠŸç‡æé«˜äº†çº¦ä¸€ç‚¹ï¼Œè·¯å¾„é•¿åº¦çš„æˆåŠŸç‡æé«˜äº†é›¶ç‚¹äº”ã€‚è¿™è¡¨æ˜ç»“åˆè§†è§‰ç†è§£ä¸è¯­è¨€æŒ‡ä»¤çš„æ–¹æ³•å¼ºåŒ–äº†è§†è§‰ç†è§£çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä¸»é¢˜ï¼šç ”ç©¶è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡éšå«å­ç›®æ ‡çš„è§†è§‰è¡¨å¾å¢å¼ºå¯¼èˆªæ€§èƒ½ã€‚</li>
<li>æ–¹æ³•ä»‹ç»ï¼šåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼ŒåŸºäºåˆ†æ®µæŒ‡ä»¤ä¸­çš„åœ°æ ‡å‚è€ƒç”Ÿæˆæƒ³è±¡å›¾åƒä½œä¸ºå¯¼èˆªçº¿ç´¢ã€‚</li>
<li>é™„åŠ æ¨¡æ€ï¼šæƒ³è±¡å›¾åƒè¢«ç”¨ä½œVLNä»£ç†çš„é¢å¤–è¾“å…¥æ¨¡æ€ï¼Œä¸è¯­è¨€æŒ‡ä»¤ç»“åˆä½¿ç”¨ã€‚</li>
<li>å®éªŒç»“æœï¼šæ–°æ–¹æ³•æé«˜äº†å¯¼èˆªæˆåŠŸç‡å’Œè·¯å¾„é•¿åº¦æˆåŠŸç‡ã€‚</li>
<li>å¯¹æ¯”ä¹‹å‰çš„ç ”ç©¶ï¼šè¯¥æ–¹æ³•å¼ºè°ƒäº†è§†è§‰ç†è§£çš„é‡è¦æ€§ï¼Œä¸ä¼ ç»Ÿçš„ä»…ä¾èµ–è¯­è¨€æŒ‡ä»¤çš„æ–¹æ³•å½¢æˆå¯¹æ¯”ã€‚</li>
<li>æ•°æ®å’Œä»£ç å…±äº«ï¼šç ”ç©¶å›¢é˜Ÿæä¾›äº†æ•°æ®å’Œä»£ç çš„åœ¨çº¿èµ„æºé“¾æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb46584406ff1e26bad83fd7febb8919.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13493dac607132e39644f8acfd42f235.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e268db33ca1ffacff89d6729c5ac06d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-864be04b887527462f7131f13467f0c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-334c3a387c341c6be8d05da8cfde1dc5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LaPIG-Cross-Modal-Generation-of-Paired-Thermal-and-Visible-Facial-Images"><a href="#LaPIG-Cross-Modal-Generation-of-Paired-Thermal-and-Visible-Facial-Images" class="headerlink" title="LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial   Images"></a>LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial   Images</h2><p><strong>Authors:Leyang Wang, Joice Lin</strong></p>
<p>The success of modern machine learning, particularly in facial translation networks, is highly dependent on the availability of high-quality, paired, large-scale datasets. However, acquiring sufficient data is often challenging and costly. Inspired by the recent success of diffusion models in high-quality image synthesis and advancements in Large Language Models (LLMs), we propose a novel framework called LLM-assisted Paired Image Generation (LaPIG). This framework enables the construction of comprehensive, high-quality paired visible and thermal images using captions generated by LLMs. Our method encompasses three parts: visible image synthesis with ArcFace embedding, thermal image translation using Latent Diffusion Models (LDMs), and caption generation with LLMs. Our approach not only generates multi-view paired visible and thermal images to increase data diversity but also produces high-quality paired data while maintaining their identity information. We evaluate our method on public datasets by comparing it with existing methods, demonstrating the superiority of LaPIG. </p>
<blockquote>
<p>ç°ä»£æœºå™¨å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨ç¿»è¯‘ç½‘ç»œæ–¹é¢çš„æˆåŠŸï¼Œåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé«˜è´¨é‡ã€é…å¯¹ã€å¤§è§„æ¨¡æ•°æ®é›†çš„å¯è·å¾—æ€§ã€‚ç„¶è€Œï¼Œè·å–è¶³å¤Ÿçš„æ•°æ®é€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚å—æ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒåˆæˆå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢çš„æœ€æ–°æˆåŠŸå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºLLMè¾…åŠ©é…å¯¹å›¾åƒç”Ÿæˆï¼ˆLaPIGï¼‰çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨LLMç”Ÿæˆçš„æ ‡é¢˜æ„å»ºå…¨é¢ã€é«˜è´¨é‡é…å¯¹çš„å¯è§å…‰å’Œçƒ­å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼šä½¿ç”¨ArcFaceåµŒå…¥çš„å¯è§å›¾åƒåˆæˆã€ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„çƒ­å›¾åƒç¿»è¯‘ä»¥åŠä½¿ç”¨LLMçš„æ ‡é¢˜ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ç”Ÿæˆå¤šè§†è§’é…å¯¹å¯è§å…‰å’Œçƒ­å›¾åƒä»¥å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œè€Œä¸”è¿˜åœ¨ä¿æŒèº«ä»½ä¿¡æ¯çš„çŠ¶æ€ä¸‹äº§ç”Ÿé«˜è´¨é‡é…å¯¹æ•°æ®ã€‚æˆ‘ä»¬åœ¨å…¬å…±æ•°æ®é›†ä¸Šé€šè¿‡å°†å…¶ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œå¯¹æ¯”è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†LaPIGçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16376v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°ä»£æœºå™¨å­¦ä¹ åœ¨é¢éƒ¨ç¿»è¯‘ç½‘ç»œæ–¹é¢çš„æˆåŠŸé«˜åº¦ä¾èµ–äºé«˜è´¨é‡ã€é…å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†çš„å¯è·å¾—æ€§ã€‚ç„¶è€Œï¼Œè·å–è¶³å¤Ÿçš„æ•°æ®å¾€å¾€å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚å—æ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒåˆæˆå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢çš„æˆåŠŸçš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLLMè¾…åŠ©é…å¯¹å›¾åƒç”Ÿæˆï¼ˆLaPIGï¼‰çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨LLMç”Ÿæˆçš„æ ‡é¢˜æ„å»ºå…¨é¢ã€é«˜è´¨é‡é…å¯¹çš„å¯è§å…‰å’Œçƒ­å›¾åƒã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼šä½¿ç”¨ArcFaceåµŒå…¥è¿›è¡Œå¯è§å›¾åƒåˆæˆï¼Œä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰è¿›è¡Œçƒ­å›¾åƒç¿»è¯‘ï¼Œä»¥åŠä½¿ç”¨LLMè¿›è¡Œæ ‡é¢˜ç”Ÿæˆã€‚è¯¥æ–¹æ³•ä¸ä»…ç”Ÿæˆå¤šè§†è§’é…å¯¹å¯è§å…‰å’Œçƒ­å›¾åƒä»¥å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œè€Œä¸”ç”Ÿæˆé«˜è´¨é‡é…å¯¹æ•°æ®çš„åŒæ—¶ä¿æŒå…¶èº«ä»½ä¿¡æ¯ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒLaPIGä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£æœºå™¨å­¦ä¹ çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé«˜è´¨é‡ã€é…å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>è·å–è¶³å¤Ÿçš„æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒåˆæˆæ–¹é¢çš„æˆåŠŸä¸ºè§£å†³é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
<li>æå‡ºäº†åä¸ºLLMè¾…åŠ©é…å¯¹å›¾åƒç”Ÿæˆï¼ˆLaPIGï¼‰çš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>LaPIGæ¡†æ¶åŒ…æ‹¬å¯è§å›¾åƒåˆæˆã€çƒ­å›¾åƒç¿»è¯‘å’Œæ ‡é¢˜ç”Ÿæˆä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ã€‚</li>
<li>LaPIGèƒ½å¤Ÿç”Ÿæˆå¤šè§†è§’é…å¯¹å›¾åƒï¼Œå¢åŠ æ•°æ®å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96b8777b07b88bc21179988f93d98e12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa64d1ba110ac4383c0c0334af3d1f0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b78173cdb01b03f3a9314f192627e972.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5df613a31fcd249d5d5072355c031678.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Ultra-Resolution-Adaptation-with-Ease"><a href="#Ultra-Resolution-Adaptation-with-Ease" class="headerlink" title="Ultra-Resolution Adaptation with Ease"></a>Ultra-Resolution Adaptation with Ease</h2><p><strong>Authors:Ruonan Yu, Songhua Liu, Zhenxiong Tan, Xinchao Wang</strong></p>
<p>Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed \emph{URAE}. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, \textit{i.e.}, setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available \href{<a target="_blank" rel="noopener" href="https://github.com/Huage001/URAE%7D%7Bhere%7D">https://github.com/Huage001/URAE}{here}</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­è®­ç»ƒæ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»æ•°æ®å’Œå‚æ•°æ•ˆç‡ä¸¤ä¸ªå…³é”®è§’åº¦æ¢è®¨äº†è¿™ä¸€å®é™…é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€å¥—ç”¨äºè¶…åˆ†è¾¨ç‡é€‚åº”çš„å…³é”®æŒ‡å—ï¼Œç§°ä¸ºURAEã€‚åœ¨æ•°æ®æ•ˆç‡æ–¹é¢ï¼Œæˆ‘ä»¬ç†è®ºå’Œå®è¯åœ°è¯æ˜ï¼ŒæŸäº›æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®å¯ä»¥æ˜¾è‘—ä¿ƒè¿›è®­ç»ƒæ”¶æ•›ã€‚åœ¨å‚æ•°æ•ˆç‡æ–¹é¢ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå½“æ²¡æœ‰åˆæˆæ•°æ®æ—¶ï¼Œè°ƒæ•´æƒé‡çŸ©é˜µçš„å°ç»„ä»¶ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„ä½ç§©é€‚é…å™¨ï¼Œå¯ä»¥åœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œå¯¹äºåˆ©ç”¨æŒ‡å¯¼è’¸é¦çš„æ¨¡å‹ï¼ˆå¦‚FLUXï¼‰ï¼Œæˆ‘ä»¬è¡¨æ˜ç¦ç”¨æ— åˆ†ç±»æŒ‡å¯¼è‡³å…³é‡è¦ï¼Œå³åœ¨é€‚åº”è¿‡ç¨‹ä¸­å°†æŒ‡å¯¼æ¯”ä¾‹è®¾ç½®ä¸º1ã€‚å¤§é‡å®éªŒéªŒè¯ï¼ŒURAEä»…ä½¿ç”¨3Kæ ·æœ¬å’Œ2Kè¿­ä»£å°±å®ç°äº†ä¸æœ€æ–°å¼€æºæ¨¡å‹FLUX1.1 Pro Ultraç›¸å½“çš„2Kç”Ÿæˆæ€§èƒ½ï¼ŒåŒæ—¶ä¸º4Kåˆ†è¾¨ç‡ç”Ÿæˆæ ‘ç«‹äº†æ–°åŸºå‡†ã€‚ä»£ç å¯åœ¨æ­¤å¤„è·å–ï¼š[<a target="_blank" rel="noopener" href="https://github.com/Huage001/URAE]">https://github.com/Huage001/URAE]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16322v1">PDF</a> Technical Report. Codes are available   \href{<a target="_blank" rel="noopener" href="https://github.com/Huage001/URAE%7D%7Bhere%7D">https://github.com/Huage001/URAE}{here}</a></p>
<p><strong>Summary</strong><br>     æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹è¿›è¡Œé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆçš„æ¨¡å‹è®­ç»ƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡ä»æ•°æ®å’Œå‚æ•°æ•ˆç‡ä¸¤ä¸ªå…³é”®è§’åº¦æ¢è®¨äº†è¿™ä¸€é—®é¢˜ï¼Œå¹¶æå‡ºäº†è¶…åˆ†è¾¨ç‡é€‚åº”çš„æŒ‡å¯¼åŸåˆ™URAEã€‚åœ¨æ•°æ®æ•ˆç‡æ–¹é¢ï¼Œæœ¬æ–‡è¯å®åˆæˆæ•°æ®å¯ä»¥ä¿ƒè¿›è®­ç»ƒæ”¶æ•›ã€‚åœ¨å‚æ•°æ•ˆç‡æ–¹é¢ï¼Œå½“æ²¡æœ‰åˆæˆæ•°æ®æ—¶ï¼Œè°ƒæ•´æƒé‡çŸ©é˜µçš„å°ç»„ä»¶æ¯”ä½¿ç”¨ä½é˜¶é€‚é…å™¨æ›´æœ‰æ•ˆã€‚å¯¹äºä½¿ç”¨æŒ‡å¯¼è’¸é¦çš„æ¨¡å‹ï¼ˆå¦‚FLUXï¼‰ï¼Œç¦ç”¨æ— åˆ†ç±»æŒ‡å¯¼è‡³å…³é‡è¦ã€‚å®éªŒè¯æ˜ï¼ŒURAEåœ¨ä»…ä½¿ç”¨3Kæ ·æœ¬å’Œ2Kè¿­ä»£çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸æœ€æ–°å¼€æºæ¨¡å‹FLUX 1.1 Pro Ultraç›¸å½“çš„2Kç”Ÿæˆæ€§èƒ½ï¼ŒåŒæ—¶æ ‘ç«‹äº†4Kåˆ†è¾¨ç‡ç”Ÿæˆçš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´æ•°æ®å’Œè®¡ç®—èµ„æºæœ‰é™çš„æŒ‘æˆ˜ã€‚</li>
<li>URAEæŒ‡å¯¼åŸåˆ™ä»æ•°æ®å’Œå‚æ•°æ•ˆç‡ä¸¤ä¸ªè§’åº¦è§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>åˆæˆæ•°æ®å¯ä»¥ä¿ƒè¿›è®­ç»ƒæ”¶æ•›ã€‚</li>
<li>åœ¨æ²¡æœ‰åˆæˆæ•°æ®æ—¶ï¼Œè°ƒæ•´æƒé‡çŸ©é˜µçš„å°ç»„ä»¶æ¯”ä½¿ç”¨ä½é˜¶é€‚é…å™¨æ›´æœ‰æ•ˆã€‚</li>
<li>å¯¹äºä½¿ç”¨æŒ‡å¯¼è’¸é¦çš„æ¨¡å‹ï¼Œç¦ç”¨æ— åˆ†ç±»æŒ‡å¯¼æ˜¯å…³é”®ã€‚</li>
<li>URAEåœ¨ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬å’Œè¿­ä»£çš„æƒ…å†µä¸‹è¾¾åˆ°äº†å…ˆè¿›çš„ç”Ÿæˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15eee09871636128334035e16310b62c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f03d9ba40132d223058d7c4c9b97e0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e53c0f89fff15872d29cd1005031432.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f97c3e42eace8ae17780d98bce6914b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf9899b50727b742b96cc61c12e9262.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Shining-Yourself-High-Fidelity-Ornaments-Virtual-Try-on-with-Diffusion-Model"><a href="#Shining-Yourself-High-Fidelity-Ornaments-Virtual-Try-on-with-Diffusion-Model" class="headerlink" title="Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion   Model"></a>Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion   Model</h2><p><strong>Authors:Yingmao Miao, Zhanpeng Huang, Rui Han, Zibin Wang, Chenhao Lin, Chao Shen</strong></p>
<p>While virtual try-on for clothes and shoes with diffusion models has gained attraction, virtual try-on for ornaments, such as bracelets, rings, earrings, and necklaces, remains largely unexplored. Due to the intricate tiny patterns and repeated geometric sub-structures in most ornaments, it is much more difficult to guarantee identity and appearance consistency under large pose and scale variances between ornaments and models. This paper proposes the task of virtual try-on for ornaments and presents a method to improve the geometric and appearance preservation of ornament virtual try-ons. Specifically, we estimate an accurate wearing mask to improve the alignments between ornaments and models in an iterative scheme alongside the denoising process. To preserve structure details, we further regularize attention layers to map the reference ornament mask to the wearing mask in an implicit way. Experimental results demonstrate that our method successfully wears ornaments from reference images onto target models, handling substantial differences in scale and pose while preserving identity and achieving realistic visual effects. </p>
<blockquote>
<p>è™½ç„¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè¡£ç‰©å’Œé‹å­çš„è™šæ‹Ÿè¯•ç©¿å·²ç»å…·æœ‰å¸å¼•åŠ›ï¼Œä½†é¦–é¥°ï¼ˆå¦‚æ‰‹é•¯ã€æˆ’æŒ‡ã€è€³ç¯å’Œé¡¹é“¾ç­‰ï¼‰çš„è™šæ‹Ÿè¯•ç©¿ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç”±äºå¤§å¤šæ•°é¦–é¥°å…·æœ‰å¤æ‚çš„å°å›¾æ¡ˆå’Œé‡å¤çš„å‡ ä½•å­ç»“æ„ï¼Œå› æ­¤åœ¨é¦–é¥°å’Œæ¨¡å‹ä¹‹é—´çš„å¤§å§¿æ€å’Œå°ºåº¦å·®å¼‚ä¸‹ï¼Œä¿è¯èº«ä»½å’Œå¤–è§‚ä¸€è‡´æ€§è¦å›°éš¾å¾—å¤šã€‚æœ¬æ–‡æå‡ºäº†é¦–é¥°çš„è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›é¦–é¥°è™šæ‹Ÿè¯•ç©¿çš„å‡ ä½•å’Œå¤–è§‚ä¿ç•™æ€§çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¼°è®¡ä¸€ä¸ªç²¾ç¡®ä½©æˆ´é¢å…·ï¼Œä»¥åœ¨è¿­ä»£æ–¹æ¡ˆä¸­æ”¹è¿›é¦–é¥°å’Œæ¨¡å‹ä¹‹é—´çš„å¯¹é½ï¼ŒåŒæ—¶è¾…ä»¥å»å™ªè¿‡ç¨‹ã€‚ä¸ºäº†ä¿ç•™ç»“æ„ç»†èŠ‚ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¯¹æ³¨æ„åŠ›å±‚è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥éšå¼çš„æ–¹å¼å°†å‚è€ƒé¦–é¥°é¢å…·æ˜ å°„åˆ°ä½©æˆ´é¢å…·ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å°†å‚è€ƒå›¾åƒä¸­çš„é¦–é¥°æˆ´åœ¨äº†ç›®æ ‡æ¨¡å‹ä¸Šï¼Œå¤„ç†äº†å¤§é‡çš„å°ºå¯¸å’Œå§¿æ€å·®å¼‚ï¼ŒåŒæ—¶ä¿æŒäº†èº«ä»½å¹¶å®ç°äº†é€¼çœŸçš„è§†è§‰æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16065v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨è™šæ‹Ÿè¯•è¡£é¢†åŸŸçš„åº”ç”¨å·²ç»å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†å¯¹äºé¦–é¥°ç­‰é¥°å“çš„è™šæ‹Ÿè¯•æˆ´ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†é’ˆå¯¹é¦–é¥°çš„è™šæ‹Ÿè¯•æˆ´ä»»åŠ¡ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§æ”¹è¿›å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚ä¿æŒæ€§çš„æ–¹æ³•ã€‚é€šè¿‡å‡†ç¡®ç©¿æˆ´æ©æ¨¡ä¼°è®¡ä»¥åŠè¿­ä»£æ–¹æ¡ˆå’Œå»å™ªè¿‡ç¨‹ï¼Œæé«˜äº†é¥°å“ä¸æ¨¡å‹ä¹‹é—´çš„å¯¹é½æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡è§„èŒƒæ³¨æ„åŠ›å±‚ï¼Œä»¥éšå¼æ–¹å¼å°†å‚è€ƒé¥°å“æ©æ¨¡æ˜ å°„åˆ°ç©¿æˆ´æ©æ¨¡ä¸Šï¼Œä»¥ä¿ç•™ç»“æ„ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸåœ°å°†å‚è€ƒå›¾åƒä¸­çš„é¥°å“æˆ´åœ¨ç›®æ ‡æ¨¡å‹ä¸Šï¼Œå¤„ç†å°ºåº¦å’Œå§¿æ€ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼ŒåŒæ—¶ä¿æŒèº«ä»½è¯†åˆ«å¹¶å®ç°é€¼çœŸçš„è§†è§‰æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è™šæ‹Ÿè¯•æˆ´é¢†åŸŸä¸­çš„é¥°å“è¯•æˆ´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºé¥°å“çš„å¤æ‚å¾®å°å›¾æ¡ˆå’Œå‡ ä½•å­ç»“æ„çš„é‡å¤æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹é¦–é¥°çš„è™šæ‹Ÿè¯•æˆ´ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡å‡†ç¡®ç©¿æˆ´æ©æ¨¡ä¼°è®¡æ¥æé«˜é¥°å“ä¸æ¨¡å‹ä¹‹é—´çš„å¯¹é½æ€§ã€‚</li>
<li>é‡‡ç”¨äº†è¿­ä»£æ–¹æ¡ˆå’Œå»å™ªè¿‡ç¨‹æ¥ä¼˜åŒ–å¯¹å‡†æ•ˆæœã€‚</li>
<li>ä¸ºäº†ä¿ç•™ç»†èŠ‚ï¼Œé€šè¿‡è§„èŒƒæ³¨æ„åŠ›å±‚ï¼Œå°†å‚è€ƒé¥°å“æ©æ¨¡éšå¼åœ°æ˜ å°„åˆ°ç©¿æˆ´æ©æ¨¡ä¸Šã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å°ºåº¦å’Œå§¿æ€å·®å¼‚å¤§çš„æƒ…å†µä¸‹æˆåŠŸè¯•æˆ´é¥°å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3e97e0469342aa9ce41a6d76c5bf594d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6be095d67f3ecb8599584ca442a882b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d6ac73e2fd5e2827b22794f0b7b994fe.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-fMRI-based-Brain-Decoding-for-Reconstructing-Multimodal-Stimuli"><a href="#A-Survey-on-fMRI-based-Brain-Decoding-for-Reconstructing-Multimodal-Stimuli" class="headerlink" title="A Survey on fMRI-based Brain Decoding for Reconstructing Multimodal   Stimuli"></a>A Survey on fMRI-based Brain Decoding for Reconstructing Multimodal   Stimuli</h2><p><strong>Authors:Pengyu Liu, Guohua Dong, Dan Guo, Kun Li, Fengling Li, Xun Yang, Meng Wang, Xiaomin Ying</strong></p>
<p>In daily life, we encounter diverse external stimuli, such as images, sounds, and videos. As research in multimodal stimuli and neuroscience advances, fMRI-based brain decoding has become a key tool for understanding brain perception and its complex cognitive processes. Decoding brain signals to reconstruct stimuli not only reveals intricate neural mechanisms but also drives progress in AI, disease treatment, and brain-computer interfaces. Recent advancements in neuroimaging and image generation models have significantly improved fMRI-based decoding. While fMRI offers high spatial resolution for precise brain activity mapping, its low temporal resolution and signal noise pose challenges. Meanwhile, techniques like GANs, VAEs, and Diffusion Models have enhanced reconstructed image quality, and multimodal pre-trained models have boosted cross-modal decoding tasks. This survey systematically reviews recent progress in fMRI-based brain decoding, focusing on stimulus reconstruction from passive brain signals. It summarizes datasets, relevant brain regions, and categorizes existing methods by model structure. Additionally, it evaluates model performance and discusses their effectiveness. Finally, it identifies key challenges and proposes future research directions, offering valuable insights for the field. For more information and resources related to this survey, visit <a target="_blank" rel="noopener" href="https://github.com/LpyNow/BrainDecodingImage">https://github.com/LpyNow/BrainDecodingImage</a>. </p>
<blockquote>
<p>åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­ï¼Œæˆ‘ä»¬ä¼šé‡åˆ°å„ç§å„æ ·çš„å¤–éƒ¨åˆºæ¿€ï¼Œå¦‚å›¾åƒã€å£°éŸ³å’Œè§†é¢‘ã€‚éšç€å¤šæ¨¡æ€åˆºæ¿€å’Œç¥ç»ç§‘å­¦çš„ç ”ç©¶è¿›å±•ï¼ŒåŸºäºfMRIçš„è„‘è§£ç å·²æˆä¸ºç†è§£å¤§è„‘æ„ŸçŸ¥åŠå…¶å¤æ‚è®¤çŸ¥è¿‡ç¨‹çš„å…³é”®å·¥å…·ã€‚è§£ç è„‘ä¿¡å·ä»¥é‡å»ºåˆºæ¿€ä¸ä»…æ­ç¤ºäº†å¤æ‚çš„ç¥ç»æœºåˆ¶ï¼Œè¿˜æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ã€ç–¾ç—…æ²»ç–—å’Œè„‘æœºæ¥å£çš„å‘å±•ã€‚è¿‘å¹´æ¥ï¼Œç¥ç»æˆåƒå’Œå›¾åƒç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥æ˜¾è‘—æé«˜äº†åŸºäºfMRIçš„è§£ç æ•ˆæœã€‚è™½ç„¶fMRIåœ¨ç²¾ç¡®çš„å¤§è„‘æ´»åŠ¨æ˜ å°„æ–¹é¢å…·æœ‰é«˜çš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œä½†å…¶è¾ƒä½çš„æ—¶é—´åˆ†è¾¨ç‡å’Œä¿¡å·å™ªå£°æ„æˆäº†æŒ‘æˆ˜ã€‚åŒæ—¶ï¼ŒåƒGANsã€VAEså’Œæ‰©æ•£æ¨¡å‹ç­‰æŠ€æœ¯æé«˜äº†é‡å»ºå›¾åƒçš„è´¨é‡ï¼Œè€Œå¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹æ¨åŠ¨äº†è·¨æ¨¡æ€è§£ç ä»»åŠ¡ã€‚è¿™ç¯‡ç»¼è¿°ç³»ç»Ÿåœ°å›é¡¾äº†åŸºäºfMRIçš„è„‘è§£ç çš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹å…³æ³¨ä»è¢«åŠ¨è„‘ä¿¡å·ä¸­é‡å»ºåˆºæ¿€ã€‚å®ƒæ€»ç»“äº†æ•°æ®é›†ã€ç›¸å…³çš„è„‘åŒºï¼Œå¹¶æŒ‰æ¨¡å‹ç»“æ„å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œåˆ†ç±»ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜è¯„ä¼°äº†æ¨¡å‹æ€§èƒ½å¹¶è®¨è®ºäº†å…¶æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œå®ƒç¡®å®šäº†å…³é”®æŒ‘æˆ˜å¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä¸ºè¯¥é¢†åŸŸæä¾›äº†å®è´µçš„è§è§£ã€‚æœ‰å…³æ­¤ç»¼è¿°çš„æ›´å¤šä¿¡æ¯å’Œèµ„æºï¼Œè¯·è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/LpyNow/BrainDecodingImage%E3%80%82">https://github.com/LpyNow/BrainDecodingImageã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15978v1">PDF</a> 31 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€åˆºæ¿€å’Œç¥ç»ç§‘å­¦ç ”ç©¶çš„è¿›å±•ï¼ŒåŸºäºfMRIçš„è„‘è§£ç æˆä¸ºç†è§£å¤§è„‘æ„ŸçŸ¥å’Œå…¶å¤æ‚è®¤çŸ¥è¿‡ç¨‹çš„å…³é”®å·¥å…·ã€‚è¿‘æœŸç¥ç»æˆåƒå’Œå›¾åƒç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥å¤§å¹…æå‡äº†fMRIè§£ç çš„ç²¾ç¡®åº¦ã€‚æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†fMRIè§£ç çš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹å…³æ³¨ä»è¢«åŠ¨è„‘ä¿¡å·é‡å»ºåˆºæ¿€çš„ç ”ç©¶ã€‚æ–‡ç« æ€»ç»“äº†æ•°æ®é›†ã€ç›¸å…³è„‘åŒºï¼Œå¹¶æŒ‰æ¨¡å‹ç»“æ„åˆ†ç±»ç°æœ‰æ–¹æ³•ï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶è®¨è®ºå…¶æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶æŒ‡å‡ºäº†å…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚æ›´å¤šè¯¦æƒ…è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/LpyNow/BrainDecodingImage">https://github.com/LpyNow/BrainDecodingImage</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åˆºæ¿€å’Œç¥ç»ç§‘å­¦ç ”ç©¶çš„è¿›å±•æ¨åŠ¨äº†åŸºäºfMRIçš„è„‘è§£ç æŠ€æœ¯çš„é‡è¦æ€§ã€‚</li>
<li>fMRIåœ¨è„‘æ´»åŠ¨æ˜ å°„ä¸­å…·æœ‰é«˜ç©ºé—´åˆ†è¾¨ç‡ï¼Œä½†å…¶ä½æ—¶é—´åˆ†è¾¨ç‡å’Œä¿¡å·å™ªå£°å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æŠ€æœ¯è¿›æ­¥å¦‚GANsã€VAEså’ŒDiffusion Modelsæå‡äº†é‡å»ºçš„å›¾åƒè´¨é‡ã€‚</li>
<li>å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹å¢å¼ºäº†è·¨æ¨¡æ€è§£ç ä»»åŠ¡çš„æ•ˆæœã€‚</li>
<li>æ–‡ç« ç»¼è¿°äº†fMRIè§£ç çš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹å…³æ³¨ä»è¢«åŠ¨è„‘ä¿¡å·é‡å»ºåˆºæ¿€çš„ç ”ç©¶æ–¹æ³•å’Œæ•°æ®é›†ã€‚</li>
<li>æ–‡ç« æ€»ç»“äº†ç›¸å…³çš„è„‘åŒºï¼Œå¹¶è®¨è®ºäº†æ¨¡å‹æ€§èƒ½åŠæœ‰æ•ˆæ€§è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77f2a2461628bc52d38e660ae654ccad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3039ad2e8105b05fe4ba5c8bc92f241.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f72ea1a43607d2e858716d2e898ce38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc7afff11e594f57d3ac0a291a9cfe49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-181a316df839df3e41d915a1d6ed8d19.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Acc3D-Accelerating-Single-Image-to-3D-Diffusion-Models-via-Edge-Consistency-Guided-Score-Distillation"><a href="#Acc3D-Accelerating-Single-Image-to-3D-Diffusion-Models-via-Edge-Consistency-Guided-Score-Distillation" class="headerlink" title="Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge   Consistency Guided Score Distillation"></a>Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge   Consistency Guided Score Distillation</h2><p><strong>Authors:Kendong Liu, Zhiyu Zhu, Hui Liu, Junhui Hou</strong></p>
<p>We present Acc3D to tackle the challenge of accelerating the diffusion process to generate 3D models from single images. To derive high-quality reconstructions through few-step inferences, we emphasize the critical issue of regularizing the learning of score function in states of random noise. To this end, we propose edge consistency, i.e., consistent predictions across the high signal-to-noise ratio region, to enhance a pre-trained diffusion model, enabling a distillation-based refinement of the endpoint score function. Building on those distilled diffusion models, we propose an adversarial augmentation strategy to further enrich the generation detail and boost overall generation quality. The two modules complement each other, mutually reinforcing to elevate generative performance. Extensive experiments demonstrate that our Acc3D not only achieves over a $20\times$ increase in computational efficiency but also yields notable quality improvements, compared to the state-of-the-arts. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºAcc3Dï¼Œæ—¨åœ¨è§£å†³åŠ é€Ÿæ‰©æ•£è¿‡ç¨‹ï¼Œä»å•å¹…å›¾åƒç”Ÿæˆ3Dæ¨¡å‹çš„æŒ‘æˆ˜ã€‚ä¸ºäº†é€šè¿‡å°‘æ•°å‡ æ­¥æ¨æ–­è·å¾—é«˜è´¨é‡çš„é‡å»ºï¼Œæˆ‘ä»¬å¼ºè°ƒäº†éšæœºå™ªå£°çŠ¶æ€ä¸‹å¯¹è¯„åˆ†å‡½æ•°å­¦ä¹ è¿›è¡Œæ­£åˆ™åŒ–çš„å…³é”®é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºè¾¹ç¼˜ä¸€è‡´æ€§ï¼Œå³åœ¨é«˜ä¿¡å™ªæ¯”åŒºåŸŸå†…çš„ä¸€è‡´é¢„æµ‹ï¼Œä»¥å¢å¼ºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä»è€Œå®ç°åŸºäºè’¸é¦çš„ç«¯ç‚¹è¯„åˆ†å‡½æ•°çš„æ”¹è¿›ã€‚åŸºäºè¿™äº›è’¸é¦æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºå¯¹æŠ—å¢å¼ºç­–ç•¥ï¼Œä»¥ä¸°å¯Œç”Ÿæˆç»†èŠ‚å¹¶æé«˜æ•´ä½“ç”Ÿæˆè´¨é‡ã€‚è¿™ä¸¤ä¸ªæ¨¡å—ç›¸äº’è¡¥å……ï¼Œç›¸äº’åŠ å¼ºï¼Œæé«˜äº†ç”Ÿæˆæ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Acc3Dä¸ä»…å®ç°äº†è¶…è¿‡20å€çš„è®¡ç®—æ•ˆç‡æå‡ï¼Œè€Œä¸”åœ¨ä¸å…¶ä»–æœ€æ–°æŠ€æœ¯ç›¸æ¯”æ—¶ï¼Œè¿˜å®ç°äº†æ˜¾è‘—çš„è´¨é‡æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Acc3Dæ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿä»å•å¹…å›¾åƒç”Ÿæˆ3Dæ¨¡å‹çš„æ‰©æ•£è¿‡ç¨‹ã€‚é€šè¿‡å¼ºè°ƒéšæœºå™ªå£°çŠ¶æ€ä¸‹å­¦ä¹ è¯„åˆ†å‡½æ•°çš„é‡è¦æ€§ï¼Œä»¥åŠæå‡ºè¾¹ç¼˜ä¸€è‡´æ€§æ¥æé«˜é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†åŸºäºè’¸é¦çš„ç»ˆç‚¹è¯„åˆ†å‡½æ•°ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§å¯¹æŠ—æ€§å¢å¼ºç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥ä¸°å¯Œç”Ÿæˆç»†èŠ‚å¹¶æé«˜æ•´ä½“ç”Ÿæˆè´¨é‡ã€‚ä¸¤ä¸ªæ¨¡å—ç›¸äº’è¡¥å……ï¼Œå…±åŒæé«˜äº†ç”Ÿæˆæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒAcc3Dä¸ä»…å®ç°äº†è¶…è¿‡20å€çš„è®¡ç®—æ•ˆç‡æå‡ï¼Œè€Œä¸”åœ¨è´¨é‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Acc3Dæ—¨åœ¨è§£å†³ä»å•å¹…å›¾åƒç”Ÿæˆ3Dæ¨¡å‹çš„æ‰©æ•£è¿‡ç¨‹åŠ é€Ÿé—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼ºè°ƒéšæœºå™ªå£°çŠ¶æ€ä¸‹å­¦ä¹ è¯„åˆ†å‡½æ•°çš„é‡è¦æ€§æ¥ä¼˜åŒ–é‡å»ºè´¨é‡ã€‚</li>
<li>æå‡ºäº†è¾¹ç¼˜ä¸€è‡´æ€§ï¼Œä»¥æé«˜é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åŸºäºè’¸é¦çš„ç»ˆç‚¹è¯„åˆ†å‡½æ•°ä¼˜åŒ–è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¯¹æŠ—æ€§å¢å¼ºç­–ç•¥ç”¨äºä¸°å¯Œç”Ÿæˆç»†èŠ‚å¹¶æé«˜ç”Ÿæˆè´¨é‡ã€‚</li>
<li>ä¸¤ä¸ªæ¨¡å—ç›¸äº’è¡¥å……ï¼Œå…±åŒæé«˜ç”Ÿæˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f56d8932a8520f52fb268fb0fdbd3719.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8ee5864af118f186f307894b3e8c1f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf47ae867c44a952ea3b4dc482fa0554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-673261e5ced3701ef92d916d0c5013dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-798795ce64b7084c70883a224e41f50c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Jasmine-Harnessing-Diffusion-Prior-for-Self-supervised-Depth-Estimation"><a href="#Jasmine-Harnessing-Diffusion-Prior-for-Self-supervised-Depth-Estimation" class="headerlink" title="Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation"></a>Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation</h2><p><strong>Authors:Jiyuan Wang, Chunyu Lin, Cheng Guan, Lang Nie, Jing He, Haodong Li, Kang Liao, Yao Zhao</strong></p>
<p>In this paper, we propose Jasmine, the first Stable Diffusion (SD)-based self-supervised framework for monocular depth estimation, which effectively harnesses SDâ€™s visual priors to enhance the sharpness and generalization of unsupervised prediction. Previous SD-based methods are all supervised since adapting diffusion models for dense prediction requires high-precision supervision. In contrast, self-supervised reprojection suffers from inherent challenges (e.g., occlusions, texture-less regions, illumination variance), and the predictions exhibit blurs and artifacts that severely compromise SDâ€™s latent priors. To resolve this, we construct a novel surrogate task of hybrid image reconstruction. Without any additional supervision, it preserves the detail priors of SD models by reconstructing the images themselves while preventing depth estimation from degradation. Furthermore, to address the inherent misalignment between SDâ€™s scale and shift invariant estimation and self-supervised scale-invariant depth estimation, we build the Scale-Shift GRU. It not only bridges this distribution gap but also isolates the fine-grained texture of SD output against the interference of reprojection loss. Extensive experiments demonstrate that Jasmine achieves SoTA performance on the KITTI benchmark and exhibits superior zero-shot generalization across multiple datasets. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºStable Diffusionï¼ˆSDï¼‰çš„ç¬¬ä¸€ä¸ªè‡ªç›‘ç£æ¡†æ¶Jasmineï¼Œç”¨äºå•ç›®æ·±åº¦ä¼°è®¡ã€‚å®ƒæœ‰æ•ˆåœ°åˆ©ç”¨äº†SDçš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜äº†æ— ç›‘ç£é¢„æµ‹çš„æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¹‹å‰åŸºäºSDçš„æ–¹æ³•éƒ½æ˜¯ç›‘ç£çš„ï¼Œå› ä¸ºå°†æ‰©æ•£æ¨¡å‹ç”¨äºå¯†é›†é¢„æµ‹éœ€è¦å¤§é‡çš„é«˜ç²¾åº¦ç›‘ç£ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè‡ªç›‘ç£çš„é‡æ–°æŠ•å½±é¢ä¸´å›ºæœ‰çš„æŒ‘æˆ˜ï¼ˆä¾‹å¦‚é®æŒ¡ã€æ— çº¹ç†åŒºåŸŸã€å…‰ç…§å˜åŒ–ï¼‰ï¼Œé¢„æµ‹ç»“æœå‡ºç°æ¨¡ç³Šå’Œä¼ªå½±ï¼Œä¸¥é‡æŸå®³SDçš„æ½œåœ¨å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ··åˆå›¾åƒé‡å»ºçš„æ–°å‹æ›¿ä»£ä»»åŠ¡ã€‚åœ¨ä¸å¢åŠ ä»»ä½•ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œå®ƒé€šè¿‡é‡å»ºå›¾åƒæœ¬èº«æ¥ä¿ç•™SDæ¨¡å‹çš„ç»†èŠ‚å…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶é˜²æ­¢æ·±åº¦ä¼°è®¡é€€åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³SDçš„è§„æ¨¡å’Œç§»ä½ä¸å˜ä¼°è®¡ä¸è‡ªç›‘ç£è§„æ¨¡ä¸å˜æ·±åº¦ä¼°è®¡ä¹‹é—´çš„å›ºæœ‰ä¸åŒ¹é…é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†Scale-Shift GRUã€‚å®ƒä¸ä»…å¼¥è¡¥äº†è¿™ç§åˆ†å¸ƒå·®è·ï¼Œè¿˜éš”ç¦»äº†SDè¾“å‡ºçš„ç²¾ç»†çº¹ç†ï¼Œé¿å…äº†é‡æ–°æŠ•å½±æŸå¤±çš„å¹²æ‰°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒJasmineåœ¨KITTIåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15905v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Jasmineï¼Œé¦–ä¸ªåŸºäºStable Diffusionï¼ˆSDï¼‰çš„è‡ªæˆ‘ç›‘ç£æ¡†æ¶ï¼Œç”¨äºå•ç›®æ·±åº¦ä¼°è®¡ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåˆ©ç”¨äº†SDçš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜äº†æ— ç›‘ç£é¢„æµ‹çš„æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å‰çš„SDæ–¹æ³•å‡æ˜¯æœ‰ç›‘ç£çš„ï¼Œå› ä¸ºå°†æ‰©æ•£æ¨¡å‹ç”¨äºå¯†é›†é¢„æµ‹éœ€è¦é«˜ç²¾åº¦ç›‘ç£ã€‚ä¸æ­¤ç›¸åï¼Œè‡ªæˆ‘ç›‘ç£çš„é‡æŠ•å½±é¢ä¸´å†…åœ¨æŒ‘æˆ˜ï¼ˆå¦‚é®æŒ¡ã€æ— çº¹ç†åŒºåŸŸã€å…‰ç…§å˜åŒ–ï¼‰ï¼Œé¢„æµ‹ç»“æœå‡ºç°æ¨¡ç³Šå’Œä¼ªå½±ï¼Œä¸¥é‡æŸå®³SDçš„æ½œåœ¨å…ˆéªŒã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ç§æ–°çš„æ··åˆå›¾åƒé‡å»ºæ›¿ä»£ä»»åŠ¡ï¼Œæ— éœ€ä»»ä½•é¢å¤–ç›‘ç£ï¼Œå³å¯åœ¨é‡å»ºå›¾åƒçš„åŒæ—¶ä¿ç•™SDæ¨¡å‹çš„ç»†èŠ‚å…ˆéªŒï¼Œé˜²æ­¢æ·±åº¦ä¼°è®¡é€€åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³SDå°ºåº¦ä¸ç§»ä½ä¸å˜ä¼°è®¡å’Œè‡ªæˆ‘ç›‘ç£å°ºåº¦ä¸å˜æ·±åº¦ä¼°è®¡ä¹‹é—´çš„å›ºæœ‰ä¸åŒ¹é…é—®é¢˜ï¼Œæœ¬æ–‡æ„å»ºäº†Scale-Shift GRUã€‚å®ƒä¸ä»…å¼¥è¡¥äº†è¿™ç§åˆ†å¸ƒå·®è·ï¼Œè€Œä¸”éš”ç¦»äº†SDè¾“å‡ºä¸­çš„ç²¾ç»†çº¹ç†ï¼Œé¿å…äº†é‡æŠ•å½±æŸå¤±çš„å¹²æ‰°ã€‚å®éªŒè¡¨æ˜ï¼ŒJasmineåœ¨KITTIåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†å‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Jasmineæ˜¯é¦–ä¸ªåŸºäºStable Diffusionçš„è‡ªæˆ‘ç›‘ç£æ¡†æ¶ï¼Œç”¨äºå•ç›®æ·±åº¦ä¼°è®¡ã€‚</li>
<li>Jasmineåˆ©ç”¨SDçš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜æ— ç›‘ç£é¢„æµ‹çš„æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ­¤å‰SDæ–¹æ³•ä¸»è¦é‡‡å–æœ‰ç›‘ç£æ–¹å¼ï¼Œå› ä¸ºå¯†é›†é¢„æµ‹éœ€è¦é«˜ç²¾åº¦ç›‘ç£ã€‚</li>
<li>è‡ªæˆ‘ç›‘ç£çš„é‡æŠ•å½±å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚é®æŒ¡ã€æ— çº¹ç†åŒºåŸŸå’Œå…‰ç…§å˜åŒ–ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒJasmineå¼•å…¥äº†æ··åˆå›¾åƒé‡å»ºçš„æ›¿ä»£ä»»åŠ¡ï¼Œæ— éœ€é¢å¤–ç›‘ç£ã€‚</li>
<li>Scale-Shift GRUçš„å¼•å…¥è§£å†³äº†SDå°ºåº¦ä¸è‡ªæˆ‘ç›‘ç£æ·±åº¦ä¼°è®¡ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fea0a823a3d0e6d8fd792dbac2bdfb2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e9e0a924e01262778ffb3e244cb9388.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-677cff62fa6ad345e1a008c81d85fe7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d08308e41099c6c6b784e94dbef00638.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="UniCoRN-Latent-Diffusion-based-Unified-Controllable-Image-Restoration-Network-across-Multiple-Degradations"><a href="#UniCoRN-Latent-Diffusion-based-Unified-Controllable-Image-Restoration-Network-across-Multiple-Degradations" class="headerlink" title="UniCoRN: Latent Diffusion-based Unified Controllable Image Restoration   Network across Multiple Degradations"></a>UniCoRN: Latent Diffusion-based Unified Controllable Image Restoration   Network across Multiple Degradations</h2><p><strong>Authors:Debabrata Mandal, Soumitri Chattopadhyay, Guansen Tong, Praneeth Chakravarthula</strong></p>
<p>Image restoration is essential for enhancing degraded images across computer vision tasks. However, most existing methods address only a single type of degradation (e.g., blur, noise, or haze) at a time, limiting their real-world applicability where multiple degradations often occur simultaneously. In this paper, we propose UniCoRN, a unified image restoration approach capable of handling multiple degradation types simultaneously using a multi-head diffusion model. Specifically, we uncover the potential of low-level visual cues extracted from images in guiding a controllable diffusion model for real-world image restoration and we design a multi-head control network adaptable via a mixture-of-experts strategy. We train our model without any prior assumption of specific degradations, through a smartly designed curriculum learning recipe. Additionally, we also introduce MetaRestore, a metalens imaging benchmark containing images with multiple degradations and artifacts. Extensive evaluations on several challenging datasets, including our benchmark, demonstrate that our method achieves significant performance gains and can robustly restore images with severe degradations. Project page: <a target="_blank" rel="noopener" href="https://codejaeger.github.io/unicorn-gh">https://codejaeger.github.io/unicorn-gh</a> </p>
<blockquote>
<p>å›¾åƒä¿®å¤å¯¹äºå¢å¼ºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„é€€åŒ–å›¾åƒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ä¸€æ¬¡åªå¤„ç†ä¸€ç§é€€åŒ–ç±»å‹ï¼ˆä¾‹å¦‚æ¨¡ç³Šã€å™ªå£°æˆ–é›¾éœ¾ï¼‰ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ï¼Œå› ä¸ºå¤šç§é€€åŒ–ç»å¸¸åŒæ—¶å‘ç”Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UniCoRNï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„å›¾åƒä¿®å¤æ–¹æ³•ï¼Œä½¿ç”¨å¤šå¤´æ‰©æ•£æ¨¡å‹åŒæ—¶å¤„ç†å¤šç§é€€åŒ–ç±»å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°äº†ä»å›¾åƒä¸­æå–çš„ä½çº§è§†è§‰çº¿ç´¢åœ¨æŒ‡å¯¼å¯æ§æ‰©æ•£æ¨¡å‹è¿›è¡ŒçœŸå®å›¾åƒä¿®å¤æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå¯é€šè¿‡æ··åˆä¸“å®¶ç­–ç•¥è¿›è¡Œé€‚åº”çš„å¤šå¤´æ§åˆ¶ç½‘ç»œã€‚æˆ‘ä»¬çš„æ¨¡å‹æ— éœ€å¯¹ç‰¹å®šé€€åŒ–åšå‡ºä»»ä½•å…ˆéªŒå‡è®¾ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è¯¾ç¨‹å­¦ä¹ é…æ–¹è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†MetaRestoreï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šç§é€€åŒ–å’Œäººå·¥åˆ¶å“çš„é‡‘å±æˆåƒåŸºå‡†æµ‹è¯•ã€‚åœ¨æˆ‘ä»¬åŸºå‡†æµ‹è¯•ç­‰å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶èƒ½å¤Ÿç¨³å¥åœ°ä¿®å¤ä¸¥é‡é€€åŒ–çš„å›¾åƒã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://codejaeger.github.io/unicorn-gh">https://codejaeger.github.io/unicorn-gh</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15868v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUniCoRNçš„ç»Ÿä¸€å›¾åƒæ¢å¤æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¤šå¤´æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šç§ç±»å‹çš„é€€åŒ–ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä»å›¾åƒä¸­æå–çš„ä½çº§è§†è§‰çº¿ç´¢æ¥æŒ‡å¯¼å¯æ§æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ··åˆä¸“å®¶ç­–ç•¥è®¾è®¡äº†ä¸€ä¸ªå¤šå¤´æ§åˆ¶ç½‘ç»œï¼Œå¹¶è¿›è¡Œäº†æ™ºèƒ½è®¾è®¡çš„è¯¾ç¨‹å­¦ä¹ é…æ–¹è®­ç»ƒã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†åŒ…å«å¤šç§é€€åŒ–å’Œä¼ªå½±çš„MetaRestoreé‡‘å±é€é•œæˆåƒåŸºå‡†ã€‚åœ¨å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶èƒ½ç¨³å¥åœ°æ¢å¤ä¸¥é‡é€€åŒ–çš„å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniCoRNæ˜¯ä¸€ç§èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šç§ç±»å‹é€€åŒ–çš„ç»Ÿä¸€å›¾åƒæ¢å¤æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨ä½çº§è§†è§‰çº¿ç´¢æ¥æŒ‡å¯¼å¯æ§æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>é€šè¿‡æ··åˆä¸“å®¶ç­–ç•¥è®¾è®¡å¤šå¤´æ§åˆ¶ç½‘ç»œã€‚</li>
<li>æ¨¡å‹è®­ç»ƒé‡‡ç”¨äº†æ™ºèƒ½è®¾è®¡çš„è¯¾ç¨‹å­¦ä¹ é…æ–¹ã€‚</li>
<li>å¼•å…¥äº†åŒ…å«å¤šç§é€€åŒ–å’Œä¼ªå½±çš„MetaRestoreåŸºå‡†ã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7d0b31ed65f0656edcae438af2fe5a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1559b88d0ccf33779220fb5bd2ebfc86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ba4e2f1701071997e0f7df4a128b0da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19afd42183057c1e87cd34998567febd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Controlling-Avatar-Diffusion-with-Learnable-Gaussian-Embedding"><a href="#Controlling-Avatar-Diffusion-with-Learnable-Gaussian-Embedding" class="headerlink" title="Controlling Avatar Diffusion with Learnable Gaussian Embedding"></a>Controlling Avatar Diffusion with Learnable Gaussian Embedding</h2><p><strong>Authors:Xuan Gao, Jingtao Zhou, Dongyu Liu, Yuqi Zhou, Juyong Zhang</strong></p>
<p>Recent advances in diffusion models have made significant progress in digital human generation. However, most existing models still struggle to maintain 3D consistency, temporal coherence, and motion accuracy. A key reason for these shortcomings is the limited representation ability of commonly used control signals(e.g., landmarks, depth maps, etc.). In addition, the lack of diversity in identity and pose variations in public datasets further hinders progress in this area. In this paper, we analyze the shortcomings of current control signals and introduce a novel control signal representation that is optimizable, dense, expressive, and 3D consistent. Our method embeds a learnable neural Gaussian onto a parametric head surface, which greatly enhances the consistency and expressiveness of diffusion-based head models. Regarding the dataset, we synthesize a large-scale dataset with multiple poses and identities. In addition, we use real&#x2F;synthetic labels to effectively distinguish real and synthetic data, minimizing the impact of imperfections in synthetic data on the generated head images. Extensive experiments show that our model outperforms existing methods in terms of realism, expressiveness, and 3D consistency. Our code, synthetic datasets, and pre-trained models will be released in our project page: <a target="_blank" rel="noopener" href="https://ustc3dv.github.io/Learn2Control/">https://ustc3dv.github.io/Learn2Control/</a> </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹åœ¨æ•°å­—äººç±»ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹åœ¨ä¿æŒ3Dä¸€è‡´æ€§ã€æ—¶é—´è¿è´¯æ€§å’Œè¿åŠ¨ç²¾åº¦æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚è¿™äº›ä¸è¶³çš„ä¸€ä¸ªå…³é”®åŸå› æ˜¯å¸¸ç”¨æ§åˆ¶ä¿¡å·ï¼ˆä¾‹å¦‚åœ°æ ‡ã€æ·±åº¦å›¾ç­‰ï¼‰çš„è¡¨ç¤ºèƒ½åŠ›æœ‰é™ã€‚æ­¤å¤–ï¼Œå…¬å…±æ•°æ®é›†ä¸­èº«ä»½å’Œå§¿æ€å˜åŒ–çš„ç¼ºä¹è¿›ä¸€æ­¥é˜»ç¢äº†è¯¥é¢†åŸŸçš„è¿›æ­¥ã€‚</p>
</blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†å½“å‰æ§åˆ¶ä¿¡å·çš„ä¸è¶³ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¯ä¼˜åŒ–ã€å¯†é›†ã€è¡¨è¾¾æ€§å¼ºä¸”3Dä¸€è‡´çš„æ§åˆ¶ä¿¡å·è¡¨ç¤ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¯å­¦ä¹ çš„ç¥ç»é«˜æ–¯åµŒå…¥åˆ°å‚æ•°åŒ–å¤´è¡¨é¢ï¼Œè¿™å¤§å¤§æé«˜äº†åŸºäºæ‰©æ•£çš„å¤´æ¨¡å‹çš„ä¸€è‡´æ€§å’Œè¡¨ç°åŠ›ã€‚å…³äºæ•°æ®é›†ï¼Œæˆ‘ä»¬åˆæˆäº†ä¸€ä¸ªå…·æœ‰å¤šç§å§¿æ€å’Œèº«ä»½çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨çœŸå®&#x2F;åˆæˆæ ‡ç­¾æ¥æœ‰æ•ˆåŒºåˆ†çœŸå®å’Œåˆæˆæ•°æ®ï¼Œæœ€å°åŒ–åˆæˆæ•°æ®ä¸­çš„ç¼ºé™·å¯¹ç”Ÿæˆçš„å¤´å›¾åƒçš„å½±å“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨çœŸå®æ€§ã€è¡¨ç°åŠ›å’Œ3Dä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç ã€åˆæˆæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://ustc3dv.github.io/Learn2Control/">https://ustc3dv.github.io/Learn2Control/</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15809v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://ustc3dv.github.io/Learn2Control/">https://ustc3dv.github.io/Learn2Control/</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹åœ¨æ•°å­—äººç±»ç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´3Dä¸€è‡´æ€§ã€æ—¶é—´è¿è´¯æ€§å’Œè¿åŠ¨å‡†ç¡®æ€§ç­‰é—®é¢˜ã€‚æœ¬æ–‡åˆ†æäº†å½“å‰æ§åˆ¶ä¿¡å·çš„ä¸è¶³ï¼Œå¹¶æå‡ºä¸€ç§æ–°å‹çš„å¯ä¼˜åŒ–ã€å¯†é›†ã€è¡¨è¾¾æ€§å¼ºä¸”3Dä¸€è‡´çš„æ§åˆ¶ä¿¡å·è¡¨ç¤ºæ–¹æ³•ã€‚é€šè¿‡å°†å¯å­¦ä¹ çš„ç¥ç»é«˜æ–¯åµŒå…¥åˆ°å‚æ•°åŒ–å¤´éƒ¨è¡¨é¢ï¼Œè¯¥æ–¹æ³•å¤§å¤§æé«˜äº†åŸºäºæ‰©æ•£çš„å¤´éƒ¨æ¨¡å‹çš„ä¸€è‡´æ€§å’Œè¡¨ç°åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡åˆæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šå§¿æ€å’Œèº«ä»½çš„æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨çœŸå®&#x2F;åˆæˆæ ‡ç­¾æ¥æœ‰æ•ˆåŒºåˆ†çœŸå®å’Œåˆæˆæ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®æ€§ã€è¡¨ç°åŠ›å’Œ3Dä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ•°å­—äººç±»ç”Ÿæˆä¸Šå–å¾—è¿›å±•ï¼Œä½†å­˜åœ¨3Dä¸€è‡´æ€§ã€æ—¶é—´è¿è´¯æ€§å’Œè¿åŠ¨å‡†ç¡®æ€§çš„é—®é¢˜ã€‚</li>
<li>å½“å‰æ§åˆ¶ä¿¡å·è¡¨ç¤ºæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´ä¼˜åŒ–ã€å¯†é›†ã€è¡¨è¾¾æ€§å¼ºä¸”3Dä¸€è‡´çš„æ§åˆ¶ä¿¡å·ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ§åˆ¶ä¿¡å·è¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡åµŒå…¥å¯å­¦ä¹ çš„ç¥ç»é«˜æ–¯åˆ°å‚æ•°åŒ–å¤´éƒ¨è¡¨é¢ï¼Œæé«˜æ¨¡å‹çš„ä¸€è‡´æ€§å’Œè¡¨ç°åŠ›ã€‚</li>
<li>åˆæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šå§¿æ€å’Œèº«ä»½çš„æ•°æ®é›†ï¼Œä»¥æ”¯æŒæ›´å¹¿æ³›çš„è®­ç»ƒå’Œåº”ç”¨ã€‚</li>
<li>ä½¿ç”¨çœŸå®&#x2F;åˆæˆæ ‡ç­¾æ¥åŒºåˆ†çœŸå®å’Œåˆæˆæ•°æ®ï¼Œå‡å°‘åˆæˆæ•°æ®ä¸å®Œç¾å¯¹ç”Ÿæˆå¤´éƒ¨å›¾åƒçš„å½±å“ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¨¡å‹åœ¨çœŸå®æ€§ã€è¡¨ç°åŠ›å’Œ3Dä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c81e1a2148611d9c39dc6cd022babed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc25442d1fd20ed57fcf072e24d4baa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-209b86650fc285da2f96378c7ba06a33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e366fb53a6c80691f48e769016175bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b1e3c95e7989c66e62e9b3b75be17c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-197a71fec2276ce5b13150ac536e5938.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Aware-Diffusion-Guided-Refinement-of-3D-Scenes"><a href="#Uncertainty-Aware-Diffusion-Guided-Refinement-of-3D-Scenes" class="headerlink" title="Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes"></a>Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes</h2><p><strong>Authors:Sarosij Bose, Arindam Dutta, Sayak Nag, Junge Zhang, Jiachen Li, Konstantinos Karydis, Amit K. Roy Chowdhury</strong></p>
<p>Reconstructing 3D scenes from a single image is a fundamentally ill-posed task due to the severely under-constrained nature of the problem. Consequently, when the scene is rendered from novel camera views, existing single image to 3D reconstruction methods render incoherent and blurry views. This problem is exacerbated when the unseen regions are far away from the input camera. In this work, we address these inherent limitations in existing single image-to-3D scene feedforward networks. To alleviate the poor performance due to insufficient information beyond the input imageâ€™s view, we leverage a strong generative prior in the form of a pre-trained latent video diffusion model, for iterative refinement of a coarse scene represented by optimizable Gaussian parameters. To ensure that the style and texture of the generated images align with that of the input image, we incorporate on-the-fly Fourier-style transfer between the generated images and the input image. Additionally, we design a semantic uncertainty quantification module that calculates the per-pixel entropy and yields uncertainty maps used to guide the refinement process from the most confident pixels while discarding the remaining highly uncertain ones. We conduct extensive experiments on real-world scene datasets, including in-domain RealEstate-10K and out-of-domain KITTI-v2, showing that our approach can provide more realistic and high-fidelity novel view synthesis results compared to existing state-of-the-art methods. </p>
<blockquote>
<p>ä»å•ä¸€å›¾åƒé‡å»º3Dåœºæ™¯æ˜¯ä¸€ä¸ªæ ¹æœ¬ä¸Šçš„ä¸é€‚å®šä»»åŠ¡ï¼Œå› ä¸ºè¿™ä¸ªé—®é¢˜å…·æœ‰ä¸¥é‡çš„çº¦æŸä¸è¶³çš„ç‰¹æ€§ã€‚å› æ­¤ï¼Œå½“åœºæ™¯ä»æ–°è§†è§’å‘ˆç°æ—¶ï¼Œç°æœ‰çš„å•ä¸€å›¾åƒåˆ°3Dé‡å»ºæ–¹æ³•ä¼šäº§ç”Ÿä¸è¿è´¯å’Œæ¨¡ç³Šçš„è§†å›¾ã€‚å½“æœªè§çš„åŒºåŸŸè¿œç¦»è¾“å…¥ç›¸æœºæ—¶ï¼Œè¿™ä¸ªé—®é¢˜æ›´åŠ ä¸¥é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†ç°æœ‰å•ä¸€å›¾åƒåˆ°3Dåœºæ™¯å‰é¦ˆç½‘ç»œçš„è¿™äº›å›ºæœ‰å±€é™æ€§ã€‚ä¸ºäº†ç¼“è§£ç”±äºè¾“å…¥å›¾åƒè§†å›¾ä¹‹å¤–ä¿¡æ¯ä¸è¶³è€Œå¯¼è‡´çš„æ€§èƒ½ä¸ä½³é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆå…ˆéªŒï¼Œå¯¹ç”±å¯ä¼˜åŒ–é«˜æ–¯å‚æ•°è¡¨ç¤ºçš„ç²—ç³™åœºæ™¯è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆçš„å›¾åƒçš„é£æ ¼å’Œçº¹ç†ä¸è¾“å…¥å›¾åƒä¸€è‡´ï¼Œæˆ‘ä»¬åœ¨ç”Ÿæˆçš„å›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´è¿›è¡Œäº†å®æ—¶çš„å‚…é‡Œå¶é£æ ¼è½¬æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—ï¼Œè¯¥æ¨¡å—è®¡ç®—åƒç´ ç†µå¹¶äº§ç”Ÿä¸ç¡®å®šæ€§æ˜ å°„ï¼Œç”¨äºå¼•å¯¼ä»æœ€ç¡®å®šçš„åƒç´ å¼€å§‹çš„ä¼˜åŒ–è¿‡ç¨‹ï¼ŒåŒæ—¶ä¸¢å¼ƒå…¶ä½™é«˜åº¦ä¸ç¡®å®šçš„åƒç´ ã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬RealEstate-10Ké¢†åŸŸå†…çš„æ•°æ®é›†å’ŒKITTI-v2è·¨é¢†åŸŸæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æä¾›æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æ›´çœŸå®ã€æ›´é«˜è´¨é‡çš„å…¨æ–°è§†è§’åˆæˆç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15742v1">PDF</a> 13 pages, 7 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡è§£å†³äº†ä»å•å¼ å›¾åƒé‡å»º3Dåœºæ™¯æ—¶é¢ä¸´çš„æ ¹æœ¬é—®é¢˜ï¼ŒåŒ…æ‹¬æ¸²æŸ“ä¸ä¸€è‡´å’Œæ¨¡ç³Šè§†å›¾ç­‰ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜å€ŸåŠ©é¢„è®­ç»ƒçš„æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæ”¹è¿›äº†å•ä¸€çš„å›¾åƒåˆ°3Dåœºæ™¯çš„åé¦ˆç½‘ç»œã€‚ç»“åˆFourieré£æ ¼è¿ç§»å’Œè¯­ä¹‰ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒé£æ ¼ä¸è¾“å…¥å›¾åƒä¸€è‡´ï¼ŒåŒæ—¶æé«˜æ¸²æŸ“è´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®åœºæ™¯æ•°æ®é›†ä¸Šèƒ½ç”Ÿæˆæ›´çœŸå®ã€é«˜ä¿çœŸåº¦çš„è§†å›¾åˆæˆç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•å›¾åƒé‡å»ºçš„3Dåœºæ™¯å­˜åœ¨å›ºæœ‰çš„é—®é¢˜ï¼Œå¯¼è‡´æ¸²æŸ“ç»“æœä¸ä¸€è‡´å’Œæ¨¡ç³Šã€‚</li>
<li>é¢„è®­ç»ƒçš„æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹è¢«ç”¨äºè¿­ä»£ä¼˜åŒ–ï¼Œæ”¹è¿›å•ä¸€å›¾åƒåˆ°3Dåœºæ™¯çš„åé¦ˆç½‘ç»œæ€§èƒ½ã€‚</li>
<li>Fourieré£æ ¼è¿ç§»æŠ€æœ¯ç¡®ä¿ç”Ÿæˆçš„å›¾åƒé£æ ¼ä¸è¾“å…¥å›¾åƒä¸€è‡´ã€‚</li>
<li>è®¾è®¡äº†è¯­ä¹‰ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—ï¼Œç”¨äºè®¡ç®—åƒç´ çº§åˆ«çš„ç†µå¹¶ç”Ÿæˆä¸ç¡®å®šæ€§åœ°å›¾ï¼ŒæŒ‡å¯¼ä¼˜åŒ–è¿‡ç¨‹ä¸“æ³¨äºæœ€å¯é çš„åƒç´ ï¼ŒåŒæ—¶å¿½ç•¥é«˜åº¦ä¸ç¡®å®šçš„éƒ¨åˆ†ã€‚</li>
<li>æ–¹æ³•åœ¨çœŸå®åœºæ™¯æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒéªŒè¯ã€‚</li>
<li>ä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½æä¾›æ›´çœŸå®ã€é«˜ä¿çœŸåº¦çš„è§†å›¾åˆæˆç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af1bab3545466fa679137215ee97ed9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b14f9d249ed314e66e85d276d42807c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84bbda6b2cc7022337a619ff62fddece.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee8337751acd1e1659f43f72b9fb2173.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-as-a-Noise-Aware-Latent-Reward-Model-for-Step-Level-Preference-Optimization"><a href="#Diffusion-Model-as-a-Noise-Aware-Latent-Reward-Model-for-Step-Level-Preference-Optimization" class="headerlink" title="Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization"></a>Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization</h2><p><strong>Authors:Tao Zhang, Cheng Da, Kun Ding, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, Chunhong Pan</strong></p>
<p>Preference optimization for diffusion models aims to align them with human preferences for images. Previous methods typically leverage Vision-Language Models (VLMs) as pixel-level reward models to approximate human preferences. However, when used for step-level preference optimization, these models face challenges in handling noisy images of different timesteps and require complex transformations into pixel space. In this work, we demonstrate that diffusion models are inherently well-suited for step-level reward modeling in the latent space, as they can naturally extract features from noisy latent images. Accordingly, we propose the Latent Reward Model (LRM), which repurposes components of diffusion models to predict preferences of latent images at various timesteps. Building on LRM, we introduce Latent Preference Optimization (LPO), a method designed for step-level preference optimization directly in the latent space. Experimental results indicate that LPO not only significantly enhances performance in aligning diffusion models with general, aesthetic, and text-image alignment preferences, but also achieves 2.5-28$\times$ training speedup compared to existing preference optimization methods. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/Kwai-Kolors/LPO">https://github.com/Kwai-Kolors/LPO</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„åå¥½ä¼˜åŒ–æ—¨åœ¨ä½¿å®ƒä»¬ç¬¦åˆäººç±»å¯¹äºå›¾åƒçš„çœ‹æ³•ã€‚ä¹‹å‰çš„æ–¹æ³•é€šå¸¸åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºåƒç´ çº§å¥–åŠ±æ¨¡å‹æ¥è¿‘ä¼¼äººç±»åå¥½ã€‚ç„¶è€Œï¼Œå½“ç”¨äºæ­¥éª¤çº§åå¥½ä¼˜åŒ–æ—¶ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†ä¸åŒæ—¶é—´æ­¥çš„å™ªå£°å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”éœ€è¦å°†å¤æ‚è½¬æ¢åˆ°åƒç´ ç©ºé—´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰©æ•£æ¨¡å‹å¤©ç„¶é€‚åˆåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ­¥éª¤çº§å¥–åŠ±å»ºæ¨¡ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä»å™ªå£°æ½œåœ¨å›¾åƒä¸­è‡ªç„¶æå–ç‰¹å¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨å¥–åŠ±æ¨¡å‹ï¼ˆLRMï¼‰ï¼Œè¯¥æ¨¡å‹é‡æ–°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç»„ä»¶æ¥é¢„æµ‹ä¸åŒæ—¶é—´æ­¥çš„æ½œåœ¨å›¾åƒçš„åå¥½ã€‚åŸºäºLRMï¼Œæˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºæ½œåœ¨ç©ºé—´ä¸­çš„æ­¥éª¤çº§åå¥½ä¼˜åŒ–è€Œè®¾è®¡çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLPOä¸ä»…æ˜¾è‘—æé«˜äº†æ‰©æ•£æ¨¡å‹ä¸ä¸€èˆ¬ã€ç¾å­¦å’Œæ–‡æœ¬-å›¾åƒå¯¹é½åå¥½çš„å¯¹é½æ€§èƒ½ï¼Œè€Œä¸”ä¸ç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”å®ç°äº†2.5-28å€çš„è®­ç»ƒåŠ é€Ÿã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kwai-Kolors/LPO%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Kwai-Kolors/LPOè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01051v2">PDF</a> 20 pages, 14 tables, 15 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿å›¾åƒä¸äººç±»åå¥½å¯¹é½ã€‚å…ˆå‰çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºåƒç´ çº§å¥–åŠ±æ¨¡å‹æ¥è¿‘ä¼¼äººç±»åå¥½ï¼Œä½†åœ¨å¤„ç†ä¸åŒæ—¶é—´æ­¥é•¿çš„å™ªå£°å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ­¥çº§å¥–åŠ±å»ºæ¨¡çš„å¤©ç„¶ä¼˜åŠ¿ï¼Œå¹¶æå‡ºäº†æ½œåœ¨å¥–åŠ±æ¨¡å‹ï¼ˆLRMï¼‰å’Œæ½œåœ¨åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLPOä¸ä»…å¤§å¤§æé«˜äº†æ‰©æ•£æ¨¡å‹ä¸é€šç”¨ã€ç¾å­¦å’Œæ–‡æœ¬å›¾åƒå¯¹é½åå¥½çš„å¯¹é½æ€§èƒ½ï¼Œè€Œä¸”ä¸ç°æœ‰åå¥½ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”å®ç°äº†2.5-28å€çš„è®­ç»ƒåŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åå¥½ä¼˜åŒ–çš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹ç”Ÿæˆçš„å›¾åƒä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†ä¸åŒæ—¶é—´æ­¥é•¿çš„å™ªå£°å›¾åƒæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ­¥çº§å¥–åŠ±å»ºæ¨¡å…·æœ‰å¤©ç„¶ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºäº†æ½œåœ¨å¥–åŠ±æ¨¡å‹ï¼ˆLRMï¼‰æ¥é¢„æµ‹ä¸åŒæ—¶é—´æ­¥é•¿ä¸‹æ½œåœ¨å›¾åƒçš„åå¥½ã€‚</li>
<li>åŸºäºLRMï¼Œå¼•å…¥äº†æ½œåœ¨åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰æ–¹æ³•ï¼Œä¸“ä¸ºåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ­¥çº§åå¥½ä¼˜åŒ–è®¾è®¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLPOæ˜¾è‘—æé«˜äº†æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†æ˜¾è‘—çš„è®­ç»ƒåŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-161865cd05a38b6d9a4c35335b20cac7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee307810b1e611e60b713b096e7a2935.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6088490955f4353a0574dad9b3a16909.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bbb79a5459f145937d74958321af3cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41404ecbf428f8fca083cd14d3f9d6a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4c1bf4f272373ac7db9ffda5d9946a9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Switti-Designing-Scale-Wise-Transformers-for-Text-to-Image-Synthesis"><a href="#Switti-Designing-Scale-Wise-Transformers-for-Text-to-Image-Synthesis" class="headerlink" title="Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis"></a>Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</h2><p><strong>Authors:Anton Voronov, Denis Kuznedelev, Mikhail Khoroshikh, Valentin Khrulkov, Dmitry Baranchuk</strong></p>
<p>This work presents Switti, a scale-wise transformer for text-to-image generation. We start by adapting an existing next-scale prediction autoregressive (AR) architecture to T2I generation, investigating and mitigating training stability issues in the process. Next, we argue that scale-wise transformers do not require causality and propose a non-causal counterpart facilitating ~21% faster sampling and lower memory usage while also achieving slightly better generation quality. Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. By disabling guidance at these scales, we achieve an additional sampling acceleration of ~32% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7x faster. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Swittiï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è§„æ¨¡è½¬æ¢å™¨ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡å¯¹ç°æœ‰çš„ä¸‹ä¸€å°ºåº¦é¢„æµ‹è‡ªå›å½’ï¼ˆARï¼‰æ¶æ„è¿›è¡Œæ”¹ç¼–ï¼Œå°†å…¶åº”ç”¨äºT2Iç”Ÿæˆï¼ŒåŒæ—¶ç ”ç©¶å’Œè§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§é—®é¢˜ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¤ä¸ºè§„æ¨¡è½¬æ¢å™¨ä¸éœ€è¦å› æœæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªéå› æœçš„å¯¹åº”æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯å®ç°çº¦21%æ›´å¿«çš„é‡‡æ ·å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶å®ç°ç•¥å¾®æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°åœ¨é«˜åˆ†è¾¨ç‡å°ºåº¦ä¸Šä¸éœ€è¦æ— åˆ†ç±»æŒ‡å¯¼ï¼Œç”šè‡³å¯èƒ½ä¼šé™ä½æ€§èƒ½ã€‚é€šè¿‡åœ¨è¿™äº›å°ºåº¦ä¸Šç¦ç”¨æŒ‡å¯¼ï¼Œæˆ‘ä»¬å®ç°äº†é¢å¤–çš„çº¦32%çš„é‡‡æ ·åŠ é€Ÿï¼Œå¹¶æ”¹è¿›äº†ç»†ç²’åº¦ç»†èŠ‚çš„ç”Ÿæˆã€‚å¹¿æ³›çš„äººç±»åå¥½ç ”ç©¶å’Œè‡ªåŠ¨åŒ–è¯„ä¼°è¡¨æ˜ï¼ŒSwittiè¶…è¶Šäº†ç°æœ‰çš„T2I ARæ¨¡å‹ï¼Œä¸æœ€å…ˆè¿›çš„T2Iæ‰©æ•£æ¨¡å‹ç«äº‰ï¼ŒåŒæ—¶é€Ÿåº¦æé«˜äº†é«˜è¾¾7å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01819v4">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†Swittiï¼Œä¸€ç§ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å°ºåº¦å˜æ¢å™¨ã€‚é€šè¿‡å¯¹ç°æœ‰æ¬¡å°ºåº¦é¢„æµ‹è‡ªå›å½’ï¼ˆARï¼‰æ¶æ„çš„é€‚åº”ï¼Œç ”ç©¶äº†è®­ç»ƒç¨³å®šæ€§é—®é¢˜å¹¶è¿›è¡Œäº†ç¼“è§£ã€‚æå‡ºéå› æœæ€§å°ºåº¦å˜æ¢å™¨ï¼Œå®ç°æ›´å¿«çš„é‡‡æ ·é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ç”Ÿæˆè´¨é‡ç•¥æœ‰æé«˜ã€‚æ­¤å¤–ï¼Œå‘ç°é«˜åˆ†è¾¨ç‡ä¸‹çš„æ— åˆ†ç±»å™¨å¼•å¯¼å¹¶ä¸å¿…è¦ï¼Œç”šè‡³å¯èƒ½é™ä½æ€§èƒ½ã€‚ç¦ç”¨è¿™äº›å°ºåº¦çš„å¼•å¯¼ï¼Œè¿›ä¸€æ­¥æé«˜é‡‡æ ·é€Ÿåº¦å¹¶æ”¹å–„ç»†èŠ‚ç”Ÿæˆã€‚äººç±»åå¥½ç ”ç©¶å’Œè‡ªåŠ¨åŒ–è¯„ä¼°æ˜¾ç¤ºï¼ŒSwittiä¼˜äºç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒè‡ªå›å½’æ¨¡å‹ï¼Œå¹¶ä¸å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç«äº‰ï¼Œé€Ÿåº¦æé«˜é«˜è¾¾7å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Swittiæ˜¯ä¸€ç§ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å°ºåº¦å˜æ¢å™¨ã€‚</li>
<li>é€šè¿‡é€‚åº”ç°æœ‰æ¬¡å°ºåº¦é¢„æµ‹è‡ªå›å½’æ¶æ„ï¼Œè§£å†³äº†è®­ç»ƒç¨³å®šæ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†éå› æœæ€§å°ºåº¦å˜æ¢å™¨ï¼Œå®ç°æ›´å¿«çš„é‡‡æ ·é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ã€‚</li>
<li>æ— éœ€åœ¨é«˜åˆ†è¾¨ç‡ä¸‹ä½¿ç”¨åˆ†ç±»å™¨å¼•å¯¼ï¼Œç¦ç”¨è¿™äº›å°ºåº¦çš„å¼•å¯¼å¯è¿›ä¸€æ­¥æé«˜é‡‡æ ·é€Ÿåº¦å’Œæ”¹å–„ç»†èŠ‚ç”Ÿæˆã€‚</li>
<li>Swittiçš„ç”Ÿæˆè´¨é‡ä¼˜äºç°æœ‰æ–‡æœ¬åˆ°å›¾åƒè‡ªå›å½’æ¨¡å‹ï¼Œä¸å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç›¸å½“ã€‚</li>
<li>Swittiå®ç°äº†é«˜è¾¾7å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d132772d4f8759ef0ead68a6e2223157.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e9685472d8992f2d6e013b1bd0542fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d632639bfbef8573a447467198d68746.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2dad1937285cae7b35b76c66fe17077f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-deb928de8a273e96fed416e9c98e1502.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9f20558ede7a306ab06748472de0a73.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Pathways-on-the-Image-Manifold-Image-Editing-via-Video-Generation"><a href="#Pathways-on-the-Image-Manifold-Image-Editing-via-Video-Generation" class="headerlink" title="Pathways on the Image Manifold: Image Editing via Video Generation"></a>Pathways on the Image Manifold: Image Editing via Video Generation</h2><p><strong>Authors:Noam Rotstein, Gal Yona, Daniel Silver, Roy Velich, David BensaÃ¯d, Ron Kimmel</strong></p>
<p>Recent advances in image editing, driven by image diffusion models, have shown remarkable progress. However, significant challenges remain, as these models often struggle to follow complex edit instructions accurately and frequently compromise fidelity by altering key elements of the original image. Simultaneously, video generation has made remarkable strides, with models that effectively function as consistent and continuous world simulators. In this paper, we propose merging these two fields by utilizing image-to-video models for image editing. We reformulate image editing as a temporal process, using pretrained video models to create smooth transitions from the original image to the desired edit. This approach traverses the image manifold continuously, ensuring consistent edits while preserving the original imageâ€™s key aspects. Our approach achieves state-of-the-art results on text-based image editing, demonstrating significant improvements in both edit accuracy and image preservation. Visit our project page at <a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Frame2Frame">https://rotsteinnoam.github.io/Frame2Frame</a>. </p>
<blockquote>
<p>ç”±å›¾åƒæ‰©æ•£æ¨¡å‹é©±åŠ¨çš„å›¾ç‰‡ç¼–è¾‘é¢†åŸŸçš„æœ€æ–°è¿›å±•å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹å¾€å¾€éš¾ä»¥å‡†ç¡®éµå¾ªå¤æ‚çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œå¹¶ä¸”ç»å¸¸æ”¹å˜åŸå§‹å›¾åƒçš„å…³é”®å…ƒç´ ï¼Œä»è€Œå½±å“ä¿çœŸåº¦ã€‚åŒæ—¶ï¼Œè§†é¢‘ç”Ÿæˆå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œæ¨¡å‹æœ‰æ•ˆåœ°å……å½“äº†è¿è´¯å’Œè¿ç»­çš„ä¸–ç•Œæ¨¡æ‹Ÿå™¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡åˆ©ç”¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œå›¾ç‰‡ç¼–è¾‘æ¥èåˆè¿™ä¸¤ä¸ªé¢†åŸŸã€‚æˆ‘ä»¬å°†å›¾åƒç¼–è¾‘é‡æ–°æ„æƒ³ä¸ºä¸€ä¸ªæ—¶é—´è¿‡ç¨‹ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹æ¥åˆ›å»ºä»åŸå§‹å›¾åƒåˆ°æ‰€éœ€ç¼–è¾‘çš„å¹³æ»‘è¿‡æ¸¡ã€‚è¿™ç§æ–¹æ³•åœ¨å›¾åƒæµå½¢ä¸Šè¿ç»­éå†ï¼Œç¡®ä¿ç¼–è¾‘çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„å…³é”®æ–¹é¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäºæ–‡æœ¬çš„å›¾ç‰‡ç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨ç¼–è¾‘å‡†ç¡®æ€§å’Œå›¾åƒä¿ç•™æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Frame2Frame%E6%BB%A4%E6%9B%B4%E5%A4%9A%E4%BF%A1%E5%B9%BF%E3%80%82">https://rotsteinnoam.github.io/Frame2Frameäº†è§£æ›´å¤šä¿¡æ¯ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16819v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å¤æ‚æŒ‡ä»¤è·Ÿéšå’Œä¿çœŸåº¦ä¿æŒçš„æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œè§†é¢‘ç”Ÿæˆé¢†åŸŸä¹Ÿå–å¾—äº†é•¿è¶³çš„å‘å±•ã€‚æœ¬æ–‡æå‡ºå°†å›¾åƒç¼–è¾‘ä¸è§†é¢‘ç”Ÿæˆç›¸ç»“åˆï¼Œåˆ©ç”¨å›¾åƒåˆ°è§†é¢‘çš„æ¨¡å‹è¿›è¡Œå›¾åƒç¼–è¾‘ã€‚é€šè¿‡é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹åˆ›å»ºä»åŸå§‹å›¾åƒåˆ°æœŸæœ›ç¼–è¾‘çš„å¹³æ»‘è¿‡æ¸¡ï¼Œå°†å›¾åƒç¼–è¾‘é‡æ–°æ„æƒ³ä¸ºä¸€ä¸ªæ—¶é—´è¿‡ç¨‹ï¼Œç¡®ä¿ç¼–è¾‘çš„ä¸€è‡´æ€§å¹¶ä¿ç•™åŸå§‹å›¾åƒçš„å…³é”®è¦ç´ ã€‚è¯¥æ–¹æ³•åœ¨æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†æœ€æ–°ç»“æœï¼Œæ˜¾è‘—æé«˜äº†ç¼–è¾‘å‡†ç¡®æ€§å’Œå›¾åƒä¿ç•™æ•ˆæœã€‚è¯¦æƒ…è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://rotsteinnoam.github.io/Frame2Frame">https://rotsteinnoam.github.io/Frame2Frame</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´è·Ÿéšå¤æ‚æŒ‡ä»¤å’Œä¿æŒä¿çœŸåº¦çš„æŒ‘æˆ˜ã€‚</li>
<li>è§†é¢‘ç”Ÿæˆé¢†åŸŸå·²æœ‰æœ‰æ•ˆçš„æŒç»­ä¸–ç•Œæ¨¡æ‹Ÿå™¨æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†å°†å›¾åƒç¼–è¾‘ä¸è§†é¢‘ç”Ÿæˆç›¸ç»“åˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒåˆ°è§†é¢‘çš„æ¨¡å‹è¿›è¡Œå›¾åƒç¼–è¾‘ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹åˆ›å»ºä»åŸå§‹å›¾åƒåˆ°æœŸæœ›ç¼–è¾‘çš„å¹³æ»‘è¿‡æ¸¡ã€‚</li>
<li>å›¾åƒç¼–è¾‘è¢«é‡æ–°æ„æƒ³ä¸ºä¸€ä¸ªæ—¶é—´è¿‡ç¨‹ï¼Œç¡®ä¿ç¼–è¾‘çš„ä¸€è‡´æ€§å¹¶ä¿ç•™åŸå§‹å›¾åƒçš„å…³é”®è¦ç´ ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—æœ€æ–°æˆæœï¼Œæé«˜äº†ç¼–è¾‘å‡†ç¡®æ€§å’Œå›¾åƒä¿ç•™æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa81f50bb614d6a50598f5ebeb8ef86b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af85b05da4766f4665f7bb10765f0765.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc043f4cab323197ccf1cade71a10936.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0612d2fad51192fc2eb43f6776fc60fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23fbcdc43e3b62f8e50a91b8e48f4d12.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="V2X-R-Cooperative-LiDAR-4D-Radar-Fusion-for-3D-Object-Detection-with-Denoising-Diffusion"><a href="#V2X-R-Cooperative-LiDAR-4D-Radar-Fusion-for-3D-Object-Detection-with-Denoising-Diffusion" class="headerlink" title="V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with   Denoising Diffusion"></a>V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with   Denoising Diffusion</h2><p><strong>Authors:Xun Huang, Jinlong Wang, Qiming Xia, Siheng Chen, Bisheng Yang, Xin Li, Cheng Wang, Chenglu Wen</strong></p>
<p>Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D object detection using LiDAR and camera data. However, these methods suffer from performance degradation in adverse weather conditions. The weather-robust 4D radar provides Doppler and additional geometric information, raising the possibility of addressing this challenge. To this end, we present V2X-R, the first simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R contains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point clouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes. Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for 3D object detection and implement it with various fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D radar feature as a condition to prompt the diffusion model to denoise noisy LiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline demonstrates superior performance in the V2X-R dataset. Over and above this, our MDD module further improved the performance of basic fusion model by up to 5.73%&#x2F;6.70% in foggy&#x2F;snowy conditions with barely disrupting normal performance. The dataset and code will be publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ylwhxht/V2X-R">https://github.com/ylwhxht/V2X-R</a>. </p>
<blockquote>
<p>å½“å‰çš„è½¦è½½é€šè®¯ç³»ç»Ÿï¼ˆVehicle-to-Everythingï¼Œç®€ç§°V2Xï¼‰å·²ç»é€šè¿‡æ¿€å…‰é›·è¾¾å’Œæ‘„åƒå¤´æ•°æ®æ˜¾è‘—æé«˜äº†ä¸‰ç»´ç‰©ä½“æ£€æµ‹èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ä¼šå‡ºç°æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚å¤©æ°”ç¨³å®šçš„å››ç»´é›·è¾¾æä¾›äº†å¤šæ™®å‹’å’Œé¢å¤–çš„å‡ ä½•ä¿¡æ¯ï¼Œä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†å¯èƒ½æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†V2X-Rï¼Œè¿™æ˜¯é¦–ä¸ªç»“åˆäº†æ¿€å…‰é›·è¾¾ã€æ‘„åƒå¤´å’Œå››ç»´é›·è¾¾çš„æ¨¡æ‹ŸV2Xæ•°æ®é›†ã€‚V2X-RåŒ…å«12,079ä¸ªåœºæ™¯ï¼Œå…¶ä¸­åŒ…æ‹¬æ¿€å…‰é›·è¾¾å’Œå››ç»´é›·è¾¾ç‚¹äº‘37,727å¸§ã€å›¾åƒæ•°æ®é«˜è¾¾150,908å¼ ä»¥åŠæ ‡æ³¨äº†çš„ä¸‰ç»´è½¦è¾†è¾¹ç•Œæ¡†æ•°é‡è¾¾170,859ä¸ªã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åˆä½œå¼æ¿€å…‰é›·è¾¾å››ç»´é›·è¾¾èåˆç®¡é“è¿›è¡Œä¸‰ç»´ç‰©ä½“æ£€æµ‹ï¼Œå¹¶é‡‡ç”¨äº†å¤šç§èåˆç­–ç•¥æ¥å®ç°ã€‚ä¸ºäº†å®ç°å¤©æ°”ç¨³å®šçš„æ£€æµ‹ï¼Œæˆ‘ä»¬åœ¨èåˆç®¡é“ä¸­é¢å¤–æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€é™å™ªæ‰©æ•£æ¨¡å—ï¼ˆMDDï¼‰ã€‚MDDåˆ©ç”¨å¤©æ°”ç¨³å®šçš„å››ç»´é›·è¾¾ç‰¹å¾ä½œä¸ºæ¡ä»¶æ¥æç¤ºæ‰©æ•£æ¨¡å‹å¯¹å™ªå£°å¹²æ‰°çš„æ¿€å…‰é›·è¾¾ç‰¹å¾è¿›è¡Œå»å™ªå¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¿€å…‰é›·è¾¾å››ç»´é›·è¾¾èåˆç®¡é“åœ¨V2X-Ræ•°æ®é›†ä¸Šè¡¨ç°å“è¶Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„MDDæ¨¡å—è¿›ä¸€æ­¥æ”¹å–„äº†åŸºæœ¬èåˆæ¨¡å‹åœ¨é›¾å¤©å’Œé›ªå¤©ç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼Œæå‡äº†é«˜è¾¾5.73%å’Œ6.70%ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“æ­£å¸¸ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/ylwhxht/V2X-R">https://github.com/ylwhxht/V2X-R</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08402v4">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è½¦è¾†ä¸ç¯å¢ƒäº¤äº’ï¼ˆV2Xï¼‰é¢†åŸŸçš„æœ€æ–°ç ”ç©¶æˆæœã€‚åœ¨ç°æœ‰çš„LiDARå’Œç›¸æœºæ•°æ®åŸºç¡€ä¸Šï¼Œé€šè¿‡ä½¿ç”¨å¤©æ°”ç¨³å¥çš„4Dé›·è¾¾æ•°æ®ï¼Œæ„å»ºäº†ä¸€ä¸ªæ¨¡æ‹Ÿçš„V2Xæ•°æ®é›†V2X-Rã€‚è¯¥æ•°æ®é›†åŒ…å«å¤šç§åœºæ™¯ä¸‹çš„LiDARå’Œ4Dé›·è¾¾ç‚¹äº‘æ•°æ®ã€å›¾åƒä»¥åŠæ ‡æ³¨çš„3Dè½¦è¾†è¾¹ç•Œæ¡†ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ååŒLiDAR-4Dé›·è¾¾èåˆç®¡é“ï¼Œç”¨äºå®ç°å¤©æ°”ç¨³å¥çš„3Dç›®æ ‡æ£€æµ‹ã€‚å…¶ä¸­ï¼Œå¤šæ¨¡æ€å»å™ªæ‰©æ•£æ¨¡å—ï¼ˆMDDï¼‰èƒ½å¤Ÿåœ¨èåˆç®¡é“ä¸­è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥èåˆç®¡é“åœ¨V2X-Ræ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ä¼˜è¶Šï¼Œå¹¶ä¸”åœ¨é›¾å¤©å’Œé›ªå¤©çš„æ¡ä»¶ä¸‹ï¼Œé€šè¿‡MDDæ¨¡å—çš„è¾…åŠ©ï¼Œæ€§èƒ½æå‡å°¤ä¸ºæ˜¾è‘—ã€‚æ•°æ®é›†å’Œç›¸å…³ä»£ç å·²å…¬å¼€å¯ä¾›ä¸‹è½½ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰V2Xç³»ç»Ÿçš„3Då¯¹è±¡æ£€æµ‹ä¸»è¦ä¾èµ–LiDARå’Œç›¸æœºæ•°æ®ï¼Œä½†åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹æ€§èƒ½ä¼šä¸‹é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ¨¡æ‹Ÿçš„V2Xæ•°æ®é›†V2X-Rï¼Œæ•´åˆäº†LiDARã€ç›¸æœºå’Œ4Dé›·è¾¾æ•°æ®ã€‚</li>
<li>V2X-Ræ•°æ®é›†åŒ…å«å¤§é‡åœºæ™¯å’Œæ ‡æ³¨çš„3Dè½¦è¾†è¾¹ç•Œæ¡†ï¼Œæœ‰åŠ©äºæ”¹è¿›å’Œæµ‹è¯•3Dç›®æ ‡æ£€æµ‹ç®—æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ååŒLiDAR-4Dé›·è¾¾èåˆç®¡é“ç”¨äºæé«˜3Dç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ€§èƒ½ã€‚</li>
<li>èåˆç®¡é“ä¸­çš„å¤šæ¨¡æ€å»å™ªæ‰©æ•£æ¨¡å—ï¼ˆMDDï¼‰èƒ½å¤Ÿè¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥èåˆç®¡é“åœ¨V2X-Ræ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8ee640db0486ff24df06fde3177bebb2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c6fed5e200bb255f7bd293f52f44307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd1a61097db38d16aed0137c4fc4275d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04f3d9596d93c990304d6e9d05600699.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e33e2cb8243350cb628b060decee4321.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c91d679dba6f111556e386ae4b226fb2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Diffusion-Attribution-Score-Evaluating-Training-Data-Influence-in-Diffusion-Model"><a href="#Diffusion-Attribution-Score-Evaluating-Training-Data-Influence-in-Diffusion-Model" class="headerlink" title="Diffusion Attribution Score: Evaluating Training Data Influence in   Diffusion Model"></a>Diffusion Attribution Score: Evaluating Training Data Influence in   Diffusion Model</h2><p><strong>Authors:Jinxu Lin, Linwei Tao, Minjing Dong, Chang Xu</strong></p>
<p>As diffusion models become increasingly popular, the misuse of copyrighted and private images has emerged as a major concern. One promising solution to mitigate this issue is identifying the contribution of specific training samples in generative models, a process known as data attribution. Existing data attribution methods for diffusion models typically quantify the contribution of a training sample by evaluating the change in diffusion loss when the sample is included or excluded from the training process. However, we argue that the direct usage of diffusion loss cannot represent such a contribution accurately due to the calculation of diffusion loss. Specifically, these approaches measure the divergence between predicted and ground truth distributions, which leads to an indirect comparison between the predicted distributions and cannot represent the variances between model behaviors. To address these issues, we aim to measure the direct comparison between predicted distributions with an attribution score to analyse the training sample importance, which is achieved by Diffusion Attribution Score (\textit{DAS}). Underpinned by rigorous theoretical analysis, we elucidate the effectiveness of DAS. Additionally, we explore strategies to accelerate DAS calculations, facilitating its application to large-scale diffusion models. Our extensive experiments across various datasets and diffusion models demonstrate that DAS significantly surpasses previous benchmarks in terms of the linear data-modelling score, establishing new state-of-the-art performance. Code is available at \hyperlink{here}{<a target="_blank" rel="noopener" href="https://github.com/Jinxu-Lin/DAS%7D">https://github.com/Jinxu-Lin/DAS}</a>. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå¯¹ç‰ˆæƒå’Œç§äººå›¾åƒçš„æ»¥ç”¨å·²æˆä¸ºä¸€ä¸ªä¸»è¦çš„é—®é¢˜ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆæ˜¯è¯†åˆ«ç”Ÿæˆæ¨¡å‹ä¸­ç‰¹å®šè®­ç»ƒæ ·æœ¬çš„è´¡çŒ®ï¼Œè¿™ä¸€è¿‡ç¨‹è¢«ç§°ä¸ºæ•°æ®å½’å› ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ•°æ®å½’å› æ–¹æ³•é€šå¸¸é€šè¿‡è¯„ä¼°è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«æˆ–æ’é™¤è®­ç»ƒæ ·æœ¬æ—¶çš„æ‰©æ•£æŸå¤±å˜åŒ–æ¥é‡åŒ–è®­ç»ƒæ ·æœ¬çš„è´¡çŒ®ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºç›´æ¥ä½¿ç”¨æ‰©æ•£æŸå¤±ä¸èƒ½å‡†ç¡®åœ°è¡¨ç¤ºè¿™ç§è´¡çŒ®ï¼Œå› ä¸ºæ‰©æ•£æŸå¤±çš„è®¡ç®—æ–¹å¼å­˜åœ¨é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™äº›æ–¹æ³•æµ‹é‡é¢„æµ‹åˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œè¿™å¯¼è‡´é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„é—´æ¥æ¯”è¾ƒï¼Œå¹¶ä¸èƒ½ä»£è¡¨æ¨¡å‹è¡Œä¸ºä¹‹é—´çš„å·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡å½’å› åˆ†æ•°æ¥æµ‹é‡é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„ç›´æ¥æ¯”è¾ƒï¼Œä»¥åˆ†æè®­ç»ƒæ ·æœ¬çš„é‡è¦æ€§ï¼Œè¿™æ˜¯é€šè¿‡æ‰©æ•£å½’å› åˆ†æ•°ï¼ˆDASï¼‰å®ç°çš„ã€‚åŸºäºä¸¥è°¨çš„ç†è®ºåˆ†æï¼Œæˆ‘ä»¬é˜æ˜äº†DASçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åŠ é€ŸDASè®¡ç®—çš„ç­–ç•¥ï¼Œä¿ƒè¿›å…¶åœ¨å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬åœ¨å„ç§æ•°æ®é›†å’Œæ‰©æ•£æ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDASåœ¨çº¿æ€§æ•°æ®å»ºæ¨¡åˆ†æ•°æ–¹é¢æ˜¾è‘—è¶…è¿‡äº†ä»¥å‰çš„åŸºå‡†æµ‹è¯•ï¼Œå–å¾—äº†æœ€æ–°çš„æœ€ä½³æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jinxu-Lin/DAS">https://github.com/Jinxu-Lin/DAS</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18639v3">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€æ‰©æ•£æ¨¡å‹çš„æ™®åŠï¼Œç‰ˆæƒå’Œç§äººå›¾åƒçš„é”™è¯¯ä½¿ç”¨æˆä¸ºä¸»è¦å…³æ³¨ç‚¹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œäººä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ•°æ®å½’å±ï¼Œæ—¨åœ¨ç¡®å®šè®­ç»ƒæ ·æœ¬åœ¨ç”Ÿæˆæ¨¡å‹ä¸­çš„è´¡çŒ®ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡è¯„ä¼°åŒ…å«æˆ–æ’é™¤è®­ç»ƒæ ·æœ¬æ—¶çš„æ‰©æ•£æŸå¤±å˜åŒ–æ¥è¡¡é‡å…¶è´¡çŒ®ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸»å¼ ç›´æ¥ä½¿ç”¨æ‰©æ•£æŸå¤±æ— æ³•å‡†ç¡®åæ˜ è´¡çŒ®ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ‰©æ•£å½’å±åˆ†æ•°ï¼ˆDASï¼‰ï¼Œé€šè¿‡æµ‹é‡é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„ç›´æ¥æ¯”è¾ƒæ¥åˆ†æè®­ç»ƒæ ·æœ¬çš„é‡è¦æ€§ã€‚DASä¸ä»…å—åˆ°ä¸¥è°¨çš„ç†è®ºåˆ†ææ”¯æŒï¼Œè€Œä¸”æˆ‘ä»¬è¿˜æ¢ç´¢äº†åŠ é€ŸDASè®¡ç®—çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºå¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒDASåœ¨çº¿æ€§æ•°æ®å»ºæ¨¡åˆ†æ•°ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æ™®åŠå¸¦æ¥ç‰ˆæƒå’Œç§äººå›¾åƒè¯¯ç”¨é—®é¢˜ã€‚</li>
<li>æ•°æ®å½’å±æ˜¯ç¡®å®šè®­ç»ƒæ ·æœ¬åœ¨ç”Ÿæˆæ¨¡å‹ä¸­è´¡çŒ®çš„è§£å†³æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡è¯„ä¼°æ‰©æ•£æŸå¤±å˜åŒ–æ¥è¡¡é‡è®­ç»ƒæ ·æœ¬çš„è´¡çŒ®ã€‚</li>
<li>ç›´æ¥ä½¿ç”¨æ‰©æ•£æŸå¤±æ— æ³•å‡†ç¡®åæ˜ è´¡çŒ®ï¼Œå› ä¸ºé¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´çš„åå·®æ— æ³•ä»£è¡¨æ¨¡å‹è¡Œä¸ºçš„å·®å¼‚ã€‚</li>
<li>æå‡ºæ–°çš„æ–¹æ³•â€”â€”æ‰©æ•£å½’å±åˆ†æ•°ï¼ˆDASï¼‰ï¼Œé€šè¿‡æµ‹é‡é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„ç›´æ¥æ¯”è¾ƒæ¥åˆ†æè®­ç»ƒæ ·æœ¬çš„é‡è¦æ€§ã€‚</li>
<li>DASå—åˆ°ä¸¥è°¨çš„ç†è®ºåˆ†ææ”¯æŒï¼Œå¹¶æ¢ç´¢äº†åŠ é€Ÿè®¡ç®—çš„æ–¹æ³•ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDASåœ¨çº¿æ€§æ•°æ®å»ºæ¨¡åˆ†æ•°ä¸Šæ˜¾è‘—è¶…è¶Šå…ˆå‰æ–¹æ³•ï¼Œè¾¾åˆ°æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-896adf85dec304486ab7ce9ecdc7ea23.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SINGAPO-Single-Image-Controlled-Generation-of-Articulated-Parts-in-Objects"><a href="#SINGAPO-Single-Image-Controlled-Generation-of-Articulated-Parts-in-Objects" class="headerlink" title="SINGAPO: Single Image Controlled Generation of Articulated Parts in   Objects"></a>SINGAPO: Single Image Controlled Generation of Articulated Parts in   Objects</h2><p><strong>Authors:Jiayi Liu, Denys Iliash, Angel X. Chang, Manolis Savva, Ali Mahdavi-Amiri</strong></p>
<p>We address the challenge of creating 3D assets for household articulated objects from a single image. Prior work on articulated object creation either requires multi-view multi-state input, or only allows coarse control over the generation process. These limitations hinder the scalability and practicality for articulated object modeling. In this work, we propose a method to generate articulated objects from a single image. Observing the object in resting state from an arbitrary view, our method generates an articulated object that is visually consistent with the input image. To capture the ambiguity in part shape and motion posed by a single view of the object, we design a diffusion model that learns the plausible variations of objects in terms of geometry and kinematics. To tackle the complexity of generating structured data with attributes in multiple domains, we design a pipeline that produces articulated objects from high-level structure to geometric details in a coarse-to-fine manner, where we use a part connectivity graph and part abstraction as proxies. Our experiments show that our method outperforms the state-of-the-art in articulated object creation by a large margin in terms of the generated object realism, resemblance to the input image, and reconstruction quality. </p>
<blockquote>
<p>æˆ‘ä»¬åº”å¯¹äº†ä»å•ä¸€å›¾åƒä¸ºå®¶åº­å…³èŠ‚å¼ç‰©ä½“åˆ›å»º3Dèµ„äº§æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚å…ˆå‰å…³äºå…³èŠ‚å¼ç‰©ä½“åˆ›å»ºçš„å·¥ä½œè¦ä¹ˆéœ€è¦å¤šè§†è§’å¤šçŠ¶æ€è¾“å…¥ï¼Œè¦ä¹ˆåªèƒ½å¯¹ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œç²—ç•¥æ§åˆ¶ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†å…³èŠ‚å¼ç‰©ä½“å»ºæ¨¡çš„å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»å•ä¸€å›¾åƒç”Ÿæˆå…³èŠ‚å¼ç‰©ä½“çš„æ–¹æ³•ã€‚ä»ä»»æ„è§†è§’è§‚å¯Ÿç‰©ä½“å¤„äºé™æ­¢çŠ¶æ€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆä¸€ä¸ªä¸è¾“å…¥å›¾åƒè§†è§‰ä¸Šä¸€è‡´çš„å…³èŠ‚å¼ç‰©ä½“ã€‚ä¸ºäº†æ•æ‰ç”±ç‰©ä½“çš„å•ä¸€è§†å›¾å¼•èµ·çš„éƒ¨åˆ†å½¢çŠ¶å’Œè¿åŠ¨çš„ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥å­¦ä¹ ç‰©ä½“åœ¨å‡ ä½•å’ŒåŠ¨åŠ›å­¦æ–¹é¢çš„åˆç†å˜åŒ–ã€‚ä¸ºäº†è§£å†³ç”Ÿæˆå…·æœ‰å¤šä¸ªé¢†åŸŸå±æ€§çš„ç»“æ„åŒ–æ•°æ®çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç®¡é“ï¼Œä»¥ä»é«˜çº§ç»“æ„åˆ°å‡ ä½•ç»†èŠ‚çš„æ–¹å¼ï¼Œä»¥ç”±ç²—åˆ°ç»†çš„æ–¹å¼ç”Ÿæˆå…³èŠ‚å¼ç‰©ä½“ï¼Œå…¶ä¸­æˆ‘ä»¬ä½¿ç”¨éƒ¨ä»¶è¿æ¥å›¾å’Œéƒ¨ä»¶æŠ½è±¡ä½œä¸ºä»£ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆç‰©ä½“çš„é€¼çœŸæ€§ã€ä¸è¾“å…¥å›¾åƒçš„ç›¸ä¼¼æ€§ä»¥åŠé‡å»ºè´¨é‡æ–¹é¢å¤§å¤§è¶…è¶Šäº†å½“å‰å…³èŠ‚å¼ç‰©ä½“åˆ›å»ºçš„æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16499v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://3dlg-hcvc.github.io/singapo">https://3dlg-hcvc.github.io/singapo</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³ä»å•ä¸€å›¾åƒåˆ›å»º3Då®¶åº­å…³èŠ‚å¯¹è±¡çš„æŒ‘æˆ˜ã€‚æå‡ºä¸€ç§æ–¹æ³•ï¼Œä»…éœ€è§‚å¯Ÿç‰©ä½“çš„é™æ­¢çŠ¶æ€å³å¯ç”Ÿæˆå…³èŠ‚å¯¹è±¡ï¼Œä¸è¾“å…¥å›¾åƒè§†è§‰ä¸€è‡´ã€‚è®¾è®¡æ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ å¯¹è±¡åœ¨å‡ ä½•å’ŒåŠ¨åŠ›å­¦æ–¹é¢çš„åˆç†å˜åŒ–ï¼Œè§£å†³å•ä¸€è§†å›¾ç‰©ä½“éƒ¨åˆ†å½¢çŠ¶å’Œè¿åŠ¨æ¨¡ç³Šçš„é—®é¢˜ã€‚é‡‡ç”¨ä»é«˜çº§ç»“æ„åˆ°å‡ ä½•ç»†èŠ‚çš„ç²—åˆ°ç»†ç®¡é“ç”Ÿæˆå…³èŠ‚å¯¹è±¡ï¼Œä½¿ç”¨éƒ¨åˆ†è¿æ¥å›¾å’Œéƒ¨åˆ†æŠ½è±¡ä½œä¸ºä»£ç†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ›å»ºå…³èŠ‚å¯¹è±¡çš„çœŸå®æ€§ã€å¯¹è¾“å…¥å›¾åƒçš„ç›¸ä¼¼æ€§ä»¥åŠé‡å»ºè´¨é‡æ–¹é¢å¤§å¹…è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å†³äº†ä»å•ä¸€å›¾åƒåˆ›å»º3Då®¶åº­å…³èŠ‚å¯¹è±¡çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–¹æ³•ï¼Œä»…é€šè¿‡è§‚å¯Ÿç‰©ä½“çš„é™æ­¢çŠ¶æ€å³å¯ç”Ÿæˆå…³èŠ‚å¯¹è±¡ã€‚</li>
<li>è®¾è®¡çš„æ‰©æ•£æ¨¡å‹èƒ½å­¦ä¹ å¯¹è±¡åœ¨å‡ ä½•å’ŒåŠ¨åŠ›å­¦æ–¹é¢çš„åˆç†å˜åŒ–ã€‚</li>
<li>è§£å†³å•ä¸€è§†å›¾ä¸‹ç‰©ä½“éƒ¨åˆ†å½¢çŠ¶å’Œè¿åŠ¨çš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨ä»é«˜çº§ç»“æ„åˆ°å‡ ä½•ç»†èŠ‚çš„ç²—åˆ°ç»†ç®¡é“ç”Ÿæˆå¯¹è±¡ã€‚</li>
<li>ä½¿ç”¨éƒ¨åˆ†è¿æ¥å›¾å’Œéƒ¨åˆ†æŠ½è±¡ä½œä¸ºç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä»£ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dba274938ad32a004f31ecfa0546ce60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9d7adfbab7591689948ad0428ec42ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d1383a1c267fbba2ff31f432db95cfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a4ebf62f585acdc945e4b8b47eaef23.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Causal-Deciphering-and-Inpainting-in-Spatio-Temporal-Dynamics-via-Diffusion-Model"><a href="#Causal-Deciphering-and-Inpainting-in-Spatio-Temporal-Dynamics-via-Diffusion-Model" class="headerlink" title="Causal Deciphering and Inpainting in Spatio-Temporal Dynamics via   Diffusion Model"></a>Causal Deciphering and Inpainting in Spatio-Temporal Dynamics via   Diffusion Model</h2><p><strong>Authors:Yifan Duan, Jian Zhao,  pengcheng, Junyuan Mao, Hao Wu, Jingyu Xu, Shilong Wang, Caoyuan Ma, Kai Wang, Kun Wang, Xuelong Li</strong></p>
<p>Spatio-temporal (ST) prediction has garnered a De facto attention in earth sciences, such as meteorological prediction, human mobility perception. However, the scarcity of data coupled with the high expenses involved in sensor deployment results in notable data imbalances. Furthermore, models that are excessively customized and devoid of causal connections further undermine the generalizability and interpretability. To this end, we establish a causal framework for ST predictions, termed CaPaint, which targets to identify causal regions in data and endow model with causal reasoning ability in a two-stage process. Going beyond this process, we utilize the back-door adjustment to specifically address the sub-regions identified as non-causal in the upstream phase. Specifically, we employ a novel image inpainting technique. By using a fine-tuned unconditional Diffusion Probabilistic Model (DDPM) as the generative prior, we in-fill the masks defined as environmental parts, offering the possibility of reliable extrapolation for potential data distributions. CaPaint overcomes the high complexity dilemma of optimal ST causal discovery models by reducing the data generation complexity from exponential to quasi-linear levels. Extensive experiments conducted on five real-world ST benchmarks demonstrate that integrating the CaPaint concept allows models to achieve improvements ranging from 4.3% to 77.3%. Moreover, compared to traditional mainstream ST augmenters, CaPaint underscores the potential of diffusion models in ST enhancement, offering a novel paradigm for this field. Our project is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/12345-DFCC">https://anonymous.4open.science/r/12345-DFCC</a>. </p>
<blockquote>
<p>æ—¶ç©ºé¢„æµ‹åœ¨åœ°çƒç§‘å­¦é¢†åŸŸï¼Œå¦‚æ°”è±¡é¢„æµ‹å’Œäººç±»ç§»åŠ¨æ„ŸçŸ¥ä¸­ï¼Œå·²ç»å¾—åˆ°äº†å¹¿æ³›çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œæ•°æ®çš„ç¨€ç¼ºæ€§ä»¥åŠä¼ æ„Ÿå™¨éƒ¨ç½²çš„é«˜æˆæœ¬å¯¼è‡´äº†æ•°æ®ä¸å¹³è¡¡çš„æ˜¾è‘—é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿‡åº¦å®šåˆ¶ä¸”ç¼ºä¹å› æœè”ç³»çš„æ¨¡å‹è¿›ä¸€æ­¥å‰Šå¼±äº†å…¶é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†æ—¶ç©ºé¢„æµ‹çš„å› æœæ¡†æ¶ï¼Œç§°ä¸ºCaPaintã€‚è¯¥æ¡†æ¶æ—¨åœ¨è¯†åˆ«æ•°æ®ä¸­çš„å› æœåŒºåŸŸï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹èµ‹äºˆæ¨¡å‹å› æœæ¨ç†èƒ½åŠ›ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨åé—¨è°ƒæ•´æ¥ä¸“é—¨è§£å†³ä¸Šæ¸¸é˜¶æ®µè¯†åˆ«ä¸ºéå› æœçš„å­åŒºåŸŸã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„å›¾åƒä¿®å¤æŠ€æœ¯ã€‚é€šè¿‡ä½¿ç”¨å¾®è°ƒçš„æ— æ¡ä»¶æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ä½œä¸ºç”Ÿæˆå…ˆéªŒï¼Œæˆ‘ä»¬å¡«å……äº†å®šä¹‰ä¸ºç¯å¢ƒéƒ¨åˆ†çš„æ©è†œï¼Œä¸ºæ½œåœ¨æ•°æ®åˆ†å¸ƒæä¾›äº†å¯é å¤–æ¨çš„å¯èƒ½æ€§ã€‚CaPainté€šè¿‡å°†æ•°æ®ç”Ÿæˆå¤æ‚æ€§ä»æŒ‡æ•°çº§åˆ«é™ä½åˆ°å‡†çº¿æ€§çº§åˆ«ï¼Œå…‹æœäº†æœ€ä¼˜æ—¶ç©ºå› æœå‘ç°æ¨¡å‹çš„é«˜å¤æ‚æ€§å›°å¢ƒã€‚åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œçš„æ—¶ç©ºåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œèå…¥CaPaintæ¦‚å¿µåï¼Œæ¨¡å‹çš„æ”¹è¿›èŒƒå›´ä»4.3%åˆ°77.3%ã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿçš„ä¸»æµæ—¶ç©ºå¢å¼ºå™¨ç›¸æ¯”ï¼ŒCaPaintçªæ˜¾äº†æ‰©æ•£æ¨¡å‹åœ¨æ—¶ç©ºå¢å¼ºæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè¿™ä¸€é¢†åŸŸæä¾›äº†æ–°çš„èŒƒå¼ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/%E7%AC%BC%E8%AE%BA-%DF%CC-%E9%A1%B5%E4%B8%8A%E5%8F%AF%E4%BB%A5%E6%89%BE%E5%88%B0%E6%88%91%E7%9A%84%E9%A1%B9%E7%9B%AE%E3%80%82">https://anonymous.4open.science/r/12345-DFCCä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19608v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ•°æ®åœ¨æ—¶ç©ºé¢„æµ‹ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†å…¶ç¨€ç¼ºæ€§å’Œé«˜æ˜‚çš„ä¼ æ„Ÿå™¨éƒ¨ç½²æˆæœ¬å¯¼è‡´äº†æ˜¾è‘—çš„æ•°æ®ä¸å¹³è¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å› æœæ—¶ç©ºé¢„æµ‹æ¡†æ¶CaPaintï¼Œæ—¨åœ¨è¯†åˆ«æ•°æ®ä¸­çš„å› æœåŒºåŸŸï¼Œèµ‹äºˆæ¨¡å‹å› æœæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨åå‘è°ƒæ•´æ³•è§£å†³ä¸Šæ¸¸é˜¶æ®µè¯†åˆ«çš„éå› æœå­åŒºåŸŸé—®é¢˜ï¼Œå¹¶é‡‡ç”¨æ–°å‹å›¾åƒå¡«å……æŠ€æœ¯ã€‚ç»“åˆå¾®è°ƒçš„æ— æ¡ä»¶æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œå¯¹å®šä¹‰ä¸ºç¯å¢ƒéƒ¨åˆ†çš„æ©ç è¿›è¡Œå¡«å……ï¼Œä¸ºæ½œåœ¨æ•°æ®åˆ†å¸ƒæä¾›å¯é çš„é¢„æµ‹å¯èƒ½æ€§ã€‚CaPaintå°†æœ€ä¼˜æ—¶ç©ºå› æœå‘ç°æ¨¡å‹çš„é«˜å¤æ‚åº¦é—®é¢˜ä»æŒ‡æ•°çº§åˆ«é™ä½åˆ°å‡†çº¿æ€§çº§åˆ«ã€‚åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œçš„æ—¶ç©ºåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œèå…¥CaPaintæ¦‚å¿µä½¿å¾—æ¨¡å‹æ”¹è¿›èŒƒå›´ä»4.3%è‡³77.3%ã€‚å¯¹æ¯”ä¼ ç»Ÿä¸»æµæ—¶ç©ºå¢å¼ºå™¨ï¼ŒCaPaintçªæ˜¾äº†æ‰©æ•£æ¨¡å‹åœ¨æ—¶ç©ºå¢å¼ºçš„æ½œåŠ›ï¼Œä¸ºè¿™ä¸€é¢†åŸŸæä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ—¶ç©ºé¢„æµ‹åœ¨åœ°çƒç§‘å­¦é¢†åŸŸå—åˆ°é‡è§†ï¼Œå¦‚æ°”è±¡é¢„æµ‹å’Œäººä¸ºç§»åŠ¨æ„ŸçŸ¥ï¼Œä½†æ•°æ®ç¨€ç¼ºå’Œä¸å¹³è¡¡æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºåä¸ºCaPaintçš„å› æœæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ—¶ç©ºé¢„æµ‹çš„å› æœé—®é¢˜ï¼ŒåŒ…æ‹¬è¯†åˆ«å› æœåŒºåŸŸå¹¶ä¸ºæ¨¡å‹èµ‹äºˆå› æœæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨åå‘è°ƒæ•´æ³•å¤„ç†ä¸Šæ¸¸é˜¶æ®µè¯†åˆ«çš„éå› æœå­åŒºåŸŸï¼Œå¹¶é‡‡ç”¨æ–°é¢–çš„å›¾åƒå¡«å……æŠ€æœ¯æ¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆå¾®è°ƒçš„æ— æ¡ä»¶æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ä½œä¸ºç”Ÿæˆå…ˆéªŒï¼Œå¯¹å®šä¹‰ä¸ºç¯å¢ƒéƒ¨åˆ†çš„æ©ç è¿›è¡Œå¡«å……ï¼Œå®ç°å¯é çš„æ•°æ®åˆ†å¸ƒå¤–æ¨ã€‚</li>
<li>CaPaintæˆåŠŸé™ä½æœ€ä¼˜æ—¶ç©ºå› æœå‘ç°æ¨¡å‹çš„æ•°æ®ç”Ÿæˆå¤æ‚åº¦ã€‚</li>
<li>åœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šï¼Œèå…¥CaPaintçš„æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œæ”¹è¿›èŒƒå›´å¹¿æ³›ã€‚</li>
<li>å¯¹æ¯”å…¶ä»–ä¸»æµæ—¶ç©ºå¢å¼ºæŠ€æœ¯ï¼ŒCaPaintå‡¸æ˜¾æ‰©æ•£æ¨¡å‹åœ¨æ—¶ç©ºå¢å¼ºä¸­çš„æ½œåŠ›ï¼Œä¸ºç›¸å…³é¢†åŸŸæä¾›æ–°çš„è§†è§’å’Œç ”ç©¶è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec287a58ffa2ca546a398b19e4481b69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f28d10238da464aa10815bd2a4e5c72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4766984dacb8a24f86da83292d47d5cc.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Diffusion-Driven-Semantic-Communication-for-Generative-Models-with-Bandwidth-Constraints"><a href="#Diffusion-Driven-Semantic-Communication-for-Generative-Models-with-Bandwidth-Constraints" class="headerlink" title="Diffusion-Driven Semantic Communication for Generative Models with   Bandwidth Constraints"></a>Diffusion-Driven Semantic Communication for Generative Models with   Bandwidth Constraints</h2><p><strong>Authors:Lei Guo, Wei Chen, Yuxuan Sun, Bo Ai, Nikolaos Pappas, Tony Quek</strong></p>
<p>Diffusion models have been extensively utilized in AI-generated content (AIGC) in recent years, thanks to the superior generation capabilities. Combining with semantic communications, diffusion models are used for tasks such as denoising, data reconstruction, and content generation. However, existing diffusion-based generative models do not consider the stringent bandwidth limitation, which limits its application in wireless communication. This paper introduces a diffusion-driven semantic communication framework with advanced VAE-based compression for bandwidth-constrained generative model. Our designed architecture utilizes the diffusion model, where the signal transmission process through the wireless channel acts as the forward process in diffusion. To reduce bandwidth requirements, we incorporate a downsampling module and a paired upsampling module based on a variational auto-encoder with reparameterization at the receiver to ensure that the recovered features conform to the Gaussian distribution. Furthermore, we derive the loss function for our proposed system and evaluate its performance through comprehensive experiments. Our experimental results demonstrate significant improvements in pixel-level metrics such as peak signal to noise ratio (PSNR) and semantic metrics like learned perceptual image patch similarity (LPIPS). These enhancements are more profound regarding the compression rates and SNR compared to deep joint source-channel coding (DJSCC). </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ç”±äºå…¶å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›ï¼Œè¿‘å¹´æ¥åœ¨äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç»“åˆè¯­ä¹‰é€šä¿¡ï¼Œæ‰©æ•£æ¨¡å‹è¢«ç”¨äºå»å™ªã€æ•°æ®é‡å»ºå’Œå†…å®¹ç”Ÿæˆç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹å¹¶æ²¡æœ‰è€ƒè™‘åˆ°ä¸¥æ ¼çš„å¸¦å®½é™åˆ¶ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æ— çº¿é€šä¿¡ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ‰©æ•£é©±åŠ¨çš„è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰å…ˆè¿›çš„VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰å‹ç¼©æŠ€æœ¯ï¼Œé€‚ç”¨äºå¸¦å®½å—é™çš„ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬è®¾è®¡çš„æ¶æ„åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­é€šè¿‡æ— çº¿ä¿¡é“ä¼ è¾“ä¿¡å·çš„è¿‡ç¨‹å……å½“æ‰©æ•£çš„å‰å‘è¿‡ç¨‹ã€‚ä¸ºäº†å‡å°‘å¸¦å®½è¦æ±‚ï¼Œæˆ‘ä»¬èå…¥äº†ä¸‹é‡‡æ ·æ¨¡å—å’ŒåŸºäºå˜åˆ†è‡ªç¼–ç å™¨çš„é…å¯¹ä¸Šé‡‡æ ·æ¨¡å—ï¼Œå¹¶åœ¨æ¥æ”¶å™¨è¿›è¡Œé‡å‚æ•°åŒ–ï¼Œä»¥ç¡®ä¿æ¢å¤çš„ç‰¹å¾ç¬¦åˆé«˜æ–¯åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å¯¼äº†æ‰€æç³»ç»Ÿçš„æŸå¤±å‡½æ•°ï¼Œå¹¶é€šè¿‡å…¨é¢çš„å®éªŒè¯„ä¼°äº†å…¶æ€§èƒ½ã€‚å®éªŒç»“æœåœ¨åƒç´ çº§çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œè¯­ä¹‰çº§çš„æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰ç­‰æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ç›¸è¾ƒäºæ·±åº¦è”åˆæºä¿¡é“ç¼–ç ï¼ˆDJSCCï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‹ç¼©ç‡å’Œä¿¡å™ªæ¯”æ–¹é¢è¡¨ç°å‡ºäº†æ›´ä¸ºæ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18468v2">PDF</a> accepted to IEEE for possible publication</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œåˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¿›è¡Œå‹ç¼©ï¼Œä»¥åº”å¯¹å¸¦å®½é™åˆ¶ä¸‹çš„ç”Ÿæˆæ¨¡å‹åº”ç”¨é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†æ— çº¿ä¿¡é“ä¸­çš„ä¿¡å·ä¼ è¾“è¿‡ç¨‹è§†ä¸ºæ‰©æ•£çš„æ­£å‘è¿‡ç¨‹ï¼Œé€šè¿‡åŠ å…¥ä¸‹é‡‡æ ·æ¨¡å—å’Œä¸Šé‡‡æ ·æ¨¡å—æ¥é™ä½å¸¦å®½è¦æ±‚ï¼Œå¹¶åœ¨æ¥æ”¶å™¨ç«¯è¿›è¡Œé‡å‚æ•°åŒ–ï¼Œç¡®ä¿æ¢å¤çš„ç‰¹å¾ç¬¦åˆé«˜æ–¯åˆ†å¸ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨åƒç´ çº§å’Œè¯­ä¹‰çº§æŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰ä¸­å¹¿æ³›åº”ç”¨ï¼Œç”¨äºé™å™ªã€æ•°æ®é‡å»ºå’Œå†…å®¹ç”Ÿæˆç­‰ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ‰©æ•£ç”Ÿæˆæ¨¡å‹æœªè€ƒè™‘ä¸¥æ ¼çš„å¸¦å®½é™åˆ¶ï¼Œé™åˆ¶äº†å…¶åœ¨æ— çº¿é€šä¿¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œä»¥è§£å†³å¸¦å®½é™åˆ¶é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¿›è¡Œå‹ç¼©ï¼Œå¹¶åŠ å…¥ä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·æ¨¡å—ä»¥é™ä½å¸¦å®½è¦æ±‚ã€‚</li>
<li>æ¡†æ¶å°†æ— çº¿ä¿¡é“ä¸­çš„ä¿¡å·ä¼ è¾“è¿‡ç¨‹è§†ä¸ºæ‰©æ•£çš„æ­£å‘è¿‡ç¨‹ã€‚</li>
<li>æ¥æ”¶å™¨ç«¯è¿›è¡Œé‡å‚æ•°åŒ–ï¼Œç¡®ä¿æ¢å¤çš„ç‰¹å¾ç¬¦åˆé«˜æ–¯åˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3e307fa1243c60c724c28b0fd1105a54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97056f0b43e006e54b2c0c0cf8a8659f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f35745e2f1cbe47b1b629f5ebb90a1bb.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1cf4dc24bc373ca0d837435871375477.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  Attentional Triple-Encoder Network in Spatiospectral Domains for Medical   Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-8442cae68b84efc64254cdf2e747c143.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  GAN-enhanced Simulation-driven DNN Testing in Absence of Ground Truth
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14807.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
