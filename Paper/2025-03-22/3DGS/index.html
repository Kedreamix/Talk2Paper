<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-49aed1bd4f31f94caeaddc75b9eb1cdd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-22-æ›´æ–°"><a href="#2025-03-22-æ›´æ–°" class="headerlink" title="2025-03-22 æ›´æ–°"></a>2025-03-22 æ›´æ–°</h1><h2 id="1000-FPS-4D-Gaussian-Splatting-for-Dynamic-Scene-Rendering"><a href="#1000-FPS-4D-Gaussian-Splatting-for-Dynamic-Scene-Rendering" class="headerlink" title="1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering"></a>1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering</h2><p><strong>Authors:Yuheng Yuan, Qiuhong Shen, Xingyi Yang, Xinchao Wang</strong></p>
<p>4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) \textbf{Short-Lifespan Gaussians}: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) \textbf{Inactive Gaussians}: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present \textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a $41\times$ reduction in storage and $9\times$ faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at <a target="_blank" rel="noopener" href="https://4dgs-1k.github.io/">https://4DGS-1K.github.io</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œ4Dé«˜æ–¯ç‚¹é“ºå±•æ³•ï¼ˆ4DGSï¼‰å› å…¶é‡å»ºåŠ¨æ€åœºæ™¯çš„æ–¹æ³•è€Œå¤‡å—å…³æ³¨ã€‚å°½ç®¡å…¶èƒ½è¾¾æˆé«˜å“è´¨æ•ˆæœï¼Œä½†4DGSé€šå¸¸éœ€è¦å¤§é‡å­˜å‚¨ç©ºé—´å¹¶é¢ä¸´æ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†è¿™äº›é—®é¢˜ï¼Œå¹¶è¯†åˆ«å‡ºä¸¤ä¸ªä¸»è¦çš„æ—¶ç©ºå†—ä½™æ¥æºã€‚ï¼ˆQ1ï¼‰<strong>çŸ­æš‚å¯¿å‘½é«˜æ–¯ç‚¹</strong>ï¼š4DGSä½¿ç”¨å¤§é‡çŸ­æš‚æ—¶ç©ºè·¨åº¦çš„é«˜æ–¯ç‚¹æ¥è¡¨ç¤ºåœºæ™¯åŠ¨æ€ï¼Œå¯¼è‡´é«˜æ–¯ç‚¹æ•°é‡è¿‡å¤šã€‚ï¼ˆQ2ï¼‰<strong>éæ´»è·ƒé«˜æ–¯ç‚¹</strong>ï¼šåœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­ï¼Œåªæœ‰ä¸€å°éƒ¨åˆ†é«˜æ–¯ç‚¹å¯¹æ¯ä¸€å¸§æœ‰æ‰€è´¡çŒ®ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ‰€æœ‰é«˜æ–¯ç‚¹åœ¨å…‰æ …åŒ–æ—¶éƒ½è¢«å¤„ç†ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å†—ä½™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›å†—ä½™é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>4DGS-1K</strong>ï¼Œåœ¨ç°ä»£GPUä¸Šè¿è¡Œé€Ÿåº¦è¶…è¿‡1000å¸§æ¯ç§’ã€‚é’ˆå¯¹Q1é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶ç©ºå˜åŒ–åˆ†æ•°è¿™ä¸€æ–°çš„ä¿®å‰ªæ ‡å‡†ï¼Œè¯¥æ ‡å‡†æœ‰æ•ˆåœ°ç§»é™¤äº†çŸ­æš‚å¯¿å‘½çš„é«˜æ–¯ç‚¹å¹¶é¼“åŠ±ä½¿ç”¨å…·æœ‰è¾ƒé•¿æ—¶ç©ºè·¨åº¦çš„é«˜æ–¯ç‚¹æ¥æ•æ‰åœºæ™¯åŠ¨æ€ã€‚é’ˆå¯¹Q2é—®é¢˜ï¼Œæˆ‘ä»¬å­˜å‚¨äº†è¿ç»­å¸§ä¸­æ´»è·ƒé«˜æ–¯ç‚¹çš„æ©è†œï¼Œæ˜¾è‘—å‡å°‘äº†æ¸²æŸ“è¿‡ç¨‹ä¸­çš„å†—ä½™è®¡ç®—ã€‚ä¸å¸¸è§„4DGSç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å­˜å‚¨ç©ºé—´çš„41å€å‹ç¼©å’Œå¤æ‚åŠ¨æ€åœºæ™¯çš„æ¸²æŸ“é€Ÿåº¦9å€æå‡ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„å¯è§†è´¨é‡ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢<a target="_blank" rel="noopener" href="https://4dgs-1k.github.ioäº†è§£æ›´å¤šä¿¡æ¯./">https://4DGS-1K.github.ioäº†è§£æ›´å¤šä¿¡æ¯ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16422v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†4Dé«˜æ–¯å±•å¹³æŠ€æœ¯ï¼ˆ4DGSï¼‰åœ¨å¤„ç†åŠ¨æ€åœºæ™¯é‡å»ºæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚å­˜å‚¨éœ€æ±‚å¤§ã€æ¸²æŸ“é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªå…³é”®çš„æ—¶é—´å†—ä½™æ¥æºï¼šçŸ­å¯¿å‘½é«˜æ–¯å’Œæœªæ¿€æ´»çš„é«˜æ–¯ã€‚é’ˆå¯¹è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¼•å…¥ç©ºé—´æ—¶é—´å˜åŒ–è¯„åˆ†ä½œä¸ºæ–°çš„ä¿®å‰ªæ ‡å‡†ï¼Œä»¥å»é™¤çŸ­å¯¿å‘½é«˜æ–¯ï¼›å¹¶ä¸ºè¿ç»­å¸§ä¸­çš„æ´»è·ƒé«˜æ–¯å­˜å‚¨æ©è†œï¼Œä»è€Œå‡å°‘æ¸²æŸ“è¿‡ç¨‹ä¸­çš„å†—ä½™è®¡ç®—ã€‚å› æ­¤ï¼Œæ–°çš„æ–¹æ³•åœ¨ä¿è¯è§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†å­˜å‚¨å‡å°‘41å€ã€å¤æ‚åŠ¨æ€åœºæ™¯çš„æ¸²æŸ“é€Ÿåº¦æé«˜9å€ã€‚è¯¦æƒ…è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®ç½‘é¡µã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>4DGSæŠ€æœ¯åœ¨å¤„ç†åŠ¨æ€åœºæ™¯é‡å»ºæ—¶é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬å­˜å‚¨éœ€æ±‚å¤§å’Œæ¸²æŸ“é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºäº†ä¸¤ä¸ªå…³é”®çš„æ—¶é—´å†—ä½™æ¥æºï¼šçŸ­å¯¿å‘½é«˜æ–¯å’Œæœªæ¿€æ´»çš„é«˜æ–¯ã€‚</li>
<li>é€šè¿‡å¼•å…¥ç©ºé—´æ—¶é—´å˜åŒ–è¯„åˆ†ä½œä¸ºæ–°çš„ä¿®å‰ªæ ‡å‡†ï¼Œå¯æœ‰æ•ˆå»é™¤çŸ­å¯¿å‘½é«˜æ–¯ã€‚</li>
<li>ä¸ºè¿ç»­å¸§ä¸­çš„æ´»è·ƒé«˜æ–¯å­˜å‚¨æ©è†œï¼Œä»¥å‡å°‘æ¸²æŸ“è¿‡ç¨‹ä¸­çš„å†—ä½™è®¡ç®—ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„4DGSç›¸æ¯”ï¼Œæ–°æ–¹æ³•å®ç°äº†å­˜å‚¨çš„æ˜¾è‘—å‡å°‘å’Œæ¸²æŸ“é€Ÿåº¦çš„æ˜¾è‘—æé«˜ã€‚</li>
<li>é¡¹ç›®è¯¦ç»†ä¿¡æ¯å¯è®¿é—®å…¶é¡¹ç›®ç½‘é¡µè·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7181eb55a60559a766973025680ff3bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e307f6f84588ec38c34e6d297ad7c3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b104379fa86b03939609494db9ad7c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7167631541b249590bd1ce1b99d1586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b3736ff6f10dc5d8bcc38bd298c6e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3709f60772ca8f51cbd8dbf9183d1fe.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="M3-3D-Spatial-MultiModal-Memory"><a href="#M3-3D-Spatial-MultiModal-Memory" class="headerlink" title="M3: 3D-Spatial MultiModal Memory"></a>M3: 3D-Spatial MultiModal Memory</h2><p><strong>Authors:Xueyan Zou, Yuchen Song, Ri-Zhao Qiu, Xuanbin Peng, Jianglong Ye, Sifei Liu, Xiaolong Wang</strong></p>
<p>We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs&#x2F;LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3â€™s feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†3Dç©ºé—´å¤šæ¨¡æ€è®°å¿†ï¼ˆM3ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡è§†é¢‘æºä¸ºè§†è§‰æ„ŸçŸ¥ä¿ç•™ä¸­ç­‰è§„æ¨¡é™æ€åœºæ™¯çš„ä¿¡æ¯ã€‚é€šè¿‡æ•´åˆ3Dé«˜æ–¯æ‘Šé“ºæŠ€æœ¯ä¸åŸºç¡€æ¨¡å‹ï¼ŒM3å»ºç«‹äº†ä¸€ä¸ªå¤šæ¨¡æ€è®°å¿†ï¼Œèƒ½å¤Ÿåœ¨å„ä¸ªç²’åº¦ä¸Šå‘ˆç°ç‰¹å¾è¡¨ç¤ºï¼Œæ¶µç›–å¹¿æ³›çš„çŸ¥è¯†ã€‚åœ¨æ¢ç´¢ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ä¹‹å‰ç‰¹å¾é“ºè®¾å·¥ä½œä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š(1)ä¸ºæ¯ä¸ªé«˜æ–¯åŸå§‹æ•°æ®ä¿å­˜é«˜ç»´ç‰¹å¾çš„è®¡ç®—çº¦æŸï¼›(2)æç‚¼ç‰¹å¾ä¸åŸºç¡€æ¨¡å‹ç‰¹å¾ä¹‹é—´çš„ä¸åŒ¹é…æˆ–ä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†M3ï¼Œå…¶ä¸»è¦ç»„ä»¶åŒ…æ‹¬ä¸»è¦åœºæ™¯ç»„ä»¶å’Œé«˜æ–¯è®°å¿†æ³¨æ„åŠ›ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†ã€‚ä¸ºäº†éªŒè¯M3ï¼Œæˆ‘ä»¬å¯¹ç‰¹å¾ç›¸ä¼¼æ€§å’Œä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œäº†å…¨é¢çš„å®šé‡è¯„ä¼°ï¼Œä»¥åŠå®šæ€§å¯è§†åŒ–ä»¥çªå‡ºé«˜æ–¯è®°å¿†æ³¨æ„åŠ›çš„åƒç´ è½¨è¿¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶µç›–äº†å¤šç§åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€æ„ŸçŸ¥æ¨¡å‹ä»¥åŠå¤§å‹å¤šæ¨¡æ€å’Œè¯­è¨€æ¨¡å‹ï¼ˆLMMs&#x2F;LLMsï¼‰ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯æ˜M3åœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨å››è¶³æœºå™¨äººçš„å®¤å†…åœºæ™¯ä¸­åº”ç”¨äº†M3çš„ç‰¹å¾å­—æ®µã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å£°ç§°M3æ˜¯é¦–ä¸ªè§£å†³3Dç‰¹å¾è’¸é¦ä¸­æ ¸å¿ƒå‹ç¼©æŒ‘æˆ˜çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16413v1">PDF</a> ICLR2025 homepage: <a target="_blank" rel="noopener" href="https://m3-spatial-memory.github.io/">https://m3-spatial-memory.github.io</a> code:   <a target="_blank" rel="noopener" href="https://github.com/MaureenZOU/m3-spatial">https://github.com/MaureenZOU/m3-spatial</a></p>
<p><strong>Summary</strong><br>    æå‡ºä¸€ç§åä¸ºM3çš„3Dç©ºé—´å¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿï¼Œé‡‡ç”¨è§†é¢‘æºå¯¹ä¸­ç­‰é™æ€åœºæ™¯è¿›è¡Œä¿¡æ¯å­˜å‚¨ï¼Œä¸ºè§†è§‰æ„ŸçŸ¥è®¾è®¡å¤šæ¨¡æ€è®°å¿†ã€‚é€šè¿‡ç»“åˆ3Dé«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯å’ŒåŸºç¡€æ¨¡å‹ï¼ŒM3å»ºç«‹äº†ä¸€ä¸ªå¤šæ¨¡æ€è®°å¿†ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒç²’åº¦ä¸Šå‘ˆç°ç‰¹å¾è¡¨ç¤ºï¼Œæ¶µç›–å¹¿æ³›çš„çŸ¥è¯†ã€‚ç ”ç©¶è§£å†³äº†å…ˆå‰ç‰¹å¾æ¶‚æŠ¹å·¥ä½œä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡ä¸»è¦åœºæ™¯ç»„ä»¶å’Œé«˜æ–¯è®°å¿†æ³¨æ„åŠ›ç­‰å…³é”®ç»„ä»¶å®ç°é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†ã€‚é€šè¿‡å®šé‡è¯„ä¼°ç‰¹å¾ç›¸ä¼¼æ€§å’Œä¸‹æ¸¸ä»»åŠ¡ä»¥åŠå®šæ€§å¯è§†åŒ–çªå‡ºæ˜¾ç¤ºé«˜æ–¯è®°å¿†æ³¨æ„åŠ›çš„åƒç´ è½¨è¿¹æ¥éªŒè¯M3ã€‚æ­¤å¤–ï¼ŒM3è¿˜é€‚ç”¨äºå®¤å†…åœºæ™¯çš„å››è‚¢æœºå™¨äººéƒ¨ç½²ã€‚M3æ˜¯é¦–ä¸ªè§£å†³3Dç‰¹å¾è’¸é¦ä¸­æ ¸å¿ƒå‹ç¼©æŒ‘æˆ˜çš„å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>M3æ˜¯ä¸€ä¸ª3Dç©ºé—´å¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡è§†é¢‘æºå­˜å‚¨ä¸­ç­‰é™æ€åœºæ™¯çš„ä¿¡æ¯ã€‚</li>
<li>M3ç»“åˆäº†3Dé«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯å’ŒåŸºç¡€æ¨¡å‹ï¼Œå»ºç«‹äº†ä¸€ä¸ªå¤šæ¨¡æ€è®°å¿†ï¼Œèƒ½å‘ˆç°ä¸åŒç²’åº¦çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>ç ”ç©¶è§£å†³äº†å…ˆå‰ç‰¹å¾æ¶‚æŠ¹å·¥ä½œä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šé«˜ç»´ç‰¹å¾çš„å­˜å‚¨è®¡ç®—çº¦æŸä»¥åŠè’¸é¦ç‰¹å¾ä¸åŸºç¡€æ¨¡å‹ç‰¹å¾ä¹‹é—´çš„ä¸åŒ¹é…æˆ–ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>M3é€šè¿‡ä¸»è¦åœºæ™¯ç»„ä»¶å’Œé«˜æ–¯è®°å¿†æ³¨æ„åŠ›ç­‰å…³é”®ç»„ä»¶å®ç°é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†ã€‚</li>
<li>é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°éªŒè¯äº†M3çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ç‰¹å¾ç›¸ä¼¼æ€§ã€ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä»¥åŠé«˜æ–¯è®°å¿†æ³¨æ„åŠ›çš„å¯è§†åŒ–ã€‚</li>
<li>M3é€‚ç”¨äºå®¤å†…åœºæ™¯çš„å››è‚¢æœºå™¨äººéƒ¨ç½²ï¼Œå±•ç¤ºäº†å…¶å®é™…åº”ç”¨çš„æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dff9ab96a997bec35448ab117431b450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9c4e572fbfc8defcb314123219752b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62677e8c27e882f152540f73b0d5a3dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90cd95f1f80a8632c11f6aca19e5beff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49aed1bd4f31f94caeaddc75b9eb1cdd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Gaussian-Graph-Network-Learning-Efficient-and-Generalizable-Gaussian-Representations-from-Multi-view-Images"><a href="#Gaussian-Graph-Network-Learning-Efficient-and-Generalizable-Gaussian-Representations-from-Multi-view-Images" class="headerlink" title="Gaussian Graph Network: Learning Efficient and Generalizable Gaussian   Representations from Multi-view Images"></a>Gaussian Graph Network: Learning Efficient and Generalizable Gaussian   Representations from Multi-view Images</h2><p><strong>Authors:Shengjun Zhang, Xin Fei, Fangfu Liu, Haixu Song, Yueqi Duan</strong></p>
<p>3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis performance. While conventional methods require per-scene optimization, more recently several feed-forward methods have been proposed to generate pixel-aligned Gaussian representations with a learnable network, which are generalizable to different scenes. However, these methods simply combine pixel-aligned Gaussians from multiple views as scene representations, thereby leading to artifacts and extra memory cost without fully capturing the relations of Gaussians from different images. In this paper, we propose Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian representations. Specifically, we construct Gaussian Graphs to model the relations of Gaussian groups from different views. To support message passing at Gaussian level, we reformulate the basic graph operations over Gaussian representations, enabling each Gaussian to benefit from its connected Gaussian groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling layer to aggregate various Gaussian groups for efficient representations. We conduct experiments on the large-scale RealEstate10K and ACID datasets to demonstrate the efficiency and generalization of our method. Compared to the state-of-the-art methods, our model uses fewer Gaussians and achieves better image quality with higher rendering speed. </p>
<blockquote>
<p>3Dé«˜æ–¯å·ç§¯ï¼ˆ3DGSï¼‰å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å…¨æ–°è§†è§’åˆæˆæ€§èƒ½ã€‚è™½ç„¶ä¼ ç»Ÿæ–¹æ³•éœ€è¦è¿›è¡Œåœºæ™¯ä¼˜åŒ–ï¼Œä½†æœ€è¿‘å·²ç»æå‡ºäº†å‡ ç§å‰é¦ˆæ–¹æ³•ï¼Œä½¿ç”¨å¯å­¦ä¹ ç½‘ç»œç”Ÿæˆåƒç´ å¯¹é½çš„é«˜æ–¯è¡¨ç¤ºï¼Œè¿™äº›æ–¹æ³•å¯æ¨å¹¿åˆ°ä¸åŒçš„åœºæ™¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åªæ˜¯ç®€å•åœ°å°†æ¥è‡ªå¤šä¸ªè§†è§’çš„åƒç´ å¯¹é½é«˜æ–¯ç»„åˆèµ·æ¥ä½œä¸ºåœºæ™¯è¡¨ç¤ºï¼Œå› æ­¤å¯¼è‡´äº†ä¼ªåƒå’Œé¢å¤–çš„å†…å­˜æˆæœ¬ï¼Œè€Œæ²¡æœ‰å®Œå…¨æ•è·æ¥è‡ªä¸åŒå›¾åƒçš„é«˜æ–¯ä¹‹é—´çš„å…³ç³»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ–¯å›¾ç½‘ç»œï¼ˆGGNï¼‰æ¥ç”Ÿæˆé«˜æ•ˆä¸”å¯æ¨å¹¿çš„é«˜æ–¯è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºé«˜æ–¯å›¾æ¥æ¨¡æ‹Ÿæ¥è‡ªä¸åŒè§†è§’çš„é«˜æ–¯ç»„ä¹‹é—´çš„å…³ç³»ã€‚ä¸ºäº†æ”¯æŒé«˜æ–¯çº§åˆ«çš„æ¶ˆæ¯ä¼ é€’ï¼Œæˆ‘ä»¬å¯¹é«˜æ–¯è¡¨ç¤ºä¸Šçš„åŸºæœ¬å›¾å½¢æ“ä½œè¿›è¡Œé‡æ–°è¡¨è¿°ï¼Œä½¿æ¯ä¸ªé«˜æ–¯éƒ½èƒ½ä»å…¶è¿æ¥çš„é«˜æ–¯ç»„ä¸­å—ç›Šï¼Œå¹¶è¿›è¡Œé«˜æ–¯ç‰¹å¾èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé«˜æ–¯æ± åŒ–å±‚æ¥èšåˆå„ç§é«˜æ–¯ç»„ä»¥å®ç°é«˜æ•ˆè¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡RealEstate10Kå’ŒACIDæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬æ–¹æ³•çš„æ•ˆç‡å’Œé€šç”¨æ€§ã€‚ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨æ›´å°‘çš„é«˜æ–¯ï¼Œå›¾åƒè´¨é‡æ›´é«˜ï¼Œæ¸²æŸ“é€Ÿåº¦æ›´å¿«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16338v1">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé«˜æ–¯å›¾ç½‘ç»œï¼ˆGaussian Graph Networkï¼ŒGGNï¼‰çš„3Dé«˜æ–¯èåˆæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé«˜æ•ˆä¸”é€šç”¨çš„é«˜æ–¯è¡¨ç¤ºã€‚é€šè¿‡æ„å»ºé«˜æ–¯å›¾æ¥æ¨¡æ‹Ÿä¸åŒè§†è§’çš„é«˜æ–¯ç»„å…³ç³»ï¼Œå¹¶é‡æ–°å®šä¹‰äº†é«˜æ–¯å±‚é¢çš„ä¿¡æ¯ä¼ é€’æœºåˆ¶ã€‚è¯¥æ–¹æ³•è¿˜åŒ…æ‹¬é«˜æ–¯ç‰¹å¾èåˆåŠé«˜æ–¯æ± åŒ–å±‚çš„è®¾è®¡ï¼Œä»¥æå‡æ•ˆç‡å¹¶æ”¹è¿›è¡¨ç¤ºè´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¡¨ç°å‡ºé«˜æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨æ›´å°‘çš„é«˜æ–¯æ•°å³å¯è¾¾åˆ°æ›´å¥½çš„å›¾åƒè´¨é‡å’Œæ›´é«˜çš„æ¸²æŸ“é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGSå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ–°è§†è§’åˆæˆæ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•éœ€è¦é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œä¼˜åŒ–ï¼Œè€Œæœ€æ–°æ–¹æ³•åˆ™é€šè¿‡å¯å­¦ä¹ ç½‘ç»œç”Ÿæˆåƒç´ å¯¹é½çš„é«˜æ–¯è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºå…·æœ‰è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç®€å•åœ°å°†å¤šä¸ªè§†è§’çš„åƒç´ å¯¹é½é«˜æ–¯ç»„åˆä¸ºåœºæ™¯è¡¨ç¤ºï¼Œå¯¼è‡´ä¼ªå½±å’Œé¢å¤–çš„å†…å­˜æˆæœ¬ï¼Œæœªèƒ½å……åˆ†æ•æ‰ä¸åŒå›¾åƒé«˜æ–¯ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>æå‡ºGaussian Graph Network (GGN)æ¥ç”Ÿæˆé«˜æ•ˆä¸”é€šç”¨çš„é«˜æ–¯è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡æ„å»ºé«˜æ–¯å›¾æ¥æ¨¡æ‹Ÿä¸åŒè§†è§’çš„é«˜æ–¯ç»„å…³ç³»ã€‚</li>
<li>å®šä¹‰é«˜æ–¯å±‚é¢çš„ä¿¡æ¯ä¼ é€’æœºåˆ¶ï¼Œæ¯ä¸ªé«˜æ–¯éƒ½èƒ½ä»ç›¸è¿çš„é«˜æ–¯ç»„ä¸­å—ç›Šã€‚</li>
<li>è®¾è®¡äº†é«˜æ–¯ç‰¹å¾èåˆåŠé«˜æ–¯æ± åŒ–å±‚ï¼Œç”¨äºé«˜æ•ˆè¡¨ç¤ºã€‚</li>
<li>åœ¨å¤§å‹æ•°æ®é›†RealEstate10Kå’ŒACIDä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1404c9dbf6707dc5e6a045aa48cb45a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39a8b901899411e16a84614350cd26c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0ec436595c12ec134690490e1c9f938.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3513ac9e8f169403333385f0df7665f4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OccluGaussian-Occlusion-Aware-Gaussian-Splatting-for-Large-Scene-Reconstruction-and-Rendering"><a href="#OccluGaussian-Occlusion-Aware-Gaussian-Splatting-for-Large-Scene-Reconstruction-and-Rendering" class="headerlink" title="OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene   Reconstruction and Rendering"></a>OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene   Reconstruction and Rendering</h2><p><strong>Authors:Shiyong Liu, Xiao Tang, Zhihao Li, Yingfan He, Chongjie Ye, Jianzhuang Liu, Binxiao Huang, Shunbo Zhou, Xiaofei Wu</strong></p>
<p>In large-scale scene reconstruction using 3D Gaussian splatting, it is common to partition the scene into multiple smaller regions and reconstruct them individually. However, existing division methods are occlusion-agnostic, meaning that each region may contain areas with severe occlusions. As a result, the cameras within those regions are less correlated, leading to a low average contribution to the overall reconstruction. In this paper, we propose an occlusion-aware scene division strategy that clusters training cameras based on their positions and co-visibilities to acquire multiple regions. Cameras in such regions exhibit stronger correlations and a higher average contribution, facilitating high-quality scene reconstruction. We further propose a region-based rendering technique to accelerate large scene rendering, which culls Gaussians invisible to the region where the viewpoint is located. Such a technique significantly speeds up the rendering without compromising quality. Extensive experiments on multiple large scenes show that our method achieves superior reconstruction results with faster rendering speed compared to existing state-of-the-art approaches. Project page: <a target="_blank" rel="noopener" href="https://occlugaussian.github.io/">https://occlugaussian.github.io</a>. </p>
<blockquote>
<p>åœ¨åˆ©ç”¨3Dé«˜æ–¯æ‹¼æ¥ï¼ˆ3DGSï¼‰è¿›è¡Œå¤§è§„æ¨¡åœºæ™¯é‡å»ºæ—¶ï¼Œé€šå¸¸ä¼šå°†åœºæ™¯åˆ†å‰²æˆå¤šä¸ªè¾ƒå°çš„åŒºåŸŸå¹¶è¿›è¡Œåˆ†åˆ«é‡å»ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„åˆ†å‰²æ–¹æ³•éƒ½æ˜¯æ— è§†é®æŒ¡çš„ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªåŒºåŸŸéƒ½å¯èƒ½åŒ…å«ä¸¥é‡çš„é®æŒ¡åŒºåŸŸã€‚å› æ­¤ï¼Œè¿™äº›åŒºåŸŸå†…çš„ç›¸æœºå…³è”æ€§è¾ƒä½ï¼Œå¯¹æ•´ä½“é‡å»ºçš„å¹³å‡è´¡çŒ®è¾ƒå°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ„ŸçŸ¥é®æŒ¡çš„åœºæ™¯åˆ†å‰²ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®ç›¸æœºçš„ä½ç½®å’Œå…±è§†æ€§å¯¹è®­ç»ƒç›¸æœºè¿›è¡Œèšç±»ï¼Œä»¥è·å¾—å¤šä¸ªåŒºåŸŸã€‚è¿™äº›åŒºåŸŸå†…çš„ç›¸æœºè¡¨ç°å‡ºæ›´å¼ºçš„å…³è”æ€§å’Œæ›´é«˜çš„å¹³å‡è´¡çŒ®ï¼Œæœ‰åŠ©äºå®ç°é«˜è´¨é‡çš„åœºæ™¯é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºåŒºåŸŸçš„æ¸²æŸ“æŠ€æœ¯ï¼Œä»¥åŠ é€Ÿå¤§åœºæ™¯çš„æ¸²æŸ“ï¼Œè¯¥æŠ€æœ¯ä¼šå‰”é™¤å¯¹äºè§†ç‚¹æ‰€åœ¨åŒºåŸŸä¸å¯è§çš„Gausså›¾åƒã€‚è¿™ç§æŠ€æœ¯åœ¨ä¸æŸå®³è´¨é‡çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚åœ¨å¤šä¸ªå¤§å‹åœºæ™¯ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ›´ä¼˜è¶Šçš„é‡å»ºç»“æœå¹¶æé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://occlugaussian.github.io./">https://occlugaussian.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16177v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://occlugaussian.github.io/">https://occlugaussian.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºé®æŒ¡æ„ŸçŸ¥çš„åœºæ™¯åˆ†å‰²ç­–ç•¥ï¼Œç”¨äºå¤§è§„æ¨¡åœºæ™¯é‡å»ºã€‚é€šè¿‡æ ¹æ®ç›¸æœºçš„ä½ç½®å’Œå…±è§†æ€§è¿›è¡Œèšç±»ï¼Œè·å–å¤šä¸ªåŒºåŸŸï¼Œæé«˜äº†ç›¸æœºé—´çš„ç›¸å…³æ€§ï¼Œè¿›è€Œæå‡äº†åœºæ™¯é‡å»ºçš„è´¨é‡ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åŸºäºåŒºåŸŸçš„æ¸²æŸ“æŠ€æœ¯ï¼Œèƒ½åŠ é€Ÿå¤§åœºæ™¯çš„æ¸²æŸ“é€Ÿåº¦ï¼Œè€Œä¸ä¼šç‰ºç‰²æ¸²æŸ“è´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºæ•ˆæœå’Œæ¸²æŸ“é€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åœºæ™¯åˆ†å‰²æ–¹æ³•æœªè€ƒè™‘é®æŒ¡é—®é¢˜ï¼Œå¯¼è‡´åŒºåŸŸå†…å­˜åœ¨ä¸¥é‡é®æŒ¡ï¼Œå½±å“ç›¸æœºé—´çš„ç›¸å…³æ€§åŠé‡å»ºè´¨é‡ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„é®æŒ¡æ„ŸçŸ¥åœºæ™¯åˆ†å‰²ç­–ç•¥èƒ½å¤Ÿæ ¹æ®ç›¸æœºçš„ä½ç½®å’Œå…±è§†æ€§è¿›è¡Œèšç±»ï¼Œå½¢æˆå¤šä¸ªåŒºåŸŸï¼Œæé«˜ç›¸æœºé—´çš„ç›¸å…³æ€§ã€‚</li>
<li>åœ¨è¿™äº›åŒºåŸŸå†…ï¼Œç›¸æœºè¡¨ç°å‡ºæ›´å¼ºçš„ç›¸å…³æ€§å’Œæ›´é«˜çš„å¹³å‡è´¡çŒ®ï¼Œæœ‰åŠ©äºæé«˜åœºæ™¯é‡å»ºçš„è´¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåŒºåŸŸçš„æ¸²æŸ“æŠ€æœ¯ï¼Œèƒ½å‰”é™¤è§†ç‚¹æ‰€åœ¨åŒºåŸŸä¸å¯è§çš„Gaussianï¼Œæ˜¾è‘—åŠ é€Ÿå¤§åœºæ™¯çš„æ¸²æŸ“é€Ÿåº¦ã€‚</li>
<li>è¯¥æŠ€æœ¯åœ¨ä¿è¯æ¸²æŸ“è´¨é‡çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜æ•ˆçš„æ¸²æŸ“ã€‚</li>
<li>é€šè¿‡å¯¹å¤šä¸ªå¤§åœºæ™¯çš„å¹¿æ³›å®éªŒï¼Œè¯æ˜æœ¬æ–‡æ–¹æ³•å®ç°äº†ä¼˜è¶Šçš„é‡å»ºç»“æœå’Œæ›´å¿«çš„æ¸²æŸ“é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f0ad270f0ed7b4278ca8a6c43e37327.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-590a751393655395ef0a1a8aa4acd124.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b34874535b2c01839adcc6e16fccd86c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08c43cbb605f6cebcbc6cf3777c4b97e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-Close-up-Novel-View-Synthesis-via-Pseudo-labeling"><a href="#Enhancing-Close-up-Novel-View-Synthesis-via-Pseudo-labeling" class="headerlink" title="Enhancing Close-up Novel View Synthesis via Pseudo-labeling"></a>Enhancing Close-up Novel View Synthesis via Pseudo-labeling</h2><p><strong>Authors:Jiatong Xia, Libo Sun, Lingqiao Liu</strong></p>
<p>Recent methods, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated remarkable capabilities in novel view synthesis. However, despite their success in producing high-quality images for viewpoints similar to those seen during training, they struggle when generating detailed images from viewpoints that significantly deviate from the training set, particularly in close-up views. The primary challenge stems from the lack of specific training data for close-up views, leading to the inability of current methods to render these views accurately. To address this issue, we introduce a novel pseudo-label-based learning strategy. This approach leverages pseudo-labels derived from existing training data to provide targeted supervision across a wide range of close-up viewpoints. Recognizing the absence of benchmarks for this specific challenge, we also present a new dataset designed to assess the effectiveness of both current and future methods in this area. Our extensive experiments demonstrate the efficacy of our approach. </p>
<blockquote>
<p>æœ€è¿‘çš„æ–¹æ³•ï¼Œå¦‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ï¼Œåœ¨æ–°å‹è§†å›¾åˆæˆä¸­å±•ç¤ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬åœ¨ç”Ÿæˆä¸è®­ç»ƒæœŸé—´æ‰€è§ç›¸ä¼¼çš„è§†è§’çš„é«˜è´¨é‡å›¾åƒæ–¹é¢éå¸¸æˆåŠŸï¼Œä½†åœ¨ç”Ÿæˆä»ä¸è®­ç»ƒé›†åå·®è¾ƒå¤§çš„è§†è§’å‡ºå‘çš„è¯¦ç»†å›¾åƒæ—¶å´è¡¨ç°æŒ£æ‰ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å†™é•œå¤´è§†è§’ä¸‹ã€‚ä¸»è¦æŒ‘æˆ˜æºäºç‰¹å†™é•œå¤´è§†è§’çš„å…·ä½“è®­ç»ƒæ•°æ®çš„ç¼ºä¹ï¼Œå¯¼è‡´å½“å‰æ–¹æ³•æ— æ³•å‡†ç¡®æ¸²æŸ“è¿™äº›è§†å›¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºä¼ªæ ‡ç­¾çš„å­¦ä¹ ç­–ç•¥ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ¥è‡ªç°æœ‰è®­ç»ƒæ•°æ®çš„ä¼ªæ ‡ç­¾ï¼Œä¸ºå„ç§ç‰¹å†™é•œå¤´è§†è§’æä¾›æœ‰é’ˆå¯¹æ€§çš„ç›‘ç£ã€‚ç”±äºå½“å‰ç¼ºä¹é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å½“å‰å’Œæœªæ¥æ–¹æ³•åœ¨è¿™ä¸ªé¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15908v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>NeRFå’Œ3DGSç­‰æ–¹æ³•åœ¨æ–°å‹è§†è§’åˆæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨ç”Ÿæˆä¸è®­ç»ƒé›†è§†è§’å·®å¼‚è¾ƒå¤§çš„è¯¦ç»†å›¾åƒæ—¶ï¼Œå°¤å…¶æ˜¯è¿‘è·ç¦»è§†è§’çš„å›¾åƒï¼Œå­˜åœ¨å›°éš¾ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºç¼ºä¹é’ˆå¯¹è¿‘è·ç¦»è§†è§’çš„ç‰¹å®šè®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´ç°æœ‰æ–¹æ³•æ— æ³•å‡†ç¡®æ¸²æŸ“è¿™äº›è§†å›¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¼ªæ ‡ç­¾çš„å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨ç°æœ‰è®­ç»ƒæ•°æ®ç”Ÿæˆçš„ä¼ªæ ‡ç­¾ï¼Œä¸ºå¹¿æ³›èŒƒå›´çš„è¿‘è·ç¦»è§†è§’æä¾›æœ‰é’ˆå¯¹æ€§çš„ç›‘ç£ã€‚ç”±äºç¼ºå°‘é’ˆå¯¹æ­¤æŒ‘æˆ˜çš„åŸºå‡†æµ‹è¯•é›†ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å½“å‰å’Œæœªæ¥æ–¹æ³•åœ¨è¿™ä¸ªé¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFå’Œ3DGSç­‰æ–¹æ³•åœ¨æ–°å‹è§†è§’åˆæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
<li>åœ¨ç”Ÿæˆä¸è®­ç»ƒé›†è§†è§’å·®å¼‚å¤§çš„è¯¦ç»†å›¾åƒï¼Œå°¤å…¶æ˜¯è¿‘è·ç¦»è§†è§’å›¾åƒæ—¶ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç¼ºä¹é’ˆå¯¹è¿‘è·ç¦»è§†è§’çš„ç‰¹å®šè®­ç»ƒæ•°æ®æ˜¯ä¸»è¦çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºä¼ªæ ‡ç­¾çš„å­¦ä¹ ç­–ç•¥ï¼Œä¸ºå¹¿æ³›èŒƒå›´çš„è¿‘è·ç¦»è§†è§’æä¾›æœ‰é’ˆå¯¹æ€§çš„ç›‘ç£ã€‚</li>
<li>ä¸ºè¯„ä¼°æ–¹æ³•åœ¨è¿™ä¸ªé¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œæ¨å‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ã€‚</li>
<li>å¼•å…¥çš„ä¼ªæ ‡ç­¾å­¦ä¹ ç­–ç•¥æé«˜äº†åœ¨è¿‘è·ç¦»è§†è§’å›¾åƒç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cace697124d53f935d910d9e4a8064df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35bfb8cd72d4fb1dd76b239d517e1704.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8442cae68b84efc64254cdf2e747c143.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b8c2d326a2be14d22344c1b82c15e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffdc84062a38f7079f11ae44df246d06.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VideoRFSplat-Direct-Scene-Level-Text-to-3D-Gaussian-Splatting-Generation-with-Flexible-Pose-and-Multi-View-Joint-Modeling"><a href="#VideoRFSplat-Direct-Scene-Level-Text-to-3D-Gaussian-Splatting-Generation-with-Flexible-Pose-and-Multi-View-Joint-Modeling" class="headerlink" title="VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting   Generation with Flexible Pose and Multi-View Joint Modeling"></a>VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting   Generation with Flexible Pose and Multi-View Joint Modeling</h2><p><strong>Authors:Hyojun Go, Byeongjun Park, Hyelin Nam, Byung-Hoon Kim, Hyungjin Chung, Changick Kim</strong></p>
<p>We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D generative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabilize training and inference. In this work, we propose an architecture and a sampling strategy to jointly model multi-view images and camera poses when fine-tuning a video generation model. Our core idea is a dual-stream architecture that attaches a dedicated pose generation model alongside a pre-trained video generation model via communication blocks, generating multi-view images and camera poses through separate streams. This design reduces interference between the pose and image modalities. Additionally, we propose an asynchronous sampling strategy that denoises camera poses faster than multi-view images, allowing rapidly denoised poses to condition multi-view generation, reducing mutual ambiguity and enhancing cross-modal consistency. Trained on multiple large-scale real-world datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms existing text-to-3D direct generation methods that heavily depend on post-hoc refinement via score distillation sampling, achieving superior results without such refinement. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºVideoRFSplatï¼Œè¿™æ˜¯ä¸€ç§ç›´æ¥çš„æ–‡æœ¬åˆ°3Dæ¨¡å‹ï¼Œåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ç”Ÿæˆç°å®ä¸–ç•Œçš„æ— é™åœºæ™¯çš„çœŸå®3Dé«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰ã€‚ä¸ºäº†ç”Ÿæˆå¤šæ ·åŒ–çš„ç›¸æœºå§¿æ€å’Œç°å®ä¸–ç•Œåœºæ™¯çš„æ— é™ç©ºé—´èŒƒå›´ï¼ŒåŒæ—¶ç¡®ä¿å¯¹ä»»æ„æ–‡æœ¬æç¤ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¹‹å‰çš„æ–¹æ³•æ˜¯é€šè¿‡å¾®è°ƒ2Dç”Ÿæˆæ¨¡å‹æ¥è”åˆå»ºæ¨¡ç›¸æœºå§¿æ€å’Œå¤šè§†è§’å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å°†2Dç”Ÿæˆæ¨¡å‹æ‰©å±•åˆ°è”åˆå»ºæ¨¡æ—¶ä¼šå‡ºç°ä¸ç¨³å®šï¼Œè¿™æ˜¯ç”±äºæ¨¡æ€å·®è·é€ æˆçš„ï¼Œéœ€è¦é¢å¤–çš„æ¨¡å‹æ¥ç¨³å®šè®­ç»ƒå’Œæ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¶æ„å’Œé‡‡æ ·ç­–ç•¥ï¼Œåœ¨å¾®è°ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹æ—¶è”åˆå»ºæ¨¡å¤šè§†è§’å›¾åƒå’Œç›¸æœºå§¿æ€ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¸€ç§åŒæµæ¶æ„ï¼Œå®ƒé€šè¿‡é€šä¿¡å—å°†ä¸€ä¸ªä¸“ç”¨çš„å§¿æ€ç”Ÿæˆæ¨¡å‹ä¸é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹é™„åŠ åœ¨ä¸€èµ·ï¼Œé€šè¿‡å•ç‹¬çš„æµç”Ÿæˆå¤šè§†è§’å›¾åƒå’Œç›¸æœºå§¿æ€ã€‚è¿™ç§è®¾è®¡å‡å°‘äº†å§¿æ€å’Œå›¾åƒæ¨¡æ€ä¹‹é—´çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼‚æ­¥é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½æ›´å¿«åœ°æ¶ˆé™¤ç›¸æœºå§¿æ€çš„å™ªå£°ï¼Œå…è®¸å¿«é€Ÿå»å™ªçš„å§¿æ€è°ƒèŠ‚å¤šè§†è§’ç”Ÿæˆï¼Œå‡å°‘ç›¸äº’æ¨¡ç³Šå¹¶å¢å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚åœ¨å¤šä¸ªå¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼ˆRealEstate10Kã€MVImgNetã€DL3DV-10Kã€ACIDï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒVideoRFSplatè¶…è¶Šäº†ç°æœ‰çš„æ–‡æœ¬åˆ°3Dç›´æ¥ç”Ÿæˆæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¸¥é‡ä¾èµ–äºé€šè¿‡åˆ†æ•°è’¸é¦é‡‡æ ·è¿›è¡Œäº‹åç»†åŒ–ï¼Œå¹¶åœ¨æ²¡æœ‰æ­¤ç±»ç»†åŒ–çš„æƒ…å†µä¸‹å–å¾—ä¼˜è¶Šç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15855v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gohyojun15.github.io/VideoRFSplat/">https://gohyojun15.github.io/VideoRFSplat/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºVideoRFSplatæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç›´æ¥ç”±æ–‡æœ¬åˆ°3Dçš„æ¨¡å‹ï¼Œåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ç”Ÿæˆé€¼çœŸçš„3Dé«˜æ–¯æº…å‡ºï¼ˆ3DGSï¼‰ä»¥è¡¨ç°æ— ç•Œé™çš„çœŸå®ä¸–ç•Œåœºæ™¯ã€‚ä¸ºç”Ÿæˆå¤šæ ·åŒ–çš„ç›¸æœºå§¿æ€å’ŒçœŸå®ä¸–ç•Œåœºæ™¯çš„æ— ç•Œé™ç©ºé—´èŒƒå›´ï¼ŒåŒæ—¶ç¡®ä¿å¯¹ä»»æ„æ–‡æœ¬æç¤ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå‰äººæ–¹æ³•ä¼šå¯¹2Dç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥è”åˆå»ºæ¨¡ç›¸æœºå§¿æ€å’Œå¤šè§†è§’å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ‰©å±•åˆ°è”åˆå»ºæ¨¡æ—¶ä¼šå‡ºç°ä¸ç¨³å®šç°è±¡ï¼Œè¿™æ˜¯ç”±äºæ¨¡æ€å·®è·é€ æˆçš„ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¶æ„å’Œé‡‡æ ·ç­–ç•¥ï¼Œåœ¨å¾®è°ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹æ—¶è”åˆå»ºæ¨¡å¤šè§†è§’å›¾åƒå’Œç›¸æœºå§¿æ€ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯é‡‡ç”¨åŒæµæ¶æ„ï¼Œé€šè¿‡é€šä¿¡å—å°†ä¸€ä¸ªä¸“ç”¨çš„å§¿æ€ç”Ÿæˆæ¨¡å‹ä¸é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿æ¥èµ·æ¥ï¼Œé€šè¿‡ä¸åŒæµç”Ÿæˆå¤šè§†è§’å›¾åƒå’Œç›¸æœºå§¿æ€ï¼Œå‡å°‘å§¿æ€å’Œå›¾åƒæ¨¡æ€ä¹‹é—´çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§å¼‚æ­¥é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½æ›´å¿«åœ°å¯¹ç›¸æœºå§¿æ€è¿›è¡Œå»å™ªå¤„ç†ï¼Œå…è®¸å¿«é€Ÿå»å™ªçš„ç›¸æœºå§¿æ€ä½œä¸ºå¤šè§†è§’ç”Ÿæˆçš„çº¦æŸæ¡ä»¶ï¼Œå‡å°‘äº†äº’æ¨¡ç³Šå¹¶å¢å¼ºäº†è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚åœ¨å¤šä¸ªå¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒVideoRFSplatè¶…è¶Šäº†ç°æœ‰çš„æ–‡æœ¬åˆ°3Dç›´æ¥ç”Ÿæˆæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¾èµ–äºäº‹åä¿®è®¢å¾—åˆ†è’¸é¦é‡‡æ ·çš„æ–¹æ³•ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå³ä½¿æ²¡æœ‰è¿™ç§ä¿®è®¢ä¹Ÿèƒ½å®ç°ä¼˜è¶Šçš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoRFSplatæ˜¯ä¸€ç§åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹å®ç°ä»æ–‡æœ¬åˆ°é€¼çœŸçš„ä¸‰ç»´é«˜æ–¯æº…å‡ºæ¨¡å‹çš„ç›´æ¥è½¬æ¢ã€‚</li>
<li>ä¼ ç»Ÿçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬æç¤ºæ—¶ä¼šé‡åˆ°ç›¸æœºå§¿æ€å»ºæ¨¡å’Œå¤šè§†è§’å›¾åƒç”Ÿæˆçš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œéœ€è¦åˆ©ç”¨åŒæµæ¶æ„ç»“åˆä¸“ç”¨çš„å§¿æ€ç”Ÿæˆæ¨¡å‹å’Œé¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¼•å…¥é€šä¿¡å—å®ç°åŒæµçš„è”åˆæ“ä½œï¼Œåˆ†åˆ«å¤„ç†å¤šè§†è§’å›¾åƒå’Œç›¸æœºå§¿æ€çš„ç”Ÿæˆï¼Œå‡å°‘æ¨¡æ€é—´çš„å¹²æ‰°ã€‚</li>
<li>æå‡ºå¼‚æ­¥é‡‡æ ·ç­–ç•¥ä»¥åŠ é€Ÿç›¸æœºå§¿æ€çš„å»å™ªè¿‡ç¨‹ï¼Œæå‡æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚é€šè¿‡è¿…é€Ÿæ›´æ–°çš„å»å™ªå§¿æ€æ¡ä»¶åŒ–å¤šè§†è§’å›¾åƒçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œé™ä½äº†æ¨¡æ€ä¹‹é—´çš„æ­§ä¹‰æ€§å¹¶å¢å¼ºäº†è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5e182d8f9be850a13c7849e8e82d028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b842347f67ff709161d2299363513bd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0b6193625b5224d0f03cc1064ae9edb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b83c117d7b5b19784fdffb711e6ee56f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="BARD-GS-Blur-Aware-Reconstruction-of-Dynamic-Scenes-via-Gaussian-Splatting"><a href="#BARD-GS-Blur-Aware-Reconstruction-of-Dynamic-Scenes-via-Gaussian-Splatting" class="headerlink" title="BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian   Splatting"></a>BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian   Splatting</h2><p><strong>Authors:Yiren Lu, Yunlai Zhou, Disheng Liu, Tuo Liang, Yu Yin</strong></p>
<p>3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene reconstruction, and recent advancements have extended its application to dynamic scenes. However, the quality of reconstructions depends heavily on high-quality input images and precise camera poses, which are not that trivial to fulfill in real-world scenarios. Capturing dynamic scenes with handheld monocular cameras, for instance, typically involves simultaneous movement of both the camera and objects within a single exposure. This combined motion frequently results in image blur that existing methods cannot adequately handle. To address these challenges, we introduce BARD-GS, a novel approach for robust dynamic scene reconstruction that effectively handles blurry inputs and imprecise camera poses. Our method comprises two main components: 1) camera motion deblurring and 2) object motion deblurring. By explicitly decomposing motion blur into camera motion blur and object motion blur and modeling them separately, we achieve significantly improved rendering results in dynamic regions. In addition, we collect a real-world motion blur dataset of dynamic scenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS effectively reconstructs high-quality dynamic scenes under realistic conditions, significantly outperforming existing methods. </p>
<blockquote>
<p>3Dé«˜æ–¯ç‚¹ç§¯æ³•ï¼ˆ3DGSï¼‰åœ¨é™æ€åœºæ™¯é‡å»ºæ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ï¼Œå¹¶ä¸”æœ€è¿‘çš„è¿›å±•å·²ç»å°†å…¶åº”ç”¨æ‰©å±•åˆ°äº†åŠ¨æ€åœºæ™¯ã€‚ç„¶è€Œï¼Œé‡å»ºçš„è´¨é‡å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºé«˜è´¨é‡è¾“å…¥å›¾åƒå’Œç²¾ç¡®çš„ç›¸æœºå§¿æ€ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­å¾€å¾€éš¾ä»¥æ»¡è¶³ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨æ‰‹æŒå•çœ¼ç›¸æœºæ•æ‰åŠ¨æ€åœºæ™¯é€šå¸¸æ¶‰åŠç›¸æœºå’Œåœºæ™¯å†…ç‰©ä½“çš„åŒæ—¶ç§»åŠ¨ã€‚è¿™ç§ç»„åˆè¿åŠ¨å¾€å¾€ä¼šå¯¼è‡´å›¾åƒæ¨¡ç³Šï¼Œç°æœ‰æ–¹æ³•æ— æ³•å……åˆ†å¤„ç†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†BARD-GSï¼Œè¿™æ˜¯ä¸€ç§ç¨³å¥çš„åŠ¨æ€åœºæ™¯é‡å»ºæ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ¨¡ç³Šè¾“å…¥å’Œç›¸æœºå§¿æ€ä¸å‡†ç¡®çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼š1ï¼‰ç›¸æœºè¿åŠ¨å»æ¨¡ç³Šå’Œ2ï¼‰ç‰©ä½“è¿åŠ¨å»æ¨¡ç³Šã€‚æˆ‘ä»¬é€šè¿‡æ˜ç¡®åœ°å°†è¿åŠ¨æ¨¡ç³Šåˆ†è§£æˆç›¸æœºè¿åŠ¨æ¨¡ç³Šå’Œç‰©ä½“è¿åŠ¨æ¨¡ç³Šï¼Œå¹¶åˆ†åˆ«å¯¹å…¶è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°äº†åŠ¨æ€åŒºåŸŸçš„æ¸²æŸ“ç»“æœæ˜¾è‘—æ”¹å–„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ”¶é›†äº†ä¸€ä¸ªåŠ¨æ€åœºæ™¯çš„çœŸå®è¿åŠ¨æ¨¡ç³Šæ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBARD-GSåœ¨çœŸå®æ¡ä»¶ä¸‹æœ‰æ•ˆåœ°é‡å»ºäº†é«˜è´¨é‡åŠ¨æ€åœºæ™¯ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15835v1">PDF</a> CVPR2025. Project page at <a target="_blank" rel="noopener" href="https://vulab-ai.github.io/BARD-GS/">https://vulab-ai.github.io/BARD-GS/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŠ¨æ€åœºæ™¯é‡å»ºçš„éœ€æ±‚å’ŒæŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºBARD-GSçš„æ–°å‹æ–¹æ³•ï¼Œå®ƒå¯æœ‰æ•ˆå¤„ç†æ¨¡ç³Šè¾“å…¥å’Œä¸å‡†ç¡®çš„ç›¸æœºå§¿æ€ã€‚é€šè¿‡åˆ†ç¦»å¹¶åˆ†åˆ«å»ºæ¨¡ç›¸æœºè¿åŠ¨æ¨¡ç³Šå’Œç‰©ä½“è¿åŠ¨æ¨¡ç³Šï¼Œåœ¨åŠ¨æ€åŒºåŸŸçš„æ¸²æŸ“ç»“æœå¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚åŒæ—¶ï¼Œæ”¶é›†çœŸå®ä¸–ç•Œçš„è¿åŠ¨æ¨¡ç³Šæ•°æ®é›†ä»¥è¯„ä¼°æ–¹æ³•æ€§èƒ½ï¼Œå®éªŒè¯æ˜BARD-GSåœ¨çœŸå®æ¡ä»¶ä¸‹å¯¹åŠ¨æ€åœºæ™¯çš„é‡å»ºè´¨é‡æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGSåœ¨é™æ€å’ŒåŠ¨æ€åœºæ™¯çš„é‡å»ºä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>é‡å»ºè´¨é‡é«˜åº¦ä¾èµ–äºé«˜è´¨é‡è¾“å…¥å›¾åƒå’Œç²¾ç¡®çš„ç›¸æœºå§¿æ€ã€‚</li>
<li>æ‰‹æŒå•ç›®ç›¸æœºæ‹æ‘„åŠ¨æ€åœºæ™¯æ—¶ï¼Œç›¸æœºå’Œç‰©ä½“çš„åŒæ—¶è¿åŠ¨å¸¸å¯¼è‡´å›¾åƒæ¨¡ç³Šï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†ã€‚</li>
<li>BARD-GSæ–¹æ³•é€šè¿‡æ˜ç¡®åˆ†è§£å’Œåˆ†åˆ«å»ºæ¨¡ç›¸æœºè¿åŠ¨æ¨¡ç³Šå’Œç‰©ä½“è¿åŠ¨æ¨¡ç³Šï¼Œå®ç°äº†æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>æ”¶é›†çœŸå®ä¸–ç•Œçš„è¿åŠ¨æ¨¡ç³Šæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°BARD-GSæ€§èƒ½ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜ï¼ŒBARD-GSåœ¨çœŸå®æ¡ä»¶ä¸‹å¯¹åŠ¨æ€åœºæ™¯çš„é‡å»ºæ•ˆæœä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0f5988a4c3777e57aff2f6b8b1d129e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fd34435b13e555f51617feba2acabf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eac2bb345cbc7cac81c1dc40fbf466cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3991b4acaf9406afca67b62a21d25e31.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CHROME-Clothed-Human-Reconstruction-with-Occlusion-Resilience-and-Multiview-Consistency-from-a-Single-Image"><a href="#CHROME-Clothed-Human-Reconstruction-with-Occlusion-Resilience-and-Multiview-Consistency-from-a-Single-Image" class="headerlink" title="CHROME: Clothed Human Reconstruction with Occlusion-Resilience and   Multiview-Consistency from a Single Image"></a>CHROME: Clothed Human Reconstruction with Occlusion-Resilience and   Multiview-Consistency from a Single Image</h2><p><strong>Authors:Arindam Dutta, Meng Zheng, Zhongpai Gao, Benjamin Planche, Anwesha Choudhuri, Terrence Chen, Amit K. Roy-Chowdhury, Ziyan Wu</strong></p>
<p>Reconstructing clothed humans from a single image is a fundamental task in computer vision with wide-ranging applications. Although existing monocular clothed human reconstruction solutions have shown promising results, they often rely on the assumption that the human subject is in an occlusion-free environment. Thus, when encountering in-the-wild occluded images, these algorithms produce multiview inconsistent and fragmented reconstructions. Additionally, most algorithms for monocular 3D human reconstruction leverage geometric priors such as SMPL annotations for training and inference, which are extremely challenging to acquire in real-world applications. To address these limitations, we propose CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. Specifically, CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. CHROME achieves significant improvements in terms of both novel view synthesis (upto 3 db PSNR) and geometric reconstruction under challenging conditions. </p>
<blockquote>
<p>ä»å•ä¸€å›¾åƒé‡å»ºç©¿è¡£äººç±»æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨èŒƒå›´ã€‚å°½ç®¡ç°æœ‰çš„å•ç›®ç©¿è¡£äººç±»é‡å»ºè§£å†³æ–¹æ¡ˆå·²ç»æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬é€šå¸¸å‡è®¾äººç±»ä¸»ä½“å¤„äºä¸€ä¸ªæ— é®æŒ¡çš„ç¯å¢ƒä¸­ã€‚å› æ­¤ï¼Œå½“é‡åˆ°é‡å¤–é®æŒ¡å›¾åƒæ—¶ï¼Œè¿™äº›ç®—æ³•ä¼šäº§ç”Ÿå¤šè§†è§’ä¸ä¸€è‡´å’Œç¢ç‰‡åŒ–çš„é‡å»ºç»“æœã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°å•ç›®3Däººç±»é‡å»ºç®—æ³•åˆ©ç”¨å‡ ä½•å…ˆéªŒï¼ˆå¦‚SMPLæ³¨é‡Šï¼‰è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­æéš¾è·å–ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CHROMEï¼šä»å•å¼ å›¾åƒè¿›è¡Œå…·æœ‰é®æŒ¡æ¢å¤èƒ½åŠ›å’Œå¤šè§†è§’ä¸€è‡´æ€§çš„ç©¿è¡£äººç±»é‡å»ºã€‚è¿™æ˜¯ä¸€ç§æ–°å‹ç®¡é“è®¾è®¡ï¼Œæ—¨åœ¨ä»å•ä¸ªé®æŒ¡å›¾åƒä¸­ä»¥å¤šè§†è§’ä¸€è‡´æ€§é‡å»ºå…·æœ‰é®æŒ¡æ¢å¤èƒ½åŠ›çš„3Däººç±»ï¼Œè€Œæ— éœ€çœŸå®å‡ ä½•å…ˆéªŒæ³¨é‡Šæˆ–3Dç›‘ç£ã€‚å…·ä½“æ¥è¯´ï¼ŒCHROMEé¦–å…ˆåˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹ä»é®æŒ¡è¾“å…¥ä¸­åˆæˆæ— é®æŒ¡çš„äººä½“å›¾åƒï¼Œä¸ç°æˆçš„å§¿åŠ¿æ§åˆ¶ç›¸ç»“åˆï¼Œåœ¨åˆæˆè¿‡ç¨‹ä¸­æ˜ç¡®æ‰§è¡Œè·¨è§†è§’ä¸€è‡´æ€§ã€‚ç„¶åï¼Œè®­ç»ƒä¸€ä¸ª3Dé‡å»ºæ¨¡å‹ï¼Œæ ¹æ®é®æŒ¡è¾“å…¥å’Œåˆæˆè§†å›¾é¢„æµ‹ä¸€ç»„3Dé«˜æ–¯åˆ†å¸ƒï¼Œå¯¹é½è·¨è§†è§’ç»†èŠ‚ä»¥äº§ç”Ÿè¿è´¯å’Œå‡†ç¡®çš„3Dè¡¨ç¤ºã€‚CHROMEåœ¨æ–°å‹è§†è§’åˆæˆï¼ˆé«˜è¾¾3åˆ†è´PSNRï¼‰å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹çš„å‡ ä½•é‡å»ºæ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15671v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é‡å»ºæŠ€æœ¯â€”â€”CHROMEï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨å•å¼ é®æŒ¡å›¾åƒä¸­å®ç°å…·æœ‰é®æŒ¡æ¢å¤èƒ½åŠ›çš„ä¸‰ç»´äººä½“é‡å»ºï¼Œå¹¶ä¸”å…·æœ‰å¤šè§†è§’ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•æ— éœ€çœŸå®å‡ ä½•å…ˆéªŒæ ‡æ³¨å’Œä¸‰ç»´ç›‘ç£ä¿¡æ¯ï¼Œé€šè¿‡åˆæˆæ— é®æŒ¡äººä½“å›¾åƒå’Œå¤šè§†è§’ä¸€è‡´æ€§çº¦æŸï¼Œæé«˜äº†åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é‡å»ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡å»ºæŠ€æœ¯ä¸»è¦æŒ‘æˆ˜åœ¨äºå¤„ç†é®æŒ¡é—®é¢˜ã€‚ç°æœ‰çš„æŠ€æœ¯é€šå¸¸å‡è®¾äººä½“å¤„äºæ— é®æŒ¡ç¯å¢ƒä¸­ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>CHROMEæŠ€æœ¯è§£å†³äº†è¿™ä¸€éš¾é¢˜ï¼Œå®ç°äº†åœ¨å•å¼ é®æŒ¡å›¾åƒä¸Šçš„ä¸‰ç»´äººä½“é‡å»ºï¼Œå¹¶å…·æœ‰é®æŒ¡æ¢å¤èƒ½åŠ›ã€‚</li>
<li>CHROMEæŠ€æœ¯é‡‡ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œä»é®æŒ¡è¾“å…¥ä¸­åˆæˆæ— é®æŒ¡äººä½“å›¾åƒï¼Œä¸ç°æˆçš„å§¿åŠ¿æ§åˆ¶å…¼å®¹ï¼Œæ˜ç¡®æ‰§è¡Œè·¨è§†è§’ä¸€è‡´æ€§çº¦æŸã€‚</li>
<li>è¯¥æŠ€æœ¯é€šè¿‡è®­ç»ƒä¸€ä¸ªä¸‰ç»´é‡å»ºæ¨¡å‹ï¼Œé¢„æµ‹ä¸€ç»„åŸºäºé®æŒ¡è¾“å…¥å’Œåˆæˆè§†è§’çš„ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒï¼Œé€šè¿‡è·¨è§†è§’ç»†èŠ‚å¯¹é½ç”Ÿæˆè¿è´¯ä¸”ç²¾ç¡®çš„ä¸‰ç»´è¡¨ç¤ºã€‚</li>
<li>CHROMEæŠ€æœ¯åœ¨æ–°å‹è§†è§’åˆæˆå’Œå‡ ä½•é‡å»ºæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸‹ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€çœŸå®çš„å‡ ä½•å…ˆéªŒæ ‡æ³¨å’Œä¸‰ç»´ç›‘ç£ä¿¡æ¯ï¼Œé™ä½äº†å®é™…åº”ç”¨ä¸­çš„éš¾åº¦å’Œæˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8f98aaec679a0dac16e779f30f2b476a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a70eb82968e8b791c6e54c4beb7d9b5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54a45333e4f8c9539af19302e62f5a5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be7fc9c550078f9331b0faa3c964f696.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60e3b6a0bdf2c1e05c87315b44e13817.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion"><a href="#Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion" class="headerlink" title="Synthetic Prior for Few-Shot Drivable Head Avatar Inversion"></a>Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</h2><p><strong>Authors:Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart</strong></p>
<p>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle three major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, the use of real data is strictly regulated (e.g., under the General Data Protection Regulation, which mandates frequent deletion of models and data to accommodate a situation when a participantâ€™s consent is withdrawn). Synthetic data, free from these constraints, is an appealing alternative. Third, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to SOTA monocular and GAN-based methods, SynShot significantly improves novel view and expression synthesis. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSynShotçš„æ–°æ–¹æ³•ï¼Œç”¨äºåŸºäºåˆæˆå…ˆéªŒçš„å°‘æ•°é•œå¤´é©¾é©¶å¤´éƒ¨é˜¿å‡¡è¾¾çš„åè½¬ã€‚æˆ‘ä»¬è§£å†³äº†ä¸‰å¤§æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œè®­ç»ƒå¯æ§çš„3Dç”Ÿæˆç½‘ç»œéœ€è¦å¤§é‡çš„ä¸åŒåºåˆ—ï¼Œè€Œå›¾åƒå’Œé«˜å“è´¨è·Ÿè¸ªç½‘æ ¼çš„é…å¯¹å¹¶ä¸æ€»æ˜¯å¯ç”¨ã€‚å…¶æ¬¡ï¼ŒçœŸå®æ•°æ®çš„ä½¿ç”¨å—åˆ°ä¸¥æ ¼ç›‘ç®¡ï¼ˆä¾‹å¦‚ï¼Œåœ¨ã€Šé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ã€‹ä¸‹ï¼Œå½“å‚ä¸è€…åŒæ„æ’¤å›æ—¶ï¼Œç»å¸¸éœ€è¦åˆ é™¤æ¨¡å‹å’Œæ•°æ®ï¼‰ã€‚ä¸å—è¿™äº›çº¦æŸçš„åˆæˆæ•°æ®æ˜¯ä¸€ä¸ªå¸å¼•äººçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç¬¬ä¸‰ï¼Œæœ€å…ˆè¿›çš„å•çœ¼é˜¿å‡¡è¾¾æ¨¡å‹åœ¨æ¨å¹¿åˆ°æ–°çš„è§†è§’å’Œè¡¨æƒ…æ—¶é‡åˆ°å›°éš¾ï¼Œç¼ºä¹å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¸”ç»å¸¸è¿‡åº¦é€‚åº”ç‰¹å®šçš„è§‚ç‚¹åˆ†å¸ƒã€‚å—åªæ¥å—åˆæˆæ•°æ®è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»åŒ…å«ä¸åŒèº«ä»½ã€è¡¨æƒ…å’Œè§‚ç‚¹çš„å¤§é‡åˆæˆå¤´éƒ¨æ•°æ®ä¸­å­¦ä¹ å…ˆéªŒæ¨¡å‹ã€‚å‡­å€Ÿå°‘æ•°è¾“å…¥å›¾åƒï¼ŒSynShotå¾®è°ƒäº†é¢„è®­ç»ƒçš„åˆæˆå…ˆéªŒï¼Œä»¥å¼¥åˆé¢†åŸŸé—´çš„å·®è·ï¼Œå¹¶å»ºæ¨¡ä¸€ä¸ªé€¼çœŸçš„å¤´éƒ¨é˜¿å‡¡è¾¾ï¼Œå¯ä»¥æ¨å¹¿åˆ°æ–°çš„è¡¨æƒ…å’Œè§†è§’ã€‚æˆ‘ä»¬ä½¿ç”¨3Dé«˜æ–¯å–·ç»˜å’Œå·ç§¯ç¼–ç å™¨-è§£ç å™¨æ¥è¾“å‡ºUVçº¹ç†ç©ºé—´çš„é«˜æ–¯å‚æ•°ï¼Œå¯¹å¤´éƒ¨å„éƒ¨åˆ†çš„ä¸åŒå»ºæ¨¡å¤æ‚æ€§è¿›è¡Œå»ºæ¨¡ï¼ˆä¾‹å¦‚ï¼Œçš®è‚¤ä¸å¤´å‘ï¼‰ã€‚ä¸ºäº†è€ƒè™‘å¤´éƒ¨å„éƒ¨åˆ†ï¼ˆä¾‹å¦‚çš®è‚¤å’Œå¤´å‘ï¼‰çš„ä¸åŒå»ºæ¨¡å¤æ‚æ€§ï¼Œæˆ‘ä»¬åœ¨å…ˆéªŒä¸­åµŒå…¥äº†å¯¹å¢åŠ æ¯ä¸ªéƒ¨åˆ†åŸå§‹æ•°é‡çš„ä¸Šé‡‡æ ·æ§åˆ¶ã€‚ä¸æœ€å…ˆè¿›çš„å•çœ¼å’ŒåŸºäºGANçš„æ–¹æ³•ç›¸æ¯”ï¼ŒSynShotæ˜¾è‘—æé«˜äº†æ–°è§†è§’å’Œè¡¨æƒ…çš„åˆæˆæ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06903v2">PDF</a> Accepted to CVPR25 Website: <a target="_blank" rel="noopener" href="https://zielon.github.io/synshot/">https://zielon.github.io/synshot/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SynShotæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆæˆå…ˆéªŒçš„å°‘æ•°äººå¤´é©¾é©¶å¤´åƒåè½¬æŠ€æœ¯çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹å¤šæ ·åºåˆ—å›¾åƒå’Œé«˜å“è´¨è¿½è¸ªç½‘æ ¼çš„è®­ç»ƒæ•°æ®ã€çœŸå®æ•°æ®ä½¿ç”¨å—åˆ°ä¸¥æ ¼ç›‘ç®¡ä»¥åŠå½“å‰å•çœ¼å¤´åƒæ¨¡å‹åœ¨æ–°è§†è§’å’Œè¡¨æƒ…ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¸å¼ºã€‚SynShoté€šè¿‡å­¦ä¹ ä»å¤§é‡åˆæˆå¤´åƒæ•°æ®ä¸­è·å–çš„å…ˆéªŒæ¨¡å‹ï¼Œç»“åˆå°‘é‡è¾“å…¥å›¾åƒï¼Œå¾®è°ƒé¢„è®­ç»ƒåˆæˆå…ˆéªŒä»¥å¼¥åŸŸå·®è·ï¼Œä»è€Œå»ºæ¨¡å‡ºçœŸå®æ„Ÿå¤´åƒå¹¶èƒ½æ³›åŒ–åˆ°æ–°çš„è¡¨æƒ…å’Œè§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SynShotæ˜¯ä¸€ç§åŸºäºåˆæˆå…ˆéªŒçš„å°‘æ•°äººå¤´é©¾é©¶å¤´åƒåè½¬æŠ€æœ¯ã€‚</li>
<li>å®ƒè§£å†³äº†è®­ç»ƒå¯æ§çš„3Dç”Ÿæˆç½‘ç»œé¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹å¤šæ ·åºåˆ—å›¾åƒå’Œé«˜å“è´¨è¿½è¸ªç½‘æ ¼çš„è®­ç»ƒæ•°æ®ã€çœŸå®æ•°æ®ä½¿ç”¨çš„ç›‘ç®¡é—®é¢˜ä»¥åŠç°æœ‰æ¨¡å‹åœ¨æ–°è§†è§’å’Œè¡¨æƒ…ä¸Šçš„æ³›åŒ–éš¾é¢˜ã€‚</li>
<li>SynShoté€šè¿‡ç»“åˆå¤§é‡åˆæˆå¤´åƒæ•°æ®çš„å…ˆéªŒæ¨¡å‹ä¸å°‘é‡è¾“å…¥å›¾åƒï¼Œå¾®è°ƒé¢„è®­ç»ƒåˆæˆå…ˆéªŒï¼Œä»¥å¼¥åŸŸå·®è·ï¼Œè¾¾åˆ°å»ºæ¨¡çœŸå®æ„Ÿå¤´åƒçš„ç›®æ ‡ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨3Dé«˜æ–¯å–·æ¶‚æŠ€æœ¯å’Œå·ç§¯ç¼–ç å™¨-è§£ç å™¨ï¼Œåœ¨UVçº¹ç†ç©ºé—´ä¸­è¾“å‡ºé«˜æ–¯å‚æ•°ã€‚</li>
<li>ä¸ºäº†åº”å¯¹å¤´éƒ¨ä¸åŒéƒ¨åˆ†å»ºæ¨¡å¤æ‚æ€§çš„å·®å¼‚ï¼ˆå¦‚çš®è‚¤å’Œå¤´å‘ï¼‰ï¼ŒSynShotåµŒå…¥å…ˆéªŒï¼Œå…·æœ‰é’ˆå¯¹æ¯ä¸ªéƒ¨åˆ†åŸå§‹æ•°é‡çš„ä¸Šé‡‡æ ·æ§åˆ¶åŠŸèƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd0cc9a3a4a5ca64e316c0f6919c49d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc4fd6a640e4f770326012aab1b0575e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34b5f656b91512006dff6313ea06256e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91869d7ecc0a53622008866a9ae4c5fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8020a341dbd40737d91ef6aa6c3efdeb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfb08e011b3ab317551b561dbc247e94.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NFL-BA-Improving-Endoscopic-SLAM-with-Near-Field-Light-Bundle-Adjustment"><a href="#NFL-BA-Improving-Endoscopic-SLAM-with-Near-Field-Light-Bundle-Adjustment" class="headerlink" title="NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle   Adjustment"></a>NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle   Adjustment</h2><p><strong>Authors:Andrea Dunn Beltran, Daniel Rho, Stephen Pizer, Marc Niethammer, Roni Sengupta</strong></p>
<p>Simultaneous Localization And Mapping (SLAM) from endoscopy videos can enable autonomous navigation, guidance to unsurveyed regions, blindspot detections, and 3D visualizations, which can significantly improve patient outcomes and endoscopy experience for both physicians and patients. Existing dense SLAM algorithms often assume distant and static lighting and optimize scene geometry and camera parameters by minimizing a photometric rendering loss, often called Photometric Bundle Adjustment. However, endoscopy videos exhibit dynamic near-field lighting due to the co-located light and camera moving extremely close to the surface. In addition, low texture surfaces in endoscopy videos cause photometric bundle adjustment of the existing SLAM frameworks to perform poorly compared to indoor&#x2F;outdoor scenes. To mitigate this problem, we introduce Near-Field Lighting Bundle Adjustment Loss (NFL-BA) which explicitly models near-field lighting as a part of Bundle Adjustment loss and enables better performance for low texture surfaces. Our proposed NFL-BA can be applied to any neural-rendering based SLAM framework. We show that by replacing traditional photometric bundle adjustment loss with our proposed NFL-BA results in improvement, using neural implicit SLAM and 3DGS SLAMs. In addition to producing state-of-the-art tracking and mapping results on colonoscopy C3VD dataset we also show improvement on real colonoscopy videos. See results at <a target="_blank" rel="noopener" href="https://asdunnbe.github.io/NFL-BA/">https://asdunnbe.github.io/NFL-BA/</a> </p>
<blockquote>
<p>é€šè¿‡å†…çª¥é•œè§†é¢‘å®ç°çš„åŒæ­¥å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰å¯ä»¥æ”¯æŒè‡ªä¸»å¯¼èˆªã€å¯¹æœªæµ‹ç»˜åŒºåŸŸçš„æŒ‡å¯¼ã€ç›²ç‚¹æ£€æµ‹å’Œ3Då¯è§†åŒ–ï¼Œè¿™å¯ä»¥æ˜¾è‘—æ”¹å–„æ‚£è€…æ²»ç–—æ•ˆæœå’ŒåŒ»ç”Ÿä¸æ‚£è€…çš„å†…çª¥é•œæ£€æŸ¥ä½“éªŒã€‚ç°æœ‰çš„å¯†é›†SLAMç®—æ³•é€šå¸¸å‡è®¾å…‰æºè·ç¦»é¥è¿œä¸”é™æ€ï¼Œé€šè¿‡æœ€å°åŒ–è¢«ç§°ä¸ºå…‰åº¦æ†ç»‘è°ƒæ•´çš„å…‰åº¦æ¸²æŸ“æŸå¤±æ¥ä¼˜åŒ–åœºæ™¯å‡ ä½•å’Œç›¸æœºå‚æ•°ã€‚ç„¶è€Œï¼Œå†…çª¥é•œè§†é¢‘å‘ˆç°å‡ºåŠ¨æ€è¿‘åœºç…§æ˜ï¼Œè¿™æ˜¯ç”±äºå…‰æºå’Œç›¸æœºä¸è¡¨é¢éå¸¸æ¥è¿‘è€Œç§»åŠ¨é€ æˆçš„ã€‚æ­¤å¤–ï¼Œå†…çª¥é•œè§†é¢‘ä¸­çš„ä½çº¹ç†è¡¨é¢å¯¼è‡´ç°æœ‰SLAMæ¡†æ¶çš„å…‰åº¦æ†ç»‘è°ƒæ•´æ€§èƒ½ä¸å®¤å†…&#x2F;å®¤å¤–åœºæ™¯ç›¸æ¯”è¡¨ç°è¾ƒå·®ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¿‘åœºç…§æ˜æ†ç»‘è°ƒæ•´æŸå¤±ï¼ˆNFL-BAï¼‰ï¼Œå®ƒæ˜¾å¼åœ°å°†è¿‘åœºç…§æ˜ä½œä¸ºæ†ç»‘è°ƒæ•´æŸå¤±çš„ä¸€éƒ¨åˆ†ï¼Œä»è€Œå®ç°å¯¹ä½çº¹ç†è¡¨é¢è¿›è¡Œæ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºçš„NFL-BAå¯ä»¥åº”ç”¨äºä»»ä½•åŸºäºç¥ç»æ¸²æŸ“çš„SLAMæ¡†æ¶ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œé€šè¿‡ç”¨æˆ‘ä»¬æå‡ºçš„NFL-BAæ›¿æ¢ä¼ ç»Ÿçš„å…‰åº¦æ†ç»‘è°ƒæ•´æŸå¤±ï¼Œä½¿ç”¨ç¥ç»éšå¼SLAMå’Œ3DGS SLAMsä¼šæœ‰æ”¹è¿›æ•ˆæœã€‚é™¤äº†åœ¨ç»“è‚ é•œæ£€æŸ¥C3VDæ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„è·Ÿè¸ªå’Œæ˜ å°„ç»“æœå¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨çœŸå®ç»“è‚ é•œæ£€æŸ¥è§†é¢‘ä¸Šçš„æ”¹è¿›ã€‚å…·ä½“æˆæœå¯å‚è§ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://asdunnbe.github.io/NFL-BA/%E3%80%82">https://asdunnbe.github.io/NFL-BA/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13176v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å†…é•œè§†é¢‘ä¸­çš„åŒæ­¥å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰å¯å®ç°è‡ªä¸»å¯¼èˆªã€å¯¼å‘æœªå‹˜æµ‹åŒºåŸŸã€ç›²ç‚¹æ£€æµ‹åŠä¸‰ç»´å¯è§†åŒ–ï¼Œå¯æ˜¾è‘—æ”¹å–„æ‚£è€…åŠåŒ»å¸ˆçš„å†…é•œä½“éªŒå¹¶æå‡æ‚£è€…æ²»ç–—æ•ˆæœã€‚é’ˆå¯¹å†…é•œè§†é¢‘ä¸­çš„åŠ¨æ€è¿‘åœºç…§æ˜å’Œä½çº¹ç†è¡¨é¢é—®é¢˜ï¼Œå¼•å…¥è¿‘åœºç…§æ˜æ†ç»‘è°ƒæ•´æŸå¤±ï¼ˆNFL-BAï¼‰ï¼Œå¯¹ä»»ä½•åŸºäºç¥ç»æ¸²æŸ“çš„SLAMæ¡†æ¶éƒ½èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚é‡‡ç”¨NFL-BAæ›¿æ¢ä¼ ç»Ÿå…‰åº¦æ†ç»‘è°ƒæ•´æŸå¤±ï¼Œèƒ½æ˜¾è‘—æå‡ç¥ç»éšå¼SLAMå’Œ3DGS SLAMçš„è¡¨ç°ï¼Œåœ¨ç»“è‚ é•œæ£€æŸ¥C3VDæ•°æ®é›†å’ŒçœŸå®ç»“è‚ é•œè§†é¢‘ä¸Šéƒ½å±•ç°äº†å“è¶Šçš„è¿½è¸ªå’Œæ˜ å°„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLAMæŠ€æœ¯ä»å†…é•œè§†é¢‘ä¸­å¯åŠ©åŠ›è‡ªä¸»å¯¼èˆªã€å¯¼å‘æœªå‹˜æµ‹åŒºåŸŸç­‰ï¼Œæ˜¾è‘—æ”¹è¿›åŒ»ç–—ä¸ä½“éªŒã€‚</li>
<li>å†…é•œè§†é¢‘å­˜åœ¨åŠ¨æ€è¿‘åœºç…§æ˜ä¸ä½çº¹ç†è¡¨é¢é—®é¢˜ï¼Œç°æœ‰SLAMç®—æ³•æ€§èƒ½å—é™ã€‚</li>
<li>å¼•å…¥NFL-BAèƒ½æœ‰æ•ˆè§£å†³åŠ¨æ€è¿‘åœºç…§æ˜é—®é¢˜ï¼Œæå‡åœ¨ä½çº¹ç†è¡¨é¢çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>NFL-BAé€‚ç”¨äºæ‰€æœ‰åŸºäºç¥ç»æ¸²æŸ“çš„SLAMæ¡†æ¶ã€‚</li>
<li>ç”¨NFL-BAæ›¿æ¢ä¼ ç»Ÿå…‰åº¦æ†ç»‘è°ƒæ•´æŸå¤±èƒ½æ˜¾è‘—æé«˜SLAMæŠ€æœ¯è¡¨ç°ã€‚</li>
<li>åœ¨ç»“è‚ é•œæ£€æŸ¥æ•°æ®é›†åŠçœŸå®åœºæ™¯ä¸­ï¼Œåº”ç”¨NFL-BAçš„SLAMæŠ€æœ¯å…·æœ‰ä¼˜ç§€è¿½è¸ªä¸æ˜ å°„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c45a38c15e3ec31c9ed65a183ff4a8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-613dc2343dfed646ffd1d95d61a5d4c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33e2fdabe60fa882517dd7f9c979dea2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e2224104e6762dc2ca8b7393cdc3abc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multi-View-Pose-Agnostic-Change-Localization-with-Zero-Labels"><a href="#Multi-View-Pose-Agnostic-Change-Localization-with-Zero-Labels" class="headerlink" title="Multi-View Pose-Agnostic Change Localization with Zero Labels"></a>Multi-View Pose-Agnostic Change Localization with Zero Labels</h2><p><strong>Authors:Chamuditha Jayanga Galappaththige, Jason Lai, Lloyd Windrim, Donald Dansereau, Niko Suenderhauf, Dimity Miller</strong></p>
<p>Autonomous agents often require accurate methods for detecting and localizing changes in their environment, particularly when observations are captured from unconstrained and inconsistent viewpoints. We propose a novel label-free, pose-agnostic change detection method that integrates information from multiple viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS) representation of the scene. With as few as 5 images of the post-change scene, our approach can learn an additional change channel in a 3DGS and produce change masks that outperform single-view techniques. Our change-aware 3D scene representation additionally enables the generation of accurate change masks for unseen viewpoints. Experimental results demonstrate state-of-the-art performance in complex multi-object scenes, achieving a 1.7x and 1.5x improvement in Mean Intersection Over Union and F1 score respectively over other baselines. We also contribute a new real-world dataset to benchmark change detection in diverse challenging scenes in the presence of lighting variations. </p>
<blockquote>
<p>è‡ªä¸»ä»£ç†é€šå¸¸éœ€è¦å‡†ç¡®çš„æ–¹æ³•æ¥æ£€æµ‹å¹¶å®šä½å…¶ç¯å¢ƒä¸­çš„å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»ä¸å—é™åˆ¶å’Œä¸ä¸€è‡´çš„è§†è§’æ•è·è§‚å¯Ÿç»“æœæ—¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— æ ‡ç­¾ã€ä¸å—å§¿æ€å½±å“çš„å˜åŒ–æ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ•´åˆäº†æ¥è‡ªå¤šä¸ªè§†è§’çš„ä¿¡æ¯ï¼Œä»¥æ„å»ºä¸€ç§å¯¹å˜åŒ–æœ‰æ„ŸçŸ¥çš„3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰åœºæ™¯è¡¨ç¤ºã€‚ä»…ä½¿ç”¨å˜åŒ–ååœºæ™¯çš„5å¼ å›¾åƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨3DGSä¸­å­¦ä¹ é¢å¤–çš„å˜åŒ–é€šé“ï¼Œå¹¶äº§ç”Ÿä¼˜äºå•è§†å›¾æŠ€æœ¯çš„å˜åŒ–è’™ç‰ˆã€‚æˆ‘ä»¬çš„å¯¹å˜åŒ–æœ‰æ„ŸçŸ¥çš„3Dåœºæ™¯è¡¨ç¤ºè¿˜å…è®¸ä¸ºæœªè§è¿‡çš„è§†è§’ç”Ÿæˆå‡†ç¡®çš„å˜åŒ–è’™ç‰ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤æ‚çš„å¤šå¯¹è±¡åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡äº¤å¹¶æ¯”å’ŒF1åˆ†æ•°æ–¹é¢åˆ†åˆ«å®ç°äº†æ¯”å…¶ä»–åŸºå‡†æ–¹æ³•1.7å€å’Œ1.5å€çš„æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜ä¸ºåœ¨å…‰ç…§å˜åŒ–å­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šæ ·ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­æ£€æµ‹å˜åŒ–æä¾›äº†ä¸€ä¸ªæ–°çš„ç°å®ä¸–ç•Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03911v2">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong><br>è‡ªä¸»æ™ºèƒ½ä½“åœ¨ç¯å¢ƒå˜åŒ–çš„æ£€æµ‹å’Œå®šä½æ–¹é¢éœ€è¦å‡†ç¡®çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§‚å¯Ÿç‚¹ä¸å—é™åˆ¶ä¸”ä¸ä¸€è‡´çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…æ ‡ç­¾ã€æ— å§¿æ€å˜åŒ–çš„æ£€æµ‹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ•´åˆå¤šè§†è§’çš„ä¿¡æ¯æ¥æ„å»ºå¯¹ç¯å¢ƒå˜åŒ–æœ‰æ„ŸçŸ¥çš„3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºã€‚ä»…éœ€å°‘é‡å˜åŒ–åçš„åœºæ™¯å›¾åƒï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨3DGSä¸­å­¦ä¹ é¢å¤–çš„å˜åŒ–é€šé“ï¼Œå¹¶äº§ç”Ÿä¼˜äºå•è§†è§’æŠ€æœ¯çš„å˜åŒ–è’™ç‰ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å˜åŒ–æ„ŸçŸ¥3Dåœºæ™¯è¡¨ç¤ºèƒ½å¤Ÿä¸ºæœªè§è¿‡çš„è§†è§’ç”Ÿæˆå‡†ç¡®çš„å˜åŒ–è’™ç‰ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤æ‚çš„å¤šå¯¹è±¡åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼ŒMean Intersection Over Unionå’ŒF1åˆ†æ•°åˆ†åˆ«æé«˜äº†1.7å€å’Œ1.5å€ã€‚æˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªæ–°çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œä»¥åœ¨å…‰ç…§å˜åŒ–çš„æƒ…å†µä¸‹å¯¹å¤šå˜åœºæ™¯ä¸­çš„å˜åŒ–æ£€æµ‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§å…æ ‡ç­¾ã€æ— å§¿æ€å˜åŒ–çš„æ£€æµ‹æ–¹æ³•ç”¨äºç¯å¢ƒå˜åŒ–æ£€æµ‹ã€‚</li>
<li>é€šè¿‡æ•´åˆå¤šè§†è§’ä¿¡æ¯æ„å»ºå˜åŒ–æ„ŸçŸ¥çš„3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºã€‚</li>
<li>ä»…éœ€å°‘é‡å˜åŒ–åçš„åœºæ™¯å›¾åƒï¼Œèƒ½åœ¨3DGSä¸­å­¦ä¹ é¢å¤–çš„å˜åŒ–é€šé“ã€‚</li>
<li>äº§ç”Ÿçš„å˜åŒ–è’™ç‰ˆæ€§èƒ½ä¼˜äºå•è§†è§’æŠ€æœ¯ã€‚</li>
<li>å˜åŒ–æ„ŸçŸ¥çš„3Dåœºæ™¯è¡¨ç¤ºèƒ½ç”Ÿæˆæœªè§è§†è§’çš„å‡†ç¡®å˜åŒ–è’™ç‰ˆã€‚</li>
<li>åœ¨å¤æ‚å¤šå¯¹è±¡åœºæ™¯ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¾ƒå…¶ä»–æ–¹æ³•æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d8e07fa3cd0a478475edd96f5cd7645.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa547a9350da8a0ffeaa1386f776b58a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eea2a0130289ec8fafd70b8b401a29f8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Gaussian-Eigen-Models-for-Human-Heads"><a href="#Gaussian-Eigen-Models-for-Human-Heads" class="headerlink" title="Gaussian Eigen Models for Human Heads"></a>Gaussian Eigen Models for Human Heads</h2><p><strong>Authors:Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies</strong></p>
<p>Current personalized neural head avatars face a trade-off: lightweight models lack detail and realism, while high-quality, animatable avatars require significant computational resources, making them unsuitable for commodity devices. To address this gap, we introduce Gaussian Eigen Models (GEM), which provide high-quality, lightweight, and easily controllable head avatars. GEM utilizes 3D Gaussian primitives for representing the appearance combined with Gaussian splatting for rendering. Building on the success of mesh-based 3D morphable face models (3DMM), we define GEM as an ensemble of linear eigenbases for representing the head appearance of a specific subject. In particular, we construct linear bases to represent the position, scale, rotation, and opacity of the 3D Gaussians. This allows us to efficiently generate Gaussian primitives of a specific head shape by a linear combination of the basis vectors, only requiring a low-dimensional parameter vector that contains the respective coefficients. We propose to construct these linear bases (GEM) by distilling high-quality compute-intense CNN-based Gaussian avatar models that can generate expression-dependent appearance changes like wrinkles. These high-quality models are trained on multi-view videos of a subject and are distilled using a series of principal component analyses. Once we have obtained the bases that represent the animatable appearance space of a specific human, we learn a regressor that takes a single RGB image as input and predicts the low-dimensional parameter vector that corresponds to the shown facial expression. In a series of experiments, we compare GEMâ€™s self-reenactment and cross-person reenactment results to state-of-the-art 3D avatar methods, demonstrating GEMâ€™s higher visual quality and better generalization to new expressions. </p>
<blockquote>
<p>å½“å‰ä¸ªæ€§åŒ–ç¥ç»å¤´éƒ¨åŒ–èº«é¢ä¸´ä¸€ä¸ªæƒè¡¡ï¼šè½»é‡çº§æ¨¡å‹ç¼ºä¹ç»†èŠ‚å’Œé€¼çœŸåº¦ï¼Œè€Œé«˜è´¨é‡ã€å¯åŠ¨ç”»çš„åŒ–èº«éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºï¼Œä½¿å…¶ä¸é€‚åˆæ™®é€šè®¾å¤‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†é«˜æ–¯ç‰¹å¾æ¨¡å‹ï¼ˆGEMï¼‰ï¼Œå®ƒæä¾›é«˜è´¨é‡ã€è½»ä¾¿ä¸”æ˜“äºæ§åˆ¶çš„å¤´éƒ¨åŒ–èº«ã€‚GEMä½¿ç”¨3Dé«˜æ–¯åŸå§‹å›¾å½¢æ¥è¡¨ç¤ºå¤–è§‚ï¼Œå¹¶ç»“åˆé«˜æ–¯å–·ç»˜è¿›è¡Œæ¸²æŸ“ã€‚åŸºäºåŸºäºç½‘æ ¼çš„3Då¯å˜å½¢é¢éƒ¨æ¨¡å‹ï¼ˆ3DMMï¼‰çš„æˆåŠŸï¼Œæˆ‘ä»¬å°†GEMå®šä¹‰ä¸ºè¡¨ç¤ºç‰¹å®šä¸»ä½“å¤´éƒ¨å¤–è§‚çš„çº¿æ€§ç‰¹å¾åŸºé›†åˆã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æ„å»ºäº†è¡¨ç¤ºä½ç½®ã€å°ºåº¦ã€æ—‹è½¬å’Œé€æ˜åº¦çš„çº¿æ€§åŸºã€‚è¿™å…è®¸æˆ‘ä»¬é€šè¿‡çº¿æ€§ç»„åˆåŸºå‘é‡æœ‰æ•ˆåœ°ç”Ÿæˆç‰¹å®šå¤´éƒ¨å½¢çŠ¶çš„é«˜æ–¯åŸå§‹å›¾å½¢ï¼Œä»…éœ€è¦ä¸€ä¸ªä½ç»´å‚æ•°å‘é‡ï¼Œå…¶ä¸­åŒ…å«ç›¸åº”çš„ç³»æ•°ã€‚æˆ‘ä»¬æè®®é€šè¿‡è’¸é¦é«˜è´¨é‡çš„è®¡ç®—å¯†é›†å‹CNNé«˜æ–¯åŒ–èº«æ¨¡å‹æ¥æ„å»ºè¿™äº›çº¿æ€§åŸºï¼ˆGEMï¼‰ï¼Œè¯¥æ¨¡å‹å¯ä»¥ç”Ÿæˆä¸è¡¨æƒ…ç›¸å…³çš„å¤–è§‚å˜åŒ–ï¼Œå¦‚çš±çº¹ã€‚è¿™äº›é«˜è´¨é‡æ¨¡å‹åœ¨ä¸»ä½“çš„å¤šè§†è§’è§†é¢‘ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨ä¸€ç³»åˆ—ä¸»æˆåˆ†åˆ†æè¿›è¡Œæç‚¼ã€‚ä¸€æ—¦æˆ‘ä»¬è·å¾—äº†ä»£è¡¨ç‰¹å®šäººç±»å¯åŠ¨ç”»å¤–è§‚ç©ºé—´çš„åŸºåœ°ï¼Œæˆ‘ä»¬å°±å­¦ä¹ ä¸€ä¸ªå›å½’å™¨ï¼Œå®ƒæ¥å—å•å¼ RGBå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹ä¸æ‰€ç¤ºé¢éƒ¨è¡¨æƒ…ç›¸å¯¹åº”çš„ä½ç»´å‚æ•°å‘é‡ã€‚åœ¨ä¸€ç³»åˆ—å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†GEMçš„è‡ªæˆ‘é‡æ–°æ¼”ç»å’Œè·¨äººé‡æ–°æ¼”ç»çš„ç»“æœä¸æœ€å…ˆè¿›çš„3DåŒ–èº«æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œè¯æ˜äº†GEMæ›´é«˜çš„è§†è§‰è´¨é‡å’Œå¯¹æ–°è¡¨æƒ…çš„æ›´å¥½æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04545v3">PDF</a> Accepted to CVPR25 Website: <a target="_blank" rel="noopener" href="https://zielon.github.io/gem/">https://zielon.github.io/gem/</a></p>
<p><strong>Summary</strong><br>     å¼•å…¥é«˜æ–¯ç‰¹å¾æ¨¡å‹ï¼ˆGEMï¼‰è§£å†³ä¸ªæ€§åŒ–ç¥ç»å¤´éƒ¨åŒ–èº«é¢ä¸´çš„å›°å¢ƒï¼Œå³è½»é‡åŒ–æ¨¡å‹ç¼ºä¹ç»†èŠ‚å’ŒçœŸå®æ„Ÿï¼Œè€Œé«˜è´¨é‡ã€å¯åŠ¨ç”»çš„åŒ–èº«éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºï¼Œä¸é€‚ç”¨äºæ™®é€šè®¾å¤‡ã€‚GEMåˆ©ç”¨3Dé«˜æ–¯åŸå§‹è¡¨ç¤ºå¤–è§‚ï¼Œç»“åˆé«˜æ–¯å±•å¼€è¿›è¡Œæ¸²æŸ“ï¼Œæä¾›é«˜è´¨é‡ã€è½»ä¾¿ä¸”æ˜“äºæ§åˆ¶çš„å¤´éƒ¨åŒ–èº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ä¸ªæ€§åŒ–ç¥ç»å¤´éƒ¨åŒ–èº«é¢ä¸´è½»é‡åŒ–ä¸é«˜è´¨é‡ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>é«˜æ–¯ç‰¹å¾æ¨¡å‹ï¼ˆGEMï¼‰åˆ©ç”¨3Dé«˜æ–¯åŸå§‹å’ŒGaussian splattingæ¸²æŸ“æŠ€æœ¯æ¥æä¾›é«˜è´¨é‡ã€è½»ä¾¿çš„å¤´éƒ¨åŒ–èº«è§£å†³æ–¹æ¡ˆã€‚</li>
<li>GEMæ˜¯åŸºäº3Då¯å˜å½¢é¢éƒ¨æ¨¡å‹ï¼ˆ3DMMï¼‰çš„çº¿æ€§ç‰¹å¾åŸºæ¥è¡¨ç¤ºç‰¹å®šä¸»ä½“çš„å¤´éƒ¨å¤–è§‚ã€‚</li>
<li>GEMé€šè¿‡æ„å»ºçº¿æ€§åŸºæ¥è¡¨ç¤º3Dé«˜æ–¯çš„ä½ç½®ã€å°ºåº¦ã€æ—‹è½¬å’Œé€æ˜åº¦ã€‚</li>
<li>é«˜è´¨é‡è®¡ç®—å¯†é›†å‹çš„CNNé«˜æ–¯åŒ–èº«æ¨¡å‹è¢«ç”¨æ¥ç”Ÿæˆè¡¨æƒ…ç›¸å…³çš„å¤–è§‚å˜åŒ–ï¼Œå¦‚çš±çº¹ã€‚</li>
<li>é€šè¿‡ä¸»æˆåˆ†åˆ†æç³»åˆ—è¿›è¡Œè’¸é¦ï¼Œå¾—åˆ°ä»£è¡¨ç‰¹å®šäººç±»å¯åŠ¨ç”»å¤–è§‚ç©ºé—´çš„çº¿æ€§åŸºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰3DåŒ–èº«æ–¹æ³•ç›¸æ¯”ï¼ŒGEMåœ¨è‡ªæˆ‘è¡¨æ¼”å’Œè·¨äººè¡¨æ¼”æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„è§†è§‰è´¨é‡å’Œæ›´å¥½çš„æ–°è¡¨æƒ…æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eeffbb39627defba37c3c39b8c642a18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5af958ac51e806a6a04279e5c3a8e4f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c8a050527e7fa73a744ccffe8d9c088.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7c92d96a50e4945e497e59fb29abe24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8546ec1800df6e0cf1fdd834df1aecca.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MG-SLAM-Structure-Gaussian-Splatting-SLAM-with-Manhattan-World-Hypothesis"><a href="#MG-SLAM-Structure-Gaussian-Splatting-SLAM-with-Manhattan-World-Hypothesis" class="headerlink" title="MG-SLAM: Structure Gaussian Splatting SLAM with Manhattan World   Hypothesis"></a>MG-SLAM: Structure Gaussian Splatting SLAM with Manhattan World   Hypothesis</h2><p><strong>Authors:Shuhong Liu, Tianchen Deng, Heng Zhou, Liuzhuozheng Li, Hongyu Wang, Danwei Wang, Mingrui Li</strong></p>
<p>Gaussian Splatting SLAMs have made significant advancements in improving the efficiency and fidelity of real-time reconstructions. However, these systems often encounter incomplete reconstructions in complex indoor environments, characterized by substantial holes due to unobserved geometry caused by obstacles or limited view angles. To address this challenge, we present Manhattan Gaussian SLAM, an RGB-D system that leverages the Manhattan World hypothesis to enhance geometric accuracy and completeness. By seamlessly integrating fused line segments derived from structured scenes, our method ensures robust tracking in textureless indoor areas. Moreover, The extracted lines and planar surface assumption allow strategic interpolation of new Gaussians in regions of missing geometry, enabling efficient scene completion. Extensive experiments conducted on both synthetic and real-world scenes demonstrate that these advancements enable our method to achieve state-of-the-art performance, marking a substantial improvement in the capabilities of Gaussian SLAM systems. </p>
<blockquote>
<p>é«˜æ–¯æ¨¡ç³Šç‚¹åˆ†å‰²æŠ€æœ¯ï¼ˆSLAMsï¼‰åœ¨æå‡å®æ—¶é‡å»ºçš„æ•ˆç‡å’Œä¿çœŸåº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿåœ¨å¤æ‚çš„å®¤å†…ç¯å¢ƒä¸­ç»å¸¸é‡åˆ°é‡å»ºä¸å®Œæ•´çš„é—®é¢˜ï¼Œä¸»è¦è¡¨ç°ä¸ºç”±äºéšœç¢ç‰©æˆ–æœ‰é™è§†è§’å¯¼è‡´çš„æœªè§‚å¯Ÿåˆ°çš„å‡ ä½•ç»“æ„è€Œäº§ç”Ÿçš„å¤§é‡ç©ºæ´ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ›¼å“ˆé¡¿é«˜æ–¯SLAMç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ›¼å“ˆé¡¿ä¸–ç•Œå‡è®¾æ¥æé«˜å‡ ä½•å‡†ç¡®æ€§å’Œå®Œæ•´æ€§çš„RGB-Dç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ— ç¼é›†æˆä»ç»“æ„åŒ–åœºæ™¯ä¸­æ´¾ç”Ÿå‡ºçš„èåˆçº¿æ®µæ¥ç¡®ä¿åœ¨çº¹ç†ç¼ºå¤±çš„å®¤å†…åŒºåŸŸè¿›è¡Œç¨³å¥çš„è·Ÿè¸ªã€‚æ­¤å¤–ï¼Œæå–çš„çº¿æ¡å’Œå¹³é¢è¡¨é¢å‡è®¾å…è®¸åœ¨æœ‰ç¼ºå¤±å‡ ä½•ç»“æ„çš„åŒºåŸŸä¸­è¿›è¡Œæ–°çš„é«˜æ–¯å€¼çš„ç­–ç•¥æ€§æ’å€¼ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„åœºæ™¯è¡¥å…¨ã€‚åœ¨åˆæˆåœºæ™¯å’ŒçœŸå®åœºæ™¯ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¿™äº›è¿›å±•ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œæ ‡å¿—ç€é«˜æ–¯SLAMç³»ç»Ÿçš„èƒ½åŠ›å¾—åˆ°äº†å®è´¨æ€§æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20031v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é«˜æ–¯æ··åˆSLAMåœ¨æå‡å®æ—¶é‡å»ºçš„æ•ˆç‡å’Œä¿çœŸåº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨å¤æ‚çš„å®¤å†…ç¯å¢ƒä¸­ï¼Œè¿™äº›ç³»ç»Ÿç»å¸¸é‡åˆ°ä¸å®Œæ•´é‡å»ºçš„é—®é¢˜ï¼Œè¡¨ç°ä¸ºç”±äºéšœç¢ç‰©æˆ–æœ‰é™è§†è§’å¯¼è‡´çš„æœªè§‚æµ‹åˆ°çš„å‡ ä½•ç»“æ„äº§ç”Ÿçš„å¤§é‡ç©ºæ´ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºæ›¼å“ˆé¡¿é«˜æ–¯SLAMç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªRGB-Dç³»ç»Ÿï¼Œåˆ©ç”¨æ›¼å“ˆé¡¿ä¸–ç•Œå‡è®¾æé«˜å‡ ä½•ç²¾åº¦å’Œå®Œæ•´æ€§ã€‚é€šè¿‡æ— ç¼é›†æˆæ¥è‡ªç»“æ„åŒ–åœºæ™¯çš„èåˆçº¿æ®µï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿åœ¨çº¹ç†è¾ƒå°‘çš„å®¤å†…åŒºåŸŸå®ç°ç¨³å¥è·Ÿè¸ªã€‚æ­¤å¤–ï¼Œæå–çš„çº¿æ¡å’Œå¹³é¢è¡¨é¢å‡è®¾å…è®¸åœ¨ç¼ºå¤±å‡ ä½•åŒºåŸŸè¿›è¡Œæ–°çš„é«˜æ–¯æˆ˜ç•¥æ’å€¼ï¼Œä»è€Œå®ç°é«˜æ•ˆåœºæ™¯å®Œæˆã€‚åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¿™äº›è¿›æ­¥ä½¿æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ ‡å¿—ç€é«˜æ–¯SLAMç³»ç»Ÿçš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ–¯æ··åˆSLAMå·²æ˜¾è‘—æé«˜å®æ—¶é‡å»ºçš„æ•ˆç‡å’Œä¿çœŸåº¦ã€‚</li>
<li>åœ¨å¤æ‚å®¤å†…ç¯å¢ƒä¸­ï¼Œé«˜æ–¯æ··åˆSLAMé¢ä¸´ä¸å®Œæ•´é‡å»ºé—®é¢˜ï¼Œè¡¨ç°ä¸ºå¤§é‡ç©ºæ´ã€‚</li>
<li>æ›¼å“ˆé¡¿é«˜æ–¯SLAMæ˜¯ä¸€ä¸ªRGB-Dç³»ç»Ÿï¼Œåˆ©ç”¨æ›¼å“ˆé¡¿ä¸–ç•Œå‡è®¾å¢å¼ºå‡ ä½•ç²¾åº¦å’Œå®Œæ•´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ— ç¼é›†æˆèåˆçº¿æ®µï¼Œç¡®ä¿åœ¨çº¹ç†è¾ƒå°‘çš„å®¤å†…åŒºåŸŸå®ç°ç¨³å¥è·Ÿè¸ªã€‚</li>
<li>æå–çš„çº¿æ¡å’Œå¹³é¢è¡¨é¢å‡è®¾å…è®¸åœ¨ç¼ºå¤±å‡ ä½•åŒºåŸŸè¿›è¡Œæ–°çš„é«˜æ–¯æ’å€¼ï¼Œå®ç°é«˜æ•ˆåœºæ™¯å®Œæˆã€‚</li>
<li>æ›¼å“ˆé¡¿é«˜æ–¯SLAMåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸Šçš„å®éªŒè¡¨ç°è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.20031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e09af0665ea0e0c4028fe1d819b48e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05a2419e540e126a498294b438401b08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0918055d43e3d5fbf954198ae6a125ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6f0cff953ddc00e48437796cd7743ca.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-8442cae68b84efc64254cdf2e747c143.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  GAN-enhanced Simulation-driven DNN Testing in Absence of Ground Truth
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e4ed3c6d3226799ea29effeea1e08026.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-22  Zero-1-to-A Zero-Shot One Image to Animatable Head Avatars Using Video   Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
