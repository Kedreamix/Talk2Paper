<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-03-22  1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-49aed1bd4f31f94caeaddc75b9eb1cdd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    54 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-22-更新"><a href="#2025-03-22-更新" class="headerlink" title="2025-03-22 更新"></a>2025-03-22 更新</h1><h2 id="1000-FPS-4D-Gaussian-Splatting-for-Dynamic-Scene-Rendering"><a href="#1000-FPS-4D-Gaussian-Splatting-for-Dynamic-Scene-Rendering" class="headerlink" title="1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering"></a>1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering</h2><p><strong>Authors:Yuheng Yuan, Qiuhong Shen, Xingyi Yang, Xinchao Wang</strong></p>
<p>4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) \textbf{Short-Lifespan Gaussians}: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) \textbf{Inactive Gaussians}: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present \textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a $41\times$ reduction in storage and $9\times$ faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at <a target="_blank" rel="noopener" href="https://4dgs-1k.github.io/">https://4DGS-1K.github.io</a>. </p>
<blockquote>
<p>近期，4D高斯点铺展法（4DGS）因其重建动态场景的方法而备受关注。尽管其能达成高品质效果，但4DGS通常需要大量存储空间并面临渲染速度较慢的问题。在这项研究中，我们深入探讨了这些问题，并识别出两个主要的时空冗余来源。（Q1）<strong>短暂寿命高斯点</strong>：4DGS使用大量短暂时空跨度的高斯点来表示场景动态，导致高斯点数量过多。（Q2）<strong>非活跃高斯点</strong>：在渲染过程中，只有一小部分高斯点对每一帧有所贡献。尽管如此，所有高斯点在光栅化时都被处理，导致计算开销冗余。为了应对这些冗余问题，我们提出了<strong>4DGS-1K</strong>，在现代GPU上运行速度超过1000帧每秒。针对Q1问题，我们引入了时空变化分数这一新的修剪标准，该标准有效地移除了短暂寿命的高斯点并鼓励使用具有较长时空跨度的高斯点来捕捉场景动态。针对Q2问题，我们存储了连续帧中活跃高斯点的掩膜，显著减少了渲染过程中的冗余计算。与常规4DGS相比，我们的方法实现了存储空间的41倍压缩和复杂动态场景的渲染速度9倍提升，同时保持相当的可视质量。请访问我们的项目页面<a target="_blank" rel="noopener" href="https://4dgs-1k.github.io了解更多信息./">https://4DGS-1K.github.io了解更多信息。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16422v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文研究了4D高斯展平技术（4DGS）在处理动态场景重建时面临的挑战，如存储需求大、渲染速度慢的问题。针对这些问题，本文提出了两个关键的时间冗余来源：短寿命高斯和未激活的高斯。针对这两个问题，本文提出了解决方案，包括引入空间时间变化评分作为新的修剪标准，以去除短寿命高斯；并为连续帧中的活跃高斯存储掩膜，从而减少渲染过程中的冗余计算。因此，新的方法在保证视觉质量的同时，实现了存储减少41倍、复杂动态场景的渲染速度提高9倍。详情请参见我们的项目网页。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>4DGS技术在处理动态场景重建时面临的挑战包括存储需求大和渲染速度慢的问题。</li>
<li>研究指出了两个关键的时间冗余来源：短寿命高斯和未激活的高斯。</li>
<li>通过引入空间时间变化评分作为新的修剪标准，可有效去除短寿命高斯。</li>
<li>为连续帧中的活跃高斯存储掩膜，以减少渲染过程中的冗余计算。</li>
<li>与传统的4DGS相比，新方法实现了存储的显著减少和渲染速度的显著提高。</li>
<li>项目详细信息可访问其项目网页获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16422">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7181eb55a60559a766973025680ff3bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e307f6f84588ec38c34e6d297ad7c3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b104379fa86b03939609494db9ad7c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7167631541b249590bd1ce1b99d1586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b3736ff6f10dc5d8bcc38bd298c6e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3709f60772ca8f51cbd8dbf9183d1fe.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="M3-3D-Spatial-MultiModal-Memory"><a href="#M3-3D-Spatial-MultiModal-Memory" class="headerlink" title="M3: 3D-Spatial MultiModal Memory"></a>M3: 3D-Spatial MultiModal Memory</h2><p><strong>Authors:Xueyan Zou, Yuchen Song, Ri-Zhao Qiu, Xuanbin Peng, Jianglong Ye, Sifei Liu, Xiaolong Wang</strong></p>
<p>We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs&#x2F;LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3’s feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation. </p>
<blockquote>
<p>我们提出了3D空间多模态记忆（M3），这是一种多模态记忆系统，旨在通过视频源为视觉感知保留中等规模静态场景的信息。通过整合3D高斯摊铺技术与基础模型，M3建立了一个多模态记忆，能够在各个粒度上呈现特征表示，涵盖广泛的知识。在探索中，我们发现了之前特征铺设工作中的两个关键挑战：(1)为每个高斯原始数据保存高维特征的计算约束；(2)提炼特征与基础模型特征之间的不匹配或信息丢失。为了解决这些挑战，我们提出了M3，其主要组件包括主要场景组件和高斯记忆注意力，能够实现高效的训练和推理。为了验证M3，我们对特征相似性和下游任务进行了全面的定量评估，以及定性可视化以突出高斯记忆注意力的像素轨迹。我们的方法涵盖了多种基础模型，包括视觉语言模型（VLMs）、感知模型以及大型多模态和语言模型（LMMs&#x2F;LLMs）。此外，为了证明M3在现实世界中的适用性，我们在四足机器人的室内场景中应用了M3的特征字段。值得注意的是，我们声称M3是首个解决3D特征蒸馏中核心压缩挑战的工作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16413v1">PDF</a> ICLR2025 homepage: <a target="_blank" rel="noopener" href="https://m3-spatial-memory.github.io/">https://m3-spatial-memory.github.io</a> code:   <a target="_blank" rel="noopener" href="https://github.com/MaureenZOU/m3-spatial">https://github.com/MaureenZOU/m3-spatial</a></p>
<p><strong>Summary</strong><br>    提出一种名为M3的3D空间多模态记忆系统，采用视频源对中等静态场景进行信息存储，为视觉感知设计多模态记忆。通过结合3D高斯涂抹技术和基础模型，M3建立了一个多模态记忆，能够在不同粒度上呈现特征表示，涵盖广泛的知识。研究解决了先前特征涂抹工作中的两个关键挑战，并通过主要场景组件和高斯记忆注意力等关键组件实现高效训练和推理。通过定量评估特征相似性和下游任务以及定性可视化突出显示高斯记忆注意力的像素轨迹来验证M3。此外，M3还适用于室内场景的四肢机器人部署。M3是首个解决3D特征蒸馏中核心压缩挑战的工作。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>M3是一个3D空间多模态记忆系统，旨在通过视频源存储中等静态场景的信息。</li>
<li>M3结合了3D高斯涂抹技术和基础模型，建立了一个多模态记忆，能呈现不同粒度的特征表示。</li>
<li>研究解决了先前特征涂抹工作中的两个关键挑战：高维特征的存储计算约束以及蒸馏特征与基础模型特征之间的不匹配或信息丢失。</li>
<li>M3通过主要场景组件和高斯记忆注意力等关键组件实现高效训练和推理。</li>
<li>通过定量和定性评估验证了M3的有效性，包括特征相似性、下游任务性能以及高斯记忆注意力的可视化。</li>
<li>M3适用于室内场景的四肢机器人部署，展示了其实际应用的潜力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16413">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dff9ab96a997bec35448ab117431b450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9c4e572fbfc8defcb314123219752b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62677e8c27e882f152540f73b0d5a3dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90cd95f1f80a8632c11f6aca19e5beff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49aed1bd4f31f94caeaddc75b9eb1cdd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Gaussian-Graph-Network-Learning-Efficient-and-Generalizable-Gaussian-Representations-from-Multi-view-Images"><a href="#Gaussian-Graph-Network-Learning-Efficient-and-Generalizable-Gaussian-Representations-from-Multi-view-Images" class="headerlink" title="Gaussian Graph Network: Learning Efficient and Generalizable Gaussian   Representations from Multi-view Images"></a>Gaussian Graph Network: Learning Efficient and Generalizable Gaussian   Representations from Multi-view Images</h2><p><strong>Authors:Shengjun Zhang, Xin Fei, Fangfu Liu, Haixu Song, Yueqi Duan</strong></p>
<p>3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis performance. While conventional methods require per-scene optimization, more recently several feed-forward methods have been proposed to generate pixel-aligned Gaussian representations with a learnable network, which are generalizable to different scenes. However, these methods simply combine pixel-aligned Gaussians from multiple views as scene representations, thereby leading to artifacts and extra memory cost without fully capturing the relations of Gaussians from different images. In this paper, we propose Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian representations. Specifically, we construct Gaussian Graphs to model the relations of Gaussian groups from different views. To support message passing at Gaussian level, we reformulate the basic graph operations over Gaussian representations, enabling each Gaussian to benefit from its connected Gaussian groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling layer to aggregate various Gaussian groups for efficient representations. We conduct experiments on the large-scale RealEstate10K and ACID datasets to demonstrate the efficiency and generalization of our method. Compared to the state-of-the-art methods, our model uses fewer Gaussians and achieves better image quality with higher rendering speed. </p>
<blockquote>
<p>3D高斯卷积（3DGS）展现了令人印象深刻的全新视角合成性能。虽然传统方法需要进行场景优化，但最近已经提出了几种前馈方法，使用可学习网络生成像素对齐的高斯表示，这些方法可推广到不同的场景。然而，这些方法只是简单地将来自多个视角的像素对齐高斯组合起来作为场景表示，因此导致了伪像和额外的内存成本，而没有完全捕获来自不同图像的高斯之间的关系。在本文中，我们提出了高斯图网络（GGN）来生成高效且可推广的高斯表示。具体来说，我们构建高斯图来模拟来自不同视角的高斯组之间的关系。为了支持高斯级别的消息传递，我们对高斯表示上的基本图形操作进行重新表述，使每个高斯都能从其连接的高斯组中受益，并进行高斯特征融合。此外，我们设计了一个高斯池化层来聚合各种高斯组以实现高效表示。我们在大规模RealEstate10K和ACID数据集上进行了实验，以证明我们方法的效率和通用性。与最先进的方法相比，我们的模型使用更少的高斯，图像质量更高，渲染速度更快。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16338v1">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于高斯图网络（Gaussian Graph Network，GGN）的3D高斯融合方法，用于生成高效且通用的高斯表示。通过构建高斯图来模拟不同视角的高斯组关系，并重新定义了高斯层面的信息传递机制。该方法还包括高斯特征融合及高斯池化层的设计，以提升效率并改进表示质量。实验证明，该方法在大型数据集上表现出高效性和泛化能力，与现有方法相比，使用更少的高斯数即可达到更好的图像质量和更高的渲染速度。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS展现出令人印象深刻的新视角合成性能。</li>
<li>传统方法需要针对每个场景进行优化，而最新方法则通过可学习网络生成像素对齐的高斯表示，这些表示具有跨场景泛化能力。</li>
<li>现有方法简单地将多个视角的像素对齐高斯组合为场景表示，导致伪影和额外的内存成本，未能充分捕捉不同图像高斯之间的关系。</li>
<li>提出Gaussian Graph Network (GGN)来生成高效且通用的高斯表示。</li>
<li>通过构建高斯图来模拟不同视角的高斯组关系。</li>
<li>定义高斯层面的信息传递机制，每个高斯都能从相连的高斯组中受益。</li>
<li>设计了高斯特征融合及高斯池化层，用于高效表示。</li>
<li>在大型数据集RealEstate10K和ACID上的实验证明了该方法的效率和泛化能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16338">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1404c9dbf6707dc5e6a045aa48cb45a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39a8b901899411e16a84614350cd26c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0ec436595c12ec134690490e1c9f938.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3513ac9e8f169403333385f0df7665f4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OccluGaussian-Occlusion-Aware-Gaussian-Splatting-for-Large-Scene-Reconstruction-and-Rendering"><a href="#OccluGaussian-Occlusion-Aware-Gaussian-Splatting-for-Large-Scene-Reconstruction-and-Rendering" class="headerlink" title="OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene   Reconstruction and Rendering"></a>OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene   Reconstruction and Rendering</h2><p><strong>Authors:Shiyong Liu, Xiao Tang, Zhihao Li, Yingfan He, Chongjie Ye, Jianzhuang Liu, Binxiao Huang, Shunbo Zhou, Xiaofei Wu</strong></p>
<p>In large-scale scene reconstruction using 3D Gaussian splatting, it is common to partition the scene into multiple smaller regions and reconstruct them individually. However, existing division methods are occlusion-agnostic, meaning that each region may contain areas with severe occlusions. As a result, the cameras within those regions are less correlated, leading to a low average contribution to the overall reconstruction. In this paper, we propose an occlusion-aware scene division strategy that clusters training cameras based on their positions and co-visibilities to acquire multiple regions. Cameras in such regions exhibit stronger correlations and a higher average contribution, facilitating high-quality scene reconstruction. We further propose a region-based rendering technique to accelerate large scene rendering, which culls Gaussians invisible to the region where the viewpoint is located. Such a technique significantly speeds up the rendering without compromising quality. Extensive experiments on multiple large scenes show that our method achieves superior reconstruction results with faster rendering speed compared to existing state-of-the-art approaches. Project page: <a target="_blank" rel="noopener" href="https://occlugaussian.github.io/">https://occlugaussian.github.io</a>. </p>
<blockquote>
<p>在利用3D高斯拼接（3DGS）进行大规模场景重建时，通常会将场景分割成多个较小的区域并进行分别重建。然而，现有的分割方法都是无视遮挡的，这意味着每个区域都可能包含严重的遮挡区域。因此，这些区域内的相机关联性较低，对整体重建的平均贡献较小。在本文中，我们提出了一种感知遮挡的场景分割策略，该策略根据相机的位置和共视性对训练相机进行聚类，以获得多个区域。这些区域内的相机表现出更强的关联性和更高的平均贡献，有助于实现高质量的场景重建。此外，我们还提出了一种基于区域的渲染技术，以加速大场景的渲染，该技术会剔除对于视点所在区域不可见的Gauss图像。这种技术在不损害质量的情况下显著提高了渲染速度。在多个大型场景上的广泛实验表明，与现有的最先进的方法相比，我们的方法实现了更优越的重建结果并提高了渲染速度。项目页面：<a target="_blank" rel="noopener" href="https://occlugaussian.github.io./">https://occlugaussian.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16177v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://occlugaussian.github.io/">https://occlugaussian.github.io</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一个基于遮挡感知的场景分割策略，用于大规模场景重建。通过根据相机的位置和共视性进行聚类，获取多个区域，提高了相机间的相关性，进而提升了场景重建的质量。同时，本文还提出了一种基于区域的渲染技术，能加速大场景的渲染速度，而不会牺牲渲染质量。实验证明，该方法在重建效果和渲染速度上均优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有场景分割方法未考虑遮挡问题，导致区域内存在严重遮挡，影响相机间的相关性及重建质量。</li>
<li>本文提出的遮挡感知场景分割策略能够根据相机的位置和共视性进行聚类，形成多个区域，提高相机间的相关性。</li>
<li>在这些区域内，相机表现出更强的相关性和更高的平均贡献，有助于提高场景重建的质量。</li>
<li>提出了一种基于区域的渲染技术，能剔除视点所在区域不可见的Gaussian，显著加速大场景的渲染速度。</li>
<li>该技术在保证渲染质量的前提下，实现了高效的渲染。</li>
<li>通过对多个大场景的广泛实验，证明本文方法实现了优越的重建结果和更快的渲染速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16177">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f0ad270f0ed7b4278ca8a6c43e37327.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-590a751393655395ef0a1a8aa4acd124.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b34874535b2c01839adcc6e16fccd86c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08c43cbb605f6cebcbc6cf3777c4b97e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-Close-up-Novel-View-Synthesis-via-Pseudo-labeling"><a href="#Enhancing-Close-up-Novel-View-Synthesis-via-Pseudo-labeling" class="headerlink" title="Enhancing Close-up Novel View Synthesis via Pseudo-labeling"></a>Enhancing Close-up Novel View Synthesis via Pseudo-labeling</h2><p><strong>Authors:Jiatong Xia, Libo Sun, Lingqiao Liu</strong></p>
<p>Recent methods, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated remarkable capabilities in novel view synthesis. However, despite their success in producing high-quality images for viewpoints similar to those seen during training, they struggle when generating detailed images from viewpoints that significantly deviate from the training set, particularly in close-up views. The primary challenge stems from the lack of specific training data for close-up views, leading to the inability of current methods to render these views accurately. To address this issue, we introduce a novel pseudo-label-based learning strategy. This approach leverages pseudo-labels derived from existing training data to provide targeted supervision across a wide range of close-up viewpoints. Recognizing the absence of benchmarks for this specific challenge, we also present a new dataset designed to assess the effectiveness of both current and future methods in this area. Our extensive experiments demonstrate the efficacy of our approach. </p>
<blockquote>
<p>最近的方法，如神经辐射场（NeRF）和三维高斯拼贴（3DGS），在新型视图合成中展示了显著的能力。尽管它们在生成与训练期间所见相似的视角的高质量图像方面非常成功，但在生成从与训练集偏差较大的视角出发的详细图像时却表现挣扎，特别是在特写镜头视角下。主要挑战源于特写镜头视角的具体训练数据的缺乏，导致当前方法无法准确渲染这些视图。为了解决这一问题，我们引入了一种基于伪标签的学习策略。该方法利用来自现有训练数据的伪标签，为各种特写镜头视角提供有针对性的监督。由于当前缺乏针对这一挑战的基准测试，我们还推出了一个新的数据集，旨在评估当前和未来方法在这个领域的有效性。我们的大量实验证明了我们的方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15908v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>NeRF和3DGS等方法在新型视角合成方面表现出卓越的能力，但在生成与训练集视角差异较大的详细图像时，尤其是近距离视角的图像，存在困难。主要挑战在于缺乏针对近距离视角的特定训练数据，导致现有方法无法准确渲染这些视图。为解决这一问题，我们提出了一种基于伪标签的学习策略，利用现有训练数据生成的伪标签，为广泛范围的近距离视角提供有针对性的监督。由于缺少针对此挑战的基准测试集，我们还推出了一个新的数据集，旨在评估当前和未来方法在这个领域的有效性。实验证明我们的方法有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF和3DGS等方法在新型视角合成方面表现出卓越的能力。</li>
<li>在生成与训练集视角差异大的详细图像，尤其是近距离视角图像时，现有方法存在困难。</li>
<li>缺乏针对近距离视角的特定训练数据是主要的挑战。</li>
<li>提出了一种基于伪标签的学习策略，为广泛范围的近距离视角提供有针对性的监督。</li>
<li>为评估方法在这个领域的有效性，推出了一个新的数据集。</li>
<li>引入的伪标签学习策略提高了在近距离视角图像生成的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15908">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cace697124d53f935d910d9e4a8064df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35bfb8cd72d4fb1dd76b239d517e1704.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8442cae68b84efc64254cdf2e747c143.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b8c2d326a2be14d22344c1b82c15e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffdc84062a38f7079f11ae44df246d06.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VideoRFSplat-Direct-Scene-Level-Text-to-3D-Gaussian-Splatting-Generation-with-Flexible-Pose-and-Multi-View-Joint-Modeling"><a href="#VideoRFSplat-Direct-Scene-Level-Text-to-3D-Gaussian-Splatting-Generation-with-Flexible-Pose-and-Multi-View-Joint-Modeling" class="headerlink" title="VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting   Generation with Flexible Pose and Multi-View Joint Modeling"></a>VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting   Generation with Flexible Pose and Multi-View Joint Modeling</h2><p><strong>Authors:Hyojun Go, Byeongjun Park, Hyelin Nam, Byung-Hoon Kim, Hyungjin Chung, Changick Kim</strong></p>
<p>We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D generative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabilize training and inference. In this work, we propose an architecture and a sampling strategy to jointly model multi-view images and camera poses when fine-tuning a video generation model. Our core idea is a dual-stream architecture that attaches a dedicated pose generation model alongside a pre-trained video generation model via communication blocks, generating multi-view images and camera poses through separate streams. This design reduces interference between the pose and image modalities. Additionally, we propose an asynchronous sampling strategy that denoises camera poses faster than multi-view images, allowing rapidly denoised poses to condition multi-view generation, reducing mutual ambiguity and enhancing cross-modal consistency. Trained on multiple large-scale real-world datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms existing text-to-3D direct generation methods that heavily depend on post-hoc refinement via score distillation sampling, achieving superior results without such refinement. </p>
<blockquote>
<p>我们提出VideoRFSplat，这是一种直接的文本到3D模型，利用视频生成模型生成现实世界的无限场景的真实3D高斯喷溅（3DGS）。为了生成多样化的相机姿态和现实世界场景的无限空间范围，同时确保对任意文本提示的泛化能力，之前的方法是通过微调2D生成模型来联合建模相机姿态和多视角图像。然而，这些方法在将2D生成模型扩展到联合建模时会出现不稳定，这是由于模态差距造成的，需要额外的模型来稳定训练和推理。在这项工作中，我们提出了一种架构和采样策略，在微调视频生成模型时联合建模多视角图像和相机姿态。我们的核心思想是一种双流架构，它通过通信块将一个专用的姿态生成模型与预训练的视频生成模型附加在一起，通过单独的流生成多视角图像和相机姿态。这种设计减少了姿态和图像模态之间的干扰。此外，我们提出了一种异步采样策略，该策略能更快地消除相机姿态的噪声，允许快速去噪的姿态调节多视角生成，减少相互模糊并增强跨模态一致性。在多个大规模真实世界数据集（RealEstate10K、MVImgNet、DL3DV-10K、ACID）上进行训练，VideoRFSplat超越了现有的文本到3D直接生成方法，这些方法严重依赖于通过分数蒸馏采样进行事后细化，并在没有此类细化的情况下取得优越结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15855v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gohyojun15.github.io/VideoRFSplat/">https://gohyojun15.github.io/VideoRFSplat/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出VideoRFSplat模型，这是一种直接由文本到3D的模型，利用视频生成模型生成逼真的3D高斯溅出（3DGS）以表现无界限的真实世界场景。为生成多样化的相机姿态和真实世界场景的无界限空间范围，同时确保对任意文本提示的泛化能力，前人方法会对2D生成模型进行微调以联合建模相机姿态和多视角图像。然而，这些方法在扩展到联合建模时会出现不稳定现象，这是由于模态差距造成的。为此，本文提出了一种架构和采样策略，在微调视频生成模型时联合建模多视角图像和相机姿态。核心思想是采用双流架构，通过通信块将一个专用的姿态生成模型与预训练的视频生成模型连接起来，通过不同流生成多视角图像和相机姿态，减少姿态和图像模态之间的干扰。此外，还提出了一种异步采样策略，该策略能更快地对相机姿态进行去噪处理，允许快速去噪的相机姿态作为多视角生成的约束条件，减少了互模糊并增强了跨模态一致性。在多个大规模真实世界数据集上进行训练后，VideoRFSplat超越了现有的文本到3D直接生成方法，特别是在依赖于事后修订得分蒸馏采样的方法上表现出色，即使没有这种修订也能实现优越的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoRFSplat是一种利用视频生成模型实现从文本到逼真的三维高斯溅出模型的直接转换。</li>
<li>传统的视频生成模型在处理文本提示时会遇到相机姿态建模和多视角图像生成的问题。为解决这些问题，需要利用双流架构结合专用的姿态生成模型和预训练的视频生成模型。</li>
<li>通过引入通信块实现双流的联合操作，分别处理多视角图像和相机姿态的生成，减少模态间的干扰。</li>
<li>提出异步采样策略以加速相机姿态的去噪过程，提升模型的稳定性和效率。通过迅速更新的去噪姿态条件化多视角图像的生成过程，降低了模态之间的歧义性并增强了跨模态一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15855">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f5e182d8f9be850a13c7849e8e82d028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b842347f67ff709161d2299363513bd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0b6193625b5224d0f03cc1064ae9edb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b83c117d7b5b19784fdffb711e6ee56f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="BARD-GS-Blur-Aware-Reconstruction-of-Dynamic-Scenes-via-Gaussian-Splatting"><a href="#BARD-GS-Blur-Aware-Reconstruction-of-Dynamic-Scenes-via-Gaussian-Splatting" class="headerlink" title="BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian   Splatting"></a>BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian   Splatting</h2><p><strong>Authors:Yiren Lu, Yunlai Zhou, Disheng Liu, Tuo Liang, Yu Yin</strong></p>
<p>3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene reconstruction, and recent advancements have extended its application to dynamic scenes. However, the quality of reconstructions depends heavily on high-quality input images and precise camera poses, which are not that trivial to fulfill in real-world scenarios. Capturing dynamic scenes with handheld monocular cameras, for instance, typically involves simultaneous movement of both the camera and objects within a single exposure. This combined motion frequently results in image blur that existing methods cannot adequately handle. To address these challenges, we introduce BARD-GS, a novel approach for robust dynamic scene reconstruction that effectively handles blurry inputs and imprecise camera poses. Our method comprises two main components: 1) camera motion deblurring and 2) object motion deblurring. By explicitly decomposing motion blur into camera motion blur and object motion blur and modeling them separately, we achieve significantly improved rendering results in dynamic regions. In addition, we collect a real-world motion blur dataset of dynamic scenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS effectively reconstructs high-quality dynamic scenes under realistic conditions, significantly outperforming existing methods. </p>
<blockquote>
<p>3D高斯点积法（3DGS）在静态场景重建方面表现出了显著潜力，并且最近的进展已经将其应用扩展到了动态场景。然而，重建的质量很大程度上取决于高质量输入图像和精确的相机姿态，这在现实场景中往往难以满足。例如，使用手持单眼相机捕捉动态场景通常涉及相机和场景内物体的同时移动。这种组合运动往往会导致图像模糊，现有方法无法充分处理。为了解决这些挑战，我们引入了BARD-GS，这是一种稳健的动态场景重建新方法，能够有效处理模糊输入和相机姿态不准确的问题。我们的方法主要包括两个组成部分：1）相机运动去模糊和2）物体运动去模糊。我们通过明确地将运动模糊分解成相机运动模糊和物体运动模糊，并分别对其进行建模，实现了动态区域的渲染结果显著改善。此外，我们还收集了一个动态场景的真实运动模糊数据集来评估我们的方法。大量实验表明，BARD-GS在真实条件下有效地重建了高质量动态场景，显著优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15835v1">PDF</a> CVPR2025. Project page at <a target="_blank" rel="noopener" href="https://vulab-ai.github.io/BARD-GS/">https://vulab-ai.github.io/BARD-GS/</a></p>
<p><strong>Summary</strong></p>
<p>基于动态场景重建的需求和挑战，提出了名为BARD-GS的新型方法，它可有效处理模糊输入和不准确的相机姿态。通过分离并分别建模相机运动模糊和物体运动模糊，在动态区域的渲染结果得到了显著提升。同时，收集真实世界的运动模糊数据集以评估方法性能，实验证明BARD-GS在真实条件下对动态场景的重建质量显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS在静态和动态场景的重建中显示出巨大潜力。</li>
<li>重建质量高度依赖于高质量输入图像和精确的相机姿态。</li>
<li>手持单目相机拍摄动态场景时，相机和物体的同时运动常导致图像模糊，现有方法难以处理。</li>
<li>BARD-GS方法通过明确分解和分别建模相机运动模糊和物体运动模糊，实现了显著改善。</li>
<li>收集真实世界的运动模糊数据集，用于评估BARD-GS性能。</li>
<li>广泛实验证明，BARD-GS在真实条件下对动态场景的重建效果优于现有方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15835">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e0f5988a4c3777e57aff2f6b8b1d129e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fd34435b13e555f51617feba2acabf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eac2bb345cbc7cac81c1dc40fbf466cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3991b4acaf9406afca67b62a21d25e31.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CHROME-Clothed-Human-Reconstruction-with-Occlusion-Resilience-and-Multiview-Consistency-from-a-Single-Image"><a href="#CHROME-Clothed-Human-Reconstruction-with-Occlusion-Resilience-and-Multiview-Consistency-from-a-Single-Image" class="headerlink" title="CHROME: Clothed Human Reconstruction with Occlusion-Resilience and   Multiview-Consistency from a Single Image"></a>CHROME: Clothed Human Reconstruction with Occlusion-Resilience and   Multiview-Consistency from a Single Image</h2><p><strong>Authors:Arindam Dutta, Meng Zheng, Zhongpai Gao, Benjamin Planche, Anwesha Choudhuri, Terrence Chen, Amit K. Roy-Chowdhury, Ziyan Wu</strong></p>
<p>Reconstructing clothed humans from a single image is a fundamental task in computer vision with wide-ranging applications. Although existing monocular clothed human reconstruction solutions have shown promising results, they often rely on the assumption that the human subject is in an occlusion-free environment. Thus, when encountering in-the-wild occluded images, these algorithms produce multiview inconsistent and fragmented reconstructions. Additionally, most algorithms for monocular 3D human reconstruction leverage geometric priors such as SMPL annotations for training and inference, which are extremely challenging to acquire in real-world applications. To address these limitations, we propose CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. Specifically, CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. CHROME achieves significant improvements in terms of both novel view synthesis (upto 3 db PSNR) and geometric reconstruction under challenging conditions. </p>
<blockquote>
<p>从单一图像重建穿衣人类是计算机视觉中的一项基本任务，具有广泛的应用范围。尽管现有的单目穿衣人类重建解决方案已经显示出有希望的结果，但它们通常假设人类主体处于一个无遮挡的环境中。因此，当遇到野外遮挡图像时，这些算法会产生多视角不一致和碎片化的重建结果。此外，大多数单目3D人类重建算法利用几何先验（如SMPL注释）进行训练和推理，这在现实世界应用中极难获取。为了解决这些局限性，我们提出了CHROME：从单张图像进行具有遮挡恢复能力和多视角一致性的穿衣人类重建。这是一种新型管道设计，旨在从单个遮挡图像中以多视角一致性重建具有遮挡恢复能力的3D人类，而无需真实几何先验注释或3D监督。具体来说，CHROME首先利用多视角扩散模型从遮挡输入中合成无遮挡的人体图像，与现成的姿势控制相结合，在合成过程中明确执行跨视角一致性。然后，训练一个3D重建模型，根据遮挡输入和合成视图预测一组3D高斯分布，对齐跨视角细节以产生连贯和准确的3D表示。CHROME在新型视角合成（高达3分贝PSNR）和具有挑战性的条件下的几何重建方面都取得了显著的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15671v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的重建技术——CHROME，该技术能够在单张遮挡图像中实现具有遮挡恢复能力的三维人体重建，并且具有多视角一致性。该方法无需真实几何先验标注和三维监督信息，通过合成无遮挡人体图像和多视角一致性约束，提高了在复杂环境下的重建效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>重建技术主要挑战在于处理遮挡问题。现有的技术通常假设人体处于无遮挡环境中，导致在实际应用中表现不佳。</li>
<li>CHROME技术解决了这一难题，实现了在单张遮挡图像上的三维人体重建，并具有遮挡恢复能力。</li>
<li>CHROME技术采用多视角扩散模型，从遮挡输入中合成无遮挡人体图像，与现成的姿势控制兼容，明确执行跨视角一致性约束。</li>
<li>该技术通过训练一个三维重建模型，预测一组基于遮挡输入和合成视角的三维高斯分布，通过跨视角细节对齐生成连贯且精确的三维表示。</li>
<li>CHROME技术在新型视角合成和几何重建方面取得了显著改进，特别是在复杂环境下。</li>
<li>该方法无需真实的几何先验标注和三维监督信息，降低了实际应用中的难度和成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15671">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8f98aaec679a0dac16e779f30f2b476a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a70eb82968e8b791c6e54c4beb7d9b5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54a45333e4f8c9539af19302e62f5a5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be7fc9c550078f9331b0faa3c964f696.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60e3b6a0bdf2c1e05c87315b44e13817.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion"><a href="#Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion" class="headerlink" title="Synthetic Prior for Few-Shot Drivable Head Avatar Inversion"></a>Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</h2><p><strong>Authors:Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart</strong></p>
<p>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle three major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, the use of real data is strictly regulated (e.g., under the General Data Protection Regulation, which mandates frequent deletion of models and data to accommodate a situation when a participant’s consent is withdrawn). Synthetic data, free from these constraints, is an appealing alternative. Third, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to SOTA monocular and GAN-based methods, SynShot significantly improves novel view and expression synthesis. </p>
<blockquote>
<p>我们提出了一种名为SynShot的新方法，用于基于合成先验的少数镜头驾驶头部阿凡达的反转。我们解决了三大挑战。首先，训练可控的3D生成网络需要大量的不同序列，而图像和高品质跟踪网格的配对并不总是可用。其次，真实数据的使用受到严格监管（例如，在《通用数据保护条例》下，当参与者同意撤回时，经常需要删除模型和数据）。不受这些约束的合成数据是一个吸引人的替代方案。第三，最先进的单眼阿凡达模型在推广到新的视角和表情时遇到困难，缺乏强大的先验知识，并且经常过度适应特定的观点分布。受只接受合成数据训练的机器学习模型的启发，我们提出了一种方法，该方法从包含不同身份、表情和观点的大量合成头部数据中学习先验模型。凭借少数输入图像，SynShot微调了预训练的合成先验，以弥合领域间的差距，并建模一个逼真的头部阿凡达，可以推广到新的表情和视角。我们使用3D高斯喷绘和卷积编码器-解码器来输出UV纹理空间的高斯参数，对头部各部分的不同建模复杂性进行建模（例如，皮肤与头发）。为了考虑头部各部分（例如皮肤和头发）的不同建模复杂性，我们在先验中嵌入了对增加每个部分原始数量的上采样控制。与最先进的单眼和基于GAN的方法相比，SynShot显著提高了新视角和表情的合成效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06903v2">PDF</a> Accepted to CVPR25 Website: <a target="_blank" rel="noopener" href="https://zielon.github.io/synshot/">https://zielon.github.io/synshot/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了SynShot方法，这是一种基于合成先验的少数人头驾驶头像反转技术的新方法。该方法解决了三大挑战：缺乏多样序列图像和高品质追踪网格的训练数据、真实数据使用受到严格监管以及当前单眼头像模型在新视角和表情上的泛化能力不强。SynShot通过学习从大量合成头像数据中获取的先验模型，结合少量输入图像，微调预训练合成先验以弥域差距，从而建模出真实感头像并能泛化到新的表情和视角。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SynShot是一种基于合成先验的少数人头驾驶头像反转技术。</li>
<li>它解决了训练可控的3D生成网络面临的三大挑战：缺乏多样序列图像和高品质追踪网格的训练数据、真实数据使用的监管问题以及现有模型在新视角和表情上的泛化难题。</li>
<li>SynShot通过结合大量合成头像数据的先验模型与少量输入图像，微调预训练合成先验，以弥域差距，达到建模真实感头像的目标。</li>
<li>该方法采用3D高斯喷涂技术和卷积编码器-解码器，在UV纹理空间中输出高斯参数。</li>
<li>为了应对头部不同部分建模复杂性的差异（如皮肤和头发），SynShot嵌入先验，具有针对每个部分原始数量的上采样控制功能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bd0cc9a3a4a5ca64e316c0f6919c49d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc4fd6a640e4f770326012aab1b0575e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34b5f656b91512006dff6313ea06256e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91869d7ecc0a53622008866a9ae4c5fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8020a341dbd40737d91ef6aa6c3efdeb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfb08e011b3ab317551b561dbc247e94.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NFL-BA-Improving-Endoscopic-SLAM-with-Near-Field-Light-Bundle-Adjustment"><a href="#NFL-BA-Improving-Endoscopic-SLAM-with-Near-Field-Light-Bundle-Adjustment" class="headerlink" title="NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle   Adjustment"></a>NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle   Adjustment</h2><p><strong>Authors:Andrea Dunn Beltran, Daniel Rho, Stephen Pizer, Marc Niethammer, Roni Sengupta</strong></p>
<p>Simultaneous Localization And Mapping (SLAM) from endoscopy videos can enable autonomous navigation, guidance to unsurveyed regions, blindspot detections, and 3D visualizations, which can significantly improve patient outcomes and endoscopy experience for both physicians and patients. Existing dense SLAM algorithms often assume distant and static lighting and optimize scene geometry and camera parameters by minimizing a photometric rendering loss, often called Photometric Bundle Adjustment. However, endoscopy videos exhibit dynamic near-field lighting due to the co-located light and camera moving extremely close to the surface. In addition, low texture surfaces in endoscopy videos cause photometric bundle adjustment of the existing SLAM frameworks to perform poorly compared to indoor&#x2F;outdoor scenes. To mitigate this problem, we introduce Near-Field Lighting Bundle Adjustment Loss (NFL-BA) which explicitly models near-field lighting as a part of Bundle Adjustment loss and enables better performance for low texture surfaces. Our proposed NFL-BA can be applied to any neural-rendering based SLAM framework. We show that by replacing traditional photometric bundle adjustment loss with our proposed NFL-BA results in improvement, using neural implicit SLAM and 3DGS SLAMs. In addition to producing state-of-the-art tracking and mapping results on colonoscopy C3VD dataset we also show improvement on real colonoscopy videos. See results at <a target="_blank" rel="noopener" href="https://asdunnbe.github.io/NFL-BA/">https://asdunnbe.github.io/NFL-BA/</a> </p>
<blockquote>
<p>通过内窥镜视频实现的同步定位与地图构建（SLAM）可以支持自主导航、对未测绘区域的指导、盲点检测和3D可视化，这可以显著改善患者治疗效果和医生与患者的内窥镜检查体验。现有的密集SLAM算法通常假设光源距离遥远且静态，通过最小化被称为光度捆绑调整的光度渲染损失来优化场景几何和相机参数。然而，内窥镜视频呈现出动态近场照明，这是由于光源和相机与表面非常接近而移动造成的。此外，内窥镜视频中的低纹理表面导致现有SLAM框架的光度捆绑调整性能与室内&#x2F;室外场景相比表现较差。为了缓解这个问题，我们引入了近场照明捆绑调整损失（NFL-BA），它显式地将近场照明作为捆绑调整损失的一部分，从而实现对低纹理表面进行更好的性能表现。我们提出的NFL-BA可以应用于任何基于神经渲染的SLAM框架。我们展示，通过用我们提出的NFL-BA替换传统的光度捆绑调整损失，使用神经隐式SLAM和3DGS SLAMs会有改进效果。除了在结肠镜检查C3VD数据集上实现最先进的跟踪和映射结果外，我们还展示了在真实结肠镜检查视频上的改进。具体成果可参见网址：<a target="_blank" rel="noopener" href="https://asdunnbe.github.io/NFL-BA/%E3%80%82">https://asdunnbe.github.io/NFL-BA/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13176v2">PDF</a> </p>
<p><strong>Summary</strong><br>     内镜视频中的同步定位与地图构建（SLAM）可实现自主导航、导向未勘测区域、盲点检测及三维可视化，可显著改善患者及医师的内镜体验并提升患者治疗效果。针对内镜视频中的动态近场照明和低纹理表面问题，引入近场照明捆绑调整损失（NFL-BA），对任何基于神经渲染的SLAM框架都能带来更好的性能。采用NFL-BA替换传统光度捆绑调整损失，能显著提升神经隐式SLAM和3DGS SLAM的表现，在结肠镜检查C3VD数据集和真实结肠镜视频上都展现了卓越的追踪和映射效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLAM技术从内镜视频中可助力自主导航、导向未勘测区域等，显著改进医疗与体验。</li>
<li>内镜视频存在动态近场照明与低纹理表面问题，现有SLAM算法性能受限。</li>
<li>引入NFL-BA能有效解决动态近场照明问题，提升在低纹理表面的性能表现。</li>
<li>NFL-BA适用于所有基于神经渲染的SLAM框架。</li>
<li>用NFL-BA替换传统光度捆绑调整损失能显著提高SLAM技术表现。</li>
<li>在结肠镜检查数据集及真实场景中，应用NFL-BA的SLAM技术具有优秀追踪与映射效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13176">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8c45a38c15e3ec31c9ed65a183ff4a8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-613dc2343dfed646ffd1d95d61a5d4c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33e2fdabe60fa882517dd7f9c979dea2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e2224104e6762dc2ca8b7393cdc3abc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multi-View-Pose-Agnostic-Change-Localization-with-Zero-Labels"><a href="#Multi-View-Pose-Agnostic-Change-Localization-with-Zero-Labels" class="headerlink" title="Multi-View Pose-Agnostic Change Localization with Zero Labels"></a>Multi-View Pose-Agnostic Change Localization with Zero Labels</h2><p><strong>Authors:Chamuditha Jayanga Galappaththige, Jason Lai, Lloyd Windrim, Donald Dansereau, Niko Suenderhauf, Dimity Miller</strong></p>
<p>Autonomous agents often require accurate methods for detecting and localizing changes in their environment, particularly when observations are captured from unconstrained and inconsistent viewpoints. We propose a novel label-free, pose-agnostic change detection method that integrates information from multiple viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS) representation of the scene. With as few as 5 images of the post-change scene, our approach can learn an additional change channel in a 3DGS and produce change masks that outperform single-view techniques. Our change-aware 3D scene representation additionally enables the generation of accurate change masks for unseen viewpoints. Experimental results demonstrate state-of-the-art performance in complex multi-object scenes, achieving a 1.7x and 1.5x improvement in Mean Intersection Over Union and F1 score respectively over other baselines. We also contribute a new real-world dataset to benchmark change detection in diverse challenging scenes in the presence of lighting variations. </p>
<blockquote>
<p>自主代理通常需要准确的方法来检测并定位其环境中的变化，特别是在从不受限制和不一致的视角捕获观察结果时。我们提出了一种新型的无标签、不受姿态影响的变化检测方法，该方法整合了来自多个视角的信息，以构建一种对变化有感知的3D高斯拼贴（3DGS）场景表示。仅使用变化后场景的5张图像，我们的方法可以在3DGS中学习额外的变化通道，并产生优于单视图技术的变化蒙版。我们的对变化有感知的3D场景表示还允许为未见过的视角生成准确的变化蒙版。实验结果表明，在复杂的多对象场景中，我们的方法在平均交并比和F1分数方面分别实现了比其他基准方法1.7倍和1.5倍的改进。我们还为在光照变化存在的情况下，在多样且具有挑战性的场景中检测变化提供了一个新的现实世界数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03911v2">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong><br>自主智能体在环境变化的检测和定位方面需要准确的方法，特别是在观察点不受限制且不一致的情况下。本文提出了一种新颖的免标签、无姿态变化的检测法，该方法通过整合多视角的信息来构建对环境变化有感知的3D高斯拼贴（3DGS）表示。仅需少量变化后的场景图像，该方法能够在3DGS中学习额外的变化通道，并产生优于单视角技术的变化蒙版。此外，我们的变化感知3D场景表示能够为未见过的视角生成准确的变化蒙版。实验结果表明，在复杂的多对象场景中，该方法达到了最先进的性能，相较于其他基线方法，Mean Intersection Over Union和F1分数分别提高了1.7倍和1.5倍。我们还贡献了一个新的真实世界数据集，以在光照变化的情况下对多变场景中的变化检测进行基准测试。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种免标签、无姿态变化的检测方法用于环境变化检测。</li>
<li>通过整合多视角信息构建变化感知的3D高斯拼贴（3DGS）表示。</li>
<li>仅需少量变化后的场景图像，能在3DGS中学习额外的变化通道。</li>
<li>产生的变化蒙版性能优于单视角技术。</li>
<li>变化感知的3D场景表示能生成未见视角的准确变化蒙版。</li>
<li>在复杂多对象场景中达到最先进的性能，较其他方法有显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8d8e07fa3cd0a478475edd96f5cd7645.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa547a9350da8a0ffeaa1386f776b58a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eea2a0130289ec8fafd70b8b401a29f8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Gaussian-Eigen-Models-for-Human-Heads"><a href="#Gaussian-Eigen-Models-for-Human-Heads" class="headerlink" title="Gaussian Eigen Models for Human Heads"></a>Gaussian Eigen Models for Human Heads</h2><p><strong>Authors:Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies</strong></p>
<p>Current personalized neural head avatars face a trade-off: lightweight models lack detail and realism, while high-quality, animatable avatars require significant computational resources, making them unsuitable for commodity devices. To address this gap, we introduce Gaussian Eigen Models (GEM), which provide high-quality, lightweight, and easily controllable head avatars. GEM utilizes 3D Gaussian primitives for representing the appearance combined with Gaussian splatting for rendering. Building on the success of mesh-based 3D morphable face models (3DMM), we define GEM as an ensemble of linear eigenbases for representing the head appearance of a specific subject. In particular, we construct linear bases to represent the position, scale, rotation, and opacity of the 3D Gaussians. This allows us to efficiently generate Gaussian primitives of a specific head shape by a linear combination of the basis vectors, only requiring a low-dimensional parameter vector that contains the respective coefficients. We propose to construct these linear bases (GEM) by distilling high-quality compute-intense CNN-based Gaussian avatar models that can generate expression-dependent appearance changes like wrinkles. These high-quality models are trained on multi-view videos of a subject and are distilled using a series of principal component analyses. Once we have obtained the bases that represent the animatable appearance space of a specific human, we learn a regressor that takes a single RGB image as input and predicts the low-dimensional parameter vector that corresponds to the shown facial expression. In a series of experiments, we compare GEM’s self-reenactment and cross-person reenactment results to state-of-the-art 3D avatar methods, demonstrating GEM’s higher visual quality and better generalization to new expressions. </p>
<blockquote>
<p>当前个性化神经头部化身面临一个权衡：轻量级模型缺乏细节和逼真度，而高质量、可动画的化身需要巨大的计算资源，使其不适合普通设备。为了解决这一差距，我们引入了高斯特征模型（GEM），它提供高质量、轻便且易于控制的头部化身。GEM使用3D高斯原始图形来表示外观，并结合高斯喷绘进行渲染。基于基于网格的3D可变形面部模型（3DMM）的成功，我们将GEM定义为表示特定主体头部外观的线性特征基集合。特别是，我们构建了表示位置、尺度、旋转和透明度的线性基。这允许我们通过线性组合基向量有效地生成特定头部形状的高斯原始图形，仅需要一个低维参数向量，其中包含相应的系数。我们提议通过蒸馏高质量的计算密集型CNN高斯化身模型来构建这些线性基（GEM），该模型可以生成与表情相关的外观变化，如皱纹。这些高质量模型在主体的多视角视频上进行训练，并使用一系列主成分分析进行提炼。一旦我们获得了代表特定人类可动画外观空间的基地，我们就学习一个回归器，它接受单张RGB图像作为输入，并预测与所示面部表情相对应的低维参数向量。在一系列实验中，我们将GEM的自我重新演绎和跨人重新演绎的结果与最先进的3D化身方法进行比较，证明了GEM更高的视觉质量和对新表情的更好泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04545v3">PDF</a> Accepted to CVPR25 Website: <a target="_blank" rel="noopener" href="https://zielon.github.io/gem/">https://zielon.github.io/gem/</a></p>
<p><strong>Summary</strong><br>     引入高斯特征模型（GEM）解决个性化神经头部化身面临的困境，即轻量化模型缺乏细节和真实感，而高质量、可动画的化身需要巨大的计算资源，不适用于普通设备。GEM利用3D高斯原始表示外观，结合高斯展开进行渲染，提供高质量、轻便且易于控制的头部化身。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前个性化神经头部化身面临轻量化与高质量之间的权衡。</li>
<li>高斯特征模型（GEM）利用3D高斯原始和Gaussian splatting渲染技术来提供高质量、轻便的头部化身解决方案。</li>
<li>GEM是基于3D可变形面部模型（3DMM）的线性特征基来表示特定主体的头部外观。</li>
<li>GEM通过构建线性基来表示3D高斯的位置、尺度、旋转和透明度。</li>
<li>高质量计算密集型的CNN高斯化身模型被用来生成表情相关的外观变化，如皱纹。</li>
<li>通过主成分分析系列进行蒸馏，得到代表特定人类可动画外观空间的线性基。</li>
<li>实验表明，与现有3D化身方法相比，GEM在自我表演和跨人表演方面表现出更高的视觉质量和更好的新表情泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04545">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eeffbb39627defba37c3c39b8c642a18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5af958ac51e806a6a04279e5c3a8e4f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c8a050527e7fa73a744ccffe8d9c088.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7c92d96a50e4945e497e59fb29abe24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8546ec1800df6e0cf1fdd834df1aecca.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MG-SLAM-Structure-Gaussian-Splatting-SLAM-with-Manhattan-World-Hypothesis"><a href="#MG-SLAM-Structure-Gaussian-Splatting-SLAM-with-Manhattan-World-Hypothesis" class="headerlink" title="MG-SLAM: Structure Gaussian Splatting SLAM with Manhattan World   Hypothesis"></a>MG-SLAM: Structure Gaussian Splatting SLAM with Manhattan World   Hypothesis</h2><p><strong>Authors:Shuhong Liu, Tianchen Deng, Heng Zhou, Liuzhuozheng Li, Hongyu Wang, Danwei Wang, Mingrui Li</strong></p>
<p>Gaussian Splatting SLAMs have made significant advancements in improving the efficiency and fidelity of real-time reconstructions. However, these systems often encounter incomplete reconstructions in complex indoor environments, characterized by substantial holes due to unobserved geometry caused by obstacles or limited view angles. To address this challenge, we present Manhattan Gaussian SLAM, an RGB-D system that leverages the Manhattan World hypothesis to enhance geometric accuracy and completeness. By seamlessly integrating fused line segments derived from structured scenes, our method ensures robust tracking in textureless indoor areas. Moreover, The extracted lines and planar surface assumption allow strategic interpolation of new Gaussians in regions of missing geometry, enabling efficient scene completion. Extensive experiments conducted on both synthetic and real-world scenes demonstrate that these advancements enable our method to achieve state-of-the-art performance, marking a substantial improvement in the capabilities of Gaussian SLAM systems. </p>
<blockquote>
<p>高斯模糊点分割技术（SLAMs）在提升实时重建的效率和保真度方面取得了显著进展。然而，这些系统在复杂的室内环境中经常遇到重建不完整的问题，主要表现为由于障碍物或有限视角导致的未观察到的几何结构而产生的大量空洞。为了应对这一挑战，我们提出了曼哈顿高斯SLAM系统，这是一个利用曼哈顿世界假设来提高几何准确性和完整性的RGB-D系统。我们的方法通过无缝集成从结构化场景中派生出的融合线段来确保在纹理缺失的室内区域进行稳健的跟踪。此外，提取的线条和平面表面假设允许在有缺失几何结构的区域中进行新的高斯值的策略性插值，从而实现高效的场景补全。在合成场景和真实场景上进行的广泛实验表明，这些进展使得我们的方法达到了最先进的性能水平，标志着高斯SLAM系统的能力得到了实质性提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20031v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>高斯混合SLAM在提升实时重建的效率和保真度方面取得了显著进展。然而，在复杂的室内环境中，这些系统经常遇到不完整重建的问题，表现为由于障碍物或有限视角导致的未观测到的几何结构产生的大量空洞。为解决这一挑战，我们提出曼哈顿高斯SLAM系统，这是一个RGB-D系统，利用曼哈顿世界假设提高几何精度和完整性。通过无缝集成来自结构化场景的融合线段，我们的方法确保在纹理较少的室内区域实现稳健跟踪。此外，提取的线条和平面表面假设允许在缺失几何区域进行新的高斯战略插值，从而实现高效场景完成。在合成和真实场景上的广泛实验表明，这些进步使我们的方法达到最先进的性能，标志着高斯SLAM系统的能力得到了显著改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高斯混合SLAM已显著提高实时重建的效率和保真度。</li>
<li>在复杂室内环境中，高斯混合SLAM面临不完整重建问题，表现为大量空洞。</li>
<li>曼哈顿高斯SLAM是一个RGB-D系统，利用曼哈顿世界假设增强几何精度和完整性。</li>
<li>该方法通过无缝集成融合线段，确保在纹理较少的室内区域实现稳健跟踪。</li>
<li>提取的线条和平面表面假设允许在缺失几何区域进行新的高斯插值，实现高效场景完成。</li>
<li>曼哈顿高斯SLAM在合成和真实场景上的实验表现达到或超越了现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.20031">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4e09af0665ea0e0c4028fe1d819b48e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05a2419e540e126a498294b438401b08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0918055d43e3d5fbf954198ae6a125ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6f0cff953ddc00e48437796cd7743ca.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-22/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-8442cae68b84efc64254cdf2e747c143.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-03-22  GAN-enhanced Simulation-driven DNN Testing in Absence of Ground Truth
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e4ed3c6d3226799ea29effeea1e08026.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-03-22  Zero-1-to-A Zero-Shot One Image to Animatable Head Avatars Using Video   Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">15444.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
