<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  GenEAva Generating Cartoon Avatars with Fine-Grained Facial Expressions   from Realistic Diffusion-based Faces">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1435311e9978fd317f4fc524d343a769.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    64 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-12-æ›´æ–°"><a href="#2025-04-12-æ›´æ–°" class="headerlink" title="2025-04-12 æ›´æ–°"></a>2025-04-12 æ›´æ–°</h1><h2 id="GenEAva-Generating-Cartoon-Avatars-with-Fine-Grained-Facial-Expressions-from-Realistic-Diffusion-based-Faces"><a href="#GenEAva-Generating-Cartoon-Avatars-with-Fine-Grained-Facial-Expressions-from-Realistic-Diffusion-based-Faces" class="headerlink" title="GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions   from Realistic Diffusion-based Faces"></a>GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions   from Realistic Diffusion-based Faces</h2><p><strong>Authors:Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal</strong></p>
<p>Cartoon avatars have been widely used in various applications, including social media, online tutoring, and gaming. However, existing cartoon avatar datasets and generation methods struggle to present highly expressive avatars with fine-grained facial expressions and are often inspired from real-world identities, raising privacy concerns. To address these challenges, we propose a novel framework, GenEAva, for generating high-quality cartoon avatars with fine-grained facial expressions. Our approach fine-tunes a state-of-the-art text-to-image diffusion model to synthesize highly detailed and expressive facial expressions. We then incorporate a stylization model that transforms these realistic faces into cartoon avatars while preserving both identity and expression. Leveraging this framework, we introduce the first expressive cartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135 fine-grained facial expressions, featuring 13,230 expressive cartoon avatars with a balanced distribution across genders, racial groups, and age ranges. We demonstrate that our fine-tuned model generates more expressive faces than the state-of-the-art text-to-image diffusion model SDXL. We also verify that the cartoon avatars generated by our framework do not include memorized identities from fine-tuning data. The proposed framework and dataset provide a diverse and expressive benchmark for future research in cartoon avatar generation. </p>
<blockquote>
<p>å¡é€šå¤´åƒå·²å¹¿æ³›åº”ç”¨äºå„ç§åº”ç”¨ç¨‹åºï¼ŒåŒ…æ‹¬ç¤¾äº¤åª’ä½“ã€åœ¨çº¿æ•™å­¦å’Œæ¸¸æˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¡é€šå¤´åƒæ•°æ®é›†å’Œç”Ÿæˆæ–¹æ³•éš¾ä»¥å‘ˆç°å…·æœ‰é«˜åº¦è¡¨æƒ…å’Œç²¾ç»†é¢éƒ¨è¡¨æƒ…çš„å¤´åƒï¼Œå¹¶ä¸”é€šå¸¸å—åˆ°çœŸå®èº«ä»½çš„å¯å‘ï¼Œè¿™å¼•å‘äº†éšç§æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶GenEAvaï¼Œç”¨äºç”Ÿæˆå…·æœ‰ç²¾ç»†é¢éƒ¨è¡¨æƒ…çš„é«˜è´¨é‡å¡é€šå¤´åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¾®è°ƒæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥åˆæˆé«˜åº¦è¯¦ç»†å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„é¢éƒ¨è¡¨æƒ…ã€‚ç„¶åï¼Œæˆ‘ä»¬èå…¥äº†ä¸€ç§é£æ ¼åŒ–æ¨¡å‹ï¼Œå°†è¿™äº›é€¼çœŸçš„é¢å­”è½¬åŒ–ä¸ºå¡é€šå¤´åƒï¼ŒåŒæ—¶ä¿ç•™èº«ä»½å’Œè¡¨æƒ…ã€‚å€ŸåŠ©è¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é¦–ä¸ªå¯Œæœ‰è¡¨ç°åŠ›çš„å¡é€šå¤´åƒæ•°æ®é›†GenEAva 1.0ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæ•æ‰135ç§ç²¾ç»†é¢éƒ¨è¡¨æƒ…ï¼ŒåŒ…å«13,230ä¸ªå¯Œæœ‰è¡¨ç°åŠ›çš„å¡é€šå¤´åƒï¼Œåœ¨æ€§åˆ«ã€ç§æ—å’Œå¹´é¾„èŒƒå›´å†…å…·æœ‰å¹³è¡¡åˆ†å¸ƒã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹æ¯”æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹SDXLç”Ÿæˆçš„è¡¨æƒ…æ›´åŠ ä¸°å¯Œã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†ç”±æˆ‘ä»¬æ¡†æ¶ç”Ÿæˆçš„å¡é€šå¤´åƒä¸åŒ…æ‹¬å¾®è°ƒæ•°æ®ä¸­çš„è®°å¿†èº«ä»½ã€‚æ‰€æå‡ºçš„æ¡†æ¶å’Œæ•°æ®é›†ä¸ºå¡é€šå¤´åƒç”Ÿæˆçš„æœªæ¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¤šæ ·åŒ–å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07945v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGenEAvaçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ä¸”è¡¨æƒ…ç»†è…»çš„å¡é€šå¤´åƒã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒå…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œåˆæˆé«˜åº¦è¯¦ç»†çš„è¡¨æƒ…ã€‚æ¥ç€ï¼Œé‡‡ç”¨é£æ ¼åŒ–æ¨¡å‹å°†è¿™äº›çœŸå®é¢å­”è½¬åŒ–ä¸ºå¡é€šå¤´åƒï¼ŒåŒæ—¶ä¿ç•™èº«ä»½å’Œè¡¨æƒ…ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæ¨å‡ºäº†é¦–ä¸ªè¡¨æƒ…ä¸°å¯Œçš„å¡é€šå¤´åƒæ•°æ®é›†GenEAva 1.0ï¼Œä¸“é—¨æ•æ‰135ç§ç»†è…»è¡¨æƒ…ï¼ŒåŒ…å«13,230ä¸ªè¡¨æƒ…ä¸°å¯Œçš„å¡é€šå¤´åƒï¼Œå‡è¡¡åˆ†å¸ƒåœ¨æ€§åˆ«ã€ç§æ—å’Œå¹´é¾„èŒƒå›´ä¸­ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„å¡é€šå¤´åƒæ¯”SDXLæ¨¡å‹æ›´å¯Œæœ‰è¡¨ç°åŠ›ï¼Œä¸”ä¸åŒ…å«å¾®è°ƒæ•°æ®ä¸­çš„è®°å¿†èº«ä»½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¡é€šå¤´åƒæ•°æ®é›†å’Œç”Ÿæˆæ–¹æ³•éš¾ä»¥å‘ˆç°é«˜åº¦ç»†è…»çš„è¡¨æƒ…ï¼Œå¹¶å—åˆ°éšç§é—®é¢˜çš„å…³æ³¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶GenEAvaï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ä¸”è¡¨æƒ…ç»†è…»çš„å¡é€šå¤´åƒã€‚</li>
<li>é€šè¿‡å¾®è°ƒå…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œåˆæˆé«˜åº¦è¯¦ç»†çš„è¡¨æƒ…ã€‚</li>
<li>å¼•å…¥é£æ ¼åŒ–æ¨¡å‹å°†çœŸå®é¢å­”è½¬åŒ–ä¸ºå¡é€šå¤´åƒï¼ŒåŒæ—¶ä¿ç•™èº«ä»½å’Œè¡¨æƒ…ã€‚</li>
<li>æ¨å‡ºé¦–ä¸ªè¡¨æƒ…ä¸°å¯Œçš„å¡é€šå¤´åƒæ•°æ®é›†GenEAva 1.0ï¼ŒåŒ…å«å¤šç§æ€§åˆ«ã€ç§æ—å’Œå¹´é¾„èŒƒå›´çš„ç»†è…»è¡¨æƒ…ã€‚</li>
<li>å¯¹æ¯”å®éªŒæ˜¾ç¤ºGenEAvaæ¡†æ¶ç”Ÿæˆçš„å¡é€šå¤´åƒæ¯”SDXLæ¨¡å‹æ›´å…·è¡¨ç°åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f49ebe198ed1f528822eb13ed63684c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82e8168578e7aa3c40e8db616b86dc7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41c6340054a94bfa6fa4796ca675cde1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Revisiting-Likelihood-Based-Out-of-Distribution-Detection-by-Modeling-Representations"><a href="#Revisiting-Likelihood-Based-Out-of-Distribution-Detection-by-Modeling-Representations" class="headerlink" title="Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling   Representations"></a>Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling   Representations</h2><p><strong>Authors:Yifan Ding, Arturas Aleksandrauskas, Amirhossein Ahmadian, Jonas Unger, Fredrik Lindsten, Gabriel Eilertsen</strong></p>
<p>Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\href{<a target="_blank" rel="noopener" href="https://github.com/limchaos/Likelihood-OOD.git%7D%7B/texttt%7Bhttps://github.com/limchaos/Likelihood-OOD.git%7D%7D$">https://github.com/limchaos/Likelihood-OOD.git}{\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$</a>. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆOODæ£€æµ‹ï¼‰å¯¹äºç¡®ä¿æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„å¯é æ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®çš„åº”ç”¨ç¨‹åºä¸­ã€‚åŸºäºå¯èƒ½æ€§çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å†å²ä¸Šé¢ä¸´ç€å¯¹å¼‚å¸¸æ£€æµ‹æ€§èƒ½ä¸æ»¡æ„çš„æ‰¹è¯„ï¼Œåœ¨åº”ç”¨äºå›¾åƒæ•°æ®æ—¶ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä¼šä¸ºå¼‚å¸¸æ•°æ®åˆ†é…è¾ƒé«˜çš„å¯èƒ½æ€§ï¼Œé«˜äºå†…éƒ¨åˆ†å¸ƒçš„æ ·æœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†å¯èƒ½æ€§æœ¬èº«å¹¶éå­˜åœ¨é—®é¢˜ã€‚ç›¸åï¼Œå›¾åƒç©ºé—´ä¸­çš„å‡ ä¸ªå±æ€§é˜»æ­¢äº†å¯èƒ½æ€§ä½œä¸ºæœ‰æ•ˆçš„æ£€æµ‹åˆ†æ•°ã€‚ç»™å®šä¸€ä¸ªè¶³å¤Ÿå¥½çš„å¯èƒ½æ€§ä¼°è®¡å™¨ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æµå…¬å¼ï¼Œæˆ‘ä»¬è¯æ˜äº†åŸºäºå¯èƒ½æ€§çš„æ–¹æ³•ä»ç„¶å¯ä»¥åº”ç”¨åœ¨é¢„è®­ç»ƒç¼–ç å™¨çš„è¡¨ç¤ºç©ºé—´ä¸­ï¼Œå¹¶ä¸”è¡¨ç°å¯ä¸æœ€å…ˆè¿›çš„æ£€æµ‹å™¨ä¸ç›¸ä¸Šä¸‹ã€‚æˆ‘ä»¬çš„å·¥ä½œä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/limchaos/Likelihood-OOD.git">https://github.com/limchaos/Likelihood-OOD.git</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07793v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æŒ‡å‡ºï¼Œåœ¨è¶³å¤Ÿå¥½çš„æ¦‚ç‡æµä¼°è®¡å™¨ï¼ˆåŸºäºæ‰©æ•£æ¨¡å‹ï¼‰çš„æ”¯æŒä¸‹ï¼Œå¯¹é¢„è®­ç»ƒç¼–ç å™¨çš„è¡¨ç¤ºç©ºé—´åº”ç”¨åŸºäºå¯èƒ½æ€§çš„æ–¹æ³•ï¼Œå¯ä»¥è¡¨ç°å‡ºä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ï¼Œæ‰“ç ´äº†ä»¥å¾€è®¤ä¸ºåŸºäºå¯èƒ½æ€§çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒæ•°æ®ä¸­è¿›è¡ŒOODæ£€æµ‹æ€§èƒ½ä¸ä½³çš„è§‚ç‚¹ã€‚è¯æ˜äº†å¯èƒ½æ€§çš„å›ºæœ‰ä¼˜åŠ¿å¹¶å¼ºè°ƒé€‚å½“ä½¿ç”¨æ–¹æ³•å’Œå·¥å…·çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡ºäºåˆ†å¸ƒæ£€æµ‹ï¼ˆOODï¼‰å¯¹ç¡®ä¿æ·±åº¦å­¦ä¹ ç³»ç»Ÿå¯é æ€§çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ä¼ ç»Ÿçš„åŸºäºå¯èƒ½æ€§çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒæ•°æ®ä¸­è¿›è¡ŒOODæ£€æµ‹çš„è¡¨ç°ä¸å°½å¦‚äººæ„ï¼Œå¸¸å¸¸è¢«æ‰¹è¯„å¯¹OODæ•°æ®çš„å¯èƒ½æ€§èµ‹äºˆè¿‡é«˜çš„è¯„ä»·ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºï¼Œé—®é¢˜å¹¶éåœ¨äºå¯èƒ½æ€§æœ¬èº«çš„ç¼ºé™·ï¼Œè€Œæ˜¯å›¾åƒç©ºé—´ä¸­çš„æŸäº›ç‰¹æ€§é˜»æ­¢äº†å¯èƒ½æ€§ä½œä¸ºæœ‰æ•ˆçš„æ£€æµ‹åˆ†æ•°ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æµå…¬å¼ä½œä¸ºä¼°è®¡å™¨æ—¶ï¼Œå¦‚æœè¶³å¤Ÿå¥½ï¼ŒåŸºäºå¯èƒ½æ€§çš„æ–¹æ³•å¯ä»¥åœ¨é¢„è®­ç»ƒç¼–ç å™¨çš„è¡¨ç¤ºç©ºé—´åº”ç”¨æ—¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„GitHubä»£ç åº“é“¾æ¥ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…è·å–å’Œä½¿ç”¨å…¶æ–¹æ³•ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†é€‰æ‹©åˆé€‚çš„æ–¹æ³•å’Œå·¥å…·çš„é‡è¦æ€§ï¼Œåœ¨æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„è®¾è®¡å’Œåº”ç”¨è¿‡ç¨‹ä¸­åº”æ³¨é‡é€‚åº”æ€§å’Œåˆ›æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-62cd9bb82204a1ee028ee88ed029fd5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5277f8e9841b48329f4ac1092e4de5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30a5d7b2d126d6163d0396a867c39b66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fef8668403fcd28ea74373a6dab16f6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PhaseGen-A-Diffusion-Based-Approach-for-Complex-Valued-MRI-Data-Generation"><a href="#PhaseGen-A-Diffusion-Based-Approach-for-Complex-Valued-MRI-Data-Generation" class="headerlink" title="PhaseGen: A Diffusion-Based Approach for Complex-Valued MRI Data   Generation"></a>PhaseGen: A Diffusion-Based Approach for Complex-Valued MRI Data   Generation</h2><p><strong>Authors:Moritz Rempe, Fabian HÃ¶rst, Helmut Becker, Marco Schlimbach, Lukas Rotkopf, Kevin KrÃ¶ninger, Jens Kleesiek</strong></p>
<p>Magnetic resonance imaging (MRI) raw data, or k-Space data, is complex-valued, containing both magnitude and phase information. However, clinical and existing Artificial Intelligence (AI)-based methods focus only on magnitude images, discarding the phase data despite its potential for downstream tasks, such as tumor segmentation and classification. In this work, we introduce $\textit{PhaseGen}$, a novel complex-valued diffusion model for generating synthetic MRI raw data conditioned on magnitude images, commonly used in clinical practice. This enables the creation of artificial complex-valued raw data, allowing pretraining for models that require k-Space information. We evaluate PhaseGen on two tasks: skull-stripping directly in k-Space and MRI reconstruction using the publicly available FastMRI dataset. Our results show that training with synthetic phase data significantly improves generalization for skull-stripping on real-world data, with an increased segmentation accuracy from $41.1%$ to $80.1%$, and enhances MRI reconstruction when combined with limited real-world data. This work presents a step forward in utilizing generative AI to bridge the gap between magnitude-based datasets and the complex-valued nature of MRI raw data. This approach allows researchers to leverage the vast amount of avaliable image domain data in combination with the information-rich k-Space data for more accurate and efficient diagnostic tasks. We make our code publicly $\href{<a target="_blank" rel="noopener" href="https://github.com/TIO-IKIM/PhaseGen%7D%7B/text%7Bavailable">https://github.com/TIO-IKIM/PhaseGen}{\text{available</a> here}}$. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åŸå§‹æ•°æ®ï¼Œä¹Ÿç§°ä¸ºk-Spaceæ•°æ®ï¼Œæ˜¯å¤æ•°å½¢å¼çš„ï¼ŒåŒ…å«å¹…åº¦å’Œç›¸ä½ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸´åºŠå’ŒåŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„æ–¹æ³•åªå…³æ³¨å¹…åº¦å›¾åƒï¼Œå°½ç®¡ç›¸ä½æ•°æ®å¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ï¼‰å…·æœ‰æ½œåŠ›ï¼Œä½†ä»ç„¶è¢«ä¸¢å¼ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†$\textit{PhaseGen}$ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤æ•°æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®åœ¨ä¸´åºŠå®è·µä¸­å¸¸ç”¨çš„å¹…åº¦å›¾åƒç”ŸæˆåˆæˆMRIåŸå§‹æ•°æ®ã€‚è¿™ä½¿å¾—å¯ä»¥åˆ›å»ºäººå·¥çš„å¤æ•°åŸå§‹æ•°æ®ï¼Œå…è®¸å¯¹éœ€è¦k-Spaceä¿¡æ¯çš„æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šå¯¹PhaseGenè¿›è¡Œäº†è¯„ä¼°ï¼šç›´æ¥åœ¨k-Spaceä¸­è¿›è¡Œé¢…éª¨å‰¥ç¦»å’Œä½¿ç”¨å…¬å¼€å¯ç”¨çš„FastMRIæ•°æ®é›†è¿›è¡ŒMRIé‡å»ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆæˆç›¸ä½æ•°æ®è¿›è¡Œè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜åœ¨ç°å®æ•°æ®ä¸Šè¿›è¡Œé¢…éª¨å‰¥ç¦»çš„æ³›åŒ–èƒ½åŠ›ï¼Œåˆ†å‰²å‡†ç¡®åº¦ä»41.1ï¼…æé«˜åˆ°80.1ï¼…ï¼Œå¹¶ä¸”åœ¨ä¸æœ‰é™ç°å®æ•°æ®ç»“åˆæ—¶ï¼Œå¯ä»¥å¢å¼ºMRIé‡å»ºã€‚è¿™é¡¹å·¥ä½œåˆ©ç”¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å¼¥åˆäº†åŸºäºå¹…åº¦çš„æ•°æ®é›†å’ŒMRIåŸå§‹æ•°æ®çš„å¤æ•°æ€§è´¨ä¹‹é—´çš„å·®è·ã€‚è¿™ç§æ–¹æ³•å…è®¸ç ”ç©¶äººå‘˜ç»“åˆå¤§é‡å¯ç”¨çš„å›¾åƒåŸŸæ•°æ®å’Œä¸°å¯Œçš„k-Spaceä¿¡æ¯ï¼Œä»¥è¿›è¡Œæ›´å‡†ç¡®å’Œé«˜æ•ˆçš„è¯Šæ–­ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€$\href{<a target="_blank" rel="noopener" href="https://github.com/TIO-IKIM/PhaseGen%7D%7B/text%7B%E5%8F%AF%E5%9C%A8%E6%AD%A4%E5%A4%84%E8%8E%B7%E5%8F%96%7D%7D$">https://github.com/TIO-IKIM/PhaseGen}{\text{å¯åœ¨æ­¤å¤„è·å–}}$</a>.</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07560v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPhaseGençš„æ–°å‹å¤æ‚å€¼æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯æ ¹æ®å¹…åº¦å›¾åƒç”ŸæˆåˆæˆMRIåŸå§‹æ•°æ®ã€‚è¿™é¡¹æŠ€æœ¯åœ¨ä¸´åºŠå®è·µä¸­å¸¸ç”¨ï¼Œèƒ½å¤Ÿåˆ›å»ºäººå·¥å¤æ‚å€¼åŸå§‹æ•°æ®ï¼Œä¸ºéœ€è¦k-Spaceä¿¡æ¯çš„æ¨¡å‹æä¾›é¢„è®­ç»ƒã€‚åœ¨FastMRIå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„ä¸¤é¡¹ä»»åŠ¡è¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨åˆæˆç›¸ä½æ•°æ®è¿›è¡Œè®­ç»ƒæ˜¾è‘—æé«˜äº†åœ¨ç°å®æ•°æ®ä¸Šçš„é¢…éª¨å‰¥ç¦»åˆ†å‰²å‡†ç¡®åº¦ï¼Œä»41.1%æé«˜åˆ°80.1%ï¼Œå¹¶åœ¨ç»“åˆæœ‰é™ç°å®æ•°æ®æ—¶è¿›è¡ŒMRIé‡å»ºæ—¶è¡¨ç°å‡ºå¢å¼ºçš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œåˆ©ç”¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç¼©å°äº†åŸºäºå¹…åº¦çš„æ•°æ®é›†å’Œå¤æ‚å€¼MRIåŸå§‹æ•°æ®ä¹‹é—´çš„å·®è·ï¼Œå…è®¸ç ”ç©¶äººå‘˜ç»“åˆä¸°å¯Œçš„k-Spaceæ•°æ®å’Œå¤§é‡çš„å›¾åƒåŸŸæ•°æ®ï¼Œä»¥è¿›è¡Œæ›´å‡†ç¡®å’Œé«˜æ•ˆçš„è¯Šæ–­ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PhaseGenæ˜¯ä¸€ç§å¤æ‚å€¼æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®å¹…åº¦å›¾åƒç”ŸæˆåˆæˆMRIåŸå§‹æ•°æ®ã€‚</li>
<li>ä¸´åºŠå’Œç°æœ‰çš„äººå·¥æ™ºèƒ½æ–¹æ³•ä¸»è¦å…³æ³¨å¹…åº¦å›¾åƒï¼Œå¿½è§†äº†ç›¸ä½æ•°æ®åœ¨è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</li>
<li>PhaseGençš„å¼•å…¥ä½¿å¾—åˆ›å»ºäººå·¥å¤æ‚å€¼åŸå§‹æ•°æ®æˆä¸ºå¯èƒ½ï¼Œä¸ºéœ€è¦k-Spaceä¿¡æ¯çš„æ¨¡å‹æä¾›äº†é¢„è®­ç»ƒçš„æœºä¼šã€‚</li>
<li>åœ¨FastMRIæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ˜¾ç¤ºåˆæˆç›¸ä½æ•°æ®è®­ç»ƒåœ¨é¢…éª¨å‰¥ç¦»åˆ†å‰²å’ŒMRIé‡å»ºä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•ç¼©å°äº†åŸºäºå¹…åº¦çš„æ•°æ®é›†å’ŒMRIåŸå§‹æ•°æ®çš„å·®è·ï¼Œä¿ƒè¿›äº†æ›´å‡†ç¡®çš„è¯Šæ–­ã€‚</li>
<li>PhaseGenä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œé“¾æ¥ä¸º[é“¾æ¥åœ°å€]ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aeb54075b1c426db844542a1c0ce7b40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d35871f4f138c574330b73abad3f648.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1435311e9978fd317f4fc524d343a769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc1c51c4cee0eee77ec1e7ada0e0da8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b84c82423c3b158e859848ee5200e9cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dcf550b7b6815722c8b7087c7c2ee50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8cf2816276162be4122bc0fe113c7c49.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Conditional-Data-Synthesis-Augmentation"><a href="#Conditional-Data-Synthesis-Augmentation" class="headerlink" title="Conditional Data Synthesis Augmentation"></a>Conditional Data Synthesis Augmentation</h2><p><strong>Authors:Xinyu Tian, Xiaotong Shen</strong></p>
<p>Reliable machine learning and statistical analysis rely on diverse, well-distributed training data. However, real-world datasets are often limited in size and exhibit underrepresentation across key subpopulations, leading to biased predictions and reduced performance, particularly in supervised tasks such as classification. To address these challenges, we propose Conditional Data Synthesis Augmentation (CoDSA), a novel framework that leverages generative models, such as diffusion models, to synthesize high-fidelity data for improving model performance across multimodal domains including tabular, textual, and image data. CoDSA generates synthetic samples that faithfully capture the conditional distributions of the original data, with a focus on under-sampled or high-interest regions. Through transfer learning, CoDSA fine-tunes pre-trained generative models to enhance the realism of synthetic data and increase sample density in sparse areas. This process preserves inter-modal relationships, mitigates data imbalance, improves domain adaptation, and boosts generalization. We also introduce a theoretical framework that quantifies the statistical accuracy improvements enabled by CoDSA as a function of synthetic sample volume and targeted region allocation, providing formal guarantees of its effectiveness. Extensive experiments demonstrate that CoDSA consistently outperforms non-adaptive augmentation strategies and state-of-the-art baselines in both supervised and unsupervised settings. </p>
<blockquote>
<p>å¯é çš„äººå·¥æ™ºèƒ½æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡åˆ†æä¾èµ–äºå¤šæ ·ä¸”åˆ†å¸ƒè‰¯å¥½çš„è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æ•°æ®é›†å¾€å¾€è§„æ¨¡æœ‰é™ï¼Œå¹¶åœ¨å…³é”®å­ç¾¤ä½“ä¸­è¡¨ç°å‡ºç°è±¡è¡¨å¾ä¸è¶³ï¼Œå¯¼è‡´é¢„æµ‹å‡ºç°åå·®å’Œæ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¦‚åˆ†ç±»è¿™æ ·çš„ç›‘ç£ä»»åŠ¡ä¸­ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¡ä»¶æ•°æ®åˆæˆå¢å¼ºï¼ˆCoDSAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰åˆæˆé«˜ä¿çœŸæ•°æ®ï¼Œä»¥æ”¹å–„è·¨å¤šæ¨¡å¼åŸŸï¼ˆåŒ…æ‹¬è¡¨æ ¼ã€æ–‡æœ¬å’Œå›¾åƒæ•°æ®ï¼‰çš„æ¨¡å‹æ€§èƒ½ã€‚CoDSAç”Ÿæˆåˆæˆæ ·æœ¬ï¼Œå¿ å®æ•æ‰åŸå§‹æ•°æ®çš„æ¡ä»¶åˆ†å¸ƒï¼Œé‡ç‚¹å…³æ³¨æ¬ é‡‡æ ·æˆ–é«˜å…´è¶£åŒºåŸŸã€‚é€šè¿‡è¿ç§»å­¦ä¹ ï¼ŒCoDSAå¯¹é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å¢å¼ºåˆæˆæ•°æ®çš„çœŸå®æ€§å’Œç¨€ç–åŒºåŸŸçš„æ ·æœ¬å¯†åº¦ã€‚è¿™ä¸€è¿‡ç¨‹ä¿ç•™äº†è·¨æ¨¡æ€å…³ç³»ï¼Œç¼“è§£äº†æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œæé«˜äº†åŸŸé€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œé‡åŒ–ç”±CoDSAå¯ç”¨çš„ç»Ÿè®¡ç²¾åº¦æ”¹è¿›ä¸åˆæˆæ ·æœ¬é‡å’Œç›®æ ‡åŒºåŸŸåˆ†é…çš„å‡½æ•°å…³ç³»ï¼Œä¸ºå…¶æä¾›æœ‰æ•ˆçš„æ­£å¼ä¿è¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ç›‘ç£å’Œéç›‘ç£ç¯å¢ƒä¸­ï¼ŒCoDSAå§‹ç»ˆä¼˜äºéè‡ªé€‚åº”å¢å¼ºç­–ç•¥å’Œæœ€æ–°çš„åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07426v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†æ¡ä»¶æ•°æ®åˆæˆå¢å¼ºï¼ˆCoDSAï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰åˆæˆé«˜è´¨é‡æ•°æ®ï¼Œä»¥æ”¹å–„åŒ…æ‹¬è¡¨æ ¼ã€æ–‡æœ¬å’Œå›¾åƒæ•°æ®ç­‰å¤šæ¨¡å¼é¢†åŸŸçš„æ¨¡å‹æ€§èƒ½ã€‚CoDSAç”Ÿæˆçš„æ•°æ®æ ·æœ¬èƒ½å¤Ÿå¿ å®æ•æ‰åŸå§‹æ•°æ®çš„æ¡ä»¶åˆ†å¸ƒï¼Œé‡ç‚¹å…³æ³¨æ¬ é‡‡æ ·æˆ–é«˜å…´è¶£åŒºåŸŸã€‚é€šè¿‡è¿ç§»å­¦ä¹ ï¼ŒCoDSAå¯¹é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜åˆæˆæ•°æ®çš„çœŸå®æ€§å’Œç¨€ç–åŒºåŸŸçš„æ ·æœ¬å¯†åº¦ã€‚æ­¤æ¡†æ¶èƒ½å¤Ÿä¿ç•™è·¨æ¨¡æ€å…³ç³»ï¼Œç¼“è§£æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œæ”¹å–„é¢†åŸŸé€‚åº”æ€§å’Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒCoDSAåœ¨ç›‘ç£å’ŒåŠç›‘ç£è®¾ç½®ä¸­å§‹ç»ˆä¼˜äºéè‡ªé€‚åº”å¢å¼ºç­–ç•¥å’Œæœ€æ–°åŸºçº¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çœŸå®ä¸–ç•Œçš„æ•°æ®é›†åœ¨å¤§å°å’Œä»£è¡¨æ€§æ–¹é¢å¸¸å¸¸æœ‰é™ï¼Œä¼šå¯¼è‡´æ¨¡å‹é¢„æµ‹åå·®å’Œæ€§èƒ½ä¸‹é™ã€‚</li>
<li>CoDSAæ¡†æ¶åˆ©ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰åˆæˆé«˜è´¨é‡æ•°æ®ï¼Œä»¥æ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>CoDSAèƒ½å¤Ÿå…³æ³¨æ¬ é‡‡æ ·æˆ–é«˜å…´è¶£åŒºåŸŸï¼Œå¹¶ç”Ÿæˆå¿ å®äºåŸå§‹æ•°æ®æ¡ä»¶åˆ†å¸ƒçš„åˆæˆæ ·æœ¬ã€‚</li>
<li>é€šè¿‡è¿ç§»å­¦ä¹ ï¼ŒCoDSAèƒ½å¤Ÿå¢å¼ºåˆæˆæ•°æ®çš„çœŸå®æ€§å’Œæé«˜ç¨€ç–åŒºåŸŸçš„æ ·æœ¬å¯†åº¦ã€‚</li>
<li>CoDSAèƒ½å¤Ÿä¿ç•™è·¨æ¨¡æ€å…³ç³»ï¼Œç¼“è§£æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œæ”¹å–„é¢†åŸŸé€‚åº”æ€§ã€‚</li>
<li>CoDSAæä¾›äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œé‡åŒ–åˆæˆæ ·æœ¬ä½“ç§¯å’Œç›®æ ‡åŒºåŸŸåˆ†é…æ‰€å¸¦æ¥çš„ç»Ÿè®¡ç²¾åº¦æ”¹è¿›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒCoDSAåœ¨å¤šä¸ªè®¾ç½®ä¸­å‡ä¼˜äºå…¶ä»–å¢å¼ºç­–ç•¥å’ŒåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98af9b6bc5547cb86d8bc7f3adfb98a4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Routing-to-the-Right-Expertise-A-Trustworthy-Judge-for-Instruction-based-Image-Editing"><a href="#Routing-to-the-Right-Expertise-A-Trustworthy-Judge-for-Instruction-based-Image-Editing" class="headerlink" title="Routing to the Right Expertise: A Trustworthy Judge for   Instruction-based Image Editing"></a>Routing to the Right Expertise: A Trustworthy Judge for   Instruction-based Image Editing</h2><p><strong>Authors:Chenxi Sun, Hongzhi Zhang, Qi Wang, Fuzheng Zhang</strong></p>
<p>Instruction-based Image Editing (IIE) models have made significantly improvement due to the progress of multimodal large language models (MLLMs) and diffusion models, which can understand and reason about complex editing instructions. In addition to advancing current IIE models, accurately evaluating their output has become increasingly critical and challenging. Current IIE evaluation methods and their evaluation procedures often fall short of aligning with human judgment and often lack explainability. To address these limitations, we propose JUdgement through Routing of Expertise (JURE). Each expert in JURE is a pre-selected model assumed to be equipped with an atomic expertise that can provide useful feedback to judge output, and the router dynamically routes the evaluation task of a given instruction and its output to appropriate experts, aggregating their feedback into a final judge. JURE is trustworthy in two aspects. First, it can effortlessly provide explanations about its judge by examining the routed experts and their feedback. Second, experimental results demonstrate that JURE is reliable by achieving superior alignment with human judgments, setting a new standard for automated IIE evaluation. Moreover, JUREâ€™s flexible design is future-proof - modular experts can be seamlessly replaced or expanded to accommodate advancements in IIE, maintaining consistently high evaluation quality. Our evaluation data and results are available at <a target="_blank" rel="noopener" href="https://github.com/Cyyyyyrus/JURE.git">https://github.com/Cyyyyyrus/JURE.git</a>. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼ˆIIEï¼‰æ¨¡å‹ç”±äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„è¿›å±•è€Œå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œæ¨ç†å¤æ‚çš„ç¼–è¾‘æŒ‡ä»¤ã€‚é™¤äº†æ¨è¿›å½“å‰çš„IIEæ¨¡å‹å¤–ï¼Œå‡†ç¡®è¯„ä¼°å®ƒä»¬çš„è¾“å‡ºä¹Ÿå˜å¾—æ—¥ç›Šå…³é”®å’Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰çš„IIEè¯„ä¼°æ–¹æ³•åŠå…¶æµç¨‹å¾€å¾€ä¸äººç±»åˆ¤æ–­ä¸ç¬¦ï¼Œå¹¶ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡ä¸“ä¸šè·¯ç”±è¿›è¡Œè¯„åˆ¤ï¼ˆJUREï¼‰ã€‚åœ¨JUREä¸­çš„æ¯ä¸ªä¸“å®¶éƒ½æ˜¯é¢„å…ˆé€‰æ‹©å¥½çš„æ¨¡å‹ï¼Œå‡è®¾å…·å¤‡åŸå­ä¸“ä¸šçŸ¥è¯†ï¼Œå¯ä»¥ä¸ºåˆ¤æ–­è¾“å‡ºæä¾›æœ‰ç”¨çš„åé¦ˆï¼Œè·¯ç”±å™¨ä¼šåŠ¨æ€åœ°å°†ç»™å®šæŒ‡ä»¤çš„è¯„ä¼°ä»»åŠ¡åŠå…¶è¾“å‡ºè·¯ç”±åˆ°åˆé€‚çš„ä¸“å®¶ï¼Œå¹¶æ±‡æ€»ä»–ä»¬çš„åé¦ˆä»¥åšå‡ºæœ€ç»ˆåˆ¤æ–­ã€‚JUREåœ¨ä¸¤ä¸ªæ–¹é¢å€¼å¾—ä¿¡èµ–ã€‚é¦–å…ˆï¼Œé€šè¿‡æ£€æŸ¥è·¯ç”±çš„ä¸“å®¶åŠå…¶åé¦ˆï¼Œå®ƒå¯ä»¥è½»æ¾åœ°å¯¹å…¶åˆ¤æ–­æä¾›è§£é‡Šã€‚å…¶æ¬¡ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä¸äººç±»åˆ¤æ–­ç›¸æ¯”ï¼ŒJUREæ›´åŠ å¯é ï¼Œä¸ºè‡ªåŠ¨åŒ–IIEè¯„ä¼°æ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚æ­¤å¤–ï¼ŒJUREçš„è®¾è®¡å…·æœ‰å‰ç»æ€§ï¼Œæ¨¡å—åŒ–ä¸“å®¶å¯ä»¥è½»æ¾æ›¿æ¢æˆ–æ‰©å±•ä»¥é€‚åº”IIEçš„è¿›æ­¥ï¼Œä»è€Œä¿æŒå§‹ç»ˆå¦‚ä¸€çš„é«˜è¯„ä¼°è´¨é‡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ•°æ®å’Œç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Cyyyyyrus/JURE.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Cyyyyyrus/JURE.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07424v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼ˆIIEï¼‰æ¨¡å‹å› å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„è¿›å±•è€Œå¾—åˆ°æ˜¾è‘—æ”¹å–„ï¼Œèƒ½å¤Ÿç†è§£å¹¶å¤„ç†å¤æ‚çš„ç¼–è¾‘æŒ‡ä»¤ã€‚é’ˆå¯¹å½“å‰IIEæ¨¡å‹è¯„ä¼°æ–¹æ³•ä¸å…¶ç¨‹åºä¸äººç±»åˆ¤æ–­çš„å¯¹é½ä¸è¶³å’Œç¼ºä¹è§£é‡Šæ€§çš„é—®é¢˜ï¼Œæå‡ºé€šè¿‡è·¯ç”±ä¸“ä¸šçŸ¥è¯†è¿›è¡Œè¯„åˆ¤ï¼ˆJUREï¼‰ã€‚JUREä¸­çš„ä¸“å®¶è¢«è§†ä¸ºæ‹¥æœ‰åŸå­ä¸“ä¸šçŸ¥è¯†ï¼Œå¯æä¾›æœ‰ç”¨çš„åé¦ˆæ¥è¯„ä¼°è¾“å‡ºï¼Œè·¯ç”±å™¨ä¼šåŠ¨æ€åœ°å°†ç»™å®šæŒ‡ä»¤åŠå…¶è¾“å‡ºçš„è¯„ä¼°ä»»åŠ¡è·¯ç”±åˆ°åˆé€‚çš„ä¸“å®¶ï¼Œå¹¶æ±‡é›†ä»–ä»¬çš„åé¦ˆä½œå‡ºæœ€ç»ˆåˆ¤æ–­ã€‚JUREåœ¨ä¸¤ä¸ªæ–¹é¢å€¼å¾—ä¿¡èµ–ï¼šä¸€æ˜¯å¯ä»¥è½»æ¾è§£é‡Šå…¶åˆ¤æ–­ä¾æ®ï¼›äºŒæ˜¯ä¸äººç±»åˆ¤æ–­å¯¹é½çš„å¯é æ€§é«˜ï¼Œä¸ºè‡ªåŠ¨åŒ–IIEè¯„ä¼°è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚æ­¤å¤–ï¼ŒJUREçµæ´»çš„è®¾è®¡èƒ½å¤Ÿé€‚åº”æœªæ¥çš„IIEè¿›å±•ï¼Œä¿æŒä¸€è´¯çš„è¯„ä¼°è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IIEæ¨¡å‹ç”±äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„è¿›æ­¥è€Œæœ‰æ‰€æå‡ï¼Œèƒ½å¤Ÿç†è§£å’Œå¤„ç†å¤æ‚çš„ç¼–è¾‘æŒ‡ä»¤ã€‚</li>
<li>å½“å‰IIEæ¨¡å‹çš„è¯„ä¼°æ–¹æ³•ç»å¸¸æ— æ³•ä¸äººç±»åˆ¤æ–­å¯¹é½ï¼Œä¸”ç¼ºä¹è§£é‡Šæ€§ã€‚</li>
<li>æå‡ºé€šè¿‡è·¯ç”±ä¸“ä¸šçŸ¥è¯†è¿›è¡Œè¯„åˆ¤ï¼ˆJUREï¼‰çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>JUREä¸­çš„ä¸“å®¶æä¾›åŸå­ä¸“ä¸šçŸ¥è¯†æ¥è¯„ä¼°è¾“å‡ºã€‚</li>
<li>è·¯ç”±å™¨åŠ¨æ€åœ°å°†æŒ‡ä»¤å’Œè¾“å‡ºè·¯ç”±åˆ°åˆé€‚çš„ä¸“å®¶è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>JUREå…·å¤‡è§£é‡Šæ€§ã€ä¸äººç±»åˆ¤æ–­å¯¹é½çš„å¯é æ€§ï¼Œä¸ºæœªæ¥IIEæ¨¡å‹çš„è¿›å±•æä¾›äº†çµæ´»çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>JUREçš„è¯„ä»·æ•°æ®å’Œç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Cyyyyyrus/JURE.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Cyyyyyrus/JURE.gitä¸Šæ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-151436a7301ba89d2dfb5beca7c8c22a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6ffa8519ddddadbbf89d1d5e86794db.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22e41b343f63024327dd1c1d1a9f5a23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56111e34e68fd5b358fad137b25b0120.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ID-Booth-Identity-consistent-Face-Generation-with-Diffusion-Models"><a href="#ID-Booth-Identity-consistent-Face-Generation-with-Diffusion-Models" class="headerlink" title="ID-Booth: Identity-consistent Face Generation with Diffusion Models"></a>ID-Booth: Identity-consistent Face Generation with Diffusion Models</h2><p><strong>Authors:Darian TomaÅ¡eviÄ‡, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir Å truc, Peter Peer</strong></p>
<p>Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at <a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth">https://github.com/dariant/ID-Booth</a>. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆå»ºæ¨¡æŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—èƒ½å¤Ÿç”Ÿæˆé€‚ç”¨äºå„ç§é¢†åŸŸçš„é«˜è´¨é‡åˆæˆæ•°æ®ï¼ŒåŒ…æ‹¬äººè„¸è¯†åˆ«ã€‚åœ¨è¿™é‡Œï¼Œæœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹é€šå¸¸ä¾èµ–äºå¯¹åŠŸèƒ½å¼ºå¤§çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶è®¾å®šå’Œå¾®è°ƒï¼Œä»¥ä¿ƒè¿›æ‰€éœ€èº«ä»½çš„é€¼çœŸå›¾åƒåˆæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾€å¾€ä¸è€ƒè™‘ä¸»ä½“çš„èº«ä»½ï¼Œå¯¼è‡´ç”Ÿæˆå›¾åƒä¸é¢„æœŸèº«ä»½ä¹‹é—´çš„ä¸€è‡´æ€§è¾ƒå·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé‡‡ç”¨åŸºäºèº«ä»½çš„è®­ç»ƒç›®æ ‡çš„æ–¹æ³•å¾€å¾€ä¼šåœ¨èº«ä»½çš„å„ä¸ªæ–¹é¢è¿‡åº¦æ‹Ÿåˆï¼Œä»è€Œé™ä½äº†å¯ä»¥ç”Ÿæˆçš„å›¾åƒå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºID-Boothã€‚ID-Boothç”±ä¸€ä¸ªè´Ÿè´£æ•°æ®ç”Ÿæˆçš„é™å™ªç½‘ç»œã€ä¸€ä¸ªç”¨äºå°†å›¾åƒæ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´åŠå…¶åå‘æ˜ å°„çš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ç»„æˆï¼Œè¯¥æ–‡æœ¬ç¼–ç å™¨å…è®¸åŸºäºæç¤ºæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äº†ä¸€ç§æ–°å‹çš„ä¸‰é‡èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åˆæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°èº«ä»½ä¸€è‡´çš„å›¾åƒç”Ÿæˆã€‚åˆ©ç”¨æœ€å…ˆè¿›çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œå¤šç§æç¤ºè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨èº«ä»½å†…éƒ¨ä¸€è‡´æ€§å’Œèº«ä»½é—´å¯åˆ†æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒæ—¶å®ç°äº†è¾ƒé«˜çš„å›¾åƒå¤šæ ·æ€§ã€‚å› æ­¤ï¼Œç”Ÿæˆçš„æ•°æ®å…è®¸æœ‰æ•ˆåœ°æ‰©å……å°è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä»¥ä¿æŠ¤éšç§çš„æ–¹å¼è®­ç»ƒæ€§èƒ½æ›´å¥½çš„è¯†åˆ«æ¨¡å‹ã€‚ID-Boothæ¡†æ¶çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth">https://github.com/dariant/ID-Booth</a>å…¬å¼€è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07392v1">PDF</a> IEEE International Conference on Automatic Face and Gesture   Recognition (FG) 2025, 14 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ç”Ÿæˆå¼æ¡†æ¶ID-Boothï¼Œç”¨äºç”Ÿæˆå…·æœ‰èº«ä»½ä¸€è‡´æ€§çš„é«˜è´¨é‡åˆæˆæ•°æ®ã€‚è¯¥æ¡†æ¶é€šè¿‡å»å™ªç½‘ç»œè´Ÿè´£æ•°æ®ç”Ÿæˆï¼Œä½¿ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å®ç°å›¾åƒä¸ä½ç»´æ½œåœ¨ç©ºé—´ä¹‹é—´çš„æ˜ å°„ï¼Œå¹¶é€šè¿‡æ–‡æœ¬ç¼–ç å™¨å®ç°åŸºäºæç¤ºçš„ç”Ÿæˆè¿‡ç¨‹æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒID-Boothå…·æœ‰æ›´å¥½çš„èº«ä»½ä¸€è‡´æ€§ã€æ›´é«˜çš„å›¾åƒå¤šæ ·æ€§å’Œæ›´å¥½çš„è·¨èº«ä»½åˆ†ç¦»èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶äº§ç”Ÿçš„æ•°æ®å¯ç”¨äºæœ‰æ•ˆåœ°å¢å¼ºå°è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä»¥éšç§ä¿æŠ¤çš„æ–¹å¼è®­ç»ƒæ€§èƒ½æ›´å¥½çš„è¯†åˆ«æ¨¡å‹ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æœ€æ–°è¿›å±•çš„ç”Ÿæˆå»ºæ¨¡æŠ€æœ¯å¯ä»¥é€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼Œè¿™äº›æ•°æ®é€‚ç”¨äºå„ç§é¢†åŸŸï¼ŒåŒ…æ‹¬äººè„¸è¯†åˆ«ã€‚</li>
<li>å½“å‰å…ˆè¿›æŠ€æœ¯é€šå¸¸ä¾èµ–äºå¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶è®¾å®šå’Œå¾®è°ƒæ¥åˆæˆå…·æœ‰ç°å®æ„Ÿçš„å›¾åƒã€‚ä½†è¿™äº›æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´ä¸è€ƒè™‘èº«ä»½ï¼Œå¯¼è‡´ç”Ÿæˆèº«ä»½ä¸å®é™…æ„å›¾ä¹‹é—´çš„ä¸ä¸€è‡´ã€‚</li>
<li>é‡‡ç”¨èº«ä»½ä¸ºåŸºç¡€çš„è®­ç»ƒç›®æ ‡çš„æ–¹æ³•å¯èƒ½ä¼šè¿‡äºé€‚åº”èº«ä»½çš„å„ä¸ªæ–¹é¢ï¼Œä»è€Œé™ä½å¯ç”Ÿæˆçš„å›¾åƒå¤šæ ·æ€§ã€‚</li>
<li>ID-Boothæ¡†æ¶é€šè¿‡ç»“åˆå»å™ªç½‘ç»œã€å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨è§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚å®ƒé‡‡ç”¨æ–°é¢–çš„ä¸‰é‡èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œå®ç°äº†èº«ä»½ä¸€è‡´çš„å›¾åƒç”Ÿæˆå¹¶ä¿ç•™äº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„åˆæˆèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒID-Boothåœ¨èº«ä»½ä¸€è‡´æ€§å’Œå›¾åƒå¤šæ ·æ€§æ–¹é¢è¡¨ç°æ›´ä½³ã€‚æ­¤å¤–ï¼Œå®ƒäº§ç”Ÿçš„æ•°æ®èƒ½æœ‰æ•ˆå¢å¼ºå°è§„æ¨¡æ•°æ®é›†å¹¶è®­ç»ƒå‡ºæ€§èƒ½æ›´ä½³çš„è¯†åˆ«æ¨¡å‹ã€‚æ¡†æ¶çš„æºä»£ç å·²å…¬å¼€æä¾›ã€‚</li>
<li>ID-Boothæ¡†æ¶èƒ½å¤Ÿä»¥ä¸€ç§éšç§ä¿æŠ¤çš„æ–¹å¼ç”Ÿæˆåˆæˆæ•°æ®ï¼Œä»è€Œæé«˜äººè„¸è¯†åˆ«ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-413d4fb3e1ee96ac10a4c2c2ef7c5dac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30a1325fe4dfc46da49d23433810504b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-814022dbf79c1a1cc8eb9da0ac5986c4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Novel-Diffusion-Models-for-Multimodal-3D-Hand-Trajectory-Prediction"><a href="#Novel-Diffusion-Models-for-Multimodal-3D-Hand-Trajectory-Prediction" class="headerlink" title="Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction"></a>Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</h2><p><strong>Authors:Junyi Ma, Wentao Bao, Jingyi Xu, Guanzhong Sun, Xieyuanli Chen, Hesheng Wang</strong></p>
<p>Predicting hand motion is critical for understanding human intentions and bridging the action space between human movements and robot manipulations. Existing hand trajectory prediction (HTP) methods forecast the future hand waypoints in 3D space conditioned on past egocentric observations. However, such models are only designed to accommodate 2D egocentric video inputs. There is a lack of awareness of multimodal environmental information from both 2D and 3D observations, hindering the further improvement of 3D HTP performance. In addition, these models overlook the synergy between hand movements and headset camera egomotion, either predicting hand trajectories in isolation or encoding egomotion only from past frames. To address these limitations, we propose novel diffusion models (MMTwin) for multimodal 3D hand trajectory prediction. MMTwin is designed to absorb multimodal information as input encompassing 2D RGB images, 3D point clouds, past hand waypoints, and text prompt. Besides, two latent diffusion models, the egomotion diffusion and the HTP diffusion as twins, are integrated into MMTwin to predict camera egomotion and future hand trajectories concurrently. We propose a novel hybrid Mamba-Transformer module as the denoising model of the HTP diffusion to better fuse multimodal features. The experimental results on three publicly available datasets and our self-recorded data demonstrate that our proposed MMTwin can predict plausible future 3D hand trajectories compared to the state-of-the-art baselines, and generalizes well to unseen environments. The code and pretrained models will be released at <a target="_blank" rel="noopener" href="https://github.com/IRMVLab/MMTwin">https://github.com/IRMVLab/MMTwin</a>. </p>
<blockquote>
<p>é¢„æµ‹æ‰‹éƒ¨è¿åŠ¨å¯¹äºç†è§£äººç±»æ„å›¾ä»¥åŠå»ºç«‹äººç±»è¿åŠ¨å’Œæœºå™¨äººæ“ä½œä¹‹é—´çš„åŠ¨ä½œç©ºé—´æ¡¥æ¢è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹ï¼ˆHTPï¼‰æ–¹æ³•åŸºäºè¿‡å»çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§‚æµ‹æ¥é¢„æµ‹æœªæ¥æ‰‹éƒ¨åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„è½¨è¿¹ç‚¹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»…è®¾è®¡ç”¨äºå¤„ç†äºŒç»´ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘è¾“å…¥ã€‚å®ƒä»¬æ— æ³•æ„è¯†åˆ°æ¥è‡ªäºŒç»´å’Œä¸‰ç»´è§‚å¯Ÿçš„å¤šå…ƒç¯å¢ƒä¿¡æ¯ï¼Œé˜»ç¢äº†ä¸‰ç»´HTPæ€§èƒ½çš„è¿›ä¸€æ­¥æé«˜ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹å¿½è§†äº†æ‰‹éƒ¨åŠ¨ä½œä¸å¤´æˆ´å¼ç›¸æœºè‡ªä¸»è¿åŠ¨ä¹‹é—´çš„ååŒä½œç”¨ï¼Œè¦ä¹ˆå•ç‹¬é¢„æµ‹æ‰‹éƒ¨è½¨è¿¹ï¼Œè¦ä¹ˆä»…ä»è¿‡å»å‡ å¸§ä¸­ç¼–ç è‡ªä¸»è¿åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå¤šæ¨¡æ€ä¸‰ç»´æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹çš„æ–°å‹æ‰©æ•£æ¨¡å‹MMTwinã€‚MMTwinè¢«è®¾è®¡æˆå¸æ”¶å¤šæ¨¡æ€ä¿¡æ¯ä½œä¸ºè¾“å…¥ï¼ŒåŒ…æ‹¬äºŒç»´RGBå›¾åƒã€ä¸‰ç»´ç‚¹äº‘ã€è¿‡å»çš„æ‰‹éƒ¨è½¨è¿¹ç‚¹å’Œæ–‡æœ¬æç¤ºã€‚æ­¤å¤–ï¼Œä¸¤ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹â€”â€”è‡ªä¸»è¿åŠ¨æ‰©æ•£å’ŒHTPæ‰©æ•£ä½œä¸ºåŒèƒèƒæ¨¡å‹è¢«é›†æˆåˆ°MMTwinä¸­ï¼Œä»¥åŒæ—¶é¢„æµ‹ç›¸æœºè‡ªä¸»è¿åŠ¨å’Œæœªæ¥æ‰‹éƒ¨è½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆMamba-Transformeræ¨¡å—ä½œä¸ºHTPæ‰©æ•£çš„å»å™ªæ¨¡å‹ï¼Œä»¥æ›´å¥½åœ°èåˆå¤šæ¨¡æ€ç‰¹å¾ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†å’Œæˆ‘ä»¬è‡ªè¡Œå½•åˆ¶çš„æ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æˆ‘ä»¬æå‡ºçš„MMTwinç›¸æ¯”ï¼Œæœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹å¯ä»¥é¢„æµ‹æ›´åˆç†çš„æœªæ¥ä¸‰ç»´æ‰‹éƒ¨è½¨è¿¹ï¼Œå¹¶ä¸”èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„ç¯å¢ƒã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/IRMVLab/MMTwin%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/IRMVLab/MMTwinä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07375v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºä¸€ç§åŸºäºå¤šæ¨¡æ€ä¿¡æ¯çš„ä¸‰ç»´æ‰‹è½¨è¿¹é¢„æµ‹æ‰©æ•£æ¨¡å‹MMTwinï¼Œå¯èåˆäºŒç»´RGBå›¾åƒã€ä¸‰ç»´ç‚¹äº‘ã€è¿‡å»çš„æ‰‹è½¨è¿¹å’Œæ–‡æœ¬æç¤ºç­‰å¤šç§ä¿¡æ¯ã€‚é‡‡ç”¨ä¸¤ä¸ªæ½œå¼æ‰©æ•£æ¨¡å‹ï¼šegomotionæ‰©æ•£å’ŒHTPæ‰©æ•£ï¼ŒåŒæ—¶é¢„æµ‹ç›¸æœºè¿åŠ¨å’Œæœªæ¥æ‰‹è½¨è¿¹ã€‚é‡‡ç”¨æ–°å‹æ··åˆMamba-Transformeræ¨¡å—ä½œä¸ºHTPæ‰©æ•£çš„å»å™ªæ¨¡å‹ï¼Œå®ç°å¤šæ¨¡æ€ç‰¹å¾èåˆã€‚åœ¨å…¬å¼€æ•°æ®é›†å’Œè‡ªæˆ‘å½•åˆ¶æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMMTwinèƒ½é¢„æµ‹å¯ä¿¡çš„æœªæ¥ä¸‰ç»´æ‰‹è½¨è¿¹ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰æ‰‹è½¨è¿¹é¢„æµ‹æ–¹æ³•ä¸»è¦åŸºäºè¿‡å»äºŒç»´è§‚å¯Ÿæ•°æ®ï¼Œç¼ºä¹å¤šæ¨¡æ€ç¯å¢ƒä¿¡æ¯çš„èåˆã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ‰©æ•£æ¨¡å‹MMTwinï¼Œç”¨äºå¤šæ¨¡æ€ä¸‰ç»´æ‰‹è½¨è¿¹é¢„æµ‹ï¼Œèƒ½å¤ŸèåˆåŒ…æ‹¬äºŒç»´RGBå›¾åƒã€ä¸‰ç»´ç‚¹äº‘ç­‰ç¯å¢ƒä¿¡æ¯ã€‚</li>
<li>MMTwinåŒ…å«ä¸¤ä¸ªæ½œå¼æ‰©æ•£æ¨¡å‹ï¼šegomotionæ‰©æ•£å’ŒHTPæ‰©æ•£ï¼Œåˆ†åˆ«é¢„æµ‹ç›¸æœºè¿åŠ¨å’Œæœªæ¥æ‰‹è½¨è¿¹ã€‚</li>
<li>é‡‡ç”¨æ··åˆMamba-Transformeræ¨¡å—ä½œä¸ºå»å™ªæ¨¡å‹ï¼Œå®ç°å¤šæ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆèåˆã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜MMTwinèƒ½å¤Ÿé¢„æµ‹æœªæ¥å¯ä¿¡çš„ä¸‰ç»´æ‰‹è½¨è¿¹ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>MMTwinå…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œèƒ½å¤Ÿé€‚åº”æœªè§è¿‡ç¯å¢ƒçš„æ‰‹è½¨è¿¹é¢„æµ‹ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8c3d4e037e0f8bceb7513cb3a00a5a34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf5cb9bbf7f5e25bd5a5c62fcbebb0e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e721dacdcac6e8e81ca101069188fc25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b5fa7039a5d739fd8b04a9e9abeea51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9f7ae313ac0a373d5dd49aa222a501e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5fd4cf49f90016af622e02f13392ced.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MoEDiff-SR-Mixture-of-Experts-Guided-Diffusion-Model-for-Region-Adaptive-MRI-Super-Resolution"><a href="#MoEDiff-SR-Mixture-of-Experts-Guided-Diffusion-Model-for-Region-Adaptive-MRI-Super-Resolution" class="headerlink" title="MoEDiff-SR: Mixture of Experts-Guided Diffusion Model for   Region-Adaptive MRI Super-Resolution"></a>MoEDiff-SR: Mixture of Experts-Guided Diffusion Model for   Region-Adaptive MRI Super-Resolution</h2><p><strong>Authors:Zhe Wang, Yuhua Ru, Aladine Chetouani, Fang Chen, Fabian Bauer, Liping Zhang, Didier Hans, Rachid Jennane, Mohamed Jarraya, Yung Hsin Chen</strong></p>
<p>Magnetic Resonance Imaging (MRI) at lower field strengths (e.g., 3T) suffers from limited spatial resolution, making it challenging to capture fine anatomical details essential for clinical diagnosis and neuroimaging research. To overcome this limitation, we propose MoEDiff-SR, a Mixture of Experts (MoE)-guided diffusion model for region-adaptive MRI Super-Resolution (SR). Unlike conventional diffusion-based SR models that apply a uniform denoising process across the entire image, MoEDiff-SR dynamically selects specialized denoising experts at a fine-grained token level, ensuring region-specific adaptation and enhanced SR performance. Specifically, our approach first employs a Transformer-based feature extractor to compute multi-scale patch embeddings, capturing both global structural information and local texture details. The extracted feature embeddings are then fed into an MoE gating network, which assigns adaptive weights to multiple diffusion-based denoisers, each specializing in different brain MRI characteristics, such as centrum semiovale, sulcal and gyral cortex, and grey-white matter junction. The final output is produced by aggregating the denoised results from these specialized experts according to dynamically assigned gating probabilities. Experimental results demonstrate that MoEDiff-SR outperforms existing state-of-the-art methods in terms of quantitative image quality metrics, perceptual fidelity, and computational efficiency. Difference maps from each expert further highlight their distinct specializations, confirming the effective region-specific denoising capability and the interpretability of expert contributions. Additionally, clinical evaluation validates its superior diagnostic capability in identifying subtle pathological features, emphasizing its practical relevance in clinical neuroimaging. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ZWang78/MoEDiff-SR">https://github.com/ZWang78/MoEDiff-SR</a>. </p>
<blockquote>
<p>åœ¨ä½åœºå¼ºï¼ˆä¾‹å¦‚3Tï¼‰çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­ï¼Œç”±äºç©ºé—´åˆ†è¾¨ç‡æœ‰é™ï¼Œæ•è·å¯¹ä¸´åºŠè¯Šæ–­å’Œç¥ç»å½±åƒç ”ç©¶è‡³å…³é‡è¦çš„ç²¾ç»†è§£å‰–ç»†èŠ‚å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MoEDiff-SRï¼Œè¿™æ˜¯ä¸€ç§å—ä¸“å®¶æ··åˆï¼ˆMoEï¼‰å¼•å¯¼çš„åŒºåŸŸè‡ªé€‚åº”MRIè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ‰©æ•£æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ‰©æ•£çš„SRæ¨¡å‹ä¸åŒï¼Œåè€…åœ¨æ•´ä¸ªå›¾åƒä¸Šåº”ç”¨ç»Ÿä¸€çš„å»å™ªè¿‡ç¨‹ï¼ŒMoEDiff-SRåœ¨ç²¾ç»†çš„æ ‡è®°çº§åˆ«åŠ¨æ€é€‰æ‹©ä¸“ä¸šçš„å»å™ªä¸“å®¶ï¼Œç¡®ä¿åŒºåŸŸç‰¹å®šçš„é€‚åº”æ€§å’Œå¢å¼ºçš„SRæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé‡‡ç”¨åŸºäºTransformerçš„ç‰¹å¾æå–å™¨æ¥è®¡ç®—å¤šå°ºåº¦è¡¥ä¸åµŒå…¥ï¼Œæ•æ‰å…¨å±€ç»“æ„ä¿¡æ¯å’Œå±€éƒ¨çº¹ç†ç»†èŠ‚ã€‚æå–çš„ç‰¹å¾åµŒå…¥éšåè¢«è¾“å…¥åˆ°MoEé—¨æ§ç½‘ç»œä¸­ï¼Œè¯¥ç½‘ç»œä¸ºå¤šä¸ªåŸºäºæ‰©æ•£çš„å»å™ªå™¨åˆ†é…è‡ªé€‚åº”æƒé‡ï¼Œæ¯ä¸ªå»å™ªå™¨éƒ½ä¸“æ³¨äºä¸åŒçš„è„‘éƒ¨MRIç‰¹å¾ï¼Œä¾‹å¦‚åŠåµä¸­å¿ƒã€æ²Ÿå’Œå›çŠ¶çš®å±‚ä»¥åŠç°ç™½è´¨äº¤ç•Œå¤„ã€‚æœ€ç»ˆè¾“å‡ºæ˜¯é€šè¿‡æ ¹æ®åŠ¨æ€åˆ†é…çš„é—¨æ§æ¦‚ç‡èšåˆè¿™äº›ä¸“ä¸šä¸“å®¶çš„å»å™ªç»“æœè€Œäº§ç”Ÿçš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoEDiff-SRåœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ã€æ„ŸçŸ¥ä¿çœŸåº¦å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ¥è‡ªæ¯ä¸ªä¸“å®¶çš„å·®å¼‚æ˜ å°„è¿›ä¸€æ­¥çªå‡ºäº†å…¶ç‹¬ç‰¹çš„ä¸“ä¸šç‰¹ç‚¹ï¼Œè¯å®äº†å…¶åœ¨ç‰¹å®šåŒºåŸŸçš„å»å™ªèƒ½åŠ›å’Œä¸“å®¶è´¡çŒ®çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œä¸´åºŠè¯„ä¼°éªŒè¯äº†å…¶åœ¨è¯†åˆ«ç»†å¾®ç—…ç†ç‰¹å¾æ–¹é¢çš„ä¼˜è¶Šè¯Šæ–­èƒ½åŠ›ï¼Œå¼ºè°ƒå…¶åœ¨ä¸´åºŠç¥ç»å½±åƒä¸­çš„å®é™…ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/ZWang78/MoEDiff-SR%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/ZWang78/MoEDiff-SRæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07308v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MoEDiff-SRï¼Œä¸€ç§åŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„åŒºåŸŸè‡ªé€‚åº”MRIè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ä½åœºå¼ºMRIç©ºé—´åˆ†è¾¨ç‡æœ‰é™çš„æŒ‘æˆ˜ã€‚è¯¥æŠ€æœ¯é€šè¿‡åŠ¨æ€é€‰æ‹©ä¸“é—¨çš„å»å™ªä¸“å®¶ï¼Œåœ¨ç»†ç²’åº¦ä»¤ç‰Œçº§åˆ«è¿›è¡ŒåŒºåŸŸç‰¹å®šé€‚åº”ï¼Œç¡®ä¿SRæ€§èƒ½çš„æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoEDiff-SRåœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ã€æ„ŸçŸ¥ä¿çœŸåº¦å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œä¸´åºŠè¯„ä¼°éªŒè¯äº†å…¶åœ¨è¯†åˆ«ç»†å¾®ç—…ç†ç‰¹å¾æ–¹é¢çš„å“è¶Šè¯Šæ–­èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoEDiff-SRæ˜¯ä¸€ç§åŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåŒºåŸŸè‡ªé€‚åº”MRIè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ã€‚</li>
<li>ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹åœ¨æ•´ä¸ªå›¾åƒä¸Šåº”ç”¨å‡åŒ€å»å™ªè¿‡ç¨‹ï¼Œè€ŒMoEDiff-SRåˆ™é€šè¿‡åŠ¨æ€é€‰æ‹©å»å™ªä¸“å®¶è¿›è¡ŒåŒºåŸŸç‰¹å®šé€‚åº”ã€‚</li>
<li>MoEDiff-SRä½¿ç”¨åŸºäºTransformerçš„ç‰¹å¾æå–å™¨è®¡ç®—å¤šå°ºåº¦è¡¥ä¸åµŒå…¥ï¼Œæ•æ‰å…¨å±€ç»“æ„ä¿¡æ¯å’Œå±€éƒ¨çº¹ç†ç»†èŠ‚ã€‚</li>
<li>MoEé—¨æ§ç½‘ç»œæ ¹æ®å¤šä¸ªä¸“æ³¨äºä¸åŒå¤§è„‘MRIç‰¹æ€§çš„ä¸“ä¸šæ‰©æ•£å»å™ªå™¨ï¼ˆå¦‚ä¸­æ¢åŠåµåœ†ä¸­å¿ƒã€è„‘æ²Ÿå’Œè„‘å›çš®å±‚ã€ç°ç™½è´¨äº¤ç•Œå¤„ï¼‰è¿›è¡Œè‡ªé€‚åº”æƒé‡åˆ†é…ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMoEDiff-SRåœ¨å›¾åƒè´¨é‡ã€æ„ŸçŸ¥ä¿çœŸåº¦å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
<li>ä¸“å®¶å·®å¼‚å›¾çªå‡ºäº†å„è‡ªçš„ä¸“ä¸šç‰¹ç‚¹ï¼ŒéªŒè¯äº†åŒºåŸŸç‰¹å®šå»å™ªèƒ½åŠ›å’Œä¸“å®¶è´¡çŒ®çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-71d5e7d1e3acaa865308d79b8e12dad1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edaf1b9cdaa26826196657a69e225f67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f60971dcbce1ef3842695d5700d88422.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5376e986068f2477053c7d5ef8d9d2cd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Compass-Control-Multi-Object-Orientation-Control-for-Text-to-Image-Generation"><a href="#Compass-Control-Multi-Object-Orientation-Control-for-Text-to-Image-Generation" class="headerlink" title="Compass Control: Multi Object Orientation Control for Text-to-Image   Generation"></a>Compass Control: Multi Object Orientation Control for Text-to-Image   Generation</h2><p><strong>Authors:Rishubh Parihar, Vaibhav Agrawal, Sachidanand VS, R. Venkatesh Babu</strong></p>
<p>Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware \textbf{compass} tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study. </p>
<blockquote>
<p>ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ§åˆ¶æ–¹æ³•è™½ç„¶å¼ºå¤§ï¼Œä½†æ— æ³•å®ç°æ˜ç¡®çš„3Då¯¹è±¡ä¸­å¿ƒæ§åˆ¶ï¼Œä¾‹å¦‚æ— æ³•ç²¾ç¡®æ§åˆ¶å¯¹è±¡çš„æ–¹å‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„å¤šå¯¹è±¡æ–¹å‘æ§åˆ¶é—®é¢˜ã€‚è¿™èƒ½å¤Ÿå®ç°ä¸ºæ¯ä¸ªå¯¹è±¡æä¾›ç²¾ç¡®æ–¹å‘æ§åˆ¶çš„å¤šæ ·åŒ–å¤šå¯¹è±¡åœºæ™¯ç”Ÿæˆã€‚å…³é”®æ€æƒ³æ˜¯ä½¿ç”¨ä¸€ç»„æ–¹å‘æ„ŸçŸ¥çš„æŒ‡å—é’ˆä»¤ç‰Œï¼ˆä¸€ä¸ªç”¨äºæ¯ä¸ªå¯¹è±¡ï¼‰ä»¥åŠæ–‡æœ¬ä»¤ç‰Œæ¥è°ƒæ§æ‰©æ•£æ¨¡å‹ã€‚ä¸€ä¸ªè½»é‡çº§çš„ç¼–ç å™¨ç½‘ç»œä»¥å¯¹è±¡æ–¹å‘ä½œä¸ºè¾“å…¥æ¥é¢„æµ‹è¿™äº›æŒ‡å—é’ˆä»¤ç‰Œã€‚è¯¥æ¨¡å‹æ˜¯åœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç¨‹åºç”Ÿæˆçš„åœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯åŒ…å«ä¸€ä¸ªæˆ–ä¸¤ä¸ªä½äºå¹³é¢èƒŒæ™¯ä¸Šçš„3Dèµ„äº§ã€‚ç„¶è€Œï¼Œç›´æ¥è®­ç»ƒè¯¥æ¡†æ¶ä¼šå¯¼è‡´æ–¹å‘æ§åˆ¶ä¸ä½³ä»¥åŠå¯¹è±¡ä¹‹é—´çš„çº ç¼ ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œå¹²é¢„ï¼Œå¹¶é™åˆ¶æ¯ä¸ªæŒ‡å—é’ˆä»¤ç‰Œçš„è·¨æ³¨æ„åŠ›å›¾åˆ°å…¶å¯¹åº”çš„å¯¹è±¡åŒºåŸŸã€‚è®­ç»ƒåçš„æ¨¡å‹èƒ½å¤Ÿå¯¹ä»¥ä¸‹æ–¹é¢å®ç°ç²¾ç¡®çš„æ–¹å‘æ§åˆ¶ï¼ša)è®­ç»ƒä¸­æœªè§è¿‡çš„å¤æ‚å¯¹è±¡ï¼›b)å…·æœ‰è¶…è¿‡ä¸¤ä¸ªå¯¹è±¡çš„å¤šå¯¹è±¡åœºæ™¯ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå½“ä¸ä¸ªäººåŒ–æ–¹æ³•ç›¸ç»“åˆæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨å¤šç§ä¸Šä¸‹æ–‡ä¸­ç²¾ç¡®æ§åˆ¶æ–°å¯¹è±¡çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–¹å‘æ§åˆ¶å’Œæ–‡æœ¬å¯¹é½æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè¿™é€šè¿‡å¹¿æ³›çš„è¯„ä¼°å’Œä¸€é¡¹ç”¨æˆ·ç ”ç©¶å¾—åˆ°äº†é‡åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06752v2">PDF</a> CVPR 2025 Camera Ready. Project page:   <a target="_blank" rel="noopener" href="https://rishubhpar.github.io/compasscontrol">https://rishubhpar.github.io/compasscontrol</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„å¤šç›®æ ‡æ–¹å‘æ§åˆ¶é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥ä¸€ç»„é¢å‘æ–¹å‘æ€§çš„æŒ‡å—é’ˆæ ‡è®°å’Œè½»é‡çº§ç¼–ç å™¨ç½‘ç»œï¼Œå®ç°äº†æ¯ä¸ªå¯¹è±¡çš„ç²¾ç¡®æ–¹å‘æ§åˆ¶ï¼Œä»è€Œç”Ÿæˆäº†å¤šæ ·åŒ–çš„å¤šç›®æ ‡åœºæ™¯ã€‚æ¨¡å‹ç»è¿‡åˆæˆæ•°æ®é›†è®­ç»ƒï¼Œé€šè¿‡å¯¹ç”Ÿæˆè¿‡ç¨‹çš„å¹²é¢„å’Œçº¦æŸäº¤å‰æ³¨æ„åŠ›å›¾ï¼Œå®ç°äº†å¯¹å¤æ‚å¯¹è±¡å’Œæ–°åœºæ™¯ä¸­å¤šä¸ªå¯¹è±¡çš„ç²¾ç¡®æ–¹å‘æ§åˆ¶ã€‚ç»“åˆä¸ªæ€§åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶ä¸åŒèƒŒæ™¯ä¸‹çš„æ–°å¯¹è±¡çš„æ–¹å‘ã€‚æœ¬æ–‡çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ–¹å‘æ§åˆ¶å’Œæ–‡æœ¬å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æŒ‡å—é’ˆæ ‡è®°å®ç°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„å¤šç›®æ ‡æ–¹å‘æ§åˆ¶ã€‚</li>
<li>é€šè¿‡è½»é‡çº§ç¼–ç å™¨ç½‘ç»œé¢„æµ‹æŒ‡å—é’ˆæ ‡è®°ï¼Œä»¥æ§åˆ¶å¯¹è±¡æ–¹å‘ã€‚</li>
<li>æ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«å•ä¸ªæˆ–ä¸¤ä¸ª3Dèµ„äº§åœ¨çº¯èƒŒæ™¯ä¸Šçš„åœºæ™¯ã€‚</li>
<li>é€šè¿‡å¹²é¢„ç”Ÿæˆè¿‡ç¨‹å’Œçº¦æŸäº¤å‰æ³¨æ„åŠ›å›¾ï¼Œè§£å†³æ–¹å‘æ§åˆ¶ä¸å‡†ç¡®å’Œå¯¹è±¡çº ç¼ çš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹å®ç°å¯¹å¤æ‚å¯¹è±¡å’Œæ–°åœºæ™¯ä¸­å¤šä¸ªå¯¹è±¡çš„ç²¾ç¡®æ–¹å‘æ§åˆ¶ï¼Œå±•ç°å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç»“åˆä¸ªæ€§åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶ä¸åŒèƒŒæ™¯ä¸‹çš„æ–°å¯¹è±¡çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-92ca86d2f0c688aa115bb5bc56de10bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed35c1bd7ef2fb4aff5d0f637ed94416.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f3ab88cacb1f0cfbb2e3c791b4666a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7655129300f1112574d111e423eaa2a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-443723f016076811fcfe017589519781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a960d32b746050e6ce429b39cb55f81.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Hyperbolic-Diffusion-Recommender-Model"><a href="#Hyperbolic-Diffusion-Recommender-Model" class="headerlink" title="Hyperbolic Diffusion Recommender Model"></a>Hyperbolic Diffusion Recommender Model</h2><p><strong>Authors:Meng Yuan, Yutian Xiao, Wei Chen, Chu Zhao, Deqing Wang, Fuzhen Zhuang</strong></p>
<p>Diffusion models (DMs) have emerged as the new state-of-the-art family of deep generative models. To gain deeper insights into the limitations of diffusion models in recommender systems, we investigate the fundamental structural disparities between images and items. Consequently, items often exhibit distinct anisotropic and directional structures that are less prevalent in images. However, the traditional forward diffusion process continuously adds isotropic Gaussian noise, causing anisotropic signals to degrade into noise, which impairs the semantically meaningful representations in recommender systems.   Inspired by the advancements in hyperbolic spaces, we propose a novel \textit{\textbf{H}yperbolic} \textit{\textbf{D}iffusion} \textit{\textbf{R}ecommender} \textit{\textbf{M}odel} (named HDRM). Unlike existing directional diffusion methods based on Euclidean space, the intrinsic non-Euclidean structure of hyperbolic space makes it particularly well-adapted for handling anisotropic diffusion processes. In particular, we begin by formulating concepts to characterize latent directed diffusion processes within a geometrically grounded hyperbolic space. Subsequently, we propose a novel hyperbolic latent diffusion process specifically tailored for users and items. Drawing upon the natural geometric attributes of hyperbolic spaces, we impose structural restrictions on the space to enhance hyperbolic diffusion propagation, thereby ensuring the preservation of the intrinsic topology of user-item graphs. Extensive experiments on three benchmark datasets demonstrate the effectiveness of HDRM. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»æˆä¸ºæœ€æ–°çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹å®¶æ—ã€‚ä¸ºäº†æ›´æ·±å…¥åœ°äº†è§£æ‰©æ•£æ¨¡å‹åœ¨æ¨èç³»ç»Ÿä¸­çš„å±€é™æ€§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å›¾åƒå’Œé¡¹ç›®ä¹‹é—´æ ¹æœ¬çš„ç»“æ„æ€§å·®å¼‚ã€‚å› æ­¤ï¼Œé¡¹ç›®é€šå¸¸è¡¨ç°å‡ºä¸åŒçš„å„å‘å¼‚æ€§å’Œæ–¹å‘æ€§ç»“æ„ï¼Œè¿™äº›åœ¨å›¾åƒä¸­å¹¶ä¸å¸¸è§ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸æ–­æ·»åŠ å„å‘åŒæ€§çš„é«˜æ–¯å™ªå£°ï¼Œå¯¼è‡´å„å‘å¼‚æ€§çš„ä¿¡å·é™ä¸ºå™ªå£°ï¼Œä»è€ŒæŸå®³äº†æ¨èç³»ç»Ÿä¸­çš„è¯­ä¹‰è¡¨ç¤ºã€‚å—è¶…æ›²é¢ç©ºé—´å‘å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„<strong>Hyperbolic Diffusion Recommender Model</strong>ï¼Œç®€ç§°HDRMæ¨¡å‹ã€‚ä¸åŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„ç°æœ‰æ–¹å‘æ€§æ‰©æ•£æ–¹æ³•ä¸åŒï¼Œè¶…æ›²é¢ç©ºé—´çš„å†…åœ¨éæ¬§å‡ é‡Œå¾—ç»“æ„ä½¿å…¶ç‰¹åˆ«é€‚åˆäºå¤„ç†å„å‘å¼‚æ€§æ‰©æ•£è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ¶å®šæ¦‚å¿µï¼Œä»¥è¡¨å¾å‡ ä½•æ¥åœ°è¶…æ›²é¢ç©ºé—´å†…çš„æ½œåœ¨å®šå‘æ‰©æ•£è¿‡ç¨‹ã€‚éšåï¼Œæˆ‘ä»¬é’ˆå¯¹ç”¨æˆ·å’Œé¡¹ç›®æå‡ºäº†ä¸€ç§æ–°å‹çš„è¶…æ›²é¢æ½œåœ¨æ‰©æ•£è¿‡ç¨‹ã€‚åˆ©ç”¨è¶…æ›²é¢ç©ºé—´çš„è‡ªç„¶å‡ ä½•å±æ€§ï¼Œæˆ‘ä»¬å¯¹ç©ºé—´æ–½åŠ ç»“æ„æ€§é™åˆ¶ï¼Œä»¥å¢å¼ºè¶…æ›²é¢æ‰©æ•£ä¼ æ’­ï¼Œä»è€Œç¡®ä¿ç”¨æˆ·-é¡¹ç›®å›¾çš„å†…åœ¨æ‹“æ‰‘ç»“æ„å¾—ä»¥ä¿ç•™ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†HDRMæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01541v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­å¤„äºå‰æ²¿åœ°ä½ã€‚ä¸ºäº†åœ¨æ¨èç³»ç»Ÿä¸­æ›´æ·±å…¥åœ°äº†è§£æ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ï¼Œç ”ç©¶è€…å¯¹æ¯”äº†å›¾åƒå’Œç‰©å“ä¹‹é—´çš„åŸºæœ¬ç»“æ„å·®å¼‚ã€‚ç‰©å“å¸¸å…·æœ‰ç‹¬ç‰¹çš„å®šå‘ç»“æ„ï¼Œè€Œä¼ ç»Ÿçš„å‰å‘æ‰©æ•£è¿‡ç¨‹æ·»åŠ çš„æ˜¯åŒå‘é«˜æ–¯å™ªå£°ï¼Œè¿™ä¼šæŸå®³è¯­ä¹‰è¡¨ç¤ºã€‚å—åŒæ›²ç©ºé—´å‘å±•çš„å¯å‘ï¼Œæå‡ºäº†æ–°å‹çš„â€œè¶…æ‰©æ•£æ¨èæ¨¡å‹â€ï¼ˆHDRMï¼‰ã€‚ä¸åŒäºåŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å®šå‘æ‰©æ•£æ–¹æ³•ï¼ŒåŒæ›²ç©ºé—´çš„éæ¬§å‡ é‡Œå¾—ç»“æ„æ›´é€‚åˆå¤„ç†å®šå‘æ‰©æ•£è¿‡ç¨‹ã€‚HDRMåœ¨å‡ ä½•åŒæ›²ç©ºé—´ä¸­æè¿°äº†æ½œåœ¨å®šå‘æ‰©æ•£è¿‡ç¨‹çš„ç‰¹ç‚¹ï¼Œå¹¶é’ˆå¯¹ç”¨æˆ·å’Œç‰©å“æå‡ºäº†æ–°å‹çš„åŒæ›²æ½œåœ¨æ‰©æ•£è¿‡ç¨‹ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†HDRMçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­å æ®é¢†å…ˆåœ°ä½ï¼Œä½†åœ¨æ¨èç³»ç»Ÿä¸­æœ‰å±€é™æ€§ã€‚</li>
<li>ç‰©å“ä¸å›¾åƒåœ¨ç»“æ„ä¸Šæœ‰å·®å¼‚ï¼Œç‰©å“å¸¸å…·æœ‰ç‹¬ç‰¹çš„å®šå‘ç»“æ„ã€‚</li>
<li>ä¼ ç»Ÿçš„å‰å‘æ‰©æ•£è¿‡ç¨‹æ·»åŠ åŒå‘é«˜æ–¯å™ªå£°ï¼Œä¸åˆ©äºæ¨èç³»ç»Ÿä¸­çš„è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>å—åŒæ›²ç©ºé—´å‘å±•çš„å¯å‘ï¼Œæå‡ºäº†è¶…æ‰©æ•£æ¨èæ¨¡å‹ï¼ˆHDRMï¼‰ã€‚</li>
<li>åŒæ›²ç©ºé—´çš„éæ¬§å‡ é‡Œå¾—ç»“æ„æ›´é€‚åˆå¤„ç†å®šå‘æ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>HDRMåœ¨å‡ ä½•åŒæ›²ç©ºé—´ä¸­æè¿°äº†æ½œåœ¨å®šå‘æ‰©æ•£è¿‡ç¨‹çš„ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-218add44081c720e0bc6d289e0d79e49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a034830a34a924635f3c01e4457b7c32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-073a78127de13c115970844a7f441b4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63648dbc4450e4693f62bde20c41c6d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d02d8c83b2b62c8073e52087af21185c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47cdc74af82e620d90b9f5c914abbe1d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Image Augmentation Agent for Weakly Supervised Semantic Segmentation"></a>Image Augmentation Agent for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets. </p>
<blockquote>
<p>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰ä»…ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„WSSSæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è®¾è®¡æ–°çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°æ¥ç”Ÿæˆæ›´å‡†ç¡®çš„å¯†é›†æ ‡ç­¾ï¼Œå¿½è§†äº†å›ºå®šæ•°æ®é›†æ‰€å¸¦æ¥çš„é™åˆ¶ï¼Œè¿™äº›é™åˆ¶å¯èƒ½ä¼šé™åˆ¶æ€§èƒ½çš„æå‡ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæä¾›æ›´å¤šå¯è®­ç»ƒå›¾åƒçš„å¤šæ ·æ€§å¯ä»¥ä¸ºWSSSæä¾›æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå¹¶å¸®åŠ©æ¨¡å‹ç†è§£æ›´å…¨é¢çš„è¯­ä¹‰æ¨¡å¼ã€‚å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºå›¾åƒå¢å¼ºä»£ç†ï¼ˆIAAï¼‰ï¼Œå®ƒè¡¨æ˜ä»æ•°æ®ç”Ÿæˆçš„è§’åº¦å¢å¼ºWSSSæ˜¯å¯èƒ½çš„ã€‚IAAä¸»è¦è®¾è®¡äº†ä¸€ä¸ªå¢å¼ºä»£ç†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹è‡ªåŠ¨ä¸ºWSSSç”Ÿæˆé¢å¤–çš„å›¾åƒã€‚åœ¨å®è·µä¸­ï¼Œä¸ºäº†è§£å†³LLMæç¤ºç”Ÿæˆä¸­çš„ä¸ç¨³å®šé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æç¤ºè‡ªæˆ‘ä¼˜åŒ–æœºåˆ¶ã€‚å®ƒå…è®¸LLMé‡æ–°è¯„ä¼°ç”Ÿæˆçš„æç¤ºçš„åˆç†æ€§ï¼Œä»¥äº§ç”Ÿæ›´è¿è´¯çš„æç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­æ’å…¥äº†ä¸€ä¸ªåœ¨çº¿è¿‡æ»¤å™¨ï¼Œä»¥åŠ¨æ€ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„WSSSæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20439v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åä¸ºImage Augmentation Agentï¼ˆIAAï¼‰çš„æ–¹æ³•ï¼Œä»æ•°æ®ç”Ÿæˆçš„è§’åº¦æå‡å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé¢å¤–çš„å›¾åƒï¼Œå¢å¼ºWSSSçš„è®­ç»ƒå›¾åƒå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œä¸ºè§£å†³LLMsåœ¨ç”Ÿæˆæç¤ºæ—¶çš„ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§æç¤ºè‡ªæˆ‘ä¼˜åŒ–æœºåˆ¶ï¼Œå¹¶é€šè¿‡åœ¨çº¿è¿‡æ»¤å™¨ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¹³è¡¡æ€§ã€‚åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„WSSSæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WSSSæ–¹æ³•ä¸»è¦å…³æ³¨è®¾è®¡æ–°çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ä»¥ç”Ÿæˆæ›´å‡†ç¡®çš„å¯†é›†æ ‡ç­¾ï¼Œä½†å›ºå®šæ•°æ®é›†çš„å±€é™æ€§é™åˆ¶äº†æ€§èƒ½æå‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºImage Augmentation Agentï¼ˆIAAï¼‰çš„æ–°æ–¹æ³•ï¼Œä»æ•°æ®ç”Ÿæˆè§’åº¦æå‡WSSSæ€§èƒ½ã€‚</li>
<li>IAAåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé¢å¤–å›¾åƒï¼Œå¢åŠ è®­ç»ƒå›¾åƒçš„å¤šæ ·æ€§ã€‚</li>
<li>è®¾è®¡äº†æç¤ºè‡ªæˆ‘ä¼˜åŒ–æœºåˆ¶ï¼Œè§£å†³LLMsåœ¨ç”Ÿæˆæç¤ºæ—¶çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡åœ¨çº¿è¿‡æ»¤å™¨ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¹³è¡¡æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒIAAæ–¹æ³•åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„WSSSæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2faf4e44c136b1ea2e4916ea7b41f252.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c828c78f63c0bce584112a1fab7d7695.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Distillation-of-Discrete-Diffusion-through-Dimensional-Correlations"><a href="#Distillation-of-Discrete-Diffusion-through-Dimensional-Correlations" class="headerlink" title="Distillation of Discrete Diffusion through Dimensional Correlations"></a>Distillation of Discrete Diffusion through Dimensional Correlations</h2><p><strong>Authors:Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji</strong></p>
<p>Diffusion models have demonstrated exceptional performances in various fields of generative modeling, but suffer from slow sampling speed due to their iterative nature. While this issue is being addressed in continuous domains, discrete diffusion models face unique challenges, particularly in capturing dependencies between elements (e.g., pixel relationships in image, sequential dependencies in language) mainly due to the computational cost of processing high-dimensional joint distributions. In this paper, (i) we propose â€œmixtureâ€ models for discrete diffusion that are capable of treating dimensional correlations while remaining scalable, and (ii) we provide a set of loss functions for distilling the iterations of existing models. Two primary theoretical insights underpin our approach: First, conventional models with element-wise independence can well approximate the data distribution, but essentially require {\it many sampling steps}. Second, our loss functions enable the mixture models to distill such many-step conventional models into just a few steps by learning the dimensional correlations. Our experimental results show the effectiveness of the proposed method in distilling pretrained discrete diffusion models across image and language domains. The code used in the paper is available at <a target="_blank" rel="noopener" href="https://github.com/sony/di4c">https://github.com/sony/di4c</a> . </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡çš„å„ä¸ªé¢†åŸŸéƒ½è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶è¿­ä»£æ€§è´¨ï¼Œé‡‡æ ·é€Ÿåº¦è¾ƒæ…¢ã€‚è™½ç„¶è¿™ä¸ªé—®é¢˜æ­£åœ¨è¿ç»­åŸŸä¸­å¾—åˆ°è§£å†³ï¼Œä½†ç¦»æ•£æ‰©æ•£æ¨¡å‹é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•è·å…ƒç´ ä¹‹é—´çš„ä¾èµ–å…³ç³»æ–¹é¢ï¼ˆä¾‹å¦‚ï¼Œå›¾åƒä¸­çš„åƒç´ å…³ç³»ï¼Œè¯­è¨€ä¸­çš„åºåˆ—ä¾èµ–ï¼‰ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå¤„ç†é«˜ç»´è”åˆåˆ†å¸ƒçš„è®¡ç®—æˆæœ¬é€ æˆçš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œï¼ˆiï¼‰æˆ‘ä»¬ä¸ºç¦»æ•£æ‰©æ•£æå‡ºäº†â€œæ··åˆâ€æ¨¡å‹ï¼Œè¿™ç§æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†ç»´åº¦å…³è”çš„åŒæ—¶ä¿æŒå¯æ‰©å±•æ€§ï¼›ï¼ˆiiï¼‰æˆ‘ä»¬ä¸ºç°æœ‰æ¨¡å‹çš„è¿­ä»£æä¾›äº†ä¸€ç»„æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¸¤ä¸ªä¸»è¦çš„ç†è®ºè§è§£ï¼šé¦–å…ˆï¼Œå…·æœ‰å…ƒç´ ç‹¬ç«‹æ€§çš„ä¼ ç»Ÿæ¨¡å‹å¯ä»¥å¾ˆå¥½åœ°è¿‘ä¼¼æ•°æ®åˆ†å¸ƒï¼Œä½†åŸºæœ¬ä¸Šéœ€è¦å¤šæ¬¡é‡‡æ ·æ­¥éª¤ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°ä½¿æ··åˆæ¨¡å‹èƒ½å¤Ÿé€šè¿‡å­¦ä¹ ç»´åº¦ç›¸å…³æ€§ï¼Œå°†å¤šæ¬¡æ­¥éª¤çš„ä¼ ç»Ÿæ¨¡å‹æç‚¼ä¸ºä»…å‡ æ­¥ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè¯­è¨€é¢†åŸŸçš„é¢„è®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹æç‚¼ä¸­éå¸¸æœ‰æ•ˆã€‚è®ºæ–‡ä¸­ä½¿ç”¨çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sony/di4c">https://github.com/sony/di4c</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08709v3">PDF</a> 39 pages, GitHub link added</p>
<p><strong>æ‘˜è¦</strong><br>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡çš„å„ä¸ªé¢†åŸŸéƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶è¿­ä»£æ€§è´¨ï¼Œé‡‡æ ·é€Ÿåº¦è¾ƒæ…¢ã€‚è™½ç„¶è¿ç»­åŸŸçš„é—®é¢˜æ­£åœ¨å¾—åˆ°è§£å†³ï¼Œç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨æ•æ‰å…ƒç´ é—´ä¾èµ–å…³ç³»æ–¹é¢é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œä¾‹å¦‚åœ¨å›¾åƒä¸­çš„åƒç´ å…³ç³»å’Œè¯­è¨€ä¸­çš„åºåˆ—ä¾èµ–å…³ç³»ï¼Œè¿™ä¸»è¦å½’å› äºå¤„ç†é«˜ç»´è”åˆåˆ†å¸ƒçš„è®¡ç®—æˆæœ¬ã€‚æœ¬æ–‡æå‡ºæ··åˆæ¨¡å‹æ¥å¤„ç†ç¦»æ•£æ‰©æ•£çš„ç»´åº¦ç›¸å…³æ€§ï¼ŒåŒæ—¶ä¿æŒå¯æ‰©å±•æ€§ï¼Œå¹¶æä¾›ä¸€å¥—æŸå¤±å‡½æ•°æ¥æç‚¼ç°æœ‰æ¨¡å‹çš„è¿­ä»£è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¸¤ä¸ªä¸»è¦ç†è®ºè§è§£ï¼šä¸€æ˜¯ä¼ ç»Ÿå…ƒç´ ç‹¬ç«‹æ¨¡å‹å¯ä»¥å¾ˆå¥½åœ°è¿‘ä¼¼æ•°æ®åˆ†å¸ƒï¼Œä½†éœ€è¦å¤šæ¬¡é‡‡æ ·æ­¥éª¤ï¼›äºŒæ˜¯æˆ‘ä»¬çš„æŸå¤±å‡½æ•°ä½¿æ··åˆæ¨¡å‹èƒ½å¤Ÿé€šè¿‡å­¦ä¹ ç»´åº¦ç›¸å…³æ€§ï¼Œå°†å¤šæ¬¡æ­¥éª¤çš„ä¼ ç»Ÿæ¨¡å‹æç‚¼ä¸ºå°‘æ•°æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè¯­è¨€é¢†åŸŸçš„é¢„è®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹æç‚¼ä¸­æ•ˆæœæ˜¾è‘—ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sony/di4c%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sony/di4cæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡çš„å¤šä¸ªé¢†åŸŸè¡¨ç°ä¼˜ç§€ï¼Œä½†é‡‡æ ·é€Ÿåº¦æ…¢ã€‚</li>
<li>ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨æ•æ‰å…ƒç´ é—´ä¾èµ–å…³ç³»æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é«˜ç»´è”åˆåˆ†å¸ƒçš„å¤„ç†æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æœ¬æ–‡æå‡ºæ··åˆæ¨¡å‹å¤„ç†ç¦»æ•£æ‰©æ•£çš„ç»´åº¦ç›¸å…³æ€§ï¼Œå…¼é¡¾æ€§èƒ½ä¸å¯æ‰©å±•æ€§ã€‚</li>
<li>å¼•å…¥æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç°æœ‰æ¨¡å‹çš„è¿­ä»£è¿‡ç¨‹ï¼Œå‡å°‘é‡‡æ ·æ­¥éª¤ã€‚</li>
<li>æ–¹æ³•åŸºäºä¸¤ä¸ªç†è®ºè§è§£ï¼šä¼ ç»Ÿæ¨¡å‹éœ€è¦å¤šæ¬¡é‡‡æ ·æ­¥éª¤ï¼Œè€Œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°èƒ½å‡å°‘è¿™äº›æ­¥éª¤ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè¯­è¨€é¢†åŸŸçš„é¢„è®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹æç‚¼ä¸­æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>ç›¸å…³ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08709">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80da2d844462b74161fec54f166de7ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89fcb9e12870d5bcbd379668b5ca3a21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-270784a0ea044f39f25e5e8ec0eb4189.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="KnobGen-Controlling-the-Sophistication-of-Artwork-in-Sketch-Based-Diffusion-Models"><a href="#KnobGen-Controlling-the-Sophistication-of-Artwork-in-Sketch-Based-Diffusion-Models" class="headerlink" title="KnobGen: Controlling the Sophistication of Artwork in Sketch-Based   Diffusion Models"></a>KnobGen: Controlling the Sophistication of Artwork in Sketch-Based   Diffusion Models</h2><p><strong>Authors:Pouyan Navard, Amin Karimi Monsefi, Mengxi Zhou, Wei-Lun Chao, Alper Yilmaz, Rajiv Ramnath</strong></p>
<p>Recent advances in diffusion models have significantly improved text-to-image (T2I) generation, but they often struggle to balance fine-grained precision with high-level control. Methods like ControlNet and T2I-Adapter excel at following sketches by seasoned artists but tend to be overly rigid, replicating unintentional flaws in sketches from novice users. Meanwhile, coarse-grained methods, such as sketch-based abstraction frameworks, offer more accessible input handling but lack the precise control needed for detailed, professional use. To address these limitations, we propose KnobGen, a dual-pathway framework that democratizes sketch-based image generation by seamlessly adapting to varying levels of sketch complexity and user skill. KnobGen uses a Coarse-Grained Controller (CGC) module for high-level semantics and a Fine-Grained Controller (FGC) module for detailed refinement. The relative strength of these two modules can be adjusted through our knob inference mechanism to align with the userâ€™s specific needs. These mechanisms ensure that KnobGen can flexibly generate images from both novice sketches and those drawn by seasoned artists. This maintains control over the final output while preserving the natural appearance of the image, as evidenced on the MultiGen-20M dataset and a newly collected sketch dataset. </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„è¿›å±•æå¤§åœ°æ¨åŠ¨äº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”ŸæˆæŠ€æœ¯çš„å‘å±•ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨å¹³è¡¡ç²¾ç»†ç²’åº¦çš„ç²¾åº¦ä¸é«˜çº§æ§åˆ¶æ–¹é¢å¸¸é‡åˆ°å›°éš¾ã€‚ä¾‹å¦‚ï¼ŒControlNetå’ŒT2I-Adapterè¿™ç±»æ–¹æ³•æ“…é•¿éµå¾ªèµ„æ·±è‰ºæœ¯å®¶çš„è‰å›¾ï¼Œä½†å®ƒä»¬å¾€å¾€è¿‡äºåƒµåŒ–ï¼Œä¼šå¤åˆ¶æ–°æ‰‹ç”¨æˆ·è‰å›¾ä¸­çš„æ— æ„ç¼ºé™·ã€‚åŒæ—¶ï¼ŒåŸºäºç²—ç•¥è‰å›¾çš„æ–¹æ³•ï¼Œå¦‚åŸºäºè‰å›¾æŠ½è±¡æ¡†æ¶çš„æ–¹æ³•ï¼Œè™½ç„¶æä¾›äº†æ›´æ˜“äºå¤„ç†çš„è¾“å…¥ï¼Œä½†å´ç¼ºä¹ç”¨äºä¸“ä¸šç”¨é€”æ‰€éœ€çš„ç²¾ç¡®æ§åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†KnobGenâ€”â€”ä¸€ä¸ªåŒè·¯å¾„æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ— ç¼é€‚åº”ä¸åŒçº§åˆ«çš„è‰å›¾å¤æ‚æ€§å’Œç”¨æˆ·æŠ€èƒ½æ¥å®ç°åŸºäºè‰å›¾çš„å›¾åƒç”Ÿæˆçš„æ°‘ä¸»åŒ–ã€‚KnobGenä½¿ç”¨ç²—ç²’åº¦æ§åˆ¶å™¨ï¼ˆCGCï¼‰æ¨¡å—ç”¨äºé«˜çº§è¯­ä¹‰å’Œç»†ç²’åº¦æ§åˆ¶å™¨ï¼ˆFGCï¼‰æ¨¡å—ç”¨äºè¯¦ç»†è°ƒæ•´ã€‚è¿™ä¸¤ç§æ¨¡å—çš„ç›¸å¯¹å¼ºåº¦å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„æ—‹é’®æ¨æ–­æœºåˆ¶è¿›è¡Œè°ƒæ•´ï¼Œä»¥ç¬¦åˆç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚ã€‚è¿™äº›æœºåˆ¶ç¡®ä¿äº†KnobGenèƒ½å¤Ÿçµæ´»åœ°ç”Ÿæˆä»æ–°æ‰‹è‰å›¾åˆ°èµ„æ·±è‰ºæœ¯å®¶æ‰€ç»˜è‰å›¾ç”Ÿæˆçš„å›¾åƒã€‚è¿™ä¿æŒäº†å¯¹æœ€ç»ˆè¾“å‡ºçš„æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒçš„è‡ªç„¶å¤–è§‚ï¼Œè¿™åœ¨MultiGen-20Mæ•°æ®é›†å’Œæ–°æ”¶é›†çš„è‰å›¾æ•°æ®é›†ä¸Šå¾—åˆ°äº†è¯å®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01595v3">PDF</a> Accepted to CVPR 2025 Workshop on CVEU</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ–¹é¢çš„æœ€æ–°è¿›å±•å·²åœ¨æé«˜ç²¾ç»†ç²’åº¦ç²¾åº¦ä¸é«˜çº§æ§åˆ¶ä¹‹é—´è¾¾åˆ°å¹³è¡¡ã€‚æå‡ºçš„KnobGenæ¡†æ¶é€šè¿‡åŒè·¯å¾„è®¾è®¡ï¼Œå®ç°äº†å¯¹ä¸åŒå¤æ‚åº¦è‰å›¾å’Œç”¨æˆ·æŠ€èƒ½çš„é€‚åº”æ€§è°ƒæ•´ã€‚è¯¥æ¡†æ¶åŒ…å«ç²—ç²’åº¦æ§åˆ¶å™¨å’Œç»†ç²’åº¦æ§åˆ¶å™¨æ¨¡å—ï¼Œé€šè¿‡æ—‹é’®æ¨ç†æœºåˆ¶è°ƒæ•´ä¸¤è€…çš„ç›¸å¯¹å¼ºåº¦ï¼Œä»¥æ»¡è¶³ç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚ã€‚ç¡®ä¿ç”Ÿæˆå›¾åƒæ—¢ç¬¦åˆæ–°æ‰‹è‰å›¾ï¼Œåˆèƒ½é€‚åº”ä¸“ä¸šè‰ºæœ¯å®¶çš„ç»˜å›¾ï¼Œä»è€Œåœ¨æ§åˆ¶æœ€ç»ˆè¾“å‡ºå’Œä¿æŒå›¾åƒè‡ªç„¶å¤–è§‚ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†éœ€å¹³è¡¡ç²¾ç»†ç²’åº¦ç²¾åº¦ä¸é«˜çº§æ§åˆ¶ã€‚</li>
<li>KnobGenæ¡†æ¶é€šè¿‡åŒè·¯å¾„è®¾è®¡åº”å¯¹ä¸åŒå¤æ‚åº¦è‰å›¾å’Œç”¨æˆ·æŠ€èƒ½çš„å·®å¼‚ã€‚</li>
<li>KnobGenåŒ…å«ç²—ç²’åº¦æ§åˆ¶å™¨å’Œç»†ç²’åº¦æ§åˆ¶å™¨æ¨¡å—ï¼Œåˆ†åˆ«è´Ÿè´£é«˜çº§è¯­ä¹‰å’Œè¯¦ç»†ä¿®é¥°ã€‚</li>
<li>é€šè¿‡æ—‹é’®æ¨ç†æœºåˆ¶è°ƒæ•´ä¸¤ä¸ªæ§åˆ¶å™¨çš„ç›¸å¯¹å¼ºåº¦ã€‚</li>
<li>KnobGenèƒ½å¤Ÿé€‚åº”æ–°æ‰‹å’Œä¸“ä¸šè‰ºæœ¯å®¶çš„è‰å›¾ï¼Œç”Ÿæˆç›¸åº”çš„å›¾åƒã€‚</li>
<li>KnobGenåœ¨ä¿æŒå›¾åƒè‡ªç„¶å¤–è§‚çš„åŒæ—¶ï¼Œæä¾›äº†å¯¹æœ€ç»ˆè¾“å‡ºçš„ç²¾ç»†æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01595">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4ae4b3aae90339e026195c1e33143f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-938fa263c37bd8559c410df677d8c454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a90781afc64af8af4bac187139373fa8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0246c35b3101917ea6e58b39ff36caf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51c1449a725045e65e8d1ac584b60d71.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Neural-Approximate-Mirror-Maps-for-Constrained-Diffusion-Models"><a href="#Neural-Approximate-Mirror-Maps-for-Constrained-Diffusion-Models" class="headerlink" title="Neural Approximate Mirror Maps for Constrained Diffusion Models"></a>Neural Approximate Mirror Maps for Constrained Diffusion Models</h2><p><strong>Authors:Berthy T. Feng, Ricardo Baptista, Katherine L. Bouman</strong></p>
<p>Diffusion models excel at creating visually-convincing images, but they often struggle to meet subtle constraints inherent in the training data. Such constraints could be physics-based (e.g., satisfying a PDE), geometric (e.g., respecting symmetry), or semantic (e.g., including a particular number of objects). When the training data all satisfy a certain constraint, enforcing this constraint on a diffusion model makes it more reliable for generating valid synthetic data and solving constrained inverse problems. However, existing methods for constrained diffusion models are restricted in the constraints they can handle. For instance, recent work proposed to learn mirror diffusion models (MDMs), but analytical mirror maps only exist for convex constraints and can be challenging to derive. We propose neural approximate mirror maps (NAMMs) for general, possibly non-convex constraints. Our approach only requires a differentiable distance function from the constraint set. We learn an approximate mirror map that transforms data into an unconstrained space and a corresponding approximate inverse that maps data back to the constraint set. A generative model, such as an MDM, can then be trained in the learned mirror space and its samples restored to the constraint set by the inverse map. We validate our approach on a variety of constraints, showing that compared to an unconstrained diffusion model, a NAMM-based MDM substantially improves constraint satisfaction. We also demonstrate how existing diffusion-based inverse-problem solvers can be easily applied in the learned mirror space to solve constrained inverse problems. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè§†è§‰ä¸Šæœ‰è¯´æœåŠ›çš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ»¡è¶³è®­ç»ƒæ•°æ®ä¸­çš„å¾®å¦™çº¦æŸæ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚è¿™äº›çº¦æŸå¯èƒ½æ˜¯åŸºäºç‰©ç†çš„ï¼ˆä¾‹å¦‚ï¼Œæ»¡è¶³åå¾®åˆ†æ–¹ç¨‹ï¼‰ã€å‡ ä½•çš„ï¼ˆä¾‹å¦‚ï¼Œä¿æŒå¯¹ç§°æ€§ï¼‰æˆ–è¯­ä¹‰çš„ï¼ˆä¾‹å¦‚ï¼ŒåŒ…å«ç‰¹å®šæ•°é‡çš„å¯¹è±¡ï¼‰ã€‚å½“è®­ç»ƒæ•°æ®éƒ½æ»¡è¶³æŸä¸ªçº¦æŸæ—¶ï¼Œå¯¹æ‰©æ•£æ¨¡å‹å¼ºåˆ¶å®æ–½æ­¤çº¦æŸä½¿å…¶ç”Ÿæˆæœ‰æ•ˆåˆæˆæ•°æ®å’Œè§£å†³çº¦æŸé€†é—®é¢˜æ›´åŠ å¯é ã€‚ç„¶è€Œï¼Œç°æœ‰çš„çº¦æŸæ‰©æ•£æ¨¡å‹æ–¹æ³•åœ¨å¤„ç†çº¦æŸæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¾‹å¦‚ï¼Œæœ€è¿‘çš„å·¥ä½œæå‡ºäº†å­¦ä¹ é•œåƒæ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰ï¼Œä½†è§£æé•œåƒæ˜ å°„åªå­˜åœ¨äºå‡¸çº¦æŸä¸­ï¼Œå¹¶ä¸”æ¨å¯¼å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬é’ˆå¯¹ä¸€èˆ¬ã€å¯èƒ½æ˜¯éå‡¸çº¦æŸï¼Œæå‡ºäº†ç¥ç»è¿‘ä¼¼é•œåƒæ˜ å°„ï¼ˆNAMMsï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦ä¸€ä¸ªå¯åŒºåˆ†çš„è·ç¦»å‡½æ•°æ¥è‡ªçº¦æŸé›†ã€‚æˆ‘ä»¬å­¦ä¹ ä¸€ä¸ªå°†æ•°æ®è½¬æ¢åˆ°æ— çº¦æŸç©ºé—´çš„è¿‘ä¼¼é•œåƒæ˜ å°„ï¼Œä»¥åŠä¸€ä¸ªç›¸åº”çš„è¿‘ä¼¼é€†æ˜ å°„å°†æ•°æ®è¿”å›åˆ°çº¦æŸé›†ã€‚ç„¶åï¼Œå¯ä»¥åœ¨å­¦ä¹ çš„é•œåƒç©ºé—´ä¸­è®­ç»ƒå¦‚MDMè¿™æ ·çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶é€šè¿‡é€†æ˜ å°„å°†å…¶æ ·æœ¬æ¢å¤åˆ°çº¦æŸé›†ã€‚æˆ‘ä»¬é€šè¿‡å¤šç§çº¦æŸéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œä¸æ— çº¦æŸçš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼ŒåŸºäºNAMMçš„MDMåœ¨çº¦æŸæ»¡è¶³æ–¹é¢æœ‰äº†å®è´¨æ€§çš„æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜æ¼”ç¤ºäº†å¦‚ä½•åœ¨å­¦ä¹ çš„é•œåƒç©ºé—´ä¸­è½»æ¾åº”ç”¨ç°æœ‰çš„åŸºäºæ‰©æ•£çš„é€†é—®é¢˜æ±‚è§£å™¨æ¥è§£å†³çº¦æŸé€†é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12816v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»è¿‘ä¼¼é•œåƒæ˜ å°„ï¼ˆNAMMsï¼‰å¤„ç†å„ç±»çº¦æŸï¼ŒåŒ…æ‹¬ç‰©ç†ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸã€‚è¯¥æ–¹æ³•åœ¨çº¦æŸæ»¡è¶³çš„è®­ç»ƒæ•°æ®åŸºç¡€ä¸Šï¼Œæé«˜æ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ‰æ•ˆåˆæˆæ•°æ®å’Œè§£å†³çº¦æŸé€†é—®é¢˜çš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨é•œåƒç©ºé—´å­¦ä¹ æ˜ å°„ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½åœ¨è¯¥ç©ºé—´è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡é€†å‘æ˜ å°„å°†æ•°æ®æ¢å¤åˆ°çº¦æŸé›†å†…ã€‚å®éªŒè¯æ˜ï¼Œæ­¤æ–¹æ³•æ˜¾è‘—æé«˜äº†çº¦æŸæ»¡è¶³çš„ç¨‹åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè§†è§‰ä¸Šä»¤äººä¿¡æœçš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ»¡è¶³è®­ç»ƒæ•°æ®ä¸­çš„å¾®å¦™çº¦æŸæ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ã€‚</li>
<li>å½“è®­ç»ƒæ•°æ®æ»¡è¶³ç‰¹å®šçº¦æŸæ—¶ï¼Œå¯¹æ‰©æ•£æ¨¡å‹å¼ºåˆ¶æ‰§è¡Œæ­¤çº¦æŸå¯æé«˜å…¶ç”Ÿæˆæœ‰æ•ˆåˆæˆæ•°æ®å’Œè§£å†³çº¦æŸé€†é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰å¤„ç†çº¦æŸçš„æ‰©æ•£æ¨¡å‹æ–¹æ³•å—åˆ°é™åˆ¶ï¼Œåªèƒ½å¤„ç†ç‰¹å®šç±»å‹çš„çº¦æŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”ç¥ç»è¿‘ä¼¼é•œåƒæ˜ å°„ï¼ˆNAMMsï¼‰ï¼Œç”¨äºå¤„ç†ä¸€èˆ¬ã€å¯èƒ½æ˜¯éå‡¸çš„çº¦æŸã€‚</li>
<li>è¯¥æ–¹æ³•åªéœ€è¦ä¸€ä¸ªå¯åŒºåˆ†çš„è·ç¦»å‡½æ•°æ¥è‡ªçº¦æŸé›†ã€‚</li>
<li>é€šè¿‡å­¦ä¹ å°†æ•°æ®è½¬æ¢ä¸ºæ— çº¦æŸç©ºé—´ï¼Œå¹¶å­¦ä¹ ç›¸åº”çš„é€†å‘æ˜ å°„å°†æ•°æ®è¿”å›åˆ°çº¦æŸé›†ï¼Œå®ç°äº†åœ¨é•œåƒç©ºé—´ä¸­çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e894ed9683cd79159d6e42d77bd75ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9ddade93bf07677281e97205daa6e1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-660f0c4479603682c6f10cca5476b9f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bdf21850c3bf7ca93b999c53bf0d0cd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Balancing-Act-Distribution-Guided-Debiasing-in-Diffusion-Models"><a href="#Balancing-Act-Distribution-Guided-Debiasing-in-Diffusion-Models" class="headerlink" title="Balancing Act: Distribution-Guided Debiasing in Diffusion Models"></a>Balancing Act: Distribution-Guided Debiasing in Diffusion Models</h2><p><strong>Authors:Rishubh Parihar, Abhijnya Bhat, Abhipsa Basu, Saswat Mallick, Jogendra Nath Kundu, R. Venkatesh Babu</strong></p>
<p>Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single&#x2F;multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰å‰æ‰€æœªæœ‰çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹å¹¿æ³›åº”ç”¨äºæ•°æ®å¢å¼ºå’Œåˆ›æ„åº”ç”¨ã€‚ç„¶è€Œï¼ŒDMsä¼šåæ˜ å‡ºè®­ç»ƒæ•°æ®é›†å­˜åœ¨çš„åè§ã€‚åœ¨äººè„¸çš„èƒŒæ™¯ä¸‹ï¼ŒDMæ›´å€¾å‘äºæŸä¸€äººå£ç»Ÿè®¡äºšç»„è€Œéå…¶ä»–äºšç»„ï¼ˆä¾‹å¦‚ï¼Œå¥³æ€§ä¸ç”·æ€§ï¼‰ï¼Œè¿™ç‰¹åˆ«ä»¤äººæ‹…å¿§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸éœ€è¦é¢å¤–æ•°æ®æˆ–æ¨¡å‹é‡æ–°è®­ç»ƒçš„æ–¹æ³•æ¥å¯¹DMsè¿›è¡Œå»åå¤„ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å¸ƒæŒ‡å¯¼ï¼ˆDistribution Guidanceï¼‰çš„æ–¹æ³•ï¼Œå¼ºåˆ¶ç”Ÿæˆçš„å›¾åƒéµå¾ªè§„å®šçš„å±æ€§åˆ†å¸ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å»ºç«‹åœ¨è¿™æ ·ä¸€ä¸ªå…³é”®è§è§£ä¹‹ä¸Šï¼šå»å™ªUNetçš„æ½œåœ¨ç‰¹å¾åŒ…å«ä¸°å¯Œçš„äººå£ç»Ÿè®¡è¯­ä¹‰ï¼Œå¯ä»¥è¢«ç”¨æ¥å¼•å¯¼å»åç”Ÿæˆã€‚æˆ‘ä»¬è®­ç»ƒäº†å±æ€§åˆ†å¸ƒé¢„æµ‹å™¨ï¼ˆADPï¼‰â€”â€”ä¸€ä¸ªå¯ä»¥å°†æ½œåœ¨ç‰¹å¾æ˜ å°„åˆ°å±æ€§åˆ†å¸ƒçš„å°å‹å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆmlpï¼‰ã€‚ADPä½¿ç”¨æ¥è‡ªç°æœ‰å±æ€§åˆ†ç±»å™¨çš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚æå‡ºçš„å¸¦æœ‰ADPçš„åˆ†å¸ƒæŒ‡å¯¼ä½¿æˆ‘ä»¬èƒ½å¤Ÿå®ç°å…¬å¹³ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é™ä½äº†å•ä¸€&#x2F;å¤šä¸ªå±æ€§ä¸Šçš„åè§ï¼Œå¹¶ä¸”åœ¨æ— æ¡ä»¶å’Œæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸Šéƒ½å¤§å¹…è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†é€šè¿‡åˆ©ç”¨æˆ‘ä»¬çš„ç”Ÿæˆæ•°æ®å¯¹è®­ç»ƒé›†è¿›è¡Œé‡æ–°å¹³è¡¡æ¥è®­ç»ƒå…¬å¹³å±æ€§åˆ†ç±»å™¨çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.18206v4">PDF</a> CVPR 2024. Project Page : <a target="_blank" rel="noopener" href="https://ab-34.github.io/balancing_act/">https://ab-34.github.io/balancing_act/</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰å‰æ‰€æœªæœ‰çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå¹¿æ³›åº”ç”¨äºæ•°æ®å¢å¼ºå’Œåˆ›æ„åº”ç”¨ã€‚ç„¶è€Œï¼ŒDMsä¼šåæ˜ å‡ºè®­ç»ƒæ•°æ®é›†ä¸­çš„åè§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨æ•°æ®ä¸Šï¼Œæ›´å€¾å‘äºæŸä¸€äººç¾¤ï¼ˆå¦‚å¥³æ€§ä¸ç”·æ€§ï¼‰ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€é¢å¤–æ•°æ®æˆ–æ¨¡å‹é‡è®­çš„å»é™¤DMåè§çš„æ–¹æ³•â€”â€”åˆ†å¸ƒæŒ‡å¯¼ã€‚æˆ‘ä»¬åˆ©ç”¨å»å™ªUNetçš„æ½œåœ¨ç‰¹å¾å…·æœ‰ä¸°å¯Œçš„ç§æ—è¯­ä¹‰è¿™ä¸€å…³é”®è§è§£ï¼Œå¹¶ä»¥æ­¤æ¥å®ç°å»åç”Ÿæˆã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå±æ€§åˆ†å¸ƒé¢„æµ‹å™¨ï¼ˆADPï¼‰ï¼Œä¸€ä¸ªå¯ä»¥å°†æ½œåœ¨ç‰¹å¾æ˜ å°„åˆ°å±æ€§åˆ†å¸ƒçš„å°mlpã€‚ADPä½¿ç”¨æ¥è‡ªç°æœ‰å±æ€§åˆ†ç±»å™¨çš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ADPå®ç°çš„åˆ†å¸ƒæŒ‡å¯¼å’Œå±æ€§åˆ†å¸ƒé¢„æµ‹å™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨å•ä¸€æˆ–å¤šå±æ€§ä¸Šå‡å°‘åè§ï¼Œå¹¶åœ¨æ— æ¡ä»¶å’Œæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä½¿ç”¨ç”Ÿæˆæ•°æ®é‡æ–°å¹³è¡¡è®­ç»ƒé›†æ¥è®­ç»ƒå…¬å¹³å±æ€§åˆ†ç±»å™¨çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å…·æœ‰å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå¹¿æ³›åº”ç”¨äºæ•°æ®å¢å¼ºå’Œåˆ›æ„åº”ç”¨ã€‚</li>
<li>DMsä¼šåæ˜ å‡ºè®­ç»ƒæ•°æ®ä¸­çš„åè§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨æ•°æ®ä¸Šã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å»é™¤DMåè§çš„æ–¹æ³•â€”â€”åˆ†å¸ƒæŒ‡å¯¼ã€‚</li>
<li>åˆ†å¸ƒæŒ‡å¯¼åˆ©ç”¨å»å™ªUNetçš„æ½œåœ¨ç‰¹å¾æ¥å®ç°å»åç”Ÿæˆã€‚</li>
<li>æå‡ºäº†å±æ€§åˆ†å¸ƒé¢„æµ‹å™¨ï¼ˆADPï¼‰ï¼Œå°†æ½œåœ¨ç‰¹å¾æ˜ å°„åˆ°å±æ€§åˆ†å¸ƒã€‚</li>
<li>ADPä½¿ç”¨ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.18206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4894b6e9131c4a4625ee8873c4f8c436.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa9ce610dc80d3f4be09d5a01de7cd0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49e8031d53a14866eb45c876076ab708.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ded56cf1c3463b76da4058d34e4008f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dddf78ce5af9d4db4fe479a4f7f09b0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-12/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-12/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-12/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ed2e095b9e8263cc566dfb72ce8d86af.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  PRAD Periapical Radiograph Analysis Dataset and Benchmark Model   Development
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-12/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-78d329a97928847e1f22ee2eabef323d.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  SuperQ-GRASP Superquadrics-based Grasp Pose Estimation on Larger   Objects for Mobile-Manipulation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
