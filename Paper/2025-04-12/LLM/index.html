<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  Cat, Rat, Meow On the Alignment of Language Model and Human   Term-Similarity Judgments">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e3110fc88f85108cbd07ad02836dce36.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-12-æ›´æ–°"><a href="#2025-04-12-æ›´æ–°" class="headerlink" title="2025-04-12 æ›´æ–°"></a>2025-04-12 æ›´æ–°</h1><h2 id="Cat-Rat-Meow-On-the-Alignment-of-Language-Model-and-Human-Term-Similarity-Judgments"><a href="#Cat-Rat-Meow-On-the-Alignment-of-Language-Model-and-Human-Term-Similarity-Judgments" class="headerlink" title="Cat, Rat, Meow: On the Alignment of Language Model and Human   Term-Similarity Judgments"></a>Cat, Rat, Meow: On the Alignment of Language Model and Human   Term-Similarity Judgments</h2><p><strong>Authors:Lorenz Linhardt, Tom NeuhÃ¤user, Lenka TÄ›tkovÃ¡, Oliver Eberle</strong></p>
<p>Small and mid-sized generative language models have gained increasing attention. Their size and availability make them amenable to being analyzed at a behavioral as well as a representational level, allowing investigations of how these levels interact. We evaluate 32 publicly available language models for their representational and behavioral alignment with human similarity judgments on a word triplet task. This provides a novel evaluation setting to probe semantic associations in language beyond common pairwise comparisons. We find that (1) even the representations of small language models can achieve human-level alignment, (2) instruction-tuned model variants can exhibit substantially increased agreement, (3) the pattern of alignment across layers is highly model dependent, and (4) alignment based on modelsâ€™ behavioral responses is highly dependent on model size, matching their representational alignment only for the largest evaluated models. </p>
<blockquote>
<p>å°å‹å’Œä¸­å‹ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹æ­£å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚å®ƒä»¬çš„å¤§å°å’Œå¯ç”¨æ€§ä½¿å…¶èƒ½å¤Ÿåœ¨è¡Œä¸ºå­¦å’Œä»£è¡¨æ€§å±‚é¢è¿›è¡Œåˆ†æï¼Œä»è€Œç ”ç©¶è¿™ä¸¤ä¸ªå±‚é¢å¦‚ä½•ç›¸äº’ä½œç”¨ã€‚æˆ‘ä»¬è¯„ä¼°äº†32ä¸ªå…¬å¼€å¯ç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œä»¥å®ƒä»¬åœ¨è¯ä¸‰å…ƒç»„ä»»åŠ¡ä¸­ä¸äººç±»ç›¸ä¼¼æ€§åˆ¤æ–­çš„ä»£è¡¨æ€§å’Œè¡Œä¸ºä¸€è‡´æ€§ä¸ºæ ‡å‡†ã€‚è¿™ä¸ºé™¤äº†å¸¸è§çš„é…å¯¹æ¯”è¾ƒä¹‹å¤–çš„è¯­è¨€è¯­ä¹‰å…³è”æä¾›äº†ä¸€ä¸ªæ–°å‹è¯„ä¼°ç¯å¢ƒã€‚æˆ‘ä»¬å‘ç°ï¼šï¼ˆ1ï¼‰å³ä½¿æ˜¯å°å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºä¹Ÿå¯ä»¥è¾¾åˆ°äººç±»æ°´å¹³çš„å¯¹é½ï¼›ï¼ˆ2ï¼‰ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„æ¨¡å‹å˜ä½“å¯ä»¥è¡¨ç°å‡ºå¤§å¹…å¢åŠ çš„ä¸€è‡´æ€§ï¼›ï¼ˆ3ï¼‰å¯¹é½æ¨¡å¼çš„è·¨å±‚æ€§é«˜åº¦ä¾èµ–äºæ¨¡å‹ï¼›ï¼ˆ4ï¼‰åŸºäºæ¨¡å‹è¡Œä¸ºååº”çš„å¯¹é½é«˜åº¦ä¾èµ–äºæ¨¡å‹çš„å¤§å°ï¼Œä»…åœ¨ä¸æ‰€è¯„ä¼°çš„æœ€å¤§æ¨¡å‹åŒ¹é…æ—¶ï¼Œæ‰ä¸ä»£è¡¨æ€§å¯¹é½ç›¸åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07965v1">PDF</a> ICLR 2025 Workshop on Representational Alignment (Re-Align)</p>
<p><strong>Summary</strong></p>
<p>å°å‹å’Œä¸­ç­‰è§„æ¨¡çš„ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚è¿™äº›æ¨¡å‹çš„å¤§å°å’Œå¯ç”¨æ€§ä½¿å…¶æ—¢å¯ä»¥åœ¨è¡Œä¸ºå±‚é¢è¿›è¡Œåˆ†æï¼Œä¹Ÿå¯ä»¥åœ¨è¡¨å¾å±‚é¢è¿›è¡Œåˆ†æï¼Œå¹¶å…è®¸ç ”ç©¶è¿™ä¸¤ä¸ªå±‚é¢ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚æœ¬æ–‡è¯„ä¼°äº†32ä¸ªå…¬å¼€å¯ç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œåœ¨è¯ä¸‰å…ƒç»„ä»»åŠ¡ä¸Šå¯¹äººç±»ç›¸ä¼¼æ€§åˆ¤æ–­è¿›è¡Œè¡¨å¾å’Œè¡Œä¸ºå¯¹é½ã€‚è¿™ä¸ºåœ¨è¶…è¶Šå¸¸è§äºŒå…ƒå¯¹æ¯”çš„æƒ…å¢ƒä¸‹æ¢æŸ¥è¯­è¨€ä¸­çš„è¯­ä¹‰å…³è”æä¾›äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°ç¯å¢ƒã€‚ç ”ç©¶å‘ç°ï¼š1ï¼‰å³ä½¿æ˜¯å°å‹è¯­è¨€æ¨¡å‹çš„è¡¨å¾ä¹Ÿèƒ½è¾¾åˆ°äººç±»æ°´å¹³çš„å¯¹é½ï¼›2ï¼‰æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å˜ä½“å¯ä»¥è¡¨ç°å‡ºå¤§å¹…å¢åŠ çš„å…±è¯†ï¼›3ï¼‰å¯¹é½æ¨¡å¼çš„å±‚é—´å˜åŒ–åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ¨¡å‹æœ¬èº«ï¼›4ï¼‰åŸºäºæ¨¡å‹è¡Œä¸ºååº”çš„æ ¡å‡†é«˜åº¦ä¾èµ–äºæ¨¡å‹è§„æ¨¡ï¼Œä»…åœ¨è¯„ä»·çš„æœ€å¤§æ¨¡å‹ä¸­ä¸å…¶è¡¨å¾å¯¹é½ç›¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°å‹å’Œä¸­ç­‰è§„æ¨¡çš„ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹æ­£å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚</li>
<li>åœ¨è¯ä¸‰å…ƒç»„ä»»åŠ¡ä¸Šè¯„ä¼°è¯­è¨€æ¨¡å‹ä¸äººç±»ç›¸ä¼¼æ€§åˆ¤æ–­çš„å¯¹é½æƒ…å†µï¼Œä¸ºè¯„ä¼°è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰å…³è”æä¾›äº†æ–°ç¯å¢ƒã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹çš„è¡¨å¾ä¹Ÿå¯ä»¥è¾¾åˆ°äººç±»æ°´å¹³çš„å¯¹é½ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥è¡¨ç°å‡ºæ›´é«˜çš„å…±è¯†ã€‚</li>
<li>è¯­è¨€æ¨¡å‹çš„å¯¹é½æ¨¡å¼åœ¨ä¸åŒå±‚æ¬¡ä¸Šçš„è¡¨ç°å·®å¼‚è¾ƒå¤§ï¼Œè¿™å–å†³äºæ¨¡å‹æœ¬èº«çš„ç‰¹æ€§ã€‚</li>
<li>åŸºäºæ¨¡å‹è¡Œä¸ºååº”çš„æ ¡å‡†ä¸æ¨¡å‹è§„æ¨¡ç´§å¯†ç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb5d49dbfe26805d21e2b3d17e229c94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3210b782122f6a6016429ca65a00f833.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b6efa906459e53dd68fd79e8557bf56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27d4d75f472bc0685765030e54744ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1674ee997074f3be1c7de8e87048a23.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="C3PO-Critical-Layer-Core-Expert-Collaborative-Pathway-Optimization-for-Test-Time-Expert-Re-Mixing"><a href="#C3PO-Critical-Layer-Core-Expert-Collaborative-Pathway-Optimization-for-Test-Time-Expert-Re-Mixing" class="headerlink" title="C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization   for Test-Time Expert Re-Mixing"></a>C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization   for Test-Time Expert Re-Mixing</h2><p><strong>Authors:Zhongyang Li, Ziyue Li, Tianyi Zhou</strong></p>
<p>Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or â€œre-mixingâ€ the experts in different layers jointly for each test sample. Since the test sampleâ€™s ground truth is unknown, we propose to optimize a surrogate objective defined by the sampleâ€™s â€œsuccessful neighborsâ€ from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples&#x2F;tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core expertsâ€™ mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to â€œCritical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)â€. We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt&#x2F;prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoEâ€™s advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE. </p>
<blockquote>
<p>æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨ä¸¥é‡çš„ä¸“å®¶è·¯å¾„æ¬¡ä¼˜é—®é¢˜â€”â€”æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»é¢„è®­ç»ƒä¸­å­¦åˆ°çš„ç®€å•ä¸“å®¶é€‰æ‹©ç•™ä¸‹äº†ä»¤äººæƒŠè®¶çš„10-20%çš„å‡†ç¡®åº¦æå‡ç©ºé—´ã€‚å—è¿™ä¸€è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç±»æ–°å‹çš„æµ‹è¯•æ—¶é—´ä¼˜åŒ–æ–¹æ³•ï¼Œé’ˆå¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œé‡æ–°æƒé‡æˆ–â€œé‡æ–°æ··åˆâ€ä¸åŒå±‚çš„ä¸“å®¶ã€‚ç”±äºæµ‹è¯•æ ·æœ¬çš„çœŸå®å€¼æœªçŸ¥ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å‚è€ƒæ ·æœ¬é›†ä¸­æ ·æœ¬çš„â€œæˆåŠŸé‚»å±…â€æ¥ä¼˜åŒ–æ›¿ä»£ç›®æ ‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸‰ç§æ›¿ä»£æ–¹æ³•å’ŒåŸºäºæ¨¡å¼å¯»æ‰¾ã€æ ¸å›å½’ä»¥åŠç±»ä¼¼å‚è€ƒæ ·æœ¬&#x2F;ä»»åŠ¡å¹³å‡æŸå¤±çš„ç®—æ³•ã€‚ä¸ºäº†å‡å°‘ä¼˜åŒ–æ•´ä¸ªè·¯å¾„çš„æˆæœ¬ï¼Œæˆ‘ä»¬å°†ç®—æ³•ä»…åº”ç”¨äºå…³é”®å±‚ä¸­æ ¸å¿ƒä¸“å®¶çš„æ··åˆæƒé‡ï¼Œè¿™äº›æ ¸å¿ƒä¸“å®¶äº«æœ‰ç›¸ä¼¼çš„æ€§èƒ½ä½†èŠ‚çœäº†å¤§é‡çš„è®¡ç®—ã€‚è¿™å¯¼è‡´äº†â€œå…³é”®å±‚ã€æ ¸å¿ƒä¸“å®¶ã€ååŒè·¯å¾„ä¼˜åŒ–ï¼ˆC3POï¼‰â€ã€‚æˆ‘ä»¬å°†C3POåº”ç”¨äºæœ€è¿‘çš„ä¸¤ä¸ªMoE LLMï¼Œå¹¶åœ¨å…­ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚å®ƒå§‹ç»ˆæé«˜äº†åŸºç¡€æ¨¡å‹çš„å‡†ç¡®åº¦7-15%ï¼Œå¹¶ä¸”å¤§å¹…è¶…è¶Šäº†å¹¿æ³›ä½¿ç”¨çš„æµ‹è¯•æ—¶é—´å­¦ä¹ åŸºçº¿ï¼Œå¦‚ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæç¤º&#x2F;å‰ç¼€è°ƒæ•´ã€‚æ­¤å¤–ï¼ŒC3POä½¿å…·æœ‰1-3Bæ´»è·ƒå‚æ•°MoE LLMçš„æ€§èƒ½è¶…è¶Šäº†å…·æœ‰7-9Bå‚æ•°çš„LLMï¼Œä»è€Œæé«˜äº†MoEåœ¨æ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å½»åº•æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†MoEåœ¨æµ‹è¯•æ—¶é—´æ”¹è¿›æ–¹é¢çš„æ–°è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07964v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºä¸“å®¶æ··åˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨ä¸“å®¶è·¯å¾„é€‰æ‹©é—®é¢˜ï¼Œç ”ç©¶å‘ç°é¢„è®­ç»ƒä¸­çš„ä¸“å®¶é€‰æ‹©ä¼šç•™ä¸‹é«˜è¾¾10%-20%çš„å‡†ç¡®æ€§æå‡ç©ºé—´ã€‚ä¸ºä¼˜åŒ–æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åœ¨çº¿æµ‹è¯•ä¼˜åŒ–æ–¹æ³•ï¼Œé’ˆå¯¹ä¸åŒæµ‹è¯•æ ·æœ¬ï¼Œå…±åŒè°ƒæ•´ä¸åŒå±‚çº§ä¸“å®¶çš„æƒé‡ã€‚ç”±äºæµ‹è¯•æ ·æœ¬çš„çœŸå®æ ‡ç­¾æœªçŸ¥ï¼Œæˆ‘ä»¬é€šè¿‡å‚è€ƒé›†ä¸­æ ·æœ¬çš„æˆåŠŸé‚»å±…æ¥å®šä¹‰æ›¿ä»£ç›®æ ‡å¹¶è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºæ¨¡å¼å¯»æ‰¾ã€æ ¸å›å½’å’Œç›¸ä¼¼å‚è€ƒæ ·æœ¬å¹³å‡æŸå¤±çš„ä¸‰ç§æ›¿ä»£æ–¹æ¡ˆå’Œç®—æ³•ã€‚ä¸ºé™ä½ä¼˜åŒ–æ•´ä¸ªè·¯å¾„çš„æˆæœ¬ï¼Œæˆ‘ä»¬çš„ç®—æ³•ä»…åº”ç”¨äºå…³é”®å±‚æ ¸å¿ƒä¸“å®¶çš„æ··åˆæƒé‡ï¼Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œå½¢æˆâ€œå…³é”®å±‚ã€æ ¸å¿ƒä¸“å®¶ã€ååŒè·¯å¾„ä¼˜åŒ–ï¼ˆC3POï¼‰â€ã€‚C3POåœ¨è¿‘æœŸçš„å¤§å‹MoE LLMæ¨¡å‹ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨å…­ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶å‡†ç¡®ç‡è¾ƒåŸºç¡€æ¨¡å‹æé«˜äº†7%-15%ï¼Œå¹¶å¤§å¹…è¶…è¶Šäº†æµ‹è¯•æ—¶é—´å­¦ä¹ åŸºçº¿æ–¹æ³•ï¼Œå¦‚ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæç¤ºå‰ç¼€è°ƒæ•´ç­‰ã€‚æ­¤å¤–ï¼ŒC3POä½¿å¾—å…·æœ‰1-3Bæ´»åŠ¨å‚æ•°çš„å¤§å‹MoE LLMæ¨¡å‹æ€§èƒ½è¶…è¶Šå…·æœ‰7-9Bå‚æ•°çš„LLMæ¨¡å‹ï¼Œè¿›ä¸€æ­¥å‡¸æ˜¾MoEåœ¨æ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æ·±å…¥ç ”ç©¶æ­ç¤ºäº†æå‡MoEæµ‹è¯•æ—¶é—´æ€§èƒ½çš„å…¨æ–°æ´å¯Ÿå’Œè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®å‘ç°</strong></p>
<ol>
<li>åŸºäºä¸“å®¶æ··åˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨ä¸“å®¶è·¯å¾„é€‰æ‹©é—®é¢˜ï¼Œå­˜åœ¨æ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ç©ºé—´ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„åœ¨çº¿æµ‹è¯•ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´ä¸åŒå±‚çº§ä¸“å®¶çš„æƒé‡æ¥è§£å†³ä¸“å®¶è·¯å¾„é€‰æ‹©é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å‚è€ƒé›†ä¸­æ ·æœ¬çš„æˆåŠŸé‚»å±…å®šä¹‰æ›¿ä»£ç›®æ ‡å¹¶è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸‰ç§åŸºäºæ¨¡å¼å¯»æ‰¾ã€æ ¸å›å½’å’Œç›¸ä¼¼å‚è€ƒæ ·æœ¬å¹³å‡æŸå¤±çš„æ›¿ä»£æ–¹æ¡ˆå’Œç®—æ³•ã€‚</li>
<li>C3POç®—æ³•ä»…ä¼˜åŒ–å…³é”®å±‚æ ¸å¿ƒä¸“å®¶çš„æ··åˆæƒé‡ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>C3POåœ¨MoE LLMæ¨¡å‹ä¸Šæ˜¾è‘—æé«˜å‡†ç¡®æ€§ï¼Œå¹¶ä¼˜äºå…¶ä»–æµ‹è¯•æ—¶é—´å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-115890515d458eed96712d03e71cedb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d53e968474e8612c2cd236f4ac9e3916.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38373e0e8b694409a0ef5afd167a9c3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-980ad11545408e06d102a8a6113ff6c8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GLUS-Global-Local-Reasoning-Unified-into-A-Single-Large-Language-Model-for-Video-Segmentation"><a href="#GLUS-Global-Local-Reasoning-Unified-into-A-Single-Large-Language-Model-for-Video-Segmentation" class="headerlink" title="GLUS: Global-Local Reasoning Unified into A Single Large Language Model   for Video Segmentation"></a>GLUS: Global-Local Reasoning Unified into A Single Large Language Model   for Video Segmentation</h2><p><strong>Authors:Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang</strong></p>
<p>This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between â€œRefâ€ and â€œVOSâ€: they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that global and local consistency can be unified into a single video segmentation MLLM: a set of sparse â€œcontext framesâ€ provides global information, while a stream of continuous â€œquery framesâ€ conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our project page is at <a target="_blank" rel="noopener" href="https://glus-video.github.io/">https://glus-video.github.io/</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡ŒæŒ‡ä»£è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆRefVOSï¼‰çš„æ–°å‹æ¡†æ¶ã€‚ä¹‹å‰çš„MLLMæ–¹æ³•é€šå¸¸é¢ä¸´â€œRefâ€å’Œâ€œVOSâ€ä¹‹é—´çš„å›°å¢ƒï¼šå®ƒä»¬è¦ä¹ˆä¸“æ³¨äºç†è§£ä¸€äº›å…³é”®å¸§ï¼ˆå…¨å±€æ¨ç†ï¼‰ï¼Œè¦ä¹ˆè·Ÿè¸ªè¿ç»­å¸§ä¸Šçš„å¯¹è±¡ï¼ˆå±€éƒ¨æ¨ç†ï¼‰ï¼Œå¹¶ä¾èµ–å¤–éƒ¨VOSæˆ–å¸§é€‰æ‹©å™¨æ¥ç¼“è§£å¦ä¸€ç«¯çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„GLUSæ¡†æ¶è¡¨æ˜ï¼Œå…¨å±€å’Œå±€éƒ¨ä¸€è‡´æ€§å¯ä»¥ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€çš„è§†é¢‘åˆ†å‰²MLLMä¸­ï¼šä¸€ç»„ç¨€ç–çš„â€œä¸Šä¸‹æ–‡å¸§â€æä¾›å…¨å±€ä¿¡æ¯ï¼Œè€Œä¸€ç³»åˆ—è¿ç»­çš„â€œæŸ¥è¯¢å¸§â€è¿›è¡Œå±€éƒ¨å¯¹è±¡è·Ÿè¸ªã€‚è¿™é€šè¿‡è”åˆè®­ç»ƒMLLMä¸é¢„è®­ç»ƒçš„VOSè®°å¿†åº“æ¥åŒæ—¶æ¶ˆåŒ–çŸ­ç¨‹å’Œé•¿ç¨‹æ—¶é—´ä¿¡æ¯ï¼Œå¾—åˆ°äº†è¿›ä¸€æ­¥çš„æ”¯æŒã€‚ä¸ºäº†æ”¹å–„MLLMæœ‰é™ä¸Šä¸‹æ–‡çª—å£å†…çš„ä¿¡æ¯æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹è±¡å¯¹æ¯”å­¦ä¹ æ¥åŒºåˆ†éš¾ä»¥åŒºåˆ†çš„å‡é˜³æ€§å¯¹è±¡ï¼Œä»¥åŠè‡ªæˆ‘å®Œå–„æ¡†æ¶æ¥è¯†åˆ«å…³é”®å¸§å¹¶è¿›è¡Œä¼ æ’­ã€‚é€šè¿‡é›†ä½“æ•´åˆè¿™äº›è§è§£ï¼Œæˆ‘ä»¬çš„GLUSæä¾›äº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„åŸºå‡†ï¼Œåœ¨MeViSå’ŒRef-Youtube-VOSåŸºå‡†ä¸Šå®ç°äº†MLLMçš„æœ€æ–°æ°´å¹³ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://glus-video.github.io/%E3%80%82">https://glus-video.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07962v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡ŒæŒ‡ä»£è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆRefVOSï¼‰çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶èåˆäº†å…¨å±€å’Œå±€éƒ¨ä¸€è‡´æ€§ï¼Œé€šè¿‡ç¨€ç–çš„â€œä¸Šä¸‹æ–‡å¸§â€æä¾›å…¨å±€ä¿¡æ¯ï¼Œé€šè¿‡è¿ç»­çš„â€œæŸ¥è¯¢å¸§â€è¿›è¡Œå±€éƒ¨å¯¹è±¡è·Ÿè¸ªã€‚æ­¤å¤–ï¼Œé€šè¿‡è”åˆè®­ç»ƒMLLMä¸é¢„è®­ç»ƒçš„VOSè®°å¿†é“¶è¡Œï¼ŒåŒæ—¶æ¶ˆåŒ–çŸ­ç¨‹å’Œé•¿ç¨‹æ—¶é—´ä¿¡æ¯ã€‚ä¸ºäº†æé«˜MLLMæœ‰é™ä¸Šä¸‹æ–‡çª—å£å†…çš„ä¿¡æ¯æ•ˆç‡ï¼Œå¼•å…¥äº†å¯¹è±¡å¯¹æ¯”å­¦ä¹ å’Œè‡ªæˆ‘å®Œå–„æ¡†æ¶ã€‚è¿™äº›ç­–ç•¥å…±åŒä¸ºMLLMsåœ¨MeViSå’ŒRef-Youtube-VOSåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€æ–°çŠ¶æ€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆRefVOSï¼‰æ–°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå…¨å±€å’Œå±€éƒ¨ä¸€è‡´æ€§æ¥è§£å†³è§†é¢‘å¯¹è±¡åˆ†å‰²ä¸­çš„éš¾é¢˜ã€‚</li>
<li>ç¨€ç–çš„ä¸Šä¸‹æ–‡å¸§å’Œè¿ç»­çš„æŸ¥è¯¢å¸§åˆ†åˆ«æä¾›å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒMLLMå’Œé¢„è®­ç»ƒçš„VOSè®°å¿†é“¶è¡Œï¼ŒåŒæ—¶å¤„ç†çŸ­ç¨‹å’Œé•¿ç¨‹æ—¶é—´ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥äº†å¯¹è±¡å¯¹æ¯”å­¦ä¹ ä»¥æé«˜åœ¨æœ‰é™ä¸Šä¸‹æ–‡çª—å£å†…çš„ä¿¡æ¯æ•ˆç‡ã€‚</li>
<li>è‡ªæˆ‘å®Œå–„æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«å…³é”®å¸§å¹¶è¿›è¡Œä¼ æ’­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2e4162cab4e2325903db118e13bfe942.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3c221397e99f939a899cefe5ccc2e5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d85594e10665a496e06c35bb2c6da6c9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MM-IFEngine-Towards-Multimodal-Instruction-Following"><a href="#MM-IFEngine-Towards-Multimodal-Instruction-Following" class="headerlink" title="MM-IFEngine: Towards Multimodal Instruction Following"></a>MM-IFEngine: Towards Multimodal Instruction Following</h2><p><strong>Authors:Shengyuan Ding, Shenxi Wu, Xiangyu Zhao, Yuhang Zang, Haodong Duan, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Dahua Lin, Jiaqi Wang</strong></p>
<p>The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2$%$), MIA (+7.6$%$), and IFEval (+12.3$%$). The full data and evaluation code will be released on <a target="_blank" rel="noopener" href="https://github.com/SYuan03/MM-IFEngine">https://github.com/SYuan03/MM-IFEngine</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æŒ‡ä»¤è·Ÿéšï¼ˆIFï¼‰èƒ½åŠ›è¡¡é‡çš„æ˜¯å®ƒä»¬å¯¹ç”¨æˆ·æŒ‡ä»¤çš„ç†è§£ç¨‹åº¦ä»¥åŠæ‰§è¡ŒæŒ‡ä»¤çš„å‡†ç¡®æ€§ã€‚ç°æœ‰çš„å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšè®­ç»ƒæ•°æ®ç¨€ç¼ºï¼ŒåŸºå‡†æµ‹è¯•åŒ…å«çš„æŒ‡ä»¤ç®€å•åŸå­åŒ–ï¼Œå¯¹äºéœ€è¦ç²¾ç¡®è¾“å‡ºçº¦æŸçš„ä»»åŠ¡ï¼Œè¯„ä¼°ç­–ç•¥å¹¶ä¸ç²¾ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MM-IFEngineï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç®¡é“ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å›¾ç‰‡æŒ‡ä»¤å¯¹ã€‚æˆ‘ä»¬çš„MM-IFEngineç®¡é“äº§ç”Ÿäº†å¤§è§„æ¨¡ã€å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®MM-IFInstruct-23kï¼Œé€‚ç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¹¶æ‰©å±•ä¸ºç”¨äºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„MM-IFDPO-23kã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†MM-IFEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œå¤šæ ·åŒ–çš„å¤šæ¨¡æ€æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼ˆ1ï¼‰è¾“å‡ºå“åº”çš„ç»„æˆçº§çº¦æŸä»¥åŠä¸è¾“å…¥å›¾åƒç›¸å…³çš„æ„ŸçŸ¥çº§çº¦æŸï¼›ï¼ˆ2ï¼‰åŒ…å«åŸºäºè§„åˆ™çš„è¯„ä»·å’Œåˆ¤æ–­æ¨¡å‹çš„å…¨é¢è¯„ä¼°æµç¨‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†SFTå’ŒDPOå®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨MM-IFInstruct-23kå’ŒMM-IFDPO-23kä¸Šå¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œåœ¨MM-IFEvalï¼ˆ+10.2%ï¼‰ã€MIAï¼ˆ+7.6%ï¼‰å’ŒIFEvalï¼ˆ+12.3%ï¼‰ç­‰IFåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚å®Œæ•´çš„æ•°æ®å’Œè¯„ä¼°ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/SYuan03/MM-IFEngine%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/SYuan03/MM-IFEngineä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07957v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºã€åŸºå‡†æµ‹è¯•ç®€å•ä»¥åŠè¯„ä¼°ç­–ç•¥ä¸ç²¾ç¡®çš„é—®é¢˜ï¼Œæå‡ºäº†MM-IFEngineç®¡é“ï¼Œç”Ÿæˆé«˜è´¨é‡å›¾åƒæŒ‡ä»¤å¯¹ã€‚è¯¥ç®¡é“ç”Ÿæˆå¤§è§„æ¨¡ã€å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„MM-IFInstruct-23kè®­ç»ƒæ•°æ®ï¼Œé€‚ç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¹¶æ‰©å±•ä¸ºMM-IFDPO-23kç”¨äºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†MM-IFEvalåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ç»„åˆçº§è¾“å‡ºå“åº”å’Œä¸è¾“å…¥å›¾åƒç›¸å…³çš„æ„ŸçŸ¥çº§çº¦æŸï¼Œä»¥åŠåŒ…å«åŸºäºè§„åˆ™è¯„ä¼°å’Œåˆ¤æ–­æ¨¡å‹çš„å…¨é¢è¯„ä¼°æµç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨MM-IFEvalï¼ˆ+10.2%ï¼‰ã€MIAï¼ˆ+7.6%ï¼‰å’ŒIFEvalï¼ˆ+12.3%ï¼‰ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹MM-IFInstruct-23kå’ŒMM-IFDPO-23kè¿›è¡Œå¾®è°ƒå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å¯¹äºç†è§£ç”¨æˆ·æŒ‡ä»¤å¹¶æ‰§è¡Œæ­£ç¡®æ“ä½œè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è®­ç»ƒæ•°æ®ç¼ºä¹å¤šæ ·æ€§ä¸”è´¨é‡ä¸é«˜ï¼Œä½¿å¾—æ¨¡å‹åœ¨å¤æ‚çš„æŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸­è¡¨ç°å—é™ã€‚</li>
<li>MM-IFEngineç®¡é“è§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œç”Ÿæˆå¤§è§„æ¨¡é«˜è´¨é‡å›¾åƒæŒ‡ä»¤å¯¹æ•°æ®é›†MM-IFInstruct-23kã€‚</li>
<li>MM-IFEngineæ”¯æŒç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œé€‚ç”¨äºå¤šæ¨¡æ€æŒ‡ä»¤éµå¾ªä»»åŠ¡ã€‚</li>
<li>MM-IFEvalåŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æµç¨‹ï¼ŒåŒ…æ‹¬åŸºäºè§„åˆ™è¯„ä¼°å’Œåˆ¤æ–­æ¨¡å‹ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤ä¸‹çš„è¡¨ç°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨MM-IFEngineç”Ÿæˆçš„è®­ç»ƒæ•°æ®å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-60eb18377b817551c593665983d74304.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-518f7b37f13bc2fd756a427602f34ec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b3e531ea2262af871e4604f06f284cf.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Redefining-Machine-Translation-on-Social-Network-Services-with-Large-Language-Models"><a href="#Redefining-Machine-Translation-on-Social-Network-Services-with-Large-Language-Models" class="headerlink" title="Redefining Machine Translation on Social Network Services with Large   Language Models"></a>Redefining Machine Translation on Social Network Services with Large   Language Models</h2><p><strong>Authors:Hongcheng Guo, Fei Zhao, Shaosheng Cao, Xinze Lyu, Ziyan Liu, Yue Wang, Boyang Wang, Zhoujun Li, Chonggang Lu, Zhe Xu, Yao Hu</strong></p>
<p>The globalization of social interactions has heightened the need for machine translation (MT) on Social Network Services (SNS), yet traditional models struggle with culturally nuanced content like memes, slang, and pop culture references. While large language models (LLMs) have advanced general-purpose translation, their performance on SNS-specific content remains limited due to insufficient specialized training data and evaluation benchmarks. This paper introduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel dataset developed through three innovations: (1) Supervised Finetuning with Dual-LLM Back-Translation Sampling, an unsupervised sampling method using LLM-based back-translation to select diverse data for large-scale finetuning; (2) Rewritten Preference Optimization (RePO), an algorithm that identifies and corrects erroneous preference pairs through expert annotation, building reliable preference corpora; and (3) RedTrans-Bench, the first benchmark for SNS translation, evaluating phenomena like humor localization, emoji semantics, and meme adaptation. Experiments show RedTrans outperforms state-of-the-art LLMs. Besides, RedTrans has already been deployed in a real-world production environment, demonstrating that domain-specific adaptation, effectively bridges the gap between generic and culturally grounded translation systems. </p>
<blockquote>
<p>ç¤¾äº¤äº’åŠ¨çš„å…¨çƒåŒ–å¢å¼ºäº†ç¤¾äº¤ç½‘ç»œæœåŠ¡ï¼ˆSNSï¼‰å¯¹æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰çš„éœ€æ±‚ï¼Œä½†ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†å¸¦æœ‰æ–‡åŒ–è‰²å½©çš„æƒ…å¢ƒå¦‚memeã€ä¿šè¯­å’Œæµè¡Œæ–‡åŒ–å¼•ç”¨æ—¶æ„Ÿåˆ°å›°éš¾ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æé«˜äº†é€šç”¨ç¿»è¯‘çš„æ€§èƒ½ï¼Œä½†ç”±äºç¼ºä¹ä¸“ä¸šè®­ç»ƒæ•°æ®å’Œè¯„ä¼°åŸºå‡†ï¼Œå®ƒä»¬åœ¨SNSç‰¹å®šå†…å®¹ä¸Šçš„è¡¨ç°ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRedTransçš„å®šåˆ¶åŒ–çš„ç”¨äºSNSç¿»è¯‘çš„72Bå¤§å‹è¯­è¨€æ¨¡å‹ã€‚RedTransçš„è®­ç»ƒåŸºäºä¸€é¡¹åˆ›æ–°å¼€å‘çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯æ„å»ºï¼šï¼ˆ1ï¼‰ä½¿ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åå‘ç¿»è¯‘æ¥é€‰æ‹©å¤šæ ·åŒ–æ•°æ®çš„ç›‘ç£å¾®è°ƒåŒå¤§å‹è¯­è¨€æ¨¡å‹åå‘ç¿»è¯‘é‡‡æ ·ï¼›ï¼ˆ2ï¼‰é‡å†™åå¥½ä¼˜åŒ–ï¼ˆRePOï¼‰ç®—æ³•é€šè¿‡ä¸“å®¶æ³¨é‡Šæ¥è¯†åˆ«å’Œçº æ­£é”™è¯¯åå¥½å¯¹ï¼Œå»ºç«‹å¯é çš„åå¥½è¯­æ–™åº“ï¼›ï¼ˆ3ï¼‰RedTrans-Benchæ˜¯é¦–ä¸ªé’ˆå¯¹SNSç¿»è¯‘è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„å¹³å°ï¼Œè¯„ä¼°è¯¸å¦‚å¹½é»˜å®šä½ã€è¡¨æƒ…ç¬¦å·è¯­ä¹‰å’Œmemeé€‚åº”ç­‰ç°è±¡ã€‚å®éªŒè¡¨æ˜ï¼ŒRedTransåœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸçš„è¡¨ç°å¤„äºé¢†å…ˆåœ°ä½ã€‚æ­¤å¤–ï¼ŒRedTranså·²ç»éƒ¨ç½²åœ¨çœŸå®ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¯æ˜ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§æœ‰æ•ˆåœ°ç¼©å°äº†é€šç”¨ç¿»è¯‘ç³»ç»Ÿå’ŒåŸºäºæ–‡åŒ–çš„ç¿»è¯‘ç³»ç»Ÿä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07901v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨ç¤¾äº¤ç½‘ç»œæœåŠ¡ï¼ˆSNSï¼‰ä¸Šï¼Œæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰çš„éœ€æ±‚éšç€ç¤¾äº¤äº’åŠ¨çš„å…¨çƒåŒ–è€Œå¢åŠ ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†å¦‚è¡¨æƒ…åŒ…ã€ä¿šè¯­å’Œæµè¡Œæ–‡åŒ–å¼•ç”¨ç­‰å……æ»¡æ–‡åŒ–ç»†å¾®å·®åˆ«çš„å†…å®¹æ—¶æ˜¾å¾—æ‰è¥Ÿè§è‚˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨ç¿»è¯‘æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ä¸“é—¨åŒ–çš„è®­ç»ƒæ•°æ®å’Œè¯„ä¼°åŸºå‡†æµ‹è¯•ï¼Œå®ƒä»¬åœ¨SNSç‰¹å®šå†…å®¹ä¸Šçš„è¡¨ç°ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ä»‹ç»äº†RedTransï¼Œä¸€ä¸ªé’ˆå¯¹SNSç¿»è¯‘å®šåˆ¶çš„72Bå¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒé€šè¿‡ä½¿ç”¨ä¸‰ç§åˆ›æ–°æ–¹æ³•å¼€å‘çš„æ–°å‹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼š1ï¼‰ä½¿ç”¨åŒLLMåå‘ç¿»è¯‘é‡‡æ ·çš„ç›‘ç£å¾®è°ƒï¼›2ï¼‰é‡å†™åå¥½ä¼˜åŒ–ï¼ˆRePOï¼‰ç®—æ³•ï¼Œé€šè¿‡ä¸“å®¶æ³¨é‡Šè¯†åˆ«å’Œçº æ­£é”™è¯¯åå¥½å¯¹ï¼Œå»ºç«‹å¯é çš„åå¥½è¯­æ–™åº“ï¼›ä»¥åŠ3ï¼‰RedTrans-Benchï¼Œé¦–ä¸ªSNSç¿»è¯‘çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å¹½é»˜å®šä½ã€è¡¨æƒ…ç¬¦å·è¯­ä¹‰å’Œè¡¨æƒ…åŒ…é€‚åº”ç­‰ç°è±¡ã€‚å®éªŒè¡¨æ˜ï¼ŒRedTransä¼˜äºæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒRedTranså·²åœ¨çœŸå®ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œè¯æ˜é¢†åŸŸç‰¹å®šé€‚åº”æœ‰æ•ˆåœ°ç¼©å°äº†é€šç”¨ç¿»è¯‘ç³»ç»Ÿå’Œæ–‡åŒ–æ ¹åŸºç¿»è¯‘ç³»ç»Ÿä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤ç½‘ç»œçš„å…¨çƒåŒ–å¢åŠ äº†å¯¹æœºå™¨ç¿»è¯‘çš„éœ€æ±‚ï¼Œä½†ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†åŒ…å«æ–‡åŒ–ç»†å¾®å·®åˆ«çš„å†…å®¹æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šç”¨ç¿»è¯‘æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨ç¤¾äº¤ç½‘ç»œå¹³å°ç‰¹å®šå†…å®¹ä¸Šçš„è¡¨ç°ä»ç„¶å—é™ã€‚</li>
<li>RedTransæ˜¯ä¸€ä¸ªé’ˆå¯¹ç¤¾äº¤ç½‘ç»œæœåŠ¡çš„72Bå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºç¿»è¯‘ã€‚</li>
<li>RedTransçš„å¼€å‘ä¾èµ–äºä¸‰ç§åˆ›æ–°æ–¹æ³•ï¼šç›‘ç£å¾®è°ƒã€é‡å†™åå¥½ä¼˜åŒ–ç®—æ³•å’ŒRedTrans-BenchåŸºå‡†æµ‹è¯•ã€‚</li>
<li>RedTransæ˜¾ç¤ºå‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>RedTranså·²åœ¨çœŸå®ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œè¯æ˜äº†å…¶åœ¨ç‰¹å®šé¢†åŸŸç¿»è¯‘ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4adc2ff1fc50a454409ba046371c4567.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ad2c0eb188befaf81eb01e97b47d0f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba4bc8bbc465ea3369956de213fa967e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef2e2eaf3d56c074eb5e3df43bd939d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd6e99a28e24104ae4cd6c6c81d6e19e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08b06c297af103c799ffd375cb5ca96b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3110fc88f85108cbd07ad02836dce36.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Adversarial-Robustness-to-Bias-Elicitation-in-Large-Language-Models-Scalable-Automated-Assessment-with-LLM-as-a-Judge"><a href="#Benchmarking-Adversarial-Robustness-to-Bias-Elicitation-in-Large-Language-Models-Scalable-Automated-Assessment-with-LLM-as-a-Judge" class="headerlink" title="Benchmarking Adversarial Robustness to Bias Elicitation in Large   Language Models: Scalable Automated Assessment with LLM-as-a-Judge"></a>Benchmarking Adversarial Robustness to Bias Elicitation in Large   Language Models: Scalable Automated Assessment with LLM-as-a-Judge</h2><p><strong>Authors:Riccardo Cantini, Alessio Orsino, Massimo Ruggiero, Domenico Talia</strong></p>
<p>Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å½»åº•æ”¹å˜äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œæ¨åŠ¨äº†æœºå™¨ç¿»è¯‘ã€æ‘˜è¦å’Œå¯¹è¯ä»£ç†äººçš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå®ƒä»¬è¶Šæ¥è¶Šå¤šåœ°èå…¥å…³é”®ç¤¾ä¼šé¢†åŸŸï¼Œå¼•å‘äº†äººä»¬å¯¹åµŒå…¥åè§çš„æ‹…å¿§ï¼Œè¿™äº›åè§å¯èƒ½ä¼šå»¶ç»­åˆ»æ¿å°è±¡å¹¶æŸå®³å…¬å¹³æ€§ã€‚è¿™äº›åè§æ¥æºäºå„ç§æ¥æºï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®ä¸­çš„å†å²ä¸å¹³ç­‰ã€è¯­è¨€å¤±è¡¡å’Œå¯¹æŠ—æ€§æ“ä½œã€‚å°½ç®¡é‡‡å–äº†ç¼“è§£æªæ–½ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMä»ç„¶å®¹æ˜“å—åˆ°æ—¨åœ¨å¼•å‘åè§ååº”çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œä»¥è¯„ä¼°LLMå¯¹æŠ—å¯¹æŠ—æ€§åè§æ¿€å‘çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼š(i)ä½¿ç”¨å¤šä»»åŠ¡æ–¹æ³•ç³»ç»Ÿåœ°æ¢æµ‹æ¨¡å‹ï¼Œé’ˆå¯¹å„ç§ç¤¾ä¼šæ–‡åŒ–ç»´åº¦çš„åè§ï¼Œ(ii)é€šè¿‡å®‰å…¨åˆ†æ•°é‡åŒ–ç¨³å¥æ€§ï¼Œä½¿ç”¨LLM-as-a-Judgeæ–¹æ³•è¿›è¡Œæ¨¡å‹å“åº”çš„è‡ªåŠ¨è¯„ä¼°ï¼Œ(iii)é‡‡ç”¨çªç ´æŠ€æœ¯æ¥ç ”ç©¶å®‰å…¨æœºåˆ¶çš„æ¼æ´ã€‚æˆ‘ä»¬çš„åˆ†æç ”ç©¶äº†å…ˆè¿›çš„å°å‹å’Œå¤§å‹æ¨¡å‹ä¸­çš„å¸¸è§åè§åŠå…¶å¯¹æ¨¡å‹å®‰å…¨çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†é’ˆå¯¹å…³é”®é¢†åŸŸï¼ˆå¦‚åŒ»å­¦ï¼‰è¿›è¡Œå¾®è°ƒçš„ä¸“ä¸šé¢†åŸŸæ¨¡å‹çš„å®‰å…¨æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªç»è¿‡æ•´ç†çš„åè§ç›¸å…³æç¤ºæ•°æ®é›†CLEAR-Biasï¼Œä»¥ä¿ƒè¿›ç³»ç»Ÿçš„è„†å¼±æ€§åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†æ¨¡å‹è§„æ¨¡ä¸å®‰å…¨ä¹‹é—´çš„å…³é”®æƒè¡¡ï¼Œæœ‰åŠ©äºå¼€å‘æ›´å…¬å¹³ã€æ›´ç¨³å¥çš„æœªæ¥è¯­è¨€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07887v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¼•å‘äº†é©å‘½ï¼Œæ¨åŠ¨äº†æœºå™¨ç¿»è¯‘ã€æ‘˜è¦å’Œå¯¹è¯ä»£ç†äººçš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå®ƒä»¬æ—¥ç›Šèå…¥å…³é”®ç¤¾ä¼šé¢†åŸŸå¼•å‘äº†äººä»¬å¯¹å†…åµŒåè§çš„æ‹…å¿§ï¼Œè¿™äº›åè§å¯èƒ½åŠ©é•¿åˆ»æ¿å°è±¡å¹¶å±åŠå…¬å¹³æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œä»¥è¯„ä¼°LLMå¯¹æŠ—åè§æ¿€å‘çš„ç¨³å¥æ€§ã€‚é€šè¿‡ç³»ç»Ÿåœ°å¯¹æ¨¡å‹è¿›è¡Œå¤šä»»åŠ¡æµ‹è¯•ï¼Œé‡åŒ–æ¨¡å‹å¯¹å„ç§ç¤¾ä¼šæ–‡åŒ–ç»´åº¦åè§çš„ç¨³å¥æ€§ï¼Œå¹¶åˆ©ç”¨LLM-as-a-Judgeæ–¹æ³•è¯„ä¼°æ¨¡å‹å“åº”çš„è‡ªåŠ¨åŒ–è¯„ä¼°å®‰å…¨åˆ†æ•°ï¼Œä»¥åŠè¿ç”¨è¶Šç‹±æŠ€æœ¯æ¥æ¢è®¨å®‰å…¨æœºåˆ¶çš„æ¼æ´ã€‚åˆ†æè¡¨æ˜ï¼Œå¤§å°å…ˆè¿›çš„æ¨¡å‹æ™®éå­˜åœ¨åè§ï¼Œå¹¶å¯¹æ¨¡å‹å®‰å…¨äº§ç”Ÿå½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†é’ˆå¯¹å…³é”®é¢†åŸŸï¼ˆå¦‚åŒ»å­¦ï¼‰è¿›è¡Œå¾®è°ƒçš„ä¸“ä¸šé¢†åŸŸæ¨¡å‹çš„å®‰å…¨æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªåè§ç›¸å…³æç¤ºçš„ç²¾é€‰æ•°æ®é›†CLEAR-Biasï¼Œä»¥ä¿ƒè¿›ç³»ç»Ÿçš„è„†å¼±æ€§åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹å¤§å°ä¸å®‰å…¨ä¹‹é—´å­˜åœ¨å…³é”®æƒè¡¡ï¼Œæœ‰åŠ©äºå¼€å‘æ›´å…¬å¹³ã€æ›´ç¨³å¥çš„æœªæ¥è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæ¨åŠ¨äº†é‡å¤§è¿›å±•ï¼Œä½†é›†æˆåˆ°å…³é”®ç¤¾ä¼šé¢†åŸŸå¼•å‘äº†å…³äºå†…åµŒåè§çš„æ‹…å¿§ã€‚</li>
<li>LLMä¸­çš„åè§å¯èƒ½æºäºè®­ç»ƒæ•°æ®çš„å†å²ä¸å¹³ç­‰ã€è¯­è¨€ä¸å¹³è¡¡å’Œå¯¹æ‰‹æ“çºµã€‚</li>
<li>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼ŒLLMä»æ˜“å—åˆ°å¯¹æ‰‹æ”»å‡»çš„å½±å“ï¼Œè¿™äº›æ”»å‡»æ—¨åœ¨æ¿€å‘æœ‰åè§çš„å›åº”ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¯„ä¼°LLMå¯¹æŠ—åè§æ¿€å‘çš„ç¨³å¥æ€§çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚</li>
<li>é€šè¿‡å¤šä»»åŠ¡æµ‹è¯•ã€é‡åŒ–å®‰å…¨æ€§å’Œè¿ç”¨è¶Šç‹±æŠ€æœ¯æ¥ç³»ç»Ÿæ¢ç©¶æ¨¡å‹çš„åè§å’Œæ¼æ´ã€‚</li>
<li>åˆ†æå‘ç°å¤§å‹å’Œå°å‹å…ˆè¿›æ¨¡å‹å‡å­˜åœ¨æ™®éåè§ï¼Œå¹¶å¯¹æ¨¡å‹å®‰å…¨äº§ç”Ÿå½±å“ã€‚</li>
<li>å‘å¸ƒäº†CLEAR-Biasæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›ç³»ç»Ÿçš„è„†å¼±æ€§åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹å¤§å°ä¸å®‰å…¨ä¹‹é—´çš„æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3294d0ef3446c6311c8b3f5d35b0bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5cb7732c0cbfa07b16f5c9a2d67d8c3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Robust-Hallucination-Detection-in-LLMs-via-Adaptive-Token-Selection"><a href="#Robust-Hallucination-Detection-in-LLMs-via-Adaptive-Token-Selection" class="headerlink" title="Robust Hallucination Detection in LLMs via Adaptive Token Selection"></a>Robust Hallucination Detection in LLMs via Adaptive Token Selection</h2><p><strong>Authors:Mengjia Niu, Hamed Haddadi, Guansong Pang</strong></p>
<p>Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMsâ€™ internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰å¼•å‘äº†é‡å¤§çš„å®‰å…¨æ‹…å¿§ï¼Œé˜»ç¢äº†å…¶æ›´å¹¿æ³›çš„éƒ¨ç½²ã€‚æœ€è¿‘åœ¨å¹»è§‰æ£€æµ‹æ–¹é¢çš„ç ”ç©¶å·²ç»è¯æ˜ï¼ŒLLMçš„å†…éƒ¨è¡¨ç¤ºåŒ…å«çœŸå®æ€§æç¤ºï¼Œå¯ä»¥è¢«ç”¨äºæ£€æµ‹å™¨è®­ç»ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ£€æµ‹å™¨çš„æ€§èƒ½ä¸¥é‡ä¾èµ–äºé¢„å…ˆç¡®å®šçš„æ ‡è®°çš„å†…éƒ¨è¡¨ç¤ºï¼Œåœ¨å¤„ç†é•¿åº¦å„å¼‚ã€å¹»è§‰å®ä½“ç¨€ç–åˆ†å¸ƒçš„è‡ªç”±å½¢å¼ç”Ÿæˆæ—¶ï¼Œå…¶æ€§èƒ½æ³¢åŠ¨å¾ˆå¤§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HaMIï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡è‡ªé€‚åº”é€‰æ‹©å’Œå­¦ä¹ æœ€èƒ½æŒ‡ç¤ºå¹»è§‰çš„å…³é”®æ ‡è®°æ¥å®ç°ç¨³å¥å¹»è§‰æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡å°†å¹»è§‰æ£€æµ‹ä»»åŠ¡åˆ›æ–°åœ°è¡¨è¿°ä¸ºåºåˆ—å†…æ ‡è®°çº§åˆ«è¡¨ç¤ºçš„å¤šå®ä¾‹ï¼ˆHaMIï¼‰å­¦ä¹ ï¼Œä»è€Œåœ¨ç”Ÿæˆåºåˆ—ä¸Šå®ç°æ ‡è®°é€‰æ‹©å’Œå¹»è§‰æ£€æµ‹çš„è”åˆä¼˜åŒ–ï¼Œä»è€Œåœ¨å¤šç§å½¢å¼çš„ç”Ÿæˆåºåˆ—ä¸Šå®ç°ç¨³å¥æ€§ã€‚åœ¨å››ä¸ªå¹»è§‰åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼ŒHaMIæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07863v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰å¸¦æ¥å®‰å…¨æ‹…å¿§ï¼Œé˜»ç¢å…¶å¹¿æ³›åº”ç”¨ã€‚æœ€æ–°ç ”ç©¶æŒ‡å‡ºLLMçš„å†…éƒ¨è¡¨å¾åŒ…å«çœŸå®æ€§çº¿ç´¢ï¼Œå¯ç”¨äºè®­ç»ƒæ£€æµ‹å™¨ã€‚ç„¶è€Œï¼Œæ£€æµ‹å™¨æ€§èƒ½ä¸¥é‡ä¾èµ–äºé¢„è®¾æ ‡è®°çš„å†…éƒ¨è¡¨å¾ï¼Œåœ¨å¤„ç†ä¸åŒé•¿åº¦ã€å¹»è§‰å®ä½“åˆ†å¸ƒç¨€ç–çš„è‡ªç”±å½¢å¼ç”Ÿæˆæ–¹é¢æ€§èƒ½æ³¢åŠ¨è¾ƒå¤§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºHaMIæ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”é€‰æ‹©å’Œå­¦ä¹ æœ€èƒ½æŒ‡ç¤ºå¹»è§‰çš„å…³é”®æ ‡è®°ï¼Œå®ç°ç¨³å¥çš„å¹»è§‰æ£€æµ‹ã€‚é€šè¿‡åˆ›æ–°åœ°å°†å¹»è§‰æ£€æµ‹ä»»åŠ¡è¡¨è¿°ä¸ºåºåˆ—å†…æ ‡è®°çº§åˆ«çš„å¤šå®ä¾‹ï¼ˆHaMIï¼‰å­¦ä¹ ï¼Œåœ¨å¤šç§å½¢å¼çš„ç”Ÿæˆåºåˆ—ä¸Šè”åˆä¼˜åŒ–æ ‡è®°é€‰æ‹©å’Œå¹»è§‰æ£€æµ‹ï¼Œä»è€Œå®ç°ç¨³å¥æ€§ã€‚åœ¨å››ä¸ªå¹»è§‰åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼ŒHaMIæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰å¸¦æ¥å®‰å…¨æ‹…å¿§ã€‚</li>
<li>LLMçš„å†…éƒ¨è¡¨å¾åŒ…å«çœŸå®æ€§çº¿ç´¢ï¼Œå¯ç”¨äºè®­ç»ƒæ£€æµ‹å™¨ã€‚</li>
<li>ç°æœ‰æ£€æµ‹å™¨åœ¨å¤„ç†è‡ªç”±å½¢å¼ç”Ÿæˆæ—¶æ€§èƒ½æ³¢åŠ¨è¾ƒå¤§ã€‚</li>
<li>æå‡ºHaMIæ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”é€‰æ‹©å’Œå­¦ä¹ å…³é”®æ ‡è®°å®ç°ç¨³å¥çš„å¹»è§‰æ£€æµ‹ã€‚</li>
<li>å°†å¹»è§‰æ£€æµ‹ä»»åŠ¡è¡¨è¿°ä¸ºåºåˆ—å†…æ ‡è®°çº§åˆ«çš„å¤šå®ä¾‹å­¦ä¹ ã€‚</li>
<li>HaMIæ–¹æ³•åœ¨å¤šç§å½¢å¼çš„ç”Ÿæˆåºåˆ—ä¸Šè”åˆä¼˜åŒ–æ ‡è®°é€‰æ‹©å’Œå¹»è§‰æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92bb122b5c000de3dd840864d4ba36b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-246ce70cc535341b9d95b1dee0386c17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9168a08e590a34ba80ba6a9314579122.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdd1aa50fa2a3dc08a02c3f32615c33c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-501cf3e2bff914ec5c6f5659bfb50552.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="2D-Curri-DPO-Two-Dimensional-Curriculum-Learning-for-Direct-Preference-Optimization"><a href="#2D-Curri-DPO-Two-Dimensional-Curriculum-Learning-for-Direct-Preference-Optimization" class="headerlink" title="2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference   Optimization"></a>2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference   Optimization</h2><p><strong>Authors:Mengyang Li, Zhong Zhang</strong></p>
<p>Aligning large language models with human preferences is crucial for their safe deployment. While Direct Preference Optimization (DPO) offers an efficient alternative to reinforcement learning from human feedback, traditional DPO methods are limited by their reliance on single preference pairs. Recent work like Curriculum-DPO integrates multiple pairs using a one-dimensional difficulty curriculum based on pairwise distinguishability (PD), but overlooks the complexity of the input prompt itself. To address this, we propose 2D-Curri-DPO, a novel framework employing a two-dimensional curriculum that jointly models Prompt Complexity (PC) and Pairwise Distinguishability. This framework introduces dual difficulty metrics to quantify prompt semantic complexity and response preference clarity, defines a curriculum strategy space encompassing multiple selectable strategies for task adaptation, and incorporates a KL-divergence-based adaptive mechanism for dynamic reference model updates to enhance training stability. Comprehensive experiments demonstrate that 2D-Curri-DPO significantly outperforms standard DPO and prior curriculum methods across multiple benchmarks, including MT-Bench, Vicuna Bench, and WizardLM. Our approach achieves state-of-the-art performance on challenging test sets like UltraFeedback. Ablation studies confirm the benefits of the 2D structure and adaptive mechanisms, while analysis provides guidance for strategy selection. These findings demonstrate that effective alignment requires modeling both prompt complexity and pairwise distinguishability, establishing adaptive, multi-dimensional curriculum learning as a powerful and interpretable new paradigm for preference-based language model optimization. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½å¯¹äºå…¶å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ã€‚è™½ç„¶ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸ºé€šè¿‡äººç±»åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ æä¾›äº†æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ä¼ ç»ŸDPOæ–¹æ³•å—é™äºå¯¹å•ä¸€åå¥½å¯¹çš„ä¾èµ–ã€‚æœ€è¿‘çš„å·¥ä½œå¦‚è¯¾ç¨‹å¼DPOï¼ˆCurriculum-DPOï¼‰é€šè¿‡åŸºäºæˆå¯¹åŒºåˆ†æ€§ï¼ˆPDï¼‰çš„ä¸€ç»´éš¾åº¦è¯¾ç¨‹æ•´åˆäº†å¤šä¸ªé…å¯¹ï¼Œä½†å¿½ç•¥äº†è¾“å…¥æç¤ºæœ¬èº«çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†äºŒç»´è¯¾ç¨‹å¼DPOï¼ˆ2D-Curri-DPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨äºŒç»´è¯¾ç¨‹çš„æ–°å‹æ¡†æ¶ï¼ŒåŒæ—¶å»ºæ¨¡æç¤ºå¤æ‚æ€§ï¼ˆPCï¼‰å’Œæˆå¯¹åŒºåˆ†æ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥åŒé‡éš¾åº¦æŒ‡æ ‡æ¥é‡åŒ–æç¤ºè¯­ä¹‰å¤æ‚æ€§å’Œå“åº”åå¥½æ¸…æ™°åº¦ï¼Œå®šä¹‰äº†ä¸€ä¸ªæ¶µç›–å¤šä¸ªå¯é€‰ç­–ç•¥çš„è¯¾ç¨‹ç­–ç•¥ç©ºé—´ä»¥è¿›è¡Œä»»åŠ¡é€‚åº”ï¼Œå¹¶é‡‡ç”¨äº†åŸºäºKLæ•£åº¦çš„è‡ªé€‚åº”æœºåˆ¶è¿›è¡ŒåŠ¨æ€å‚è€ƒæ¨¡å‹æ›´æ–°ï¼Œä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒäºŒç»´è¯¾ç¨‹å¼DPOæ˜¾è‘—ä¼˜äºæ ‡å‡†DPOå’Œå…ˆå‰è¯¾ç¨‹æ–¹æ³•ï¼ŒåŒ…æ‹¬MT-Benchã€Vicuna Benchå’ŒWizardLMç­‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•é›†å¦‚UltraFeedbackä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚æ¶ˆèç ”ç©¶è¯å®äº†äºŒç»´ç»“æ„å’Œè‡ªé€‚åº”æœºåˆ¶çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶åˆ†æä¸ºç­–ç•¥é€‰æ‹©æä¾›äº†æŒ‡å¯¼ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæœ‰æ•ˆçš„å¯¹é½éœ€è¦åŒæ—¶å¯¹æç¤ºå¤æ‚æ€§å’Œæˆå¯¹åŒºåˆ†æ€§è¿›è¡Œå»ºæ¨¡ï¼Œå»ºç«‹è‡ªé€‚åº”çš„å¤šç»´è¯¾ç¨‹å­¦ä¹ å·²æˆä¸ºåŸºäºåå¥½çš„è¯­è¨€æ¨¡å‹ä¼˜åŒ–çš„å¼ºå¤§ä¸”å¯è§£é‡Šçš„æ–°èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07856v1">PDF</a> 12 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½ä¸äººç±»åå¥½å¯¹äºå…¶å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ã€‚é’ˆå¯¹ä¼ ç»Ÿç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•ä¾èµ–äºå•ä¸€åå¥½å¯¹çš„å±€é™æ€§ï¼Œæœ€è¿‘çš„ç ”ç©¶å¦‚Curriculum-DPOé€šè¿‡ä¸€ç»´éš¾åº¦è¯¾ç¨‹æ¥æ•´åˆå¤šä¸ªåå¥½å¯¹ã€‚ç„¶è€Œï¼Œå®ƒå¿½ç•¥äº†è¾“å…¥æç¤ºæœ¬èº«çš„å¤æ‚æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„2D-Curri-DPOæ¡†æ¶ï¼Œé‡‡ç”¨äºŒç»´è¯¾ç¨‹è”åˆå»ºæ¨¡æç¤ºå¤æ‚æ€§å’Œé…å¯¹åŒºåˆ†åº¦ã€‚è¯¥æ¡†æ¶å¼•å…¥åŒé‡éš¾åº¦æŒ‡æ ‡æ¥é‡åŒ–æç¤ºè¯­ä¹‰å¤æ‚æ€§å’Œå“åº”åå¥½æ¸…æ™°åº¦ï¼Œå®šä¹‰äº†ä¸€ä¸ªåŒ…å«å¤šä¸ªå¯é€‰ç­–ç•¥çš„è¯¾ç¨‹ç­–ç•¥ç©ºé—´ä»¥é€‚åº”ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨äº†åŸºäºKLæ•£åº¦çš„è‡ªé€‚åº”æœºåˆ¶è¿›è¡ŒåŠ¨æ€å‚è€ƒæ¨¡å‹æ›´æ–°ï¼Œä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œ2D-Curri-DPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†DPOå’Œå…¶ä»–è¯¾ç¨‹æ–¹æ³•ï¼ŒåŒ…æ‹¬MT-Benchã€Vicuna Benchå’ŒWizardLMã€‚è¯¥æ¡†æ¶åœ¨æŒ‘æˆ˜æ€§æµ‹è¯•é›†UltraFeedbackä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¯å®äº†äºŒç»´ç»“æ„å’Œè‡ªé€‚åº”æœºåˆ¶çš„ä¼˜åŠ¿ï¼Œåˆ†æä¸ºç­–ç•¥é€‰æ‹©æä¾›äº†æŒ‡å¯¼ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæœ‰æ•ˆçš„å¯¹é½éœ€è¦åŒæ—¶å¯¹æç¤ºå¤æ‚æ€§å’Œé…å¯¹åŒºåˆ†åº¦è¿›è¡Œå»ºæ¨¡ï¼Œç¡®ç«‹äº†è‡ªé€‚åº”ã€å¤šç»´çš„è¯¾ç¨‹å­¦ä¹ ä½œä¸ºåŸºäºåå¥½çš„è¯­è¨€æ¨¡å‹ä¼˜åŒ–çš„å¼ºå¤§ä¸”å¯è§£é‡Šçš„æ–°èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½å¯¹å…¶å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•ä¾èµ–äºå•ä¸€åå¥½å¯¹ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>2D-Curri-DPOæ¡†æ¶é‡‡ç”¨äºŒç»´è¯¾ç¨‹è”åˆå»ºæ¨¡æç¤ºå¤æ‚æ€§å’Œé…å¯¹åŒºåˆ†åº¦ã€‚</li>
<li>æ¡†æ¶å¼•å…¥åŒé‡éš¾åº¦æŒ‡æ ‡ï¼Œé‡åŒ–æç¤ºè¯­ä¹‰å¤æ‚æ€§å’Œå“åº”åå¥½æ¸…æ™°åº¦ã€‚</li>
<li>å®šä¹‰äº†åŒ…å«å¤šä¸ªå¯é€‰ç­–ç•¥çš„è¯¾ç¨‹ç­–ç•¥ç©ºé—´ï¼Œé€‚åº”ä¸åŒä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>é‡‡ç”¨åŸºäºKLæ•£åº¦çš„è‡ªé€‚åº”æœºåˆ¶è¿›è¡ŒåŠ¨æ€å‚è€ƒæ¨¡å‹æ›´æ–°ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fa234b82d775dde240306f8519ad08d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9182bf71fe5ec139591c2eac3e17fbeb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4838cd1b06b39b7ed888c5e217b0a56.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Understanding-Learner-LLM-Chatbot-Interactions-and-the-Impact-of-Prompting-Guidelines"><a href="#Understanding-Learner-LLM-Chatbot-Interactions-and-the-Impact-of-Prompting-Guidelines" class="headerlink" title="Understanding Learner-LLM Chatbot Interactions and the Impact of   Prompting Guidelines"></a>Understanding Learner-LLM Chatbot Interactions and the Impact of   Prompting Guidelines</h2><p><strong>Authors:Cansu Koyuturk, Emily Theophilou, Sabrina Patania, Gregor Donabauer, Andrea Martinenghi, Chiara Antico, Alessia Telari, Alessia Testa, Sathya Bursic, Franca Garzotto, Davinia Hernandez-Leo, Udo Kruschwitz, Davide Taibi, Simona Amenta, Martin Ruskov, Dimitri Ognibene</strong></p>
<p>Large Language Models (LLMs) have transformed human-computer interaction by enabling natural language-based communication with AI-powered chatbots. These models are designed to be intuitive and user-friendly, allowing users to articulate requests with minimal effort. However, despite their accessibility, studies reveal that users often struggle with effective prompting, resulting in inefficient responses. Existing research has highlighted both the limitations of LLMs in interpreting vague or poorly structured prompts and the difficulties users face in crafting precise queries. This study investigates learner-AI interactions through an educational experiment in which participants receive structured guidance on effective prompting. We introduce and compare three types of prompting guidelines: a task-specific framework developed through a structured methodology and two baseline approaches. To assess user behavior and prompting efficacy, we analyze a dataset of 642 interactions from 107 users. Using Von NeuMidas, an extended pragmatic annotation schema for LLM interaction analysis, we categorize common prompting errors and identify recurring behavioral patterns. We then evaluate the impact of different guidelines by examining changes in user behavior, adherence to prompting strategies, and the overall quality of AI-generated responses. Our findings provide a deeper understanding of how users engage with LLMs and the role of structured prompting guidance in enhancing AI-assisted communication. By comparing different instructional frameworks, we offer insights into more effective approaches for improving user competency in AI interactions, with implications for AI literacy, chatbot usability, and the design of more responsive AI systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä½¿äººä¸AIé©±åŠ¨çš„èŠå¤©æœºå™¨äººè¿›è¡ŒåŸºäºè‡ªç„¶è¯­è¨€äº¤æµï¼Œä»è€Œæ”¹å˜äº†äººæœºäº¤äº’æ–¹å¼ã€‚è¿™äº›æ¨¡å‹è¢«è®¾è®¡æˆç›´è§‚å’Œç”¨æˆ·å‹å¥½ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾è¡¨è¾¾è¯·æ±‚ã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒä»¬æ˜“äºè®¿é—®ï¼Œä½†ç ”ç©¶è¡¨æ˜ï¼Œç”¨æˆ·åœ¨æœ‰æ•ˆæç¤ºæ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´å“åº”æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰ç ”ç©¶å·²ç»çªå‡ºäº†LLMåœ¨è§£é‡Šæ¨¡ç³Šæˆ–ç»“æ„ä¸è‰¯æç¤ºæ–¹é¢çš„å±€é™æ€§ä»¥åŠç”¨æˆ·åœ¨åˆ¶ä½œç²¾ç¡®æŸ¥è¯¢æ–¹é¢æ‰€é¢ä¸´çš„å›°éš¾ã€‚æœ¬ç ”ç©¶é€šè¿‡æ•™è‚²å®éªŒè°ƒæŸ¥äº†å­¦ä¹ è€…ä¸AIçš„äº’åŠ¨ï¼Œå‚ä¸è€…æ¥å—äº†å…³äºæœ‰æ•ˆæç¤ºçš„ç»“æ„åŒ–æŒ‡å¯¼ã€‚æˆ‘ä»¬ä»‹ç»å¹¶æ¯”è¾ƒäº†ä¸‰ç§ç±»å‹çš„æç¤ºå‡†åˆ™ï¼šé€šè¿‡ç»“æ„åŒ–æ–¹æ³•å¼€å‘çš„ä»»åŠ¡ç‰¹å®šæ¡†æ¶ï¼Œä»¥åŠä¸¤ç§åŸºå‡†æ–¹æ³•ã€‚ä¸ºäº†è¯„ä¼°ç”¨æˆ·è¡Œä¸ºå’Œæç¤ºæ•ˆæœï¼Œæˆ‘ä»¬åˆ†æäº†æ¥è‡ª107åç”¨æˆ·çš„642ä¸ªäº’åŠ¨æ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨ç”¨äºLLMäº¤äº’åˆ†æçš„æ‰©å±•è¯­ç”¨æ³¨é‡Šæ¨¡å¼å†¯Â·çº½è¿ˆè¾¾æ–¯ï¼ˆNeuMidasï¼‰ï¼Œå¯¹å¸¸è§çš„æç¤ºé”™è¯¯è¿›è¡Œåˆ†ç±»å¹¶ç¡®å®šåå¤å‡ºç°çš„è¡Œä¸ºæ¨¡å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è§‚å¯Ÿç”¨æˆ·è¡Œä¸ºçš„å˜åŒ–ã€éµå¾ªæç¤ºç­–ç•¥çš„æƒ…å†µä»¥åŠAIç”Ÿæˆçš„æ€»ä½“å“åº”è´¨é‡æ¥è¯„ä¼°ä¸åŒæŒ‡å¯¼æ–¹é’ˆçš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæä¾›äº†ç”¨æˆ·å¯¹LLMçš„å‚ä¸æ–¹å¼å’Œç»“æ„åŒ–æç¤ºæŒ‡å¯¼åœ¨å¢å¼ºAIè¾…åŠ©é€šä¿¡æ–¹é¢çš„ä½œç”¨çš„ç†è§£ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒçš„æ•™å­¦æ¡†æ¶ï¼Œæˆ‘ä»¬æä¾›äº†æé«˜ç”¨æˆ·åœ¨AIäº¤äº’ä¸­çš„èƒ½åŠ›æ›´æœ‰æ•ˆæ–¹æ³•çš„ç›¸å…³è§è§£ï¼Œå¯¹AIç´ å…»ã€èŠå¤©æœºå™¨äººçš„å¯ç”¨æ€§ã€è®¾è®¡å“åº”æ›´å¿«çš„AIç³»ç»Ÿæœ‰å¯ç¤ºä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07840v1">PDF</a> Accepted for AIED 2025, the 26th International Conference on   Artificial Intelligence in Education, July 22 - 26, 2025, Palermo, Italy</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å®ç°ä¸AIé©±åŠ¨çš„èŠå¤©æœºå™¨äººçš„è‡ªç„¶è¯­è¨€äº¤æµï¼Œæ”¹å˜äº†äººä¸è®¡ç®—æœºçš„äº¤äº’æ–¹å¼ã€‚è¿™äº›æ¨¡å‹è®¾è®¡å¾—ç›´è§‚å’Œç”¨æˆ·å‹å¥½ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾è¡¨è¾¾è¯·æ±‚ã€‚ç„¶è€Œï¼Œå°½ç®¡å…¶æ˜“äºè®¿é—®ï¼Œä½†ç ”ç©¶è¡¨æ˜ç”¨æˆ·å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°æç¤ºï¼Œå¯¼è‡´å“åº”æ•ˆç‡ä½ä¸‹ã€‚æœ¬ç ”ç©¶é€šè¿‡æ•™è‚²å®éªŒè°ƒæŸ¥äº†å­¦ä¹ è€…ä¸AIçš„äº¤äº’ï¼Œå‚ä¸è€…æ¥å—äº†å…³äºæœ‰æ•ˆæç¤ºçš„ç»“æ„åŒ–æŒ‡å¯¼ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸‰ç§æç¤ºæŒ‡å—ç±»å‹å¹¶è¿›è¡Œæ¯”è¾ƒï¼šé€šè¿‡ç»“æ„åŒ–æ–¹æ³•å¼€å‘çš„ä»»åŠ¡ç‰¹å®šæ¡†æ¶ï¼Œä»¥åŠä¸¤ç§åŸºçº¿æ–¹æ³•ã€‚ä¸ºäº†è¯„ä¼°ç”¨æˆ·è¡Œä¸ºå’Œæç¤ºæ•ˆæœï¼Œæˆ‘ä»¬åˆ†æäº†æ¥è‡ª107åç”¨æˆ·çš„642ä¸ªäº¤äº’æ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨ç”¨äºLLMäº¤äº’åˆ†æçš„æ‰©å±•è¯­ç”¨æ³¨é‡Šæ¨¡å¼â€œå†¯Â·çº½è¿ˆè¾¾æ–¯â€ï¼Œå¯¹å¸¸è§çš„æç¤ºé”™è¯¯è¿›è¡Œåˆ†ç±»å¹¶ç¡®å®šåå¤å‡ºç°çš„è¡Œä¸ºæ¨¡å¼ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡è§‚å¯Ÿç”¨æˆ·è¡Œä¸ºçš„å˜åŒ–ã€éµå¾ªæç¤ºç­–ç•¥çš„æƒ…å†µä»¥åŠAIç”Ÿæˆçš„å“åº”çš„æ•´ä½“è´¨é‡æ¥è¯„ä¼°ä¸åŒæŒ‡å—çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçš„å‘ç°ä¸ºæ›´å¥½åœ°äº†è§£ç”¨æˆ·å¦‚ä½•ä¸LLMäº’åŠ¨ä»¥åŠç»“æ„åŒ–æç¤ºæŒ‡å¯¼åœ¨æé«˜AIè¾…åŠ©æ²Ÿé€šä¸­çš„ä½œç”¨æä¾›äº†æ·±å…¥äº†è§£ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒçš„æ•™å­¦æ¡†æ¶ï¼Œæˆ‘ä»¬æä¾›äº†å…³äºå¦‚ä½•æé«˜ç”¨æˆ·åœ¨ä¸AIäº’åŠ¨ä¸­çš„èƒ½åŠ›æ›´æœ‰æ•ˆçš„é€”å¾„çš„è§è§£ï¼Œè¿™å¯¹AIç´ å…»ã€èŠå¤©æœºå™¨äººçš„å¯ç”¨æ€§ã€ä»¥åŠè®¾è®¡æ›´å“åº”å¼çš„AIç³»ç»Ÿå…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è‡ªç„¶è¯­è¨€äº¤æµæ”¹å˜äº†äººæœºäº’åŠ¨æ–¹å¼ï¼Œä½†ç”¨æˆ·åœ¨æœ‰æ•ˆæç¤ºæ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´å“åº”æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>ç ”ç©¶é€šè¿‡æ•™è‚²å®éªŒè°ƒæŸ¥äº†å­¦ä¹ è€…ä¸AIçš„äº¤äº’ï¼Œå¹¶å¼•å…¥äº†ä¸‰ç§ä¸åŒç±»å‹çš„æç¤ºæŒ‡å—è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>é€šè¿‡åˆ†æå¤§é‡ç”¨æˆ·ä¸AIçš„äº¤äº’æ•°æ®ï¼Œå‘ç°äº†å¸¸è§çš„æç¤ºé”™è¯¯å’Œåå¤å‡ºç°çš„è¡Œä¸ºæ¨¡å¼ã€‚</li>
<li>ç»“æ„åŒ–çš„æç¤ºæŒ‡å—æœ‰åŠ©äºæé«˜ç”¨æˆ·ä¸AIçš„äº¤äº’æ•ˆæœï¼Œæ”¹å–„AIç”Ÿæˆçš„å“åº”è´¨é‡ã€‚</li>
<li>ä¸åŒæ•™å­¦æ¡†æ¶åœ¨æé«˜ç”¨æˆ·ä¸AIäº’åŠ¨èƒ½åŠ›æ–¹é¢çš„æ•ˆæœæœ‰æ‰€ä¸åŒï¼Œè¿™ä¸ºæˆ‘ä»¬æä¾›äº†æ”¹è¿›æ–¹å‘ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹æå‡AIç´ å…»ã€èŠå¤©æœºå™¨äººå¯ç”¨æ€§ä»¥åŠè®¾è®¡æ›´å“åº”å¼çš„AIç³»ç»Ÿå…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07840">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ca977d5fc1c7c297755359acb6ea76ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4dfb47fe353f4a4bbe42c74e986428d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac3eff450353e787e064d1bc16d5baee.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Plan-and-Refine-Diverse-and-Comprehensive-Retrieval-Augmented-Generation"><a href="#Plan-and-Refine-Diverse-and-Comprehensive-Retrieval-Augmented-Generation" class="headerlink" title="Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented   Generation"></a>Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented   Generation</h2><p><strong>Authors:Alireza Salemi, Chris Samarinas, Hamed Zamani</strong></p>
<p>This paper studies the limitations of (retrieval-augmented) large language models (LLMs) in generating diverse and comprehensive responses, and introduces the Plan-and-Refine (P&amp;R) framework based on a two phase system design. In the global exploration phase, P&amp;R generates a diverse set of plans for the given input, where each plan consists of a list of diverse query aspects with corresponding additional descriptions. This phase is followed by a local exploitation phase that generates a response proposal for the input query conditioned on each plan and iteratively refines the proposal for improving the proposal quality. Finally, a reward model is employed to select the proposal with the highest factuality and coverage. We conduct our experiments based on the ICAT evaluation methodologyâ€“a recent approach for answer factuality and comprehensiveness evaluation. Experiments on the two diverse information seeking benchmarks adopted from non-factoid question answering and TREC search result diversification tasks demonstrate that P&amp;R significantly outperforms baselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a 15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study confirms the substantial efficacy of the P&amp;R framework. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†ï¼ˆæ£€ç´¢å¢å¼ºï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå¤šæ ·åŒ–å’Œå…¨é¢å“åº”æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶åŸºäºä¸¤é˜¶æ®µç³»ç»Ÿè®¾è®¡å¼•å…¥äº†Plan-and-Refineï¼ˆP&amp;Rï¼‰æ¡†æ¶ã€‚åœ¨å…¨å±€æ¢ç´¢é˜¶æ®µï¼ŒP&amp;Rä¸ºç»™å®šè¾“å…¥ç”Ÿæˆå¤šæ ·åŒ–çš„è®¡åˆ’é›†ï¼Œå…¶ä¸­æ¯ä¸ªè®¡åˆ’ç”±ä¸€ç³»åˆ—å¤šæ ·åŒ–çš„æŸ¥è¯¢æ–¹é¢å’Œç›¸åº”çš„é™„åŠ æè¿°ç»„æˆã€‚æ¥ä¸‹æ¥æ˜¯å±€éƒ¨å¼€é‡‡é˜¶æ®µï¼Œè¯¥é˜¶æ®µæ ¹æ®æ¯ä¸ªè®¡åˆ’å¯¹è¾“å…¥æŸ¥è¯¢ç”Ÿæˆå“åº”ææ¡ˆï¼Œå¹¶è¿­ä»£æ”¹è¿›ææ¡ˆä»¥æé«˜ææ¡ˆè´¨é‡ã€‚æœ€åï¼Œé‡‡ç”¨å¥–åŠ±æ¨¡å‹æ¥é€‰æ‹©å…·æœ‰æœ€é«˜äº‹å®å’Œè¦†ç›–ç‡çš„ææ¡ˆã€‚æˆ‘ä»¬çš„å®éªŒåŸºäºICATè¯„ä¼°æ–¹æ³•â€”â€”ä¸€ç§ç”¨äºç­”æ¡ˆäº‹å®å’Œå…¨é¢æ€§çš„æœ€æ–°è¯„ä¼°æ–¹æ³•ã€‚åœ¨éäº‹å®å‹é—®ç­”å’ŒTRECæœç´¢ç»“æœå¤šæ ·åŒ–ä»»åŠ¡ä¸­é‡‡ç”¨çš„ä¸¤ç§å¤šæ ·åŒ–ä¿¡æ¯æœç´¢åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒP&amp;Ræ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œåœ¨ANTIQUEæ•°æ®é›†ä¸Šæœ€å¤šæé«˜äº†13.1%ï¼Œåœ¨TRECæ•°æ®é›†ä¸Šæé«˜äº†15.41%ã€‚æ­¤å¤–ï¼Œå°è§„æ¨¡çš„ç”¨æˆ·ç ”ç©¶è¯å®äº†P&amp;Ræ¡†æ¶çš„å®è´¨æ€§æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07794v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è®¡åˆ’å’Œå®Œå–„ï¼ˆP&amp;Rï¼‰æ¡†æ¶èƒ½å¤Ÿè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå¤šæ ·åŒ–å’Œå…¨é¢å“åº”æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åŸºäºä¸¤é˜¶æ®µç³»ç»Ÿè®¾è®¡ï¼Œé¦–å…ˆåœ¨å…¨çƒæ¢ç´¢é˜¶æ®µç”Ÿæˆç»™å®šè¾“å…¥çš„å¤šæ ·åŒ–è®¡åˆ’ï¼Œç„¶ååœ¨å±€éƒ¨å¼€å‘é˜¶æ®µæ ¹æ®æ¯ä¸ªè®¡åˆ’ç”Ÿæˆå“åº”ææ¡ˆå¹¶è¿­ä»£æ”¹è¿›ææ¡ˆè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒP&amp;Råœ¨ICATè¯„ä¼°æ–¹æ³•çš„åŸºç¡€ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œåœ¨éäº‹å®é—®ç­”å’ŒTRECæœç´¢ç»“æœå¤šæ ·åŒ–ä»»åŠ¡çš„ä¸¤ä¸ªä¸åŒä¿¡æ¯æœç´¢åŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ€é«˜å¯æé«˜13.1%ï¼ˆåœ¨ANTIQUEæ•°æ®é›†ä¸Šï¼‰å’Œ15.41%ï¼ˆåœ¨TRECæ•°æ®é›†ä¸Šï¼‰ã€‚å°è§„æ¨¡çš„ç”¨æˆ·ç ”ç©¶ä¹Ÿè¯å®äº†P&amp;Ræ¡†æ¶çš„æ˜¾è‘—æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå¤šæ ·åŒ–å’Œå…¨é¢çš„å“åº”æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è®¡åˆ’å’Œå®Œå–„ï¼ˆP&amp;Rï¼‰æ¡†æ¶åŸºäºä¸¤é˜¶æ®µç³»ç»Ÿè®¾è®¡æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>åœ¨å…¨çƒæ¢ç´¢é˜¶æ®µï¼ŒP&amp;Rç”Ÿæˆç»™å®šè¾“å…¥çš„å¤šæ ·åŒ–è®¡åˆ’ã€‚</li>
<li>åœ¨å±€éƒ¨å¼€å‘é˜¶æ®µï¼Œæ ¹æ®æ¯ä¸ªè®¡åˆ’ç”Ÿæˆå“åº”ææ¡ˆå¹¶è¿­ä»£æ”¹è¿›ã€‚</li>
<li>ä½¿ç”¨å¥–åŠ±æ¨¡å‹é€‰æ‹©æœ€å…·äº‹å®å’Œè¦†ç›–ç‡çš„ææ¡ˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒP&amp;Råœ¨éäº‹å®é—®ç­”å’ŒTRECæœç´¢ç»“æœå¤šæ ·åŒ–ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6f157ce3f3c5a0936abbb91be240952.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SF2T-Self-supervised-Fragment-Finetuning-of-Video-LLMs-for-Fine-Grained-Understanding"><a href="#SF2T-Self-supervised-Fragment-Finetuning-of-Video-LLMs-for-Fine-Grained-Understanding" class="headerlink" title="SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained   Understanding"></a>SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained   Understanding</h2><p><strong>Authors:Yangliu Hu, Zikai Song, Na Feng, Yawei Luo, Junqing Yu, Yi-Ping Phoebe Chen, Wei Yang</strong></p>
<p>Video-based Large Language Models (Video-LLMs) have witnessed substantial advancements in recent years, propelled by the advancement in multi-modal LLMs. Although these models have demonstrated proficiency in providing the overall description of videos, they struggle with fine-grained understanding, particularly in aspects such as visual dynamics and video details inquiries. To tackle these shortcomings, we find that fine-tuning Video-LLMs on self-supervised fragment tasks, greatly improve their fine-grained video understanding abilities. Hence we propose two key contributions:(1) Self-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning method, employs the rich inherent characteristics of videos for training, while unlocking more fine-grained understanding ability of Video-LLMs. Moreover, it relieves researchers from labor-intensive annotations and smartly circumvents the limitations of natural language, which often fails to capture the complex spatiotemporal variations in videos; (2) A novel benchmark dataset, namely FineVidBench, for rigorously assessing Video-LLMsâ€™ performance at both the scene and fragment levels, offering a comprehensive evaluation of their capabilities. We assessed multiple models and validated the effectiveness of SF$^2$T on them. Experimental results reveal that our approach improves their ability to capture and interpret spatiotemporal details. </p>
<blockquote>
<p>åŸºäºè§†é¢‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰è¿‘å¹´æ¥å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè¿™å¾—ç›Šäºå¤šæ¨¡æ€LLMsçš„è¿›æ­¥ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨æä¾›è§†é¢‘æ•´ä½“æè¿°æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç²¾ç»†ç†è§£æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘åŠ¨æ€å’Œç»†èŠ‚æŸ¥è¯¢ç­‰æ–¹é¢ã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬å‘ç°å¯¹Video-LLMsè¿›è¡Œè‡ªç›‘ç£ç‰‡æ®µä»»åŠ¡çš„å¾®è°ƒï¼Œå¯ä»¥å¤§å¤§æé«˜å…¶ç²¾ç»†è§†é¢‘ç†è§£çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®è´¡çŒ®ï¼šï¼ˆ1ï¼‰è‡ªç›‘ç£ç‰‡æ®µå¾®è°ƒï¼ˆSF$^2$Tï¼‰æ˜¯ä¸€ç§æ–°å‹çš„æ— è´¹åŠ›å¾®è°ƒæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨è§†é¢‘ä¸°å¯Œçš„å†…åœ¨ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶è§£é”Video-LLMsçš„æ›´å¤šç²¾ç»†ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®ƒå‡è½»äº†ç ”ç©¶äººå‘˜å¯¹åŠ³åŠ¨å¯†é›†å‹æ³¨é‡Šçš„ä¾èµ–ï¼Œå¹¶å·§å¦™åœ°å…‹æœäº†è‡ªç„¶è¯­è¨€çš„é™åˆ¶ï¼Œåè€…é€šå¸¸æ— æ³•æ•æ‰è§†é¢‘ä¸­çš„å¤æ‚æ—¶ç©ºå˜åŒ–ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªç”¨äºä¸¥æ ¼è¯„ä¼°Video-LLMsåœ¨åœºæ™¯å’Œç‰‡æ®µçº§åˆ«çš„æ€§èƒ½çš„æ–°å‹åŸºå‡†æ•°æ®é›†FineVidBenchï¼Œå¯¹å®ƒä»¬çš„èƒ½åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªæ¨¡å‹ï¼Œå¹¶éªŒè¯äº†å®ƒä»¬åœ¨SF$^2$Tä¸Šçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å®ƒä»¬æ•æ‰å’Œè§£é‡Šæ—¶ç©ºç»†èŠ‚çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07745v1">PDF</a> Accepted to CVPR2025</p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘ç±»å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰è¿‘å¹´æ¥åœ¨å¤šæ¨¡æ€LLMsçš„æ¨åŠ¨ä¸‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ç²¾ç»†ç†è§£è§†é¢‘æ–¹é¢çš„ä¸è¶³ï¼Œå¦‚è§†è§‰åŠ¨æ€å’Œè§†é¢‘ç»†èŠ‚æŸ¥è¯¢ç­‰æ–¹é¢ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å‘ç°é€šè¿‡è‡ªæˆ‘ç›‘ç£ç‰‡æ®µä»»åŠ¡å¯¹Video-LLMsè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥å¤§å¤§æé«˜å…¶ç²¾ç»†è§†é¢‘ç†è§£çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤é¡¹å…³é”®è´¡çŒ®ï¼šä¸€æ˜¯è‡ªæˆ‘ç›‘ç£ç‰‡æ®µå¾®è°ƒï¼ˆSF^2Tï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†é¢‘çš„ä¸°å¯Œå†…åœ¨ç‰¹æ€§è¿›è¡Œè®­ç»ƒï¼Œè§£é”äº†Video-LLMsçš„ç²¾ç»†ç†è§£èƒ½åŠ›ï¼Œæ— éœ€ç¹ççš„äººå·¥æ ‡æ³¨ï¼Œæœ‰æ•ˆè§„é¿äº†è‡ªç„¶è¯­è¨€åœ¨æ•æ‰è§†é¢‘å¤æ‚æ—¶ç©ºå˜åŒ–æ–¹é¢çš„å±€é™ï¼›äºŒæ˜¯æ¨å‡ºä¸€ä¸ªåä¸ºFineVidBenchçš„æ–°åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºä¸¥æ ¼è¯„ä¼°Video-LLMsåœ¨åœºæ™¯å’Œç‰‡æ®µä¸¤ä¸ªå±‚æ¬¡ä¸Šçš„æ€§èƒ½ï¼Œä¸ºå…¨é¢è¯„ä»·å…¶èƒ½åŠ›æä¾›äº†å¹³å°ã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨æé«˜æ•æ‰å’Œè§£é‡Šæ—¶ç©ºç»†èŠ‚çš„èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-LLMsè¿‘å¹´è™½æœ‰å¤šæ¨¡æ€è¿›å±•ï¼Œä½†åœ¨è§†é¢‘ç²¾ç»†ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>è‡ªæˆ‘ç›‘ç£ç‰‡æ®µå¾®è°ƒï¼ˆSF^2Tï¼‰æ–¹æ³•æ—¨åœ¨æé«˜Video-LLMsçš„ç²¾ç»†è§†é¢‘ç†è§£èƒ½åŠ›ã€‚</li>
<li>SF^2Tåˆ©ç”¨è§†é¢‘çš„ä¸°å¯Œå†…åœ¨ç‰¹æ€§è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€å¤§é‡äººå·¥æ ‡æ³¨ã€‚</li>
<li>SF^2Tæœ‰æ•ˆè§„é¿äº†è‡ªç„¶è¯­è¨€åœ¨æ•æ‰è§†é¢‘å¤æ‚æ—¶ç©ºå˜åŒ–æ–¹é¢çš„å±€é™ã€‚</li>
<li>FineVidBenchæ•°æ®é›†ç”¨äºè¯„ä¼°Video-LLMsåœ¨åœºæ™¯å’Œç‰‡æ®µä¸¤ä¸ªå±‚æ¬¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒéªŒè¯äº†SF^2Tæ–¹æ³•åœ¨æé«˜æ•æ‰å’Œè§£é‡Šè§†é¢‘æ—¶ç©ºç»†èŠ‚çš„èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06d151bd308ac9e9827c04ace53ec607.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8106253022ea39913793e035a953597a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12f988f68dfb95911f8719f03269f314.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f5f35897273a2b18286ce5d42af5322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-723c71f7ced37f6aa90a8380396c1d72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44956df753b7cc9bc5f9d3ca1580e6f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-961cea864c85aeaa3aed9adbc7c00d7c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Cross-Domain-Code-Search-without-Fine-Tuning"><a href="#Zero-Shot-Cross-Domain-Code-Search-without-Fine-Tuning" class="headerlink" title="Zero-Shot Cross-Domain Code Search without Fine-Tuning"></a>Zero-Shot Cross-Domain Code Search without Fine-Tuning</h2><p><strong>Authors:Keyu Liang, Zhongxin Liu, Chao Liu, Zhiyuan Wan, David Lo, Xiaohu Yang</strong></p>
<p>Code search aims to retrieve semantically relevant code snippets for natural language queries. While pre-trained language models (PLMs) have shown remarkable performance in this task, they struggle in cross-domain scenarios, often requiring costly fine-tuning or facing performance drops in zero-shot settings. RAPID, which generates synthetic data for model fine-tuning, is currently the only effective method for zero-shot cross-domain code search. Despite its effectiveness, RAPID demands substantial computational resources for fine-tuning and needs to maintain specialized models for each domain, underscoring the need for a zero-shot, fine-tuning-free approach for cross-domain code search.   The key to tackling zero-shot cross-domain code search lies in bridging the gaps among domains. In this work, we propose to break the query-code matching process of code search into two simpler tasks: query-comment matching and code-code matching. Our empirical study reveals the strong complementarity among the three matching schemas in zero-shot cross-domain settings, i.e., query-code, query-comment, and code-code matching. Based on the findings, we propose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain code search. Specifically, CodeBridge uses Large Language Models (LLMs) to generate comments and pseudo-code, then combines query-code, query-comment, and code-code matching via PLM-based similarity scoring and sampling-based fusion. Experimental results show that our approach outperforms the state-of-the-art PLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average of 21.4% and 24.9% in MRR, respectively, across three datasets. Our approach also yields results that are better than or comparable to those of the zero-shot cross-domain code search approach RAPID, which requires costly fine-tuning. </p>
<blockquote>
<p>ä»£ç æœç´¢æ—¨åœ¨é’ˆå¯¹è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ£€ç´¢è¯­ä¹‰ç›¸å…³çš„ä»£ç ç‰‡æ®µã€‚è™½ç„¶é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨è·¨åŸŸåœºæ™¯ä¸­å®ƒä»¬é¢ä¸´æŒ‘æˆ˜ï¼Œé€šå¸¸éœ€è¦æ˜‚è´µçš„å¾®è°ƒï¼Œæˆ–åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­é¢ä¸´æ€§èƒ½ä¸‹é™ã€‚RAPIDæ˜¯ä¸€ç§ä¸ºæ¨¡å‹å¾®è°ƒç”Ÿæˆåˆæˆæ•°æ®çš„æ–¹æ³•ï¼Œç›®å‰æ˜¯é›¶æ ·æœ¬è·¨åŸŸä»£ç æœç´¢ä¸­å”¯ä¸€æœ‰æ•ˆçš„æ–¹æ³•ã€‚å°½ç®¡å…¶æœ‰æ•ˆï¼Œä½†RAPIDéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºè¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸”éœ€è¦ä¸ºæ¯ä¸ªåŸŸç»´æŠ¤ä¸“ç”¨æ¨¡å‹ï¼Œè¿™çªæ˜¾äº†é›¶æ ·æœ¬ã€æ— éœ€å¾®è°ƒçš„è·¨åŸŸä»£ç æœç´¢æ–¹æ³•çš„å¿…è¦æ€§ã€‚è§£å†³é›¶æ ·æœ¬è·¨åŸŸä»£ç æœç´¢çš„å…³é”®åœ¨äºç¼©å°åŸŸä¹‹é—´çš„å·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®å°†ä»£ç æœç´¢çš„æŸ¥è¯¢-ä»£ç åŒ¹é…è¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªæ›´ç®€å•çš„ä»»åŠ¡ï¼šæŸ¥è¯¢-æ³¨é‡ŠåŒ¹é…å’Œä»£ç -ä»£ç åŒ¹é…ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶æ­ç¤ºäº†ä¸‰ç§åŒ¹é…æ¨¡å¼åœ¨é›¶æ ·æœ¬è·¨åŸŸè®¾ç½®ä¸­çš„å¼ºçƒˆäº’è¡¥æ€§ï¼Œå³æŸ¥è¯¢-ä»£ç ã€æŸ¥è¯¢-æ³¨é‡Šå’Œä»£ç -ä»£ç åŒ¹é…ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†CodeBridgeï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè·¨åŸŸä»£ç æœç´¢çš„é›¶æ ·æœ¬ã€æ— éœ€å¾®è°ƒçš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒCodeBridgeä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ³¨é‡Šå’Œä¼ªä»£ç ï¼Œç„¶åé€šè¿‡åŸºäºPLMçš„ç›¸ä¼¼æ€§è¯„åˆ†å’ŒåŸºäºé‡‡æ ·çš„èåˆï¼Œç»“åˆæŸ¥è¯¢-ä»£ç ã€æŸ¥è¯¢-æ³¨é‡Šå’Œä»£ç -ä»£ç åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„PLMé©±åŠ¨çš„ä»£ç æœç´¢æ–¹æ³•CoCoSoDaå’ŒUniXcoderï¼Œåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡MRRåˆ†åˆ«æé«˜äº†21.4%å’Œ24.9%ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¾—åˆ°çš„ç»“æœä¼˜äºæˆ–ç­‰äºéœ€è¦æ˜‚è´µå¾®è°ƒçš„é›¶æ ·æœ¬è·¨åŸŸä»£ç æœç´¢æ–¹æ³•RAPIDçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07740v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è·¨åŸŸä»£ç æœç´¢é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€å¾®è°ƒçš„é›¶é—¨æ§›è·¨åŸŸä»£ç æœç´¢æ–¹æ³•CodeBridgeã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆæ³¨é‡Šå’Œä¼ªä»£ç ï¼Œç»“åˆæŸ¥è¯¢ä»£ç ã€æŸ¥è¯¢æ³¨é‡Šå’Œä»£ç ä»£ç çš„åŒ¹é…ï¼Œé€šè¿‡åŸºäºPLMçš„ç›¸ä¼¼åº¦è¯„åˆ†å’ŒåŸºäºé‡‡æ ·çš„èåˆï¼Œå®ç°è·¨åŸŸä»£ç æœç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰PLMä»£ç æœç´¢æ–¹æ³•ï¼Œå¹¶ä¸éœ€è¦æ˜‚è´µå¾®è°ƒæˆæœ¬çš„è·¨åŸŸä»£ç æœç´¢æ–¹æ³•RAPIDç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Code searchæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ£€ç´¢è¯­ä¹‰ç›¸å…³çš„ä»£ç ç‰‡æ®µã€‚</li>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨ä»£ç æœç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è·¨åŸŸåœºæ™¯ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æœ‰æ•ˆçš„è·¨åŸŸä»£ç æœç´¢æ–¹æ³•RAPIDéœ€è¦æ˜‚è´µçš„å¾®è°ƒæˆæœ¬å’Œä¸ºæ¯ä¸ªé¢†åŸŸç»´æŠ¤ä¸“ç”¨æ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€å¾®è°ƒçš„é›¶é—¨æ§›è·¨åŸŸä»£ç æœç´¢æ–¹æ³•CodeBridgeã€‚</li>
<li>CodeBridgeåˆ©ç”¨LLMsç”Ÿæˆæ³¨é‡Šå’Œä¼ªä»£ç ï¼Œç»“åˆæŸ¥è¯¢ä¸ä»£ç ã€æŸ¥è¯¢ä¸æ³¨é‡Šä»¥åŠä»£ç ä»£ç çš„åŒ¹é…æ¥è§£å†³è·¨åŸŸä»£ç æœç´¢é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜CodeBridgeåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰PLMä»£ç æœç´¢æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4d733ebbec5792859c3079fa205c98c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GPT-Carry-On-Training-Foundation-Model-for-Customization-Could-Be-Simple-Scalable-and-Affordable"><a href="#GPT-Carry-On-Training-Foundation-Model-for-Customization-Could-Be-Simple-Scalable-and-Affordable" class="headerlink" title="GPT Carry-On: Training Foundation Model for Customization Could Be   Simple, Scalable and Affordable"></a>GPT Carry-On: Training Foundation Model for Customization Could Be   Simple, Scalable and Affordable</h2><p><strong>Authors:Jianqiao Wangni</strong></p>
<p>Modern large language foundation models (LLM) have now entered the daily lives of millions of users. We ask a natural question whether it is possible to customize LLM for every user or every task. From system and industrial economy consideration, general continue-training or fine-tuning still require substantial computation and memory of training GPU nodes, whereas most inference nodes under deployment, possibly with lower-end GPUs, are configured to make forward pass fastest possible. We propose a framework to take full advantages of existing LLMs and systems of online service. We train an additional branch of transformer blocks on the final-layer embedding of pretrained LLMs, which is the base, then a carry-on module merge the base models to compose a customized LLM. We can mix multiple layers, or multiple LLMs specialized in different domains such as chat, coding, math, to form a new mixture of LLM that best fit a new task. As the base model donâ€™t need to update parameters, we are able to outsource most computation of the training job on inference nodes, and only train a lightweight carry-on on training nodes, where we consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM. We tested Qwen and DeepSeek opensourced models for continue-pretraining and got faster loss convergence. We use it to improve solving math questions with extremely small computation and model size, with 1000 data samples of chain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on, and the results are promising. </p>
<blockquote>
<p>ç°ä»£å¤§å‹è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»èå…¥æ•°ç™¾ä¸‡ç”¨æˆ·çš„æ—¥å¸¸ç”Ÿæ´»ä¸­ã€‚æˆ‘ä»¬è‡ªç„¶åœ°æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œæ˜¯å¦å¯ä»¥ä¸ºæ¯ä¸ªç”¨æˆ·æˆ–æ¯ä¸ªä»»åŠ¡å®šåˆ¶LLMã€‚ä»ç³»ç»Ÿå’Œå·¥ä¸šç»æµçš„è§’åº¦æ¥çœ‹ï¼Œä¸€èˆ¬çš„ç»§ç»­è®­ç»ƒæˆ–å¾®è°ƒä»ç„¶éœ€è¦å¤§é‡çš„è®¡ç®—å’Œè®­ç»ƒGPUèŠ‚ç‚¹çš„å†…å­˜ï¼Œè€Œå¤§å¤šæ•°æ­£åœ¨éƒ¨ç½²çš„æ¨ç†èŠ‚ç‚¹ï¼Œå¯èƒ½é…ç½®æœ‰è¾ƒä½ç«¯çš„GPUï¼Œè¢«è®¾ç½®ä¸ºå°½å¯èƒ½å¿«åœ°è¿›è¡Œå‰å‘ä¼ é€’ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œä»¥å……åˆ†åˆ©ç”¨ç°æœ‰çš„LLMå’Œåœ¨çº¿æœåŠ¡ç³»ç»Ÿã€‚æˆ‘ä»¬åœ¨é¢„è®­ç»ƒLLMçš„æœ€ç»ˆå±‚åµŒå…¥åŸºç¡€ä¸Šè®­ç»ƒé¢å¤–çš„è½¬æ¢å™¨å—åˆ†æ”¯ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªç»§ç»­æ¨¡å—åˆå¹¶åŸºç¡€æ¨¡å‹ï¼Œä»¥ç»„æˆå®šåˆ¶çš„LLMã€‚æˆ‘ä»¬å¯ä»¥æ··åˆå¤šå±‚ï¼Œæˆ–è€…æ··åˆä¸“é—¨ç”¨äºä¸åŒé¢†åŸŸçš„å¤šä¸ªLLMï¼Œå¦‚èŠå¤©ã€ç¼–ç ã€æ•°å­¦ç­‰ï¼Œä»¥å½¢æˆæœ€é€‚åˆæ–°ä»»åŠ¡çš„æ–°æ··åˆLLMã€‚ç”±äºåŸºç¡€æ¨¡å‹æ— éœ€æ›´æ–°å‚æ•°ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°†åœ¨æ¨ç†èŠ‚ç‚¹ä¸Šçš„å¤§éƒ¨åˆ†è®­ç»ƒä»»åŠ¡å¤–åŒ…ï¼Œä»…å¯¹è®­ç»ƒèŠ‚ç‚¹è¿›è¡Œè½»é‡çº§çš„ç»§ç»­è®­ç»ƒï¼Œåœ¨30B LLMä¸Šä½¿ç”¨ä¸åˆ°1GBçš„GPUå†…å­˜å³å¯è®­ç»ƒä¸€ä¸ª100Mçš„ç»§ç»­å±‚ã€‚æˆ‘ä»¬æµ‹è¯•äº†Qwenå’ŒDeepSeekçš„å¼€æºæ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œå¹¶è·å¾—äº†æ›´å¿«çš„æŸå¤±æ”¶æ•›ã€‚æˆ‘ä»¬ç”¨å®ƒæ¥æ”¹å–„è§£å†³æ•°å­¦é—®é¢˜çš„æ€§èƒ½ï¼Œåªéœ€æå°é‡çš„è®¡ç®—å’Œæ¨¡å‹å¤§å°ï¼Œä½¿ç”¨1000ä¸ªæ€ç»´é“¾æ•°æ®æ ·æœ¬ï¼Œä¸¤å±‚ç»§ç»­å±‚å‚æ•°ä»…1MBï¼Œç»“æœä»¤äººé¼“èˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07513v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²èå…¥ç”¨æˆ·çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œä½†é’ˆå¯¹æ¯ä¸ªç”¨æˆ·æˆ–ä»»åŠ¡çš„å®šåˆ¶åŒ–éœ€æ±‚ä»å­˜åœ¨æŒ‘æˆ˜ã€‚é’ˆå¯¹ç³»ç»Ÿä¸ç»æµå› ç´ çš„è€ƒé‡ï¼Œé€šç”¨æŒç»­è®­ç»ƒæˆ–å¾®è°ƒä»éœ€è¦å¤§é‡è®¡ç®—ä¸å†…å­˜èµ„æºã€‚æœ¬æ–‡æå‡ºä¸€ç§æ¡†æ¶ï¼Œå……åˆ†åˆ©ç”¨ç°æœ‰çš„LLMä¸åœ¨çº¿æœåŠ¡ç³»ç»Ÿï¼Œé€šè¿‡è®­ç»ƒé¢å¤–çš„transformerå—æ¥ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶åœ¨å…¶ä¸Šé™„åŠ ä¸€ä¸ªç»§ç»­æ¨¡å—ï¼Œç»„åˆå½¢æˆå®šåˆ¶åŒ–LLMã€‚è¯¥æ¡†æ¶å¯ä»¥èåˆå¤šå±‚æˆ–ä¸åŒé¢†åŸŸçš„å¤šä¸ªLLMï¼Œä»¥é€‚åº”æ–°ä»»åŠ¡éœ€æ±‚ã€‚ç”±äºåŸºç¡€æ¨¡å‹æ— éœ€æ›´æ–°å‚æ•°ï¼Œå¤§éƒ¨åˆ†è®¡ç®—ä»»åŠ¡å¯å¤–åŒ…ç»™æ¨ç†èŠ‚ç‚¹ï¼Œä»…å¯¹è½»é‡çº§çš„ç»§ç»­æ¨¡å—è¿›è¡Œè®­ç»ƒï¼Œé™ä½äº†GPUå†…å­˜çš„ä½¿ç”¨ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç»§ç»­é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚Qwenå’ŒDeepSeekï¼‰ä¸Šå®ç°äº†æ›´å¿«çš„æŸå¤±æ”¶æ•›ï¼Œå¹¶åœ¨è§£å†³æ•°å­¦é—®é¢˜çš„åœºæ™¯ä¸­å±•ç°äº†è‰¯å¥½æ•ˆæœï¼Œä»…éœ€æå°è®¡ç®—é‡å’Œæ¨¡å‹å¤§å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå·²å¹¿æ³›åº”ç”¨äºæ—¥å¸¸ç”Ÿæ´»ï¼Œä½†é’ˆå¯¹ä¸ªæ€§åŒ–ä»»åŠ¡å’Œç”¨æˆ·éœ€æ±‚è¿›è¡Œå®šåˆ¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>é€šç”¨æŒç»­è®­ç»ƒæˆ–å¾®è°ƒLLMéœ€è¦å¤§é‡è®¡ç®—ä¸å†…å­˜èµ„æºã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒé¢å¤–çš„transformerå—å¹¶ç»“åˆç»§ç»­æ¨¡å—ï¼Œä»¥å½¢æˆå®šåˆ¶åŒ–çš„LLMã€‚</li>
<li>è¯¥æ¡†æ¶å¯èåˆå¤šå±‚æˆ–ä¸åŒé¢†åŸŸçš„LLMä»¥é€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>åŸºç¡€æ¨¡å‹æ— éœ€æ›´æ–°å‚æ•°ï¼Œé™ä½äº†å¯¹æ¨ç†èŠ‚ç‚¹çš„è®¡ç®—ä¸å†…å­˜éœ€æ±‚ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ç»§ç»­é¢„è®­ç»ƒæ¨¡å‹ä¸Šå®ç°äº†æ›´å¿«çš„æŸå¤±æ”¶æ•›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f905a38710fe5ddb17110b207dcdbee5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b752abc8739c103645bda0e11d405739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9e5a58b83a92e4c6582f96b744161fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbe381cc3bc5832de554cd5aef6866d0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MDIT-A-Model-free-Data-Interpolation-Method-for-Diverse-Instruction-Tuning"><a href="#MDIT-A-Model-free-Data-Interpolation-Method-for-Diverse-Instruction-Tuning" class="headerlink" title="MDIT: A Model-free Data Interpolation Method for Diverse Instruction   Tuning"></a>MDIT: A Model-free Data Interpolation Method for Diverse Instruction   Tuning</h2><p><strong>Authors:Yangning Li, Zihua Lan, Lv Qingsong, Yinghui Li, Hai-Tao Zheng</strong></p>
<p>As Large Language Models (LLMs) are increasingly applied across various tasks, instruction tuning has emerged as a critical method for enhancing model performance. However, current data management strategies face substantial challenges in generating diverse and comprehensive data, restricting further improvements in model performance. To address this gap, we propose MDIT, a novel model-free data interpolation method for diverse instruction tuning, which generates varied and high-quality instruction data by performing task interpolation. Moreover, it contains diversity-based clustering strategies to ensure the diversity of the training data. Extensive experiments show that our method achieves superior performance in multiple benchmark tasks. The LLMs finetuned with MDIT show significant improvements in numerous tasks such as general question answering, math reasoning, and code generation. MDIT offers an efficient and automatic data synthetic method, generating diverse instruction data without depending on external resources while expanding the application potential of LLMs in complex environments. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼ŒæŒ‡ä»¤å¾®è°ƒå·²æˆä¸ºæé«˜æ¨¡å‹æ€§èƒ½çš„å…³é”®æ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®ç®¡ç†ç­–ç•¥åœ¨ç”Ÿæˆå¤šæ ·ä¸”å…¨é¢çš„æ•°æ®æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„è¿›ä¸€æ­¥æé«˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MDITï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ ·æŒ‡ä»¤å¾®è°ƒçš„æ–°å‹æ— æ¨¡å‹æ•°æ®æ’å€¼æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä»»åŠ¡æ’å€¼ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…å«åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ï¼Œä»¥ç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä½¿ç”¨MDITå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚MDITæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”è‡ªåŠ¨çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–å¤–éƒ¨èµ„æºçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šæ ·çš„æŒ‡ä»¤æ•°æ®ï¼Œä»è€Œæ‰©å¤§äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07288v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼ŒæŒ‡ä»¤å¾®è°ƒä½œä¸ºæå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®æ–¹æ³•å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®ç®¡ç†ç­–ç•¥åœ¨ç”Ÿæˆå¤šæ ·ä¸”å…¨é¢çš„æ•°æ®æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MDITï¼Œä¸€ç§ç”¨äºå¤šæ ·æŒ‡ä»¤å¾®è°ƒçš„æ— æ¨¡å‹æ•°æ®æ’å€¼æ–°æ–¹æ³•ã€‚å®ƒé€šè¿‡ä»»åŠ¡æ’å€¼ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ï¼Œå¹¶åŒ…å«åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ä»¥ç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½¿ç”¨MDITè¿›è¡Œå¾®è°ƒçš„LLMåœ¨é€šç”¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚MDITæä¾›äº†ä¸€ç§é«˜æ•ˆã€è‡ªåŠ¨çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–å¤–éƒ¨èµ„æºçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šæ ·çš„æŒ‡ä»¤æ•°æ®ï¼Œæ‰©å¤§äº†LLMåœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsæ­£å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡ï¼ŒæŒ‡ä»¤å¾®è°ƒæ˜¯æå‡æ€§èƒ½çš„å…³é”®æ–¹æ³•ã€‚</li>
<li>å½“å‰æ•°æ®ç®¡ç†ç­–ç•¥åœ¨ç”Ÿæˆå¤šæ ·ã€å…¨é¢çš„æ•°æ®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MDITæ˜¯ä¸€ç§æ–°å‹çš„æ— æ¨¡å‹æ•°æ®æ’å€¼æ–¹æ³•ï¼Œç”¨äºå¤šæ ·æŒ‡ä»¤å¾®è°ƒã€‚</li>
<li>MDITé€šè¿‡ä»»åŠ¡æ’å€¼ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ã€‚</li>
<li>MDITåŒ…å«åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ä»¥ç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚</li>
<li>å®éªŒè¯æ˜MDITåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†LLMåœ¨å¤šé¡¹ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d9ecc47a7f024b165b8e08f58a72181.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b6a6318144dfe97301a0be55520d871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ca0717a3bb250b92dffe9f245901782.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01ac9b0bd86640f683c58e921e5a12d5.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Face-LLaVA-Facial-Expression-and-Attribute-Understanding-through-Instruction-Tuning"><a href="#Face-LLaVA-Facial-Expression-and-Attribute-Understanding-through-Instruction-Tuning" class="headerlink" title="Face-LLaVA: Facial Expression and Attribute Understanding through   Instruction Tuning"></a>Face-LLaVA: Facial Expression and Attribute Understanding through   Instruction Tuning</h2><p><strong>Authors:Ashutosh Chaubey, Xulang Guan, Mohammad Soleymani</strong></p>
<p>The human face plays a central role in social communication, necessitating the use of performant computer vision tools for human-centered applications. We propose Face-LLaVA, a multimodal large language model for face-centered, in-context learning, including facial expression and attribute recognition. Additionally, Face-LLaVA is able to generate natural language descriptions that can be used for reasoning. Leveraging existing visual databases, we first developed FaceInstruct-1M, a face-centered database for instruction tuning MLLMs for face processing. We then developed a novel face-specific visual encoder powered by Face-Region Guided Cross-Attention that integrates face geometry with local visual features. We evaluated the proposed method across nine different datasets and five different face processing tasks, including facial expression recognition, action unit detection, facial attribute detection, age estimation and deepfake detection. Face-LLaVA achieves superior results compared to existing open-source MLLMs and competitive performance compared to commercial solutions. Our model output also receives a higher reasoning rating by GPT under a zero-shot setting across all the tasks. Both our dataset and model wil be released at <a target="_blank" rel="noopener" href="https://face-llava.github.io/">https://face-llava.github.io</a> to support future advancements in social AI and foundational vision-language research. </p>
<blockquote>
<p>äººè„¸åœ¨ç¤¾ä¼šäº¤æµä¸­å…·æœ‰æ ¸å¿ƒä½œç”¨ï¼Œå› æ­¤éœ€è¦ä¸ºä»¥äººä¸ºä¸­å¿ƒçš„åº”ç”¨ä½¿ç”¨é«˜æ€§èƒ½è®¡ç®—æœºè§†è§‰å·¥å…·ã€‚æˆ‘ä»¬æå‡ºäº†Face-LLaVAï¼Œè¿™æ˜¯ä¸€ç§é¢å‘äººè„¸çš„ä¸Šä¸‹æ–‡å­¦ä¹ å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬é¢éƒ¨è¡¨æƒ…å’Œå±æ€§è¯†åˆ«ã€‚æ­¤å¤–ï¼ŒFace-LLaVAèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ï¼Œå¯ç”¨äºæ¨ç†ã€‚æˆ‘ä»¬å€ŸåŠ©ç°æœ‰çš„è§†è§‰æ•°æ®åº“ï¼Œé¦–å…ˆå¼€å‘äº†é¢å‘äººè„¸çš„æ•°æ®åº“FaceInstruct-1Mï¼Œç”¨äºè°ƒæ•´é¢å‘äººè„¸å¤„ç†çš„MLLMã€‚ç„¶åæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹çš„äººè„¸ç‰¹å®šè§†è§‰ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨ç”±Face-Region Guided Cross-Attentioné©±åŠ¨ï¼Œå¯ä»¥é›†æˆäººè„¸å‡ ä½•ä¸å±€éƒ¨è§†è§‰ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªä¸åŒçš„æ•°æ®é›†å’Œäº”ä¸ªä¸åŒçš„äººè„¸å¤„ç†ä»»åŠ¡ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬é¢éƒ¨è¡¨æƒ…è¯†åˆ«ã€åŠ¨ä½œå•å…ƒæ£€æµ‹ã€é¢éƒ¨å±æ€§æ£€æµ‹ã€å¹´é¾„ä¼°è®¡å’Œæ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚Face-LLaVAä¸ç°æœ‰çš„å¼€æºMLLMç›¸æ¯”å–å¾—äº†ä¼˜è¶Šçš„ç»“æœï¼Œå¹¶åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å®ç°äº†é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„GPTæ›´é«˜çš„æ¨ç†è¯„åˆ†ï¼Œä¸å•†ä¸šè§£å†³æ–¹æ¡ˆç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://face-llava.github.ioä¸Šå‘å¸ƒ,ä»¥æ”¯æŒç¤¾äº¤aiå’ŒåŸºç¡€è§†è§‰è¯­è¨€ç ”ç©¶çš„æœªæ¥å‘å±•./">https://face-llava.github.ioä¸Šå‘å¸ƒï¼Œä»¥æ”¯æŒç¤¾äº¤AIå’ŒåŸºç¡€è§†è§‰è¯­è¨€ç ”ç©¶çš„æœªæ¥å‘å±•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07198v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://face-llava.github.io/">https://face-llava.github.io</a></p>
<p><strong>Summary</strong><br>äººè„¸è¯†åˆ«æŠ€æœ¯åœ¨ç¤¾äº¤é€šè®¯ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä¸ºæ­¤éœ€è¦è¿ç”¨é«˜æ€§èƒ½è®¡ç®—æœºè§†è§‰å·¥å…·ä»¥åº”ç”¨äºäººç±»ä¸ºä¸­å¿ƒçš„åœºåˆã€‚ç ”ç©¶æå‡ºäº†Face-LLaVAï¼Œä¸€ä¸ªä¸ºä»¥äººè„¸ä¸ºä¸­å¿ƒçš„åœºæ™¯å¼€å‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡é¢éƒ¨è¡¨æƒ…å’Œå±æ€§è¯†åˆ«åŠŸèƒ½ï¼Œå¹¶èƒ½ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ç”¨äºæ¨ç†ã€‚ç ”ç©¶å»ºç«‹äº†FaceInstruct-1Mæ•°æ®åº“ç”¨äºæŒ‡ä»¤è°ƒæ•´MLLMsäººè„¸å¤„ç†åŠŸèƒ½ï¼Œå¹¶å¼€å‘å‡ºä¸€ç§æ–°å‹çš„äººè„¸ç‰¹å®šè§†è§‰ç¼–ç å™¨Face-Region Guided Cross-Attentionã€‚æ¨¡å‹æ€§èƒ½é€šè¿‡å¤šé¡¹ä»»åŠ¡æµ‹è¯•å±•ç°å‡ºä¼˜å¼‚æˆæœï¼Œç›¸æ¯”ç°æœ‰å¼€æºæ¨¡å‹æ›´èƒœä¸€ç­¹ä¸”å’Œå•†ä¸šåŒ–è§£å†³æ–¹æ¡ˆç«äº‰åŠ›ç›¸å½“ã€‚æ­¤å¤–æ¨¡å‹è¾“å‡ºå¯é€šè¿‡GPTé›¶æ ·æœ¬è®¾ç½®è¿›è¡Œæ¨ç†è¯„ä»·ã€‚ç ”ç©¶çš„æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://face-llava.github.ioå‘å¸ƒ,ä»¥æ”¯æŒæœªæ¥ç¤¾äº¤äººå·¥æ™ºèƒ½å’ŒåŸºç¡€è§†è§‰è¯­è¨€ç ”ç©¶çš„å‘å±•./">https://face-llava.github.ioå‘å¸ƒï¼Œä»¥æ”¯æŒæœªæ¥ç¤¾äº¤äººå·¥æ™ºèƒ½å’ŒåŸºç¡€è§†è§‰è¯­è¨€ç ”ç©¶çš„å‘å±•ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººè„¸è¯†åˆ«åœ¨ç¤¾äº¤é€šè®¯ä¸­æ‰®æ¼”æ ¸å¿ƒè§’è‰²ï¼Œéœ€è¦é«˜æ€§èƒ½è®¡ç®—æœºè§†è§‰å·¥å…·æ”¯æŒã€‚</li>
<li>æå‡ºFace-LLaVAæ¨¡å‹ï¼Œä¸ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äººè„¸ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­åº”ç”¨æä¾›è§£å†³æ–¹æ¡ˆã€‚</li>
<li>Face-LLaVAå…·å¤‡é¢éƒ¨è¡¨æƒ…å’Œå±æ€§è¯†åˆ«åŠŸèƒ½ï¼Œèƒ½ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ä»¥æ”¯æŒæ¨ç†ã€‚</li>
<li>ç ”ç©¶å»ºç«‹FaceInstruct-1Mæ•°æ®åº“ç”¨äºä¼˜åŒ–æ¨¡å‹çš„é¢éƒ¨å¤„ç†æ€§èƒ½ã€‚</li>
<li>å¼•å…¥Face-Region Guided Cross-Attentionæ–°å‹äººè„¸ç‰¹å®šè§†è§‰ç¼–ç å™¨ï¼Œç»“åˆé¢éƒ¨å‡ ä½•ä¸å±€éƒ¨è§†è§‰ç‰¹å¾ã€‚</li>
<li>Face-LLaVAåœ¨å¤šä¸ªæ•°æ®é›†å’Œä»»åŠ¡æµ‹è¯•ä¸­å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬é¢éƒ¨è¡¨æƒ…è¯†åˆ«ã€åŠ¨ä½œå•å…ƒæ£€æµ‹ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07198">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2c498a382c2c44020fd6944ba0640515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-820813fb20fcf0bc3fba72e44516473f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a72d95239f528aa787be1233717269b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af826c5b7568ac6733cda154323e226c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-859162fc5dc3e5450e362d7aa562a4ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d4752b21af542648e7b07214d4f3a64.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Boundary-representation-learning-via-Transformer"><a href="#Boundary-representation-learning-via-Transformer" class="headerlink" title="Boundary representation learning via Transformer"></a>Boundary representation learning via Transformer</h2><p><strong>Authors:Qiang Zou, Lizhen Zhu</strong></p>
<p>The recent rise of generative artificial intelligence (AI), powered by Transformer networks, has achieved remarkable success in natural language processing, computer vision, and graphics. However, the application of Transformers in computer-aided design (CAD), particularly for processing boundary representation (B-rep) models, remains largely unexplored. To bridge this gap, this paper introduces Boundary Representation Transformer (BRT), a novel method adapting Transformer for B-rep learning. B-rep models pose unique challenges due to their irregular topology and continuous geometric definitions, which are fundamentally different from the structured and discrete data Transformers are designed for. To address this, BRT proposes a continuous geometric embedding method that encodes B-rep surfaces (trimmed and untrimmed) into B&#39;ezier triangles, preserving their shape and continuity without discretization. Additionally, BRT employs a topology-aware embedding method that organizes these geometric embeddings into a sequence of discrete tokens suitable for Transformers, capturing both geometric and topological characteristics within B-rep models. This enables the Transformerâ€™s attention mechanism to effectively learn shape patterns and contextual semantics of boundary elements in a B-rep model. Extensive experiments demonstrate that BRT achieves state-of-the-art performance in part classification and feature recognition tasks. </p>
<blockquote>
<p>æœ€è¿‘å…´èµ·çš„åŸºäºTransformerç½‘ç»œçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆå°±ã€‚ç„¶è€Œï¼ŒTransformeråœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰æ¨¡å‹æ–¹é¢ï¼Œä»è¢«å¤§å¤§å¿½è§†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡å¼•å…¥äº†è¾¹ç•Œè¡¨ç¤ºè½¬æ¢å™¨ï¼ˆBRTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€‚åº”äºB-repå­¦ä¹ çš„æ–°å‹æ–¹æ³•ã€‚B-repæ¨¡å‹ç”±äºå…¶ä¸è§„åˆ™æ‹“æ‰‘å’Œè¿ç»­å‡ ä½•å®šä¹‰è€Œå¸¦æ¥ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œè¿™äº›ç‰¹æ€§ä¸Transformerè®¾è®¡çš„ç»“æ„åŒ–ç¦»æ•£æ•°æ®å­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒBRTæå‡ºäº†ä¸€ç§è¿ç»­å‡ ä½•åµŒå…¥æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†B-repè¡¨é¢ï¼ˆä¿®å‰ªå’Œæœªä¿®å‰ªï¼‰ç¼–ç ä¸ºBezierä¸‰è§’å½¢ï¼Œåœ¨ä¿æŒå…¶å½¢çŠ¶å’Œè¿ç»­æ€§çš„åŒæ—¶æ— éœ€ç¦»æ•£åŒ–ã€‚æ­¤å¤–ï¼ŒBRTè¿˜é‡‡ç”¨äº†ä¸€ç§æ‹“æ‰‘æ„ŸçŸ¥åµŒå…¥æ–¹æ³•ï¼Œå°†è¿™äº›å‡ ä½•åµŒå…¥æ•´ç†æˆé€‚åˆTransformerçš„ç¦»æ•£ä»¤ç‰Œåºåˆ—ï¼Œèƒ½å¤Ÿæ•æ‰B-repæ¨¡å‹ä¸­çš„å‡ ä½•å’Œæ‹“æ‰‘ç‰¹å¾ã€‚è¿™ä½¿å¾—Transformerçš„æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ B-repæ¨¡å‹ä¸­è¾¹ç•Œå…ƒç´ çš„å½¢çŠ¶æ¨¡å¼å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBRTåœ¨é›¶ä»¶åˆ†ç±»å’Œç‰¹å¾è¯†åˆ«ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07134v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºTransformerç½‘ç»œçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰æ¨¡å‹æ–¹é¢ä»é²œæœ‰æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºBoundary Representation Transformerï¼ˆBRTï¼‰æ–°æ–¹æ³•ï¼Œé€‚åº”Transformerè¿›è¡ŒB-repå­¦ä¹ ã€‚é’ˆå¯¹B-repæ¨¡å‹çš„ä¸è§„åˆ™æ‹“æ‰‘å’Œè¿ç»­å‡ ä½•å®šä¹‰å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒBRTæå‡ºä¸€ç§è¿ç»­å‡ ä½•åµŒå…¥æ–¹æ³•ï¼Œå°†è¾¹ç•Œè¡¨ç¤ºï¼ˆä¿®å‰ªå’Œæœªä¿®å‰ªï¼‰ç¼–ç ä¸ºBÃ©zierä¸‰è§’å½¢ï¼Œä¿ç•™å…¶å½¢çŠ¶å’Œè¿ç»­æ€§è€Œä¸è¿›è¡Œç¦»æ•£åŒ–ã€‚æ­¤å¤–ï¼ŒBRTé‡‡ç”¨æ‹“æ‰‘æ„ŸçŸ¥åµŒå…¥æ–¹æ³•ï¼Œå°†è¿™äº›å‡ ä½•åµŒå…¥ç»„ç»‡æˆé€‚åˆTransformerçš„ç¦»æ•£ä»¤ç‰Œåºåˆ—ï¼Œæ•æ‰B-repæ¨¡å‹ä¸­çš„å‡ ä½•å’Œæ‹“æ‰‘ç‰¹å¾ã€‚è¿™ä½¿å¾—Transformerçš„æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ è¾¹ç•Œå…ƒç´ çš„å½¢çŠ¶æ¨¡å¼å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBRTåœ¨é›¶ä»¶åˆ†ç±»å’Œç‰¹å¾è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å¤šä¸ªé¢†åŸŸå–å¾—æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¤„ç†è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰æ¨¡å‹æ–¹é¢ä»å­˜åœ¨å·®è·ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Boundary Representation Transformerï¼ˆBRTï¼‰æ–°æ–¹æ³•ï¼Œæ—¨åœ¨é€‚åº”Transformerè¿›è¡ŒB-repå­¦ä¹ ã€‚</li>
<li>BRTè§£å†³äº†B-repæ¨¡å‹çš„ä¸è§„åˆ™æ‹“æ‰‘å’Œè¿ç»­å‡ ä½•å®šä¹‰çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</li>
<li>BRTé€šè¿‡è¿ç»­å‡ ä½•åµŒå…¥æ–¹æ³•å°†B-repæ¨¡å‹ç¼–ç ä¸ºBÃ©zierä¸‰è§’å½¢ï¼Œä¿æŒå…¶å½¢çŠ¶å’Œè¿ç»­æ€§ï¼Œé¿å…ç¦»æ•£åŒ–ã€‚</li>
<li>BRTé‡‡ç”¨æ‹“æ‰‘æ„ŸçŸ¥åµŒå…¥æ–¹æ³•ï¼Œå°†å‡ ä½•åµŒå…¥ç»„ç»‡æˆç¦»æ•£ä»¤ç‰Œåºåˆ—ï¼Œé€‚åˆTransformerå¤„ç†ã€‚</li>
<li>BRTèƒ½å¤Ÿæ•æ‰B-repæ¨¡å‹ä¸­çš„å‡ ä½•å’Œæ‹“æ‰‘ç‰¹å¾ï¼Œé€šè¿‡Transformerçš„æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ è¾¹ç•Œå…ƒç´ çš„å½¢çŠ¶æ¨¡å¼å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a602a916481793f078a12e387edb4103.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning"><a href="#VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning" class="headerlink" title="VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning"></a>VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning</h2><p><strong>Authors:Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang</strong></p>
<p>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs. </p>
<blockquote>
<p>æœ€è¿‘å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„è¿›å±•æå¤§åœ°æå‡äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶åœ¨æ–‡æœ¬å’Œå›¾åƒé¢†åŸŸå±•ç°å‡ºäº†ä¸€å®šçš„å‰æ™¯ï¼Œä½†å®ƒä»¬åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†ä½¿ç”¨GRPOçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åœ¨è§†é¢‘MLLMsä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æé«˜æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›çš„åŒæ—¶ä¿æŒå…¶é€šç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRFTå¯¹äºç‰¹å®šä»»åŠ¡çš„æ”¹è¿›éå¸¸æ•°æ®é«˜æ•ˆã€‚é€šè¿‡åœ¨æœ‰é™çš„æ ·æœ¬ä¸Šå¯¹æ—¶ç©ºæ„ŸçŸ¥ç›®æ ‡è¿›è¡Œå¤šä»»åŠ¡RFTï¼Œæˆ‘ä»¬å¼€å‘å‡ºäº†VideoChat-R1ï¼Œè¿™æ˜¯ä¸€æ¬¾å¼ºå¤§çš„è§†é¢‘MLLMï¼Œå®ƒåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å…¶å¯¹è¯èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚ä¸Qwen2.5-VL-7Bç›¸æ¯”ï¼ŒVideoChat-R1åœ¨è¯¸å¦‚æ—¶é—´å®šä½ï¼ˆ+31.8ï¼‰å’Œå¯¹è±¡è·Ÿè¸ªï¼ˆ+31.2ï¼‰ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½æé«˜äº†æ•°å€ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ä¸€èˆ¬é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMEï¼ˆ+0.9ï¼‰ã€MVBenchï¼ˆ+1.0ï¼‰å’Œæ„ŸçŸ¥æµ‹è¯•ï¼ˆ+0.9ï¼‰ï¼‰æ–¹é¢ä¹Ÿå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†RFTåœ¨è§†é¢‘MLLMä¸“é¡¹ä»»åŠ¡å¢å¼ºæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½ä¸ºæœªæ¥è§†é¢‘MLLMçš„å¼ºåŒ–å­¦ä¹ ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06958v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†å¼ºåŒ–ç²¾ç»†è°ƒèŠ‚ï¼ˆRFTï¼‰ç»“åˆç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨è§†é¢‘MLLMsä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æå‡æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›çš„åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRFTåœ¨ç‰¹å®šä»»åŠ¡æ”¹è¿›æ–¹é¢éå¸¸æ³¨é‡æ•°æ®æ•ˆç‡ã€‚é€šè¿‡å¤šä»»åŠ¡RFTåœ¨æ—¶ç©ºæ„ŸçŸ¥ç›®æ ‡ä¸Šçš„æœ‰é™æ ·æœ¬è®­ç»ƒï¼ŒæˆåŠŸå¼€å‘å‡ºVideoChat-R1ï¼Œå…¶åœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œä¸ç‰ºç‰²å¯¹è¯èƒ½åŠ›ï¼Œå±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚ä¸Qwen2.5-VL-7Bç›¸æ¯”ï¼ŒVideoChat-R1åœ¨è¯¸å¦‚æ—¶åºå®šä½ï¼ˆ+31.8ï¼‰å’Œå¯¹è±¡è·Ÿè¸ªï¼ˆ+31.2ï¼‰ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¤§å¹…æå‡ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ä¸€èˆ¬é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿæœ‰æ‰€æ”¹è¿›ï¼Œå¦‚VideoMMEï¼ˆ+0.9ï¼‰ã€MVBenchï¼ˆ+1.0ï¼‰å’ŒPerception Testï¼ˆ+0.9ï¼‰ã€‚æœ¬ç ”ç©¶çªæ˜¾äº†RFTåœ¨è§†é¢‘MLLMsä¸“é¡¹ä»»åŠ¡å¢å¼ºæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥è§†é¢‘MLLMsçš„å¼ºåŒ–å­¦ä¹ ç ”ç©¶æä¾›å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—è¿›å±•ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ç»“åˆå¼ºåŒ–ç²¾ç»†è°ƒèŠ‚ï¼ˆRFTï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æå‡è§†é¢‘MLLMsçš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>RFTæ˜¯æ•°æ®é«˜æ•ˆçš„ï¼Œé€‚ç”¨äºç‰¹å®šä»»åŠ¡çš„æ”¹è¿›ã€‚</li>
<li>æ–°æ¨¡å‹VideoChat-R1åœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼ŒåŒæ—¶ä¿æŒå¯¹è¯èƒ½åŠ›ã€‚</li>
<li>VideoChat-R1ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹åœ¨æ—¶åºå®šä½å’Œå¯¹è±¡è·Ÿè¸ªç­‰ä»»åŠ¡ä¸Šæœ‰æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>VideoChat-R1åœ¨ä¸€èˆ¬é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46621a1e4484a9f27274987e9b6a9ba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89b3adc13557b37016357b42472fc020.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3358aff3ad40e151d7647349d49081f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c69c12accc911d1f6cd0de6c91a6c1cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e754349e60c319da42a2b28e9441666.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29a1999ed19e6b996b60e278dcd16437.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Affordable-AI-Assistants-with-Knowledge-Graph-of-Thoughts"><a href="#Affordable-AI-Assistants-with-Knowledge-Graph-of-Thoughts" class="headerlink" title="Affordable AI Assistants with Knowledge Graph of Thoughts"></a>Affordable AI Assistants with Knowledge Graph of Thoughts</h2><p><strong>Authors:Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, JÃ³n Gunnar Hannesson, Grzegorz KwaÅ›niewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler</strong></p>
<p>Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose the Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini, while reducing costs by over 36x compared to GPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and 37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a scalable, affordable, and high-performing solution for AI assistants. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½åŠ©æ‰‹çš„å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¸åŒé¢†åŸŸæ‰§è¡Œå¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å‰æ²¿çš„LLMé©±åŠ¨çš„æ™ºèƒ½ä»£ç†é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿è¥æˆæœ¬é«˜æ˜‚å’Œåœ¨å¤æ‚åŸºå‡†æµ‹è¯•ï¼ˆå¦‚GAIAï¼‰ä¸ŠæˆåŠŸç‡æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œçŸ¥è¯†å›¾è°±æ€ç»´â€ï¼ˆKGoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹æ¶æ„ï¼Œå®ƒå°†LLMæ¨ç†ä¸åŠ¨æ€æ„å»ºçš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ç›¸ç»“åˆã€‚KGoTæå–å¹¶ç»“æ„åŒ–ä»»åŠ¡ç›¸å…³çŸ¥è¯†ï¼Œå½¢æˆåŠ¨æ€KGè¡¨ç¤ºï¼Œé€šè¿‡æ•°å­¦æ±‚è§£å™¨ã€ç½‘ç»œçˆ¬è™«å’ŒPythonè„šæœ¬ç­‰å¤–éƒ¨å·¥å…·è¿›è¡Œè¿­ä»£å¢å¼ºã€‚è¿™ç§ä»»åŠ¡ç›¸å…³çŸ¥è¯†çš„ç»“æ„åŒ–è¡¨ç¤ºä½¿å¾—ä½æˆæœ¬æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£å†³å¤æ‚ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨GAIAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒKGoTçš„ä»»åŠ¡æˆåŠŸç‡æ¯”ä½¿ç”¨GPT-4o miniçš„Hugging Face Agentsæé«˜äº†29%ï¼ŒåŒæ—¶æˆæœ¬é™ä½äº†è¶…è¿‡36å€ã€‚è¿‘æœŸçš„æ¨ç†æ¨¡å‹æ”¹è¿›ä¹Ÿç±»ä¼¼ï¼Œå¦‚Qwen2.5-32Bå’ŒDeepseek-R1-70Båˆ†åˆ«æé«˜äº†36%å’Œ37.5%ã€‚KGoTä¸ºäººå·¥æ™ºèƒ½åŠ©æ‰‹æä¾›äº†å¯æ‰©å±•ã€ç»æµå®æƒ ä¸”é«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02670v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>çŸ¥è¯†å›¾è°±æ€ç»´ï¼ˆKGoTï¼‰æ˜¯ä¸€ç§åˆ›æ–°çš„AIåŠ©æ‰‹æ¶æ„ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸åŠ¨æ€æ„å»ºçš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ç›¸ç»“åˆï¼Œè§£å†³äº†AIåŠ©ç†åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³å’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚KGoTé€šè¿‡æå–å’Œç»“æ„åŒ–ä»»åŠ¡ç›¸å…³çŸ¥è¯†ï¼Œå½¢æˆåŠ¨æ€çŸ¥è¯†å›¾è°±è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æ•°å­¦æ±‚è§£å™¨ã€ç½‘ç»œçˆ¬è™«å’ŒPythonè„šæœ¬ç­‰å¤–éƒ¨å·¥å…·è¿›è¡Œè¿­ä»£å¢å¼ºã€‚è¿™ç§ç»“æ„åŒ–çš„çŸ¥è¯†è¡¨ç¤ºä½¿ä½æˆæœ¬æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£å†³å¤æ‚ä»»åŠ¡ï¼Œåœ¨GAIAåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsæ­£åœ¨æ¨åŠ¨AIåŠ©æ‰‹çš„å‘å±•ï¼Œä½†ä»é¢ä¸´é«˜æˆæœ¬å’Œå¤æ‚ä»»åŠ¡æˆåŠŸç‡ä½çš„æŒ‘æˆ˜ã€‚</li>
<li>KGoTæ˜¯ä¸€ç§æ–°å‹çš„AIåŠ©æ‰‹æ¶æ„ï¼Œé›†æˆäº†LLMæ¨ç†å’ŒåŠ¨æ€æ„å»ºçš„çŸ¥è¯†å›¾è°±ã€‚</li>
<li>KGoTé€šè¿‡æå–å’Œç»“æ„åŒ–ä»»åŠ¡ç›¸å…³çŸ¥è¯†ï¼Œå½¢æˆåŠ¨æ€çŸ¥è¯†å›¾è°±è¡¨ç¤ºï¼Œæé«˜æ¨¡å‹è§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>KGoTåœ¨GAIAåŸºå‡†æµ‹è¯•ä¸Šç›¸æ¯”Hugging Face Agents with GPT-4o miniæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜29%ï¼Œæˆæœ¬é™ä½è¶…è¿‡36å€ã€‚</li>
<li>KGoTå¯¹è¿‘æœŸçš„æ¨ç†æ¨¡å‹ä¹Ÿæœ‰ç±»ä¼¼çš„æ”¹è¿›æ•ˆæœï¼Œå¦‚Qwen2.5-32Bå’ŒDeepseek-R1-70Båˆ†åˆ«æé«˜äº†36%å’Œ37.5%ã€‚</li>
<li>KGoTæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€ç»æµå®æƒ ä¸”é«˜æ€§èƒ½çš„AIåŠ©æ‰‹è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cce4fc92a67ab49291829eb66a0abff7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcf5f46e6c631dfe5990d702a2b41b98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64b267c45e37dac8c7c78eb96ecf29d6.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SPIN-Bench-How-Well-Do-LLMs-Plan-Strategically-and-Reason-Socially"><a href="#SPIN-Bench-How-Well-Do-LLMs-Plan-Strategically-and-Reason-Socially" class="headerlink" title="SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?"></a>SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?</h2><p><strong>Authors:Jianzhu Yao, Kevin Wang, Ryan Hsieh, Haisu Zhou, Tianqing Zou, Zerui Cheng, Zhangyang Wang, Pramod Viswanath</strong></p>
<p>Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and humanâ€“AI teaming. Project Website: <a target="_blank" rel="noopener" href="https://spinbench.github.io/">https://spinbench.github.io/</a> </p>
<blockquote>
<p>ç¤¾ä¼šäº’åŠ¨ä¸­çš„æ¨ç†å’Œç­–ç•¥è¡Œä¸ºæ˜¯æ™ºèƒ½çš„æ ‡å¿—ã€‚è¿™ç§æ¨ç†å½¢å¼è¿œæ¯”é™æ€ç¯å¢ƒä¸­çš„å­¤ç«‹è§„åˆ’æˆ–æ¨ç†ä»»åŠ¡ï¼ˆä¾‹å¦‚æ•°å­¦é—®é¢˜è§£å†³ï¼‰æ›´ä¸ºå¤æ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæˆ˜ç•¥è§„åˆ’äº’åŠ¨è°ˆåˆ¤ï¼ˆSPIN-Benchï¼‰â€ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¤šé¢†åŸŸè¯„ä¼°ï¼Œæ—¨åœ¨è¡¡é‡æˆ˜ç•¥è§„åˆ’å’Œç¤¾ä¼šæ¨ç†çš„æ™ºèƒ½æ°´å¹³ã€‚è™½ç„¶è®¸å¤šç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ç‹­éš˜çš„è§„åˆ’æˆ–å•ä»£ç†æ¨ç†ä¸Šï¼Œä½†SPIN-Benchç»“åˆäº†ç»å…¸çš„PDDLä»»åŠ¡ã€ç«æŠ€æ£‹ç›˜æ¸¸æˆã€åˆä½œå¡ç‰Œæ¸¸æˆå’Œå¤šä»£ç†è°ˆåˆ¤åœºæ™¯åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚è¯¥æ¡†æ¶æ—¢åŒ…æ‹¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä¹ŸåŒ…æ‹¬ä¸€ä¸ªæ¨¡æ‹Ÿå’Œè¯„ä¼°å¤šç§ç¤¾äº¤è®¾ç½®çš„åœºæ‰€ï¼Œä»¥æµ‹è¯•AIä»£ç†çš„æ¨ç†å’Œç­–ç•¥è¡Œä¸ºã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜è¡ŒåŠ¨ç©ºé—´ã€çŠ¶æ€å¤æ‚æ€§å’Œäº¤äº’ä»£ç†çš„æ•°é‡æ¥åˆ¶å®šSPIN-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥æ¨¡æ‹Ÿå¤šç§ç¤¾äº¤ç¯å¢ƒï¼Œå…¶ä¸­æˆåŠŸä¸ä»…å–å†³äºæ–¹æ³•æ€§å’Œé€æ­¥çš„å†³ç­–åˆ¶å®šï¼Œè¿˜å–å†³äºå¯¹å…¶ä»–ï¼ˆå¯¹æŠ—æ€§æˆ–åˆä½œæ€§ï¼‰å‚ä¸è€…çš„æ¦‚å¿µæ¨æ–­ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè™½ç„¶å½“ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åŸºæœ¬äº‹å®æ£€ç´¢å’ŒçŸ­æœŸè§„åˆ’æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦åœ¨å¤§çŠ¶æ€ç©ºé—´è¿›è¡Œæ·±åº¦å¤šè·³æ¨ç†å’Œä¸ç¡®å®šæ€§ä¸‹ç¤¾ä¼šé€‚åº”æ€§åè°ƒçš„ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬ä¼šé‡åˆ°æ˜¾è‘—çš„æ€§èƒ½ç“¶é¢ˆã€‚æˆ‘ä»¬æœŸæœ›SPIN-Benchèƒ½æˆä¸ºæœªæ¥å…³äºç¨³å¥çš„å¤šä»£ç†è§„åˆ’ã€ç¤¾ä¼šæ¨ç†å’Œäººæœºåä½œçš„ç ”ç©¶å‚¬åŒ–å‰‚ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://spinbench.github.io/">https://spinbench.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12349v3">PDF</a> 42 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSPIN-Benchçš„æ–°å¤šé¢†åŸŸè¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨æµ‹é‡æ™ºèƒ½æˆ˜ç•¥è§„åˆ’å’Œç¤¾äº¤æ¨ç†çš„æ™ºåŠ›æ°´å¹³ã€‚è¯¥è¯„ä¼°ç»“åˆäº†ç»å…¸PDDLä»»åŠ¡ã€ç«æŠ€æ£‹ç±»æ¸¸æˆã€åˆä½œå¡ç‰Œæ¸¸æˆå’Œå¤šæ™ºèƒ½ä½“è°ˆåˆ¤åœºæ™¯ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­æ¨¡æ‹Ÿå’Œè¯„ä¼°å„ç§ç¤¾äº¤è®¾ç½®ï¼Œä»¥æµ‹è¯•äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“çš„æ¨ç†å’Œæˆ˜ç•¥è¡Œä¸ºã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åŸºæœ¬äº‹å®æ£€ç´¢å’ŒçŸ­æœŸè§„åˆ’æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦æ·±åº¦å¤šè·³æ¨ç†å’Œä¸ç¡®å®šæ¡ä»¶ä¸‹çš„ç¤¾ä¼šåè°ƒä»»åŠ¡ä¸­ä»å­˜åœ¨æ˜¾è‘—æ€§èƒ½ç“¶é¢ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPIN-Benchæ˜¯ä¸€ç§æ–°çš„å¤šé¢†åŸŸè¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨æµ‹é‡æ™ºèƒ½æˆ˜ç•¥è§„åˆ’å’Œç¤¾äº¤æ¨ç†çš„æ™ºèƒ½æ°´å¹³ã€‚</li>
<li>SPIN-Benchç»“åˆäº†å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç»å…¸PDDLä»»åŠ¡ã€ç«æŠ€æ£‹ç±»æ¸¸æˆã€åˆä½œå¡ç‰Œæ¸¸æˆå’Œå¤šæ™ºèƒ½ä½“è°ˆåˆ¤åœºæ™¯ã€‚</li>
<li>è¯¥è¯„ä¼°æ–¹æ³•é€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜åŠ¨ä½œç©ºé—´ã€çŠ¶æ€å¤æ‚æ€§å’Œäº¤äº’æ™ºèƒ½ä½“çš„æ•°é‡æ¥æ¨¡æ‹Ÿå„ç§ç¤¾äº¤ç¯å¢ƒã€‚</li>
<li>å½“ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦æ·±åº¦å¤šè·³æ¨ç†å’Œä¸ç¡®å®šæ¡ä»¶ä¸‹çš„ç¤¾ä¼šåè°ƒä»»åŠ¡æ—¶é¢ä¸´æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>SPIN-Benchå¯æ¨åŠ¨æœªæ¥åœ¨é²æ£’å¤šæ™ºèƒ½ä½“è§„åˆ’ã€ç¤¾äº¤æ¨ç†å’Œäººæœºåä½œç­‰é¢†åŸŸçš„ç ”ç©¶ã€‚</li>
<li>SPIN-Benchæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥æ¨¡æ‹Ÿå’Œè¯„ä¼°ç¤¾äº¤è®¾ç½®ä¸­çš„æ™ºèƒ½ä½“è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09bbd46d8c3b086c52fa633d0828d5a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a25042afb63f292282ad1b051554613a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f776a21069c79f66209d467df66bfa69.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-12/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d1630944d86d3d07c5301dd38b99683c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  Synthesizing High-Quality Programming Tasks with LLM-based Expert and   Student Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-12/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-64299d139655edfe7483ba354d68ccb4.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  GLUS Global-Local Reasoning Unified into A Single Large Language Model   for Video Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
