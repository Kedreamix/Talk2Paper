<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-12  GLUS Global-Local Reasoning Unified into A Single Large Language Model   for Video Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-64299d139655edfe7483ba354d68ccb4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    63 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-12-更新"><a href="#2025-04-12-更新" class="headerlink" title="2025-04-12 更新"></a>2025-04-12 更新</h1><h2 id="GLUS-Global-Local-Reasoning-Unified-into-A-Single-Large-Language-Model-for-Video-Segmentation"><a href="#GLUS-Global-Local-Reasoning-Unified-into-A-Single-Large-Language-Model-for-Video-Segmentation" class="headerlink" title="GLUS: Global-Local Reasoning Unified into A Single Large Language Model   for Video Segmentation"></a>GLUS: Global-Local Reasoning Unified into A Single Large Language Model   for Video Segmentation</h2><p><strong>Authors:Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang</strong></p>
<p>This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between “Ref” and “VOS”: they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that global and local consistency can be unified into a single video segmentation MLLM: a set of sparse “context frames” provides global information, while a stream of continuous “query frames” conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our project page is at <a target="_blank" rel="noopener" href="https://glus-video.github.io/">https://glus-video.github.io/</a>. </p>
<blockquote>
<p>本文提出了一种利用多模态大型语言模型（MLLM）进行指代视频对象分割（RefVOS）的新型框架。之前的MLLM方法通常面临“Ref”和“VOS”之间的困境：它们要么专注于理解一些关键帧（全局推理），要么跟踪连续帧上的对象（局部推理），并依赖外部VOS或帧选择器来缓解另一端的挑战。然而，我们的GLUS框架表明，全局和局部一致性可以统一到一个单一的视频分割MLLM中：一组稀疏的“上下文帧”提供全局信息，而一系列连续的“查询帧”进行局部对象跟踪。这得到了与预训练VOS内存库联合训练MLLM的支持，以同时消化短程和长程的时间信息。为了提高MLLM有限上下文窗口内的信息效率，我们引入了对象对比学习来区分难以区分的假阳性对象，以及自我完善框架来识别关键帧并进行传播。通过整合这些见解，我们的GLUS提供了一个简单而有效的基线，在MeViS和Ref-Youtube-VOS基准测试上达到了MLLM的最新水平。我们的项目页面是<a target="_blank" rel="noopener" href="https://glus-video.github.io/%E3%80%82">https://glus-video.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07962v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>在解决视频对象分割的问题时，本文提出了一种新型的多模态大型语言模型框架GLUS。该框架融合了全局和局部一致性，通过稀疏的“上下文帧”提供全局信息，并利用连续的“查询帧”进行局部对象跟踪。此外，通过联合训练MLLM和预训练的VOS记忆库，同时消化短期和长期的时间信息。为了提高MLLM有限上下文窗口内的信息效率，引入了对象对比学习和自我优化框架。GLUS在MeViS和Ref-Youtube-VOS基准测试中实现了最新水平的结果。项目网页是<a target="_blank" rel="noopener" href="https://glus-video.github.io/">网站链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入新型多模态大型语言模型框架GLUS用于视频对象分割（RefVOS）。</li>
<li>融合全局和局部一致性，解决以往MLLM在Ref和VOS之间的权衡问题。</li>
<li>通过稀疏的上下文帧和连续的查询帧进行全局和局部信息的处理。</li>
<li>联合训练MLLM和预训练的VOS记忆库，同时处理短期和长期时间信息。</li>
<li>采用对象对比学习提高信息效率。</li>
<li>引入自我优化框架来识别关键帧并进行传播。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07962">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2e4162cab4e2325903db118e13bfe942.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3c221397e99f939a899cefe5ccc2e5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d85594e10665a496e06c35bb2c6da6c9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SoTA-with-Less-MCTS-Guided-Sample-Selection-for-Data-Efficient-Visual-Reasoning-Self-Improvement"><a href="#SoTA-with-Less-MCTS-Guided-Sample-Selection-for-Data-Efficient-Visual-Reasoning-Self-Improvement" class="headerlink" title="SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual   Reasoning Self-Improvement"></a>SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual   Reasoning Self-Improvement</h2><p><strong>Authors:Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang</strong></p>
<p>In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at <a target="_blank" rel="noopener" href="https://github.com/si0wang/ThinkLite-VL">https://github.com/si0wang/ThinkLite-VL</a>. </p>
<blockquote>
<p>在这篇论文中，我们提出了一种有效的方法，利用显著更少的训练样本增强视觉推理能力，这完全依赖于自我提升，无需知识蒸馏。我们的关键见解是，强化微调（RFT）期间训练数据的难度至关重要。适当的挑战性样本即使数据集很小，也能大幅提升推理能力。尽管这是直观的，但主要挑战仍然在于准确量化样本的难度，以实现有效的数据过滤。为此，我们提出了一种重新利用蒙特卡洛树搜索（MCTS）的新方法来实现这一点。从我们开始精选的7万个开源训练样本中，我们引入了一种基于MCTS的选择方法，根据VLMs解决问题所需的迭代次数来量化样本难度。MCTS中的这种逐步推理明确了模型需要思考的时间，并更好地识别了真正具有挑战性的样本。我们过滤并保留了1.1万个样本，在Qwen2.5-VL-7B-Instruct上进行RFT，从而得到我们的最终模型ThinkLite-VL。在八个基准测试上的评估结果表明，ThinkLite-VL使用仅1.1万个训练样本，无需知识蒸馏，即可将Qwen2.5-VL-7B-Instruct的平均性能提高7%。这显著优于所有现有的7B级推理VLMs以及使用经典选择方法（如基于准确度的过滤）的相当基准线。值得注意的是，在MathVista上，ThinkLite-VL-7B达到了75.1的SOTA准确率，超越了Qwen2.5-VL-72B、GPT-4o和O1。我们的代码、数据和模型可在<a target="_blank" rel="noopener" href="https://github.com/si0wang/ThinkLite-VL%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/si0wang/ThinkLite-VL上获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07934v1">PDF</a> 21 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>该论文提出了一种利用自我提升强化学习的方法，在少量训练样本下提升视觉推理能力。论文的关键在于认识到强化精细训练时样本的难度至关重要。通过提出一种基于蒙特卡洛树搜索（MCTS）的筛选方法，量化样本难度并筛选出真正具有挑战性的样本进行训练，最终实现了模型性能的提升。评估结果显示，使用仅11k训练样本的ThinkLite-VL模型在多个基准测试中平均性能提升了7%，显著优于现有其他类似规模的推理视觉语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种基于自我提升强化学习的方法，能在少量训练样本下增强视觉推理能力。</li>
<li>论文强调在强化精细训练时样本的难度至关重要，通过筛选真正具有挑战性的样本进行训练可以提升模型性能。</li>
<li>论文使用蒙特卡洛树搜索（MCTS）筛选训练样本，基于模型解决问题所需的迭代次数量化样本难度。</li>
<li>ThinkLite-VL模型使用仅11k训练样本，在多个基准测试中平均性能提升了7%，显著优于现有其他类似规模的推理视觉语言模型。</li>
<li>ThinkLite-VL模型在MathVista测试上达到了当时最先进的准确率75.1%，超越了其他大型语言模型如Qwen2.5-VL-72B和GPT-4o等。</li>
<li>论文提供的代码、数据和模型可以在公开代码库中找到，方便后续研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07934">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-64299d139655edfe7483ba354d68ccb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6130e694a0065b05a61ed03bd6f97802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4db81e81336e1ed4f49924a5ada80d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1a8b6d5576264e336efff5fbdab597d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cbe035ba416646140e86b8b113fc9ed.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SpecReason-Fast-and-Accurate-Inference-Time-Compute-via-Speculative-Reasoning"><a href="#SpecReason-Fast-and-Accurate-Inference-Time-Compute-via-Speculative-Reasoning" class="headerlink" title="SpecReason: Fast and Accurate Inference-Time Compute via Speculative   Reasoning"></a>SpecReason: Fast and Accurate Inference-Time Compute via Speculative   Reasoning</h2><p><strong>Authors:Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, Ravi Netravali</strong></p>
<p>Recent advances in inference-time compute have significantly improved performance on complex tasks by generating long chains of thought (CoTs) using Large Reasoning Models (LRMs). However, this improved accuracy comes at the cost of high inference latency due to the length of generated reasoning sequences and the autoregressive nature of decoding. Our key insight in tackling these overheads is that LRM inference, and the reasoning that it embeds, is highly tolerant of approximations: complex tasks are typically broken down into simpler steps, each of which brings utility based on the semantic insight it provides for downstream steps rather than the exact tokens it generates. Accordingly, we introduce SpecReason, a system that automatically accelerates LRM inference by using a lightweight model to (speculatively) carry out simpler intermediate reasoning steps and reserving the costly base model only to assess (and potentially correct) the speculated outputs. Importantly, SpecReason’s focus on exploiting the semantic flexibility of thinking tokens in preserving final-answer accuracy is complementary to prior speculation techniques, most notably speculative decoding, which demands token-level equivalence at each step. Across a variety of reasoning benchmarks, SpecReason achieves 1.5-2.5$\times$ speedup over vanilla LRM inference while improving accuracy by 1.0-9.9%. Compared to speculative decoding without SpecReason, their combination yields an additional 19.4-44.2% latency reduction. We open-source SpecReason at <a target="_blank" rel="noopener" href="https://github.com/ruipeterpan/specreason">https://github.com/ruipeterpan/specreason</a>. </p>
<blockquote>
<p>最近，推理时间计算方面的进展通过利用大型推理模型（LRMs）生成长的思维链（CoTs）已经在复杂任务上显著提高了性能。然而，这种提高的准确性是以高推理延迟为代价的，这主要是由于生成的推理序列的长度和解码的自回归性质。我们解决这些开销的关键见解是，LRM推理及其所嵌入的推理对近似值有着很高的容忍度：复杂任务通常被分解为更简单的步骤，每个步骤的实用性都是基于它为后续步骤提供的语义见解，而不是它生成的精确标记。因此，我们引入了SpecReason系统，它通过使用轻量级模型来自动加速LRM推理，（推测地）执行更简单的中间推理步骤，仅使用昂贵的基准模型来评估（和可能纠正）推测的输出。重要的是，SpecReason专注于利用思维标记的语义灵活性来保持最终答案的准确性，这与先前的推测技术互补，尤其是推测解码，它要求每一步的标记级别等效。在多种推理基准测试中，SpecReason实现了比普通LRM推理快1.5-2.5倍的速度提升，同时提高了1.0-9.9%的准确率。与没有SpecReason的推测解码相比，它们的组合导致额外的19.4-44.2%延迟减少。我们在<a target="_blank" rel="noopener" href="https://github.com/ruipeterpan/specreason">https://github.com/ruipeterpan/specreason</a>开源SpecReason。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07891v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型推理模型（LRMs）的长链思维（CoTs）技术显著提高了复杂任务的性能，但这也导致了推理序列长度增加和自回归解码带来的推理时间延迟问题。为解决这些问题，我们提出了SpecReason系统，它通过采用轻量级模型进行推测性推理步骤来加速LRM推理过程，只在必要时才调用昂贵的基准模型进行评估和校正。SpecReason专注于利用思维符号的语义灵活性来保持最终答案的准确性，与先前的推测技术相比具有互补性。实验结果指出，相较于原始LRM推理，SpecReason提供了更高的加速和准确度改善；同时与没有SpecReason的推测解码相比，二者的结合带来了额外的延迟降低。我们已在GitHub上开源SpecReason。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型推理模型（LRMs）能够通过生成长链思维（CoTs）提高复杂任务性能。</li>
<li>LRM推理存在高推理时间延迟问题，主要源于生成的推理序列长度和自回归解码。</li>
<li>SpecReason系统旨在解决这些时间延迟问题，它通过轻量级模型进行推测性推理步骤来加速LRM推理过程。</li>
<li>SpecReason保留了思维符号的语义灵活性，从而提高最终答案的准确性。它是对先前推测技术的重要补充。</li>
<li>与原始LRM推理相比，SpecReason提供了更高的加速效果和准确度改善。相较于没有SpecReason的推测解码，二者的结合带来了额外的延迟降低。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07891">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c0c011fa5777a5e4e6d96b7854d9b99e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68be6c1e7ba96494067d8b0cf8335b47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c088baf2fc42e0ff24a30d0f242fe992.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e16ac672f980424cd9a2733fa62c8970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-900ed80dfecc668fae1e336eea120b91.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Enhancing-Large-Language-Models-through-Neuro-Symbolic-Integration-and-Ontological-Reasoning"><a href="#Enhancing-Large-Language-Models-through-Neuro-Symbolic-Integration-and-Ontological-Reasoning" class="headerlink" title="Enhancing Large Language Models through Neuro-Symbolic Integration and   Ontological Reasoning"></a>Enhancing Large Language Models through Neuro-Symbolic Integration and   Ontological Reasoning</h2><p><strong>Authors:Ruslan Idelfonso Magana Vsevolodovna, Marco Monti</strong></p>
<p>Large Language Models (LLMs) demonstrate impressive capabilities in natural language processing but suffer from inaccuracies and logical inconsistencies known as hallucinations. This compromises their reliability, especially in domains requiring factual accuracy. We propose a neuro-symbolic approach integrating symbolic ontological reasoning and machine learning methods to enhance the consistency and reliability of LLM outputs. Our workflow utilizes OWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking, and a lightweight machine learning model (logistic regression) for mapping natural language statements into logical forms compatible with the ontology. When inconsistencies between LLM outputs and the ontology are detected, the system generates explanatory feedback to guide the LLM towards a corrected, logically coherent response in an iterative refinement loop. We present a working Python prototype demonstrating this pipeline. Experimental results in a defined domain suggest significant improvements in semantic coherence and factual accuracy of LLM outputs, showcasing the potential of combining LLM fluency with the rigor of formal semantics. </p>
<blockquote>
<p>大型语言模型（LLM）在自然语言处理中展示了令人印象深刻的能力，但存在被称为“幻觉”的不准确性和逻辑不一致性。这损害了它们的可靠性，尤其是在需要事实准确性的领域。我们提出了一种神经符号方法，将符号本体推理和机器学习方法进行整合，以提高LLM输出的一致性和可靠性。我们的工作流程利用OWL本体、符号推理器（例如HermiT）进行一致性检查，以及轻量级机器学习模型（逻辑回归）将自然语言陈述映射到与本体兼容的逻辑形式。当检测到LLM输出与本体之间的不一致时，系统会生成解释性反馈，以指导LLM在迭代细化循环中做出更正、逻辑连贯的响应。我们展示了一个工作Python原型，演示了这个管道。在特定领域的实验结果表明，LLM输出的语义连贯性和事实准确性得到了显著提高，展示了结合LLM流畅性和形式语义严谨性的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07640v1">PDF</a> 11 pages, 1 figure, includes prototype implementation and   experimental evaluation. Submitted for consideration in the arXiv Artificial   Intelligence category (cs.AI)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在自然语言处理中展现出令人印象深刻的能力，但存在被称为“幻觉”的不准确性和逻辑不一致性问题，这影响了其可靠性，尤其是在需要事实准确性的领域。本文提出了一种神经符号方法，通过结合符号本体推理和机器学习方法来提高大型语言模型输出的一致性和可靠性。该方法利用OWL本体、符号推理器（例如HermiT）进行一致性检查，并使用轻量级机器学习模型（逻辑回归）将自然语言陈述映射到与本体兼容的逻辑形式。当检测到大型语言模型输出与本体之间的不一致性时，系统会生成解释性反馈，以指导大型语言模型在迭代优化循环中做出更正确、逻辑连贯的回应。实验结果表明，在特定领域里语义连贯性和事实准确性得到了显著提高，展示了结合大型语言模型的流畅性和形式语义学的严谨性的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在自然语言处理中表现出强大的能力，但存在不准确性及逻辑不一致性问题。</li>
<li>神经符号方法结合了符号本体推理和机器学习，旨在提高大型语言模型输出的一致性和可靠性。</li>
<li>使用OWL本体和符号推理器（如HermiT）进行一致性检查。</li>
<li>通过轻量级机器学习模型（逻辑回归）将自然语言转化为逻辑形式，与本体兼容。</li>
<li>当检测到不一致性时，系统生成解释性反馈以指导大型语言模型的迭代优化。</li>
<li>在特定领域的实验结果表明，该方法提高了语义连贯性和事实准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07640">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a7c48eb9f24678bf20ab4b0268048a32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbfdfb3d90970cc6ce633b4f6d22a6e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VLM-R1-A-Stable-and-Generalizable-R1-style-Large-Vision-Language-Model"><a href="#VLM-R1-A-Stable-and-Generalizable-R1-style-Large-Vision-Language-Model" class="headerlink" title="VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model"></a>VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model</h2><p><strong>Authors:Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, Tiancheng Zhao</strong></p>
<p>Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs’ performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the “OD aha moment”, the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at <a target="_blank" rel="noopener" href="https://github.com/om-ai-lab/VLM-R1">https://github.com/om-ai-lab/VLM-R1</a> </p>
<blockquote>
<p>最近，DeepSeek R1的研究表明，通过简单而有效的设计，强化学习（RL）可以显著提高大型语言模型（LLM）的推理能力。R1的核心在于其基于规则的奖励公式，它利用具有确定性标准答案的任务来启用精确且稳定的奖励计算。在视觉领域，我们同样观察到许多视觉理解任务本身就具备定义良好的标准注释。这一特性使它们自然地与基于规则的奖励机制兼容。受此观察结果的启发，我们研究了将R1风格的强化学习扩展到视觉语言模型（VLM），旨在增强它们的视觉推理能力。为此，我们开发了VLM-R1专用框架，旨在利用强化学习提高VLM在一般视觉语言任务上的性能。使用该框架，我们进一步探索了将强化学习应用于视觉领域的可行性。实验结果表明，基于强化学习的模型不仅在视觉理解任务上表现出竞争力，而且在泛化能力上也超越了监督微调（SFT）。此外，我们进行了全面的消融研究，揭示了一系列值得关注的见解，包括对象检测中的奖励破解现象、”OD瞬间领悟”的出现、训练数据质量的影响以及不同模型大小下强化学习的扩展行为。通过这些分析，我们旨在加深对强化学习如何增强视觉语言模型能力的理解，我们希望我们的发现和开源贡献将支持视觉语言强化学习社区的持续进步。我们的代码和模型可在[<a target="_blank" rel="noopener" href="https://github.com/om-ai-lab/VLM-R">https://github.com/om-ai-lab/VLM-R</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07615v1">PDF</a> 11 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了DeepSeek R1如何通过强化学习（RL）提高大型语言模型（LLM）的推理能力。其核心在于基于规则的奖励制定，利用具有确定性答案的任务来实现精确且稳定的奖励计算。受视觉领域任务自然兼容于基于规则的奖励机制的启发，研究者们将R1风格的强化学习扩展到视觉语言模型（VLM），开发了VLM-R1框架来提高VLM在通用视觉语言任务上的推理能力。实验结果表明，基于RL的模型不仅在视觉理解任务上表现有竞争力，而且在泛化能力上也超越了监督微调（SFT）。此外，研究者还通过全面的消融研究获得了一系列值得注意的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习（RL）可以显著提高大型语言模型（LLM）的推理能力。</li>
<li>DeepSeek R1的核心在于其基于规则的奖励制定，利用具有确定性答案的任务稳定计算奖励。</li>
<li>视觉理解任务天然地与基于规则的奖励机制兼容。</li>
<li>VLM-R1框架被开发出来，旨在利用强化学习提高视觉语言模型（VLM）在通用视觉语言任务上的性能。</li>
<li>基于RL的模型在视觉理解任务上表现优秀，且泛化能力超过监督微调（SFT）。</li>
<li>消融研究揭示了强化学习在物体检测中的奖励黑客现象、物体检测的“啊哈时刻”的出现、训练数据质量的影响以及不同模型大小下强化学习的扩展行为。</li>
<li>这些研究深化了对强化学习如何增强视觉语言模型能力的理解，并为该领域的持续进步提供了支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07615">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e8f30623d3c2e16c44bf301e96b9dfd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d20c2434fd742c70e7d20e928953e89c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8105e3e2538999b6cdd524e012dd576b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b17ce1f73a418665b50ebc40d7e1ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6ef4b9706c82a9d9bcc440230d92b48.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TokenFocus-VQA-Enhancing-Text-to-Image-Alignment-with-Position-Aware-Focus-and-Multi-Perspective-Aggregations-on-LVLMs"><a href="#TokenFocus-VQA-Enhancing-Text-to-Image-Alignment-with-Position-Aware-Focus-and-Multi-Perspective-Aggregations-on-LVLMs" class="headerlink" title="TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware   Focus and Multi-Perspective Aggregations on LVLMs"></a>TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware   Focus and Multi-Perspective Aggregations on LVLMs</h2><p><strong>Authors:Zijian Zhang, Xuhui Zheng, Xuecheng Wu, Chong Peng, Xuezhi Cao</strong></p>
<p>While text-to-image (T2I) generation models have achieved remarkable progress in recent years, existing evaluation methodologies for vision-language alignment still struggle with the fine-grained semantic matching. Current approaches based on global similarity metrics often overlook critical token-level correspondences between textual descriptions and visual content. To this end, we present TokenFocus-VQA, a novel evaluation framework that leverages Large Vision-Language Models (LVLMs) through visual question answering (VQA) paradigm with position-specific probability optimization. Our key innovation lies in designing a token-aware loss function that selectively focuses on probability distributions at pre-defined vocabulary positions corresponding to crucial semantic elements, enabling precise measurement of fine-grained semantical alignment. The proposed framework further integrates ensemble learning techniques to aggregate multi-perspective assessments from diverse LVLMs architectures, thereby achieving further performance enhancement. Evaluated on the NTIRE 2025 T2I Quality Assessment Challenge Track 1, our TokenFocus-VQA ranks 2nd place (0.8445, only 0.0001 lower than the 1st method) on public evaluation and 2nd place (0.8426) on the official private test set, demonstrating superiority in capturing nuanced text-image correspondences compared to conventional evaluation methods. </p>
<blockquote>
<p>近年来，文本到图像（T2I）生成模型取得了显著进展，但现有的视觉语言对齐评估方法仍然面临着精细语义匹配方面的挑战。基于全局相似度指标的当前方法往往忽视了文本描述和视觉内容之间关键的词级对应关系。为此，我们提出了TokenFocus-VQA，这是一个新的评估框架，它利用大型视觉语言模型（LVLMs），通过位置特定概率优化的视觉问答（VQA）范式。我们的关键创新在于设计了一个词感知损失函数，该函数有选择地关注对应于关键语义元素的事先定义的词汇位置的概率分布，从而实现精细语义对齐的精确测量。所提出的框架还结合了集成学习技术，从多种LVLMs架构中汇集多角度评估，从而实现了进一步的性能提升。在NTIRE 2025 T2I质量评估挑战赛Track 1上，我们的TokenFocus-VQA在公共评估中位列第二（0.8445，仅比第一名方法低0.0001），在官方私有测试集上也位列第二（0.8426），这证明了其在捕捉微妙的文本图像对应关系方面优于传统评估方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07556v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为TokenFocus-VQA的新型评估框架，该框架利用大型视觉语言模型（LVLMs）通过视觉问答（VQA）范式进行位置特定概率优化。其创新之处在于设计了一种标记感知损失函数，该函数能够选择性地关注关键语义元素对应的预定义词汇位置的概率分布，从而实现对精细粒度语义对齐的精确测量。此外，该框架还结合了集成学习技术，从不同架构的LVLMs中聚合多角度评估，从而进一步提高性能。在NTIRE 2025 T2I质量评估挑战赛轨道1上，TokenFocus-VQA取得了第二名的成绩，表明其在捕捉微妙的文本图像对应方面相比传统评估方法具有优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TokenFocus-VQA是一种新型的视觉语言对齐评估框架，结合了视觉问答（VQA）和大型视觉语言模型（LVLMs）。</li>
<li>该框架通过位置特定概率优化，实现了对文本描述与视觉内容之间关键令牌级别对应关系的关注。</li>
<li>框架的核心创新在于设计了一种标记感知损失函数，用于精确测量精细粒度语义对齐。</li>
<li>TokenFocus-VQA通过集成学习技术整合了多个视觉语言模型的评估结果，提高了性能。</li>
<li>该框架在NTIRE 2025 T2I质量评估竞赛中取得了第二名的成绩，证明了其有效性。</li>
<li>相比传统评估方法，TokenFocus-VQA在捕捉文本图像细微对应关系方面表现出优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07556">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f6a3cdf247833d2bba933883fa8f98c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82529202e319056130d5a45735681b4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5939323a7d8aa60503aa16f59d33d802.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0185ee95443a260596e0186f24756398.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fcc69bca19ad2b3c38f10b3f5100c02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3077ef0770727d635820590cfd19b3d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Supervised-Optimism-Correction-Be-Confident-When-LLMs-Are-Sure"><a href="#Supervised-Optimism-Correction-Be-Confident-When-LLMs-Are-Sure" class="headerlink" title="Supervised Optimism Correction: Be Confident When LLMs Are Sure"></a>Supervised Optimism Correction: Be Confident When LLMs Are Sure</h2><p><strong>Authors:Junjie Zhang, Rushuai Yang, Shunyu Liu, Ting-En Lin, Fei Huang, Yi Chen, Yongbin Li, Dacheng Tao</strong></p>
<p>In this work, we establish a novel theoretical connection between supervised fine-tuning and offline reinforcement learning under the token-level Markov decision process, revealing that large language models indeed learn an implicit $Q$-function for inference. Through this theoretical lens, we demonstrate that the widely used beam search method suffers from unacceptable over-optimism, where inference errors are inevitably amplified due to inflated $Q$-value estimations of suboptimal steps. To address this limitation, we propose Supervised Optimism Correction(SOC), which introduces a simple yet effective auxiliary loss for token-level $Q$-value estimations during supervised fine-tuning. Specifically, the auxiliary loss employs implicit value regularization to boost model confidence in expert-demonstrated responses, thereby suppressing over-optimism toward insufficiently supervised responses. Extensive experiments on mathematical reasoning benchmarks, including GSM8K, MATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search across a series of open-source models. </p>
<blockquote>
<p>在这项工作中，我们在基于词级别的马尔可夫决策过程下，建立了监督微调与离线强化学习之间的新型理论联系，揭示了大型语言模型确实为推理学习了隐式的Q函数。通过这一理论视角，我们证明了广泛使用的贪心搜索方法存在不可接受的过度乐观问题，由于次优步骤的Q值估计膨胀，推理错误不可避免地会被放大。为了解决这一局限性，我们提出了监督乐观校正（SOC），它为词级别的Q值估计在监督微调过程中引入了一种简单有效的辅助损失。具体来说，辅助损失采用隐式价值正则化来提升模型对专家演示的反应信心，从而抑制对不足监督反应的过度乐观态度。在GSM8K、MATH和GAOKAO等数学推理基准测试上的大量实验，展示了所提出的SOC与贪心搜索在一系列开源模型中的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07527v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于token级别的Markov决策过程，本文建立了监督微调与离线强化学习之间的新型理论联系，揭示大型语言模型确实为推理学习了隐式Q函数。文章指出广泛使用的贪心搜索方法存在不可接受的过度乐观问题，由于次优步骤的Q值估计膨胀，推理错误会不可避免地放大。为解决此问题，本文提出了监督乐观校正（SOC），在监督微调期间为token级别的Q值估计引入简单有效的辅助损失。通过隐性价值正则化来提升模型对专家演示答案的信心，从而抑制对缺乏监督答案的过度乐观态度。在GSM8K、MATH和GAOKAO等数学推理基准测试上的大量实验，展示了带有贪心搜索的SOC方法的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文建立了监督微调与离线强化学习之间的理论联系，揭示了大型语言模型学习隐式Q函数用于推理。</li>
<li>贪心搜索方法存在过度乐观问题，导致推理错误。</li>
<li>为解决贪心搜索方法的局限，提出了监督乐观校正（SOC）。</li>
<li>SOC通过引入辅助损失来改进Q值估计，采用隐性价值正则化来提升模型信心。</li>
<li>实验表明，SOC方法在多个数学推理基准测试上表现优越。</li>
<li>SOC方法适用于多种开源模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07527">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-166676115d7d720742fa86c41f9613de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9b3416e8862142ba9beffd55da77534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-701b1c6cfbb5120b7352df1147faaaf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c4647e09cff145a88a880801b607ffc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Why-We-Feel-Breaking-Boundaries-in-Emotional-Reasoning-with-Multimodal-Large-Language-Models"><a href="#Why-We-Feel-Breaking-Boundaries-in-Emotional-Reasoning-with-Multimodal-Large-Language-Models" class="headerlink" title="Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal   Large Language Models"></a>Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal   Large Language Models</h2><p><strong>Authors:Yuxiang Lin, Jingdong Sun, Zhi-Qi Cheng, Jue Wang, Haomin Liang, Zebang Cheng, Yifei Dong, Jun-Yan He, Xiaojiang Peng, Xian-Sheng Hua</strong></p>
<p>Most existing emotion analysis emphasizes which emotion arises (e.g., happy, sad, angry) but neglects the deeper why. We propose Emotion Interpretation (EI), focusing on causal factors-whether explicit (e.g., observable objects, interpersonal interactions) or implicit (e.g., cultural context, off-screen events)-that drive emotional responses. Unlike traditional emotion recognition, EI tasks require reasoning about triggers instead of mere labeling. To facilitate EI research, we present EIBench, a large-scale benchmark encompassing 1,615 basic EI samples and 50 complex EI samples featuring multifaceted emotions. Each instance demands rationale-based explanations rather than straightforward categorization. We further propose a Coarse-to-Fine Self-Ask (CFSA) annotation pipeline, which guides Vision-Language Models (VLLMs) through iterative question-answer rounds to yield high-quality labels at scale. Extensive evaluations on open-source and proprietary large language models under four experimental settings reveal consistent performance gaps-especially for more intricate scenarios-underscoring EI’s potential to enrich empathetic, context-aware AI applications. Our benchmark and methods are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench">https://github.com/Lum1104/EIBench</a>, offering a foundation for advanced multimodal causal analysis and next-generation affective computing. </p>
<blockquote>
<p>现有大部分情感分析主要关注哪种情绪出现（例如快乐、悲伤、愤怒），却忽视了更深层的原因。我们提出情感解读（EI），重点关注推动情绪反应的因果因素，无论这些因果因素是显性的（如可观事物、人际互动），还是隐性的（如文化背景、屏幕外事件）。与传统的情感识别不同，情感解读任务需要进行触发因素推理，而非简单的标签标注。为了促进情感解读研究，我们推出了EIBench，这是一个大规模基准测试，包含1615个基本情感解读样本和50个复杂的情感解读样本，涉及多面情绪。每个实例都需要基于理性的解释，而非简单的分类。我们还进一步提出了从粗糙到精细的自我提问（CFSA）注释管道，该管道通过引导视觉语言模型（VLLM）进行迭代问答回合，以大规模生成高质量标签。在四种实验设置下，对开源和专有大型语言模型的广泛评估显示了一致的性能差距，尤其是在更复杂场景中，这凸显了情感解读在丰富具有同理心和上下文感知的AI应用方面的潜力。我们的基准测试和方法已在<a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%EF%BC%8C%E4%B8%BA%E5%85%88%E8%BF%9B%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9B%A0%E6%9E%9C%E5%88%86%E6%9E%90%E5%92%8C%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%83%85%E6%84%9F%E8%AE%A1%E7%AE%97%E6%8F%90%E4%BE%9B%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/Lum1104/EIBench上公开可用，为先进的多模态因果分析和下一代情感计算提供了基础。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07521v1">PDF</a> Accepted at CVPR Workshop NEXD 2025. 21 pages, Project:   <a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench">https://github.com/Lum1104/EIBench</a></p>
<p><strong>Summary</strong><br>情感分析不再仅仅关注产生的情绪类型（如快乐、悲伤、愤怒等），而是深入到情感背后的原因。为此，我们提出了情感解读（EI），重点研究驱动情感反应的因果因素，包括显性的（如可见物体、人际互动）和隐性的（如文化背景、屏幕外事件）。情感解读任务需要进行触发原因分析而非单纯的标签归类。为推进情感解读研究，我们推出了包含大规模基准数据的EIBench，其中包括涵盖多重情感的复杂样本。每个样本都需要基于理性的解释而非简单的分类。此外，我们还提出了从粗糙到精细的自我提问（CFSA）注释管道，通过迭代问答环节指导视觉语言模型（VLLMs），大规模生成高质量标签。在各种不同情境的实验设置下，对于开源和自有大型语言模型的全面评估揭示了普遍存在的性能差距，尤其是针对更复杂的场景。我们的基准数据和方法可供公众访问（<a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench%EF%BC%89%EF%BC%8C%E4%B8%BA%E5%85%88%E8%BF%9B%E7%9A%84%E6%A8%A1%E6%80%81%E5%9B%A0%E6%9E%9C%E5%88%86%E6%9E%90%E5%92%8C%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%83%85%E6%84%9F%E8%AE%A1%E7%AE%97%E5%A5%A0%E5%AE%9A%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/Lum1104/EIBench），为先进的模态因果分析和下一代情感计算奠定了基础。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有情感分析主要关注情绪类型，忽略了情感背后的深层原因。</li>
<li>提出情感解读（EI）概念，重点研究驱动情感反应的因果因素，包括显性和隐性因素。</li>
<li>情感解读任务要求对触发原因进行推理，而非简单的标签归类。</li>
<li>推出大规模基准数据EIBench，包含复杂样本，强调基于理性的解释而非简单分类。</li>
<li>提出从粗糙到精细的自我提问（CFSA）注释管道，指导视觉语言模型生成高质量标签。</li>
<li>对不同情境下的语言模型进行全面评估，揭示性能差距，强调情感解读在复杂场景下的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07521">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-75572500d902af3fbd8abe6f45e95a36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc6e33b57fb7e3e51a09f652af82fdb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6231af177a1fce424098921faaefb94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d535e7099d3f06c7d98bd32ee2245864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f5fc7a8aff5237e953bcd53ef359d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-798c403c4105bfea7dc240b46b56618b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MDIT-A-Model-free-Data-Interpolation-Method-for-Diverse-Instruction-Tuning"><a href="#MDIT-A-Model-free-Data-Interpolation-Method-for-Diverse-Instruction-Tuning" class="headerlink" title="MDIT: A Model-free Data Interpolation Method for Diverse Instruction   Tuning"></a>MDIT: A Model-free Data Interpolation Method for Diverse Instruction   Tuning</h2><p><strong>Authors:Yangning Li, Zihua Lan, Lv Qingsong, Yinghui Li, Hai-Tao Zheng</strong></p>
<p>As Large Language Models (LLMs) are increasingly applied across various tasks, instruction tuning has emerged as a critical method for enhancing model performance. However, current data management strategies face substantial challenges in generating diverse and comprehensive data, restricting further improvements in model performance. To address this gap, we propose MDIT, a novel model-free data interpolation method for diverse instruction tuning, which generates varied and high-quality instruction data by performing task interpolation. Moreover, it contains diversity-based clustering strategies to ensure the diversity of the training data. Extensive experiments show that our method achieves superior performance in multiple benchmark tasks. The LLMs finetuned with MDIT show significant improvements in numerous tasks such as general question answering, math reasoning, and code generation. MDIT offers an efficient and automatic data synthetic method, generating diverse instruction data without depending on external resources while expanding the application potential of LLMs in complex environments. </p>
<blockquote>
<p>随着大型语言模型（LLM）在各项任务中的广泛应用，指令微调作为一种提高模型性能的关键方法应运而生。然而，当前的数据管理策略在生成多样且全面的数据方面面临巨大挑战，限制了模型性能的进一步提高。为了解决这一差距，我们提出了MDIT，这是一种用于多样指令微调的新型无模型数据插值方法，它通过任务插值生成多样且高质量的任务指令数据。此外，它包含基于多样性的聚类策略，以确保训练数据的多样性。大量实验表明，我们的方法在多个基准测试任务上取得了卓越的性能。使用MDIT微调的大型语言模型在通用问答、数学推理和代码生成等多项任务中取得了显著的改进。MDIT提供了一种高效且自动的数据合成方法，在不需要依赖外部资源的情况下生成多样化的指令数据，同时扩展了大型语言模型在复杂环境中的应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07288v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLMs）在多项任务中的应用日益广泛，指令微调作为一种提升模型性能的关键方法备受关注。然而，当前的数据管理策略在生成多样且全面的数据方面面临巨大挑战，限制了模型性能的进一步提升。为解决这一难题，我们提出了MDIT，一种用于多样指令微调的新型模型外数据插值方法。它通过任务插值生成多样且高质量的任务指令数据，并包含基于多样性的聚类策略，以确保训练数据的多样性。实验表明，该方法在多个基准测试任务上表现优异。使用MDIT微调后的LLMs在通用问答、数学推理和代码生成等多项任务中表现出显著改进。MDIT提供了一种高效且自动的数据合成方法，能够在不依赖外部资源的情况下生成多样的指令数据，扩大了LLM在复杂环境中的应用潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在多项任务中的应用越来越广泛，指令微调是提高其性能的关键方法。</li>
<li>当前数据管理策略在生成多样且全面的数据方面存在挑战。</li>
<li>MDIT是一种新型的模型外数据插值方法，用于生成多样且高质量的指令数据。</li>
<li>MDIT通过任务插值实现数据生成，并包含基于多样性的聚类策略。</li>
<li>实验表明MDIT在多个基准测试任务上表现优异。</li>
<li>使用MDIT微调后的LLMs在多项任务中表现出显著改进，如通用问答、数学推理和代码生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07288">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6d9ecc47a7f024b165b8e08f58a72181.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6a6318144dfe97301a0be55520d871.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ca0717a3bb250b92dffe9f245901782.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01ac9b0bd86640f683c58e921e5a12d5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RAISE-Reinforenced-Adaptive-Instruction-Selection-For-Large-Language-Models"><a href="#RAISE-Reinforenced-Adaptive-Instruction-Selection-For-Large-Language-Models" class="headerlink" title="RAISE: Reinforenced Adaptive Instruction Selection For Large Language   Models"></a>RAISE: Reinforenced Adaptive Instruction Selection For Large Language   Models</h2><p><strong>Authors:Lv Qingsong, Yangning Li, Zihua Lan, Zishan Xu, Jiwei Tang, Yinghui Li, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu</strong></p>
<p>In the instruction fine-tuning of large language models (LLMs), it has become a consensus that a few high-quality instructions are superior to a large number of low-quality instructions. At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training. These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks. So we designed a dynamic, task-objective-driven instruction selection framework RAISE(Reinforenced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instruction at each step based on the expected impact of instruction on model performance improvement. Our approach is well interpretable and has strong task-specific optimization capabilities. By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy. Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods. Notably, RAISE achieves superior performance by updating only 1% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness. </p>
<blockquote>
<p>在大规模语言模型（LLM）的指令微调中，已经达成一种共识，即少数高质量指令优于大量低质量指令。目前，已经提出了许多指令选择方法，但大多数方法都是基于启发式质量指标来选择指令，并且只在训练前考虑数据选择。这些设计导致指令微调优化不足，固定的启发式指标通常难以针对特定任务进行优化。因此，我们设计了一个动态的、任务目标驱动的指令选择框架RAISE（Reinforenced自适应指令SElection），它将整个指令微调过程纳入优化，根据指令对模型性能改进的预期影响，在每一步选择指令。我们的方法具有良好的可解释性，并具有较强的任务特定优化能力。通过将动态指令选择建模为序列决策过程，我们使用强化学习来训练我们的选择策略。大量的实验和结果分析证明了我们的方法与其他指令选择方法的优越性。值得注意的是，RAISE仅在1%的训练步骤中进行更新就实现了优于全数据训练的性能，证明了其效率和有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型的指令微调中，高质量指令优于大量低质量指令。传统方法基于启发式质量指标选择指令，仅在训练前考虑数据选择，导致指令微调优化不足。本文提出动态、任务目标驱动的指令选择框架RAISE，将指令微调过程纳入优化，根据指令对模型性能提升的预期影响进行动态选择。RAISE方法可解释性强，具有强大的任务特定优化能力，将动态指令选择建模为序列决策过程，并使用强化学习训练选择策略。实验证明，RAISE方法与其他指令选择方法相比具有优越性，仅在1%的训练步骤中更新数据即可实现卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在大型语言模型的指令微调中，高质量指令优于大量低质量指令。</li>
<li>传统指令选择方法基于启发式质量指标，仅在训练前考虑数据选择，存在优化不足的问题。</li>
<li>RAISE是一种动态、任务目标驱动的指令选择框架，将指令微调过程纳入优化。</li>
<li>RAISE根据指令对模型性能提升的预期影响进行动态选择。</li>
<li>RAISE方法可解释性强，具有强大的任务特定优化能力。</li>
<li>RAISE将动态指令选择建模为序列决策过程，并使用强化学习训练选择策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07282">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ec946fae2f29b4a3a17e4ef27e19477c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7fd0052fbdc3e6d1653cb1d9c819d0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e7404c0af9e0d21a2b5687c5f496812.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdc1718f1e76b21d2e6b3960268e5b17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcab05466fc3a5c490de506abc717a19.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Face-LLaVA-Facial-Expression-and-Attribute-Understanding-through-Instruction-Tuning"><a href="#Face-LLaVA-Facial-Expression-and-Attribute-Understanding-through-Instruction-Tuning" class="headerlink" title="Face-LLaVA: Facial Expression and Attribute Understanding through   Instruction Tuning"></a>Face-LLaVA: Facial Expression and Attribute Understanding through   Instruction Tuning</h2><p><strong>Authors:Ashutosh Chaubey, Xulang Guan, Mohammad Soleymani</strong></p>
<p>The human face plays a central role in social communication, necessitating the use of performant computer vision tools for human-centered applications. We propose Face-LLaVA, a multimodal large language model for face-centered, in-context learning, including facial expression and attribute recognition. Additionally, Face-LLaVA is able to generate natural language descriptions that can be used for reasoning. Leveraging existing visual databases, we first developed FaceInstruct-1M, a face-centered database for instruction tuning MLLMs for face processing. We then developed a novel face-specific visual encoder powered by Face-Region Guided Cross-Attention that integrates face geometry with local visual features. We evaluated the proposed method across nine different datasets and five different face processing tasks, including facial expression recognition, action unit detection, facial attribute detection, age estimation and deepfake detection. Face-LLaVA achieves superior results compared to existing open-source MLLMs and competitive performance compared to commercial solutions. Our model output also receives a higher reasoning rating by GPT under a zero-shot setting across all the tasks. Both our dataset and model wil be released at <a target="_blank" rel="noopener" href="https://face-llava.github.io/">https://face-llava.github.io</a> to support future advancements in social AI and foundational vision-language research. </p>
<blockquote>
<p>人脸在社会交流中具有核心作用，这要求使用高性能的计算机视觉工具进行以人为中心的应用。我们提出了Face-LLaVA，这是一种面向人脸的中心化上下文学习多模态大型语言模型，包括面部表情和属性识别。此外，Face-LLaVA能够生成可用于推理的自然语言描述。我们利用现有的视觉数据库，首先开发了FaceInstruct-1M，这是一个用于指令微调面向人脸处理的大型语言模型的人脸中心化数据库。然后我们开发了一种新型的人脸特定视觉编码器，该编码器由Face-Region Guided Cross-Attention驱动，集成了人脸几何与局部视觉特征。我们在九个不同的数据集和五个不同的人脸处理任务上评估了所提出的方法，包括面部表情识别、动作单元检测、面部属性检测、年龄估计和深度伪造检测。Face-LLaVA相较于现有的开源大型语言模型取得了优越的结果，并在所有任务中实现了与商业解决方案相竞争的性能。我们的模型输出还得到了GPT在零样本设置下的更高推理评分。我们的数据集和模型都将在<a target="_blank" rel="noopener" href="https://face-llava.github.io发布,以支持社会人工智能和基础视觉语言研究的未来发展./">https://face-llava.github.io发布，以支持社会人工智能和基础视觉语言研究的未来发展。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07198v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://face-llava.github.io/">https://face-llava.github.io</a></p>
<p><strong>Summary</strong></p>
<p>人脸识别在社会交流中具有重要作用，需要高性能计算机视觉工具支持面向人类的应用。本研究提出了Face-LLaVA，一种用于面部为中心的多模态大型语言模型，支持上下文学习，包括面部表情和属性识别。此外，Face-LLaVA能够生成自然语言描述，用于推理。研究构建了FaceInstruct-1M数据库，并开发了一种新型面部特定视觉编码器，集成了面部几何与局部视觉特征。在九个数据集和五个面部处理任务上的实验表明，Face-LLaVA在表情识别等方面表现卓越，相比其他开源大型语言模型有显著提升。其模型输出也在GPT零样本设定下获得了较高的推理评分。该数据集与模型将为推动社会人工智能和基本视觉语言研究提供支持。更多详情可访问：<a target="_blank" rel="noopener" href="https://face-llava.github.io/">https://face-llava.github.io</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人脸在社会交流中的重要性及其对于高性能计算机视觉工具的需求。</li>
<li>Face-LLaVA多模态大型语言模型的提出，用于面部为中心、支持上下文学习的任务。</li>
<li>Face-LLaVA能够进行面部表情和属性识别，并能生成自然语言描述进行推理。</li>
<li>利用现有视觉数据库建立了FaceInstruct-1M面部为中心的数据集。</li>
<li>开发了一种新型面部特定视觉编码器，集成面部几何与局部视觉特征的技术。</li>
<li>Face-LLaVA在不同数据集上的实验结果展示其卓越性能，特别是在面部表情识别等方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07198">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2c498a382c2c44020fd6944ba0640515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-820813fb20fcf0bc3fba72e44516473f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a72d95239f528aa787be1233717269b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af826c5b7568ac6733cda154323e226c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-859162fc5dc3e5450e362d7aa562a4ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d4752b21af542648e7b07214d4f3a64.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="HypoEval-Hypothesis-Guided-Evaluation-for-Natural-Language-Generation"><a href="#HypoEval-Hypothesis-Guided-Evaluation-for-Natural-Language-Generation" class="headerlink" title="HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation"></a>HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation</h2><p><strong>Authors:Mingxuan Li, Hanchen Li, Chenhao Tan</strong></p>
<p>Large language models (LLMs) have demonstrated great potential for automating the evaluation of natural language generation. Previous frameworks of LLM-as-a-judge fall short in two ways: they either use zero-shot setting without consulting any human input, which leads to low alignment, or fine-tune LLMs on labeled data, which requires a non-trivial number of samples. Moreover, previous methods often provide little reasoning behind automated evaluations. In this paper, we propose HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM’s assigned scores on each decomposed dimension to acquire overall scores. With only 30 human evaluations, HypoEval achieves state-of-the-art performance in alignment with both human rankings (Spearman correlation) and human scores (Pearson correlation), on average outperforming G-Eval by 11.86% and fine-tuned Llama-3.1-8B-Instruct with at least 3 times more human evaluations by 11.95%. Furthermore, we conduct systematic studies to assess the robustness of HypoEval, highlighting its effectiveness as a reliable and interpretable automated evaluation framework. </p>
<blockquote>
<p>大型语言模型（LLM）在自动化评估自然语言生成方面展现出了巨大的潜力。以前的LLM评估框架在两个方面存在不足：它们要么使用零样本设置，不参考任何人工输入，导致对齐程度低，要么对标注数据进行微调，这需要大量的样本。此外，以前的方法往往对自动化评估背后的推理依据提供得很少。在本文中，我们提出了假设引导评估框架（HypoEval），首先使用少量的人工评估语料库来生成更详细的人类判断规则，然后采用类似清单的方法，结合LLM在每个分解维度上的得分，以获得总体得分。仅通过30次人工评估，HypoEval就达到了最先进的人机对齐性能（体现在斯皮尔曼相关系数和皮尔逊相关系数上），平均而言，相对于G-Eval提升了11.86%，并且在至少3倍以上的人工评估情况下，相对于经过训练的Llama-3.1-8B-Instruct提升了11.95%。此外，我们还进行了系统的研究来评估HypoEval的稳健性，强调了其作为可靠和可解释的自动化评估框架的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07174v1">PDF</a> 22 pages, 3 figures, code link:   <a target="_blank" rel="noopener" href="https://github.com/ChicagoHAI/HypoEval-Gen">https://github.com/ChicagoHAI/HypoEval-Gen</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种假设引导的评价框架HypoEval，该框架使用少量的人类评价数据生成更详细的评价准则，并结合清单式的方法，通过大型语言模型（LLM）对各个分解维度进行评分，从而获取总体评分。HypoEval只需30个人类评价即可达到与最新技术相当的性能水平，与人类排名和评分的对齐度较高，且在鲁棒性评估中表现出了其可靠性和可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在自动评估自然语言生成方面展现巨大潜力，但之前的方法存在不足。</li>
<li>提出了一种假设引导的评价框架HypoEval。</li>
<li>使用少量的人类评价数据生成详细的评价准则。</li>
<li>结合清单式方法，通过LLM对各个分解维度进行评分，获得总体评分。</li>
<li>仅用30个人类评价即可达到与最新技术相当的性能水平。</li>
<li>与人类排名和评分的对齐度高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07174">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-52fbcdcfad5e36714c9d8a9dcec030ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c078e7832efa6ca48d95b191c601c73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdd74fff946991743697425861fad073.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Holistic-Capability-Preservation-Towards-Compact-Yet-Comprehensive-Reasoning-Models"><a href="#Holistic-Capability-Preservation-Towards-Compact-Yet-Comprehensive-Reasoning-Models" class="headerlink" title="Holistic Capability Preservation: Towards Compact Yet Comprehensive   Reasoning Models"></a>Holistic Capability Preservation: Towards Compact Yet Comprehensive   Reasoning Models</h2><p><strong>Authors: Ling Team, Caizhi Tang, Chilin Fu, Chunwei Wu, Jia Guo, Jianwen Wang, Jingyu Hu, Liang Jiang, Meng Li, Peng Jiao, Pingping Liu, Shaomian Zheng, Shiwei Liang, Shuaicheng Li, Yalin Zhang, Yingting Wu, Yongkang Liu, Zhenyu Huang</strong></p>
<p>This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill’s reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at <a target="_blank" rel="noopener" href="https://huggingface.co/inclusionAI">https://huggingface.co/inclusionAI</a> </p>
<blockquote>
<p>本技术报告介绍了Ring-Lite-Distill，这是一个轻量级的推理模型，源于我们开源的混合专家（MoE）大型语言模型（LLM）Ling-Lite。本研究表明，通过精心的高质量数据收集和巧妙的训练模式，紧凑的MoE模型Ling-Lite可以进一步训练，实现出色的推理能力，同时保持其参数高效架构，仅使用2.75亿个激活参数，建立有效的轻量化推理架构。在构建此模型时，我们不仅仅着眼于提高解决高难度数学问题的先进推理能力，而是致力于开发具有更全面能力覆盖范围的推理模型。我们的方法确保了不同难度级别的推理任务的覆盖，同时保留了一般能力，如指令遵循、工具使用和知识保留。我们证明，Ring-Lite-Distill的推理能力与DeepSeek-R1-Distill-Qwen-7B相当，而其一般能力则显著超过了DeepSeek-R1-Distill-Qwen-7B。模型可在<a target="_blank" rel="noopener" href="https://huggingface.co/inclusionAI">https://huggingface.co/inclusionAI</a> 访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07158v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>Ling-Lite模型通过精心的高品质数据整合和创新训练模式，实现了出色的推理能力，同时保持了参数效率。其进一步训练出的轻量级推理模型Ring-Lite-Distill，具有卓越的性能和全面的能力覆盖。该模型不仅在高难度数学问题解决等高级推理任务上表现出色，还具备通用能力，如指令遵循、工具使用和知识保留等。其推理能力与DeepSeek-R1-Distill-Qwen-7B相当，但通用能力更胜一筹。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ring-Lite-Distill是基于Ling-Lite模型的轻量级推理模型。</li>
<li>Ling-Lite通过高质量数据整合和创新训练模式实现了出色的推理能力。</li>
<li>Ring-Lite-Distill具有卓越的性能和全面的能力覆盖，包括高级推理和通用能力。</li>
<li>该模型在保持参数效率的同时，实现了高水平的推理能力。</li>
<li>Ring-Lite-Distill的推理能力与DeepSeek-R1-Distill-Qwen-7B相当。</li>
<li>模型具有通用能力，如指令遵循、工具使用和知识保留等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07158">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d54d371caf577f988b38227b5c15d41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df81c4c0da0f66bbf4c6217e0376fab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d1788c5eaee4f28b7d5fbe8c389e635.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6eefcbf9684755ca8ef8e2d00a1161b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50695317975e22051bfa8203d963dc0a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning"><a href="#VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning" class="headerlink" title="VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning"></a>VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning</h2><p><strong>Authors:Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang</strong></p>
<p>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs. </p>
<blockquote>
<p>近期强化学习领域的进展极大地提升了多模态大型语言模型的推理能力。虽然诸如集团相对策略优化（GRPO）和基于规则的奖励机制等方法在文本和图像领域展现出巨大的潜力，但它们在视频理解方面的应用仍然有限。本文系统地探讨了使用GRPO的强化微调（RFT）在视频MLLMs中的应用，旨在提升时空感知能力的同时保持通用能力。我们的实验表明，RFT对于特定任务的改进非常注重数据效率。通过在有限的样本上对时空感知目标进行多任务RFT训练，我们开发出了VideoChat-R1，这是一款强大的视频MLLM，它在时空感知任务上实现了最先进的性能，同时不牺牲对话能力，并展现出新兴的时空推理能力。与Qwen2.5-VL-7B相比，VideoChat-R1在诸如时间定位（+31.8）和对象跟踪（+31.2）等任务上的性能提升了数倍。此外，它在通用问答基准测试（如VideoMME（+0.9）、MVBench（+1.0）和Perception Test（+0.9））上也取得了显著改善。我们的研究突出了RFT在视频MLLM专项任务增强方面的潜力。我们希望我们的研究能为未来视频MLLM的强化学习研究提供有价值的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06958v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>强化学习在提升多模态大型语言模型（MLLMs）的推理能力方面取得了显著进展。本文系统探讨了使用Group Relative Policy Optimization（GRPO）的Reinforcement Fine-Tuning（RFT）在视频MLLMs中的应用，旨在提高时空感知能力的同时保持通用能力。实验表明，RFT在特定任务改进方面非常注重数据效率。通过有限样本的时空感知目标上的多任务RFT，我们开发了VideoChat-R1，它在时空感知任务上实现了卓越性能，同时不牺牲聊天能力，并展现出新兴的时空推理能力。与Qwen2.5-VL-7B相比，VideoChat-R1在诸如时间定位（提高31.8）和对象跟踪（提高31.2）等任务上的性能提升了数倍。此外，它在一般问答基准测试中也有显著改善，如VideoMME（+ 0.9），MVBench（+ 1.0）和Perception Test（+ 0.9）。本文的研究结果突显了RFT在视频MLLMs特定任务增强方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习在提升多模态大型语言模型的推理能力上有了新进展。</li>
<li>本文探讨了Reinforcement Fine-Tuning (RFT) 与Group Relative Policy Optimization (GRPO) 在视频MLLMs中的应用。</li>
<li>RFT旨在提高视频MLLMs的时空感知能力，同时保持其通用能力。</li>
<li>实验显示RFT是数据高效的，能在特定任务上实现显著改进。</li>
<li>通过多任务RFT，开发了VideoChat-R1模型，它在时空感知任务上表现卓越。</li>
<li>VideoChat-R1相较于Qwen2.5-VL-7B，在时空感知任务如时间定位和对象跟踪上的性能有显著提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-46621a1e4484a9f27274987e9b6a9ba6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89b3adc13557b37016357b42472fc020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3358aff3ad40e151d7647349d49081f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c69c12accc911d1f6cd0de6c91a6c1cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e754349e60c319da42a2b28e9441666.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29a1999ed19e6b996b60e278dcd16437.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SPIN-Bench-How-Well-Do-LLMs-Plan-Strategically-and-Reason-Socially"><a href="#SPIN-Bench-How-Well-Do-LLMs-Plan-Strategically-and-Reason-Socially" class="headerlink" title="SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?"></a>SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?</h2><p><strong>Authors:Jianzhu Yao, Kevin Wang, Ryan Hsieh, Haisu Zhou, Tianqing Zou, Zerui Cheng, Zhangyang Wang, Pramod Viswanath</strong></p>
<p>Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and human–AI teaming. Project Website: <a target="_blank" rel="noopener" href="https://spinbench.github.io/">https://spinbench.github.io/</a> </p>
<blockquote>
<p>在社会互动中的推理和战略行为是智能的标志。这种推理形式远比静态环境中的孤立规划或推理任务（例如数学问题解决）更为复杂。在本文中，我们提出了“战略规划、互动与谈判（SPIN-Bench）”，这是一种新的多领域评估，旨在衡量战略规划和社会推理的智能水平。虽然许多现有的基准测试主要集中在狭隘的规划或单代理推理上，但SPIN-Bench结合了经典PDDL任务、竞技棋类游戏、合作卡牌游戏和多代理谈判场景在一个统一框架中。该框架既包括一个基准测试，也包括一个模拟和评估各种社交设置的场所，以测试人工智能代理的推理和战略行为。我们通过系统地改变动作空间、状态复杂性和交互代理的数量来制定SPIN-Bench基准测试，以模拟各种社交环境，在这些环境中，成功不仅取决于方法和逐步的决策制定，还取决于对其他（对抗性或合作性）参与者的概念推断。我们的实验表明，虽然当代大型语言模型在处理基本事实检索和短期规划方面表现良好，但在需要在大状态空间中进行深度多跳推理和不确定环境下的社会适应性协调的任务中，它们会遇到显著的性能瓶颈。我们期望SPIN-Bench能成为未来研究稳健的多代理规划、社会推理和人机协作领域的催化剂。项目网站：<a target="_blank" rel="noopener" href="https://spinbench.github.io/">https://spinbench.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12349v3">PDF</a> 42 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>该论文介绍了社会互动中的推理与策略行为是智能的重要表现。文章提出了一个新的多领域评估方法——战略计划互动谈判基准测试（SPIN-Bench），旨在测量战略规划和社交推理的智能水平。不同于现有的侧重于单一计划或单一智能体推理的基准测试，SPIN-Bench结合了经典的PDDL任务、竞技棋盘游戏、合作卡牌游戏和多智能体谈判场景在一个统一框架中。该框架包括一个模拟多种社交设置的基准测试环境，以测试人工智能智能体的推理和策略行为。研究结果表明，当前的大型语言模型虽然能较好地处理基本事实检索和短期规划任务，但在需要深度多跳推理和不确定环境下的社会适应性协调任务时，性能瓶颈显著。SPIN-Bench有望成为未来研究稳健多智能体规划、社交推理和人机协作的重要推动力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章中提到，社会互动中的推理和策略行为是智能的一个重要体现，而这需要比静态环境中的孤立规划或推理任务更为高级的智力水平。</li>
<li>介绍了一种新的多领域评估方法——战略规划互动谈判基准测试（SPIN-Bench），用以测量智能体的战略规划和社交推理能力。</li>
<li>SPIN-Bench框架结合了多种任务类型，包括经典的PDDL任务、竞技棋盘游戏、合作卡牌游戏和多智能体谈判场景，以模拟真实的社交环境。</li>
<li>当前的大型语言模型在处理需要深度多跳推理和不确定环境下的社会适应性协调任务时存在性能瓶颈。</li>
<li>SPIN-Bench为未来的研究提供了一个平台，包括研究稳健的多智能体规划、社交推理和人机协作等关键领域。</li>
<li>通过系统地改变动作空间、状态复杂性和交互智能体的数量，SPIN-Bench能够模拟各种社交设置，以测试AI智能体的推理和策略行为。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12349">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-09bbd46d8c3b086c52fa633d0828d5a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a25042afb63f292282ad1b051554613a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f776a21069c79f66209d467df66bfa69.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-12/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-12/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e3110fc88f85108cbd07ad02836dce36.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-12  Cat, Rat, Meow On the Alignment of Language Model and Human   Term-Similarity Judgments
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-11/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-23ad0ad70d5d46832db0d11e5dfb6ea0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-04-11  F5R-TTS Improving Flow-Matching based Text-to-Speech with Group   Relative Policy Optimization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19710k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
