<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  GLUS Global-Local Reasoning Unified into A Single Large Language Model   for Video Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-64299d139655edfe7483ba354d68ccb4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    63 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-12-æ›´æ–°"><a href="#2025-04-12-æ›´æ–°" class="headerlink" title="2025-04-12 æ›´æ–°"></a>2025-04-12 æ›´æ–°</h1><h2 id="GLUS-Global-Local-Reasoning-Unified-into-A-Single-Large-Language-Model-for-Video-Segmentation"><a href="#GLUS-Global-Local-Reasoning-Unified-into-A-Single-Large-Language-Model-for-Video-Segmentation" class="headerlink" title="GLUS: Global-Local Reasoning Unified into A Single Large Language Model   for Video Segmentation"></a>GLUS: Global-Local Reasoning Unified into A Single Large Language Model   for Video Segmentation</h2><p><strong>Authors:Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang</strong></p>
<p>This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between â€œRefâ€ and â€œVOSâ€: they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that global and local consistency can be unified into a single video segmentation MLLM: a set of sparse â€œcontext framesâ€ provides global information, while a stream of continuous â€œquery framesâ€ conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our project page is at <a target="_blank" rel="noopener" href="https://glus-video.github.io/">https://glus-video.github.io/</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡ŒæŒ‡ä»£è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆRefVOSï¼‰çš„æ–°å‹æ¡†æ¶ã€‚ä¹‹å‰çš„MLLMæ–¹æ³•é€šå¸¸é¢ä¸´â€œRefâ€å’Œâ€œVOSâ€ä¹‹é—´çš„å›°å¢ƒï¼šå®ƒä»¬è¦ä¹ˆä¸“æ³¨äºç†è§£ä¸€äº›å…³é”®å¸§ï¼ˆå…¨å±€æ¨ç†ï¼‰ï¼Œè¦ä¹ˆè·Ÿè¸ªè¿ç»­å¸§ä¸Šçš„å¯¹è±¡ï¼ˆå±€éƒ¨æ¨ç†ï¼‰ï¼Œå¹¶ä¾èµ–å¤–éƒ¨VOSæˆ–å¸§é€‰æ‹©å™¨æ¥ç¼“è§£å¦ä¸€ç«¯çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„GLUSæ¡†æ¶è¡¨æ˜ï¼Œå…¨å±€å’Œå±€éƒ¨ä¸€è‡´æ€§å¯ä»¥ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€çš„è§†é¢‘åˆ†å‰²MLLMä¸­ï¼šä¸€ç»„ç¨€ç–çš„â€œä¸Šä¸‹æ–‡å¸§â€æä¾›å…¨å±€ä¿¡æ¯ï¼Œè€Œä¸€ç³»åˆ—è¿ç»­çš„â€œæŸ¥è¯¢å¸§â€è¿›è¡Œå±€éƒ¨å¯¹è±¡è·Ÿè¸ªã€‚è¿™å¾—åˆ°äº†ä¸é¢„è®­ç»ƒVOSå†…å­˜åº“è”åˆè®­ç»ƒMLLMçš„æ”¯æŒï¼Œä»¥åŒæ—¶æ¶ˆåŒ–çŸ­ç¨‹å’Œé•¿ç¨‹çš„æ—¶é—´ä¿¡æ¯ã€‚ä¸ºäº†æé«˜MLLMæœ‰é™ä¸Šä¸‹æ–‡çª—å£å†…çš„ä¿¡æ¯æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹è±¡å¯¹æ¯”å­¦ä¹ æ¥åŒºåˆ†éš¾ä»¥åŒºåˆ†çš„å‡é˜³æ€§å¯¹è±¡ï¼Œä»¥åŠè‡ªæˆ‘å®Œå–„æ¡†æ¶æ¥è¯†åˆ«å…³é”®å¸§å¹¶è¿›è¡Œä¼ æ’­ã€‚é€šè¿‡æ•´åˆè¿™äº›è§è§£ï¼Œæˆ‘ä»¬çš„GLUSæä¾›äº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„åŸºçº¿ï¼Œåœ¨MeViSå’ŒRef-Youtube-VOSåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†MLLMçš„æœ€æ–°æ°´å¹³ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://glus-video.github.io/%E3%80%82">https://glus-video.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07962v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>åœ¨è§£å†³è§†é¢‘å¯¹è±¡åˆ†å‰²çš„é—®é¢˜æ—¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶GLUSã€‚è¯¥æ¡†æ¶èåˆäº†å…¨å±€å’Œå±€éƒ¨ä¸€è‡´æ€§ï¼Œé€šè¿‡ç¨€ç–çš„â€œä¸Šä¸‹æ–‡å¸§â€æä¾›å…¨å±€ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è¿ç»­çš„â€œæŸ¥è¯¢å¸§â€è¿›è¡Œå±€éƒ¨å¯¹è±¡è·Ÿè¸ªã€‚æ­¤å¤–ï¼Œé€šè¿‡è”åˆè®­ç»ƒMLLMå’Œé¢„è®­ç»ƒçš„VOSè®°å¿†åº“ï¼ŒåŒæ—¶æ¶ˆåŒ–çŸ­æœŸå’Œé•¿æœŸçš„æ—¶é—´ä¿¡æ¯ã€‚ä¸ºäº†æé«˜MLLMæœ‰é™ä¸Šä¸‹æ–‡çª—å£å†…çš„ä¿¡æ¯æ•ˆç‡ï¼Œå¼•å…¥äº†å¯¹è±¡å¯¹æ¯”å­¦ä¹ å’Œè‡ªæˆ‘ä¼˜åŒ–æ¡†æ¶ã€‚GLUSåœ¨MeViSå’ŒRef-Youtube-VOSåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚é¡¹ç›®ç½‘é¡µæ˜¯<a target="_blank" rel="noopener" href="https://glus-video.github.io/">ç½‘ç«™é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°å‹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶GLUSç”¨äºè§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆRefVOSï¼‰ã€‚</li>
<li>èåˆå…¨å±€å’Œå±€éƒ¨ä¸€è‡´æ€§ï¼Œè§£å†³ä»¥å¾€MLLMåœ¨Refå’ŒVOSä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç¨€ç–çš„ä¸Šä¸‹æ–‡å¸§å’Œè¿ç»­çš„æŸ¥è¯¢å¸§è¿›è¡Œå…¨å±€å’Œå±€éƒ¨ä¿¡æ¯çš„å¤„ç†ã€‚</li>
<li>è”åˆè®­ç»ƒMLLMå’Œé¢„è®­ç»ƒçš„VOSè®°å¿†åº“ï¼ŒåŒæ—¶å¤„ç†çŸ­æœŸå’Œé•¿æœŸæ—¶é—´ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨å¯¹è±¡å¯¹æ¯”å­¦ä¹ æé«˜ä¿¡æ¯æ•ˆç‡ã€‚</li>
<li>å¼•å…¥è‡ªæˆ‘ä¼˜åŒ–æ¡†æ¶æ¥è¯†åˆ«å…³é”®å¸§å¹¶è¿›è¡Œä¼ æ’­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e4162cab4e2325903db118e13bfe942.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3c221397e99f939a899cefe5ccc2e5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d85594e10665a496e06c35bb2c6da6c9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SoTA-with-Less-MCTS-Guided-Sample-Selection-for-Data-Efficient-Visual-Reasoning-Self-Improvement"><a href="#SoTA-with-Less-MCTS-Guided-Sample-Selection-for-Data-Efficient-Visual-Reasoning-Self-Improvement" class="headerlink" title="SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual   Reasoning Self-Improvement"></a>SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual   Reasoning Self-Improvement</h2><p><strong>Authors:Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang</strong></p>
<p>In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at <a target="_blank" rel="noopener" href="https://github.com/si0wang/ThinkLite-VL">https://github.com/si0wang/ThinkLite-VL</a>. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨æ˜¾è‘—æ›´å°‘çš„è®­ç»ƒæ ·æœ¬å¢å¼ºè§†è§‰æ¨ç†èƒ½åŠ›ï¼Œè¿™å®Œå…¨ä¾èµ–äºè‡ªæˆ‘æå‡ï¼Œæ— éœ€çŸ¥è¯†è’¸é¦ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æœŸé—´è®­ç»ƒæ•°æ®çš„éš¾åº¦è‡³å…³é‡è¦ã€‚é€‚å½“çš„æŒ‘æˆ˜æ€§æ ·æœ¬å³ä½¿æ•°æ®é›†å¾ˆå°ï¼Œä¹Ÿèƒ½å¤§å¹…æå‡æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡è¿™æ˜¯ç›´è§‚çš„ï¼Œä½†ä¸»è¦æŒ‘æˆ˜ä»ç„¶åœ¨äºå‡†ç¡®é‡åŒ–æ ·æœ¬çš„éš¾åº¦ï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ•°æ®è¿‡æ»¤ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡æ–°åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„æ–°æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä»æˆ‘ä»¬å¼€å§‹ç²¾é€‰çš„7ä¸‡ä¸ªå¼€æºè®­ç»ƒæ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºMCTSçš„é€‰æ‹©æ–¹æ³•ï¼Œæ ¹æ®VLMsè§£å†³é—®é¢˜æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°æ¥é‡åŒ–æ ·æœ¬éš¾åº¦ã€‚MCTSä¸­çš„è¿™ç§é€æ­¥æ¨ç†æ˜ç¡®äº†æ¨¡å‹éœ€è¦æ€è€ƒçš„æ—¶é—´ï¼Œå¹¶æ›´å¥½åœ°è¯†åˆ«äº†çœŸæ­£å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ã€‚æˆ‘ä»¬è¿‡æ»¤å¹¶ä¿ç•™äº†1.1ä¸‡ä¸ªæ ·æœ¬ï¼Œåœ¨Qwen2.5-VL-7B-Instructä¸Šè¿›è¡ŒRFTï¼Œä»è€Œå¾—åˆ°æˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹ThinkLite-VLã€‚åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒThinkLite-VLä½¿ç”¨ä»…1.1ä¸‡ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæ— éœ€çŸ¥è¯†è’¸é¦ï¼Œå³å¯å°†Qwen2.5-VL-7B-Instructçš„å¹³å‡æ€§èƒ½æé«˜7%ã€‚è¿™æ˜¾è‘—ä¼˜äºæ‰€æœ‰ç°æœ‰çš„7Bçº§æ¨ç†VLMsä»¥åŠä½¿ç”¨ç»å…¸é€‰æ‹©æ–¹æ³•ï¼ˆå¦‚åŸºäºå‡†ç¡®åº¦çš„è¿‡æ»¤ï¼‰çš„ç›¸å½“åŸºå‡†çº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨MathVistaä¸Šï¼ŒThinkLite-VL-7Bè¾¾åˆ°äº†75.1çš„SOTAå‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†Qwen2.5-VL-72Bã€GPT-4oå’ŒO1ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/si0wang/ThinkLite-VL%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/si0wang/ThinkLite-VLä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07934v1">PDF</a> 21 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨è‡ªæˆ‘æå‡å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œåœ¨å°‘é‡è®­ç»ƒæ ·æœ¬ä¸‹æå‡è§†è§‰æ¨ç†èƒ½åŠ›ã€‚è®ºæ–‡çš„å…³é”®åœ¨äºè®¤è¯†åˆ°å¼ºåŒ–ç²¾ç»†è®­ç»ƒæ—¶æ ·æœ¬çš„éš¾åº¦è‡³å…³é‡è¦ã€‚é€šè¿‡æå‡ºä¸€ç§åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„ç­›é€‰æ–¹æ³•ï¼Œé‡åŒ–æ ·æœ¬éš¾åº¦å¹¶ç­›é€‰å‡ºçœŸæ­£å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œæœ€ç»ˆå®ç°äº†æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ä»…11kè®­ç»ƒæ ·æœ¬çš„ThinkLite-VLæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æ€§èƒ½æå‡äº†7%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å…¶ä»–ç±»ä¼¼è§„æ¨¡çš„æ¨ç†è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªæˆ‘æå‡å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œèƒ½åœ¨å°‘é‡è®­ç»ƒæ ·æœ¬ä¸‹å¢å¼ºè§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒåœ¨å¼ºåŒ–ç²¾ç»†è®­ç»ƒæ—¶æ ·æœ¬çš„éš¾åº¦è‡³å…³é‡è¦ï¼Œé€šè¿‡ç­›é€‰çœŸæ­£å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒå¯ä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è®ºæ–‡ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç­›é€‰è®­ç»ƒæ ·æœ¬ï¼ŒåŸºäºæ¨¡å‹è§£å†³é—®é¢˜æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°é‡åŒ–æ ·æœ¬éš¾åº¦ã€‚</li>
<li>ThinkLite-VLæ¨¡å‹ä½¿ç”¨ä»…11kè®­ç»ƒæ ·æœ¬ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æ€§èƒ½æå‡äº†7%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å…¶ä»–ç±»ä¼¼è§„æ¨¡çš„æ¨ç†è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ThinkLite-VLæ¨¡å‹åœ¨MathVistaæµ‹è¯•ä¸Šè¾¾åˆ°äº†å½“æ—¶æœ€å…ˆè¿›çš„å‡†ç¡®ç‡75.1%ï¼Œè¶…è¶Šäº†å…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹å¦‚Qwen2.5-VL-72Bå’ŒGPT-4oç­‰ã€‚</li>
<li>è®ºæ–‡æä¾›çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯ä»¥åœ¨å…¬å¼€ä»£ç åº“ä¸­æ‰¾åˆ°ï¼Œæ–¹ä¾¿åç»­ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-64299d139655edfe7483ba354d68ccb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6130e694a0065b05a61ed03bd6f97802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4db81e81336e1ed4f49924a5ada80d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1a8b6d5576264e336efff5fbdab597d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cbe035ba416646140e86b8b113fc9ed.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SpecReason-Fast-and-Accurate-Inference-Time-Compute-via-Speculative-Reasoning"><a href="#SpecReason-Fast-and-Accurate-Inference-Time-Compute-via-Speculative-Reasoning" class="headerlink" title="SpecReason: Fast and Accurate Inference-Time Compute via Speculative   Reasoning"></a>SpecReason: Fast and Accurate Inference-Time Compute via Speculative   Reasoning</h2><p><strong>Authors:Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, Ravi Netravali</strong></p>
<p>Recent advances in inference-time compute have significantly improved performance on complex tasks by generating long chains of thought (CoTs) using Large Reasoning Models (LRMs). However, this improved accuracy comes at the cost of high inference latency due to the length of generated reasoning sequences and the autoregressive nature of decoding. Our key insight in tackling these overheads is that LRM inference, and the reasoning that it embeds, is highly tolerant of approximations: complex tasks are typically broken down into simpler steps, each of which brings utility based on the semantic insight it provides for downstream steps rather than the exact tokens it generates. Accordingly, we introduce SpecReason, a system that automatically accelerates LRM inference by using a lightweight model to (speculatively) carry out simpler intermediate reasoning steps and reserving the costly base model only to assess (and potentially correct) the speculated outputs. Importantly, SpecReasonâ€™s focus on exploiting the semantic flexibility of thinking tokens in preserving final-answer accuracy is complementary to prior speculation techniques, most notably speculative decoding, which demands token-level equivalence at each step. Across a variety of reasoning benchmarks, SpecReason achieves 1.5-2.5$\times$ speedup over vanilla LRM inference while improving accuracy by 1.0-9.9%. Compared to speculative decoding without SpecReason, their combination yields an additional 19.4-44.2% latency reduction. We open-source SpecReason at <a target="_blank" rel="noopener" href="https://github.com/ruipeterpan/specreason">https://github.com/ruipeterpan/specreason</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ¨ç†æ—¶é—´è®¡ç®—æ–¹é¢çš„è¿›å±•é€šè¿‡åˆ©ç”¨å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ç”Ÿæˆé•¿çš„æ€ç»´é“¾ï¼ˆCoTsï¼‰å·²ç»åœ¨å¤æ‚ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æé«˜çš„å‡†ç¡®æ€§æ˜¯ä»¥é«˜æ¨ç†å»¶è¿Ÿä¸ºä»£ä»·çš„ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç”Ÿæˆçš„æ¨ç†åºåˆ—çš„é•¿åº¦å’Œè§£ç çš„è‡ªå›å½’æ€§è´¨ã€‚æˆ‘ä»¬è§£å†³è¿™äº›å¼€é”€çš„å…³é”®è§è§£æ˜¯ï¼ŒLRMæ¨ç†åŠå…¶æ‰€åµŒå…¥çš„æ¨ç†å¯¹è¿‘ä¼¼å€¼æœ‰ç€å¾ˆé«˜çš„å®¹å¿åº¦ï¼šå¤æ‚ä»»åŠ¡é€šå¸¸è¢«åˆ†è§£ä¸ºæ›´ç®€å•çš„æ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤çš„å®ç”¨æ€§éƒ½æ˜¯åŸºäºå®ƒä¸ºåç»­æ­¥éª¤æä¾›çš„è¯­ä¹‰è§è§£ï¼Œè€Œä¸æ˜¯å®ƒç”Ÿæˆçš„ç²¾ç¡®æ ‡è®°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpecReasonç³»ç»Ÿï¼Œå®ƒé€šè¿‡ä½¿ç”¨è½»é‡çº§æ¨¡å‹æ¥è‡ªåŠ¨åŠ é€ŸLRMæ¨ç†ï¼Œï¼ˆæ¨æµ‹åœ°ï¼‰æ‰§è¡Œæ›´ç®€å•çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»…ä½¿ç”¨æ˜‚è´µçš„åŸºå‡†æ¨¡å‹æ¥è¯„ä¼°ï¼ˆå’Œå¯èƒ½çº æ­£ï¼‰æ¨æµ‹çš„è¾“å‡ºã€‚é‡è¦çš„æ˜¯ï¼ŒSpecReasonä¸“æ³¨äºåˆ©ç”¨æ€ç»´æ ‡è®°çš„è¯­ä¹‰çµæ´»æ€§æ¥ä¿æŒæœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œè¿™ä¸å…ˆå‰çš„æ¨æµ‹æŠ€æœ¯äº’è¡¥ï¼Œå°¤å…¶æ˜¯æ¨æµ‹è§£ç ï¼Œå®ƒè¦æ±‚æ¯ä¸€æ­¥çš„æ ‡è®°çº§åˆ«ç­‰æ•ˆã€‚åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSpecReasonå®ç°äº†æ¯”æ™®é€šLRMæ¨ç†å¿«1.5-2.5å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶æé«˜äº†1.0-9.9%çš„å‡†ç¡®ç‡ã€‚ä¸æ²¡æœ‰SpecReasonçš„æ¨æµ‹è§£ç ç›¸æ¯”ï¼Œå®ƒä»¬çš„ç»„åˆå¯¼è‡´é¢å¤–çš„19.4-44.2%å»¶è¿Ÿå‡å°‘ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ruipeterpan/specreason">https://github.com/ruipeterpan/specreason</a>å¼€æºSpecReasonã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07891v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„é•¿é“¾æ€ç»´ï¼ˆCoTsï¼‰æŠ€æœ¯æ˜¾è‘—æé«˜äº†å¤æ‚ä»»åŠ¡çš„æ€§èƒ½ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´äº†æ¨ç†åºåˆ—é•¿åº¦å¢åŠ å’Œè‡ªå›å½’è§£ç å¸¦æ¥çš„æ¨ç†æ—¶é—´å»¶è¿Ÿé—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SpecReasonç³»ç»Ÿï¼Œå®ƒé€šè¿‡é‡‡ç”¨è½»é‡çº§æ¨¡å‹è¿›è¡Œæ¨æµ‹æ€§æ¨ç†æ­¥éª¤æ¥åŠ é€ŸLRMæ¨ç†è¿‡ç¨‹ï¼Œåªåœ¨å¿…è¦æ—¶æ‰è°ƒç”¨æ˜‚è´µçš„åŸºå‡†æ¨¡å‹è¿›è¡Œè¯„ä¼°å’Œæ ¡æ­£ã€‚SpecReasonä¸“æ³¨äºåˆ©ç”¨æ€ç»´ç¬¦å·çš„è¯­ä¹‰çµæ´»æ€§æ¥ä¿æŒæœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œä¸å…ˆå‰çš„æ¨æµ‹æŠ€æœ¯ç›¸æ¯”å…·æœ‰äº’è¡¥æ€§ã€‚å®éªŒç»“æœæŒ‡å‡ºï¼Œç›¸è¾ƒäºåŸå§‹LRMæ¨ç†ï¼ŒSpecReasonæä¾›äº†æ›´é«˜çš„åŠ é€Ÿå’Œå‡†ç¡®åº¦æ”¹å–„ï¼›åŒæ—¶ä¸æ²¡æœ‰SpecReasonçš„æ¨æµ‹è§£ç ç›¸æ¯”ï¼ŒäºŒè€…çš„ç»“åˆå¸¦æ¥äº†é¢å¤–çš„å»¶è¿Ÿé™ä½ã€‚æˆ‘ä»¬å·²åœ¨GitHubä¸Šå¼€æºSpecReasonã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰èƒ½å¤Ÿé€šè¿‡ç”Ÿæˆé•¿é“¾æ€ç»´ï¼ˆCoTsï¼‰æé«˜å¤æ‚ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>LRMæ¨ç†å­˜åœ¨é«˜æ¨ç†æ—¶é—´å»¶è¿Ÿé—®é¢˜ï¼Œä¸»è¦æºäºç”Ÿæˆçš„æ¨ç†åºåˆ—é•¿åº¦å’Œè‡ªå›å½’è§£ç ã€‚</li>
<li>SpecReasonç³»ç»Ÿæ—¨åœ¨è§£å†³è¿™äº›æ—¶é—´å»¶è¿Ÿé—®é¢˜ï¼Œå®ƒé€šè¿‡è½»é‡çº§æ¨¡å‹è¿›è¡Œæ¨æµ‹æ€§æ¨ç†æ­¥éª¤æ¥åŠ é€ŸLRMæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>SpecReasonä¿ç•™äº†æ€ç»´ç¬¦å·çš„è¯­ä¹‰çµæ´»æ€§ï¼Œä»è€Œæé«˜æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚å®ƒæ˜¯å¯¹å…ˆå‰æ¨æµ‹æŠ€æœ¯çš„é‡è¦è¡¥å……ã€‚</li>
<li>ä¸åŸå§‹LRMæ¨ç†ç›¸æ¯”ï¼ŒSpecReasonæä¾›äº†æ›´é«˜çš„åŠ é€Ÿæ•ˆæœå’Œå‡†ç¡®åº¦æ”¹å–„ã€‚ç›¸è¾ƒäºæ²¡æœ‰SpecReasonçš„æ¨æµ‹è§£ç ï¼ŒäºŒè€…çš„ç»“åˆå¸¦æ¥äº†é¢å¤–çš„å»¶è¿Ÿé™ä½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0c011fa5777a5e4e6d96b7854d9b99e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68be6c1e7ba96494067d8b0cf8335b47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c088baf2fc42e0ff24a30d0f242fe992.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e16ac672f980424cd9a2733fa62c8970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-900ed80dfecc668fae1e336eea120b91.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Enhancing-Large-Language-Models-through-Neuro-Symbolic-Integration-and-Ontological-Reasoning"><a href="#Enhancing-Large-Language-Models-through-Neuro-Symbolic-Integration-and-Ontological-Reasoning" class="headerlink" title="Enhancing Large Language Models through Neuro-Symbolic Integration and   Ontological Reasoning"></a>Enhancing Large Language Models through Neuro-Symbolic Integration and   Ontological Reasoning</h2><p><strong>Authors:Ruslan Idelfonso Magana Vsevolodovna, Marco Monti</strong></p>
<p>Large Language Models (LLMs) demonstrate impressive capabilities in natural language processing but suffer from inaccuracies and logical inconsistencies known as hallucinations. This compromises their reliability, especially in domains requiring factual accuracy. We propose a neuro-symbolic approach integrating symbolic ontological reasoning and machine learning methods to enhance the consistency and reliability of LLM outputs. Our workflow utilizes OWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking, and a lightweight machine learning model (logistic regression) for mapping natural language statements into logical forms compatible with the ontology. When inconsistencies between LLM outputs and the ontology are detected, the system generates explanatory feedback to guide the LLM towards a corrected, logically coherent response in an iterative refinement loop. We present a working Python prototype demonstrating this pipeline. Experimental results in a defined domain suggest significant improvements in semantic coherence and factual accuracy of LLM outputs, showcasing the potential of combining LLM fluency with the rigor of formal semantics. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨è¢«ç§°ä¸ºâ€œå¹»è§‰â€çš„ä¸å‡†ç¡®æ€§å’Œé€»è¾‘ä¸ä¸€è‡´æ€§ã€‚è¿™æŸå®³äº†å®ƒä»¬çš„å¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦äº‹å®å‡†ç¡®æ€§çš„é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¥ç»ç¬¦å·æ–¹æ³•ï¼Œå°†ç¬¦å·æœ¬ä½“æ¨ç†å’Œæœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡Œæ•´åˆï¼Œä»¥æé«˜LLMè¾“å‡ºçš„ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œæµç¨‹åˆ©ç”¨OWLæœ¬ä½“ã€ç¬¦å·æ¨ç†å™¨ï¼ˆä¾‹å¦‚HermiTï¼‰è¿›è¡Œä¸€è‡´æ€§æ£€æŸ¥ï¼Œä»¥åŠè½»é‡çº§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆé€»è¾‘å›å½’ï¼‰å°†è‡ªç„¶è¯­è¨€é™ˆè¿°æ˜ å°„åˆ°ä¸æœ¬ä½“å…¼å®¹çš„é€»è¾‘å½¢å¼ã€‚å½“æ£€æµ‹åˆ°LLMè¾“å‡ºä¸æœ¬ä½“ä¹‹é—´çš„ä¸ä¸€è‡´æ—¶ï¼Œç³»ç»Ÿä¼šç”Ÿæˆè§£é‡Šæ€§åé¦ˆï¼Œä»¥æŒ‡å¯¼LLMåœ¨è¿­ä»£ç»†åŒ–å¾ªç¯ä¸­åšå‡ºæ›´æ­£ã€é€»è¾‘è¿è´¯çš„å“åº”ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªå·¥ä½œPythonåŸå‹ï¼Œæ¼”ç¤ºäº†è¿™ä¸ªç®¡é“ã€‚åœ¨ç‰¹å®šé¢†åŸŸçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMè¾“å‡ºçš„è¯­ä¹‰è¿è´¯æ€§å’Œäº‹å®å‡†ç¡®æ€§å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†ç»“åˆLLMæµç•…æ€§å’Œå½¢å¼è¯­ä¹‰ä¸¥è°¨æ€§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07640v1">PDF</a> 11 pages, 1 figure, includes prototype implementation and   experimental evaluation. Submitted for consideration in the arXiv Artificial   Intelligence category (cs.AI)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨è¢«ç§°ä¸ºâ€œå¹»è§‰â€çš„ä¸å‡†ç¡®æ€§å’Œé€»è¾‘ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œè¿™å½±å“äº†å…¶å¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦äº‹å®å‡†ç¡®æ€§çš„é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»ç¬¦å·æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆç¬¦å·æœ¬ä½“æ¨ç†å’Œæœºå™¨å­¦ä¹ æ–¹æ³•æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨OWLæœ¬ä½“ã€ç¬¦å·æ¨ç†å™¨ï¼ˆä¾‹å¦‚HermiTï¼‰è¿›è¡Œä¸€è‡´æ€§æ£€æŸ¥ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆé€»è¾‘å›å½’ï¼‰å°†è‡ªç„¶è¯­è¨€é™ˆè¿°æ˜ å°„åˆ°ä¸æœ¬ä½“å…¼å®¹çš„é€»è¾‘å½¢å¼ã€‚å½“æ£€æµ‹åˆ°å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºä¸æœ¬ä½“ä¹‹é—´çš„ä¸ä¸€è‡´æ€§æ—¶ï¼Œç³»ç»Ÿä¼šç”Ÿæˆè§£é‡Šæ€§åé¦ˆï¼Œä»¥æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿­ä»£ä¼˜åŒ–å¾ªç¯ä¸­åšå‡ºæ›´æ­£ç¡®ã€é€»è¾‘è¿è´¯çš„å›åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç‰¹å®šé¢†åŸŸé‡Œè¯­ä¹‰è¿è´¯æ€§å’Œäº‹å®å‡†ç¡®æ€§å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„æµç•…æ€§å’Œå½¢å¼è¯­ä¹‰å­¦çš„ä¸¥è°¨æ€§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨ä¸å‡†ç¡®æ€§åŠé€»è¾‘ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>ç¥ç»ç¬¦å·æ–¹æ³•ç»“åˆäº†ç¬¦å·æœ¬ä½“æ¨ç†å’Œæœºå™¨å­¦ä¹ ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚</li>
<li>ä½¿ç”¨OWLæœ¬ä½“å’Œç¬¦å·æ¨ç†å™¨ï¼ˆå¦‚HermiTï¼‰è¿›è¡Œä¸€è‡´æ€§æ£€æŸ¥ã€‚</li>
<li>é€šè¿‡è½»é‡çº§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆé€»è¾‘å›å½’ï¼‰å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºé€»è¾‘å½¢å¼ï¼Œä¸æœ¬ä½“å…¼å®¹ã€‚</li>
<li>å½“æ£€æµ‹åˆ°ä¸ä¸€è‡´æ€§æ—¶ï¼Œç³»ç»Ÿç”Ÿæˆè§£é‡Šæ€§åé¦ˆä»¥æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>åœ¨ç‰¹å®šé¢†åŸŸçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†è¯­ä¹‰è¿è´¯æ€§å’Œäº‹å®å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a7c48eb9f24678bf20ab4b0268048a32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbfdfb3d90970cc6ce633b4f6d22a6e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VLM-R1-A-Stable-and-Generalizable-R1-style-Large-Vision-Language-Model"><a href="#VLM-R1-A-Stable-and-Generalizable-R1-style-Large-Vision-Language-Model" class="headerlink" title="VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model"></a>VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model</h2><p><strong>Authors:Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, Tiancheng Zhao</strong></p>
<p>Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMsâ€™ performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the â€œOD aha momentâ€, the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at <a target="_blank" rel="noopener" href="https://github.com/om-ai-lab/VLM-R1">https://github.com/om-ai-lab/VLM-R1</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒDeepSeek R1çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚R1çš„æ ¸å¿ƒåœ¨äºå…¶åŸºäºè§„åˆ™çš„å¥–åŠ±å…¬å¼ï¼Œå®ƒåˆ©ç”¨å…·æœ‰ç¡®å®šæ€§æ ‡å‡†ç­”æ¡ˆçš„ä»»åŠ¡æ¥å¯ç”¨ç²¾ç¡®ä¸”ç¨³å®šçš„å¥–åŠ±è®¡ç®—ã€‚åœ¨è§†è§‰é¢†åŸŸï¼Œæˆ‘ä»¬åŒæ ·è§‚å¯Ÿåˆ°è®¸å¤šè§†è§‰ç†è§£ä»»åŠ¡æœ¬èº«å°±å…·å¤‡å®šä¹‰è‰¯å¥½çš„æ ‡å‡†æ³¨é‡Šã€‚è¿™ä¸€ç‰¹æ€§ä½¿å®ƒä»¬è‡ªç„¶åœ°ä¸åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶å…¼å®¹ã€‚å—æ­¤è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œæ—¨åœ¨å¢å¼ºå®ƒä»¬çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†VLM-R1ä¸“ç”¨æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æé«˜VLMåœ¨ä¸€èˆ¬è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä½¿ç”¨è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºè§†è§‰é¢†åŸŸçš„å¯è¡Œæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹ä¸ä»…åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œè€Œä¸”åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¹Ÿè¶…è¶Šäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œæ­ç¤ºäº†ä¸€ç³»åˆ—å€¼å¾—å…³æ³¨çš„è§è§£ï¼ŒåŒ…æ‹¬å¯¹è±¡æ£€æµ‹ä¸­çš„å¥–åŠ±ç ´è§£ç°è±¡ã€â€ODç¬é—´é¢†æ‚Ÿâ€çš„å‡ºç°ã€è®­ç»ƒæ•°æ®è´¨é‡çš„å½±å“ä»¥åŠä¸åŒæ¨¡å‹å¤§å°ä¸‹å¼ºåŒ–å­¦ä¹ çš„æ‰©å±•è¡Œä¸ºã€‚é€šè¿‡è¿™äº›åˆ†æï¼Œæˆ‘ä»¬æ—¨åœ¨åŠ æ·±å¯¹å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„ç†è§£ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å‘ç°å’Œå¼€æºè´¡çŒ®å°†æ”¯æŒè§†è§‰è¯­è¨€å¼ºåŒ–å­¦ä¹ ç¤¾åŒºçš„æŒç»­è¿›æ­¥ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/om-ai-lab/VLM-R">https://github.com/om-ai-lab/VLM-R</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07615v1">PDF</a> 11 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DeepSeek R1å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒåœ¨äºåŸºäºè§„åˆ™çš„å¥–åŠ±åˆ¶å®šï¼Œåˆ©ç”¨å…·æœ‰ç¡®å®šæ€§ç­”æ¡ˆçš„ä»»åŠ¡æ¥å®ç°ç²¾ç¡®ä¸”ç¨³å®šçš„å¥–åŠ±è®¡ç®—ã€‚å—è§†è§‰é¢†åŸŸä»»åŠ¡è‡ªç„¶å…¼å®¹äºåŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶çš„å¯å‘ï¼Œç ”ç©¶è€…ä»¬å°†R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå¼€å‘äº†VLM-R1æ¡†æ¶æ¥æé«˜VLMåœ¨é€šç”¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºRLçš„æ¨¡å‹ä¸ä»…åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°æœ‰ç«äº‰åŠ›ï¼Œè€Œä¸”åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¹Ÿè¶…è¶Šäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜é€šè¿‡å…¨é¢çš„æ¶ˆèç ”ç©¶è·å¾—äº†ä¸€ç³»åˆ—å€¼å¾—æ³¨æ„çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>DeepSeek R1çš„æ ¸å¿ƒåœ¨äºå…¶åŸºäºè§„åˆ™çš„å¥–åŠ±åˆ¶å®šï¼Œåˆ©ç”¨å…·æœ‰ç¡®å®šæ€§ç­”æ¡ˆçš„ä»»åŠ¡ç¨³å®šè®¡ç®—å¥–åŠ±ã€‚</li>
<li>è§†è§‰ç†è§£ä»»åŠ¡å¤©ç„¶åœ°ä¸åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶å…¼å®¹ã€‚</li>
<li>VLM-R1æ¡†æ¶è¢«å¼€å‘å‡ºæ¥ï¼Œæ—¨åœ¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨é€šç”¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>åŸºäºRLçš„æ¨¡å‹åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä¸”æ³›åŒ–èƒ½åŠ›è¶…è¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚</li>
<li>æ¶ˆèç ”ç©¶æ­ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨ç‰©ä½“æ£€æµ‹ä¸­çš„å¥–åŠ±é»‘å®¢ç°è±¡ã€ç‰©ä½“æ£€æµ‹çš„â€œå•Šå“ˆæ—¶åˆ»â€çš„å‡ºç°ã€è®­ç»ƒæ•°æ®è´¨é‡çš„å½±å“ä»¥åŠä¸åŒæ¨¡å‹å¤§å°ä¸‹å¼ºåŒ–å­¦ä¹ çš„æ‰©å±•è¡Œä¸ºã€‚</li>
<li>è¿™äº›ç ”ç©¶æ·±åŒ–äº†å¯¹å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„ç†è§£ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æŒç»­è¿›æ­¥æä¾›äº†æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8f30623d3c2e16c44bf301e96b9dfd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d20c2434fd742c70e7d20e928953e89c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8105e3e2538999b6cdd524e012dd576b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b17ce1f73a418665b50ebc40d7e1ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6ef4b9706c82a9d9bcc440230d92b48.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TokenFocus-VQA-Enhancing-Text-to-Image-Alignment-with-Position-Aware-Focus-and-Multi-Perspective-Aggregations-on-LVLMs"><a href="#TokenFocus-VQA-Enhancing-Text-to-Image-Alignment-with-Position-Aware-Focus-and-Multi-Perspective-Aggregations-on-LVLMs" class="headerlink" title="TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware   Focus and Multi-Perspective Aggregations on LVLMs"></a>TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware   Focus and Multi-Perspective Aggregations on LVLMs</h2><p><strong>Authors:Zijian Zhang, Xuhui Zheng, Xuecheng Wu, Chong Peng, Xuezhi Cao</strong></p>
<p>While text-to-image (T2I) generation models have achieved remarkable progress in recent years, existing evaluation methodologies for vision-language alignment still struggle with the fine-grained semantic matching. Current approaches based on global similarity metrics often overlook critical token-level correspondences between textual descriptions and visual content. To this end, we present TokenFocus-VQA, a novel evaluation framework that leverages Large Vision-Language Models (LVLMs) through visual question answering (VQA) paradigm with position-specific probability optimization. Our key innovation lies in designing a token-aware loss function that selectively focuses on probability distributions at pre-defined vocabulary positions corresponding to crucial semantic elements, enabling precise measurement of fine-grained semantical alignment. The proposed framework further integrates ensemble learning techniques to aggregate multi-perspective assessments from diverse LVLMs architectures, thereby achieving further performance enhancement. Evaluated on the NTIRE 2025 T2I Quality Assessment Challenge Track 1, our TokenFocus-VQA ranks 2nd place (0.8445, only 0.0001 lower than the 1st method) on public evaluation and 2nd place (0.8426) on the official private test set, demonstrating superiority in capturing nuanced text-image correspondences compared to conventional evaluation methods. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„è§†è§‰è¯­è¨€å¯¹é½è¯„ä¼°æ–¹æ³•ä»ç„¶é¢ä¸´ç€ç²¾ç»†è¯­ä¹‰åŒ¹é…æ–¹é¢çš„æŒ‘æˆ˜ã€‚åŸºäºå…¨å±€ç›¸ä¼¼åº¦æŒ‡æ ‡çš„å½“å‰æ–¹æ³•å¾€å¾€å¿½è§†äº†æ–‡æœ¬æè¿°å’Œè§†è§‰å†…å®¹ä¹‹é—´å…³é”®çš„è¯çº§å¯¹åº”å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TokenFocus-VQAï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ï¼Œé€šè¿‡ä½ç½®ç‰¹å®šæ¦‚ç‡ä¼˜åŒ–çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰èŒƒå¼ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åœ¨äºè®¾è®¡äº†ä¸€ä¸ªè¯æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°æœ‰é€‰æ‹©åœ°å…³æ³¨å¯¹åº”äºå…³é”®è¯­ä¹‰å…ƒç´ çš„äº‹å…ˆå®šä¹‰çš„è¯æ±‡ä½ç½®çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œå®ç°ç²¾ç»†è¯­ä¹‰å¯¹é½çš„ç²¾ç¡®æµ‹é‡ã€‚æ‰€æå‡ºçš„æ¡†æ¶è¿˜ç»“åˆäº†é›†æˆå­¦ä¹ æŠ€æœ¯ï¼Œä»å¤šç§LVLMsæ¶æ„ä¸­æ±‡é›†å¤šè§’åº¦è¯„ä¼°ï¼Œä»è€Œå®ç°äº†è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚åœ¨NTIRE 2025 T2Iè´¨é‡è¯„ä¼°æŒ‘æˆ˜èµ›Track 1ä¸Šï¼Œæˆ‘ä»¬çš„TokenFocus-VQAåœ¨å…¬å…±è¯„ä¼°ä¸­ä½åˆ—ç¬¬äºŒï¼ˆ0.8445ï¼Œä»…æ¯”ç¬¬ä¸€åæ–¹æ³•ä½0.0001ï¼‰ï¼Œåœ¨å®˜æ–¹ç§æœ‰æµ‹è¯•é›†ä¸Šä¹Ÿä½åˆ—ç¬¬äºŒï¼ˆ0.8426ï¼‰ï¼Œè¿™è¯æ˜äº†å…¶åœ¨æ•æ‰å¾®å¦™çš„æ–‡æœ¬å›¾åƒå¯¹åº”å…³ç³»æ–¹é¢ä¼˜äºä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07556v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTokenFocus-VQAçš„æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€šè¿‡è§†è§‰é—®ç­”ï¼ˆVQAï¼‰èŒƒå¼è¿›è¡Œä½ç½®ç‰¹å®šæ¦‚ç‡ä¼˜åŒ–ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºè®¾è®¡äº†ä¸€ç§æ ‡è®°æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°èƒ½å¤Ÿé€‰æ‹©æ€§åœ°å…³æ³¨å…³é”®è¯­ä¹‰å…ƒç´ å¯¹åº”çš„é¢„å®šä¹‰è¯æ±‡ä½ç½®çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œå®ç°å¯¹ç²¾ç»†ç²’åº¦è¯­ä¹‰å¯¹é½çš„ç²¾ç¡®æµ‹é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜ç»“åˆäº†é›†æˆå­¦ä¹ æŠ€æœ¯ï¼Œä»ä¸åŒæ¶æ„çš„LVLMsä¸­èšåˆå¤šè§’åº¦è¯„ä¼°ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚åœ¨NTIRE 2025 T2Iè´¨é‡è¯„ä¼°æŒ‘æˆ˜èµ›è½¨é“1ä¸Šï¼ŒTokenFocus-VQAå–å¾—äº†ç¬¬äºŒåçš„æˆç»©ï¼Œè¡¨æ˜å…¶åœ¨æ•æ‰å¾®å¦™çš„æ–‡æœ¬å›¾åƒå¯¹åº”æ–¹é¢ç›¸æ¯”ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TokenFocus-VQAæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€å¯¹é½è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆäº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä½ç½®ç‰¹å®šæ¦‚ç‡ä¼˜åŒ–ï¼Œå®ç°äº†å¯¹æ–‡æœ¬æè¿°ä¸è§†è§‰å†…å®¹ä¹‹é—´å…³é”®ä»¤ç‰Œçº§åˆ«å¯¹åº”å…³ç³»çš„å…³æ³¨ã€‚</li>
<li>æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºè®¾è®¡äº†ä¸€ç§æ ‡è®°æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œç”¨äºç²¾ç¡®æµ‹é‡ç²¾ç»†ç²’åº¦è¯­ä¹‰å¯¹é½ã€‚</li>
<li>TokenFocus-VQAé€šè¿‡é›†æˆå­¦ä¹ æŠ€æœ¯æ•´åˆäº†å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ç»“æœï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨NTIRE 2025 T2Iè´¨é‡è¯„ä¼°ç«èµ›ä¸­å–å¾—äº†ç¬¬äºŒåçš„æˆç»©ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>ç›¸æ¯”ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ï¼ŒTokenFocus-VQAåœ¨æ•æ‰æ–‡æœ¬å›¾åƒç»†å¾®å¯¹åº”å…³ç³»æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6a3cdf247833d2bba933883fa8f98c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82529202e319056130d5a45735681b4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5939323a7d8aa60503aa16f59d33d802.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0185ee95443a260596e0186f24756398.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fcc69bca19ad2b3c38f10b3f5100c02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3077ef0770727d635820590cfd19b3d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Supervised-Optimism-Correction-Be-Confident-When-LLMs-Are-Sure"><a href="#Supervised-Optimism-Correction-Be-Confident-When-LLMs-Are-Sure" class="headerlink" title="Supervised Optimism Correction: Be Confident When LLMs Are Sure"></a>Supervised Optimism Correction: Be Confident When LLMs Are Sure</h2><p><strong>Authors:Junjie Zhang, Rushuai Yang, Shunyu Liu, Ting-En Lin, Fei Huang, Yi Chen, Yongbin Li, Dacheng Tao</strong></p>
<p>In this work, we establish a novel theoretical connection between supervised fine-tuning and offline reinforcement learning under the token-level Markov decision process, revealing that large language models indeed learn an implicit $Q$-function for inference. Through this theoretical lens, we demonstrate that the widely used beam search method suffers from unacceptable over-optimism, where inference errors are inevitably amplified due to inflated $Q$-value estimations of suboptimal steps. To address this limitation, we propose Supervised Optimism Correction(SOC), which introduces a simple yet effective auxiliary loss for token-level $Q$-value estimations during supervised fine-tuning. Specifically, the auxiliary loss employs implicit value regularization to boost model confidence in expert-demonstrated responses, thereby suppressing over-optimism toward insufficiently supervised responses. Extensive experiments on mathematical reasoning benchmarks, including GSM8K, MATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search across a series of open-source models. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åœ¨åŸºäºè¯çº§åˆ«çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸‹ï¼Œå»ºç«‹äº†ç›‘ç£å¾®è°ƒä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„æ–°å‹ç†è®ºè”ç³»ï¼Œæ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ç¡®å®ä¸ºæ¨ç†å­¦ä¹ äº†éšå¼çš„Qå‡½æ•°ã€‚é€šè¿‡è¿™ä¸€ç†è®ºè§†è§’ï¼Œæˆ‘ä»¬è¯æ˜äº†å¹¿æ³›ä½¿ç”¨çš„è´ªå¿ƒæœç´¢æ–¹æ³•å­˜åœ¨ä¸å¯æ¥å—çš„è¿‡åº¦ä¹è§‚é—®é¢˜ï¼Œç”±äºæ¬¡ä¼˜æ­¥éª¤çš„Qå€¼ä¼°è®¡è†¨èƒ€ï¼Œæ¨ç†é”™è¯¯ä¸å¯é¿å…åœ°ä¼šè¢«æ”¾å¤§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç›‘ç£ä¹è§‚æ ¡æ­£ï¼ˆSOCï¼‰ï¼Œå®ƒä¸ºè¯çº§åˆ«çš„Qå€¼ä¼°è®¡åœ¨ç›‘ç£å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è¾…åŠ©æŸå¤±ã€‚å…·ä½“æ¥è¯´ï¼Œè¾…åŠ©æŸå¤±é‡‡ç”¨éšå¼ä»·å€¼æ­£åˆ™åŒ–æ¥æå‡æ¨¡å‹å¯¹ä¸“å®¶æ¼”ç¤ºçš„ååº”ä¿¡å¿ƒï¼Œä»è€ŒæŠ‘åˆ¶å¯¹ä¸è¶³ç›‘ç£ååº”çš„è¿‡åº¦ä¹è§‚æ€åº¦ã€‚åœ¨GSM8Kã€MATHå’ŒGAOKAOç­‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒï¼Œå±•ç¤ºäº†æ‰€æå‡ºçš„SOCä¸è´ªå¿ƒæœç´¢åœ¨ä¸€ç³»åˆ—å¼€æºæ¨¡å‹ä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07527v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºtokençº§åˆ«çš„Markovå†³ç­–è¿‡ç¨‹ï¼Œæœ¬æ–‡å»ºç«‹äº†ç›‘ç£å¾®è°ƒä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„æ–°å‹ç†è®ºè”ç³»ï¼Œæ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ç¡®å®ä¸ºæ¨ç†å­¦ä¹ äº†éšå¼Qå‡½æ•°ã€‚æ–‡ç« æŒ‡å‡ºå¹¿æ³›ä½¿ç”¨çš„è´ªå¿ƒæœç´¢æ–¹æ³•å­˜åœ¨ä¸å¯æ¥å—çš„è¿‡åº¦ä¹è§‚é—®é¢˜ï¼Œç”±äºæ¬¡ä¼˜æ­¥éª¤çš„Qå€¼ä¼°è®¡è†¨èƒ€ï¼Œæ¨ç†é”™è¯¯ä¼šä¸å¯é¿å…åœ°æ”¾å¤§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ç›‘ç£ä¹è§‚æ ¡æ­£ï¼ˆSOCï¼‰ï¼Œåœ¨ç›‘ç£å¾®è°ƒæœŸé—´ä¸ºtokençº§åˆ«çš„Qå€¼ä¼°è®¡å¼•å…¥ç®€å•æœ‰æ•ˆçš„è¾…åŠ©æŸå¤±ã€‚é€šè¿‡éšæ€§ä»·å€¼æ­£åˆ™åŒ–æ¥æå‡æ¨¡å‹å¯¹ä¸“å®¶æ¼”ç¤ºç­”æ¡ˆçš„ä¿¡å¿ƒï¼Œä»è€ŒæŠ‘åˆ¶å¯¹ç¼ºä¹ç›‘ç£ç­”æ¡ˆçš„è¿‡åº¦ä¹è§‚æ€åº¦ã€‚åœ¨GSM8Kã€MATHå’ŒGAOKAOç­‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒï¼Œå±•ç¤ºäº†å¸¦æœ‰è´ªå¿ƒæœç´¢çš„SOCæ–¹æ³•çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å»ºç«‹äº†ç›‘ç£å¾®è°ƒä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œæ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹å­¦ä¹ éšå¼Qå‡½æ•°ç”¨äºæ¨ç†ã€‚</li>
<li>è´ªå¿ƒæœç´¢æ–¹æ³•å­˜åœ¨è¿‡åº¦ä¹è§‚é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†é”™è¯¯ã€‚</li>
<li>ä¸ºè§£å†³è´ªå¿ƒæœç´¢æ–¹æ³•çš„å±€é™ï¼Œæå‡ºäº†ç›‘ç£ä¹è§‚æ ¡æ­£ï¼ˆSOCï¼‰ã€‚</li>
<li>SOCé€šè¿‡å¼•å…¥è¾…åŠ©æŸå¤±æ¥æ”¹è¿›Qå€¼ä¼°è®¡ï¼Œé‡‡ç”¨éšæ€§ä»·å€¼æ­£åˆ™åŒ–æ¥æå‡æ¨¡å‹ä¿¡å¿ƒã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSOCæ–¹æ³•åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>SOCæ–¹æ³•é€‚ç”¨äºå¤šç§å¼€æºæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-166676115d7d720742fa86c41f9613de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9b3416e8862142ba9beffd55da77534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-701b1c6cfbb5120b7352df1147faaaf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c4647e09cff145a88a880801b607ffc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Why-We-Feel-Breaking-Boundaries-in-Emotional-Reasoning-with-Multimodal-Large-Language-Models"><a href="#Why-We-Feel-Breaking-Boundaries-in-Emotional-Reasoning-with-Multimodal-Large-Language-Models" class="headerlink" title="Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal   Large Language Models"></a>Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal   Large Language Models</h2><p><strong>Authors:Yuxiang Lin, Jingdong Sun, Zhi-Qi Cheng, Jue Wang, Haomin Liang, Zebang Cheng, Yifei Dong, Jun-Yan He, Xiaojiang Peng, Xian-Sheng Hua</strong></p>
<p>Most existing emotion analysis emphasizes which emotion arises (e.g., happy, sad, angry) but neglects the deeper why. We propose Emotion Interpretation (EI), focusing on causal factors-whether explicit (e.g., observable objects, interpersonal interactions) or implicit (e.g., cultural context, off-screen events)-that drive emotional responses. Unlike traditional emotion recognition, EI tasks require reasoning about triggers instead of mere labeling. To facilitate EI research, we present EIBench, a large-scale benchmark encompassing 1,615 basic EI samples and 50 complex EI samples featuring multifaceted emotions. Each instance demands rationale-based explanations rather than straightforward categorization. We further propose a Coarse-to-Fine Self-Ask (CFSA) annotation pipeline, which guides Vision-Language Models (VLLMs) through iterative question-answer rounds to yield high-quality labels at scale. Extensive evaluations on open-source and proprietary large language models under four experimental settings reveal consistent performance gaps-especially for more intricate scenarios-underscoring EIâ€™s potential to enrich empathetic, context-aware AI applications. Our benchmark and methods are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench">https://github.com/Lum1104/EIBench</a>, offering a foundation for advanced multimodal causal analysis and next-generation affective computing. </p>
<blockquote>
<p>ç°æœ‰å¤§éƒ¨åˆ†æƒ…æ„Ÿåˆ†æä¸»è¦å…³æ³¨å“ªç§æƒ…ç»ªå‡ºç°ï¼ˆä¾‹å¦‚å¿«ä¹ã€æ‚²ä¼¤ã€æ„¤æ€’ï¼‰ï¼Œå´å¿½è§†äº†æ›´æ·±å±‚çš„åŸå› ã€‚æˆ‘ä»¬æå‡ºæƒ…æ„Ÿè§£è¯»ï¼ˆEIï¼‰ï¼Œé‡ç‚¹å…³æ³¨æ¨åŠ¨æƒ…ç»ªååº”çš„å› æœå› ç´ ï¼Œæ— è®ºè¿™äº›å› æœå› ç´ æ˜¯æ˜¾æ€§çš„ï¼ˆå¦‚å¯è§‚äº‹ç‰©ã€äººé™…äº’åŠ¨ï¼‰ï¼Œè¿˜æ˜¯éšæ€§çš„ï¼ˆå¦‚æ–‡åŒ–èƒŒæ™¯ã€å±å¹•å¤–äº‹ä»¶ï¼‰ã€‚ä¸ä¼ ç»Ÿçš„æƒ…æ„Ÿè¯†åˆ«ä¸åŒï¼Œæƒ…æ„Ÿè§£è¯»ä»»åŠ¡éœ€è¦è¿›è¡Œè§¦å‘å› ç´ æ¨ç†ï¼Œè€Œéç®€å•çš„æ ‡ç­¾æ ‡æ³¨ã€‚ä¸ºäº†ä¿ƒè¿›æƒ…æ„Ÿè§£è¯»ç ”ç©¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EIBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1615ä¸ªåŸºæœ¬æƒ…æ„Ÿè§£è¯»æ ·æœ¬å’Œ50ä¸ªå¤æ‚çš„æƒ…æ„Ÿè§£è¯»æ ·æœ¬ï¼Œæ¶‰åŠå¤šé¢æƒ…ç»ªã€‚æ¯ä¸ªå®ä¾‹éƒ½éœ€è¦åŸºäºç†æ€§çš„è§£é‡Šï¼Œè€Œéç®€å•çš„åˆ†ç±»ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æå‡ºäº†ä»ç²—ç³™åˆ°ç²¾ç»†çš„è‡ªæˆ‘æé—®ï¼ˆCFSAï¼‰æ³¨é‡Šç®¡é“ï¼Œè¯¥ç®¡é“é€šè¿‡å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰è¿›è¡Œè¿­ä»£é—®ç­”å›åˆï¼Œä»¥å¤§è§„æ¨¡ç”Ÿæˆé«˜è´¨é‡æ ‡ç­¾ã€‚åœ¨å››ç§å®éªŒè®¾ç½®ä¸‹ï¼Œå¯¹å¼€æºå’Œä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºäº†ä¸€è‡´çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨æ›´å¤æ‚åœºæ™¯ä¸­ï¼Œè¿™å‡¸æ˜¾äº†æƒ…æ„Ÿè§£è¯»åœ¨ä¸°å¯Œå…·æœ‰åŒç†å¿ƒå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„AIåº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œæ–¹æ³•å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%EF%BC%8C%E4%B8%BA%E5%85%88%E8%BF%9B%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9B%A0%E6%9E%9C%E5%88%86%E6%9E%90%E5%92%8C%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%83%85%E6%84%9F%E8%AE%A1%E7%AE%97%E6%8F%90%E4%BE%9B%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/Lum1104/EIBenchä¸Šå…¬å¼€å¯ç”¨ï¼Œä¸ºå…ˆè¿›çš„å¤šæ¨¡æ€å› æœåˆ†æå’Œä¸‹ä¸€ä»£æƒ…æ„Ÿè®¡ç®—æä¾›äº†åŸºç¡€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07521v1">PDF</a> Accepted at CVPR Workshop NEXD 2025. 21 pages, Project:   <a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench">https://github.com/Lum1104/EIBench</a></p>
<p><strong>Summary</strong><br>æƒ…æ„Ÿåˆ†æä¸å†ä»…ä»…å…³æ³¨äº§ç”Ÿçš„æƒ…ç»ªç±»å‹ï¼ˆå¦‚å¿«ä¹ã€æ‚²ä¼¤ã€æ„¤æ€’ç­‰ï¼‰ï¼Œè€Œæ˜¯æ·±å…¥åˆ°æƒ…æ„ŸèƒŒåçš„åŸå› ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æƒ…æ„Ÿè§£è¯»ï¼ˆEIï¼‰ï¼Œé‡ç‚¹ç ”ç©¶é©±åŠ¨æƒ…æ„Ÿååº”çš„å› æœå› ç´ ï¼ŒåŒ…æ‹¬æ˜¾æ€§çš„ï¼ˆå¦‚å¯è§ç‰©ä½“ã€äººé™…äº’åŠ¨ï¼‰å’Œéšæ€§çš„ï¼ˆå¦‚æ–‡åŒ–èƒŒæ™¯ã€å±å¹•å¤–äº‹ä»¶ï¼‰ã€‚æƒ…æ„Ÿè§£è¯»ä»»åŠ¡éœ€è¦è¿›è¡Œè§¦å‘åŸå› åˆ†æè€Œéå•çº¯çš„æ ‡ç­¾å½’ç±»ã€‚ä¸ºæ¨è¿›æƒ…æ„Ÿè§£è¯»ç ”ç©¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŒ…å«å¤§è§„æ¨¡åŸºå‡†æ•°æ®çš„EIBenchï¼Œå…¶ä¸­åŒ…æ‹¬æ¶µç›–å¤šé‡æƒ…æ„Ÿçš„å¤æ‚æ ·æœ¬ã€‚æ¯ä¸ªæ ·æœ¬éƒ½éœ€è¦åŸºäºç†æ€§çš„è§£é‡Šè€Œéç®€å•çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä»ç²—ç³™åˆ°ç²¾ç»†çš„è‡ªæˆ‘æé—®ï¼ˆCFSAï¼‰æ³¨é‡Šç®¡é“ï¼Œé€šè¿‡è¿­ä»£é—®ç­”ç¯èŠ‚æŒ‡å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰ï¼Œå¤§è§„æ¨¡ç”Ÿæˆé«˜è´¨é‡æ ‡ç­¾ã€‚åœ¨å„ç§ä¸åŒæƒ…å¢ƒçš„å®éªŒè®¾ç½®ä¸‹ï¼Œå¯¹äºå¼€æºå’Œè‡ªæœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å…¨é¢è¯„ä¼°æ­ç¤ºäº†æ™®éå­˜åœ¨çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹æ›´å¤æ‚çš„åœºæ™¯ã€‚æˆ‘ä»¬çš„åŸºå‡†æ•°æ®å’Œæ–¹æ³•å¯ä¾›å…¬ä¼—è®¿é—®ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench%EF%BC%89%EF%BC%8C%E4%B8%BA%E5%85%88%E8%BF%9B%E7%9A%84%E6%A8%A1%E6%80%81%E5%9B%A0%E6%9E%9C%E5%88%86%E6%9E%90%E5%92%8C%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%83%85%E6%84%9F%E8%AE%A1%E7%AE%97%E5%A5%A0%E5%AE%9A%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/Lum1104/EIBenchï¼‰ï¼Œä¸ºå…ˆè¿›çš„æ¨¡æ€å› æœåˆ†æå’Œä¸‹ä¸€ä»£æƒ…æ„Ÿè®¡ç®—å¥ å®šäº†åŸºç¡€ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æƒ…æ„Ÿåˆ†æä¸»è¦å…³æ³¨æƒ…ç»ªç±»å‹ï¼Œå¿½ç•¥äº†æƒ…æ„ŸèƒŒåçš„æ·±å±‚åŸå› ã€‚</li>
<li>æå‡ºæƒ…æ„Ÿè§£è¯»ï¼ˆEIï¼‰æ¦‚å¿µï¼Œé‡ç‚¹ç ”ç©¶é©±åŠ¨æƒ…æ„Ÿååº”çš„å› æœå› ç´ ï¼ŒåŒ…æ‹¬æ˜¾æ€§å’Œéšæ€§å› ç´ ã€‚</li>
<li>æƒ…æ„Ÿè§£è¯»ä»»åŠ¡è¦æ±‚å¯¹è§¦å‘åŸå› è¿›è¡Œæ¨ç†ï¼Œè€Œéç®€å•çš„æ ‡ç­¾å½’ç±»ã€‚</li>
<li>æ¨å‡ºå¤§è§„æ¨¡åŸºå‡†æ•°æ®EIBenchï¼ŒåŒ…å«å¤æ‚æ ·æœ¬ï¼Œå¼ºè°ƒåŸºäºç†æ€§çš„è§£é‡Šè€Œéç®€å•åˆ†ç±»ã€‚</li>
<li>æå‡ºä»ç²—ç³™åˆ°ç²¾ç»†çš„è‡ªæˆ‘æé—®ï¼ˆCFSAï¼‰æ³¨é‡Šç®¡é“ï¼ŒæŒ‡å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æ ‡ç­¾ã€‚</li>
<li>å¯¹ä¸åŒæƒ…å¢ƒä¸‹çš„è¯­è¨€æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæ­ç¤ºæ€§èƒ½å·®è·ï¼Œå¼ºè°ƒæƒ…æ„Ÿè§£è¯»åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-75572500d902af3fbd8abe6f45e95a36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc6e33b57fb7e3e51a09f652af82fdb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6231af177a1fce424098921faaefb94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d535e7099d3f06c7d98bd32ee2245864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f5fc7a8aff5237e953bcd53ef359d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-798c403c4105bfea7dc240b46b56618b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MDIT-A-Model-free-Data-Interpolation-Method-for-Diverse-Instruction-Tuning"><a href="#MDIT-A-Model-free-Data-Interpolation-Method-for-Diverse-Instruction-Tuning" class="headerlink" title="MDIT: A Model-free Data Interpolation Method for Diverse Instruction   Tuning"></a>MDIT: A Model-free Data Interpolation Method for Diverse Instruction   Tuning</h2><p><strong>Authors:Yangning Li, Zihua Lan, Lv Qingsong, Yinghui Li, Hai-Tao Zheng</strong></p>
<p>As Large Language Models (LLMs) are increasingly applied across various tasks, instruction tuning has emerged as a critical method for enhancing model performance. However, current data management strategies face substantial challenges in generating diverse and comprehensive data, restricting further improvements in model performance. To address this gap, we propose MDIT, a novel model-free data interpolation method for diverse instruction tuning, which generates varied and high-quality instruction data by performing task interpolation. Moreover, it contains diversity-based clustering strategies to ensure the diversity of the training data. Extensive experiments show that our method achieves superior performance in multiple benchmark tasks. The LLMs finetuned with MDIT show significant improvements in numerous tasks such as general question answering, math reasoning, and code generation. MDIT offers an efficient and automatic data synthetic method, generating diverse instruction data without depending on external resources while expanding the application potential of LLMs in complex environments. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ï¼ŒæŒ‡ä»¤å¾®è°ƒä½œä¸ºä¸€ç§æé«˜æ¨¡å‹æ€§èƒ½çš„å…³é”®æ–¹æ³•åº”è¿è€Œç”Ÿã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®ç®¡ç†ç­–ç•¥åœ¨ç”Ÿæˆå¤šæ ·ä¸”å…¨é¢çš„æ•°æ®æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„è¿›ä¸€æ­¥æé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MDITï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ ·æŒ‡ä»¤å¾®è°ƒçš„æ–°å‹æ— æ¨¡å‹æ•°æ®æ’å€¼æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä»»åŠ¡æ’å€¼ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ã€‚æ­¤å¤–ï¼Œå®ƒåŒ…å«åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ï¼Œä»¥ç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚ä½¿ç”¨MDITå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚MDITæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”è‡ªåŠ¨çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œåœ¨ä¸éœ€è¦ä¾èµ–å¤–éƒ¨èµ„æºçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šæ ·åŒ–çš„æŒ‡ä»¤æ•°æ®ï¼ŒåŒæ—¶æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07288v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šé¡¹ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼ŒæŒ‡ä»¤å¾®è°ƒä½œä¸ºä¸€ç§æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®æ–¹æ³•å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®ç®¡ç†ç­–ç•¥åœ¨ç”Ÿæˆå¤šæ ·ä¸”å…¨é¢çš„æ•°æ®æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚ä¸ºè§£å†³è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MDITï¼Œä¸€ç§ç”¨äºå¤šæ ·æŒ‡ä»¤å¾®è°ƒçš„æ–°å‹æ¨¡å‹å¤–æ•°æ®æ’å€¼æ–¹æ³•ã€‚å®ƒé€šè¿‡ä»»åŠ¡æ’å€¼ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ï¼Œå¹¶åŒ…å«åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ï¼Œä»¥ç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä½¿ç”¨MDITå¾®è°ƒåçš„LLMsåœ¨é€šç”¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚MDITæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”è‡ªåŠ¨çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–å¤–éƒ¨èµ„æºçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šæ ·çš„æŒ‡ä»¤æ•°æ®ï¼Œæ‰©å¤§äº†LLMåœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤šé¡¹ä»»åŠ¡ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼ŒæŒ‡ä»¤å¾®è°ƒæ˜¯æé«˜å…¶æ€§èƒ½çš„å…³é”®æ–¹æ³•ã€‚</li>
<li>å½“å‰æ•°æ®ç®¡ç†ç­–ç•¥åœ¨ç”Ÿæˆå¤šæ ·ä¸”å…¨é¢çš„æ•°æ®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MDITæ˜¯ä¸€ç§æ–°å‹çš„æ¨¡å‹å¤–æ•°æ®æ’å€¼æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®ã€‚</li>
<li>MDITé€šè¿‡ä»»åŠ¡æ’å€¼å®ç°æ•°æ®ç”Ÿæˆï¼Œå¹¶åŒ…å«åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ã€‚</li>
<li>å®éªŒè¡¨æ˜MDITåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä½¿ç”¨MDITå¾®è°ƒåçš„LLMsåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œå¦‚é€šç”¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d9ecc47a7f024b165b8e08f58a72181.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6a6318144dfe97301a0be55520d871.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ca0717a3bb250b92dffe9f245901782.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01ac9b0bd86640f683c58e921e5a12d5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RAISE-Reinforenced-Adaptive-Instruction-Selection-For-Large-Language-Models"><a href="#RAISE-Reinforenced-Adaptive-Instruction-Selection-For-Large-Language-Models" class="headerlink" title="RAISE: Reinforenced Adaptive Instruction Selection For Large Language   Models"></a>RAISE: Reinforenced Adaptive Instruction Selection For Large Language   Models</h2><p><strong>Authors:Lv Qingsong, Yangning Li, Zihua Lan, Zishan Xu, Jiwei Tang, Yinghui Li, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu</strong></p>
<p>In the instruction fine-tuning of large language models (LLMs), it has become a consensus that a few high-quality instructions are superior to a large number of low-quality instructions. At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training. These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks. So we designed a dynamic, task-objective-driven instruction selection framework RAISE(Reinforenced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instruction at each step based on the expected impact of instruction on model performance improvement. Our approach is well interpretable and has strong task-specific optimization capabilities. By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy. Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods. Notably, RAISE achieves superior performance by updating only 1% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œå·²ç»è¾¾æˆä¸€ç§å…±è¯†ï¼Œå³å°‘æ•°é«˜è´¨é‡æŒ‡ä»¤ä¼˜äºå¤§é‡ä½è´¨é‡æŒ‡ä»¤ã€‚ç›®å‰ï¼Œå·²ç»æå‡ºäº†è®¸å¤šæŒ‡ä»¤é€‰æ‹©æ–¹æ³•ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•éƒ½æ˜¯åŸºäºå¯å‘å¼è´¨é‡æŒ‡æ ‡æ¥é€‰æ‹©æŒ‡ä»¤ï¼Œå¹¶ä¸”åªåœ¨è®­ç»ƒå‰è€ƒè™‘æ•°æ®é€‰æ‹©ã€‚è¿™äº›è®¾è®¡å¯¼è‡´æŒ‡ä»¤å¾®è°ƒä¼˜åŒ–ä¸è¶³ï¼Œå›ºå®šçš„å¯å‘å¼æŒ‡æ ‡é€šå¸¸éš¾ä»¥é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€çš„ã€ä»»åŠ¡ç›®æ ‡é©±åŠ¨çš„æŒ‡ä»¤é€‰æ‹©æ¡†æ¶RAISEï¼ˆReinforencedè‡ªé€‚åº”æŒ‡ä»¤SElectionï¼‰ï¼Œå®ƒå°†æ•´ä¸ªæŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹çº³å…¥ä¼˜åŒ–ï¼Œæ ¹æ®æŒ‡ä»¤å¯¹æ¨¡å‹æ€§èƒ½æ”¹è¿›çš„é¢„æœŸå½±å“ï¼Œåœ¨æ¯ä¸€æ­¥é€‰æ‹©æŒ‡ä»¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„ä»»åŠ¡ç‰¹å®šä¼˜åŒ–èƒ½åŠ›ã€‚é€šè¿‡å°†åŠ¨æ€æŒ‡ä»¤é€‰æ‹©å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒæˆ‘ä»¬çš„é€‰æ‹©ç­–ç•¥ã€‚å¤§é‡çš„å®éªŒå’Œç»“æœåˆ†æè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸å…¶ä»–æŒ‡ä»¤é€‰æ‹©æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRAISEä»…åœ¨1%çš„è®­ç»ƒæ­¥éª¤ä¸­è¿›è¡Œæ›´æ–°å°±å®ç°äº†ä¼˜äºå…¨æ•°æ®è®­ç»ƒçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œé«˜è´¨é‡æŒ‡ä»¤ä¼˜äºå¤§é‡ä½è´¨é‡æŒ‡ä»¤ã€‚ä¼ ç»Ÿæ–¹æ³•åŸºäºå¯å‘å¼è´¨é‡æŒ‡æ ‡é€‰æ‹©æŒ‡ä»¤ï¼Œä»…åœ¨è®­ç»ƒå‰è€ƒè™‘æ•°æ®é€‰æ‹©ï¼Œå¯¼è‡´æŒ‡ä»¤å¾®è°ƒä¼˜åŒ–ä¸è¶³ã€‚æœ¬æ–‡æå‡ºåŠ¨æ€ã€ä»»åŠ¡ç›®æ ‡é©±åŠ¨çš„æŒ‡ä»¤é€‰æ‹©æ¡†æ¶RAISEï¼Œå°†æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹çº³å…¥ä¼˜åŒ–ï¼Œæ ¹æ®æŒ‡ä»¤å¯¹æ¨¡å‹æ€§èƒ½æå‡çš„é¢„æœŸå½±å“è¿›è¡ŒåŠ¨æ€é€‰æ‹©ã€‚RAISEæ–¹æ³•å¯è§£é‡Šæ€§å¼ºï¼Œå…·æœ‰å¼ºå¤§çš„ä»»åŠ¡ç‰¹å®šä¼˜åŒ–èƒ½åŠ›ï¼Œå°†åŠ¨æ€æŒ‡ä»¤é€‰æ‹©å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒé€‰æ‹©ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼ŒRAISEæ–¹æ³•ä¸å…¶ä»–æŒ‡ä»¤é€‰æ‹©æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜è¶Šæ€§ï¼Œä»…åœ¨1%çš„è®­ç»ƒæ­¥éª¤ä¸­æ›´æ–°æ•°æ®å³å¯å®ç°å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œé«˜è´¨é‡æŒ‡ä»¤ä¼˜äºå¤§é‡ä½è´¨é‡æŒ‡ä»¤ã€‚</li>
<li>ä¼ ç»ŸæŒ‡ä»¤é€‰æ‹©æ–¹æ³•åŸºäºå¯å‘å¼è´¨é‡æŒ‡æ ‡ï¼Œä»…åœ¨è®­ç»ƒå‰è€ƒè™‘æ•°æ®é€‰æ‹©ï¼Œå­˜åœ¨ä¼˜åŒ–ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>RAISEæ˜¯ä¸€ç§åŠ¨æ€ã€ä»»åŠ¡ç›®æ ‡é©±åŠ¨çš„æŒ‡ä»¤é€‰æ‹©æ¡†æ¶ï¼Œå°†æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹çº³å…¥ä¼˜åŒ–ã€‚</li>
<li>RAISEæ ¹æ®æŒ‡ä»¤å¯¹æ¨¡å‹æ€§èƒ½æå‡çš„é¢„æœŸå½±å“è¿›è¡ŒåŠ¨æ€é€‰æ‹©ã€‚</li>
<li>RAISEæ–¹æ³•å¯è§£é‡Šæ€§å¼ºï¼Œå…·æœ‰å¼ºå¤§çš„ä»»åŠ¡ç‰¹å®šä¼˜åŒ–èƒ½åŠ›ã€‚</li>
<li>RAISEå°†åŠ¨æ€æŒ‡ä»¤é€‰æ‹©å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒé€‰æ‹©ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ec946fae2f29b4a3a17e4ef27e19477c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7fd0052fbdc3e6d1653cb1d9c819d0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e7404c0af9e0d21a2b5687c5f496812.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdc1718f1e76b21d2e6b3960268e5b17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcab05466fc3a5c490de506abc717a19.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Face-LLaVA-Facial-Expression-and-Attribute-Understanding-through-Instruction-Tuning"><a href="#Face-LLaVA-Facial-Expression-and-Attribute-Understanding-through-Instruction-Tuning" class="headerlink" title="Face-LLaVA: Facial Expression and Attribute Understanding through   Instruction Tuning"></a>Face-LLaVA: Facial Expression and Attribute Understanding through   Instruction Tuning</h2><p><strong>Authors:Ashutosh Chaubey, Xulang Guan, Mohammad Soleymani</strong></p>
<p>The human face plays a central role in social communication, necessitating the use of performant computer vision tools for human-centered applications. We propose Face-LLaVA, a multimodal large language model for face-centered, in-context learning, including facial expression and attribute recognition. Additionally, Face-LLaVA is able to generate natural language descriptions that can be used for reasoning. Leveraging existing visual databases, we first developed FaceInstruct-1M, a face-centered database for instruction tuning MLLMs for face processing. We then developed a novel face-specific visual encoder powered by Face-Region Guided Cross-Attention that integrates face geometry with local visual features. We evaluated the proposed method across nine different datasets and five different face processing tasks, including facial expression recognition, action unit detection, facial attribute detection, age estimation and deepfake detection. Face-LLaVA achieves superior results compared to existing open-source MLLMs and competitive performance compared to commercial solutions. Our model output also receives a higher reasoning rating by GPT under a zero-shot setting across all the tasks. Both our dataset and model wil be released at <a target="_blank" rel="noopener" href="https://face-llava.github.io/">https://face-llava.github.io</a> to support future advancements in social AI and foundational vision-language research. </p>
<blockquote>
<p>äººè„¸åœ¨ç¤¾ä¼šäº¤æµä¸­å…·æœ‰æ ¸å¿ƒä½œç”¨ï¼Œè¿™è¦æ±‚ä½¿ç”¨é«˜æ€§èƒ½çš„è®¡ç®—æœºè§†è§‰å·¥å…·è¿›è¡Œä»¥äººä¸ºä¸­å¿ƒçš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†Face-LLaVAï¼Œè¿™æ˜¯ä¸€ç§é¢å‘äººè„¸çš„ä¸­å¿ƒåŒ–ä¸Šä¸‹æ–‡å­¦ä¹ å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬é¢éƒ¨è¡¨æƒ…å’Œå±æ€§è¯†åˆ«ã€‚æ­¤å¤–ï¼ŒFace-LLaVAèƒ½å¤Ÿç”Ÿæˆå¯ç”¨äºæ¨ç†çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚æˆ‘ä»¬åˆ©ç”¨ç°æœ‰çš„è§†è§‰æ•°æ®åº“ï¼Œé¦–å…ˆå¼€å‘äº†FaceInstruct-1Mï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæŒ‡ä»¤å¾®è°ƒé¢å‘äººè„¸å¤„ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„äººè„¸ä¸­å¿ƒåŒ–æ•°æ®åº“ã€‚ç„¶åæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹çš„äººè„¸ç‰¹å®šè§†è§‰ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨ç”±Face-Region Guided Cross-Attentioné©±åŠ¨ï¼Œé›†æˆäº†äººè„¸å‡ ä½•ä¸å±€éƒ¨è§†è§‰ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªä¸åŒçš„æ•°æ®é›†å’Œäº”ä¸ªä¸åŒçš„äººè„¸å¤„ç†ä»»åŠ¡ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬é¢éƒ¨è¡¨æƒ…è¯†åˆ«ã€åŠ¨ä½œå•å…ƒæ£€æµ‹ã€é¢éƒ¨å±æ€§æ£€æµ‹ã€å¹´é¾„ä¼°è®¡å’Œæ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚Face-LLaVAç›¸è¾ƒäºç°æœ‰çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹å–å¾—äº†ä¼˜è¶Šçš„ç»“æœï¼Œå¹¶åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å®ç°äº†ä¸å•†ä¸šè§£å†³æ–¹æ¡ˆç›¸ç«äº‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹è¾“å‡ºè¿˜å¾—åˆ°äº†GPTåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ›´é«˜æ¨ç†è¯„åˆ†ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¨¡å‹éƒ½å°†åœ¨<a target="_blank" rel="noopener" href="https://face-llava.github.ioå‘å¸ƒ,ä»¥æ”¯æŒç¤¾ä¼šäººå·¥æ™ºèƒ½å’ŒåŸºç¡€è§†è§‰è¯­è¨€ç ”ç©¶çš„æœªæ¥å‘å±•./">https://face-llava.github.ioå‘å¸ƒï¼Œä»¥æ”¯æŒç¤¾ä¼šäººå·¥æ™ºèƒ½å’ŒåŸºç¡€è§†è§‰è¯­è¨€ç ”ç©¶çš„æœªæ¥å‘å±•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07198v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://face-llava.github.io/">https://face-llava.github.io</a></p>
<p><strong>Summary</strong></p>
<p>äººè„¸è¯†åˆ«åœ¨ç¤¾ä¼šäº¤æµä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œéœ€è¦é«˜æ€§èƒ½è®¡ç®—æœºè§†è§‰å·¥å…·æ”¯æŒé¢å‘äººç±»çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†Face-LLaVAï¼Œä¸€ç§ç”¨äºé¢éƒ¨ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒåŒ…æ‹¬é¢éƒ¨è¡¨æƒ…å’Œå±æ€§è¯†åˆ«ã€‚æ­¤å¤–ï¼ŒFace-LLaVAèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ï¼Œç”¨äºæ¨ç†ã€‚ç ”ç©¶æ„å»ºäº†FaceInstruct-1Mæ•°æ®åº“ï¼Œå¹¶å¼€å‘äº†ä¸€ç§æ–°å‹é¢éƒ¨ç‰¹å®šè§†è§‰ç¼–ç å™¨ï¼Œé›†æˆäº†é¢éƒ¨å‡ ä½•ä¸å±€éƒ¨è§†è§‰ç‰¹å¾ã€‚åœ¨ä¹ä¸ªæ•°æ®é›†å’Œäº”ä¸ªé¢éƒ¨å¤„ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFace-LLaVAåœ¨è¡¨æƒ…è¯†åˆ«ç­‰æ–¹é¢è¡¨ç°å“è¶Šï¼Œç›¸æ¯”å…¶ä»–å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚å…¶æ¨¡å‹è¾“å‡ºä¹Ÿåœ¨GPTé›¶æ ·æœ¬è®¾å®šä¸‹è·å¾—äº†è¾ƒé«˜çš„æ¨ç†è¯„åˆ†ã€‚è¯¥æ•°æ®é›†ä¸æ¨¡å‹å°†ä¸ºæ¨åŠ¨ç¤¾ä¼šäººå·¥æ™ºèƒ½å’ŒåŸºæœ¬è§†è§‰è¯­è¨€ç ”ç©¶æä¾›æ”¯æŒã€‚æ›´å¤šè¯¦æƒ…å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://face-llava.github.io/">https://face-llava.github.io</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººè„¸åœ¨ç¤¾ä¼šäº¤æµä¸­çš„é‡è¦æ€§åŠå…¶å¯¹äºé«˜æ€§èƒ½è®¡ç®—æœºè§†è§‰å·¥å…·çš„éœ€æ±‚ã€‚</li>
<li>Face-LLaVAå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æå‡ºï¼Œç”¨äºé¢éƒ¨ä¸ºä¸­å¿ƒã€æ”¯æŒä¸Šä¸‹æ–‡å­¦ä¹ çš„ä»»åŠ¡ã€‚</li>
<li>Face-LLaVAèƒ½å¤Ÿè¿›è¡Œé¢éƒ¨è¡¨æƒ…å’Œå±æ€§è¯†åˆ«ï¼Œå¹¶èƒ½ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°è¿›è¡Œæ¨ç†ã€‚</li>
<li>åˆ©ç”¨ç°æœ‰è§†è§‰æ•°æ®åº“å»ºç«‹äº†FaceInstruct-1Mé¢éƒ¨ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹é¢éƒ¨ç‰¹å®šè§†è§‰ç¼–ç å™¨ï¼Œé›†æˆé¢éƒ¨å‡ ä½•ä¸å±€éƒ¨è§†è§‰ç‰¹å¾çš„æŠ€æœ¯ã€‚</li>
<li>Face-LLaVAåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå±•ç¤ºå…¶å“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨è¡¨æƒ…è¯†åˆ«ç­‰æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07198">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c498a382c2c44020fd6944ba0640515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-820813fb20fcf0bc3fba72e44516473f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a72d95239f528aa787be1233717269b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af826c5b7568ac6733cda154323e226c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-859162fc5dc3e5450e362d7aa562a4ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d4752b21af542648e7b07214d4f3a64.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="HypoEval-Hypothesis-Guided-Evaluation-for-Natural-Language-Generation"><a href="#HypoEval-Hypothesis-Guided-Evaluation-for-Natural-Language-Generation" class="headerlink" title="HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation"></a>HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation</h2><p><strong>Authors:Mingxuan Li, Hanchen Li, Chenhao Tan</strong></p>
<p>Large language models (LLMs) have demonstrated great potential for automating the evaluation of natural language generation. Previous frameworks of LLM-as-a-judge fall short in two ways: they either use zero-shot setting without consulting any human input, which leads to low alignment, or fine-tune LLMs on labeled data, which requires a non-trivial number of samples. Moreover, previous methods often provide little reasoning behind automated evaluations. In this paper, we propose HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLMâ€™s assigned scores on each decomposed dimension to acquire overall scores. With only 30 human evaluations, HypoEval achieves state-of-the-art performance in alignment with both human rankings (Spearman correlation) and human scores (Pearson correlation), on average outperforming G-Eval by 11.86% and fine-tuned Llama-3.1-8B-Instruct with at least 3 times more human evaluations by 11.95%. Furthermore, we conduct systematic studies to assess the robustness of HypoEval, highlighting its effectiveness as a reliable and interpretable automated evaluation framework. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–è¯„ä¼°è‡ªç„¶è¯­è¨€ç”Ÿæˆæ–¹é¢å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ä»¥å‰çš„LLMè¯„ä¼°æ¡†æ¶åœ¨ä¸¤ä¸ªæ–¹é¢å­˜åœ¨ä¸è¶³ï¼šå®ƒä»¬è¦ä¹ˆä½¿ç”¨é›¶æ ·æœ¬è®¾ç½®ï¼Œä¸å‚è€ƒä»»ä½•äººå·¥è¾“å…¥ï¼Œå¯¼è‡´å¯¹é½ç¨‹åº¦ä½ï¼Œè¦ä¹ˆå¯¹æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè¿™éœ€è¦å¤§é‡çš„æ ·æœ¬ã€‚æ­¤å¤–ï¼Œä»¥å‰çš„æ–¹æ³•å¾€å¾€å¯¹è‡ªåŠ¨åŒ–è¯„ä¼°èƒŒåçš„æ¨ç†ä¾æ®æä¾›å¾—å¾ˆå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å‡è®¾å¼•å¯¼è¯„ä¼°æ¡†æ¶ï¼ˆHypoEvalï¼‰ï¼Œé¦–å…ˆä½¿ç”¨å°‘é‡çš„äººå·¥è¯„ä¼°è¯­æ–™åº“æ¥ç”Ÿæˆæ›´è¯¦ç»†çš„äººç±»åˆ¤æ–­è§„åˆ™ï¼Œç„¶åé‡‡ç”¨ç±»ä¼¼æ¸…å•çš„æ–¹æ³•ï¼Œç»“åˆLLMåœ¨æ¯ä¸ªåˆ†è§£ç»´åº¦ä¸Šçš„å¾—åˆ†ï¼Œä»¥è·å¾—æ€»ä½“å¾—åˆ†ã€‚ä»…é€šè¿‡30æ¬¡äººå·¥è¯„ä¼°ï¼ŒHypoEvalå°±è¾¾åˆ°äº†æœ€å…ˆè¿›çš„äººæœºå¯¹é½æ€§èƒ½ï¼ˆä½“ç°åœ¨æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°å’Œçš®å°”é€Šç›¸å…³ç³»æ•°ä¸Šï¼‰ï¼Œå¹³å‡è€Œè¨€ï¼Œç›¸å¯¹äºG-Evalæå‡äº†11.86%ï¼Œå¹¶ä¸”åœ¨è‡³å°‘3å€ä»¥ä¸Šçš„äººå·¥è¯„ä¼°æƒ…å†µä¸‹ï¼Œç›¸å¯¹äºç»è¿‡è®­ç»ƒçš„Llama-3.1-8B-Instructæå‡äº†11.95%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ç³»ç»Ÿçš„ç ”ç©¶æ¥è¯„ä¼°HypoEvalçš„ç¨³å¥æ€§ï¼Œå¼ºè°ƒäº†å…¶ä½œä¸ºå¯é å’Œå¯è§£é‡Šçš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07174v1">PDF</a> 22 pages, 3 figures, code link:   <a target="_blank" rel="noopener" href="https://github.com/ChicagoHAI/HypoEval-Gen">https://github.com/ChicagoHAI/HypoEval-Gen</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å‡è®¾å¼•å¯¼çš„è¯„ä»·æ¡†æ¶HypoEvalï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å°‘é‡çš„äººç±»è¯„ä»·æ•°æ®ç”Ÿæˆæ›´è¯¦ç»†çš„è¯„ä»·å‡†åˆ™ï¼Œå¹¶ç»“åˆæ¸…å•å¼çš„æ–¹æ³•ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹å„ä¸ªåˆ†è§£ç»´åº¦è¿›è¡Œè¯„åˆ†ï¼Œä»è€Œè·å–æ€»ä½“è¯„åˆ†ã€‚HypoEvalåªéœ€30ä¸ªäººç±»è¯„ä»·å³å¯è¾¾åˆ°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½æ°´å¹³ï¼Œä¸äººç±»æ’åå’Œè¯„åˆ†çš„å¯¹é½åº¦è¾ƒé«˜ï¼Œä¸”åœ¨é²æ£’æ€§è¯„ä¼°ä¸­è¡¨ç°å‡ºäº†å…¶å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªåŠ¨è¯„ä¼°è‡ªç„¶è¯­è¨€ç”Ÿæˆæ–¹é¢å±•ç°å·¨å¤§æ½œåŠ›ï¼Œä½†ä¹‹å‰çš„æ–¹æ³•å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å‡è®¾å¼•å¯¼çš„è¯„ä»·æ¡†æ¶HypoEvalã€‚</li>
<li>ä½¿ç”¨å°‘é‡çš„äººç±»è¯„ä»·æ•°æ®ç”Ÿæˆè¯¦ç»†çš„è¯„ä»·å‡†åˆ™ã€‚</li>
<li>ç»“åˆæ¸…å•å¼æ–¹æ³•ï¼Œé€šè¿‡LLMå¯¹å„ä¸ªåˆ†è§£ç»´åº¦è¿›è¡Œè¯„åˆ†ï¼Œè·å¾—æ€»ä½“è¯„åˆ†ã€‚</li>
<li>ä»…ç”¨30ä¸ªäººç±»è¯„ä»·å³å¯è¾¾åˆ°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚</li>
<li>ä¸äººç±»æ’åå’Œè¯„åˆ†çš„å¯¹é½åº¦é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-52fbcdcfad5e36714c9d8a9dcec030ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c078e7832efa6ca48d95b191c601c73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdd74fff946991743697425861fad073.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Holistic-Capability-Preservation-Towards-Compact-Yet-Comprehensive-Reasoning-Models"><a href="#Holistic-Capability-Preservation-Towards-Compact-Yet-Comprehensive-Reasoning-Models" class="headerlink" title="Holistic Capability Preservation: Towards Compact Yet Comprehensive   Reasoning Models"></a>Holistic Capability Preservation: Towards Compact Yet Comprehensive   Reasoning Models</h2><p><strong>Authors: Ling Team, Caizhi Tang, Chilin Fu, Chunwei Wu, Jia Guo, Jianwen Wang, Jingyu Hu, Liang Jiang, Meng Li, Peng Jiao, Pingping Liu, Shaomian Zheng, Shiwei Liang, Shuaicheng Li, Yalin Zhang, Yingting Wu, Yongkang Liu, Zhenyu Huang</strong></p>
<p>This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distillâ€™s reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at <a target="_blank" rel="noopener" href="https://huggingface.co/inclusionAI">https://huggingface.co/inclusionAI</a> </p>
<blockquote>
<p>æœ¬æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº†Ring-Lite-Distillï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ¨ç†æ¨¡å‹ï¼Œæºäºæˆ‘ä»¬å¼€æºçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰Ling-Liteã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒçš„é«˜è´¨é‡æ•°æ®æ”¶é›†å’Œå·§å¦™çš„è®­ç»ƒæ¨¡å¼ï¼Œç´§å‡‘çš„MoEæ¨¡å‹Ling-Liteå¯ä»¥è¿›ä¸€æ­¥è®­ç»ƒï¼Œå®ç°å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶å‚æ•°é«˜æ•ˆæ¶æ„ï¼Œä»…ä½¿ç”¨2.75äº¿ä¸ªæ¿€æ´»å‚æ•°ï¼Œå»ºç«‹æœ‰æ•ˆçš„è½»é‡åŒ–æ¨ç†æ¶æ„ã€‚åœ¨æ„å»ºæ­¤æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä¸ä»…ä»…ç€çœ¼äºæé«˜è§£å†³é«˜éš¾åº¦æ•°å­¦é—®é¢˜çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ï¼Œè€Œæ˜¯è‡´åŠ›äºå¼€å‘å…·æœ‰æ›´å…¨é¢èƒ½åŠ›è¦†ç›–èŒƒå›´çš„æ¨ç†æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†ä¸åŒéš¾åº¦çº§åˆ«çš„æ¨ç†ä»»åŠ¡çš„è¦†ç›–ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸€èˆ¬èƒ½åŠ›ï¼Œå¦‚æŒ‡ä»¤éµå¾ªã€å·¥å…·ä½¿ç”¨å’ŒçŸ¥è¯†ä¿ç•™ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒRing-Lite-Distillçš„æ¨ç†èƒ½åŠ›ä¸DeepSeek-R1-Distill-Qwen-7Bç›¸å½“ï¼Œè€Œå…¶ä¸€èˆ¬èƒ½åŠ›åˆ™æ˜¾è‘—è¶…è¿‡äº†DeepSeek-R1-Distill-Qwen-7Bã€‚æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/inclusionAI">https://huggingface.co/inclusionAI</a> è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07158v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>Ling-Liteæ¨¡å‹é€šè¿‡ç²¾å¿ƒçš„é«˜å“è´¨æ•°æ®æ•´åˆå’Œåˆ›æ–°è®­ç»ƒæ¨¡å¼ï¼Œå®ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†å‚æ•°æ•ˆç‡ã€‚å…¶è¿›ä¸€æ­¥è®­ç»ƒå‡ºçš„è½»é‡çº§æ¨ç†æ¨¡å‹Ring-Lite-Distillï¼Œå…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œå…¨é¢çš„èƒ½åŠ›è¦†ç›–ã€‚è¯¥æ¨¡å‹ä¸ä»…åœ¨é«˜éš¾åº¦æ•°å­¦é—®é¢˜è§£å†³ç­‰é«˜çº§æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿˜å…·å¤‡é€šç”¨èƒ½åŠ›ï¼Œå¦‚æŒ‡ä»¤éµå¾ªã€å·¥å…·ä½¿ç”¨å’ŒçŸ¥è¯†ä¿ç•™ç­‰ã€‚å…¶æ¨ç†èƒ½åŠ›ä¸DeepSeek-R1-Distill-Qwen-7Bç›¸å½“ï¼Œä½†é€šç”¨èƒ½åŠ›æ›´èƒœä¸€ç­¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ring-Lite-Distillæ˜¯åŸºäºLing-Liteæ¨¡å‹çš„è½»é‡çº§æ¨ç†æ¨¡å‹ã€‚</li>
<li>Ling-Liteé€šè¿‡é«˜è´¨é‡æ•°æ®æ•´åˆå’Œåˆ›æ–°è®­ç»ƒæ¨¡å¼å®ç°äº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Ring-Lite-Distillå…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œå…¨é¢çš„èƒ½åŠ›è¦†ç›–ï¼ŒåŒ…æ‹¬é«˜çº§æ¨ç†å’Œé€šç”¨èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ°´å¹³çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Ring-Lite-Distillçš„æ¨ç†èƒ½åŠ›ä¸DeepSeek-R1-Distill-Qwen-7Bç›¸å½“ã€‚</li>
<li>æ¨¡å‹å…·æœ‰é€šç”¨èƒ½åŠ›ï¼Œå¦‚æŒ‡ä»¤éµå¾ªã€å·¥å…·ä½¿ç”¨å’ŒçŸ¥è¯†ä¿ç•™ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d54d371caf577f988b38227b5c15d41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df81c4c0da0f66bbf4c6217e0376fab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d1788c5eaee4f28b7d5fbe8c389e635.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6eefcbf9684755ca8ef8e2d00a1161b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50695317975e22051bfa8203d963dc0a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning"><a href="#VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning" class="headerlink" title="VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning"></a>VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning</h2><p><strong>Authors:Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang</strong></p>
<p>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs. </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„è¿›å±•æå¤§åœ°æå‡äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶è¯¸å¦‚é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ç­‰æ–¹æ³•åœ¨æ–‡æœ¬å’Œå›¾åƒé¢†åŸŸå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†ä½¿ç”¨GRPOçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åœ¨è§†é¢‘MLLMsä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æå‡æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›çš„åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRFTå¯¹äºç‰¹å®šä»»åŠ¡çš„æ”¹è¿›éå¸¸æ³¨é‡æ•°æ®æ•ˆç‡ã€‚é€šè¿‡åœ¨æœ‰é™çš„æ ·æœ¬ä¸Šå¯¹æ—¶ç©ºæ„ŸçŸ¥ç›®æ ‡è¿›è¡Œå¤šä»»åŠ¡RFTè®­ç»ƒï¼Œæˆ‘ä»¬å¼€å‘å‡ºäº†VideoChat-R1ï¼Œè¿™æ˜¯ä¸€æ¬¾å¼ºå¤§çš„è§†é¢‘MLLMï¼Œå®ƒåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å¯¹è¯èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚ä¸Qwen2.5-VL-7Bç›¸æ¯”ï¼ŒVideoChat-R1åœ¨è¯¸å¦‚æ—¶é—´å®šä½ï¼ˆ+31.8ï¼‰å’Œå¯¹è±¡è·Ÿè¸ªï¼ˆ+31.2ï¼‰ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡äº†æ•°å€ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨é€šç”¨é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMEï¼ˆ+0.9ï¼‰ã€MVBenchï¼ˆ+1.0ï¼‰å’ŒPerception Testï¼ˆ+0.9ï¼‰ï¼‰ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†RFTåœ¨è§†é¢‘MLLMä¸“é¡¹ä»»åŠ¡å¢å¼ºæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½ä¸ºæœªæ¥è§†é¢‘MLLMçš„å¼ºåŒ–å­¦ä¹ ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06958v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬æ–‡ç³»ç»Ÿæ¢è®¨äº†ä½¿ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰çš„Reinforcement Fine-Tuningï¼ˆRFTï¼‰åœ¨è§†é¢‘MLLMsä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æé«˜æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›çš„åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRFTåœ¨ç‰¹å®šä»»åŠ¡æ”¹è¿›æ–¹é¢éå¸¸æ³¨é‡æ•°æ®æ•ˆç‡ã€‚é€šè¿‡æœ‰é™æ ·æœ¬çš„æ—¶ç©ºæ„ŸçŸ¥ç›®æ ‡ä¸Šçš„å¤šä»»åŠ¡RFTï¼Œæˆ‘ä»¬å¼€å‘äº†VideoChat-R1ï¼Œå®ƒåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¸ç‰ºç‰²èŠå¤©èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚ä¸Qwen2.5-VL-7Bç›¸æ¯”ï¼ŒVideoChat-R1åœ¨è¯¸å¦‚æ—¶é—´å®šä½ï¼ˆæé«˜31.8ï¼‰å’Œå¯¹è±¡è·Ÿè¸ªï¼ˆæé«˜31.2ï¼‰ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡äº†æ•°å€ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ä¸€èˆ¬é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¦‚VideoMMEï¼ˆ+ 0.9ï¼‰ï¼ŒMVBenchï¼ˆ+ 1.0ï¼‰å’ŒPerception Testï¼ˆ+ 0.9ï¼‰ã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœçªæ˜¾äº†RFTåœ¨è§†é¢‘MLLMsç‰¹å®šä»»åŠ¡å¢å¼ºæ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸Šæœ‰äº†æ–°è¿›å±•ã€‚</li>
<li>æœ¬æ–‡æ¢è®¨äº†Reinforcement Fine-Tuning (RFT) ä¸Group Relative Policy Optimization (GRPO) åœ¨è§†é¢‘MLLMsä¸­çš„åº”ç”¨ã€‚</li>
<li>RFTæ—¨åœ¨æé«˜è§†é¢‘MLLMsçš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶é€šç”¨èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºRFTæ˜¯æ•°æ®é«˜æ•ˆçš„ï¼Œèƒ½åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>é€šè¿‡å¤šä»»åŠ¡RFTï¼Œå¼€å‘äº†VideoChat-R1æ¨¡å‹ï¼Œå®ƒåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>VideoChat-R1ç›¸è¾ƒäºQwen2.5-VL-7Bï¼Œåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡å¦‚æ—¶é—´å®šä½å’Œå¯¹è±¡è·Ÿè¸ªä¸Šçš„æ€§èƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-46621a1e4484a9f27274987e9b6a9ba6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89b3adc13557b37016357b42472fc020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3358aff3ad40e151d7647349d49081f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c69c12accc911d1f6cd0de6c91a6c1cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e754349e60c319da42a2b28e9441666.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29a1999ed19e6b996b60e278dcd16437.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SPIN-Bench-How-Well-Do-LLMs-Plan-Strategically-and-Reason-Socially"><a href="#SPIN-Bench-How-Well-Do-LLMs-Plan-Strategically-and-Reason-Socially" class="headerlink" title="SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?"></a>SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?</h2><p><strong>Authors:Jianzhu Yao, Kevin Wang, Ryan Hsieh, Haisu Zhou, Tianqing Zou, Zerui Cheng, Zhangyang Wang, Pramod Viswanath</strong></p>
<p>Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and humanâ€“AI teaming. Project Website: <a target="_blank" rel="noopener" href="https://spinbench.github.io/">https://spinbench.github.io/</a> </p>
<blockquote>
<p>åœ¨ç¤¾ä¼šäº’åŠ¨ä¸­çš„æ¨ç†å’Œæˆ˜ç•¥è¡Œä¸ºæ˜¯æ™ºèƒ½çš„æ ‡å¿—ã€‚è¿™ç§æ¨ç†å½¢å¼è¿œæ¯”é™æ€ç¯å¢ƒä¸­çš„å­¤ç«‹è§„åˆ’æˆ–æ¨ç†ä»»åŠ¡ï¼ˆä¾‹å¦‚æ•°å­¦é—®é¢˜è§£å†³ï¼‰æ›´ä¸ºå¤æ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæˆ˜ç•¥è§„åˆ’ã€äº’åŠ¨ä¸è°ˆåˆ¤ï¼ˆSPIN-Benchï¼‰â€ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¤šé¢†åŸŸè¯„ä¼°ï¼Œæ—¨åœ¨è¡¡é‡æˆ˜ç•¥è§„åˆ’å’Œç¤¾ä¼šæ¨ç†çš„æ™ºèƒ½æ°´å¹³ã€‚è™½ç„¶è®¸å¤šç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ç‹­éš˜çš„è§„åˆ’æˆ–å•ä»£ç†æ¨ç†ä¸Šï¼Œä½†SPIN-Benchç»“åˆäº†ç»å…¸PDDLä»»åŠ¡ã€ç«æŠ€æ£‹ç±»æ¸¸æˆã€åˆä½œå¡ç‰Œæ¸¸æˆå’Œå¤šä»£ç†è°ˆåˆ¤åœºæ™¯åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚è¯¥æ¡†æ¶æ—¢åŒ…æ‹¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä¹ŸåŒ…æ‹¬ä¸€ä¸ªæ¨¡æ‹Ÿå’Œè¯„ä¼°å„ç§ç¤¾äº¤è®¾ç½®çš„åœºæ‰€ï¼Œä»¥æµ‹è¯•äººå·¥æ™ºèƒ½ä»£ç†çš„æ¨ç†å’Œæˆ˜ç•¥è¡Œä¸ºã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜åŠ¨ä½œç©ºé—´ã€çŠ¶æ€å¤æ‚æ€§å’Œäº¤äº’ä»£ç†çš„æ•°é‡æ¥åˆ¶å®šSPIN-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥æ¨¡æ‹Ÿå„ç§ç¤¾äº¤ç¯å¢ƒï¼Œåœ¨è¿™äº›ç¯å¢ƒä¸­ï¼ŒæˆåŠŸä¸ä»…å–å†³äºæ–¹æ³•å’Œé€æ­¥çš„å†³ç­–åˆ¶å®šï¼Œè¿˜å–å†³äºå¯¹å…¶ä»–ï¼ˆå¯¹æŠ—æ€§æˆ–åˆä½œæ€§ï¼‰å‚ä¸è€…çš„æ¦‚å¿µæ¨æ–­ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè™½ç„¶å½“ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åŸºæœ¬äº‹å®æ£€ç´¢å’ŒçŸ­æœŸè§„åˆ’æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦åœ¨å¤§çŠ¶æ€ç©ºé—´ä¸­è¿›è¡Œæ·±åº¦å¤šè·³æ¨ç†å’Œä¸ç¡®å®šç¯å¢ƒä¸‹çš„ç¤¾ä¼šé€‚åº”æ€§åè°ƒçš„ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬ä¼šé‡åˆ°æ˜¾è‘—çš„æ€§èƒ½ç“¶é¢ˆã€‚æˆ‘ä»¬æœŸæœ›SPIN-Benchèƒ½æˆä¸ºæœªæ¥ç ”ç©¶ç¨³å¥çš„å¤šä»£ç†è§„åˆ’ã€ç¤¾ä¼šæ¨ç†å’Œäººæœºåä½œé¢†åŸŸçš„å‚¬åŒ–å‰‚ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://spinbench.github.io/">https://spinbench.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12349v3">PDF</a> 42 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†ç¤¾ä¼šäº’åŠ¨ä¸­çš„æ¨ç†ä¸ç­–ç•¥è¡Œä¸ºæ˜¯æ™ºèƒ½çš„é‡è¦è¡¨ç°ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤šé¢†åŸŸè¯„ä¼°æ–¹æ³•â€”â€”æˆ˜ç•¥è®¡åˆ’äº’åŠ¨è°ˆåˆ¤åŸºå‡†æµ‹è¯•ï¼ˆSPIN-Benchï¼‰ï¼Œæ—¨åœ¨æµ‹é‡æˆ˜ç•¥è§„åˆ’å’Œç¤¾äº¤æ¨ç†çš„æ™ºèƒ½æ°´å¹³ã€‚ä¸åŒäºç°æœ‰çš„ä¾§é‡äºå•ä¸€è®¡åˆ’æˆ–å•ä¸€æ™ºèƒ½ä½“æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼ŒSPIN-Benchç»“åˆäº†ç»å…¸çš„PDDLä»»åŠ¡ã€ç«æŠ€æ£‹ç›˜æ¸¸æˆã€åˆä½œå¡ç‰Œæ¸¸æˆå’Œå¤šæ™ºèƒ½ä½“è°ˆåˆ¤åœºæ™¯åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªæ¨¡æ‹Ÿå¤šç§ç¤¾äº¤è®¾ç½®çš„åŸºå‡†æµ‹è¯•ç¯å¢ƒï¼Œä»¥æµ‹è¯•äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“çš„æ¨ç†å’Œç­–ç•¥è¡Œä¸ºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶èƒ½è¾ƒå¥½åœ°å¤„ç†åŸºæœ¬äº‹å®æ£€ç´¢å’ŒçŸ­æœŸè§„åˆ’ä»»åŠ¡ï¼Œä½†åœ¨éœ€è¦æ·±åº¦å¤šè·³æ¨ç†å’Œä¸ç¡®å®šç¯å¢ƒä¸‹çš„ç¤¾ä¼šé€‚åº”æ€§åè°ƒä»»åŠ¡æ—¶ï¼Œæ€§èƒ½ç“¶é¢ˆæ˜¾è‘—ã€‚SPIN-Benchæœ‰æœ›æˆä¸ºæœªæ¥ç ”ç©¶ç¨³å¥å¤šæ™ºèƒ½ä½“è§„åˆ’ã€ç¤¾äº¤æ¨ç†å’Œäººæœºåä½œçš„é‡è¦æ¨åŠ¨åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä¸­æåˆ°ï¼Œç¤¾ä¼šäº’åŠ¨ä¸­çš„æ¨ç†å’Œç­–ç•¥è¡Œä¸ºæ˜¯æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦ä½“ç°ï¼Œè€Œè¿™éœ€è¦æ¯”é™æ€ç¯å¢ƒä¸­çš„å­¤ç«‹è§„åˆ’æˆ–æ¨ç†ä»»åŠ¡æ›´ä¸ºé«˜çº§çš„æ™ºåŠ›æ°´å¹³ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šé¢†åŸŸè¯„ä¼°æ–¹æ³•â€”â€”æˆ˜ç•¥è§„åˆ’äº’åŠ¨è°ˆåˆ¤åŸºå‡†æµ‹è¯•ï¼ˆSPIN-Benchï¼‰ï¼Œç”¨ä»¥æµ‹é‡æ™ºèƒ½ä½“çš„æˆ˜ç•¥è§„åˆ’å’Œç¤¾äº¤æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SPIN-Benchæ¡†æ¶ç»“åˆäº†å¤šç§ä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬ç»å…¸çš„PDDLä»»åŠ¡ã€ç«æŠ€æ£‹ç›˜æ¸¸æˆã€åˆä½œå¡ç‰Œæ¸¸æˆå’Œå¤šæ™ºèƒ½ä½“è°ˆåˆ¤åœºæ™¯ï¼Œä»¥æ¨¡æ‹ŸçœŸå®çš„ç¤¾äº¤ç¯å¢ƒã€‚</li>
<li>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦æ·±åº¦å¤šè·³æ¨ç†å’Œä¸ç¡®å®šç¯å¢ƒä¸‹çš„ç¤¾ä¼šé€‚åº”æ€§åè°ƒä»»åŠ¡æ—¶å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>SPIN-Benchä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¹³å°ï¼ŒåŒ…æ‹¬ç ”ç©¶ç¨³å¥çš„å¤šæ™ºèƒ½ä½“è§„åˆ’ã€ç¤¾äº¤æ¨ç†å’Œäººæœºåä½œç­‰å…³é”®é¢†åŸŸã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜åŠ¨ä½œç©ºé—´ã€çŠ¶æ€å¤æ‚æ€§å’Œäº¤äº’æ™ºèƒ½ä½“çš„æ•°é‡ï¼ŒSPIN-Benchèƒ½å¤Ÿæ¨¡æ‹Ÿå„ç§ç¤¾äº¤è®¾ç½®ï¼Œä»¥æµ‹è¯•AIæ™ºèƒ½ä½“çš„æ¨ç†å’Œç­–ç•¥è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-09bbd46d8c3b086c52fa633d0828d5a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a25042afb63f292282ad1b051554613a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f776a21069c79f66209d467df66bfa69.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-12/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-12/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e3110fc88f85108cbd07ad02836dce36.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  Cat, Rat, Meow On the Alignment of Language Model and Human   Term-Similarity Judgments
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-11/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-23ad0ad70d5d46832db0d11e5dfb6ea0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-11  F5R-TTS Improving Flow-Matching based Text-to-Speech with Group   Relative Policy Optimization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19710k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
