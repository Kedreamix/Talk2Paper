<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  Zero-Shot Low-dose CT Denoising via Sinogram Flicking">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f37add635ac54012d4ac8396fac1fa28.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-12-æ›´æ–°"><a href="#2025-04-12-æ›´æ–°" class="headerlink" title="2025-04-12 æ›´æ–°"></a>2025-04-12 æ›´æ–°</h1><h2 id="Zero-Shot-Low-dose-CT-Denoising-via-Sinogram-Flicking"><a href="#Zero-Shot-Low-dose-CT-Denoising-via-Sinogram-Flicking" class="headerlink" title="Zero-Shot Low-dose CT Denoising via Sinogram Flicking"></a>Zero-Shot Low-dose CT Denoising via Sinogram Flicking</h2><p><strong>Authors:Yongyi Shi, Ge Wang</strong></p>
<p>Many low-dose CT imaging methods rely on supervised learning, which requires a large number of paired noisy and clean images. However, obtaining paired images in clinical practice is challenging. To address this issue, zero-shot self-supervised methods train denoising networks using only the information within a single image, such as ZS-N2N. However, these methods often employ downsampling operations that degrade image resolution. Additionally, the training dataset is inherently constrained to the image itself. In this paper, we propose a zero-shot low-dose CT imaging method based on sinogram flicking, which operates within a single image but generates many copies via random conjugate ray matching. Specifically, two conjugate X-ray pencil beams measure the same path; their expected values should be identical, while their noise levels vary during measurements. By randomly swapping portions of the conjugate X-rays in the sinogram domain, we generate a large set of sinograms with consistent content but varying noise patterns. When displayed dynamically, these sinograms exhibit a flickering effect due to their identical structural content but differing noise patterns-hence the term sinogram flicking. We train the network on pairs of sinograms with the same content but different noise distributions using a lightweight model adapted from ZS-NSN. This process is repeated to obtain the final results. A simulation study demonstrates that our method outperforms state-of-the-art approaches such as ZS-N2N. </p>
<blockquote>
<p>è®¸å¤šä½å‰‚é‡CTæˆåƒæ–¹æ³•ä¾èµ–äºç›‘ç£å­¦ä¹ ï¼Œè¿™éœ€è¦å¤§é‡é…å¯¹çš„æœ‰å™ªå£°å’Œæ¸…æ´å›¾åƒã€‚ç„¶è€Œï¼Œåœ¨ä¸´åºŠå®è·µä¸­è·å–é…å¯¹å›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé›¶æ ·æœ¬è‡ªç›‘ç£æ–¹æ³•ä»…ä½¿ç”¨å•å¹…å›¾åƒå†…çš„ä¿¡æ¯è¿›è¡Œå»å™ªç½‘ç»œè®­ç»ƒï¼Œä¾‹å¦‚ZS-N2Nã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é‡‡ç”¨é™é‡‡æ ·æ“ä½œï¼Œä¼šé™ä½å›¾åƒåˆ†è¾¨ç‡ã€‚æ­¤å¤–ï¼Œè®­ç»ƒæ•°æ®é›†æœ¬è´¨ä¸Šå—é™äºå›¾åƒæœ¬èº«ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¾›å›¾é—ªçƒçš„é›¶å‰‚é‡ä½å‰‚é‡CTæˆåƒæ–¹æ³•ï¼Œå®ƒåœ¨å•ä¸ªå›¾åƒå†…è¿›è¡Œæ“ä½œï¼Œä½†é€šè¿‡éšæœºå…±è½­å°„çº¿åŒ¹é…ç”Ÿæˆå¤šä¸ªå‰¯æœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œä¸¤æ¡å…±è½­çš„Xå°„çº¿é“…ç¬”å…‰æŸæµ‹é‡ç›¸åŒçš„è·¯å¾„ï¼›å®ƒä»¬çš„æœŸæœ›å€¼åº”è¯¥ç›¸åŒï¼Œè€Œå™ªå£°æ°´å¹³åœ¨æµ‹é‡è¿‡ç¨‹ä¸­ä¼šæœ‰æ‰€ä¸åŒã€‚é€šè¿‡éšæœºäº¤æ¢è¾›å›¾ä¸­çš„å…±è½­Xå°„çº¿çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å¤§é‡å…·æœ‰ä¸€è‡´å†…å®¹ä½†å™ªå£°æ¨¡å¼å„å¼‚çš„è¾›å›¾ã€‚ç”±äºå®ƒä»¬å…·æœ‰ç›¸åŒçš„ç»“æ„å†…å®¹ä½†å™ªå£°æ¨¡å¼ä¸åŒï¼Œå½“åŠ¨æ€æ˜¾ç¤ºæ—¶ï¼Œè¿™äº›è¾›å›¾ä¼šè¡¨ç°å‡ºé—ªçƒæ•ˆæœâ€”â€”å› æ­¤ç§°ä¸ºè¾›å›¾é—ªçƒã€‚æˆ‘ä»¬ä½¿ç”¨å…·æœ‰ç›¸åŒå†…å®¹ä½†å™ªå£°åˆ†å¸ƒä¸åŒçš„è¾›å›¾å¯¹è¿›è¡Œç½‘ç»œè®­ç»ƒï¼Œä½¿ç”¨ä»ZS-NSNæ”¹ç¼–çš„è½»é‡çº§æ¨¡å‹ã€‚è¯¥è¿‡ç¨‹ä¼šé‡å¤è¿›è¡Œä»¥è·å–æœ€ç»ˆç»“æœã€‚æ¨¡æ‹Ÿç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¦‚ZS-N2Nã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07927v1">PDF</a> 4 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦æä¾›äº†ä¸€ç§åŸºäºé›¶æ ·æœ¬è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•çš„ä½å‰‚é‡CTæˆåƒæ–°æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡éšæœºäº¤æ¢æ­£å¼¦å›¾åŸŸä¸­çš„å…±è½­å°„çº¿éƒ¨åˆ†ï¼Œç”Ÿæˆå…·æœ‰ä¸€è‡´å†…å®¹ä½†ä¸åŒå™ªå£°æ¨¡å¼çš„å¤šä¸ªæ­£å¼¦å›¾å‰¯æœ¬ã€‚è®­ç»ƒç½‘ç»œä½¿ç”¨å…·æœ‰ç›¸åŒå†…å®¹ä½†ä¸åŒå™ªå£°åˆ†å¸ƒçš„æ­£å¼¦å›¾å¯¹è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨è½»é‡çº§æ¨¡å‹ï¼Œä»è€Œåœ¨ä¸ä¾èµ–å¤§é‡é…å¯¹å™ªå£°å’Œæ¸…æ´å›¾åƒçš„æƒ…å†µä¸‹æå‡å›¾åƒå»å™ªæ•ˆæœã€‚æ¨¡æ‹Ÿç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºé›¶æ ·æœ¬è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•çš„ä½å‰‚é‡CTæˆåƒæŠ€æœ¯ã€‚</li>
<li>é€šè¿‡éšæœºäº¤æ¢æ­£å¼¦å›¾åŸŸä¸­çš„å…±è½­å°„çº¿éƒ¨åˆ†ï¼Œç”Ÿæˆå…·æœ‰ä¸€è‡´å†…å®¹ä½†ä¸åŒå™ªå£°æ¨¡å¼çš„æ­£å¼¦å›¾å‰¯æœ¬ã€‚</li>
<li>è®­ç»ƒç½‘ç»œä½¿ç”¨å…·æœ‰ç›¸åŒå†…å®¹ä½†ä¸åŒå™ªå£°åˆ†å¸ƒçš„æ­£å¼¦å›¾å¯¹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é‡‡ç”¨äº†è½»é‡çº§æ¨¡å‹ä»¥é€‚åº”è¿™ç§è®­ç»ƒæ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆå»å™ªã€‚</li>
<li>è¯¥æŠ€æœ¯èƒ½æœ‰æ•ˆè§£å†³ä¸´åºŠå®è·µä¸­é…å¯¹å›¾åƒè·å–å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>æ¨¡æ‹Ÿç ”ç©¶è¡¨æ˜ï¼Œè¯¥æŠ€æœ¯ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯å¦‚ZS-N2Næœ‰æ›´å¥½çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c889bc12ec1a10a7b6dcb0eb2e94d05a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d43fb905eea6daeb51c71526d0de4f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3fbe0d6fe1ed72d830601d5ffe77786.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b423af0e96845f2660ad110f3816a903.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Nonlocal-Retinex-Based-Variational-Model-and-its-Deep-Unfolding-Twin-for-Low-Light-Image-Enhancement"><a href="#Nonlocal-Retinex-Based-Variational-Model-and-its-Deep-Unfolding-Twin-for-Low-Light-Image-Enhancement" class="headerlink" title="Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for   Low-Light Image Enhancement"></a>Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for   Low-Light Image Enhancement</h2><p><strong>Authors:Daniel Torres, Joan Duran, Julia Navarro, Catalina Sbert</strong></p>
<p>Images captured under low-light conditions present significant limitations in many applications, as poor lighting can obscure details, reduce contrast, and hide noise. Removing the illumination effects and enhancing the quality of such images is crucial for many tasks, such as image segmentation and object detection. In this paper, we propose a variational method for low-light image enhancement based on the Retinex decomposition into illumination, reflectance, and noise components. A color correction pre-processing step is applied to the low-light image, which is then used as the observed input in the decomposition. Moreover, our model integrates a novel nonlocal gradient-type fidelity term designed to preserve structural details. Additionally, we propose an automatic gamma correction module. Building on the proposed variational approach, we extend the model by introducing its deep unfolding counterpart, in which the proximal operators are replaced with learnable networks. We propose cross-attention mechanisms to capture long-range dependencies in both the nonlocal prior of the reflectance and the nonlocal gradient-based constraint. Experimental results demonstrate that both methods compare favorably with several recent and state-of-the-art techniques across different datasets. In particular, despite not relying on learning strategies, the variational model outperforms most deep learning approaches both visually and in terms of quality metrics. </p>
<blockquote>
<p>åœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹æ•æ‰çš„å›¾åƒåœ¨è®¸å¤šåº”ç”¨ä¸­å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå› ä¸ºå…‰çº¿ä¸è¶³ä¼šæ©ç›–ç»†èŠ‚ã€é™ä½å¯¹æ¯”åº¦å’Œéšè—å™ªå£°ã€‚å»é™¤ç…§æ˜æ•ˆæœã€æé«˜æ­¤ç±»å›¾åƒçš„è´¨é‡å¯¹äºå›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ç­‰è®¸å¤šä»»åŠ¡è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºRetinexåˆ†è§£ï¼ˆåˆ†è§£å‡ºç…§æ˜ã€åå°„å’Œå™ªå£°æˆåˆ†ï¼‰çš„ä½å…‰ç…§å›¾åƒå¢å¼ºå˜åˆ†æ–¹æ³•ã€‚å¯¹ä½å…‰ç…§å›¾åƒåº”ç”¨äº†é¢œè‰²æ ¡æ­£é¢„å¤„ç†æ­¥éª¤ï¼Œç„¶åå°†å…¶ä½œä¸ºè§‚å¯Ÿè¾“å…¥ç”¨äºåˆ†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é›†æˆäº†ä¸€ç§æ–°å‹çš„éå±€éƒ¨æ¢¯åº¦å‹ä¿çœŸåº¦é¡¹ï¼Œæ—¨åœ¨ä¿ç•™ç»“æ„ç»†èŠ‚ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è‡ªåŠ¨ä¼½é©¬æ ¡æ­£æ¨¡å—ã€‚åŸºäºæ‰€æå‡ºçš„å˜åˆ†æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥å…¶æ·±åº¦å±•å¼€å¯¹åº”ç‰©æ¥æ‰©å±•æ¨¡å‹ï¼Œå…¶ä¸­è¿‘ç«¯ç®—å­è¢«å¯å­¦ä¹ çš„ç½‘ç»œæ‰€æ›¿ä»£ã€‚æˆ‘ä»¬æå‡ºäº†äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œä»¥æ•è·åå°„çš„éå±€éƒ¨å…ˆéªŒå’ŒåŸºäºéå±€éƒ¨æ¢¯åº¦çš„çº¦æŸä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸¤ç§æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šå‡ä¼˜äºæœ€è¿‘çš„ä¸€äº›æœ€å…ˆè¿›æŠ€æœ¯ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå°½ç®¡ä¸ä¾èµ–äºå­¦ä¹ ç­–ç•¥ï¼Œå˜åˆ†æ¨¡å‹åœ¨è§†è§‰å’Œè´¨é‡æŒ‡æ ‡æ–¹é¢å‡ä¼˜äºå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07810v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºRetinexåˆ†è§£çš„ä½å…‰å›¾åƒå¢å¼ºå˜åˆ†æ–¹æ³•ï¼Œé€šè¿‡åˆ†è§£å›¾åƒä¸ºç…§æ˜ã€åå°„å’Œå™ªå£°æˆåˆ†ï¼Œæé«˜ä½å…‰å›¾åƒçš„è§†è§‰è´¨é‡ã€‚æ–¹æ³•åŒ…å«é¢œè‰²æ ¡æ­£é¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶é‡‡ç”¨æ–°å‹éå±€éƒ¨æ¢¯åº¦å‹ä¿çœŸåº¦é¡¹ä»¥ä¿ç•™ç»“æ„ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå¼•å…¥è‡ªåŠ¨ä¼½é©¬æ ¡æ­£æ¨¡å—ï¼Œå¹¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯å®ç°æ¨¡å‹å±•å¼€ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ•æ‰åå°„çš„éå±€éƒ¨å…ˆéªŒå’ŒåŸºäºæ¢¯åº¦çš„çº¦æŸçš„é•¿æœŸä¾èµ–æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨è§†è§‰å’Œè´¨é‡æŒ‡æ ‡æ–¹é¢ï¼Œå³ä½¿ä¸ä¾èµ–å­¦ä¹ ç­–ç•¥ï¼Œå˜åˆ†æ¨¡å‹ä¹Ÿèƒ½è¶…è¶Šå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½å…‰å›¾åƒåœ¨å¤šç§åº”ç”¨ä¸­å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå¦‚å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ç­‰ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºRetinexåˆ†è§£çš„ä½å…‰å›¾åƒå¢å¼ºå˜åˆ†æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å›¾åƒè´¨é‡å¹¶å»é™¤ç…§æ˜å½±å“ã€‚</li>
<li>é‡‡ç”¨é¢œè‰²æ ¡æ­£é¢„å¤„ç†æ­¥éª¤å’Œéå±€éƒ¨æ¢¯åº¦å‹ä¿çœŸåº¦é¡¹ä»¥ä¿ç•™ç»“æ„ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥è‡ªåŠ¨ä¼½é©¬æ ¡æ­£æ¨¡å—è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚</li>
<li>ç»“åˆæ·±åº¦å­¦ä¹ æŠ€æœ¯å®ç°æ¨¡å‹çš„å±•å¼€åŒ–ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿æœŸä¾èµ–æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc7bd2559b64b23735186a9b86891085.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PRAD-Periapical-Radiograph-Analysis-Dataset-and-Benchmark-Model-Development"><a href="#PRAD-Periapical-Radiograph-Analysis-Dataset-and-Benchmark-Model-Development" class="headerlink" title="PRAD: Periapical Radiograph Analysis Dataset and Benchmark Model   Development"></a>PRAD: Periapical Radiograph Analysis Dataset and Benchmark Model   Development</h2><p><strong>Authors:Zhenhuan Zhou, Yuchen Zhang, Ruihong Xu, Xuansen Zhao, Tao Li</strong></p>
<p>Deep learning (DL), a pivotal technology in artificial intelligence, has recently gained substantial traction in the domain of dental auxiliary diagnosis. However, its application has predominantly been confined to imaging modalities such as panoramic radiographs and Cone Beam Computed Tomography, with limited focus on auxiliary analysis specifically targeting Periapical Radiographs (PR). PR are the most extensively utilized imaging modality in endodontics and periodontics due to their capability to capture detailed local lesions at a low cost. Nevertheless, challenges such as resolution limitations and artifacts complicate the annotation and recognition of PR, leading to a scarcity of publicly available, large-scale, high-quality PR analysis datasets. This scarcity has somewhat impeded the advancement of DL applications in PR analysis. In this paper, we present PRAD-10K, a dataset for PR analysis. PRAD-10K comprises 10,000 clinical periapical radiograph images, with pixel-level annotations provided by professional dentists for nine distinct anatomical structures, lesions, and artificial restorations or medical devices, We also include classification labels for images with typical conditions or lesions. Furthermore, we introduce a DL network named PRNet to establish benchmarks for PR segmentation tasks. Experimental results demonstrate that PRNet surpasses previous state-of-the-art medical image segmentation models on the PRAD-10K dataset. The codes and dataset will be made publicly available. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ˜¯äººå·¥æ™ºèƒ½ä¸­çš„ä¸€é¡¹å…³é”®æŠ€æœ¯ï¼Œæœ€è¿‘åœ¨ç‰™ç§‘è¾…åŠ©è¯Šæ–­é¢†åŸŸè·å¾—äº†å¤§é‡çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒçš„åº”ç”¨ä¸»è¦é›†ä¸­åœ¨å…¨æ™¯æ”¾å°„å½±åƒå’Œé”¥å½¢æŸè®¡ç®—æœºæ–­å±‚æ‰«æç­‰æˆåƒæ¨¡å¼ä¸Šï¼Œå¯¹äºæ ¹å°–å‘¨æ”¾å°„å½±åƒï¼ˆPRï¼‰çš„è¾…åŠ©åˆ†æå…³æ³¨æœ‰é™ã€‚ç”±äºPRèƒ½å¤Ÿä»¥ä½æˆæœ¬æ•è·è¯¦ç»†çš„å±€éƒ¨ç—…å˜ï¼Œå› æ­¤åœ¨ç‰™é«“å­¦å’Œç‰™å‘¨ç—…å­¦ä¸­æ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„æˆåƒæ–¹å¼ã€‚ç„¶è€Œï¼Œåˆ†è¾¨ç‡é™åˆ¶å’Œä¼ªå½±ç­‰æŒ‘æˆ˜ä½¿PRçš„æ ‡æ³¨å’Œè¯†åˆ«å˜å¾—å¤æ‚ï¼Œå¯¼è‡´å…¬å¼€å¯ç”¨çš„å¤§è§„æ¨¡é«˜è´¨é‡PRåˆ†ææ•°æ®é›†ç¨€ç¼ºã€‚è¿™ç§ç¨€ç¼ºçŠ¶å†µåœ¨ä¸€å®šç¨‹åº¦ä¸Šé˜»ç¢äº†DLåœ¨PRåˆ†æä¸­çš„åº”ç”¨å‘å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç”¨äºPRåˆ†æçš„PRAD-10Kæ•°æ®é›†ã€‚PRAD-10KåŒ…å«10,000å¼ ä¸´åºŠæ ¹å°–å‘¨æ”¾å°„å½±åƒå›¾åƒï¼Œç”±ä¸“ä¸šç‰™åŒ»æä¾›åƒç´ çº§æ ‡æ³¨ï¼Œæ¶µç›–ä¹ç§ä¸åŒçš„è§£å‰–ç»“æ„ã€ç—…å˜ä»¥åŠäººå·¥ä¿®å¤ä½“æˆ–åŒ»ç–—è®¾å¤‡ã€‚æˆ‘ä»¬è¿˜ä¸ºå…¸å‹çŠ¶å†µæˆ–ç—…å˜çš„å›¾åƒæä¾›äº†åˆ†ç±»æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåä¸ºPRNetçš„DLç½‘ç»œï¼Œä¸ºPRåˆ†å‰²ä»»åŠ¡å»ºç«‹åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRNetåœ¨PRAD-10Kæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†ä¹‹å‰æœ€å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07760v1">PDF</a> 11 pages &amp; Under Review</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨ç‰™ç§‘è¾…åŠ©è¯Šæ–­é¢†åŸŸçš„åº”ç”¨é€æ¸å—åˆ°å…³æ³¨ï¼Œå°¤å…¶åœ¨å…¨æ™¯å°„å½±å’Œé”¥å½¢æŸè®¡ç®—æœºæ–­å±‚æ‰«æç­‰æˆåƒæ–¹å¼ä¸­ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ ¹å°–å‘¨å°„å½±ï¼ˆPRï¼‰çš„è¾…åŠ©åˆ†æå°šæ˜¾ä¸è¶³ã€‚PRæ˜¯ç‰™é«“ç—…å­¦å’Œç‰™å‘¨ç—…å­¦ä¸­æœ€å¹¿æ³›ä½¿ç”¨çš„æˆåƒæ–¹å¼ï¼Œèƒ½å¤Ÿä½æˆæœ¬åœ°æ•è·å±€éƒ¨è¯¦ç»†ç—…å˜ã€‚ä½†PRåˆ†æé¢ä¸´åˆ†è¾¨ç‡é™åˆ¶å’Œä¼ªå½±ç­‰æŒ‘æˆ˜ï¼Œé«˜è´¨é‡çš„å¤§å‹å…¬å¼€æ•°æ®é›†è¾ƒä¸ºç¨€ç¼ºï¼Œåˆ¶çº¦äº†æ·±åº¦å­¦ä¹ åœ¨PRåˆ†æä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†PRAD-10Kæ•°æ®é›†å’ŒPRNetç½‘ç»œï¼Œä¸ºPRåˆ†ææä¾›åŸºå‡†æµ‹è¯•ã€‚PRAD-10KåŒ…å«10,000å¼ ä¸´åºŠæ ¹å°–å‘¨å°„å½±å›¾åƒï¼Œç”±ä¸“ä¸šç‰™åŒ»æä¾›åƒç´ çº§æ³¨é‡Šï¼Œæ¶µç›–ä¹ç§ä¸åŒçš„è§£å‰–ç»“æ„ã€ç—…å˜å’Œäººå·¥ä¿®å¤ä½“æˆ–åŒ»ç–—è®¾å¤‡ã€‚PRNetåœ¨PRAD-10Kæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡å…¶ä»–å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨ç‰™ç§‘è¾…åŠ©è¯Šæ–­ä¸­çš„åº”ç”¨é€æ¸å—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¨æ™¯å°„å½±å’Œé”¥å½¢æŸè®¡ç®—æœºæ–­å±‚æ‰«æé¢†åŸŸã€‚</li>
<li>æ ¹å°–å‘¨å°„å½±ï¼ˆPRï¼‰æ˜¯æœ€å¸¸ç”¨çš„ç‰™é«“ç—…å­¦å’Œç‰™å‘¨ç—…å­¦æˆåƒæ–¹å¼ï¼Œä½†ç”±äºåˆ†è¾¨ç‡é™åˆ¶å’Œä¼ªå½±ç­‰é—®é¢˜ï¼Œå…¶åˆ†æé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç›®å‰ç¼ºä¹å…¬å¼€çš„å¤§å‹é«˜è´¨é‡PRåˆ†ææ•°æ®é›†ï¼Œé™åˆ¶äº†æ·±åº¦å­¦ä¹ åœ¨PRåˆ†æä¸­çš„åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†PRAD-10Kæ•°æ®é›†ï¼ŒåŒ…å«ä¸“ä¸šç‰™åŒ»æä¾›çš„åƒç´ çº§æ³¨é‡Šï¼Œæ¶µç›–å¤šç§è§£å‰–ç»“æ„ã€ç—…å˜ç­‰ã€‚</li>
<li>åŒæ—¶ä»‹ç»äº†PRNetç½‘ç»œï¼Œè¯¥ç½‘ç»œåœ¨PRAD-10Kæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡å°†å…¬å¼€æ•°æ®é›†å’Œä»£ç ï¼Œä¸ºPRåˆ†ææä¾›åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1c860b7ceca170690e7680fd78729dd1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f97c2a21c06ebfd98486a9e2f2a95679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed2e095b9e8263cc566dfb72ce8d86af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dc983e5ca437f39a6b280f61897a31d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Multi-Organ-Segmentation-Tools-for-Multi-Parametric-T1-weighted-Abdominal-MRI"><a href="#Benchmarking-Multi-Organ-Segmentation-Tools-for-Multi-Parametric-T1-weighted-Abdominal-MRI" class="headerlink" title="Benchmarking Multi-Organ Segmentation Tools for Multi-Parametric   T1-weighted Abdominal MRI"></a>Benchmarking Multi-Organ Segmentation Tools for Multi-Parametric   T1-weighted Abdominal MRI</h2><p><strong>Authors:Nicole Tran, Anisa Prasad, Yan Zhuang, Tejas Sudharshan Mathai, Boah Kim, Sydney Lewis, Pritam Mukherjee, Jianfei Liu, Ronald M. Summers</strong></p>
<p>The segmentation of multiple organs in multi-parametric MRI studies is critical for many applications in radiology, such as correlating imaging biomarkers with disease status (e.g., cirrhosis, diabetes). Recently, three publicly available tools, such as MRSegmentator (MRSeg), TotalSegmentator MRI (TS), and TotalVibeSegmentator (VIBE), have been proposed for multi-organ segmentation in MRI. However, the performance of these tools on specific MRI sequence types has not yet been quantified. In this work, a subset of 40 volumes from the public Duke Liver Dataset was curated. The curated dataset contained 10 volumes each from the pre-contrast fat saturated T1, arterial T1w, venous T1w, and delayed T1w phases, respectively. Ten abdominal structures were manually annotated in these volumes. Next, the performance of the three public tools was benchmarked on this curated dataset. The results indicated that MRSeg obtained a Dice score of 80.7 $\pm$ 18.6 and Hausdorff Distance (HD) error of 8.9 $\pm$ 10.4 mm. It fared the best ($p &lt; .05$) across the different sequence types in contrast to TS and VIBE. </p>
<blockquote>
<p>åœ¨å¤šå‚æ•°MRIç ”ç©¶ä¸­ï¼Œå¤šå™¨å®˜åˆ†å‰²å¯¹äºæ”¾å°„å­¦ä¸­çš„è®¸å¤šåº”ç”¨è‡³å…³é‡è¦ï¼Œä¾‹å¦‚å°†æˆåƒç”Ÿç‰©æ ‡å¿—ç‰©ä¸ç–¾ç—…çŠ¶æ€ï¼ˆå¦‚è‚ç¡¬åŒ–ã€ç³–å°¿ç—…ç­‰ï¼‰è¿›è¡Œå…³è”ã€‚æœ€è¿‘ï¼Œæå‡ºäº†ä¸‰ç§å…¬å¼€å·¥å…·ï¼ŒåŒ…æ‹¬MRSegmentatorï¼ˆMRSegï¼‰ã€TotalSegmentator MRIï¼ˆTSï¼‰å’ŒTotalVibeSegmentatorï¼ˆVIBEï¼‰ï¼Œç”¨äºMRIä¸­çš„å¤šå™¨å®˜åˆ†å‰²ã€‚ç„¶è€Œï¼Œè¿™äº›å·¥å…·åœ¨ç‰¹å®šMRIåºåˆ—ç±»å‹ä¸Šçš„æ€§èƒ½å°šæœªå¾—åˆ°é‡åŒ–ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä»å…¬å…±Dukeè‚è„æ•°æ®é›†ä¸­æŒ‘é€‰äº†40ä¸ªä½“ç§¯çš„å­é›†ã€‚è¯¥ç²¾é€‰æ•°æ®é›†åŒ…å«æ¥è‡ªé¢„é¥±å’Œè„‚è‚ªT1ã€åŠ¨è„‰T1wã€é™è„‰T1wå’Œå»¶è¿ŸT1wé˜¶æ®µçš„å„10ä¸ªä½“ç§¯ï¼Œå¹¶åœ¨è¿™äº›ä½“ç§¯ä¸­æ‰‹åŠ¨æ ‡æ³¨äº†10ä¸ªè…¹éƒ¨ç»“æ„ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨è¿™ç»„ç²¾é€‰æ•°æ®é›†ä¸Šå¯¹è¿™ä¸‰ç§å…¬å¼€å·¥å…·çš„æ€§èƒ½è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒMRSegçš„Diceå¾—åˆ†ä¸º80.7Â±18.6ï¼ŒHausdorff Distanceï¼ˆHDï¼‰è¯¯å·®ä¸º8.9Â±10.4æ¯«ç±³ã€‚ä¸TSå’ŒVIBEç›¸æ¯”ï¼Œå®ƒåœ¨ä¸åŒçš„åºåˆ—ç±»å‹ä¸­è¡¨ç°æœ€ä½³ï¼ˆp &lt; .05ï¼‰ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07729v1">PDF</a> Published at SPIE Medical Imaging 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šå‚æ•°MRIç ”ç©¶ä¸­å¤šå™¨å®˜åˆ†å‰²å¯¹æ”¾å°„å­¦åº”ç”¨è‡³å…³é‡è¦ï¼Œå¦‚å°†å½±åƒç”Ÿç‰©æ ‡å¿—ç‰©ä¸ç–¾ç—…çŠ¶æ€ç›¸å…³è”ï¼ˆå¦‚è‚ç¡¬åŒ–ã€ç³–å°¿ç—…ç­‰ï¼‰ã€‚è¿‘æœŸæ¨å‡ºä¸‰æ¬¾å…¬å¼€å·¥å…·MRSegmentatorã€TotalSegmentator MRIå’ŒTotalVibeSegmentatorç”¨äºMRIå¤šå™¨å®˜åˆ†å‰²ï¼Œä½†è¿™äº›å·¥å…·åœ¨ç‰¹å®šMRIåºåˆ—ä¸Šçš„è¡¨ç°å°šæœªé‡åŒ–ã€‚æœ¬ç ”ç©¶é€‰ç”¨å…¬å…±Duke Liver Datasetçš„40ä½“ç§¯å­é›†ï¼ŒåŒ…å«ä¸åŒé˜¶æ®µçš„T1åŠ æƒå›¾åƒï¼Œå¯¹è…¹éƒ¨10ä¸ªç»“æ„è¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨ï¼Œä»¥è¯„ä¼°è¿™ä¸‰æ¬¾å·¥å…·çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒMRSegçš„Diceå¾—åˆ†ä¸º80.7Â±18.6ï¼ŒHausdorff Distanceè¯¯å·®ä¸º8.9Â±10.4mmï¼Œåœ¨ä¸åŒåºåˆ—ä¸­è¡¨ç°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå‚æ•°MRIç ”ç©¶ä¸­å¤šå™¨å®˜åˆ†å‰²å¯¹æ”¾å°„å­¦åº”ç”¨éå¸¸é‡è¦ã€‚</li>
<li>å­˜åœ¨å¤šæ¬¾å…¬å¼€å·¥å…·ç”¨äºMRIå¤šå™¨å®˜åˆ†å‰²ï¼ŒåŒ…æ‹¬MRSegmentatorã€TotalSegmentator MRIå’ŒTotalVibeSegmentatorã€‚</li>
<li>è¿™äº›å·¥å…·åœ¨ç‰¹å®šMRIåºåˆ—ä¸Šçš„è¡¨ç°å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ã€‚</li>
<li>ç ”ç©¶é€‰ç”¨Duke Liver Datasetçš„40ä½“ç§¯å­é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>ç ”ç©¶å¯¹è…¹éƒ¨10ä¸ªç»“æ„è¿›è¡Œäº†æ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
<li>MRSegåœ¨å¤šç§MRIåºåˆ—ä¸Šè¡¨ç°æœ€ä½³ï¼ŒDiceå¾—åˆ†ä¸º80.7Â±18.6ï¼ŒHausdorff Distanceè¯¯å·®ä¸º8.9Â±10.4mmã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a673b1e1288bcbe8665d93bd347f2c49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5eccf8cdb6210ea6d5f083186582428.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc432d4dae371440535ffdbf12087715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f22fadf146bbe8f11d24ff114624dc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-457929924a0adca284e3911b2af027ce.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PhaseGen-A-Diffusion-Based-Approach-for-Complex-Valued-MRI-Data-Generation"><a href="#PhaseGen-A-Diffusion-Based-Approach-for-Complex-Valued-MRI-Data-Generation" class="headerlink" title="PhaseGen: A Diffusion-Based Approach for Complex-Valued MRI Data   Generation"></a>PhaseGen: A Diffusion-Based Approach for Complex-Valued MRI Data   Generation</h2><p><strong>Authors:Moritz Rempe, Fabian HÃ¶rst, Helmut Becker, Marco Schlimbach, Lukas Rotkopf, Kevin KrÃ¶ninger, Jens Kleesiek</strong></p>
<p>Magnetic resonance imaging (MRI) raw data, or k-Space data, is complex-valued, containing both magnitude and phase information. However, clinical and existing Artificial Intelligence (AI)-based methods focus only on magnitude images, discarding the phase data despite its potential for downstream tasks, such as tumor segmentation and classification. In this work, we introduce $\textit{PhaseGen}$, a novel complex-valued diffusion model for generating synthetic MRI raw data conditioned on magnitude images, commonly used in clinical practice. This enables the creation of artificial complex-valued raw data, allowing pretraining for models that require k-Space information. We evaluate PhaseGen on two tasks: skull-stripping directly in k-Space and MRI reconstruction using the publicly available FastMRI dataset. Our results show that training with synthetic phase data significantly improves generalization for skull-stripping on real-world data, with an increased segmentation accuracy from $41.1%$ to $80.1%$, and enhances MRI reconstruction when combined with limited real-world data. This work presents a step forward in utilizing generative AI to bridge the gap between magnitude-based datasets and the complex-valued nature of MRI raw data. This approach allows researchers to leverage the vast amount of avaliable image domain data in combination with the information-rich k-Space data for more accurate and efficient diagnostic tasks. We make our code publicly $\href{<a target="_blank" rel="noopener" href="https://github.com/TIO-IKIM/PhaseGen%7D%7B/text%7Bavailable">https://github.com/TIO-IKIM/PhaseGen}{\text{available</a> here}}$. </p>
<blockquote>
<p>æ ¸ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åŸå§‹æ•°æ®æˆ–k-ç©ºé—´æ•°æ®æ˜¯å¤æ•°å½¢å¼çš„ï¼ŒåŒ…å«å¹…åº¦å’Œç›¸ä½ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸´åºŠå’ŒåŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„æ–¹æ³•ä»…å…³æ³¨å¹…åº¦å›¾åƒï¼Œå°½ç®¡ç›¸ä½æ•°æ®å¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è‚¿ç˜¤åˆ†å‰²å’Œåˆ†ç±»ï¼‰å…·æœ‰æ½œåŠ›ï¼Œä½†ä»å°†å…¶ä¸¢å¼ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†$\textit{PhaseGen}$ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤æ•°æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®å¹…åº¦å›¾åƒç”Ÿæˆæ¨¡æ‹ŸMRIåŸå§‹æ•°æ®ï¼Œè¿™åœ¨ä¸´åºŠå®è·µä¸­æ˜¯å¸¸è§çš„ã€‚è¿™èƒ½å¤Ÿåˆ›å»ºäººå·¥çš„å¤æ•°åŸå§‹æ•°æ®ï¼Œå…è®¸å¯¹éœ€è¦k-ç©ºé—´ä¿¡æ¯çš„æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šå¯¹PhaseGenè¿›è¡Œäº†è¯„ä¼°ï¼šç›´æ¥åœ¨k-ç©ºé—´ä¸­è¿›è¡Œé¢…éª¨å‰¥ç¦»ï¼Œä»¥åŠä½¿ç”¨å…¬å¼€å¯ç”¨çš„FastMRIæ•°æ®é›†è¿›è¡ŒMRIé‡å»ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åˆæˆç›¸ä½æ•°æ®è¿›è¡Œè®­ç»ƒæ˜¾è‘—æé«˜äº†åœ¨ç°å®æ•°æ®ä¸Šè¿›è¡Œé¢…éª¨å‰¥ç¦»çš„æ³›åŒ–èƒ½åŠ›ï¼Œåˆ†å‰²å‡†ç¡®åº¦ä»41.1%æé«˜åˆ°80.1%ï¼Œå¹¶ä¸”åœ¨ä¸æœ‰é™çš„ç°å®æ•°æ®ç›¸ç»“åˆæ—¶ï¼Œå¢å¼ºäº†MRIé‡å»ºçš„æ•ˆæœã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¥å¼¥è¡¥åŸºäºå¹…åº¦çš„æ•°æ®é›†ä¸MRIåŸå§‹æ•°æ®çš„å¤æ•°æ€§è´¨ä¹‹é—´çš„å·®è·ã€‚è¿™ç§æ–¹æ³•å…è®¸ç ”ç©¶äººå‘˜ç»“åˆå¤§é‡å¯ç”¨çš„å›¾åƒåŸŸæ•°æ®å’Œä¸°å¯Œçš„k-ç©ºé—´æ•°æ®ï¼Œä»¥æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆåœ°æ‰§è¡Œè¯Šæ–­ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/TIO-IKIM/PhaseGen">https://github.com/TIO-IKIM/PhaseGen</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07560v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹å¤æ‚å€¼æ‰©æ•£æ¨¡å‹PhaseGenï¼Œèƒ½å¤ŸåŸºäºå¹…åº¦å›¾åƒç”Ÿæˆæ¨¡æ‹Ÿçš„MRIåŸå§‹æ•°æ®ã€‚è¯¥æ¨¡å‹å¯åº”ç”¨äºç”Ÿæˆæ¨¡æ‹ŸMRI k-Spaceæ•°æ®ï¼Œä»è€Œæ»¡è¶³å¯¹åŸå§‹k-Spaceä¿¡æ¯çš„è®­ç»ƒéœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨åˆæˆç›¸ä½æ•°æ®è¿›è¡Œè®­ç»ƒå¯æœ‰æ•ˆæé«˜åœ¨ç°å®æ•°æ®ä¸Šçš„é¢…éª¨å‰¥ç¦»åˆ†æ®µå‡†ç¡®æ€§ï¼Œå¹¶æå‡MRIé‡å»ºè´¨é‡ã€‚è¿™é¡¹ç ”ç©¶ä¸ºåˆ©ç”¨ç”Ÿæˆäººå·¥æ™ºèƒ½å¼¥è¡¥äº†åŸºäºå¹…åº¦æ•°æ®é›†ä¸å¤æ‚å€¼MRIåŸå§‹æ•°æ®ä¹‹é—´çš„å·®è·æä¾›äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PhaseGenæ˜¯ä¸€ç§å¤æ‚å€¼æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»å¹…åº¦å›¾åƒç”Ÿæˆæ¨¡æ‹ŸMRIåŸå§‹æ•°æ®ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨k-Spaceç”Ÿæˆäººå·¥æ•°æ®ï¼Œä¸ºéœ€è¦k-Spaceä¿¡æ¯çš„æ¨¡å‹æä¾›é¢„è®­ç»ƒæœºä¼šã€‚</li>
<li>é€šè¿‡é¢…éª¨å‰¥ç¦»å®éªŒè¯æ˜ï¼Œä½¿ç”¨åˆæˆç›¸ä½æ•°æ®è¿›è¡Œè®­ç»ƒæ˜¾è‘—æé«˜åˆ†æ®µå‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆæœ‰é™ç°å®æ•°æ®ï¼ŒMRIé‡å»ºè´¨é‡å¾—åˆ°æå‡ã€‚</li>
<li>è¯¥ç ”ç©¶é¦–æ¬¡åˆ©ç”¨ç”ŸæˆAIæ¥ç¼©å°åŸºäºå¹…åº¦çš„æ•°æ®é›†ä¸MRIåŸå§‹æ•°æ®çš„å·®å¼‚ã€‚</li>
<li>å…¬å¼€å¯ç”¨çš„PhaseGenä»£ç ä¸ºç ”ç©¶è€…åˆ©ç”¨å¤§é‡å›¾åƒåŸŸæ•°æ®å’Œä¸°å¯Œçš„k-Spaceä¿¡æ¯æä¾›äº†å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aeb54075b1c426db844542a1c0ce7b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d35871f4f138c574330b73abad3f648.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1435311e9978fd317f4fc524d343a769.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc1c51c4cee0eee77ec1e7ada0e0da8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b84c82423c3b158e859848ee5200e9cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dcf550b7b6815722c8b7087c7c2ee50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cf2816276162be4122bc0fe113c7c49.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SydneyScapes-Image-Segmentation-for-Australian-Environments"><a href="#SydneyScapes-Image-Segmentation-for-Australian-Environments" class="headerlink" title="SydneyScapes: Image Segmentation for Australian Environments"></a>SydneyScapes: Image Segmentation for Australian Environments</h2><p><strong>Authors:Hongyu Lyu, Julie Stephany Berrio, Mao Shan, Stewart Worrall</strong></p>
<p>Autonomous Vehicles (AVs) are being partially deployed and tested across various global locations, including China, the USA, Germany, France, Japan, Korea, and the UK, but with limited demonstrations in Australia. The integration of machine learning (ML) into AV perception systems highlights the need for locally labelled datasets to develop and test algorithms in specific environments. To address this, we introduce SydneyScapes - a dataset tailored for computer vision tasks of image semantic, instance, and panoptic segmentation. This dataset, collected from Sydney and surrounding cities in New South Wales (NSW), Australia, consists of 756 images with high-quality pixel-level annotations. It is designed to assist AV industry and researchers by providing annotated data and tools for algorithm development, testing, and deployment in the Australian context. Additionally, we offer benchmarking results using state-of-the-art algorithms to establish reference points for future research and development. The dataset is publicly available at <a target="_blank" rel="noopener" href="https://hdl.handle.net/2123/33051">https://hdl.handle.net/2123/33051</a>. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰æ­£åœ¨å…¨çƒå„åœ°ï¼ŒåŒ…æ‹¬ä¸­å›½ã€ç¾å›½ã€å¾·å›½ã€æ³•å›½ã€æ—¥æœ¬ã€éŸ©å›½å’Œè‹±å›½è¿›è¡Œéƒ¨åˆ†éƒ¨ç½²å’Œæµ‹è¯•ï¼Œä½†åœ¨æ¾³å¤§åˆ©äºšçš„æ¼”ç¤ºéå¸¸æœ‰é™ã€‚æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰èå…¥AVæ„ŸçŸ¥ç³»ç»Ÿå‡¸æ˜¾äº†é’ˆå¯¹ç‰¹å®šç¯å¢ƒå¼€å‘å’Œæµ‹è¯•ç®—æ³•æ—¶å¯¹æœ¬åœ°æ ‡æ³¨æ•°æ®é›†çš„éœ€æ±‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SydneyScapesæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸“ä¸ºå›¾åƒè¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå…¨è§†é‡åˆ†å‰²ç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡é‡èº«å®šåˆ¶ã€‚è¯¥æ•°æ®é›†æ”¶é›†è‡ªæ¾³å¤§åˆ©äºšæ–°å—å¨å°”å£«å·æ‚‰å°¼åŠå…¶å‘¨è¾¹åŸå¸‚ï¼ŒåŒ…å«756å¼ é«˜è´¨é‡åƒç´ çº§æ³¨é‡Šå›¾åƒã€‚å…¶ç›®çš„æ˜¯ä¸ºè‡ªåŠ¨é©¾é©¶è¡Œä¸šå’Œç ”ç©¶è€…æä¾›æ³¨é‡Šæ•°æ®å’Œå·¥å…·ï¼Œä»¥ååŠ©å…¶åœ¨æ¾³å¤§åˆ©äºšç¯å¢ƒä¸‹è¿›è¡Œç®—æ³•å¼€å‘ã€æµ‹è¯•å’Œéƒ¨ç½²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨æœ€å…ˆè¿›çš„ç®—æ³•æä¾›äº†åŸºå‡†æµ‹è¯•ç»“æœï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œå‘å±•å»ºç«‹å‚è€ƒç‚¹ã€‚æ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://hdl.handle.net/2123/33051%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://hdl.handle.net/2123/33051å…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07542v1">PDF</a> </p>
<p><strong>Summary</strong><br>    è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨å…¨çƒèŒƒå›´å†…æ­£åœ¨éƒ¨ç½²å’Œæµ‹è¯•ï¼Œå…¶ä¸­ä¸­å›½ã€ç¾å›½ç­‰åœ°å·²ç»å¹¿æ³›åº”ç”¨ï¼Œæ¾³å¤§åˆ©äºšçš„æ¼”ç¤ºä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿä¸­æœºå™¨å­¦ä¹ ç®—æ³•åœ¨ç‰¹å®šç¯å¢ƒä¸‹å¼€å‘å’Œæµ‹è¯•çš„éœ€æ±‚ï¼Œæ¨å‡ºäº†SydneyScapesæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«é’ˆå¯¹å›¾åƒè¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ•°æ®ã€‚æ•°æ®é›†ä»æ¾³å¤§åˆ©äºšæ–°å—å¨å°”å£«å·çš„æ‚‰å°¼åŠå‘¨è¾¹åŸå¸‚æ”¶é›†ï¼ŒåŒ…å«756å¼ é«˜è´¨é‡åƒç´ çº§æ³¨é‡Šçš„å›¾åƒã€‚æ•°æ®é›†ç”¨äºå¸®åŠ©è‡ªåŠ¨é©¾é©¶è¡Œä¸šå’Œç ”ç©¶è€…è¿›è¡Œç®—æ³•å¼€å‘ã€æµ‹è¯•å’Œéƒ¨ç½²åœ¨æ¾³å¤§åˆ©äºšçš„ç¯å¢ƒä¸­ã€‚æ­¤å¤–è¿˜æä¾›æœ€æ–°çš„ç®—æ³•åŸºå‡†æµ‹è¯•ç»“æœä»¥ä¾›æœªæ¥ç ”ç©¶å‚è€ƒï¼Œè¯¥æ•°æ®é›†å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨å…¨çƒèŒƒå›´å†…æ­£åœ¨éƒ¨ç½²å’Œæµ‹è¯•ï¼Œä½†æ¾³å¤§åˆ©äºšçš„æ¼”ç¤ºä»ç„¶æœ‰é™ã€‚</li>
<li>æœºå™¨å­¦ä¹ çš„é›†æˆå¯¹è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿçš„ç®—æ³•å¼€å‘åœ¨ç‰¹å®šç¯å¢ƒä¸‹æœ‰éœ€æ±‚ã€‚</li>
<li>SydneyScapesæ•°æ®é›†æ˜¯ä¸“ä¸ºè®¡ç®—æœºè§†è§‰ä»»åŠ¡è®¾è®¡çš„ï¼ŒåŒ…æ‹¬å›¾åƒè¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ã€‚</li>
<li>SydneyScapesæ•°æ®é›†ä»æ‚‰å°¼å’Œæ–°å—å¨å°”å£«å·å‘¨è¾¹åŸå¸‚æ”¶é›†ï¼ŒåŒ…å«é«˜è´¨é‡åƒç´ çº§æ³¨é‡Šçš„å›¾åƒã€‚</li>
<li>è¯¥æ•°æ®é›†æ—¨åœ¨å¸®åŠ©è‡ªåŠ¨é©¾é©¶è¡Œä¸šå’Œç ”ç©¶äººå‘˜åœ¨æ¾³å¤§åˆ©äºšç¯å¢ƒä¸‹è¿›è¡Œç®—æ³•å¼€å‘ã€æµ‹è¯•å’Œéƒ¨ç½²ã€‚</li>
<li>SydneyScapesæ•°æ®é›†æä¾›äº†æœ€æ–°çš„ç®—æ³•åŸºå‡†æµ‹è¯•ç»“æœï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d98878a4c167f6d5ecd72250648b38a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5601909c3243d68b172a77cabe13b24f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec1dd97016c89681d456ad12ac6a493f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef6df00f0e65831ff99858446aa242b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-353ffc82e9548cb93be94650d42e5b9c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Synthetic-CT-Generation-from-Time-of-Flight-Non-Attenutaion-Corrected-PET-for-Whole-Body-PET-Attenuation-Correction"><a href="#Synthetic-CT-Generation-from-Time-of-Flight-Non-Attenutaion-Corrected-PET-for-Whole-Body-PET-Attenuation-Correction" class="headerlink" title="Synthetic CT Generation from Time-of-Flight Non-Attenutaion-Corrected   PET for Whole-Body PET Attenuation Correction"></a>Synthetic CT Generation from Time-of-Flight Non-Attenutaion-Corrected   PET for Whole-Body PET Attenuation Correction</h2><p><strong>Authors:Weijie Chen, James Wang, Alan McMillan</strong></p>
<p>Positron Emission Tomography (PET) imaging requires accurate attenuation correction (AC) to account for photon loss due to tissue density variations. In PET&#x2F;MR systems, computed tomography (CT), which offers a straightforward estimation of AC is not available. This study presents a deep learning approach to generate synthetic CT (sCT) images directly from Time-of-Flight (TOF) non-attenuation corrected (NAC) PET images, enhancing AC for PET&#x2F;MR. We first evaluated models pre-trained on large-scale natural image datasets for a CT-to-CT reconstruction task, finding that the pre-trained model outperformed those trained solely on medical datasets. The pre-trained model was then fine-tuned using an institutional dataset of 35 TOF NAC PET and CT volume pairs, achieving the lowest mean absolute error (MAE) of 74.49 HU and highest peak signal-to-noise ratio (PSNR) of 28.66 dB within the body contour region. Visual assessments demonstrated improved reconstruction of both bone and soft tissue structures from TOF NAC PET images. This work highlights the effectiveness of using pre-trained deep learning models for medical image translation tasks. Future work will assess the impact of sCT on PET attenuation correction and explore additional neural network architectures and datasets to further enhance performance and practical applications in PET imaging. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒéœ€è¦ç²¾ç¡®çš„è¡°å‡æ ¡æ­£ï¼ˆACï¼‰æ¥å¼¥è¡¥å› ç»„ç»‡å¯†åº¦å˜åŒ–è€ŒæŸå¤±çš„å…‰å­ã€‚åœ¨PET&#x2F;MRç³»ç»Ÿä¸­ï¼Œæä¾›ACç›´æ¥ä¼°ç®—çš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å¹¶ä¸å¯ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œç›´æ¥ä»é£è¡Œæ—¶é—´ï¼ˆTOFï¼‰éè¡°å‡æ ¡æ­£ï¼ˆNACï¼‰PETå›¾åƒç”ŸæˆåˆæˆCTï¼ˆsCTï¼‰å›¾åƒï¼Œä»¥å¢å¼ºPET&#x2F;MRçš„ACã€‚æˆ‘ä»¬é¦–å…ˆè¯„ä¼°äº†åœ¨å¤§å‹è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡ŒCT-to-CTé‡å»ºä»»åŠ¡çš„æ•ˆæœï¼Œå‘ç°é¢„è®­ç»ƒæ¨¡å‹ä¼˜äºä»…ä½¿ç”¨åŒ»ç–—æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨æœºæ„æä¾›çš„35å¯¹TOF NAC PETå’ŒCTä½“ç§¯æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œåœ¨ä½“è½®å»“åŒºåŸŸå†…è¾¾åˆ°æœ€ä½çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸º74.49 HUå’Œæœ€é«˜çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸º28.66 dBã€‚è§†è§‰è¯„ä¼°è¡¨æ˜ï¼Œä»TOF NAC PETå›¾åƒä¸­é‡å»ºçš„éª¨éª¼å’Œè½¯ç»„ç»‡ç»“æ„å‡æœ‰æ‰€æ”¹å–„ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ä½¿ç”¨é¢„è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡ŒåŒ»å­¦å›¾åƒç¿»è¯‘ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚æœªæ¥çš„å·¥ä½œå°†è¯„ä¼°sCTå¯¹PETè¡°å‡æ ¡æ­£çš„å½±å“ï¼Œå¹¶æ¢ç´¢å…¶ä»–ç¥ç»ç½‘ç»œæ¶æ„å’Œæ•°æ®é›†ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½å¹¶åœ¨PETæˆåƒä¸­çš„å®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07450v1">PDF</a> 4 pages, 2 figures, ISBI 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç›´æ¥ä»é£è¡Œæ—¶é—´éè¡°å‡æ ¡æ­£PETå›¾åƒç”ŸæˆåˆæˆCTï¼ˆsCTï¼‰å›¾åƒï¼Œä»¥æé«˜PET&#x2F;MRçš„è¡°å‡æ ¡æ­£ã€‚ç ”ç©¶è¯„ä¼°äº†é¢„è®­ç»ƒäºå¤§è§„æ¨¡è‡ªç„¶å›¾åƒæ•°æ®é›†çš„æ¨¡å‹ï¼Œå‘ç°å…¶è¾ƒä»…è®­ç»ƒäºåŒ»ç–—æ•°æ®é›†çš„æ¨¡å‹è¡¨ç°æ›´ä½³ã€‚ç»æœºæ„æ•°æ®é›†å¾®è°ƒåï¼Œæ¨¡å‹åœ¨ä½“è…”åŒºåŸŸå†…è¾¾åˆ°æœ€ä½å¹³å‡ç»å¯¹è¯¯å·®74.49 HUåŠæœ€é«˜å³°å€¼ä¿¡å™ªæ¯”28.66 dBã€‚è§†è§‰è¯„ä¼°æ˜¾ç¤ºï¼Œä»TOF NAC PETå›¾åƒé‡å»ºçš„éª¨éª¼å’Œè½¯ç»„ç»‡ç»“æ„æœ‰æ‰€æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PETæˆåƒéœ€è¦è¿›è¡Œè¡°å‡æ ¡æ­£ä»¥è¡¥å¿å› ç»„ç»‡å¯†åº¦å˜åŒ–å¯¼è‡´çš„å…‰å­æŸå¤±ã€‚</li>
<li>åœ¨PET&#x2F;MRç³»ç»Ÿä¸­ï¼Œé€šå¸¸ä½¿ç”¨çš„CTå¹¶ä¸æä¾›è¡°å‡æ ¡æ­£çš„ç›´è§‚ä¼°è®¡ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç›´æ¥ä»éè¡°å‡æ ¡æ­£PETå›¾åƒç”ŸæˆåˆæˆCTå›¾åƒï¼Œä»¥æé«˜PET&#x2F;MRçš„è¡°å‡æ ¡æ­£æ•ˆæœã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†é¢„è®­ç»ƒäºå¤§è§„æ¨¡è‡ªç„¶å›¾åƒæ•°æ®é›†çš„æ¨¡å‹æ€§èƒ½ï¼Œå‘ç°å…¶è¡¨ç°ä¼˜äºä»…ä½¿ç”¨åŒ»ç–—æ•°æ®é›†çš„æ¨¡å‹ã€‚</li>
<li>ç»è¿‡æœºæ„æ•°æ®é›†çš„å¾®è°ƒï¼Œæ¨¡å‹åœ¨ä½“è…”åŒºåŸŸè¾¾åˆ°äº†è¾ƒä½çš„å¹³å‡ç»å¯¹è¯¯å·®å’Œè¾ƒé«˜çš„å³°å€¼ä¿¡å™ªæ¯”ã€‚</li>
<li>è§†è§‰è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹èƒ½å¤Ÿä»éè¡°å‡æ ¡æ­£PETå›¾åƒä¸­æ”¹å–„éª¨éª¼å’Œè½¯ç»„ç»‡ç»“æ„çš„é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46625096e27bc60039ddf01bb289afae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4935c7ec4c731e1530977f4af2c1f45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2324ab868e6185dde3d9a53715372ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84002300447576d0fd3fe4287fb95838.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49b1098e0267e344ece5b672e1e08d3c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RadZero-Similarity-Based-Cross-Attention-for-Explainable-Vision-Language-Alignment-in-Radiology-with-Zero-Shot-Multi-Task-Capability"><a href="#RadZero-Similarity-Based-Cross-Attention-for-Explainable-Vision-Language-Alignment-in-Radiology-with-Zero-Shot-Multi-Task-Capability" class="headerlink" title="RadZero: Similarity-Based Cross-Attention for Explainable   Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability"></a>RadZero: Similarity-Based Cross-Attention for Explainable   Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability</h2><p><strong>Authors:Jonggwon Park, Soobum Kim, Byungmu Yoon, Kyoyun Choi</strong></p>
<p>Recent advancements in multi-modal models have significantly improved vision-language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution images, and offer limited interpretability in attention mechanisms. To address these challenges, we introduce RadZero, a novel similarity-based cross-attention framework for vision-language alignment in radiology with zero-shot multi-task capability. RadZero leverages large language models to extract minimal semantic sentences from radiology reports and employs a multi-positive contrastive learning strategy to effectively capture relationships between images and multiple relevant textual descriptions. It also utilizes a pre-trained vision encoder with additional trainable Transformer layers, allowing efficient high-resolution image processing. By computing similarity between text embeddings and local image patch features, RadZero enables zero-shot inference with similarity probability for classification and pixel-level cross-modal similarity maps for grounding and segmentation. Experimental results on public chest radiograph benchmarks show that RadZero outperforms state-of-the-art methods in zero-shot classification, grounding, and segmentation. Furthermore, cross-modal similarity map analysis highlights its potential for improving explainability in vision-language alignment. Additionally, qualitative evaluation demonstrates RadZeroâ€™s capability for open-vocabulary semantic segmentation, further validating its effectiveness in medical imaging. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒçš„å¤šæ¨¡æ€æ¨¡å‹æ–¹é¢ï¼Œæœ€è¿‘çš„è¿›å±•æå¤§åœ°æ”¹å–„äº†è§†è§‰ä¸è¯­è¨€çš„å¯¹é½åœ¨æ”¾å°„å­¦é¢†åŸŸçš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å¤æ‚çš„æ”¾å°„å­¦æŠ¥å‘Šè¿›è¡Œå­¦ä¹ ï¼Œä¾èµ–äºä½åˆ†è¾¨ç‡çš„å›¾åƒï¼Œå¹¶ä¸”åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è§£é‡Šæ€§æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RadZeroï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç›¸ä¼¼æ€§äº¤å‰æ³¨æ„åŠ›æ¡†æ¶çš„æ”¾å°„å­¦è§†è§‰ä¸è¯­è¨€å¯¹é½æ–¹æ³•ï¼Œå…·æœ‰é›¶æ ·æœ¬å¤šä»»åŠ¡èƒ½åŠ›ã€‚RadZeroåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–æœ€å°çš„è¯­ä¹‰å¥å­ï¼Œå¹¶é‡‡ç”¨å¤šé˜³æ€§å¯¹æ¯”å­¦ä¹ ç­–ç•¥æœ‰æ•ˆåœ°æ•è·å›¾åƒä¸å¤šä¸ªç›¸å…³æ–‡æœ¬æè¿°ä¹‹é—´çš„å…³ç³»ã€‚å®ƒè¿˜åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä»¥åŠé¢å¤–çš„å¯è®­ç»ƒTransformerå±‚ï¼Œå®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†ã€‚é€šè¿‡è®¡ç®—æ–‡æœ¬åµŒå…¥å’Œå±€éƒ¨å›¾åƒè¡¥ä¸ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ŒRadZeroèƒ½å¤Ÿå®ç°é›¶æ ·æœ¬æ¨ç†ï¼Œä½¿ç”¨ç›¸ä¼¼æ€§æ¦‚ç‡è¿›è¡Œåˆ†ç±»ï¼Œä»¥åŠåƒç´ çº§çš„è·¨æ¨¡æ€ç›¸ä¼¼æ€§åœ°å›¾ç”¨äºå®šä½å’Œæ–¹å‘åˆ†å‰²ã€‚åœ¨å…¬å…±èƒ¸éƒ¨Xå°„çº¿åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRadZeroåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å®šä½å’Œåˆ†å‰²æ–¹é¢çš„æ€§èƒ½è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè·¨æ¨¡æ€ç›¸ä¼¼æ€§åœ°å›¾åˆ†æå‡¸æ˜¾äº†å…¶åœ¨è§†è§‰ä¸è¯­è¨€å¯¹é½çš„è§£é‡Šæ€§æ–¹é¢çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œå®šæ€§è¯„ä¼°è¯æ˜äº†RadZeroåœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨åŒ»å­¦å½±åƒä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07416v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RadZeroæ˜¯ä¸€ç§åŸºäºç›¸ä¼¼æ€§äº¤å‰æ³¨æ„æœºåˆ¶çš„å…¨æ–°æ¡†æ¶ï¼Œç”¨äºè§£å†³æ”¾å°„å­¦ä¸­çš„è§†è§‰è¯­è¨€å¯¹é½é—®é¢˜ã€‚å®ƒé‡‡ç”¨å¤šæ¨¡æ€æ¨¡å‹ï¼Œå…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚éš¾ä»¥åˆ©ç”¨å¤æ‚æ”¾å°„å­¦æŠ¥å‘Šè¿›è¡Œå­¦ä¹ ã€ä¾èµ–ä½åˆ†è¾¨ç‡å›¾åƒä»¥åŠæ³¨æ„åŠ›æœºåˆ¶çš„å¯è§£é‡Šæ€§æœ‰é™ç­‰æŒ‘æˆ˜ã€‚RadZeroå…·æœ‰é›¶æ ·æœ¬å¤šä»»åŠ¡èƒ½åŠ›ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–å…³é”®è¯­ä¹‰å¥å­ï¼Œå¹¶é‡‡ç”¨å¤šé˜³æ€§å¯¹æ¯”å­¦ä¹ ç­–ç•¥æœ‰æ•ˆæ•æ‰å›¾åƒä¸å¤šä¸ªç›¸å…³æ–‡æœ¬æè¿°ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡è®¡ç®—æ–‡æœ¬åµŒå…¥å’Œå±€éƒ¨å›¾åƒè¡¥ä¸ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ŒRadZeroå®ç°äº†é›¶æ ·æœ¬æ¨æ–­ï¼Œå¯ç”¨äºåˆ†ç±»å’Œåƒç´ çº§åˆ«çš„è·¨æ¨¡æ€ç›¸ä¼¼æ€§æ˜ å°„ï¼Œä¸ºå®šä½å’Œåˆ†ææä¾›æœ‰åŠ›æ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRadZeroåœ¨å…¬å…±èƒ¸éƒ¨Xå…‰å½±åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç°å‡ºå…¶åœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å®šä½å’Œåˆ†å‰²æ–¹é¢çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè·¨æ¨¡æ€ç›¸ä¼¼æ€§æ˜ å°„åˆ†æçªæ˜¾å…¶åœ¨æé«˜è§†è§‰è¯­è¨€å¯¹é½çš„å¯è§£é‡Šæ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RadZeroæ˜¯ä¸€ä¸ªåŸºäºç›¸ä¼¼æ€§äº¤å‰æ³¨æ„æœºåˆ¶çš„æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›æ”¾å°„å­¦ä¸­çš„è§†è§‰è¯­è¨€å¯¹é½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬å¤æ‚æŠ¥å‘Šåˆ©ç”¨å›°éš¾ã€ä½åˆ†è¾¨ç‡å›¾åƒä¾èµ–å’Œæœ‰é™çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>RadZeroå…·å¤‡é›¶æ ·æœ¬å¤šä»»åŠ¡èƒ½åŠ›ï¼Œè¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–å…³é”®è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨å¤šé˜³æ€§å¯¹æ¯”å­¦ä¹ ç­–ç•¥æœ‰æ•ˆæ•æ‰å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>é€šè¿‡è®¡ç®—æ–‡æœ¬åµŒå…¥å’Œå›¾åƒè¡¥ä¸ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§å®ç°é›¶æ ·æœ¬æ¨æ–­ã€‚</li>
<li>RadZeroåœ¨åˆ†ç±»ã€å®šä½å’Œåˆ†å‰²æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-167f6de78808980aa57d24d460f78a0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33d5656c42a69d59c0346f27f961f27f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee7ab2704b3be1dad73e7883c0199f45.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Leveraging-LLMs-for-Multimodal-Retrieval-Augmented-Radiology-Report-Generation-via-Key-Phrase-Extraction"><a href="#Leveraging-LLMs-for-Multimodal-Retrieval-Augmented-Radiology-Report-Generation-via-Key-Phrase-Extraction" class="headerlink" title="Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report   Generation via Key Phrase Extraction"></a>Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report   Generation via Key Phrase Extraction</h2><p><strong>Authors:Kyoyun Choi, Byungmu Yoon, Soobum Kim, Jonggwon Park</strong></p>
<p>Automated radiology report generation (RRG) holds potential to reduce radiologistsâ€™ workload, especially as recent advancements in large language models (LLMs) enable the development of multimodal models for chest X-ray (CXR) report generation. However, multimodal LLMs (MLLMs) are resource-intensive, requiring vast datasets and substantial computational cost for training. To address these challenges, we propose a retrieval-augmented generation approach that leverages multimodal retrieval and LLMs to generate radiology reports while mitigating hallucinations and reducing computational demands. Our method uses LLMs to extract key phrases from radiology reports, effectively focusing on essential diagnostic information. Through exploring effective training strategies, including image encoder structure search, adding noise to text embeddings, and additional training objectives, we combine complementary pre-trained image encoders and adopt contrastive learning between text and semantic image embeddings. We evaluate our approach on MIMIC-CXR dataset, achieving state-of-the-art results on CheXbert metrics and competitive RadGraph F1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method demonstrates robust generalization for multi-view RRG, making it suitable for comprehensive clinical applications. </p>
<blockquote>
<p>è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰å…·æœ‰å‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿå·¥ä½œé‡çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œä½¿å¾—ç”¨äºèƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æŠ¥å‘Šç”Ÿæˆçš„è·¨æ¨¡æ€æ¨¡å‹çš„å¼€å‘æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œè·¨æ¨¡æ€LLMï¼ˆMLLMï¼‰èµ„æºå¯†é›†ï¼Œéœ€è¦å¤§é‡çš„æ•°æ®é›†å’Œå¤§é‡çš„è®¡ç®—æˆæœ¬è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºæ£€ç´¢çš„ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è·¨æ¨¡æ€æ£€ç´¢å’ŒLLMç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šï¼ŒåŒæ—¶å‡è½»è™šæ„ç°è±¡å¹¶é™ä½è®¡ç®—éœ€æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨LLMä»æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–å…³é”®çŸ­è¯­ï¼Œæœ‰æ•ˆåœ°å…³æ³¨å…³é”®è¯Šæ–­ä¿¡æ¯ã€‚é€šè¿‡æ¢ç´¢æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å›¾åƒç¼–ç å™¨ç»“æ„æœç´¢ã€å‘æ–‡æœ¬åµŒå…¥æ·»åŠ å™ªå£°ä»¥åŠé¢å¤–çš„è®­ç»ƒç›®æ ‡ï¼Œæˆ‘ä»¬ç»“åˆäº†é¢„è®­ç»ƒçš„å›¾åƒç¼–ç å™¨çš„äº’è¡¥æ€§ï¼Œå¹¶åœ¨æ–‡æœ¬å’Œè¯­ä¹‰å›¾åƒåµŒå…¥ä¹‹é—´é‡‡ç”¨å¯¹æ¯”å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨CheXbertæŒ‡æ ‡ä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œåœ¨RadGraph F1æŒ‡æ ‡ä¸Šä¸MLLMç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”æ— éœ€å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šè§†å›¾RRGä¸­è¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºå…¨é¢çš„ä¸´åºŠåº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07415v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨åŒ–ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šï¼ˆRRGï¼‰çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°è¿›å±•ï¼Œä¸ºç”Ÿæˆèƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æŠ¥å‘Šçš„å¤šæ¨¡æ€æ¨¡å‹å¼€å‘æä¾›äº†å¯èƒ½ã€‚é’ˆå¯¹å¤šæ¨¡æ€LLMsèµ„æºå¯†é›†ã€éœ€è¦å¤§é‡æ•°æ®é›†å’Œé«˜æ˜‚çš„è®¡ç®—æˆæœ¬çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€æ£€ç´¢å’ŒLLMsç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šï¼Œå‡å°‘å¹»è§‰ç°è±¡å¹¶é™ä½è®¡ç®—éœ€æ±‚ã€‚é€šè¿‡æœ‰æ•ˆè®­ç»ƒç­–ç•¥ï¼Œç»“åˆé¢„è®­ç»ƒå›¾åƒç¼–ç å™¨ï¼Œé‡‡ç”¨æ–‡æœ¬å’Œè¯­ä¹‰å›¾åƒåµŒå…¥ä¹‹é—´çš„å¯¹æ¯”å­¦ä¹ ï¼Œåœ¨MIMIC-CXRæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå…·æœ‰å‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿå·¥ä½œé‡çš„æ½œåŠ›ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„åº”ç”¨ä¸ºè¿™ä¸€é¢†åŸŸå¸¦æ¥äº†æ–°å¯èƒ½ã€‚</li>
<li>å¤šæ¨¡æ€LLMså­˜åœ¨èµ„æºå¯†é›†å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šæ¨¡æ€æ£€ç´¢å’ŒLLMsæ¥ç”ŸæˆæŠ¥å‘Šï¼Œå‡å°‘å¹»è§‰ç°è±¡å¹¶é™ä½è®¡ç®—éœ€æ±‚ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨LLMsæå–å…³é”®çŸ­è¯­ï¼Œèšç„¦äºé‡è¦çš„è¯Šæ–­ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å›¾åƒç¼–ç å™¨ç»“æ„æœç´¢ã€æ–‡æœ¬åµŒå…¥æ·»åŠ å™ªå£°å’Œé¢å¤–çš„è®­ç»ƒç›®æ ‡ï¼Œè¯¥æ–¹æ³•åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f909d0e45200b7b94d6db697c328ffd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ad4251b9236103ca92836a71275532b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7a5e04e3409343740aa9e0bc7b3bdda.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Zeus-Zero-shot-LLM-Instruction-for-Union-Segmentation-in-Multimodal-Medical-Imaging"><a href="#Zeus-Zero-shot-LLM-Instruction-for-Union-Segmentation-in-Multimodal-Medical-Imaging" class="headerlink" title="Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal   Medical Imaging"></a>Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal   Medical Imaging</h2><p><strong>Authors:Siyuan Dai, Kai Ye, Guodong Liu, Haoteng Tang, Liang Zhan</strong></p>
<p>Medical image segmentation has achieved remarkable success through the continuous advancement of UNet-based and Transformer-based foundation backbones. However, clinical diagnosis in the real world often requires integrating domain knowledge, especially textual information. Conducting multimodal learning involves visual and text modalities shown as a solution, but collecting paired vision-language datasets is expensive and time-consuming, posing significant challenges. Inspired by the superior ability in numerous cross-modal tasks for Large Language Models (LLMs), we proposed a novel Vision-LLM union framework to address the issues. Specifically, we introduce frozen LLMs for zero-shot instruction generation based on corresponding medical images, imitating the radiology scanning and report generation process. {To better approximate real-world diagnostic processes}, we generate more precise text instruction from multimodal radiology images (e.g., T1-w or T2-w MRI and CT). Based on the impressive ability of semantic understanding and rich knowledge of LLMs. This process emphasizes extracting special features from different modalities and reunion the information for the ultimate clinical diagnostic. With generated text instruction, our proposed union segmentation framework can handle multimodal segmentation without prior collected vision-language datasets. To evaluate our proposed method, we conduct comprehensive experiments with influential baselines, the statistical results and the visualized case study demonstrate the superiority of our novel method.} </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸéšç€åŸºäºUNetå’ŒTransformerçš„åŸºç¡€éª¨å¹²ç½‘ç»œçš„ä¸æ–­å‘å±•ï¼Œå·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„ä¸´åºŠè¯Šæ–­å¾€å¾€éœ€è¦ç»“åˆé¢†åŸŸçŸ¥è¯†ï¼Œå°¤å…¶æ˜¯æ–‡æœ¬ä¿¡æ¯ã€‚è¿›è¡Œå¤šæ¨¡æ€å­¦ä¹ æ¶‰åŠè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¢«è¯æ˜æ˜¯ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œä½†æ”¶é›†é…å¯¹çš„è§†è§‰è¯­è¨€æ•°æ®é›†æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œè¿™æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªè·¨æ¨¡æ€ä»»åŠ¡ä¸­çš„å“è¶Šèƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„Vision-LLMè”åˆæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥å†»ç»“çš„LLMè¿›è¡Œé›¶å°„å‡»æŒ‡ä»¤ç”Ÿæˆï¼Œè¿™äº›æŒ‡ä»¤åŸºäºç›¸åº”çš„åŒ»å­¦å›¾åƒï¼Œæ¨¡ä»¿æ”¾å°„å­¦æ‰«æå’ŒæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ºäº†æ›´å¥½åœ°è¿‘ä¼¼çœŸå®ä¸–ç•Œçš„è¯Šæ–­è¿‡ç¨‹ï¼Œæˆ‘ä»¬ä»å¤šæ¨¡æ€æ”¾å°„å­¦å›¾åƒï¼ˆå¦‚T1åŠ æƒæˆ–T2åŠ æƒMRIå’ŒCTï¼‰ç”Ÿæˆæ›´ç²¾ç¡®çš„æ–‡æœ¬æŒ‡ä»¤ã€‚åŸºäºLLMä»¤äººå°è±¡æ·±åˆ»çš„è¯­ä¹‰ç†è§£èƒ½åŠ›å’Œä¸°å¯Œçš„çŸ¥è¯†ã€‚è¿™ä¸ªè¿‡ç¨‹å¼ºè°ƒä»ä¸åŒæ¨¡æ€ä¸­æå–ç‰¹æ®Šç‰¹å¾ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯èåˆè¿›è¡Œæœ€ç»ˆçš„ä¸´åºŠè¯Šæ–­ã€‚ä½¿ç”¨ç”Ÿæˆçš„æ–‡æœ¬æŒ‡ä»¤ï¼Œæˆ‘ä»¬æå‡ºçš„è”åˆåˆ†å‰²æ¡†æ¶å¯ä»¥è¿›è¡Œå¤šæ¨¡æ€åˆ†å‰²ï¼Œè€Œæ— éœ€äº‹å…ˆæ”¶é›†è§†è§‰è¯­è¨€æ•°æ®é›†ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä¸æœ‰å½±å“åŠ›çš„åŸºçº¿è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»Ÿè®¡ç»“æœå’Œå¯è§†åŒ–æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†æˆ‘ä»¬æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07336v1">PDF</a> 21 pages, 4 figures, In Press by a journal</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå› UNetå’ŒTransformeråŸºç¡€éª¨å¹²çš„æŒç»­è¿›æ­¥è€Œå–å¾—æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„ä¸´åºŠè¯Šæ–­éœ€è¦ç»“åˆé¢†åŸŸçŸ¥è¯†ï¼Œå°¤å…¶æ˜¯æ–‡æœ¬ä¿¡æ¯ã€‚è™½ç„¶å¤šæ¨¡æ€å­¦ä¹ èåˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œä½†æ”¶é›†é…å¯¹è§†è¯­è¨€æ•°æ®é›†æ—¢æ˜‚è´µåˆè€—æ—¶ï¼ŒæŒ‘æˆ˜é‡é‡ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­å‡ºè‰²èƒ½åŠ›çš„å¯å‘ï¼Œæå‡ºæ–°å‹Vision-LLMè”åˆæ¡†æ¶ï¼Œå¼•å…¥å†»ç»“çš„LLMè¿›è¡Œé›¶å°„å‡»æŒ‡ä»¤ç”Ÿæˆï¼ŒåŸºäºç›¸åº”åŒ»å­¦å›¾åƒè¿›è¡Œæ¨¡ä»¿æ”¾å°„æ‰«æå’ŒæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ¡†æ¶ä»å¤šæ¨¡æ€æ”¾å°„å›¾åƒç”Ÿæˆç²¾ç¡®æ–‡æœ¬æŒ‡ä»¤ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿç°å®è¯Šæ–­è¿‡ç¨‹ã€‚ç»“åˆLLMçš„è¯­ä¹‰ç†è§£å’Œä¸°å¯ŒçŸ¥è¯†ï¼Œè¯¥æ¡†æ¶å¼ºè°ƒä»ä¸åŒæ¨¡æ€æå–ç‰¹æ®Šç‰¹å¾å¹¶é‡æ–°æ•´åˆä¿¡æ¯ä»¥è¿›è¡Œæœ€ç»ˆä¸´åºŠè¯Šæ–­ã€‚å€ŸåŠ©ç”Ÿæˆçš„æ–‡æœ¬æŒ‡ä»¤ï¼Œè¯¥è”åˆåˆ†å‰²æ¡†æ¶æ— éœ€é¢„å…ˆæ”¶é›†è§†è¯­è¨€æ•°æ®é›†å³å¯å¤„ç†å¤šæ¨¡æ€åˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸæŒç»­åˆ©ç”¨UNetå’ŒTransformeråŸºç¡€éª¨å¹²æŠ€æœ¯å–å¾—è¿›å±•ã€‚</li>
<li>ä¸´åºŠè¯Šæ–­éœ€ç»“åˆé¢†åŸŸçŸ¥è¯†å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>å¤šæ¨¡æ€å­¦ä¹ èåˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œä½†æ”¶é›†é…å¯¹è§†è¯­è¨€æ•°æ®é›†å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥å†»ç»“çš„LLMè¿›è¡Œé›¶å°„å‡»æŒ‡ä»¤ç”Ÿæˆï¼ŒåŸºäºåŒ»å­¦å›¾åƒæ¨¡ä»¿æ”¾å°„æ‰«æå’ŒæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>æ¡†æ¶ç”Ÿæˆç²¾ç¡®æ–‡æœ¬æŒ‡ä»¤ä»¥æ¨¡æ‹Ÿç°å®è¯Šæ–­è¿‡ç¨‹ã€‚</li>
<li>æ¡†æ¶ç»“åˆLLMçš„è¯­ä¹‰ç†è§£å’Œä¸°å¯ŒçŸ¥è¯†ï¼Œå¼ºè°ƒç‰¹å¾æå–å’Œä¿¡æ¯æ•´åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f92ec0982b687fad5ee60509dfeb7fb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec05b15adc11e6ea690ac834637cca8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f37add635ac54012d4ac8396fac1fa28.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MoEDiff-SR-Mixture-of-Experts-Guided-Diffusion-Model-for-Region-Adaptive-MRI-Super-Resolution"><a href="#MoEDiff-SR-Mixture-of-Experts-Guided-Diffusion-Model-for-Region-Adaptive-MRI-Super-Resolution" class="headerlink" title="MoEDiff-SR: Mixture of Experts-Guided Diffusion Model for   Region-Adaptive MRI Super-Resolution"></a>MoEDiff-SR: Mixture of Experts-Guided Diffusion Model for   Region-Adaptive MRI Super-Resolution</h2><p><strong>Authors:Zhe Wang, Yuhua Ru, Aladine Chetouani, Fang Chen, Fabian Bauer, Liping Zhang, Didier Hans, Rachid Jennane, Mohamed Jarraya, Yung Hsin Chen</strong></p>
<p>Magnetic Resonance Imaging (MRI) at lower field strengths (e.g., 3T) suffers from limited spatial resolution, making it challenging to capture fine anatomical details essential for clinical diagnosis and neuroimaging research. To overcome this limitation, we propose MoEDiff-SR, a Mixture of Experts (MoE)-guided diffusion model for region-adaptive MRI Super-Resolution (SR). Unlike conventional diffusion-based SR models that apply a uniform denoising process across the entire image, MoEDiff-SR dynamically selects specialized denoising experts at a fine-grained token level, ensuring region-specific adaptation and enhanced SR performance. Specifically, our approach first employs a Transformer-based feature extractor to compute multi-scale patch embeddings, capturing both global structural information and local texture details. The extracted feature embeddings are then fed into an MoE gating network, which assigns adaptive weights to multiple diffusion-based denoisers, each specializing in different brain MRI characteristics, such as centrum semiovale, sulcal and gyral cortex, and grey-white matter junction. The final output is produced by aggregating the denoised results from these specialized experts according to dynamically assigned gating probabilities. Experimental results demonstrate that MoEDiff-SR outperforms existing state-of-the-art methods in terms of quantitative image quality metrics, perceptual fidelity, and computational efficiency. Difference maps from each expert further highlight their distinct specializations, confirming the effective region-specific denoising capability and the interpretability of expert contributions. Additionally, clinical evaluation validates its superior diagnostic capability in identifying subtle pathological features, emphasizing its practical relevance in clinical neuroimaging. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ZWang78/MoEDiff-SR">https://github.com/ZWang78/MoEDiff-SR</a>. </p>
<blockquote>
<p>åœ¨ä½åœºå¼ºï¼ˆä¾‹å¦‚3Tï¼‰çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­ï¼Œå…¶ç©ºé—´åˆ†è¾¨ç‡æœ‰é™ï¼Œéš¾ä»¥æ•è·å¯¹ä¸´åºŠè¯Šæ–­å’Œç¥ç»å½±åƒå­¦ç ”ç©¶è‡³å…³é‡è¦çš„ç²¾ç»†è§£å‰–ç»“æ„ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MoEDiff-SRï¼Œè¿™æ˜¯ä¸€ç§å—ä¸“å®¶æ··åˆï¼ˆMoEï¼‰å¼•å¯¼çš„åŒºåŸŸè‡ªé€‚åº”MRIè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ‰©æ•£æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ‰©æ•£çš„SRæ¨¡å‹ä¸åŒï¼Œè¿™äº›æ¨¡å‹åœ¨æ•´ä¸ªå›¾åƒä¸Šåº”ç”¨ç»Ÿä¸€çš„å»å™ªè¿‡ç¨‹ï¼ŒMoEDiff-SRåœ¨ç»†ç²’åº¦æ ‡è®°çº§åˆ«åŠ¨æ€é€‰æ‹©ä¸“ä¸šå»å™ªä¸“å®¶ï¼Œç¡®ä¿åŒºåŸŸç‰¹å®šçš„é€‚åº”æ€§å’Œå¢å¼ºçš„SRæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨åŸºäºTransformerçš„ç‰¹å¾æå–å™¨æ¥è®¡ç®—å¤šå°ºåº¦è¡¥ä¸åµŒå…¥ï¼Œæ•è·å…¨å±€ç»“æ„ä¿¡æ¯å’Œå±€éƒ¨çº¹ç†ç»†èŠ‚ã€‚æå–çš„ç‰¹å¾åµŒå…¥éšåè¢«è¾“å…¥åˆ°MoEé—¨æ§ç½‘ç»œä¸­ï¼Œè¯¥ç½‘ç»œä¸ºå¤šä¸ªåŸºäºæ‰©æ•£çš„å»å™ªå™¨åˆ†é…è‡ªé€‚åº”æƒé‡ï¼Œæ¯ä¸ªå»å™ªå™¨éƒ½ä¸“é—¨å¤„ç†ä¸åŒçš„è„‘éƒ¨MRIç‰¹å¾ï¼Œå¦‚ä¸­å¿ƒåŠåµåœ†ä¸­å¿ƒã€æ²Ÿå’Œå›çŠ¶çš®å±‚ä»¥åŠç°ç™½è´¨äº¤ç•Œå¤„ã€‚æœ€ç»ˆè¾“å‡ºæ˜¯é€šè¿‡æ ¹æ®åŠ¨æ€åˆ†é…çš„é—¨æ§æ¦‚ç‡èšåˆè¿™äº›ä¸“ä¸šä¸“å®¶çš„å»å™ªç»“æœè€Œäº§ç”Ÿçš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoEDiff-SRåœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ã€æ„ŸçŸ¥ä¿çœŸåº¦å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ¥è‡ªæ¯ä¸ªä¸“å®¶çš„å·®å¼‚å›¾è¿›ä¸€æ­¥çªå‡ºäº†å®ƒä»¬çš„ç‰¹æ®Šä¸“é•¿ï¼Œè¯å®äº†æœ‰æ•ˆçš„åŒºåŸŸç‰¹å®šå»å™ªèƒ½åŠ›å’Œä¸“å®¶è´¡çŒ®çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œä¸´åºŠè¯„ä¼°éªŒè¯äº†å…¶åœ¨è¯†åˆ«ç»†å¾®ç—…ç†ç‰¹å¾æ–¹é¢çš„å“è¶Šè¯Šæ–­èƒ½åŠ›ï¼Œå¼ºè°ƒå…¶åœ¨ä¸´åºŠç¥ç»å½±åƒå­¦ä¸­çš„å®é™…ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/ZWang78/MoEDiff-SR">https://github.com/ZWang78/MoEDiff-SR</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07308v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MoEDiff-SRæ˜¯ä¸€ç§åŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰å¼•å¯¼çš„åŒºåŸŸè‡ªé€‚åº”MRIè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯ï¼Œè§£å†³äº†ä½ç£åœºå¼ºåº¦MRIç©ºé—´åˆ†è¾¨ç‡å—é™çš„é—®é¢˜ã€‚å®ƒé‡‡ç”¨åŠ¨æ€é€‰æ‹©ç‰¹å®šå»å™ªä¸“å®¶çš„æ–¹å¼ï¼Œç¡®ä¿åŒºåŸŸé€‚åº”æ€§å’Œå¢å¼ºSRæ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoEDiff-SRåœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ã€æ„ŸçŸ¥ä¿çœŸåº¦å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoEDiff-SRè§£å†³äº†ä½åœºå¼ºMRIç©ºé—´åˆ†è¾¨ç‡ä½çš„é—®é¢˜ï¼Œè¯¥æŠ€æœ¯æœ‰åŠ©äºæé«˜åŒºåŸŸè‡ªé€‚åº”çš„MRIè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ã€‚</li>
<li>MoEDiff-SRåˆ©ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰çš„æ–¹æ³•åŠ¨æ€é€‰æ‹©å»å™ªä¸“å®¶ï¼Œä¸åŒäºä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹å¯¹å…¨å›¾è¿›è¡Œç»Ÿä¸€å¤„ç†ã€‚</li>
<li>æŠ€æœ¯åˆ©ç”¨è½¬æ¢å™¨ç‰¹å¾æå–å™¨è®¡ç®—å¤šå°ºåº¦æ–‘å—åµŒå…¥ï¼Œæ•è·å…¨å±€ç»“æ„ä¿¡æ¯å’Œå±€éƒ¨çº¹ç†ç»†èŠ‚ã€‚</li>
<li>é€šè¿‡MoEé—¨æ§ç½‘ç»œåˆ†é…æƒé‡ç»™å¤šä¸ªæ‰©æ•£å»å™ªå™¨ï¼Œé’ˆå¯¹MRIä¸åŒç‰¹æ€§ï¼ˆå¦‚å¤§è„‘çš„ä¸åŒéƒ¨ä½ï¼‰æœ‰ä¸åŒçš„å»å™ªå™¨è´Ÿè´£ã€‚</li>
<li>ä¸“å®¶åˆ†å·¥äº§ç”Ÿçš„é«˜è´¨é‡å·®å¼‚æ˜ å°„æ­ç¤ºäº†å„é¢†åŸŸçš„ä¸“é•¿ï¼Œå¢å¼ºäº†åŒºåŸŸç‰¹å®šå»å™ªèƒ½åŠ›å’Œä¸“å®¶è´¡çŒ®çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ä¸´åºŠè¯„ä¼°éªŒè¯äº†å…¶åœ¨è¯†åˆ«ç»†å¾®ç—…ç†ç‰¹å¾æ–¹é¢çš„å“è¶Šè¯Šæ–­èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨ä¸´åºŠç¥ç»å½±åƒä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71d5e7d1e3acaa865308d79b8e12dad1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edaf1b9cdaa26826196657a69e225f67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f60971dcbce1ef3842695d5700d88422.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5376e986068f2477053c7d5ef8d9d2cd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="nnLandmark-A-Self-Configuring-Method-for-3D-Medical-Landmark-Detection"><a href="#nnLandmark-A-Self-Configuring-Method-for-3D-Medical-Landmark-Detection" class="headerlink" title="nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection"></a>nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection</h2><p><strong>Authors:Alexandra Ertl, Shuhan Xiao, Stefan Denner, Robin Peretzke, David Zimmerer, Peter Neher, Fabian Isensee, Klaus Maier-Hein</strong></p>
<p>Landmark detection plays a crucial role in medical imaging tasks that rely on precise spatial localization, including specific applications in diagnosis, treatment planning, image registration, and surgical navigation. However, manual annotation is labor-intensive and requires expert knowledge. While deep learning shows promise in automating this task, progress is hindered by limited public datasets, inconsistent benchmarks, and non-standardized baselines, restricting reproducibility, fair comparisons, and model generalizability. This work introduces nnLandmark, a self-configuring deep learning framework for 3D medical landmark detection, adapting nnU-Net to perform heatmap-based regression. By leveraging nnU-Netâ€™s automated configuration, nnLandmark eliminates the need for manual parameter tuning, offering out-of-the-box usability. It achieves state-of-the-art accuracy across two public datasets, with a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML) dental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset (AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm. With its strong generalization, reproducibility, and ease of deployment, nnLandmark establishes a reliable baseline for 3D landmark detection, supporting research in anatomical localization and clinical workflows that depend on precise landmark identification. The code will be available soon. </p>
<blockquote>
<p>åœ°æ ‡æ£€æµ‹åœ¨ä¾èµ–ç²¾ç¡®ç©ºé—´å®šä½çš„åŒ»å­¦æˆåƒä»»åŠ¡ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼ŒåŒ…æ‹¬è¯Šæ–­ã€æ²»ç–—è®¡åˆ’ã€å›¾åƒæ³¨å†Œå’Œæ‰‹æœ¯å¯¼èˆªç­‰ç‰¹å®šåº”ç”¨ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æ ‡æ³¨æ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†å‹å·¥ä½œï¼Œéœ€è¦ä¸“ä¸šçŸ¥è¯†ã€‚æ·±åº¦å­¦ä¹ è™½ç„¶æœ‰æœ›è‡ªåŠ¨åŒ–è¿™é¡¹ä»»åŠ¡ï¼Œä½†å…¬å…±æ•°æ®é›†çš„å±€é™æ€§ã€åŸºå‡†æµ‹è¯•çš„ä¸ä¸€è‡´æ€§ä»¥åŠéæ ‡å‡†åŒ–çš„åŸºçº¿é™åˆ¶äº†å¯é‡å¤æ€§ã€å…¬å¹³æ¯”è¾ƒå’Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†nnLandmarkï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº3DåŒ»å­¦åœ°æ ‡æ£€æµ‹çš„è‡ªæˆ‘é…ç½®çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨nnU-Netè¿›è¡ŒåŸºäºçƒ­å›¾çš„å›å½’ã€‚é€šè¿‡åˆ©ç”¨nnU-Netçš„è‡ªåŠ¨é…ç½®ï¼ŒnnLandmarkæ¶ˆé™¤äº†å¯¹æ‰‹åŠ¨å‚æ•°è°ƒæ•´çš„éœ€æ±‚ï¼Œæä¾›äº†å¼€ç®±å³ç”¨çš„å¯ç”¨æ€§ã€‚å®ƒåœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œåœ¨ä¸‹é¢Œç£¨ç‰™åœ°æ ‡ï¼ˆMMLï¼‰ç‰™ç§‘CTæ•°æ®é›†ä¸Šçš„å¹³å‡å¾„å‘è¯¯å·®ï¼ˆMREï¼‰ä¸º1.5æ¯«ç±³ï¼Œåœ¨åŸºäºMRIçš„è§£å‰–æ ‡è®°æ•°æ®é›†ï¼ˆAFIDsï¼‰ä¸Šçš„å¹³å‡å¾„å‘è¯¯å·®ä¸º1.2æ¯«ç±³ï¼Œå…¶ä¸­nnLandmarkä¸1.5æ¯«ç±³çš„åŒ»å¸ˆé—´å˜å¼‚åº¦ä¸€è‡´ã€‚å‡­å€Ÿå…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€å¯é‡å¤æ€§å’Œæ˜“äºéƒ¨ç½²çš„ç‰¹ç‚¹ï¼ŒnnLandmarkä¸º3Dåœ°æ ‡æ£€æµ‹å»ºç«‹äº†å¯é çš„åŸºçº¿ï¼Œæ”¯æŒä¾èµ–ç²¾ç¡®åœ°æ ‡è¯†åˆ«çš„è§£å‰–å®šä½å’Œä¸´åºŠå·¥ä½œæµç¨‹çš„ç ”ç©¶ã€‚ä»£ç å¾ˆå¿«å°†å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06742v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒä¸­çš„åœ°æ ‡æ£€æµ‹åœ¨è¯Šæ–­ã€æ²»ç–—è®¡åˆ’ã€å›¾åƒæ³¨å†Œå’Œæ‰‹æœ¯å¯¼èˆªç­‰åº”ç”¨ä¸­å…·æœ‰å…³é”®ä½œç”¨ã€‚æ‰‹åŠ¨æ ‡æ³¨æ˜¯ä¸€é¡¹è€—æ—¶ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„å·¥ä½œã€‚æ·±åº¦å­¦ä¹ åœ¨åœ°æ ‡è‡ªåŠ¨æ£€æµ‹ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å—åˆ°å…¬å¼€æ•°æ®é›†é™åˆ¶ç­‰å› ç´ å½±å“è¿›å±•ã€‚æœ¬æ–‡ä»‹ç»çš„nnLandmarkæ¡†æ¶é‡‡ç”¨æ·±åº¦å­¦ä¹ è‡ªé€‚åº”é…ç½®æŠ€æœ¯ï¼Œå®ç°åŒ»å­¦å›¾åƒä¸­ä¸‰ç»´åœ°æ ‡æ£€æµ‹çš„è‡ªåŠ¨åŒ–é…ç½®ï¼Œå¹¶å®ç°äº†é«˜ç²¾åº¦æ•ˆæœã€‚å…¶åœ¨ç‰™ç§‘CTå’Œè„‘éƒ¨MRIæ•°æ®é›†ä¸Šçš„å¹³å‡å¾„å‘è¯¯å·®è¾¾åˆ°äº†è¡Œä¸šé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ°æ ‡æ£€æµ‹åœ¨åŒ»å­¦æˆåƒä¸­å…·æœ‰å…³é”®ä½œç”¨ï¼Œå¯¹äºä¾èµ–ç²¾ç¡®ç©ºé—´å®šä½çš„åº”ç”¨å°¤ä¸ºé‡è¦ã€‚</li>
<li>æ‰‹åŠ¨æ ‡æ³¨æ˜¯ä¸€ä¸ªè€—æ—¶çš„è¿‡ç¨‹ï¼Œå¹¶ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œç»éªŒã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨åœ°æ ‡è‡ªåŠ¨æ£€æµ‹é¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œä½†å—é™äºå…¬å¼€æ•°æ®é›†ä¸è¶³å’Œç¼ºä¹æ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>nnLandmarkæ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªé€‚åº”é…ç½®æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒä¸­çš„ä¸‰ç»´åœ°æ ‡æ£€æµ‹ã€‚</li>
<li>nnLandmarkæ¶ˆé™¤äº†æ‰‹åŠ¨å‚æ•°è°ƒæ•´çš„éœ€è¦ï¼Œæé«˜äº†æ˜“ç”¨æ€§ã€‚</li>
<li>nnLandmarkåœ¨ç‰™ç§‘CTå’Œè„‘éƒ¨MRIæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35832a036e694321a13b138e8fdfc5ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56f31d8e17987f5df6e3c2a26e275812.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ae1e50ebedff6992dd74c47878d1260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0ecc339981d64879190465ae20ee5dd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Subjective-Visual-Quality-Assessment-for-High-Fidelity-Learning-Based-Image-Compression"><a href="#Subjective-Visual-Quality-Assessment-for-High-Fidelity-Learning-Based-Image-Compression" class="headerlink" title="Subjective Visual Quality Assessment for High-Fidelity Learning-Based   Image Compression"></a>Subjective Visual Quality Assessment for High-Fidelity Learning-Based   Image Compression</h2><p><strong>Authors:Mohsen Jenadeleh, Jon Sneyers, Panqi Jia, Shima Mohammadi, Joao Ascenso, Dietmar Saupe</strong></p>
<p>Learning-based image compression methods have recently emerged as promising alternatives to traditional codecs, offering improved rate-distortion performance and perceptual quality. JPEG AI represents the latest standardized framework in this domain, leveraging deep neural networks for high-fidelity image reconstruction. In this study, we present a comprehensive subjective visual quality assessment of JPEG AI-compressed images using the JPEG AIC-3 methodology, which quantifies perceptual differences in terms of Just Noticeable Difference (JND) units. We generated a dataset of 50 compressed images with fine-grained distortion levels from five diverse sources. A large-scale crowdsourced experiment collected 96,200 triplet responses from 459 participants. We reconstructed JND-based quality scales using a unified model based on boosted and plain triplet comparisons. Additionally, we evaluated the alignment of multiple objective image quality metrics with human perception in the high-fidelity range. The CVVDP metric achieved the overall highest performance; however, most metrics including CVVDP were overly optimistic in predicting the quality of JPEG AI-compressed images. These findings emphasize the necessity for rigorous subjective evaluations in the development and benchmarking of modern image codecs, particularly in the high-fidelity range. Another technical contribution is the introduction of the well-known Meng-Rosenthal-Rubin statistical test to the field of Quality of Experience research. This test can reliably assess the significance of difference in performance of quality metrics in terms of correlation between metrics and ground truth. The complete dataset, including all subjective scores, is publicly available at <a target="_blank" rel="noopener" href="https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25">https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25</a>. </p>
<blockquote>
<p>åŸºäºå­¦ä¹ çš„å›¾åƒå‹ç¼©æ–¹æ³•ä½œä¸ºä¼ ç»Ÿç¼–ç å™¨çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ï¼Œæä¾›äº†æ”¹è¿›çš„é€Ÿç‡å¤±çœŸæ€§èƒ½å’Œæ„ŸçŸ¥è´¨é‡ã€‚JPEG AIä»£è¡¨æ­¤é¢†åŸŸçš„æœ€æ–°æ ‡å‡†åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œé«˜ä¿çœŸå›¾åƒé‡å»ºã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨JPEG AIC-3æ–¹æ³•è®ºå¯¹JPEG AIå‹ç¼©å›¾åƒè¿›è¡Œä¸»è§‚è§†è§‰è´¨é‡è¯„ä¼°ï¼Œè¯¥æ–¹æ³•ä»¥åˆšåˆšå¯å¯Ÿè§‰å·®å¼‚ï¼ˆJNDï¼‰å•ä½é‡åŒ–æ„ŸçŸ¥å·®å¼‚ã€‚æˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«æ¥è‡ªäº”ä¸ªä¸åŒæºçš„50å¼ å‹ç¼©å›¾åƒçš„æ•°æ®é›†ï¼Œè¿™äº›å›¾åƒå…·æœ‰ç²¾ç»†çš„å¤±çœŸçº§åˆ«ã€‚å¤§è§„æ¨¡çš„ä¼—åŒ…å®éªŒæ”¶é›†äº†æ¥è‡ª459åå‚ä¸è€…çš„96,200ä¸ªä¸‰å…ƒç»„å“åº”ã€‚æˆ‘ä»¬åŸºäºå¢å¼ºçš„å’Œæ™®é€šä¸‰å…ƒç»„æ¯”è¾ƒé‡å»ºäº†åŸºäºJNDçš„è´¨é‡é‡è¡¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†å¤šä¸ªå®¢è§‚å›¾åƒè´¨é‡æŒ‡æ ‡ä¸é«˜ä¿çœŸèŒƒå›´å†…çš„äººç±»æ„ŸçŸ¥çš„ä¸€è‡´æ€§ã€‚CVVDPæŒ‡æ ‡æ€»ä½“æ€§èƒ½æœ€ä½³ï¼›ç„¶è€Œï¼ŒåŒ…æ‹¬CVVDPåœ¨å†…çš„å¤§å¤šæ•°æŒ‡æ ‡åœ¨é¢„æµ‹JPEG AIå‹ç¼©å›¾åƒçš„è´¨é‡æ—¶è¿‡äºä¹è§‚ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨ç°ä»£å›¾åƒç¼–ç å™¨çš„å¼€å‘å’ŒåŸºå‡†æµ‹è¯•ä¸­ä¸¥æ ¼è¿›è¡Œä¸»è§‚è¯„ä»·çš„å¿…è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ä¿çœŸèŒƒå›´å†…ã€‚å¦ä¸€ä¸ªæŠ€æœ¯è´¡çŒ®æ˜¯å°†è‘—åçš„Meng-Rosenthal-Rubinç»Ÿè®¡æµ‹è¯•å¼•å…¥åˆ°ä½“éªŒè´¨é‡ç ”ç©¶é¢†åŸŸã€‚è¯¥æµ‹è¯•å¯ä»¥å¯é åœ°è¯„ä¼°è´¨é‡æŒ‡æ ‡æ€§èƒ½å·®å¼‚çš„æ˜¾è‘—æ€§ï¼Œä»¥æŒ‡æ ‡ä¸åŸºå‡†ä¹‹é—´çš„ç›¸å…³æ€§ä¸ºå‡†ã€‚æ‰€æœ‰ä¸»è§‚åˆ†æ•°å‡åŒ…å«åœ¨å®Œæ•´æ•°æ®é›†ä¸­ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25å…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06301v2">PDF</a> 7 pages, 5 figures, 3 tables, submitted to QoMEX 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå­¦ä¹ çš„å›¾åƒå‹ç¼©æ–¹æ³•å·²æˆä¸ºä¼ ç»Ÿç¼–ç æŠ€æœ¯çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæä¾›äº†æ”¹è¿›çš„é€Ÿç‡å¤±çœŸæ€§èƒ½å’Œæ„ŸçŸ¥è´¨é‡ã€‚æœ¬ç ”ç©¶å¯¹JPEG AIå‹ç¼©å›¾åƒè¿›è¡Œäº†å…¨é¢çš„ä¸»è§‚è§†è§‰è´¨é‡è¯„ä¼°ï¼Œé‡‡ç”¨JPEG AIC-3æ–¹æ³•è®ºé‡åŒ–æ„ŸçŸ¥å·®å¼‚ã€‚å®éªŒç”Ÿæˆäº†50å¼ å…·æœ‰ç²¾ç»†å¤±çœŸçº§åˆ«çš„å‹ç¼©å›¾åƒæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡ä¼—åŒ…å®éªŒæ”¶é›†äº†å‚ä¸è€…çš„ååº”ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨é«˜è´¨é‡èŒƒå›´å†…ï¼Œå¤§å¤šæ•°å®¢è§‚å›¾åƒè´¨é‡æŒ‡æ ‡åœ¨é¢„æµ‹JPEG AIå‹ç¼©å›¾åƒè´¨é‡æ—¶è¿‡äºä¹è§‚ã€‚å› æ­¤ï¼Œåœ¨ç°ä»£å›¾åƒç¼–ç æŠ€æœ¯å¼€å‘å’Œè¯„ä¼°ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜è´¨é‡èŒƒå›´å†…ï¼Œéœ€è¦ä¸¥æ ¼çš„å®¢è§‚è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å°†Meng-Rosenthal-Rubinç»Ÿè®¡æµ‹è¯•å¼•å…¥ç”¨æˆ·ä½“éªŒç ”ç©¶é¢†åŸŸï¼Œä»¥è¯„ä¼°è´¨é‡æŒ‡æ ‡æ€§èƒ½å·®å¼‚çš„æ˜¾è‘—æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å­¦ä¹ å‹å›¾åƒå‹ç¼©æ–¹æ³•å·²æ˜¾ç¤ºå‡ºå¯¹ä¼ ç»Ÿç¼–ç æŠ€æœ¯çš„ä¼˜åŠ¿ï¼Œè¡¨ç°åœ¨æ”¹å–„é€Ÿç‡å¤±çœŸæ€§èƒ½å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ã€‚</li>
<li>JPEG AIä»£è¡¨è¯¥é¢†åŸŸçš„æœ€æ–°æ ‡å‡†åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œé«˜ä¿çœŸå›¾åƒé‡å»ºã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡JPEG AIC-3æ–¹æ³•è®ºå¯¹JPEG AIå‹ç¼©å›¾åƒè¿›è¡Œäº†å…¨é¢çš„ä¸»è§‚è§†è§‰è´¨é‡è¯„ä¼°ã€‚</li>
<li>ç”Ÿæˆäº†åŒ…å«50å¼ å…·æœ‰ç²¾ç»†å¤±çœŸçº§åˆ«çš„å‹ç¼©å›¾åƒæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡ä¼—åŒ…å®éªŒæ”¶é›†å‚ä¸è€…ååº”ã€‚</li>
<li>åœ¨é«˜ä¿çœŸèŒƒå›´å†…ï¼Œå¤§å¤šæ•°å®¢è§‚å›¾åƒè´¨é‡æŒ‡æ ‡é¢„æµ‹JPEG AIå‹ç¼©å›¾åƒè´¨é‡æ—¶è¡¨ç°è¿‡äºä¹è§‚ã€‚</li>
<li>ä¸¥æ ¼çš„å®¢è§‚è¯„ä¼°å¯¹äºç°ä»£å›¾åƒç¼–ç æŠ€æœ¯çš„å¼€å‘å’Œè¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥Meng-Rosenthal-Rubinç»Ÿè®¡æµ‹è¯•æ¥è¯„ä¼°è´¨é‡æŒ‡æ ‡æ€§èƒ½å·®å¼‚çš„æ˜¾è‘—æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-43e7ca7bba9b58eb2dcb522c6a416e1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51985f0939e88877aabe62b8bd3d75b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54445f47a8fefce6c0a69928f39b8bc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6696ab71fc0970d034d8eabbb6251c85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45a0c29158a5e45bc05eea1a3c5844c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbddc62196b060ab2fa726ac7d817554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90e057c528def1f8d0b0d675f1495866.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77ccdcf9639a9439b76e1d36f5ceca67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b763085b9aa6e19cb69ebffa151663b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Soybean-Disease-Detection-via-Interpretable-Hybrid-CNN-GNN-Integrating-MobileNetV2-and-GraphSAGE-with-Cross-Modal-Attention"><a href="#Soybean-Disease-Detection-via-Interpretable-Hybrid-CNN-GNN-Integrating-MobileNetV2-and-GraphSAGE-with-Cross-Modal-Attention" class="headerlink" title="Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating   MobileNetV2 and GraphSAGE with Cross-Modal Attention"></a>Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating   MobileNetV2 and GraphSAGE with Cross-Modal Attention</h2><p><strong>Authors:Md Abrar Jahin, Soudeep Shahriar, M. F. Mridha, Md. Jakir Hossen, Nilanjan Dey</strong></p>
<p>Soybean leaf disease detection is critical for agricultural productivity but faces challenges due to visually similar symptoms and limited interpretability in conventional methods. While Convolutional Neural Networks (CNNs) excel in spatial feature extraction, they often neglect inter-image relational dependencies, leading to misclassifications. This paper proposes an interpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that synergizes MobileNetV2 for localized feature extraction and GraphSAGE for relational modeling. The framework constructs a graph where nodes represent leaf images, with edges defined by cosine similarity-based adjacency matrices and adaptive neighborhood sampling. This design captures fine-grained lesion features and global symptom patterns, addressing inter-class similarity challenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM visualizations, generating heatmaps to highlight disease-influential regions. Evaluated on a dataset of ten soybean leaf diseases, the model achieves $97.16%$ accuracy, surpassing standalone CNNs ($\le95.04%$) and traditional machine learning models ($\le77.05%$). Ablation studies validate the sequential architectureâ€™s superiority over parallel or single-model configurations. With only 2.3 million parameters, the lightweight MobileNetV2-GraphSAGE combination ensures computational efficiency, enabling real-time deployment in resource-constrained environments. The proposed approach bridges the gap between accurate classification and practical applicability, offering a robust, interpretable tool for agricultural diagnostics while advancing CNN-GNN integration in plant pathology research. </p>
<blockquote>
<p>å¤§è±†å¶ç—…æ£€æµ‹å¯¹å†œä¸šç”Ÿäº§åŠ›è‡³å…³é‡è¦ï¼Œä½†ç”±äºç—‡çŠ¶è§†è§‰ç›¸ä¼¼æ€§å’Œä¼ ç»Ÿæ–¹æ³•çš„æœ‰é™è§£é‡Šæ€§ï¼Œå®ƒé¢ä¸´ç€æŒ‘æˆ˜ã€‚å°½ç®¡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨ç©ºé—´ç‰¹å¾æå–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†å›¾åƒé—´çš„å…³ç³»ä¾èµ–æ€§ï¼Œä»è€Œå¯¼è‡´è¯¯åˆ†ç±»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„æ··åˆåºè´¯CNN-å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒMobileNetV2è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–å’ŒGraphSAGEè¿›è¡Œå…³ç³»å»ºæ¨¡ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å¶å›¾åƒï¼Œè¾¹ç”±åŸºäºä½™å¼¦ç›¸ä¼¼æ€§çš„é‚»æ¥çŸ©é˜µå’Œè‡ªé€‚åº”é‚»åŸŸé‡‡æ ·å®šä¹‰ã€‚è¿™ç§è®¾è®¡æ•æ‰äº†ç²¾ç»†çš„ç—…å˜ç‰¹å¾å’Œå…¨å±€ç—‡çŠ¶æ¨¡å¼ï¼Œè§£å†³äº†ç±»é—´ç›¸ä¼¼æ€§çš„æŒ‘æˆ˜ã€‚é€šè¿‡Grad-CAMå’ŒEigen-CAMå¯è§†åŒ–å®ç°è·¨æ¨¡æ€è§£é‡Šæ€§ï¼Œç”Ÿæˆçƒ­å›¾ä»¥çªå‡ºæ˜¾ç¤ºå½±å“ç–¾ç—…çš„åŒºåŸŸã€‚åœ¨åŒ…å«åç§å¤§è±†å¶ç—…çš„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº†97.16%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†å•ç‹¬çš„CNNï¼ˆâ‰¤95.04%ï¼‰å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆâ‰¤77.05%ï¼‰ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†åºè´¯æ¶æ„ä¼˜äºå¹¶è¡Œæˆ–å•ä¸€æ¨¡å‹é…ç½®ã€‚ä»…æœ‰230ä¸‡ä¸ªå‚æ•°çš„è½»é‡çº§MobileNetV2-GraphSAGEç»„åˆç¡®ä¿äº†è®¡ç®—æ•ˆç‡ï¼Œå¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°å®æ—¶éƒ¨ç½²ã€‚æ‰€æå‡ºçš„æ–¹æ³•å¼¥åˆäº†å‡†ç¡®åˆ†ç±»ä¸å®é™…é€‚ç”¨ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºå†œä¸šè¯Šæ–­æä¾›äº†ä¸€ä¸ªç¨³å¥ã€å¯è§£é‡Šçš„å·¥å…·ï¼ŒåŒæ—¶æ¨åŠ¨äº†CNN-GNNåœ¨æ¤ç‰©ç—…ç†å­¦ç ”ç©¶ä¸­çš„é›†æˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01284v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å¯è§£é‡Šçš„æ··åˆåºè´¯CNN-å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¡†æ¶ï¼Œç”¨äºå¤§è±†å¶ç—…æ£€æµ‹ã€‚è¯¥æ¡†æ¶ç»“åˆMobileNetV2è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–å’ŒGraphSAGEè¿›è¡Œå…³ç³»å»ºæ¨¡ï¼Œé€šè¿‡æ„å»ºå›¾åƒå›¾æ¥æ•æ‰ç²¾ç»†ç—…å˜ç‰¹å¾å’Œå…¨å±€ç—‡çŠ¶æ¨¡å¼ï¼Œè§£å†³ç±»é—´ç›¸ä¼¼æ€§çš„æŒ‘æˆ˜ã€‚æ¨¡å‹åœ¨åç§å¤§è±†å¶ç—…æ•°æ®é›†ä¸Šå–å¾—äº†97.16%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šå•ä¸€CNNå’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å…¶æ¶æ„å…·æœ‰è½»é‡ã€é«˜æ•ˆçš„ç‰¹ç‚¹ï¼Œé€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒã€‚è¯¥æ¨¡å‹ä¸ºå†œä¸šè¯Šæ–­æä¾›äº†ä¸€ä¸ªå¼ºå¤§ã€å¯è§£é‡Šçš„å·¥å…·ï¼Œæ¨åŠ¨äº†CNN-GNNåœ¨æ¤ç‰©ç—…ç†å­¦ç ”ç©¶ä¸­çš„èåˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æ··åˆåºè´¯CNN-å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¡†æ¶ç”¨äºå¤§è±†å¶ç—…æ£€æµ‹ã€‚</li>
<li>ç»“åˆMobileNetV2è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–å’ŒGraphSAGEè¿›è¡Œå…³ç³»å»ºæ¨¡ã€‚</li>
<li>é€šè¿‡æ„å»ºå›¾åƒå›¾æ¥æ•æ‰ç²¾ç»†ç—…å˜ç‰¹å¾å’Œå…¨å±€ç—‡çŠ¶æ¨¡å¼ã€‚</li>
<li>æ¨¡å‹å®ç°äº†è·¨æ¨¡æ€è§£é‡Šæ€§ï¼Œé€šè¿‡Grad-CAMå’ŒEigen-CAMå¯è§†åŒ–ç”Ÿæˆçƒ­å›¾çªå‡ºç–¾ç—…å½±å“åŒºåŸŸã€‚</li>
<li>åœ¨åç§å¤§è±†å¶ç—…æ•°æ®é›†ä¸Šå–å¾—äº†97.16%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šå…¶ä»–æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹å…·æœ‰è½»é‡ã€é«˜æ•ˆçš„ç‰¹ç‚¹ï¼Œé€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57302d14a803e8c297c4ebbf948bfa45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73b153e735671da03a7e80b1073f6c96.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MedCT-A-Clinical-Terminology-Graph-for-Generative-AI-Applications-in-Healthcare"><a href="#MedCT-A-Clinical-Terminology-Graph-for-Generative-AI-Applications-in-Healthcare" class="headerlink" title="MedCT: A Clinical Terminology Graph for Generative AI Applications in   Healthcare"></a>MedCT: A Clinical Terminology Graph for Generative AI Applications in   Healthcare</h2><p><strong>Authors:Ye Chen, Dongdong Huang, Haoyun Xu, Cong Fu, Lin Sheng, Qingli Zhou, Yuqiang Shen, Kai Wang</strong></p>
<p>We introduce the worldâ€™s first clinical terminology for the Chinese healthcare community, namely MedCT, accompanied by a clinical foundation model MedBERT and an entity linking model MedLink. The MedCT system enables standardized and programmable representation of Chinese clinical data, successively stimulating the development of new medicines, treatment pathways, and better patient outcomes for the populous Chinese community. Moreover, the MedCT knowledge graph provides a principled mechanism to minimize the hallucination problem of large language models (LLMs), therefore achieving significant levels of accuracy and safety in LLM-based clinical applications. By leveraging the LLMsâ€™ emergent capabilities of generativeness and expressiveness, we were able to rapidly built a production-quality terminology system and deployed to real-world clinical field within three months, while classical terminologies like SNOMED CT have gone through more than twenty years development. Our experiments show that the MedCT system achieves state-of-the-art (SOTA) performance in semantic matching and entity linking tasks, not only for Chinese but also for English. We also conducted a longitudinal field experiment by applying MedCT and LLMs in a representative spectrum of clinical tasks, including electronic health record (EHR) auto-generation and medical document search for diagnostic decision making. Our study shows a multitude of values of MedCT for clinical workflows and patient outcomes, especially in the new genre of clinical LLM applications. We present our approach in sufficient engineering detail, such that implementing a clinical terminology for other non-English societies should be readily reproducible. We openly release our terminology, models and algorithms, along with real-world clinical datasets for the development. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºä¸­æ–‡åŒ»ç–—ç¤¾åŒºå¼•å…¥äº†ä¸–ç•Œä¸Šé¦–ä¸ªä¸´åºŠæœ¯è¯­â€”â€”MedCTï¼Œå®ƒé…å¤‡äº†ä¸´åºŠåŸºç¡€æ¨¡å‹MedBERTå’Œå®ä½“é“¾æ¥æ¨¡å‹MedLinkã€‚MedCTç³»ç»Ÿèƒ½å¤Ÿå®ç°ä¸­æ–‡ä¸´åºŠæ•°æ®çš„æ ‡å‡†åŒ–å’Œå¯ç¼–ç¨‹è¡¨ç¤ºï¼Œè¿›è€Œåˆºæ¿€æ–°è¯ç ”å‘ã€æ²»ç–—è·¯å¾„çš„æ¢ç´¢ï¼Œä¸ºäººå£ä¼—å¤šçš„ä¸­æ–‡ç¤¾åŒºå¸¦æ¥æ›´å¥½çš„æ‚£è€…ç–—æ•ˆã€‚æ­¤å¤–ï¼ŒMedCTçŸ¥è¯†å›¾è°±æä¾›äº†ä¸€ç§æœ‰åŸåˆ™çš„æœºåˆ¶ï¼Œä»¥æœ€å°åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹»è§‰é—®é¢˜ï¼Œä»è€Œåœ¨åŸºäºLLMçš„ä¸´åºŠåº”ç”¨ä¸­å®ç°æ˜¾è‘—æ°´å¹³å’Œå‡†ç¡®æ€§åŠå®‰å…¨æ€§ã€‚é€šè¿‡åˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›å’Œè¡¨ç°åŠ›ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸‰ä¸ªæœˆå†…å¿«é€Ÿæ„å»ºäº†ä¸€ä¸ªç”Ÿäº§è´¨é‡æœ¯è¯­ç³»ç»Ÿå¹¶éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠé¢†åŸŸï¼Œè€ŒåƒSNOMED CTè¿™æ ·çš„ç»å…¸æœ¯è¯­å´ç»å†äº†è¶…è¿‡äºŒåå¹´çš„å‘å±•ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMedCTç³»ç»Ÿåœ¨è¯­ä¹‰åŒ¹é…å’Œå®ä½“é“¾æ¥ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°ï¼ˆSOTAï¼‰çš„æ€§èƒ½ï¼Œä¸ä»…é€‚ç”¨äºä¸­æ–‡ï¼Œä¹Ÿé€‚ç”¨äºè‹±æ–‡ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åœ¨ä¸€ç³»åˆ—å…·æœ‰ä»£è¡¨æ€§çš„ä¸´åºŠä»»åŠ¡ä¸­åº”ç”¨MedCTå’ŒLLMè¿›è¡Œäº†çºµå‘å®åœ°å®éªŒï¼ŒåŒ…æ‹¬è‡ªåŠ¨ç”Ÿæˆç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰å’Œç”¨äºè¯Šæ–­å†³ç­–çš„åŒ»ç–—æ–‡ä»¶æœç´¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒMedCTåœ¨ä¸´åºŠå·¥ä½œæµç¨‹å’Œæ‚£è€…ç–—æ•ˆæ–¹é¢å…·æœ‰å¤šç§ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°å‹ä¸´åºŠLLMåº”ç”¨ä¸­ã€‚æˆ‘ä»¬ä»¥è¶³å¤Ÿçš„å·¥ç¨‹ç»†èŠ‚å‘ˆç°äº†æˆ‘ä»¬æ–¹æ³•ï¼Œä½¿å¾—ä¸ºå…¶ä»–éè‹±è¯­ç¤¾ä¼šå®æ–½ä¸´åºŠæœ¯è¯­åº”è¯¥å¾ˆå®¹æ˜“å¤åˆ¶ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„æœ¯è¯­ã€æ¨¡å‹å’Œç®—æ³•ï¼Œä»¥åŠç”¨äºå¼€å‘çš„çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06465v3">PDF</a> Accepted into ICCS 2025 and published in Springerâ€™s LNCS Series</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä¸­æ–‡åŒ»ç–—ç¤¾åŒºçš„ä¸–ç•Œé¦–ä¸ªä¸´åºŠæœ¯è¯­ç³»ç»ŸMedCTï¼Œå®ƒé…å¤‡äº†ä¸´åºŠåŸºç¡€æ¨¡å‹MedBERTå’Œå®ä½“é“¾æ¥æ¨¡å‹MedLinkã€‚MedCTç³»ç»Ÿèƒ½å¤Ÿå®ç°ä¸­æ–‡ä¸´åºŠæ•°æ®çš„æ ‡å‡†åŒ–å’Œå¯ç¼–ç¨‹è¡¨ç¤ºï¼Œåˆºæ¿€æ–°è¯ç ”å‘ã€æ²»ç–—è·¯å¾„çš„ä¼˜åŒ–ï¼Œä»¥åŠé’ˆå¯¹åºå¤§ä¸­æ–‡ç¤¾åŒºçš„æ›´å¥½æ‚£è€…æ²»ç–—æ•ˆæœçš„å®ç°ã€‚æ­¤å¤–ï¼ŒMedCTçŸ¥è¯†å›¾è°±æä¾›äº†å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹»æƒ³é—®é¢˜çš„åŸåˆ™æœºåˆ¶ï¼Œä»è€Œåœ¨LLMä¸´åºŠåº”ç”¨ä¸­å®ç°äº†é«˜å‡†ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚é€šè¿‡åˆ©ç”¨LLMsçš„ç”Ÿæˆèƒ½åŠ›å’Œè¡¨ç°åŠ›ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸‰ä¸ªæœˆå†…å¿«é€Ÿæ„å»ºç”Ÿäº§è´¨é‡æœ¯è¯­ç³»ç»Ÿå¹¶éƒ¨ç½²åˆ°ç°å®ä¸´åºŠç¯å¢ƒä¸­ï¼Œè€ŒåƒSNOMED CTè¿™æ ·çš„ç»å…¸æœ¯è¯­åˆ™ç»å†äº†è¶…è¿‡äºŒåå¹´çš„å‘å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒMedCTç³»ç»Ÿåœ¨è¯­ä¹‰åŒ¹é…å’Œå®ä½“é“¾æ¥ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¸ä»…é€‚ç”¨äºä¸­æ–‡ï¼Œä¹Ÿé€‚ç”¨äºè‹±æ–‡ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åº”ç”¨MedCTå’ŒLLMsäºä¸€ç³»åˆ—å…·æœ‰ä»£è¡¨æ€§çš„ä¸´åºŠä»»åŠ¡è¿›è¡Œäº†çºµå‘å®åœ°å®éªŒï¼ŒåŒ…æ‹¬è‡ªåŠ¨ç”Ÿæˆç”µå­å¥åº·è®°å½•å’Œç”¨äºè¯Šæ–­å†³ç­–çš„åŒ»ç–—æ–‡ä»¶æœç´¢ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMedCTå¯¹ä¸´åºŠå·¥ä½œæµç¨‹å’Œæ‚£è€…æ²»ç–—æ•ˆæœå…·æœ‰å¤šé‡ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°å‹ä¸´åºŠLLMåº”ç”¨ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰è¶³å¤Ÿçš„å·¥ç¨‹ç»†èŠ‚ï¼Œä½¿å¾—ä¸ºå…¶ä»–éè‹±è¯­ç¤¾ä¼šå®æ–½ä¸´åºŠæœ¯è¯­ç³»ç»Ÿå˜å¾—å®¹æ˜“å¤åˆ¶ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†æœ¯è¯­ã€æ¨¡å‹å’Œç®—æ³•ï¼Œä»¥åŠç”¨äºå¼€å‘çš„çœŸå®ä¸´åºŠæ•°æ®é›†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥é’ˆå¯¹ä¸­æ–‡åŒ»ç–—ç¤¾åŒºçš„ä¸–ç•Œé¦–ä¸ªä¸´åºŠæœ¯è¯­ç³»ç»ŸMedCTï¼Œå…·å¤‡æ ‡å‡†åŒ–å’Œå¯ç¼–ç¨‹è¡¨ç¤ºä¸­æ–‡ä¸´åºŠæ•°æ®çš„èƒ½åŠ›ã€‚</li>
<li>MedCTç³»ç»Ÿåˆºæ¿€æ–°è¯ç ”å‘ã€ä¼˜åŒ–æ²»ç–—è·¯å¾„ï¼Œå¹¶æ”¹å–„æ‚£è€…æ²»ç–—æ•ˆæœã€‚</li>
<li>MedCTçŸ¥è¯†å›¾è°±æœ‰æ•ˆå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»æƒ³é—®é¢˜ï¼Œæé«˜å‡†ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œè¡¨ç°åŠ›ï¼Œå¿«é€Ÿæ„å»ºå¹¶éƒ¨ç½²ç”Ÿäº§è´¨é‡æœ¯è¯­ç³»ç»Ÿã€‚</li>
<li>MedCTç³»ç»Ÿåœ¨è¯­ä¹‰åŒ¹é…å’Œå®ä½“é“¾æ¥ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œé€‚ç”¨äºä¸­æ–‡å’Œè‹±æ–‡ã€‚</li>
<li>å®åœ°å®éªŒè¯æ˜MedCTå¯¹ä¸´åºŠå·¥ä½œæµç¨‹å’Œæ‚£è€…æ²»ç–—æ•ˆæœå…·æœ‰æ˜¾è‘—ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°å‹ä¸´åºŠLLMåº”ç”¨ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d9e1e1d732ae1d679908aff98c7681b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0527702ae758f107304f7fa751aa586d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef68893e5b4951d5a799099e9a9c258a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28c9b99f6287b9ad22ccb57c33937535.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7506299b3b123e5fef53f79946355856.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-160dead930219cfc43eacd0beb96b7fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19720fba9e1b01bf5962b3127fa62bd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-810e1f5549febb67eafb389ac19b7a68.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="BYOCL-Build-Your-Own-Consistent-Latent-with-Hierarchical-Representative-Latent-Clustering"><a href="#BYOCL-Build-Your-Own-Consistent-Latent-with-Hierarchical-Representative-Latent-Clustering" class="headerlink" title="BYOCL: Build Your Own Consistent Latent with Hierarchical Representative   Latent Clustering"></a>BYOCL: Build Your Own Consistent Latent with Hierarchical Representative   Latent Clustering</h2><p><strong>Authors:Jiayue Dai, Yunya Wang, Yihan Fang, Yuetong Chen, Butian Xiong</strong></p>
<p>To address the semantic inconsistency issue with SAM or other single-image segmentation models handling image sequences, we introduce BYOCL. This novel model outperforms SAM in extensive experiments, showcasing its Hierarchical prototype capabilities across CLIP and other representations. BYOCL significantly reduces time and space consumption by dividing inputs into smaller batches, achieving exponential time reduction compared to previous methods. Our approach leverages the SAM image encoder for feature extraction, followed by Intra-Batch and Inter-Batch clustering algorithms. Extensive experiments demonstrate that BYOCL far exceeds the previous state-of-the-art single image segmentation model. Our work is the first to apply consistent segmentation using foundation models without requiring training, utilizing plug-and-play modules for any latent space, making our method highly efficientModels are available at \href{<a target="_blank" rel="noopener" href="https://github.com/cyt1202/BYOCL.git">https://github.com/cyt1202/BYOCL.git</a> </p>
<blockquote>
<p>ä¸ºäº†è§£å†³SAMæˆ–å…¶ä»–å¤„ç†å›¾åƒåºåˆ—çš„å•å›¾åƒåˆ†å‰²æ¨¡å‹ä¸­çš„è¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†BYOCLã€‚è¿™ä¸€æ–°å‹æ¨¡å‹åœ¨å¤§é‡å®éªŒä¸­è¡¨ç°å‡ºä¼˜äºSAMçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨CLIPå’Œå…¶ä»–è¡¨ç¤ºæ³•ä¸­çš„åˆ†å±‚åŸå‹èƒ½åŠ›ã€‚BYOCLé€šè¿‡å°†è¾“å…¥åˆ†æˆè¾ƒå°çš„æ‰¹æ¬¡ï¼Œæ˜¾è‘—å‡å°‘äº†æ—¶é—´å’Œç©ºé—´æ¶ˆè€—ï¼Œä¸å‰äººæ–¹æ³•ç›¸æ¯”å®ç°äº†æŒ‡æ•°çº§çš„æ—¶é—´å‡å°‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨SAMå›¾åƒç¼–ç å™¨è¿›è¡Œç‰¹å¾æå–ï¼Œéšåä½¿ç”¨æ‰¹å†…å’Œæ‰¹é—´èšç±»ç®—æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBYOCLè¿œè¿œè¶…è¿‡äº†ä¹‹å‰çš„æœ€å…ˆè¿›çš„å•å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚æˆ‘ä»¬çš„å·¥ä½œæ˜¯é¦–æ¬¡åº”ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œä¸€è‡´åˆ†å‰²ï¼Œæ— éœ€è®­ç»ƒï¼Œåˆ©ç”¨å³æ’å³ç”¨æ¨¡å—é€‚åº”ä»»ä½•æ½œåœ¨ç©ºé—´ï¼Œä½¿æˆ‘ä»¬çš„æ–¹æ³•é«˜åº¦æœ‰æ•ˆã€‚æ¨¡å‹å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/cyt1202/BYOCL.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/cyt1202/BYOCL.gitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15060v2">PDF</a> 5 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºSAMæˆ–å…¶ä»–å•å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨å¤„ç†å›¾åƒåºåˆ—æ—¶å­˜åœ¨çš„è¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†BYOCLæ¨¡å‹ã€‚è¯¥æ–°å‹æ¨¡å‹åœ¨å¹¿æ³›å®éªŒä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå±•ç°äº†å…¶è·¨CLIPå’Œå…¶ä»–è¡¨å¾çš„åˆ†å±‚åŸå‹èƒ½åŠ›ã€‚BYOCLé€šè¿‡å°†è¾“å…¥åˆ’åˆ†ä¸ºè¾ƒå°çš„æ‰¹æ¬¡ï¼Œæ˜¾è‘—å‡å°‘äº†æ—¶é—´å’Œç©ºé—´æ¶ˆè€—ï¼Œä¸å‰äººæ–¹æ³•ç›¸æ¯”å®ç°äº†æŒ‡æ•°çº§çš„æ—¶é—´å‡å°‘ã€‚è¯¥æ¨¡å‹åˆ©ç”¨SAMå›¾åƒç¼–ç å™¨è¿›è¡Œç‰¹å¾æå–ï¼Œéšåé‡‡ç”¨Intra-Batchå’ŒInter-Batchèšç±»ç®—æ³•ã€‚å¹¿æ³›å®éªŒè¯æ˜ï¼ŒBYOCLè¿œè¶…ä¹‹å‰æœ€å…ˆè¿›çš„å•å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œå¹¶ä¸”æ˜¯é¦–æ¬¡åº”ç”¨æ— éœ€è®­ç»ƒçš„åŸºç¡€æ¨¡å‹è¿›è¡Œä¸€è‡´åˆ†å‰²ï¼Œåˆ©ç”¨ä»»æ„æ½œåœ¨ç©ºé—´çš„å³æ’å³ç”¨æ¨¡å—ï¼Œä½¿è¯¥æ–¹æ³•å…·æœ‰é«˜åº¦é«˜æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BYOCLæ¨¡å‹è§£å†³äº†SAMå’Œå…¶ä»–å•å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨å¤„ç†å›¾åƒåºåˆ—æ—¶çš„è¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>BYOCLé€šè¿‡åˆ’åˆ†è¾“å…¥ä¸ºè¾ƒå°çš„æ‰¹æ¬¡ï¼Œæ˜¾è‘—å‡å°‘äº†æ—¶é—´å’Œç©ºé—´çš„æ¶ˆè€—ã€‚</li>
<li>BYOCLæ¨¡å‹åˆ©ç”¨SAMå›¾åƒç¼–ç å™¨è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶é‡‡ç”¨Intra-Batchå’ŒInter-Batchèšç±»ç®—æ³•ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜BYOCLæ¨¡å‹æ€§èƒ½è¶…è¶Šäº†ä¹‹å‰çš„å•å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>BYOCLæ¨¡å‹æ˜¯é¦–ä¸ªåº”ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œä¸€è‡´åˆ†å‰²çš„æ¨¡å‹ï¼Œæ— éœ€è®­ç»ƒã€‚</li>
<li>BYOCLæ¨¡å‹å…·æœ‰é«˜åº¦çš„çµæ´»æ€§ï¼Œå¯ä»¥é€‚åº”ä»»æ„æ½œåœ¨ç©ºé—´ï¼Œå³æ’å³ç”¨ã€‚</li>
<li>BYOCLæ¨¡å‹æé«˜äº†å•å›¾åƒåˆ†å‰²æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9745264707f911e19ae55dc21e00fe1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cd1bd805e8c8844c2ff0d120e0f32bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5744e91b7d6662d5f187f320c0782e29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89a4ccf1f10c66f8c1c3e8ef8093ae70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65b4dd1c91fd7c2e988074a0ee80ed51.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Multi-view-Hybrid-Graph-Convolutional-Network-for-Volume-to-mesh-Reconstruction-in-Cardiovascular-MRI"><a href="#Multi-view-Hybrid-Graph-Convolutional-Network-for-Volume-to-mesh-Reconstruction-in-Cardiovascular-MRI" class="headerlink" title="Multi-view Hybrid Graph Convolutional Network for Volume-to-mesh   Reconstruction in Cardiovascular MRI"></a>Multi-view Hybrid Graph Convolutional Network for Volume-to-mesh   Reconstruction in Cardiovascular MRI</h2><p><strong>Authors:NicolÃ¡s Gaggion, Benjamin A. Matheson, Yan Xia, Rodrigo Bonazzola, Nishant Ravikumar, Zeike A. Taylor, Diego H. Milone, Alejandro F. Frangi, Enzo Ferrante</strong></p>
<p>Cardiovascular magnetic resonance imaging is emerging as a crucial tool to examine cardiac morphology and function. Essential to this endeavour are anatomical 3D surface and volumetric meshes derived from CMR images, which facilitate computational anatomy studies, biomarker discovery, and in-silico simulations. Traditional approaches typically follow complex multi-step pipelines, first segmenting images and then reconstructing meshes, making them time-consuming and prone to error propagation. In response, we introduce HybridVNet, a novel architecture for direct image-to-mesh extraction seamlessly integrating standard convolutional neural networks with graph convolutions, which we prove can efficiently handle surface and volumetric meshes by encoding them as graph structures. To further enhance accuracy, we propose a multi-view HybridVNet architecture which processes both long axis and short axis CMR, showing that it can increase the performance of cardiac MR mesh generation. Our model combines traditional convolutional networks with variational graph generative models, deep supervision and mesh-specific regularisation. Experiments on a comprehensive dataset from the UK Biobank confirm the potential of HybridVNet to significantly advance cardiac imaging and computational cardiology by efficiently generating high-fidelity meshes from CMR images. Multi-view HybridVNet outperforms the state-of-the-art, achieving improvements of up to $\sim$27% reduction in Mean Contour Distance (from 1.86 mm to 1.35 mm for the LV Myocardium), up to $\sim$18% improvement in Hausdorff distance (from 4.74 mm to 3.89mm, for the LV Endocardium), and up to $\sim$8% in Dice Coefficient (from 0.78 to 0.84, for the LV Myocardium), highlighting its superior accuracy. </p>
<blockquote>
<p>å¿ƒè¡€ç®¡ç£å…±æŒ¯æˆåƒæ­£æˆä¸ºä¸€ç§ç”¨äºæ£€æŸ¥å¿ƒè„å½¢æ€å’ŒåŠŸèƒ½çš„é‡è¦å·¥å…·ã€‚åœ¨è¿™ä¸€å·¥ä½œä¸­ï¼Œä»CMRå›¾åƒä¸­å¾—å‡ºçš„è§£å‰–ä¸‰ç»´è¡¨é¢å’Œä½“ç§¯ç½‘æ ¼å°¤ä¸ºå…³é”®ï¼Œè¿™äº›ç½‘æ ¼ä¿ƒè¿›äº†è®¡ç®—è§£å‰–å­¦ç ”ç©¶ã€ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°å’Œè®¡ç®—æœºæ¨¡æ‹Ÿã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éµå¾ªå¤æ‚çš„å¤šæ­¥ç®¡é“ï¼Œé¦–å…ˆåˆ†å‰²å›¾åƒï¼Œç„¶åé‡å»ºç½‘æ ¼ï¼Œè¿™ä½¿å¾—å®ƒä»¬è€—æ—¶ä¸”å®¹æ˜“å‡ºé”™ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬å¼•å…¥äº†HybridVNetï¼Œè¿™æ˜¯ä¸€ç§ç›´æ¥å›¾åƒåˆ°ç½‘æ ¼æå–çš„æ–°å‹æ¶æ„ï¼Œæ— ç¼é›†æˆäº†æ ‡å‡†å·ç§¯ç¥ç»ç½‘ç»œå’Œå›¾å·ç§¯ã€‚æˆ‘ä»¬å·²ç»è¯æ˜å®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†è¡¨é¢å’Œä½“ç§¯ç½‘æ ¼ï¼Œé€šè¿‡å°†å®ƒä»¬ç¼–ç ä¸ºå›¾å½¢ç»“æ„æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šè§†è§’HybridVNetæ¶æ„ï¼Œè¯¥æ¶æ„æ—¢å¤„ç†é•¿è½´CMRåˆå¤„ç†çŸ­è½´CMRï¼Œè¡¨æ˜å®ƒå¯ä»¥æé«˜å¿ƒè„MRç½‘æ ¼ç”Ÿæˆçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»“åˆäº†ä¼ ç»Ÿå·ç§¯ç½‘ç»œã€å˜å›¾ç”Ÿæˆæ¨¡å‹ã€æ·±åº¦ç›‘ç£å’Œç½‘æ ¼ç‰¹å®šæ­£åˆ™åŒ–ã€‚åœ¨è‹±å›½ç”Ÿç‰©é“¶è¡Œç»¼åˆæ•°æ®é›†ä¸Šçš„å®éªŒè¯å®äº†HybridVNetçš„æ½œåŠ›ï¼Œå®ƒå¯ä»¥é€šè¿‡ä»CMRå›¾åƒé«˜æ•ˆç”Ÿæˆé«˜ä¿çœŸç½‘æ ¼æ¥æ¨åŠ¨å¿ƒè„æˆåƒå’Œè®¡ç®—å¿ƒè„ç—…å­¦çš„å‘å±•ã€‚å¤šè§†è§’HybridVNetè¡¨ç°ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œå¹³å‡è½®å»“è·ç¦»å‡å°‘äº†çº¦27%ï¼ˆå·¦å¿ƒå®¤å¿ƒè‚Œä»1.86æ¯«ç±³é™è‡³1.35æ¯«ç±³ï¼‰ï¼Œè±ªæ–¯å¤šå¤«è·ç¦»æé«˜äº†çº¦18%ï¼ˆå·¦å¿ƒå®¤å†…è†œä»4.74æ¯«ç±³é™è‡³3.89æ¯«ç±³ï¼‰ï¼Œè¿ªæ°æ–¯ç‰¹æ‹‰ç³»æ•°æé«˜äº†çº¦8%ï¼ˆå·¦å¿ƒå®¤å¿ƒè‚Œä»0.78æé«˜åˆ°0.84ï¼‰ï¼Œçªæ˜¾äº†å…¶å“è¶Šå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.13706v3">PDF</a> </p>
<p><strong>Summary</strong><br>     å¿ƒè¡€ç®¡ç£å…±æŒ¯æˆåƒï¼ˆCMRï¼‰æ˜¯æ£€æŸ¥å¿ƒè„å½¢æ€å’ŒåŠŸèƒ½çš„é‡è¦å·¥å…·ã€‚æœ¬ç ”ç©¶å¼•å…¥HybridVNetï¼Œä¸€ç§ç›´æ¥å›¾åƒåˆ°ç½‘æ ¼æå–çš„æ–°æ¶æ„ï¼Œé›†æˆæ ‡å‡†å·ç§¯ç¥ç»ç½‘ç»œå’Œå›¾å·ç§¯ï¼Œèƒ½é«˜æ•ˆå¤„ç†è¡¨é¢å’Œä½“ç§¯ç½‘æ ¼ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜å‡†ç¡®æ€§ï¼Œæå‡ºå¤šè§†è§’HybridVNetæ¶æ„ï¼Œå¤„ç†é•¿è½´å’ŒçŸ­è½´CMRå›¾åƒï¼Œç»“åˆä¼ ç»Ÿå·ç§¯ç½‘ç»œã€å˜åˆ†å›¾ç”Ÿæˆæ¨¡å‹ã€æ·±åº¦ç›‘ç£å’Œç½‘æ ¼ç‰¹å®šæ­£åˆ™åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒHybridVNetåœ¨å¿ƒè„MRç½‘æ ¼ç”Ÿæˆæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè¡€ç®¡ç£å…±æŒ¯æˆåƒï¼ˆCMRï¼‰æ˜¯æ£€æŸ¥å¿ƒè„å½¢æ€å’ŒåŠŸèƒ½çš„å…³é”®å·¥å…·ã€‚</li>
<li>CMRå›¾åƒä¸­çš„è§£å‰–3Dè¡¨é¢å’Œä½“ç§¯ç½‘æ ¼å¯¹äºè®¡ç®—è§£å‰–å­¦ã€ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°å’Œè®¡ç®—æœºæ¨¡æ‹Ÿè‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿçš„å›¾åƒåˆ°ç½‘æ ¼æå–æ–¹æ³•å¤æ‚ä¸”è€—æ—¶ï¼Œæ˜“å‡ºé”™ã€‚</li>
<li>HybridVNetæ¶æ„èƒ½ç›´æ¥ä»CMRå›¾åƒä¸­æå–ç½‘æ ¼ï¼Œé›†æˆå·ç§¯ç¥ç»ç½‘ç»œå’Œå›¾å·ç§¯ã€‚</li>
<li>å¤šè§†è§’HybridVNetæ¶æ„èƒ½æé«˜å¿ƒè„MRç½‘æ ¼ç”Ÿæˆçš„æ€§èƒ½ã€‚</li>
<li>HybridVNetç»“åˆä¼ ç»Ÿå·ç§¯ç½‘ç»œã€å˜åˆ†å›¾ç”Ÿæˆæ¨¡å‹ã€æ·±åº¦ç›‘ç£å’Œç½‘æ ¼ç‰¹å®šæ­£åˆ™åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.13706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b74d4f8cfc473f9619da17d14d93e8ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a86fd08e5ca53fe793237c66aded830d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-caf73a308ee44dc6809f0a183f397122.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-471782981d01d8ce3932599781e2b7a1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-12/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e6d70814353477fd06bcd1c535227952.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  Empowering Global Voices A Data-Efficient, Phoneme-Tone Adaptive   Approach to High-Fidelity Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-12/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ed2e095b9e8263cc566dfb72ce8d86af.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-12  PRAD Periapical Radiograph Analysis Dataset and Benchmark Model   Development
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
